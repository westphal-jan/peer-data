{"id": "1502.06895", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "On the consistency theory of high dimensional variable screening", "abstract": "The performance of variable screening depends on both the computing power and the ability to drastically reduce the number of variables without discarding the important ones. If the data dimension $p $is substantially larger than the sample size $n $, variable screening becomes critical, since 1) faster selection algorithms for the characteristics are required; 2) Conditions that guarantee consistency of selection may not hold.\\ par This article examines a class of linear screening methods and establishes the consistency theory for this particular class. Specifically, we demonstrate the weak diagonally dominant (WDD) condition as a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods $SIS $and $HOLP $as strong screening consistency (subject to additional limitations) with high probability if $n; O\\ ho / ho (condition for detection) and the condition for detection is unlimited.", "histories": [["v1", "Tue, 24 Feb 2015 17:52:20 GMT  (19kb)", "http://arxiv.org/abs/1502.06895v1", "Screening theory"], ["v2", "Tue, 31 Mar 2015 18:38:42 GMT  (19kb)", "http://arxiv.org/abs/1502.06895v2", "Correction on the rate of $\\kappa$ in Lemma 4 and Theorem 6"], ["v3", "Sat, 6 Jun 2015 07:33:02 GMT  (21kb)", "http://arxiv.org/abs/1502.06895v3", "adding comments on REC"]], "COMMENTS": "Screening theory", "reviews": [], "SUBJECTS": "math.ST cs.LG stat.ML stat.TH", "authors": ["xiangyu wang", "chenlei leng", "david b dunson"], "accepted": true, "id": "1502.06895"}, "pdf": {"name": "1502.06895.pdf", "metadata": {"source": "CRF", "title": "On the consistency theory of high dimensional variable screening", "authors": ["Xiangyu Wang", "Chenlei Leng", "David Dunson"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n06 89\n5v 1\n[ m\nat h.\nST ]\n2 4\nFe b\nVariable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold.\nThis article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the weak diagonally dominant (WDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS andHOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((\u03c1s + \u03c3/\u03c4)2 log p) under random designs. In addition, we relate the WDD condition to the irrepresentable condition, and highlight limitations of SIS."}, {"heading": "1 Introduction", "text": "The rapidly growing data dimension has brought new challenges to statistical variable selection, a crucial technique for identifying important variables to facilitate interpretation and improve prediction accuracy. Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010). Regularized methods can consistently recover the support of coefficients, i.e., the non-zero signals, via optimizing regularized loss functions under certain conditions (Zhao and Yu, 2006; Wainwright, 2009;\nLee et al., 2013). However, in the big data era when p far exceeds n, such regularized methods might fail due to two reasons. First, the conditions that guarantee variable selection consistency might fail to hold when p >> n; Second, the computation burden of the corresponding optimization problem increases dramatically with large p.\nBearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique \u201cSure Independence Screening\u201d (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman\u2019s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption. Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance. However, such improvement is limited by the extra computational burden caused by their iterative framework, which is reported to be high when p is large (Wang and Leng, 2013). To ameliorate concerns in both screening performance and computational efficiency, Wang and Leng (2013) develop a new type of screening method termed \u201cHigh-dimensional ordinary least-square projection\u201d (HOLP ). This new screener relaxes the strong marginal assumption required by SIS and can be computed efficiently (complexity is O(n2p)), thus scalable to ultra-high dimensionality.\nThis article focuses on linear models for tractability. As computation is one vital concern for designing a good screening method, we primarily focus on a class of linear screeners that can be efficiently computed, and study their theoretical properties. The main contributions of this article lie in three aspects.\n1. We define the notion of strong screening consistency to provide a unified framework\nfor analyzing screening methods. In particular, we show a necessary and sufficient condition for a screening method to be strong screening consistent is that the screening matrix is weak diagonally dominant (WDD). This condition gives insights into the design of screening matrices, while providing a framework to assess the effectiveness of screening methods.\n2. We relate WDD to the irrepresentable condition (Zhao and Yu, 2006) that is necessary\nand sufficient for sign consistency of lasso (Tibshirani, 1996). In contrast to the irrepresentable condition (IC) that is specific to the design matrix, WDD involves another ancillary matrix that can be chosen arbitrarily. Such flexibility allows WDD to hold even when IC fails if the ancillary matrix is carefully chosen (as in HOLP ). When the ancillary matrix is chosen as the design matrix, certain equivalence is shown between WDD and IC, revealing the difficulty for SIS to achieve screening consistency.\n3. We study the behavior of SIS and HOLP under random designs, and prove that\na sample size of n = O ( (\u03c1s + \u03c3/\u03c4)2 log p ) is sufficient for SIS and HOLP to be screening consistent, where s is the sparsity, \u03c1 measures the diversity of signals and \u03c4/\u03c3 evaluates the signal-to-noise ratio. This is to be compared to the sign consistency results in Wainwright (2009) where the design matrix is fixed and assumed to follow the irrepresentable condition.\nThe article is organized as follows. In Section 1, we set up the basic problem and describe the framework of variable screening. In Section 2, we provide a deterministic necessary and sufficient condition for consistent screening. Its relationship with the irrepresentable condition is discussed in Section 3. In Section 4, we prove the consistency of SIS and HOLP under random designs by showing the WDD condition is satisfied with large probability, although the requirement on SIS is much more restictive."}, {"heading": "2 Linear screening", "text": "Consider the usual linear regression\nY = X\u03b2 + \u01eb,\nwhere Y is the n\u00d7 1 response vector, X is the n\u00d7 p design matrix and \u01eb is the noise. The regression task is to learn the coefficient vector \u03b2. In the high dimensional setting where p >> n, a sparsity assumption is often imposed on \u03b2 so that only a small portion of the coordinates are non-zero. Such an assumption splits the task of learning \u03b2 into two phases. The first is to recover the support of \u03b2, i.e., the location of non-zero coefficients; The second is to estimate the value of these non-zero signals. This article mainly focuses on the first phase.\nAs pointed out in the introduction, when the dimensionality is too high, using regularization methods methods raises concerns both computationally and theoretically. To reduce the dimensionality, Fan and Lv (2008) suggest a variable screening framework by finding a\nsubmodel\nMd = {i : |\u03b2\u0302i| is among the largest d coordinates of |\u03b2\u0302|} or M\u03b3 = {i : |\u03b2\u0302i| > \u03b3}.\nLet Q = {1, 2, \u00b7 \u00b7 \u00b7 , p} and define S as the true model with s = |S| being its cardinarlity. The hope is that the submodel size |Md| or |M\u03b3| will be smaller or comparable to n, while S \u2286 Md or S \u2286 M\u03b3. To achieve this goal two steps are usually involved in the screening analysis. The first is to show there exists some \u03b3 such that mini\u2208S |\u03b2\u0302i| > \u03b3 and the second step is to bound the size of |M\u03b3| such that |M\u03b3| = O(n). To unify these steps for a more comprehensive theoretical framework, we put forward a slightly stronger definition of screening consistency in this article.\nDefinition 2.1. (Strong screening consistency) An estimator \u03b2\u0302 (of \u03b2) is strong screening consistent if it satisfies that\nmin i\u2208S |\u03b2\u0302i| > max i 6\u2208S |\u03b2\u0302i| (1)\nand\nsign(\u03b2\u0302i) = sign(\u03b2i), \u2200i \u2208 S. (2)\nRemark 2.1. This definition does not differ much from the usual screening property studied in the literature, which requires mini\u2208S |\u03b2\u0302i| > max(n\u2212s)i 6\u2208S |\u03b2\u0302i|, where max(k) denotes the kth largest item.\nThe key of strong screening consistency is the property (1) that requires the estimator to preserve consistent ordering of the zero and non-zero coefficients. It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for \u03b2\u0302i, i 6\u2208 S is needed. As shown later, such relaxation tremendously reduces the restriction on the design matrix, and allows screening methods to work for a broader choice of X .\nThe focus of this article is to study the theoretical properties of a special class of screeners\nthat take the linear form as\n\u03b2\u0302 = AY\nfor some p \u00d7 n ancillary matrix A. Examples include sure independence screening (SIS) where A = XT/n and high-dimensional ordinary least-square projection (HOLP ) where A = XT (XXT )\u22121. We choose to study the class of linear estimators because linear screening\nis computationally efficient and theoretically tractable. We note that the usual ordinary least-squares estimator is also a special case of linear estimators although it is not well defined for p > n."}, {"heading": "3 Deterministic guarantees", "text": "In this section, we derive the necessary and sufficient condition that guarantees \u03b2\u0302 = AY to be strong screening consistent. The design matrix X and the error \u01eb are treated as fixed in this section and we will investigate random designs later. We consider the set of sparse coefficient vectors defined by\nB(s, \u03c1) = { \u03b2 \u2208 Rp : |supp(\u03b2)| \u2264 s, maxi\u2208supp(\u03b2) |\u03b2i| mini\u2208supp(\u03b2) |\u03b2i| \u2264 \u03c1 } .\nThe set B(s, \u03c1) contains vectors having at most s non-zero coordinates with the ratio of the largest and smallest coordinate bounded by \u03c1. Before proceeding to the main result of this section, we introduce some terminology that helps to establish the theory.\nDefinition 3.1. (Weak diagonally dominant matrix) A p \u00d7 p symmetric matrix \u03a6 is weak diagonally dominant with sparsity s if for any I \u2286 Q, |I| \u2264 s\u2212 1 and i \u2208 Q \\ I\n\u03a6ii > C0max\n{\n\u2211 j\u2208I |\u03a6ij + \u03a6kj|, \u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|\n}\n+ |\u03a6ik| \u2200k 6= i, k \u2208 Q \\ I,\nwhere C0 \u2265 1 is a constant.\nNotice this definition implies that for i \u2208 Q \\ I\n\u03a6ii \u2265 C0 ( \u2211\nj\u2208I |\u03a6ij + \u03a6kj|+\n\u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|\n)\n/2 \u2265 C0 \u2211\nj\u2208I |\u03a6ij |, (3)\nwhich is related to the usual diagonally dominant matrix. The weak diagonally dominant matrix provides a necessary and sufficient condition for any linear estimators \u03b2\u0302 = AY to be strong screening consistent. More precisely, we have the following result.\nTheorem 1. For the noiseless case where \u01eb = 0, a linear estimator \u03b2\u0302 = AY is strong screening consistent for every \u03b2 \u2208 B(s, \u03c1), if and only if the screening matrix \u03a6 = AX is weak diagonally dominant with sparsity s and C0 \u2265 \u03c1.\nThe noiseless case is a good starting point to analyze \u03b2\u0302. Intuitively, in order to preserve the correct order of the coefficients in \u03b2\u0302 = AX\u03b2, one needs AX to be close to a diagonally\ndominant matrix, so that \u03b2\u0302i, i \u2208 MS will take advantage of the large diagonal terms in AX to dominate \u03b2\u0302i, i 6\u2208 MS that is just linear combinations of off-diagonal terms.\nWhen noise is considered, the condition in Theorem 1 needs to be changed slightly to accommodate extra discrepancies. In addition, the smallest non-zero coefficient has to be lower bounded to ensure a certain level of signal-to-noise ratio. Thus, we augment our previous definition of B(s, \u03c1) to have a signal strength control\nB\u03c4 (s, \u03c1) = {\u03b2 \u2208 B(s, \u03c1)| min i\u2208supp(\u03b2) |\u03b2i| \u2265 \u03c4}.\nThen we can obtain the following modified Theorem.\nTheorem 2. With noise, the linear estimator \u03b2\u0302 = AY is strong screening consistent for every \u03b2 \u2208 B\u03c4 (s, \u03c1) if \u03a6 = AX \u2212 2\u03c4\u22121\u2016A\u01eb\u2016\u221eIp is weak diagonally dominant with sparsity s and C0 \u2265 \u03c1.\nThe condition in Theorem 2 can be further tailored to a necessary and sufficient version with extra manipulation on the noise term. Nevertheless, this might not be useful in practice due to the randomness in noise. In addition, the current version of Theorem 2 is already tight in the sense that there exists some noise vector \u01eb such that the condition in Theorem 2 is also necessary for strong screening consistency.\nTheorems 1 and 2 establish ground rules for verifying consistency of a given screener and provide practical guidance for screening design. In Section 4, we consider some concrete examples of ancillary matrix A and prove that conditions in Theorems 1 and 2 are satisfied by the corresponding screeners with large probability under random designs."}, {"heading": "4 Relationship with the irrepresentable condition", "text": "The weak diagonally dominant (WDD) condition is closely related to the strong irrepresentable condition (IC) proposed in Zhao and Yu (2006) as a necessary and sufficient condition for sign consistency of lasso. Assume each column of X is standardized to have mean zero. Letting C = XTX/n and \u03b2 be a given coefficient vector, the IC is expressed as\n\u2016CSc,SC\u22121S,S \u00b7 sign(\u03b2S)\u2016\u221e \u2264 1\u2212 \u03b8 (4)\nfor some \u03b8 > 0, where CA,B represents the sub-matrix of C with row indices in A and column indices in B. The authors enumerate several scenarios of C such that IC is satisfied. Following result verifies some of these scenarios for screening matrix \u03a6.\nCorollary 1. If \u03a6ii = 1, \u2200i and |\u03a6ij| < c/(2s), \u2200i 6= j for some 0 \u2264 c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 1/c. If |\u03a6ij| < r|i\u2212j|, \u2200i, j for some 0 < r < 1 as defined in Corollary 3 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 (1\u2212r)2/(4r).\nA more explicit but nontrivial relationship between IC and WDD is illustrated below\nwhen |S| = 2.\nTheorem 3. Assume \u03a6ii = 1, \u2200i and |\u03a6ij | < r, \u2200i 6= j. If \u03a6 is weak diagonally dominant with sparsity 2 and C0 \u2265 \u03c1, then \u03a6 satisfies\n\u2016\u03a6Sc,S\u03a6\u22121S,S \u00b7 sign(\u03b2S)\u2016\u221e \u2264 \u03c1\u22121\n1\u2212 r\nfor all \u03b2 \u2208 B(2, \u03c1). On the other hand, if \u03a6 satisfies the irrepresentable condition for all \u03b2 \u2208 B(2, \u03c1) for some \u03b8, then \u03a6 is a weak diagonally dominant matrix with sparsity 2 and\nC0 \u2265 1 1\u2212 \u03b8 1\u2212 r 1 + r .\nTheorem 3 demonstrates certain equivalence between IC and WDD. However, it is worth noting that IC is directly imposed on the covariance matrix XTX/n. This makes IC a strong assumption that is easily violated; for example, when the predictors are highly correlated. In contrast to IC, WDD is imposed on matrix AX where there is still flexibility for choosing A. As we show in Section 4, the ancillary matrix A defined in HOLP satisfies WDD even when predictors are highly correlated and IC fails to hold. Therefore, WDD is a weaker constraint in some sense.\nFor sure independence screening, the screening matrix \u03a6 = XTX/n coincides with the covariance matrix, making WDD and IC effectively equivalent. The following theorem formalizes this.\nTheorem 4. Let A = XT/n and standardize columns of X to have sample variance one. Assume X satisfies the sparse Riesz condition (Zhang and Huang, 2008), i.e,\nmin \u03c0\u2286Q, |\u03c0|\u2264s\n\u03bbmin(X T \u03c0X\u03c0/n) \u2265 \u00b5,\nfor some \u00b5 > 0. Now if AX is weak diagonally dominant with sparsity s + 1 and C0 \u2265 \u03c1 with \u03c1 > \u221a s/\u00b5, then X satisfies the irrepresentable condition for any \u03b2 \u2208 B(s, \u03c1).\nIn other words, under the condition \u03c1 > \u221a s/\u00b5, the strong screening consistency of SIS\nfor B(s+ 1, \u03c1) implies the model selection consistency of lasso for B(s, \u03c1).\nTheorem 4 illustrates the difficulty of SIS. The necessary condition that guarantees good screening performance of SIS also guarantees the model selection consistency of lasso. However, such a strong necessary condition does not mean that SIS should be avoided in practice given its substantial advantages in terms of simplicity and computational efficiency. The strong screening consistency defined in this article is stronger than conditions commonly used in justifying screening procedures as in Fan and Lv (2008)."}, {"heading": "5 Screening under random designs", "text": "In this section, we consider linear screening under random designs when X and \u01eb are Gaussian. The theory developed in this section can be easily extended to a broader family of distributions, for example, where \u01eb follows a sub-Gaussian distribution (Vershynin, 2010) and X follows an elliptical distribution (Fan and Lv, 2008; Wang and Leng, 2013). We focus on the Gaussian case for conciseness. Let \u01eb \u223c N(0, \u03c32) and X \u223c N(0,\u03a3). We prove the screening consistency of SIS and HOLP by verifying the condition in Theorem 2. Recall the ancillary matrices for SIS and HOLP are defined respectively as\nASIS = X T/n, AHOLP = X T (XXT )\u22121.\nFor simplicity, we assume \u03a3ii = 1 for i = 1, 2, \u00b7 \u00b7 \u00b7 , p. To verify the WDD condition, it is essential to quantify the magnitude of the entries of AX and A\u01eb.\nLemma 1. Let \u03a6 = ASISX, then for any t > 0 and i 6= j \u2208 Q, we have\nP\n( |\u03a6ii \u2212 \u03a3ii| \u2265 t ) \u2264 2 exp { \u2212min ( t2n 8e2K , tn 2eK )} ,\nand\nP\n( |\u03a6ij \u2212 \u03a3ij | \u2265 t ) \u2264 6 exp { \u2212min ( t2n 72e2K , tn 6eK )} ,\nwhere K = \u2016X 2(1) \u2212 1\u2016\u03c81 is a constant, X 2(1) is a chi-square random variable with one degree of freedom and the norm \u2016 \u00b7 \u2016\u03c81 is defined in Vershynin (2010).\nLemma 1 states that the screening matrix \u03a6 = ASISX for SIS will eventually converge to the covariance matrix \u03a3 in l\u221e when n tends to infinity and log p = o(n). Thus, the screening performance of SIS strongly relies on the structure of \u03a3. In particular, the (asymptotically) necessary and sufficient condition for SIS being strong screening consistent is \u03a3 satisfying the WDD condition. For the noise term, we have the following lemma.\nLemma 2. Let \u03b7 = ASIS\u01eb. For any t > 0 and i \u2208 Q, we have\nP (|\u03b7i| \u2265 \u03c3t) \u2264 6 exp { \u2212min ( t2n 72e2K , tn 6eK )} ,\nwhere K is defined the same as in Lemma 1.\nThe proof of Lemma 2 is essentially the same as the proof of off-diagonal terms in Lemma 1 and is thus omitted. As indicated before, the necessary and sufficient condition for SIS to be strong screening consistent is that \u03a3 follows WDD. As WDD is usually hard to verify, we consider a stronger sufficient condition inspired by Corollary 1.\nTheorem 5. Let r = maxi 6=j |\u03a3ij|. If r < 12\u03c1s , then for any \u03b4 > 0, if the sample size satisfies\nn > 144K\n(\n1 + 2\u03c1s+ 2\u03c3/\u03c4\n1\u2212 2\u03c1sr\n)2\nlog(3p/\u03b4), (5)\nwhereK is defined in Lemma 1, then with probability at least 1\u2212\u03b4, \u03a6 = ASISX\u22122\u03c4\u22121\u2016ASIS\u01eb\u2016\u221eIp is weak diagonally dominant with sparsity s and C0 \u2265 \u03c1. In other words, SIS is screening consistent for any \u03b2 \u2208 B\u03c4 (s, \u03c1).\nThe requirement that maxi 6=j |\u03a3ij | < 1/(\u03c1sr) or the necessary and sufficient condition that \u03a3 is WDD strictly constrains the correlation structure of X , causing the difficulty for SIS to be strong screening consistent. Moreover, these constraints are independent of sample sizes and cannot be overcome by large n. As shown below, HOLP relaxes these constraints by a constraint on the conditional number \u03ba = \u03bbmax(\u03a3)/\u03bbmin(\u03a3). For HOLP we instead have the following result.\nLemma 3. Let \u03a6 = AHOLPX. Assume p > c0n for some c0 > 1, then for any C > 0 there exists some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i \u2208 Q, j 6= i, we have\nP\n(\n|\u03a6ii| < c1\u03ba\u22121 n\np\n) \u2264 2e\u2212Cn, P ( |\u03a6ii| > c2\u03ba n\np\n)\n\u2264 2e\u2212Cn\nand\nP\n(\n|\u03a6ij | > c4\u03ba 3 2 t\n\u221a n\np\n)\n\u2264 5e\u2212Cn + 2e\u2212t2/2,\nwhere c4 = \u221a c2(c0\u2212c1)\u221a c3(c0\u22121) .\nLemma 3 quantifies the entries of the screening matrix for HOLP . As illustrated in the\nlemma, regardless of the covariance \u03a3, diagonal terms of \u03a6 are always O(n p ) and the offdiagonal terms are O( \u221a n p ). Thus, with n \u2265 O(s2), \u03a6 is likely to satisfy the WDD condition with large probability. For the noise vector we have the following result.\nLemma 4. Let \u03b7 = AHOLP \u01eb. Assume p > c0n for some c0 > 1, then for any C > 0 there exist the same c1, c2, c3 as in Lemma 3 such that for any t > 0 and i \u2208 Q,\nP\n(\n|\u03b7i| \u2265 2\u03c3\n\u221a c2\u03bat\n1\u2212 c\u221210\n\u221a n\np\n)\n< 4e\u2212Cn + 2e\u2212t 2/2,\nif n \u2265 8C/(c0 \u2212 1)2.\nThe following theorem results after combining Lemma 3 and 4.\nTheorem 6. Assume p > c0n for some c0 > 1. For any \u03b4 > 0, if the sample size satisfies\nn > max\n{\n2C \u2032\u03ba5(\u03c1s + \u03c3/\u03c4)2 log(3p/\u03b4), 8C (c0 \u2212 1)2 } , (6)\nwhere C \u2032 = max{4c 2 4\nc2 1 , 4c2 c2 1 (1\u2212c\u22121 0 )2 } and c1, c2, c3, c4, C are the same constants defined in Lemma\n3, then with probability at least 1\u2212 \u03b4, \u03a6 = AHOLPX \u2212 2\u03c4\u22121\u2016AHOLP \u01eb\u2016\u221eIp is weak diagonally dominant with sparsity s and C0 \u2265 \u03c1. This implies HOLP is screening consistent for any \u03b2 \u2208 B\u03c4 (s, \u03c1).\nThere are several interesting observations on equation (5) and (6). First, (\u03c1s + \u03c3/\u03c4)2 appears in both expressions, suggesting the optimal screener might also possess a similar sample size requirement in the form of\nnopt = O\n(\n(\u03c1s + \u03c3/\u03c4)2 log(p)\n)\n.\nWe note that \u03c1s evaluates the sparsity and the diversity of the signal \u03b2 while \u03c3/\u03c4 is closely related to the signal-to-noise ratio. Furthermore, HOLP relaxes the correlation constraint r < 1/(2\u03c1s) or the covariance constraint (\u03a3 is WDD) with the conditional number constraint. Thus for any \u03a3, as long as the sample size is large enough, strong screening consistency is assured. Finally, HOLP provides an example to satisfy the WDD condition in answer to the question raised in Section 2."}, {"heading": "6 Concluding remarks", "text": "This article studies the theoretical properties of a class of high dimensional variable screening methods. In particular, we establish a necessary and sufficient condition in the form of weak\ndiagonally dominant screening matrices for strong screening consistency of a linear screener. We verify the condition for both SIS and HOLP under random designs. In addition, we show a close relationship between WDD and the irrepresentable condition, highlighting the difficulty of using SIS in screening for arbitrarily correlated predictors.\nFor future work, it is of interest to see how linear screening can be adapted to compressed sensing (Xue and Zou, 2011) and how techniques such as preconditioning (Jia and Rohe, 2012) can improve the performance of marginal screening and variable selection."}, {"heading": "A Proofs for Section 3", "text": "In this section, we prove the two theorems in Section 3.\nProof of Theorem 1. If \u03a6 is weak diagonally dominant with sparsity s and C0 \u2265 \u03c1, we have for any I \u2286 Q and |I| \u2264 s\u2212 1,\n\u03a6ii > \u03c1max\n{\n\u2211 j\u2208I |\u03a6ij + \u03a6kj|, \u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|\n}\n+ |\u03a6ik| \u2200k 6= i \u2208 Q \\ I.\nRecall \u03b2\u0302 = \u03a6\u03b2. Suppose S is the index set of non-zero predictors. For any i \u2208 S, k 6\u2208 S, of we fix I = S \\ {i}, we have\n|\u03b2\u0302i| = |\u03a6ii\u03b2i + \u2211\nj\u2208I \u03a6ij\u03b2j| \u2265 |\u03b2i|(\u03a6ii +\n\u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6ij)\n= |\u03b2i|(\u03a6ii + \u2211\nj\u2208I\n\u03b2j \u03b2i (\u03a6ij + \u03a6kj) + \u03a6ki \u2212 \u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6kj \u2212 \u03a6ki)\n> \u2212|\u03b2i|( \u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6kj + \u03a6ki) = \u2212 |\u03b2i| \u03b2i ( \u2211 j\u2208I \u03b2j\u03a6kj + \u03b2i\u03a6ki)\n= \u2212sign(\u03b2i) \u00b7 \u03b2\u0302k.\nSimilarly we have\n|\u03b2\u0302i| = |\u03a6ii\u03b2i + \u2211\nj\u2208I \u03a6ij\u03b2j| \u2265 |\u03b2i|(\u03a6ii +\n\u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6ij)\n= |\u03b2i|(\u03a6ii + \u2211\nj\u2208I\n\u03b2j \u03b2i (\u03a6ij \u2212 \u03a6kj)\u2212 \u03a6ki + \u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6kj + \u03a6ki)\n> |\u03b2i|( \u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6kj + \u03a6ki) = sign(\u03b2i) \u00b7 \u03b2\u0302k.\nTherefore, whatever value sign(\u03b2i) is, it always holds that |\u03b2\u0302i| > |\u03b2\u0302k|. Since this result is true for any i \u2208 S, k 6\u2208 S, we have\nmin i\u2208S |\u03b2\u0302i| > max k 6\u2208S |\u03b2\u0302k|.\nTo prove the sign consistency for non-zero coefficients, notice that for i \u2208 S,\n\u03a6ii > \u03c1( \u2211\nj\u2208I |\u03a6ij + \u03a6kj |+\n\u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|)/2 \u2265 \u03c1 \u2211 j\u2208I |\u03a6ij|.\nThus,\n\u03b2\u0302i\u03b2i = \u03a6ii\u03b2 2 i +\n\u2211 j\u2208I \u03a6ij\u03b2j\u03b2i = \u03b2 2 i (\u03a6ii + \u2211 j\u2208I \u03b2j \u03b2i \u03a6ij) > 0.\nOn the other hand, if \u03b2\u0302 is screening consistent, i.e., |\u03b2\u0302i| \u2265 |\u03b2\u0302k| and \u03b2\u0302i\u03b2i \u2265 0, we can construct S = I \u222a {i} for any fixed i, k, I. Without loss of generality, we assume \u03a6ik \u2265 0. If we select \u03b2 such that \u03b2i > 0, then the strong screening consistency implies \u03b2\u0302i > \u03b2\u0302k and \u03b2\u0302i > \u2212\u03b2\u0302k. From \u03b2\u0302i > \u03b2\u0302k we have\n\u03a6ii\u03b2i + \u2211\nj\u2208I \u03a6ij\u03b2j >\n\u2211 j\u2208I \u03a6kj\u03b2j + \u03a6ki\u03b2i.\nBy rearranging terms and selecting \u03b2 \u2208 B(s, \u03c1) as \u03b2i = 1, \u03b2j = \u2212\u03c1 \u00b7 sign(\u03a6ij \u2212 \u03a6kj), j \u2208 S we have\n\u03a6ii > \u2212 \u2211\nj\u2208I (\u03a6ij \u2212 \u03a6kj)\u03b2j + \u03a6ki = \u03c1\n\u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|+ |\u03a6ki|.\nFollowing the same argument on \u03b2\u0302i \u2265 \u2212\u03b2\u0302k with a choice of \u03b2i = 1, \u03b2j = \u2212\u03c1 \u00b7 sign(\u03a6ij + \u03a6kj), j \u2208 S we have\n\u03a6ii > \u03c1 \u2211\nj\u2208I |\u03a6ij + \u03a6kj|+ |\u03a6ki|.\nThis concludes the proof.\nProof of Theorem 2. Proof of Lemma 3 follows almost the same as the sufficiency part of Theorem 1. Notice that now the definition of \u03b2\u0302 becomes\n\u03b2\u0302 = XT (XXT )\u22121X\u03b2 +XT (XXT )\u22121\u01eb.\nIf the condition holds, i.e., for any i \u2208 S, I = S \\ {i} and k 6\u2208 S, we have\n\u03a6ii > \u03c1max\n{\n\u2211 j\u2208I |\u03a6ij + \u03a6kj|, \u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|\n}\n+ |\u03a6ik|+ 2\u03c4\u22121\u2016XT (XXT )\u22121\u01eb\u2016\u221e.\nDefining \u03b7 = XT (XXT )\u22121\u01eb, we have for any i \u2208 S,\n|\u03b2\u0302i| = |\u03a6ii\u03b2i + \u2211\nj\u2208I \u03a6ij\u03b2j + \u03b7i| \u2265 |\u03b2i|(\u03a6ii +\n\u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6ij + \u03b2 \u22121 i \u03b7i)\n= |\u03b2i|(\u03a6ii + \u2211\nj\u2208I\n\u03b2j \u03b2i (\u03a6ij + \u03a6kj) + \u03a6ki + \u03b2 \u22121 i (\u03b7i + \u03b7k)\u2212 \u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6kj \u2212 \u03a6ki \u2212 \u03b2\u22121i \u03b7k)\n> \u2212|\u03b2i|( \u2211\nj\u2208I\n\u03b2j \u03b2i \u03a6kj + \u03a6ki + \u03b2 \u22121 i \u03b7k) = \u2212 |\u03b2i| \u03b2i ( \u2211 j\u2208I \u03b2j\u03a6kj + \u03b2i\u03a6ki + \u03b7k)\n= \u2212sign(\u03b2i) \u00b7 \u03b2\u0302k,\nSimilarly, we can prove |\u03b2\u0302i| > sign(\u03b2i) \u00b7 \u03b2\u0302k, and thus |\u03b2\u0302i| > |\u03b2\u0302k|, which implies that\nmin i\u2208S |\u03b2\u0302i| > max k 6\u2208S |\u03b2\u0302k|.\nThe weak sign consistency is established since\n\u03b2\u0302i\u03b2i = \u03a6ii\u03b2 2 i +\n\u2211 j\u2208I \u03a6ij\u03b2j\u03b2i + \u03b7i\u03b2i = \u03b2 2 i (\u03a6ii + \u2211 j\u2208I \u03b2j \u03b2i \u03a6ij + \u03b2 \u22121 i \u03b7i) > 0,\nfor any \u03b2i 6= 0.\nThe tightness of this theorem is given by the case when \u01eb = 0, for which the condition\nhas already been shown to be necessary and sufficient in Theorem 1."}, {"heading": "B Proofs for Section 4", "text": "In this section, we prove results from Section 4.\nProof of Corollary 1. Letting I \u2286 Q, |I| \u2264 s\u2212 1, we have for any i 6= k \u2208 Q \\ I,\n\u03a6ii\u2212 1\nc max\n{\n\u2211 j\u2208I |\u03a6ij + \u03a6kj |, \u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj |\n}\n+ |\u03a6ik| \u2265 1\u2212 1\nc\n(\n2(s\u2212 1) c 2s + c 2s\n)\n= 1\n2s > 0.\nThis completes the proof for the first case.\nNow for the second case, notice that the sum of an entire row (except the diagonal term)\ncan be bounded by \u2211 j 6=i |\u03a6ij| < 2 \u2211\u221e k=1 r k < 2r 1\u2212r . Therefore, we have\n\u03a6ii\u2212 (1\u2212 r)2\n4r max\n{\n\u2211 j\u2208I |\u03a6ij + \u03a6kj|, \u2211 j\u2208I |\u03a6ij \u2212 \u03a6kj|\n}\n\u2212 |\u03a6ik| > 1\u2212 (1\u2212 r)2\n2r\n\u2211 j 6=i |\u03a6ij | \u2212 r = 0.\nProof of Theorem 3. First, from WDD to IC: Without loss of generality, we assume S = {1, 2}. For any k \u2208 Q \\ S, we have \u2223\n\u2223 \u2223 \u2223 [\u03a6k1 \u03a6k2]\u03a6 \u22121 S, Ssign(\u03b2S)\n\u2223 \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2223 sign(\u03b21)(\u03a6k1 \u2212 \u03a612\u03a6k2) + sign(\u03b22)(\u2212\u03a612\u03a6k1 + \u03a6k2) 1\u2212 \u03a6212 \u2223 \u2223 \u2223 \u2223 .\nThe r.h.s. becomes |\u03a6k1 + \u03a6k2|(1 \u2212 \u03a612)/(1 \u2212 \u03a6212) when sign(\u03b21) = sign(\u03b22) and |\u03a6k1 \u2212\n\u03a6k2|(1 + \u03a612)/(1\u2212 \u03a6212) when sign(\u03b21) = \u2212sign(\u03b22). In either case we have\n\u2223 \u2223 \u2223 \u2223 [\u03a6k1 \u03a6k2]\u03a6 \u22121 S, Ssign(\u03b2S) \u2223 \u2223 \u2223 \u2223 \u2264 max\n{ |\u03a61k + \u03a62k|, |\u03a61k \u2212 \u03a62k| }\n1\u2212 r < \u03c1\u22121 1\u2212 r .\nSecond, from IC to WDD: Let I \u2286 Q, |I| = 1 and i 6= k \u2208 Q \\ I. Without loss of generality, we assume i = 1, k = 2, and we construct S = {1, 2}. Now for any j \u2208 I, using the same formula as shown above, we have\n1\u2212 \u03b8 \u2265 \u2223 \u2223 \u2223\n\u2223\n[\u03a6j1 \u03a6j2]\u03a6 \u22121 S, Ssign(\u03b2S)\n\u2223 \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2223 sign(\u03b21)(\u03a6j1 \u2212 \u03a612\u03a6j2) + sign(\u03b22)(\u2212\u03a612\u03a6j1 + \u03a6j2) 1\u2212 \u03a6212 \u2223 \u2223 \u2223 \u2223 .\nUsing the same result on the r.h.s., i.e., it becomes |\u03a6k1 + \u03a6k2|(1 \u2212 \u03a612)/(1 \u2212 \u03a6212) when sign(\u03b21) = sign(\u03b22) and |\u03a6k1\u2212\u03a6k2|(1+\u03a612)/(1\u2212\u03a6212) when sign(\u03b21) = \u2212sign(\u03b22), we have for any j \u2208 I that\nmax\n{ |\u03a61j + \u03a62j |, |\u03a61j \u2212 \u03a62j | } \u2264 (1\u2212 \u03b8)(1 + r).\nAs a result, we have\n\u2211 j\u2208I max\n{ |\u03a61j + \u03a62j |, |\u03a61j \u2212 \u03a62j | } < (1\u2212 \u03b8)(1 + r) < (1\u2212 \u03b8)1 + r 1\u2212 r ( \u03a611 \u2212 |\u03a612| ) ,\nwhich implies\n\u03a611 > 1 1\u2212 \u03b8 1\u2212 r 1 + r \u2211\nj\u2208I max\n{ |\u03a61j + \u03a62j |, |\u03a61j \u2212 \u03a62j | } + |\u03a612|.\nProof of Theorem 4. We just need to check (4). We prove the absolute value of the first coordinate of CSc, SC \u22121 S, S \u00b7 sign(\u03b2S) is less than one, and the rest just follow the same argument. From the condition we know C = XTX/n is weak diagonally dominant. Then equation (3) implies that for any I \u2286 Q with |I| = s, we have for any k 6\u2208 I,\n\u03c1 \u2211\ni\u2208I |Cki| < 1.\nNow for any S \u2286 Q with |S| = s, we choose I = S and let \u03b1T be the first row of CSc, S =\nXTScXS/n, we have\n|\u03b1T (XTSXS/n)\u22121sign(\u03b2S)| \u2264 \u2016\u03b1\u20162\u2016sign(\u03b2S)\u20162\u00b5\u22121.\nBecause \u03c1 \u2211s i=1 |\u03b1i| < 1, we have\n\u03c12 s \u2211\ni=1\n\u03b12i < \u03c1 2(\ns \u2211\ni=1\n|\u03b1i|)2 < 1,\nwhich implies that\n|\u03b1T (XTSXS/n)\u22121sign(\u03b2S)| \u2264 \u03c1\u22121 \u221a s\u00b5\u22121 =\n\u221a s \u03c1\u00b5 < 1."}, {"heading": "C Proofs for Section 5 (SIS)", "text": "Proofs in Section 6 are divided into two parts. In this section, we provide the proofs related to SIS, and leave those pertaining to HOLP to the next section. The proof requires the following proposition,\nProposition 1. Assume Xi \u223c X 2(1), i = 1, 2, \u00b7 \u00b7 \u00b7 , n, where X 2(1) is the chi-square distribution with one degree of freedom. Then for any t > 0, we have\nP (| \u2211n\ni=1Xi n \u2212 1| \u2265 t) \u2264 2 exp { \u2212min ( t2n 8e2K , tn 2eK )} ,\nwhere K = \u2016X 2(1)\u2212 1\u2016\u03c81. Alternatively, for any C > 0, there exists some 0 < c3 < 1 < c4 such that,\nP ( \u2211n i=1Xi n \u2264 c3) \u2264 e\u2212Cn, (7)\nand\nP ( \u2211n i=1Xi n \u2265 c4) \u2264 e\u2212Cn.\nProof. It is a direct application of Proposition 5.16 in Vershynin (2010). Notice that in the proof of Proposition 5.16 we have C = 2e2 and c = e/2 for X 2(1)\u2212 1.\nProof of Lemma 1. For diagonal term we have for any i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , p}\n\u03a6ii \u2212 \u03a3ii = \u2211n k=1 x 2 ik\nn \u2212 1,\nwhere xik, k = 1, 2, \u00b7 \u00b7 \u00b7 , n\u2019s are n iid standard normal random variables. Using Proposition 1, we have for any t > 0,\nP\n( |\u03a6ii \u2212 \u03a3ii| \u2265 t ) \u2264 2 exp { \u2212min ( t2n 8e2K , tn 2eK )} . (8)\nFor the off-diagonal term, we have for any i 6= j,\n\u03a6ij \u2212 \u03a3ij = \u2211n k=1 xikxjk n \u2212 \u03a3ij\n=\n\u2211n k=1(xik + xjk) 2 2n \u2212 \u2211n k=1 x 2 ik 2n \u2212 \u2211n k=1 x 2 jk 2n \u2212 \u03a3ij\n= 1\n2\n(\u2211n k=1(xik + xjk) 2\nn \u2212 (2 + 2\u03a3ij)\n)\n\u2212 1 2\n(\u2211n k=1 x 2 ik n \u2212 1 ) \u2212 1 2 ( \u2211n k=1 x 2 jk n \u2212 1 ) .\nNotice that xik + xjk \u223c N(0, 2 + 2\u03a3ij). Hence the three terms in the above equation can be bounded using the same inequality before, i.e., for any t > 0,\nP\n( |\u03a6ij \u2212 \u03a3ij | \u2265 (2 + \u03a3ij)t ) \u2264 6 exp { \u2212min ( t2n 8e2 , tn 2e )} .\nClearly, we have \u03a3ij \u2264 \u221a \u03a3ii \u221a \u03a3jj \u2264 1. Therefore, we have\nP\n( |\u03a6ij \u2212 \u03a3ij | \u2265 t ) \u2264 6 exp { \u2212min ( t2n 72e2K , tn 6eK )} .\nProof of Lemma 2. The proof is essentially the same for proving the off diagonal terms of \u03a6 as in Lemma 1. The only difference is that E(\u03a6ij) = \u03a3ij while E(X\u01eb) = 0. Note\n\u03b7i/\u03c3 =\n\u2211n k=1 xik\u01ebk/\u03c3\nn =\n\u2211n k=1(xik + \u01ebk/\u03c3) 2 2n \u2212 \u2211n k=1 x 2 ik 2n \u2212 \u2211n k=1 \u01eb 2 k/\u03c3 2 2n .\nUsing Proposition 1, we have\nP\n( |\u03b7i/\u03c3| \u2265 t ) \u2264 6 exp { \u2212min ( t2n 72e2K , tn 6eK )} .\nNow we turn to the proof of Theorem 5.\nProof of Theorem 5. Taking union bound on the results from Lemma 1 and 2, we have for any t > 0,\nP\n(\nmin i\u2208Q\n\u03a6ii \u2264 1\u2212 t ) \u2264 2p exp { \u2212min ( t2n 8e2K , tn 2eK )} ,\nP\n(\nmax i 6=j\n|\u03a6ij | \u2265 r + t ) \u2264 6(p2 \u2212 p) exp { \u2212min ( t2n 72e2K , tn 6eK )} ,\nand\nP\n(\nmax i\u2208Q\n|\u03b7i| \u2265 \u03c3t ) \u2264 6p exp { \u2212min ( t2n 72e2K , tn 6eK )} .\nThus, when p > 2 we have\nP\n(\nmin i\u2208Q \u03a6ii \u2264 1\u2212 t or max i 6=j |\u03a6ij | \u2265 r + t or max i\u2208Q\n|\u03b7i| \u2265 \u03c3t ) \u2264 7p2 exp { \u2212min ( t2n 72e2K , tn 6eK )} .\nIn other words, for any \u03b4 > 0, when n \u2265 K log(7p2/\u03b4), with probability at least 1 \u2212 \u03b4, we have\nmin i\u2208Q\n\u03a6ii \u2265 1\u2212 6 \u221a 2e\n\u221a\nK log(7p2/\u03b4)\nn , max i 6=j |\u03a6ij| \u2264 r + 6\n\u221a 2e\n\u221a\nK log(7p2/\u03b4)\nn ,\nand\nmax i\u2208Q\n|\u03b7i| \u2264 6 \u221a 2e\u03c3\n\u221a\nK log(7p2/\u03b4)\nn .\nA sufficient condition for \u03a6 to be weak diagonally dominant is that\nmin i \u03a6ii > 2\u03c1smax i 6=j |\u03a6ij |+ 2\u03c4\u22121max i |\u03b7i|.\nPlugging in the values and solving the inequality, we have (notice that 7p2/\u03b4 < 9p2/\u03b42) \u03a6 is WDD as long as\nn > 144K\n(\n1 + 2\u03c1s+ 2\u03c3/\u03c4\n1\u2212 2\u03c1sr\n)2\nlog(3p/\u03b4).\nThis completes the proof."}, {"heading": "D Proofs for Section 5 (HOLP)", "text": "In this section we prove Lemma 3, 4 and Theorem 5. Several propositions and lemmas are needed for establishing the whole theory. We list all prerequisite results without proofs but provide readers references for complete proofs.\nLet P \u2208 O(p) be a p \u00d7 p orthogonal matrix from the orthogonal group O(p). Let H denote the first n columns of P . Then H is in the Stiefel manifold (Chikuse, 2003). In general, the Stiefel manifold Vn,p is the space whose points are n-frames in Rp represented as the set of p\u00d7 n matrices X such that XTX = In. Mathematically, we can write\nVn,p = {X \u2208 Rp\u00d7n : XTX = In}.\nThere is a natural measure (dX) called Haar measure on the Stiefel manifold, invariant under both right orthogonal and left orthogonal transformations. We standardize it to obtain a probability measure as [dX ] = (dX)/V (n, p), where V (n, p) = 2n\u03c0np/2/\u0393n(1/2p).\nLemma 5. (Chikuse, 2003, Page 41-44) Supposed that a p \u00d7 n random matrix Z has the density function of the form\nfZ(Z) = |\u03a3|\u2212n/2g(ZT\u03a3\u22121Z),\nwhich is invariant under the right-orthogonal transformation of Z, where \u03a3 is a p\u00d7p positive definite matrix. Then its orientation Hz = Z(Z TZ)\u22121/2 has the matrix angular central Gaussian distribution (MACG) with a probability density function\nMACG(\u03a3) = |\u03a3|\u2212n/2|HTz \u03a3\u22121Hz|\u2212p/2.\nIn particular, if Z is a p\u00d7 n matrix whose distribution is invariant under both the left- and right-orthogonal transformations, then HY , with Y = BZ for BB T = \u03a3, has the MACG(\u03a3) distribution.\nWhen n = 1, the MACG distribution becomes the angular central Gaussian distribution, a description of the multivariate Gaussian distribution on the unit sphere (Watson et al., 1983).\nLemma 6. (Chikuse, 2003, Page 70, Decomposition of the Stiefel manifold) Let H be a p\u00d7n random matrix on Vn,p, and write\nH = (H1 H2),\nwith H1 being a p\u00d7 q matrix where 0 < q < n. Then we can write\nH2 = G(H1)U1,\nwhere G(H1) is any matrix chosen so that (H1 G(H1)) \u2208 O(p); as H2 runs over Vn\u2212q,p, U1 runs over Vn\u2212q,p\u2212q and the relationship is one to one. The differential form [dH ] for the normalized invariant measure on Vn,p is decomposed as the product\n[dH ] = [dH1][dU1]\nof those [dH1] and [dU1] on Vq,p and Vn\u2212q,p\u2212q, respectively.\nLemma 7. [Lemma 4 in Fan and Lv (2008)]Let U be uniformly distributed on the Stiefel manifold Vn,p. Then for any C > 0, there exist c \u2032 1, c \u2032 2 with 0 < c \u2032 1 < 1 < c \u2032 2, such that\nP\n(\neT1UU T e1 < c \u2032 1\nn p\n)\n\u2264 2e\u2212Cn,\nand\nP\n(\neT1UU T e1 > c \u2032 2\nn p\n)\n\u2264 4e\u2212Cn.\nSome of our proof requires concentration properties of a random Gaussian matrix and\nX 21 random variables. For a Wigner matrix, we have the following result.\nLemma 8. Assume Z is a n \u00d7 p matrix with p > c0n for some c0 > 1. Each entry of Z follows a Gaussian distribution with mean zero and variance one and are independent. Then for any t > 0, with probability at least 1\u2212 2 exp(\u2212t2/2), we have\n(1\u2212 c\u221210 \u2212 t/p)2 \u2264 \u03bbmin(ZZT/p) < \u03bbmax(ZZT/p) \u2264 (1 + c\u221210 + t/p)2.\nFor any C > 0, taking t = \u221a 2Cn, we have with probability 1\u2212 2 exp(\u2212Cn/2),\n(1\u2212 c\u221210 \u2212 \u221a 2C\nc0 \u221a n )2 \u2264 \u03bbmin(ZZT/p) \u2264 (1 + c\u221210 +\n\u221a 2C c0 \u221a n )2.\nProof. This is essentially Corollary 5.35 in Vershynin (2010).\nThe conditional number of \u03a3 is controled by \u03ba, which simulaneously controls the largest\nand the smallest eigenvalues.\nProposition 2. Assume the conditional number of \u03a3 is \u03ba and \u03a3ii = 1 for i = 1, 2, \u00b7 \u00b7 \u00b7 , p,\nthen we have\n\u03bbmin(\u03a3) \u2265 \u03ba\u22121 and \u03bbmax(\u03a3) \u2264 \u03ba.\nProof. Notice that p = tr(\u03a3) = \u2211p\ni=1 \u03bbi. Therefore, we have\np/\u03bbmax \u2265 p\u03ba\u22121 and p/\u03bbmin(\u03a3) \u2264 p\u03ba,\nwhich completes the proof.\nNow we prove the main results for HOLP.\nProof of Lemma 3. Consider a transformed n \u00d7 p random matrix Z = X\u03a3\u22121/2, which, by definition, follows standard multivariate Gaussian. Consider its SVD decomposition,\nZ = V DUT ,\nwhere V \u2208 O(n), D is a diagonal matrix and U is a p\u00d7 n random matrix belonging to the Stiefel manifold Vn,p. With such notion, we can rewrite the projection matrix as\nXT (XXT )\u22121X = \u03a31/2U(UT\u03a3U)\u22121UT\u03a31/2 = HHT ,\nwhere H = \u03a31/2U(UT\u03a3U)\u22121/2 and H \u2208 Vn,p\u22121. Therefore, the two quantities that we are interested in are \u03a6ii = e T i HH Tei (diagonal term) and \u03a6ij = e T i HH Tej (off-diagonal term), where eTi is the p\u2212dimensional unit vector with the ith coordinate being one. The proof is divided into two parts, where in the first part we consider diagonal terms and the second part takes care of off-diagonal terms.\nPart I: First, we consider the diagonal term eTi HH Tei. Recall the definition of H and\neTi HH Tei = e T i \u03a3\n1 2U(UT\u03a3U)\u22121UT\u03a3 1\n2 ei.\nThere always exists some orthogonal matrix Q that rotates the vector \u03a3 1\n2 ei to the direction\nof e1, i.e,\n\u03a3 1 2v = \u2016\u03a3 12v\u2016Qe1.\nThen we have\neTi HH Tei = \u2016\u03a3\n1 2 ei\u20162eT1QTU(UT\u03a3U)\u22121UTQe1 = \u2016\u03a3 1 2 v\u20162eT1 U\u0303(UT\u03a3U)\u22121U\u0303e1,\nwhere U\u0303 = QTU is uniformly distributed on Vn,p, because U is uniformly distributed on Vn,p (see discussion in the beginning). Now the magnitude of eTi HH T eI can be evaluated in two parts. For the norm of the vector \u03a3 1 2 v, we have\n\u03bbmin(\u03a3) \u2264 eTi \u03a3ei = \u2016\u03a3 1 2 e)i\u20162 \u2264 \u03bbmax(\u03a3), (9)\nand for the remaining part,\neT1 U\u0303(U T\u03a3U)\u22121U\u0303e1 \u2264 \u03bbmax((UT\u03a3U)\u22121)\u2016U\u0303e1\u20162 \u2264 \u03bbmin(\u03a3)\u22121\u2016U\u0303e1\u20162,\nand\neT1 U\u0303(U T\u03a3U)\u22121U\u0303e1 \u2265 \u03bbmin((UT\u03a3U)\u22121)\u2016U\u0303e1\u20162 \u2265 \u03bbmax(\u03a3)\u22121\u2016U\u0303e1\u20162.\nConsequently, we have\neTi HH Tei \u2264\n\u03bbmax(\u03a3) \u03bbmin(\u03a3) eT1UU T e1, e T i HH Tei \u2265 \u03bbmin(\u03a3) \u03bbmax(\u03a3) eT1 UU T e1. (10)\nTherefore, following Proposition 7, for any C > 0 we have\nP\n(\neTi HH Tei < c \u2032 1c4\u03ba\n\u22121n\np\n)\n\u2264 2e\u2212Cn,\nand\nP\n(\neTi HH Tei > c \u2032 2c \u22121 4 \u03ba\nn1\np\n)\n\u2264 2e\u2212Cn.\nDenoting c\u20321c4 by c1 and c \u2032 2c \u22121 4 by c2, we obtain the equation in Lemma 3.\nPart II: Second, for off-diagonal terms, although the proof is almost identical to the proof of Lemma 5 in Wang and Leng (2013), we still provide a complete version here due to the importance of this result.\nThe proof depends on the decomposition of Stiefel manifold. Without loss of generality, we prove the bound only for eT2HH Te1, then the other off-diagonal terms should follow exactly the same argument. According to Lemma 6, we can decompose H = (T1, H2) with T1 = G(H2)H1, where H2 is a p\u00d7 (n\u2212 1) matrix, H1 is a (p\u2212n+1)\u00d7 1 vector and G(H2) is a matrix such that (G(H2), H2) \u2208 O(p). The invariant measure on the Stiefel manifold can\nbe decomposed as\n[H ] = [H1][H2]\nwhere [H1] and [H2] are Haar measures on V1,n\u2212p+1, Vn\u22121,p (Notice that q = n \u2212 1 in this decomposition) respectively. As pointed out before, H has the MACG(\u03a3) distribution, which possesses a density as\np(H) \u221d |HT\u03a3\u22121H|\u2212p/2[dH ].\nUsing the identity for matrix determinant\n\u2223 \u2223 \u2223 \u2223 \u2223 A B\nC D\n\u2223 \u2223 \u2223 \u2223 \u2223 = |A||D \u2212 CA\u22121B| = |D||A\u2212 BD\u22121C|,\nwe have\nP (H1, H2) \u221d |HT2 \u03a3\u22121H2|\u2212p/2(T T1 \u03a3\u22121T1 \u2212 T T1 \u03a3\u22121H2(HT2 \u03a3\u22121H2)\u22121HT2 \u03a3\u22121T1)\u2212p/2\n= |HT2 \u03a3\u22121H2|\u2212p/2(HT1 G(H2)T (\u03a3\u22121 \u2212 \u03a3\u22121H2(HT2 \u03a3\u22121H2)\u22121HT2 \u03a3\u22121)G(H2)H1)\u2212p/2 = |HT2 \u03a3\u22121H2|\u2212p/2(HT1 G(H2)T\u03a3\u22121/2(I \u2212 T2)\u03a3\u22121/2G(H2)H1)\u2212p/2,\nwhere T2 = \u03a3 \u22121/2H2(H T 2 \u03a3 \u22121H2) \u22121HT2 \u03a3 \u22121/2 is an orthogonal projection onto the linear space spanned by the columns of \u03a3\u22121/2H2. It is easy to verify the following result by using the definition of G(H2),\n[\u03a31/2G(H2)(G(H2) T\u03a3G(H2)) \u22121/2, \u03a3\u22121/2H2(H T 2 \u03a3 \u22121H2) \u22121/2] \u2208 O(p),\nand therefore we have\nI \u2212 T2 = \u03a31/2G(H2)(G(H2)T\u03a3G(H2))\u22121G(H2)T\u03a31/2,\nwhich simplifies the density function as\nP (H1, H2) \u221d |HT2 \u03a3\u22121H2|\u2212p/2(HT1 (G(H2)T\u03a3G(H2))\u22121H1)\u2212p/2.\nNow it becomes clear thatH1|H2 follows the Angular Central Gaussian distribution ACG(\u03a3\u2032),\nwhere\n\u03a3\u2032 = G(H2) T\u03a3G(H2).\nNext, we relate the target quantity eT1HH Te2 to the distribution of H1. Notice that for\nany orthogonal matrix Q \u2208 O(n), we have\neT1HH Te2 = e T 1HQQ THTe2 = e T 1H \u2032H \u2032T e2.\nWrite H \u2032 = HQ = (T \u20321, H \u2032 2), where T \u2032 1 = [T \u2032(1) 1 , T \u2032(2) 1 , \u00b7 \u00b7 \u00b7 , T \u2032(p) 1 ], H \u2032 2 = [H \u2032(i,j) 2 ]. If we choose Q such that the first row of H \u20322 are all zero (this is possible as we can choose the first column of Q being the first row of H upon normalizing), i.e.,\neT1H \u2032 = [T \u2032(1) 1 , 0, \u00b7 \u00b7 \u00b7 , 0] eT2H \u2032 = [T \u2032(2) 1 , H \u2032(2,1) 2 , \u00b7 \u00b7 \u00b7 , H \u2032(2,n\u22121) 2 ],\nthen immediately we have eT1HH T e2 = e T 1H \u2032H \u2032T e2 = T \u2032(1) 1 T \u2032(2) 1 . This indicates that\neT1HH Te2\n(d) = T (1) 1 T (2) 1\n\u2223 \u2223 \u2223 \u2223 eT1H2 = 0.\nAs shown at the beginning, H1 followsACG(\u03a3 \u2032) conditional onH2. LetH1 = (h1, h2, \u00b7 \u00b7 \u00b7 , hp)T\nand let xT = (x1, x2, \u00b7 \u00b7 \u00b7 , xp\u2212n+1) \u223c N(0,\u03a3\u2032), then we have\nhi (d) = xi \u221a\nx21 + \u00b7 \u00b7 \u00b7+ x2p\u2212n+1 .\nNotice that T1 = G(H2)H1, a linear transformation on H1. Defining y = G(H2)x, we have\nT (i) 1 (d) = yi \u221a\ny21 + \u00b7 \u00b7 \u00b7+ y2p , (11)\nwhere y \u223c N(0, G(H)\u03a3\u2032G(H)T ) is a degenerate Gaussian distribution. This degenerate distribution contains an interesting form. Letting z \u223c N(0,\u03a3), we know y can be expressed as y = G(H)G(H)Tz. Write G(H2)\nT as [g1, g2] where g1 is a (p\u2212 n+ 1)\u00d7 1 vector and g2 is a (p\u2212 n+ 1)\u00d7 (p\u2212 1) matrix, then we have\nG(H2)G(H2) T =\n(\ngT1 g1 g T 1 g2 gT2 g1 g T 2 g2\n)\n.\nWe can also write HT2 = [0n\u22121,1, h2] where h2 is a (n \u2212 1) \u00d7 (p \u2212 1) matrix, and using the\northogonality, i.e., [H2 G(H2)][H2 G(H2)] T = Ip, we have\ngT1 g1 = 1, g T 1 g2 = 01,p\u22121 and g T 2 g2 = Ip\u22121 \u2212 h2hT2 .\nBecause h2 is a set of orthogonal basis in the p \u2212 1 dimensional space, gT2 g2 is therefore an orthogonal projection onto the space {h2}\u22a5 and gT2 g2 = AAT where A = gT2 (g2gT2 )\u22121/2 is a (p\u2212 1)\u00d7 (p\u2212 n) orientation matrix on {h2}\u22a5. Together, we have\ny =\n(\n1 0 0 AAT\n)\nz.\nThis relationship allows us to marginalize y1 out with y following a degenerate Gaussian distribution.\nWe now turn to transform the condition eT1H2 = 0 onto constraints on the distribution of\nT (i) 1 . Letting t 2 1 = e T 1HH Te1, then e T 1H2 = 0 is equivalent to T (1)2 1 = e T 1HH Te1 = t 2 1, which implies that\neT1HH Te2\n(d) = T (1) 1 T (2) 1\n\u2223 \u2223 \u2223 \u2223 T (1)2 1 = e T 1HH Te1.\nBecause the magnitude of eT1HH Te1 has been obtained in Part I, we can now condition on the value of eT1HH Te1 to obtain the bound on T (2) 1 . From T (1)2 1 = t 2 1, we obtain that,\n(1\u2212 t21)y21 = t21(y22 + y23 + \u00b7 \u00b7 \u00b7+ y2p). (12)\nNotice this constraint is imposed on the norm of y\u0303 = (y2, y3, \u00b7 \u00b7 \u00b7 , yp) and is thus independent of (y2/\u2016y\u0303\u2016, \u00b7 \u00b7 \u00b7 , yp/\u2016y\u0303\u2016). Equation (12) also implies that\n(1\u2212 t21)(y21 + y22 + \u00b7 \u00b7 \u00b7+ y2p) = y22 + y23 + \u00b7 \u00b7 \u00b7+ y2p. (13)\nTherefore, combining (11) with (12), (13) and integrating y1 out, we have\nT (i) 1 | T (1) 1 = t1 (d) =\n\u221a\n1\u2212 t21yi \u221a\ny22 + \u00b7 \u00b7 \u00b7+ y2p , i = 2, 3, \u00b7 \u00b7 \u00b7 , p,\nwhere (y2, y3, \u00b7 \u00b7 \u00b7 , yp) \u223c N(0, AAT\u03a322AAT ) with \u03a322 being the covariance matrix of z2, \u00b7 \u00b7 \u00b7 , zp. To bound the numerator, we use the classical tail bound on the normal distribution as\nfor any t > 0, (\u03c3i = \u221a var(yi) \u2264 \u221a \u03bbmax(AAT\u03a322AAT ) \u2264 \u03bbmax(\u03a3)1/2 \u2264 \u03ba1/2, Proposition 2),\nP (|yi| > t\u03c3i) \u2264 2e\u2212t 2/2. (14)\nFor the denominator, letting z\u0303 \u223c N(0, Ip\u22121), we have\ny\u0303 = AAT\u03a3 1/2 22 z\u0303 and y\u0303 T y\u0303 = z\u0303T\u03a3 1/2 22 AA T\u03a3 1/2 22 z\u0303 (d) =\np\u2212n \u2211\ni=1\n\u03bbiX 2i (1),\nwhere X 2i (1) are iid chi-square random variables and \u03bbi are non-zero eigenvalues of matrix \u03a3\n1/2 22 AA T\u03a3 1/2 22 . Here \u03bbi\u2019s are naturally upper bounded by \u03bbmax(\u03a3). To give a lower bound,\nnotice that \u03a3 1/2 22 AA T\u03a3 1/2 22 and A\u03a322A T possess the same set of non-zero eigenvalues, thus\nmin i\n\u03bbi \u2265 \u03bbmin(A\u03a322AT ) \u2265 \u03bbmin(\u03a3).\nTherefore,\n\u03bbmin(\u03a3) \u2211p\u2212n i=1 X 2i (1) p\u2212 n \u2264 y\u0303T y\u0303 p\u2212 n \u2264 \u03bbmax(\u03a3) \u2211p\u2212n i=1 X 2i (1) p\u2212 n .\nThe quantity \u2211p\u2212n i=1 X 2i (1)\np\u2212n can be bounded by Proposition 1. Combining with Proposition 2,\nwe have for any C > 0, there exists some c3 > 0 such that\nP\n( y\u0303T y\u0303/(p\u2212 n) < c3\u03ba\u22121 ) \u2264 e\u2212C(p\u2212n).\nTherefore, T (2) 1 can be bounded as\nP\n( |T (2)1 | < \u221a 1\u2212 t21\u03bat\u221a c3 \u221a p\u2212 n \u2223 \u2223T (1) 1 = t1 ) \u2264 e\u2212C(p\u2212n) + 2e\u2212t2/2.\nUsing the results from the diagonal term, we have\nP\n(\nt21 > c2\u03ba n\np\n) \u2264 2e\u2212Cn. and P ( t21 < c1\u03ba \u22121n\np\n)\n\u2264 2e\u2212Cn.\nConsequently, we have\nP\n(\n|eT1HHTe2| > c4\u03ba 3 2 t\n\u221a n\np\n)\n= P\n(\n|T (1)1 T (2) 1 | > c4\u03ba 3 2 t\n\u221a n\np\n\u2223 \u2223T (1) 1 = t1\n)\n\u2264 P (\nT (1)2 1 > c2\u03ba\nn p |T (1)1 = t1\n)\n+ P\n( |T (2)1 | < \u03bat \u221a 1\u2212 c1n/p\u221a c3 \u221a p\u2212 n \u2223 \u2223T (1) 1 = t1 )\n\u2264 5e\u2212Cn + 2e\u2212t2/2,\nwhere c4 = \u221a c2(c0\u22121)\u221a c3(c0\u22121) .\nProof of Lemma 4. Notice that conditioning on X , for any fixed index i, eTi X T (XXT )\u22121\u01eb follows a normal distribution with mean zero and variance \u03c32\u2016eTi XT (XXT )\u22121\u201622. We can first bound the variance and then apply the normal tail bound (14) again to obtain an upper bound for the error term.\nThe variance term follows\n\u03c32eTi X T (XXT )\u22122Xei \u2264 \u03c32\u03bbmax ( (XXT )\u22121 ) eTi HH Tei.\nThe eTi HH Tei part can be bounded according to Lemma 3, while the first part follows\n\u03bbmax ( (XXT )\u22121 ) = \u03bbmax ( (Z\u03a3ZT )\u22121 ) \u2264 \u03bb\u22121min(ZZT )\u03bb\u22121min(\u03a3) = \u03ba p \u03bb\u22121min(p \u22121ZZT ).\nThus, using Lemma 8 and 3, we have\n\u03c32\u2016eTi XT (XXT )\u22121\u201622 \u2264 4\u03c32c2 (1\u2212 c\u221210 )2 n\u03ba2 p2 , (15)\nwith probability at least 1\u2212 4 exp(\u2212Cn) if n > 8C/(c0 \u2212 1)2. Now combining (15) and (14) we have for any t > 0,\nP\n(\n|eTi XT (XXT )\u22121\u01eb| \u2265 2\u03c3\n\u221a c2\u03bat\n1\u2212 c\u221210\n\u221a n\np\n)\n< 4e\u2212Cn + 2e\u2212t 2/2.\nProof of Theorem 6. The proof depends on Lemma 3 and 4, and a careful choice of the value of t in these two lemmas. We first take union bounds of the two lemmas to obtain\nP (min i\u2208Q\n|\u03a6ii| < c1\u03ba\u22121 n p ) \u2264 2pe\u2212Cn,\nP (max i 6=j\n|\u03a6ij | > c4\u03ba 3 2 t\n\u221a n\np ) \u2264 5(p2 \u2212 p)e\u2212Cn + 2(p2 \u2212 p)e\u2212t2/2,\nand\nP\n(\n\u2016XT (XXT )\u22121\u01eb\u2016\u221e \u2265 2\u03c3\n\u221a c2\u03bat\n1\u2212 c\u221210\n\u221a n\np\n)\n< 4pe\u2212Cn + 2pe\u2212t 2/2.\nNotice that once we have\nmin i |\u03a6ii| > 2s\u03c1max ij |\u03a6ij |+ 2\u03c4\u22121\u2016XT (XXT )\u22121\u01eb\u2016\u221e, (16)\nthen the proof is complete because \u03a6\u2212 2\u03c4\u22121\u2016XT (XXT )\u22121\u01eb\u2016\u221e is already a weak diagonally dominant matrix. Let t = \u221a Cn/\u03bd. The above equation then requires\nc1\u03ba \u22121n p \u2212 2c4\n\u221a C\u03ba 3 2 s\u03c1\n\u03bd\nn p \u2212 2\u03c3\n\u221a c2C\u03bat\n(1\u2212 c\u221210 )\u03c4\u03bd n p\n= (c1\u03ba \u22121 \u2212 2c4\n\u221a C\u03ba 3 2 s\u03c1\n\u03bd \u2212 2\u03c3\n\u221a c2C\u03ba\n(1\u2212 c\u221210 )\u03c4\u03bd ) n p > 0,\nwhich implies that\n\u03bd > 2c4\n\u221a C\u03ba5/2\u03c1s\nc1 +\n2\u03c3 \u221a c2C\u03ba 2\nc1(1\u2212 c\u221210 )\u03c4 = C1\u03ba\n5/2\u03c1s+ C2\u03ba 2\u03c4\u22121\u03c3 > 1, (17)\nwhere C1 = 2c4\n\u221a C\nc1 , C2 =\n2 \u221a c2C\nc1(1\u2212c\u221210 ) . Therefore, the probability that (16) does not hold is\nP\n(\n{ (16) does not hold }\n)\n< (p+ 5p2)e\u2212Cn + 2p2e\u2212Cn/\u03bd < (7 + 1\nn )p2e\u2212Cn/\u03bd 2 ,\nwhere the second inequality is due to the fact that p > n and \u03bd > 1. Now for any \u03b4 > 0, (16) holds with probability at least 1\u2212 \u03b4 requires that\nn \u2265 \u03bd 2\nC\n( log(7 + 1/n) + 2 log p\u2212 log \u03b4 ) ,\nwhich is certainly satisfied if (notice that \u221a 8 < 3),\nn \u2265 2\u03bd 2\nC log\n3p\n\u03b4 .\nNow pushing \u03bd to the limit as shown in (17) gives the precise condition we need, i.e.\nn > 2C \u2032\u03ba5(\u03c1s+ \u03c4\u22121\u03c3)2 log 3p\n\u03b4 ,\nwhere C \u2032 = max{4c 2 4\nc2 1 , 4c2 c2 1 (1\u2212c\u22121 0 )2 }."}], "references": [{"title": "Compressive sensing", "author": ["R. Baraniuk"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Baraniuk,? \\Q2007\\E", "shortCiteRegEx": "Baraniuk", "year": 2007}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": null, "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "The dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Candes", "T. Tao"], "venue": null, "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Statistics on special manifolds, volume 174", "author": ["Y. Chikuse"], "venue": null, "citeRegEx": "Chikuse,? \\Q2003\\E", "shortCiteRegEx": "Chikuse", "year": 2003}, {"title": "High dimensional variable selection via tilting", "author": ["H. Cho", "P. Fryzlewicz"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Cho and Fryzlewicz,? \\Q2012\\E", "shortCiteRegEx": "Cho and Fryzlewicz", "year": 2012}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan and Li,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li", "year": 2001}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["J. Fan", "J. Lv"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Fan and Lv,? \\Q2008\\E", "shortCiteRegEx": "Fan and Lv", "year": 2008}, {"title": "Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584", "author": ["J. Jia", "K. Rohe"], "venue": null, "citeRegEx": "Jia and Rohe,? \\Q2012\\E", "shortCiteRegEx": "Jia and Rohe", "year": 2012}, {"title": "On model selection consistency of m-estimators with geometrically decomposable penalties", "author": ["J.D. Lee", "Y. Sun", "J.E. Taylor"], "venue": "Advances in Neural Processing Information Systems", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Robust rank correlation based screening", "author": ["G. Li", "H. Peng", "J. Zhang", "L Zhu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Forward regression for ultra-high dimensional variable screening", "author": ["H. Theory. Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2009\\E", "shortCiteRegEx": "Wang", "year": 2009}, {"title": "High dimensional ordinary least square projection for variable", "author": ["C. Leng"], "venue": "American Statistical Association,", "citeRegEx": "X. and Leng,? \\Q2013\\E", "shortCiteRegEx": "X. and Leng", "year": 2013}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Zhang", "C.-H"], "venue": null, "citeRegEx": "Zhang and C..H.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and C..H.", "year": 2010}, {"title": "The sparsity and bias of the lasso selection", "author": ["Zhang", "C.-H", "J. Huang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al.", "startOffset": 123, "endOffset": 153}, {"referenceID": 0, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al.", "startOffset": 123, "endOffset": 153}, {"referenceID": 6, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).", "startOffset": 196, "endOffset": 288}, {"referenceID": 2, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).", "startOffset": 196, "endOffset": 288}, {"referenceID": 1, "context": "Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing (Donoho, 2006; Baraniuk, 2007), with a core focus on regularized methods (Tibshirani, 1996; Fan and Li, 2001; Candes and Tao, 2007; Bickel et al., 2009; Zhang, 2010).", "startOffset": 196, "endOffset": 288}, {"referenceID": 7, "context": "However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice.", "startOffset": 141, "endOffset": 159}, {"referenceID": 15, "context": "We relate WDD to the irrepresentable condition (Zhao and Yu, 2006) that is necessary 2", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved.", "startOffset": 32, "endOffset": 50}, {"referenceID": 6, "context": "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique \u201cSure Independence Screening\u201d (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman\u2019s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption.", "startOffset": 32, "endOffset": 612}, {"referenceID": 6, "context": "Bearing these concerns in mind, Fan and Lv (2008) propose the concept of \u201cvariable screening\u201d, a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having non-zero coefficients preserved. They propose a marginal correlation based fast screening technique \u201cSure Independence Screening\u201d (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high (Fan and Lv, 2008), which is easily violated in the practice. Li et al. (2012) extends the marginal correlation to the Spearman\u2019s rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption. Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem.", "startOffset": 32, "endOffset": 787}, {"referenceID": 4, "context": "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem.", "startOffset": 16, "endOffset": 42}, {"referenceID": 4, "context": "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance.", "startOffset": 16, "endOffset": 341}, {"referenceID": 4, "context": "Wang (2009) and Cho and Fryzlewicz (2012) take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods eliminate the strong marginal assumption in Fan and Lv (2008) and have been shown to achieve better empirical performance. However, such improvement is limited by the extra computational burden caused by their iterative framework, which is reported to be high when p is large (Wang and Leng, 2013). To ameliorate concerns in both screening performance and computational efficiency, Wang and Leng (2013) develop a new type of screening method termed \u201cHigh-dimensional ordinary least-square projection\u201d (HOLP ).", "startOffset": 16, "endOffset": 682}, {"referenceID": 7, "context": "To reduce the dimensionality, Fan and Lv (2008) suggest a variable screening framework by finding a 3", "startOffset": 30, "endOffset": 48}, {"referenceID": 15, "context": "It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for \u03b2\u0302i, i 6\u2208 S is needed.", "startOffset": 52, "endOffset": 71}, {"referenceID": 15, "context": "It is weaker than variable selection consistency in Zhao and Yu (2006). The requirement in (2) can be seen as a relaxation of the sign consistency defined in Zhao and Yu (2006), as no requirement for \u03b2\u0302i, i 6\u2208 S is needed.", "startOffset": 52, "endOffset": 177}, {"referenceID": 15, "context": "4 Relationship with the irrepresentable condition The weak diagonally dominant (WDD) condition is closely related to the strong irrepresentable condition (IC) proposed in Zhao and Yu (2006) as a necessary and sufficient condition for sign consistency of lasso.", "startOffset": 171, "endOffset": 190}, {"referenceID": 15, "context": "If \u03a6ii = 1, \u2200i and |\u03a6ij| < c/(2s), \u2200i 6= j for some 0 \u2264 c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 1/c.", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": "If \u03a6ii = 1, \u2200i and |\u03a6ij| < c/(2s), \u2200i 6= j for some 0 \u2264 c < 1 as defined in Corollary 1 and 2 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 1/c. If |\u03a6ij| < r|i\u2212j|, \u2200i, j for some 0 < r < 1 as defined in Corollary 3 in Zhao and Yu (2006), then \u03a6 is a weak diagonally dominant matrix with sparsity s and C0 \u2265 (1\u2212r)2/(4r).", "startOffset": 97, "endOffset": 283}, {"referenceID": 7, "context": "The strong screening consistency defined in this article is stronger than conditions commonly used in justifying screening procedures as in Fan and Lv (2008).", "startOffset": 140, "endOffset": 158}, {"referenceID": 7, "context": "The theory developed in this section can be easily extended to a broader family of distributions, for example, where \u01eb follows a sub-Gaussian distribution (Vershynin, 2010) and X follows an elliptical distribution (Fan and Lv, 2008; Wang and Leng, 2013).", "startOffset": 214, "endOffset": 253}, {"referenceID": 8, "context": "For future work, it is of interest to see how linear screening can be adapted to compressed sensing (Xue and Zou, 2011) and how techniques such as preconditioning (Jia and Rohe, 2012) can improve the performance of marginal screening and variable selection.", "startOffset": 163, "endOffset": 183}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector.", "startOffset": 11, "endOffset": 146}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n.", "startOffset": 11, "endOffset": 273}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174.", "startOffset": 11, "endOffset": 411}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting.", "startOffset": 11, "endOffset": 526}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing.", "startOffset": 11, "endOffset": 690}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties.", "startOffset": 11, "endOffset": 795}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space.", "startOffset": 11, "endOffset": 972}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849\u2013911. Jia, J. and Rohe, K. (2012). Preconditioning to comply with the irrepresentable condition.", "startOffset": 11, "endOffset": 1163}, {"referenceID": 0, "context": "References Baraniuk, R. (2007). Compressive sensing. IEEE Signal Processing Magazine, 24(4). Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705\u20131732. Candes, E. and Tao, T. (2007). The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313\u20132351. Chikuse, Y. (2003). Statistics on special manifolds, volume 174. Springer Science & Business Media. Cho, H. and Fryzlewicz, P. (2012). High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593\u2013622. Donoho, D. L. (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348\u20131360. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849\u2013911. Jia, J. and Rohe, K. (2012). Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584. Lee, J. D., Sun, Y., and Taylor, J. E. (2013). On model selection consistency of m-estimators with geometrically decomposable penalties.", "startOffset": 11, "endOffset": 1304}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening.", "startOffset": 0, "endOffset": 181}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6.", "startOffset": 0, "endOffset": 362}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing.", "startOffset": 0, "endOffset": 439}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371\u2013380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.", "startOffset": 0, "endOffset": 546}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371\u2013380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894\u2013942. Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the lasso selection in highdimensional linear regression.", "startOffset": 0, "endOffset": 688}, {"referenceID": 11, "context": "Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512\u20131524. Wang, X. and Leng, C. (2013). High dimensional ordinary least square projection for variable screening. Technical Report. Watson, G. S., Watson, G. S., Watson, G. S., Statisticien, P., and Watson, G. S. (1983). Statistics on spheres, volume 6. Wiley New York. Xue, L. and Zou, H. (2011). Sure independence screening and compressed random sensing. Biometrika, 98(2):371\u2013380. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894\u2013942. Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the lasso selection in highdimensional linear regression. The Annals of Statistics, 36(4):1567\u20131594. Zhao, P. and Yu, B. (2006). On model selection consistency of lasso.", "startOffset": 0, "endOffset": 842}, {"referenceID": 3, "context": "Then H is in the Stiefel manifold (Chikuse, 2003).", "startOffset": 34, "endOffset": 49}, {"referenceID": 7, "context": "[Lemma 4 in Fan and Lv (2008)]Let U be uniformly distributed on the Stiefel manifold Vn,p.", "startOffset": 12, "endOffset": 30}, {"referenceID": 11, "context": "Part II: Second, for off-diagonal terms, although the proof is almost identical to the proof of Lemma 5 in Wang and Leng (2013), we still provide a complete version here due to the importance of this result.", "startOffset": 107, "endOffset": 128}], "year": 2017, "abstractText": "Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the weak diagonally dominant (WDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS andHOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((\u03c1s + \u03c3/\u03c4)2 log p) under random designs. In addition, we relate the WDD condition to the irrepresentable condition, and highlight limitations of SIS.", "creator": "LaTeX with hyperref package"}}}