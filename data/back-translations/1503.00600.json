{"id": "1503.00600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2015", "title": "An $\\mathcal{O}(n\\log n)$ projection operator for weighted $\\ell_1$-norm regularization with sum constraint", "abstract": "We provide a simple and efficient algorithm for the projection operator for weighted $\\ ell _ 1 $standard regulation, which is subject to a sum limitation, together with an elementary proof. The implementation of the proposed algorithm can be downloaded from the author's homepage.", "histories": [["v1", "Mon, 2 Mar 2015 16:35:02 GMT  (9kb)", "http://arxiv.org/abs/1503.00600v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang"], "accepted": false, "id": "1503.00600"}, "pdf": {"name": "1503.00600.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Weiran Wang"], "emails": ["weiranwang@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n00 60\n0v 1\n[ cs\n.L G\n] 2\nM ar\n1 The problem\nIn this report, we consider the following optimization problem:\nmin x\n1 2 \u2016x\u2212 y\u2016 2 +\nn \u2211\ni=1\ndi |xi| , (1)\ns.t. x\u22a41 = 1,\nwhere y = [y1, . . . , yn] \u22a4 \u2208 Rn, di \u2265 0, i = 1, . . . , n, and 1 is the n-dimensional vector consisting of all 1\u2019s. This is a quadratic program and the objective function is strictly convex (even though it is non-smooth), so there is a unique solution which we denote by x = [x1, . . . , xn]\n\u22a4 with a slight abuse of notation. Notice if d1 = \u00b7 \u00b7 \u00b7 = dn and the constraint were absent, the problem has a closed form solution known as the soft-shrinkage operator (see, e.g., Beck and Teboulle, 2009), which is widely used for solving \u21131regularized problem in learning sparse representations. But our problem (1) is more involved due to the constraint that couples all dimensions of x. Nonetheless, we give an efficient algorithm with time complexity O(n logn) for this problem using only the KKT theorem.\nRemark 1.1. Our motivation for (1) also comes from sparse coding. Yu et al. (2009) propose the local coordinate coding (LCC) algorithm for learning sparse representations induced by locality. Given a data sample u \u2208 Rn and a set of landmark points {vj} C j=1 where vj \u2208 R\nn, j = 1, . . . , C, the LCC algorithm reconstructs u from the landmark points while enforcing the faraway landmark points to contribute less than nearby landmark points (or to have smaller reconstruction coefficients). Let the reconstruction coefficient of vj be wj , j = 1, . . . , C. Then the optimization problem for these coefficients in LCC is\nmin w\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 u\u2212 C \u2211\nj=1\nwjvj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2 + \u03bb C \u2211\nj=1\n\u2016u\u2212 vj\u2016 2 |wj | (2)\ns.t.\nC \u2211\nj=1\nwj = 1,\nwhere \u03bb > 0 is some trade-off parameter. The constraint in (2) ensures that the representation is translation invariant. There are different ways of solving this problem, e.g., Elhamifar and Vidal (2011) have a similar optimization problem which they solve with Alternating Direction Method of Multipliers (Boyd et al., 2011).\nOne simple way of solving (2) is to use the gradient proximal algorithm and its Nesterov\u2019s acceleration scheme (see Beck and Teboulle, 2009 and the reference therein), where one iteratively takes a short gradient step for the smooth quadratic term and projects the new estimate with the weighted \u21131 regularization term subject to the sum constraint, where the projection operator solves exactly (1).\n2 The solution\nWe solve the problem (1) using only the KKT theorem (Nocedal and Wright, 2006), which states the necessary and sufficient condition1 satisfied by the solution x. The Lagrangian of (1) is\nL(x, \u03b1) = 1\n2 \u2016x\u2212 y\u2016\n2 +\nn \u2211\ni=1\ndi |xi|+ \u03b1(x \u22a41\u2212 1), (3)\nwhere \u03b1 is the Lagrange multipliers associated with the constraint. And the KKT system of this problem is\nxi \u2212 yi + di + \u03b1 = 0, if xi > 0, (4a)\nxi \u2212 yi \u2212 di + \u03b1 = 0, if xi < 0, (4b)\n\u2212di \u2264 \u2212yi + \u03b1 \u2264 di, if xi = 0, (4c) n \u2211\ni=1\nxi = 1, (4d)\nwhere we have used the fact that the sub-differential of |x| is [\u22121, 1] at x = 0 to obtain (4c). Denote y\u2212i = yi\u2212di, y +\ni = yi+di, i = 1, . . . , n, which can be computed beforehand. We can then rewrite (4) in terms of \u03b1:\n\u03b1 < y\u2212i \u21d0\u21d2 xi > 0, (5a) \u03b1 > y+i \u21d0\u21d2 xi < 0, (5b)\ny\u2212i \u2264 \u03b1 \u2264 y +\ni \u21d0\u21d2 xi = 0, (5c) \u2211\ni: xi>0\n(y\u2212i \u2212 \u03b1) + \u2211\ni: xi<0\n(y+i \u2212 \u03b1) = 1. (5d)\nObviously, the Lagrange multiplier \u03b1 is the key to our problem. Once the value of \u03b1 is determined, we can easily obtain the optimal solution by setting\nxi = y \u2212 i \u2212 \u03b1 if y \u2212 i > \u03b1, (6a) xi = y + i \u2212 \u03b1 if y + i < \u03b1, (6b) xi = 0 otherwise. (6c)\nWe can sort all dimensions of y\u2212i and y + i together (a total of 2N scalars) into an ascending z-sequence:\nz1 \u2264 z2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 z2N . (7)\nAn important observation is that the z-sequence partitions the real axis into 4N + 1 disjoint sets, each being either a single point set {zj}, j = 1, . . . , 2N or an open interval of the form (\u2212\u221e, z1), (zj , zj+1), j = 1, . . . , 2N \u2212 1, or (z2N ,\u221e) and the Lagrange multiplier \u03b1 for the solution must lie in one of them.\nWe then test each of the 4N + 1 sets as follows. Assuming that \u03b1 lies in one set, we can use (5a)\u2013(5c) to conjecture the positive, negative, and zero dimensions of a possible solution x\u0302. After that, we use (5d) to compute a hypothesized value \u03b1\u0302 for the Lagrange multiplier, i.e.,\n\u03b1 =\n\u2211\ni: x\u0302i>0\ny\u2212i + \u2211\ni: x\u0302i<0\ny+i \u2212 1\n\u2211\ni: x\u0302i>0\n1 + \u2211\ni: x\u0302i<0\n1 . (8)\n1Strictly speaking, our objective is convex and non-smooth, so the condition is that the zero vector 0 lies in the sub-differential\nat the solution x.\nIf the computed \u03b1\u0302 indeed lies in the assumed set (a point or an open interval), we have a KKT point and thus the solution.\nSince the problem (1) is strictly convex and there exists a unique global optimum, this procedure will find the exact solution with no more than 4N+1 tests. We can do this efficiently by sorting y\u2212i and y +\ni separately (O(n log n) operations) and gradually merging the two sorted sequences (an O(n) operation). Therefore the total cost of our procedure for solving (1) is of order O(n log n).\nAlgorithm 1 gives the detailed pseudocode for solving (1), whose MATLAB and C++ implementation can be downloaded at https://eng.ucmerced.edu/people/wwang5.\nReferences\nA. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sciences, 2(1):183\u2013202, 2009.\nS. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1): 1\u2013122, 2011.\nE. Elhamifar and R. Vidal. Sparse manifold clustering and embedding. In J. Shawe-Taylor, R. S. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS), volume 24, pages 55\u201363. MIT Press, Cambridge, MA, 2011.\nJ. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research and Financial Engineering. Springer-Verlag, New York, second edition, 2006.\nK. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local coordinate coding. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems (NIPS), volume 22. MIT Press, Cambridge, MA, 2009.\nAlgorithm 1 Pseudo-code of our projection operator for (1). Input: y \u2208 Rn and d = [d1, . . . , dn] where di \u2265 0, i = 1, . . . , n. Sort y \u2212 d into y\u2212: y\u22121 \u2264 y \u2212 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 y \u2212 n . And sort y + d into y +: y+1 \u2264 y + 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 y + n .\ni\u2190 1, j \u2190 1 % i/j index of the dimension of y\u2212/y+ that will be merged next. % s1/s2 stores the sum of dimensions of y\n\u2212/y+ that are strictly greater/smaller than the current estimate of \u03b1. s1 \u2190 \u2211n i=1\ny\u2212i , s2 \u2190 0, t\u2190 n % t is the number of nonzero dimensions of the hypothesized x. if (s1 + s2) < t \u00b7 y\u22121 then\n\u03b1\u2190 (s1 + s2)/t; return % \u03b1 < y\u22121 , all dimensions of x are positive. end if while true do\n% Test a single point set. if y\u2212i < y + j then\nk \u2190 i % y\u2212i is the next value in the z-sequence. while (y\u2212k = y \u2212\ni ) && (k \u2264 n) do s1 \u2190 s1 \u2212 y \u2212\nk , t\u2190 t\u2212 1, k \u2190 k + 1 % Skip the contiguous block of identical dimensions in y \u2212.\nend while if (s1 + s2 \u2212 1) = t \u00b7 y \u2212\ni then\n\u03b1\u2190 y\u2212i ; return % \u03b1 happens to lie in a single point set. else\nleft\u2190 y\u2212i , i\u2190 k % Otherwise, \u03b1 lies in a open interval with left boundary left. end if\nelse\nif y\u2212i > y + j then\n% y+j is the next value in the z-sequence. if (s1 + s2 \u2212 1) = t \u00b7 y +\nj then\n\u03b1\u2190 y+j ; return % \u03b1 happens to lie in a single point set. else\nleft\u2190 y+j % Otherwise, \u03b1 lies in a open interval with left boundary left. while (y+j = left) && (j \u2264 n) do\ns2 \u2190 s2 + y + j , t\u2190 t+ 1, j \u2190 j + 1 % Skip the contiguous block of identical entries in y +.\nend while\nend if\nelse\nk \u2190 i % y\u2212i = y + j is the next value in the z-sequence. while (y\u2212k = y \u2212\ni ) && (k \u2264 n) do s1 \u2190 s1 \u2212 y \u2212\nk , t\u2190 t\u2212 1, k \u2190 k + 1 end while if (s1 + s2 \u2212 1) = t \u00b7 y \u2212\ni then\n\u03b1\u2190 y\u2212i ; return else\nleft\u2190 y\u2212i , i\u2190 k while (y+j = left) && (j \u2264 n) do\ns2 \u2190 s2 + y +\nj , t\u2190 t+ 1, j \u2190 j + 1 end while\nend if\nend if\nend if % Find the right boundary of the open interval and test if it contains \u03b1. if y\u2212i < y + j then\nright\u2190 y\u2212i else\nright\u2190 y+j end if if t \u00b7 left < (s1 + s2 \u2212 1) && t \u00b7 right > (s1 + s2 \u2212 1) then \u03b1\u2190 (s1 + s2 \u2212 1)/t; return % \u03b1 lies in the open interval (left, right). end if\nend while\nOutput: \u03b1 is the Lagrange multiplier of the problem (1), use (6) to obtain x."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Sparse manifold clustering and embedding", "author": ["E. Elhamifar", "R. Vidal"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Elhamifar and Vidal.,? \\Q2011\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2011}, {"title": "Numerical Optimization. Springer Series in Operations Research and Financial Engineering", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Nonlinear learning using local coordinate coding", "author": ["K. Yu", "T. Zhang", "Y. Gong"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": ", Beck and Teboulle, 2009), which is widely used for solving l1regularized problem in learning sparse representations. But our problem (1) is more involved due to the constraint that couples all dimensions of x. Nonetheless, we give an efficient algorithm with time complexity O(n logn) for this problem using only the KKT theorem. Remark 1.1. Our motivation for (1) also comes from sparse coding. Yu et al. (2009) propose the local coordinate coding (LCC) algorithm for learning sparse representations induced by locality.", "startOffset": 2, "endOffset": 415}, {"referenceID": 1, "context": ", Elhamifar and Vidal (2011) have a similar optimization problem which they solve with Alternating Direction Method of Multipliers (Boyd et al., 2011).", "startOffset": 131, "endOffset": 150}, {"referenceID": 1, "context": ", Elhamifar and Vidal (2011) have a similar optimization problem which they solve with Alternating Direction Method of Multipliers (Boyd et al.", "startOffset": 2, "endOffset": 29}, {"referenceID": 3, "context": "2 The solution We solve the problem (1) using only the KKT theorem (Nocedal and Wright, 2006), which states the necessary and sufficient condition satisfied by the solution x.", "startOffset": 67, "endOffset": 93}], "year": 2015, "abstractText": "We provide a simple and efficient algorithm for the projection operator for weighted l1-norm regularization subject to a sum constraint, together with an elementary proof. The implementation of the proposed algorithm can be downloaded from the author\u2019s homepage. 1 The problem In this report, we consider the following optimization problem: min x 1 2 \u2016x\u2212 y\u2016 2 + n", "creator": "LaTeX with hyperref package"}}}