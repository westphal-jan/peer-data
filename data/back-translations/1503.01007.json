{"id": "1503.01007", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "abstract": "While machine learning is currently very successful in several areas of application, we are still a long way from true artificial intelligence. In this paper, we examine fundamental problems of sequence prediction that lie outside the scope of what can be learned with common methods such as recurring networks. We show that simple algorithms can be learned from sequential data with a recurring network associated with traceable stacks. We focus our study on algorithmically generated sequences such as $a ^ n b ^ {n} $that can only be learned by models that have the ability to count. Our study highlights certain machine learning topics that deserve more attention, such as addressing the shortcomings of purely gradient training of non-convex models. Progress in this direction is made by incorporating a search-based strategy. Once trained, we show that our method is capable of generalizing sequences up to any size.", "histories": [["v1", "Tue, 3 Mar 2015 16:50:28 GMT  (662kb,D)", "http://arxiv.org/abs/1503.01007v1", null], ["v2", "Fri, 6 Mar 2015 22:41:39 GMT  (669kb,D)", "http://arxiv.org/abs/1503.01007v2", null], ["v3", "Wed, 20 May 2015 19:23:44 GMT  (552kb,D)", "http://arxiv.org/abs/1503.01007v3", null], ["v4", "Mon, 1 Jun 2015 20:37:55 GMT  (510kb,D)", "http://arxiv.org/abs/1503.01007v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["armand joulin", "tomas mikolov"], "accepted": true, "id": "1503.01007"}, "pdf": {"name": "1503.01007.pdf", "metadata": {"source": "META", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "authors": ["Armand Joulin", "Tomas Mikolov"], "emails": ["AJOULIN@FB.COM", "TMIKOLOV@FB.COM"], "sections": [{"heading": "1. Introduction", "text": "Machine learning aims to find regularities in data to perform various tasks. History shows that there are two major sources of breakthroughs: scaling up the existing approaches to larger datasets, and development of novel, more accurate approaches (LeCun et al., 1998; Elman, 1990; Breiman, 2001). In the recent years, a lot of progress has been made in scaling up algorithms, by either using alternative hardware such as GPUs (Ciresan et al., 2011) or by taking advantage of large clusters (Recht et al., 2011). While improving the speed of current methods is crucial to work on the very large datasets currently available (Bottou, 2010), it is also vital to develop novel approaches able to tackle new problems.\nRecently, deep neural networks have become very successful at various tasks, leading to a shift in the computer vision (Krizhevsky et al., 2012) and speech recognition communities (Dahl et al., 2012). This breakthrough is com-\nmonly attributed to two aspects of deep networks: their similarity to the hierarchical, recurrent structure of the neocortex and the theoretical justification that certain patterns are more efficiently represented by functions employing multiple non-linearities instead of a single one (Minsky & Papert, 1969; Bengio & LeCun, 2007).\nIt is important to question which patterns are difficult to represent and learn with the current state of the art methods. This would potentially give us hints to design new approaches which will hopefully advance machine learning research further. In the past, this approach was common and lead to crucial results: the well-known XOR problem is an example of trivial classification problem that cannot be solved using linear classifiers, but can be solved with a nonlinear one. This lead to the use of non-linear hidden layers (Rumelhart et al., 1985) and kernels methods (Bishop, 2006). Another example is the parity problem introduced by Minsky & Papert (1969): it demonstrates that while a single non-linearity is sufficient to represent any function, it is not guaranteed to represent it efficiently, and in some cases can even require exponentially many more parameters (and thus, also training data). This lead to the use of architectures that have several layers of non-linearities, currently known as deep models.\nFollowing this line of work, we study basic patterns which are difficult to represent and learn for deep models. In particular, we study simple sequences of symbols generated from simple algorithms. Interestingly, we find that such patterns are difficult to learn even for some advanced deep learning models, such as recurrent networks. We attempt to increase the learning capabilities of recurrent nets by allowing them to learn structured memory, similar to pushdown stack that is widely used to parse context free languages.\nAn example of very simple problem of this type is a sequence anbn, i.e., a sequence where the regularity is in the equal number of symbols a and b. A model that solves this task must be able to generalize for any reasonable n, i.e., after training on sequences up to some fixed length, our model should be able to recognize longer sequences generated from the same algorithm.\nar X\niv :1\n50 3.\n01 00\n7v 1\n[ cs\n.N E\n] 3\nM ar\n2 01\n5\nWhile these patterns seem relatively basic, solving them could potentially lead to approaches able to learn small algorithms from sequential data. This can be useful for numerous applications in fields requiring some form of planning. We are aware that the model we propose in this paper is too simple to learn all possible algorithms, but it is interesting to see what problems can possibly be solved.\nOur definition of a stack in a recurrent net is through constraining part of the recurrent matrix, similar to (Mikolov et al., 2014) where it was shown that diagonal recurrent matrix can help the recurrent net to store longer memory. In case of a stack, we show that simple structural constrains can allow the network to operate as if it did perform the PUSH and POP stack operations.\nOur work can be seen as a follow up of the research done in early nineties, when similar types of stack RNNs were studied (Pollack, 1991; Das et al., 1992; Mozer & Das, 1993; Zeng et al., 1994). Among recent papers with similar motivation, we are aware of the Neural Turing Machine (Graves et al., 2014) and Memory Networks (Weston et al., 2014). In our work, we tried to isolate the fundamental problems, and solve them in a general way, using standard optimization tools such as stochastic gradient descent and discrete search algorithms."}, {"heading": "2. Algorithmic Patterns", "text": "We are interested in sequences generated by simple short algorithms and our goal is to learn these algorithms by performing sequence prediction. We are mostly interested in discrete patterns that seem to be related to those that occur in the real world, such as various forms of a long term memory.\nMore precisely, we suppose that we have only access to a long stream of data which is obtained by concatenating sequences generated by a given algorithm. We do not have access to the boundary of any sequence nor to sequences which are not generated by the algorithm. We denote the regularities in these sequences of symbols as Algorithmic patterns. In this paper, we focus on algorithmic patterns\nwhich involve some form of counting, addition, multiplication and memorization. In Table 1, we show some examples of these patterns as well as the stream of data obtained by concatenating them. In particular, the mirror reverse grammar is a context free grammar defined by the following rules : X \u2192 bXb, X \u2192 cXc, X \u2192 a (Christiansen & Chater, 1999). The description of an algorithm can often be given in a form of a context free grammar, however in general we are interested in sequential patterns that have a short description length in some general Turing-complete computational system.\nWe use the unary numeral system to represent algorithms such as counting, addition or multiplication. This allows us to focus on designing a model which could learn these algorithms if the input is given in its simpler form. Learning encoder and decoder from other numeral system to the unary one is out of the scope of this paper."}, {"heading": "3. Related work", "text": "The algorithmic patterns we study in this paper are closely related to context free and context sensitive grammars which were widely studied in the past. Some works used recurrent networks with hardwired symbolic structures (Gru\u0308nwald, 1996; Crocker, 1996; Fanty, 1994). These networks are continuous implementation of symbolic system, and can deal with recursive symbolic systems in computational linguistics. While theses approaches are interesting to understand the link between symbolic systems and neural networks, they are often hand designed for each specific grammar.\nWiles & Elman (1995) has shown that it was possible to learn a standard recurrent network able to count on a limited range. They train a recurrent network on sequences of the form anbn for n = 1 to 12 and it is able to generalize up to n = 18. While this is a promising result, their network has relatively small capacity which makes it hard to generalize beyond n = 18. This suggests that their neural network does not truly learn how to count but instead remember seen patterns. Rodriguez et al. (1999) further\nstudied the behavior of this network. Gru\u0308nwald (1996) designs a hardwired second order recurrent network to tackle sequences of the form (abk)n. They show that they are able to tackle sequences up to k = 120. Their work is purely exploratory and does not discuss if it is possible to learn such regularities directly from sequences. Christiansen & Chater (1999) extended these results to grammars with larger vocabularies. When the sequences relies on more than two symbols, the network must learn dependencies between its internal representation of the symbols. While they show that recurrent networks have the capacity to learn more complex sequences, their model does not have the capacity to generalize to longer sequences generated by the same algorithm. They also focus on mirror recursive grammars, which we also study.\nBeside using standard recurrent networks, other structures have been used to deal with recursive patterns. For example, Tabor (2000) uses a model based on pushdown dynamical automata and Bode\u0301n & Wiles (2000) use a sequential cascaded network (Pollack, 1991). While Tabor (2000) hardwires his network, Bode\u0301n & Wiles (2000) show promising results on sequences of the form anbncn but fails to generalize to more than n = 12 when trained on sequences with n between 1 and 10.\nCloser to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993). Zeng et al. (1994) uses a discrete external stack which may be hard to learn on long sequences. Das et al. (1992) learns a continuous stack which has some similarity with ours. However their model does not seem to be able to generalize to long sequences. Also, as in (Das et al., 1993; Mozer & Das, 1993), their model is train on supervised data, while we aim at learning on the more challenging task of sequence prediction."}, {"heading": "4. Model", "text": ""}, {"heading": "4.1. Simple recurrent network", "text": "We consider sequential data that comes in the form of discrete tokens, such as characters or words. We suppose that these sequences are generated by an algorithm and concatenated to form a long stream of data. For example, given algorithmic patters of the form {anbn | n > 0}, a possible stream of data would be aabbaaabbbab. Our goal is to design a model able to predict the next symbol in these streams of data. Our approach is based on a standard model called recurrent neural network (RNN) and popularized by Elman (1990).\nA RNN consists of an input layer, a hidden layer with a recurrent time-delayed connection and an output layer. The recurrent connection allows the propagation through time\nof information about the state of the hidden layer. Given a sequence of tokens, a RNN takes as input the one-hot encoding xt of the current token and predicts the probability yt of next one. Between the current token representation and the prediction, there is a hidden layer with m units which stores additional information about the previous tokens seen in the sequence. More precisely, at each time t, the state of the hidden layer ht is updated based on its previous state ht\u22121 and the encoding xt of the current token, according to the following equation:\nht = \u03c3 (Uxt +Rht\u22121) , (1)\nwhere \u03c3(x) = 1/(1 + exp(x)) is the sigmoid activation function applied coordinate wise, U is the d\u00d7m token embedding matrix and R is the m \u00d7 m matrix of recurrent weights. Given the state of these hidden units, the network then outputs the probability vector yt of the next token, according to the following equation:\nyt = f (V ht) , (2)\nwhere f is the softmax function and V is the m \u00d7 d output matrix, where d is the number of different tokens. This architecture is able to learn relatively long complex patterns similar in nature to the ones captured by N-grams. While this has made them interesting for language modeling (Mikolov, 2012), it may not have the capacity to learn how the algorithmic patterns are generated. For example, an RNN with enough capacity - or hidden units - will be able to learn sequences of the form anbn up to the maximum n seen in the training set, but it will not be able to generalize to longer sequences. Basically instead of learning to count, the RNN will tend to memorize all patterns in a similar way N-grams would do.\nA possible solution to this problem is to add linear hidden units to the RNN which can potentially remember the length of the sequence. However, linear hidden units are not simple to learn in a recurrent network because of the exploding gradient problem (Bengio et al., 1994). In the next section, we follow another direction introduced in Das et al. (1992): we use a continuous version of a pushdown stack as an external memory which has the theoretical capacity to learn simple algorithmic patterns."}, {"heading": "4.2. Pushdown network", "text": "Our simple algorithmic patterns are very similar to context free grammars (CFGs). These grammars can be recognized by pushdown automaton, i.e., automaton which employs a stack. To design a RNN based model able to learn our algorithmic patterns, it seems natural to design a stack-like structure which can be learnt from the data. This type of\nmodel has been widely used in the early nineties with some encouraging success (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993). In this section, we propose a novel approach based on similar ideas. We propose two optimization schemes: stochastic gradient descent (Werbos, 1988). and a combination of stochastic gradient descent and search based optimization.\nStack is a type of persistent memory which can be only accessed through its topmost element. Two basic operations can be done with a stack: POP removes the top element and PUSH adds a new symbol on top of the stack. At any given time, the model can thus choose (i) to push a new element on the top of the stack, (ii) to pop the element on the top of the stack, or (iii) do nothing. For simplicity, at first we consider a version where the model has to perform either a PUSH or a POP at any given time. We suppose that this decision is made by a 2-dimensional variable at depending on the state of the hidden variable ht. More precisely, we have:\nat = f (Aht) , (3)\nwhere A is a 2 \u00d7 m matrix and f is a softmax function. We denote by at[PUSH], the value of the PUSH action, and at[POP] the value of the POP action. Note that at[PUSH]+ at[POP] = 1.\nWe suppose that our stack is stored at time t in a vector st of size p. Note that p could be increased on demand and does not have to be fixed. The top element is stored\nat position 0, with value st[0]. Depending on the action variable at, the top element will be replaced by either a non-linear transformation of the hidden layer ht (in case of PUSH) or the value stored below it (in case of POP). More precisely:\nst[0] = at[PUSH]\u03c3(Dht) + at[POP]st\u22121[1], (4)\nwhere D is 1\u00d7m matrix and \u03c3(x) is the sigmoid function. If at[POP] is equal to 1, the top element is replaced by the value below (all values are moved by one position up in the stack structure). If at[PUSH] is equal to 1, we move all values down in the stack and add a value on top of the stack. More precisely, for an element stored at a depth i > 0 in the stack, we have the update rule:\nst[i] = at[PUSH]st\u22121[i\u2212 1] + at[POP]st\u22121[i+ 1]. (5)\nWe use the stack to carry information to the hidden layer at the next time step. When the stack is empty, st is set to\u22121. The hidden layer ht is now updated as:\nht = \u03c3 (Uxt +Rht\u22121 + Pst\u22121[0]) , (6)\nwhere P is a m\u00d7 k recurrent matrix, where k is the depth used in the stack to predict the next hidden. In this paper, we focus on understanding how to store memory with a stack. We thus assume for the rest of this paper, that the recurrent matrix R is equal to 0, as shown in Figure 1.\nStack with a no-operation. It is straight-forward to extend our model to deal with an additional action NO-OP. We simply extend the action vector by one element. Then, the stack update rule also allows to not change the context of the stack. For example, Eq. (4) is replaced by:\nst[0] = at[PUSH]\u03c3(Dht)+at[POP]st\u22121[1]+at[NO-OP]st\u22121[0],\nwith at[PUSH] + at[POP] + at[NO-OP] = 1.\nExtension to multiple stacks. Using a single stack has serious limitations, especially considering that at each time step, only one action can be performed - the model cannot for example push two values in one simulation step. While the simple model with a single stack is sufficient to learn trivial algorithmic patterns, it is necessary to use more powerful model to deal with more challenging ones. One way to solve this problem would be to learn how many steps need to be performed before the computation is finished. This would allow for example to push two or more values on one stack. In this paper, we use a simpler way to increase capacity of the model, which is to use more stacks in parallel. The stacks interact through the hidden layer allowing them to process more challenging patterns.\nTraining with SGD. The model presented above is continuous and can thus be trained with stochastic gradient descent (SGD) method and back-propagation through time (Rumelhart et al., 1985; Williams & Zipser, 1995; Werbos, 1988). We use gradient clipping to avoid gradient explosion (Mikolov, 2012). The details of the implementation are given in the experiment section.\nDiscrete model. Continuous stacks may introduce small imprecision which could cause numerical issues when we work on very long sequences. Also, using a continuous representation is slower at test time as it requires to update the whole stack during the forward propagation. In the result section, we discuss the benefit of discretizing our model at test time. Even though we learn a continuous model during training, we observe that this discretization seems to help in many cases.\nThis suggests to use a discrete model also during training. In the next section, we propose a simple discrete optimization scheme to train our model."}, {"heading": "4.3. Search-based learning and combination with SGD", "text": "Training the above recurrent stack network with SGD seems to be an elegant solution. However, for the type of problems that we are interested in this paper, it seems that SGD may not be adapted: ultimately, we want to learn how to operate stack-based long term memory, and the PUSH and POP operations are in principle discrete. Thus, there is a significant risk that for non-trivial problems, SGD based training will always get stuck in local minima. An example of such unwanted behavior may be learning a bigram model first (understanding the frequency of symbol co-occurrence), which may not be an optimal solution. Every subsequent attempt to move outside of such solution may be prevented by the nature of SGD.\nTo avoid getting stuck in local minima where the chance of finding better solution becomes zero, one can consider adding random noise to the training examples. A more principled approach would be to include a search based strategy. As a model trained with SGD can only follow one path during its optimization (it has only one set of weights), any non-linear choice made during optimization of the parameters may prevent finding a good local minimum. Using search with many models, we may expect that different models will choose to explore different parts of the search space, and on average should find much better solutions than pure SGD.\nObviously, the size of the search space increases exponentially even for a modest number of possible actions. For our simple experiments, we studied simplified stack models that uses MAX function over push 0 (PUSH0), push 1 (PUSH1) and POP actions. Thus, it can be seen as a dis-\ncrete version of the model described in the previous section. At every step, it executes exactly one action. Instead of having just one model, we keep a pool of models (we use 100). The mapping from the hidden layer to the output can easily be learned with gradient descent. However, this cannot be done for the action outputs - we do not have the targets, and the choice of actions has very non-linear behavior that often influences the state of the stacks far into the future.\nTo obtain the target for the action outputs, we use a strategy inspired by reinforcement learning (Sutton & Barto, 1998): we sample it using the probability distribution over actions computed by the current model. This stochastic choice quickly makes the models different, which is exactly what we aim for - diversity seems crucial for any efficient search strategy. We continue sampling the targets for several time steps (30 in our experiments; using significantly more steps would make the models less diverse, following the law of big numbers). After that, we train all models using just targets for the output predictions for another 2000 steps. Then, we run evaluation of all models on novel data, again using 2000 steps, to see which model performs the best. All models that have below average performance are replaced in the next training epoch by the best performing model. The last tricks are to keep one copy of the best model unchanged, and train one copy with just SGD and no target action sampling.\nSuch training has one important advantage over SGD: it is stable. In principle, the performance cannot degrade (if we would evaluate the models on infinitely large validation set to pick the best performing one). Also, guiding the search based on the learned distribution over actions did prove important in our experiments which will be described in the next section."}, {"heading": "5. Experiments and results", "text": "We evaluate our model on two problems. In the first one, we consider different simple sequences generated by simple algorithms and the goal is to learn their generation rule. We consider similar patterns to the one studied previously (Das et al., 1992; Rodriguez et al., 1999; Bode\u0301n & Wiles, 2000).\nIn the second experiment, we consider the synthetic tasks introduced in Weston et al. (2014). In these tasks, we have a finite world of 4 characters, 3 objects and 5 rooms. Each task is a story about this finite world and the goal is to answer questions of increasing complexity about the state of the world. These tasks have been designed to test the capacity of a model to hold discrete information about this synthetic world for long period of time.\nImplementation details. We implemented the model following the graph in Figure 1. The recurrent matrix R defined in Eq. (1) is set to 0. When trained with SGD and backpropagation through time with 50 steps (Werbos, 1988), we use an hard clipping of 15 (Mikolov, 2012), and an initial learning rate of 0.1. We decay our learning rate by multiplying by 0.999 each time the entropy on the validation set is not decreasing. We use a depth k (defined Eq. (6)) of 2 for our stacks during the experiments. The only free parameters are the number of hidden units, stacks and if we use the NO-OP operation. Our implementation is in C++ and runs on on a single core machine."}, {"heading": "5.1. Learning simple algorithmic patterns", "text": "We consider a set of patterns generated by algorithms with short description length. We generate sequences from a given algorithm and concatenate them into longer sequences. This is thus an unsupervised task, i.e., the boundaries of each generated sequences are not known. We are studying patterns which are related to simple algorithms such as counting, memorization, addition and multiplication. We show examples of generated sequences in Table 1. Our goal is to evaluate if a model has the capacity to understand the generation rule used to produce the sequences. We thus use at test time sequences of size it has never seen before during training. More precisely our experimental setting is the following: We train our model on sequences generated with n up to N < 20. We use a validation set made of sequence generated with n up to 20 and we test on sequences generated with n up to 60. We vary N for the different algorithmic patterns and approaches and keep the one with the lowest entropy on the validation set. During training, we use 1000 randomly generated sequences at each epoch, and we incrementally increase the parameter n every few epochs until it reaches 20. We find that this incremental strategy allows both our model and RNNs to learn first simple patterns and then usually generalize\nthem better (Elman, 1993). At test time, we measure the performance by counting the number of correctly predicted sequences. A sequence is considered as correctly predicted if we correctly predict its deterministic part, shown in bold in Table 1.\nWe compare our model to two RNN models, one with 40 hidden units (RNN 40) and one with 100 (RNN 100). For most of the experiments, our model has 40 hidden units and 10 stacks (Ours 40-10), with a depth of k = 2 and with only PUSH and POP actions.\nCounting. We show results on patterns generated by \u201ccounting\u201d algorithms in Figure 2. The performances are evaluated for fixed n from 2 to 60. As we can see that both RNNs overfit on the training set, and are not able to generalize to longer sequences. Note that if we would use linear hidden units in RNN, it would be in theory possible to make them work on these sequences, but learning a linear RNN is quite challenging because of the exploding gradient problem (Bengio et al., 1994). On the other hand, our model is able to generalize on all of these problems. Despite having a comparable number of parameters with RNN 40, our model is able to avoid overfitting. We show in Table 3 an example of actions done by our model with a single stack (Ours 10-1) on sequences of the form anbn. For clarity, we show an example on sequences generated with n equal to 3 and 4, and we use discretization at test time. We can see that, our model learns to push an element when it sees an a and to pop one when it sees a b. At the end of the sequence, the stack is almost empty and with an access to a depth of k = 2, it is able to predict the end of the sequence. In Table 4, we also show an example of actions done by our model on sequences of the form anb3n. These sequences cannot be learn with a single stack and require interactions between at least two stacks to be solved. For clarity we use only two stacks (and 20 hidden units) and discretization on a sequence generated with n = 3. We see\nthat the two stacks are able to interact to correctly predict the deterministic part of the sequence, as well as when the sequence ends.\nOther algorithmic patterns. We also consider patterns related to addition, multiplication and memorization. We\nconsider sequences of the form anbmcn+m of addition and anbmcnm for multiplication. For memorization, we consider sequences generated from a mirror recursive grammar (Christiansen & Chater, 1999). This sequences are generated by the rules: X \u2192 bXb, X \u2192 cXc and X \u2192 a. The character a plays the role of a delimiter. We show an example of sequences generated by this grammar in Table 1. Figure 3, we show that our model works well for addition, which is not too surprising since this is close to counting. Note however, that we use only one stack and 10 hidden units, because our model with 40 units with 10 stacks is overfitting as shown below. For memorization and multiplication, we observe some variances in our performances, we thus train 10 different models, and keep 5 with the lowest entropy on the validation set. We show their average performances on the test set in Figure 4 for memorization and Figure 5 for multiplication. On memorization, we see that our model is able to learn this task even though it does not generalize as well as for counting or addition. Note that for memorization, we use smaller epochs of 100 sequences and for multiplication, we use the NO-OP.\nOn multiplication, we see some limitation of our model. It seems that it is not able to generalize much on this task. Our model is marginally better than RNNs but is still overfitting on the training set. For many difficult tasks, we observe that both RNN and our model are quite sensitive to the initialization. For example, it seems that for both memorization and multiplication, we find that three out of ten trained models learn with SGD converged to the same so-\nlution than the one that a simple feedforward network (or bigram model) would find. This suggests that for this type of discrete noiseless tasks, learning our model with only SGD may be a problem.\nDiscretization. Using continuous actions and stacks seems to overfit sometimes on the training set. A simple solution is to discretize them during validation to pick models which are using their memory in a discrete way. Figure 6, we show that this simple trick can help for some sequences. This result suggest that using discrete models may lead to more stable solution.\nComparison between SGD and search + SGD. A simple way to break our model is to consider sequences where there is an unbalanced number of as and bs in the sequence, e.g., anb6n. This is a standard problem in supervised classification with unbalanced number of training example per class. When the statistics of the training data are known, a natural solution is to reweigh each class (e.g., tf-idf for text). However in our case, we aim at learning sequences online with potential change in their statistics, we thus cannot use such solution. On the other hand, the search algorithm described above seems to avoid this problem by exploring a larger set of possible combination of discrete actions. While our study is still preliminary, using a search algorithm on top of SGD allows us to solve sequences such as anb4n or anb6n, where training our model with SGD fails."}, {"heading": "5.2. Simulated world question answering", "text": "In this section, we consider the question answering tasks proposed in Weston et al. (2014). They propose a simulated world containing 4 characters, 3 objects and 5 rooms. At each time step, a character perform an action - moving around, picking up an object or dropping it. The actions are written into a text using simple automated grammar and every so often, a question is asked about the location of a person or an object. There are two types of questions: either the question concerns the current location of an entity or it concerns a location previously visited, as shown in Figure 7.\nThese tasks require the models to be able to store information for potentially long period of time. It also requires to store information about multiple objects and persons. Typically, this means that for our model, it will requires a large amount of stacks. These tasks require that we keep information for a long period of time, we thus use the NO-OP operation.\nIn Table 2, we compare our model to RNN, the Longer short term memory model (LSTM, Hochreiter & Schmidhuber (1997)) and the memory network (memNN, Weston et al. (2014)). memNN is a supervised method which used annotation about the support sentences to answer the questions. On the other hand, LSTM, RNN and our model are unsupervised. For LSTM, we use the numbers published in Weston et al. (2014). We see that our results are similar to the one obtained by LSTM or RNN. This suggest that our model is basically using its stacks in a similar way as the recurrent matrix of an RNN. Note that our model has less parameters than RNN or LSTM, since we set the recurrent matrix R to 0. Like LSTM, our model fails to capture long discrete patterns even in a relatively low noise setting. We think that there are two reasons for this result: first the number of combinations of entities and places in this database pushes the limit of our model representation power. Second, we think that our model struggles to store complex information required to solve these tasks. This suggests that more complex structures than stacks may be required for this type of tasks."}, {"heading": "6. Discussion and future work", "text": "In this section, we discuss the achieved results, limitations of our model and potential directions to solve them.\nContinuous versus Discrete model and search. In our paper, we show that certain simple algorithmic patterns can be efficiently learned using continuous optimization technique (stochastic gradient descent) and a continuous model representation (in our case a RNN). Note that our model works much better than prior work based on RNN from the nineties that attempts to solve problems of similar type (Das et al., 1992; Zeng et al., 1994; Wiles & Elman, 1995). At the same time, our model seems simpler than many other models used for this type of tasks (Tabor, 2000; Bode\u0301n & Wiles, 2000; Graves et al., 2014). Notably, our model is relatively robust and works for a wide range of hyper-parameters (number of stacks, size of the hidden layer or learning rate).\nAt the same time, we believe that using a continuous representation and an non-convex continuous optimization ap-\nproach are not the right tools to properly learn algorithms. It seems more natural to attempt to solve these problems with a discrete approach, such as for example a search based approach. This motivated our approach where we combined the continuous and discrete optimization, which allowed us to solve certain problems that seemed to be too difficult for purely continuous optimization.\nIt is possible that the future of learning of algorithmic patterns will involve such combination of discrete and continuous optimization. However, more work is needed to understand better the limitations of both approaches, and to understand better how to combine them effectively.\nStack-based long-term memory. While in theory using multiple stacks for representing memory is as powerful as a Turing complete system (as we can simulate the tape with just two stacks), complicated interactions between stacks need to be learned to capture more complex algorithmic patterns. For example, certain operations such as accessing distant elements in the memory can be performed much more simply with RAM (random access memory) than with\na stack-based memory. It would be interesting to consider in the future other forms of memory which may be more flexible. For example, Weston et al. (2014) and Graves et al. (2014) use more advanced forms of a long term memory.\nLearning with other numeral systems. In this paper, we focus on the unary numeral system and we show that given this simple numeral representation, our model can learn simple algorithms. In theory it would be possible to extend our model to other numeral systems by adding an encoder before the input of our network and a decoder after the output. These encoder and decoder could be learn from the data to transform any numeral system into the unary one. Indeed, in this paper we used a one-hot representation for the discrete symbols for both the input and the output. This type of vectors can be easily replaced by the activation of the last (or first) layer of an encoder (or decoder).\nLanguage modeling. We also show that our model can be still applied to standard problems such as statistical language modeling. We compare our model with 40 hidden units and 10 stacks to a RNN with either 40 or 50 hidden\nunits on the Penn Treebank Corpus1 to see if our model is robust enough to work on language modeling. We add back the recurrent matrixR to our model (defined in Eq. (1)) and we use the NO-OP action. We use a depth k = 1 to have the same number of parameters than the RNN 50. We show the results in Table 5. Our model performs a bit better than an RNN with 40 hidden units but has similar performance with the RNN with 50 hidden units.\nNot surprisingly, the model does not learn to use stacks in any interesting way, and rather uses them in a similar way as the hidden layer of RNN is normally used. This is because complexity of the real language is so high that one cannot expect to learn algorithms from such data. We believe that in the future, it will be necessary to start learning the natural language from simple examples, and add the complexity only after the system will understand the basic patterns. As we did show in Section 5.2, even learning of a simple associative memory can be quite challenging task, and our model can be difficult to be trained even on such a low complexity language."}, {"heading": "7. Acknowledgment", "text": "We would like to thank Arthur Szlam, Keith Adams, Jason Weston, Yann LeCun and the rest of the Facebook AI Research team for their useful comments.\n1The pre-processed dataset is available as part of archive at http://www.fit.vutbr.cz/ imikolov/rnnlm/simple-examples.tgz"}, {"heading": "8. Conclusion", "text": "We have shown that simple stack-based recurrent net can solve certain basic problems such as anbn, without any hints (such as where the sequence starts), while using just SGD. The solution generalizes to much larger n than what the model is trained on. This is quite positive result, considering the prior work on this difficult topic.\nNonetheless, the SGD seems to be severely limited and when moving to more complex tasks, we had to combine it with search-based learning to solve more complex tasks of the form anbkn which require to learn much more complex manipulation with the stack memory. While we were successful at solving these toy problems, it is clear that fully scalable solution to learning algorithmic patterns in sequential data is still an open problem.\nWe hope our work will help other researchers who currently work on deep learning to realize limitations of the popular architectures and training algorithms. This is necessary if we want to advance the field towards artificial intelligence, and avoid the pitfall of illusionary progress achieved by scaling models to larger sizes and training them on more data."}], "references": [{"title": "Scaling learning algorithms towards ai", "author": ["Bengio", "Yoshua", "LeCun", "Yann"], "venue": "Large-scale kernel machines,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Pattern recognition and machine learning", "author": ["Bishop", "Christopher M"], "venue": "springer New York,", "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Context-free and contextsensitive dynamics in recurrent neural networks", "author": ["Bod\u00e9n", "Mikael", "Wiles", "Janet"], "venue": "Connection Science,", "citeRegEx": "Bod\u00e9n et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bod\u00e9n et al\\.", "year": 2000}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "Leon"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou and Leon.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and Leon.", "year": 2010}, {"title": "Toward a connectionist model of recursion in human linguistic performance", "author": ["Christiansen", "Morten H", "Chater", "Nick"], "venue": "Cognitive Science,", "citeRegEx": "Christiansen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Christiansen et al\\.", "year": 1999}, {"title": "Highperformance neural networks for visual object classification", "author": ["Ciresan", "Dan C", "Meier", "Ueli", "Masci", "Jonathan", "Gambardella", "Luca M", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1102.0183,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "Mechanisms for sentence processing", "author": ["Crocker", "Matthew W"], "venue": "Centre for Cognitive Science, University of Edinburgh,", "citeRegEx": "Crocker and W.,? \\Q1996\\E", "shortCiteRegEx": "Crocker and W.", "year": 1996}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Using prior knowledge in a nnpda to learn context-free languages", "author": ["Das", "Sreerupa", "Giles", "C Lee", "Sun", "Guo-Zheng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Das et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Das et al\\.", "year": 1993}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Learning and development in neural networks: The importance of starting", "author": ["Elman", "Jeffrey L"], "venue": "small. Cognition,", "citeRegEx": "Elman and L.,? \\Q1993\\E", "shortCiteRegEx": "Elman and L.", "year": 1993}, {"title": "Context-free parsing in connectionist networks", "author": ["Fanty", "Mark"], "venue": "Parallel natural language processing,", "citeRegEx": "Fanty and Mark.,? \\Q1994\\E", "shortCiteRegEx": "Fanty and Mark.", "year": 1994}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "A recurrent network that performs a context-sensitive prediction task", "author": ["Gr\u00fcnwald", "Peter"], "venue": "In Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society: July 12-15,", "citeRegEx": "Gr\u00fcnwald and Peter.,? \\Q1996\\E", "shortCiteRegEx": "Gr\u00fcnwald and Peter.", "year": 1996}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Designing a counter: Another case study of dynamics and activation landscapes in recurrent networks", "author": ["Holldobler", "Steffen", "Kalinke", "Yvonne", "Lehmann", "Helko"], "venue": "In KI-97: Advances in Artificial Intelligence,", "citeRegEx": "Holldobler et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Holldobler et al\\.", "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Statistical language models based on neural networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "Marc\u2019Aurelio. Learning longer memory in recurrent neural networks", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Chopra", "Sumit", "Mathieu", "Michael", "Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "A connectionist symbol manipulator that discovers the structure of contextfree languages", "author": ["Mozer", "Michael C", "Das", "Sreerupa"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Mozer et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mozer et al\\.", "year": 1993}, {"title": "The induction of dynamical recognizers", "author": ["Pollack", "Jordan B"], "venue": "Machine Learning,", "citeRegEx": "Pollack and B.,? \\Q1991\\E", "shortCiteRegEx": "Pollack and B.", "year": 1991}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "A recurrent neural network that learns to count", "author": ["Rodriguez", "Paul", "Wiles", "Janet", "Elman", "Jeffrey L"], "venue": "Connection Science,", "citeRegEx": "Rodriguez et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 1999}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Fractal encoding of context-free grammars in connectionist networks", "author": ["Tabor", "Whitney"], "venue": "Expert Systems,", "citeRegEx": "Tabor and Whitney.,? \\Q2000\\E", "shortCiteRegEx": "Tabor and Whitney.", "year": 2000}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural Networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}, {"title": "Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks", "author": ["Wiles", "Janet", "Elman", "Jeff"], "venue": "In Proceedings of the Seventeenth Annual Conference of the Cognitive Science Society,", "citeRegEx": "Wiles et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wiles et al\\.", "year": 1995}, {"title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "author": ["Williams", "Ronald J", "Zipser", "David"], "venue": "Back-propagation: Theory, architectures and applications,", "citeRegEx": "Williams et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1995}, {"title": "Discrete recurrent neural networks for grammatical inference", "author": ["Zeng", "Zheng", "Goodman", "Rodney M", "Smyth", "Padhraic"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Zeng et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 18, "context": "History shows that there are two major sources of breakthroughs: scaling up the existing approaches to larger datasets, and development of novel, more accurate approaches (LeCun et al., 1998; Elman, 1990; Breiman, 2001).", "startOffset": 171, "endOffset": 219}, {"referenceID": 6, "context": "has been made in scaling up algorithms, by either using alternative hardware such as GPUs (Ciresan et al., 2011) or by taking advantage of large clusters (Recht et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 23, "context": ", 2011) or by taking advantage of large clusters (Recht et al., 2011).", "startOffset": 49, "endOffset": 69}, {"referenceID": 17, "context": "Recently, deep neural networks have become very successful at various tasks, leading to a shift in the computer vision (Krizhevsky et al., 2012) and speech recognition communities (Dahl et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 8, "context": ", 2012) and speech recognition communities (Dahl et al., 2012).", "startOffset": 43, "endOffset": 62}, {"referenceID": 25, "context": "This lead to the use of non-linear hidden layers (Rumelhart et al., 1985) and kernels methods (Bishop, 2006).", "startOffset": 49, "endOffset": 73}, {"referenceID": 25, "context": "This lead to the use of non-linear hidden layers (Rumelhart et al., 1985) and kernels methods (Bishop, 2006). Another example is the parity problem introduced by Minsky & Papert (1969): it demonstrates that while a single non-linearity is sufficient to represent any function, it is not guaranteed to represent it efficiently, and in some cases can even require exponentially many more parameters (and thus, also training data).", "startOffset": 50, "endOffset": 185}, {"referenceID": 20, "context": "Our definition of a stack in a recurrent net is through constraining part of the recurrent matrix, similar to (Mikolov et al., 2014) where it was shown that diagonal recurrent matrix can help the recurrent net to store longer memory.", "startOffset": 110, "endOffset": 132}, {"referenceID": 31, "context": "Our work can be seen as a follow up of the research done in early nineties, when similar types of stack RNNs were studied (Pollack, 1991; Das et al., 1992; Mozer & Das, 1993; Zeng et al., 1994).", "startOffset": 122, "endOffset": 193}, {"referenceID": 13, "context": "Among recent papers with similar motivation, we are aware of the Neural Turing Machine (Graves et al., 2014) and Memory Networks (Weston et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 24, "context": "Rodriguez et al. (1999) further", "startOffset": 0, "endOffset": 24}, {"referenceID": 31, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 110, "endOffset": 197}, {"referenceID": 16, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 110, "endOffset": 197}, {"referenceID": 9, "context": "Also, as in (Das et al., 1993; Mozer & Das, 1993), their model is train on supervised data, while we aim at learning on the more challenging task of sequence prediction.", "startOffset": 12, "endOffset": 49}, {"referenceID": 9, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993). Zeng et al. (1994) uses a discrete external stack which may be hard to learn on long sequences.", "startOffset": 111, "endOffset": 218}, {"referenceID": 9, "context": "Closer to our approach, many works have used external memory modules with a recurrent network, such as stacks (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993). Zeng et al. (1994) uses a discrete external stack which may be hard to learn on long sequences. Das et al. (1992) learns a continuous stack which has some similarity with ours.", "startOffset": 111, "endOffset": 313}, {"referenceID": 1, "context": "However, linear hidden units are not simple to learn in a recurrent network because of the exploding gradient problem (Bengio et al., 1994).", "startOffset": 118, "endOffset": 139}, {"referenceID": 0, "context": "However, linear hidden units are not simple to learn in a recurrent network because of the exploding gradient problem (Bengio et al., 1994). In the next section, we follow another direction introduced in Das et al. (1992): we use a continuous version of a pushdown stack as an external memory which has the theoretical capacity to learn simple algorithmic patterns.", "startOffset": 119, "endOffset": 222}, {"referenceID": 31, "context": "model has been widely used in the early nineties with some encouraging success (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 79, "endOffset": 166}, {"referenceID": 16, "context": "model has been widely used in the early nineties with some encouraging success (Das et al., 1992; 1993; Zeng et al., 1994; Holldobler et al., 1997; Mozer & Das, 1993).", "startOffset": 79, "endOffset": 166}, {"referenceID": 25, "context": "The model presented above is continuous and can thus be trained with stochastic gradient descent (SGD) method and back-propagation through time (Rumelhart et al., 1985; Williams & Zipser, 1995; Werbos, 1988).", "startOffset": 144, "endOffset": 207}, {"referenceID": 24, "context": "We consider similar patterns to the one studied previously (Das et al., 1992; Rodriguez et al., 1999; Bod\u00e9n & Wiles, 2000).", "startOffset": 59, "endOffset": 122}, {"referenceID": 1, "context": "Note that if we would use linear hidden units in RNN, it would be in theory possible to make them work on these sequences, but learning a linear RNN is quite challenging because of the exploding gradient problem (Bengio et al., 1994).", "startOffset": 212, "endOffset": 233}, {"referenceID": 31, "context": "Note that our model works much better than prior work based on RNN from the nineties that attempts to solve problems of similar type (Das et al., 1992; Zeng et al., 1994; Wiles & Elman, 1995).", "startOffset": 133, "endOffset": 191}, {"referenceID": 13, "context": "At the same time, our model seems simpler than many other models used for this type of tasks (Tabor, 2000; Bod\u00e9n & Wiles, 2000; Graves et al., 2014).", "startOffset": 93, "endOffset": 148}, {"referenceID": 13, "context": "(2014) and Graves et al. (2014) use more advanced forms of a long term memory.", "startOffset": 11, "endOffset": 32}], "year": 2015, "abstractText": "While machine learning is currently very successful in several application domains, we are still very far from a real Artificial Intelligence. In this paper, we study basic sequence prediction problems that are beyond the scope of what is learnable with popular methods such as recurrent networks. We show that simple algorithms can be learnt from sequential data with a recurrent network associated with trainable stacks. We focus our study on algorithmically generated sequences such as ab, that can only be learnt by models which have the capacity to count. Our study highlights certain topics in machine learning that deserve more attention, such as addressing the shortcomings of purely gradient based training of non-convex models. We achieve progress in this direction by incorporating search based strategy. Once trained, we show that our method is able generalize to sequences up to an arbitrary size.", "creator": "LaTeX with hyperref package"}}}