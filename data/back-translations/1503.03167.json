{"id": "1503.03167", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Deep Convolutional Inverse Graphics Network", "abstract": "This work introduces the Deep Convolution Inverse Graphics Network (DC-IGN), which aims to learn an interpretable representation of images unraveled in relation to various transformations, such as object rotations, illumination variations, and texture. The DC-IGN model consists of several layers of folding and unfolding operators and is trained with the stochastic Gradient Variational Bayes (SGVB) algorithm. We suggest training methods to encourage neurons in the graphic code layer to have semantic meaning and force each group to clearly represent a specific transformation (pose, light, texture, shape, etc.). Faced with a static face image, our model can regenerate the input image with different pose, lighting, or even texture and shape variations from the base.", "histories": [["v1", "Wed, 11 Mar 2015 04:08:42 GMT  (8277kb,D)", "http://arxiv.org/abs/1503.03167v1", "First two authors contributed equally"], ["v2", "Mon, 16 Mar 2015 04:57:24 GMT  (8267kb,D)", "http://arxiv.org/abs/1503.03167v2", "First two authors contributed equally"], ["v3", "Tue, 17 Mar 2015 02:22:07 GMT  (8267kb,D)", "http://arxiv.org/abs/1503.03167v3", "First two authors contributed equally"], ["v4", "Mon, 22 Jun 2015 02:10:00 GMT  (8878kb,D)", "http://arxiv.org/abs/1503.03167v4", "First two authors contributed equally"]], "COMMENTS": "First two authors contributed equally", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG cs.NE", "authors": ["tejas d kulkarni", "william f whitney", "pushmeet kohli", "joshua b tenenbaum"], "accepted": true, "id": "1503.03167"}, "pdf": {"name": "1503.03167.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Inverse Graphics Network", "authors": ["Tejas D. Kulkarni", "Will Whitney", "Pushmeet Kohli", "Joshua B. Tenenbaum"], "emails": ["tejask@mit.edu", "wwhitney@mit.edu", "pkohli@microsoft.com", "jbt@mit.edu"], "sections": [{"heading": null, "text": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11]. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model\u2019s efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling."}, {"heading": "1 Introduction", "text": "Deep learning has led to remarkable breakthroughs in automatically learning hierarchical representations from images. Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations. However, there is relatively little work on characterizing what is the optimal representation of the data. While researchers such Cohen et al. [6] have considered this problem by proposing a theoretical framework to learn irre-\nducible representations having both invariances and equivariances, coming up with the best representation for any given task is an open question.\nThe Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement. Computer graphics is a function to go from compact description of scenes (graphics code) to images. This graphics code based representation is often disentangled to allow rendering scenes with fine-grained control over transformations such as object location, pose, lighting, texture and shape. Therefore, it naturally produces representations satisfying the abovementioned properties.\nRecent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations. Recently, Tieleman et al. [24] moved beyond this two stage pipeline by using a generic encoder network with a domainspecific decoder network to approximate a 2D rendering function. However, none of these approaches have been demonstrated to automatically produce a semantically interpretable graphics code and to learn an approximate 3D rendering engine to re-produce images.\nIn this paper, we present an approach for learning interpretable graphics codes. Given a set of images, we use a hybrid encoder-decoder model (trained using unsupervised and/or semi-supervised learning) to learn a representation that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, texture and subtle changes in 3D geometry. To achieve this, we employ a deep directed graphical model with many layers of convolution and de-convolution operators that is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11]. Without the loss of generality, we present 3D face analysis as a working example throughout this paper. We choose faces due to their rich intrinsic shape and appearance variability. Moreover, ac-\nar X\niv :1\n50 3.\n03 16\n7v 1\n[ cs\n.C V\n] 1\n1 M\nar 2\ncess to large number of training data allows us to carefully tease apart the effects of training size, learning algorithm and modeling biases.\nWe propose various training procedures to encourage neurons in the graphics code layer to have semantic meaning and forces each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). To learn a disentangled representation, we train with random variations on different transformations in each mini-batch of data. Another approach we adopt is to train using data where each mini-batch has a set of active and inactive transformations (but we do not provide target values as in supervised learning). For example, a nodding face would have the 3D elevation transformation active but its shape, texture and other affine transformations would be inactive. We exploit this type of a learning procedure to force chosen neurons in the graphics code layer to specifically represent active transformations, thereby automatically creating a disentangled representation. Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model\u2019s efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic\nfor generative modeling."}, {"heading": "2 Related Work", "text": "As mentioned before, a number of generative models have been proposed in the literature to obtain abstract visual representations. Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.\nRelatively recently, Kingma et al. [11] proposed the SGVB algorithm to learn generative models with continuous latent variables. In this work, a feed-forward neural network (encoder) is used to approximate the posterior distribution and a decoder network serves to enable stochastic reconstruction of observations. In order to handle fine-grained geometry of faces, we work with relatively large scale images (150x150). Therefore, our approach extends and applies the SGVB algorithm to jointly train and utilize many layers of convolution and de-convolution operators for the encoder and decoder network respectively. The decoder network is a function that transform a compact graphics code ( 200 dimensions) to a 150x150 image. We propose using unpooling (nearest neighbor sampling) followed by convolution to handle the massive increase in dimensionality while keeping the number of parameters be bounded. Generative stochastic networks [3] which learn the transi-\ntion operator of a Markov chain whose stationary distribution samples from the data distribution are also relevant to our work. Another piece of related work is the deconvolution network by Zeileret al. [29] which is used to produce image representations using convolutional decomposition of images under a sparsity constraint.\nIn comparison to existing approaches, it is important to note that our encoder network produces interpretable and disentangled representations, which is necessary to learn a meaningful 3D graphics engine. A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17]. However, most such methods rely on hand-crafted rendering engines. The exception to this is work by Hinton et al. [9] (transforming autoencoders) and Tieleman [24] which uses a domain specific decoder to reconstruct input images. Our work is similar in spirit to these works but has some key differences: (a) It uses a very generic convolutional architecture in the encoder and decoder networks to enable efficient learning on large datasets and image sizes, (b) it can handle single static frames as opposed to pair of images required in [9], and (c) it is generative."}, {"heading": "3 Model", "text": "As shown in figure 1, the basic structure of the Deep Convolutional Inverse Graphics Network (DC-IGN) consists of: the encoder network that captures distribution over graphics codes Z given data x and a decoder network which learns the conditional distribution to produce x\u0302 given Z. Z can be a disentangled representation containing several factored set of latent variables zi \u2208 Z such as pose, light and shape. This is important to learn a meaningful approximation of a 3D graphics engine and helps tease apart the generalization capability of the model with respect to different types of transformations on static images.\nLet us denote the encoder output of DC-IGN to be ye = encoder(x). The encoder output is used to parametrize the variational approximation Q(zi|ye), where Q is chosen to be a multivariate normal distribution. There are two reasons for using this parametrization: (1) Gradients of samples with respect to parameters \u03b8 of Q can be easily obtained using the reparametrization trick proposed in [11], and (2) Various statistical shape models trained on 3D scanner data such as faces have the same multivariate normal latent distribution (see section 4.1). Given that model parameters We connect ye and zi, the distribution parameters \u03b8 = (\u00b5zi ,\u03a3zi) and latents Z can then be expressed as:\n\u00b5zi = We \u2217 ye (1) \u03a3zi = diag(exp(We \u2217 ye)) (2) \u2200i, zi \u223c N (\u00b5zi ,\u03a3zi) (3)\nWe present several training procedures for DC-IGN to learn\n\ud835\udf191 \ud835\udefc1\n\ud835\udf19L 1\nOutput\nfirst sample in batch x1\nfrom encoder to encoder\nsame as output for x1\nz[4,n]z3z2z1\nlater samples in batch xi z[4,n]z3z2z1\nunique for each xi in batch\nzero error signal for clamped outputs\nzero error signal for clamped outputs\nerror signal from decoder\n\u2207zki = z k i - mean z k\nBackpropagation\nz[4,n]z3z2z1\nz[4,n]z3z2z1\nBackpropagation with invariance targeting\nz[4,n]z3z2z1\nk \u2208 batch\nk \u2208 batch\nCaption: Training on a minibatch in which only \ud835\udf19, the azimuth angle of the face, changes. During the forward step, the output from each component z_k != z_1 of the encoder is forced to be the same for each sample in the batch. This reflects the fact that the generating variables of the image which correspond to the desired values of these latents are unchanged throughout the batch. By holding these outputs constant throughout the batch, z_1 is forced to explain all the variance within the batch, i.e. the full range of changes to the image caused by changing \ud835\udf19.\nDuring the backward step, backpropagation of gradients happens only through the latent z_1, with gradients for z_k != z_1 set to zero. This corresponds with the clamped output from those latents throughout the batch.\nCaption: In order to directly enforce invariance of the latents corresponding to properties of the image which do not change within a given batch, we calculate gradients for the z_k != z_1 which move them towards the mean of each invariant latent over the batch. This is equivalent to regularizing the latents z_{[2,n]} by the L2 norm of (zk - mean zk).\nrepresentations with different characteristics:"}, {"heading": "3.1 Random Transformations", "text": "For our simplest training procedure, we construct minibatches of randomly selected training samples and train the DC-IGN network in figure 1 according to the SGVB procedure. This training leads to networks with extremely strong reconstruction performance. We explore several uses of the representations g nerated by these networks in section 4."}, {"heading": "3.2 Specific Transformations", "text": "One of the main goals of this work is for the DC-IGNs to learn a representation of the data which consists of disentangled and semantically interpretable latent variables. Following Bengio et al.[2] we would like only a small subset of the latent variables to change for sequences of inputs corresponding to real-world events.\nOne natural choice of target representation for information about scenes is that already designed for use in graphics engines. If we can deconstruct a face image by splitting it into variables for pose, light, and shape, we can trivially represent the same kinds of transformations that these variables are used for in graphics applications. Figure 3 depicts the representation which we will attempt to learn.\nTo achieve this goal, we perform a training procedure which directly targets this definition of disentanglement. We organize our data into minibatches corresponding to changes in only a single scene variable (azimuth angle, elevation angle, and azimuth angle of the light source); these are transformations which might occur in the real world. We will term these the extrinsic variables. To train this representation, we adopt a procedure that is simila to SGVB, with the key differences bein :\n1. Select at random a latent variable ztrain which we wish to correspond to one of {azimuth angle, elevation angle, azimuth of light source}\n2. Select at random a minibatch in which that extrinsic scene variable changes\n3. Show the network the first example in the minibatch and capture its latent representation z1\nOutput\nfrom encoder to encoder\nBackpropagation\n4. For all subsequent examples in the minibatch, hold all zi 6= ztrain constant and equal to z1i (these latents are \u201dclamped\u201d)\n5. Backpropagate error as usual in the decoder, but pass zero gradients for all zi 6= ztrain. The gradient at zi should be passed through unchanged.\n6. Continue backpropagation through the encoder using the modified gradient.\nAlong with these we generate minibatches in which the three extrinsic scene variables are held fixed but all other properties of the image (variables controlling the identity and appearance of the face) change randomly. These intrinsic properties of the model are represented by the remainder of the latent variables. Training of these remaining latent variables is similar to the procedure above, but instead of a single element being trained, ztrain = z[4,n]. That is, we clamp the latent variables corresponding to the extrinsic properties of pose and lighting and only allow the intrinsic latents z[4,n] to change within a batch; they are similarly the only latents given nonzero gradients. These minibatches varying intrinsic properties are interspersed stochastically with those varying the extrinsic properties. The probability of selecting an intrinsic batch is proportional to a hyperparameter we term the \u201dshape bias\u201d, for it represents a \u201dbias\u201d in the training set towards examples which vary the underlying shape of the face.\nThis training method leads to networks whose latent variables have a strong equivariance with the generating parameters, as shown in figure 9. This allows the value of the true generating parameter to be trivially extracted from the latent representation. However, this training method\ndoes not directly enforce the invariance of the clamped latents to transformations to which one might like them to be insensitive. Instead, it gets its invariances from the tendency of neural networks\u2019 representations to approach locally optimal compression. Since information about, for example, the elevation of the face is explicitly required to be stored in z2, any information about the elevation stored in any other zi would be redundant and wasteful of that zi\u2019s information-carrying capacity."}, {"heading": "3.2.1 Invariance targeting", "text": "In order to further improve the invariance of the procedure defined in section 3.2, we propose a procedure for explicitly training invariance of the latent variables to orthogonal transformations. As depicted in figure 4, we replace the zero gradients of the zi 6= ztrain from section 3.2 with an error gradient pointing in the direction of greatest difference from the mean of the latent variables we desire to be invariant in this minibatch. During training, this pushes the values which we desire to be constant throughout the batch towards one another, decreasing the variation that these latents experience due to orthogonal variables. This regularizing force needs to be scaled to be much smaller than the true training signal; empirically, a factor of 1/100 works well."}, {"heading": "4 Experiments", "text": "We experimented with two common visual perception tasks: (1) invariant recognition task to predict whether two faces are same or equal, and (2) using the learnt DCIGN representation as a distance metric in likelihood-free\n\ud835\udf191 \ud835\udefc1\n\ud835\udefc \ud835\udf19L 1 \ud835\udf19L z[4,n]z = z3z2z1 \ud835\udf19corresponds to Output first sample in batch x1 from encoder to encoder intrinsic properties (shape, texture, etc) same as output for x1 z[4,n]z3z2z1 later samples in batch xi z[4,n]z3z2z1 unique for each xi in batch zero error signal for clamped outputs zero error signal for clamped outputs\nerror signal from decoder\n\u2207zk = zk - mean zk\nBackpropagation z[4,n]z3z2z1 z[4,n]z3z2z1\nBackpropagation with invariance targeting\nz[4,n]z3z2z1\nk \u2208 batch\nk \u2208 batch\nCaption: Training on a minibatch in which only \ud835\udf19, the azimuth angle of the face, changes. During the forward step, the output from each component z_k != z_1 of the encoder is forced to be the same for each sample in the batch. This reflects the fact that the generating variables of the image which correspond to the desired values of these latents are unchanged throughout the batch. By holding these outputs constant throughout the batch, z_1 is forced to explain all the variance within the batch, i.e. the full range of changes to the image caused by changing \ud835\udf19. During the backward step, backpropagation of gradients happens only through the latent z_1, with gradients for z_k != z_1 set to zero. This corresponds with the clamped output from those latents throughout the batch.\nCaption: In order to directly enforce invariance of the latents corresponding to\nproperties of the image which do not change within a given batch, we calculate\ngradients for the z_k != z_1 which move them towards the mean of each\ninvariant latent over the batch. This is equivalent to regularizing the latents z_{[2,n]} by the L2 norm of (zk - mean zk).\ni i\nFigure 4: Backpropagation with invariance targeting. In order to directly enforce invariance of the latents corresponding to properties of the image which do not change within a given batch, we calculate gradients for each zk predicted latent in the batch which moves it towards the mean of each invariant latent over the batch. This is equivalent to regularizing the latents z[2,n] by the L2 norm of zk \u2212 mean\nk\u2208batch\nzk.\nbayesian inference. These tasks stress the importance of a disentangled representation \u2013 given a factored set of zi\u2019s, invariance follows automatically by ignoring subsets of zi\u2019s. For face recognition, if we denote zidentity \u2208 {zshape, ztexture} to be latents corresponding to facial identity and {zpose, zlight} to be rest of the variables, then we can extract an invariant represent of any given face by using zidentity and ignoring all other zi\u2019s. Similarly, for using Z as a distance metric, we may wish to preserve all transformations to be equivariant by keeping all Z\u2019s around.\nVetter et al.[5] first proposed using 3D laser scanned face data to learn a deformable 3D face model (3DMM). During training of 3DMM model, 3D faces are manually aligned to be in a canonical pose and a statistical shape based model is obtained with tunable vector based representation. In all our experiments, we train using the synthetic data obtained from the 3DMM model and test it out on real face data with ground truth 3D annotations (geometry and texture) obtained from [22]. This model allows us to generate synthetic images of faces by systematically varying pose, light, shape and texture. Moreover, 3DMM also gives us a good ground truth metric to test the generalization capabilities of our learnt 3D rendering function. We trained our model on about 10000 batches of faces, where each batch consists of about 30 faces on average with random variations on face identity variables (shape/texture), pose and lighting. We used rmsprop[24] learning algorithm during training and set the meta learning rate to be equal to 0.0005, the momentum decay to be 0.1 and weight decay to be 0.01."}, {"heading": "4.1 3D Statistical Shape Modeling", "text": "In order to test the equivariance power of DC-IGN\u2019s learnt representation, we use Z as a summary statistic in a generative model based on the 3DMM face model. Given a pretrained DC-IGN, we are interested in the following question \u2013 how well can we recover latents of a compositional\nStochastic Scene Generator\nGraphics Simulator\nDC-IGN ENCODER\ndata ID\nS \u21e0 P (S)\nIR \u21e0 g(S)\nDistance Metric (Likelihood-free)\nABC distance function (eg. L1) \u21e2(\u232b(ID), \u232b(IR))\n\u232b(ID) \u232b(IR)\nDECODER\nFigure 5: Deep DC-IGN encoder with a domain-specific decoder: In order to test the equivariance of DC-IGNs learnt representation, we can use DC-IGN encoder output as summary statistic for likelihood-free bayesian inference. We use the 3DMM Basel face model as prior on 3D faces. The prior P (S) is a multivariate gaussian distribution which generates a set of latent variables Si denoting pose, lighting, shape and texture. The scene S can be passed to a graphics simulator which produces the hypothesis image IR. Given IR and observation image ID, the task is to infer P (S|ID). Since the likelihood function is not available in closed-form, we can simply resort to approximate Bayesian computation (ABC) based MCMC[27]. The DC-IGN encoder produces summary statistics for both the observation \u03bd(ID) and current hypothesis \u03bd(IR), where the current hypothesis is accepted or rejected.\n3D shape model by using latent representations Z? After convergence, this inference procedure produces 3D vertices and reflectance values for each vertex given a single static image. Intuitively, we are essentially using DC-IGN representations along with a rich domain-specific face decoder model for coarse-to-fine inference. Alternatively, we can also use the learnt representations from our model to learn data-driven proposals similar to [10].\nBayesian inference in our setting is difficult as the likelihood is unavailable in closed form due to the presence of the rendering pipeline and subsequent DC-IGN computations. However, there has been considerable progress in the statistics community to perform likelihood free Bayesian inference, which is often termed as Approximate Bayesian Computation (ABC). Therefore, ABC provides a natural formulation to describe inverse graphics in likelihood-free settings. We define a simple distance function on the summary statistics obtained from the DC-IGN-encoder net-\nworks and apply a variant of the probabilistic approximate MCMC algorithm [27] to do inference.\nWe used a pre-trained DC-IGN encoder-decoder network (from any one of the training procedures) on the synthetic face dataset, followed by stripping the decoder part of the network. The DC-IGN encoder can now be used as a summary statistic function \u03bd : x \u2192 Z, which calculates the latent representation Z for any given image x. We replace the DC-IGN decoder network by a richer generative model based on the 3DMM face model. This is similar in spirit to Nair et al.[21], where a domain-specific decoder is used along with a generic encoder to do analysis-by-synthesis based generative modeling.\nThe 3DMM face model consists of a set of two face related latent variables (Sshape, Stexture,), each with\n200 dimensions and sampled from {N (0, 1)}200i=1. Additionally, there are scene variables including: (Slight, Selevation) \u223c Uniform(\u221290, 90) and {Sjcamera}j\u2208{x,y,z} \u223c Uniform(0, 1). Let us denote IB(.) as the indicator function of the set B and A ,ID = {IR|\u03c1(\u03bd(ID), \u03bd(IR)) \u2264 }, where \u03c1 can be a suitable distance metric such as L1. As shown in figure 5, g(S) denotes the graphics simulator which generates rendering primitives (images, mesh, etc.) given a scene S. We can now formulate the image interpretation task as approximately sampling the posterior distribution of scene S given observations (ID):\n\u03c0(S|ID) \u2248 \u03c0 (S|ID), (4) where \u03c0 (S|ID) (5)\n= \u222b \u03c0 (S, IR|ID)dIR (6)\n\u221d \u03c0(S)\u03b4g(S)(IR)IA ,ID (IR)\u222b A ,ID\u00d7S \u03c0(S)\u03b4g(S)(IR)dS (7)\nDuring inference, we make use of elliptical proposals on large set of coupled continuous random variables S. Neal [4] and Murray et al. [20] studied generative models with Gaussian priors having zero mean and arbitrary covariance structure. Let us assume that X \u223c N (0,\u03a3). Given a state S of the scene, a new state S\u2032 can be efficiently proposed as follows: S\u2032 = \u221a 1\u2212 \u03b12S + \u03b1\u03b8, where \u03b8 \u223c N (0,\u03a3) and \u03b1 \u223c Uniform(\u22121, 1). Based on figure 6(a), as inference proceeds the pixel difference between the current hypothesis of the model and observation decreases. We also quantitatively compare DC-IGN\u2019s latent representation with a pre-trained deep convolutional network (trained on Imagenet [25] version:imagenet-caffe-ref ). We used several intermediate layers of the pre-trained network as the summary statistics and show quantitative performance on test data in comparison to various DC-IGN models. The DC-IGN models were trained with different number of dimensions for Z. From figure 6(b), it is clear that DC-IGN code with a relatively larger dimensions is necessary to capture the rich set of transformations on real faces. This emphasizes the fact that to obtain a disentangled representation, data reconstruction through a small bottleneck graphics code layer might be more beneficial than just tuning for recognition as is the case with supervised CNNs."}, {"heading": "4.2 Recognition", "text": "In order to test the invariance properties of DC-IGN, we performed a task to predict whether two given faces are same or different. This experiment was first proposed by Yildrim et al[28] and we closely follow this procedure for all neural network baselines.\nThe test dataset from Yildrim et al. consists of 96 pairs\nof faces with varying degree of pose, lighting, shape and texture. In order to ensure that the task is interesting and hard, they also obtained accuracy of the prediction from human subjects using Amazon Turk. Obtaining an invariant representation is hard as the two faces can have drastically different transformations. We also obtained a finetuned CNN on the SURF-W dataset from Yildrim et al., which was fine-tuned with 400 famous people under different light-pose conditions. As shown in figure 7, humans perform at 78 percent which is higher than all deep learning based system. It is perhaps surprising that DC-IGN performs competitively with other baseline models even when trained completely unsupervised using the random transform training procedure in section 3.1.\nIn order to calculate test accuracy of the same vs different task for all baseline models, we follow the same-vsdifferent test procedure first proposed in [28]. We first scaled each test image representation independently to be centered at 0 and to have a standard deviation of 1. Then, for each pair of images, we calculated the Pearson correlation coefficient between the two network representations. We searched for a threshold correlation \u2208 [1, 1] such that the any given model\u2019s performance will be highest with respect to ground truth. The search was such that the pairs of correlation values lower than the threshold were assigned different, and the pairs of equal or higher correlation values than the threshold were assigned same by the model. We report results based upon the threshold that gives the highest performance for each baseline model separately."}, {"heading": "4.3 Generalization of the rendering function", "text": "The decoder network learns an approximate rendering engine as shown in figure(8,10). Given a static test image, the encoder network produces the latents Z depicting scene variables such as light, pose, shape etc. Similar to an offthe-shelf rendering engine, we can selectively clamp zi\u2019s and vary the rest. For example, as shown in figure 10, given the original test image, we can vary zlight by clamping all\nother nodes and conditionally generating the image through the decoder network. It is perhaps surprising that the learnt decoder network is able to approximately function like a 3D rendering engine. We performed similar analysis on other transformations as shown in figure 8.\nWe also quantitatively measured the decoder\u2019s generalization capability to render across arbitrary viewpoints and lighting conditions. We generated 100 test batches from the 3DMM model where each batch contained a distinct face with varying pose (azimuth in range -1.5 to 1.5 with 0.05 step size). Within each batch, we compute the latent representation Z for every image and extracted the latents corresponding to face identity zidentity \u2208 z[4, n] as shown in figure 3. Within each batch, there is a face image with pose azimuth angle equal to zero (frontal image). For same face identity, we ideally expect zidentity of all images within a batch to be equal. We calculated the cosine distance between the latent representation zidentity of every image with the frontal image and averaged the score across all batches. As shown in figure 9(d), images close to the frontal face have almost identical latent representations but the similarity decays as azimuth goes away from zero. In other words, the invariance characteristics gradually degrades with viewpoint extrapolation. We perform similar analysis by generating separate datasets by varying elevation and lighting conditions.\nWe tested our model with three different DC-IGN network configurations \u2013 (1) DC-IGN with shape bias (see section3.2), (2) DC-IGN without shape bias and (3) DCIGN without shape bias but with targeted invariance (see section 3.2.1). All three networks do reasonably well in predicting azimuth, light and elevation (figure 9(a,b,c). Interestingly, as shown in figure 9(a), the encoder network seems to have learnt a switch node to separately process left and right profile faces. However, as shown in figure 9(d,e,f), the network with high shape bias seems to have the strongest invariance properties. This intuitively makes sense as this network saw relatively larger variations in facial identities. It is also interesting to note (figure 9(a) and figure8(b)) that in comparison to other transformations, generalization across the azimuth suffers most from radical extrapolation. This is likely because the lateral out-of-plane rotation transformation is probably one of the hardest transformations to learn for faces. However, this may improve by training the model with additional training data."}, {"heading": "5 Discussion", "text": "We have shown that it is possible to train a deep convolutional inverse graphics network with interpretable graphics code layer representation just from raw images. By utilizing a deep convolution and de-convolution architecture\nwithin a variational autoencoder formulation, our model can be jointly trained using back-propagation using the stochastic variational objective function [11]. We proposed various training procedures to force the network to learn disentangled representations. Using 3D face analysis as a working example, we have demonstrated the invariant and equivariant characteristics of the learnt representation. Moreover, as highlighted in section 4.1, the DC-IGN deconvolutional network based decoder can be replaced by a domain-specific decoder for fine-grained model-based inference.\nTo scale our approach to handle more complex scenes, it will likely be important to experiment with deeper architectures in order to handle large number of object categories within a single network architecture. It is also very appealing to design a spatio-temporal based convolutional architecture to utilize motion in order to handle complicated object transformations. The current formulation of SGVB is restricted to continuous latent variables. However, real-world visual scenes contain unknown number of objects that move in and out of frame. Therefore, it might be necessary to extend this formulation to handle discrete distributions. There has been recent work[14] on trying to approximate discrete distribution by using particle based variational approximations. An interesting future direction is to construct a mixed continuous-discrete variational approximationQmixed and to iteratively inter-leave optimization of the discrete parameters with SGVB.\nWe hope that our work motivates further research into auto-\nmatically learning disentangled representations using variants of different encoder and decoder networks. The most general form of a decoder network can be thought of as a stochastic graphics program that conditions images given its latent graphics code. In the future, we aim to explore a hierarchy of coarse-to-fine encoder-decoder programs, where each encoder reasonably predicts a small set of latent variables and passes a noisy graphics code for the subsequent encoder-decoder program for refinement. This explicit hierarchy to sequentially handle different transformations may allow us to learn the network using relatively fewer training examples."}, {"heading": "Acknowledgements", "text": "We thank Thomas Vetter for giving us access to the Basel face model. T. Kulkarni was graciously supported by the Leventhal Fellowship. This research was supported by ONR award N000141310333, ARO MURI W911NF-131-2012 and CBMM."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Regression and classification using gaussian process priors", "author": ["J. Bernardo", "J. Berger", "A. Dawid", "A. Smith"], "venue": "In Bayesian Statistics 6: Proceedings of the sixth Valencia international meeting,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "A morphable model for the synthesis of 3d faces", "author": ["V. Blanz", "T. Vetter"], "venue": "In Proceedings of the 26th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Learning the irreducible representations of commutative lie groups", "author": ["T. Cohen", "M. Welling"], "venue": "arXiv preprint arXiv:1402.4437,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["I. Goodfellow", "H. Lee", "Q.V. Le", "A. Saxe", "A.Y. Ng"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The informed sampler: A discriminative approach to bayesian inference in generative computer vision models", "author": ["V. Jampani", "S. Nowozin", "M. Loper", "P.V. Gehler"], "venue": "arXiv preprint arXiv:1402.0859,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Inverse graphics with probabilistic cad models", "author": ["T.D. Kulkarni", "V.K. Mansinghka", "P. Kohli", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1407.1339,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Variational particle approximations", "author": ["T.D. Kulkarni", "A. Saeedi", "S. Gershman"], "venue": "arXiv preprint arXiv:1402.5715,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Opendr: An approximate differentiable renderer", "author": ["M.M. Loper", "M.J. Black"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Approximate bayesian image interpretation using generative probabilistic graphics programs", "author": ["V. Mansinghka", "T.D. Kulkarni", "Y.N. Perov", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In Artificial Neural Networks and Machine Learning\u2013", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Elliptical slice sampling", "author": ["I. Murray", "R.P. Adams", "D.J. MacKay"], "venue": "arXiv preprint arXiv:1001.0175,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Analysis-bysynthesis by learning to invert generative black boxes", "author": ["V. Nair", "J. Susskind", "G.E. Hinton"], "venue": "In Artificial Neural Networks-ICANN", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "A 3d face model for pose and illumination invariant face recognition", "author": ["P. Paysan", "R. Knothe", "B. Amberg", "S. Romdhani", "T. Vetter"], "venue": "Proceedings of the 6th IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS) for Security, Safety and Monitoring in Smart Environments,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Optimizing Neural Networks that Generate Images", "author": ["T. Tieleman"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Approximate bayesian computation (abc) gives exact results under the assumption of model error", "author": ["R.D. Wilkinson"], "venue": "Statistical applications in genetics and molecular biology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Explaining monkey face patch system as deep inverse graphics", "author": ["I. Yildrim", "T. Kulkarni", "W. Freiwald", "J. Tenenbaum"], "venue": "In Computational and Systems Neuroscience,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 14, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 52, "endOffset": 60}, {"referenceID": 11, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 52, "endOffset": 60}, {"referenceID": 7, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 122, "endOffset": 133}, {"referenceID": 22, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 122, "endOffset": 133}, {"referenceID": 15, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 122, "endOffset": 133}, {"referenceID": 0, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 153, "endOffset": 164}, {"referenceID": 25, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 153, "endOffset": 164}, {"referenceID": 18, "context": "Models such as Convolutional Neural Networks (CNNs) [15, 12], Restricted Boltzmann Machines (RBM) based generative models [8, 23, 16], and Auto-encoders [1, 26, 19] have been successfully applied to produce multiple layers of increasingly abstract visual representations.", "startOffset": 153, "endOffset": 164}, {"referenceID": 5, "context": "[6] have considered this problem by proposing a theoretical framework to learn irreducible representations having both invariances and equivariances, coming up with the best representation for any given task is an open question.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement.", "startOffset": 122, "endOffset": 131}, {"referenceID": 5, "context": "The Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement.", "startOffset": 122, "endOffset": 131}, {"referenceID": 6, "context": "The Vision as inverse graphics approach offers an interesting perspective to various desiderata for a good representation [2, 6, 7]: invariance, meaningfulness of representations, abstractions and disentanglement.", "startOffset": 122, "endOffset": 131}, {"referenceID": 9, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 17, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 16, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 12, "context": "Recent work in inverse graphics [10, 18, 17, 13] follows a general strategy of first defining a probabilistic or nonprobabilistic model with latent parameters, followed by an inference or optimization algorithms to find the most appropriate set of latent parameters given observations.", "startOffset": 32, "endOffset": 48}, {"referenceID": 23, "context": "[24] moved beyond this two stage pipeline by using a generic encoder network with a domainspecific decoder network to approximate a 2D rendering function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "To achieve this, we employ a deep directed graphical model with many layers of convolution and de-convolution operators that is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 10, "context": "We follow the variational autoencoder[11] architecture with several variations.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.", "startOffset": 28, "endOffset": 39}, {"referenceID": 22, "context": "Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.", "startOffset": 28, "endOffset": 39}, {"referenceID": 15, "context": "Unlike most RBM based models[8, 23, 16], our approach is trained using back-propagation from the data reconstruction term and the variational bound.", "startOffset": 28, "endOffset": 39}, {"referenceID": 10, "context": "[11] proposed the SGVB algorithm to learn generative models with continuous latent variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Generative stochastic networks [3] which learn the transi-", "startOffset": 31, "endOffset": 34}, {"referenceID": 28, "context": "[29] which is used to produce image representations using convolutional decomposition of images under a sparsity constraint.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17].", "startOffset": 92, "endOffset": 104}, {"referenceID": 17, "context": "A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17].", "startOffset": 92, "endOffset": 104}, {"referenceID": 16, "context": "A number of inverse graphics inspired methods have recently been proposed in the literature [10, 18, 17].", "startOffset": 92, "endOffset": 104}, {"referenceID": 8, "context": "[9] (transforming autoencoders) and Tieleman [24] which uses a domain specific decoder to reconstruct input images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[9] (transforming autoencoders) and Tieleman [24] which uses a domain specific decoder to reconstruct input images.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "Our work is similar in spirit to these works but has some key differences: (a) It uses a very generic convolutional architecture in the encoder and decoder networks to enable efficient learning on large datasets and image sizes, (b) it can handle single static frames as opposed to pair of images required in [9], and (c) it is generative.", "startOffset": 309, "endOffset": 312}, {"referenceID": 10, "context": "There are two reasons for using this parametrization: (1) Gradients of samples with respect to parameters \u03b8 of Q can be easily obtained using the reparametrization trick proposed in [11], and (2) Various statistical shape models trained on 3D scanner data such as faces have the same multivariate normal latent distribution (see section 4.", "startOffset": 182, "endOffset": 186}, {"referenceID": 1, "context": "[2] we would like only a small subset of the latent variables to change for sequences of inputs corresponding to real-world events.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] first proposed using 3D laser scanned face data to learn a deformable 3D face model (3DMM).", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "In all our experiments, we train using the synthetic data obtained from the 3DMM model and test it out on real face data with ground truth 3D annotations (geometry and texture) obtained from [22].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "We used rmsprop[24] learning algorithm during training and set the meta learning rate to be equal to 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "Since the likelihood function is not available in closed-form, we can simply resort to approximate Bayesian computation (ABC) based MCMC[27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "Alternatively, we can also use the learnt representations from our model to learn data-driven proposals similar to [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "works and apply a variant of the probabilistic approximate MCMC algorithm [27] to do inference.", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "[21], where a domain-specific decoder is used along with a generic encoder to do analysis-by-synthesis based generative modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Neal [4] and Murray et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 19, "context": "[20] studied generative models with Gaussian priors having zero mean and arbitrary covariance structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We also quantitatively compare DC-IGN\u2019s latent representation with a pre-trained deep convolutional network (trained on Imagenet [25] version:imagenet-caffe-ref ).", "startOffset": 129, "endOffset": 133}, {"referenceID": 27, "context": "This experiment was first proposed by Yildrim et al[28] and we closely follow this procedure for all neural network baselines.", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "In order to calculate test accuracy of the same vs different task for all baseline models, we follow the same-vsdifferent test procedure first proposed in [28].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "We searched for a threshold correlation \u2208 [1, 1] such that the any given model\u2019s performance will be highest with respect to ground truth.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "We searched for a threshold correlation \u2208 [1, 1] such that the any given model\u2019s performance will be highest with respect to ground truth.", "startOffset": 42, "endOffset": 48}, {"referenceID": 10, "context": "within a variational autoencoder formulation, our model can be jointly trained using back-propagation using the stochastic variational objective function [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "There has been recent work[14] on trying to approximate discrete distribution by using particle based variational approximations.", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN) that aims to learn an interpretable representation of images that is disentangled with respect to various transformations such as object out-of-plane rotations, lighting variations, and texture. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11]. We propose training procedures to encourage neurons in the graphics code layer to have semantic meaning and force each group to distinctly represent a specific transformation (pose,light,texture,shape etc.). Given a static face image, our model can re-generate the input image with different pose, lighting or even texture and shape variations from the base face. We present qualitative and quantitative results of the model\u2019s efficacy to learn a 3D rendering engine. Moreover, we also utilize the learnt representation for two important visual recognition tasks: (1) an invariant face recognition task and (2) using the representation as a summary statistic for generative modeling.", "creator": "LaTeX with hyperref package"}}}