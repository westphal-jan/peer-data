{"id": "1503.04843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2015", "title": "More General Queries and Less Generalization Error in Adaptive Data Analysis", "abstract": "Adaptivity is an important feature of data analysis - typically the choice of questions to be asked about a dataset depends on previous interactions with the same dataset. However, generalization errors are typically limited in a non-adaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS' 14) initiated the formal investigation of this problem and provided the first upper and lower limits of the achievable generalization error for adaptive data analysis.", "histories": [["v1", "Mon, 16 Mar 2015 20:48:42 GMT  (23kb,D)", "http://arxiv.org/abs/1503.04843v1", null], ["v2", "Tue, 10 Nov 2015 02:01:05 GMT  (0kb,I)", "http://arxiv.org/abs/1503.04843v2", "This paper was merged with another manuscript and is now subsumed byarXiv:1511.02513"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["raef bassily", "adam smith", "thomas steinke", "jonathan ullman"], "accepted": false, "id": "1503.04843"}, "pdf": {"name": "1503.04843.pdf", "metadata": {"source": "CRF", "title": "More General Queries and Less Generalization Error in Adaptive Data Analysis", "authors": ["Raef Bassily", "Adam Smith", "Thomas Steinke", "Jonathan Ullman"], "emails": ["bassily@psu.edu", "asmith@psu.edu", "tsteinke@seas.harvard.edu", "jullman@cs.columbia.edu"], "sections": [{"heading": null, "text": "Specifically, suppose there is an unknown distribution P and a set of n independent samples x is drawn from P . We seek an algorithm that, given x as input, \u201caccurately\u201d answers a sequence of adaptively chosen \u201cqueries\u201d about the unknown distribution P . How many samples n must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy?\nIn this work we make two new contributions towards resolving this question:\n1. We give upper bounds on the number of samples n that are needed to answer statistical queries that improve over the bounds of Dwork et al.\n2. We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and the important class of convex risk minimization queries.\nAs in Dwork et al., our algorithms are based on a connection between differential privacy and generalization error, but we feel that our analysis is simpler and more modular, which may be useful for studying these questions in the future.\n\u2217Pennsylvania State University, Department of Computer Science and Engineering. {bassily,asmith}@psu.edu \u2020Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. tsteinke@seas.harvard.edu \u2021Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. jullman@cs.columbia.edu\nar X\niv :1\n50 3.\n04 84\n3v 1\n[ cs\n.L G\n] 1\n6 M\nar 2\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Overview of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Overview of Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Related Work in Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . 4"}, {"heading": "2 Preliminaries 5", "text": "2.1 Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Oracles for Adaptive Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"}, {"heading": "3 From Privacy to Accuracy for Adaptive Queries 8", "text": "3.1 Differential Privacy Implies Low Expected Error . . . . . . . . . . . . . . . . . . 8 3.2 Amplifying the Success Probability . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2.1 The Bounded Censor Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 13"}, {"heading": "4 From Function Approximation to Optimization Queries 15", "text": "4.1 Oracles for Adaptive Minimization Queries . . . . . . . . . . . . . . . . . . . . . 15 4.2 Amplifying the Success Probability . . . . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "5 Applications 17", "text": "5.1 Low-Sensitivity and Statistical Queries . . . . . . . . . . . . . . . . . . . . . . . . 17 5.2 Optimization Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2.1 Minimization Over Arbitrary Finite Sets . . . . . . . . . . . . . . . . . . . 18 5.2.2 Convex Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nAcknowledgements 19\nReferences 19"}, {"heading": "1 Introduction", "text": "A common problem in empirical research is testing the significance of multiple \u201chypotheses\u201d on a finite sample of data drawn from some population. An observation is deemed significant if it is unlikely to have occurred by chance alone, and a \u201cfalse discovery\u201d occurs if the analyst incorrectly declares an observation significant. Unfortunately, false discovery has been identified as a substantial problem in the scientific community (see e.g. [Ioa05]). This problem persists despite decades of research by Statisticians on methods for preventing false discovery, such as the widely used Bonferroni Correction [Bon36, Dun61] and the Benjamini-Hochberg Procedure [BH95].\nFalse discovery is often attributed to misuse of statistics. However, an alternative explanation is that the prevalence of false discovery arises from the inherent adaptivity in the data analysis process\u2014the choice of hypotheses to test depends on previous interactions with the data (see e.g. [Ioa05, GL13]). This problem was first formally modeled and studied in a striking recent paper by Dwork, Feldman, Hardt, Pitassi, Reingold, and Roth [DFH+15], who gave the first non-trivial algorithms for provably ensuring statistical validity in adaptive data analysis. The problem was also studied through a computational lens by [HU14, SU14], who showed that there is an inherent computational barrier to preventing false discovery in adaptive settings.\nIn this work, we extend the results of Dwork et al. [DFH+15] along two axes. First, we prove quantitatively stronger bounds on the statistical accuracy that can be achieved. Second, we significantly generalize the types of statistics that can be computed in adaptive settings. Like the algorithms of Dwork et al., all of our algorithms rely on the connection between differential privacy [DMNS06] and statistical validity, and our analysis generalizes, strengthens, and (in our opinion) simplifies this connection."}, {"heading": "1.1 Overview of Results", "text": "Following these previous papers, we formalize the problem of adaptive data analysis as follows: There is a distribution P over some finite universe X , and an oracle O that does not know P but is given n samples from P . Using its sample, the oracle must answer queries on P . Here, a query q maps a distribution P to a real-valued answer, and we assume the queries come from some family Q. The oracle\u2019s answer a to a query q is \u03b1-accurate if |a \u2212 q(P )| \u2264 \u03b1 with high probability. Importantly, the oracle\u2019s goal is to provide answers that \u201cgeneralize\u201d to the underlying distribution, rather than answers that are specific to the sample.\nWe model adaptivity by allowing the analyst to make a sequence of queries q1,q2, . . . , qk \u2208Q to the oracle, who responds with answers a1, a2, . . . , ak . In the adaptive setting, the query qj may depend on the previous queries and answers q1, a1, . . . , qj\u22121, aj\u22121 arbitrarily. We say the oracle is \u03b1-accurate given n samples for k adaptively chosen queries if, when given n samples from an arbitrary distribution P , the oracle accurately responds to any adaptive analyst that makes at most k queries with high probability.\nDwork et al. [DFH+15] considered the family of statistical queries [Kea93]. A statistical query q is specified by a function p : X \u2192 [0,1] and is defined as q(P ) = Ez\u2190RP [p(z)]. A simple analysis shows that, when k queries are specified non adaptively, (i.e. independent of previous answers), then simply evaluating the query on the sample is \u03b1-accurate with high probability as long as the number of samples n satisfies n & logk/\u03b12. However, when the queries can be chosen adaptively, the empirical average performs exponentially worse, requiring n & k/\u03b12 to guarantee accuracy.\nDwork et al. [DFH+15], showed how to achieve dramatically better accuracy guarantees for statistical queries, namely that n & min { \u221a k \u03b12.5 , \u221a log |X |\u00b7log3/2 k \u03b13.5 } samples suffice, which can be\nexponentially smaller than what is needed when outputting the sample mean. However, these bounds suffer from a large dependence on the accuracy level \u03b1.\nOur first contribution is to give a tighter and simpler analysis of (a slight variant of) their algorithms that yields the best known accuracy guarantees for adaptive statistical queries. In particular, we show that O\u0303( \u221a k/\u03b12) samples suffice, which is the optimal dependence on \u03b1.\nAlthough statistical queries are a surprisingly general primitive for data analysis, we would like to be able to ask more general queries on the distribution P that capture a wider variety of machine learning and data mining tasks. Specifically, in this work we consider two general types of queries: low-sensitivity queries and search queries.\nLow-sensitivity queries are a generalization of statistical queries that are specified by an arbitrary function p : X n \u2192 R satisfying |p(x) \u2212 p(x\u2032)| \u2264 1/n for every x,x\u2032 \u2208 X n differing on exactly one element. The query applied to the population is defined to be q(P ) = Ex\u2190RP n[p(x)].\nSearch queries are a broad generalization of low-sensitivity queries to arbitrary output domains. The query is specified by a loss function L : X n \u00d7\u0398 \u2192 R that is low-sensitivity in its first parameter, and the goal is to output \u03b8 \u2208\u0398 that is \u201cbest\u201d in the sense that it minimizes the average loss. Specifically, q(P ) = argmin\u03b8\u2208\u0398Ez\u2190RP n[L(z;\u03b8)]. An important special case is when L is convex in the parameter \u03b8, which captures fundamental tasks in machine learning such as linear and logistic regression.\nOur second contribution is to give the first bounds on the sample complexity required to answer a large number of adaptively chosen low-sensitivity queries and search queries. Our sample complexity bounds are summarized in Figure 1.\nOn Optimality: Although our bounds improve on [DFH+15] for statistical queries, and are the first non-trivial bounds for other families of queries, we cannot prove that any of these bounds are optimal. Even for non-adaptive statistical queries, n & log(k)/\u03b12 samples are necessary, and [HU14, SU14] showed that n & min{ \u221a k, \u221a log |X |}/\u03b1 samples are necessary to answer adaptively chosen statistical queries. These lower bounds are information theoretic and apply to computationally unbounded oracles.\nHowever, it is known that, for every family of queries we consider, the differentially private mechanisms we use achieve optimal sample complexity [BUV14, BST14]. It is also known that the privacy parameters we require cannot be improved. Thus, any improvement to our bounds (beyond logarithmic factors) must come from using an oracle that is not differentially private.\nOn Computational Complexity: Throughout, we will assume that the analyst only issues queries q such that q(x) can be evaluated in time poly(n, log |X |), and are not the bottleneck in computation. Thus the empirical answer can be given in time poly(n, log |X |). When k n2 our algorithms have similar running time. However, when answering k n2 queries, our algorithms suffer running time poly(n, |X |). Since the oracle\u2019s input is of size n \u00b7 log |X |, these algorithms cannot be considered computationally efficient. For example, if X = {0,1}d for some dimension d, then in the non adaptive setting poly(n,d) running time would suffice, whereas our algorithms require poly(n,2d) running time. Unfortunately, this running time is known to be optimal, as [HU14, SU14] (building on hardness results in privacy [Ull13]) showed that, assuming exponentially hard one-way functions exist, any poly(n,2o(d)) time oracle that answers k =\u03c9(n2) statistical queries is not even 1/3-accurate."}, {"heading": "1.2 Overview of Techniques", "text": "Following Dwork et al. [DFH+15], the main technique we use is a connection between differential privacy and generalization. Intuitively, differential privacy guarantees that the distribution of outputs given by the oracle does not depend \u201ctoo much\u201d on any one of the samples it is given. Differential privacy can be seen as a strong stability guarantee that behaves well under adaptive data analysis. Thus, this work fits within the rich line of work in machine learning connecting algorithmic stability and generalization (cf. [BE02, SSSS10]).\nWe show that a differentially private algorithm that provides answers to adaptive queries that are close to the empirical value on the sample gives answers that generalise to the underlying distribution. In particular, for the class of low-sensitivity queries, we obtain the following theorem.\nTheorem 1.1 (Main \u201cTransfer Theorem\u201d). Let O be an oracle that takes as input a sample x \u2208 X n and answers k adaptively chosen low-sensitivity queries. Suppose O satisfies the following.\n1. The answers O gives are \u03b1-accurate with respect to the sample x. That is,\nE [ max j\u2208k \u2223\u2223\u2223qj(x)\u2212 aj \u2223\u2223\u2223] \u2264 \u03b1, where q1, \u00b7 \u00b7 \u00b7 ,qk : X n\u2192 R are the low-sensitivity queries that are asked and a1, \u00b7 \u00b7 \u00b7 , ak \u2208 R are the answers given. The expectation is taken only over O\u2019s random coins\u2014the sample x, and the sequence of queries q1, \u00b7 \u00b7 \u00b7 ,qk can be arbitrary.\n2. O satisfies (\u03b1,\u03b1)-differential privacy. (Definition 2.6)\nThen there exists an oracleO\u2032 that takes as input a sample x\u2032\u2190R P n \u2032 where P is an arbitrary distribution over X , and answers k adaptively chosen low-sensitivity queries such that, if n\u2032 =O(n\u00b7log(1/\u03b2)), then O gives O(\u03b1)-accurate answers with respect to the population P with high probability. That is,\nP [ max j\u2208k \u2223\u2223\u2223\u2223\u2223 Ex\u2190RP n [qj(x)]\u2212 aj \u2223\u2223\u2223\u2223\u2223 \u2264O(\u03b1)] \u2265 1\u2212 \u03b2,\nwhere the probability is taken only over the choice of x and O\u2019s random coins. Moreover, the running time of O\u2032 is roughly the same as that of O.\nThe results in Figure 1 follow from combining Theorem 1.1 (or an analogous theorem for minimization queries) with known differentially private algorithms.\nCompared to the results of Dwork et al., Theorem 1.1 requires a weaker privacy guarantee: we require (\u03b1,\u03b1)-differential privacy, instead of (roughly) (\u03b1, (\u03b2/k)1/\u03b1)-differential privacy. It also generalizes this phenomenon beyond statistical queries.\nOur analysis differs from that of Dwork et al. in two key ways. First, Dwork et al. analyze the algorithm one query at a time. That is, they show that if the oracle is differentially private for suitable parameters, then the oracle answers each query qj accurately. In order to argue that the oracle answers every query accurately (even with just a non-zero probability), they need to prove that each query is answered accurately with very high probability. To do so, they rely on an involved analysis that is heavily tailored to statistical queries and introduces a poor dependence on the privacy parameters.\nWe give a simple proof that, if the oracle is differentially private, then it answers every query accurately with at least constant probability. As a feature of its simplicity, the analysis generalizes to richer families of queries. Our proof introduces the concept of a monitoring algorithm. Roughly, the monitoring algorithm simulates the interaction between the oracle and the adversary and outputs a query based on the entire interaction, thereby reducing analyzing the entire interaction to analyzing just the monitoring algorithm. In our setting, we will have the monitoring algorithm output the least accurate query in the whole interaction. Thus, if there were even one query in the interaction that is inaccurate, then the monitoring algorithm would output a single query that is inaccurate. By a simple argument, there is at least a constant probability that the monitor\u2019s query is answered accurately, so there is also a constant probability that every query in the interaction is answered accurately.\nThe next step is to amplify the probability of success. (Dwork et al. avoid this step.) In the non-adaptive setting, amplification can be easily achieved by running the oracle a few times on independent samples, and taking some sort of median of its answers. However, in the adaptive setting, even if each sample is independent, the answers of the oracle may not be independent. Instead, we have the oracle split its sample into a small number of subsamples. To answer any given query, it will use only one of the subsamples. We then design a \u201ccensor\u201d algorithm that can identify when the oracle gives an incorrect answer, and when this occurs, the oracle will throw out that subsample and use a new one. For any given subsample, the oracle will answer every query correctly with constant probability, therefore with high probability we only need to throw out a small number of subsamples, and thus are unlikely to exhaust the supply of subsamples."}, {"heading": "1.3 Related Work in Differential Privacy", "text": "Each of our results requires instantiating the oracle with a suitable differentially private algorithm. For statistical queries, the optimal mechanisms are the well known Gaussian Mecha-\nnism (slightly refined by [SU15]) when k is small and the Private Multiplicative Weights Mechanism [HR10] when k is large. For arbitrary low-sensitivity queries, the Laplace Mechanism is again optimal when k is small, and for large k we can use the Median Mechanism [RR10].\nWhen considering arbitrary search queries over an arbitrary finite range, the optimal algorithm is the Exponential Mechanism [MT07]. The first efficient differentially private algorithms for the important special case of convex minimization queries over an infinite domain were given by Dwork and Lei [DL09] and by Chaudhuri, Monteleone, and Sarwate [CMS11], leading to a long line of subsequent work. For the case of small k, we use the worst-case optimal algorithm of Bassily, Smith, and Thakurta [BST14]. When k is large, we can use an algorithm of [Ull15] that accurately answers exponentially many such queries."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Queries", "text": "Given a distribution P over X or a sample x = (x1, \u00b7 \u00b7 \u00b7 ,xn) \u2208 X n, we would like to answer queries about P or x from some family Q. We will work with several families of queries.\n\u2022 Statistical Queries: These queries are specified by a predicate q : X \u2192 [0,1], and (abusing notation) are defined as\nq(P ) = E z\u2190RP [q(z)] , q(x) = 1 n \u2211 i\u2208[n] q(xi).\nThe error of an answer a to a statistical query q with respect to P or x is defined to be\nerrx (q,a) = a\u2212 q(x) and errP (q,a) = a\u2212 q(P ).\n\u2022 \u2206-Sensitive Queries: For \u2206 \u2208 [0,1], n \u2208 N, these queries are specified by a function q : X n\u2192 R satisfying |q(x)\u2212 q(x\u2032)| \u2264 \u2206 for every pair x,x\u2032 \u2208 X n differing in only one entry. We define (abusing notation)\nq(P ) = E z\u2190RP n [q(z)] .\nThe error of an answer a to a \u2206-sensitive query q with respect to P or x is defined to be\nerrx (q,a) = a\u2212 q(x) and errP (q,a) = E z\u2190RP n [errz (q,a)] = a\u2212 q(P ).\nWe denote the set of all \u2206-sensitive queries by Q\u2206. If \u2206 = O(1/n) we say the query is low sensitivity. Note that 1/n-sensitive queries are a strict generalization of statistical queries.\n\u2022 Minimization Queries: These queries are specified by a loss function L : X n\u00d7\u0398\u2192R. We require that L has sensitivity \u2206 with respect to its first parameter, that is,\nsup \u03b8\u2208\u0398, x,x\u2032\u2208X n s.t. dHam(x,x\u2032)=1\n|L(x;\u03b8)\u2212L(x\u2032;\u03b8)| \u2264 \u2206 .\nHere \u0398 is an arbitrary set of items (sometimes called \u201cparameter values\u201d) among which we aim to chose the parameter with minimal loss, either with respect to a particular input data set x, or with respect to expectation over a distribution P .\nThe error of an answer \u03b8 \u2208\u0398 to a minimization query L : X n \u00d7\u0398\u2192R with respect to x is defined to be\nerrx (L,\u03b8) = L(x,\u03b8)\u2212min \u03b8\u2217\u2208\u0398\nL(x,\u03b8\u2217)\nand, with respect to P , is\nerrP (L,\u03b8) = E z\u2190RP n [errz (L,\u03b8)] = E z\u2190RP n [L(z,\u03b8)]\u2212 E z\u2190RP n [ min \u03b8\u2217\u2208\u0398 L(z,\u03b8\u2217) ] .\nNote that min\u03b8\u2217\u2208\u0398 E z\u2190RP n [L(z,\u03b8\u2217)] \u2265 E z\u2190RP n [min\u03b8\u2217\u2208\u0398 L(z,\u03b8\u2217)], whence\nE z\u2190RP n [L(z,\u03b8)]\u2212min \u03b8\u2217\u2208\u0398 E z\u2190RP n\n[L(z,\u03b8\u2217)] \u2264 errP (L,\u03b8) .\nNote that minimization queries (with \u0398 = R) generalize low-sensitivity queries: Given a \u2206-sensitive q : X n \u2192 R, we can define L(x,\u03b8) = |\u03b8 \u2212 q(x)| to obtain a minimization query with the same answer.\nWe denote the set of minimization queries by Qmin. We highlight two special cases:\n\u2013 Minimization for Finite Sets We denote by Qmin,D the set of minimization queries where \u0398 is finite with size at most D.\n\u2013 Convex Minimization Queries If \u0398 \u2282 Rd is closed and convex and L(x; \u00b7) is convex on \u0398 for every data set x, then the query can be answered nonprivately up to any desired error \u03b1, in time polynomial in d and \u03b1. We denote the set of all convex minimization queries by QCM."}, {"heading": "2.2 Oracles for Adaptive Queries", "text": "Our goal is to design an oracle O that answers queries on P using only independent samples x1, . . . ,xn\u2190R P . Our focus is the case where the queries are chosen adaptively and adversarially.\nSpecifically, O is a stateful algorithm that holds a collection of samples x1, . . . ,xn \u2208 X , takes a query q from some family Q as input, and returns an answer a. We require that when x1, . . . ,xn are independent samples from P , the answer a is \u201cclose\u201d to q(P ) in a sense that is appropriate for the family of queries. Moreover we require that this condition holds for every query in an adaptively chosen sequence q1, . . . , qk . Formally, we define the an accuracy game between an O and a stateful adversary A in Figure 2.\nDefinition 2.1 (Accuracy). An oracle O is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given n samples in X if for every adversary A,\nP Accn,k,Q[O,A] [ max j\u2208[k] \u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223 \u2264 \u03b1] \u2265 1\u2212 \u03b2. We will make use of average accuracy.\nDefinition 2.2 (Average Accuracy). An oracle O is \u03b1-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples in X if for every adversary A,\nE Accn,k,Q[O,A] [ max j\u2208[k] \u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223] \u2264 \u03b1. We will also use a definition of accuracy relative to the sample given to the oracle (Figure 3).\nDefinition 2.3 (Sample Accuracy). An oracle O is (\u03b1,\u03b2)-accurate with respect to samples of size n from X for k adaptively chosen queries from Q if for every adversary A,\nP SampAccn,k,Q[O,A]\n[ \u2200j \u2208 [k] \u2223\u2223\u2223\u2223errx (qj , aj)\u2223\u2223\u2223\u2223 \u2264 \u03b1] \u2265 1\u2212 \u03b2. Definition 2.4 (Average Sample Accuracy). An oracle O is \u03b1-accurate on average with respect to samples of size n from X for k adaptively chosen queries from Q if for every adversary A,\nE SampAccn,k,Q[O,A] [ max j\u2208[k] \u2223\u2223\u2223\u2223errx (qj , aj)\u2223\u2223\u2223\u2223] \u2264 \u03b1. By Markov\u2019s inequality, \u03b1\u2032-accuracy on average implies (\u03b1,\u03b2) accuracy for suitable choices\nof \u03b1,\u03b2:\nFact 2.5. For all \u03b1,\u03b2 > 0, ifO is \u03b1\u03b2-accurate on average with respect to the population (resp. sample) for k adaptively chosen queries from Q given n samples in X , then O is (\u03b1,\u03b2)-accurate with respect to the population (resp. sample) for k adaptively chosen queries from Q given n samples in X .\nBecause of the dependence on \u03b2 in the assumption, we only apply this result when \u03b2 = \u2126(1)."}, {"heading": "2.3 Differential Privacy", "text": "A key tool in our analysis is differential privacy [DMNS06]. Informally, an algorithm is differentially private if it is randomized and the distribution of its outputs does not depend \u201cmuch\u201d on any one element of its input sample. We say that two samples x,x\u2032 \u2208 X n are neighbors if they differ on exactly one entry, and denote this relation by x \u223c x\u2032.\nDefinition 2.6 (Differential Privacy). LetM : X n\u2192R be a randomized algorithm. We sayM is (\u03b5,\u03b4)-differentially private if for every x \u223c x\u2032 and every R \u2286R,\nP [M(x) \u2208 R] \u2264 e\u03b5 \u00b7P [ M(x\u2032) \u2208 R ] + \u03b4.\nA useful fact about differential privacy is that it is robust to post-processing.\nLemma 2.7 ([DMNS06]). If M : X n \u2192 R is (\u03b5,\u03b4)-differentially private, and f : R \u2192 R\u2032 is any (possibly randomized) function, then f \u25e6M : X n\u2192R\u2032 is also (\u03b5,\u03b4)-differentially private."}, {"heading": "3 From Privacy to Accuracy for Adaptive Queries", "text": "We begin by showing that differentially private algorithms that answer adaptive low-sensitivity queries and are accurate with respect to the sample, are accurate with respect to the population."}, {"heading": "3.1 Differential Privacy Implies Low Expected Error", "text": "We start with the following key lemma, which analyzes the expectation of q(x) versus q(P ) when q is allowed to depend on x in a differentially private manner.\nLemma 3.1. Let M : X n \u2192 Q\u2206 be (\u03b5,\u03b4)-differentially private where Q\u2206 is the class of \u2206-sensitive queries q : X n\u2192R. Let P be a distribution on X and let x\u2190R P n. Then\u2223\u2223\u2223\u2223\u2223 Ex,M [q(P ) | q =M(x)]\u2212 Ex,M [q(x) | q =M(x)]\n\u2223\u2223\u2223\u2223\u2223 \u2264 2\u2206(e\u03b5 \u2212 1 + \u03b4)n. Proof. The proof proceeds via a hybrid argument. Let x\u2032 \u2190R P n be independent of x. For ` \u2208 [n]\u222a {0}, define x` \u2208 X n by\nx`i = xi if i > ` and x ` i = x \u2032 i if i \u2264 `.\nThen x0 = x, xn = x\u2032, and x` \u223c x`\u22121 are neighboring databases for every ` \u2208 [n]. For ` \u2208 [n], define B` : X n \u00d7X n\u2192R by\nB`(y,z) = q(z)\u2212 q(z\u2212`) +\u2206 for q =M(y),\nwhere z\u2212` is z with the `-th row replaced by some arbitrary fixed element of X .\nSince x`\u2212` = x `\u22121 \u2212` , we have\u2223\u2223\u2223\u2223E [q(P ) | q =M(x)]\u2212E [q(x) | q =M(x)]\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223E[q(x\u2032) | q =M(x)]\u2212E [q(x) | q =M(x)]\u2223\u2223\u2223\u2223 \u2264\n\u2211 `\u2208[n] \u2223\u2223\u2223\u2223E [q(x`) | q =M(x)]\u2212E [q(x`\u22121) | q =M(x)]\u2223\u2223\u2223\u2223 =\n\u2211 `\u2208[n] \u2223\u2223\u2223\u2223E [(q(x`)\u2212 q(x`\u2212`) +\u2206)\u2212 (q(x`\u22121)\u2212 q(x`\u22121\u2212` ) +\u2206) | q =M(x)]\u2223\u2223\u2223\u2223 =\n\u2211 `\u2208[n] \u2223\u2223\u2223\u2223E [B`(x,x`)\u2212B`(x,x`\u22121)]\u2223\u2223\u2223\u2223 . Thus, it suffices to show that\n\u2223\u2223\u2223\u2223E [B`(x,x`)\u2212B`(x,x`\u22121)]\u2223\u2223\u2223\u2223 \u2264 2\u2206(e\u03b5 \u2212 1 + \u03b4) for all ` \u2208 [n]. Since q is \u2206-sensitive, for every `,y,z, we have 0 \u2264 B`(y,z) \u2264 2\u2206. Moreover, by construction, B`(y,z) is a ( ,\u03b4)-differentially private function of y. We will use these two facts to prove the desired bound.\nWe need one more observation to complete the proof. Note that each x` has the same marginal distribution, namely n independent samples from P . Thus, x` has the same marginal distribution as x (although they are not independent). Moreover, for every ` \u2208 [n] \u222a {0}, the pair (x`,x) has the same distribution as (x,x`): Each corresponding pair of rows (x`i ,xi) is independent from the other rows. For i \u2264 `, x`i and xi are independent samples from P . For i > `, x`i = xi and they are a sample from P .\nConsider the random variables B`(x,x`) and B`(x,x`\u22121) for some ` \u2208 [n]. Now we use privacy and the earlier observation:\nB`(x,x `) \u223c B`(x`,x) \u223c(\u03b5,\u03b4) B`(x`\u22121,x) \u223c B`(x,x`\u22121),\nwhere \u223c denotes having the same distribution and \u223c(\u03b5,\u03b4) denotes having (\u03b5,\u03b4)-indistinguishable distributions.1 Thus B`(x,x`\u22121) and B`(x,x`) are (\u03b5,\u03b4)-indistinguishable.\nWe have\nE [ B`(x,x `\u22121) ] = \u222b 2\u2206\n0 P\n[ B`(x,x `\u22121) \u2265 t ] dt\n\u2264 \u222b 2\u2206\n0\n( e\u03b5P [ B`(x,x `) \u2265 t ] + \u03b4 ) dt\n=e\u03b5 \u222b 2\u2206\n0 P\n[ B`(x,x `) \u2265 t ] dt + 2\u2206\u03b4\n=e\u03b5E [ B`(x,x `) ] + 2\u2206\u03b4\n\u2264E [ B`(x,x `) ] + 2\u2206(e\u03b5 \u2212 1 + \u03b4),\nas E [ B`(x,x`) ] \u2264 2\u2206. This gives one half of the result, the other side is symmetric.\n1In the spirit of differential privacy, distributions A and B over R are (\u03b5,\u03b4)-indistinguishable if for every (measurable) R \u2286R, P [A \u2208 R] \u2264 e\u03b5 \u00b7P [B \u2208 R] + \u03b4 and vice versa.\nNow that we have Lemma 3.1, we can prove the following result that differentially private oracles that are accurate with respect to their sample are also accurate with respect to the population from which that sample was drawn.\nTheorem 3.2. Assume that O is\n1. (\u03b5,\u03b4)-differentially private for \u03b5 = \u03b4 = \u03b1/10\u2206n \u2264 1/4, and\n2. \u03b1/2-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q \u2286Q\u2206 (the family of \u2206-sensitive queries on X ).\nThen O is \u03b1-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X .\nProof. Suppose, for the sake of contradiction, that O is not \u03b1-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X . That is, there exists an adversary A such that\nE Accn,k,Q[O,A] [ max j\u2208[k] \u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223] > \u03b1. Fix such an adversaryA and the distribution P that it outputs. Now define a meta algorithm M, which we call the monitor. Note thatM has access to the \u201ctrue\u201d answers with respect to P .\nAlgorithmM =M(O,A,P ) : X n\u2192Q\u2206. Draw a sample x \u2208 X n from P and let O = O(x, \u00b7) have this sample. Simulate A and O interacting. Let q1, \u00b7 \u00b7 \u00b7 ,qk \u2208Q be the queries given by A. Let a1, \u00b7 \u00b7 \u00b7 , ak \u2208R be the corresponding answers given by O. Pick the j \u2208 [k] that maximizes\n\u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223. If aj \u2212 qj(P ) \u2265 0, output qj and halt. If aj \u2212 qj(P ) < 0, output \u2212qj and halt.\n(Note that Q\u2206 is closed under negation.)\nClaim 3.3. M is (\u03b5,\u03b4)-differentially private.\nProof. This follows because O is (\u03b5,\u03b4)-differentially private andM is a post-processing of the output of O (Lemma 2.7).\nClaim 3.4. E x,M [q(x)\u2212 q(P ) | q =M(x)] > \u03b1/2.\nProof. Let q be the query that M(x) returns and a the corresponding answer from O. By the assumption that O is not \u03b1-accurate on average with respect to the population,\nE x,M [a\u2212 q(P )] > \u03b1.\nThe assumption that O is \u03b1/2-accurate on average with respect to the sample implies\nE M [|a\u2212 q(x)|] \u2264 \u03b1/2.\nThus E x,M [q(x)\u2212 q(P ) | q =M(x)] = E x,M [a\u2212 q(P )]\u2212 E x,M [a\u2212 q(x)] > \u03b1 \u2212\u03b1/2 = \u03b1/2,\nas required.\nNow combine Claim 3.4 and Lemma 3.1 to get\n\u03b1 2 < E x,M [q(x)\u2212 q(P ) | q =M(x)] \u2264 2\u2206n(e\u03b5 \u2212 1 + \u03b4).\nNow we use the assumption that \u03b5,\u03b4 = \u03b1/10\u2206n \u2264 1/4. We have e\u03b5 \u2212 1 \u2264 54\u03b5 \u2264 \u03b1/8\u2206n. Thus 2\u2206n(e\u03b5 \u2212 1 + \u03b4) \u2264 2\u2206n(\u03b1/8\u2206n+\u03b1/10\u2206n) < \u03b1/2, a contradiction. This completes the proof."}, {"heading": "3.2 Amplifying the Success Probability", "text": "Theorem 3.2 only gives an average error guarantee, where in applications we would like a high confidence bound on the error.\nBy Markov\u2019s Inequality (Fact 2.5) the maximum error is bounded with at least constant probability. And we will amplify this probability by \u201cserial repetition.\u201d We will run the oracle O along with a \u201ccensor\u201d C that ensures that the oracle is giving accurate answers. If C detects that O has given an inaccurate answer, then a fresh sample is drawn and a new instance of O is started. Since O has a constant probability of giving good answers for all k queries, the number of times a fresh sample is needed is bounded by O(log(1/\u03b2)) with probability at least 1\u2212 \u03b2, as required. The challenge is constructing a suitable censor C, that can detect when the oracle is inaccurate using only a small number of samples. We define such a censor as follows.\nDefinition 3.5 (Bounded Censor). An oracle C is a \u03b2-sound c-bounded censor for k sensitivity-\u2206 queries with threshold \u03b1 given n samples in X if, for every adversaryA, the algorithm answers every query qj with aj \u2208 {>,\u22a5,?} and\nP Accn,k,Q\u2206 [C,A]\n\u2200j \u2208 [k]  aj => =\u21d2 \u2223\u2223\u2223qj(P )\u2223\u2223\u2223 > \u03b12 aj =\u22a5 =\u21d2 \u2223\u2223\u2223qj(P )\u2223\u2223\u2223 \u2264 \u03b1 aj = ? \u21d0\u21d2 |{i \u2208 [j \u2212 1] : ai =>}| \u2265 c   \u2265 1\u2212 \u03b2.\nIntuitively, a bounded censor is an algorithm which answers k sensitivity-\u2206 queries and approximately determines whether or not they are above a threshold. It outputs > if the answer is above the threshold and \u22a5 if the answer is below the threshold. However, the bounded censor may cease to give useful answers after c query answers have been above the threshold. Notice that a bounded censor is a weaker object than an accurate oracle, so there is no circularity in our approach of using a bounded censor to boost the accuracy of the oracle.\nFirst, we will show that a bounded censor with suitable parameters, plus an oracle that is accurate on average, implies the existence of an oracle that is accurate with high probability. We do so by way of the construction in Figure 4.\nNote that the semantics of F O,C are a bit different from the semantics of an oracle defined in Section 2.2. For notational convenience, we have written F as if it has access to a sampling\noracle for P , rather than a finite sample. Moreover, a new sample of size n is drawn every time a\u0302j = >, which can potentially occur k times, meaning that the number of samples drawn can be as large as n\u2032 + n \u00b7 k. However, since our analysis shows that with probability at least 1 \u2212 \u03b2, a\u0302j => at most c\u22121 times. Thus, we could easily rewrite the algorithm to have a finite sample of size n\u2032 +nc\u0307, which is split into one sample of size n\u2032 and c samples of size n, and the algorithm will halt if it exhausts its set of samples, which occurs with probability at most \u03b2.\nTheorem 3.6. Assume that\n1. O is \u03b1/4-accurate on average with respect to the population for k adaptively chosen queries from Q \u2286Q\u2206 given n samples in X , and\n2. C is a \u03b2/2-sound c-bounded censor for k + c sensitivity-\u2206 queries with threshold \u03b1 given n\u2032 samples in X , where c = dlog2(4/\u03b2)e.\nThen F O,C is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given n\u2032 +n \u00b7 c samples in X .\nWe will show (in Theorem 3.7) that we only need n\u2032 =O(n+ log(1/\u03b2) \u00b7 logk). Thus Theorem 3.6 states that we can amplify from \u03b1/4 expected accuracy to (\u03b1,\u03b2)-accuracy with a log(1/\u03b2) factor increase in sample complexity.\nProof of Theorem 3.6. By Definition 3.5 and the fact that E z\u2190RP n\n[ q\u0302j(z) ] = errP ( qj , aj ) , with proba-\nbility at least 1\u2212 \u03b2/2, for all j \u2208 [k], (i) a\u0302j =\u22a5 implies \u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223 \u2264 \u03b1,\n(ii) a\u0302j => implies \u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223 > \u03b12 , and\n(iii) a\u0302j = ? implies \u2223\u2223\u2223\u2223{j \u2208 [k] : a\u0302j =>}\u2223\u2223\u2223\u2223 \u2265 c.\nWe condition on the event that these statements hold. By condition (i), for each j \u2208 [k],\u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223 \u2264 \u03b1, so F O,C only outputs accurate answers, as required.\nBy condition (iii), F O,C only halts early if the number of times > is returned by C exceeds c \u2212 1. Note that halting early ensures that the number of samples needed by F O,C is bounded by n\u2032 +n \u00b7 c, as required.\nIt remains to show that the number of times> is returned by C is at most c\u22121 with probability at least 1\u2212 \u03b2/2. By condition (ii), > is only returned by C only when \u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223 > \u03b12 . However, by Fact 2.5, O is (\u03b1/2,1/2)-accurate with respect to the population for k adaptively chosen queries from Q given n samples in X . This means that the probability that\n\u2223\u2223\u2223\u2223errP (qj , aj)\u2223\u2223\u2223\u2223 > \u03b12 for any j \u2208 [k] is at most 1/2, where the probability is over only the coins of O and the choice of x. Every time > is returned by C, the coins of O and its sample x are redrawn independently. Thus, the probability that > is returned m or more times is at most 2\u2212m. In particular, the probability that > is returned c \u2212 1 or more times is at most \u03b2/2, as required."}, {"heading": "3.2.1 The Bounded Censor Algorithm", "text": "Now we turn our attention to constructing an appropriate bounded censor C, which is given in Figure 5. This construction is inspired by the sparse vector algorithm [DNR+09, HR10, DNPR10, RR10] and the analysis is inspired by [Rot14].\nTheorem 3.7. The Bounded Censor algorithm C(\u03b1,n,c,k) in Figure 5 is a \u03b2-sound c-bounded censor for k sensitivity-\u2206 queries with threshold \u03b1 given n samples in X assuming\nn \u2265 8(n\u2206) 2\n\u03b12 (log(2/\u03b2) + (c+ 1) \u00b7 log(k + 1)) .\nIn the common scenario where we have \u2206 =O(1/n), the bounded censor C only requires n \u2265O (\nlog(1/\u03b2) + c \u00b7 logk \u03b12\n) .\nWhen C is invoked in Theorem 3.6, we have c =O(log(1/\u03b2)). So the bounded censor only needs n \u2265O(log(1/\u03b2) \u00b7 log(k)/\u03b12), which is small enough not to be a bottleneck in any of our bounds.\nProof of Theorem 3.7. Fix an adversary A. Without loss of generality, we may assume that A is deterministic. By Definition 3.5, we must verify that\nP Accn,k,Q\u2206 [C,A]\n\u2200j \u2208 [k]  aj => =\u21d2 \u2223\u2223\u2223qj(P )\u2223\u2223\u2223 > \u03b12 aj =\u22a5 =\u21d2 \u2223\u2223\u2223qj(P )\u2223\u2223\u2223 \u2264 \u03b1 aj = ? =\u21d2 |{i \u2208 [j \u2212 1] : ai =>}| \u2265 c   \u2265 1\u2212 \u03b2.\nIt suffices to show that, with probability at least 1\u2212\u03b2, for every query q thatA could potentially ask2, |q(x)\u2212q(P )| \u2264 \u03b1/4. We will prove in two steps: (i) We show that for any fixed single query q we have |q(x) \u2212 q(P )| \u2264 \u03b1/4 with high probability over the sample x. And (ii) we bound the number of queries that A could potentially ask and apply a union bound. Note that we must take a union bound over all the queries that A could potentially ask, not just the queries that A does ask in a particular instance of Accn,k,Q\u2206[C,A].\nFor (i), we use the following standard concentration inequality.\nLemma 3.8 (McDiarmid\u2019s Inequality [McD89]). Let X1, \u00b7 \u00b7 \u00b7 ,Xn \u2208 X be independent random variables and f : X n\u2192R. Suppose f is \u2206-sensitive\u2014that is,\n\u2200x \u2208 X n \u2200i \u2208 [n] \u2200y \u2208 X \u2223\u2223\u2223f (x1, \u00b7 \u00b7 \u00b7 ,xi\u22121,xi ,xi+1, \u00b7 \u00b7 \u00b7 ,xn)\u2212 f (x1, \u00b7 \u00b7 \u00b7 ,xi\u22121, y,xi+1, \u00b7 \u00b7 \u00b7 ,xn)\u2223\u2223\u2223 \u2264 \u2206.\nThen\n\u2200\u03bb > 0 P [ f (X1, \u00b7 \u00b7 \u00b7 ,Xn)\u2212E [f (X1, \u00b7 \u00b7 \u00b7 ,Xn)] > \u03bb ] \u2264 exp ( \u22122\u03bb2\nn\u22062\n) .\nThus, for any q \u2208Q\u2206, we have\nP x\u2190RP n\n[|q(x)\u2212 q(P )| > \u03b1/4] \u2264 2 \u00b7 exp ( \u2212\u03b12\n8\u22062n\n) . (1)\nFor (ii): Since A is deterministic, the queries it asks during Accn,k,Q\u2206[C,A] are determined by the answers given by C. In particular, each query A potentially asks can be specified by a string s \u2208 \u22c3k `=1{>,\u22a5}\n` of length at most k with at most c occurances of >. This string represents the answers given by C up until the time the query is asked. (Note that we omit the final ? symbol, as is it immaterial.) By counting the number of such strings, we see that the number of possible queries A can ask is bounded by (k + 1)c+1. Thus we take a union bound over the (k + 1)c+1 potential queries of A.\nBy applying this union bound to the bound in (1), the overall failure probability of C is bounded by\n2 \u00b7 exp ( \u2212\u03b12\n8\u22062n\n) \u00b7 (k + 1)c+1 \u2264 \u03b2,\nas required.\nCombining our theorem about accuracy on average (Theorem 3.2) with our amplification technique from the previous section (Theorems 3.6 and 3.7) gives the following bounds.\nTheorem 3.9. Assume that O is\n1. (\u03b5,\u03b4)-differentially private for \u03b5 = \u03b4 = \u03b1/40\u2206n, and\n2. \u03b1/8-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q \u2286Q\u2206 (the family of \u2206-sensitive queries on X ).\nThen there exists an oracle O\u2032 that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries fromQ givenO((n+(n\u2206/\u03b1)2 \u00b7logk)\u00b7log(1/\u03b2)) samples from X . Moreover, the running time of O\u2032 is at most a O(log(1/\u03b2)) factor more than that of O.\n2Excluding the queries that are only asked after the t \u2265 c condition is reached."}, {"heading": "4 From Function Approximation to Optimization Queries", "text": "We now extend our results for low-sensitivity queries to more general minimization queries."}, {"heading": "4.1 Oracles for Adaptive Minimization Queries", "text": "Analogous to Theorem 3.2, we can show that an oracle that answers minimization queries on its input and is also differentially private gives answers that generalize to the whole distribution.\nTheorem 4.1. Assume that the oracle O is\n\u2022 (\u03b5,\u03b4)-differentially private for \u03b5 = \u03b4 = \u03b120\u2206n \u2264 1 2 and,\n\u2022 \u03b1/2-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q \u2286Qmin (the family of \u2206-sensitive convex minimization queries on X ).3\nThen O is \u03b1-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X .\nProof. Suppose, for the sake of contradiction, that O is not \u03b1-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X . That is, there exists an adversary A such that\nE Accn,k,Q[O,A] [ max j\u2208[k] \u2223\u2223\u2223\u2223errP (Lj ,\u03b8j)\u2223\u2223\u2223\u2223] > \u03b1. Fix such an adversary A and the distribution P that it outputs. Now define a monitor M as follows.\nAlgorithmM =M(O,A,P ) : X n\u2192L\u00d7\u0398. Draw a sample x \u2208 X n from P and let O = O(x, \u00b7) have this sample. Simulate A and O interacting. Let L1, \u00b7 \u00b7 \u00b7 ,Lk be the loss functions given by A. Let \u03b81, \u00b7 \u00b7 \u00b7 ,\u03b8k be the corresponding answers given by O. Choose j \u2208 [k] that maximizes errP ( Lj ,\u03b8j ) .\nOutput (Lj ,\u03b8j ).\nAs for Theorem 3.2, we prove the theorem via a series of claims. We omit proofs where they are sufficiently similar to those in the proof of Theorem 3.2.\nClaim 4.2. M is (\u03b5,\u03b4)-differentially private.\nClaim 4.3. E x,M\n[ errP (L,\u03b8) | (L,\u03b8) =M(x) ] > \u03b1.\nProof. This follows from our supposition that O is not accurate with respect to the population and the fact that errP (L,\u03b8) \u2265 0 for all L and \u03b8.\n3Recall that for many natural minimization queries, the sensitivity \u2206 scales as 1/n, so that \u2206n is a constant.\nClaim 4.4. For all x \u2208 X n, E M [errx (L,\u03b8) | (L,\u03b8) =M(x)] \u2264 \u03b1 2 .\nProof. This follows from our assumption that O is accurate with respect to the sample.\nClaim 4.5. Define q : X n \u2192 R by q(x) = errx (L,\u03b8) for fixed values of L : X n \u00d7\u0398 \u2192 R and \u03b8 \u2208 \u0398. Then we have q is 2\u2206-sensitive.\nProof. Let x,x\u2032 \u2208 X n be neighboring and \u03b8\u0302 = argmin\u03b8\u2217\u2208\u0398 L(x,\u03b8\u2217). Since L is \u2206-sensitive,\nq(x) = L(x,\u03b8)\u2212min \u03b8\u2217\u2208\u0398\nL(x,\u03b8\u2217) = L(x,\u03b8)\u2212L(x, \u03b8\u0302)\n\u2264 L(x\u2032 ,\u03b8)\u2212L(x\u2032 , \u03b8\u0302) + 2\u2206 \u2264 L(x\u2032 ,\u03b8)\u2212min\n\u03b8\u2217\u2208\u0398 L(x\u2032 ,\u03b8\u2217) + 2\u2206\n= q(x\u2032) + 2\u2206.\nClaim 4.6.\nE M,x\n[ errP (L,\u03b8) | (L,\u03b8) =M(x) ] \u2264 E M,x [errx (L,\u03b8) | (L,\u03b8) =M(x)] + 4\u2206n(e\u03b5 \u2212 1 + \u03b4).\nProof.\nE M,x\n[ errP (L,\u03b8) | (L,\u03b8) =M(x) ] = E M,x [ E x\u2032\u2190RP n [errx\u2032 (L,\u03b8)] | (L,\u03b8) =M(x) ] \u2264 E M,x [errx (L,\u03b8) | (L,\u03b8) =M(x)]\n+ \u2223\u2223\u2223\u2223\u2223 EM,x,x\u2032 [errx\u2032 (L,\u03b8)\u2212 errx (L,\u03b8) | (L,\u03b8) =M(x)] \u2223\u2223\u2223\u2223\u2223 \u2264 E M,x [errx (L,\u03b8) | (L,\u03b8) =M(x)]\n+ 4\u2206n(e\u03b5 \u2212 1 + \u03b4),\nwhere the final inequality follows from Lemma 3.1 and Claims 4.2 and 4.5.\nCombining Claims 4.3, 4.4, and 4.6 gives\n\u03b1 < E M,x\n[ errP (L,\u03b8) | (L,\u03b8) =M(x) ] \u2264 E M,x [errx (L,\u03b8) | (L,\u03b8) =M(x)] + 4\u2206n(e\u03b5 \u2212 1 + \u03b4) \u2264 \u03b1/2 + 4\u2206n(e\u03b5 \u2212 1 + \u03b4).\nBy assumption e\u03b5 \u2212 1 \u2264 54\u03b5 \u2264 \u03b1/16\u2206n and, hence, 4\u2206n(e \u03b5 \u2212 1 + \u03b4) \u2264 \u03b1/4 + \u03b1/5 < \u03b1/2, which\ncompletes the proof by contradiction."}, {"heading": "4.2 Amplifying the Success Probability", "text": "Similar to Theorem 3.6, we can amplify the success probability of an algorithm for minimization queries.\nTheorem 4.7. Assume that\n1. O is \u03b1/4-accurate on average with respect to the population for k adaptively chosen queries from Q \u2286Qmin given n samples in X , and\n2. C is a \u03b2/2-sound c-bounded censor for k + c sensitivity-\u2206 queries with threshold \u03b1 given n\u2032 samples in X , where c = dlog2(4/\u03b2)e.\nThen F O,C is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given n\u2032 +n \u00b7 c samples in X .\nThe proof is almost identical to that of Theorem 3.6. The only difference is that, for a \u2206-sensitive loss function Lj , the query to the censor q\u0302j(z) = errz ( Lj ,\u03b8j ) is now a 2\u2206-sensitive real-valued query (Claim 4.5). Combining Theorems 4.1, 4.7, and 3.7 gives the following.\nTheorem 4.8. Assume that O is\n1. (\u03b5,\u03b4)-differentially private for \u03b5 = \u03b4 = \u03b1/80\u2206n, and\n2. \u03b1/8-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q \u2286Qmin.\nThen there exists an oracle O\u2032 that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries fromQ givenO((n+(n\u2206/\u03b1)2 \u00b7logk)\u00b7log(1/\u03b2)) samples from X . Moreover, the running time of O\u2032 is at most a O(log(1/\u03b2)) factor more than that of O."}, {"heading": "5 Applications", "text": ""}, {"heading": "5.1 Low-Sensitivity and Statistical Queries", "text": "We now plug known differentially private mechanisms in to Theorem 3.9 to obtain oracles that provide strong error guarantees with high probability for both low-sensitivity and statistical queries.\nCorollary 5.1 (Theorem 3.9 and [DMNS06, SU15]). There is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q\u2206 where \u2206 = O(1/n) given n samples from X for\nn \u2265O \u221ak \u00b7 loglogk \u00b7 log(1/\u03b1) \u00b7 log(1/\u03b2)\u03b12  The oracle runs in time poly(n, log |X |, log(1/\u03b2)) per query.\nCorollary 5.2 (Theorem 3.9 and [RR10]). There is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q\u2206 where \u2206 =O(1/n) given n samples from X for\nn =O  log |X | \u00b7 logk \u00b7\u221alog(1/\u03b1) \u00b7 log(1/\u03b2)\u03b13 \nThe oracle runs in time poly(|X |n) per query. The case where \u2206 is not O(1/n) can be handled by rescaling the output of the query.\nCorollary 5.3 (Theorem 3.9 and [HR10]). There is an oracle O that is \u03b1-accurate on average with respect to the population for k adaptively chosen queries from QSQ given n samples from X for\nn =O \u221alog |X | \u00b7 logk \u00b7\u221alog(1/\u03b1) \u00b7 log(1/\u03b2)\u03b13 \nThe oracle runs in time poly(n, |X |) per query."}, {"heading": "5.2 Optimization Queries", "text": "The results of the Section 4 can be combined with existing differentially private algorithms for minimizing \u201cempirical risk\u201d (that is, loss with respect to the sample x) to obtain algorithms for answering adaptive sequences of minimization queries. We provide a few specific instantiations here, based on known differentially private mechanisms."}, {"heading": "5.2.1 Minimization Over Arbitrary Finite Sets", "text": "Corollary 5.4 (Theorem 4.8 and [MT07]). Let \u0398 be a finite set of size at most D. Let Q \u2282 Qmin be the set of sensitivity-1/n loss functions bounded between 0 and C. Then there is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Qmin given\nn \u2265O  log(DC/\u03b1) \u00b7\u221ak \u00b7 log(1/\u03b1) \u00b7 log(1/\u03b2)\u03b12  samples from X . The running time of the oracle is dominated by O((k + log(1/\u03b2)) \u00b7D) evaluations of the loss function."}, {"heading": "5.2.2 Convex Minimization", "text": "We state bounds for convex minimization queries for some of the most common parameter regimes in applications. In the first two corollaries, we consider 1-Lipschitz4 loss functions over a bounded domain.\nCorollary 5.5 (Theorem 4.8 and [BST14]). Let \u0398 be a closed, convex subset of Rd set such that max\u03b8\u2208\u0398 \u2016\u03b8\u20162 \u2264 1. LetQ \u2282Qmin be the set of convex 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = O\u0303 \u221adk \u00b7 log(1/\u03b2)\u03b12 \nsamples fromQ. The running time of the oracle is dominated by k \u00b7n2 evaluations of the gradient \u2207L. 4A loss function L : X \u00d7Rd \u2192R is 1-Lipschitz if for every \u03b8,\u03b8\u2032 \u2208Rd , x \u2208 X , |L(\u03b8,x)\u2212L(\u03b8\u2032 ,x)| \u2264 \u2016\u03b8 \u2212\u03b8\u2032\u20162.\nCorollary 5.6 (Theorem 4.8 and [Ull15]). Let \u0398 be a closed, convex subset of Rd set such that max\u03b8\u2208\u0398 \u2016\u03b8\u20162 \u2264 1. LetQ \u2282Qmin be the set of convex 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = O\u0303 \u221alog |X | \u00b7 (\u221ad + logk) \u00b7 log(1/\u03b2)\u03b13 \nsamples from X . The running time of the oracle is dominated by poly(n, |X |) and k \u00b7 n2 evaluations of the gradient \u2207L.\nIn the next two corollaries, we consider 1-strongly convex5, Lipschitz loss functions over a bounded domain.\nCorollary 5.7 (Theorem 4.8 and [BST14]). Let \u0398 be a closed, convex subset of Rd set such that max\u03b8\u2208\u0398 \u2016\u03b8\u20162 \u2264 1. Let Q \u2282 Qmin be the set of 1-strongly convex, 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = O\u0303 (\u221a dk\n\u03b13/2 \u00b7 log(1/\u03b2) ) samples from X . The running time of the oracle is dominated by k \u00b7n2 evaluations of the gradient \u2207L.\nCorollary 5.8 (Theorem 4.8 and [Ull15]). Let \u0398 be a closed, convex subset of Rd set such that max\u03b8\u2208\u0398 \u2016\u03b8\u20162 \u2264 1. Let Q \u2282 Qmin be the set of 1-strongly convex 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (\u03b1,\u03b2)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = O\u0303 (\u221a log |X | \u00b7 ( \u221a d\n\u03b15/2 + logk \u03b13\n) \u00b7 log(1/\u03b2) ) samples from X . The running time of the oracle is dominated by poly(n, |X |) and k \u00b7 n2 evaluations of the gradient \u2207L."}, {"heading": "Acknowledgements", "text": "We thank Aaron Roth for suggesting the technique that we used to prove Theorem 3.7. We would also like to thank Mark Bun, Moritz Hardt, and Salil Vadhan for helpful discussions."}], "references": [{"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing", "author": ["Yoav Benjamini", "Yosef Hochberg"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Benjamini and Hochberg.,? \\Q1995\\E", "shortCiteRegEx": "Benjamini and Hochberg.", "year": 1995}, {"title": "Bonferroni. Teoria statistica delle classi e calcolo delle probabilita", "author": ["Carlo Emilio"], "venue": "Pubbl. d. R. Ist. Super. di Sci. Econom. e Commerciali di Firenze.,", "citeRegEx": "Emilio,? \\Q1936\\E", "shortCiteRegEx": "Emilio", "year": 1936}, {"title": "Private empirical risk minimization: Efficient algorithms and tight error bounds", "author": ["Raef Bassily", "Adam Smith", "Abhradeep Thakurta"], "venue": "In FOCS,", "citeRegEx": "Bassily et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2014}, {"title": "Fingerprinting codes and the price of approximate differential privacy", "author": ["Mark Bun", "Jonathan Ullman", "Salil P. Vadhan"], "venue": "In STOC,", "citeRegEx": "Bun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bun et al\\.", "year": 2014}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Preserving statistical validity in adaptive data analysis", "author": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": "In STOC. ACM,", "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "Differential privacy and robust statistics. In STOC, pages 371\u2013380", "author": ["Cynthia Dwork", "Jing Lei"], "venue": "ACM, May 31\u2013June", "citeRegEx": "Dwork and Lei.,? \\Q2009\\E", "shortCiteRegEx": "Dwork and Lei.", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Differential privacy under continual observation", "author": ["Cynthia Dwork", "Moni Naor", "Toniann Pitassi", "Guy N. Rothblum"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Dwork et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2010}, {"title": "On the complexity of differentially private data release: efficient algorithms and hardness results", "author": ["Cynthia Dwork", "Moni Naor", "Omer Reingold", "Guy N. Rothblum", "Salil P. Vadhan"], "venue": "In STOC,", "citeRegEx": "Dwork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2009}, {"title": "Multiple comparisons among means", "author": ["Olive Jean Dunn"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Dunn.,? \\Q1961\\E", "shortCiteRegEx": "Dunn.", "year": 1961}, {"title": "The garden of forking paths: Why multiple comparisons can be a problem, even when there is no \u201cfishing expedition", "author": ["Andrew Gelman", "Eric Loken"], "venue": "or \u201cphacking\u201d and the research hypothesis was posited ahead of time. Manuscript.,", "citeRegEx": "Gelman and Loken.,? \\Q2013\\E", "shortCiteRegEx": "Gelman and Loken.", "year": 2013}, {"title": "A multiplicative weights mechanism for privacypreserving data analysis", "author": ["Moritz Hardt", "Guy Rothblum"], "venue": "In Proc. 51st Foundations of Computer Science (FOCS),", "citeRegEx": "Hardt and Rothblum.,? \\Q2010\\E", "shortCiteRegEx": "Hardt and Rothblum.", "year": 2010}, {"title": "Preventing false discovery in interactive data analysis is hard", "author": ["Moritz Hardt", "Jonathan Ullman"], "venue": "In FOCS. IEEE, October", "citeRegEx": "Hardt and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Ullman.", "year": 2014}, {"title": "Why most published research findings are false", "author": ["John P.A. Ioannidis"], "venue": "PLoS Medicine,", "citeRegEx": "Ioannidis.,? \\Q2005\\E", "shortCiteRegEx": "Ioannidis.", "year": 2005}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael J. Kearns"], "venue": "In STOC, pages 392\u2013401", "citeRegEx": "Kearns.,? \\Q1993\\E", "shortCiteRegEx": "Kearns.", "year": 1993}, {"title": "On the method of bounded differences", "author": ["Colin McDiarmid"], "venue": "Surveys in combinatorics,", "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In FOCS,", "citeRegEx": "McSherry and Talwar.,? \\Q2007\\E", "shortCiteRegEx": "McSherry and Talwar.", "year": 2007}, {"title": "Interactive privacy via the median mechanism", "author": ["Aaron Roth", "Tim Roughgarden"], "venue": "In STOC, pages 765\u2013774", "citeRegEx": "Roth and Roughgarden.,? \\Q2010\\E", "shortCiteRegEx": "Roth and Roughgarden.", "year": 2010}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "author": ["Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1410.1228,", "citeRegEx": "Steinke and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2014}, {"title": "Between pure and approximate differential privacy", "author": ["Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1501.06095,", "citeRegEx": "Steinke and Ullman.,? \\Q2015\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2015}, {"title": "Answering n2+o(1) counting queries with differential privacy is hard", "author": ["Jonathan Ullman"], "venue": "In STOC, pages 361\u2013370", "citeRegEx": "Ullman.,? \\Q2013\\E", "shortCiteRegEx": "Ullman.", "year": 2013}, {"title": "Private multiplicative weights beyond linear queries", "author": ["Jonathan Ullman"], "venue": "In PODS. ACM, May 31\u2013June", "citeRegEx": "Ullman.,? \\Q2015\\E", "shortCiteRegEx": "Ullman.", "year": 2015}], "referenceMentions": [], "year": 2015, "abstractText": "Adaptivity is an important feature of data analysis\u2014typically the choice of questions asked about a dataset depends on previous interactions with the same dataset. However, generalization error is typically bounded in a non-adaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. (STOC \u201915) and Hardt and Ullman (FOCS \u201914) initiated the formal study of this problem, and gave the first upper and lower bounds on the achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution P and a set of n independent samples x is drawn from P . We seek an algorithm that, given x as input, \u201caccurately\u201d answers a sequence of adaptively chosen \u201cqueries\u201d about the unknown distribution P . How many samples n must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions towards resolving this question: 1. We give upper bounds on the number of samples n that are needed to answer statistical queries that improve over the bounds of Dwork et al. 2. We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and the important class of convex risk minimization queries. As in Dwork et al., our algorithms are based on a connection between differential privacy and generalization error, but we feel that our analysis is simpler and more modular, which may be useful for studying these questions in the future. \u2217Pennsylvania State University, Department of Computer Science and Engineering. {bassily,asmith}@psu.edu \u2020Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. tsteinke@seas.harvard.edu \u2021Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. jullman@cs.columbia.edu ar X iv :1 50 3. 04 84 3v 1 [ cs .L G ] 1 6 M ar 2 01 5", "creator": "LaTeX with hyperref package"}}}