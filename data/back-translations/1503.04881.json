{"id": "1503.04881", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2015", "title": "Long Short-Term Memory Over Tree Structures", "abstract": "Chain Structured Short-Term Memory (LSTM) has proven effective in a variety of problems such as speech recognition and machine translation. In this paper, we propose extending it to tree structures in which a memory cell can reflect the history memories of multiple child cells or descending cells in a recursive process. We call the S-LSTM model, which provides a principal method of observing long-distance interactions with hierarchies, such as speech or image-parse structures. We use the semantic composition models to understand the meaning of text, a fundamental problem in understanding natural languages, and show that it surpasses a modern recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that the use of the given structures is helpful in achieving better performance than without considering the structures.", "histories": [["v1", "Mon, 16 Mar 2015 23:59:02 GMT  (242kb)", "http://arxiv.org/abs/1503.04881v1", "On February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML)"]], "COMMENTS": "On February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["xiaodan zhu", "parinaz sobhani", "hongyu guo"], "accepted": true, "id": "1503.04881"}, "pdf": {"name": "1503.04881.pdf", "metadata": {"source": "META", "title": "Long Short-Term Memory Over Tree Structures", "authors": ["Xiaodan Zhu", "Parinaz Sobhani"], "emails": ["XIAODAN.ZHU@NRC-CNRC.GC.CA", "PSOBH090@UOTTAWA.CA", "HONGYU.GUO@NRC-CNRC.GC.CA"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n04 88\n1v 1\n[ cs\n.C L\n] 1"}, {"heading": "1. Introduction", "text": "Recent years have seen a revival of the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), with its effectiveness being demonstrated on a wide range of problems such as speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014; Cho et al., 2014), and image-to-text conversion (Vinyals et al., 2014),\nOn February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML).\namong many others, in which history is summarized and coded in the memory cell in a full-order time sequence.\nRecursion is a fundamental process associated with many problems\u2014a recursive process and hierarchical structure so formed are common in different modalities. For example, semantics of sentences in human languages is believed to be carried by not merely a linear concatenation of words; instead, sentences have parse structures (Manning & Schu\u0308tze, 1999). Image understanding, as another example, benefits from recursive modeling over structures, which yielded the state-of-the-art performance on tasks like scene segmentation (Socher et al., 2011).\nIn this paper, we extend LSTM to tree structures, in which we learn memory cells that can reflect the history memories of multiple child cells and hence multiple descendant cells. We call the model S-LSTM. Compared with previous recursive neural networks (Socher et al., 2013; 2012), S-LSTM has the potentials of avoiding gradient vanishing and hence may model long-distance interaction over trees. This is a desirable characteristic as many of such structures are deep. S-LSTM can be considered as bringing the merits of a recursive neural network and a recurrent neural network together1. In short, S-LSTM wires memory blocks in a partial-order structures instead of in a full-order sequence as in a chain-structured LSTM.\nWe leverage the S-LSTM model to solve a semantic composition problem that learns the meaning for a piece of texts\u2014learning good representations for meaning of text is core to automatically understanding human languages. More specifically, we experiment with the models on the\n1As both of them can be shortened to be RNN, in the rest of this paper we refer to a Recurrent Neural Network as RNN and a Recursive Neural Network as RvNN.\nStanford Sentiment Tree Bank (Socher et al., 2013) to determine the sentiment for different granularities of phrases in a tree. The dataset has favorable properties: in addition to being a benchmark for much previous work, it provides with human annotations at all nodes of the trees, enabling us to comprehensively explore the properties of S-LSTM. We experimentally show that S-LSTM outperforms a stateof-the-art recursive model by simply replacing the original tensor-enhanced composition with the S-LSTM memory block we propose here. We showed that utilizing the given structures is helpful in achieving a better performance than that without considering the structures."}, {"heading": "2. Related Work", "text": "Recursive neural networks Recursion is a fundamental process in different modalities. In recent years, recursive neural networks (RvNN) have been introduced and demonstrated to achieve state-of-the-art performances on different problems such as semantic analysis in natural language processing and image segmentation (Socher et al., 2013; 2011). These networks are defined over recursive tree structures\u2014a tree node is a vector computed from its children. In a recursive fashion, the information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up manner through the tree. Derivatives of errors are computed with backpropagation over structures (Goller & Kchler, 1996).\nIn addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004), amongst others. For instance, Legrand and Collobert leverage neural networks over greedy syntactic parsing (Pinheiro & Collobert, 2014). In (Irsoy & Cardie, 2014), a deep recursive neural network is proposed . Nevertheless, over the often deep structures, the networks are potentially subject to the vanishing gradient problem, resulting in difficulties in leveraging long-distance dependencies in the structures. In this paper, we propose the S-LSTM model that wires memory blocks in recursive structures. We compare our model with the RvNN models presented in (Socher et al., 2013), as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block. We show the advantages of our proposed model in achieving significantly better results.\nRecurrent neural networks and LSTM Unlike a feedforward network, a recurrent neural network (RNN) shares their hidden states across time. The sequential history is summarized in a hidden vector. RNN also suffers from the decaying of gradient, or less frequently, blowing-up of gradient problem. LSTM replaces the hidden vector of a recurrent neural network with memory blocks which\nare equipped with gates; it can in principle keep longterm memory by training proper gating weights (refer to (Graves, 2008) for intuitive illustrations and good discussions), and it has practically showed to be very useful, achieving the state of the art on a range of problems including speech recognition (Graves et al., 2013), digit handwriting recognition (Liwicki et al., 2007; Graves, 2012), and achieve interesting results on statistical machine translation (Sutskever et al., 2014; Cho et al., 2014) and music composition (Eck & Schmidhuber, 2002b;a). In (Graves et al., 2013), a deep LSTM network achieved the state-of-the-art results on the TIMIT phoneme recognition benchmark. In (Sutskever et al., 2014; Cho et al., 2014), a pair of LSTM networks are trained to encode and decode human language for automatic machine translation, which is in particular effective for the more challenging long sentence translation. In (Liwicki et al., 2007; Graves, 2012), LSTM networks are found to be very useful for digit writing recognition because of the network\u2019s capability of memorizing context information in a long sequence. In (Eck & Schmidhuber, 2002b;a), LSTM networks are trained to effectively capture global structures of the temporal data. With the memory cells, LSTM is able to keep track of temporally distant events that indicate global music structures. As a result, LSTM can be successfully trained to compose music, where other RNNs have failed to do so.\nAlthough promising results have been observed by applying chain-structured LSTM, many other interesting problems are inherently associated with input structures that are more complicated than a sequence. For example, sentences in human languages are believed to be carried by not merely a linear sequence of words; instead, meaning is thought to interweave with structures. While a sequential application of LSTM may capture structural information implicitly, in practice it sometimes lacks the claimed power. For example, even simply reversing the input sequences may result in significant differences in modeling performances, in tasks such as machine translation and speech recognition. Unlike in previous work, we propose here to directly wire memory blocks in recursive structures. We show the proposed S-LSTM model does utilize the structures and achieve results better than those ignoring such priori structures."}, {"heading": "3. The Model", "text": "Model brief In this paper, we extend LSTM to structures, in which a memory cell can reflect the history memories of multiple child cells and hence multiple descendant cells in a hierarchical structure. As intuitively showed in Figure 1, the root of the tree can in principle consider information from long-distance interactions over the tree\u2014in this fig-\nure, the gray and light-blue leaf. In the figure, the small circle (\u201d\u25e6\u201d) or short line (\u201d\u2212\u201d) at each arrowhead indicates pass and block of information, respectively. Note that the figure shows a binary case, while in real models a soft version of gating is applied, where a gating signal is in the range of [0, 1], often enforced with a logistic sigmoid function. Through learning the gating signals, as detailed later in this section, S-LSTM provides a principled way of considering long-distance interplays over the input structures.\nThe memory block Each node in Figure 1 is composed of a S-LSTM memory block. We present a specific wiring of such a block in Figure 2. Each memory block contains one input gate and one output gate. The number of forget gates depends on the structure, i.e., the number of children of a node. In this paper, we assume there are two children at each nodes, same as in (Socher et al., 2013) and therefore we use their data in our experiments. That is, we have two forget gates. Extension of the model to handle more children is rather straightforward.\nAs shown in the figure, the hidden vectors of the two children, denoted as hLt\u22121 for the left child and h R t\u22121 for the right, are taken in as input of the current block. The input gate it consider four resources of information: the hidden vectors (hLt\u22121 and h R t\u22121) and cell vectors (c L t\u22121 and c R t\u22121) of its two children. These four sources of information are also used to form the gating signals for the left forget gate fLt\u22121 and right forget gate fRt\u22121, where the weights used to combining them are specific to each of these gates, denoted as\ndifferent W in the formulas below. Different from the process in a regular LSTM, the cell here considers the copies from both children\u2019s cell vectors (cLt\u22121, c R t\u22121), gated with separated forget gates. The left and right forget gates can be controlled independently, allowing the pass-through of information from children\u2019s cell vectors. The output gate ot considers the hidden vectors from the children and the current cell vector. In turn, the hidden vector ht and the cell vector ct of the current block are passed to the parent and are used depending on if the current block is a left or right child of its parent. In this way, the memory cell, through merging the gated cell vectors of the children, can reflect multiple direct or indirect descendant cells. As a result, the long-distance interplays over the structures can be captured. More specifically, the forward computation of a S-LSTM memory block is specified in the following equations.\nit = \u03c3(W L hih L t\u22121 +W R hih R t\u22121 +W L cic L t\u22121\n+WRci c R t\u22121 + bi) (1)\nfLt = \u03c3(W L hf l hLt\u22121 +W R hf l hRt\u22121 +W L cf l cLt\u22121\n+WRcf l cRt\u22121 + bfl) (2)\nfRt = \u03c3(W L hf r hLt\u22121 +W R hf r hRt\u22121 +W L cf r cLt\u22121\n+WRcf r cRt\u22121 + bfr ) (3)\nxt = W L hxh L t\u22121 +W R hxh R t\u22121 + bx (4)\nct = f L t \u2297 c L t\u22121 + f R t \u2297 c R t\u22121 + it \u2297 tanh(xt) (5)\not = \u03c3(W L hoh L t\u22121 +W R hoh R t\u22121 +Wcoct + bo) (6)\nht = ot \u2297 tanh(ct) (7)\nwhere \u03c3 is the element-wise logistic function used to confine the gating signals to be in the range of [0, 1]; fL and fR are the left and right forget gate, respectively; b is bias and W is network weight matrices; the sign \u2297 is a Hadamard product, i.e., element-wise product. The subscripts of the weight matrices indicate what they are used for. For example, Who is a matrix mapping a hidden vector to an output gate.\nBackpropagation over structures During training, the gradient of the objective function with respect to each parameter can be calculated efficiently via backpropagation over structures (Goller & Kchler, 1996; Socher et al., 2013). The major difference from that of (Socher et al., 2013) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children. Obtaining the backprop formulas is tedious but we list them below to facilitate duplication of our work 2. We will discuss the specific objective function later in experiments. For each memory block, assume that the error passed to the hidden vector is \u01ebht . The derivatives of the output gate \u03b4ot , left forget gate \u03b4 fl t , right forget gate \u03b4 fr t , and input gate \u03b4it are computed as:\n\u01ebht = \u2202O\n\u2202ht (8)\n\u03b4ot = \u01eb h t \u2297 tanh(ct)\u2297 \u03c3 \u2032(ot) (9)\n\u03b4 fl t = \u01eb c t \u2297 c L t\u22121 \u2297 \u03c3 \u2032(fLt ) (10)\n\u03b4 fr t = \u01eb c t \u2297 c R t\u22121 \u2297 \u03c3 \u2032(fRt ) (11)\n\u03b4it = \u01eb c t \u2297 tanh(xt)\u2297 \u03c3 \u2032(it) (12)\n2The code will be published at www.icml-placeholderonly.com\nwhere \u03c3\u2032(x) is the element-wise derivative of the logistic function over vector x. Since it can be computed with the activation of x, we abuse the notation a bit to write it over the activated vectors in these equations. \u01ebct is the derivative over the cell vector. So if the current node is the left child of its parent, we use Equation (13) to calculate \u01ebct , otherwise Formula (14) is used:\n\u01ebct =\u01eb h t \u2297 ot \u2297 g \u2032(ct) + \u01eb c t+1 \u2297 f L t+1+\n(WLci) T \u03b4it+1 + (W L cfl )T \u03b4flt+1+\n(WLcfr ) T \u03b4 fr t+1 + (Wco) T \u03b4ot (13)\n\u01ebct =\u01eb h t \u2297 ot \u2297 g \u2032(ct) + \u01eb c t+1 \u2297 f R t+1+\n(WRci ) T \u03b4it+1 + (W R cfl )T \u03b4flt+1+\n(WRcfr ) T \u03b4 fr t+1 + (Wco) T \u03b4ot (14)\nwhere g\u2032(x) is the element-wise derivative of the tanh function. It can also be directly calculated from the tanh activation of x. The superscript T over the weight matrices means matrix transpose.\nWith derivatives at each gate computed, the derivatives of the weight matrices used in Formula (1)-(7) can be calculated accordingly, which is omitted here. We checked the correctness of the S-LSTM implementation with the standard approximated gradient approach.\nObjective over trees The objective function defined over structures can be complicated, which could consider the output structures depending on the properties of problem. Following (Socher et al., 2013), the overall objective function we used to learn S-LSTM in this paper is simply minimizing the overall cross-entropy errors and a sum of that at all nodes."}, {"heading": "4. Experiment Set-up", "text": "As discussed earlier, recursion is a basic process inherent to many problems. In this paper, we leverage the proposed model to solve semantic composition for the meanings of pieces of text, a fundamental problem in understanding human languages.\nWe specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (Socher et al., 2013). In obtaining the sentiment of a long piece of text, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-ofphrases models (Pang & Lee, 2008; Liu & Zhang, 2012).\nMore recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; 2013; Kalchbrenner et al., 2014), a more principled approach to modeling the formation of semantics. In this paper, we put the proposed LSTM memory blocks at tree nodes\u2014we replaced the tensorenhanced composition layer at each tree node presented in (Socher et al., 2013) with a S-LSTM memory block. We used the same dataset, the Stanford Sentiment Tree Bank, to evaluate the performances of the models. In addition to being a benchmark for much previous work, the data provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM."}, {"heading": "4.1. Data Set", "text": "The Stanford Sentiment Tree Bank (Socher et al., 2013) contains about 11,800 sentences from the movie reviews that were originally discussed in (Pang & Lee, 2005). The sentences were parsed with the Stanford parser (Klein & Manning, 2003). Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (Socher et al., 2013) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences). For the root sentiment, the training, development, and test sentences are 8544, 1101, and 2210, respectively. The phrase sentiment task includes 318582, 41447, and 82600 phrases for the three sets. Following (Socher et al., 2013), we also use the classification accuracy to measure the performances."}, {"heading": "4.2. Training Details", "text": "As mentioned before, we follow (Socher et al., 2013) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings. For all phrases, the error is calculated as a regularized sum:\nE(\u03b8) = \u2211\ni\n\u2211\nj\ntij logy seni j + \u03bb \u2016\u03b8\u2016 2 2 (15)\nwhere yseni \u2208 Rc\u00d71 is predicted distribution and ti \u2208 R\nc\u00d71 the target distribution. c is the number of classes or categories, and j \u2208 c denotes the j-th element of the multinomial target distribution; i iterates over nodes, \u03b8 are model parameters, and \u03bb is a regularization parameter. We tuned our model against the development data set as split in (Socher et al., 2013)."}, {"heading": "5. Results", "text": "To understand the modeling advantages of S-LSTM over the structures, we conducted four sets of experiments.\nDefault setting In the default setting, we conducted experiments as in (Socher et al., 2013). Table 1 shows the accuracies of different models on the test set of the Stanford Sentiment Tree Bank. We present the results on 5-category sentiment prediction at both the sentence level (i.e., the ROOTS column in the table) and for all phrases including roots (the PHRASES column) 3. In Table 1, NB and SVM are naive Bayes and support vector machine classifiers, respectively; RvNN corresponds to RNN in (Socher et al., 2013). As described earlier, we refer to recursive neural networks to as RvNN to avoid confusion with recurrent neural networks. RNTN is different from RvNN in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions.\nTable 1 showed that S-LSTM achieved the best predictive performance, when compared to all the models reported in (Socher et al., 2013). The S-LSTM results reported here were obtained by setting the size of the hidden units to be 100, batch size to be 10, and learning rate to be 0.1. In our experiments, we only tuned these hyper-parameters, and we feel that more finer tuning, such as discriminating the classification weights between the leaves (word embedding) and other nodes, using different numbers of hidden units for the memory blocks (e.g., for the hidden layers of words), or different initializations of word embedding, may further improve the performances reported here.\nTo evaluate the S-SLTM model\u2019s convergence behavior, Figure 3 depicts the converging time during training. More specifically, we show two sub-figures: one for roots (upper sub-figure) and the other for all phrases (lower sub-figure). From these figures, we can observe that S-LSTM converge faster than the RNTN. For instance, for the phrase-level task, S-LSTM started to converge after about 20 minutes but the RNTN needed over 180 minutes. S-LSTM has\n3The Stanford CoreNLP package (http://nlp.stanford.edu/sentiment/code.html) only gives approximate accuracies for 2-category sentiment, which are not included here in the table.\nmuch less parameters than RNTN and the forward and backward propagation can be computed efficiently.\nMore real-life settings We further compare S-LSTM with RNTN in two more experimental settings. In the first setting we only keep the training signals at the roots to train S-LSTM and RNTN, depicted as model (1) and (2) in Table 2. ROOT LBLS besides the model names stands for root labels; that is, only the gold labels of the sentence level are used to train the model. In most sentiment analysis circumstances, phrase level annotations are not available: most nodes in a tree are fragments that may not be that interesting; e.g., the fragment \u201cof a good movie\u201d 4. Also, annotating all phrases is expensive. However, these should not be regarded as comments on the value of the Sentiment Tree Bank. Detailed annotations in the tree bank enable much interesting work to be possible, e.g., the study of the effect of negation in changing sentiment (Zhu et al., 2014).\nThe second setting, corresponding to model (3) and (4) in Table 2, is only slightly different, in which we keep annotation for the tree leafs as well, to simulate that a sentiment lexicon is available and it covers all leafs (words) (LEAFLBLS along the side of the model names stands for leaf labels), and so there is no out-of-vocabulary concern. Using real sentiment lexicons is expected to have a performance between the two settings here.\n4Phrase-level sentiment analysis is often defined over a very small subset of phrases of interest, such as in the phrase-level task defined in (Wilson et al., 2005; Mohammad et al., 2013).\nResults in the table show that in both settings, S-LSTM outperforms RNTN by a large margin. When only root labels are used to train the models, S-LSTM obtains an accuracy of 43.5, compared with 29.1 of RNTN. When the leaf labels are also used, S-LSTM achieves an accuracy of 44.1 and RNTN 34.9. All these improvements are statistically significant (p < 0.05). For the RNTN, without supervising signals from the internal nodes, the composition parameters may not be learned well, potentially because the tensor has much more parameters to learn. On the other hand, through controlling its gates, the S-LSTM shows a very good ability to learn from the trees.\nPerformance over different levels of trees In Figure 4, we further depict the performances of models on different levels of nodes in the trees. In the Figure, the x-axis corresponds to different depths or lengths and y-axis is accuracy. The depth here is defined as the longest distance between the root of a phrase and their descendant leafs. The Length is simply the number of words of a node, where depth is not necessarily to be length\u2014e.g., a balanced tree with 4 leafs has different depths than the unbalanced tree with the same number of leafs. The trends of the two figure are similar. In both figures, S-LSTM performs better at all depths, showing its advantages on nodes at depth. As the deeper levels of the tree tend to have more complicated syntax and semantics, S-LSTM can model such more complicated syntax and semantics better.\nExplicit structures vs. no structures Some efforts in the literature attempt to learn distributed representation by utilizing input structures when available, and others prefer to assume chain-structured recurrent neural networks can actually capture the structures implicitly though a linear coding process. In this paper, we attempt to give some empirical evidences in our experiment setting by comparing several different models. First, a special case for the S-LSTM model is considered, in which no sentential structures are given. Instead, words are read from left to right and combined in that order. We call it left recursive S-LSTM, or S-\nLSTM-LR in short. Similarly, we also experimented with a right recursive S-LSTM, S-LSTM-RR, in which words are read from right to left instead. Since for these models, phrase-level training signals are not available\u2014the nodes here do not correspond to that in the original Standford Sentiment Tree Bank, but the roots and leafs annotations are still the same, so we run two versions of our experiments: one uses only training signals from roots and the other includes also leaf annotations.\nIt can be observed from Table 3 that the given parsing structure helps improve the predictive accuracy. In the case of using only root labels, the left recursive S-LSTM and right recursive S-LSTM have similar performance (40.2 and 40.3, respectively), both inferior to S-LSTM (43.5). When using gold leaf labels, the gaps are smaller, but still, using the parse structure are better. Note that in real applications, where there is no out-of-vocabulary issue (i.e., some leafs are not seen in the sentiment dictionaries), the difference between S-LSTM and the recursive version without using the structures are expected to be between the gaps we observed here."}, {"heading": "6. Conclusions", "text": "We aim to extend the conventional chain-structured long short-term memory to explicitly consider structures. In this paper we particularly study tree structures, in which\nthe proposed S-LSTM memory cell can reflect the history memories of multiple descendants through gated copying of memory vectors. The model provides a principled way to consider long-distance interplays over the structures. We leveraged the model to learn distributed sentiment representations for texts, and showed that it outperforms a stateof-the-art recursive model by replacing its tensor-enhanced composition layers with the S-LSTM memory blocks. We showed that the structure information is useful in helping S-LSTM achieve the state-of-the-art performance.\nThe research community seems to contain two lines of wisdom; one attempts to learn distributed representation by utilizing structures when available, and the other prefers to believe recurrent neural networks can actually capture the structures implicitly through a linear-chain coding process. In this paper, we also attempt to give some empirical evidences toward answering the question. It is at least for the settings of our experiments that the explicit input structures are helpful in inferring the high-level (e.g., root) semantics."}], "references": [{"title": "Conkey, p. finding linguistic structure with recurrent neural networks. lawrence erlbaum assoc publ", "author": ["N Chater"], "venue": "In in: proceedings of the fourteenth annual conference of the cognitive science society. (pp", "citeRegEx": "Chater,? \\Q1992\\E", "shortCiteRegEx": "Chater", "year": 1992}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "translation. CoRR,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning with compositional semantics as structural inference for subsentential sentiment analysis", "author": ["Choi", "Yejin", "Cardie", "Claire"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Choi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2008}, {"title": "Finding temporal structure in music: Blues improvisation with lstm recurrent networks", "author": ["Eck", "Douglas", "Schmidhuber", "Jrgen"], "venue": "In NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS OF THE 2002 IEEE WORKSHOP,", "citeRegEx": "Eck et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eck et al\\.", "year": 2002}, {"title": "Learning the longterm structure of the blues", "author": ["Eck", "Douglas", "Schmidhuber", "Jrgen"], "venue": "In IN PROC. INTL. CONF, pp", "citeRegEx": "Eck et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eck et al\\.", "year": 2002}, {"title": "Learning taskdependent distributed representations by backpropagation through structure", "author": ["Goller", "Christoph", "Kchler", "Andreas"], "venue": "In In Proc. of the ICNN-96,", "citeRegEx": "Goller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goller et al\\.", "year": 1996}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "PhD thesis, Technische Universitat Munchen,", "citeRegEx": "Graves and Alex.,? \\Q2008\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2008}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "A general framework for unsupervised processing of structured", "author": ["Hammer", "Barbara", "Micheli", "Alessio", "Sperduti", "Alessandro", "Strickert", "Marc"], "venue": null, "citeRegEx": "Hammer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hammer et al\\.", "year": 2004}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Ozan", "Cardie", "Claire"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2096\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2096}, {"title": "A convolutional neural network for modelling sentences", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "A survey of opinion mining and sentiment analysis", "author": ["Liu", "Bing", "Zhang", "Lei"], "venue": "Mining Text Data,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Manning", "Christopher D", "Sch\u00fctze", "Hinrich"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Identifying purpose behind electoral tweets", "author": ["Mohammad", "Saif M", "Kiritchenko", "Svetlana", "Martin", "Joel"], "venue": "In Proceedings of the 2nd International Workshop on Issues of Sentiment Discovery and Opinion Mining,", "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Sentiment composition", "author": ["Moilanen", "Karo", "Pulman", "Stephen"], "venue": "In Proceedings of RANLP", "citeRegEx": "Moilanen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moilanen et al\\.", "year": 2007}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P.H.O. Pinheiro", "R. Collobert"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Pinheiro and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Pinheiro and Collobert", "year": 2014}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Zhu", "Xiaodan", "Guo", "Hongyu", "Mohammad", "Saif", "Kiritchenko", "Svetlana"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Recent years have seen a revival of the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), with its effectiveness being demonstrated on a wide range of problems such as speech recognition (Graves et al., 2013), machine translation (Sutskever et al.", "startOffset": 201, "endOffset": 222}, {"referenceID": 23, "context": ", 2013), machine translation (Sutskever et al., 2014; Cho et al., 2014), and image-to-text conversion (Vinyals et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 1, "context": ", 2013), machine translation (Sutskever et al., 2014; Cho et al., 2014), and image-to-text conversion (Vinyals et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 24, "context": ", 2014), and image-to-text conversion (Vinyals et al., 2014),", "startOffset": 38, "endOffset": 60}, {"referenceID": 21, "context": "Image understanding, as another example, benefits from recursive modeling over structures, which yielded the state-of-the-art performance on tasks like scene segmentation (Socher et al., 2011).", "startOffset": 171, "endOffset": 192}, {"referenceID": 0, "context": "In addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004), amongst others.", "startOffset": 137, "endOffset": 211}, {"referenceID": 9, "context": "In addition, the literature has also included many other efforts of applying feedforward-based neural network over structures, including (Goller & Kchler, 1996; Chater, 1992; Starzyk et al.; Hammer et al., 2004), amongst others.", "startOffset": 137, "endOffset": 211}, {"referenceID": 8, "context": "LSTM replaces the hidden vector of a recurrent neural network with memory blocks which are equipped with gates; it can in principle keep longterm memory by training proper gating weights (refer to (Graves, 2008) for intuitive illustrations and good discussions), and it has practically showed to be very useful, achieving the state of the art on a range of problems including speech recognition (Graves et al., 2013), digit handwriting recognition (Liwicki et al.", "startOffset": 395, "endOffset": 416}, {"referenceID": 23, "context": ", 2007; Graves, 2012), and achieve interesting results on statistical machine translation (Sutskever et al., 2014; Cho et al., 2014) and music composition (Eck & Schmidhuber, 2002b;a).", "startOffset": 90, "endOffset": 132}, {"referenceID": 1, "context": ", 2007; Graves, 2012), and achieve interesting results on statistical machine translation (Sutskever et al., 2014; Cho et al., 2014) and music composition (Eck & Schmidhuber, 2002b;a).", "startOffset": 90, "endOffset": 132}, {"referenceID": 8, "context": "In (Graves et al., 2013), a deep LSTM network achieved the state-of-the-art results on the TIMIT phoneme recognition benchmark.", "startOffset": 3, "endOffset": 24}, {"referenceID": 23, "context": "In (Sutskever et al., 2014; Cho et al., 2014), a pair of LSTM networks are trained to encode and decode human language for automatic machine translation, which is in particular effective for the more challenging long sentence translation.", "startOffset": 3, "endOffset": 45}, {"referenceID": 1, "context": "In (Sutskever et al., 2014; Cho et al., 2014), a pair of LSTM networks are trained to encode and decode human language for automatic machine translation, which is in particular effective for the more challenging long sentence translation.", "startOffset": 3, "endOffset": 45}, {"referenceID": 22, "context": "More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; 2013; Kalchbrenner et al., 2014), a more principled approach to modeling the formation of semantics.", "startOffset": 50, "endOffset": 150}, {"referenceID": 12, "context": "More recent work has started to model composition (Moilanen & Pulman, 2007; Choi & Cardie, 2008; Socher et al., 2012; 2013; Kalchbrenner et al., 2014), a more principled approach to modeling the formation of semantics.", "startOffset": 50, "endOffset": 150}, {"referenceID": 25, "context": ", the study of the effect of negation in changing sentiment (Zhu et al., 2014).", "startOffset": 60, "endOffset": 78}, {"referenceID": 16, "context": "Phrase-level sentiment analysis is often defined over a very small subset of phrases of interest, such as in the phrase-level task defined in (Wilson et al., 2005; Mohammad et al., 2013).", "startOffset": 142, "endOffset": 186}], "year": 2015, "abstractText": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-theart recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "creator": "LaTeX with hyperref package"}}}