{"id": "1503.06394", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2015", "title": "Large-scale log-determinant computation through stochastic Chebyshev expansions", "abstract": "Logarithms of determinants of large positive defined matrices are ubiquitous in machine learning applications, including graphical and Gaussian process models, partition functions of discrete graphical models, minimal volume ellipsoids, metric learning, and kernel learning. Log-determinant calculations include cholesque decomposition at the expense of the cube in the number of variables, i.e. the matrix dimension that makes them prohibitive for large-scale applications. We propose a linear-time-dependent randomized algorithm to approach log determinants for very large-scale positive definitive and general non-ingulate matrices using a stochastic trace approximation, the Hutchinson method, coupled with polynomial expansions of Chebyshev, both based on efficient matrix vector multiplications. We establish rigorous additive and multiplication approximation errors depending on the matrix of the conditional matrix.", "histories": [["v1", "Sun, 22 Mar 2015 06:55:12 GMT  (670kb,D)", "http://arxiv.org/abs/1503.06394v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.NA", "authors": ["insu han", "dmitry malioutov", "jinwoo shin"], "accepted": true, "id": "1503.06394"}, "pdf": {"name": "1503.06394.pdf", "metadata": {"source": "CRF", "title": "Large-scale Log-determinant Computation through Stochastic Chebyshev Expansions", "authors": ["Insu Han", "Dmitry Malioutov", "Jinwoo Shin"], "emails": ["hawki17@kaist.ac.kr", "dmaliout@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Scalability of machine learning algorithms for extremely large data-sets and models has been increasingly the focus of attention for the machine learning community, with prominent examples such as first-order stochastic optimization methods and randomized linear algebraic computations. One of the important tasks from linear algebra that appears in a variety of machine learning problems is computing the log-determinant of a large positive definite matrix. For example, serving as the normalization constant for multivariate Gaussian models, log-determinants of covariance (and precision) matrices play an important role in inference, model selection and learning both the structure and the parameters for Gaussian Graphical models and Gaussian processes [25, 23, 10]. Log-determinants also play an important role in a variety of Bayesian machine learning problems, including sampling and variational inference [17]. In addition, metric and kernel learning problems attempt to learn quadratic forms adapted to the data, and formulations involving Bregman divergences of log-determinants have become very popular [9, 30]. Finally, log-determinant computation also appears in some discrete probabilistic models, e.g., tree mixture models [20, 1] and Markov random fields [31]. In planar Markov random fields [26, 16] inference and learning involve log-determinants of general non-singular matrices.\nFor a positive semi-definite matrix B \u2208 Rd\u00d7d, numerical linear algebra experts recommend to compute logdeterminant using the Cholesky decomposition. Suppose the Cholesky decomposition isB = LLT , then log det(B) = 2 \u2211 i logLii. The computational complexity of Cholesky decomposition is cubic with respect to the number of variables, i.e., O(d3).1 For large-scale applications involving more than tens of thousands of variables, this operation is not feasible. Our aim in this paper is to compute accurate approximate log-determinants for matrices of much larger size involving tens of millions of variables. \u2217Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea. Emails: hawki17@kaist.ac.kr \u2020IBM T. J. Watson Research, Yorktown Heights, NY, USA, Email: dmaliout@gmail.com \u2021Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea. Email: jinwoos@kaist.ac.kr 1 For sparse matrices with a small tree-width, the complexity of Cholesky decomposition is cubic in the tree-width.\nar X\niv :1\n50 3.\n06 39\n4v 1\n[ cs\n.D S]\n2 2\nM ar\nContribution. Our approach to compute accurate approximations of log-determinant for a positive definite matrix uses a combination of stochastic trace-estimators and Chebyshev polynomial expansions. Using the Chebyshev polynomials, we first approximate the log-determinant by the trace of power series of the input matrix. We then use a stochastic trace-estimator, called the Hutchison method [14], to estimate the trace using multiplications between the input matrix and random vectors. The main assumption for our method is that the matrix-vector product can be computed efficiently. For example, the time-complexity of the proposed algorithm grows linearly with respect to the number of non-zero entries in the input matrix. We also extend our approach to general non-singular matrices to compute the absolute values of their log-determinants. We establish rigorous additive and multiplicative approximation error bounds for approximating the log-determinant under the proposed algorithm. Our theoretical results provide an analytic understanding on our Chebyshev-Hutchison method depending on sampling number, polynomial degree and the condition number (i.e., the ratio between the largest and smallest singular values) of the input matrix. In particular, they imply that if the condition number is O(1), then the algorithm provides \u03b5-approximation guarantee (in multiplicative or additive) in linear time for any constant \u03b5 > 0.\nWe first apply our algorithm to obtain a randomized linear-time approximation scheme for counting the number of spanning trees in a certain class of graphs where it could be used for efficient inference in tree mixture models [20, 1]. We also apply our algorithm for finding maximum likelihood parameter estimates of Gaussian Markov random fields of size 5000\u00d7 5000 (involving 25 million variables!), which is infeasible for the Cholesky decomposition. Our experiments show that our proposed algorithm is orders of magnitude faster than the Cholesky decomposition and Schur completion for sparse matrices and provides solutions with 99.9% accuracy in approximation. It can also solve problems of dimension tens of millions in a few minutes on our single commodity computer. Furthermore, the proposed algorithm is very easy to parallelize and hence has a potential to handle even a bigger size. In particular, the Schur method was used as a part of QUIC algorithm [13] for sparse inverse covariance estimation with over million variables, hence our algorithm could be used to further improve its speed and scale.\nRelated work. Stochastic trace estimators have been studied in the literature in a number of applications. [6, 18] have used a stochastic trace estimator to compute the diagonal of a matrix or of matrix inverse. Polynomial approximations to band-pass filters have been used to count the number of eigenvalues in certain intervals [11]. Stochastic approximations of score equations have been applied in [27] to learn large-scale Gaussian processes. The works closest to ours which have used stochastic trace estimators for Gaussian process parameter learning are [33] and [3] which instead use Taylor expansions and Cauchy integral formula, respectively. A recent improved analysis using Taylor expansions has also appeared in [8]. However, as reported in Section 5, our method using Chebyshev expansions provides much better accuracy in experiments than that using Taylor expansions, and [3] need Krylov-subspace linear system solver that is computationally expensive. [22] also use Chebyshev polynomials for log-determinant computation, but the method is deterministic and only applicable to polynomials of small degree. The novelty of our work is combining the Chebyshev approximation with Hutchison trace estimators, which allows us to design a linear-time algorithm with rigorous approximation guarantees.\nOrganization. The structure of the paper is as follows. We introduce the necessary background in Section 2.2, and describe our algorithm with approximation guarantees in Section 3. Section 4 provides the proof of approximation guarantee of our algorithm, and we report experimental results in Section 5."}, {"heading": "2 Background", "text": "In this section, we describe the preliminaries for our approach to approximate the log-determinant of a positive definite matrix. Our approach combines the following two techniques: (a) designing a trace-estimator for the log-determinant of positive definite matrix via Chebyshev approximation [19] and (b) approximating the trace of positive definite matrix via Monte Carlo methods, e.g., Hutchison method [14]."}, {"heading": "2.1 Chebyshev Approximation", "text": "The Chebyshev approximation technique is used to approximate analytic function with certain orthonormal polynomials. We use pn(x) to denote the Chebyshev approximation of degree n for a given function f : [\u22121, 1]\u2192 R:\nf(x) \u2248 pn(x) = n\u2211 j=0 cjTj(x),\nwhere the coefficient ci and the i-th Chebyshev polynomial Ti(x) are defined as\nci =  1 n+ 1 n\u2211 k=0 f(xk) T0(xk) if i = 0 2\nn+ 1 n\u2211 k=0 f(xk) Ti(xk) otherwise\n(1)\nTi+1(x) = 2xTi(x)\u2212 Ti\u22121(x) for i \u2265 1 (2)\nwhere xk = cos ( \u03c0(k+1/2) n+1 ) for k = 0, 1, 2, . . . n and T0(x) = 1, T1(x) = x.\nChebyshev approximation for scalar functions can be naturally generalized to matrix functions. Using the Chebyshev approximation pn(x) for function f(x) = log(1\u2212x) we obtain the following approximation to the log-determinant of a positive definite matrix B \u2208 Rd\u00d7d:\nlog detB = log det (I \u2212A) = d\u2211 i=1 log(1\u2212 \u03bbi)\n\u2248 d\u2211 i=1 pn(\u03bbi) = d\u2211 i=1 n\u2211 j=0 cjTj(\u03bbi)\n= n\u2211 j=0 cj d\u2211 i=1 Tj(\u03bbi) = n\u2211 j=0 cjtr (Tj (A)) ,\nwhere A = I \u2212 B has eigenvalues 0 \u2264 \u03bb1, . . . , \u03bbd \u2264 1 and the last equality is from the fact that \u2211d i=1 p(\u03bbi) = tr(p(A)) for any polynomial p.2 We remark that other polynomial approximations, e.g., Taylor, can also be used to approximate log-determinants. We focus on the Chebyshev approximation in this paper due to its superior empirical performance and rigorous error analysis."}, {"heading": "2.2 Trace Approximation via Monte-Carlo Method", "text": "The main challenge to compute the log-determinant of a positive definite matrix in the previous section is calculating the trace of Tj (A) efficiently without evaluating the entire matrixAk. We consider a Monte-Carlo approach for estimating the trace of a matrix. First, a random vector z is drawn from some fixed distribution, such that the expectation of z>Az is equal to the trace of A. By sampling m such i.i.d random vectors, and averaging we obtain an estimate of tr(A).\nIt is known that the Hutchinson method, where components of the random vectors Z are i.i.d Rademacher random variables, i.e., Pr(+1) = Pr(\u22121) = 12 , has the smallest variance among such Monte-Carlo methods [14, 5]. It has been used extensively in many applications [4, 14, 2]. Formally, the Hutchinson trace estimator trm(A) is known to satisfy the following:\nE [ trm(A) := 1\nm m\u2211 i=1 z>i Azi\n] = tr(A)\n2tr(\u00b7) denotes the trace of a matrix.\nVar [trm(A)] = 2\n( \u2016A\u20162 \u2212\nn\u2211 i=1 A2ii\n) .\nNote that computing z>Az requires only multiplications between a matrix and a vector, which is particularly appealing when evaluating A itself is expensive, e.g., A = Bk for some matrix B and large k. Furthermore, for the case A = Tj (X), one can compute z>Tj (X) z more efficiently using the following recursion on the vector wj = Tj(X)z:\nwj+1 = 2Xwj \u2212 wj\u22121,\nwhich follows directly from (2)."}, {"heading": "3 Log-determinant Approximation Scheme", "text": "Now we are ready to present algorithms to approximate the absolute value of log-determinant of an arbitrary nonsingular square matrixC. Without loss of generality, we assume that singular values ofC are in the interval [\u03c3min, \u03c3max] for some \u03c3min, \u03c3max > 0, i.e., the condition number \u03ba(C) is at most \u03bamax := \u03c3max/\u03c3min. The proposed algorithms are not sensitive to tight knowledge of \u03c3min, \u03c3max, but some loose lower and upper bounds on them, respectively, suffice.\nWe first present a log-determinant approximation scheme for positive definite matrices in Section 3.1 and that for general non-singular ones in Section 3.2 later."}, {"heading": "3.1 Algorithm for Positive Definite Matrices", "text": "In this section, we describe our proposed algorithm for estimating the log-determinant of a positive definite matrix whose eigenvalues are less than one, i.e., \u03c3max < 1. It is used as a subroutine for estimating the log-determinant of a general non-singular matrix in the next section. The formal description of the algorithm is given in what follows.\nAlgorithm 1 Log-determinant approximation for positive definite matrices with \u03c3max < 1 Input: positive definite matrix B \u2208 Rd\u00d7d with eigenvalues in [\u03b4 , 1 \u2212 \u03b4] for some \u03b4 > 0, sampling number m and polynomial degree n Initialize: A\u2190 I \u2212B, \u0393\u2190 0 for i = 0 to n do ci \u2190 i-th coefficient of Chebyshev approximation for log(1\u2212 (1\u22122\u03b4)x+12 )\nend for for i = 1 to m do\nDraw a Rademacher random vector v and u\u2190 c0 v if n > 1 then\nw0 \u2190 v and w1 \u2190 Av u\u2190 u + c1Av for j = 2 to n do w2 \u2190 2Aw1 \u2212w0 u\u2190 u + cj w2 w0 \u2190 w1 and w1 \u2190 w2\nend for end if \u0393\u2190 \u0393 + v>u/m\nend for Output: \u0393\nWe establish the following theoretical guarantee of the above algorithm, where its proof is given in Section 4.3.\nTheorem 1 Given \u03b5, \u03b6 \u2208 (0, 1), consider the following inputs for Algorithm 1:\n\u2022 B \u2208 Rd\u00d7d be a positive definite matrix with eigenvalues in [\u03b4, 1\u2212 \u03b4] for some \u03b4 \u2208 (0, 1/2). \u2022 m \u2265 54\u03b5\u22122 log (\n2 \u03b6 ) \u2022 n \u2265 log ( 20 \u03b5 (\u221a 2 \u03b4\u22121\u22121 ) log(2(1/\u03b4\u22121)) log (1/1\u2212\u03b4) ) log (\u221a 2\u2212\u03b4+ \u221a \u03b4\u221a\n2\u2212\u03b4\u2212 \u221a \u03b4 ) = O (\u221a 1\u03b4 log ( 1\u03b5\u03b4 )) Then, it follows that\nPr [ |log detB \u2212 \u0393| \u2264 \u03b5 |log detB| ] \u2265 1\u2212 \u03b6\nwhere \u0393 is the output of Algorithm 1.\nThe bound on polynomial degree n in the above theorem is relatively tight, e.g., it implies to choose n = 14 for \u03b4 = 0.1 and \u03b5 = 0.01. However, our bound on sampling number m is not, where we observe that m \u2248 30 is sufficient for high accuracy in our experiments. We also remark that the time-complexity of Algorithm 1 is O(mn\u2016B\u20160), where \u2016B\u20160 is the number of non-zero entries of B. This is because the algorithm requires only multiplications of matrices and vectors. In particular, if m,n = O(1), the complexity is linear with respect to the input size. Therefore, Theorem 1 implies that one can choose m,n = O(1) for \u03b5-multiplicative approximation with probability 1\u2212 \u03b6 given constants \u03b5, \u03b6 > 0."}, {"heading": "3.2 Algorithm for General Non-Singular Matrices", "text": "Now, we are ready to present our linear-time approximation scheme for the log-determinant of general non-singular matrix C, through generalizing the algorithm in the previous section. The idea is simple: run Algorithm 1 with normalization of positive definite matrix CTC. This is formally described in what follows.\nAlgorithm 2 Log-determinant approximation for general non-singular matrices Input: matrix C \u2208 Rd\u00d7d with singular values are in the interval [\u03c3min, \u03c3max] for some \u03c3min, \u03c3max > 0, sampling number m and polynomial degree n\nInitialize: B \u2190 1 \u03c32min+\u03c3 2 max\nCTC, \u03b4 \u2190 \u03c3 2 min\n\u03c32min+\u03c3 2 max\n\u0393\u2190 Output of Algorithm 1 for inputs B,m, n, \u03b4 Output: \u0393\u2190 ( \u0393 + d log (\u03c32min + \u03c3 2 max) ) /2\nAlgorithm 2 is motivated to design from the equality log |detC| = 12 log detC TC. Given non-singular matrix C, one need to choose appropriate \u03c3max, \u03c3min to run it. In most applications, \u03c3max is easy to choose, e.g., one can choose\n\u03c3max = \u221a \u2016C\u20161\u2016C\u2016\u221e,\nor one can run the power iteration [15] to estimate a better bound. On the other hand, \u03c3min is relatively not easy to obtain depending on problems. It is easy to obtain in the problem of counting spanning trees we studied in Section 3.3, and it is explicitly given as a parameter in many machine learning log-determinant applications [31]. In general, one can use the inverse power iteration [15] to estimate it. Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].\nThe time-complexity of Algorithm 2 is still O(mn\u2016C\u20160) instead of O(mn\u2016CTC\u20160) since Algorithm 1 requires multiplication of matrix CTC and vectors. We state the following additive error bound of the above algorithm.\nTheorem 2 Given \u03b5, \u03b6 \u2208 (0, 1), consider the following inputs for Algorithm 2:\n\u2022 C \u2208 Rd\u00d7d be a matrix such that singular values are in the interval [\u03c3min, \u03c3max] for some \u03c3min, \u03c3max > 0. \u2022 m \u2265M ( \u03b5, \u03c3max\u03c3min , \u03b6 ) and n \u2265 N ( \u03b5, \u03c3max\u03c3min ) , where\nM(\u03b5, \u03ba, \u03b6) := 14 \u03b52 ( log ( 1 + \u03ba2 ))2 log 2 \u03b6\nN (\u03b5, \u03ba) := log ( 20 \u03b5 (\u221a 2\u03ba2 + 1\u2212 1 ) log (1+\u03ba2) log(2\u03ba2) log(1+\u03ba\u22122) ) log (\u221a\n2\u03ba2+1+1\u221a 2\u03ba2+1\u22121\n) = O (\u03ba log \u03ba \u03b5 ) Then, it follows that\nPr [ |log (|detC|)\u2212 \u0393| \u2264 \u03b5d ] \u2265 1\u2212 \u03b6\nwhere \u0393 is the output of Algorithm 2.\nProof. The proof of Theorem 2 is quite straightforward using Theorem 1 for B with the facts that\n2 log |detC| = log detB + d log (\u03c32min + \u03c32max)\nand | log detB| \u2264 d log (\n1 + \u03c32max \u03c32min\n) .\nWe remark that the condition number \u03c3max/\u03c3min decides the complexity of Algorithm 2. As one can expect, the approximation quality and algorithm complexity become worse for matrices with very large condition numbers, as the Chebyshev approximation for the function log x near the point 0 is more challenging and requires higher degree approximations.\nWhen \u03c3max \u2265 1 and \u03c3min \u2264 1, i.e. we have mixed signs for logs of the singular values, a multiplicative error bound (as stated in Theorem 1) can not be obtained since the log-determinant can be zero in the worst case. On the other hand, when \u03c3max < 1 or \u03c3min > 1, we further show that the above algorithm achieves an \u03b5-multiplicative approximation guarantee, as stated in the following corollaries.\nCorollary 3 Given \u03b5, \u03b6 \u2208 (0, 1), consider the following inputs for Algorithm 2:\n\u2022 C \u2208 Rd\u00d7d be a matrix such that singular values are in the interval [\u03c3min, \u03c3max] for some \u03c3max < 1. \u2022 m \u2265M ( \u03b5 log 1\u03c3max , \u03c3max \u03c3min , \u03b6 )\n\u2022 n \u2265 N ( \u03b5 log 1\u03c3max , \u03c3max \u03c3min ) Then, it follows that\nPr [ |log |detC| \u2212 \u0393| \u2264 \u03b5 |log |detC||] \u2265 1\u2212 \u03b6\nwhere \u0393 is the output of Algorithm 2.\nCorollary 4 Given \u03b5, \u03b6 \u2208 (0, 1), consider the following inputs for Algorithm 2:\n\u2022 C \u2208 Rd\u00d7d be a matrix such that singular values are in the interval [\u03c3min, \u03c3max] for some \u03c3min > 1. \u2022 m \u2265M ( \u03b5 log \u03c3min,\n\u03c3max \u03c3min\n, \u03b6 )\n\u2022 n \u2265 N ( \u03b5 log \u03c3min,\n\u03c3max \u03c3min ) Then, it follows that\nPr [ |log detC \u2212 \u0393| \u2264 \u03b5 log detC] \u2265 1\u2212 \u03b6\nwhere \u0393 is the output of Algorithm 2.\nThe proofs of the above corollaries are given in the supplementary material due to the space limitation."}, {"heading": "3.3 Application to Counting Spanning Trees", "text": "We apply Algorithm 2 to a concrete problem, where we study counting the number of spanning trees in a simple undirected graph G = (V,E) where there exists a vertex i\u2217 such that (i\u2217, j) \u2208 E for all j \u2208 V \\ {i\u2217}. Counting spanning trees is one of classical well-studied counting problems, and also necessary in machine learning applications, e.g., tree mixture models [20, 1]. We denote the maximum and average degrees of vertices in V \\ {i\u2217} by \u2206max and \u2206avg > 1, respectively. In addition, we let L(G) denote the Laplacian matrix ofG. Then, from Kirchhoff\u2019s matrix-tree theorem, the number of spanning tree \u03c4(G) is equal to\n\u03c4(G) = detL(i\u2217),\nwhere L(i\u2217) is the (|V | \u2212 1) \u00d7 (|V | \u2212 1) sub matrix of L(G) that is obtained by eliminating the row and column corresponding to i\u2217. Now, it is easy to check that eigenvalues of L(i\u2217) are in [1, 2\u2206max \u2212 1]. Under these observations, we derive the following corollary.\nCorollary 5 Given 0 < \u03b5 < 2\u2206avg\u22121 , \u03b6 \u2208 (0, 1), consider the following inputs for Algorithm 2:\n\u2022 C = L(i\u2217) \u2022 m \u2265M ( \u03b5(\u2206avg\u22121) 4 , 2\u2206max \u2212 1, \u03b6 )\n\u2022 n \u2265 N ( \u03b5(\u2206avg\u22121) 4 , 2\u2206max \u2212 1 )\nThen, it follows that"}, {"heading": "Pr [| log \u03c4(G)\u2212 \u0393| \u2264 \u03b5 log \u03c4(G)] \u2265 1\u2212 \u03b6", "text": "where \u0393 is the output of Algorithm 2.\nThe proof of the above corollary is given in the supplementary material due to the space limitation. We remark that the running time of Algorithm 2 with inputs in the above theorem is O(nm\u2206avg|V |). Therefore, for \u03b5, \u03b6 = \u2126(1) and \u2206avg = O(1), i.e., G is sparse, one can choose n,m = O(1) so that the running time of Algorithm 2 is O(|V |)."}, {"heading": "4 Proof of Theorem 1", "text": "In order to prove Theorem 1, we first introduce some necessary background and lemmas on error bounds of Chebyshev approximation and Hutchinson method we introduced in Section 2.1 and Section 2.2, respectively."}, {"heading": "4.1 Convergence Rate for Chebyshev Approximation", "text": "Intuitively, one can expect that the approximated Chebyshev polynomial converges to its original function as degree n goes to\u221e. Formally, the following error bound is known [7, 32].\nTheorem 6 Suppose f is analytic with |f(z)| \u2264 M in the region bounded by the ellipse with foci \u00b11 and major and minor semiaxis lengths summing to K > 1. Let pn denote the interpolant of f of degree n in th Chebyshev points as defined in section 2.1, then for each n \u2265 0,\nmax x\u2208[\u22121,1]\n|f(x)\u2212 pn(x)| \u2264 4M\n(K \u2212 1)Kn\nTo prove Theorem 1 and Theorem 2, we are in particular interested in\nf(x) = log(1\u2212 x), for x \u2208 [\u03b4, 1\u2212 \u03b4].\nSince Chebyshev approximation is defined in the interval [\u22121, 1], e.g., see Section 2.1, one can use the following linear mapping g : [\u03b4, 1\u2212 \u03b4]\u2192 [\u22121, 1] so that\nmax x\u2208[\u22121,1]\n|(f \u25e6 g)(x)\u2212 pn(x)|\n= max x\u2208[\u03b4,1\u2212\u03b4] \u2223\u2223f (x)\u2212 (pn \u25e6 g\u22121)(x)\u2223\u2223 For notational convenience, we use pn(x) to denote (pn \u25e6 g\u22121)(x) in what follows.\nWe choose the ellipse region, denoted by EK , in the complex plane with foci \u00b11 and its semimajor axis length is 1/(1\u2212\u03b4) where f\u25e6g is analytic on and inside. The length of semimajor axis of the ellipse is equal to \u221a (1/(1\u2212 \u03b4))2 \u2212 1. Hence, the convergence rate K can be set to\nK = 1\n1\u2212 \u03b4 +\n\u221a( 1\n1\u2212 \u03b4\n)2 \u2212 1 = \u221a 2\u2212 \u03b4 + \u221a \u03b4\n\u221a 2\u2212 \u03b4 \u2212 \u221a \u03b4 > 1\nThe constant M can be also obtained using the fact that |log z| = |log |z|+ i arg (z)| \u2264 \u221a\n(log |z|)2 + \u03c02 for any z \u2208 C as follows:\nmax z\u2208EK |(f \u25e6 g)(z)| = max z\u2208EK |(f \u25e6 g)(z)| = max z\u2208EK |log (1\u2212 g(z))|\n\u2264 max z\u2208EK\n\u221a (log |1\u2212 g(z)|)2 + \u03c02\n= \u221a log2 ( 2 ( 1 \u03b4 \u2212 1 )) + \u03c02 \u2264 5 log ( 2 ( 1 \u03b4 \u2212 1 )) := M.\nHence, for x \u2208 [\u03b4, 1\u2212 \u03b4],\n|log (1\u2212 x)\u2212 pn(x)| \u2264 20 log\n( 2 (\n1 \u03b4 \u2212 1 )) (K \u2212 1)Kn\nUnder these observations, we establish the following lemma that is a \u2018matrix version\u2019 of Theorem 6.\nLemma 7 Let B \u2208 Rd\u00d7d be a positive definite matrix whose eigenvalues are in [\u03b4, 1 \u2212 \u03b4] for \u03b4 \u2208 (0, 1/2). Then, it holds that \u2223\u2223 log detB \u2212 tr(pn(I \u2212B))\u2223\u2223 \u2264 20d log (2 ( 1\u03b4 \u2212 1))\n(K \u2212 1)Kn\nwhere K = \u221a 2\u2212\u03b4+ \u221a \u03b4\u221a\n2\u2212\u03b4\u2212 \u221a \u03b4 .\nProof. Let \u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbd \u2208 [\u03b4, 1\u2212 \u03b4] be eigenvalues of matrix A = I \u2212B. Then, we have\n|log det(I \u2212A)\u2212 tr (pn(A))| = |tr (log(I \u2212A))\u2212 tr (pn(A))|\n= \u2223\u2223\u2223\u2223\u2223 d\u2211 i=1 log(1\u2212 \u03bbi)\u2212 d\u2211 i=1 pn(\u03bbi) \u2223\u2223\u2223\u2223\u2223 \u2264\nd\u2211 i=1 |log(1\u2212 \u03bbi)\u2212 pn(\u03bbi)|\n\u2264 d\u2211 i=1\n20 log ( 2 (\n1 \u03b4 \u2212 1 )) (K \u2212 1)Kn\nwhere we use Theorem 6. This completes the proof of Lemma 7."}, {"heading": "4.2 Approximation Error of Hutchinson Method", "text": "In this section, we use the same notation, e.g., f, pn, used in the previous section and we analyze the Hutchinson\u2019s trace estimator trm(\u00b7) defined in Section 2.2. To begin with, we state the following theorem that is proven in [24].\nTheorem 8 Let A \u2208 Rd\u00d7d be a positive definite or negative definite matrix. Given \u03b50, \u03b60 \u2208 (0, 1), it holds that\nPr [|trm(A)\u2212 tr(A)| \u2264 \u03b50 tr(A)] \u2265 1\u2212 \u03b60\nif sampling number m is no smaller than 6 \u03b5\u221220 log(2/\u03b60).\nThe theorem above provides a lower-bound on the sampling complexity of Hutchinson method, which is independent of a given matrix A. To prove Theorem 1, we need an error bound on trm(pn(A)). However, in general we may not know whether or not pn(A) is positive definite or negative definite. We can guarantee that the eigenvalues of pn(A) will be negative using the following lemma.\nLemma 9 pn(x) is a negative-valued polynomial in the interval [\u03b4, 1\u2212 \u03b4] if\n20 log ( 2 (\n1 \u03b4 \u2212 1 )) (K \u2212 1)Kn \u2264 log ( 1 1\u2212 \u03b4 ) where we recall that K = \u221a 2\u2212\u03b4+ \u221a \u03b4\u221a\n2\u2212\u03b4\u2212 \u221a \u03b4 .\nProof. From Theorem 6, we have\nmax [\u03b4,1\u2212\u03b4] pn(x) = max [\u03b4,1\u2212\u03b4]\nf(x) + (pn(x)\u2212 f(x))\n\u2264 max [\u03b4,1\u2212\u03b4] f(x) + max [\u03b4,1\u2212\u03b4] |pn(x)\u2212 f(x)|\n\u2264 log (1\u2212 \u03b4) + 20 log\n( 2( 1\u03b4 \u2212 1) ) (K \u2212 1)Kn \u2264 0,\nwhere we use 20 log(2(1/\u03b4\u22121))(K\u22121)Kn \u2264 \u2212 log(1\u2212 \u03b4). This completes the proof of Lemma 9."}, {"heading": "4.3 Proof of the Theorem 1", "text": "Now we are ready to prove Theorem 1. First, one can check that sampling number n in the condition of Theorem 1 satisfies\n20 log ( 2( 1\u03b4 \u2212 1) ) (K \u2212 1)Kn \u2264 \u03b5 2 log ( 1 1\u2212 \u03b4 ) . (3)\nHence, from Lemma 9, it follows that pn(A) is negative definite whereA = I\u2212B and eigenvalues ofB are in [\u03b4, 1\u2212\u03b4]. Hence, we can apply Theorem 8 as\nPr [ |tr (pn(A))\u2212 trm (pn(A))| \u2264 \u03b5\n3 |tr (pn(A))|\n] \u2265 1\u2212 \u03b6, (4)\nfor m \u2265 54\u03b5\u22122 log (\n2 \u03b6\n) . In addition, from Theorem 7, we have\n|tr (pn(A))| \u2212 |log detB| \u2264 |log detB \u2212 tr (pn(A))|\n\u2264 20d log (2(1/\u03b4 \u2212 1)) (K \u2212 1)Kn\n\u2264 \u03b5 2 d log\n( 1\n1\u2212 \u03b4\n) \u2264 \u03b5\n2 |log detB| ,\nwhich implies that\n|tr (pn(A))| \u2264 (\u03b5 2 + 1 ) |log detB| \u2264 3 2 |log detB| . (5)\nCombining (3), (4) and (5) leads to the conclusion of Theorem 1 as follows: 1\u2212 \u03b6 \u2264 Pr [ |tr (pn(A))\u2212 trm (pn(A))| \u2264 \u03b5\n3 |tr (pn(A))| ] \u2264 Pr [ |tr (pn(A))\u2212 trm (pn(A))| \u2264 \u03b5\n2 |log detB| ] \u2264 Pr[|tr (pn(A))\u2212 trm (pn(A))|+ | log detB \u2212 tr (pn(A)) |\n\u2264 \u03b5 2 |log detB|+ \u03b5 2 |log detB|]\n\u2264 Pr [|log detB \u2212 trm (pn(A))| \u2264 \u03b5 |log detB|] = Pr [|log detB \u2212 \u0393| \u2264 \u03b5 |log detB|] ,\nwhere \u0393 = trm (pn(A))."}, {"heading": "5 Experiments", "text": "We now study our proposed algorithm on numerical experiments with simulated and real data."}, {"heading": "5.1 Performance Evaluation and Comparison", "text": "We first investigate the empirical performance of our proposed algorithm on large sparse random matrices. We generate a random matrix C \u2208 Rd\u00d7d, where the number of non-zero entries per each row is around 10. We first select five nonzero off-diagonal entries in each row with values uniformly distributed in [\u22121, 1]. To make the matrix symmetric, we set the entries in transposed positions to the same values. Finally, to guarantee positive definiteness, we set its diagonal entries to absolute row-sums and add a small weight, 10\u22123.\nFigure 1 (a) shows the running time of Algorithm 2 from d = 103 to 3\u00d7 107, where we choose m = 10, n = 15, \u03c3min = 10\n\u22123 and \u03c3max = \u2016C\u20161. It scales roughly linearly over a large range of sizes. We use a machine with 3.40 Ghz Intel I7 processor with 24 GB RAM. It takes only 500 seconds for a matrix of size 3\u00d7 107 with 3\u00d7 108 non-zero entries. In Figure 1 (b), we study the relative accuracy compared to the exact log-determinant computation up-to size 3\u00d7 104. Relative errors are very small, below 0.1%, and appear to only improve for higher dimensions.\nUnder the same setup, we also compare the running time of our algorithm with other algorithm for computing determinants: Cholesky decomposition and Schur complement. The latter was used for sparse inverse covariance\nestimation with over a million variables [13] and we run the code implemented by the authors. The running time of the algorithms are reported in Figure 1 (c). The proposed algorithm is dramatically faster than both exact algorithms. We also compare the accuracy of our algorithm to a related stochastic algorithm that uses Taylor expansions [33]. For a fair comparison we use a large number of samples, n = 1000, for both algorithms to focus on the polynomial approximation errors. The results are reported in Figure 1 (d), showing that our algorithm using Chebyshev expansions is superior in accuracy compared to the one based on Taylor series."}, {"heading": "5.2 Maximum Likelihood Estimation for GMRF", "text": "GMRF with 25 million variables for synthetic data. We now apply our proposed algorithm for maximum likelihood (ML) estimation in Gaussian Markov Random Fields (GMRF) [25]. GMRF is a multi-variate joint Gaussian distribution defined with respect to a graph. Each node of the graph corresponds to a random variable in the Gaussian distribution, where the graph captures the conditional independence relationships (Markov properties) among the random variables. The model has been extensively used in many applications in computer vision, spatial statistics, and other fields. The inverse covariance matrix J (also called information or precision matrix) is positive definite and sparse: Jij is non-zero only if the edge {i, j} is contained in the graph.\nWe first consider a GMRF on a square grid of size 5000 \u00d7 5000 (with d = 25 million variables) with precision matrix J \u2208 Rd\u00d7d parameterized by \u03c1, i.e., each node has four neighbors with partial correlation \u03c1. We generate a sample x from the GMRF model (using Gibbs sampler) for parameter \u03c1 = \u22120.22. The log-likelihood of the sample is: log p(x|\u03c1) = log det J(\u03c1) \u2212 x>J(\u03c1)x + G, where J(\u03c1) is a matrix of dimension 25 \u00d7 106 and 108 non-zero entries, and G is a constant independent of \u03c1. We use Algorithm 2 to estimate the log-likelihood as a function of \u03c1, as reported in Figure 3. The estimated log-likelihood is maximized at the correct (hidden) value \u03c1 = \u22120.22.\nGMRF with 6 million variables for Ozone data. We also consider GMRF parameter estimation from real spatial data with missing values. We use the data-set from [3] that provides satellite measurements of Ozone levels over the entire earth following the satellite tracks. We use a resolution of 0.1 degrees in lattitude and longitude, giving a spatial field of size 1681 \u00d7 3601, with over 6 million variables. The data-set includes 172 thousands measurements. To estimate the log-likelihood in presence of missing values, we use the Schur-complement formula for determinants.\nLet the precision matrix for the entire field be J = ( Jo Jo,z Jz,o Jz ) , where subsets xo and xz denote the observed and unobserved components of x. The marginal precision matrix of xo is J\u0304o = Jo \u2212 Jo,zJ\u22121z Jz,o. Its log-determinant is computed as log(det(J\u0304o)) = log det(J)\u2212 log det(Jz) via Schur complements. To evaluate the quadratic term x\u2032oJ\u0304oxo of the log-likelihood we need a single linear solve using an iterative solver. We use a linear combination of the thinplate model and the thin-membrane models [25], with two parameters \u03b1 and \u03b2: J = \u03b1I + (\u03b2)Jtp + (1 \u2212 \u03b2)Jtm and\nobtain ML estimates using Algorithm 2. Note that \u03c3min(J) = \u03b1. We show the sparse measurements in Figure 2 (a) and the GMRF interpolation using fitted values of parameters in Figure 2 (b)."}, {"heading": "6 Conclusion", "text": "Tools from numerical linear algebra, e.g. determinants, matrix inversion and linear solvers, eigenvalue computation and other matrix decompositions, have been playing an important theoretical and computational role for machine learning applications. While most matrix computations admit polynomial-time algorithms, they are often infeasible for largescale or high-dimensional data-sets. In this paper, we design and analyze a high accuracy linear-time approximation algorithm for the logarithm of matrix determinants, where its exact computation requires cubic-time. Furthermore, it is very easy to parallelize since it requires only (separable) matrix-vector multiplications. We believe that the proposed algorithm will find numerous applications in machine learning problems."}, {"heading": "Acknowledgement", "text": "We would like to thank Haim Avron and Jie Chen for fruitful comments on Chebyshev approximations, and Cho-Jui Hsieh for providing the code for Shur complement-based log-det computation."}, {"heading": "A Proof of Corollary 3", "text": "For given \u03b5 < 2log(\u03c32max) , set \u03b50 = \u03b5 2 log\n( 1\n\u03c32max\n) . Since all eigenvalues of CTC are positive and less than 1, it follows\nthat\n\u2223\u2223log det (CTC)\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 d\u2211 i=1 log \u03bbi \u2223\u2223\u2223\u2223\u2223 \u2265 d log ( 1 \u03c32max ) where \u03bbi are i-th eigenvalues of CTC. Thus,\n\u03b50 = \u03b5\n2 log\n( 1\n\u03c32max\n) \u2264 \u03b5\n2 \u2223\u2223log detCTC\u2223\u2223 d = \u03b5 |log (|detC|)| d\nWe use \u03b50 instead of \u03b5 from Theorem 2, then following\nPr [ |log (|detC|)\u2212 \u0393| \u2264 \u03b5 |log (|detC|)| ] \u2265 1\u2212 \u03b6\nholds if m and n satifies below condition."}, {"heading": "B Proof of Corollary 4", "text": "Similar to proof of Corollary 3, set \u03b50 = \u03b52 log \u03c3 2 min. Since eigenvalues of C\nTC are greater than 1,\u2223\u2223log det (CTC)\u2223\u2223 \u2265 d log \u03c32min and \u03b50 \u2264 \u03b5 |log(| detC|)|d . From Theorem 2, we substitute \u03b50 into \u03b5 and\nPr [ |log detC \u2212 \u0393| \u2264 \u03b5 |log detC| ] \u2265 1\u2212 \u03b6\nholds if m and n satifies below condition."}, {"heading": "C Proof of Corollary 5", "text": "For \u03b50 = \u03b5(\u2206avg \u2212 1)/2, \u03b6 \u2208 (0, 1), Theorem 2 provides the following inequality:"}, {"heading": "Pr (| log detL(i\u2217)\u2212 \u0393| \u2264 \u03b50(|V | \u2212 1)) \u2265 1\u2212 \u03b6.", "text": "Observe that since vertex i\u2217 is connected all other vertices, the number of spanning tree, i.e., detL(i\u2217), is greater than 2(|V |\u22121)(\u2206avg\u22121)/2. Hence, we have\nPr (| log detL(i\u2217)\u2212 \u0393| \u2264 \u03b50(|V | \u2212 1)) \u2264 Pr (| log detL(i\u2217)\u2212 \u0393| \u2264 \u03b5 log detL(i\u2217)) .\nThis completes the proof of Corollary 5."}], "references": [{"title": "Learning mixtures of tree graphical models", "author": ["A. Anandkumar", "F. Huang", "D.J. Hsu", "S.M. Kakade"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Robust inversion, dimensionality reduction, and randomized sampling", "author": ["A. Aravkin", "M.P. Friedlander", "F.J. Herrmann", "T. Van Leeuwen"], "venue": "Mathematical Programming,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Parameter estimation in high dimensional gaussian distributions", "author": ["E. Aune", "D.P. Simpson", "J. Eidsvik"], "venue": "Statistics and Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Counting triangles in large graphs using randomized matrix trace estimation", "author": ["H. Avron"], "venue": "In Workshop on Largescale Data Mining: Theory and Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix", "author": ["H. Avron", "S. Toledo"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "An estimator for the diagonal of a matrix", "author": ["C Bekas", "E Kokiopoulou", "Y. Saad"], "venue": "Applied numerical mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Barycentric lagrange interpolation", "author": ["J.P. Berrut", "L.N. Trefethen"], "venue": "SIAM Review,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A randomized algorithm for approximating the log determinant of a symmetric positive definite matrix", "author": ["Boutsidis", "Christos", "Drineas", "Petros", "Kambadur", "Prabhanjan", "Zouzias", "Anastasios"], "venue": "arXiv preprint arXiv:1503.00374,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Efficient estimation of eigenvalue counts in an interval", "author": ["E. Di Napoli", "E. Polizzi", "Y. Saad"], "venue": "arXiv preprint arXiv:1308.4275,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Uber die abgrenzung der eigenwerte einer matrix", "author": ["Gershgorin", "Semyon Aranovich"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1931}, {"title": "BIG & QUIC: Sparse inverse covariance estimation for a million variables", "author": ["C.J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P.K. Ravikumar", "R. Poldrack"], "venue": "In Adv. in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines", "author": ["M.F. Hutchinson"], "venue": "Communications in Statistics-Simulation and Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Computing an eigenvector with inverse iteration", "author": ["Ipsen", "Ilse CF"], "venue": "SIAM review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Learning planar ising models", "author": ["J.K. Johnson", "P. Netrapalli", "M. Chertkov"], "venue": "preprint arXiv:1011.3494,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J.C. MacKay"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Low-rank variance estimation in large-scale gmrf models", "author": ["D.M. Malioutov", "J.K. Johnson", "A.S. Willsky"], "venue": "In IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2006.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Learning with mixtures of trees", "author": ["M. Meila", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Bounds for norms of the matrix inverse and the smallest singular value", "author": ["N. Mora\u010da"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Chebyshev approximation of log-determinants of spatial weight matrices", "author": ["R.K. Pace", "J.P. LeSage"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": "MIT press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Improved bounds on sample size for implicit matrix trace estimators", "author": ["F. Roosta-Khorasani", "U. Ascher"], "venue": "arXiv preprint arXiv:1308.2475,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Gaussian Markov random fields: theory and applications", "author": ["H. Rue", "L. Held"], "venue": "CRC Press,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Efficient exact inference in planar ising models", "author": ["N.N. Schraudolph", "D. Kamenetsky"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Stochastic approximation of score functions for gaussian processes", "author": ["M.L. Stein", "J. Chen", "M. Anitescu"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Random matrices: The distribution of the smallest singular values", "author": ["T. Tao", "V. Vu"], "venue": "Geometric And Functional Analysis,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Inverse littlewood-offord theorems and the condition number of random discrete matrices", "author": ["T. Tao", "V.H. Vu"], "venue": "Annals of Mathematics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Minimum volume ellipsoid", "author": ["S. Van Aelst", "P. Rousseeuw"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Log-determinant relaxation for approximate inference in discrete markov random fields", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Signal Processing, IEEE Trans. on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Error bounds for approximation in chebyshev points", "author": ["Xiang", "Shuhuang", "Chen", "Xiaojun", "Wang", "Haiyong"], "venue": "Numerische Mathematik,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Approximate implementation of the logarithm of the matrix determinant in gaussian process regression", "author": ["Y. Zhang", "W.E. Leithead"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}], "referenceMentions": [{"referenceID": 22, "context": "For example, serving as the normalization constant for multivariate Gaussian models, log-determinants of covariance (and precision) matrices play an important role in inference, model selection and learning both the structure and the parameters for Gaussian Graphical models and Gaussian processes [25, 23, 10].", "startOffset": 298, "endOffset": 310}, {"referenceID": 20, "context": "For example, serving as the normalization constant for multivariate Gaussian models, log-determinants of covariance (and precision) matrices play an important role in inference, model selection and learning both the structure and the parameters for Gaussian Graphical models and Gaussian processes [25, 23, 10].", "startOffset": 298, "endOffset": 310}, {"referenceID": 15, "context": "Log-determinants also play an important role in a variety of Bayesian machine learning problems, including sampling and variational inference [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 8, "context": "In addition, metric and kernel learning problems attempt to learn quadratic forms adapted to the data, and formulations involving Bregman divergences of log-determinants have become very popular [9, 30].", "startOffset": 195, "endOffset": 202}, {"referenceID": 27, "context": "In addition, metric and kernel learning problems attempt to learn quadratic forms adapted to the data, and formulations involving Bregman divergences of log-determinants have become very popular [9, 30].", "startOffset": 195, "endOffset": 202}, {"referenceID": 17, "context": ", tree mixture models [20, 1] and Markov random fields [31].", "startOffset": 22, "endOffset": 29}, {"referenceID": 0, "context": ", tree mixture models [20, 1] and Markov random fields [31].", "startOffset": 22, "endOffset": 29}, {"referenceID": 28, "context": ", tree mixture models [20, 1] and Markov random fields [31].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "In planar Markov random fields [26, 16] inference and learning involve log-determinants of general non-singular matrices.", "startOffset": 31, "endOffset": 39}, {"referenceID": 14, "context": "In planar Markov random fields [26, 16] inference and learning involve log-determinants of general non-singular matrices.", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "We then use a stochastic trace-estimator, called the Hutchison method [14], to estimate the trace using multiplications between the input matrix and random vectors.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We first apply our algorithm to obtain a randomized linear-time approximation scheme for counting the number of spanning trees in a certain class of graphs where it could be used for efficient inference in tree mixture models [20, 1].", "startOffset": 226, "endOffset": 233}, {"referenceID": 0, "context": "We first apply our algorithm to obtain a randomized linear-time approximation scheme for counting the number of spanning trees in a certain class of graphs where it could be used for efficient inference in tree mixture models [20, 1].", "startOffset": 226, "endOffset": 233}, {"referenceID": 11, "context": "In particular, the Schur method was used as a part of QUIC algorithm [13] for sparse inverse covariance estimation with over million variables, hence our algorithm could be used to further improve its speed and scale.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "[6, 18] have used a stochastic trace estimator to compute the diagonal of a matrix or of matrix inverse.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[6, 18] have used a stochastic trace estimator to compute the diagonal of a matrix or of matrix inverse.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "Polynomial approximations to band-pass filters have been used to count the number of eigenvalues in certain intervals [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "Stochastic approximations of score equations have been applied in [27] to learn large-scale Gaussian processes.", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "The works closest to ours which have used stochastic trace estimators for Gaussian process parameter learning are [33] and [3] which instead use Taylor expansions and Cauchy integral formula, respectively.", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "The works closest to ours which have used stochastic trace estimators for Gaussian process parameter learning are [33] and [3] which instead use Taylor expansions and Cauchy integral formula, respectively.", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "A recent improved analysis using Taylor expansions has also appeared in [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "However, as reported in Section 5, our method using Chebyshev expansions provides much better accuracy in experiments than that using Taylor expansions, and [3] need Krylov-subspace linear system solver that is computationally expensive.", "startOffset": 157, "endOffset": 160}, {"referenceID": 19, "context": "[22] also use Chebyshev polynomials for log-determinant computation, but the method is deterministic and only applicable to polynomials of small degree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": ", Hutchison method [14].", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": ", Pr(+1) = Pr(\u22121) = 12 , has the smallest variance among such Monte-Carlo methods [14, 5].", "startOffset": 82, "endOffset": 89}, {"referenceID": 4, "context": ", Pr(+1) = Pr(\u22121) = 12 , has the smallest variance among such Monte-Carlo methods [14, 5].", "startOffset": 82, "endOffset": 89}, {"referenceID": 3, "context": "It has been used extensively in many applications [4, 14, 2].", "startOffset": 50, "endOffset": 60}, {"referenceID": 12, "context": "It has been used extensively in many applications [4, 14, 2].", "startOffset": 50, "endOffset": 60}, {"referenceID": 1, "context": "It has been used extensively in many applications [4, 14, 2].", "startOffset": 50, "endOffset": 60}, {"referenceID": 13, "context": "\u03c3max = \u221a \u2016C\u20161\u2016C\u2016\u221e, or one can run the power iteration [15] to estimate a better bound.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "3, and it is explicitly given as a parameter in many machine learning log-determinant applications [31].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "In general, one can use the inverse power iteration [15] to estimate it.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 80, "endOffset": 88}, {"referenceID": 25, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 120, "endOffset": 128}, {"referenceID": 18, "context": "Furthermore, the smallest singular value is easy to compute for random matrices [29, 28] and diagonal-dominant matrices [12, 21].", "startOffset": 120, "endOffset": 128}, {"referenceID": 17, "context": ", tree mixture models [20, 1].", "startOffset": 22, "endOffset": 29}, {"referenceID": 0, "context": ", tree mixture models [20, 1].", "startOffset": 22, "endOffset": 29}, {"referenceID": 6, "context": "Formally, the following error bound is known [7, 32].", "startOffset": 45, "endOffset": 52}, {"referenceID": 29, "context": "Formally, the following error bound is known [7, 32].", "startOffset": 45, "endOffset": 52}, {"referenceID": 21, "context": "To begin with, we state the following theorem that is proven in [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "dimension, (b) relative accuracy, (c) comparison in running time with Cholesky decomposition and Schur complement and (d) comparison in accuracy with Taylor approximation in [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 11, "context": "estimation with over a million variables [13] and we run the code implemented by the authors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "We also compare the accuracy of our algorithm to a related stochastic algorithm that uses Taylor expansions [33].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "We now apply our proposed algorithm for maximum likelihood (ML) estimation in Gaussian Markov Random Fields (GMRF) [25].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "We use the data-set from [3] that provides satellite measurements of Ozone levels over the entire earth following the satellite tracks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 22, "context": "We use a linear combination of the thinplate model and the thin-membrane models [25], with two parameters \u03b1 and \u03b2: J = \u03b1I + (\u03b2)Jtp + (1 \u2212 \u03b2)Jtm and", "startOffset": 80, "endOffset": 84}], "year": 2015, "abstractText": "Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids, metric learning and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables, i.e., the matrix dimension, which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Schur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.", "creator": "LaTeX with hyperref package"}}}