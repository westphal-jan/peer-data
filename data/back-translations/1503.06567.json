{"id": "1503.06567", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "On some provably correct cases of variational inference for topic models", "abstract": "Variational inference is a very efficient and popular heuristics used in various forms in the context of latent variable models. It is closely related to expectation maximization (EM) and is applied when exact EM cannot be computationally performed. Although it is enormously popular, the current theoretical understanding of the effectiveness of variaitonal inference-based algorithms is very limited. In this paper, we provide the first analysis of cases where variational inference algorithms converge to the global optimum when setting topic models.", "histories": [["v1", "Mon, 23 Mar 2015 09:20:39 GMT  (48kb)", "https://arxiv.org/abs/1503.06567v1", "47 pages"], ["v2", "Sat, 22 Aug 2015 11:24:43 GMT  (49kb)", "http://arxiv.org/abs/1503.06567v2", "46 pages, Compared to previous version: clarified notation, a number of typos fixed throughout paper"]], "COMMENTS": "47 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["pranjal awasthi", "andrej risteski"], "accepted": true, "id": "1503.06567"}, "pdf": {"name": "1503.06567.pdf", "metadata": {"source": "CRF", "title": "On some provably correct cases of variational inference for topic models", "authors": ["Pranjal Awasthi", "Andrej Risteski"], "emails": ["pawashti@cs.princeton.edu.", "risteski@cs.princeton.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n06 56\n7v 2\n[ cs\n.L G\n] 2\n2 A\nMore specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012b). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003).\nIt is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient.\nWhile our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest. Our proofs rely on viewing the updates as an operation which, at each timestep, sets the new parameter estimates to be noisy convex combinations of the ground truth values, and a bounded error term which depends on the previous estimate. The weight on the ground truth values will be large, compared to the error term, which will cause the error term to eventually reach zero. The large weight on the ground truth values will be a byproduct of our model assumptions, which will imply a \u201clocal\u201d notion of anchor words for each document - words which only appear in one topic in a given document, as well as a \u201clocal\u201d notion of anchor documents for each word - documents where that word appears as part of a single topic.\n\u2217Princeton University, Computer Science Department. Email: pawashti@cs.princeton.edu. Supported by NSF grant CCF1302518. \u2020Princeton University, Computer Science Department. Email: risteski@cs.princeton.edu. Partially supported by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora\u2019s Simons Investigator Award, and a Simons Collaboration Grant.\nContents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Latent variable models and EM 3", "text": ""}, {"heading": "3 Topic models 4", "text": ""}, {"heading": "4 Variational relaxation for learning topic models 5", "text": "4.1 Simplified updates in the long document limit . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.2 Alternating KL minimization and thresholded updates . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "5 Initializations 7", "text": ""}, {"heading": "6 Case study 1: Sparse topic priors, support initialization 8", "text": ""}, {"heading": "7 Case study 2: Dominating topics, seeded initialization 9", "text": ""}, {"heading": "8 On common words 11", "text": ""}, {"heading": "9 Discussion and open problems 11", "text": ""}, {"heading": "A Notation throughout supplementary material 12", "text": ""}, {"heading": "B Case study 1: Sparse topic priors, support initialization 12", "text": "B.1 Provable convergence of tEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 B.1.1 Determining largest topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 B.1.2 Lower bounds on the \u03b3td,i and \u03b2 t i,j variables . . . . . . . . . . . . . . . . . . . . . . . . 13\nB.1.3 Upper bound on the \u03b2ti,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.1.4 Upper bounds on the \u03b3 values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 B.1.5 Phase II: Alternating minimization - upper and lower bound evolution . . . . . . . . . 16\nB.2 Iterative tEM updates, incomplete tEM updates . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB.3.1 Constructing a no-false-positives test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3.2 Finding the topic supports from identifying pairs . . . . . . . . . . . . . . . . . . . . . 21 B.3.3 Finding the identifying pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.3.4 Finding the document supports . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"}, {"heading": "C Case study 2: Dominating topics, seeded initialization 24", "text": "C.1 Estimates on the dominating topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.2 Phase I: Determining the anchor words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nC.2.1 Lower bounds on the \u03b2ti,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2.2 Decreasing \u03b2ti\u2032,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nC.3 Discriminative words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.3.1 Bounds on the \u03b2ti,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.3.2 Decreasing \u03b2ti\u2032,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 C.4 Determining dominant topic and parameter range . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.5 Getting the supports correct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.6 Alternating minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33"}, {"heading": "D Justification of prior assumptions 33", "text": "D.1 Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 D.2 Weak topic correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 D.3 Dominant topic equidistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 D.4 Independent topic inclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37"}, {"heading": "E On common words 37", "text": "E.1 Phase I with common words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 E.2 Phase II of analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 E.3 Generalizing Case Study 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"}, {"heading": "F Estimates on number of documents 43", "text": ""}, {"heading": "1 Introduction", "text": "Over the last few years, heuristics for non-convex optimization have emerged as one of the most fascinating phenomena for theoretical study in machine learning. Methods like alternating minimization, EM, variational inference and the like enjoy immense popularity among ML practitioners, and with good reason: they\u2019re vastly more efficient than alternate available methods like convex relaxations, and are usually easily modified to suite different applications.\nTheoretical understanding however is sparse and we know of very few instances where these methods come with formal guarantees. Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007). The recent work of (Balakrishnan et al., 2014) also characterizes global convergence properties of the EM algorithm for more general settings. Another line of recent work has focused on a different heuristic called alternating minimization in the context of dictionary learning. (Agarwal et al., 2013), (Arora et al., 2015) prove that with appropriate initialization, alternating minimization can provably recover the ground truth. (Netrapalli et al., 2013) have proven similar results in the context of phase retreival.\nAnother popular heuristic which has so far eluded such attempts is known as variational inference (Jordan et al., 1999). We provide the first characterization of global convergence of variational inference based algorithms for topic models (Blei et al., 2003). We show that under natural assumptions on the topicword matrix and the topic priors, along with natural initialization, variational inference converges to the parameters of the underlying ground truth model. To prove our result we need to overcome a number of technical hurdles which are unique to the nature of variational inference. Firstly, the difficulty in analyzing alternating minimization methods for dictionary learning is alleviated by the fact that one can come up with closed form expressions for the updates of the dictionary matrix. We do not have this luxury. Second, the \u201cnorm\u201d in which variational inference naturally operates is KL divergence, which can be difficult to work with. We stress that the focus of this work is not to identify new instances of topic modeling that were previously not known to be efficiently solvable, but rather providing understanding about the behaviour of variational inference, the defacto method for learning and inference in the context of topic models."}, {"heading": "2 Latent variable models and EM", "text": "We briefly review expectation-maximization (EM) and variational methods. We will be dealing with latent variable models, where the observations Xi are generated according to a distribution\nP (Xi|\u03b8) = P (Zi|\u03b8)P (Xi|Zi, \u03b8)\nwhere \u03b8 are parameters of the models, and Zi are termed as hidden variables. Given the observations Xi, a common task in this context is to find the maximum likelihood value of the parameter \u03b8:\nargmax\u03b8 \u2211\ni\nlog(P (Xi|\u03b8))\nThe expectation-maximization (EM) algorithm is an iterative method to achieve this, dating all the way back to (Dempster et al., 1977) and (Sundberg, 1974) in the 70s. In the above framework it can be formulated as the following procedure, maintaining estimates \u03b8t, P\u0303 t(Z) of the model parameters and the posterior distribution over the hidden variables:\n\u2022 E-step: Compute the distribution P\u0303 t(Z) = P (Z|X, \u03b8t\u22121)\n\u2022 M-step: Set \u03b8t to be argmax\u03b8 \u2211\ni\nEP\u0303 t\u22121 [logP (Xi, Zi|\u03b8)]\nIt\u2019s implicitly assumed however, that both steps can be performed efficiently. Sadly, that is not the case in many scenarios. A common approach then is to relax the above formulation to a tractable form. This is achieved by choosing an appropriate family of distributions F , and perform the following updates:\n\u2022 Variational E-step: Compute the distribution P\u0303 t(Z) = minP t\u2208FKL(P t(Z)||P (Z|X, \u03b8t\u22121)\n\u2022 Variational M-step: Set \u03b8t to be argmax\u03b8 \u2211 iEP\u0303 t\u22121 [logP (Xi, Zi|\u03b8)]\nBy picking the family F appropriately, it\u2019s often possible to make both steps above run in polynomial time. None of the above two families of approximations, however, usually come with any guarantees. With EM, the problem is ensuring that one does not get stuck in a local optimum. With variational EM, additionally, we are faced with the issue of in principle not even exploring the entire space of solutions."}, {"heading": "3 Topic models", "text": "We will focus on a particular latent variable model, which is very often studied - topic models (Blei et al., 2003). The generative model here is as follows: there is a prior distribution over topics \u03b1. Then, each document is generated by the following process:\n\u2022 Sample a proportion of topics \u03b31, \u03b32, . . . , \u03b3k according to \u03b1.\n\u2022 For each position in the document, pick a topic according to a multinomial distribution with parameters \u03b31, . . . , \u03b3k.\n\u2022 Conditioned on topic i being picked at that position, pick a word j from a multinomial with parameters (\u03b2i,1, \u03b2i,2, . . . , \u03b2i,k)\nIn this paper we will be interested in topic priors which result in sparse documents and where the correlation of the distributions for different topics is small. These types of properties are very commonly assumed, and are satisfied by the Dirichlet prior, one of the most popular priors in topic modeling. (Originally introduced by (Blei et al., 2003).)\nThe body of work on topic models is vast (Blei and Lafferty, 2009). Prior theoretical work relevant in the context of this paper includes the sequence of works by (Arora et al., 2012b),(Arora et al., 2013), as well as (Anandkumar et al., 2013), (Ding et al., 2013), (Ding et al., 2014) and (Bansal et al., 2014). (Arora et al., 2012b) and (Arora et al., 2013) assume that the topic-word matrix contains \u201canchor words\u201d. This means that each topic has a word which appears in that topic, and no other. (Anandkumar et al., 2013) on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic. Neither paper needs any assumption on the topic priors, and can handle (almost) arbitrarily short documents.\nThe assumptions we make on the word-topic matrix will be related to the ones in the above works, but our documents will need to be long, so that the empirical counts of the words are close to their expected counts. Our priors will also be more structured. This is expected since we are trying to analyze an existing heuristic rather than develop a new algorithmic strategy. The case where the documents are short seems significantly more difficult. Namely, in that case there are two issues to consider. One is proving the variational approximation to the posterior distribution over topics is not too bad. The second is proving that the updates do actually reach the global optimum. Assuming long documents allows us to focus on the second issue alone, which is already challenging. On a high level, the instances we consider will have the following structure:\n\u2022 The topics will satisfy a weighted expansion property: for any set S of topics of constant size, for any topic i in this set, the probability mass on words which belong to i, and no other topic in S will be large. (Similar to the expansion in (Anandkumar et al., 2013), but only over constant sized subsets.)\n\u2022 The number of topics per document will be small. Further, the probability of including a given topic in a document is almost independent of any other topics that might be included in the document already. Similar properties are satisfied by the Dirichlet prior, one of the most popular priors in topic modeling. (Originally introduced by (Blei et al., 2003).) The documents will also have a \u201cdominating topic\u201d, similarly as in (Bansal et al., 2014).\n\u2022 For each word j, and a topic i it appears in, there will be a decent proportion of documents that contain topic i and no other topic containing j. These can be viewed as \u201clocal anchor documents\u201d for that word-pair topic.\nWe state below, informally, our main result. See Sections 6 and 7 for more details.\nTheorem. Under the above mentioned assumptions, popular variants of variational inference for topic models, with suitable initializations, provably recover the ground truth model in polynomial time."}, {"heading": "4 Variational relaxation for learning topic models", "text": "In this section we briefly review the variational relaxation for topic models following closely the description in (Blei et al., 2003). Throughout the paper, we will denote by N the total number of words and K the number of topics. We will assume that we are working with a sample set of D documents. We will also denote by f\u0303d,j the fractional count of word j in document d (i.e. f\u0303d,j = Count(j)/Nd, where Count(j) is the number of times word j appears in the document, and Nd is the number of words in the document).\nFor topic models variational updates are way to approximate the computationally intractable E-step (Sontag and Roy, 2000) as described in Section 2. Recall the model parameters for topic models are the topic prior parameters \u03b1 and the topic-word matrix \u03b2. The observableX is the list of words in the document. The latent variables are the topic assignments Zj at each position j in the document and the topic proportions \u03b3. The variational E-step hence becomes P\u0303 t(Z, \u03b3) = minP t\u2208FKL(P t(Z, \u03b3)||P (Z, \u03b3|X,\u03b1t, \u03b2t) for some family F of distributions. The family F one usually considered is P t(\u03b3, Z) = q(\u03b3)\u03a0Ndj=1q \u2032 j(Zj), i.e. a mean field family. In (Blei et al., 2003) it\u2019s shown that for Dirichlet priors \u03b1 the optimal distributions q, q\u2032j are a Dirichlet distribution for q, with some parameter \u03b3\u0303, and multinomials for q\u2032j , with some parameters \u03c6j . The variational EM updates are shown to have the following form.\n\u2022 In the E-step, one runs to convergence the following updates on the \u03c6 and \u03b3\u0303 parameters:\n\u03c6d,j,i \u221d \u03b2ti,wd,jeEq [log(\u03b3d)|\u03b3\u0303d]\n\u02dc\u03b3d,i = \u03b1 t d,i +\nNd \u2211\nj=1\n\u03c6d,j,i\n\u2022 In the M-step, one updates the \u03b2 and parameters as follows:\n\u03b2t+1i,j \u221d D \u2211\nd=1\nNd \u2211\nj\u2032=1\n\u03c6td,j,iwd,j,j\u2032\nwhere \u03c6td,j,i is the converged value of \u03c6d,j,i; wd,j is the word in document d, position j; wd,j,j\u2032 is an indicator variable which is 1 if the word in position j\u2032 in document d is word j.\nThe \u03b1 Dirichlet parameters do not have a closed form expression and are updated via gradient descent."}, {"heading": "4.1 Simplified updates in the long document limit", "text": "From the above updates it is difficult to give assign an intuitive meaning to the \u03b3\u0303 and \u03c6 parameters. (Indeed, it\u2019s not even clear what one would like them to be ideally at the global optimum.) We will be however working in the large document limit - and this will simplify the updates. In particular, in the E-step, in the large document limit, the first term in the update equation for \u03b3\u0303 has a vanishing contribution. In this case, we can simplify the E-update as:\n\u03c6d,j,i \u221d \u03b2ti,j\u03b3d,i\n\u03b3d,i \u221d Nd \u2211\nj=1\n\u03c6d,j,i\nNotice, importantly, in the second update we now use variables \u03b3d,i instead of \u03b3\u0303d,i, which are normalized such that K \u2211\ni=1\n\u03b3d,i = 1. These correspond to the max-likelihood topic proportions, given our current estimates \u03b2 t i,j\nfor the model parameters. The M-step will remain as is - but we will focus on the \u03b2 only, and ignore the \u03b1 updates - as the \u03b1 estimates disappeared from the E updates:\n\u03b2t+1i,j \u221d D \u2211\nd=1\nf\u0303d,j\u03b3 t d,i\nwhere \u03b3td,i is the converged value of \u03b3d,i. In this case, the intuitive meaning of the \u03b2 t and \u03b3t variables is clear: they are estimates of the the model parameters, and the max-likelihood topic proportions, given an estimate of the model parameters, respectively.\nThe way we derived them, these updates appear to be an approximate form of the variational updates in (Blei et al., 2003). However it is possible to also view them in a more principled manner. These updates approximate the posterior distribution P (Z, \u03b3|X,\u03b1t, \u03b2t) by first approximating this posterior by P (Z|X, \u03b3\u2217, \u03b1t, \u03b2t), where \u03b3\u2217 is the max-likelihood value for \u03b3, given our current estimates of \u03b1, \u03b2, and then setting P (Z|X, \u03b3\u2217, \u03b1t, \u03b2t) to be a product distribution.\nIt is intuitively clear that in the large document limit, this approximation should not be much worse than the one in (Blei et al., 2003), as the posterior concentrates around the maximum likelihood value. (And in fact, our proofs will work for finite, but long documents.) Finally, we will rewrite the above equations in\na slightly more convenient form. Denoting fd,j =\nK \u2211\ni=1\n\u03b3d,i\u03b2 t i,j , the E-step can be written as: iterate until\nconvergence\n\u03b3d,i = \u03b3d,i\nN \u2211\nj=1\nf\u0303d,j fd,j \u03b2ti,j\nThe M-step becomes:\n\u03b2t+1i,j = \u03b2 t i,j\n\u2211D d=1 f\u0303d,j ftd,j \u03b3td,i \u2211D\nd=1 \u03b3 t d,i\nwhere f td,j =\nK \u2211\ni=1\n\u03b3td,i\u03b2 t i,j and \u03b3 t d,i is the converged value of \u03b3d,i."}, {"heading": "4.2 Alternating KL minimization and thresholded updates", "text": "We will further modify the E and M-step update equations we derived above. In a slightly modified form, these updates were used in a paper by (Lee and Seung, 2000) in the context of non-negative matrix factorization. There the authors proved that under these updates \u2211D\nd=1 KL(f t d,j||f\u0303d,j) is non-decreasing. One can\neasily modify their arguments to show that the same property is preserved if the E-step is replaced by a step \u03b3td = min\u03b3td\u2208\u2206K KL(f\u0303d||fd), where \u2206K is the K-dimensional simplex - i.e. minimizing the KL divergence between the counts and the \u201dpredicted counts\u201d with respect to the \u03b3 variables. (In fact, iterating the \u03b3\nupdates above is a way to solve this convex minimization problem via a version of gradient descent which makes multiplicative updates, rather than additive updates.)\nThus the updates are performing alternating minimization using the KL divergence as the distance measure (with the difference that for the \u03b2 variables one essentially just performs a single gradient step). In this paper, we will make a modification of the M-step which is very natural. Intuitively, the update for \u03b2ti,j goes over all appearances of the word j and adds the \u201cfractional assignment\u201d of the word j to topic i under our current estimates of the variables \u03b2, \u03b3. In the modified version we will only average over those documents d, where \u03b3td,i > \u03b3 t d,i\u2032 , \u2200i\u2032 6= i.\nThe intuitive reason behind this modification is the following. The EM updates we are studying work with the KL divergence, which puts more weight on the larger entries. Thus, for the documents in Di, the estimates for \u03b3td,i should be better than they might be in the documents D \\Di. (Of course, since the terms f td,j involve all the variables \u03b3 t d,i, it is not a priori clear that this modification will gain us much, but we will prove that it in fact does.) Formally, we discuss the following three modifications of variational inference (we call them tEM, for thresholded EM):\nAlgorithm 1 KL-tEM\n\u2022 (E-step) Solve the following convex program for each document d:\nmin \u03b3t d,i\n\u2211\nj\nf\u0303d,j log( f\u0303d,j f td,j )\ns.t. (1): \u03b3td,i \u2265 0, \u2211 i \u03b3 t d,i = 1 and \u03b3 t d,i = 0 if i does not belong to document d (M-step) Let Di to be the set of documents d, s.t. \u03b3 t d,i > \u03b3 t d,i\u2032 , \u2200i\u2032 6= i.\nSet \u03b2t+1i,j = \u03b2 t i,j\n\u2211\nd\u2208Di\nf\u0303d,j ft d,j \u03b3td,i \u2211\nd\u2208Di \u03b3td,i\nAlgorithm 2 Iterative tEM\n\u2022 (E-step) Initialize \u03b3d,i uniformly among the topics in the support of document d. Repeat\n\u03b3d,i = \u03b3d,i\nN \u2211\nj=1\nf\u0303d,j fd,j \u03b2ti,j (4.1)\nuntil convergence. (M-step) Same as above.\nAlgorithm 3 Incomplete tEM\n\u2022 (E-step) Initialize \u03b3d,i with the values gotten in the previous iteration, just perform one step of 4.1. (M-step) Same as before."}, {"heading": "5 Initializations", "text": "We will consider two different strategies for initialization. First, we will consider the case where we initialize with the topic-word matrix, and the document priors having the correct support. The analysis of tEM in this case will be the cleanest. While the main focus of the paper is tEM, we\u2019ll show that this initialization can actually be done for our case efficiently.\nSecond, we will consider an initialization that is inspired by what the current LDA-c implementation uses. Concretely, we\u2019ll assume that the user has some way of finding, for each topic i, a seed document in which the proportion of topic i is at least Cl. Then, when initializing, one treats this document as if it were\npure: namely one sets \u03b20i,j to be the fractional count of word j in this document. We do not attempt to design an algorithm to find these documents."}, {"heading": "6 Case study 1: Sparse topic priors, support initialization", "text": "We start with a simple case. As mentioned, all of our results only hold in the long documents regime: we will assume for each document d, the number of sampled words is large enough, so that one can approximate the expected frequencies of the words, i.e., one can find values \u03b3\u2217d,i, such that f\u0303d,j = (1 \u00b1 \u01eb) \u2211K i=1 \u03b3 \u2217 d,i\u03b2 \u2217 i,j . We\u2019ll split the rest of the assumptions into those that apply to the topic-word matrix, and the topic priors. Let\u2019s first consider the assumptions on the topic-word matrix. We will impose conditions that ensure the topics don\u2019t overlap too much. Namely, we assume:\n\u2022 Words are discriminative: Each word appears in o(K) topics. \u2022 Almost disjoint supports : \u2200i, i\u2032, if the intersection of the supports of i and i\u2032 is S, \u2211j\u2208S \u03b2\u2217i,j \u2264 o(1) \u00b7 \u2211\nj \u03b2 \u2217 i,j .\nWe also need assumptions on the topic priors. The documents will be sparse, and all topics will be roughly equally likely to appear. There will be virtually no dependence between the topics: conditioning on the size or presence of a certain topic will not influence much the probability of another topic being included. These are analogues of distributions that have been analyzed for dictionary learning (Arora et al., 2015). Formally:\n\u2022 Sparse and gapped documents : Each of the documents in our samples has at most T = O(1) topics. Furthermore, for each document d, the largest topic i0 = argmaxi\u03b3 \u2217 d,i is such that for any other topic\ni\u2032, \u03b3\u2217d,i\u2032 \u2212 \u03b3\u2217d,i0 > \u03c1 for some (arbitrarily small) constant \u03c1. \u2022 Dominant topic equidistribution: The probability that topic i is such that \u03b3\u2217d,i > \u03b3\u2217d,i\u2032 , \u2200i\u2032 6= i is \u0398(1/K). \u2022 Weak topic correlations and independent topic inclusion: For all sets S with o(K) topics, it must be the case that: E[\u03b3\u2217d,i|\u03b3\u2217d,i is dominating] = (1 \u00b1 o(1))E[\u03b3\u2217d,i|\u03b3\u2217d,i is dominating, \u03b3\u2217d,i\u2032 = 0, i\u2032 \u2208 S]. Furthermore, for any set S of topics, s.t. |S| \u2264 T \u2212 1, Pr[\u03b3\u2217d,i > 0|\u03b3\u2217d,i\u2032\u2200i\u2032 \u2208 S] = \u0398( 1K )\nThese assumptions are a less smooth version of properties of the Dirichlet prior. Namely, it\u2019s a folklore result that Dirichlet draws are sparse with high probability, for a certain reasonable range of parameters. This was formally proven by (Telgarsky, 2013) - though sparsity there means a small number of large coordinates. It\u2019s also well known that Dirichlet essentially cannot enforce any correlation between different topics. 1\nThe above assumptions can be viewed as a local notion of separability of the model, in the following sense. First, consider a particular document d. For each topic i that participates in that document, consider the words j, which only appear in the support of topic i in the document. In some sense, these words are local anchor words for that document: these words appear only in one topic of that document. Because of the \u201dalmost disjoint supports\u201d property, there will be a decent mass on these words in each document. Similarly, consider a particular non-zero element \u03b2\u2217i,j of the topic-word matrix. Let\u2019s call Dl the set of documents where \u03b2\u2217i\u2032,j = 0 for all other topics i\n\u2032 6= i appearing in that document. These documents are like local anchor documents for that word-topic pair: in those documents, the word appears as part of only topic i. It turns out the above properties imply there is a decent number of these for any word-topic pair.\nFinally, a technical condition: we will also assume that all nonzero \u03b3\u2217d,i, \u03b2 \u2217 i,j are at least 1 poly(N) . Intuitively, this means if a topic is present, it needs to be reasonably large, and similarly for words in topics. Such assumptions also appear in the context of dictionary learning (Arora et al., 2015).\nWe will prove the following\nTheorem 1. Given an instance of topic modelling satisfying the properties specified above, where the number of documents is \u2126(K log 2 N\n\u01eb2 ), if we initialize the supports of the \u03b2 t i,j and \u03b3 t d,i variables correctly, after\nO (log(1/\u01eb\u2032) + logN) KL-tEM, iterative-tEM updates or incomplete-tEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + \u01eb\u2032, for any \u01eb\u2032 s.t. 1 + \u01eb\u2032 \u2264 1(1\u2212\u01eb)7 .\n1We show analogues of the weak topic correlations property and equidistribution in the supplementary material for com-\npleteness sake.\nTheorem 2. If the number of documents is \u2126(K4 log2 K), there is a polynomial-time procedure which with probability 1\u2212 \u2126( 1K ) correctly identifies the supports of the \u03b2\u2217i,j and \u03b3\u2217d,i variables.\nProvable convergence of tEM: The correctness of the tEM updates is proven in 3 steps:\n\u2022 Identifying dominating topic: First, we prove that if \u03b3td,i is the largest one among all topics in the document, topic i is actually the largest topic.\n\u2022 Phase I: Getting constant multiplicative factor estimates : After initialization, after O(logN) rounds, we will get to variables \u03b2ti,j , \u03b3 t d,i which are within a constant multiplicative factor from \u03b2 \u2217 i,j , \u03b3 \u2217 d,i.\n\u2022 Phase II (Alternating minimization - lower and upper bound evolution): Once the \u03b2 and \u03b3 estimates are within a constant factor of their true values, we show that the lone words and documents have a boosting effect: they cause the multiplicative upper and lower bounds to improve at each round.\nThe updates we are studying are multiplicative, not additive in nature, and the objective they are optimizing is non-convex, so the standard techniques do not work. The intuition behind our proof in Phase II can be described as follows. Consider one update for one of the variables, say \u03b2ti,j . We show that \u03b2t+1i,j \u2248 \u03b1\u03b2\u2217i,j + (1 \u2212 \u03b1)Ct\u03b2\u2217i,j for some constant Ct at time step t. \u03b1 is something fairly large (one should think of it as 1 \u2212 o(1)), and comes from the existence of the local anchor documents. A similar equation holds for the \u03b3 variables, in which case the \u201cgood\u201d term comes from the local anchor words. Furthermore, we show that the error in the \u2248 decreases over time, as does the value of Ct, so that eventually we can reach \u03b2\u2217i,j . The analysis bears a resemblance to the state evolution and density evolution methods in error decoding algorithm analysis - in the sense that we maintain a quantity about the evolving system, and analyze how it evolves under the specified iterations. The quantities we maintain are quite simple - upper and lower multiplicative bounds on our estimates at any round t.\nInitialization: Recall the goal of this phase is to recover the supports - i.e. to find out which topics are present in a document, and identify the support of each topic. We will find the topic supports first. This uses an idea inspired by (Arora et al., 2014) in the setting of dictionary learning. Roughly, we devise a test, which will take as input two documents d, d\u2032, and will try to determine if the two documents have a topic in common or not. The test will have no false positives, i.e., will never say YES, if the documents don\u2019t have a topic in common, but might say NO even if they do. We then ensure that with high probability, for each topic we find a pair of documents intersecting in that topic, such that the test says YES. 2"}, {"heading": "7 Case study 2: Dominating topics, seeded initialization", "text": "Next, we\u2019ll consider an initialization which is essentially what the current implementation of LDA-c uses. Namely, we will call the following initialization a seeded initialization:\n\u2022 For each topic i, the user supplies a document d, in which \u03b3\u2217d,i \u2265 Cl.\n\u2022 We treat the document as if it only contains topic i and initialize with \u03b20i,j = f\u2217d,j.\nWe show how to modify the previous analysis to show that with a few more assumptions, this strategy works as well. Firstly, we will have to assume anchor words, that make up a decent fraction of the mass of each topic. Second, we also assume that the words have a bounded dynamic range, i.e. the values of a word in two different topics are within a constant B from each other. The documents are still gapped, but the gap now must be larger. Finally, in roughly 1/B fraction of the documents where topic i is dominant, that topic has proportion 1 \u2212 \u03b4, for some small (but still constant) \u03b4. A similar assumption (a small fraction of almost pure documents) appeared in a recent paper by (Bansal et al., 2014). Formally, we have:\n\u2022 Small dynamic range and large fraction of anchors : For each discriminative words, if \u03b2\u2217i,j 6= 0 and \u03b2\u2217i\u2032,j 6= 0, \u03b2\u2217i,j \u2264 B\u03b2\u2217i\u2032,j . Furthermore, each topic i has anchor words, such that their total weight is at least p.\n2The detailed initialization algorithm is included in the supplementary material.\n\u2022 Gapped documents : In each document, the largest topic has proportion at least Cl, and all the other topics are at most Cs, s.t.\nCl \u2212 Cs \u2265 1\np\n( \u221a\n2\n(\np log( 1\nCl ) + (1\u2212 p) log(BCl)\n)\n+ \u221a log(1 + \u01eb)\n)\n+ \u01eb\n\u2022 Small fraction of 1 \u2212 \u03b4 dominant documents : Among all the documents where topic i is dominating, in a 8/B fraction of them, \u03b3\u2217d,i \u2265 1\u2212 \u03b4, where\n\u03b4 := min\n(\nC2l 2B3 \u2212 1 p\n( \u221a\n2\n(\np log( 1\nCl ) + (1\u2212 p) log(BCl)\n)\n+ \u221a log(1 + \u01eb)\n)\n\u2212 \u01eb, 1\u2212 \u221a\nCl\n)\nThe dependency between the parameters B, p, Cl is a little difficult to parse, but if one thinks of Cl as 1\u2212 \u03b7 for \u03b7 small, and p \u2265 1\u2212 \u03b7logB , since log( 1Cl ) \u2248 1 + \u03b7, roughly we want that Cl \u2212Cs \u226b 2 p \u221a \u03b7. (In other words, the weight we require to have on the anchors depends only logarithmically on the range B.) In the documents where the dominant topic has proportion 1\u2212 \u03b4, a similar reasoning as above gives that we want is approximately \u03b3\u2217d,i \u2265 1\u2212\n1\u2212 2\u03b7 2B3 + 2 p \u221a \u03b7. The precise statement is as follows:\nTheorem 3. Given an instance of topic modelling satisfying the properties specified above, where the number of documents is \u2126(K log 2 N\n\u01eb2 ), if we initialize with seeded initialization, after O (log(1/\u01eb \u2032) + logN) of KL-\ntEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + \u01eb\u2032, if 1 + \u01eb\u2032 \u2265 1(1\u2212\u01eb)7 .\nThe proof is carried out in a few phases:\n\u2022 Phase I: Anchor identification: We show that as long as we can identify the dominating topic in each of the documents, anchor words will make progress: after O(logN) number of rounds, the values for the topic-word estimates will be almost zero for the topics for which word w is not an anchor. For topic for which a word is an anchor we\u2019ll have a good estimate.\n\u2022 Phase II: Discriminative word identification: After the anchor words are properly identified in the previous phase, if \u03b2\u2217i,j = 0, \u03b2 t i,j will keep dropping and quickly reach almost zero. The values corresponding\nto \u03b2\u2217i,j 6= 0 will be decently estimated.\n\u2022 Phase III: Alternating minimization: After Phase I and II above, we are back to the scenario of the previous section: namely, there is improvement in each next round.\nDuring Phase I and II the intuition is the following: due to our initialization, even in the beginning, each topic is \u201dcorrelated\u201d with the correct values. In a \u03b3 update, we are minimizing KL(f\u0303d||fd) with respect to the \u03b3d variables, so we need a way to argue that whenever the \u03b2 estimates are not too bad, minimizing this quantity provides an estimate about how far the optimal \u03b3d variables are from \u03b3 \u2217 d . We show the following useful claim:\nLemma 4. If, for all topics i, KL(\u03b2\u2217i ||\u03b2ti ) \u2264 R\u03b2, and min\u03b3d\u2208\u2206KKL(f\u0303d,j||fd,j) \u2264 Rf , after running a KL divergence minimization step with respect to the \u03b3d variables, we get that ||\u03b3\u2217d\u2212\u03b3d||1 \u2264 1p ( \u221a 1 2R\u03b2+ 1 2 \u221a Rf )+\u01eb.\nThis lemma critically uses the existence of anchor words - namely we show ||\u03b2\u2217v||1 \u2265 p||v||1. Intuitively, if one thinks of v as \u03b3\u2217 \u2212 \u03b3t, ||\u03b2\u2217v||1 will be large if ||v||1 is large. Hence, if ||\u03b2\u2217 \u2212 \u03b2t||1 is not too large, whenever ||f\u2217 \u2212 f t||1 is small, so is ||\u03b3\u2217 \u2212 \u03b3t||1. We will be able to maintain R\u03b2 and Rf small enough throughout the iterations, so that we can identify the largest topic in each of the documents."}, {"heading": "8 On common words", "text": "We briefly remark on common words : words such that \u03b2\u2217i,j \u2264 \u03ba\u03b2\u2217i\u2032,j, \u2200i, i\u2032, \u03ba \u2264 B. In this case, the proofs above, as they are, will not work, 3 since common words do not have any lone documents. However, if 1 \u2212 1\u03ba100 fraction of the documents where topic i is dominant contains topic i with proportion 1\u2212 1\u03ba100 and furthermore, in each topic, the weight on these words is no more than 1\u03ba100 , then our proofs still work with either initialization4 The idea for the argument is simple: when the dominating topic is very large, we show that f\u2217d,j ftd,j is very highly correlated with \u03b2\u2217i,j \u03b2ti,j , so these documents behave like anchor documents. Namely, one can show:\nTheorem 5. If we additionally have common words satisfying the properties specified above, after O(log(1/\u01eb\u2032)+ logN) KL-tEM updates in Case Study 2, or any of the tEM variants in Case Study 1, and we use the same initializations as before, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + \u01eb\u2032, if 1 + \u01eb\u2032 \u2265 1(1\u2212\u01eb)7 ."}, {"heading": "9 Discussion and open problems", "text": "In this work we provide the first characterization of sufficient conditions when variational inference leads to optimal parameter estimates for topic models. Our proofs also suggest possible hard cases for variational inference, namely instances with large dynamic range compared to the proportion of anchor words and/or correlated topic priors. It\u2019s not hard to hand-craft such instances where support initialization performs very badly, even with only anchor and common words. We made no effort to explore the optimal relationship between the dynamic range and the proportion of anchor words, as it\u2019s not clear what are the \u201cworst case\u201d instances for this trade-off.\nSeeded initialization, on the other hand, empirically works much better. We found that when Cl \u2265 0.6, and when the proportion of anchor words is as low as 0.2, variational inference recovers the ground truth, even on instances with fairly large dynamic range. Our current proof methods are too weak to capture this observation. (In fact, even the largest topic is sometimes misidentified in the initial stages, so one cannot even run tEM, only the vanilla variational inference updates.) Analyzing the dynamics of variational inference in this regime seems like a challenging problem which would require significantly new ideas."}, {"heading": "Acknowledgements", "text": "We would like to thank Sanjeev Arora for helpful discussions in various stages of this work.\n3We stress we want to analyze whether variational inference will work or not. Handling common words algorithmically is easy: they can be detected and \u201dfiltered out\u201d initially. Then we can perform the variational inference updates over the rest of the words only. This is in fact often done in practice. 4See supplementary material.\nSupplementary material"}, {"heading": "A Notation throughout supplementary material", "text": "We will use \u2243,.,& to denote that the corresponding (in)equality holds up to constants. We will use \u21d4 to denote equivalence. We will say that an event happens with high probability, if it happens with probability 1\u2212 1Kc or 1\u2212 1Nc for some constant c."}, {"heading": "B Case study 1: Sparse topic priors, support initialization", "text": "B.1 Provable convergence of tEM\nAs a reminder, the theorem we want to prove is:\nTheorem 1. Given an instance of topic modelling satisfying the Case Study 1 properties specified above, where the number of documents is \u2126(K log 2 N\n\u01eb\u20322 ), if we initialize the supports of the \u03b2 t i,j and \u03b3 t d,i variables\ncorrectly, after O(log(1/\u01eb\u2032)+ logN) KL-tEM, iterative-tEM updates or incomplete-tEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1+ \u01eb\u2032, for any \u01eb\u2032 s.t. 1 + \u01eb\u2032 \u2264 1(1\u2212\u01eb)7 .\nThe general outline of the proof will be the following.\n\u2022 Identifying dominating topic: For the modified tEM updates, we need to make sure that the topic with maximal \u03b3td,i is the dominant.\n\u2022 Phase I: Getting constant multiplicative factor estimates : First, we\u2019ll show that after initialization, after O(logN) number of rounds, we will get to variables \u03b2ti,j , \u03b3 t d,i which are within a constant multiplicative\nfactor from \u03b2\u2217i,j , \u03b3 \u2217 d,i.\n\u2013 Lower bounds on the \u03b2 and \u03b3 variables : We\u2019ll show that determining the supports of the documents and the topic-word matrix, as well as being able to identify the documents in which topic i is large is enough to ensure that all the \u03b2ti,j and \u03b3 t i,j variables are lower bounded by\n1 C0\u03b2 \u03b2\u2217i,j and\n1 C0\u03b3 \u03b3\u2217d,i respectively for some constants C 0 \u03b2 \u2265 1, C0\u03b3 \u2265 1.\n\u2013 Improving upper bounds on the \u03b2ti,j values : We show that, if the above two properties are satisfied, we can get a multiplicative upper bound of the \u03b2ti,j values, which strictly improves at each step until it reaches a constant. This improvement is very fast: we only need a logarithmic number of steps. After this happens, we show that the \u03b3 variables corresponding to these \u03b2 estimates must be within a constant of the ground truth as well.\n\u2022 Phase II (Alternating minimization - lower and upper bound evolution): Once the \u03b2 and \u03b3 estimates are within a constant factor of their true values, we show that the lone words and documents have a boosting effect: they cause the multiplicative upper and lower bounds to improve at each round.\nA word about incorporating the \u201dcorrect supports\u201d assumption in our algorithms. For the \u03b2 variables this is obvious: we just set \u03b2ti,j = 0 if \u03b2 \u2217 i,j = 0. For the \u03b3 variables it\u2019s also fairly straightforward. In KL-tEM we mean simply that in the convex program above, we constrain \u03b3td,i = 0 if \u03b3 \u2217 d,i = 0.\nIn the iterative version, this just means that before starting the \u03b3 iterations, we set the initial value to 0 if \u03b3\u2217d,i = 0, and uniform among the rest of the variables. Same for the incomplete version.\nIn the interest of brevity, whenever we say \u201dthe supports are correct\u201d, the above is what we will mean. Recall, we use t to count the iterations for \u03b2 variables. Put another way, \u03b3td,i is the value we get for \u03b3d,i after the \u03b2 variables were updated to \u03b2ti,j . (Which of course, implies, \u03b2 t+1 i,j will be the values we get for the \u03b2 variables after the \u03b3 variables are updated to \u03b3td,i.) The proofs are for each of the variants of tEM are similar. For starters, we show everything for KL-tEM, and then just mention how to modify the arguments to get the results for the other variants in section B.2.\nB.1.1 Determining largest topic\nFirst, we show that the \u201dthresholding\u201d operation works. Namely, we show that if \u03b3td,i > \u03b3 t d,i\u2032 , \u2200i 6= i\u2032, then \u03b3\u2217d,i is the largest topic in the document (there is a unique one by the \u201dslightly gapped documents\u201d property). Furthermore, we can say that 12\u03b3 \u2217 d,i \u2264 \u03b3td,i \u2264 2\u03b3\u2217d,i.\nLemma 6. Fix a document d. Let the supports of the \u03b3 and \u03b2 variables be correct. Then, after a \u03b3 iteration, if \u03b3td,i > \u03b3 t d,i\u2032 , i 6= i\u2032, \u03b3\u2217d,i is the largest topic in the document. Furthermore, 12\u03b3\u2217d,i \u2264 \u03b3td,i \u2264 2\u03b3\u2217d,i.\nProof. Since there are a constant number of topics in the document, the largest topic has proportion \u2126(1). Consider the KL-tEM convex optimization problem. The KKT conditions are easily seen to imply5:\nN \u2211\nj=1\nf\u0303d,j f td,j \u03b2ti,j = 1 (B.1)\nFor each topic i, since we are considering a constrained optimization problem, it has to be the case that it either satisfies B.1, \u03b3td,i = 0 or \u03b3 t d,i = 1.\nLet\u2019s assume first that i satisfies B.1. Then,\n\u03b3td,i =\nN \u2211\nj=1\nf\u0303d,j f td,j \u03b2ti,j\u03b3 t d,i \u2264 \u2211\nj:\u03b2\u2217i,j 6=0\nf\u0303d,j\nLet\u2019s call the words j, which only appear in the support of topic i in the document lone for that topic, and let\u2019s denote that set as Li.\nIf Li are the lone words for topic i, \u2211\nj /\u2208Li,\u03b2\u2217i,j 6=0 f\u0303d,j = To(1) = o(1), so\n\u03b3td,i \u2264 \u2211\nj\u2208Li\n(1 + \u01eb)\u03b2\u2217i,j\u03b3 \u2217 d,i + o(1) \u2264 (1 + \u01eb)\u03b3\u2217d,i + o(1) \u2264 \u03b3\u2217d,i + o(1)\nOn the other hand, \u03b3td,i \u2265 \u2211 j\u2208Li \u03b2\u2217i,j\u03b3 \u2217 d,i \u2265 (1\u2212 \u01eb)(1\u2212 o(1))\u03b3\u2217d,i \u2265 (1\u2212 o(1))\u03b3\u2217d,i, so \u03b3td,i \u2265 \u03b3\u2217d,i \u2212 o(1).\nSince there is a constant gap of \u03c1 between the largest topic and the next largest one, the maximum \u03b3td,i is indeed the largest topic in the document. Furthermore, since (1\u2212 o(1))\u03b3\u2217d,i \u2264 \u03b3td,i \u2264 (1 + o(1))\u03b3\u2217d,i, clearly 1 2\u03b3\n\u2217 d,i \u2264 \u03b3td,i \u2264 2\u03b3\u2217d,i follows as well. On the other hand, we claim no topic which is in the support of a document d can actually have \u03b3td,i = 0.\nIf this happens, it\u2019s easy to see that \u2211N j=1 f\u0303d,j log( f\u0303d,j ft d,j ) = \u221e: one only needs to look at a summand corresponding to a lone word j for topic i. Just by virtue of the way lone words are defined, \u03b3td,i = 0 would imply f td,j = 0. It\u2019s clear that one can get a finite value for \u2211\nj f\u0303d,j log( f\u0303d,j ftd,j ) on the other hand, by just\nsetting \u03b3td,i = \u03b3 \u2217 d,i, so \u03b3 t d,i = 0 cannot happen at an optimum.\nB.1.2 Lower bounds on the \u03b3td,i and \u03b2 t i,j variables\nNext, we show that subject to the thresholding being correct, at any point in time t, all the estimates \u03b3td,i and \u03b2ti,j are appropriately lower bounded.\nThe proof is similar for both the \u03b2 and \u03b3 variables, and both for the KL-tEM and iterative tEM updates, but as mentioned before, we focus on the KL-tEM first.\nLemma 7. Fix a particular document d. Suppose that the supports of the \u03b3 and \u03b2 variables are correct. Then, \u03b3td,i \u2265 (1 \u2212 o(1))\u03b3\u2217d,i.\n5One gets these trivially, turning the constraint that \u2211K\ni=1 \u03b3 t d,i = 1 into a Lagrange multiplier\nProof. Multiplying both sides of B.1 by \u03b3td,i, we get\n\u03b3td,i =\nN \u2211\nj=1\nf\u0303d,j f td,j \u03b2ti,j\u03b3 t d,i\nAs above, let\u2019s split the above sum in two parts: lone words, and non-lone. Then clearly,\n\u03b3td,i \u2265 \u2211\nj\u2208Li\n(1\u2212 \u01eb)\u03b2\u2217i,j\u03b3\u2217d,i\nFor notational convenience, let\u2019s denote \u03b1\u0303 = \u2211 j\u2208Li \u03b2\u2217i,j . Let\u2019s estimate \u03b1\u0303. By the assumption on the size\nof the intersection of topics, \u2211\nj /\u2208Li\n\u03b2\u2217i,j \u2264 Tr = o(1)\n. Hence, \u03b1\u0303 \u2265 (1 \u2212 \u01eb)(1\u2212 o(1)) = 1\u2212 o(1). So, the claim of the lemma holds.\nThe lower bound on the \u03b2ti,j values proceeds similarly, but here we will crucially make use of the fact that for the large topics, we have both upper and lower bounds on the \u03b3td,i values.\nLemma 8. Suppose that the supports of the \u03b3 and \u03b2 variables are correct. Additionally, if i is a large topic in d, let 12\u03b3 \u2217 d,i \u2264 \u03b3td,i \u2264 2\u03b3\u2217d,i. Then, \u03b2t+1i,j \u2265 12 (1 \u2212 o(1))\u03b2\u2217i,j.\nProof. Let\u2019s call lone the documents where \u03b2\u2217i\u2032,j = 0 for all other topics i \u2032 6= i appearing in that document for the topic-word pair (i, j). Let Dl be the set of lone documents. Then, certainly it\u2019s true that\n\u03b2t+1i,j \u2265 \u03b2ti,j\n\u2211\nd\u2208Dl f\u0303d,j ftd,j \u03b3td,i \u2211D\nd=1 \u03b3 t d,i\nHowever, for a lone document, f td,j = \u03b3 t d,i \u00b7 \u03b2ti,j (it\u2019s easy to check all the other terms in the summation for f td,j vanish, because either \u03b3 t d,i\u2032 = 0 or \u03b2 t i\u2032,j = 0). Hence,\n\u03b2t+1i,j \u2265 \u2211 d\u2208Dl (1\u2212 \u01eb) \u03b3\n\u2217 d,i\u03b2 \u2217 i,j\n\u03b3td,i\u00b7\u03b2 t i,j \u03b2ti,j\u03b3 t d,i\n\u2211D d=1 \u03b3 t d,i\n= (1 \u2212 \u01eb)\u03b2\u2217i,j \u2211 d\u2208Dl \u03b3\u2217d,i\n\u2211D d=1 \u03b3 t d,i\nHowever, since the update is happening only over documents where topic i is large, \u03b3td,i \u2264 2\u03b3\u2217d,i. So, we can conclude\n\u03b2t+1i,j \u2265 (1\u2212 \u01eb)\u03b2\u2217i,j 1\n2\n\u2211\nd\u2208Dl \u03b3\u2217d,i\n\u2211D d=1 \u03b3 \u2217 d,i\nLet\u2019s call \u03b1 = \u2211 d\u2208Dl \u03b3\u2217d,i \u2211\nD d=1 \u03b3 \u2217 d,i\n, and let\u2019s analyze it\u2019s value.\nBy Lemma 51 and Lemma 50,\n\u2211\nd\u2208Dl\n\u03b3\u2217d,i \u2265 (1\u2212 \u01eb)|Dl|E[\u03b3\u2217d,i|\u03b3\u2217d,i is dominating, \u03b3\u2217d,i\u2032 = 0, \u2200i\u2032 6= i s.t. j appears in topic i\u2032]\nD \u2211\nd=1\n\u03b3\u2217d,i \u2264 (1 + \u01eb)|D|E[\u03b3\u2217d,i|\u03b3\u2217d,i is dominating]\nBy the weak topic correlations assumption, then,\n\u2211\nd\u2208Dl \u03b3\u2217d,i\n\u2211D d=1 \u03b3 \u2217 d,i\n\u2265 (1 \u2212 o(1)) |Dl||D| .\nFurthermore, by the independent topic inclusion property, each of the o(K) topics other than i that word j belongs to appears in a document with probability \u0398(1/K), so the probability that a document which\ncontains topic i contains one of them is o(1), i.e. |Dl||D| . By Lemma 52, furthermore, |Dl| |D| \u2265 1 \u2212 o(1) when \u01eb = o(1). Hence, \u03b1 \u2265 1\u2212 o(1). Altogether, we get that \u03b2t+1i,j \u2265 12 (1 \u2212 o(1))\u03b2\u2217i,j as claimed.\nB.1.3 Upper bound on the \u03b2ti,j values\nHaving established a lower bound on the \u03b2ti,j variables throughout all iterations, together with the lower bounds on the \u03b3td,i variables and the good estimates for the large topics, we will be able to prove the upper bound of the multiplicative error of \u03b2ti,j keeps improving, until \u03b2 t i,j \u2264 C\u03b2\u03b2\u2217i,j , for some constant C\u03b2 .\nLemma 9. Let the \u03b2 variables have the correct support, and \u03b2ti,j \u2265 1Cm\u03b2 \u2217 i,j, \u03b3 t d,i \u2265 1Cm \u03b3 \u2217 d,i whenever \u03b2 \u2217 i,j 6= 0, \u03b3\u2217d,i 6= 0. Let \u03b2ti,j = Ct\u03b2\u03b2\u2217i,j , where Ct\u03b2 \u2265 4Cm, and Cm is a constant. Then, in the next iteration, \u03b2t+1i,j \u2264 Ct+1\u03b2 \u03b2 \u2217 i,j, where C t+1 \u03b2 \u2264 Ct\u03b2 2 .\nProof. Without loss of generality, let\u2019s assume Cm \u2265 2. (Since certainly, if the statement of the lemma holds with a smaller constant, it holds with Cm = 2.)\nWe proceed similarly as in the prior analyses. We will split the sum into the portion corresponding to the lone and non-lone documents.\nLet\u2019s analyze the terms f\u0303d,j ftd,j \u03b3td,i corresponding to the non-lone documents. Now, f td,j \u2265 1C2m f \u2217 d,j, so f\u0303d,j ftd,j \u2264 (1 + \u01eb)C2m. Also, \u03b3td,i \u2264 2\u03b3\u2217d,i, since topic i is the dominant in document d.\nSince Cm \u2265 2, f\u0303d,jft d,j \u03b3td,i \u2264 (1 + \u01eb)C3m\u03b3\u2217d,i. Also, note that\n\u2211D d=1 \u03b3 t d,i \u2265 1Cm \u2211D d=1 \u03b3 \u2217 d,i, again, since i is the dominant topic.\nAs usual, let\u2019s denote the set of lone documents Dl:\n\u03b2t+1i,j \u2264 (1 + \u01eb)Cm \u2211 d\u2208Dl \u03b2\u2217i,j\u03b3 \u2217 d,i + \u2211 d\u2208D\\Dl C3m\u03b3 \u2217 d,i\u03b2 t i,j\n\u2211D d=1 \u03b3 \u2217 d,i\nAs in the prior proofs, let\u2019s denote by \u03b1 := \u2211 d\u2208Dl \u03b3\u2217d,i\n\u2211D d=1 \u03b3 \u2217 d,i\n.\nAs in Lemma 8, \u03b1 \u2265 1 \u2212 o(1), so \u03b2t+1i,j \u2264 (1 + \u01eb)Cm(\u03b1\u03b2\u2217i,j + (1 \u2212 \u03b1)C3m\u03b2ti,j), which in turn implies that \u03b2t+1i,j \u03b2\u2217i,j \u2264 (1+ \u01eb)Cm(\u03b1+(1\u2212\u03b1)C3mCt\u03b2). In order to ensure that \u03b2t+1i,j \u03b2\u2217i,j < Ct\u03b2 2 , it would be sufficient to prove that\n(1 + \u01eb)Cm(\u03b1+ (1\u2212 \u03b1)(C3mCt\u03b2) < Ct\u03b2 2\nwhich is equivalent to \u03b1 > C3mC t \u03b2 \u2212\nCt\u03b2 2(1+\u01eb)Cm\nC3mC t \u03b2 \u2212 1\n.\nLet\u2019s look at the right hand side. As, by assumption, Ct\u03b2 \u2265 4Cm, it follows that\nC3mC t \u03b2 \u2212 Ct\u03b2 2(1+\u01eb)Cm\nC3mC t \u03b2 \u2212 1\n\u2264 C3mC t \u03b2 \u2212\nCt\u03b2 2(1+\u01eb)Cm\nC3mC t \u03b2 \u2212 Ct\u03b2 4Cm\nHence, the right hand side is upper bounded by\nC3m \u2212 12(1+\u01eb)Cm C3m \u2212 14Cm = 1\u2212 2 1+\u01eb\u22121 4Cm\nC3m \u2212 14Cm But, since Cm is bounded by a constant, and \u03b1 = 1\u2212 o(1), the claim follows.\nB.1.4 Upper bounds on the \u03b3 values\nFinally, we show that if we ever reach a point where the \u03b2 values are both upper and lower bounded by a constant, the \u03b3 values one gets after the \u03b3 step are appropriately upper bounded by a constant. More precisely:\nLemma 10. Fix a particular document d. Let\u2019s assume the supports for the \u03b2 and \u03b3 variables are correct. Furthermore, let 1Cm \u2264 \u03b2ti,j \u03b2\u2217i,j \u2264 Cm for some constant Cm. Then, \u03b3td,i \u2264 (1 + o(1))\u03b3\u2217d,i.\nProof. As in the proof of Lemma 7, let\u2019s look at the KKT conditions for \u03b3td,i into a part corresponding to lone words Li and non-lone words. Multiplying B.1 by \u03b3 t d,i as before,\n\u03b3td,i = \u2211\nj\u2208Li\nf\u0303d,j + \u03b3 t d,i\n\u2211\nj /\u2208Li\nf\u0303d,j f td,j \u03b2ti,j\nAgain, let \u03b1\u0303 = \u2211 j\u2208Li \u03b2\u2217i,j .\nBy Lemma 7, certainly \u03b3td,i \u2265 1Cm \u03b3 \u2217 d,i. Hence, f\u0303d,j f td,j \u2264 (1 + \u01eb)C2m. So we have, \u03b3td,i \u2264 (1 + \u01eb)(\u03b1\u0303\u03b3\u2217d,i + C3m(1\u2212 \u03b1\u0303)\u03b3td,i). In other words, this implies \u03b3td,i \u2264 (1+\u01eb)\u03b1\u03031\u2212(1+\u01eb)C3m(1\u2212\u03b1\u0303)\u03b3 \u2217 d,i. Since \u03b1\u0303 = 1\u2212 o(1), it\u2019s easy to check that \u03b1\u03031\u2212C3m(1\u2212\u03b1\u0303) \u2264 1 + o(1), which is enough for what we need.\nSo, as a corollary, we finally get:\nCorollary 11. For some t0 = O(log( 1 \u03b2\u2217min ))) = O(logN) , it will be the case that for all t \u2265 t0,\n1\nC0\u03b2 \u2264 \u03b2\u2217i,j \u03b2ti,j \u2264\nC0\u03b2 for some constant C 0 \u03b2 and\n1\nC0\u03b3 \u2264 \u03b3\u2217d,i \u03b3td,i \u2264 C0\u03b3 for some constant C0\u03b3 .\nThis concludes Phase I of the analysis.\nB.1.5 Phase II: Alternating minimization - upper and lower bound evolution\nTaking Corollary 11 into consideration, we finally show that, if the \u03b2 and \u03b3 values are correct up to a constant multiplicative factor, and we have the correct support, we can improve the multiplicative error in each iteration, thus achieving convergence to the correct values.\nThis portion bears resemblance to techniques like state evolution and density evolution in the literature for iterative methods for decoding error correcting codes. In those techniques, one keeps track of a certain quantity of the system that\u2019s evolving in each iteration. In density evolution, this is the probability density function of the messages that are being passed, in state evolution, it is a certain average and variance of the variables we are estimating.\nIn our case, we keep track of the \u201dmultiplicative accuracy\u201d of our estimates \u03b3td,i, \u03b2 t i,j . In particular, we\nwill keep track of quantities Ct\u03b3 and C t \u03b2 , such that at iteration t, 1 Ct\u03b2 \u2264 \u03b2i,j\u2217 \u03b2ti,j \u2264 Ct\u03b2 and 1Ct\u03b3 \u2264 \u03b3d,i\u2217 \u03b3td,i \u2264 Ct\u03b3 after the corresponding \u03b3 iteration.\nWe will show that improvement in the quantities Ct\u03b2 causes a large enough improvement in the C t \u03b3 updates, so that after an alternating step of \u03b2 and \u03b3 updates, Ct+1\u03b2 \u2264 (Ct\u03b2)1/2. First, we show that when the \u03b2 variables are estimated up to a constant multiplicative factor, the constant for the \u03b3 values after they\u2019ve been iterated to convergence is slightly better than the constant for the \u03b2 values. More precisely:\nLemma 12. Let\u2019s assume that our current iterates \u03b2ti,j satisfy 1 Ct\u03b2 \u2264 \u03b2i,j\u2217\u03b2ti,j \u2264 C t \u03b2 for C t \u03b2 \u2265 1(1\u2212\u01eb)7 . Then, after iterating the \u03b3 updates to convergence, we will get values \u03b3td,i that satisfy (C t \u03b2)\n1/3 \u2264 \u03b3d,i\u2217 \u03b3td,i \u2264 (Ct\u03b2)1/3.\nProof. As usual, we will split the KKT conditions for \u03b3t+1,t \u2032\nd,i into two parts: one for the lone, and one for the non-lone words. Let\u2019s call the set of lone words Li, as previously. Then. we have\n\u03b3td,i = \u2211\nj\u2208Li\nf\u0303d,j + \u03b3 t d,i\n\u2211\nj /\u2208Li\nf\u0303d,j f td,j \u03b2ti,j\nAgain, let \u03b1\u0303 = \u2211 j\u2208Li \u03b2\u2217i,j = o(1), as we proved before. Let\u2019s denote as Ct\u03b3 = maxi(max( \u03b3\u2217d,i \u03b3td,i , \u03b3td,i \u03b3\u2217 d,i )). We claim that it has to hold that Ct\u03b3 \u2264 (Ct\u03b2)1/3. Assume the contrary, and let i0 = argmaxi(max( \u03b3\u2217d,i \u03b3td,i , \u03b3td,i \u03b3\u2217d,i )). Let\u2019s first assume that \u03b3td,i0 \u03b3\u2217d,i0 = Ct\u03b3 . By the definition of Ct\u03b3 ,\n\u03b3td,i0 = \u2211\nj\u2208Li0\nf\u0303d,j + \u03b3 t d,i0\n\u2211\nj /\u2208Li0\nf\u0303d,j f td,j \u03b2ti0,j \u2264 (1 + \u01eb)(\u03b1\u0303\u03b3\u2217d,i0 + (1\u2212 \u03b1\u0303)(Ct\u03b2)2(Ct\u03b3)2\u03b3\u2217d,i0)\nWe claim that (1 + \u01eb)(\u03b1\u0303+ (1\u2212 \u03b1\u0303)(Ct\u03b2)2(Ct\u03b3)2) \u2264 (Ct\u03b3)1/3 (B.2) which will be a contradiction to the definition of Ct\u03b3 . After a little rewriting, B.2 translates to \u03b1\u0303 \u2265 1\u2212 (Ct\u03b3 ) 1/3 1+\u01eb \u22121\n(Ct\u03b2C t \u03b3)\n2\u22121 . By our assumption on Ct\u03b3 , C t \u03b2 \u2264 C3\u03b3 , so the\nright hand side above is upper bounded by 1\u2212 (Ct\u03b3 ) 1/3 1+\u01eb \u22121\n(Ct\u03b3) 8\u22121 .\nBut, Lemma 10 implies that certainly Ct\u03b3 \u2264 C0\u03b3 , where C0\u03b3 is some absolute constant. The function\nf(c) = c1/3 1+\u01eb \u2212 1 c8 \u2212 1\ncan be easily seen to be monotonically decreasing on the interval of interest, and hence is lower bounded by (C0\u03b3 ) 1/3\n1+\u01eb \u22121\n(C0\u03b3) 8\u22121 , which is in terms some absolute constant smaller than one. Since \u03b1\u0303 = 1\u2212o(1). the claim we want\nis clearly true.\nThe case where \u03b3\u2217d,i0 \u03b3td,i0 = Ct\u03b3 is similar. In this case,\n\u03b3td,i0 = \u2211\nj\u2208Li0\nf\u0303d,j + \u03b3 t d,i0\n\u2211\nj /\u2208Li0\nf\u0303d, j\nf td,j \u03b2ti0,j \u2265 (1 \u2212 \u01eb)(\u03b1\u0303\u03b3\u2217d,i0 + (1\u2212 \u03b1\u0303)\n1\n(Ct\u03b2) 2(Ct\u03b3)\n2 \u03b3\u2217d,i0)\nWe then claim that\n(1\u2212 \u01eb)(\u03b1\u0303+ (1\u2212 \u03b1\u0303) 1 (Ct\u03b2) 2(Ct\u03b3) 2 ) \u2265 1 (Ct\u03b3) 1/3\n(B.3)\nAgain, B.3 rewrites to:\n\u03b1\u0303 \u2265 1 (1\u2212\u01eb)(Ct\u03b3) 1/3 \u2212 1(Ct\u03b2)2(Ct\u03b3)2\n1\u2212 1(Ct \u03b2 )2(Ct\u03b3) 2\n= 1\u2212 1\u2212 1 (1\u2212\u01eb)(Ct\u03b3) 1/3\n1\u2212 1(Ct \u03b2 Ct\u03b3) 2\nAgain, the right hand side above is upper bounded by 1 \u2212 1\u2212 1 (1\u2212\u01eb)(Ct\u03b3 ) 1/3\n1\u2212 1 (Ct\u03b3 )\n8 . But C\u03b3 \u2208 [1, C0\u03b3 ], and the\nfunction 1\u2212 1 (1\u2212\u01eb)c1/3\n1\u2212 1 c8\nis monotonically increasing, so lower bounded by\n1\u2212 1 (1\u2212\u01eb)( 1\n(1\u2212\u01eb)7 )1/3\n1\u2212 1 ( 1 (1\u2212\u01eb)7 )8\n= 1\u2212 (1 \u2212 \u01eb)4/3 1\u2212 (1 \u2212 \u01eb)56 \u2265 1 42\nHence, 1\u2212 1\u2212 1 (1\u2212\u01eb)(Ct\u03b3 ) 1/3\n1\u2212 1 (Ct\u03b3 )\n32 is upper bounded by 4142 . Again, our bound on \u03b1\u0303 gives us what we want.\nLemma 13. Let\u2019s assume that our current iterates \u03b2ti,j satisfy 1 Ct\u03b2 \u2264 \u03b2i,j\u2217 \u03b2ti,j \u2264 Ct\u03b2 , Ct\u03b2 \u2265 1(1\u2212\u01eb)7 , and after the corresponding \u03b3 update, we get 1Ct\u03b3 \u2264 \u03b3d,i\u2217\u03b3td,i \u2264 C t \u03b3 , where C t \u03b2 \u2265 (Ct\u03b3)3. Then, after one \u03b2 step, we will get new values \u03b2t+1i,j that satisfy 1\nCt+1\u03b2 \u2264 \u03b2i,j\u2217 \u03b2t+1i,j \u2264 Ct+1\u03b2 where Ct+1\u03b2 = (Ct\u03b2)1/2.\nProof. The proof proceeds in complete analogy with Lemmas 8 and 9. Again, let\u2019s tackle the lower and upper bound separately. The upper bound condition is:\n\u03b1 > (Ct\u03b2C t \u03b3)\n2 \u2212 (C t \u03b2) 1/2\n(1+\u01eb)Ct\u03b3\n(Ct\u03b3C t \u03b2) 2 \u2212 1\nUsing Ct\u03b2 \u2265 (Ct\u03b3)3, we can upper bound the expression on the right by 1 \u2212 (Ct\u03b2) 1/6 1+\u01eb \u2212 1 (Ct\u03b2) 8/3 \u2212 1 . The function\nf(c) = x1/6 1+\u01eb \u22121\nx8/3\u22121 is monotonically decreasing on the interval [1, C0\u03b2] of interest, so because \u03b1 = 1\u2212 o(1), we get\nwhat we want. Similarly, for the lower bound, we want that\n\u03b1 >\nCt\u03b3 (Ct\u03b2) 1/2(1\u2212\u01eb) \u2212 1 (Ct\u03b3C t \u03b2) 2\n1\u2212 1(Ct \u03b2 Ct\u03b3) 2\nYet again, using Ct\u03b2 \u2265 (Ct\u03b3)3, we get that the right hand side is upper bounded by\n1\u2212 1\u2212 1 (1\u2212\u01eb)C 1/6 \u03b2\n1\u2212 1 C3\u03b2\nHowever, the function f(c) = 1\u2212 1 (1\u2212\u01eb)c1/6\n1\u2212 1 c8/3\nis monotonically increasing on the interval [1, C0\u03b2], so lower bounded\nby 1\u2212 1 (1\u2212\u01eb)( 1 (1\u2212\u01eb)7 )1/6\n1\u2212 1 ( 1 (1\u2212\u01eb)7 )8/3\n= 1\u2212(1\u2212\u01eb) 1/6 1\u2212(1\u2212\u01eb)21 \u2265 1126 . Hence, 1 \u2212 1\u2212 1 (1\u2212\u01eb)C 1/6 \u03b2\n1\u2212 1 C3 \u03b2\nis upper bounded by 125126 , so using the fact\nthat \u03b1 = 1\u2212 o(1), we get what we want.\nPutting lemmas 12 and 13 together, we get:\nLemma 14. Suppose it holds that 1Ct \u2264 \u03b2i,j\u2217 \u03b2ti,j \u2264 Ct, Ct \u2265 1(1\u2212\u01eb)7 . Then, after one KL minimization step with respect to the \u03b3 variables and one \u03b2 iteration, we get new values \u03b2t+1i,j that satisfy 1 Ct+1 \u2264 \u03b2i,j\u2217\n\u03b2t+1i,j \u2264 Ct+1,\nwhere Ct+1 = \u221a Ct\nProof. By Lemma 12, after the \u03b3 iterations, we get \u03b3td,i values that satisfy the condition 1 (C\u2032)t \u2264 \u03b3d,i\u2217 \u03b3t d,i \u2264 (C\u2032)t, where (C\u2032)t = (Ct)1/3.\nThen, by Lemma 13, after the \u03b3 iteration, we will get 1Ct+1 \u2264 \u03b2i,j\u2217 \u03b2t+1i,j \u2264 Ct+1, such that Ct+1 = (Ct)1/2,\nwhich is what we need.\nHence, as a corollary, we get immediately:\nCorollary 15. Lemma 14 above implies that Phase III requires O(log( 1log(1+\u01eb\u2032) )) = O(log( 1 \u01eb\u2032 )) iterations to estimate each of the topic-word matrix and document proportion entries to within a multiplicative factor of 1 + \u01eb\u2032.\nThis finished the proof of Theorem 1 for the KL-tEM version of the updates. In the next section, we will remark on why the proofs are almost identical in the iterative and incomplete tEM version of the updates.\nB.2 Iterative tEM updates, incomplete tEM updates\nWe show how to modify the proofs to show that the iterative tEM and incomplete tEM updates work as well. We\u2019ll just sketch the arguments as they are almost identical as above.\nIn those updates, when we are performing a \u03b3 update, we initialize with \u03b3td,i = 0 whenever topic i does not belong to document d, and \u03b3td,i uniform among all the other topics. Then, the way to modify Lemmas 7, 10, 12 is simple. Instead of arguing by contradiction about what happens at the KKT conditions, one will assume that at iteration t\u2032 (t\u2032 to indicate these are the separate iterations for the \u03b3 variables that converge to the values \u03b3td,i) it holds that\n1 Ct\u2032\u03b3 \u03b3\u2217d,i \u2264 \u03b3t \u2032 d,i \u2264 Ct \u2032 \u03b3 \u03b3 \u2217 d,i. Then, as\nlong as Ct \u2032 \u03b3 is too big, compared to C t \u03b2 , one can show that C t\u2032 \u03b3 is decreasing (to C t\u2032+1 \u03b3 = (C t \u03b3) 1/2, say), using exactly the same argument we had before. Furthermore, the number of such iterations needed will clearly be logarithmic.\nBut the same argument as above proves the incomplete tEM updates work as well. Namely, even if we perform only one update of the \u03b3 variables, they are guaranteed to improve.\nB.3 Initialization\nFor completeness, we also give here a fairly easy, efficient initialization algorithm. Recall, the goal of this phase is to recover the supports - i.e. to find out which topics are present in a document, and identify the support of each topic. To reiterate the theorem statement: Theorem 2. If the number of documents is \u2126(K4 log2 K), there is a polynomial-time procedure which with probability 1\u2212 \u2126( 1K ) correctly identifies the supports of the \u03b2\u2217i,j and \u03b3\u2217d,i variables.\nWe will find the topic supports first. Roughly speaking, we will devise a test, which will take as input two documents d, d\u2032, and will try to determine if the two documents have a topic in common or not. The test will have no false positives, i.e. will never say NO, if the documents do have a topic in common, but might say NO even if they do. We will then, ensure that with high probability, for each topic we find a pair of documents intersecting in that topic, such that the test says YES.\nWe will also be able to identify which pairs intersect in exactly one topic, and from this we will be able to find all the topic supports. Having done all of this, finding the topics in each document will be easy as well. Roughly speaking, if a document doesn\u2019t contain a given topic, it will not contain all of the discriminative words in that document.\nWe give the algorithm formally as pseudocode Algorithm 4. Now, let\u2019s proceed to analyze the above algorithm, proceeding in a few parts.\nB.3.1 Constructing a no-false-positives test\nFirst, we describe how one determines the supports of the topics. Let\u2019s define Test(d, d\u2032) = YES, if \u2211\nj min{f\u2217d,j, f\u2217d\u2032,j} \u2265 12T , and NO otherwise. Then, we claim the following. Lemma 16. If d, d\u2032 both contain a topic i0, s.t. \u03b3\n\u2217 d,i0 \u2265 1/T , \u03b3\u2217d\u2032,i0 \u2265 1/T then Test(d, d\u2032) = YES. If d, d\u2032 do not contain a topic i0 in common, then Test(d, d \u2032) = NO.\nProof. Let\u2019s prove the first claim. \u2211\nj\nmin{f\u0303d,j, f\u0303d\u2032,j} \u2265 \u2211\nj\n(1\u2212 \u01eb)min{\u03b2\u2217i0,j\u03b3\u2217d,i0 , \u03b2\u2217i0,j\u03b3\u2217d\u2032,i0} \u2265\n\u2211\nj\n(1 \u2212 \u01eb)1/T\u03b2\u2217i0,j \u2265 1/2T\nAlgorithm 4 Initialization\nrepeatK4 log2 K times Sample a pair of documents (d, d\u2032). \u22b2Test if (d, d\u2032) intersect with no false positives: if \u2211\nj min{f\u2217d,j, f\u2217d\u2032,j} \u2265 12T then Sd,d\u2032 := {j, s.t.f\u2217d,j, f\u2217d\u2032,j > 0} \u22b2\u201dWeed-out\u201d words that are not in the support of the intersection of (d,d\u2019) for all documents d\u2032\u2032 6= {d, d\u2032} do\nif \u2211 j min{f\u2217d,j, f\u2217d\u2032\u2032,j} \u2265 12T and \u2211 j min{f\u2217d\u2032,j , f\u2217d\u2032\u2032,j} \u2265 12T then Sd,d\u2032 = Sd,d\u2032 \u2229 j, s.t f\u2217d\u2032\u2032,j > 0 end if\nend for\nend if\nuntil \u22b2Determine which Sa,b correspond to documents intersecting in one topic only) if Set Sa,b appears less than D/K\n2.5 times, where D is the total number of documents then Remove Sa,b.\nend if if Set Sa,b can be written as the union of two other sets Sc,d, Se,f , where neither is contained inside the other then\nRemove Sa,b. end if if Set Sa,b is strictly contained inside Sd,d\u2032 for some Sd,d\u2032 then Remove Sd,d\u2032. end if Remove duplicates. The remaining lists Sa,b are declared to be topic supports.\nNow, let\u2019s prove the second claim. Let\u2019s suppose d, d\u2032 contain no topic in common. Let\u2019s fix a topic i0 that belongs to document d. By the \u201dsmall discriminative words intersection\u201d, we\nhave the following property: \u2211\nj\u2208i0,j\u2208i\u2032\n\u03b2\u2217i,j = o(1)\nfor any other topic i\u2032 6= i0. Denoting by Toutside the words belonging to topic i0, and no topic in document d\n\u2032, and Tinside the words belonging to at least one other topic in d\u2032, we have\n\u2211\nj\u2208Tinside\n\u03b2\u2217i,j \u2264 T \u00b7 o(1) = o(1)\nFor the words j \u2208 Toutside, min{f\u2217d,j, f\u2217d\u2032,j} = 0 By the above,\n\u2211\nj\nmin{f\u0303d,j, f\u0303d\u2032,j} \u2264 (1 + \u01eb)T 2o(1) = o(1)\nThus, the test will say NO, as we wanted.\nB.3.2 Finding the topic supports from identifying pairs\nLet\u2019s call d, d\u2032 an identifying pair of documents for topic i, if d, d\u2032 intersect in topic i only, and furthermore the test says YES on that pair.\nFrom this identifying pair, we show how to find the support of the topic i in the intersection. What we\u2019d like to do is just declare the words j, s.t. f\u2217d,j, f \u2217 d\u2032,j are both non-zero as the support of topic i. Unfortunately, this doesn\u2019t quite work. The reason is that one might find words j, s.t. they belong to one topic i\u2032 in d, and another topic i\u2032\u2032 in d\u2032\u2032. Fortunately, this is easy to remedy. As per the pseudo-code above, let\u2019s call the following operation WEEDOUT (d, d\u2032):\n\u2022 Set S = {j, s.t.f\u2217d,j > 0, f\u2217d\u2032,j > 0}.\n\u2022 For all d\u2032\u2032, s.t. Test(d, d\u2032\u2032) = Y ES, Test(d\u2032, d\u2032\u2032) = Y ES:\n\u2022 Set S = S \u222a {j, s.t.f\u2217d\u2032\u2032,j > 0}\n\u2022 Return S.\nLemma 17. With probability 1\u2212\u2126( 1K ), for any pair of documents d, d\u2032 intersecting in one topic, WEEDOUT (d, d\u2032) is the support of S.\nProof. For this, we prove two things. First, it\u2019s clear that S is initialized in the first line in a way that ensures that it contains all words in the support of topic i. Furthermore, it\u2019s clear that at no point in time we will remove a word j from S that is in the support of topic i. Indeed - if Test(d, d\u2032\u2032) = Y ES and Test(d\u2032, d\u2032\u2032) = Y ES, then by Lemma 16 document d\u2032\u2032 must contain topic i. In this case, f\u2217d\u2032\u2032,j > 0, and we won\u2019t exclude j from S.\nSo, we only need to show that the words that are not in the support of topic i will get removed. Let d, d\u2032 intersect in a topic i. Let a word j be outside the support of a given topic i. Because of the independent topic inclusion property, the probability that a document d\u2032\u2032 contains topic i, and no other topic containing j is \u2126(1/K).\nSince the number of documents is \u2126(K4 log2 K), by Chernoff, the probability that there is a document d\u2032\u2032, s.t. Test(d, d\u2032\u2032) = Y ES, Test(d\u2032, d\u2032\u2032) = Y ES, but f\u2217d\u2032\u2032,j = 0, is 1 \u2212 \u2126( 1eK2 log2 K ). Union bounding over all words j, as well as pairs of documents d, d\u2032, we get that for any documents d, d\u2032 intersection in a topic i, we get the claim we want.\nB.3.3 Finding the identifying pairs\nFinally, we show how to actually find the identifying pairs. The main issue we need to handle are documents that do intersect, and the TEST returns yes, but they intersect in more than one topic. There\u2019s two ingredients to ensuring this is true in the above algorithm.\n\u2022 First, we delete all sets in the list of sets Sa,b that show up less than D2/K2.5 number of times.\n\u2022 Second, we remove sets that can be written as the union of two other sets Sc,d, Se,f , where neither of the two is contained inside the other.\n\u2022 After this, we delete the non-maximal sets in the list.\nThe following lemma holds:\nLemma 18. Each topic has \u2126(D2/K2) identifying pairs with probability 1\u2212 \u2126( 1K ).\nProof. Let Ii be the event that there are at least \u2126(D2/k2) identifying pairs for topic i. Let Ni be a random variable denoting the number of documents which have topic i as a dominating topic. Furthermore, let Mi be the event that there are at least N 2 i 2 \u2212 K \u221a\nN2i identifying pairs among the Ni ones that have i as a dominating topic. By the dominant topic equidistribution property, probability that a document d has a topic i as a dominating topic is at least C/K for some constant C. Then, clearly,\nPr[\u2229Ki=1Ii] \u2265 Pr [ \u2229Ki=1 ( Ni \u2265 1 2 C D K )] Pr [ \u2229Ki=1Mi| \u2229Ki=1 ( Ni \u2265 1 2 C D K )]\nLet\u2019s estimate Pr [ \u2229Ki=1 ( Ni \u2265 12C DK )]\nfirst. The probabilities that different documents have i0 as the dominating topic are clearly independent, so by Chernoff, if Ni is the number of documents where i is the dominating topic,\nPr[Ni \u2265 (1\u2212 \u01eb)C D K ] \u2265 1\u2212 e\u2212 \u01eb 2 3 C D K\nSince D = \u2126(K2), plugging in \u01eb = 12 , Pr[Ni < 1 2C D K ] \u2265 1 \u2212 e\u2212\u2126(K). Union bounding over all topics, we get that with probability Pr [\n\u2229Ki=1 ( Ni \u2265 12C DK )] \u2265 1\u2212 1K . Now, let\u2019s consider Pr [\n\u2229Ki=1Mi| \u2229Ki=1 ( Ni \u2265 12C DK )] . The event \u2229Ki=1 ( Ni \u2265 12C DK )\ncan be written as the disjoint union of events {D = \u222aKi=1Di, \u2200i 6= j,Di \u2229Dj = \u2205} where D is the set of all documents, Di is the set of documents that have i as the dominating topic, and |Di| \u2265 12C DK , \u2200i. (i.e. all the partitions of D into K sets of sufficiently large size). Evidently, if we prove a lower bound on Pr [\n\u2229Ki=1Mi|E ] for any such event E, it will imply a lower bound on\nPr [ \u2229Ki=1Mi| \u2229Ki=1 ( Ni \u2265 12C DK )] . For any such event, consider two documents d, d\u2032 \u2208 {Di}, i.e. having i as the dominating topic. Let Id,d\u2032 be an indicator variable denoting the event that d, d\u2032 do not intersect in an additional topic. Pr[Id,d\u2032 = 1] = 1\u2212 o(1), by the independent topic inclusion property and the events Id,d\u2032 are easily seen to be pairwise independent. Furthermore, Var[Id,d\u2032 ] = o(1). By Chebyshev\u2019s inequality,\nPr\n\n\n\u2211\nd,d\u2032\u2208Di\nId,d\u2032 \u2265 1\n2 D2i \u2212 c\n\u221a\nD2i\n\n \u2265 1\u2212 1 c2\nIf Ni = \u2126(K logK), plugging in c = K, we get that Pr\n\n\n\u2211\nd,d\u2032\u2208Di\nId,d\u2032 = \u2126(D2i )\n\n \u2265 1 \u2212 \u2126( 1 K2 ). Hence,\nPr [ \u2229Ki=1Mi|E ] \u2265 1\u2212 1K , by a union bound, which implies Pr [ \u2229Ki=1Mi| \u2229Ki=1 ( Ni \u2265 12C DK )] \u2265 1\u2212 1K . Putting all of the above together, if D = \u2126(K2 logK), with probability 1 \u2212 \u2126( 1K ), all topics have \u2126(D2/K2) identifying pairs, which is what we want.\nThe lemma implies that with probability 1\u2212\u2126( 1K ), we will not eliminate the sets Sa,b corresponding to topic supports.\nWe introduce the following concept of a \u201dconfiguration\u201d. A set of words C will be called a \u201dconfiguration\u201d if it can be constructed as the intersection of the discriminative words in some set of topics, i.e.\nDefinition. A set of words C is called a configuration if there exists a set I = {I1, . . . , I|I|} of topics, s.t.\nC = \u2229|I|i=1WIi Let\u2019s call the minimal size of a set I that can produce C the generator size of C.\nNow, we claim the following fact:\nLemma 19. If a configuration C has generator size \u2265 3, then with probability 1 \u2212 \u2126( 1K ), it cannot appear as one of the sets Sa,b after step 2 in the WEEDOUT procedure.\nProof. Since C has generator size at least 3, if two sets d, d\u2032 intersect in less than two topics, then step 1 in WEEDOUT cannot produce Sa,b which is equal to C. Hence, prior to step 2, C can only appear as Sd,d\u2032 for d, d\u2032 that intersect in at least 3 topics.\nLet Id,d\u2032 be an indicator variable denoting the fact that the pair of documents d, d\u2032 intersects in at least 3 topics. We have Pr[Id,d\u2032 = 1] \u2264 1/K3 + 1/K4 + . . . 1/KT = O(1/K3) by the independent topic inclusion property.\nIf I3 is a variable denoting the total number of documents that intersect in at least 3 topics, again by Chebyshev as in Lemma 18 we get:\nPr[I3 \u2265 \u0398(D/K3)\u2212 c\u0398( \u221a D/K3/2)] \u2265 1\u2212 1\nc2\nAgain, by putting c = \u221a K, since the number of documents is K4 log2 K, with probability 1 \u2212 1K , all\nconfigurations with generator size \u2265 3 cannot appear as one of the sets Sa,b, as we wanted.\nThis means that after the WEEDOUT step, with probability 1 \u2212 \u2126( 1K ), we will just have sets Sa,b corresponding to configurations generated by two topics or less. The options for these are severely limited: they have to be either a topic support, the union of two topic supports, or the intersection of two topic supports. We can handle this case fairly easily, as proven in the following lemma:\nLemma 20. After the end of step 3, with probability 1 \u2212 \u2126( 1K ), the only remaining Sa,b are those corresponding to topic supports.\nProof. First, when we check if some Sd,d\u2032 is the union of two other sets and delete it if yes, I claim we will delete the sets equal to configurations that correspond to unions of two topic supports (and nothing else). This is not that difficult to see: certainly the sets that do correspond to configurations of this type will get deleted.\nOn the other hand, if it\u2019s the case that Sa,b corresponds to a single topic support, we won\u2019t be able to write it as the union of two sets Sd,d\u2032, Sd\u2032\u2032,d\u2032\u2032\u2032 , unless one is contained inside the other - this is ensured by the existence of discriminative words.\nHence, after the first two passes, we will only be left with sets that are either topic supports, or intersections of two topic supports. Then, removing the non-maximal is easily seen to remove the sets that are intersections, again due to the existence of discriminative words.\nB.3.4 Finding the document supports\nNow, given the supports of each topic, for each document, we want to determine the topics which are non-zero in it. The algorithm is given in 5:\nLemma 21. If a topic i0 is such that \u03b3 \u2217 d,i0 > 0, it will be declared as \u201dIN\u201d. If a topic i0 is such that \u03b3 \u2217 d,i0 = 0, it will be declared as out.\nAlgorithm 5 Finding document supports\nInitialize R = \u2205. for each i do\nCompute Score(i) = \u2211\nj\u2208Support(i)\\R f\u0303d,j end for Find i\u2217 such that Score(i\u2217) is maximum. while Score(i\u2217) > 0 do\nOutput i\u2217 to be in the support of d. R = R \u222a support(i\u2217) Recompute Score for every other topic. Find i\u2217 with maximum score.\nend while\nProof. Consider a topic i. At any iteration of the while cycle, consider \u2211 j\u2208Support(i)\\R f\u0303d,j. Clearly, f\u0303d,j \u2265 (1\u2212 \u01eb)\u03b3\u2217d,i\u03b2\u2217i,j . Also \u2211 j\u2208R \u03b2 \u2217 i,j = To(1). Hence,\n\u2211\nj\u2208Support(i)\\R\nf\u0303d,j \u2265 (1 \u2212 \u01eb)\u03b3\u2217d,i(1\u2212 To(1)) \u2265 1\n2 \u03b3\u2217d,i\nSo, topic i will be added eventually. On the other hand, let\u2019s assume the document doesn\u2019t contain a given topic i0. Let\u2019s call B the set of words j which are in the support of i0, and belong to at least one of the topics in document d. Then, \u2211\nj\u2208i0 f\u0303d,j =\n\u2211\nj\u2208B f\u0303d,j. Let i \u2217 be the topic which is present in the document but not added yet and has\nmaximum value of \u03b3\u2217d,i. Then \u2211\nj\u2208B\nf\u0303d,j \u2264 (1 + \u01eb) \u2211\ni\u2208d\n\u2211\nj\u2208B\n\u03b3\u2217d,i\u03b2 \u2217 i,j \u2264\n(1 + \u01eb)\u03b3\u2217d,i\u2217 \u2211\ni\u2208d\n\u2211\nj\u2208B\n\u03b2\u2217i,j \u2264\n(1 + \u01eb)T\u03b3\u2217d,i\u2217o(1) \u2264 \u03b3\u2217d,i\u2217 ] \u00b7 o(1) Hence, topic i\u2217 will always get preference over i0. Once all the topics which are present in the document\nhave been added, it is clear that no more topic will be added since score will be 0.\nThis finally finishes the proof of Theorem 2."}, {"heading": "C Case study 2: Dominating topics, seeded initialization", "text": "As a reminder, seeded initialization does the following:\n\u2022 For each topic i, the user supplies a document d, in which \u03b3\u2217d,i \u2265 Cl.\n\u2022 We initialize with \u03b20i,j = f\u2217d,j. The theorem we want to show is:\nTheorem 3. Given an instance of topic modelling satisfying the Case Study 2 properties specified above, where the number of documents is \u2126(K log 2 N\n\u01eb\u20322 ), if we initialize with seeded initialization, after O(log(1/\u01eb \u2032) + logN)\nof KL-tEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1+\u01eb\u2032.\nThe proof will be in a few phases again:\n\u2022 Phase I: Anchor identification: First, we will show that as long as we can identify the dominating topic in each of the documents, the anchor words will make progress, in the sense that after O(logN) number of rounds, the values for the topic-word estimates will be almost zero for the topics for which the word is not an anchor, and lower bounded for the one for which it is.\n\u2022 Phase II: Discriminative word identification: Next, we show that as long as we can identify the dominating topics in each of the documents, and the anchor words were properly identified in the previous phase, the values of the topic-word matrix for words which do not belong to a certain topic will keep dropping until they reach almost zero, while being lower bounded for the words that do.\n\u2022 For Phase I and II above, we will need to show that the dominating topic can be identified at any step. Here we\u2019ll leverage the fact that the dominating topic is sufficiently large, as well as the fact that the anchor words have quite a large weight.\n\u2022 Phase III: Alternating minimization: Finally, we show that after Phase I and II above, we are back to the scenario of the previous section: namely, there is a \u201dboosting\u201d type of improvement in each next round.\nC.1 Estimates on the dominating topic\nBefore diving into the specifics of the phases above, we will show what the conditions we need are to be able to identify the dominating topic in each of the documents. For notational convenience, let \u2206m be the m-dimensional simplex: x \u2208 \u2206m iff \u2200i \u2208 [m], 0 \u2264 xi \u2264 1 and \u2211 i xi = 1.\nFirst, during a \u03b3 update, we are minimizing KL(f\u0303d||fd) with respect to the \u03b3d variables, so we need some way or arguing that whenever the \u03b2 estimates are not too bad, minimizing this quantity also quantifies how far the \u03b3d variables are from \u03b3 \u2217 d .\nFormally, we\u2019ll show the following:\nLemma 22. If, for all i, KL(\u03b2\u2217i ||\u03b2ti ) \u2264 R\u03b2, and min\u03b3d\u2208\u2206K KL(f\u0303d||fd) \u2264 Rf , after running a KL divergence minimization step with respect to the \u03b3d variables, we get that ||\u03b3\u2217d \u2212 \u03b3d||1 \u2264 1p ( \u221a 1 2R\u03b2 + \u221a 1 2Rf ) + \u01eb.\nWe will start with the following simple helper claim:\nLemma 23. If the word-topic matrix \u03b2 is such that in each topic the anchor words have total probability at least p, then ||\u03b2\u2217v||1 \u2265 p||v||1.\nProof.\n||\u03b2\u2217v||1 = \u2211\nj\n| \u2211\ni\n\u03b2\u2217i,jvi| \u2265 \u2211\ni\n\u2211\nj\u2208Wi\n|\u03b2\u2217i,jvi| \u2265 \u2211\ni\np|vi| \u2265 p||v||1\nLemma 24. If, for all i, KL(\u03b2\u2217i ||\u03b2ti ) \u2264 R\u03b2, and min\u03b3d\u2208\u2206K KL(f\u0303d||fd) \u2264 Rf , after running a KL divergence minimization step with respect to the \u03b3d variables, we get that ||\u03b3\u2217d \u2212 \u03b3d||1 \u2264 1p ( \u221a 1 2R\u03b2 + \u221a 1 2Rf ) + \u01eb.\nProof. First, observe that min\u03b3d\u2208\u2206K KL(f\u0303d||fd) \u2264 Rf , at the the optimal \u03b3d, we have that ||f\u0303d\u2212fd||21 \u2264 12Rf , i.e. ||f\u0303d \u2212 fd|| \u2264 \u221a 1 2Rf , by Pinsker\u2019s inequality.\nWe will show that if ||\u03b3\u2217d \u2212 \u03b3d||1 is large, so must be ||f\u0303d \u2212 fd||1, and hence KL(f\u0303d||fd) - which will contradict the above upper bound.\nLet\u2019s consider \u03b2\u2217 as N by K matrix, and \u03b3\u2217 and f\u2217 as K-dimensional vectors. Let \u03b2\u2217\u03b3\u2217 just denote matrix-vector multiplication - so f\u2217 = \u03b2\u2217\u03b3\u2217. For any other vector \u03b3\u0302, let\u2019s denote f\u0302 = \u03b2t\u03b3\u0303. Then:\n||f\u0303 \u2212 f\u0302 ||1 = ||f\u0303 \u2212 \u03b2t\u03b3\u0303||1 = ||f\u0303 \u2212 (\u03b2\u2217 + (\u03b2t \u2212 \u03b2\u2217))\u03b3\u0303||1 \u2265\n||f\u0303 \u2212 \u03b2\u2217\u03b3\u0303||1 \u2212 ||(\u03b2t \u2212 \u03b2\u2217)\u03b3\u0303||1 (C.1) Hence, ||f\u0303 \u2212 \u03b2\u2217\u03b3\u0303||1 \u2264 ||(\u03b2t \u2212 \u03b2\u2217)\u03b3\u0303||1 + ||f\u0303 \u2212 f\u0302 ||1. However, However,\n||(\u03b2t \u2212 \u03b2\u2217)\u03b3||1 \u2264 max i\n\u2211\nj\n|\u03b2ti,j \u2212 \u03b2\u2217i,j | \u2264 max i\n\u221a\n1 2 KL(\u03b2\u2217i ||\u03b2ti ) \u2264\n\u221a\n1 2 R\u03b2 (C.2)\nThe first inequality is a property of induced matrix norms, the second is via Pinsker\u2019s inequality. So, by C.1 and C.2, ||f\u0303\u2212\u03b2\u2217\u03b3\u0303||1 \u2264 \u221a 1 2R\u03b2+ \u221a 1 2Rf . But now, finally, Lemma 23 implies that ||\u03b3\u2217d\u2212\u03b3d||1 \u2264\n1 p ( \u221a 1 2Rf + \u221a 1 2R\u03b2) + \u01eb.\nLemma 25. Suppose that for the dominating topic i in a document d, \u03b3\u2217d,i \u2265 Cl, and for all other topics i\u2032, \u03b3\u2217d,i\u2032 \u2264 Cs, s.t. Cl \u2212 Cs > 1p ( \u221a 1 2Rf + \u221a 1 2R\u03b2) + \u01eb. Then, the above test identifies the largest topic. Furthermore, 12\u03b3 \u2217 d,i \u2264 \u03b3td,i \u2264 32\u03b3\u2217d,i\nProof. By Lemma 24, and the relationship between l1 and total variation distance between distributions, we have that |\u03b3td,i \u2212 \u03b3\u2217d,i| \u2264 12 ( 1 p ( \u221a 1 2Rf + \u221a 1 2R\u03b2 ) + \u01eb ) .\nFor the dominating topic i, \u03b3td,i \u2265 Cl \u2212 12 ( 1 p\n( \u221a\n1 2Rf +\n\u221a\n1 2R\u03b2\n) + \u01eb ) . On the other hand, for any other\ntopic i\u2032, \u03b3td,i\u2032 \u2264 Cs + 12 ( 1 p\n(\u221a\n1 2Rf +\n\u221a\n1 2R\u03b2\n) + \u01eb ) . Since Cl \u2212Cs \u2265 1p (\u221a 1 2Rf + \u221a 1 2R\u03b2 ) + \u01eb, \u03b3td,i > \u03b3 t d,i\u2032 , so\nthe test works.\nOn the other hand, since \u03b3td,i \u2265 \u03b3\u2217d,i \u2212 ( 1 p\n( \u221a\n1 2Rf +\n\u221a\n1 2R\u03b2\n) + \u01eb )\n\u2265 \u03b3\u2217d,i \u2212 12\u03b3\u2217d,i = 12\u03b3\u2217d,i. Similarly,\n\u03b3td,i \u2264 \u03b3\u2217d,i + 1p ( \u221a 1 2Rf + \u221a 1 2R\u03b2 ) + \u01eb \u2264 \u03b3\u2217d,i + 12\u03b3\u2217d,i = 32\u03b3\u2217d,i.\nC.2 Phase I: Determining the anchor words\nWe proceed as outlined. In this section we show that in the first phase of the algorithm, the anchor words will be identified - by this we mean that we will be able to show that if a word j is an anchor for topic i, \u03b2ti,j will be within a factor of roughly 2 from \u03b2 \u2217 i,j , and \u03b2 t i\u2032,j will be almost 0 for any other topic i\n\u2032. We will assume throughout this and the next section that we can identify what the dominating topic is, and that we have an estimate of the proportion of the dominating topic to within a factor of 2. (We won\u2019t restate this assumption in all the lemmas in favor of readability.)\nWe will return to this issue after we\u2019ve proven the claims of Phases I and II modulo this claim. The outline is the following. We show that at any point in time, by virtue of the initialization, \u03b2ti,j is pretty well lower bounded (more precisely it\u2019s at least constant times \u03b2\u2217i,j). This enables us to show that \u03b2ti\u2032,j will halve at each iteration - so in some polynomial number of iterations will be basically 0.\nC.2.1 Lower bounds on the \u03b2ti,j values\nWe proceed as outlined above. We show here that the \u03b2ti,j variables are lower bounded at any point in time. More precisely, we show the following lemma:\nLemma 26. Let j be an anchor word for topic i, and let i\u2032 6= i. Suppose that \u03b2ti\u2032,j \u2264 \u03b2ti,j. Then, \u03b2t+1i,j \u2265 (1\u2212 \u01eb)Cl\u03b2\u2217i,j holds.\nProof. We\u2019ll prove a lower bound on each of the terms f\u0303d,j ftd,j \u03b2ti,j . Since the update on the \u03b2 variables is a convex combination of terms of this type, this will imply a lower bound on \u03b2t+1i,j .\nFor this, we upper bound f td,j. We have:\nf td,j = \u03b2 t i,j\u03b3 t d,i +\n\u2211\ni\u2032 6=i\n\u03b2ti\u2032,j\u03b3 t d,i\u2032\nThis means that f td,j is a convex combination of terms, each of which is at most \u03b2 t i,j . Hence, f t d,j \u2264 \u03b2ti,j\nholds. But then f\u0303d,j ftd,j \u03b2ti,j \u2265 f\u0303d,j \u2265 (1 \u2212 \u01eb)\u03b2\u2217i,j\u03b3\u2217d,i \u2265 (1 \u2212 \u01eb)Cl\u03b2\u2217i,j . This implies \u03b2t+1i,j \u2265 (1 \u2212 \u01eb)Cl\u03b2\u2217i,j , as we wanted.\nC.2.2 Decreasing \u03b2ti\u2032,j values\nWe\u2019ll bootstrap to the above result. Namely, we\u2019ll prove that whenever \u03b2ti,j \u2265 1/C\u03b2\u03b2\u2217i,j for some constant C\u03b2 , the \u03b2ti\u2032,j values decrease multiplicatively at each round. Prior to doing that, the following lemma is useful. It will state that whenever the values of the variables \u03b2ti\u2032,j are somewhat small, we can get some reasonable lower bound on the values \u03b3td,i we get after a step of KL minimization with respect to the \u03b3 variables. Lemma 27. Let j be an anchor for topic i, and let i\u2032 6= i. Let \u03b2ti\u2032,j \u2264 b\u03b2ti,j. Then, for any document d, when performing KL divergence minimization with respect to the variables \u03b3d, for the optimum value \u03b3 t d,i, it holds that \u03b3td,i \u2265 (1 \u2212 \u01eb) p1\u2212b\u03b3\u2217d,i \u2212 b1\u2212b .\nProof. The KKT conditions B.1 imply that if we denote Ai the set of anchors in topic i, \u2211\nj\u2208Ai f\u0303d,j ftd,j \u03b2ti,j \u2264 1. By the assumption of the lemma,\nf td,j \u2264 bti,j\u03b3td,i + bbti,j(1\u2212 \u03b3td,i)\nSince f\u0303d,j \u2265 (1\u2212 \u01eb)\u03b2\u2217i,j\u03b3\u2217d,i, this implies f\u0303d,j ft d,j \u03b2ti,j \u2265 (1\u2212 \u01eb)\u03b2\u2217i,j \u03b3\u2217d,i \u03b3t d,i (1\u2212b)+b , i.e. \u2211 j\u2208Ai (1\u2212 \u01eb)\u03b2\u2217i,j \u03b3\u2217d,i \u03b3t d,i (1\u2212b)+b \u2264 1. Rearranging the terms, we get\n\u03b3td,i \u2265 (1 \u2212 \u01eb) \u2211\nj\u2208Ai\n\u03b2\u2217i,j \u03b3\u2217d,i 1\u2212 b \u2212 b 1\u2212 b \u2265 (1 \u2212 \u01eb)p\u03b3 \u2217 d,i \u2212 b 1\u2212 b\nas we needed.\nWith this in place, we show that the value \u03b2ti\u2032,j when j is an anchor for topic i 6= i\u2032, decreases by a factor of 2 after the update for the \u03b2 variables.\nThis requires one more new idea. Intuitively, if we view the update as setting \u03b2t+1i\u2032,j to \u03b2 t i,j multiplied by\na convex combination of terms f\u2217d,j ft d,j , a large number of them will be zero, just because f\u2217d,j = 0 unless topic i belongs to document d. By the topic equidistribution property then, the probability that this happens is only O(1/K), so if the weight in the convex combination on these terms is reasonable, we will multiply \u03b2ti,j by something less than 1, which is what we need.\nLemma 27 says that if \u03b3\u2217d,i is reasonably large, we will estimate it somewhat decently. If \u03b3 \u2217 d,i is small,\nthen f\u2217d,j would be small anyway. So we proceed according to this idea.\nLemma 28. Let j be an anchor for topic i. Let \u03b2ti\u2032,j \u2264 b\u03b2ti,j for i\u2032 6= i, and let \u03b2ti,j \u2265 1/C\u03b2\u03b2\u2217i,j for some constant C\u03b2. Then, \u03b2 t+1 i\u2032,j \u2264 b/2\u03b2\u2217i,j\nProof. We will split the \u03b2 update as\n\u03b2t+1i\u2032,j = \u03b2 t i\u2032,j(\n\u2211\nd\u2208D1 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n+\n\u2211\nd\u2208D2 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n+\n\u2211\nd\u2208D3 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n)\nfor some appropriately chosen partition of the documents into three groups D1, D2, D3. Let D1 be documents which do not contain topic i at all, D2 documents which do contain topic i, and \u03b3\u2217d,i \u2265 2bp , and D3 documents which do contain topic i and \u03b3\u2217d,i < 2bp . The first part will just vanish because word j is an anchord word for topic i, and topic i does not appear in it, so f\u2217d,j = 0 for all documents d \u2208 D1. The second summand we will upper bound as follows. First, we upper bound\nf\u0303d,j ftd,j . We have that\nf td,j \u2265 \u03b2ti,j\u03b3td,i \u2265 1/C\u03b2\u03b2\u2217i,j\u03b3td,i. However, we can use Lemma 27 to lower bound \u03b3td,i. We have that \u03b3td,i \u2265 (1\u2212 \u01eb)( p1\u2212b\u03b3\u2217d,i \u2212 b1\u2212b ) \u2265 (1\u2212 \u01eb) p 2(1\u2212b)\u03b3 \u2217 d,i. This alltogether implies f\u0303d,j ftd,j \u2264 11\u2212\u01eb 2(1\u2212b)C\u03b2 p . Hence,\n\u03b2ti\u2032,j\n\u2211\nd\u2208D2 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n\u2264 1 1\u2212 \u01eb 2C\u03b2 p\n(1 \u2212 b)\u03b2ti\u2032,j \u2211 d\u2208D2 \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\nFurthermore, \u2211 d \u03b3 t d,i\u2032 \u2265 12 |D|Cl. On the other hand, I claim \u2211 d\u2208D2 \u03b3td,i\u2032 = O(K/|D|). Recall that D is the set of documents where topic i\u2032 is the dominating topic - so by definition they contain topic i. On the other hand, if a document is in D2 then it contains topic i as well. However, by the independent topic inclusion property, the probability that a document with dominating topic i\u2032 contains topic i as well is O(1/K). Hence,\n\u03b2ti\u2032,j\n\u2211\nd\u2208D2 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n= O( 1\nK )b\u03b2ti,j\nFor the third summand we provide a trivial bound for the terms f\u0303d,j ftd,j \u03b2ti\u2032,j\u03b3 t d,i\u2032 :\nf\u0303d,j f td,j \u03b2ti\u2032,j\u03b3 t d,i\u2032 \u2264 (1 + \u01eb)\u03b2\u2217i,j\u03b3\u2217d,i \u2264 (1 + \u01eb)\u03b2\u2217i,j 2b p\nSince again, \u2211 d \u03b3 t d,i\u2032 \u2265 12 |D|Cl, and again, the number of document in D3 is at most O(1/K) for the same reasons as before, we have that\n\u03b2ti\u2032,j\n\u2211\nd\u2208D3 f\u2217d,j ftd,j\n\u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n\u2264 O(1/K)b\u03b2\u2217i,j = O(1/K)b\u03b2ti,j\nsince \u03b2ti,j \u2265 1C\u03b2 \u03b2 \u2217 i,j .\nFrom the above three bounds, we get that \u03b2t+1i\u2032,j \u2264 O(1/K)b\u03b2ti,j \u2264 b\n2 \u03b2ti,j .\nNow, we just have to put together the previous two claims: namely we need to show that the conditions for the decay of the non-anchor topic values, and the lower bound on the anchor-topic values are actually preserved during the iterations. We will hence show the following:\nLemma 29. Suppose we initialize with seeded initialization. Then, after t rounds, if j is an anchor word for topic i, \u03b2ti,j \u2265 (1\u2212 \u01eb)Cl\u03b2\u2217i,j, and \u03b2ti\u2032,j \u2264 2\u2212tCs\u03b2\u2217i,j.\nProof. We prove this by induction. Let\u2019s cover the base case first. In the seed document corresponding to topic i, \u03b3\u2217d,i \u2265 Cl, so at initialization \u03b20i,j \u2265 Cl\u03b2\u2217i,j . On the other hand, if topic i appears in the seed document for topic i\u2032, then after initialization \u03b20i\u2032,j \u2264 Cs\u03b2\u2217i,j < \u03b20i,j . Hence, at initialization, the claim is true.\nOn to the induction step. If the claim were true at time step t, since \u03b2ti\u2032,j \u2264 2\u2212tCs\u03b2\u2217i,j , by Lemma 26, \u03b2t+1i,j \u2265 Cl\u03b2\u2217i,j - so the lower bound still holds at time t + 1. On the other hand, since \u03b2ti,j \u2265 Cl\u03b2\u2217i,j , by Lemma 28, at time t+ 1, \u03b2ti\u2032,j \u2264 2\u2212(t+1)Cs\u03b2\u2217i,j .\nHence, the claim we want follows.\nFinally, we show the easy lemma that after the values \u03b2ti\u2032,j have decreased to (almost) 0, \u03b2 t i,j \u2265 12\u03b2\u2217i,j .\nLemma 30. Let word j be an anchor word for topic i. Suppose \u03b2ti\u2032,j \u2264 2\u2212tCs\u03b2\u2217i,j and\nt > 10 max(log(N), log( 1\n\u03b3\u2217min ), log(\n1\n\u03b2\u2217min ))\nThen 4\u03b2\u2217i,j \u2265 \u03b2t+1i,j \u2265 14\u03b2\u2217i,j.\nProof. Let us do the lower bound first. It\u2019s easy to see \u2211 i\u2032 \u03b2 t i\u2032,j\u03b3d,i\u2032 \u2264 2\u03b2ti,j\u03b3td,i. Hence,\nf\u0303d,j f td,j \u03b2ti,j\u03b3 t d,i = f\u0303d,j \u2211 i\u2032 \u03b2 t i\u2032,j\u03b3 t d,i\u2032 \u03b2ti,j\u03b3 t d,i \u2265\n1 2 f\u0303d,j \u03b2ti,j\u03b3 t d,i \u03b2ti,j\u03b3 t d,i \u2265 (1\u2212 \u01eb) 1 2 \u03b2\u2217i,j\u03b3 \u2217 d,i\nHence, after the update,\n\u03b2t+1i,j \u2265 (1\u2212 \u01eb) 1\n2 \u03b2\u2217i,j\n\u2211\nd \u03b3 \u2217 d,i\n\u2211\nd \u03b3 t d,i\n\u2265 1 4 \u03b2\u2217i,j\nsince \u03b3td,i \u2264 2\u03b3\u2217d,i. The upper bound is similar. Since \u2211\ni\u2032 \u03b2 t i\u2032,j\u03b3d,i\u2032 \u2265 \u03b2ti,j\u03b3td,i,\nf\u0303d,j f td,j \u03b2ti,j\u03b3 t d,i \u2264 f\u0303d,j \u2264 (1 + \u01eb)\u03b2\u2217i,j\u03b3\u2217d,i\nHence,\n\u03b2t+1i,j \u2264 (1 + \u01eb)\u03b2\u2217i,j \u2211 d \u03b3 \u2217 d,i \u2211\nd \u03b3 t d,i\n\u2264 2\u03b2\u2217i,j\nsince \u03b3td,i \u2265 12\u03b3\u2217d,i. This certainly implies the claim we want.\nFurthermore, the following simple application of Lemma 27 is immediate and useful:\nLemma 31. Let t > 10max(logN, log 1\u03b3\u2217min , log 1\u03b2\u2217min ). Then, \u03b3td,i \u2265 p2\u03b3\u2217d,i.\nC.3 Discriminative words\nWe established in the previous section that after logarithmic number of steps, the anchor words will be correctly identified, and estimated within a factor of 2. We show that this is enough to cause the support of the discriminative words to be correctly identified too, as well as estimate them to within a constant factor where they are non-zero.\nSame as before, we will assume in this section that we can identify the dominating topic. We will crucially rely on the fact that the discriminative words will not have a very large dynamic range comparatively to their total probability mass in a topic. The high level outline will be similar to the case for the anchor words. We will prove that if a discriminative word j is in the support of topic i, then \u03b2ti,j will always be reasonably lower bounded, and this will cause the values \u03b2ti\u2032,j to keep decaying for the topics i \u2032 that the word j does not belong to. The reason we will need the bound on the dynamic range, and the proportion of the dominating topic, and the size of the dominating topic, is to ensure that the \u03b2\u2019s are always properly lower bounded.\nC.3.1 Bounds on the \u03b2ti,j values\nFirst, we show that because the discriminative words have a small range, the values \u03b2ti,j whenever \u03b2 \u2217 i,j is non-zero are always maintained to be within some multiplicative constant (which depends on the range of the \u03b2\u2217i,j).\nAs a preliminary, notice that having identified the anchor words correctly the \u03b3 values are appropriately lower bounded after running the \u03b3 update. Namely, by Lemma 31, \u03b3td,i \u2265 p/2\u03b3\u2217d,i\nWith this in hand, we show that the \u03b2ti,j values are well upper bounded whenever \u03b2 \u2217 i,j is non-zero.\nLemma 32. At any point in time t, \u03b2ti,j \u2264 (1 + \u01eb)2BCl \u03b2 \u2217 i,j.\nProof. Since f\u0303d,j ftd,j \u03b2ti,j\u03b3 t d,i \u2264 f\u0303d,j we have:\n\u03b2t+1i,j \u2264 \u2211 d f\u0303d,j \u2211\nd \u03b3 t d,i\n\u2264 2 \u00b7 \u2211 d f\u0303d,j \u2211\nd \u03b3 \u2217 d,i\nOn the other hand, we claim that f\u0303d,j \u2264 (1+ \u01eb)B\u03b2\u2217i,j . Indeed, f\u0303d,j \u2264 (1+ \u01eb) \u2211 i \u03b3 \u2217 d,i\u03b2 \u2217 i,j , and for any other\ntopic i\u2032, \u03b2\u2217i\u2032,j \u2264 B\u03b2\u2217i,j . Hence,\n2 \u00b7 \u2211 d f\u0303d,j \u2211\nd \u03b3 \u2217 d,i\n\u2264 2(1 + \u01eb)DB\u03b2\u2217i,j \u2211\nd \u03b3 \u2217 d,i\nHowever, since \u03b3\u2217d,i \u2265 Cl, the previous expression is at most\n2(1 + \u01eb)DB\u03b2\u2217i,j DCl = 2(1 + \u01eb)B Cl \u03b2\u2217i,j\nSo, we get the claim we wanted.\nThe lower bound on the \u03b2ti,j values is a bit more involved. To show a lower bound on the \u03b2 t i,j values is maintained, we will make use of both the fact that the discriminative words have a small range, and that we have some small, but reasonable proportion of documents where \u03b3\u2217d,i \u2265 1\u2212 \u03b4. More precisely, we show:\nLemma 33. Let \u03b2ti,j \u2264 2(1+\u01eb)BCl \u03b2 \u2217 i,j for all topics i that word j belongs to, and let \u03b2 t i,j \u2265 ClB \u03b2\u2217i,j. Then, \u03b2t+1i,j \u2265 ClB \u03b2\u2217i,j as well.\nProof. Let\u2019s call D\u03b4 the documents where \u03b3 \u2217 d,i \u2265 1\u2212 \u03b4. We can certainly lower bound\n\u03b2t+1i,j \u2265 \u2211 d\u2208D\u03b4 f\u0303d,j ftd,j \u03b3td,i\u03b2 t i,j \u2211\nd\u2208D \u03b3 t d,i\nFirst, let\u2019s focus on f\u0303d,j ftd,j \u03b2ti,j . Then,\nf\u0303d,j \u2265 (1 \u2212 \u01eb)(1\u2212 \u03b4)\u03b2\u2217i,j (C.3)\nFurthermore, since \u2211 d\u2208D\u03b4 \u03b3td,i \u2265 12 \u2211 d\u2208D\u03b4 \u03b3\u2217d,i and \u2211 d \u03b3 t d,i \u2264 2 \u2211 d \u03b3 \u2217 d,i, we have that\n\u2211\nd\u2208D\u03b4 \u03b3td,i\n\u2211\nd \u03b3 t d,i\n\u2265 1 4 8 B (1 \u2212 \u03b4) = 2 B (1\u2212 \u03b4) (C.4)\nFinally, we claim that \u03b2ti,j ftd,j \u2265 12 . Massaging this inequality a bit, we get it\u2019s equivalent to:\n\u03b2ti,j f td,j \u2265 1 2 \u21d4\nf ti,j \u2264 2\u03b2ti,j \u21d4\n\u03b3td,i\u03b2 t i,j +\n\u2211\ni\u2032\n\u03b3td,i\u2032\u03b2 t i\u2032,j \u2264 2\u03b2ti,j\nThe left hand side can be upper bounded by\n\u03b3td,i\u03b2 t i,j +\n\u2211\ni\u2032\n\u03b3td,i\u2032 2(1 + \u01eb)B3\nC2l \u03b2ti,j \u2264\n\u03b3td,i\u03b2 t i,j + (1 \u2212 \u03b3td,i)\n2(1 + \u01eb)B3\nC2l \u03b2ti,j\nby the assumptions of the lemma. So, it is sufficient to show that \u03b3td,i\u03b2 t i,j + (1 \u2212 \u03b3td,i)2(1+\u01eb)B 3\nC2l \u03b2ti,j \u2264 2\u03b2ti,j , however this is equivalent after\nsome rearrangement to \u03b3td,i \u2265 1\u2212 12(1+\u01eb)B3 C2 l \u22121 .\nIt\u2019s certainly sufficient for this that \u03b3td,i \u2265 1\u2212 1B3 C2 l\n= 1\u2212 C 2 l\nB3 , but since since \u03b3 \u2217 d,i \u2265 1\u2212 \u03b4, by the definition\nof \u03b4 and Lemmas 24, 35, 36, this certainly holds. Together with C.4 and C.3, we get that\n\u03b2t+1i,j \u2265 (1\u2212 \u01eb) 2 B (1\u2212 \u03b4)2 1 2 \u03b2\u2217i,j \u2265 (1 \u2212 \u01eb) (1\u2212 \u03b4)2 B \u03b2\u2217i,j\nBut, by our assumptions, (1\u2212 \u01eb)(1\u2212 \u03b4)2 \u2265 Cl, so the claim follows.\nC.3.2 Decreasing \u03b2ti\u2032,j values\nFinally, we show that if the discriminative word j does not belong in topic i\u2032, the value for \u03b2ti\u2032,j will keep dropping. More precisely, the following is true:\nLemma 34. Let word j and topic i be such that \u03b2\u2217i\u2032,j = 0 and let \u03b2 t i\u2032,j \u2264 b. Furthermore, let for all the topics i that j belongs to hold: \u03b2ti,j \u2265 1/C\u03b2\u03b2\u2217i,j for some constant C\u03b2. Finally, let \u03b3td,i \u2265 1C\u03b3 \u03b3 \u2217 d,i for some constant C\u03b3 . Then, \u03b2 t+1 i\u2032,j \u2264 b/2.\nProof. We proceed similarly as the analogous claim for anchor words. We split the update as\n\u03b2t+1i\u2032,j = \u03b2 t i\u2032,j(\n\u2211\nd\u2208D1 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n+\n\u2211\nd\u2208D2 f\u0303d,j ftd,j \u03b3td,i\u2032 \u2211\nd \u03b3 t d,i\u2032\n)\nfor some appropriate partitioning of the documents D1, D2. Namely, let D1 be documents which do not contain any topic to which word j belongs, the D2 documents which contain at least one topic word j belongs to.\nFor all the documents in D1, f \u2217 d,j = 0, and we will provide a good bound for the terms f\u0303d,j ftd,j in D2, this way, we\u2019ll ensure \u03b2ti,j gets multiplied by a quantity which is o(1) to get \u03b2 t+1 i,j , which is of course enough for what we want. Bounding the terms in D2 is even simpler than before. We have:\nf td,j = \u2211\ni\n\u03b2ti,j\u03b3 t d,i \u2265\n1\nC\u03b2C\u03b3\n\u2211\ni\n\u03b2\u2217i,j\u03b3 \u2217 d,i =\n1\nC\u03b2C\u03b3 f\u2217d,j\nHence, f\u2217d,j ftd,j \u2264 C\u03b2C\u03b3 . Then we have:\n\u2211 d f\u0303d,j ftd,j \u03b3td,i \u2211\nd \u03b3 t d,i\n\u2264 (1 + \u01eb) \u2211 d f\u2217d,j ftd,j \u03b3td,i \u2211\nd \u03b3 t d,i\n\u2264\n4(1 + \u01eb)\n\u2211\nd f\u2217d,j ftd,j\n\u03b3\u2217d,i \u2211\nd \u03b3 \u2217 d,i\n\u2264 4(1 + \u01eb) \u2211 d\u2208D2 C\u03b2C\u03b3\u03b3 \u2217 d,i \u2211\nd \u03b3 \u2217 d,i\nBut now, by the \u201dweak topic correlation\u201d property, \u2211 d\u2208D2 \u03b3\u2217d,i \u2211\nd \u03b3 \u2217 d,i = o(1). Indeed, D consists of the documents\nwhere i\u2032 is the dominating topic. In order for the document to belong to D2, at least one of the topics word j belongs to must belong in the document as well. Since the word j only belongs to o(K) of the topics, and each document contains only a constant number of topics, by the small topic correlation property, the claim we want follows.\nBut then, clearly, 4 \u2211 d\u2208D2 C\u03b2C\u03b3\u03b3 \u2217 d,i \u2211\nd \u03b3 \u2217 d,i = o(1) as well.\nHence, \u03b2t+1i\u2032,j = o(1)\u03b2 t i\u2032,j \u2264 12\u03b2ti\u2032,j , which is what we need.\nC.4 Determining dominant topic and parameter range\nTo complete the proofs of the claims for Phase I and II, we need to show that at any point in time we correctly identify the dominant topic. Furthermore, in order to maintain the lower bounds on the estimates for the discriminative words, we will need to make sure that \u03b3td,i is large as well in the documents where \u03b3\u2217d,i \u2265 1\u2212 \u03b4.\nLet\u2019s proceed to the problem of detecting the largest topic first. By Lemma 25 all we need to do is bound Rf and R\u03b2 at any point in time during this phase. To do this, let\u2019s show the following lemma:\nLemma 35. Suppose for the anchor words \u03b2ti,j \u2265 C1\u03b2\u2217i,j, for the discriminative words \u03b2ti,j \u2265 C2\u03b2\u2217i,j . Let pi be the proportion of anchor words in topic i. Then, KL(\u03b2\u2217i ||\u03b2ti ) \u2264 pi log( 1C1 ) + (1 \u2212 pi) log( 1 C2 ).\nProof. This is quite simple. Since log is an increasing function,\nKL(\u03b2\u2217i ||\u03b2ti ) = \u2211\nj\n\u03b2\u2217i,j log( \u03b2\u2217i,j \u03b2ti,j ) \u2264 pi log( 1 C1 ) + (1 \u2212 pi) log( 1 C2 )\nLemma 36. Suppose for the anchor words \u03b2ti,j \u2265 C1\u03b2\u2217i,j, for the discriminative words \u03b2ti,j \u2265 C2\u03b2\u2217i,j. Let pi be the proportion of anchor words in topic i. Then, min\u03b3\u2208\u2206K KL(f\u0303d||fd) \u2264 log(1+\u01eb)+ ( p log( 1C1 ) + (1\u2212 p) log( 1 C2 ) ) .\nProof. Also simple. The value of KL(f\u0303d||fd) one gets by plugging in \u03b3d = \u03b3\u2217 is exactly what is stated in the lemma.\nWe\u2019ll just use the above two lemmas combined from our estimates from before. We know, for all the anchor words, that \u03b2ti,j \u2265 Cl\u03b2\u2217i,j , and that for the discriminative words, \u03b2ti,j \u2265 ClB \u03b2\u2217i,j . Hence, by Lemma 35, at any point in time KL(\u03b2\u2217i ||\u03b2ti) \u2264 p log( 1Cl ) + (1\u2212 p) log( B Cl ). So, by Lemma 25, it\u2019s enough that\nCl \u2212 Cs \u2265 1\np\n( \u221a\n2\n(\np log( 1\nCl ) + (1\u2212 p) log(BCl)\n)\n+ \u221a log(1 + \u01eb)\n)\n+ \u01eb\n.\nSince 1p\n\u221a\n2 ( p log( 1Cl ) + (1\u2212 p) log(BCl) ) \u2264 1p \u221a 2 ( log( 1Cl ) + (1\u2212 p) logB ) , to get a sense of the pa-\nrameters one can achieve, for detecting the dominant topic, (ignoring \u01eb contributions), it\u2019s sufficient that Cl \u2212 Cs \u2265 2p \u221a\nmax(log( 1Cl ), (1\u2212 p) logB) If one thinks of Cl as 1\u2212\u03b7 and p \u2265 1\u2212 \u03b7logB , since log( 1Cl ) \u2248 \u03b7 roughly we want that Cl\u2212Cs \u226b 2 p \u221a \u03b7. (One takeaway message here is that the weight we require to have on the anchors depends only logarithmically on the range B.)\nLet\u2019s finally figure out what the topic proportions must be in the \u201dheavy\u201d documents. In these, we want\n\u03b3\u2217d,i \u2265 1\u2212 C2l 2B3 + 1 p\n( \u221a\n2 ( p log( 1Cl ) + (1\u2212 p) log(BCl) ) \u2212 \u221a log(1 + \u01eb)\n)\n+ \u01eb. A similar approximation to the\nabove gives that we roughly want \u03b3\u2217d,i \u2265 1\u2212 1\u22122\u03b72B3 + 2p \u221a \u03b7.\nC.5 Getting the supports correct\nAt the end of the previous section, we argued that after O(logN) rounds, we will identify the anchor words correctly, and the supports of the discriminative words as well. Furthremore, we will also have estimated the values of the non-zero discriminative word probabilities, as well the anchor word probabilities up to a multiplicative constant. Then, I claim that from this point onward at each of the \u03b3 steps, the \u03b3t values we get will have the correct support. Namely, the following is true:\nLemma 37. Suppose for the anchor words and discriminative words j, if \u03b2\u2217i,j = 0, it\u2019s true that \u03b2 t i,j = o( 1 n ). Furthermore, suppose that if \u03b2\u2217i,j 6= 0, 1C\u03b2 \u03b2 \u2217 i,j \u2264 \u03b2ti,j \u2264 C\u03b2\u03b2\u2217i,j for some constant C\u03b2.\nThen, when performing KL minimization with respect to the \u03b3 variables, whenever \u03b3\u2217d,i = 0 we have\n\u03b3td,i = 0.\nProof. Let \u03b3\u2217d,i = 0. If \u03b3 t d,i 6= 0, then the KKT conditions imply:\nN \u2211\nj=1\nf\u0303d,j f td,j \u03b2ti,j = 1 (C.5)\nThe only terms that are non-zero in the above summation are due to words j that belong to at least one topic i\u2032 in the document. Let I be the set of words that belong to topic i as well.\nBy Lemma 31, we know that \u03b3td,i \u2265 p/2\u03b3\u2217d,i Since also \u03b2ti,j \u2265 1C\u03b2 \u03b2 \u2217 i,j , f t d,i \u2265 p2C\u03b2 f \u2217 d,j. Since \u03b2 t i,j = o( 1 n ) for\nwords j not in the support of topic I, \u2211\nj /\u2208I\nf\u0303d,j f td,j \u03b2ti,j = o(1).\nOn the other hand, for words in I, f\u0303d,j ftd,j \u03b2ti,j \u2264 (1+ \u01eb) 2C2\u03b2 p \u03b2 \u2217 i,j , so \u2211 j\u2208I f\u0303d,j ftd,j \u03b2ti,j = o(1), by the small support\nintersection property. However, this contradicts C.5, so we get what we want.\nThis means that after this phase, we will always correctly identify the supports of the \u03b3 variables as well.\nC.6 Alternating minimization\nNow, finishing the proof of Theorem 3 is trivial. Namely, because of Lemmas 37, 29, and the analogue of 29, we are basically back to the case where we have the correct supports for both the \u03b2 and \u03b3 variables. The only thing left to deal with is the fact that the \u03b2 variables are not quite zero.\nLet j be an anchor word for topic i. Let \u01eb\u2032\u2032 = 1\u2212 (1 \u2212 \u01eb\u2032)1/7. Similarly as in Lemma 31, for\nt > 10max(logN, log( 1\n\u01eb\u2032\u2032\u03b3\u2217min ), log(\n1\n\u01eb\u2032\u2032\u03b2\u2217min ))\nit holds that f\u2217d,j ftd,j \u2265 (1\u2212 \u01eb\u2032)1/7 \u03b2 \u2217 i,j\u03b3 \u2217 d,i \u03b2td,i\u03b3 t d,i . The same inequality is true if j is a lone word for topic i in document d. After the above event, the same proof from Case Study 1 implies that after O(log( 1\u01eb\u2032 )) iterations we\u2019ll get 1\n1 + \u01eb\u2032 \u03b2\u2217i,j \u2264 \u03b2ti,j \u2264 (1 + \u01eb\u2032)\u03b2\u2217i,j\nand 1\n1 + \u01eb\u2032 \u03b3\u2217i,j \u2264 \u03b3ti,j \u2264 (1 + \u01eb\u2032)\u03b3\u2217i,j\nThis finishes the proof of Theorem 3."}, {"heading": "D Justification of prior assumptions", "text": "In this section we provide a brief motivation for our choice of properties on the topic model instances we are looking at. Nothing in the other sections crucially depends on this section, so it can be freely skipped upon first reading.\nMost of our properties on the topic priors are inspired from what happens with the Dirichlet prior - specifically, variants of all of the \u201dweak correlations\u201d between topics hold for Dirichlet. Essentially the only difference between our assumptions and Dirichlet is the lack of smoothness. (Dirichlet is sparse, but only in the sense that it leads to a few \u201dlarge\u201d topics, but the other topics may be non-negligible as well.)\nTo the best of our knowledge, the lemmas proven here were not derived elsewhere, so we include them for completeness.\nFor all of the claims below, we will be concerned with the following scenario: ~\u03b3 = (\u03b31, \u03b32, . . . , \u03b3K) will be a vector of variables, and ~\u03b1 = (\u03b11, \u03b12, . . . , \u03b1k) a vector of parameters. We\nwill let ~\u03b3 be distributed as ~\u03b3 := Dir(\u03b11, \u03b12, . . . , \u03b1k), where \u03b1i = Ci/K c, for some constants Ci and c > 1.\nD.1 Sparsity\nTo characterize the sparsity of the topic proportions in a document, we will need the following lemma from (Telgarsky, 2013):\nLemma 38. (Telgarsky, 2013) For a Dirichlet distribution with parameters (C1/k c, C2/k c, . . . , Ck/k c), the probability that there are more than c0 ln k coordinates in the Dirichlet draw that are \u2265 1/kc0 is at most 1/kc0.\nIt\u2019s clear how this is related to our assumption: if one considers the coordinates \u2265 1kc0 as \u201dlarge\u201d, we assume, in a similar way, that there are only a few \u201dlarge\u201d coordinates. The difference is that we want the rest of the coordinates to be exactly zero.\nD.2 Weak topic correlations\nWe will prove that the Dirichlet distribution satisfies something akin to the weak topic correlations property. We prove that when conditioning on some small (o(K)) set of topics being small, the marginal distributions for the rest of the topic proportions are very close to the original ones. This implies our \u201dweak topic correlations\u201d property.\nThe following is true:\nLemma 39. Let ~\u03b3 = (\u03b31, \u03b32, . . . , \u03b3K) be distributed as specified above. Let S be a set of topics of size o(K), and let\u2019s denote by \u03b3S the vector of variables corresponding to the topics in the set S, and \u03b3S\u0304 the rest of the coordinates. Furthermore, let\u2019s denote by \u03b3\u0303S\u0304 the distribution of \u03b3S\u0304 conditioned on all the coordinates of \u03b3S being at most 1/K\nc1 for c1 > 1. Then, for any i \u2208 S\u0304 and \u03b3 = 1\u2212 \u03b4, any \u03b4 = \u2126(1), P\u03b3S\u0304(\u03b3i = \u03b3) = (1 \u00b1 o(1))P\u03b3\u0303S\u0304 (\u03b3i = \u03b3).\nProof. It\u2019s a folklore fact that if ~Y = Dir(~\u03b1), then\n(Y1, Y2, . . . , Yi\u22121, Yi+1, . . . , YK |Yi = yi) = (1 \u2212 yi)Dir(\u03b11, \u03b12, . . . , \u03b1i\u22121, \u03b1i+1, . . . , \u03b1K)\nApplying this inductively, we get that \u03b3\u0303S\u0304 = (1 \u2212 \u2211 j\u2208S \u03b3i)Dir(~\u03b1S\u0304). Let\u2019s denote s := \u2211\nj\u2208S \u03b3i, and s\u0303 = \u2211\ni\u2208S \u03b1i. Then, since \u03b3i \u2264 1/Kc1 for i \u2208 S, s = o(1). Similarly, s\u0303 = o(1). For notational convenience, let\u2019s call \u03b1\u03030 = \u2211 i/\u2208S \u03b1i, and \u03b10 = \u2211 i \u03b1i = \u03b1\u03030 + s\u0303. The marginal distribution of variable Yi where ~Y = Dir(~\u03b1) is Beta(\u03b1i, \u03b10 \u2212 \u03b1i). Hence,\nP\u03b3S\u0304 (\u03b3i = \u03b3) = 1\nB(\u03b1i, \u03b1\u03030 + s\u0303\u2212 \u03b1i) \u03b3\u03b1i\u22121(1 \u2212 \u03b3)\u03b1\u03030+s\u0303\u2212\u03b1i\u22121\nand\nP\u03b3\u0303S\u0304 (\u03b3i = \u03b3) = 1\nB(\u03b1i, \u03b1\u03030 \u2212 \u03b1i) (\n\u03b3 1\u2212 s ) \u03b1i\u22121 (1\u2212 \u03b3 1\u2212 s ) \u03b1\u03030\u2212\u03b1i\u22121\nThe following holds:\n\u03b3\u03b1i\u22121(1\u2212 \u03b3)\u03b1\u03030+s\u0303\u2212\u03b1i\u22121 ( \u03b31\u2212s ) \u03b1i\u22121(1\u2212 \u03b31\u2212s )\u03b1\u03030\u2212\u03b1i\u22121 =\n(1 \u2212 s)\u03b1i\u22121 ( (1 \u2212 s)(1\u2212 \u03b3) 1\u2212 s\u2212 \u03b3\n)\u2212\u03b1i\u22121\n(1 \u2212 \u03b3)s\u0303 =\n( 1 + s\n1\u2212 s\u2212 \u03b3\n)\u2212\u03b1i\u22121\n(1\u2212 \u03b3)s\u0303\nNow, I claim the above expression is 1\u00b1 o(1). We\u2019ll just prove this for each of the terms individually. Since 1 + s1\u2212s\u2212\u03b3 \u2265 1 and \u22121 \u2212 \u03b1i \u2264 \u22121, it follows that (1 + s1\u2212s\u2212\u03b3 ) \u2212\u03b1i\u22121 \u2264 1. On the other hand, by Bernoulli\u2019s inequality, (1 + s1\u2212s\u2212\u03b3 )\u2212\u03b1i\u22121 \u2265 1\u2212 (\u03b1i + 1) s1\u2212s\u2212\u03b3 \u2265 1\u2212 o(1), since \u03b3 = 1\u2212 \u03b4, for some constant \u03b4, by our assumptions. For the second term, since 1 \u2212 \u03b3 \u2264 1 and s\u0303 \u2265 0, (1 \u2212 \u03b3)s\u0303 \u2264 1. On the other hand, again by Bernoulli\u2019s inequality, (1 \u2212 \u03b3)s\u0303 \u2265 1\u2212 \u03b3s\u0303 = 1\u2212 o(1), as we needed. Comparing B(\u03b1i, \u03b1\u03030 + s\u0303\u2212\u03b1i) and B(\u03b1i, \u03b1\u03030 \u2212\u03b1i) is not so much more difficult. By definition, B(\u03b1i, \u03b10\u2212 \u03b1i) = \u222b 1 0 x\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx, so\nB(\u03b1i, \u03b10 + s\u0303\u2212 \u03b1i) B(\u03b1i, \u03b10 \u2212 \u03b1i) =\n\u222b 1\n0 x\u03b1i\u22121(1 \u2212 x)\u03b1\u03030+s\u0303\u2212\u03b1i\u22121 dx \u222b 1\n0 x \u03b1i\u22121(1 \u2212 x)\u03b1\u03030\u2212\u03b1i\u22121 dx\nWe\u2019ll just bound each of the ratios x\u03b1i\u22121(1 \u2212 x)\u03b1\u03030+s\u0303\u2212\u03b1i\u22121 x\u03b1i\u22121(1 \u2212 x)\u03b1\u03030\u2212\u03b1i\u22121\nNamely, this is just (1 \u2212 x)s\u0303. Same as above, 1 \u2212 o(1) \u2264 (1 \u2212 \u03b3)s\u0303 \u2264 1. Hence, these are within a constant from each other.\nD.3 Dominant topic equidistribution\nNow, we pass to proving a smooth version of the dominant topic equidistribution property. Namely, for a threshold x0 = o(1), we can consider a topic \u201dlarge\u201d whenever it\u2019s bigger than x0. We will show that for any topics Yi, Yj , the probabilities that Yi > x0 and Yj > x0 are within a constant from each other.\nMathematically formalizing the above statement, we will prove the following lemma:\nLemma 40. Let ~\u03b3 = (\u03b31, \u03b32, . . . , \u03b3K) be distributed as specified above. Then, P(Yi>x0) P(Yj>x0) = O(1), for any i, j if x0 = o(1).\nProof. As before, the marginal distribution of Yi is Beta(\u03b1i, \u03b10 \u2212 \u03b1i). The Beta distribution pdf is just P(x) = x \u03b1i\u22121(1\u2212x)\u03b10\u2212\u03b1i\u22121\nB(\u03b1i,\u03b10\u2212\u03b1i) , where B(\u03b1i, \u03b10 \u2212 \u03b1i) =\n\u222b 1\n0 x\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx.\nHence, the ratio we care about can be written as\n( \u222b 1 x0 x\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx)/B(\u03b1i, \u03b10 \u2212 \u03b1i) ( \u222b 1\nx0 x\u03b1j\u22121(1\u2212 x)\u03b10\u2212\u03b1j\u22121 dx)/B(\u03b1j , \u03b10 \u2212 \u03b1j)\nTo get a bound on this ratio, it\u2019s sufficient to bound the normalization constants B(\u03b1i, \u03b10 \u2212 \u03b1i) and B(\u03b1j , \u03b10\u2212\u03b1j), as well as the ratio \u222b 1 x0 x\u03b1i\u22121(1\u2212x)\u03b10\u2212\u03b1i\u22121 dx \u222b\n1 x0\nx\u03b1j\u22121(1\u2212x)\u03b10\u2212\u03b1j\u22121 dx . Let\u2019s prove first that B(\u03b1i, \u03b10\u2212\u03b1i) \u2243 B(\u03b1j , \u03b10\u2212\n\u03b1j)\nBy definition, B(\u03b1i, \u03b10 \u2212 \u03b1i) = \u222b 1 0 x\u03b1i\u22121(1 \u2212 x)\u03b10\u2212\u03b1i\u22121 dx. The way we\u2019ll analyze this quantity is that\nwe\u2019ll divide the integral in two parts, one from 0 to 12 and one from 1 2 to 1.\nSince \u03b10 = O(1), it follows that \u03b10 \u2212 \u03b1i \u2212 1 & \u22121 and \u03b10 \u2212 \u03b1i \u2212 1 . 1. Hence, (1 \u2212 x)\u03b10\u2212\u03b1i\u22121 = \u0398(1). It follows that\n\u222b 1 2\n0\nx\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx \u2243 \u222b 1 2\n0\nx\u03b1i\u22121 dx =\n\u2243 (1/2) \u03b1i \u03b1i \u2243 1 \u03b1i\nwhere the last equality follows since 12 \u2264 (1/2)\u03b1i \u2264 1. The second portion is not much more difficult. Since 12 \u2264 12\n\u03b1i\u22121 \u2264 1, it follows \u222b 1\n1 2\nx\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx \u2243 \u222b 1\n1 2\n(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx =\n\u2243 (1/2) \u03b10\u2212\u03b1i \u03b10 \u2212 \u03b1i \u2243 1 \u03b10\nwhere the last two equalities come about since \u22121 . \u03b10 \u2212 \u03b1i . 1. But the above two estimates proved that for any i, B(\u03b1i, \u03b10 \u2212 \u03b1i) \u2243 1\u03b1i , as we needed. So, we proceed onto bounding\n\u222b 1\nx0 x\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx\n\u222b 1\nx0 x\u03b1j\u22121(1\u2212 x)\u03b10\u2212\u03b1j\u22121 dx\nWe\u2019ll proceed in a similar fashion as before. We\u2019ll pick some point xT , and if x < xT , we will show that x\u03b1j\u22121(1\u2212x)\u03b10\u2212\u03b1j\u22121 is within a constant factor from x\u03b1i\u22121(1\u2212x)\u03b10\u2212\u03b1i\u22121. On the other hand, we will show that part of the integral where x > xT is dominated by the part where x < xT , which will imply the claim we need.\nLet\u2019s rewrite the ratio above a little:\nx\u03b1j\u22121(1\u2212 x)\u03b10\u2212\u03b1j\u22121 x\u03b1i\u22121(1\u2212 x)\u03b10\u2212\u03b1i\u22121 =\n( x\n1\u2212 x ) \u03b1j\u2212\u03b1i = e(\u03b1j\u2212\u03b1i) ln(\nx 1\u2212x )\nProceeding as outlined, I claim that for sufficiently large constants C1, C2, s.t. if x \u2264 1 \u2212 1 1+C1e 1 \u03b1i 1 C2 ,\nthen x \u03b1j\u22121(1\u2212x)\u03b10\u2212\u03b1j\u22121\nx\u03b1i\u22121(1\u2212x)\u03b10\u2212\u03b1i\u22121 = O(1). Let\u2019s call xT = 1\u2212 1\n1+C1e 1 \u03b1i 1 C2\n.\nThe claim is then, that if xT \u2265 x \u2265 x0, that (\u03b1j \u2212 \u03b1i) ln( x1\u2212x ) = O(1). First let\u2019s assume, \u03b1j \u2212 \u03b1i \u2265 0. Then, if ln( x1\u2212x) < 0 \u21d4 x < 12 , the condition is of course satisfied. So let\u2019s assume x \u2265 12 . When\n1 2 \u2264 x \u2264 xT , we get that x1\u2212x \u2264 C1ee\n1 \u03b1j 1 C2\n. Hence, ln( x1\u2212x ) \u2264 lnC1 + 1\u03b1j 1 C2 . It follows that if C1, C2 are\nsufficiently large,\n( x 1\u2212 x ) \u03b1j\u2212\u03b1i \u2264 eln( x1\u2212x )\u03b1j = O(1)\nOn the other hand, if \u03b1j \u2212 \u03b1i \u2264 0, when x \u2265 12 , (\u03b1j \u2212 \u03b1i) ln( x1\u2212x ) \u2264 0, so we are fine. However, since |\u03b1j \u2212 \u03b1i| \u2264 \u03b1i, it\u2019s easy to check when x \u2265 e \u2212c1/\u03b1i\n1+e\u2212c1/\u03b1i > x0, that (\u03b1j \u2212 \u03b1i) ln( x1\u2212x ) = O(1).\nFinally, we want to claim that the portion of the integral from xT to 1 is dominated by the portion from x0 to xT .\nWe can show that the latter portion is O(e\u2212K), and the first is \u2126(1). Let\u2019s lower bound the first portion. We lower bound\n\u222b xT x0 x\u03b1i\u22121(1 \u2212 x)\u03b10\u2212\u03b1i\u22121 dx by x\u03b1i\u22121T \u222b xT x0 (1 \u2212 x)\u03b10\u2212\u03b1i\u22121 dx. For the first factor in the above expression, we use Bernoulli\u2019s inequality to prove it\u2019s \u2126(1). For the second, the integral will evaluate to\n(1 \u2212 x0)\u03b10\u2212\u03b1i \u2212 (1\u2212 xT )\u03b10\u2212\u03b1i \u03b10 \u2212 \u03b1i\nLet\u2019s lower bound the first term in the numerator. If \u03b10 \u2212 \u03b1i \u2265 1, another application of Bernoulli\u2019s inequality gives: (1 \u2212 x0)\u03b10\u2212\u03b1i \u2265 1 \u2212 (\u03b10 \u2212 \u03b1i)x0 \u2265 1 \u2212 o(1). If, on the other hand, 0 \u2264 \u03b10 \u2212 \u03b1i \u2264 1, (1\u2212 x0)\u03b10\u2212\u03b1i \u2265 1\u2212 x0 \u2265 1\u2212 o(1).\nThen, I claim that (1 \u2212 xT )\u03b10\u2212\u03b1i = e\u2212\u2126(K). Indeed, for some constant C3, (\n1\n1 + C1e 1 \u03b1j 1 C2\n)\u03b10\u2212\u03b1i\n\u2264 (\n1\nC3e 1 \u03b1j 1 C2\n)\u03b10\u2212\u03b1i\n=\n= e\u2212 ln(C3e 1 \u03b1j 1 C2 )(\u03b10\u2212\u03b1i)\nHowever, since \u03b10 = \u2126(K\u03b1j) and \u03b10\u2212\u03b1i = \u2126(\u03b10), the above expression is upper bounded by e\u2212\u2126(K), which is what we were claiming. Hence, x\u03b1i\u22121T \u222b xT x0\n(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx = \u2126(1). Let\u2019s upper bound the latter portion. This expression is upper bounded by\nx\u03b1i\u22121T\n\u222b 1\nxT\n(1\u2212 x)\u03b10\u2212\u03b1i\u22121 dx = x\u03b1i\u22121T\n(\n1\n1+C1e 1 \u03b1j 1 C2\n)\u03b10\u2212\u03b1i\n\u03b10 \u2212 \u03b1i\nNow, we will separately bound each of x\u03b1i\u22121T and\n\n\n1\n1+C1e\n1 \u03b1j 1 C2\n\n\n\u03b10\u2212\u03b1i\n\u03b10\u2212\u03b1i .\nThe first term can be written as 1 x 1\u2212\u03b1i T . Now, since 1 \u2212 \u03b1i \u2265 0, we can use Bernoulli\u2019s inequality to\nlower bound x1\u2212\u03b1iT by 1 \u2212 1 1+C1e 1 \u03b1j 1 C2 (1 \u2212 \u03b1i). Since 1 1+C1e 1 \u03b1j 1 C2\n= O(1/e 1 \u03b1j ), and 1 \u2212 \u03b1i \u2264 1/2, let\u2019s say,\n1\u2212 1 1+C1e 1 \u03b1j 1 C2 (1\u2212 \u03b1i) = \u2126(1), i.e. x\u03b1i\u22121T = O(1).\nFor the second term, we already proved above that (1\u2212xT )\u03b10\u2212\u03b1i = e\u2212\u2126(K), This implies that \u222b 1 xT x\u03b1i\u22121(1\u2212\nx)\u03b10\u2212\u03b1i\u22121 dx = O(e\u2212K), which finishes the proof.\nD.4 Independent topic inclusion\nFinally, there\u2019s a very simple proxy for \u201dindependent topic inclusion\u201d. Again, as above, \u03b3\u0303S\u0304 = (1 \u2212 \u2211\nj\u2208S \u03b3i)Dir(~\u03b1S\u0304).\nBut, if we consider \u201dinclusion\u201d the probability that a given topic is \u201dnoticeable\u201d (i.e. \u2265 1nc0 , say), we can use the above Lemma 40 to show that the probability that any topic is \u201dlarge\u201d (but still o(1)) is within a constant for all the topics in S\u0304."}, {"heading": "E On common words", "text": "In this section, we show how one would modify the proofs from the previous section to handle common words as well. We stress that common words are easy to handle if one were allowed to filter them out, but we want to analyze under which conditions the variational inference updates could handle them on their own.\nThe difference in contrast to the previous sections is it\u2019s not clear how to argue progress for the common words: common words do not have lone documents. However, if we can\u2019t argue progress for the common words, then we can\u2019t argue progress for the \u03b3 variables, so the entire argument seems to fail.\nFormally, we consider the following scenario:\n\u2022 On top of the assumptions we have either in Case Study 1 or Case Study 2, we assume that there are words which show up in all topics, but their probabilities are within a constant \u03ba from each other, B \u2265 \u03ba \u2265 2. We will call these common words. (The \u03ba \u2265 2 is without loss of generality. If the claim holds for a smaller \u03ba, then it certainly holds for \u03ba = 2. The only difference is that the estimates to follow could be strengthened, but we assume \u03ba \u2265 2 to get cleaner bounds.)\n\u2022 For each topic i, if C is the set of common words, \u2211j\u2208C \u03b2\u2217i,j \u2264 1\u03ba100 , i.e. there isn\u2019t too much mass on these words.\n\u2022 Conditioned on topic i being dominant, there is a probability of 1\u2212 1\u03ba100 that the proportion of topic i is at least 1\u2212 1\u03ba100 .\nThen, recall the theorem we want to prove is:\nTheorem 41. If we additionally have common words satisfying the properties specified above, after O(log(1/\u01eb\u2032)+ logN) of KL-tEM updates in Case Study 2, or any of the tEM variants in Case Study 1, and we use the same initializations as before, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + \u01eb\u2032, if 1 + \u01eb\u2032 \u2265 1(1\u2212\u01eb)7 .\nOur analysis here is fairly loose, since the result is anyway a little weak. (e.g. 1 \u2212 1\u03ba100 is not really the best value for the proportion of the dominating topic, or the proportion of such documents required.) At any rate, it will be clear from the proofs that the dependency of the dominating topic on \u03ba has to be of the form 1 \u2212 1\u03bac , so it\u2019s not clear one would gain too much from the tightest possible analysis. The reason we are including this section is to show cases where our proof methods start breaking down.\nWe will do the proof for Case Study 1 first, after which Case Study 2 will easily follow.\nE.1 Phase I with common words\nThe outline is the same as before. We prove the lower bounds on the \u03b3 and \u03b2 variables first. Namely, we prove:\nLemma 42. Suppose that the supports of \u03b2 and \u03b3 are correct. Then, \u03b3td,i \u2265 12\u03b3\u2217d,i.\nProof. Similarly as before, multiplying both sides of B.1 by \u03b3td,i, we get that\n\u03b3td,i \u2265 \u2211\nLi\nf\u2217d,j f td,j \u03b2ti,j\u03b3 t d,i \u2265 (1\u2212 o(1))(1 \u2212 1 \u03ba100 )\u03b3\u2217d,i \u2265 1 2 \u03b3\u2217d,i\nwhere the second inequality follows since 1\u2212 1\u03ba100 fraction of the words in topic i is discriminative.\nLemma 43. Suppose that the supports of the \u03b3 and \u03b2 variables are correct. Additionally, if i is a large topic in d, let 12\u03b3 \u2217 d,i \u2264 \u03b3td,i \u2264 3\u03b3\u2217d,i. Then, for a discriminative word j for topic i, \u03b2t+1i,j \u2265 13\u03b2\u2217i,j.\nProof. Again, similarly as in Lemma 8,\n\u03b2t+1i,j \u2265 \u2211 d\u2208Dl (1\u2212 \u01eb) \u03b3\n\u2217 d,i\u03b2 \u2217 i,j\n\u03b3td,i\u00b7\u03b2 t i,j \u03b2ti,j\u03b3 t d,i\n\u2211D d=1 \u03b3 t d,i\n=\n(1\u2212 \u01eb)\u03b2\u2217i,j \u2211D d\u2208Dl \u03b3\u2217d,i\n\u2211D d=1 \u03b3 t d,i\nIn the documents where topic i is the largest, \u03b3td,i \u2264 3\u03b3\u2217d,i. So, we can conclude\n\u03b2t+1i,j \u2265 (1\u2212 \u01eb)\u03b2\u2217i,j 1\n3\n\u2211D d\u2208Dl \u03b3\u2217d,i \u2211D\nd=1 \u03b3 \u2217 d,i\nSince \u2211D d\u2208Dl \u03b3\u2217d,i\n\u2211D d=1 \u03b3 \u2217 d,i\n\u2265 (1\u2212 o(1)), as before, we get what we want.\nLemma 44. Let the \u03b2 variables have the correct support. Let j be a discriminative word for topic i, and let \u03b2ti,j \u2265 1Cm\u03b2 \u2217 i,j, \u03b3 t d,i \u2265 1Cm \u03b3 \u2217 d,i whenever \u03b2 \u2217 i,j 6= 0, \u03b3\u2217d,i 6= 0. Let \u03b2ti,j = Ct\u03b2\u03b2\u2217i,j , where Ct\u03b2 \u2265 4Cm, and Cm is a constant. Then, in the next iteration, \u03b2t+1i,j \u2264 Ct+1\u03b2 \u03b2\u2217i,j, where Ct+1\u03b2 \u2264 Ct\u03b2 2 .\nProof. The proof is exactly the same as Lemma 9.\nNow, we finally get to the upper bound of the \u03b3 values.\nLemma 45. Fix a particular document d. Let\u2019s assume the supports for the \u03b2 and \u03b3 variables are correct. Furthermore, let 1Cm \u2264 \u03b2ti,j \u03b2\u2217i,j \u2264 Cm for some constant Cm. Then, \u03b3td,i \u2264 2\u03b3\u2217d,i.\nProof. Again, multiplying B.1 by \u03b3td,i, we get\n\u03b3td,i = \u2211\nj\u2208Li\nf\u0303d,j + \u03b3 t d,i\n\u2211\nj /\u2208Li\nf\u0303d,j f td,j \u03b2ti,j + \u03b3 t d,i \u2211\nj\u2208C\nf\u0303d,j f td,j \u03b2ti,j\nIf \u03b1\u0303 = \u2211 j\u2208Li \u03b2\u2217i,j , since \u03b3 t d,i \u2265 1Cm \u03b3 \u2217 d,i,\nf\u0303d,j f td,j \u2264 (1 + \u01eb)C2m\nIf we denote \u0393 = \u2211 j\u2208C \u03b2 \u2217 i,j , then\n\u03b3td,i \u2264 (1 + \u01eb)(\u03b1\u0303\u03b3\u2217d,i + C3m(1\u2212 \u0393\u2212 \u03b1\u0303)\u03b3td,i + \u0393\u03ba4\u03b3td,i)\nEquivalently, \u03b3td,i \u2264 (1+\u01eb)\u03b1\u03031\u2212(1+\u01eb)C3m(1\u2212\u0393\u2212\u03b1\u0303)\u2212(1+\u01eb)\u0393\u03ba4 \u03b3 \u2217 d,i Then, we claim that (1+\u01eb)\u03b1\u03031\u2212(1+\u01eb)C3m(1\u2212\u0393\u2212\u03b1\u0303)\u2212(1+\u01eb)\u0393\u03ba4 \u2264 1 + 1\u03ba50 . Indeed, \u0393\u03ba4 \u2264 \u03ba\u221296, and C3m(1 \u2212 \u0393 \u2212 \u03b1\u0303) \u2264\nC3m(1 \u2212 \u03b1\u0303) = o(1). Hence,\n(1 + \u01eb)\u03b1\u0303 1\u2212 (1 + \u01eb)C3m(1\u2212 \u0393\u2212 \u03b1\u0303)\u2212 (1 + \u01eb)\u0393\u03ba4 \u2264 (1 + \u01eb)\u03b1\u0303 1\u2212 o(1)\u2212 \u03ba\u221296 \u2264 (1 + \u01eb)\u03b1\u0303 1\u2212 \u03ba\u221295\nFinally, we claim that (1+\u01eb)\u03b1\u03031\u2212\u03ba\u221295 \u2264 1 + \u03ba\u221250. Indeed, this is equivalent to\n\u03b1\u0303 \u2264 (1 + \u01eb)(1 + \u03ba\u221250)(1\u2212 \u03ba\u221295) \u2264 (1 + \u01eb)(1 + \u03ba\u221250)\nBut, since we assume \u03ba \u2265 2, the claim we need follows easily.\nE.2 Phase II of analysis\nFinally, we deal with the alternating minimization portion of the argument. How will we deal with the lack of anchor documents? The almost obvious way: if a document has topic i with proportion 1 \u2212 1\u03ba100 , it will behave for all purposes like an anchor document, because the dynamic range of word \u03b2\u2217i,j is limited, and the contribution from the other topics is not that significant.\nIntuitively, we\u2019ll show that f\u2217d,j ftd,j \u2248 \u03b2 \u2217 i,j \u03b2ti,j , so that these documents provide a \u201dpush\u201d for the value of \u03b2ti,j in\nthe correct direction.\nLemma 46. Let\u2019s assume that our current iterates \u03b2ti,j satisfy 1 Ct\u03b2 \u2264 \u03b2i,j\u2217 \u03b2ti,j \u2264 Ct\u03b2 for Ct\u03b2 \u2265 1(1\u2212\u01eb)20 . Then, after iterating the \u03b3 updates to convergence, we will get values \u03b3td,i that satisfy (C t \u03b2)\n1/10 \u2264 \u03b3d,i\u2217 \u03b3td,i \u2264 (Ct\u03b2)1/10.\nProof. As before, we have that\n\u03b3td,i = \u2211\nj\u2208Li\nf\u0303d,j + \u03b3 t d,i\n\u2211\nj /\u2208Li\nf\u0303d,j f td,j \u03b2ti,j\nLet\u2019s denote as Ct\u03b3 = maxi(max( \u03b3\u2217d,i \u03b3td,i , \u03b3td,i \u03b3\u2217 d,i )), and let, as before, assume that \u03b3td,i0 \u03b3\u2217 d,i0 = Ct\u03b3 . By the definition of Ct\u03b3 ,\n\u03b3td,i0 = \u2211\nj\u2208Li0\nf\u0303d,j + \u03b3 t d,i0\n\u2211\nj /\u2208Li0\nf\u0303d,j f td,j \u03b2ti0,j \u2264\n(1 + \u01eb)(\u03b1\u0303\u03b3\u2217d,i0 + (1\u2212 \u03b1\u0303)(Ct\u03b2)2(Ct\u03b3)2\u03b3\u2217d,i0) We claim that (1 + \u01eb)(\u03b1\u0303+ (1 \u2212 \u03b1\u0303)(Ct\u03b2)2(Ct\u03b3)2) \u2264 (Ct\u03b3)1/10 (E.1) which will be a contradiction to the definition of Ct\u03b3 . After a little rewriting, E.1 translates to \u03b1\u0303 \u2265 1 \u2212 (Ct\u03b3 ) 1/10 1+\u01eb \u22121\n(Ct \u03b2 Ct\u03b3)\n2\u22121 . By our assumption on C t \u03b3 , C t \u03b2 \u2264 C10\u03b3 , so\nthe right hand side above is upper bounded by 1\u2212 (Ct\u03b3 ) 1/10 1+\u01eb \u22121\n(Ct\u03b3) 8\u22121 .\nBut, Lemma 45 implies that certainly Ct\u03b3 \u2264 C0\u03b3 . The function\nf(c) = c1/10 1+\u01eb \u2212 1 c8 \u2212 1\ncan be easily seen to be monotonically decreasing on the interval of interest, and hence is lower bounded by (C0\u03b3 ) 1/10\n1+\u01eb \u22121\n(C0\u03b3) 8\u22121 . Since \u03b1\u0303 = (1 \u2212 o(1))(1 \u2212 1\u03ba100 ) and C0\u03b3 \u2264 3, the claim we want is clearly true.\nThe case where \u03b3\u2217d,i0 \u03b3td,i0 = Ct\u03b3 is not much more difficult. An analogous calculation as in Lemma 12 gives\nthat to get a contradiction to the definition of Ct\u03b3 , the condition required is that 1\u2212 1\u2212 1 (1\u2212\u01eb)(Ct\u03b3 ) 1/10\n1\u2212 1 (Ct\u03b3 )\n8 . As before,\nif f(c) = 1\u2212 1 (1\u2212\u01eb)c1/10\n1\u2212c8 , it s easy to check that f(c) is monotonically increasing in the interval of interest, so lower bounded by\n1\u2212 1 (1\u2212\u01eb)( 1\n(1\u2212\u01eb)20 )1/10\n1\u2212 1 (( 11\u2212\u01eb ) 20)8\n=\n1\u2212 (1 \u2212 \u01eb) 1\u2212 (1\u2212 \u01eb)160 \u2265 1 160\nBut, \u03b1\u0303 \u2265 (1\u2212 1\u03ba100 )(1\u2212 o(1)) \u2265 1\u2212 1160 , so we get what we want.\nNext, we show the following lemma.\nLemma 47. Suppose at time step t, 1Ct\u03b3 \u03b3\u2217d,i \u2264 \u03b3td,i \u2264 Ct\u03b3\u03b3\u2217d,i and 1Ct\u03b2 \u03b2 \u2217 i,j \u2264 \u03b2ti,j \u2264 Ct\u03b2\u03b2\u2217i,j , such that Ct\u03b3 \u2264 (Ct\u03b2) 1/10 for Ct\u03b2 \u2265 1(1\u2212\u01eb)20 . Then, at time step t+ 1, 1/C t+1 \u03b2 \u03b2 \u2217 i,j \u2264 \u03b2ti,j \u2264 Ct+1\u03b2 \u03b2\u2217i,j, where Ct+1\u03b2 = (Ct\u03b2)3/4\nProof. Let\u2019s assume a document d has a dominating topic of proportion at least 1\u2212 1/\u03ba100. Then, we claim that\nf\u2217d,j ftd,j \u2265 1 (Ct\u03b2) 1/4 \u03b2\u2217i,j \u03b2ti,j . We will do a sequence of rearrangements to get this condition to\na simpler form:\nf\u2217d,j f td,j \u2265 1 (Ct\u03b2) 1/4 \u03b2\u2217i,j \u03b2ti,j \u21d4\nf\u2217d,j \u03b2\u2217i,j \u2265 1 (Ct\u03b2) 1/4 f td,j \u03b2ti,j \u21d4\n\u03b3\u2217d,i + \u2211\ni\u2032\n\u03b3\u2217d,i\u2032 \u03b2\u2217i\u2032,j \u03b2\u2217i,j > 1 (Ct\u03b2) 1/4 (\u03b3td,i + \u2211\ni\u2032\n\u03b3td,i\u2032 \u03b2ti\u2032,j \u03b2ti,j )\nLet\u2019s upper bound the right hand side by some simpler quantities. We have:\n1\n(Ct\u03b2) 1/4\n(\u03b3td,i + \u2211\ni\u2032\n\u03b3td,i\u2032 \u03b2ti\u2032,j \u03b2ti,j ) \u2264\n1 (Ct\u03b2) 1/4 Ct\u03b3(\u03b3 \u2217 d,i + \u2211\ni\u2032\n\u03b3\u2217d,i\u2032 \u03b2ti\u2032,j \u03b2ti,j ) \u2264\n1\n(Ct\u03b2) 1/4\nCt\u03b3(\u03b3 \u2217 d,i + (C t \u03b2)\n2 \u2211\ni\u2032\n\u03b3\u2217d,i\u2032 \u03b2\u2217i\u2032,j \u03b2\u2217i,j )\nHence, it is sufficient to prove\n\u03b3\u2217d,i + \u2211\ni\u2032\n\u03b3\u2217d,i\u2032 \u03b2\u2217i\u2032,j \u03b2\u2217i,j \u2265 1 (Ct\u03b2) 1/4 Ct\u03b3(\u03b3 \u2217 d,i + (C t \u03b2) 2 \u2211\ni\u2032\n\u03b3\u2217d,i\u2032 \u03b2\u2217i\u2032,j \u03b2\u2217i,j ) \u21d4\n\u03b3\u2217d,i(1 \u2212 Ct\u03b3\n(Ct\u03b2) 1/4\n) \u2265 \u2211\ni\u2032\n\u03b3\u2217d,i\u2032( Ct\u03b3\n(Ct\u03b2) 1/4\n(Ct\u03b2) 2 \u2212 1) \u03b2\u2217i\u2032,j \u03b2\u2217i,j\nAgain, we can upper bound the right hand side by\n\u2211\ni\u2032\n\u03b3\u2217d,i\u2032( Ct\u03b3\n(Ct\u03b2) 1/4\n(Ct\u03b2) 2 \u2212 1)\u03ba =\n(1\u2212 \u03b3\u2217d,i)( Ct\u03b3\n(Ct\u03b2) 1/4\n(Ct\u03b2) 2 \u2212 1)\u03ba\nSo, it is sufficient to prove:\n(1\u2212 \u03b3\u2217d,i)( Ct\u03b3\n(Ct\u03b2) 1/4\n(Ct\u03b2) 2 \u2212 1)\u03ba \u2264 \u03b3\u2217d,i(1\u2212 Ct\u03b3 (Ct\u03b2) 1/4 ) \u21d4\n\u03b3\u2217d,i(1\u2212 Ct\u03b3\n(Ct\u03b2) 1/4\n+ ( Ct\u03b3\n(Ct\u03b2) 1/4\n(Ct\u03b2) 2 \u2212 1)\u03ba) \u2265 ( Ct\u03b3 (Ct\u03b2) 1/4 (Ct\u03b2) 2 \u2212 1)\u03ba \u21d4\n\u03b3\u2217d,i \u2265 1\u2212 1\u2212 C\nt \u03b3\n(Ct \u03b2 )1/4\n1\u2212 C t \u03b3\n(Ct \u03b2 )1/4\n+ ( Ct\u03b3\n(Ct \u03b2 )1/4\n(Ct\u03b2) 2 \u2212 1)\u03ba\nIt\u2019s easy to check that the expression on the right hand side as a function of Ct\u03b3 is decreasing. Hence, the RHS is upper bounded by\n1\u2212 1\u2212 1 (Ct\u03b2) 3/20\n1\u2212 1 (Ct\u03b2) 3/20 + \u03ba((C t \u03b2) 37/20 \u2212 1)\nNow, let\u2019s analyze this expression. If we let f(x) = 1\u2212 1\u2212 1 x3/20\n1\u2212 1 x3/20\n+\u03ba(x37/20\u22121) , I claim f(x) is an increasing\nfunction of x. Indeed, we can calculate it\u2019s derivative fairly easily:\nf \u2032(x) = \u2212 3 20x \u2212 2320 (1\u2212 1 x3/20 + \u03ba(x37/20 \u2212 1))\u2212 (1\u2212 1 x3/20 )(\u2212 320x\u2212 23 20 + 3720\u03bax 17 20 )\n(1\u2212 1 x3/20\n+ \u03ba(x37/20 \u2212 1))2 =\n\u2212 3 20x \u2212 2320 \u03ba(x 37 20 \u2212 1)\u2212 3720\u03bax 17 20 (1\u2212 x\u2212 320 )\n(1\u2212 1 x3/20\n+ \u03ba(x37/20 \u2212 1))2 = \u03ba 20 (40x 14/20 \u2212 (3x\u221223/40 + 37x17/20)) (1 \u2212 x3/20 + 1\u03ba ( 1x37/20 \u2212 1))2\nBy the AM-GM inequality, 3x\u221223/40 + 37x17/20 \u2265 40((x17/20)37(x\u221223/20)3)1/40 = 40x14/20, so f \u2032(x) is positive, so the RHS, as a function of Ct\u03b2 , is (x) is increasing.\nSo, it is sufficient to satisfy the inequality when Ct\u03b2 = C 0 \u03b2 . One can check however that by Lemma 43 and 44 this is true. Proceeding to the lower bound, a similar calculation as before gives that the necessary condition for progress is:\n\u03b3\u2217d,i \u2265 1\u2212 1\u2212 (C\nt \u03b2) 1/4 Ct\u03b3\n1\u2212 (C\u03b2)1/4Ct\u03b3 + 1 \u03ba (\n(Ct\u03b2) 1/4\nCt\u03b3 1 (Ct\u03b2) 2 \u2212 1)\nAgain, the right hand side expression is decreasing in C\u03b3 , so it is certainly upper bounded by\n1\u2212 1\u2212 (Ct\u03b2)3/20\n1\u2212 (Ct\u03b2)3/20 + 1\u03ba ( 1(Ct\u03b2)37/20 \u2212 1)\nNow, the claim is that this expression is increasing in Ct\u03b2 . Again, denoting f(x) = 1\u2212 1\u2212x 3/20\n1\u2212x3/20+ 1\u03ba ( 1\nx37/20 \u22121)\nf \u2032(x) = \u2212\u2212 3 20x \u221217/20(1\u2212 x3/20 + 1\u03ba ( 1x37/20 \u2212 1))\u2212 (1\u2212 x3/20)(\u2212 3 20x \u221217/20 \u2212 1\u03ba 3720x\u221257/20) (1\u2212 x3/20 + 1\u03ba ( 1x37/20 \u2212 1))2 =\n\u2212\u2212 3 20x \u221217/20 1 \u03ba( 1 x37/20 \u2212 1) + (1\u2212 x3/20) 1\u03ba 3720x\u221257/20 (1\u2212 x3/20 + 1\u03ba ( 1x37/20 \u2212 1))2 = 1 20\u03ba (\u221240x\u221254/20 + (3x\u221217/40 + 37x\u221257/20)) (1\u2212 x3/20 + 1\u03ba ( 1x37/20 \u2212 1))2\nBy the AM-GM inequality, 3x\u221217/40 + 37x\u221257/20 \u2265 40((x\u221217/20)37(x\u221257/20)37)1/40 = 40x\u221254/20, so f \u2032(x) is negative, so the RHS, as a function of Ct\u03b2 , is decreasing. So it suffices to check the inequality when Ct\u03b2 = (1\u2212 \u01eb)20. In this case, we want to check that\n1\u2212 1 \u03ba100\n\u2265 1\u2212 1\u2212 1(1\u2212\u01eb)3\n1\u2212 1(1\u2212\u01eb)3 + 1\u03ba((1 \u2212 \u01eb)37 \u2212 1)\nSince 1\u2212 1\u2212 1 (1\u2212\u01eb)3\n1\u2212 1 (1\u2212\u01eb)3 + 1\u03ba ((1\u2212\u01eb) 37\u22121) \u2264 1\u2212 3\u03ba37+3\u03ba , and \u03ba \u2265 2, this is easily seen to be true. Now, we\u2019ll split the \u03b2 update into two parts: documents where topic i is at least 1\u2212 1/\u03ba100, and the rest\nof them. In the first group, as we showed above, f\u2217d,j ftd,j \u2265 1 (Ct\u03b2) 1/2 . In the second group, we can certainly claim that f\u2217d,j ftd,j \u2265 1 Ct\u03b3C t \u03b2 from the inductive hypothesis. If we denote the set of documents where topic i is at least 1\u2212 1/\u03ba100 as D1, we get that\n\u03b2t+1i,j = \u03b2 t i,j\n\u2211\nd f\u2217d,j ftd,j\n\u03b3td,i \u2211D\ni=1 \u03b3 t d,i\n\u2265\n\u2211\nd\u2208D1 1\n(Ct\u03b2) 1/2Ct\u03b3\n\u03b2\u2217i,j\u03b3 \u2217 d,i +\n\u2211\nd\u2208D\\D1 1\n(Ct \u03b2 )2(Ct\u03b3)\n2 \u03b2 \u2217 i,j\u03b3 \u2217 d,i\n(Ct\u03b3) \u2211 d\u2208D \u03b3 \u2217 d,i\nIf we denote \u00b5 = \u2211 d\u2208D1 \u03b3\u2217d,i \u2211\nd\u2208D \u03b3 \u2217 d,i , then\n\u03b2t+1i,j \u2265 \u00b5 \u03b2\u2217i,j\n(Ct\u03b2) 1/4(Ct\u03b3)\n2 + (1\u2212 \u00b5) \u03b2\u2217i,j (Ct\u03b2) 2(Ct\u03b3) 3\nSo, to prove \u03b2t+1i,j \u2265 1C3/4\u03b2 \u03b2\u2217i,j , it\u2019s sufficient to show\n\u00b5 \u03b2\u2217i,j\n(Ct\u03b2) 1/4(Ct\u03b3)\n2 + (1 \u2212 \u00b5) \u03b2\u2217i,j (Ct\u03b2) 2(Ct\u03b3) 3 \u2265 1\nC 3/4 \u03b2\n\u21d4\n\u00b5 >\n1 (Ct\u03b2) 1/2 \u2212 1(Ct\u03b2)2(Ct\u03b3)3 1\n(Ct\u03b2) 1/4(Ct\u03b3) 2 \u2212 1(Ct\u03b2)2(Ct\u03b3)3\nGiven that Ct\u03b3 \u2264 (Ct\u03b2)1/10, it\u2019s sufficient to show\n\u00b5 >\n1 (Ct\u03b2) 1/2 \u2212 1(Ct\u03b2)23/10 1 (Ct\u03b2) 9/20 \u2212 1(Ct\u03b2)23/10 = 1\u2212 1 (Ct\u03b2) 9/20 \u2212 1(Ct\u03b2)1/2 1 (Ct\u03b2) 9/20 \u2212 1(Ct\u03b2)23/10\nCompletely analogously as before, 1 \u2212 1 (Ct \u03b2 )9/20 \u2212 1 (Ct \u03b2 )1/2\n1\n(Ct \u03b2 )9/20\n\u2212 1 (Ct\n\u03b2 )23/10\nis a decreasing function of Ct\u03b2 , so it\u2019s sufficient\nto check that \u00b5 > 1\u2212 1 (Ct \u03b2 )9/20 \u2212 1 (Ct \u03b2 )1/2\n1\n(Ct \u03b2 )9/20\n\u2212 1 (Ct\n\u03b2 )23/10\nwhen Ct\u03b2 = ( 1 1\u2212\u01eb ) 20, which is easily checked to be true.\nIn the same way, one can prove that \u03b2t+1i,j \u2264 (Ct\u03b2)3/4\u03b2\u2217i,j Putting lemmas 46 and 47 together, we get that the analogue of Lemma 14:\nLemma 48. Suppose it holds that 1Ct \u2264 \u03b2i,j\u2217 \u03b2ti,j \u2264 Ct, Ct \u2265 1(1\u2212\u01eb)20 . Then, after one KL minimization step with respect to the \u03b3 variables and one \u03b2 iteration, we get new values \u03b2t+1i,j that satisfy 1 Ct+1 \u2264 \u03b2i,j\u2217\n\u03b2t+1i,j \u2264 Ct+1,\nwhere Ct+1 = (Ct)3/4\nAs a corollary,\nCorollary 49. Phase III requires O(log( 1log(1+\u01eb) )) = O(log( 1 \u01eb )) iterations to estimate each of the topic-word matrix and document proportion entries to within a multiplicative factor of 1(1\u2212\u01eb)7\nThis finished the proof of Theorem 41 for Case Study 1.\nE.3 Generalizing Case Study 2\nFinally, the proof for Case Study 2 is quite simple. Because the dynamic range \u03ba \u2264 B for the common words, Lemmas 35 and 36 still hold, and hence we again determine the dominant topic correctly. Because of this, it\u2019s also easy to see that the lower bounds and upper bounds on the \u03b2ti,j values for the common words are maintained to be a constant, since the proof of Lemmas 32 and 33 holds for the common words verbatim. This means that the anchor words and discriminative words will be correctly determined just as before. But after that point, the analysis of Case Study 2 is exactly the same as the one for Case Study 2 \u2014 which we already covered in the above section. This finishes the proof of Theorem 41."}, {"heading": "F Estimates on number of documents", "text": "Finally, we state a few helper lemmas to estimate how many documents will be needed. The properties we need are that the empirical marginals of a dominating topic in the documents where it\u2019s dominating are close to the actual ones, and similarly that the empirical marginals of the dominating topic, conditioned on the set of topics that a discriminative word belongs to not being present are close to the actual ones.\nThe former statement is the following:\nLemma 50. Let Ei = E[\u03b3 \u2217 d,i|\u03b3\u2217d,i is dominating]. If the total number of documents is D = \u2126(K log 2 K \u01eb2 ), and Di is the number of documents where i is the dominant topic, then with high probability, for all topics i,\n(1\u2212 \u01eb)Ei \u2264 1\nDi\n\u2211\nd\u2208Di\n\u03b3\u2217d,i \u2264 (1 + \u01eb)Ei\nProof. Since documents are generated independently, Pr[ 1Di \u2211 d\u2208Di \u03b3\u2217d,i > (1+ \u01eb)Ei] \u2264 e\u2212\n\u01eb2DiEi 3 by Chernoff.\nSince there are at most T topics per document, Ei \u2265 1T , so Pr[ 1Di \u2211 d\u2208Di \u03b3\u2217d,i > (1 + \u01eb)Ei] \u2264 e\u2212\n\u01eb2Di 3T\nAn analogous statement holds for Pr[ 1Di \u2211 d\u2208Di \u03b3\u2217d,i < (1 \u2212 \u01eb)Ei]\nThen, if Di = log2 K \u01eb2 , by union bounding, we get that with high probability, for all topics, (1 \u2212 \u01eb)Ei \u2264 1 Di \u2211 d\u2208Di \u03b3\u2217d,i \u2264 (1 + \u01eb)Ei\nHowever, the probability of a topic being dominating is Ci/K for some constant Ci. So, by another Chernoff bound,\nPr[Di < (1 \u2212 \u01eb)CiD/K] \u2264 e\u2212 \u01eb2CiD 3K (F.1)\nSo, if we take D = K\u01eb2 log 2 K, with high probability, for all topics, Di = \u0398(D/K). Putting everything together, we get that if D = K log 2 K\n\u01eb2 , with high probability,\n(1\u2212 \u01eb)Ei \u2264 1\nDi\n\u2211\nd\u2208Di\n\u03b3\u2217d,i \u2264 (1 + \u01eb)Ei\nNext, we calculate how many documents are needed to match the marginals of the dominating topics, conditioned on a small subset (of size o(K)) of the topics not being included in a document. More formally,\nLemma 51. For the discriminative word j, let jS be the set of topics it belongs to. For a topic i \u2208 jS, let Let Ei,jS = E[\u03b3 \u2217 d,i|\u03b3\u2217d,i is dominating, \u03b3\u2217d,i\u2032 = 0, \u2200i\u2032 \u2208 jS]. Let Di,jS be the number of documents where i is dominating, and \u03b3\u2217d,i\u2032 = 0, \u2200i\u2032 \u2208 jS. If the number of documents D \u2265 K log2 N\u01eb2 , then with high probability, for all topics i and discriminative words j, (1\u2212 \u01eb)Ei,jS \u2264 1Di,jS \u2211 d\u2208Di,jS \u03b3\u2217d,i \u2264 (1 + \u01eb)Ei,jS\nProof. Since Ei,jS = (1\u00b1o(1))Ei, by the weak topic correlation property, an analogous proof as above shows that if we get that if Di,jS =\nlog2 K \u01eb2 , with high probability, (1\u2212 \u01eb)EiS \u2264 1DiS \u2211 d\u2208DiS \u03b3\u2217d,i \u2264 (1 + \u01eb)EiS .\nBut by the independent topic inclusion property, the probability of generating a document D with i being the dominating topic, s.t. no topics in jS appear in it is \u0398(1/K). So, again by Chernoff,\nPr[Di,jS < (1\u2212 \u01eb)CiD/K] \u2264 e\u2212 \u01eb2CiD 3K (F.2)\nIf we take D = K\u01eb2 log 2 N , Pr[Di,jS < (1\u2212 \u01eb)CiD/K] \u2264 e\u2212 log 2 N . However, since the total number of i, jS pairs is at most N2, union bounding, we get that with high probability, for all pairs i, jS,\n(1\u2212 \u01eb)Ei,jS \u2264 1\nDi,jS\n\u2211\nd\u2208Di,jS\n\u03b3\u2217d,i \u2264 (1 + \u01eb)Ei,jS\nFinally, the following short lemma to estimate the number of documents in which a word j belongs only to the dominating topic is implicit in the proof above:\nLemma 52. Let Di,jS be the number of documents where i is dominating, and \u03b3 \u2217 d,i\u2032 = 0, \u2200i\u2032 \u2208 jS. If the number of documents D \u2265 K log2 N\u01eb2 , then with high probability, for all topics i and discriminative words j, Di,jS \u2265 Di(1\u2212 \u01eb)(1\u2212 o(1))"}], "references": [{"title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli"], "venue": "In Proceedings of The 27th Conference on Learning Theory (COLT),", "citeRegEx": "Agarwal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu"], "venue": "Technical report,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Learning latent bayesian networks and topic models under expansion constraints", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "author": ["S. Arora", "R. Ge", "R. Kanna", "A. Moitra"], "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In Proceedings of The 27th Conference on Learning Theory (COLT),", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "In Proceedings of The 28th Conference on Learning Theory (COLT),", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Statistical guarantees for the em algorithm: From population to sample-based analysis", "author": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"], "venue": "arXiv preprint arXiv:1408.2156,", "citeRegEx": "Balakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2014}, {"title": "A provable svd-based algorithm for learning topics in dominant admixture corpus", "author": ["T. Bansal", "C. Bhattacharyya", "R. Kannan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A two-round variant of em for gaussian mixtures", "author": ["S. Dasgupta", "L. Schulman"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dasgupta and Schulman.,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2000}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["S. Dasgupta", "L. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Topic discovery through data dependent and random projections", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "arXiv preprint arXiv:1303.3664,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Efficient distributed topic modeling with provable guarantees", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "In Proceedings ot the 17th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D. Blei", "J. Paisley", "C. Wan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["A. Kumar", "R. Kannan"], "venue": "In Proceedings of Foundations of Computer Science (FOCS),", "citeRegEx": "Kumar and Kannan.,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Kannan.", "year": 2010}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D. Lee", "S. Seung"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lee and Seung.,? \\Q2000\\E", "shortCiteRegEx": "Lee and Seung.", "year": 2000}, {"title": "Phase retrieval using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Complexity of inference in latent dirichlet allocation", "author": ["D. Sontag", "D. Roy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sontag and Roy.,? \\Q2000\\E", "shortCiteRegEx": "Sontag and Roy.", "year": 2000}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["R. Sundberg"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "Sundberg.,? \\Q1974\\E", "shortCiteRegEx": "Sundberg.", "year": 1974}, {"title": "Dirichlet draws are sparse with high probability", "author": ["M. Telgarsky"], "venue": null, "citeRegEx": "Telgarsky.,? \\Q2013\\E", "shortCiteRegEx": "Telgarsky.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al.", "startOffset": 130, "endOffset": 155}, {"referenceID": 10, "context": "The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003).", "startOffset": 127, "endOffset": 146}, {"referenceID": 6, "context": "The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient.", "startOffset": 76, "endOffset": 96}, {"referenceID": 18, "context": "Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).", "startOffset": 174, "endOffset": 198}, {"referenceID": 11, "context": "Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).", "startOffset": 200, "endOffset": 229}, {"referenceID": 12, "context": "Among more classical results in this direction are the analyses of Lloyd\u2019s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).", "startOffset": 231, "endOffset": 260}, {"referenceID": 8, "context": "The recent work of (Balakrishnan et al., 2014) also characterizes global convergence properties of the EM algorithm for more general settings.", "startOffset": 19, "endOffset": 46}, {"referenceID": 0, "context": "(Agarwal et al., 2013), (Arora et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": ", 2013), (Arora et al., 2015) prove that with appropriate initialization, alternating minimization can provably recover the ground truth.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "(Netrapalli et al., 2013) have proven similar results in the context of phase retreival.", "startOffset": 0, "endOffset": 25}, {"referenceID": 17, "context": "Another popular heuristic which has so far eluded such attempts is known as variational inference (Jordan et al., 1999).", "startOffset": 98, "endOffset": 119}, {"referenceID": 10, "context": "We provide the first characterization of global convergence of variational inference based algorithms for topic models (Blei et al., 2003).", "startOffset": 119, "endOffset": 138}, {"referenceID": 13, "context": "The expectation-maximization (EM) algorithm is an iterative method to achieve this, dating all the way back to (Dempster et al., 1977) and (Sundberg, 1974) in the 70s.", "startOffset": 111, "endOffset": 134}, {"referenceID": 22, "context": ", 1977) and (Sundberg, 1974) in the 70s.", "startOffset": 12, "endOffset": 28}, {"referenceID": 10, "context": "3 Topic models We will focus on a particular latent variable model, which is very often studied - topic models (Blei et al., 2003).", "startOffset": 111, "endOffset": 130}, {"referenceID": 10, "context": "(Originally introduced by (Blei et al., 2003).", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": ", 2012b),(Arora et al., 2013), as well as (Anandkumar et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 2, "context": ", 2013), as well as (Anandkumar et al., 2013), (Ding et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 14, "context": ", 2013), (Ding et al., 2013), (Ding et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": ", 2013), (Ding et al., 2014) and (Bansal et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 9, "context": ", 2014) and (Bansal et al., 2014).", "startOffset": 12, "endOffset": 33}, {"referenceID": 5, "context": ", 2012b) and (Arora et al., 2013) assume that the topic-word matrix contains \u201canchor words\u201d.", "startOffset": 13, "endOffset": 33}, {"referenceID": 2, "context": "(Anandkumar et al., 2013) on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "(Similar to the expansion in (Anandkumar et al., 2013), but only over constant sized subsets.", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "(Originally introduced by (Blei et al., 2003).", "startOffset": 26, "endOffset": 45}, {"referenceID": 9, "context": ") The documents will also have a \u201cdominating topic\u201d, similarly as in (Bansal et al., 2014).", "startOffset": 69, "endOffset": 90}, {"referenceID": 10, "context": "4 Variational relaxation for learning topic models In this section we briefly review the variational relaxation for topic models following closely the description in (Blei et al., 2003).", "startOffset": 166, "endOffset": 185}, {"referenceID": 21, "context": "For topic models variational updates are way to approximate the computationally intractable E-step (Sontag and Roy, 2000) as described in Section 2.", "startOffset": 99, "endOffset": 121}, {"referenceID": 10, "context": "In (Blei et al., 2003) it\u2019s shown that for Dirichlet priors \u03b1 the optimal distributions q, q j are a Dirichlet distribution for q, with some parameter \u03b3\u0303, and multinomials for q j , with some parameters \u03c6j .", "startOffset": 3, "endOffset": 22}, {"referenceID": 10, "context": "The way we derived them, these updates appear to be an approximate form of the variational updates in (Blei et al., 2003).", "startOffset": 102, "endOffset": 121}, {"referenceID": 10, "context": "It is intuitively clear that in the large document limit, this approximation should not be much worse than the one in (Blei et al., 2003), as the posterior concentrates around the maximum likelihood value.", "startOffset": 118, "endOffset": 137}, {"referenceID": 19, "context": "In a slightly modified form, these updates were used in a paper by (Lee and Seung, 2000) in the context of non-negative matrix factorization.", "startOffset": 67, "endOffset": 88}, {"referenceID": 7, "context": "These are analogues of distributions that have been analyzed for dictionary learning (Arora et al., 2015).", "startOffset": 85, "endOffset": 105}, {"referenceID": 23, "context": "This was formally proven by (Telgarsky, 2013) - though sparsity there means a small number of large coordinates.", "startOffset": 28, "endOffset": 45}, {"referenceID": 7, "context": "Such assumptions also appear in the context of dictionary learning (Arora et al., 2015).", "startOffset": 67, "endOffset": 87}, {"referenceID": 6, "context": "This uses an idea inspired by (Arora et al., 2014) in the setting of dictionary learning.", "startOffset": 30, "endOffset": 50}, {"referenceID": 9, "context": "A similar assumption (a small fraction of almost pure documents) appeared in a recent paper by (Bansal et al., 2014).", "startOffset": 95, "endOffset": 116}, {"referenceID": 23, "context": "1 Sparsity To characterize the sparsity of the topic proportions in a document, we will need the following lemma from (Telgarsky, 2013): Lemma 38.", "startOffset": 118, "endOffset": 135}, {"referenceID": 23, "context": "(Telgarsky, 2013) For a Dirichlet distribution with parameters (C1/k , C2/k , .", "startOffset": 0, "endOffset": 17}], "year": 2015, "abstractText": "Variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. It\u2019s closely related to Expectation Maximization (EM), and is applied when exact EM is computationally infeasible. Despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. In this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. More specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012b). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003). It is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient. While our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest. Our proofs rely on viewing the updates as an operation which, at each timestep, sets the new parameter estimates to be noisy convex combinations of the ground truth values, and a bounded error term which depends on the previous estimate. The weight on the ground truth values will be large, compared to the error term, which will cause the error term to eventually reach zero. The large weight on the ground truth values will be a byproduct of our model assumptions, which will imply a \u201clocal\u201d notion of anchor words for each document words which only appear in one topic in a given document, as well as a \u201clocal\u201d notion of anchor documents for each word documents where that word appears as part of a single topic. Princeton University, Computer Science Department. Email: pawashti@cs.princeton.edu. Supported by NSF grant CCF1302518. Princeton University, Computer Science Department. Email: risteski@cs.princeton.edu. Partially supported by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora\u2019s Simons Investigator Award, and a Simons Collaboration Grant.", "creator": "LaTeX with hyperref package"}}}