{"id": "1503.06619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Fusing Continuous-valued Medical Labels using a Bayesian Model", "abstract": "Given the rapid growth in the volume of medical time series data available through portable devices, there is a need to use automated algorithms to label data. Examples of labeling include interventions, changes in activity (e.g. sleep), and changes in physiology (e.g. arrhythmias). However, automated algorithms tend to be unreliable, leading to lower quality of care. Expert comments are rare, expensive, and tend to show significant variations between and within the observer. To address these issues, it is proposed to use a Bayesian Continuous-Value Label Aggregator (BCLA) that provides a reliable estimate of label aggregation while accurately concluding the precision and distortion of each algorithm. BCLA was applied to the QT interval (pro-arrhythmic indicator), using labels from the PhysioNet / Computing in Cardiology Challenge 2006 database.", "histories": [["v1", "Mon, 23 Mar 2015 12:31:18 GMT  (221kb,D)", "https://arxiv.org/abs/1503.06619v1", null], ["v2", "Sat, 13 Jun 2015 13:06:29 GMT  (211kb,D)", "http://arxiv.org/abs/1503.06619v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tingting zhu", "nic dunkley", "joachim behar", "david a clifton", "gari d clifford"], "accepted": false, "id": "1503.06619"}, "pdf": {"name": "1503.06619.pdf", "metadata": {"source": "CRF", "title": "Fusing Continuous-valued Medical Labels using a Bayesian Model", "authors": ["Tingting Zhu", "Nic Dunkley", "Joachim Behar", "David A. Clifton", "Gari D. Clifford"], "emails": ["tingting.zhu@eng.ox.ac.uk"], "sections": [{"heading": null, "text": "there is a need to employ automated algorithms to label data. Examples of labels include interventions, changes in activity (e.g. sleep) and changes in physiology (e.g. arrhythmias). However, automated algorithms tend to be unreliable resulting in lower quality care. Expert annotations are scarce, expensive, and prone to significant interand intra-observer variance. To address these problems, a Bayesian Continuous-valued Label Aggregator(BCLA) is proposed to provide a reliable estimation of label aggregation while accurately infer the precision and bias of each algorithm.\nThe BCLA was applied to QT interval (pro-arrhythmic indicator) estimation from the electrocardiogram using labels from the 2006 PhysioNet/Computing in Cardiology Challenge database. It was compared to the mean, median, and a previously proposed Expectation Maximization (EM) label aggregation approaches. While accurately predicting each labelling algorithm\u2019s bias and precision, the root-mean-square error of the BCLA was 11.78\u00b10.63ms, significantly outperforming the best Challenge entry (15.37\u00b12.13ms) as well as the EM, mean, and median voting strategies (14.76\u00b10.52ms, 17.61\u00b10.55ms, and 14.43\u00b10.57ms respectively with p < 0.0001).\nThe BCLA could therefore provide accurate estimation for medical continuous-valued label tasks in an unsu-\npervised manner even when the ground truth is not available.\nKeywords Crowdsourcing \u00b7 Bayes methods \u00b7 Time series analysis \u00b7 Electrocardiography."}, {"heading": "1 Introduction", "text": "With human annotation of data, significant intra- and inter-observer disagreements exist [7, 21]. Expert labelling (or \u2018reading\u2019 or \u2018annotating\u2019) of medical data by physicians or clinicians often involves multiple over-reads, particularly when an individual is under-confident of the diagnosis. However, experts are scarce and expensive and can create significant delays in labelling or diagnoses. Although medical training includes periodic assessment of general competency, specific assessments for reading medical data are difficult to be performed regularly. This data processing pipeline is further complicated by the ambiguous definition of an \u2018expert\u2019. There is no empirical method for measuring level of expertise, even though label accuracy can vary greatly depending on the expert\u2019s\nexperience. As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].\nAn effective probabilistic approach to aggregating expert labels which used an Expectation Maximization (EM) algorithm, was first proposed by Dawid and Skene [6]. They applied the EM algorithm to classify the unknown true states of health (i.e. fit to undergo a general anaesthetic) of 45 patients given the decision made by five anaesthetists. Raykar et al. [16] extended this approach to measure the diameter of a suspicious lesion on a medical image using a regression model. Their assumption was that the discrepancies of the lesion diameter estimates from different expert annotators were Gaussian distributed and noisy versions of the actual true diameter. The precision of each expert annotator and the underlying ground truth were jointly modelled in an iterative process using EM. Welinder and Perona [23] proposed a Bayesian EM framework for continuous-valued labels, which explicitly modelled the precision only of each annotator to account for their varying skill levels, without modelling the bias of annotators. A more specialised form of the Bayesian model of bias was proposed by Welinder et al. [22] but for binary classification tasks. However, their model cannot account for more complex tasks such as the continuous-valued labelling.\nThe methodology proposed in the work presented in this article improves on these prior algorithms [16, 22, 23] by introducing the novelty of combining continuous-valued annotations to infer the underlying ground truth, while jointly modelling the annotator\u2019s bias and precision in an unified model using a Bayesian treatment.\nAggregating annotations (i.e. fusing multiple annotations for each piece of data from annotators with varying levels of expertise) from human and/or automated algorithms may provide a more accurate ground truth and reduce annotator inter- and intra-variability. However, most annotators are likely to have some bias regardless of their expertise [23, 25]. Bias is defined as the inverse of accuracy: It measures the average difference between the estimation and the true value, and it is annotator dependent. An example of bias is demonstrated in Fig. 1 in the context of Electrocardiogram labelling. Recently, Warby et al. [20] studied how to combine non-expert annotator\u2019s labels of sleep spindle location, a special pattern in human electroencephalography, through fusing annotations provided by non-experts. In that work, although na\u0131\u0308ve majority vote was used to aggregate the labels of the locations, they demonstrated that non-expert annotations were comparable to those provided by the experts (i.e. the by-subject spindle density correlation was 0.815). Our proposed framework, in contrast, is a statistical\napproach that models the precision and bias of each annotator, which we hypothesise would provide a superior estimation of the ground truth as determined by a collection of experts.\nIn contrast to previous works, this article proposes a Bayesian framework for aggregating multiple continuousvalued annotations in medical data labelling, which takes into account the precision and bias of the individual annotators. Moreover, we propose a generalised form which can be extended to incorporate contextual features of the physiological signal, so that we can adjust the weighting of each label based on the estimated bias and variance of the individual for different types of signal. To our knowledge, the proposed model for estimating continuous-valued labels in an unsupervised manner is novel in the medical domain."}, {"heading": "2 Materials and Methods", "text": "2.1 Bayesian Continuous-valued Label Aggregator (BCLA) Suppose that there are N records of physiological time series data labelled by R annotators. Let D= [ x\u1d40i ,y j=1 i , \u00b7 \u00b7 \u00b7 ,y j=R i ]N i=1 , where xi is a column feature vector for the ith record containing d features (i.e. the design matrix, X = [x\u1d401 , ...,x \u1d40 N]), y ji corresponds to the annotation provided by the jth annotator for the ith record, and zi represents the unknown underlying ground truth (the true time or duration of an event for example). The graphical representation of the proposed approach \u2013 the Bayesian Continuous-valued Label Aggregator (BCLA) \u2013 is shown in Fig. 2.\nIn this model, it is assumed that y ji was a noisy version of zi, with a Gaussian distribution N (y j i | zi,(\u03c3 j)2)1. Here \u03c3 j is the standard deviation of the jth annotator and represents his variance in annotation around zi. Furthermore, the bias of each annotator can be modelled as an additional term, \u03c6 j. The probability of estimating y ji can be written as:\nP[y ji | zi,(\u03c3 j)2] = N (y ji | zi +\u03c6 j,1/\u03bb j). (1)\nwhere (\u03c3 j)2 is replaced with 1/\u03bb j. \u03bb j is the precision of the jth annotator, defined as the estimated inversevariance of annotator j. Note that \u03bb j and \u03c6 j are considered to be constants for the jth annotator, i.e. all anno-\n1 The motivation for this model comes from the Central Limit Theorem. Given the assumption that the annotators are independent and identically distributed, their labels will converge to a Gaussian distribution. In the absence of prior knowledge, this assumption allows for a robust and generalizable model for the given data.\ntators are assumed to have consistent but usually different performances throughout records. Furthermore, it is assumed that the probability of a given bias of annotator j, \u03c6 j, is drawn from a Gaussian distribution with mean \u00b5\u03c6 and variance 1/\u03b1\u03c6 , is given by:\nP[\u03c6 j | \u00b5\u03c6 ,\u03b1\u03c6 ] = N (\u03c6 j | \u00b5\u03c6 ,1/\u03b1\u03c6 ). (2)\nAlthough the biases of the annotators might be derived from other distributions, they are likely to be data set dependent. In the absence of any knowledge of the underlying distribution of biases, they are assumed to be drawn from a Gaussian distribution. Furthermore, the ground truth, zi, can be assumed to be drawn from a Gaussian distribution with mean a and variance 1/b. The probability of zi is defined as follows:\nP[zi | a,b] = N (zi | a,1/b), (3)\nwhere a can be expressed as a linear regression function f (w,x) with an intercept, and w being the coefficients of the regression [16, 26]. The intercept models the overall offset predicted in the regression, which is different from the annotator specific bias in the proposed model. Under the assumption that records are independent, the likelihood of the parameter \u03b8 = {w,\u03bb ,\u03c6 ,\u03b1\u03c6 ,b,zi} for a given data set D can be formulated as:\nP[D | \u03b8 ] = N\n\u220f i=1 P[y1i , \u00b7 \u00b7 \u00b7 ,yRi | xi,\u03b8 ]. (4)\nIt is assumed that y1i , \u00b7 \u00b7 \u00b7 ,yRi are conditionally independent given the feature xi (i.e. each annotator works independently to provide annotations). This may or may not be necessarily true, especially in cases where the annotations are generated by algorithms, some of which may be variations of the same approach. Nevertheless, this assumption was made to simplify the model and subsequent derivation of the likelihood. The likelihood of the parameter \u03b8 for a given data set D can be written using the Bayes\u2019 theorem as (see detailed description in Fig. 2):\nP[\u03b8 | D] \u221d P[D | \u03b8 ] \u00b7P[\u03b8 ]\n= \u0393 (\u03b1\u03c6 | k\u03b1 ,\u03d1\u03b1)[ R\n\u220f j=1 N (\u03c6 j | \u00b5\u03c6 ,1/\u03b1\u03c6 )\u0393 (\u03bb j | k\u03bb ,\u03d1\u03bb )]\n\u0393 (b | kb,\u03d1b)[ N\n\u220f i=1\nN (zi | a,1/b) R\n\u220f j=1 N (y ji | zi +\u03c6 j,1/\u03bb j)]. (5)\nwhere \u0393 denotes a Gamma distribution and can be defined as \u0393 (z | k,\u03d1) = 1\u0393 (k)\u03d1 k z k\u22121exp(\u2212 z\u03d1 ), where k is the shape of the distribution and \u03d1 is the scale of the distribution. Gamma distribution is commonly used to model positive continuous values. It is therefore assumed that precision values, such as b, \u03bb j, and \u03b1\u03c6 were drawn from a Gamma distribution, with parameters kb, \u03d1b, k\u03bb , \u03d1\u03bb , and k\u03b1 , \u03d1\u03b1 respectively.\n2.2 The Maximum a posteriori approach\nThe estimation of \u03b8 can be solved using the maximum a posteriori (MAP) approach, which maximises the loglikelihood of the parameters, i.e. argmax \u03b8 {logP[\u03b8 | D]}. The log-likelihood can be rewritten as:\nlogP[\u03b8 | D] =\u22121 2\nN\n\u2211 i=1\nR\n\u2211 j=1 [log( 2\u03c0 \u03bb j )+(y ji \u2212\u03c6 j\u2212 zi)2\u03bb j]\n\u22121 2\nR\n\u2211 j=1 [log( 2\u03c0 \u03b1\u03c6 )+(\u03c6 j\u2212\u00b5\u03c6 )2\u03b1\u03c6 ]\n\u22121 2\nN\n\u2211 i=1 [log( 2\u03c0 b )+(zi\u2212x \u1d40 i w) 2b]\n+[(k\u03bb \u22121) log\u03bb j\u2212 log(\u0393 (k\u03bb )\u03d1 (k\u03bb ) \u03bb \u2212\n\u03bb j \u03d1\u03bb ]\n+[(k\u03b1 \u22121) log\u03b1\u03c6 \u2212 log(\u0393 (k\u03b1 )\u03d1 (k\u03b1 ) \u03b1 )\u2212 \u03b1\u03c6 \u03d1\u03b1 ]\n+[(kb\u22121) logb\u2212 log(\u0393 (kb)\u03d1 (kb) b )\u2212 b \u03d1b ]. (6)\nThe parameters in \u03b8 can be derived by equating the gradient of the log-likelihood to zero respectively as follows:\n1 \u03bb j = 1 N +2(k\u03bb \u22121) [\nN\n\u2211 i=1 (y ji \u2212\u03c6 j\u2212 zi)2 + 2 \u03d1\u03bb ]. (7)\nw = ( N\n\u2211 i=1\nxix \u1d40 i )\n\u22121 N\n\u2211 i=1 xizi. (8)\n\u03c6 j = 1\nN + \u03b1\u03c6\u03bb j [\nN\n\u2211 i=1 (y ji \u2212 zi)+\u00b5\u03c6 ( \u03b1\u03c6 \u03bb j )]. (9)\n1 \u03b1\u03c6 = 1 R+2(k\u03b1 \u22121) [\nR\n\u2211 j=1\n(\u03c6 j\u2212\u00b5\u03c6 ) 2 + 2 \u03d1\u03b1 ]. (10)\nzi = \u2211Rj=1[(y j i \u2212\u03c6 j)\u03bb j]+ (x \u1d40 i w)b\n\u2211Rj=1 \u03bb j +b . (11)\n1 b = 1 N +2(kb\u22121) [ N \u2211 i=1 (zi\u2212x\u1d40i w) 2 + 2 \u03d1b ]. (12)\nThis MAP problem can be solved using the EM algorithm in a two-step iterative process:\ni) The E-step estimates the expected true annotations for all records, z\u0302, as a weighted sum of the provided annotations, and can be estimated using equation (11). ii) The M-step is based on the current estimation of z\u0302 and given the data set D. The model parameters, w, \u03c6 , \u03b1\u03c6 , b, and \u03bb can be updated using equations (8), (9), (10), (12), and (7) accordingly in a sequential order until convergence, which is now described.\n2.3 Convergence criteria for the MAP-EM approach\nWhen solving a MAP-EM algorithm one may encounter a convergence issue, particularly when estimating a large number of parameters. The estimation of the precision may approach to infinity because the inferred annotations favour the annotator with the highest precision in each EM update step while maximising the likelihood. Instead of incorporating an additional parameter for the regularisation penalty that increases the complexity of the mode, the generalized extreme value distribution (GEVD) can be used to model the maxima of the precision distribution, denoted as \u03bbm, in order to restrict the upper bound of the precision values and guarantee a convergence in the MAP algorithm. The probability density function of the GEVD for \u03bbm can be expressed as:\nP(\u03bbm | k,\u00b5,\u03d1) = exp{\u2212[1+ k (\u03bbm\u2212\u00b5)\n\u03d1 ]\u2212\n1 k } 1\n\u03d1 [1+ k (\u03bbm\u2212\u00b5) \u03d1 ](\u22121\u2212 1 k ), (13)\nwhere k is the shape parameter, \u03d1 is the scale parameter, and \u00b5 is the location parameter. These parameters can be derived by fitting a GEVD to the maximum values drawn randomly from the prior distribution of the precision, \u0393 (\u03bb | k\u03bb ,\u03d1\u03bb ). An upper bound of the maximum precision value can then be obtained by estimating the 99th quantile of the inverse cumulative distribution function of the GEVD.\n2.4 Data description\nThe electrocardiogram (ECG) is a standard and powerful tool for assessing cardiovascular health as many detrimental heart conditions manifest as abnormalities in the ECG. The QT interval is one particular measure of ECG morphology, and refers to the elapsed time between the onset of ventricular depolarisation (the QRS complex) and the T wave offset (ventricular repolarisation) [4]. Accurate measurement of the QT interval is essential since abnormal intervals indicate a potentially serious but treatable condition, and can be a contraindication for the use of drugs or other interventions [11]. Viskin et al. [19] presented the ECGs recorded from two patients with long QT syndrome (LQTS) and from two healthy females to 902 physicians (25 QT experts who had published on the subject, 106 arrhythmia specialists, 329 cardiologists, and 442 noncardiologists) from 12 countries. No other details were given on actual training or intrinsic accuracy of these annotators. For patients with LQTS, 80% of arrhythmia specialists calculated the QTc (the heart rate corrected QT interval) correctly but only 50% of cardiologists and 40% of noncardiologists did so. In the context of QT annotation where baseline wander is frequent,\nit was observed that a few annotators consistently over- or under-estimated the QT interval [25]. Other studies have reported significant intra- and inter-observer variability in QT annotations, ranging from 10 to 30ms [3, 8]. It is important to note that experts or non-experts with different levels of training or expertise can have significantly different biases. Na\u0131\u0308ve approaches to aggregate labels from a group of annotators of unknown expertises could therefore lead to poor results. However, annotators\u2019 biases are rarely taken into account when aggregating different labels or opinions in medical labelling tasks.\nWe hypothesise that incorporating an accurate estimation of each annotator\u2019s bias into a model for fusing annotations (as described in sections 2.1 to 2.3) will result in an improved estimate of the ground truth. In order to test this hypothesis we have used two data sets: one simulated data set to ensure an absolute ground truth is available; and one real data set of QT intervals. Although we have chosen to use QT interval data, because of the availability of the numerous annotations, the method we present is more general and can be applied to other continuous-valued annotations.."}, {"heading": "2.4.1 Simulated data set", "text": "To test the reliability of the BCLA as a generative model, a simulated data set was created: a total of 548 simulated records were generated, each has 20 independent annotator, thus providing a total of 10,960 annotations (see Fig. 3). The simulated data set considered that annotators have precision values, \u03bb (i.e. 1/ \u221a \u03c3 ), which were drawn from \u0393 (4,0.0003), with assumption that the annotations provided by the best performing annotator is \u00b115ms away from the ground truth. Annotators\u2019 biases were drawn from N (10,25), a Gaussian distribution with 10ms mean and a standard deviation (1/\u221a\u03b1\u03c6 ) of 25ms. The true annotation for each record was drawn from N (400,40), a Gaussian distribution with a mean, a, of 400ms with a standard deviation (1/ \u221a b) of 40ms. In addition, it was assumed that \u03b1\u03c6 was drawn from \u0393 (3,0.0005), ensuring the mean standard deviation where the biases drawn from is 25ms. The b was drawn from \u0393 (3,0.0002), ensuring the mean standard deviation where the true annotations drawn from is 40ms. The generated 10,960 annotations were then fed into the BCLA model to evaluate its accuracy in estimating the true annotation in an unsupervised manner as well as predicting the bias and precision of each annotator."}, {"heading": "2.4.2 Real data set", "text": "The data were drawn from the QT interval annotations generated by participants in the 2006 PhysioNet/Computing in Cardiology (PCinC) Challenge [15] for labelling QT intervals with reference to Lead II in each of the 548 recordings in the Physikalisch-Technische Bundesanstalt Diagnostic ECG Database (PTBDB) [2]. The records were from 290 subjects (209 men with mean age of 55.5 years and 81 women with mean age of 61.6 years), in which 20% of the subjects were healthy controls. An example of QT interval is demonstrated in Fig. 1(c). The PTBDB database contained records of patients with a variety of ECG morphologies having different QT intervals ranging from 256 to 529 ms. The diagnostic classifications of ECG morphologies mainly included myocardial infarction, heart failure, bundle branch block, and dysrhythmia as stated in Bousseljot and Kreiseler [2].\nThere were two main categories of annotations: manual and automated (see Table 1). A total of 38,621 annotations were collected and were divided into three divisions: 20 human annotators in Division 1, 48 closed source automated algorithms in Division 2, and 21 open source automated algorithms in Division 3. Division 4 was further created here so as to combine all automated algorithms from Division 2 and 3 in order to provide a larger data set and allow a better estimation of automated QT intervals. The number of annotators per division and averaged number of annotations per record are listed in Table 1. The overall percentage of the annotators in each division with complete annotations (i.e. annotations on all 548 recordings) was: 55% in Division 1, 40% in Division 2, 43% in Division 3, and 45% in Division 4. The competition score for each entry was calculated from the root mean square error (RMSE) between the submitted and the reference QT intervals. The reference annotations were generated from Division 1\u2019s entries using a maximum of 15 participants by taking the \u201cmedian self-centering approach\u201d as reported by the competition organisers as detailed in [24]. The best-performing score for each division is also listed in Table 1. Furthermore, the majority of the QT annotations of each 2-minute record occurred within the first 5 seconds of the ECG recordings. The best scores in the first 5-second segment were similar to those of the 2-minute segment (denoted by ? in Table 1). To reduce any possible inter-beat variations, only the annotations within the first 5-second segment of each record were chosen to ensure that all annotators had approximately labelled the same region of a record with similar QT morphologies. Therefore, the motivation for choosing the first 5-second segment of each record was to consider a short segment where the QT interval is not changing dramatically (with respect to a particular beat an annotator chose), while retaining the highest num-\nber of annotations. Those that fell outside this segment were considered to be missing information and discarded in the process of the QT estimation.\nAs the manual entry (i.e. Division 1) was used to generate the reference annotations, we therefore focused on the analysis of the automated entry (i.e. Division 2, 3, and 4). In terms of parameter setting (see Table 2), annotator specific precision was drawn from \u0393 (k\u03bb ,\u03d1\u03bb ), with assumption that the annotations provided by the best performing algorithm is \u00b15ms away from the reference. Annotators\u2019 biases were considered to be drawn from N (\u00b5\u03c6 ,1/ \u221a\u03b1\u03c6 ), and \u03b1\u03c6 was modelled by \u0393 (k\u03b1 ,\u03d1\u03b1), assuming that the automated annotations tend over-estimate manual annotations as described in previous studies [1, 5, 10]. The true QT interval for each record was assumed to be drawn from N (a,1/ \u221a\nb), where b was modelled by \u0393 (kb,\u03d1b) [4, 9, 12]. Instead of assuming the mean (i.e.\na) of the underlying ground truth to be a fixed scalar, we updated it using a linear regression function, f (w,x), where the coefficients, w, were estimated using equation (8). An intercept was included in f (w,x) to model the overall offset predicted in f , and no particular features were considered in this case (i.e. xi = 1) as we were solely interested in the performance of the model.\n2.5 Methodology of validation and comparison\nThe BCLA inferred precision of individual algorithms was compared with those estimated using the EM algorithm proposed by Raykar et al. [16] (denoted as EM-R) as it served as one of the benchmarking algorithms. Furthermore, the mean and standard deviation (\u00b5\u00b1\u03c3\u00b5ms) of 100 bootstrapped (i.e. random sampling with replacement) samples across records from the BCLA model were compared with the best algorithm (i.e. the algorithm with highest precision after correction of the bias offset), EM-R, and the traditional na\u0131\u0308ve mean and median voting approaches in both simulated and real data sets. The mean absolute error (MAE) of the annotations was also calculated as it provides interpretation of the difference between the estimated and the reference annotations (with a resolution of 1ms). A two-sided Wilcoxon rank sum test (p < 0.0001) was applied to the 100 bootstrapped RMSEs and MAEs, to provide a comparison for the BCLA and EM-R versus other methodologies. In assessing the performance of the BCLA as a function of the number of annotators, a random number of annotators was selected 100 times. This was repeated with the annotator numbers varied from three to the maximum number of annotators in the division. The minimum number of annotators was chosen to be three to allow for obtaining\nresults from the median voting approach. The \u00b5\u00b1\u03c3\u00b5ms of the RMSE of the BCLA, the EM-R, the mean, and the median were calculated and compared."}, {"heading": "3 Results", "text": "The convergence of the BCLA model is guaranteed by providing a threshold using the GEVD as a stopping criteria (see Eqn (13)). In the real data set, the upper bound of the precision derived from the GEVD was 0.04, which was based on the assumption that the best performing annotator is \u00b15ms away from the reference. The number of iteration is dependent on the number of records and the number of annotations. To illustrate the practical utility of our model, it took 7.55 seconds for the BCLA to perform 5,000 iterations when considering a total of 20,712 annotations (Division 2) using MATLAB R2011a on a 2.2GHz Intel(R) i7-2670QM processor. Approximately 2,500 iterations were required to stabilise all the parameters.\n3.1 Simulated data set\nFig. 4(a) shows an example of the inferred results estimated using the EM-R and the BCLA. As the EM-R algorithm modelled jointly the precision (i.e. 1/(\u03c3)2) of each annotator and the noise of underlying ground truth, its estimated \u03c3 cannot represent the real precision of each annotator. Furthermore, EM-R algorithm does not consider the bias of each annotator, and we observe that its estimated values of \u03c3 were well above the line of identity, indicating a consistent over-estimation. In contrast, the BCLA inferred results of \u03c3 lie closely to the line of identity in the plot, indicating that the BCLA model can provide a reliable estimation of the true precision in the simulated results. In addition to precision, the BCLA modelled the bias of each annotator and the results are provided in Fig. 4(b): the estimated biases are very close to the true biases. Although not all the estimated precisions and biases of each annotator were identical to the simulated values, the BCLA model inferred annotations without any prior knowledge of who the best annotator was in an unsupervised manner.\nIn order to compare the accuracy of the inferred labels using the BCLA model, the simulated 548 annotations were bootstrapped 100 times. Each time a RMSE and MAE were generated and compared to the best annotator, mean, EM-R, and median voting strategies. The results are shown in Table 3. The RMSE and MAE results\nshow that BCLA inferred labels significantly outperformed the mean, median, EM-R, and best annotator when compared with the simulated true annotations.\n3.2 Real data set\nFig. 5 (a) to (f) show the inferred precision and bias results estimated using EM-R and BCLA for different automated divisions. As mentioned previously, the EM-R algorithm does not directly model the precision (i.e. 1/(\u03c3)2) of each annotator; its estimated \u03c3 of each annotator produces an offset from the values provided by the reference annotations. In contrast, the BCLA inferred \u03c3 results lie much closer to the line of identity in the Fig. 5 (a), (c), and (e), indicating that the BCLA model can provide a reliable estimation of the true precision of each annotator. In addition, the BCLA modelled the bias of each annotator accurately (see Fig. 5 (b), (d), and (f)). Although automated annotator 3 and 15 were predicted by the BCLA to have lower bias values than those provided by the reference, they are considered to be outliers due to the assumption made in our model: annotators\u2019 biases were drawn from a Gaussian distribution with 10ms mean and 25ms standard deviation. As Fig. 5 (g) shows, the biases of annotator 3 and 15 lie outside the 95% of the area (i.e. \u00b11.96\u03c3 of the mean under the normal distribution) predicted by the BCLA. In the case of annotator 7, its precision was underestimated (see Fig. 5(c) and (e)), which also affected the BCLA\u2019s estimation of its bias value. It was observed that only 3.47% of records were annotated by annotator 7, making it harder for the BCLA to provide a reliable estimation of its precision and bias values. In the evaluation of the inferred labels, the 548 records were bootstrapped 100 times, the RMSEs and MAEs of the BCLA model were generated and compared to the best annotator, mean, EM-R, and median voting approaches for the given reference. The results are displayed in Table 4:\nfor Division 2 using 48 algorithms, the BCLA achieved a RMSE of 12.57\u00b10.67ms, which significantly outperformed other approaches and provides an improvement of 16.48% over the next best approach (EM-R with RMSE of 15.05\u00b10.49ms); in the closed source entry Division 3 using 21 algorithms, the BCLA again exhibited a superior performance over the other methods with a RMSE of 13.90\u00b10.84, and a 19.48% improved error rate over the next best method (RMSE of 17.25\u00b12.33ms). When considering all automated entries (Division 4), the BCLA provided an even more accurate performance than on the other two data sets (Division 2 and 3) as well as over other methods tested with a RMSE of 11.78\u00b10.63ms.\nA further evaluation of the accuracies in terms of RMSE were made as a function of the number of annotators (see Fig. 6). The results were generated by sub-sampling annotators with no replacement 100 times. The benchmarking algorithm, EM-R outperformed mean and median approaches initially but then underperformed when compared to the median approach after 43 algorithms are used. The BCLA model outperformed the other methods being tested with any number of annotators considered. In practice, it is rare to have more than three to five independent algorithms for estimating a label or predicting an event. In the case where only three automated algorithms were randomly selected, the BCLA had on average 9.02%, 19.82%, and 24.56% improvement over the EM-R, median and mean voting approaches respectively.\nAlthough the lowest BCLA RMSE (11.78\u00b10.63ms) in the automated entry is larger than the best-performing human annotator in the Challenge (RMSE = 6.65ms), there were only two other human annotators who achieved a score below 10ms. Furthermore, as the annotations of automated algorithms were independently determined from the reference, whereas the reference includes the best human annotators, it is unsurprising that a combination of the automated algorithms would have worse performance."}, {"heading": "4 Discussion", "text": "In this article, a novel model, Bayesian Continuous-valued Label Aggregator, was proposed to infer the ground truth of continuous-valued labels where accurate and consistent expert annotations are not available. As a proofof-concept, the BCLA was applied to the QT interval estimation from the ECG using labels from the 2006 PhysioNet/Computing in Cardiology Challenge database, and it was compared to the mean, median, and a previously proposed Expectation Maximization label aggregation methods (i.e. EM-R). While accurately predicting each labelling participants bias and precision, the root-mean-square error of the BCLA algorithm was significantly outperformed the best Challenge entry as well as the EM-R, mean, and median voting strategies. There are two key contributions in our approach: i) the BCLA provides an estimation of continuous-valued annotations which is valuable for time-series related data as well as duration of events for physiological data; ii) It introduces a unified framework for combining continuous-valued annotations to infer the underlying ground truth, while jointly modelling annotators\u2019 biases and precisions. The BCLA operates in an unsupervised Bayesian learning framework; no reference data were used to train the model parameters and a separate training and validation test sets were\nnot required. Combining more experienced annotators would therefore provide a better estimation of the inferred ground truth. Importantly though, the BCLA does guarantee a performance better than the best annotator without any prior knowledge of who or what is the best annotator.\nNovel contextual features were introduced in our previous study [26] which allowed an algorithm to learn how varying physiological and noise conditions affect each annotator\u2019s ability to accurately label medical data. The inferred result was shown to provide an improved \u2018gold standard\u2019 for medical annotation tasks even when the ground truth is not available. As the next step, if we incorporate the context into the weighting of annotators, the BCLA is expected to have an even larger impact for noisy data sets or annotators with a variety of specialisations or skill levels. The current model assumed consistent performance of each annotator throughout the records: i.e. that is his/her performance is time-invariant. Although this might not be true over an extended period of time where an annotators performance might improve through learning, or their performance might drop due to inattention or fatigue, the nature of the data sets being considered in this work are such that we can assume that performance across records is approximately consistent for each annotator. Future work will include modelling the performance of each annotator varying across records and through time to provide a more reliable estimation of the aggregated ground truth for data sets in which intra-annotator performance is highly variant.\nOur model of the annotators currently does not factor in the possible dependency/correlation between individual annotators, which might not be the case for automated algorithms. Incorporating a correlation measure into the annotator\u2019s model could possibly allow for a better aggregation of the inferred ground truth. Annotators who are considered to be anomalous (i.e. highly correlated but have large variances and biases) should be penalised with lower weights; expert annotators (i.e. highly correlated but have small variances and biases) should be favourably voted in the model. Finally, combining annotations derived from reliable experts using the BCLA model could potentially lead to improved training for supervised labelling approaches.\nAcknowledgements TZ acknowledges the support of the RCUK Digital Economy Programme grant number EP/G036861/1 and an ARM Scholarship in Sustainable Healthcare Technology through Kellogg College. ND was supported be Cerner Corporation and the UK EPSRC. JB was supported by the UK EPSRC, the Balliol French Anderson Scholarship Fund, and MindChild Medical Inc. DAC is supported by the Royal Academy of Engineering and Balliol College. The final publication is available at Springer via http://dx.doi.org/10.1007/s10439-015-1344-1."}], "references": [{"title": "Variability of QT Interval Measurements in OpioidDependent Patients on Methadone", "author": ["W. Andrew", "V. Michael", "D. Jeff", "G.M. Nair", "C. Plater-Zyberk", "L. Griffith", "J. Ma", "C. Zachos", "M.L. Sivilotti"], "venue": "CJAM 2, 10\u201316", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Nutzung der EKG-Signaldatenbank CARDIODAT der PTB uber das Internet", "author": ["D S.A. Bousseljot R Kreiseler"], "venue": "Biomed Tech 40(1), 317\u2013318", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Dataset of manually measured QT intervals in the electrocardiogram", "author": ["I. Christov", "I. Dotsinsky", "I. Simova", "R. Prokopova", "E. Trendafilova", "S. Naydenov"], "venue": "Biomed Eng Online 5, 31", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Advanced Methods and Tools for ECG Analysis", "author": ["G.D. Clifford", "F. Azuaje", "P.E. McSharry"], "venue": "Engineering in Medicine and Biology. Artech House, Norwood, MA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Highly Automated QT Measurement Techniques in 7 Thorough QT Studies Implemented under ICH E14 Guidelines", "author": ["J.P. Couderc", "C. Garnett", "M. Li", "R. Handzel", "S. McNitt", "X. Xia", "S. Polonsky", "W. Zareba"], "venue": "Ann Noninvasive Electrocardiol 16(1), 13\u201324", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Appl Stat-J Roy St C 28(1), 20\u201328", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1979}, {"title": "Good learners for evil teachers", "author": ["O. Dekel", "O. Shamir"], "venue": "Proc 26th Annu ICML, ICML \u201909, pp. 233\u2013240. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Relation between QT and RR intervals during exercise testing in atrial fibrillation", "author": ["F.A. Ehlert", "J.J. Goldberger", "J.E. Rosenthal", "A.H. Kadish"], "venue": "Am J Cardiol 70(3), 332\u2013338", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "QT interval: how to measure it and what is \u201cnormal", "author": ["I. Goldenberg", "A.J. Moss", "W Zareba"], "venue": "J Cardiovasc Electrophysiol 17(3), 333\u2013336", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabalistic Models for Automated ECG Interval Analysis", "author": ["N.P. Hughes"], "venue": "Ph.D. thesis, University of Oxford", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Relation between QT and RR intervals is highly individual among healthy subjects: implications for heart rate correction of the QT interval", "author": ["M. Malik", "P. Frbom", "V. Batchvarov", "K. Hnatkova", "A.J. Camm"], "venue": "Heart 87(3), 220\u2013228", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Does this patient have community-acquired pneumonia?: Diagnosing pneumonia by history and physical examination", "author": ["J.P. Metlay", "W.N. Kapoor", "M.J. Fine"], "venue": "J Am Coll Cardiol 278(17), 1440\u20131445", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Interobserver variability of dynamic MR imaging of the temporomandibular joint", "author": ["F. Molinari", "L. Gentile", "P. Manicone", "R. Ursini", "L. Raffaelli", "M. Stefanetti", "A. D\u2019Addona", "T. Pirronti", "L. Bonomo"], "venue": "La Radiologia Medica 116(8), 1303\u20131312", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "The PhysioNet/ Computers in Cardiology Challenge 2006: QT interval measurement", "author": ["G.B. Moody", "H. Koch", "U. Steinhoff"], "venue": "Comput Cardiol, pp. 313 \u2013316", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from crowds", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "JMLR pp. 1297\u20131322", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Competency in interpretation of 12-lead electrocardiograms: a summary and appraisal of published evidence", "author": ["S.M. Salerno", "P.C. Alguire", "H.S. Waxman"], "venue": "Ann Intern Med 138(9), 751\u2013760", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Medical Diagnosis Models from Multiple Experts", "author": ["H. Valizadegan", "Q. Nguyen", "M. Hauskrecht"], "venue": "AMIA Annu Symp Proc, pp. 921\u2013930. AMIA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Inaccurate electrocardiographic interpretation of long QT: the majority of physicians cannot recognize a long QT when they see one", "author": ["S. Viskin", "U. Rosovski", "A.J. Sands", "E. Chen", "P.M. Kistler", "J.M. Kalman", "L.R. Chavez", "P.I. Torres", "F.E. CruzF", "O.A. Centurion", "A. Fujiki", "P. Maury", "X. Chen", "A.D. Krahn", "F. Roithinger", "L. Zhang", "G.M. Vincent", "D. Zeltser"], "venue": "Heart Rhythm 2, 569\u2013574", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Sleep-spindle detection: crowdsourcing and evaluating performance of experts, nonexperts and automated methods", "author": ["S.C. Warby", "S.L. Wendt", "P. Welinder", "E.G. Munk", "O. Carrillo", "H.B. Sorensen", "P. Jennum", "P.E. Peppard", "P. Perona", "E. Mignot"], "venue": "Nat methods 11(4), 385\u2013392", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Validation of image segmentation by estimating rater bias and variance", "author": ["S.K. Warfield", "K.H. Zou", "W.M. Wells"], "venue": "Philos T R Soc A 366, 2361\u20132375", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "P. Perona", "S.J. Belongie"], "venue": "Adv Neural Inf Process Syst 23, pp. 2424\u20132432", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Online crowdsourcing: Rating annotators and obtaining cost-effective labels", "author": ["P. Welinder", "P. Perona"], "venue": "IEEE CVPRW, pp. 25\u201332", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Assessment of the performance of electrocardiographic computer programs  Fusing Continuous-valued Medical Labels using a Bayesian Model  17 with the use of a reference data base", "author": ["J. Willems", "P. Arnaud", "J. van Bemmel", "P. Bourdillon", "C. Brohet", "S. Dalla Volta", "J. Andersen", "R. Degani", "B. Denis", "M Demeester"], "venue": "Circulation 71(3), 523 \u2013 534", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1985}, {"title": "Crowdlabel: A Crowd-sourcing Platform for Electrophysiology", "author": ["T. Zhu", "J. Behar", "T. Papastylianou", "G.D. Clifford"], "venue": "Comput Cardiol, vol. 41", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "With human annotation of data, significant intra- and inter-observer disagreements exist [7, 21].", "startOffset": 89, "endOffset": 96}, {"referenceID": 19, "context": "With human annotation of data, significant intra- and inter-observer disagreements exist [7, 21].", "startOffset": 89, "endOffset": 96}, {"referenceID": 6, "context": "As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].", "startOffset": 148, "endOffset": 171}, {"referenceID": 11, "context": "As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].", "startOffset": 148, "endOffset": 171}, {"referenceID": 12, "context": "As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].", "startOffset": 148, "endOffset": 171}, {"referenceID": 15, "context": "As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].", "startOffset": 148, "endOffset": 171}, {"referenceID": 16, "context": "As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].", "startOffset": 148, "endOffset": 171}, {"referenceID": 19, "context": "As a result, there exists a great deal of inter- and intra-expert variability among physicians depending on their experiences and level of training [7, 13, 14, 17, 18, 21].", "startOffset": 148, "endOffset": 171}, {"referenceID": 5, "context": "An effective probabilistic approach to aggregating expert labels which used an Expectation Maximization (EM) algorithm, was first proposed by Dawid and Skene [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 14, "context": "[16] extended this approach to measure the diameter of a suspicious lesion on a medical image using a regression model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Welinder and Perona [23] proposed a Bayesian EM framework for continuous-valued labels, which explicitly modelled the precision only of each annotator to account for their varying skill levels, without modelling the bias of annotators.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "[22] but for binary classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The methodology proposed in the work presented in this article improves on these prior algorithms [16, 22, 23] by introducing the novelty of combining continuous-valued annotations to infer the underlying ground truth, while jointly modelling the annotator\u2019s bias and precision in an unified model using a Bayesian treatment.", "startOffset": 98, "endOffset": 110}, {"referenceID": 20, "context": "The methodology proposed in the work presented in this article improves on these prior algorithms [16, 22, 23] by introducing the novelty of combining continuous-valued annotations to infer the underlying ground truth, while jointly modelling the annotator\u2019s bias and precision in an unified model using a Bayesian treatment.", "startOffset": 98, "endOffset": 110}, {"referenceID": 21, "context": "The methodology proposed in the work presented in this article improves on these prior algorithms [16, 22, 23] by introducing the novelty of combining continuous-valued annotations to infer the underlying ground truth, while jointly modelling the annotator\u2019s bias and precision in an unified model using a Bayesian treatment.", "startOffset": 98, "endOffset": 110}, {"referenceID": 21, "context": "However, most annotators are likely to have some bias regardless of their expertise [23, 25].", "startOffset": 84, "endOffset": 92}, {"referenceID": 23, "context": "However, most annotators are likely to have some bias regardless of their expertise [23, 25].", "startOffset": 84, "endOffset": 92}, {"referenceID": 18, "context": "[20] studied how to combine non-expert annotator\u2019s labels of sleep spindle location, a special pattern in human electroencephalography, through fusing annotations provided by non-experts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "where a can be expressed as a linear regression function f (w,x) with an intercept, and w being the coefficients of the regression [16, 26].", "startOffset": 131, "endOffset": 139}, {"referenceID": 3, "context": "The QT interval is one particular measure of ECG morphology, and refers to the elapsed time between the onset of ventricular depolarisation (the QRS complex) and the T wave offset (ventricular repolarisation) [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 17, "context": "[19] presented the ECGs recorded from two patients with long QT syndrome (LQTS) and from two healthy females to 902 physicians (25 QT experts who had published on the subject, 106 arrhythmia specialists, 329 cardiologists, and 442 noncardiologists) from 12 countries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "it was observed that a few annotators consistently over- or under-estimated the QT interval [25].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Other studies have reported significant intra- and inter-observer variability in QT annotations, ranging from 10 to 30ms [3, 8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 7, "context": "Other studies have reported significant intra- and inter-observer variability in QT annotations, ranging from 10 to 30ms [3, 8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 13, "context": "The data were drawn from the QT interval annotations generated by participants in the 2006 PhysioNet/Computing in Cardiology (PCinC) Challenge [15] for labelling QT intervals with reference to Lead II in each of the 548 recordings in the Physikalisch-Technische Bundesanstalt Diagnostic ECG Database (PTBDB) [2].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "The data were drawn from the QT interval annotations generated by participants in the 2006 PhysioNet/Computing in Cardiology (PCinC) Challenge [15] for labelling QT intervals with reference to Lead II in each of the 548 recordings in the Physikalisch-Technische Bundesanstalt Diagnostic ECG Database (PTBDB) [2].", "startOffset": 308, "endOffset": 311}, {"referenceID": 1, "context": "The diagnostic classifications of ECG morphologies mainly included myocardial infarction, heart failure, bundle branch block, and dysrhythmia as stated in Bousseljot and Kreiseler [2].", "startOffset": 180, "endOffset": 183}, {"referenceID": 22, "context": "The reference annotations were generated from Division 1\u2019s entries using a maximum of 15 participants by taking the \u201cmedian self-centering approach\u201d as reported by the competition organisers as detailed in [24].", "startOffset": 206, "endOffset": 210}, {"referenceID": 0, "context": "N (\u03bc\u03c6 ,1/ \u03b1\u03c6 ), and \u03b1\u03c6 was modelled by \u0393 (k\u03b1 ,\u03b8\u03b1), assuming that the automated annotations tend over-estimate manual annotations as described in previous studies [1, 5, 10].", "startOffset": 162, "endOffset": 172}, {"referenceID": 4, "context": "N (\u03bc\u03c6 ,1/ \u03b1\u03c6 ), and \u03b1\u03c6 was modelled by \u0393 (k\u03b1 ,\u03b8\u03b1), assuming that the automated annotations tend over-estimate manual annotations as described in previous studies [1, 5, 10].", "startOffset": 162, "endOffset": 172}, {"referenceID": 9, "context": "N (\u03bc\u03c6 ,1/ \u03b1\u03c6 ), and \u03b1\u03c6 was modelled by \u0393 (k\u03b1 ,\u03b8\u03b1), assuming that the automated annotations tend over-estimate manual annotations as described in previous studies [1, 5, 10].", "startOffset": 162, "endOffset": 172}, {"referenceID": 3, "context": "The true QT interval for each record was assumed to be drawn from N (a,1/ \u221a b), where b was modelled by \u0393 (kb,\u03b8b) [4, 9, 12].", "startOffset": 114, "endOffset": 124}, {"referenceID": 8, "context": "The true QT interval for each record was assumed to be drawn from N (a,1/ \u221a b), where b was modelled by \u0393 (kb,\u03b8b) [4, 9, 12].", "startOffset": 114, "endOffset": 124}, {"referenceID": 10, "context": "The true QT interval for each record was assumed to be drawn from N (a,1/ \u221a b), where b was modelled by \u0393 (kb,\u03b8b) [4, 9, 12].", "startOffset": 114, "endOffset": 124}, {"referenceID": 14, "context": "[16] (denoted as EM-R) as it served as one of the benchmarking algorithms.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "With the rapid increase in volume of time series medical data available through wearable devices, there is a need to employ automated algorithms to label data. Examples of labels include interventions, changes in activity (e.g. sleep) and changes in physiology (e.g. arrhythmias). However, automated algorithms tend to be unreliable resulting in lower quality care. Expert annotations are scarce, expensive, and prone to significant interand intra-observer variance. To address these problems, a Bayesian Continuous-valued Label Aggregator(BCLA) is proposed to provide a reliable estimation of label aggregation while accurately infer the precision and bias of each algorithm. The BCLA was applied to QT interval (pro-arrhythmic indicator) estimation from the electrocardiogram using labels from the 2006 PhysioNet/Computing in Cardiology Challenge database. It was compared to the mean, median, and a previously proposed Expectation Maximization (EM) label aggregation approaches. While accurately predicting each labelling algorithm\u2019s bias and precision, the root-mean-square error of the BCLA was 11.78\u00b10.63ms, significantly outperforming the best Challenge entry (15.37\u00b12.13ms) as well as the EM, mean, and median voting strategies (14.76\u00b10.52ms, 17.61\u00b10.55ms, and 14.43\u00b10.57ms respectively with p < 0.0001). The BCLA could therefore provide accurate estimation for medical continuous-valued label tasks in an unsupervised manner even when the ground truth is not available.", "creator": "LaTeX with hyperref package"}}}