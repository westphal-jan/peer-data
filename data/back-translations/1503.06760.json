{"id": "1503.06760", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Unsupervised POS Induction with Word Embeddings", "abstract": "Unsupervised word embedding has proven to be a valuable feature in monitored learning problems, but its role in unsupervised problems has been less thoroughly studied. In this paper, we show that embedding can also add value to the problem of unsupervised POS embedding. In two representative models of POS embedding, we replace multinomic vocabulary distributions with multivariate Gaussian distributions over word embedding and observe consistent improvements in eight languages. We also analyze the impact of different decisions while inducing word embedding on \"downstream\" POS embedding results.", "histories": [["v1", "Mon, 23 Mar 2015 18:32:56 GMT  (37kb,D)", "http://arxiv.org/abs/1503.06760v1", "NAACL 2015"]], "COMMENTS": "NAACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chu-cheng lin", "waleed ammar", "chris dyer", "lori s levin"], "accepted": true, "id": "1503.06760"}, "pdf": {"name": "1503.06760.pdf", "metadata": {"source": "CRF", "title": "Unsupervised POS Induction with Word Embeddings", "authors": ["Chu-Cheng Lin", "Waleed Ammar", "Chris Dyer", "Lori Levin"], "emails": ["chuchenl@cs.cmu.edu", "wammar@cs.cmu.edu", "cdyer@cs.cmu.edu", "lsl@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised POS induction is the problem of assigning word tokens to syntactic categories given only a corpus of untagged text. In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al., 2014). In each model, instead of using a conditional multinomial distribution2 to generate a word token wi \u2208 V given a POS tag ti \u2208 T , we use a conditional Gaussian distribution and generate a d-dimensional word embedding vwi \u2208 Rd given ti.\n1Unlike Yatbaz et al. (2014), we leverage easily obtainable and widely used embeddings of word types.\n2Also known as a categorical distribution.\nOur findings suggest that, in both models, substantial improvements are possible when word embeddings are used rather than opaque word types. However, the independence assumptions made by the model used to induce embeddings strongly determines its effectiveness for POS induction: embedding models that model short-range context are more effective than those that model longer-range contexts. This result is unsurprising, but it illustrates the lack of an evaluation metric that measures the syntactic (rather than semantic) information in word embeddings. Our results also confirm the conclusions of Sirts et al. (2014) who were likewise able to improve POS induction results, albeit using a custom clustering model based on the the distancedependent Chinese restaurant process (Blei and Frazier, 2011).\nOur contributions are as follows: (i) reparameterization of token-level POS induction models to use word embeddings; and (ii) a systematic evaluation of word embeddings with respect to the syntactic information they contain."}, {"heading": "2 Vector Space Word Embeddings", "text": "Word embeddings represent words in a language\u2019s vocabulary as points in a d-dimensional space such that nearby words (points) are similar in terms of their distributional properties. A variety of techniques for learning embeddings have been proposed, e.g., matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al., 2011; Collobert and Weston, 2008).\nFor the POS induction task, we specifically need embeddings that capture syntactic similarities.\nar X\niv :1\n50 3.\n06 76\n0v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\nTherefore we experiment with two types of embeddings that are known for such properties:\n\u2022 Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word. Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information. We confirm this finding in our experiments.\n\u2022 Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context.\nWe use the tool word2vec3 and Ling et al. (2015)\u2019s modified version4 to generate both plain and structured skip-gram embeddings in nine languages."}, {"heading": "3 Models for POS Induction", "text": "In this section, we briefly review two classes of models used for POS induction (HMMs and CRF autoencoders), and explain how to generate word embedding observations in each class. We will represent a sentence of length ` as w = \u3008w1, w2, . . . , w`\u3009 \u2208 V ` and a sequence of tags as t = \u3008t1, t2, . . . , t`\u3009 \u2208 T `. The embeddings of word type w \u2208 V will be written as vw \u2208 Rd."}, {"heading": "3.1 Hidden Markov Models", "text": "The hidden Markov model with multinomial emissions is a classic model for POS induction. This model makes the assumption that a latent Markov process with discrete states representing POS categories emits individual words in the vocabulary according to state (i.e., tag) specific emission distributions. An HMM thus defines the following joint distribution over sequences of observations and tags:\np(w, t) = \u220f\u0300 i=1 p(ti | ti\u22121)\u00d7 p(wi | ti) (1)\nwhere distributions p(ti | ti\u22121) represents the transition probability and p(wi | ti) is the emission probability, the probability of a particular tag generating the word at position i.5\n3https://code.google.com/p/word2vec/ 4https://github.com/wlin12/wang2vec/ 5Terms for the starting and stopping transition probabilities\nare omitted for brevity.\nWe consider two variants of the HMM as baselines:\n\u2022 p(wi | ti) is parameterized as a \u201cna\u0131\u0308ve multinomial\u201d distribution with one distinct parameter for each word type.\n\u2022 p(wi | ti) is parameterized as a multinomial logistic regression model with hand-engineered features as detailed in (Berg-Kirkpatrick et al., 2010).\nGaussian Emissions. We now consider incorporating word embeddings in the HMM. Given a tag t \u2208 T , instead of generating the observed word w \u2208 V , we generate the (pre-trained) embedding vw \u2208 Rd of that word. The conditional probability density assigned to vw | t follows a multivariate Gaussian distribution with mean \u00b5t and covariance matrix \u03a3t:\np(vw;\u00b5t,\u03a3t) = exp\n( \u221212(vw \u2212 \u00b5t) >\u03a3\u22121t (vw \u2212 \u00b5t) )\u221a\n(2\u03c0)d|\u03a3t| (2)\nThis parameterization makes the assumption that embeddings of words which are often tagged as t are concentrated around some point \u00b5t \u2208 Rd, and the concentration decays according to the covariance matrix \u03a3t.6\nNow, the joint distribution over a sequence of observations v = \u3008vw1 ,vw2 . . . ,vw`\u3009 (which corresponds to word sequence w = \u3008w1, w2, . . . , w`, \u3009) and a tag sequence t = \u3008t1, t2 . . . , t`\u3009 becomes:\np(v, t) = \u220f\u0300 i=1 p(ti | ti\u22121)\u00d7 p(vwi ;\u00b5ti ,\u03a3ti)\nWe use the Baum\u2013Welch algorithm to fit the \u00b5t and \u03a3ti parameters. In every iteration, we update \u00b5t\u2217 as follows:\n\u00b5newt\u2217 =\n\u2211 v\u2208T \u2211 i=1...` p(ti = t\n\u2217 | v)\u00d7 vwi\u2211 v\u2208T \u2211 i=1...` p(ti = t \u2217 | v) (3)\nwhere T is a data set of word embedding sequences v each of length |v| = `, and p(ti = t\u2217 | v) is the\n6\u201cessentially, all models are wrong, but some are useful\u201d \u2013 George E. P. Box\nposterior probability of label t\u2217 at position i in the sequence v. Likewise the update to \u03a3t\u2217 is:\n\u03a3newt\u2217 =\n\u2211 v\u2208T \u2211 i=1...` p(ti = t\n\u2217 | v)\u00d7 \u03b4\u03b4>\u2211 v\u2208T \u2211 i=1...` p(ti = t \u2217 | v) (4)\nwhere \u03b4 = vwi \u2212 \u00b5newt\u2217 ."}, {"heading": "3.2 Conditional Random Field Autoencoders", "text": "The second class of models this work extends is called CRF autoencoders, which we recently proposed in (Ammar et al., 2014). It is a scalable family of models for feature-rich learning from unlabeled examples. The model conditions on one copy of the structured input, and generates a reconstruction of the input via a set of interdependent latent variables which represent the linguistic structure of interest. As shown in Eq. 5, the model factorizes into two distinct parts: the encoding model p(t | w) and the reconstruction model p(w\u0302 | t); where w is the structured input (e.g., a token sequence), t is the linguistic structure of interest (e.g., a sequence of POS tags), and w\u0302 is a generic reconstruction of the input. For POS induction, the encoding model is a linearchain CRF with feature vector \u03bb and local feature functions f .\np(w\u0302,t | w) = p(t | w)\u00d7 p(w\u0302 | t)\n\u221d p(w\u0302 | t)\u00d7 exp\u03bb \u00b7 |w|\u2211 i=1 f(ti, ti\u22121,w) (5)\nIn (Ammar et al., 2014), we explored two kinds of reconstructions w\u0302: surface forms and Brown clusters (Brown et al., 1992), and used \u201cstupid multinomials\u201d as the underlying distributions for regenerating w\u0302.\nGaussian Reconstruction. In this paper, we use d-dimensional word embedding reconstructions w\u0302i = vwi \u2208 Rd, and replace the multinomial distribution of the reconstruction model with the multivariate Gaussian distribution in Eq. 2. We again use the Baum\u2013Welch algorithm to estimate \u00b5t\u2217 and \u03a3t\u2217 similar to Eq. 3. The only difference is that posterior label probabilities are now conditional on both the input sequence w and the embeddings sequence v, i.e., replace p(ti = t\u2217 | v) in Eq. 2 with p(ti = t \u2217 | w,v)."}, {"heading": "4 Experiments", "text": "In this section, we attempt to answer the following questions:\n\u2022 \u00a74.1: Do syntactically-informed word embeddings improve POS induction? Which model performs best?\n\u2022 \u00a74.2: What kind of word embeddings are suitable for POS induction?"}, {"heading": "4.1 Choice of POS Induction Models", "text": "Here, we compare the following models for POS induction:\n\u2022 Baseline: HMM with multinomial emissions (Kupiec, 1992),\n\u2022 Baseline: HMM with log-linear emissions (BergKirkpatrick et al., 2010),\n\u2022 Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7\n\u2022 Proposed: HMM with Gaussian emissions, and\n\u2022 Proposed: CRF autoencoder with Gaussian reconstructions.\nData. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012). This is the same set up we used in (Ammar et al., 2014).\nSetup. In this section, we used skip-gram (i.e., word2vec) embeddings with a context window size = 1 and with dimensionality d = 100, trained with the largest corpora for each language in (Quasthoff et al., 2006), in addition to the plain\n7We use the configuration with best performance which reconstructs Brown clusters.\ntext used to train the POS induction models.8 In the proposed models, we only show results for estimating \u00b5t, assuming a diagonal covariance matrix \u03a3t(k, k) = 0.45\u2200k \u2208 {1, . . . , d}.9 While the CRF autoencoder with multinomial reconstructions were carefully initialized as discussed in (Ammar et al., 2014), CRF autoencoder with Gaussian reconstructions were initialized uniformly at random in [\u22121, 1]. All HMM models were also randomly initialized. We tuned all hyperparameters on the English PTB corpus, then fixed them for all languages.\nEvaluation. We use the V-measure evaluation metric (Rosenberg and Hirschberg, 2007) to evaluate the predicted syntactic classes at the token level.10\nResults. The results in Fig. 1 (left) clearly suggest that we can use word embeddings to improve POS induction. Surprisingly, the feature-less Gaussian HMM model outperforms the strong feature-\n8We used the corpus/tokenize-anything.sh script in the cdec decoder (Dyer et al., 2010) to tokenize the corpora from (Quasthoff et al., 2006). The other corpora were already tokenized. In Arabic and Italian, we found a lot of discrepancies between the tokenization used for inducing word embeddings and the tokenization used for evaluation. We expect our results to improve with consistent tokenization.\n9Surprisingly, we found that estimating \u03a3t significantly degrades the performance. This may be due to overfitting (Shinozaki and Kawahara, 2007). Possible remedies include using a prior (Gauvain and Lee, 1994).\n10We found the V-measure results to be consistent with the many-to-one evaluation metric (Johnson, 2007). We only show one set of results for brevity.\nrich baselines: Multinomial Featurized HMM and Multinomial CRF Autoencoder. One explanation is that our word embeddings were induced using larger unlabeled corpora than those used to train the POS induction models. The best results are obtained using both word embeddings and feature-rich models using the Gaussian CRF autoencoder model. This set of results suggest that word embeddings and hand-engineered features play complementary roles in POS induction. It is worth noting that the CRF autoencoder model with Gaussian reconstructions did not require careful initialization.11"}, {"heading": "4.2 Choice of Embeddings", "text": "Standard skip-gram vs. structured skip-gram. On Gaussian HMMs, structured skip-gram embeddings score moderately higher than standard skipgrams. And as the context window size gets larger the gap widens. The reason may be that structured skip-gram embeddings give each position within the context window its own project matrix, so the smearing effect is not as pronounced as the window grows when compared to the standard embeddings. However the best performance is still obtained when the window is small.12\n11In (Ammar et al., 2014), we found that careful initialization for the CRF autoencoder model with multinomial reconstructions is necessary.\n12In preliminary experiments, we also compared standard skip-gram embeddings to SENNA embeddings (Collobert et al., 2011) (which are trained in a semi-supervised multi-task learning setup, with one task being POS tagging) on a subset of the English PTB corpus. As expected, the induced POS tags\nDimensions = 20 vs. 200. We also varied the number of dimensions in the word vectors (d \u2208 {20, 50, 100, 200}). The best V-measure we obtain is 0.504 (d = 20) and the worst is 0.460 (d = 100). However, we did not observe a consistent pattern as shown in Fig. 3.\nWindow size = 1 vs. 16. Finally, we varied the window size for the context surrounding target words (w \u2208 {1, 2, 4, 8, 16}). w = 1 yields the best average V-measure across the eight languages as shown in Fig. 2. This is true for both standard and structured skip-gram models. Notably, larger window sizes appear to produce word embeddings with less syntactic information. This result confirms the observations of Bansal et al. (2014)."}, {"heading": "4.3 Discussion", "text": "We have shown that (re)generating word embeddings does much better than generating opaque word types in unsupervised POS induction. At a high level, this confirms prior findings that unsupervised word embeddings capture syntactic properties of words, and shows that different embeddings capture more syntactically salient information than others. As such, we contend that unsupervised POS induction can be seen as a diagnostic metric for assessing the syntactic quality of embeddings.\nTo get a better understanding of what the multivariate Gaussian models have learned, we conduct a hill-climbing experiment on our English dataset. We\nare much better when using SENNA embeddings, yielding a Vmeasure score of 0.57 compared to 0.51 for skip-gram embeddings. Since SENNA embeddings are only available in English, we did not include it in the comparison in Fig. 1.\nseed each POS category with the average vector of 10 randomly sampled words from that category and train the model. Seeding unsurprisingly improves tagging performance. We also find words that are the nearest to the centroids generally agree with the correct category label, which validate our assumption that syntactically similar words tend to cluster in the high-dimensional embedding space. It also shows that careful initialization of model parameters can bring further improvements.\nHowever we also find that words that are close to the centroid are not necessarily representative of what linguists consider to be prototypical. For example, Hopper and Thompson (1983) show that physical, telic, past tense verbs are more prototypical with respect to case marking, agreement, and other syntactic behavior. However, the verbs nearest our centroid all seem rather abstract. In English, the nearest 5 words in the verb category are entails, aspires, attaches, foresees, deems. This may be because these words seldom serve functions other than verbs; and placing the centroid around them incurs less penalty (in contrast to physical verbs, e.g. bite, which often also act as nouns). Therefore one should be cautious in interpreting what is prototypical about them."}, {"heading": "5 Conclusion", "text": "We propose using a multivariate Gaussian model to generate vector space representations of observed words in generative or hybrid models for POS induction, as a superior alternative to using multinomial distributions to generate categorical word types. We find the performance from a simple Gaussian HMM competitive with strong feature-rich baselines. We\nfurther show that substituting the emission part of the CRF autoencoder can bring further improvements. We also confirm previous findings which suggest that smaller context windows in skip-gram models result in word embeddings which encode more syntactic information. It would be interesting to see if we can apply this approach to other tasks which require generative modeling of textual observations such as language modeling and grammar induction."}, {"heading": "Acknowledgements", "text": "This work was sponsored by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant numbers W911NF-112-0042 and W911NF-10-1-0533. The statements made herein are solely the responsibility of the authors."}], "references": [{"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Ammar et al.2014] Waleed Ammar", "Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Painless unsupervised learning with features", "author": ["Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein"], "venue": "In Proc. of NAACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Distance dependent Chinese restaurant processes. JMLR", "author": ["Blei", "Frazier2011] David M. Blei", "Peter I. Frazier"], "venue": null, "citeRegEx": "Blei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2011}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In CoNLL-X", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Dean Foster", "Lyle Ungar"], "venue": "In NIPS,", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free transla", "author": ["Dyer et al.2010] Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains", "author": ["Gauvain", "Lee1994] J. Gauvain", "Chin-Hui Lee"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Gauvain et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Gauvain et al\\.", "year": 1994}, {"title": "The iconicity of the universal categories \u201cnoun\u201d and \u201cverb", "author": ["Hopper", "Thompson1983] Paul Hopper", "Sandra Thompson"], "venue": "Iconicity in Syntax: Proceedings of a symposium on iconicity in syntax", "citeRegEx": "Hopper et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Hopper et al\\.", "year": 1983}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["Mark Johnson"], "venue": "In Proc. of EMNLP", "citeRegEx": "Johnson.,? \\Q2007\\E", "shortCiteRegEx": "Johnson.", "year": 2007}, {"title": "Robust part-ofspeech tagging using a hidden Markov model", "author": ["Julian Kupiec"], "venue": "Computer Speech and Language,", "citeRegEx": "Kupiec.,? \\Q1992\\E", "shortCiteRegEx": "Kupiec.", "year": 1992}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proc. of NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "In Proc. of LREC,", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Corpus portal for search in monolingual corpora", "author": ["Matthias Richter", "Christian Biemann"], "venue": "In Proc. of LREC,", "citeRegEx": "Quasthoff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Quasthoff et al\\.", "year": 2006}, {"title": "V-measure: A conditional entropy-based external cluster evaluation", "author": ["Rosenberg", "Hirschberg2007] Andrew Rosenberg", "Julia Hirschberg"], "venue": null, "citeRegEx": "Rosenberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2007}, {"title": "HMM training based on CV-EM and CV gaussian mixture optimization", "author": ["Shinozaki", "Kawahara2007] T. Shinozaki", "T. Kawahara"], "venue": "In Proc. of the 2007 ASRU Workshop,", "citeRegEx": "Shinozaki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shinozaki et al\\.", "year": 2007}, {"title": "POS induction with distributional and morphological information using a distance-dependent Chinese restaurant process", "author": ["Sirts et al.2014] Kairit Sirts", "Jacob Eisenstein", "Micha Elsner", "Sharon Goldwater"], "venue": "In Proc. of ACL", "citeRegEx": "Sirts et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sirts et al\\.", "year": 2014}, {"title": "Ukwabelana: An open-source morphological Zulu corpus", "author": ["Andrew van der Spuy", "Peter A. Flach"], "venue": "In Proc. of COLING,", "citeRegEx": "Spiegler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spiegler et al\\.", "year": 2010}, {"title": "Unsupervised instance-based part of speech induction using probable substitutes", "author": ["Enis R\u0131fat Sert", "Deniz Yuret"], "venue": "In Proc. of COLING", "citeRegEx": "Yatbaz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al.", "startOffset": 148, "endOffset": 162}, {"referenceID": 0, "context": "In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al., 2014).", "startOffset": 225, "endOffset": 245}, {"referenceID": 22, "context": "Unlike Yatbaz et al. (2014), we leverage easily obtainable and widely used embeddings of word types.", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "Our results also confirm the conclusions of Sirts et al. (2014) who were likewise able to improve POS induction results, albeit using a custom clustering model based on the the distancedependent Chinese restaurant process (Blei and Frazier, 2011).", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": ", matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al.", "startOffset": 23, "endOffset": 70}, {"referenceID": 9, "context": ", matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al.", "startOffset": 23, "endOffset": 70}, {"referenceID": 16, "context": "\u2022 Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word.", "startOffset": 23, "endOffset": 45}, {"referenceID": 1, "context": "Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "\u2022 Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 16, "context": ", 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context.", "startOffset": 49, "endOffset": 71}, {"referenceID": 15, "context": "We use the tool word2vec3 and Ling et al. (2015)\u2019s modified version4 to generate both plain and structured skip-gram embeddings in nine languages.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "\u2022 p(wi | ti) is parameterized as a multinomial logistic regression model with hand-engineered features as detailed in (Berg-Kirkpatrick et al., 2010).", "startOffset": 118, "endOffset": 149}, {"referenceID": 0, "context": "The second class of models this work extends is called CRF autoencoders, which we recently proposed in (Ammar et al., 2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 0, "context": "In (Ammar et al., 2014), we explored two kinds of reconstructions \u0175: surface forms and Brown clusters (Brown et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 4, "context": ", 2014), we explored two kinds of reconstructions \u0175: surface forms and Brown clusters (Brown et al., 1992), and used \u201cstupid multinomials\u201d as the underlying distributions for regenerating \u0175.", "startOffset": 86, "endOffset": 106}, {"referenceID": 14, "context": "\u2022 Baseline: HMM with multinomial emissions (Kupiec, 1992),", "startOffset": 43, "endOffset": 57}, {"referenceID": 0, "context": "\u2022 Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7", "startOffset": 61, "endOffset": 81}, {"referenceID": 22, "context": ", 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu).", "startOffset": 86, "endOffset": 109}, {"referenceID": 17, "context": "For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012).", "startOffset": 200, "endOffset": 221}, {"referenceID": 0, "context": "This is the same set up we used in (Ammar et al., 2014).", "startOffset": 35, "endOffset": 55}, {"referenceID": 18, "context": ", word2vec) embeddings with a context window size = 1 and with dimensionality d = 100, trained with the largest corpora for each language in (Quasthoff et al., 2006), in addition to the plain", "startOffset": 141, "endOffset": 165}, {"referenceID": 0, "context": "9 While the CRF autoencoder with multinomial reconstructions were carefully initialized as discussed in (Ammar et al., 2014), CRF autoencoder with Gaussian reconstructions were initialized uniformly at random in [\u22121, 1].", "startOffset": 104, "endOffset": 124}, {"referenceID": 10, "context": "sh script in the cdec decoder (Dyer et al., 2010) to tokenize the corpora from (Quasthoff et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 18, "context": ", 2010) to tokenize the corpora from (Quasthoff et al., 2006).", "startOffset": 37, "endOffset": 61}, {"referenceID": 13, "context": "We found the V-measure results to be consistent with the many-to-one evaluation metric (Johnson, 2007).", "startOffset": 87, "endOffset": 102}, {"referenceID": 0, "context": "In (Ammar et al., 2014), we found that careful initialization for the CRF autoencoder model with multinomial reconstructions is necessary.", "startOffset": 3, "endOffset": 23}, {"referenceID": 7, "context": "In preliminary experiments, we also compared standard skip-gram embeddings to SENNA embeddings (Collobert et al., 2011) (which are trained in a semi-supervised multi-task learning setup, with one task being POS tagging) on a subset of the English PTB corpus.", "startOffset": 95, "endOffset": 119}, {"referenceID": 1, "context": "This result confirms the observations of Bansal et al. (2014).", "startOffset": 41, "endOffset": 62}], "year": 2015, "abstractText": "Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the effect of various choices while inducing word embeddings on \u201cdownstream\u201d POS induction results.", "creator": "LaTeX with hyperref package"}}}