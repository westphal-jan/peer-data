{"id": "1504.03101", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2015", "title": "Convex Learning of Multiple Tasks and their Structure", "abstract": "Reducing human supervision is a key problem in machine learning, and a natural approach is to exploit the relationships (structure) between different tasks. This is the idea at the core of multi-task learning. In this context, a fundamental question is how to incorporate the task structure into the learning problem. We address this question by examining a general arithmetic framework that allows a priori knowledge of the task structure to be encoded in the form of a convex punishment; in this context, a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned taking into account a convex optimization problem that can be addressed using block coordinate methods such as alternating minimization, and for which we can demonstrate convergence to the global minimum.", "histories": [["v1", "Mon, 13 Apr 2015 09:13:23 GMT  (619kb)", "https://arxiv.org/abs/1504.03101v1", "26 pages, 1 figure, 2 tables"], ["v2", "Fri, 17 Apr 2015 20:03:21 GMT  (891kb)", "http://arxiv.org/abs/1504.03101v2", "26 pages, 1 figure, 2 tables"]], "COMMENTS": "26 pages, 1 figure, 2 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["carlo ciliberto", "youssef mroueh", "tomaso a poggio", "lorenzo rosasco"], "accepted": true, "id": "1504.03101"}, "pdf": {"name": "1504.03101.pdf", "metadata": {"source": "CRF", "title": "Convex Learning of Multiple Tasks and their Structure", "authors": ["Carlo Ciliberto", "Youssef Mroueh", "Tomaso Poggio", "Lorenzo Rosasco"], "emails": ["(cciliber@mit.edu)", "(tp@ai.mit.edu)", "(lrosasco@mit.edu)"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 4.\n03 10\n1v 2\n[ cs\n.L G\n] 1"}, {"heading": "1 Introduction", "text": "Current machine learning systems achieve remarkable results in several challenging tasks, but are limited by the amount of human supervision required. Leveraging similarity among different problems is widely acknowledged to be a key approach to reduce the need for supervised data. Indeed, this idea is at the basis of multi-task learning, where the joint solution of different problems (tasks) has the potential to exploit tasks relatedness (structure) to improve learning accuracy. This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.g. [1] and references therein), with connections to structured learning [6, 34]. The focus of our study is the development of a general regularization framework to learn multiple tasks as well as their structure. Following [25, 15] we consider a setting where tasks are modeled as the components of a vector-valued function and their\n\u2217Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Via Morego, 30, 16100, Genova, Italy, (cciliber@mit.edu)\n\u2020Poggio Lab, Massachusetts Institute of Technology, Massachusetts Ave. 77, 02139, Cambridge, MA, USA (tp@ai.mit.edu)\n\u2021DIBRIS, Universita\u0300 di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.edu)\nstructure corresponds to the choice of suitable functional spaces. Exploiting the theory of reproducing kernel Hilbert spaces for vector-valued functions (RKHSvv) [25], we consider and analyze a flexible regularization framework, within which a variety of previously proposed approaches can be recovered as special cases, see e.g. [19, 24, 26, 37, 14, 31]. Our main technical contribution is a unifying study of the minimization problem corresponding to such a regularization framework. More precisely, we devise an optimization approach that can efficiently compute a solution and for which we prove convergence under weak assumptions. Our approach is based on a barrier method that is combined with block coordinate descent techniques [33, 30]. In this sense our analysis generalizes the results in [3] for which a low-rank assumption was considered; however the extension is not straightforward, since we consider a much larger class of regularization schemes (any convex penalty). Up to our knowledge, this is the first result in multi-task learning proving the convergence of alternating minimization schemes for such a general family of problems. The RKHSvv setting allows to naturally deal both with linear and non-linear models and the approach we propose provides a general computational framework for learning output kernels as formalized in [14]. The rest of the paper is organized as follows: in Sec 2 we review basic ideas of regularization in RKHSvv. In Sec. 2.3 we discuss the equivalence of different approaches to encode known structures among multiple tasks. In Sec. 3 we discuss a general framework for learning multiple tasks and their relations where we consider a wide family of structure-inducing penalties and study an optimization strategy to solve them. This setting allows us, in Sec. 4, to recover several previous methods as special cases. Finally in Sec. 5 we evaluate the performance of the optimization method proposed.\nNotation. With Sn++ \u2282 Sn+ \u2282 Sn \u2282 Rn\u00d7n we denote respectively the space of positive definite, positive semidefinite (PSD) and symmetric n\u00d7n real-valued matrices. On denotes the space of orthonormaln\u00d7n matrices. For any square matrix M \u2208 Rn\u00d7n and p \u2265 1, we denote by \u2016M\u2016p = ( \u2211n i=1 \u03c3i(M)\np)1/p the p-Schatten norm of M , where \u03c3i(M) is the i-th largest singular value of M . For any M \u2208 Rn\u00d7m, M\u22a4 denotes the transpose of M . For any PSD matrix A \u2208 Sn+, A\u2020 denotes the pseudoinverse of A. We denote by In \u2208 Sn++ the n \u00d7 n identity matrix. The notation Ran(M) \u2286 Rm identifies the range of columns of a matrix M \u2208 Rm\u00d7n."}, {"heading": "2 Background", "text": "We study the problem of jointly learning multiple tasks by modeling individual taskpredictors as the components of a vector-valued function. Let us assume to have T supervised scalar learning problems (or tasks), each with a \u201ctraining\u201d set of inputoutput observations St = {(xit, yit)}nti=1 with xit \u2208 X input space and yit \u2208 Y output space1. Given a loss function L : R \u00d7 R \u2192 R+ that measures the per-task prediction\n1To avoid clutter in the notation, we have restricted ourselves to the typical situation where all tasks share same input and output spaces, i.e. Xt = X and Yt \u2286 R.\nerrors, we want to solve the following joint regularized learning problem\nminimize f\u2208H\nT\u2211\nt=1\n1\nnt\nnt\u2211\ni=1\nL(y(t)i , ft(x (t) i )) + \u03bb\u2016f\u20162H (1)\nwhere H is an Hilbert space of vector-valued functions f : X \u2192 YT with scalar components ft : X \u2192 Y . In order to define a suitable space of hypotheses H, in this section we briefly recall concepts from the theory of reproducing kernel Hilbert spaces for vector-valued functions (RKHSvv) and corresponding regularization theory, which plays a key role in our work. In particular, we focus on a class of reproducing kernels (known as separable kernels) that can be designed to encode specific tasks structures (see [15, 2] and Sec. 2.3). Interestingly, separable kernels are related to ideas such as defining a metric on the output space or a label encoding in multi-label problems (see Sec. 2.3)\nRemark 2.1 (Multi-task and multi-label learning). Multi-label learning is a class of supervised learning problems in which the goal is to associate input examples with a label or a set of labels chosen from a discrete set. In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28]. Multi-label learning and multi-task learning are strongly related. Indeed, surrogate problems typically consist in a set of distinct supervised learning problems (or tasks) that are solved simultaneously and therefore have a natural formulation in the multitask setting. For instance, in multi-class classification problems the \u201cOne vs All\u201d strategy is often adopted, which consists in solving a set of multiple binary classification problems, one for each class."}, {"heading": "2.1 Learning Multiple Tasks with RKHSvv", "text": "In the scalar setting, reproducing kernel Hilbert spaces have already been proved to be a powerful tool for machine learning applications. Interestingly, the theory of RKHSvv and corresponding Tikhonov regularization scheme follow closely the derivation in the scalar case.\nDefinition 2.2. Let (H, \u3008\u00b7, \u00b7\u3009H) be a Hilbert space of functions from X to RT . A symmetric, positive definite, matrix-valued function \u0393 : X \u00d7 X \u2192 RT\u00d7T is called a reproducing kernel for H if for all x \u2208 X , c \u2208 RT and f \u2208 H we have that \u0393(x, \u00b7)c \u2208 H and the following reproducing property holds \u3008f(x), c\u3009RT = \u3008f,\u0393(x, \u00b7)c\u3009H.\nIn analogy to the scalar setting, it can be proved (see [25]) that the Representer Theorem holds also for regularization in RKHSvv. In particular we have that any solution of the learning problem introduced in Eq. (1) can be written in the form\nf(x) =\nT\u2211\nt=1\nnt\u2211\ni=1\n\u0393(x, x (t) i )c (t) i (2)\nwith c(t)i \u2208 RT coefficient vectors. The choice of kernel \u0393 induces a joint representation of the inputs as well as a structure among the output components [1]; In the rest of the paper we will focus on so-called separable kernels, where these two aspects are factorized. In Section 3, we will see how separable kernels provide a natural way to learn the tasks structure as well as the tasks."}, {"heading": "2.2 Separable Kernels", "text": "Separable (reproducing) kernels are functions of the form\u0393(x, x\u2032) = k(x, x\u2032)A \u2200x, x\u2032 \u2208 X where k : X \u00d7 X \u2192 R is a scalar reproducing kernel and A \u2208 ST+ is a positive semi-definite (PSD) matrix. In this case, the representer theorem allows to rewrite problem (1) in a more compact matrix notation as\nminimize C\u2208Rn\u00d7T\nV (Y,KCA) + \u03bb tr(AC\u22a4KC). (P)\nHere Y \u2208 Rn\u00d7T is a matrix with n = \u2211Tt=1 nt rows containing the output points; K \u2208 Sn+ is the empirical kernel matrix associated to k and V : Rn\u00d7T \u00d7 Rn\u00d7T \u2192 R+ generalizes the loss in (1) and consists in a linear combination of the entry-wise application of L. Notice that this formulation accounts also the situation where not all training outputs y(t) are observed when a given input x \u2208 X is provided: in this case the functional V weights 0 the loss values of those entries of Y (and the associated entries of KCA) that are not available in training. Finally, the second term in (P) follows by observing that, for all f \u2208 H of the form f(\u00b7) = \u2211ni=1 k(xi, \u00b7)Aci, the squared norm can be written as \u2016f\u20162H = \u2211n i,j k(xi, xj)c \u22a4 i Acj = tr(AC\u22a4KC) where C \u2208 Rn\u00d7T is the matrix with i-th row corresponding to the coefficient vector ci \u2208 RT of f . Notice that we have re-ordered the index i to be in {1, . . . , n} to ease the notation."}, {"heading": "2.3 Incorporating Known Tasks Structure", "text": "Separable kernels provide a natural way to incorporate the task structure when the latter is known a priori. This strategy is quite general and indeed in the following we comment on how the matrix A can be chosen to recover several multi-task methods previously proposed in contexts such as regularization, coding/embeddings or output metric learning, postponing a more detailed discussion in the supplementary material. These observations motivate the extension in Sec. 3 of the learning problem (P) to a setting where it is possible to infer A from the data.\nRegularizers. Tasks relations can be enforced by devising suitable regularizers [15]. Interestingly, for a large class of such methods it can be shown that this is equivalent to the choice of the matrix A (or rather its pseudoinverse) [25]. If we consider the squared norm of a function f = \u2211n i=1 k(xi, \u00b7)Aci \u2208 H we have (see [15])\n\u2016f\u20162H = T\u2211\nt,s=1\nA\u2020ts\u3008ft, fs\u3009Hk (3)\nwhere At is the t-th column of A, Hk is the RKHS associated to the scalar kernel k and ft = \u2211n i=1 k(xi, \u00b7)A\u22a4t ci \u2208 Hk is the t-th component of f . The above equation suggests to interpret A\u2020 as the matrix that models the structural relations between tasks by directly coupling different predictors. For instance, by setting A\u2020 = IT + \u03b3(11\u22a4)/T , with 1 \u2208 RT the vector of all 1s, we have that the parameter \u03b3 controls the variance\u2211T\nt=1 \u2016f\u0304\u2212ft\u20162Hk of the tasks with respect to their mean f\u0304 = 1 T \u2211T t=1 ft. If we have access to some notion of similarity among tasks in the form of a graph with adjacency matrix W \u2208 ST , we can consider the regularizer\u2211Tt,s=1 Wt,s\u2016ft\u2212fs\u20162Hk+\u03b3 \u2211T t \u2016ft\u20162Hk which corresponds to A\u2020 = L+ \u03b3IT with L the graph Laplacian induced by W .\nOutput Metric. A different approach to model tasks relatedness consists in choosing a suitable metric on the output space to reflect the tasks structure [24]. Clearly a change of metric on the output space with the standard inner product \u3008y, y\u2032\u3009RT between two output points y, y\u2032 \u2208 YT corresponds to the choice of a different inner product \u3008y, y\u2032\u3009\u0398 = \u3008y, \u03b8y\u2032\u3009RT for some positive definite matrix \u0398 \u2208 ST++. Indeed this can be direct related to the choice of a suitable separable kernel. In particular, for the least squares loss function a direct equivalence holds between choosing a metric deformation associated to a \u0398 \u2208 ST++ and a separable kernel k(\u00b7, \u00b7)IT or use the canonical metric (i.e. with \u0398 = IT the identity) and kernel k(\u00b7, \u00b7)\u0398. The details of this equivalence can be found in the supplementary material.\nOutput Representation. The tasks structure can also be modeled by designing an ad-hoc embedding for the output space. This approach is particularly useful for multilabel scenarios, where output embedding can be designed to encode complex structures such as (e.g. trees, strings, graphs, etc.) [17, 21, 11]. Interestingly in these cases, or more generally whenever the embedding map L : YT \u2192 Y\u0303 , from the original to the new output space, is linear, then it is possible to show that the learning problem with new code is equivalent to (1) for a suitable choice of separable kernel with A = L\u22a4L. We refer again to the supplementary material for the details of this equivalence."}, {"heading": "3 Learning the Tasks and their Structure", "text": "Clearly, an interesting setting occurs when knowledge of the tasks structure is not available and therefore it is not possible to design a suitable separable kernel. In this case a favorable approach is to infer the tasks relations directly from the data. To this end we propose to consider the following extension of problem (P)\nminimize C\u2208Rn\u00d7T ,A\u2208ST\n+\nV (Y,KCA) + \u03bbtr(AC\u22a4KC) + F (A), (Q)\nwhere the penalty F : ST+ \u2192 R+ is designed to learn specific tasks structures encoded in the matrix A. The above regularization is general enough to encompass a large number of previously proposed approaches by simply specifying a choice of the scalar kernel and the penalty F . A detailed discussion of these connections is postponed to Section 4. In this section, we focus on computational aspects. Throughout,\nwe restrict ourselves to convex loss functions V and convex (and coercive) penalties F . In this case, the objective function in (Q) is separately convex in C and A but not jointly convex. Hence, block coordinate methods, which are often used in practice, e.g. alternating minimization over C and A, are not guaranteed to converge to a global minimum. Our study provides a general framework to provably compute a solution to problem (Q). First, In Section 3.1, we prove our main results providing a characterization of the solutions of Problem (Q) and studying a barrier method to cast their computation as a convex optimization problem. Second, in Section 3.2, we discuss how block coordinate methods can be naturally used to solve such a problem, analyze their convergence properties and discuss some general cases of interest."}, {"heading": "3.1 Characterization of Minima and A Barrier Method", "text": "We begin, in Section 3.1.1, providing a characterization of the solutions to Problem (Q) by showing that it has an equivalent formulation in terms of the minimization of a convex objective function, namely Problem (R). Depending on the behavior of the objective function on the boundary of the optimization domain, Problem (R) might not be solved using standard optimization techniques. This possible issue motivates the introduction, in Section 3.1.2, of a barrier method; a family of \u201cperturbated\u201d convex programs is introduced whose solutions are shown to converge to those of Problem (R) (and hence of the original (Q)).\n3.1.1 An Equivalent formulation for (Q) The objective functional in (Q) is not convex, therefore in principle it is hard to find a global minimizer. As it turns out however, it is possible to circumvent this issue and efficiently find a global solution to (Q). The following result represents a first step in this direction.\nTheorem 3.1. Let K \u2208 Sn+ and consider the convex set\nC = { (C,A) \u2208 Rn\u00d7T \u00d7 ST+ | Ran(C\u22a4KC) \u2286 Ran(A) } .\nThen, for any F : ST+ \u2192 R+ convex and coercive, problem\nminimize (C,A) \u2208 C\nV (Y,KC) + \u03bbtr ( A\u2020C\u22a4KC ) + F (A) (R)\nhas convex objective function and it is equivalent to (Q). In particular, the two problems achieve the same minimum value and, given a solution (CR, AR) for (R), the couple (CRA \u2020 R, AR) is a minimizer for (Q). Vice-versa, given a solution (CQ, AQ) for (Q), the couple (CQAQ, AQ) is a minimizer for (R). The above result highlights a remarkable connection between the problems (Q) (non-convex) and (R) (convex). In particular, we have the following Corollary, which provides us with a useful characterization of the local minimizers of problem (Q). Corollary 3.2. Let Q : Rn\u00d7T \u00d7 ST+ \u2192 R be the objective function of problem (Q). Then, every local minimizer for Q on the open set Rn\u00d7T \u00d7 ST++ is also a global minimizer.\nCorollary 3.2 follows from Theorem 3.1 and the fact that, on the restricted domain R\nn\u00d7T \u00d7 ST++, the map Q is the combination of the objective functional of (R) and the invertible function (C,A) 7\u2212\u2192 (CA,A). Moreover, if Q is differentiable, i.e. V and the penalty F are differentiable, this is exactly the definition of a convexifiable function, which in particular implies invexity [12]. The latter property ensures that, in the differentiable case, all the stationary points (rather than only local minimizers) are global minimizers. This result was originally proved in [14] for the special case of V the least-squares loss and F (\u00b7) = \u2016 \u00b7 \u20162F the Frobenius norm; Here we have proved its generalization to all convex losses V and penalties F . We end this section adding two comments. First, we note that, while the objective function in Problem (R) is convex, the corresponding minimization problem might not be a convex program (in the sense that the feasible set C is not identified by a set of linear equalities and non-linear convex inequalities [9]). Second, Corollary (3.2) holds only on the interior of the minimization domain Rn\u00d7T \u00d7ST+ and does not characterize the behavior of the target functional on its boundary. In fact, one can see that both issues can be tackled defining a perturbed objective functional having a suitable behavior on the boundary of the minimization domain. This is the key motivation for the barrier method we discuss in the next section.\n3.1.2 A Barrier Method to Optimize (R)\nHere we propose a barrier approach inspired by the work in [3] by introducing a perturbation of problem (R) that enforces the objective functions to be equal to +\u221e on the boundary of Rn\u00d7T \u00d7 ST+. As a consequence, each perturbed problem can be solved as a convex optimization constrained on a closed cone. The latter comment is made more precise in the following result that we prove in the supplementary material.\nTheorem 3.3. Consider the family of optimization problems\nminimize C\u2208Rn\u00d7T ,\nA\u2208ST+\nV (Y,KC) + \u03bbtr(A\u22121(C\u22a4KC + \u03b42IT )) + F (A) (S\u03b4)\nwith IT \u2208 ST++ the identity matrix. Then, for each \u03b4 > 0 the problem (S\u03b4) admits a minimum. Furthermore, the set of minimizers for (S\u03b4) converges to the set of minimizers for (R) as \u03b4 tends to zero. More precisely, given any sequence \u03b4m > 0 such that \u03b4m \u2192 0 and a sequence of minimizers (Cm, Am) \u2208 Rn\u00d7T \u00d7 ST+ for (S\u03b4), there exists a sequence (C\u2217m, A \u2217 m) \u2208 Rn\u00d7T \u00d7 ST+ of minimizers for (R) such that \u2016Cm \u2212 C\u2217m\u2016F + \u2016Am \u2212A\u2217m\u2016F \u2192 0 as m \u2192 +\u221e.\nThe barrier \u03b42tr(A\u22121) is fairly natural and can be seen as preconditioning of the problem leading to favorable computations. The proposed barrier method is similar in spirit to the approach developed in [3] and indeed Theorem 3.3 and next Corollary 3.4 are a generalization over the two main results in [3] to any convex penaltyF on the cone of PSD matrices. However, notice that since we are considering a much wider family of penalties (than the trace norm as in [3]) our results cannot directly derived from those in [3]. In the next section we discuss how to compute the solution of Problem (S\u03b4) considering a block coordinate approach.\nAlgorithm 1 CONVEX MULTI-TASK LEARNING\nInput: K,Y, \u01eb tolerance, \u03b4 perturbation parameter, S objective functional of (S\u03b4), V loss, F structure penalty. Initialize: (C,A) = (C0, A0), t = 0 repeat\nCt+1 \u2190 SUPERVISEDSTEP (V,K, Y, Ct, At) At+1 \u2190 UNSUPERVISEDSTEP(F,K, \u03b4, Ct+1, At) t \u2190 t+ 1\nuntil |S(Ct+1, At+1)\u2212 S(Ct, At)| < \u01eb"}, {"heading": "3.2 Block Coordinate Descent Methods", "text": "The characteristic block variable structure of the objective function in problem (S\u03b4), suggests that it might be beneficial to use block coordinate methods (BCM) (see [8]) to solve it. Here with BCM we identify a large class of methods that, in our setting, iterate steps of an optimization on C, with A fixed, followed by an optimization of A, for C fixed. A meta block coordinate algorithm to solve (S\u03b4) is reported in in Algorithm 1. Here we interpret each optimization step over C as a supervised step, and each optimization step over A as a an unsupervised step (in the sense that it involves the inputs but not the outputs). Indeed, when the structure matrix A is fixed, problem (R) boils down to the standard supervised multi-task learning frameworks where a priori knowledge regarding the tasks structure is available. Instead, when the coefficient matrix C is fixed, the problem of learning A can be interpreted as an unsupervised setting in which the goal is to actually find the underlying task structure [23]. Several optimization methods can be used as procedures for both SUPERVISEDSTEP and UNSUPERVISEDSTEP in Algorithm 1. In particular, a first class of methods is called Block Coordinate Descent (BCD) and identifies a wide class of iterative methods that perform (typically inexact) minimization of the objective function one block of variables at the time. Different strategies to choose which direction minimize at each step have been proposed: pre-fixed cyclic order, greedy search [30] or randomly, according to a predetermined distribution [29]. For a review of several BCD algorithms we refer the reader to [30] and references therein. A second class of methods is called alternating minimization and corresponds to the situation where at each step in Algorithm 1 and exact minimization is performed. This latter approach is favorable when a closed form solution exists for at least one block of variables (see Section 3.2.1) and has been studied extensively in [33] in the abstract setting where an oracle provides a block-wise minimizer at each iteration. The following Corollary describes the convergence properties of BCD and Alternate minimization sequences provided by applying Algorithm 1 to (S\u03b4).\nCorollary 3.4. Let the Problem (S\u03b4) be defined as in Theorem 3.3 then:\n(a) Alternating Minimization: Let the two procedures in Algorithm 1 each provide a block-wise minimizer of the functional with the other block held fixed. Then every limiting point of a minimization sequence provided by Algorithm 1, is a\nglobal minimizer for (S\u03b4).\n(b) Block Coordinate Descent: Let the two procedures in Algorithm 1 each consist in a single step of a first order optimization method (e.g. Projected Gradient Descent, Proximal methods, etc.). Then every limiting point of a minimizing sequence provided by Algorithm 1 is a global minimizer for (S\u03b4).\nCorollary (3.4) follows by applying previous results on BCD and Alternate minimization. In particular, for the proof of part (a) we refer to Theorem 4.1 in [33], while for part (b) we refer to Theorem 2 in [30]. In the following we discuss the actual implementation of both SUPERVISED and UNSUPERVISED procedures in the case where V is chosen to be least-squares loss and the penalty F to be a spectral p-Schatten norm. This should provide the reader with a practical example of how the meta-algorithm introduced in this section can be specialized to a specific multi-task learning setting.\nRemark 3.5. (Convergence of Block Coordinate Methods) Several works in multitask learning have proposed some form of BCM strategy to solve the learning problem. However, up to our knowledge, so far only the authors in [3] have considered the issue of convergence to a global optimum. Their results where proved for a specific choice of structure penalty in a framework similar to that of problem (R) (see Section 4) but do not extend straightforwardly to other settings. Corollary 3.4 aims to fill this gap, providing convergence guarantees for block coordinate methods for a large class of multi-task learning problems."}, {"heading": "3.2.1 Closed Form solutions for Alternating Minimization: Examples", "text": "Here we focus on the alternating minimization case and discuss some settings in which it is possible to obtain a closed form solution for the procedures SUPERVISEDSTEP and UNSUPERVISEDSTEP.\n(SUPERVISEDSTEP) Least Square Loss. When the loss function V is chosen to be least squares (i.e. V (Y, Z) = \u2016Y \u2212 Z\u20162F for any two matrices Y, Z \u2208 Rn\u00d7m) and the structure matrix A is fixed, a closed form solution for the coefficient matrix C returned by the SUPERVISEDSTEP procedure can be easily derived (see for instance [1]):\nvec(C) = (IT \u2297K + \u03bbA\u22121 \u2297 In)\u22121vec(Y ).\nHere, the symbol \u2297 denotes the Kronecker product, while the notation vec(M) \u2208 Rnm for a matrix M \u2208 Rn\u00d7m identifies the concatenation of its columns in a single vector. In [26] the authors proposed a faster approach to solve this problem in closed form based on Sylvester\u2019s method.\n(UNSUPERVISEDSTEP) p-Schatten penalties. We consider the case in which F is chosen to be a spectral penalty of the form F (\u00b7) = \u2016 \u00b7 \u2016pp with p \u2265 1. Also in this setting the optimization problem has a closed form solution, as shown in the following.\nProposition 3.6. Let the penalty of problem (S\u03b4) be F = \u2016 \u00b7 \u2016pp with p \u2265 1. Then, for any C \u2208 Rn\u00d7T fixed, the optimization problem (S\u03b4) in the block variable A has a minimizer of the form\nA\u03b4C = p+1 \u221a (C\u22a4KC + \u03b42IT )/\u03bb. (4)\nProposition 3.6 generalizes a similar result originally proved in in [3] for the special case p = 1 and provides an explicit formula for the UNSUPERVISEDSTEP of Algorithm 1. We report the proof in the supplementary material."}, {"heading": "4 Previous Work: Comparison and Discussion", "text": "The framework introduced in problem (Q) is quite general and accounts for several choices of loss function and task-structural priors. Section 3 has been mainly devoted to derive efficient and generic optimization procedures; in this section we focus our analysis on the modeling aspects, investigating the impact of different structure penalties on the multi-task learning problem. In particular, we will briefly review some multi-task learning method previously proposed, discussing how they can be formulated as special cases of problem (Q) (or, equivalently, (R)).\nSpectral Penalties. The penalty F = \u2016 \u00b7 \u20162F was considered in [14], together with a least squares loss function and the non convex problem (Q) is solved directly by alternating minimization. However, as pointed out in Sec. 3, solving the non convex problem (although invex, see the discussion on Corollary 3.2) directly could in principle become problematic when the alternating minimization sequence gets close to the boundary of Rn\u00d7T \u00d7ST++. A related idea is that of consideringF (A) = tr(A) (i.e. the 1-Schatten norm). This latter approach can shown to be equivalent to the Multi-Task Feature Learning setting of [3] (see supplementary material).\nCluster Tasks Learning. In [19], the authors studied a multi-task setting where tasks are assumed to be organized in a fixed number r of unknown disjoint clusters. While\nthe original formulation was conceived for linear setting, it can be easily extended to non-linear kernels and cast in our framework. Let E \u2208 {0, 1}T\u00d7r be the binary matrix whose entry Est has value 1 or 0 depending on whether task s is in cluster t or not. Set M = I \u2212 E\u2020E\u22a4, and U = 1T 11\u22a4. In [19] the authors considered a regularization setting of the form of (R) where the structure matrix A is parametrized by the matrix M in order to reflect the cluster structure of the tasks. More precisely:\nA\u22121(M) = \u01ebMU + \u01ebB(M \u2212 U) + \u01ebW (I \u2212M)\nwhere the first term characterizes a global penalty on the average of all tasks predictors, the second term penalizes the between-clusters variance, and the third term controls the tasks variance within each cluster. Clearly, it would be ideal to identify an optimal matrix A(M) minimizing problem (R). However, M belongs to a discrete non convex set, therefore authors propose a convex relaxation by constraining M to be in a convex set Sc = {M \u2208 ST+, 0 M I, tr(M) = r}. In our notations F (A) is therefore the indicator function over the set of all matrices A = A(M) such that M \u2208 Sc. The authors propose a pseudo gradient descent method to solve the problem jointly.\nConvex Multi-task Relation Learning. Starting from a multi-task Gaussian Process setting, in [37], authors propose a model where the covariance among the coefficient vectors of the T individual tasks is controlled by a matrix A \u2208 ST++ in the form of a prior. The initial maximum likelihood estimation problem is relaxed to a convex optimization with target functional of the form\n\u2016Y \u2212KC\u20162F + \u03bb1 tr(C\u22a4KC) + \u03bb2 tr(A\u22121C\u22a4KC) (5)\nconstrained to the set A = {A | A \u2208 ST++, tr(A) = 1). This setting is equivalent to problem (R) (by choosing F to be the indicator function of A) with the addition of the term tr(C\u22a4KC).\nNon-Convex Penalties. Often times, interesting structural assumptions cannot be cast in a convex form and indeed several works have proposed non-convex penalties to recover interpretable relations among multiple tasks. For instance [2] requires A to be a graph Laplacian, or [13] imposes a low-rank factorization of A in two smaller matrices. In [27, 22] different sparsity models are proposed. Interestingly, most of these methods can be naturally cast in the form of problem (Q) or (R). Unfortunately our analysis of the barrier method does not necessarily hold also in these settings and therefore Alternating Minimization is not guaranteed to lead to a stationary point."}, {"heading": "5 Experiments", "text": "We empirically evaluated the efficacy of the block coordinate optimization strategy proposed in this paper on both artificial and real datasets. Synthetic experiments were performed to assess the computational aspects of the approach, while we evaluated the quality of solutions found by the system on realistic settings."}, {"heading": "5.1 Computational Times", "text": "As discussed in Sec. 4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) [19] and Multi-task Feature Learning (MTFL [3]]), can be formulated as special cases of problem (Q) or (R). It is natural to compare the proposed alternating minimization strategy with the optimization solution originally proposed for each method. To assess the system\u2019s performance with respect to varying dimensions of the feature space and an increasing number of tasks, we chose to perform this comparison in an artificial setting. We considered a linear setting where the input data lie in Rd and are distributed according to a normal distribution with zero mean and identity covariance matrix. T linear models wt \u2208 Rd for t = 1, . . . , T were then generated according to a normal distribution in order to sample T distinct training sets, each comprising of 30 examples (x\n(t) i , y (t) i ) such that y (t) i = \u3008wt, x (t) i \u3009 + \u01eb with \u01eb Gaussian noise with zero mean and 0.1 standard deviation. On these learning problems we compared the computational performance of our alternating minimization strategy and the original optimization algorithms originally proposed for MTCL and MTFL and for which the code has been made available by the authors\u2019. In our algorithm we used A0 = I identity matrix as initialization for the alternating minimization procedure. We used a least-squares loss for all experiments. Figure 1 reports the comparison of computational times of alternating minimization and the original methods to converge to the same minima (of respectively the functional of MTCL and MTFL). We considered two settings: one where the number of tasks was fixed to T = 100 and d increased from 5 to 150 and a second one wher d was fixed to 100 and T varied bewteen 5 and 150. To account for statistical stability we repeated the experiments for each couple (T, d) and different choices of hyperparameters while generating a new random datasets at each time. We can make two observations from these results: 1) in the setting where T is kept fixed we observe a linear increase in the computational times for both original MTCL and MTFL methods, while alternating minimization is almost constant with respect to the input space dimension. 2) When d is fixed and the number of tasks increases, all optimization strategies require more time to converge. This shows that in general alternating minimization is a viable option to solve these problems and in particular, when T << min(d, n) \u2013 which is often the case in non-linear settings \u2013this method is particularly efficient."}, {"heading": "5.2 Real dataset", "text": "We assessed the benefit of adopting multi-task learning approaches on two real dataset. In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14]. We used least squares loss for all experiments.\nSarcos. Sarcos2 is a regression dataset designed to evaluate machine learning solutions for inverse dynamics problems in robotics. It consists in a collection of 21- dimensional inputs, i.e. the joint positions, velocities and acceleration of a robotic arm with 7 degrees of freedom and 7 outputs (the tasks), which report the corresponding torques measured at each joint. For each task, we randomly sampled 50, 100, 150 and 200 training examples while we kept a test set of 5000 examples in common for all tasks. We used a linear kernel and performed 5-fold crossvalidation to find the best regularization parameter according to the normalized mean squared error (nMSE) of predicted torques. We averaged the results over 10 repetitions of these experiments. The results, reported in Table 1, show clearly that to adopt a multi-task approach in this setting is favorable; however, in order to quantify more clearly such improvement, we report in Table 1 also the normalized improvement (nI) over single-task learning (STL). For each multi-task method MTL, the normalized improvement nI(MTL) is computed as the average\nnI(MTL) = 1\nnexp\nnexp\u2211\ni=1\nnMSEi(STL)\u2212 nMSEi(MTL)\u221a nMSEi(STL) \u00b7 nMSEi(MTL)\nover all the nexp = 10 experiments of the normalized differences between the nMSE achieved by respectively the STL approach and the given multi-task method MTL.\n15-Scenes. 15-Scenes3 is a dataset designed for scene recognition, consisting in a 15- class classification problem. We represented images using LLC coding [35] and trained the system on a training set comprising 50, 100 and 150 examples per class. The test set consisted in 7500 images evenly divided with respect to the 15 scenes. Table 2 reports\n2urlhttp://www.gaussianprocess.org/gpml/data/ 3http://www-cvr.ai.uiuc.edu/ponce grp/data/\nthe mean classification accuracy on 20 repetitions of the experiments. It can be noticed that while all multi-task approach seem to achieve approximately similar performance, these are consistently outperforming the STL baseline."}, {"heading": "6 Conclusions", "text": "We have studied a general multi-task learning framework where the tasks structure can be modeled compactly in a matrix. For a wide family of models, the problem of jointly learning the tasks and their relations can be cast as a convex program, generalizing previous results for special cases [3, 14]. Such an optimization can be naturally approached by block coordinate minimization, which can be seen as alternating between supervised and unsupervised learning steps optimizing respectively the tasks or their structure. We evaluated our method real data, confirming the benefit of multi-task learning when tasks share similar properties. From an optimization perspective, future work will focus on studying the theoretical properties of block coordinate methods, in particular regarding convergence rates. Indeed, the empirical evidence we report suggests that similar strategies can be remarkably efficient in the multi-task setting. From a modeling perspective, future work will focus on studying wider families of matrix-valued kernels, overcoming the limitations of separable ones. Indeed, this would allow to account also for structures in the interaction space between the input and output domains jointly, which is not the case for separable models."}, {"heading": "Appendix", "text": ""}, {"heading": "Imposing Known Structure on the Tasks", "text": ""}, {"heading": "Coding and Embedding", "text": "A common approach to encode knowledge of the tasks relations consists in mapping the output space YT in a new Y\u0303 \u2286 R\u2113 and then solve \u2113 independent standard learning problems (e.g. RLS, SVM, Boosting, etc. [17]) or a single one with a joint loss (e.g. Ranking [21]) using the mapped outputs as training observation. The goal is to implicitly exploit the structure of the new space to enforce known (or desired) relations among tasks.\nThe most popular setting for these embedding (or coding) methods is multi-class classification since in several realistic learning problems, classes can be organized in informative structures such as hierarchies or trees. Interestingly, due to the symbolic nature of the classes representation as canonical basis of RT , nonlinear embeddings are not particularly meaningful in classification contexts. Indeed the literature on coding methods for multi-task learning has been mainly concerned with the design of linear operators L : YT \u2192 Y\u0303 [17]. In the following we show that a tight connection exists between coding methods and our multi-task learning setting.\nFor a fixed linear operator L \u2208 R\u2113\u00d7T , we can solve the \u201ccoded\u201d problem using the notation of (P) and a kernel of the form \u0393 = kI\u2113 with I\u2113 the \u2113 \u00d7 \u2113 identity matrix (\u201cindependent tasks\u201d kernel)\nminimize C\u0303\u2208Rn\u00d7\u2113 V (Y\u0303 ,KC\u0303) + \u03bb tr(C\u0303\u22a4KC\u0303) (6)\nFrom the Representer theorem we know that the solution of (6) will have the form f(x) = \u2211n i=1 k(x, xi)c\u0303i = \u2211n i=1 k(x, xi)Lci, for some ci \u2208 RT and c\u0303i = Lci \u2208 L(RT ). Therefore, we can constrain (6) on matrices C\u0303 = CL with C \u2208 Rn\u00d7T , implying that the best solution for (6) belongs to the set of functions f = L \u25e6 g \u2208 HkI\u2113 with g \u2208 HkIT .\nFor those loss functions L that depend only on the inner product between the vectors of prediction and the ground truth (e.g. logistic or hinge [21, 36], see below), the \u201ccoded\u201d Problem (6) on Y\u0303 with kernel kI\u2113 is equivalent to (P) on Y with kernel kL\u22a4L. More precisely, if the multi-output loss can be written so that L(y\u0303, f(x)) = L(\u3008y\u0303, f(x)\u3009\nY\u0303 ) for all y\u0303 \u2208 Y\u0303 and x \u2208 X , we have\n\u3008y\u0303, f(x)\u3009 Y\u0303 = \u3008Ly, Lg(x)\u3009 Y\u0303 = \u3008y, L\u22a4Lg(x)\u3009Y (7)\nwhere y \u2208 Y is such that Ly = y\u0303 and L\u22a4 denotes the adjoint operator of L (in this case just the transpose matrix since L is a linear operator between vector spaces over the real field). Therefore, the two terms in the functional of (6) become\nV (Y\u0303 ,KC\u0303) = V (Y L\u22a4,KCL\u22a4) = V (Y,KCL\u22a4L)\nwhere the last equality makes use of the property in eq. (7), and\ntr(C\u0303\u22a4KC\u0303) = tr(LC\u22a4KCL\u22a4) = tr(L\u22a4LC\u22a4KC)\nproving the aforementioned equivalence between Problems (6) and (P) by choosing A = L\u22a4L.\nSemantic Label Sharing In [17] the authors proposed a strategy to solve a large multi-class visual learning problem that exploited the semantic information provided by the WordNet [16] to enforce specific relations among tasks. In particular, by designing a \u201csemantic\u201d distance between classes using the WordNet graph, the authors were able to generate a similarity matrix L \u2208 ST+ encoding the most relevant class relations. They used this matrix to map the original outputs (i.e. the canonical basis of R\nT ) into a new basis where euclidean distances between output codes would reflect the semantic ones induced by the WordNet priming. Then they applied a semi-supervised One-Vs-All approach on the new output space."}, {"heading": "Output Metric", "text": "In multi-output settings, another approach to implicitly model the tasks relations consists in changing the metric on the output space RT . In particular, we can define a matrix \u0398 \u2208 ST+ and denote the induced inner product on RT as \u3008y, y\u2032\u3009\u0398 = \u3008y,\u0398y\u2032\u3009RT for all y, y\u2032 \u2208 RT . For loss functions L such as those mentioned in Sec. 6 (e.g. hinge, logistic, etc.) that depend only on the inner product between observations and predictions, we have that for a fixed \u0398 the new loss is defined as L\u0398(y, f(x)) = L(\u3008y, f(x)\u3009\u0398) = L(\u3008y,\u0398f(x)\u3009RT ) and induces a learning problem of the form\nminimize C\u2208Rn\u00d7T V (Y\u0303 ,KC\u0398) + \u03bb tr(\u0398C\u22a4KC) (8)\nwhich is clearly equivalent to solving (P) choosing the kernel k\u0398. Notice that the second term in eq. (8) derives from the observation that with the new metric, the norm in the RKHSvv becomes \u2016f\u20162kIT = \u3008f, f\u3009kIT = \u2211n i,j \u2211T t,s k(xi, xj)\u3008ct, cs\u3009\u0398 = tr(\u0398C\u22a4KC) as required.\nmetric learning In [24] the authors proposed a metric learning framework in which both the new metric A (or \u0398) and the task predictors were estimated simultaneously. Adopting almost the same notation of Problem (Q), they used the least squares loss and imposed a penalty F (A) = \u2212log(det(A)) on the metric/structure matrix. A further penalty was also imposed on A, in order to enforce specific sparsity patterns. The only difference with our framework is that in [24] the authors do not impose the regularization term tr(AC\u22a4KC). Notice however that such term allows us to apply Theorem 3.1 and thus obtain the equivalence between (Q) and (R). This is extremely useful from the optimization perspective since, for instance, for the least squares loss and log-determinant penalty mentioned above, Problem (R) is actually convex jointly, which is not the case for the framework in [24].\nLearning the tasks and their structure\nEquivalence with the convex problem\nWe will make use of the following observation Lemma 6.1. Consider K \u2208 ST+ andC \u2208 Rn\u00d7T . Then Ran(C\u22a4KC) = Ran(C\u22a4 \u221a K) = Ran(C\u22a4K).\nProof. The second equivalence follows directly from the observation that C\u22a4K = (C\u22a4 \u221a K) \u221a K and C\u22a4 \u221a K = C\u22a4K( \u221a K)\u2020. Regarding the first equivalence, recall that for any M \u2208 RT\u00d7n, RT = Ran(M) \u2295 Ker(M), with Ker(M) denoting the null space of M . Therefore we can alternatively prove that Ker(C\u22a4KC) = Ker(C\u22a4 \u221a K). Notice that clearly Ker(C\u22a4 \u221a K) \u2286 Ker(C\u22a4KC). Now, let x \u2208 Ker(C\u22a4KC) so that 0 = x\u22a4C\u22a4KCx = x\u22a4( \u221a KC)\u22a4( \u221a KC)x. This implies that x is a singular vector of ( \u221a KC) with singular value equal to zero and therefore x \u2208 Ker(C\u22a4 \u221a K)."}, {"heading": "Proof. (Theorem 3.1)", "text": "We need to prove that C is a convex set and that tr(A\u2020C\u22a4KC) is jointly convex on C. Regarding the first part, notice that for A \u2208 ST+ and C \u2208 Rn\u00d7T the constraint Ran(C\u22a4KC) \u2286 Ran(A) can be equivalently rewritten as Ker(C\u22a4KC) \u2287 Ker(A). Therefore, using Lemma 6.1, we can check convexity of C by showing that for any arbitrary couple (A1, C1), (A2, C2) \u2208 C and any \u03b8 \u2208 [0, 1] we have Ker(\u03b8A1 + (1 \u2212 \u03b8)A2) \u2286 Ker(\u03b8C\u22a41 K + (1 \u2212 \u03b8)C\u22a42 K). Let us consider an arbitrary x \u2208 Ker(\u03b8A1 + (1\u2212 \u03b8)A2). We have\n0 = x\u22a4(\u03b8A1 + (1\u2212 \u03b8)A2)x = \u03b8x\u22a4A1x+ (1\u2212 \u03b8)x\u22a4A2x.\nSince both A1 and A2 are PSD, the terms x\u22a4Aix are necessarily non-negative for both i = 1, 2. Hence, from the equation above we have x\u22a4Aix = 0, which is equivalent to x \u2208 Ker(A1) \u2229 Ker(A2) \u2286 Ker(C\u22a41 K) \u2229 Ker(C\u22a42 K). This means that x is in the nullspace of both C\u22a41 K and C \u22a4 2 K and therefore also in the nullspace of any linear combination of the two. In particular x \u2208 Ker(\u03b8C\u22a41 K + (1\u2212 \u03b8)C\u22a42 K). The proof for the convexity of tr(A\u2020C\u22a4KC) has been already pointed out elsewhere (see for instance [5]). For completeness, we provide an simpler derivation of this result which makes use of a Schur\u2019s complement argument and simple algebraic properties in line with [14] to show that the epigraph of the function is convex. Consider A \u2208 ST+ and C \u2208 Rn\u00d7T . From simple properties of the trace we have the equivalence tr(A\u2020C\u22a4KC) = vec( \u221a KC)\u22a4(A\u2020 \u2297 IT )vec( \u221a KC), where \u2297 identifies the Kronecker product and by vec(\u00b7) we denote the vectorization operator mapping a matrix M \u2208 Rn\u00d7m to the concatenation of all its columns vec(M) \u2208 Rnm. Since Ran(A) \u2287 Ran(C\u22a4KC) = Ran(C \u221a K) we can apply the generalized Schur\u2019s com-\nplement to write the epigraph of f(A,C) = tr(A\u2020C\u22a4KC) as\nepi f = { (t, A, C) \u2223\u2223 t \u2265 tr(A\u2020C\u22a4KC) = vec(C \u221a K)\u22a4(A\u2020 \u2297 IT )vec(C \u221a K), (A,C) \u2208 C } =\n= { (t, A, C) \u2223\u2223\u2223\u2223 (\nA\u2297 IT vec(C \u221a K)\nvec(C \u221a K)\u22a4 t\n) 0,\n(A,C) \u2208 C}\nwhere we write X Y for any two symmetric matrices X,Y \u2208 Sm if and only if X \u2212 Y \u2208 Sm+ . Notice that the block components of the matrix in the equation above are all linear with respect to A,C and t and therefore the convexity of epi f follows by directly observing that for any couple (t1, A1, C1), (t2, A2, C2) \u2208 epi f , the PSD constraint holds for any convex combination of the two.\nWe finally prove that the mapping between minimizers stated in Theorem (3.1). First notice that for any (C,A) \u2208 Rn\u00d7T \u00d7 ST+ we have Q(C,A) = R(CA,A), with (CA,A) \u2208 domR since clearly Ran(A) \u2287 Ran(AC\u22a4KCA). Therefore inf {Q(C,A) |C \u2208 R\nn\u00d7T , A \u2208 ST+} \u2265 inf {R(C,A) | (C,A) \u2208 C}. Analogously, given a point (C,A) \u2208 C we have that R(C,A) = R(CA\u2020A,A) since Ran(C\u22a4K) \u2286 Ran(A) and thus V (y,KCAA\u2020) = V (y,KC). Therefore R(C,A) = R(CA\u2020A,A) = Q(CA\u2020, A), implying that inf {R(C,A) | (C,A) \u2208 C} \u2265 inf {Q(C,A) | C \u2208 Rn\u00d7T , A \u2208 ST+} and concluding the proof.\nA Barrier Method to Optimize (R) Proof. (Theorem 3.3) To prove the existence of finite minimizers we need to show that there exists a minimizing sequence for S\u03b4 such that it converges to a point in domS\u03b4 = R\nn\u00d7T \u00d7 ST++. To see this, consider a generic minimizing sequence, i.e. a sequence {(Cn, An)}n\u2208N \u2282 domS\u03b4 such that S\u03b4(Cn, An) \u2192 infC,AS\u03b4(C,A). Notice that we can separate Cn in Cn = C\u0302n,+C\u22a5n with C\u0302n \u2208 Ran(K) the range of the Gram matrix K and C\u22a5n \u2208 Ker(K) its nullspace and that therefore S\u03b4(C\u0302n, An) = S\u03b4(Cn, An). This implies that the sequence (C\u0302n, An) is bounded, since, if it was not, we would have the coercive penalty F or the tr(A\u22121n C\u0302 \u22a4 n KC\u0302n) to go to infinity as n grows. But this is not possible since S\u03b4(C\u0302n, An) \u2192 infC,AS\u03b4(C,A) < +\u221e. Therefore (C\u0302n, An) admits a converging subsequence. Suppose without loss of generality that (Cn, An) converges to a point (C\u2217, A\u2217) \u2208 domS\u03b4 = Rn\u00d7T \u00d7 ST+. We want to show that (C\u2217, A\u2217) is actually in the domS\u03b4 = Rn\u00d7T \u00d7 ST++, i.e. that A\u2217 is positive definite. But this is obvious since \u03b4 > 0 and therefore if the An were to converge to a point in ST+\\ST++, we would have that \u03b42 tr(A\u22121n ) \u2192 +\u221e and therefore S\u03b4(C\u0302n, An) \u2192 +\u221e as n \u2192 +\u221e. Finally, by the continuity of S\u03b4, we have S\u03b4(C\u0302n, An) \u2192 S\u03b4(C\u2217, A\u2217), therefore proving that (C\u2217, A\u2217) \u2208 argminC,A S\u03b4(C,A).\nThe second part of the proof requires the following preliminary steps:\n1. minC,AR(C,A) = infA,CS0(C,A) and they have same infimizers.\n2. g(\u03b4) = infA,CS\u03b4(C,A) is continuous (in fact convex) with minimum in 0.\nWe prove the first point in Lemma 6.2, while the second observation follows from the fact that the function g is the point-wise infimum of a jointly convex function over a convex set. This requires to show that \u03b42tr(A\u22121) is jointly convex which follows the same reasoning as for the convexity of tr(A\u22121C\u22a4KC) in Theorem (3.1).\nLet us consider two sequences \u03b4n > 0 and {(Cn, An)}n\u2208N \u2282 domS\u03b4 = Rn\u00d7T \u00d7 ST++ satisfying the hypothesis of the Theorem, i.e. S \u03b4n(Cn, An) = minC,AS \u03b4n(C,A). We will first prove the result for Cn in the range of the Gram matrix K . Notice that under this requirement, the (Cn, An) are bounded, since, analogously as for the proof above, if they were not we would have the coercive penalty F or the tr(A\u22121n C \u22a4 n KCn) to go to infinity as n grows. But this is not possible since S\u03b4n(Cn, An) \u2192 g(0) < +\u221e. Therefore, by points 1. and 2., g(0) = minC,AR(C,A) and the limit points of (Cn, An) are minimizers for R. This finally implies that there exists a sequence {(C\u2217n, A\u2217n)}n\u2208N \u2286 argminC,AR(C,A) such that \u2016Cn \u2212 C\u2217n\u2016F + \u2016An \u2212A\u2217n\u2016F tends to zero as n goes to infinity. To see this, suppose by contradiction that it is not true and that there exists a subsequence {(Cnk , Ank)}k\u2208N and an M > 0 such that \u2016Cnk \u2212 C\u2217\u2016F +\u2016Ank \u2212A\u2217\u2016F > M for all k > 0 and for all (C\u2217, A\u2217) \u2208 argminC,A R(C,A). Now, since (Cnk , Ank) is a subsequence of (Cn, An), we have that: (i) (Cnk , Ank) is bounded (hence admits a converging subsequence) and (ii) every converging subsequence tends to a minimizer of R. This clearly contradicts the hypothesis.\nNow, consider the general case in which Cn is not in the range of K: notice that similarly as before, Cn can be separated in Cn = C\u0302n + C\u22a5n with C\u0302n \u2208 Ran(K) the range of K and C\u22a5n \u2208 Ker(K) its nullspace. Clearly, S\u03b4n(C\u0302n, An) = S\u03b4n(Cn, An) \u2192 g(0) and therefore, from the discussion above we have a sequence {(C\u0302\u2217n, A\u2217n)}n\u2208N \u2286 argminC,AR(C,A) such that \u2016C\u0302n \u2212 C\u0302\u2217n\u2016F + \u2016An \u2212 A\u2217n\u2016F \u2192 0 as n \u2192 +\u221e. We can now observe that the sequence (C\u2217n, A \u2217 n) = (C\u0302 \u2217 n +C \u22a5 n , A \u2217 n) satisfies the statement of the Theorem: indeed (i) the (C\u2217n, A \u2217 n) are minimizers for R since R(C \u2217 n, A \u2217 n) = R(C\u0302\u2217n, A \u2217 n) and (ii) \u2016Cn \u2212 C\u2217n\u2016F = \u2016C\u0302n \u2212 C\u0302\u2217n\u2016F \u2192 0 for n \u2192 +\u221e.\nLemma 6.2. minA,CR(C,A) = infA,CS0(C,A) and they have same infimizers:\nProof. This fact follows from the observation that for all \u03b4 > 0, domS\u03b4 = domS0 is equal to the interior of domR and that all minimizers for R belong to domR. To show this second statement we will prove that for any sequence {(Cn, An)}n\u2208N \u2282 domR and converging to some point (C\u0304, A\u0304) \u2208 Rn\u00d7T \u00d7 ST+ \\ domR, we have that R(Cn, An) \u2192 +\u221e as n goes to infinity. For simplicity of notation let us denote B\u0304 = C\u0304\u22a4KC\u0304 and analogously Bn = C\u22a4n KCn. Since from hypothesis Ran(A\u0304) 6\u2287 Ran(C\u0304\u22a4KC\u0304) we have that Ker(A\u0304) 6\u2286 Ker(B\u0304), or, in other words, there exists an eigenvector v\u0304 for A\u0304 such that v \u2208 Ker(A) and \u2016B\u0304v\u0304\u20162 > 0.\nSince the sequence An converges to A\u0304, we can identify a sequence of eigenvectors vn for An such that vn \u2192 v\u0304 and their associated eigenvalue \u03bbn \u2192 0 as n goes to infinity. Notice that we can assume without loss of generality that \u03bbn > 0 for all n since \u03bbn = 0 would imply vn \u2208 Ker(An) \u2286 Ker(Bn) but we have from hypothesis that \u2016Bnvn\u20162 \u2192 \u2016B\u0304v\u0304\u2016 > 0. Therefore we have\ntr(A\u2020nBn) \u2265 \u03bb\u22121n v\u22a4n Bnvn = \u03bb\u22121n \u2016Bnvn\u201622 \u2192 +\u221e as n goes to infinity."}, {"heading": "Spectral Regularization", "text": "Proposition 3.6 follows directly from the following result\nProposition 6.3. Let A,M \u2208 Sn+ with Ran(A) \u2287 Ran(M), rank(M) = r. Let M = U\u03a3U\u22a4 be an eigendecomposition of M with U \u2208 On and \u03a3 \u2208 Sn+ a diagonal matrix with eigenvalues in decreasing order. Then, there exists a matrix A\u2217 = U\u0393U\u22a4 \u2208 Sn+ with \u0393 \u2208 Sn+ diagonal with \u0393i,i = 0 \u2200i < r, such that\ntr(A\u2020\u2217M) = tr(A \u2020M) and \u2016A\u2217\u2016p \u2264 \u2016A\u2016p \u2200p \u2265 1 (9)\nwith the equality holding if and only if A\u2217 = A.\nProof. To keep the notation uncluttered we prove the result for \u0398 = A\u2020. Consider an eigendecompositionn\u0398 = S\u039bS\u22a4 with S \u2208 On and \u039b \u2208 Sn+ diagonal with eigenvalues in decreasing order. Let us define R = U\u22a4S \u2208 On. Then\ntr(\u0398M) = tr(R\u039bR\u22a4\u03a3) = r\u2211\ni=1\n\u03c3i\nn\u2211\nj=1\nR2ij\u03bbj = r\u2211\ni=1\n\u03c3i\u03b3i\nwhere \u03c3i and \u03bbi are respectively the i-th eigenvalues of M and \u0398 and we have defined \u03b3i = \u2211n j=1 Rij\u03bbj for i \u2264 r and \u03b3i = 0 otherwise. Hence, if we consider a diagonal matrix \u0393 \u2208 Sn+ such that \u0393ii = \u03b3i and set \u0398\u2032 = U\u0393U\u22a4 we obtain the left equivalence of eq. (9), namely tr(\u0398M) = tr(\u0398\u2032M). Now, consider the p-Schatten norm of \u0398\u2032\n\u2016(\u0398\u2032)\u2020\u2016p = ( r\u2211\ni=1\n1\n\u03b3pi\n)1/p =   r\u2211\ni=1 1(\u2211n j=1 R 2 ij\u03bbj )p\n  1/p\n.\nNotice that Rij = U\u22a4i \u00b7Sj corresponds to the projection of the i-th eigenvector of M on the j-th eigenvector of \u0398. Since Ran(\u0398) = Ran(A) \u2287 Ran(M), for any eigenvector s \u2208 Rn in the nullspace of \u0398 (i.e. with associated eigenvalue \u03bb = 0), we have that U\u22a4i \u00b7 s = 0 for all i \u2264 r. Hence, \u2200i \u2264 r, 1 = R\u22a4i \u00b7 Ri = \u2211n j=1 R 2 ij = \u2211k j=1 R 2 ij , where k = rank(A). Therefore, since the R2ijs add up to 1 and the scalar function (1/x)p is convex in x \u2208 R++, we have\nr\u2211\ni=1 1(\u2211n j=1 R 2 ij\u03bbj\n)p \u2264 r\u2211\ni=1\nk\u2211\nj=1\nR2ij 1\n\u03bbpj \u2264\n\u2264 k\u2211\nj=1\n1\n\u03bbpj\nn\u2211\ni=1\nR2ij = k\u2211\nj=1\n1\n\u03bbpj = \u2016\u0398\u2020\u2016pp\nwhere we have made use of the fact that for all j = 1, . . . , n we have \u2211n\ni=1 Rij = R\u22a4j \u00b7 Rj = 1. Therefore, \u2016(\u0398\u2032)\u2020\u2016p \u2264 \u2016\u0398\u2020\u2016p. By taking A\u2032 = (\u0398\u2032)\u2020 we have the desired result.\nApplied to the minimization in problem (R) with C \u2208 Rn\u00d7T fixed and p-Schatten penalty, Proposition 6.3 states that a minimizerAC \u2208 ST+ has the same system of eigenvalues as C\u22a4KC and their spectrum have same sparsity pattern (i.e. Ran(C\u22a4KC) = Ran(A)). This observation leads directly to the closed formula to find a A\u2217 stated in Proposition 3.6.\nProof. (Proposition 3.6) Consider the eigendecomposition C\u22a4KC = M = U\u03a3U\u22a4 with U \u2208 OT and \u03a3 \u2208 ST+ diagonal with the eigenvalues arranged in descending order. We apply Proposition 6.3 and obtain the minimizer A\u2217 = U\u0393U\u22a4 for \u0393 \u2208 ST+ diagonal with same sparsity pattern as \u03a3. We can rewrite the target function as\nr\u2211\nt=1\n\u03c3t \u03b3t + \u03bb \u03b3t.\nwhere r = rank(M). Therefore, the optimization problem consists in minimizing the target function above with respect to the \u03b3ts. This is an unconstrained convex optimization of a differentiable coercive function bounded below and therefore it is sufficient to set the gradient to zero and solve with respect to the \u03b3t. It is clear that for each t = 1 . . . r, the minimizer is of the form \u03b3t = p+1 \u221a \u03c3t/\u03bb, leading to the desired solution."}, {"heading": "Linear Multi-task Learning", "text": "Several works in multi-task learning have focused on linear models where the multioutput predictor f : Rd \u2192 RT is parameterized by a matrix W \u2208 Rd\u00d7T whose columns wt \u2208 Rd are associated to the individual task-predictors ft(x) = \u3008wt, x\u3009Rd for any x \u2208 Rd. In this tasks structure can be imposed considering suitable matrix penalty \u2126 : Rd\u00d7T \u2192 R and regularization schemes of form\nmin. W\u2208Rd\u00d7T V (Y,XW ) + \u2126(W ) (10)\nwhere X \u2208 Rn\u00d7d is the matrix whose rows correspond to the (transposed) input points in the training sets, ordered accordingly to the order in Y 4. We can recognize two main classes of penalty functions. A first class correspond to methods that impose structured sparsity on the input features across the multiple tasks, for instance considering the penalty \u2126(\u00b7) = \u2016 \u00b7 \u20162,1 [3], which encourages whole rows of W to be simultaneously sparse, see also [20, 38]. A second class corresponds to spectral regularization methods defined by penalties \u2126 acting on the singular values of W . Examples in this class include methods that impose low-rank assumptions [3] on the tasks, or search after tasks-cluster structures [19]. Ideas related to a combination of the above methods can also be considered [10].\nMost Linear multi-task learning problems of the form (10) with \u2126 spectral penalty, can be formulated in terms of problem (R) for a suitable choice of F . Indeed it can be\n4Again V would weight with zeros the loss associated to entries for which examples are not available during training\nshown that for several spectral norms, such as the p-schatten norms, the penalty \u2126 can be written as\n\u2126(W ) = inf A\u2208ST\n++\ntrace(WA\u22121W\u22a4) + F\u2126(A) \u2200W \u2208 Rn\u00d7T\nHere we report the example of the nuclear norm \u2016 \u00b7 \u2016\u2217, that has already been observed in similar form in [3, 18] and that can be easily derived from Prop. 3.6 for the case p = 1.\n\u2016W\u2016\u2217 = 1\n2 inf\nA\u2208ST ++\ntrace(WA\u22121W\u22a4) + trace(A).\nIndeed, from Prop. (3.6) we have that the solution to the minimization problem is A\u2217 = \u221a (W topW ) and therefore, the minimum of such functional will be exactly\ntrace( \u221a WW\u22a4) = \u2016W\u2016\u2217.\nImpose Tasks Relationships by enforcing structure on the feature space\nRelations among tasks can be also modeled by enforcing shared structures on the input space. For instance in [3], the authors generalized a feature selection framework to the multi-task setting by formulating the linear problem\nminimize U\u2208Od,M\u2208Rd\u00d7T\nV (Y,XUM) + \u03b3\u2016M\u20162,1 (11)\nwhere X \u2208 Rn\u00d7d is the matrix whose i-th row corresponds to the input vector xi \u2208 Rd and the (2, 1)-norm \u2016M\u20162,1 = \u2211d k=1 \u2016Mk\u20162 is introduced to enforce sparsity among the rows Mk of M . This penalty generalizes feature selection to the multi-task case by directly manipulating the covariance on the input space. However, since input and output distributions are connected by the training data, it is reasonable to expect this process to indirectly affect also the covariance on the output space. Indeed, in this Section we present an interesting result connecting multi-task problems that impose structure on the input covariance and problems that instead aim to control the output covariance (i.e. in the form of (R)). To show this connection, we need to discuss in more detail the work in [3]. Although (11) is not convex, the authors prove that there exists an equivalent convex formulation of the form\nminimize W\u2208Rd\u00d7T ,D\u2208Sd+,\nRan(D)\u2287Ran(W ),tr(D)\u22641\nV (Y,XW ) + \u03b3 tr(W\u22a4D\u2020W ). (12)\nThe authors then proceed to generalize this framework to the nonlinear case using the advantages of the RKHS notation. In this setting, the original idea of identifying a low dimensional set of directions in the feature space translates naturally to the problem of finding a small set of orthogonal directions in the Hilbert space. To this end,\nthe authors perform a preprocessing step whose goal is to identify an orthonormal basis of functions \u03c81, . . . \u03c8\u2113 \u2208 Hk for set spanned by the k(xi, \u00b7) and define a matrix K\u0303 \u2208 Rn\u00d7\u2113 such that K\u0303ij = \u03c8j(xi). A possible way to do this is by considering a eigenvalue decomposition U\u03a3U\u22a4 of K and taking K\u0303 = U\u03a31/2 (taking out from \u03a31/2 the columns equal to zero). It is easy to show that the standard learning problem in RKHS settings can be cast equivalently in this new notation. However, this framework has the further advantage that it can be generalized to take into account the eventuality of a transformation in the feature space, leading to the extension of problem (12) for the non linear case\nminimize B\u2208R\u2113\u00d7T ,D\u2208S\u2113+,\nRan(D)\u2287Ran(B),tr(D)\u22641\nV (Y, K\u0303B) + \u03b3 tr(B\u22a4D\u2020B) (13)\nAs can be noticed, the structure of problem (13) is very similar to the one of problem (R) and indeed, as stated in Corollary 6.5 the two are equivalent when trace regularization is imposed on (R). However, as shown in Theorem 6.4, a more general equivalence holds.\nTheorem 6.4. Let \u03bb > 0, p \u2265 1, Rn\u00d7T , {xi, yi}ni=1 \u2282 Rd \u00d7 RT a set of input-output pairs with y \u2208 Rn\u00d7T the matrix whose i-th row corresponds to yi. Let \u03c81, . . . , \u03c8\u2113 \u2208 Hk be an orthonormal basis for span{k(xi, \u00b7}ni=1 and K\u0303 \u2208 Rn\u00d7\u2113 with K\u0303ij = \u03c8j(xi). Then\nminimize B\u2208R\u2113\u00d7T ,D\u2208S\u2113+, Ran(D)\u2287Ran(B)\nS(B,D) = V (Y, K\u0303B) + tr(B\u2217D\u2020B) + \u03bb \u2016D\u2016p (T )\nis a convex optimization problem equivalent to (R) with penalty function F (A) = \u2016A\u2016p. In particular the two problems achieve the same minimum and, given a minimizer for one problem it is possible to obtain a solution for the other and vice-versa.\nThe crucial aspect of the proof of Theorem 6.4 (which we prove below) consists in identifying the two mappings that allow to obtain a minimizer for problem (R) from a solution of (T ) and vice-versa. As a corollary of Theorem (6.4) we get the exact equivalence to the problem proposed in [3].\nCorollary 6.5. Problem (13) is equivalent to (T ) for p = 1. In particular the two problems achieve the same minimum for \u03bb = \u03b32/4. As a consequence of Theorem 6.4 this implies also that (13) is also equivalent to (R) when F (\u00b7) = \u2016 \u00b7 \u20161 = tr(\u00b7).\nThis result follows from the direct comparison of the minimizers for the problems (T ) (from Proposition 3.6) and (13) (from [3]). Notice, that although equivalent as convex optimizations, it is in general more convenient to solve problems in the form (R) rather than (T ) since in most cases T << \u2113."}, {"heading": "Proof. Theorem 6.4.", "text": "From the discussion in [3] we can rewrite problem (R) in the equivalent formulation minimize\nB\u2208R\u2113\u00d7T ,A\u2208ST+ , Ran(A)\u2287Ran(B\u22a4)\nT (B,A) = V (Y, K\u0303B) + tr(A\u2020B\u22a4B) + \u03bb \u2016A\u2016p (U)\nTherefore, to prove Theorem 6.4 it is sufficient to show that problem (T ) and (U) are equivalent. Assume without loss of generality T \u2264 \u2113. Consider an arbitrary matrixB \u2208 R \u2113\u00d7T and a singular value decomposition B = V\n( \u03a3 0 ) U\u22a4 where 0 \u2208 R(\u2113\u2212T )\u00d7T\nidentifies a matrix of all zeros, V \u2208 O\u2113, U \u2208 OT and \u03a3 \u2208 ST+ a diagonal matrix with eigenvalues in descending order. From Propositon 6.3, we obtain that the minimizers of the two functions S(B, \u00b7) and T (B, \u00b7) are unique and can be written respectively in the forms\nDB = V ( \u0393D 0 0 0 ) V \u22a4 \u2208 S\u2113+ and AB = U\u0393AU\u22a4 \u2208 ST+\nwhere \u0393D,\u0393A \u2208 ST+ have same sparsity pattern as \u03a3 and the zero matrices in the formulation of DB are of appropriate dimension. We can therefore write the minimum value achieved by S(B, \u00b7) as S(B,DB) = V (Y, K\u0303B)+ tr(\u0393\u2020D\u03a32) +\u03bb\u2016\u0393D\u2016p and the minimum achieved by T (B, \u00b7) as T (B,AB) = V (Y, K\u0303B) + tr(\u0393\u2020A\u03a32) + \u03bb\u2016\u0393A\u2016p. In the light of these equations, it can be easily cheked that by setting A(D)B = U\u0393DU\n\u22a4 \u2208 ST+ we have\nS(B,DB) = T (B,A (D) B ) \u2265 T (B,AB)\nwhere the inequality follows from the fact that AB is a minimizer for T (B, \u00b7). Analogously, we can design a matrix D(A)B \u2208 S\u2113+ such that T (B,AB) = S(B,D (A) B ) \u2265 S(B,DB). Since the minimizers AB and DB are unique, it follows that \u0393D = \u0393A. In the perspective of this result, we have that for any minimizer (B\u2217, D\u2217) \u2208 R\u2113\u00d7T \u00d7 S\u2113+ for (T ), the couple (B\u2217, A(D\u2217)B\u2217 ) \u2208 R\u2113\u00d7T \u00d7 ST+ is a minimizer for (U) and furthermore, the two functions achieve the same minimum value. The same result holds in the opposite direction."}], "references": [{"title": "Kernels for vector-valued functions: a review", "author": ["M. \u00c1lvarez", "N. Lawrence", "L. Rosasco"], "venue": "Foundations and Trends in Machine Learning, 4(3):195\u2013266,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning the Graph of Relations Among Multiple Tasks", "author": ["A. Argyriou", "S. Cl\u00e9men\u00e7on", "R. Zhang"], "venue": "Research report, Oct.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "An algorithm for transfer learning in a heterogeneous environment", "author": ["A. Argyriou", "A. Maurer", "M. Pontil"], "venue": "ECML/PKDD (1), pages 71\u201385,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 25\u201332. MIT Press, Cambridge, MA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting structured data", "author": ["G.H. Bakir", "T. Hofmann", "B. Scholkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"], "venue": "MIT Press,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"], "venue": "Journal of the American Statistical Association, 101(473):138\u2013156,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "On the convergence of block coordinate descent type methods", "author": ["A. Beck", "L. Tetruashvili"], "venue": "Technion, Israel Institute of Technology, Haifa, Israel, Tech. Rep,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning incoherent sparse and low-rank patterns from multiple tasks", "author": ["J. Chen", "J. Liu", "J. Ye"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), 5(4):22,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "On the learnability and design of output codes for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, pages 35\u201346,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Relations between invex properties", "author": ["B. Craven"], "venue": "WORLD SCIENTIFIC SERIES IN APPLI- CABLE ANALYSIS, 5:25\u201334,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning output kernels for multi-task problems", "author": ["F. Dinuzzo"], "venue": "Neurocomputing, 118:119\u2013 126,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C.S. Ong", "P. Gehler", "G. Pillonetto"], "venue": "International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, pages 615\u2013637,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Wordnet: An electronic lexical database", "author": ["C. Fellbaum"], "venue": "1998. MIT Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Semantic label sharing for learning with many categories", "author": ["R. Fergus", "H. Bernal", "Y. Weiss", "A. Torralba"], "venue": "European Conference on Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Trace lasso: a trace norm regularization for correlated designs", "author": ["E. Grave", "G.R. Obozinski", "F. Bach"], "venue": "J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2187\u20132195.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Clustered multi-task learning: a convex formulation", "author": ["L. Jacob", "F. Bach", "J.-P. Vert"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Decorrelating semantic visual attributes by resisting the urge to share", "author": ["D. Jayaraman", "F. Sha", "K. Grauman"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting structured objects with support vector machines", "author": ["T. Joachims", "T. Hofmann", "Y. Yue", "C.-N. Yu"], "venue": "Commun. ACM, 52(11):97\u2013104, Nov.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": "arXiv preprint arXiv:1206.6417,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovering structure by learning sparse graphs", "author": ["B.M. Lake", "J.B. Tenenbaum"], "venue": "Proceedings of the 32nd Cognitive Science Conference,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Block variable selection in multivariate regression and highdimensional causal inference", "author": ["A. Lozano", "V. Sindhwani"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernels for multi-task learning", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Vector-valued manifold regularization", "author": ["H.Q. Minh", "V. Sindhwani"], "venue": "International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-category and taxonomy learning: A regularization approach", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco"], "venue": "NIPS Workshop on Challenges in Learning Hierarchical Models: Transfer Learning and Optimization,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiclass learning with simplex coding", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-j. Slotine"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization, 22(2):341\u2013362,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo"], "venue": "SIAM Journal on Optimization, 23(2):1126\u20131153,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable matrix-valued kernel learning and high-dimensional nonlinear causal inference", "author": ["V. Sindhwani", "A.C. Lozano", "H.Q. Minh"], "venue": "CoRR, abs/1210.4792,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Convergence of block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications, 109:475\u2013494,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 2764\u20132770. AAAI Press,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A convex formulation for learning task relationships in multitask learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10), pages 733\u2013742, Corvallis, Oregon,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex multitask learning with flexible task clusters", "author": ["W. Zhong", "J. Kwok"], "venue": "J. Langford and J. Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML \u201912, pages 49\u201356, New York, NY, USA, July", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 24, "context": "This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.", "startOffset": 68, "endOffset": 78}, {"referenceID": 2, "context": "This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.", "startOffset": 68, "endOffset": 78}, {"referenceID": 3, "context": "This idea has motivated a variety of methods, including frequentist [25, 3, 4] and Bayesian methods (see e.", "startOffset": 68, "endOffset": 78}, {"referenceID": 0, "context": "[1] and references therein), with connections to structured learning [6, 34].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[1] and references therein), with connections to structured learning [6, 34].", "startOffset": 69, "endOffset": 76}, {"referenceID": 33, "context": "[1] and references therein), with connections to structured learning [6, 34].", "startOffset": 69, "endOffset": 76}, {"referenceID": 24, "context": "Following [25, 15] we consider a setting where tasks are modeled as the components of a vector-valued function and their", "startOffset": 10, "endOffset": 18}, {"referenceID": 14, "context": "Following [25, 15] we consider a setting where tasks are modeled as the components of a vector-valued function and their", "startOffset": 10, "endOffset": 18}, {"referenceID": 24, "context": "Exploiting the theory of reproducing kernel Hilbert spaces for vector-valued functions (RKHSvv) [25], we consider and analyze a flexible regularization framework, within which a variety of previously proposed approaches can be recovered as special cases, see e.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 23, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 25, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 36, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 30, "context": "[19, 24, 26, 37, 14, 31].", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "Our approach is based on a barrier method that is combined with block coordinate descent techniques [33, 30].", "startOffset": 100, "endOffset": 108}, {"referenceID": 29, "context": "Our approach is based on a barrier method that is combined with block coordinate descent techniques [33, 30].", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "In this sense our analysis generalizes the results in [3] for which a low-rank assumption was considered; however the extension is not straightforward, since we consider a much larger class of regularization schemes (any convex penalty).", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "The RKHSvv setting allows to naturally deal both with linear and non-linear models and the approach we propose provides a general computational framework for learning output kernels as formalized in [14].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "In particular, we focus on a class of reproducing kernels (known as separable kernels) that can be designed to encode specific tasks structures (see [15, 2] and Sec.", "startOffset": 149, "endOffset": 156}, {"referenceID": 1, "context": "In particular, we focus on a class of reproducing kernels (known as separable kernels) that can be designed to encode specific tasks structures (see [15, 2] and Sec.", "startOffset": 149, "endOffset": 156}, {"referenceID": 31, "context": "In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28].", "startOffset": 265, "endOffset": 276}, {"referenceID": 6, "context": "In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28].", "startOffset": 265, "endOffset": 276}, {"referenceID": 27, "context": "In general, due to discrete nature of the output space, these problems cannot be solved directly; hence, a so-called surrogate problem is often introduced, which is computationally tractable and whose solution allows to recover the solution of the original problem [32, 7, 28].", "startOffset": 265, "endOffset": 276}, {"referenceID": 24, "context": "In analogy to the scalar setting, it can be proved (see [25]) that the Representer Theorem holds also for regularization in RKHSvv.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The choice of kernel \u0393 induces a joint representation of the inputs as well as a structure among the output components [1]; In the rest of the paper we will focus on so-called separable kernels, where these two aspects are factorized.", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "Tasks relations can be enforced by devising suitable regularizers [15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "Interestingly, for a large class of such methods it can be shown that this is equivalent to the choice of the matrix A (or rather its pseudoinverse) [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "If we consider the squared norm of a function f = \u2211n i=1 k(xi, \u00b7)Aci \u2208 H we have (see [15])", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "A different approach to model tasks relatedness consists in choosing a suitable metric on the output space to reflect the tasks structure [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 16, "context": ") [17, 21, 11].", "startOffset": 2, "endOffset": 14}, {"referenceID": 20, "context": ") [17, 21, 11].", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ") [17, 21, 11].", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": "V and the penalty F are differentiable, this is exactly the definition of a convexifiable function, which in particular implies invexity [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "This result was originally proved in [14] for the special case of V the least-squares loss and F (\u00b7) = \u2016 \u00b7 \u2016F the Frobenius norm; Here we have proved its generalization to all convex losses V and penalties F .", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "First, we note that, while the objective function in Problem (R) is convex, the corresponding minimization problem might not be a convex program (in the sense that the feasible set C is not identified by a set of linear equalities and non-linear convex inequalities [9]).", "startOffset": 266, "endOffset": 269}, {"referenceID": 2, "context": "Here we propose a barrier approach inspired by the work in [3] by introducing a perturbation of problem (R) that enforces the objective functions to be equal to +\u221e on the boundary of R \u00d7 S +.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "The proposed barrier method is similar in spirit to the approach developed in [3] and indeed Theorem 3.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "4 are a generalization over the two main results in [3] to any convex penaltyF on the cone of PSD matrices.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "However, notice that since we are considering a much wider family of penalties (than the trace norm as in [3]) our results cannot directly derived from those in [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "However, notice that since we are considering a much wider family of penalties (than the trace norm as in [3]) our results cannot directly derived from those in [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "The characteristic block variable structure of the objective function in problem (S\u03b4), suggests that it might be beneficial to use block coordinate methods (BCM) (see [8]) to solve it.", "startOffset": 167, "endOffset": 170}, {"referenceID": 22, "context": "Instead, when the coefficient matrix C is fixed, the problem of learning A can be interpreted as an unsupervised setting in which the goal is to actually find the underlying task structure [23].", "startOffset": 189, "endOffset": 193}, {"referenceID": 29, "context": "Different strategies to choose which direction minimize at each step have been proposed: pre-fixed cyclic order, greedy search [30] or randomly, according to a predetermined distribution [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "Different strategies to choose which direction minimize at each step have been proposed: pre-fixed cyclic order, greedy search [30] or randomly, according to a predetermined distribution [29].", "startOffset": 187, "endOffset": 191}, {"referenceID": 29, "context": "For a review of several BCD algorithms we refer the reader to [30] and references therein.", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": "1) and has been studied extensively in [33] in the abstract setting where an oracle provides a block-wise minimizer at each iteration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "1 in [33], while for part (b) we refer to Theorem 2 in [30].", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "1 in [33], while for part (b) we refer to Theorem 2 in [30].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "However, up to our knowledge, so far only the authors in [3] have considered the issue of convergence to a global optimum.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "V (Y, Z) = \u2016Y \u2212 Z\u2016F for any two matrices Y, Z \u2208 R) and the structure matrix A is fixed, a closed form solution for the coefficient matrix C returned by the SUPERVISEDSTEP procedure can be easily derived (see for instance [1]):", "startOffset": 221, "endOffset": 224}, {"referenceID": 25, "context": "In [26] the authors proposed a faster approach to solve this problem in closed form based on Sylvester\u2019s method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Figure 1: Comparison of the computational performance of the alternating minimization strategy studied in this paper with respect to the optimization methods proposed for MTCL in [19] and MTFL [3] in the original papers.", "startOffset": 179, "endOffset": 183}, {"referenceID": 2, "context": "Figure 1: Comparison of the computational performance of the alternating minimization strategy studied in this paper with respect to the optimization methods proposed for MTCL in [19] and MTFL [3] in the original papers.", "startOffset": 193, "endOffset": 196}, {"referenceID": 2, "context": "6 generalizes a similar result originally proved in in [3] for the special case p = 1 and provides an explicit formula for the UNSUPERVISEDSTEP of Algorithm 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "The penalty F = \u2016 \u00b7 \u2016F was considered in [14], together with a least squares loss function and the non convex problem (Q) is solved directly by alternating minimization.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "This latter approach can shown to be equivalent to the Multi-Task Feature Learning setting of [3] (see supplementary material).", "startOffset": 94, "endOffset": 97}, {"referenceID": 18, "context": "In [19], the authors studied a multi-task setting where tasks are assumed to be organized in a fixed number r of unknown disjoint clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19] the authors considered a regularization setting of the form of (R) where the structure matrix A is parametrized by the matrix M in order to reflect the cluster structure of the tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "Starting from a multi-task Gaussian Process setting, in [37], authors propose a model where the covariance among the coefficient vectors of the T individual tasks is controlled by a matrix A \u2208 S ++ in the form of a prior.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "For instance [2] requires A to be a graph Laplacian, or [13] imposes a low-rank factorization of A in two smaller matrices.", "startOffset": 13, "endOffset": 16}, {"referenceID": 12, "context": "For instance [2] requires A to be a graph Laplacian, or [13] imposes a low-rank factorization of A in two smaller matrices.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "In [27, 22] different sparsity models are proposed.", "startOffset": 3, "endOffset": 11}, {"referenceID": 21, "context": "In [27, 22] different sparsity models are proposed.", "startOffset": 3, "endOffset": 11}, {"referenceID": 18, "context": "4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) [19] and Multi-task Feature Learning (MTFL [3]]), can be formulated as special cases of problem (Q) or (R).", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) [19] and Multi-task Feature Learning (MTFL [3]]), can be formulated as special cases of problem (Q) or (R).", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14].", "startOffset": 131, "endOffset": 134}, {"referenceID": 36, "context": "In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 13, "context": "In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning (MTFL) [3], Multi-task Relation Learning (MTRL) [37], Output Kernel Learning (OKL) [14].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "We represented images using LLC coding [35] and trained the system on a training set comprising 50, 100 and 150 examples per class.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "For a wide family of models, the problem of jointly learning the tasks and their relations can be cast as a convex program, generalizing previous results for special cases [3, 14].", "startOffset": 172, "endOffset": 179}, {"referenceID": 13, "context": "For a wide family of models, the problem of jointly learning the tasks and their relations can be cast as a convex program, generalizing previous results for special cases [3, 14].", "startOffset": 172, "endOffset": 179}], "year": 2015, "abstractText": "Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.", "creator": "LaTeX with hyperref package"}}}