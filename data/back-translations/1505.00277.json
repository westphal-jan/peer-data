{"id": "1505.00277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2015", "title": "Grounded Discovery of Coordinate Term Relationships between Software Entities", "abstract": "We present an approach to detecting coordinate-term relationships between entities in the software domain that relate to Java classes. Normally, relationships are found by studying corpus statistics associated with text entities. However, in some technical areas, we have access to additional information about the real-world objects named by entities, suggesting that coupling information about the \"grounded\" entities with corpus statistics could lead to improved methods for discovering relationships. To this end, we are developing a similarity measurement for Java classes based on distribution information about their use in software, which we combine with corpus statistics about the distribution of contexts in which the classes appear in the text. Our approach can dramatically improve the accuracy of cross-validation of this data set, from about 60% to 88%.", "histories": [["v1", "Fri, 1 May 2015 20:40:00 GMT  (1946kb,D)", "http://arxiv.org/abs/1505.00277v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.SE", "authors": ["dana movshovitz-attias", "william w cohen"], "accepted": false, "id": "1505.00277"}, "pdf": {"name": "1505.00277.pdf", "metadata": {"source": "CRF", "title": "Grounded Discovery of Coordinate Term Relationships between Software Entities", "authors": ["Dana Movshovitz-Attias", "William W. Cohen"], "emails": ["dma@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": null, "text": "We present an approach for the detection of coordinateterm relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \u201cgrounded\u201d entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs."}, {"heading": "1 Introduction", "text": "Discovering semantic relations between text entities is a key task in natural language understanding. It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].\nIn this work, we examine coordinate relations between words. According to the WordNet glossary, X and Y are defined as coordinate terms if they share a common hypernym [10, 27]. This is a symmetric relation that indicates a semantic similarity, meaning that X and Y are \u201ca type of the same thing\u201d, since they share at least one common ancestor in some hypernym taxonomy (to paraphrase the definition of Snow et al. [37]).\nSemantic similarity relations are normally discovered by comparing corpus statistics associated with the\nentities: for instance, two entities X and Y that usually appear in similar contexts are likely to be semantically similar [7, 32, 33]. However, in technical domains, we have access to additional information about the realworld objects that are named by the entities: e.g., we might have biographical data about a person entity, or a 3D structural encoding of a protein entity. In such situations, it seems plausible that a \u201dgrounded\u201d NLP method, in which corpus statistics are coupled with data on the real-world referents of X and Y , might lead to improved methods for relation discovery.\nHere we explore the idea of grounded relation discovery in the domain of software. In particular, we consider the detection of coordinate-term relationships between entities that (potentially) refer to Java classes. We use a software domain text corpus derived from the Q&A website StackOverflow (SO), in which users ask and answer questions about software development, and we extract posts which have been labeled by users as Java related. From this data, we collected a small set of entity pairs that are labeled as coordinate terms (or not) based on high-precision Hearst patterns and frequency statistics, and we attempt to label these pairs using information available from higher-recall approaches based on distributional similarity.\nWe describe an entity linking method in order to map a given text entity to an underlying class type implementation from the Java standard libraries. Next, we describe corpus and code based information that we use for the relation discovery task. Corpus based methods include distributional similarity and string matching similarity. Additionally, we use two sources of code based information: (1) we define the classcontext of a Java class in a given code repository, and are therefore able to calculate a code-based distributional similarity measure for classes, and (2) we consider the hierarchical organization of classes, described by the Java class type and namespace hierarchies. We demonstrate that using our approach, cross-validation\nar X\niv :1\n50 5.\n00 27\n7v 1\n[ cs\n.C L\n] 1\nM ay\n2 01\naccuracy on this dataset is improved from 60.9% to 88%. According to human labeling, our classifier has an F1score of 86% over the highest-ranking 1000 predicted pairs.\nWe see this work as a first step towards building a knowledge representation system for the software domain, in which text entities refer to elements from a software code base, for example classes, methods, applications and programming languages. Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].\nFigure 1 shows a visualization based on coordinate term pairs predicted using the proposed method. Java classes with similar functionality are highly connected in this graph, indicating that our method can be used to construct a code taxonomy."}, {"heading": "2 Related Work", "text": "Semantic Relation Discovery. Previous work on semantic relation discovery, in particular, coordinate term discovery, has used two main approaches. The first is based on the insight that certain lexical patterns indicate a semantic relationship with high-precision, as initially observed by Hearst [16]. For example, the conjuction pattern \u201cX and Y\u201d indicates that X and Y are coordinate terms. Other pattern-based classifier have\nbeen introduced for meronyms [13], synonyms [24], and general analogy relations [40]. The second approach relies on the notion that words that appear in a similar context are likely to be semantically similar. In contrast to pattern based classifiers, context distributional similarity approaches are normally higher in recall. [7, 32, 33, 36]. In this work we attempt to label samples extracted with high-precision Hearst patterns, using information from higher-recall methods.\nGrounded Language Learning. The aim of grounded language learning methods is to learn a mapping between natural language (words and sentences) and the observed world [14, 35, 44], where more recent work includes grounding language to the physical world [19], and grounding of entire discourses [28]. Early work in this field relied on supervised aligned sentence-tomeaning data [12, 45]. However, in later work the supervision constraint has been gradually relaxed [18,23]. Relative to prior work on grounded language acquisition, we use a very rich and complex representation of entities and their relationships (through software code). However, we consider a very constrained language task, namely coordinate term discovery.\nStatistical Language Models for Software. In recent work by NLP and software engineering researchers, statistical language models have been adapted for modeling software code. NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11]. This has been complemented by work from the programming language research community for structured prediction of code syntax trees [31]. To the best of our knowledge, there is no prior work on discovering semantic relations for software entities."}, {"heading": "3 Coordinate Term Discovery", "text": "In this section we describe a coordinate term classification pipeline, as depicted at high-level in Figure 2. All the following steps are described in detail in the sections below.\nGiven a software domain text corpus (StackOverflow) and a code repository (Java Standard Libraries), our goal is to predict a coordinate relation for \u3008X,Y \u3009, where X and Y are nouns which potentially refer to Java classes.\nWe first attempt a baseline approach of labeling the pair \u3008X,Y \u3009 based on corpus distributional similarity. Since closely related classes often exhibit morphological closeness, we use as a second baseline the string similarity of X and Y .\nNext, we map noun X to an underlying class imple-\nmentation from the code repository, named X \u2032, according to an estimated probability for p(Class X \u2032|Word X), s.t., X \u2032 = maxC p\u0302(C|X), for all other classes C. X \u2032 is then the code referent of X. Similarly, we map Y to the class Y \u2032. Given a code-based grounding for X and Y we extract information using the class implementations: (1) we define a code based distributional similarity measure, using code-context to encode the usage pattern of a class, and (2) we use the hierarchical organization of classes, described by the type and namespace hierarchies. Finally, we combine all the above information in a single SVM classifier."}, {"heading": "3.1 Baseline: Corpus Distributional Similarity.", "text": "As an initial baseline we calculate the corpus distributional similarity of nouns \u3008X,Y \u3009, following the assumption that words with similar context are likely to be semantically similar. Our implementation follows Pereira et al. [33]. We calculate the empirical context distribution for noun X\n(3.1) pX = f(c,X)/ \u2211 c\u2032 f(c\u2032, X)\nwhere f(c,X) is the frequency of occurrence of noun X in context c. We then measure the similarity of nouns X and Y using the relative entropy or Kullback-Leibler divergence (3.2) D(pX ||pY ) = \u2211 z pX(z) log pX(z) pY (z)\nAs this measure is not symmetric we finally consider the distributional similarity of X and Y as D(pX ||pY )+ D(pY ||pX).\n3.2 Baseline: String Similarity. Due to naming convention standards, many related classes often exhibit some morphological closeness. For example, classes that\nprovide Input/Output access to the file system will often contain the suffix Stream or Buffer. Likewise, many classes extend on the names of their super classes (e.g., JRadioButtonMenuItem extends the class JMenuItem). More examples can be found in Figure 1 and Table 4. We therefore include a second baseline which attempts to label the noun pair \u3008X,Y \u3009 as coordinate terms according to their string matching similarity. We use the SecondString open source Java toolkit1. Each string is tokenized by camel case (such that ArrayList is represented as Array List). We consider the SoftTFIDF distance of the tokenized strings, as defined by Cohen et al. [6].\n3.3 Entity Linking. In order to draw code based information on text entities, we define a mapping function between words and class types. Our goal is to find p(C|W ), where C is a specific class implementation and W is a word. This mapping is ambiguous, for example, since users are less likely to mention the qualified class name (e.g., java.lang.String), and usually use the class label, meaning the name of the class not including its package (e.g., String). As an example, the terms java.lang.String and java.util.Vector appears 37 and 1 times respectively in our corpus, versus the terms String and Vector which appear 35K and 1.6K times. Additionally, class names appear with several variations, including, case-insensitive versions, spelling mistakes, or informal names (e.g., array instead of ArrayList).\nTherefore, in order to approximate p(C,W ) in\n(3.3) p(C|W ) = p(C,W ) p(W )\nWe estimate a word to class-type mapping that is mediated through the class label, L, as\n(3.4) p\u0302(C,W ) = p(C,L) \u00b7 p(L,W )\nSince p(C,L) = p(C|L)p(L), this can be estimated by the corresponding MLEs\np\u0302(C,L) = p\u0302(C|L) \u00b7 p\u0302(L)\n= f(C)\u2211 C\u2032\u2208L f(C \u2032) \u00b7 f(L)\u2211 L\u2032 f(L \u2032) (3.5) where f() is the frequency function. Note that since\u2211 C\u2032\u2208L f(C\n\u2032) = f(L) we get that p\u0302(C,L) = p\u0302(C), as the class label is uniquely determined by the class qualified name (the opposite does not hold since multiple class types may correspond to the same label). Finally, the term p(L,W ) is estimated by the symmetric string\n1http://secondstring.sourceforge.net/\ndistance between the two strings, as described in Section 3.2. We consider the linking probability of \u3008X,Y \u3009 to be p\u0302(X \u2032|X) \u00b7 p\u0302(Y \u2032|Y ), where X \u2032 is the best matching class for X s.t. X \u2032 = maxC p\u0302(C|X) and similarly for Y \u2032.\n3.4 Code Distributional Similarity. Corpus distributional similarity evaluates the occurrence of words in particular semantic contexts. By defining the classcontext of a Java class, we can then similarly calculate a code distributional similarity between classes. Our definition of class context is based on the usage of a class as an argument to methods and on the API which the class provides, and it is detailed in Table 1. We observe over 23K unique contexts in our code repository. Based on these definitions we can compute the distributional similarity measure between classes X \u2032 and Y \u2032 based on their code-context distributions, as previously described for the corpus distributional similarity (Section 3.1, following Pereira et al. [33]). For the code-based case, we calculate the empirical context distribution of X \u2032 (see Equation 3.1) using f(c,X \u2032), the occurrence frequency of class X \u2032 in context c, where c is one of the ARGMethod or API-Method contexts (defined in Table 1) for methods observed in the code repository. The distributional similarity of \u3008X \u2032, Y \u2032\u3009 is then taken, using the relative entropy, as D(pX\u2032 ||pY \u2032) + D(pY \u2032 ||pX\u2032).\n3.5 Code Hierarchies and Organization. The words X and Y are defined as coordinate terms if they have the same hypernym in a given taxonomy, meaning they have at least one common ancestor in this taxonomy [36]. For the purpose of comparing two class types, we therefore define an ancestry relation between them using two taxonomies based on the code namespace and type hierarchies.\nPackage Taxonomy: A package is the standard way for defining namespaces in the Java language. It is a mechanism for organizing sets of classes which normally share a common functionality. Packages are organized in a hierarchical structure which can be easily inferred from the class name. For example, the class java.lang.String, belongs to the java.lang package, which belongs to the java package.\nType Taxonomy: The inheritance structure of classes and interfaces in the Java language defines a type hierarchy, such that class A is the ancestor of class B if B extends or implements A.\nWe define type-ancestry and package-ancestry relations between classes \u3008X \u2032, Y \u2032\u3009, based on the above taxonomies. For the type taxonomy,\nAntype(X \u2032, Y \u2032) = {# of common ancestors X \u2032 and Y \u2032 share within n higher up levels in the type taxonomy}\nfor n from 1 to 6. Anpackage is defined similarly for the package taxonomy. As an example,\nA2package(ArrayList,Vector) = 2\nas these classes both belong in the package java.util, and therefore their common level 2 ancestors are: java and java.util. Moreover,\nA1type(ArrayList,Vector) = 5\nsince both classes extend the AbstractList class, and also implement four joint interfaces: List, RandomAccess, Cloneable, and Serializable."}, {"heading": "4 Experimental Settings", "text": "4.1 Data Handling. We downloaded a dump of the interactions on the StackOverflow website2 from its launch date in 2008 and until 2012. We use only the 277K questions labeled with the user-assigned Java tag, and their 629K answers.\nText from the SO html posts was extracted with the Apache Tika toolkit3 and then tokenized with the\n2http://www.clearbits.net/creators/146-stack-exchange-datadump\n3http://tika.apache.org/\nMallet statistical NLP package [26]. In this study, we use only the text portions of the SO posts, and exclude all raw code segments, as indicated by the user-labeled <code> markup. Next, the text was POS tagged with the Stanford POS tagger [39] and parsed with the MaltParser [30]. Finally, we extract noun pairs with the conjunction dependencies: conj or inv-conj, a total of 255,150 pairs, which we use as positive training samples.\nWe use the Java standard libraries code repository as a grounding source for Java classes, as we expect that users will often refer to these classes in the Java tagged SO posts. This data includes: 7072 source code files, the implementation of 10562 class and interface types, and 477 packages. The code repository is parsed using the Eclipse JDT compiler tools, which provide APIs for accessing and manipulating Abstract Syntax Trees.\n4.2 Classification. We follow the classification pipeline described in Figure 2, using the LibLinear SVM classifier [5, 9] with the following features:\nCorpus-Based Features\n\u2022 Corpus distributional similarity (Corpus Dist. Sim.) - see Section 3.1.\n\u2022 String similarity (String Sim.) - see Section 3.2.\nCode-Based Features\n\u2022 Text to code linking probability (Text-to-code Prob.) - see Section 3.3.\n\u2022 Code distributional similarity (Code Dist. Sim.) - see Section 3.4.\n\u2022 Package and type ancestry (A1package - A6package and A 1 type - A 6 type) - see Section 3.5.\nSince the validity of the code based features above is directly related to the success of the entity linking phase,\neach of the code based features are used in the classifier once with the original value and a second time with the value weighted by the text to code linking probability.\nOf the noun pairs \u3008X,Y \u3009 in our data, we keep only pairs for which the linking probability p\u0302(X \u2032|X)\u00b7 p\u0302(Y \u2032|Y ) is greater than 0.1. Note that this guarantees that each noun must be mapped to at least one class with non-zero probability. Next, we evaluate the string morphology and its resemblance to a camel-case format, which is the acceptable formatting for Java class names. We therefore select alphanumeric terms with at least two upper-case and one lower-case characters. We name this set of noun pairs the Coord dataset.\nA key assumption underlying statistical distributional similarity approaches is that \u201chigh-interest\u201d entities are associated with higher corpus frequencies, therefore, given sufficient statistical evidence \u201chigh-interest\u201d relations can be extracted. In the software domain, real\nworld factors may introduce biases in a software-focused text corpus which may affect the corpus frequencies of classes: e.g., users may discuss classes based on the clarity of their API, the efficiency of their implementation, or simply if they are fundamental in software introduced to novice users. Another motivation for using grounded data, such as the class implementation, is that it may highlight additional aspects of interest, for example, classes that are commonly inherited from. We therefore define a second noun dataset, Coord-PMI, which attempts to address this issue, in which noun pairs are selected based on their pointwise mutual information (PMI ):\n(4.6) PMI (X,Y ) = log p(X,Y )\np(X)p(Y )\nwhere the frequency of the pair \u3008X,Y \u3009 in the corpus is positive. In this set we include coordinate term pairs with high PMI scores, which appear more rarely in the corpus and are therefore harder to predict using standard NLP techniques. The negative set in this data are noun pairs which appear frequently separately but do not appear as coordinate terms, and are therefore marked by low PMI scores.\nTo illustrate this point, we provide a sample of noun pairs with low and high PMI scores in Table 2, where pairs highlighted with bold font are labeled as coordinate terms in our data. We can see that the high PMI set contains pairs that are specific and interesting in the software domain while not necessarily being frequent words in the general domain. For example, some pairs seem to represent variable names (e.g., \u3008yearsPlayed, totalEarned\u3009), others likely refer to method names (e.g., \u3008removeListener, addListener\u3009). Some pairs refer to Java classes, such as \u3008JTextField, JComboBox\u3009 whose implementation can be found in the Java code repository. We can also see examples of pairs such as \u3008PostInsertEventListener, PostUpdateEventListener\u3009 which are likely to be user-defined classes with a relationship to the Java class java.util.EventListener. In contrast, the low PMI set contains more general software terms (e.g., code, design, server, threads)."}, {"heading": "5 Results", "text": "5.1 Classification and Feature Analysis. In Table 3 we report the cross validation accuracy of the coordinate term classifier (Code & Corpus) as well as baseline classifiers using corpus distributional similarity (Corpus Dist. Sim.), string similarity (String Sim.), all corpus features (All Corpus), or all code features (All Code). Note that using all code features is significantly more successful on this data than any of the corpus baselines (corpus baselines\u2019 accuracy is between 57%-\n65% whereas code-based accuracy is over 80%). When using both data sources, performance is improved even further (to over 85% on the Coord dataset and 88% on Coord-PMI ).\nWe provide an additional feature analysis in Table 3, and report the cross validation accuracy of classifiers using each single code feature. Interestingly, code distributional similarity (Code Dist. Sim.) is the strongest single feature, and it is a significantly better predictor than corpus distributional similarity, achieving around 67% v.s. around 58% for both datasets.\n5.2 Evaluation by Manual Labeling. The crossvalidation results above are based on labels extracted using Hearst conjunction patterns. In Figure 3 we provide an additional analysis based on manual human labeling of samples from the Coord-PMI dataset, following a procedure similar to prior researchers exploring semi-supervised methods for relation discovery [4, 21]. After all development was complete, we hand labeled the top 1000 coordinate term pairs according to the ranking by our full classifier (using all code and corpus features) and the top 1000 pairs predicted by the classifiers based on code and corpus distributional similarities only. We report the F1 results of each classifier by the rank of the predicted samples. According to our analysis, the F1 score for the text and code distributional similarity classifiers degrades quickly after the first 100 and 200 top ranked pairs, respectively. At rank 1000, the score of the full classifier is at 86%, whereas the code and text classifiers are only at 56% and 28%.\nTo highlight the strength of each of the code based features, we provide in Table 4 the top ten coordinate terms predicted using the most successful code based features. For example, the top prediction using type hierarchy ancestry (A5type) is \u3008JMenuItem, JMenu\u3009. Since JMenu extends JMenuItem, the two classes indeed share many common interfaces and classes. Alternatively, all of the top predictions using the package hierarchy ancestry (A3package) are labels that have been matched to pairs of classes that share at least 3 higher up package levels. So for example, BlockQueue has been matched to java.util.concurrent.BlockingQueue which was predicted as a coordinate term of ThreadPoolExecutor which belongs in the same package. Using code dis-\ntributional similarity, one of the top predictions is the pair \u3008GZIPOutputStream, DeflaterOutputStream\u3009, which share many common API methods such as write, flush, and close. Many of the other top predicted pairs by this feature have been mapped to the same class and therefore have the exact same context distribution.\n5.3 Taxonomy Construction. We visualize the coordinate term pairs predicted using our method (with all features), by aggregating them into a graph where entities are nodes and edges are determined by a coordinate term relation (Figure 1). Graph edges are colored using the Louvain method [2] for community detection and an entity label\u2019s size is determined by its betweenness centrality degree. We can see that high-level communities in this graph correspond to class functionality, indicating that our method can be used to create an interesting code taxonomy.\nNote that our predictions also highlight connections within functional groups that cannot be found using the package or type taxonomies directly. One example can be highlighted within the GUI functionality group. Listener classes facilitate a response mechanism to GUI Actions, such as pressing a button, or entering text, however, these classes belong in different packages than basic GUI components for historical reasons. In our graph, Action and Listener classes belong to the same communities of the GUI components they are normally used with."}, {"heading": "6 Conclusions", "text": "We have presented an approach for grounded discovery of coordinate term relationships between text entities representing Java classes. Using a simple entity linking method we map text entities to an underlying class type implementation from the Java standard libraries. With this code-based grounding, we extract information on the usage pattern of the class and its location in the Java class and namespace hierarchies. Our experimental evaluation shows that using only corpus distributional similarity for the coordinate term prediction task is unsuccessful, achieving prediction accuracy of around 58%. However, adding information based on the entities\u2019 software implementation improves accuracy dramatically to 88%. Our classifier has an F1 score of 86% according to human labeling over the top 1000 predicted pairs. We have shown that our predictions can be used to build an interesting code taxonomy which draws from the functional connections, common usage patterns, and implementation details that are shared between classes.\nReferences\n[1] Dave Binkley, Matthew Hearn, and Dawn Lawrie. Improving identifier informativeness using part of speech information. In Proc. of the Working Conference on Mining Software Repositories. ACM, 2011. [2] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008. [3] SRK Branavan, Luke S Zettlemoyer, and Regina Barzilay. Reading between the lines: Learning to map highlevel instructions to commands. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. ACL, 2010. [4] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr, and Tom M Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010. [5] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2011. [6] William W Cohen, Pradeep D Ravikumar, Stephen E Fienberg, et al. A comparison of string distance metrics for name-matching tasks. In IIWeb, 2003. [7] James Richard Curran. From distributional to semantic similarity. PhD thesis, University of Edinburgh. College of Science and Engineering. School of Informatics., 2004. [8] Anthony Fader, Stephen Soderland, and Oren Etzioni. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. ACL, 2011. [9] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9, 2008. [10] Christiane Fellbaum. Wordnet: An electronic lexical database, 1998. [11] Mark Gabel and Zhendong Su. Javert: fully automatic mining of general temporal properties from dynamic traces. In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering. ACM, 2008. [12] Ruifang Ge and Raymond J Mooney. A statistical semantic parser that integrates syntax and semantics. In Computational Natural Language Learning. ACL, 2005. [13] Roxana Girju, Adriana Badulescu, and Dan Moldovan. Learning semantic constraints for the automatic discovery of part-whole relations. In North American Chapter of the Association for Computational Linguistics on Human Language Technology. ACL, 2003. [14] Peter Gorniak and Deb Roy. Situated language understanding as filtering perceived affordances. Cognitive Science, 2007. [15] Sangmok Han, David R Wallace, and Robert C Miller. Code completion from abbreviated input. In Automated Software Engineering. IEEE, 2009. [16] Marti A Hearst. Automatic acquisition of hyponyms\nfrom large text corpora. In Proceedings of the 14th conference on Computational linguistics. ACL, 1992. [17] Ferosh Jacob and Robert Tairas. Code template inference using language models. In Southeast Regional Conference. ACM, 2010. [18] Rohit J Kate and Raymond J Mooney. Learning language semantics from ambiguous supervision. In AAAI, 2007. [19] Jayant Krishnamurthy and Thomas Kollar. Jointly learning to parse and perceive: Connecting natural language to the physical world. TACL, 2013. [20] Jayant Krishnamurthy and Tom M Mitchell. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. ACL, 2012. [21] Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a large scale knowledge base. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011. [22] Dawn Lawrie, Christopher Morrell, Henry Feild, and David Binkley. Whats in a name? a study of identifiers. In ICPC 2006. 14th IEEE International Conference on Program Comprehension. IEEE, 2006. [23] Percy Liang, Michael I Jordan, and Dan Klein. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 2009. [24] Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. Identifying synonyms among distributionally similar words. In IJCAI, 2003. [25] Thomas Lin, Oren Etzioni, et al. Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction. ACL, 2012. [26] Andrew Kachites McCallum. Mallet: A machine learning for language toolkit. 2002. [27] George A Miller. Wordnet: A lexical database for english. Communications of the ACM, 1995. [28] Thang Luong Minh, Michael C Frank, and Mark Johnson. Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning. TACL, 2013. [29] Dana Movshovitz-Attias and William W. Cohen. Natural language models for predicting programming comments. In Association for Computational Linguistics, pages 35\u201340. ACL, 2013. [30] Joakim Nivre, Johan Hall, and Jens Nilsson. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC, 2006. [31] Cyrus Omar. Structured statistical syntax tree prediction. In Proceedings of the 2013 companion publication for conference on Systems, programming, & applications: software for humanity. ACM, 2013. [32] Patrick Andre Pantel. Clustering by committee. PhD\nthesis, Department of Computing Science, University of Alberta, 2003. [33] Fernando Pereira, Naftali Tishby, and Lillian Lee. Distributional clustering of english words. In ACL, 1993. [34] Peter Schulam, Roni Rosenfeld, and Premkumar Devanbu. Building statistical language models of code. In Proc. DAPSE. IEEE, 2013. [35] Jeffrey Mark Siskind. A computational study of crosssituational techniques for learning word-to-meaning mappings. Cognition, 1996. [36] Rion Snow, Daniel Jurafsky, and Andrew Y Ng. Learning syntactic patterns for automatic hypernym discovery. In NIPS, 2004. [37] Rion Snow, Daniel Jurafsky, and Andrew Y Ng. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2006. [38] Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell. Coupled temporal scoping of relational facts. In Proceedings of the fifth ACM international conference on Web search and data mining. ACM, 2012. [39] Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. Association for Computational Linguistics, 2003. [40] Peter Turney, Michael L Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent modules to solve multiple-choice synonym and analogy problems. 2003. [41] Xiaoyin Wang, David Lo, Jing Jiang, Lu Zhang, and Hong Mei. Extracting paraphrases of technical terms from noisy parallel software corpora. In Proceedings of the ACL-IJCNLP. ACL, 2009. [42] Markus Weimer, Iryna Gurevych, and Max Mu\u0308hlha\u0308user. Automatically assessing the post quality in online discussions on software. In Proceedings of the 45th Annual Meeting of the ACL. ACL, 2007. [43] Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. Textrunner: open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. ACL, 2007. [44] Chen Yu and Dana H Ballard. On the integration of grounding language and learning objects. In AAAI, 2004. [45] Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. Uncertainty in Artificial Intelligence, 2005."}], "references": [{"title": "Improving identifier informativeness using part of speech information", "author": ["Dave Binkley", "Matthew Hearn", "Dawn Lawrie"], "venue": "In Proc. of the Working Conference on Mining Software Repositories. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Fast unfolding of communities in large networks", "author": ["Vincent D Blondel", "Jean-Loup Guillaume", "Renaud Lambiotte", "Etienne Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Reading between the lines: Learning to map highlevel instructions to commands", "author": ["SRK Branavan", "Luke S Zettlemoyer", "Regina Barzilay"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A comparison of string distance metrics for name-matching tasks", "author": ["William W Cohen", "Pradeep D Ravikumar", "Stephen E Fienberg"], "venue": "IIWeb,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "From distributional to semantic similarity", "author": ["James Richard Curran"], "venue": "PhD thesis, University of Edinburgh. College of Science and Engineering. School of Informatics.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Identifying relations for open information extraction", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing. ACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang- Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Javert: fully automatic mining of general temporal properties from dynamic traces", "author": ["Mark Gabel", "Zhendong Su"], "venue": "In Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A statistical semantic parser that integrates syntax and semantics", "author": ["Ruifang Ge", "Raymond J Mooney"], "venue": "In Computational Natural Language Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Learning semantic constraints for the automatic discovery of part-whole relations", "author": ["Roxana Girju", "Adriana Badulescu", "Dan Moldovan"], "venue": "In North American Chapter of the Association for Computational Linguistics on Human Language Technology. ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Situated language understanding as filtering perceived affordances", "author": ["Peter Gorniak", "Deb Roy"], "venue": "Cognitive Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Code completion from abbreviated input", "author": ["Sangmok Han", "David R Wallace", "Robert C Miller"], "venue": "In Automated Software Engineering", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Automatic acquisition of hyponyms  from large text corpora", "author": ["Marti A Hearst"], "venue": "In Proceedings of the 14th conference on Computational linguistics. ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Code template inference using language models", "author": ["Ferosh Jacob", "Robert Tairas"], "venue": "In Southeast Regional Conference", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Learning language semantics from ambiguous supervision", "author": ["Rohit J Kate", "Raymond J Mooney"], "venue": "In AAAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar"], "venue": "TACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Weakly supervised training of semantic parsers", "author": ["Jayant Krishnamurthy", "Tom M Mitchell"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Whats in a name? a study of identifiers", "author": ["Dawn Lawrie", "Christopher Morrell", "Henry Feild", "David Binkley"], "venue": "ICPC", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Learning semantic correspondences with less supervision", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Dekang Lin", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In IJCAI,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Entity linking at web scale", "author": ["Thomas Lin", "Oren Etzioni"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction. ACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "A machine learning for language toolkit", "author": ["Andrew Kachites McCallum. Mallet"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Wordnet: A lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning", "author": ["Thang Luong Minh", "Michael C Frank", "Mark Johnson"], "venue": "TACL,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Natural language models for predicting programming comments", "author": ["Dana Movshovitz-Attias", "William W. Cohen"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Maltparser: A data-driven parser-generator for dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In Proceedings of LREC,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Structured statistical syntax tree prediction", "author": ["Cyrus Omar"], "venue": "In Proceedings of the 2013 companion publication for conference on Systems, programming, & applications: software for humanity. ACM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Clustering by committee", "author": ["Patrick Andre Pantel"], "venue": "PhD  thesis, Department of Computing Science, University of Alberta,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Distributional clustering of english words", "author": ["Fernando Pereira", "Naftali Tishby", "Lillian Lee"], "venue": "In ACL,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1993}, {"title": "Building statistical language models of code", "author": ["Peter Schulam", "Roni Rosenfeld", "Premkumar Devanbu"], "venue": "In Proc. DAPSE. IEEE,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "A computational study of crosssituational techniques for learning word-to-meaning", "author": ["Jeffrey Mark Siskind"], "venue": "mappings. Cognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1996}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Coupled temporal scoping of relational facts", "author": ["Partha Pratim Talukdar", "Derry Wijaya", "Tom Mitchell"], "venue": "In Proceedings of the fifth ACM international conference on Web search and data mining. ACM,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. Association for Computational Linguistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Combining independent modules to solve multiple-choice synonym and analogy problems", "author": ["Peter Turney", "Michael L Littman", "Jeffrey Bigham", "Victor Shnayder"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Extracting paraphrases of technical terms from noisy parallel software corpora", "author": ["Xiaoyin Wang", "David Lo", "Jing Jiang", "Lu Zhang", "Hong Mei"], "venue": "In Proceedings of the ACL-IJCNLP. ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Automatically assessing the post quality in online discussions on software", "author": ["Markus Weimer", "Iryna Gurevych", "Max M\u00fchlh\u00e4user"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL. ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Textrunner: open information extraction on the web", "author": ["Alexander Yates", "Michael Cafarella", "Michele Banko", "Oren Etzioni", "Matthew Broadhead", "Stephen Soderland"], "venue": "In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. ACL,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2007}, {"title": "On the integration of grounding language and learning objects", "author": ["Chen Yu", "Dana H Ballard"], "venue": "In AAAI,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S Zettlemoyer", "Michael Collins"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}], "referenceMentions": [{"referenceID": 41, "context": "It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].", "startOffset": 135, "endOffset": 138}, {"referenceID": 36, "context": "It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].", "startOffset": 251, "endOffset": 255}, {"referenceID": 23, "context": "It is a critical component which enables the success of knowledge representation systems such as TextRunner [43], ReVerb [8], and NELL [4], which in turn are useful for a variety of NLP applications, including, temporal scoping [38], semantic parsing [20] and entity linking [25].", "startOffset": 275, "endOffset": 279}, {"referenceID": 25, "context": "According to the WordNet glossary, X and Y are defined as coordinate terms if they share a common hypernym [10, 27].", "startOffset": 107, "endOffset": 115}, {"referenceID": 35, "context": "[37]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Semantic similarity relations are normally discovered by comparing corpus statistics associated with the entities: for instance, two entities X and Y that usually appear in similar contexts are likely to be semantically similar [7, 32, 33].", "startOffset": 228, "endOffset": 239}, {"referenceID": 30, "context": "Semantic similarity relations are normally discovered by comparing corpus statistics associated with the entities: for instance, two entities X and Y that usually appear in similar contexts are likely to be semantically similar [7, 32, 33].", "startOffset": 228, "endOffset": 239}, {"referenceID": 31, "context": "Semantic similarity relations are normally discovered by comparing corpus statistics associated with the entities: for instance, two entities X and Y that usually appear in similar contexts are likely to be semantically similar [7, 32, 33].", "startOffset": 228, "endOffset": 239}, {"referenceID": 2, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 209, "endOffset": 221}, {"referenceID": 27, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 209, "endOffset": 221}, {"referenceID": 39, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 209, "endOffset": 221}, {"referenceID": 40, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 209, "endOffset": 221}, {"referenceID": 0, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 324, "endOffset": 337}, {"referenceID": 13, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 324, "endOffset": 337}, {"referenceID": 15, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 324, "endOffset": 337}, {"referenceID": 32, "context": "Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain [3,29,41,42] and improve a variety of code assisting applications, including code refactoring and token completion [1, 15,17,34].", "startOffset": 324, "endOffset": 337}, {"referenceID": 14, "context": "The first is based on the insight that certain lexical patterns indicate a semantic relationship with high-precision, as initially observed by Hearst [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "been introduced for meronyms [13], synonyms [24], and general analogy relations [40].", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "been introduced for meronyms [13], synonyms [24], and general analogy relations [40].", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "been introduced for meronyms [13], synonyms [24], and general analogy relations [40].", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "[7, 32, 33, 36].", "startOffset": 0, "endOffset": 15}, {"referenceID": 30, "context": "[7, 32, 33, 36].", "startOffset": 0, "endOffset": 15}, {"referenceID": 31, "context": "[7, 32, 33, 36].", "startOffset": 0, "endOffset": 15}, {"referenceID": 34, "context": "[7, 32, 33, 36].", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "The aim of grounded language learning methods is to learn a mapping between natural language (words and sentences) and the observed world [14, 35, 44], where more recent work includes grounding language to the physical world [19], and grounding of entire discourses [28].", "startOffset": 138, "endOffset": 150}, {"referenceID": 33, "context": "The aim of grounded language learning methods is to learn a mapping between natural language (words and sentences) and the observed world [14, 35, 44], where more recent work includes grounding language to the physical world [19], and grounding of entire discourses [28].", "startOffset": 138, "endOffset": 150}, {"referenceID": 42, "context": "The aim of grounded language learning methods is to learn a mapping between natural language (words and sentences) and the observed world [14, 35, 44], where more recent work includes grounding language to the physical world [19], and grounding of entire discourses [28].", "startOffset": 138, "endOffset": 150}, {"referenceID": 17, "context": "The aim of grounded language learning methods is to learn a mapping between natural language (words and sentences) and the observed world [14, 35, 44], where more recent work includes grounding language to the physical world [19], and grounding of entire discourses [28].", "startOffset": 225, "endOffset": 229}, {"referenceID": 26, "context": "The aim of grounded language learning methods is to learn a mapping between natural language (words and sentences) and the observed world [14, 35, 44], where more recent work includes grounding language to the physical world [19], and grounding of entire discourses [28].", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": "Early work in this field relied on supervised aligned sentence-tomeaning data [12, 45].", "startOffset": 78, "endOffset": 86}, {"referenceID": 43, "context": "Early work in this field relied on supervised aligned sentence-tomeaning data [12, 45].", "startOffset": 78, "endOffset": 86}, {"referenceID": 16, "context": "However, in later work the supervision constraint has been gradually relaxed [18,23].", "startOffset": 77, "endOffset": 84}, {"referenceID": 21, "context": "However, in later work the supervision constraint has been gradually relaxed [18,23].", "startOffset": 77, "endOffset": 84}, {"referenceID": 13, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 119, "endOffset": 132}, {"referenceID": 15, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 119, "endOffset": 132}, {"referenceID": 27, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 119, "endOffset": 132}, {"referenceID": 32, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 119, "endOffset": 132}, {"referenceID": 0, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 166, "endOffset": 172}, {"referenceID": 20, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 166, "endOffset": 172}, {"referenceID": 9, "context": "NLP models have been used to enhance a variety of software development tasks such as code and comment token completion [15,17,29,34], analysis of code variable names [1,22], and mining software repositories [11].", "startOffset": 207, "endOffset": 211}, {"referenceID": 29, "context": "This has been complemented by work from the programming language research community for structured prediction of code syntax trees [31].", "startOffset": 131, "endOffset": 135}, {"referenceID": 31, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[33]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "The words X and Y are defined as coordinate terms if they have the same hypernym in a given taxonomy, meaning they have at least one common ancestor in this taxonomy [36].", "startOffset": 166, "endOffset": 170}, {"referenceID": 24, "context": "Mallet statistical NLP package [26].", "startOffset": 31, "endOffset": 35}, {"referenceID": 37, "context": "Next, the text was POS tagged with the Stanford POS tagger [39] and parsed with the MaltParser [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "Next, the text was POS tagged with the Stanford POS tagger [39] and parsed with the MaltParser [30].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "We follow the classification pipeline described in Figure 2, using the LibLinear SVM classifier [5, 9] with the following features:", "startOffset": 96, "endOffset": 102}, {"referenceID": 8, "context": "We follow the classification pipeline described in Figure 2, using the LibLinear SVM classifier [5, 9] with the following features:", "startOffset": 96, "endOffset": 102}, {"referenceID": 3, "context": "In Figure 3 we provide an additional analysis based on manual human labeling of samples from the Coord-PMI dataset, following a procedure similar to prior researchers exploring semi-supervised methods for relation discovery [4, 21].", "startOffset": 224, "endOffset": 231}, {"referenceID": 19, "context": "In Figure 3 we provide an additional analysis based on manual human labeling of samples from the Coord-PMI dataset, following a procedure similar to prior researchers exploring semi-supervised methods for relation discovery [4, 21].", "startOffset": 224, "endOffset": 231}, {"referenceID": 1, "context": "Graph edges are colored using the Louvain method [2] for community detection and an entity label\u2019s size is determined by its betweenness centrality degree.", "startOffset": 49, "endOffset": 52}], "year": 2015, "abstractText": "We present an approach for the detection of coordinateterm relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \u201cgrounded\u201d entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.", "creator": "LaTeX with hyperref package"}}}