{"id": "1505.00284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2015", "title": "Bayesian Policy Reuse", "abstract": "A durable, autonomous actor should be able to respond online to new cases of tasks from a familiar area. Acting online requires \"fast\" responses in the sense of rapid convergence, especially when the task instance has a short duration, such as in applications that involve interactions with people. These requirements can be problematic for many established methods of learning to act. In areas where the actor knows that the task instance comes from a family of related tasks, though without accessing the label of a particular instance, he may choose to act through a process of political reuse from a library rather than from scratch through political learning. In policy reuse, the actor has reduced task class knowledge in the form of a library of strategies that were learned during an offline training phase from sample task instances. We formalize the problem of political reuse and present an algorithm to efficiently respond to a novel task instance by reacting to previously existing policy by relying on a response to the problem of observing the library from a familiar area.", "histories": [["v1", "Fri, 1 May 2015 21:13:00 GMT  (376kb,D)", "https://arxiv.org/abs/1505.00284v1", "32 pages, submitted to the Machine Learning Journal"], ["v2", "Mon, 14 Dec 2015 15:44:51 GMT  (418kb,D)", "http://arxiv.org/abs/1505.00284v2", "32 pages, submitted to the Machine Learning Journal"]], "COMMENTS": "32 pages, submitted to the Machine Learning Journal", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["benjamin rosman", "majd hawasly", "subramanian ramamoorthy"], "accepted": false, "id": "1505.00284"}, "pdf": {"name": "1505.00284.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Benjamin Rosman", "Majd Hawasly", "Subramanian Ramamoorthy"], "emails": ["BRosman@csir.co.za.", "M.Hawasly@ed.ac.uk.", "S.Ramamoorthy@ed.ac.uk."], "sections": [{"heading": null, "text": "? The first two authors contributed equally to this paper.\nBenjamin Rosman Mobile Intelligent Autonomous Systems (MIAS), Council for Scientific and Industrial Research (CSIR), South Africa, and the School of Computer Science and Applied Mathematics, University of the Witwatersrand, South Africa. E-mail: BRosman@csir.co.za.\nMajd Hawasly School of Informatics, University of Edinburgh, UK. E-mail: M.Hawasly@ed.ac.uk.\nSubramanian Ramamoorthy School of Informatics, University of Edinburgh, UK. E-mail: S.Ramamoorthy@ed.ac.uk.\nar X\niv :1\n50 5.\n00 28\n4v 2\n[ cs\n.A I]\n1 4\nD ec\n2 01\nKeywords Policy Reuse \u00b7 Reinforcement Learning \u00b7 Online Learning \u00b7 Online Bandits \u00b7 Transfer Learning \u00b7 Bayesian Optimisation \u00b7 Bayesian Decision Theory."}, {"heading": "1 Introduction", "text": "As robots and software agents are becoming more ubiquitous in many applications involving human interactions, greater numbers of scenarios require new forms of decision making that allow fast responses to situations that may drift or change from their nominal descriptions.\nFor example, online personalisation (Mahmud et al., 2014) is becoming a core concept in human-computer interaction (HCI), driven largely by a proliferation of new sensors and input devices which allow for a more natural means of communicating with hardware. Consider, for example, an interactive interface in a public space like a museum that aims to provide information or services to users through normal interaction means such as natural speech or body gestures. The difficulty in this setting is that the same device may be expected to interact with a wide and diverse pool of users, who differ both at the low level of interaction speeds and faculties, and at the higher level of which expressions or gestures seem appropriate for particular commands. The device should autonomously calibrate itself to the class of user, and a mismatch in that could result in a failed interaction. On the other hand, taking too long to calibrate is likely to frustrate the user (Rosman et al., 2014), who may then abandon the interaction.\nThis problem, characterised as a short-term interactive adaptation to a new situation (the user), also appears in interactive situations other than HCI. As an example, consider a system for localising and monitoring poachers in a large wildlife reserve1 that comprises an intelligent base station which can deploy lightweight airborne drones to scan particular locations in the reserve for unusual activity. While the tactics followed by the poachers in every trial would be different, the possible number of drone deployments in a single instance of this adversarial problem is limited, as the poachers can be expected to spend a limited time stalking their target before leaving.\nIn this paper, we formalise and propose a solution to the general problem inspired by these real world examples. To this end, we present a number of simulated scenarios to investigate different facets of this problem, and contrast the proposed solution with related approaches from the literature.\nThe key component of this problem is the need for efficient decision making, in the sense that the agent is required to adapt or respond to scenarios which exist only for short durations. As a result, solution methods are required to have both low convergence time and low regret. To this end, the key intuition we employ is that nearly-optimal solutions computed within a small computational and time budget are preferred to those that are optimal but unbounded in time and computation. Building on this, the question we address in this paper is how to act well (not necessarily optimally) in an efficient manner (for short duration tasks) in a large space of qualitatively-similar tasks.\nWhile it is unreasonable to expect that an arbitrary task instance could be solved from scratch in a short duration task (where, in general, the interaction\n1 Poaching of large mammals such as rhinoceroses is a major problem throughout Africa and Asia (Amin et al., 2006).\nlength is unknown), it is plausible to consider seeding the process with a set of policies of previously solved, related task instances, in what can be seen as a strategy for transfer learning (Taylor and Stone, 2009). In this sense, we prefer to quickly select a nearly-optimal pre-learnt policy, rather than learn an optimal one for each new task. For our previous examples, the interactive interface may ship with a set of different classes of user profiles which have been acquired offline, and the monitoring system may be equipped with a collection of pre-learnt behaviours to navigate the reserve when a warning is issued.\nWe term this problem of short-lived sequential policy selection for a new instance the policy reuse problem, which differs slightly from other uses of that term (see Section 1.1), and we define it as follows.\nDefinition 1 (Policy Reuse Problem) Let an agent be a decision making entity in a specific domain, equipped with a policy library \u03a0 for some tasks in that domain. The agent is presented with an unknown task which must be solved within a limited, and small, number of trials. At the beginning of each trial episode, the agent can select one policy from \u03a0 to execute for the full episode. The goal of the agent is thus to\nselect policies for the new task from \u03a0 to minimise the total regret, with respect to the performance of the best alternative from \u03a0 in hindsight, incurred in the limited task duration.\nThe online choice from a set of alternatives for minimal regret could be posed as a multi-armed bandit problem. Here, each arm corresponds to a pre-learnt policy, and our problem becomes that of a sequential, finite-horizon, optimal selection from a fixed set of policies. Solving this problem in general is difficult as it maps into the intractable finite-horizon online bandit problem (Nin\u0303o-Mora, 2011). On the other hand, traditional approaches to solve the multi-armed bandit problem involve testing each available arm on the new task in order to gauge its performance, which may be a very costly procedure from a convergence rate point of view.\nInstead, one can exploit knowledge which has been acquired offline to improve online response times. These more informed approaches to the multi-armed bandit problem exploit background domain information (e.g. contexts in contextual bandits (Strehl et al., 2006; Langford and Zhang, 2008)) or analytical forms of reward (e.g. correlated bandits (Pandey et al., 2007)) to share the credit of pulling an arm between many possible arms. This however requires prior knowledge of the domain and its metrics, how possible domain instances relate to the arms, and in some cases to be able to determine this side information for any new instance.\nWe propose a solution for policy reuse that neither requires complete knowledge of the space of possible task instances nor a metric in that space, but rather builds a surrogate model of this space from offline-captured correlations between the policies when tested under canonical operating scenarios. Our solution then maintains a Bayesian belief over the nature of the new task instance in relation to the previously-solved ones. Then, executing a policy provides the agent with information which, when combined with the model, is not only useful to evaluate the policy chosen but also to gauge the suitability of other policies. This information updates the belief, which facilitates choosing the next policy to execute.\nWe treat the policy selection in the policy reuse problem as one of optimisation of the response surface of the new task instance, although over a finite library\nof policies. Because we are dealing with tasks which we assume are of limited duration and which do not allow extensive experimenting, and in order to use information from previous trials to maintain belief distributions over the task space, we draw inspiration from the Bayesian optimisation/efficient global optimisation literature (Brochu et al., 2010) for an approach to this problem that is efficient in the number of policy executions, corresponding to function evaluations in the classical optimisation setting.\n1.1 Other Definitions of Policy Reuse\nA version of the policy reuse problem was described by Mahmud et al. (2013), where it is used to test a set of landmark policies retrieved through clustering in the space of MDPs. Additionally, the term \u2018policy reuse\u2019 has been used by Ferna\u0301ndez and Veloso (2006) in a different context. There, a learning agent is equipped with a library of previous policies to aid in exploration, as they enable the agent to collect relevant information quickly to accelerate learning. In our case, we do not expect to have enough time to learn a full policy, and so instead rely on aggressive knowledge transfer using our proposed policy reuse framework to achieve the objective of the agent.\n1.2 Contributions and Paper Organisation\nThe primary contributions made in this paper are as follows:\n1. We introduce Bayesian Policy Reuse (BPR) as a general Bayesian framework for solving the policy reuse problem as defined in Definition 1 (Section 2). 2. We present several specific instantiations of BPR using different policy selection mechanisms (Section 3.1), and compare them on an online personalisation domain (Section 4.2) as well as a domain modelling a surveillance problem (Section 4.3). 3. We provide an empirical analysis of the components of our model, considering different classes of observation signal, and the trade-off between library size and convergence rate."}, {"heading": "2 Bayesian Policy Reuse", "text": "We now pose the policy reuse transfer problem within a Bayesian framework. Bayesian Policy Reuse (BPR) builds on the intuition that, in many cases, performance of a specific policy is better, relative to the other policies in the library, in tasks within some neighbourhood of the task for which it is known to be optimal. Thus, a model that measures the similarity between a new task and other known tasks may provide indications as to which policies may be the best to reuse. We learn such a model from offline experience, and then use it online as a Bayesian prior over the task space, which is updated with new observations from the current task. Note that in this work we consider the general case where we do not have a parametrisation of the task space that allows constructing that model explicitly\n(e.g. da Silva et al. (2012)). This may be the case where aspects of the model may vary qualitatively (e.g. different personality types), or where the agent has not been exposed to enough variations of the task to learn the underlying parametric model sufficiently.\n2.1 Notation\nLet the space of task instances be X , and let a task instance x \u2208 X be specified by a Markov Decision Process (MDP). An MDP is defined as a tuple \u00b5 = (S,A, T,R, \u03b3), where S is a finite set of states; A is a finite set of actions which can be taken by the agent; T : S \u00d7 A \u00d7 S \u2192 [0, 1] is the state transition function where T (s, a, s\u2032) gives the probability of transitioning from state s to state s\u2032 after taking action a; R : S\u00d7A\u00d7S \u2192 R is the reward function, where R(s, a, s\u2032) is the reward received by the agent when transitioning from state s to s\u2032 with action a; and finally, \u03b3 \u2208 [0, 1] is a discounting factor. As T is a probability function, \u2211 s\u2032\u2208S T (s, a, s\n\u2032) = 1,\u2200a \u2208 A,\u2200s \u2208 S. Denote the space of all MDPs M. We will consider episodic tasks, i.e. tasks that have a bounded time horizon.\nA policy \u03c0 : S \u00d7 A \u2192 [0, 1] for an MDP is a distribution over states and actions, defining the probability of taking any action from a state. The return, or utility, generated from running the policy \u03c0 in an episode of a task instance is the accumulated discounted reward, U\u03c0 = \u2211k i=0 \u03b3\niri, with k being the length of the episode and ri being the reward received at step i. We refer to U\n\u03c0 generated from a policy \u03c0 in a task instance simply as the policy\u2019s performance. Solving an MDP \u00b5 is to acquire an optimal policy \u03c0\u2217 = arg max\u03c0 U\n\u03c0 which maximises the total expected return of \u00b5. For a reinforcement learning agent, T and R are typically unknown. We denote a collection of policies possessed by the agent by \u03a0, and refer to it as the policy library.\nWe complete the discussion of the formulation of a task with the definition of signals. The aim of signals is to provide the agent with auxiliary information that hints toward identifying the nature of the new task instance in the context of the previously-solved instances.\nDefinition 2 (Signal) A signal \u03c3 \u2208 \u03a3 is any information which is correlated with the performance of a policy and which is provided to the agent in an online execution of the policy on a task.\nThe most straightforward signal is the performance itself, unless this is not directly observable (e.g. in cases where the payoff may only be known after some time horizon). The information content and richness of a signal determines how easily an agent can identify the type of the new task with respect to the previously-solved types. This is discussed in more detail in Section 2.8.\nThroughout the discussion, we adopt the following notational convention: P(\u00b7) refers to a probability, E[\u00b7] refers to an expectation, H(\u00b7) refers to entropy, and \u2206(\u00b7) is a distribution.\n2.2 Overview\nFor a set of previously-solved tasks X and a set of policies \u03a0, Bayesian Policy Reuse involves two key probability models:\n\u2013 The first, P(U |X , \u03a0), where U \u2208 R is utility, is the performance model ; a probability model over performance of the library of policies \u03a0 on the set of previously-solved tasks X . This information is available in an offline phase. \u2013 The second key component is the observation model, defined as a probability distribution P(\u03a3|X , \u03a0) over \u03a3, the space of possible observation signals. Any kind of information that can be observed online and that is correlated with the performance can be a used as an observation signal. When performance information is directly observable online (e.g., not delayed), performance can be used as the signal, and in this case the observation and the performance models can be the same.\nA caricature of the BPR problem for a one-dimensional task space is shown in Figure 1, where, given a new task x\u2217 \u2208 X , the agent is required to select the best policy \u03c0\u2217 \u2208 \u03a0 in as few trials as possible, whilst minimising the accumulated regret in the interim. As shown in this example, the agent has prior knowledge in the form of performance models for each policy in \u03a0 on a set of tasks from X . The agent additionally has observation models of the signals generated by each task-policy pair, but these are not depicted in Figure 1).\n2.3 General Algorithm\nApplying a specific policy on a new task instance generates sample observation signals, which are used along with the observation model to update a \u2018similarity\u2019 measure over the previously-solved tasks (which is a distribution we call the belief ). This belief informs the selection of a policy at the next trial in an attempt to optimise expected performance. This is the core step in the operation of Bayesian Policy Reuse.\nWe present the general form of Bayesian Policy Reuse (BPR) in Algorithm 1. The policy selection step (line 3) is described in detail in Section 3, the models of observation signals (line 5) are described in Section 2.7, and the belief update (line 6) is discussed further in Section 2.9.\nAlgorithm 1 Bayesian Policy Reuse (BPR) Require: Problem space X , Policy library \u03a0, observation space \u03a3, prior over the problem space P(X ), observation model P(\u03a3|X , \u03a0), performance model P(U |X , \u03a0), number of episodes K.\n1: Initialise beliefs: \u03b20(X )\u2190\u2212 P(X ). 2: for episodes t = 1 . . .K do 3: Select a policy \u03c0t \u2208 \u03a0 using the current belief \u03b2t\u22121 and the performance model P(U |X , \u03c0t). 4: Apply \u03c0t on the task instance. 5: Obtain an observation signal \u03c3t from the environment. 6: Update the belief \u03b2t(X ) \u221d P(\u03c3t|X , \u03c0t)\u03b2t\u22121(X ). 7: end for\n2.4 Regret\nIn order to evaluate the performance of our approach, we define regret as the criterion for policy selection to be optimised by Bayesian Policy Reuse.\nDefinition 3 (Library Regret) For a library of policies \u03a0 and for a policy selection algorithm \u03be : X \u2032 \u2192 \u03a0 that selects a policy for the new task instance x\u2217 \u2208 X \u2032, the library regret of \u03be is defined as\nR\u03a0(\u03be, x\u2217) = U\u03c0 \u2217 x\u2217 \u2212 U \u03be(x\u2217) x\u2217 ,\nwhere U\u03c0x is the utility of policy \u03c0 when applied to task x, and \u03c0 \u2217 = arg max\u03c0\u2208\u03a0 U \u03c0 x\u2217 , is the best policy in hindsight in the library for the task instance x\u2217.\nDefinition 4 (Average Library Regret) For a library of policies \u03a0 and for a policy selection algorithm \u03be : X \u2032 \u2192 \u03a0, the average library regret of \u03be over K trials is defined as the average of the library regrets for the individual trials,\nRK\u03a0 (\u03be) = 1\nK K\u2211 t=1 R\u03a0(\u03be, xt),\nfor a sequence of task instances x1, x2, . . . , xK \u2208 X \u2032.\nThe metric we minimise in BPR is the average library regret RK\u03a0 (.) for K trials. That is, the goal of BPR is to not only find the right solution at the end of the K trials, possibly through expensive exploration, but also to optimise performance even when exploring in the small number of trials of the task. We will refer to this metric simply as \u2018regret\u2019 throughout the rest of the paper.\n2.5 Types\nWhen the problem space of BPR is a large task space M, modelling the true belief distribution over the complete space would typically require a large number of samples (point estimates), which would hence be expensive to maintain and use computationally. In many applications, there is a natural notion of clustering inM whereby many tasks, modelled as MDPs, are similar with only minor variations in transition dynamics or reward structures. In the context of MDPs, previous work has regarded classes of MDPs as probability distributions over task parameters (Wilson et al., 2007). A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al. (2012); Maillard and Mannor (2014). In this work we do not explicitly investigate methods of task clustering, but the algorithms presented herein are most efficient when such a cluster-based structure exists in the task space.\nWe encode the concept of task clustering by introducing a notion of task types as -balls in the space of tasks, where the tasks are clustered with respect to the performance of a collection of policies executed on them.\nDefinition 5 (Type) A type \u03c4 is a subset of tasks such that for any two tasks \u00b5i, \u00b5j from a single type \u03c4 , and for all policies \u03c0 in a set of policies \u03a0, the difference in utility is upper-bounded by some \u2208 R:\n\u00b5i, \u00b5j \u2208 \u03c4 \u21d4 |U\u03c0i \u2212 U \u03c0 j | \u2264 , \u2200\u03c0 \u2208 \u03a0,\nwhere U\u03c0i \u2208 R is the utility from executing policy \u03c0 on task \u00b5i. Then, \u00b5i and \u00b5j are -equivalent under the policies \u03a0.\nThis definition of types is similar to the concept of similarity-based contextual bandits (Slivkins, 2014), where a distance function can be defined in the joint space of contexts and arms given by an upper bound of reward differences. In our setting, we cluster the instances (contexts) that are less than -different under all the policies in the library (arms). We do not however assume any prior knowledge of the metrics in the task or policy spaces.\nFigure 1 depicted four example types, where each accounts for an -ball in performance space (only explicitly shown for \u03c41). Note that the definition does not assume that the types need to be disjoint, i.e. there may exist tasks that belong to multiple types. We denote the space of types with T .\nIn the case of disjoint types, the type space T can be used as the problem space of BPR, inducing a hierarchical structure in the space M. The environment can then be represented with the generative model shown in Figure 2(a) where a type\n\u03c4 is drawn from a hyperprior \u03c4 \u223c G0, and then a task is drawn from that type \u00b5 \u223c \u2206\u03c4 (\u00b5), where \u2206\u03c4 (.) is some probability distribution over the tasks of type \u03c4 .\nBy definition, the set of MDPs generated by a single type are -equivalent under \u03a0, hence BPR regret cannot be more than if we represent all the MDPs in \u03c4 with any one of them. Let that chosen MDP be a landmark MDP of type \u03c4 , and denote this by \u00b5\u03c4 . This reduces the hierarchical structure into the simpler model shown in Figure 2(b), where the prior acts immediately on a set of landmark MDPs \u00b5\u03c4 , \u03c4 \u2208 T . The benefit of this for BPR is that each alternative is representative for a region in the original task space, as defined by a maximum loss of . Maintaining only this reduced set of landmarks removes near-duplicate tasks from consideration, thereby reducing the cost of maintaining the belief.\nFor the remainder of this paper, we use the type space T as the problem space, although we note that the methods proposed herein do not prevent the alternative use of the full task space M.\n2.6 Performance Model\nOne of the key components of BPR is the performance model of policies on previously-solved task instances, which describes the distribution of returns from each policy on the previously-solved tasks. A performance model represents the variability in return under the various tasks in a type.\nDefinition 6 (Performance Model) For a policy \u03c0 and a type \u03c4 , the performance model P(U |\u03c4, \u03c0) is a probability distribution over the utility of \u03c0 when applied to all tasks \u00b5 \u2208 \u03c4 .\nFigure 1 depicts the performance profile for \u03c03 on type \u03c41 in the form of a Gaussian distribution. Recall that for a single type, and each policy, the domain of the performance model would be at most of size . The agent maintains performance models for all the policies it has in the library \u03a0 and for all the types it has experienced.\n2.7 Observation Model\nIn order to facilitate the task identification process, the agent learns a model of how types, policies and signals relate during the offline phase.\nDefinition 7 (Observation Model) For a policy \u03c0 and type \u03c4 and for a choice of signal space \u03a3, the observation model F\u03c4\u03c0 (\u03c3) = P(\u03c3|\u03c4, \u03c0) is a probability distribution over the signals \u03c3 \u2208 \u03a3 that may result by applying the policy \u03c0 to the type \u03c4 .\nWe consider the following offline procedure to learn the signal models for a policy library \u03a0:\n1. The type label \u03c4 is announced. 2. A set of tasks are generated from the type \u03c4 . 3. The agent runs all the policies from the library \u03a0 on all the instances of \u03c4 ,\nand observes the resultant sampled signals \u03c3\u0303 \u2208 \u03a3. 4. Empirical distributions F\u03c4\u03c0 = \u2206(\u03c3\u0303) are fitted to the data, for each type \u03c4 and\npolicy \u03c0.\nThe benefit of these models is that they provide a connection between the observable online information and the latent type label, the identification of which leads to better reuse from the policy library.\n2.8 Candidate Signals for Observation Models\nThe BPR algorithm requires that some signal information is generated from policy execution on a task, although the form of this signal remains unspecified. Here we describe the three most typical examples of information that can be used as signals in BPR, but note that this list is not exhaustive."}, {"heading": "2.8.1 State-Action-State Tuples", "text": "The most detailed information signal which could be accrued by the agent is the history of all (s, a, s\u2032) tuples encountered during the execution of a policy. Thus, the observation model in this case is an empirical estimate of the expected transition function of the MDPs under the type \u03c4 .\nThe expressiveness of this signal does have a drawback, in that it is expensive to learn and maintain these models for every possible type. Additionally, this may not generalise well, in cases with sparse sampling. On the other hand, this form of signal is useful in cases where some environmental factors may affect the behaviour of the agent in a way that does not directly relate to attaining an episodic goal. As an example, consider an aerial agent which may employ different navigation strategies under different wind conditions."}, {"heading": "2.8.2 Instantaneous Rewards", "text": "Another form of information is the instantaneous reward r \u2208 R received during the execution of a policy for some state-action pair. Then, the observation model is an empirical estimate of the expected reward function for the MDPs in the type.\nAlthough this is a more abstract signal than the state-action-state tuples, it may still provide a relatively fine-grained knowledge on the behaviour of the task when intermediate rewards are informative. It is likely to be useful in scenarios where the task has a number of subcomponents which individually contribute to overall performance, for example in assembly tasks."}, {"heading": "2.8.3 Episodic Returns", "text": "An example of a sparser kind of signal is the total utility U\u03c0\u03c4 \u2208 R accrued over the full episode of using a policy in a task. The observation model of such a scalar signal is much more compact, and thereby easier to learn and reason with, than the previous two proposals. We also note that for our envisioned applications, the execution of a policy cannot be terminated prematurely, meaning that an episodic return signal is always available to the agent before selecting a new policy.\nThis signal is useful for problems of delayed reward, where intermediate states cannot be valued easily, but the extent to which the task was successfully completed defines the return. In our framework, using episodic returns as signals has the additional advantage that this information is already captured in the performance model, which relieves the agent from maintaining two separate models, as in this case P(U |\u03c4, \u03c0) = F\u03c4\u03c0 (U) for all \u03c0 and \u03c4 .\n2.9 Belief over Types\nDefinition 8 (Type Belief) For a set of previously-solved types T and a new instance x\u2217, the Type Belief \u03b2(.) is a probability distribution over T that measures the extent to which x\u2217 matches the types of T in their observation signals.\nThe type belief, or belief for short, is a surrogate measure of similarity in type space. It approximates where a new instance may be located in relation to the known types which act as a basis of the unknown type space. The belief is initialised with the prior probability over the types, labelled G0 in Figure 2.\nIn episode t, the environment provides an observation signal \u03c3t for executing a policy \u03c0t on the new task instance. This signal is used to update \u03b2 (line 6 in Algorithm 1). The posterior over the task space is computed using Bayes\u2019 rule:\n\u03b2t(\u03c4) = P(\u03c3t|\u03c4, \u03c0t)\u03b2t\u22121(\u03c4)\u2211\n\u03c4 \u2032\u2208T P(\u03c3 t|\u03c4 \u2032, \u03c0t)\u03b2t\u22121(\u03c4 \u2032)\n(1)\n= \u03b7F\u03c4\u03c0t(\u03c3 t)\u03b2t\u22121(\u03c4), \u2200\u03c4 \u2208 T , (2)\nwhere \u03b2t\u22121 is the belief after episode t\u2212 1 and \u03b7 is a normalisation constant. We use \u03b2 to refer to \u03b2t whenever this is not ambiguous. Note how the belief is updated using the observation model."}, {"heading": "3 Policy Selection for BPR", "text": "The selection of a policy for each episode (line 3 in Algorithm 1) is a critical step in BPR. Given the current type belief, the agent is required to choose a policy\nfor the next episode to fulfil two concurrent purposes: acquire useful information about the new (current) task instance, and at the same time avoid accumulating additional regret.\nAt the core of this policy selection problem is the trade-off between exploration and exploitation. When a policy is executed at some time t, the agent receives both some utility as well as information about the true type of the new task (the signal). The agent is required to gain as much information about the task as possible, so as to choose policies optimally in the future, but at the same time minimise performance losses arising from sub-optimal policy choices2.\nOur problem can be mapped to a finite-horizon total-reward multi-armed bandits setting in which the arms represent the policies, the finite horizon is defined by the limited number of episodes, and the metric to optimise is the total reward. For this kind of setting Lai and Robbins (1985) show that index-based methods achieve optimal performance asymptotically. In our case, however, we are interested in the cumulative performance over a small number of episodes.\nClearly, a purely greedy policy selection mechanism would fail to choose exploratory options to elicit what is needed for the belief to converge to the closest type, and may result in the agent becoming trapped in a local maximum of the utility function. On the other hand, a purely exploratory policy selection mechanism could be designed to ensure that all the possible information is elicited in expectation, but this would not make an effort to improve performance instantly and thereby incur additional regret. We thus require a mechanism to explore as well as exploit; find a better policy to maximise asymptotic utility, and exploit the current estimates of which are good policies to maximise myopic utility.\nMultiple proposals have been widely considered in the multi-armed bandits (MAB) literature for these heuristics, ranging from early examples like the Gittins index for infinite horizon problems (Gittins and Jones, 1974) to more recent methods such as the knowledge gradient (Powell, 2010). Here we describe several approximate policy selection mechanisms that we use for dealing with the policy reuse problem.\n\u2013 A first approach is through -greedy exploration, where with probability 1 \u2212 we select the policy which maximises the expected utility under the belief \u03b2,\n\u03c0\u0302 = arg max \u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4) \u222b U\u2208R U P(U |\u03c4, \u03c0)dU\n= arg max \u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4) E[U |\u03c4, \u03c0],\nand with probability we select a policy from the policy library uniformly at random. This additional random exploration component perturbs the belief from local minima. \u2013 A second approach is through sampling the belief \u03b2. This involves sampling a type according to its probability in the belief \u03c4\u0302 \u223c \u03b2, and playing the best response to that type from the policy library,\n\u03c0\u0302 = arg max \u03c0\u2208\u03a0\nE[U |\u03c4\u0302 , \u03c0].\n2 Note that we denote by optimal policy the best policy in the library for a specific instance, as we are considering policy reuse problems in which learning the actual optimal policy is not feasible.\nIn this case, the sampled type acts as an approximation of the true unknown type, and exploration is achieved through the sampling process. \u2013 The third approach is through employing what we call exploration heuristics, which are functions that estimate a value for each policy which measures the extent to which it balances exploitation with a limited degree of look-ahead for exploration. This is the prevalent approach in Bayesian optimisation, where, instead of directly maximising the objective function itself (here, utility), a surrogate function that takes into account both the expected utility and a notion of the utility variance (uncertainty) is maximised (see, e.g., Brochu et al. (2010)). independent of other policies.\n3.1 Bayesian Policy Reuse with Exploration Heuristics\nBy incorporating the notion of an exploration heuristic that computes an index \u03bd\u03c0 for a policy \u03c0 into Algorithm 1, we obtain the proto-algorithm Bayesian Policy Reuse with Exploration Heuristics (BPR-EH) described in Algorithm 2.\nAlgorithm 2 Bayesian Policy Reuse with Exploration Heuristics (BPR-EH) Require: Type space T , Policy library \u03a0, observation space \u03a3, prior over the type space G0, observation model P(\u03a3|T , \u03a0), performance model P(U |T , \u03a0), number of episodes K, exploration heuristic V.\n1: Initialise beliefs: \u03b20 \u2190\u2212 G0. 2: for episodes t = 1 . . .K do 3: Compute \u03bd\u03c0 = V(\u03c0, \u03b2t\u22121) for all \u03c0 \u2208 \u03a0. 4: \u03c0t \u2190\u2212 arg max\u03c0\u2208\u03a0 \u03bd\u03c0 . 5: Apply \u03c0t to the task instance. 6: Obtain the observation signal \u03c3t from the environment. 7: Update the belief \u03b2t using \u03c3t by Equation (1). 8: end for\nNote that we are now using G0, the hyper-prior, as the prior in line 1 because we are using T as the problem space. We now define the exploration heuristics V that are used in line 3, and to this end we define four variants of the BPR-EH algorithm, as\n\u2013 BPR-PI using probability of improvement (Section 3.1.1), \u2013 BPR-EI using expected improvement (Section 3.1.1), \u2013 BPR-BE using belief entropy (Section 3.1.2), and \u2013 BPR-KG using knowledge gradient (Section 3.1.3)."}, {"heading": "3.1.1 Probability of Improvement and Expected Improvement", "text": "The first heuristic for policy selection utilises the probability with which a specific policy can achieve a hypothesised increase in performance. Assume that U+ \u2208 R is some utility which is larger than the best estimate under the current belief, U+ > U\u0304 = max\u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4)E[U |\u03c4, \u03c0]. The probability of improvement (PI) principle\nchooses the policy that maximises the term,\n\u03c0\u0302 = arg max \u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4)P(U+|\u03c4, \u03c0),\nthereby selecting the policy most likely to achieve the utility U+. The choice of U+ is not straightforward, and this choice is the primary factor affecting the performance of this exploration principle. One approach to addressing this choice, is through the related idea of expected improvement (EI). This exploration heuristic integrates over all the possible values of improvement U\u0304 < U+ < Umax, and the policy is chosen with respect to the best potential. That is,\n\u03c0\u0302 = arg max \u03c0\u2208\u03a0 \u222b Umax U\u0304 \u2211 \u03c4\u2208T \u03b2(\u03c4)P(U+|\u03c4, \u03c0)dU+\n= arg max \u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4) \u222b Umax U\u0304 P(U+|\u03c4, \u03c0)dU+\n= arg max \u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4)(1\u2212 F(U\u0304 |\u03c4, \u03c0))\n= arg min \u03c0\u2208\u03a0 \u2211 \u03c4\u2208T \u03b2(\u03c4)F(U\u0304 |\u03c4, \u03c0),\nwhere F(U |\u03c4, \u03c0) = \u222b U \u2212\u221e P(u|\u03c4, \u03c0)du is the cumulative distribution function of U for \u03c0 and \u03c4 . This heuristic therefore selects the policy most likely to result in any improvement to the expected utility."}, {"heading": "3.1.2 Belief Entropy", "text": "Both PI and EI principles select a policy which has the potential to achieve higher utility. An alternate approach is to select the policy which will have the greatest effect in reducing the uncertainty over the type space.\nThe belief entropy (BE) exploration heuristic seeks to estimate the effect of each policy in reducing uncertainty over type space, represented by the entropy of the belief. For each policy \u03c0 \u2208 \u03a0, estimate the expected entropy of the belief after executing \u03c0 as\nH(\u03b2|\u03c0) = \u2212\u03b2\u03c0 log \u03b2\u03c0,\nwhere \u03b2\u03c0 is the updated belief after seeing the signal expected from running \u03c0, given as\n\u03b2\u03c0(\u03c4) = E\u03c3\u2208\u03a3 [\u03b7F\u03c4\u03c0 (\u03c3)\u03b2(\u03c4)] (3)\n= \u222b \u03c3\u2208\u03a3 F\u03b2\u03c0 (\u03c3) [\u03b7F\u03c4\u03c0 (\u03c3)\u03b2(\u03c4)] d\u03c3, (4)\nwhere F\u03b2\u03c0 (\u03c3) is the probability of observing \u03c3 under the current belief \u03b2 when using \u03c0, and \u03b7 is the normalisation constant as before.\nThen, selecting the policy\n\u03c0\u0302 = arg min \u03c0\u2208\u03a0\nH(\u03b2|\u03c0)\nreduces the most uncertainty in the belief in expectation. This is however a purely exploratory policy. To incorporate exploitation of the current state of knowledge, we rather select\n\u03c0\u0302 = arg max \u03c0\u2208\u03a0\n( U\u0303(\u03c0)\u2212 \u03baH(\u03b2|\u03c0) ) ,\nwhere \u03ba \u2208 R is a positive constant controlling the exploration-exploitation tradeoff, and U\u0303(\u03c0) is the expected utility of \u03c0 under the current belief,\nU\u0303(\u03c0) = \u2211 \u03c4\u2208T \u03b2(\u03c4)E[U |\u03c0, \u03c4 ]. (5)"}, {"heading": "3.1.3 Knowledge Gradient", "text": "The final exploration heuristic we describe is the knowledge gradient (Powell, 2010), which aims to balance exploration and exploitation through optimising myopic return whilst maintaining asymptotic optimality. The principle behind this approach is to estimate a one step look-ahead, and select the policy which maximises utility over both the current time step and the next in terms of the information gained.\nTo select a policy using the knowledge gradient, we choose the policy which maximises the online knowledge gradient at time t\n\u03c0\u0302 = arg max \u03c0\u2208\u03a0\n( U\u0303(\u03c0) + (K \u2212 t)\u03bdt\u03c0 ) ,\ntrading-off between the expected utility U\u0303(\u03c0), given in Equation 5, and \u03bdt\u03c0, the offline knowledge gradient of \u03c0 for a horizon of K trials, weighted by the remaining number of trials. The offline knowledge gradient essentially measures the performance of a one-step look-ahead in the process, given as\n\u03bdt\u03c0 = E\u03b2 [ max \u03c0\u2032 U\u0303\u03c0(\u03c0\u2032)\u2212max \u03c0\u2032\u2032 U\u0303(\u03c0\u2032\u2032) ] , (6)\nwhere U\u0303(\u03c0) is the expected utility of \u03c0 under the current belief (Equation 5),\nU\u0303\u03c0(\u03c0\u2032) = \u2211 \u03c4\u2208T \u03b2\u03c0(\u03c4) E[U |\u03c4, \u03c0\u2032], (7)\nand \u03b2\u03c0 is the expected updated belief after playing policy \u03c0 and receiving a suitable signal, as defined in Equation 4. That is, the offline knowledge gradient is the difference in expectation, with respect to \u03b2, of the best performance of any policy at t+ 1 if \u03c0 was played at t, compared to that of the best policy at t (which may be different from \u03c0)."}, {"heading": "4 Experiments", "text": "4.1 Golf Club Selection\nAs an initial, illustrative simulated experiment we consider the problem of a robot golfer taking a shot with one of four golf clubs on an unknown golf course, where it is not possible to reliably estimate the distance to the hole, as for this example we are considering a robot with weak sensors that are not in themselves sufficient to reliably measure distance. The robot is only allowed to take K = 3 shots, which is less than the number of available clubs, from a fixed position from the hole. The task is evaluated by the stopping distance of the ball to the hole. The robot can choose any of the available clubs, and we assume that the robot uses a fixed, canonical stroke with each club.\nIn this setting, we consider the type space T to be a set of different golfing experiences the robot had before, each defined for simplicity by how far the target was (other factors, e.g. weather conditions, could be factored into this as well). The performance of a club for some hole is defined as the negative of the absolute distance of the end location of the ball from the hole, such that this quantity must be maximised.\nThen, the choice of a club corresponds to a choice of a policy. For each, the robot has a performance profile (distribution over final distance of the ball from the hole) for the different courses that the robot experienced. We assume a small selection of four clubs, with properties shown in Table 1 for the robot canonical stroke. The distances shown in this table are the ground truth values, and are not explicitly known to the robot.\nOwing to the difficulty of the outdoor perception problem over large distances, the robot cannot measure exact distances in the field, but for a feedback signal, it can crudely estimate a qualitative description of the result of a shot as falling into one of several broad categories (corresponding to concepts such as quite near and very far), which define the observation space, as shown in Figure 3.\nNote that this is not the performance itself, but a weaker observation correlated with performance. The distributions over these qualitative categories (the observation models) are known to the robot for each club on each of the training types it has encountered. For this example, we assume the robot has extensive training on four particular holes, with distances \u03c4110 = 110yds, \u03c4150 = 150yds, \u03c4170 = 170yds and \u03c4220 = 220yds. The observation models are shown in Figure 4.\nWhen the robot faces a new hole, BPR allows the robot to overcome its inability to judge the distance to the hole by using the feedback from an arbitrary shot as a signal. The feedback signal updates an estimate of the most similar previous task (the belief), using the distributions in Figure 4. This belief enables the robot to choose the club/clubs which would have been the best choice for the most similar previous task/tasks.\nFor a worked example, consider a hole 179 yards away. If a coarse estimate of the distance is feasible, it can be incorporated as a prior over T . Otherwise, an uniformed prior is used. Assume the robot is using greedy policy selection, and assume that it selects \u03c01 for the first shot due to a uniform prior, and that this resulted in an over-shot by 35 yards. The robot cannot gauge this error more accurately than that it falls into the category corresponding to \u2018over-shooting in the range of 20 to 50 yards\u2019. This signal will update the belief of the robot over the four types, and by Figure 4, the closest type to produce such a behaviour would\nbe \u03c4170 = 170 yards. The new belief dictates that the best club to use for anything like \u03c4170 is \u03c02. Using \u03c02, the hole is over-shot by 13 yards, corresponding to the category with the \u2018range 5 to 20 yards\u2019. Using the same calculation, the most similar previous type is again \u03c4170, keeping the best club as \u03c02, and allowing belief to converge. Indeed, given the ground truth in Table 1, this is the best choice for the 179 yard task. Table 2 describes this process over the course of 8 consecutive shots taken by the robot.\nFigure 5 shows the performance of BPR with greedy policy selection in the golf club selection task averaged over 100 unknown golf course holes, with ranges randomly selected between 120 and 220 yards. This shows that on average, by the second shot, the robot will have selected a club capable of bringing the ball within 10\u201315 yards of the hole.\n4.2 Online Personalisation\nIn this next experiment, we demonstrate the use of different observation signals to update beliefs, as described in Section 2.8.\nConsider an automated phone service of a bank, where the bank tries to improve the speed of answering telephonic queries using human operators by a personalised model for understanding the speech of the user and responding with a synthesised voice personalised according to that user\u2019s preferences. In a traditional speech recognition system, the user may have time to train the system to her own voice, but this is not possible in this scenario. As a result, the phone service may have a number of different pre-trained language models and responses, and over the course of many interactions with the same user, tries to estimate the best such model to use.\nLet a user i be defined by a preference model of language, \u03bbi \u2208 {1, . . . , L}, where L is the number of such models. The policy \u03c0 executed by the telephonic agent also corresponds to a choice of language model, i.e. \u03c0 \u2208 {1, . . . , L}. The goal of the agent is to identify the user preference \u03bbi, whilst minimising frustration to the user.\nAssume that each telephonic interaction proceeds using the transition system through the six states given in Figure 6. In every state, there is only one action which can be taken by the system, being to use the chosen language model. At the beginning of the call, the user is in the start state. We assume the system can identify the user by caller ID, and selects a language model. If, at any point, the system can deal with the user\u2019s request, the call ends successfully. If not, we assume the user becomes gradually more irritated with the system, passing through states frustrated and annoyed. If the user reaches state angry and still has an unresolved request, she is transferred to a human operator. This counts as an unsuccessful interaction. Alternatively, at any point the user may hang up the call, which also terminates the interaction unsuccessfully.\nThe transition dynamics of this problem depend on a parameter \u03c1 = 1\u2212 |\u03c0\u2212\u03bbi|L which describes how well the selected language model \u03c0 can be understood by a user of type \u03bbi, such that \u03c1 = 1 if the chosen model matches the user\u2019s, and it is 0 if it is the worst possible choice. An additional parameter \u03b7 governs the tradeoff between the user becoming gradually more frustrated and simply hanging up when the system does not respond as expected. In our experiments, we fix \u03b7 = 0.3, except when \u03c0 = \u03bbi where we set \u03b7 = 0.\nTo allow the use of different observation signals in this example the domain was designed in a way such that the transition dynamics and the rewards of this domain, shown in Figure 6, allow only two total utility values for any instance: U = 10 for a successful completion of the task, and U = \u22123 otherwise. Similarly, any state that transitions to the unsuccessful outcome angry receives the same reward for a transition to the unsuccessful outcome hang up. Finally, all transition probabilities between the states start, frustrated, annoyed, and angry are independent of \u03c1, and thus, of the type. This set up mirrors the fact that in general the state sequence given by the signal (s, a, s\u2032) is more informative than the reward\nsequence (s, a, r), which is in turn more informative than the total utility signal U alternative 3.\nThe results shown in Figure 7 were generated from 1,000 call interactions which proceeded according to the model in Figure 6. In this experiment, the correct language model for each user was randomly drawn from a set of 20 language models. Figure 7 shows comparative performance of BPR with sampling the belief selection mechanism when the three kinds of signals are used. As expected, the lowest regret (and variance in regret) is achieved using the most-informative (s, a, s\u2032) signal, followed by the (s, a, r) signal, and finally the total performance signal U . We do note, however, that all three signals eventually converge to zero regret if given enough time.\n4.3 Surveillance Domain\nThe surveillance domain models the monitoring problem laid out in the introduction. Assume a base station is tasked with monitoring a wildlife reserve spread out over some large geographical region. The reserve suffers from poaching and so the base station is required to detect and respond to poachers on the ground. The base station has a fixed location, and so it monitors the region by deploying a low-flying, light-weight autonomous drone to complete particular surveillance tasks using different strategies. The episodic commands issued by the base station may be to deploy to a specific location, scan for unusual activity in the targeted area and then report back. After completing each episode, the drone communicates\n3 We note that in many applications U might be the only of these signals available to the agent. For example, in the current scenario, it may not be easy or feasible to accurately gauge the frustration of the caller, making the states and the immediate rewards unobservable.\nwith the base some information of whether or not there was any suspicious activity in the designated region. The base station is required to use that information to better decide on the next strategy for the drone.\nConcretely, we consider a 26\u00d7 26 cell grid world, which represents the wildlife reserve, and the base station is situated at a fixed location in one corner. We assume that there are 68 target locations of interest, being areas with a particularly high concentration of wildlife. These areas are arranged around four \u2018hills\u2019, the tops of which provide better vantage points. Figure 8 depicts this setting.\nAt each episode, the base station deploys the drone to one of the 68 locations. The interpretation of this in BPR is that these 68 target locations each correspond to a different poacher type, or task. For each type, we assume that there is a pre-\nlearnt policy for reaching and surveying that area while dealing with local wind perturbations and avoiding obstacles such as trees.\nThe observation signal that the base station receives after each drone deployment is noise-corrupted information related to the success in identifying an intruder at the target location or somewhere nearby (in a diagonally adjacent cell). One exception is when surveying the hill centres which, by corresponding to a high vantage point, provide a weak signal stating that the intruder is in the larger area around the hill. For a distance d between the region surveyed and the region occupied by the poachers, the signal R received by the agent is\nR \u2190\u2212  200\u2212 30d+ \u03c8 if agent surveys a hilltop and d \u2264 15 200\u2212 20d+ \u03c8 if agent surveys any another location and d \u2264 3 \u03c8 otherwise,\nwhere \u03c8 \u223c N(10, 20) is Gaussian noise. A higher signal indicates more confidence in having observed a poacher in the region surrounding the target surveillance point.\nFigure 9 presents a comparison between six variants of the BPR algorithm. Four use the exploration heuristics proposed in Section 3, namely BPR-KG, BPRBE, BPR-PI, BPR-EI, in addition to sampling the belief \u03b2, and -greedy selection with = 0.3. These six variants were run on the domain and averaged over 10 random tasks, with standard deviations of the regret shown in Table 3.\nNote in Figure 9(a) that BPR-BE, BPR-KG, and BPR with sampling the belief all converge in about 15 episodes, which is approximately a quarter the number that would be required by a brute force strategy which involved testing every policy in turn. Both BPR-PI and BPR with -greedy selection fail to converge within the allotted 50 episodes. BPR-EI shows the most rapid convergence.\nWe now compare the performance of BPR to other approaches from the literature. We choose two frameworks, multi-armed bandits for which we use UCB1 (Auer et al., 2002), and Bayesian optimisation where we use GP-UCB (Srinivas et al., 2009). We note upfront that although these frameworks share many elements with our own framework in terms of the problems they solve, the assumptions they place on the problem space are different, and thus so is the information they use.\nThe results of comparing performance of these approaches are presented in Figure 10 on the surveillance domain, averaged over 50 tasks. We use BPR-EI in this experiment as it was the best performing BPR variant as seen in Figure 9.\nFor UCB1, we treat each existing policy in the library as a different arm of the bandit. \u2018Pulling\u2019 an arm corresponds to executing that policy, and the appropriate\nreward is obtained. We additionally provide UCB1 with a prior in the form of expected performance of each policy given the task distribution G0(\u03c4), which we assumed to be uniform in this case. This alleviates UCB1 from having to test each arm first on the new task (which would require 68 episodes) before it can begin the decision making process. It is still slower to converge than BPR, as information from each episode only allows UCB1 to update the performance estimate of a single policy, whereas BPR can make global updates over the policy space.\nOn the other hand, an optimisation-based approach such as GP-UCB is better suited to this problem, as it operates with the same requirement as BPR of maintaining low sample complexity. This algorithm treats the set of policies as an input space, and is required to select the point in this space which achieves the best performance on the current task. However, unlike BPR, this approach requires a metric in policy space. This information is not known in this problem, but we approximate this from performance in the training tasks. As a result of this approximation, sampling a single point in GP-UCB (corresponding to executing a policy) again only provides information about a local neighbourhood in policy space, whereas selecting the same action would allow BPR to update beliefs over the entire task space.\nFurther discussion of the differences between BPR and both bandits and optimisation approaches is provided in Sections 5.2 and 5.3.1 respectively.\nFinally, we explore the trade-off between library size and sample complexity with respect to the regret of BPR-EI, BPR-PI, BPR-BE, and BPR-KG. This is shown in Figure 11 where, for each method, the horizontal axis shows the ratio of the library size to the full task space size, the vertical axis shows the number of episodes allowed for each new instance, and regret is represented by colour. For each combination of a library size and a sample complexity, we average the regret results over 200 trials. In each of these trials, a random subset of the full task space is used as the offline policy library and the online task is drawn from the full task space. That is, tasks in the online phase include both previously-solved and new tasks.\nAs can be seen from the figure, regret can be decreased by either increasing the library size or the time allocated (in terms of number of episodes) to complete the new task. Usually, the task specification dictates the maximum allowed number of episodes, and hence, this suggests a suitable library size to be acquired in the offline phase to attain a specific regret rate. This figure also confirms the previous findings that BPR-EI is able to exceed the other variants in terms of performance.\n5 Discussion and Related Work\n5.1 Transfer Learning\nThe optimal selection from a set of provided policies for a new task is in essence a transfer learning problem (see the detailed review by Taylor and Stone (2009)). Specifically, Bayesian Policy Reuse aims to select a policy in a library \u03a0 for transferring to a new, initially unknown, instance. The criterion for this choice is that it is the best policy for the type most similar to the type of the new instance. One transfer approach that considers the similarity between source and target tasks is by Lazaric (2008), where generated (s, a, r, s\u2032) samples from the target task are\nused to estimate similarity to source tasks, which is measured by the average probability of the generated transitions happening under the source task. Then, samples from the more similar source tasks are used to seed the learning of the target task, while less similar tasks are avoided, escaping negative transfer. More recently, Brunskill and Li (2013) consider using the (s, a, r, s\u2032) similarity to compute confidence intervals of where, in a collection of MDP classes, a new instance best fits. The classes are acquired from experience by clumping together MDPs that do not differ in their transition dynamics or rewards more than a certain level. Once the class is determined, the previous knowledge of that class, in form of dynamics are rewards, is borrowed to inform the process of planning. Bayesian Policy Reuse does not assume learning is feasible, but relies on transferring a useful policy immediately. Also, we use a Bayesian measure of task similarity which allows exploiting prior knowledge of the task space, quickly incorporating observed signals for a faster response, and also, by maintaining beliefs, keeping open the possibility of new MDPs that do not cleanly fit in any of the discovered classes.\n5.2 Correlated and Contextual Bandits\nUsing a one-off signal per episode relates BPR to Correlated Bandits. In this setting, the decision-making agent is required to pull an arm from a collection of arms, and use its return to update estimates of the arm values of not only the arm that was pulled as in traditional bandits, but of a larger subset of all the\narms. In our problem setting, the arms correspond to policies, and the new task instance corresponds to the new bandit \u2018machine\u2019 that generates utilities per arm pull (policy execution).\nIn the Correlated Bandits literature, the form of correlation between the arms is known to the agent. Usually, this happens to be the functional form of the reward curve. The agent\u2019s task is then to identify the parameters of that curve, so that the hypothesis of the best arm moves in the reward curve\u2019s parameter space. For example, in Response Surface Bandits (Ginebra and Clayton, 1995), there is a known prior over the parameters of the reward curve and the metric on the policy space is known. More recently, Mersereau et al. (2009) present a greedy policy which takes advantage of the correlation between the arms in their reward functions, assuming a linear form with one parameter, with a known prior. In our work, we approach a space of tasks from a sampling point of view, where an agent experiences sample tasks and uses these to build the models of the domain. Thus we do not assume any functional form for the response surface, and we do not require the metric on the policy space to be known.\nIn our framework, we only assert assumptions on the continuity and smoothness of the surface. We treat the known types as a set of learnt bandit machines with known behaviour for each of the different arms. These behaviours define local \u2018kernels\u2019 on the response surface, which we then approximate by a sparse kernel machine. We track a hypothesis of the best arm using that space. This is to some extent similar to the Gaussian process framework, but in our case the lack of a metric on the policy space prevents the definition of the covariance functions needed there. This point is elaborated further in Section 5.3.1.\nIn another thread, Dependent Bandits (Pandey et al., 2007) assume that the arms in a multi-armed bandit can be clustered into different groups, such that the members of each have correlated reward distribution parameters. Then, each cluster is represented with one representative arm, and the algorithm proceeds in two steps: a cluster is first chosen by a variant of UCB1 (Auer et al., 2002) applied to the set of representative arms, and then the same method is used again to choose between the arms of the chosen cluster. We assume in our work that the set of previously-solved tasks span and represent the space well, but we do not dwell on how this set of tasks can be selected. Clustering is one good candidate for that, and one particular example of identifying the important types in a task space can be seen in the work of Mahmud et al. (2013).\nIn Contextual Bandits (Strehl et al., 2006; Langford and Zhang, 2008), the agent is able to observe side information (or context labels) that are related to the nature of the bandit machine, and the question becomes one of selecting the best arm for each possible context. Mapping this setting to our problem, a context represents the type, whereas the arms represent the policies. The difference is that in our case the context information (the type label) is latent, and the space of types is not fully known, meaning that the construction of a bounded set of hypotheses of policy correlation under types is not possible. In addition, our setting has that the response of the arms to contexts is only captured through limited offline sampling, but the agent is able to engage with the same context for multiple rounds.\nAnother related treatment is that of latent bandits (Maillard and Mannor, 2014) where, in the single-cluster arrival case, the experienced bandit machine is drawn from a single cluster with known reward distributions, and in the agnostic case the instances are drawn from many unknown clusters with unknown reward\ndistributions. Our setting fits in between these two extremes, as the instances are drawn from a single, but unknown, cluster with an unknown reward distribution.\n5.3 Relation to Bayesian Approaches"}, {"heading": "5.3.1 Bayesian Optimisation", "text": "If the problem of Bayesian Policy Reuse is treated as an instance of Bayesian optimisation, we consider the objective\n\u03c0\u2217 = arg max \u03c0\u2208\u03a0 E[U |x\u2217, \u03c0], (8)\nwhere x\u2217 \u2208 X is the unknown process with which the agent is interacting, and E[U |x\u2217, \u03c0] is the expected performance when playing \u03c0 on x\u2217. This optimisation involves a selection from a discrete set of alternative policies (\u03c0 \u2208 \u03a0), corresponding to sampling from the performance function at a discrete set of locations. However, sampling from this function is expensive (corresponding to executing a policy for an episode), and as a result the performance function must be optimised in as few samples as possible.\nA Bayesian optimisation solution requires the target function to be modelled as a Gaussian Process (GP). There are two issues here:\n1. Observations in BPR need not be the performance itself (see Section 2.8), while the GP model is appropriate only where these two are the same. 2. BPR does not assume knowledge of the metric in policy space. This is however required for Bayesian optimisation, so as to define a kernel function for the Gaussian process. An exception is in the case where policies all belong to a parametrised family of behaviours, placing the metric in parameter space as a proxy for policy space.\nStill, we assume smoothness and continuity of the response surface for similar tasks and policies, which is also a standard assumption in Gaussian process regression. Bayesian Policy Reuse uses a belief that tracks the most similar previouslysolved type, and then reuses the best policy for that type. This belief can be understood as the mixing coefficient in a mixture model that represents the response surface.\nTo see this, consider Figure 12 which shows an example 2D response surface. Each type is represented by a \u2018band\u2019 on that surface; a set of curves only precisely known (in terms of means and variances) at their intersections with a small collection of known policies. Projecting these intersections of some type into performance space results in a probabilistic description of the performance of the different policies on that type (the Gaussian processes in the figure), the kind of function that we are trying to optimise in Equation 8. Each of these projections would be a component of the mixture model that represents the response surface, and would be the type\u2019s contribution to it.\nAny new task instance corresponds to an unknown curve on the surface, and correspondingly to a probabilistic model in performance space. Given that the only knowledge possessed by the agent from the surface are these Gaussian processes for each known type, Bayesian Policy Reuse implicitly assumes that they act as a basis\nthat span the space of possible curves, so that the performance under any new task can be represented as a weighted average of the responses of the previously-solved types4. To this extent, the performance for the new task instance is approximately identified by a vector of weights, which in our treatment of BPR we refer to as the type belief. Thus, the BPR algorithm is one that fits a probabilistic model to an unknown performance curve (Equation 8) through sampling and weight adjusting in an approximate mixture of Gaussian processes."}, {"heading": "5.3.2 Bayesian Reinforcement Learning", "text": "Bayesian Reinforcement Learning (BRL) is a paradigm of Reinforcement Learning that handles the uncertainty in an unknown MDP in a Bayesian manner by maintaining a probability distribution over the space of possible MDPs, and updating that distribution using the observations generated from the MDP as the interaction continues (Dearden et al., 1999). In work by Wilson et al. (2007), the problem of Multi-task Reinforcement Learning of a possibly-infinite stream of MDPs is handled in a Bayesian framework. The authors model the MDP generative process using a hierarchical infinite mixture model, in which any MDP is assumed to be generated from one of a set of initially-unknown classes, and a hyper-prior controls the distribution of the classes.\nBayesian Policy Reuse can be regarded as an special instance of Bayesian Multitask Reinforcement Learning with the following construction. Assume a Markov Decision Process that has a chain of K identical states (representing the trials)\n4 Note that this will create a bias in the agent\u2019s estimated model of the type space toward the types that have been seen more often before. We assume that the environment is benign and that the offline phase is long enough to experience the necessary types.\nand a collection of viable actions that connect each state to the next in the chain. The set of actions is given by \u03a0, the policy library. The processes are parametrised with their type label \u03c4 . For each decision step, the agent takes an action (a policy \u03c0 \u2208 \u03a0) and the process returns with a performance signal, U\u03c0\u03c4 . The task of the agent is to infer the best \u2018policy\u2019 for this process (a permutation of K policies from \u03a0; \u03c00, . . . , \u03c0K\u22121) that achieves the fastest convergence of values U , and thus low convergence time and low regret. The performance/observation models act as the Bayesian prior over rewards required in Bayesian reinforcement learning."}, {"heading": "5.3.3 Other Bayesian Approaches", "text": "Engel and Ghavamzadeh (2007) introduce a Bayesian treatment to the Policy Gradient method in reinforcement learning. The gradient of some parametrised policy space is modelled as a Gaussian process, and paths sampled from the MDP (completed episodes) are used to compute the posteriors and to optimise the policy by moving in the direction of the performance gradient. The use of Gaussian processes in policy space is similar to the interpretation of our approach, but their use is to model the gradient rather than the performance itself.\nWhen no gradient information is available to guide the search, Wingate et al. (2011) propose to use MCMC to search in the space of policies which is endowed with a prior. Various kinds of hierarchical priors that can be used to bias the search are discussed. In our work, we choose the policies using exploration heuristics based on offline-acquired performance profiles rather than using kernels and policy priors. Furthermore, we have access only to a small set of policies to search through in order to optimise the time of response.\n5.4 Storage Complexity\nAs described in Section 2.8, the use of different signals entail different observation models and hence different storage complexities. Assume that |S| is the size of the state space, |A| is the size of the action space, |\u03a0| is the size of the policy library, N is the number of previously-solved types, |R| is the size of the reward space, T is the duration of an episode, and B is the number of bits needed to store one probability value. For the performance signal, the storage complexity of the observation model is upper bounded by SCU = |\u03a0|N |R|B for the average reward case, and SCU,\u03b3 = |\u03a0|N 1\u2212\u03b3 T\n1\u2212\u03b3 |R|B for the discounted reward case. For the state-action-state signals, we have SCs\u2032 = |\u03a0|N |R| |S| |A|B, and for the immediate reward signal we have SCr = |\u03a0|N |S|2 |A|B. In applications where |R| > |S| we obtain the ordering SCU < SCr < SCs\u2032 ."}, {"heading": "6 Conclusion", "text": "In this paper we address the policy reuse problem, which involves responding to an unknown task instance by selecting between a number of policies available to the agent so as to minimise regret, with respect to the best policy in the set, within a short number of episodes. This problem is motivated by many application domains\nwhere tasks have short durations such as human interaction and personalisation or monitoring tasks.\nWe introduce Bayesian Policy Reuse, a Bayesian framework for solving this problem. The algorithm tracks a probability distribution (belief) over a set of known tasks capturing their similarity to the new instance that the agent is solving. The belief is updated with the aid of side information (signals) available to the agent: observation signals acquired online for the new instance, and signal models acquired offline for each policy. To balance the trade-off between exploration and exploitation, several mechanisms for selecting policies from the belief (exploration heuristics) are also described, giving rise to different variants of the core algorithm.\nThis approach is empirically evaluated in three simulated domains where we compare the different variants of BPR, and contrast performance with related approaches. In particular, we compare the performance of BPR with a multiarmed bandit algorithm (UCB1) and a Bayesian optimisation method (GP-UCB). We also show the effect of using different kinds of observation signals on the convergence of the belief, and we illustrate the trade-off between library size and sample complexity required to achieve a required level of performance in a task.\nThe problem of policy reuse as defined in this paper has many connections with related settings from the literature, especially in the multi-armed bandit research. However, it also has certain features that does not allow it to be reduced exactly to any one of them. The contributed problem definition and the proposed Bayesian approach are first steps toward a practical solution that can be applied to real world scenarios where traditional learning approaches are not feasible.\nAcknowledgements This work has taken place in the Robust Autonomy and Decisions group within the School of Informatics, University of Edinburgh. This research has benefitted from support by the UK Engineering and Physical Sciences Research Council (grant number EP/H012338/1) and the European Commission (TOMSY and SmartSociety grants)."}], "references": [{"title": "An overview of the conservation status of and threats to rhinoceros species in the wild", "author": ["R. Amin", "K. Thomas", "R.H. Emslie", "T.J. Foose", "N. Strien"], "venue": "International Zoo Yearbook,", "citeRegEx": "Amin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2006}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["Eric Brochu", "Vlad M. Cora", "Nando De Freitas"], "venue": "arXiv preprint arXiv:1012.2599,", "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Sample complexity of multi-task reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": "In Proceedings of The 29th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Brunskill and Li.,? \\Q2013\\E", "shortCiteRegEx": "Brunskill and Li.", "year": 2013}, {"title": "Learning parameterized skills", "author": ["B.C. da Silva", "G.D. Konidaris", "A.G. Barto"], "venue": "In Proceedings of the Twenty Ninth International Conference on Machine Learning,", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Model based bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In Proceedings of the fifteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Bayesian policy gradient algorithms. In Advances in Neural Information Processing Systems", "author": ["Yaakov Engel", "Mohammad Ghavamzadeh"], "venue": "Proceedings of the 2006 Conference,", "citeRegEx": "Engel and Ghavamzadeh.,? \\Q2007\\E", "shortCiteRegEx": "Engel and Ghavamzadeh.", "year": 2007}, {"title": "Probabilistic policy reuse in a reinforcement learning agent", "author": ["F. Fern\u00e1ndez", "M. Veloso"], "venue": "In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Fern\u00e1ndez and Veloso.,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez and Veloso.", "year": 2006}, {"title": "Response surface bandits", "author": ["Josep Ginebra", "Murray K. Clayton"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Ginebra and Clayton.,? \\Q1995\\E", "shortCiteRegEx": "Ginebra and Clayton.", "year": 1995}, {"title": "A dynamic allocation index for the discounted multiarmed bandit problem", "author": ["J.C. Gittins", "D. Jones"], "venue": "Progress in Statistics,", "citeRegEx": "Gittins and Jones.,? \\Q1974\\E", "shortCiteRegEx": "Gittins and Jones.", "year": 1974}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Knowledge transfer in reinforcement learning", "author": ["Alessandro Lazaric"], "venue": "PhD thesis, PhD thesis, Politecnico di Milano,", "citeRegEx": "Lazaric.,? \\Q2008\\E", "shortCiteRegEx": "Lazaric.", "year": 2008}, {"title": "Clustering markov decision processes for continual transfer", "author": ["M.M. Hassan Mahmud", "Majd Hawasly", "Benjamin Rosman", "Subramanian Ramamoorthy"], "venue": "arXiv preprint arXiv:1311.3959,", "citeRegEx": "Mahmud et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahmud et al\\.", "year": 2013}, {"title": "Adapting interaction environments to diverse users through online action set selection", "author": ["M.M. Hassan Mahmud", "Benjamin Rosman", "Subramanian Ramamoorthy", "Pushmeet Kohli"], "venue": "In AAAI 2014 Workshop on Machine Learning for Interactive Systems,", "citeRegEx": "Mahmud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmud et al\\.", "year": 2014}, {"title": "Latent bandits", "author": ["Odalric-Ambrym Maillard", "Shie Mannor"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Maillard and Mannor.,? \\Q2014\\E", "shortCiteRegEx": "Maillard and Mannor.", "year": 2014}, {"title": "A structured multiarmed bandit problem and the greedy policy", "author": ["Adam J. Mersereau", "Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Mersereau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mersereau et al\\.", "year": 2009}, {"title": "Computing a classic index for finite-horizon bandits", "author": ["Jos\u00e9 Ni\u00f1o-Mora"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "Ni\u00f1o.Mora.,? \\Q2011\\E", "shortCiteRegEx": "Ni\u00f1o.Mora.", "year": 2011}, {"title": "Multi-armed bandit problems with dependent arms", "author": ["Sandeep Pandey", "Deepayan Chakrabarti", "Deepak Agarwal"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Pandey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pandey et al\\.", "year": 2007}, {"title": "The knowledge gradient for optimal learning", "author": ["Warren B. Powell"], "venue": "Wiley Encyclopedia of Operations Research and Management Science,", "citeRegEx": "Powell.,? \\Q2010\\E", "shortCiteRegEx": "Powell.", "year": 2010}, {"title": "On user behaviour adaptation under interface change", "author": ["Benjamin Rosman", "Subramanian Ramamoorthy", "M.M. Hassan Mahmud", "Pushmeet Kohli"], "venue": "In International Conference on Intelligent User Interfaces,", "citeRegEx": "Rosman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosman et al\\.", "year": 2014}, {"title": "Contextual bandits with similarity information", "author": ["Aleksandrs Slivkins"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Slivkins.,? \\Q2014\\E", "shortCiteRegEx": "Slivkins.", "year": 2014}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham M. Kakade", "Matthias Seeger"], "venue": "arXiv preprint arXiv:0912.3995,", "citeRegEx": "Srinivas et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2009}, {"title": "Experience-efficient learning in associative bandit problems", "author": ["Alexander L Strehl", "Chris Mesterharm", "Michael L Littman", "Haym Hirsh"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "Bayesian policy search with policy priors", "author": ["David Wingate", "Noah D. Goodman", "Daniel M. Roy", "Leslie P. Kaelbling", "Joshua B. Tenenbaum"], "venue": "In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Two,", "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "For example, online personalisation (Mahmud et al., 2014) is becoming a core concept in human-computer interaction (HCI), driven largely by a proliferation of new sensors and input devices which allow for a more natural means of communicating with hardware.", "startOffset": 36, "endOffset": 57}, {"referenceID": 20, "context": "On the other hand, taking too long to calibrate is likely to frustrate the user (Rosman et al., 2014), who may then abandon the interaction.", "startOffset": 80, "endOffset": 101}, {"referenceID": 0, "context": "1 Poaching of large mammals such as rhinoceroses is a major problem throughout Africa and Asia (Amin et al., 2006).", "startOffset": 95, "endOffset": 114}, {"referenceID": 24, "context": "length is unknown), it is plausible to consider seeding the process with a set of policies of previously solved, related task instances, in what can be seen as a strategy for transfer learning (Taylor and Stone, 2009).", "startOffset": 193, "endOffset": 217}, {"referenceID": 17, "context": "Solving this problem in general is difficult as it maps into the intractable finite-horizon online bandit problem (Ni\u00f1o-Mora, 2011).", "startOffset": 114, "endOffset": 131}, {"referenceID": 23, "context": "contexts in contextual bandits (Strehl et al., 2006; Langford and Zhang, 2008)) or analytical forms of reward (e.", "startOffset": 31, "endOffset": 78}, {"referenceID": 11, "context": "contexts in contextual bandits (Strehl et al., 2006; Langford and Zhang, 2008)) or analytical forms of reward (e.", "startOffset": 31, "endOffset": 78}, {"referenceID": 18, "context": "correlated bandits (Pandey et al., 2007)) to share the credit of pulling an arm between many possible arms.", "startOffset": 19, "endOffset": 40}, {"referenceID": 2, "context": "Because we are dealing with tasks which we assume are of limited duration and which do not allow extensive experimenting, and in order to use information from previous trials to maintain belief distributions over the task space, we draw inspiration from the Bayesian optimisation/efficient global optimisation literature (Brochu et al., 2010) for an approach to this problem that is efficient in the number of policy executions, corresponding to function evaluations in the classical optimisation setting.", "startOffset": 321, "endOffset": 342}, {"referenceID": 12, "context": "A version of the policy reuse problem was described by Mahmud et al. (2013), where it is used to test a set of landmark policies retrieved through clustering in the space of MDPs.", "startOffset": 55, "endOffset": 76}, {"referenceID": 7, "context": "Additionally, the term \u2018policy reuse\u2019 has been used by Fern\u00e1ndez and Veloso (2006) in a different context.", "startOffset": 55, "endOffset": 83}, {"referenceID": 4, "context": "da Silva et al. (2012)).", "startOffset": 3, "endOffset": 23}, {"referenceID": 25, "context": "In the context of MDPs, previous work has regarded classes of MDPs as probability distributions over task parameters (Wilson et al., 2007).", "startOffset": 117, "endOffset": 138}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013).", "startOffset": 86, "endOffset": 107}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al.", "startOffset": 87, "endOffset": 332}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al. (2012); Maillard and Mannor (2014).", "startOffset": 87, "endOffset": 351}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al. (2012); Maillard and Mannor (2014). In this work we do not explicitly investigate methods of task clustering, but the algorithms presented herein are most efficient when such a cluster-based structure exists in the task space.", "startOffset": 87, "endOffset": 379}, {"referenceID": 21, "context": "This definition of types is similar to the concept of similarity-based contextual bandits (Slivkins, 2014), where a distance function can be defined in the joint space of contexts and arms given by an upper bound of reward differences.", "startOffset": 90, "endOffset": 106}, {"referenceID": 9, "context": "Multiple proposals have been widely considered in the multi-armed bandits (MAB) literature for these heuristics, ranging from early examples like the Gittins index for infinite horizon problems (Gittins and Jones, 1974) to more recent methods such as the knowledge gradient (Powell, 2010).", "startOffset": 194, "endOffset": 219}, {"referenceID": 19, "context": "Multiple proposals have been widely considered in the multi-armed bandits (MAB) literature for these heuristics, ranging from early examples like the Gittins index for infinite horizon problems (Gittins and Jones, 1974) to more recent methods such as the knowledge gradient (Powell, 2010).", "startOffset": 274, "endOffset": 288}, {"referenceID": 9, "context": "For this kind of setting Lai and Robbins (1985) show that index-based methods achieve optimal performance asymptotically.", "startOffset": 25, "endOffset": 48}, {"referenceID": 2, "context": ", Brochu et al. (2010)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 19, "context": "The final exploration heuristic we describe is the knowledge gradient (Powell, 2010), which aims to balance exploration and exploitation through optimising myopic return whilst maintaining asymptotic optimality.", "startOffset": 70, "endOffset": 84}, {"referenceID": 1, "context": "We choose two frameworks, multi-armed bandits for which we use UCB1 (Auer et al., 2002), and Bayesian optimisation where we use GP-UCB (Srinivas et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 22, "context": ", 2002), and Bayesian optimisation where we use GP-UCB (Srinivas et al., 2009).", "startOffset": 55, "endOffset": 78}, {"referenceID": 23, "context": "The optimal selection from a set of provided policies for a new task is in essence a transfer learning problem (see the detailed review by Taylor and Stone (2009)).", "startOffset": 139, "endOffset": 163}, {"referenceID": 12, "context": "One transfer approach that considers the similarity between source and target tasks is by Lazaric (2008), where generated (s, a, r, s\u2032) samples from the target task are", "startOffset": 90, "endOffset": 105}, {"referenceID": 3, "context": "More recently, Brunskill and Li (2013) consider using the (s, a, r, s\u2032) similarity to compute confidence intervals of where, in a collection of MDP classes, a new instance best fits.", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "For example, in Response Surface Bandits (Ginebra and Clayton, 1995), there is a known prior over the parameters of the reward curve and the metric on the policy space is known.", "startOffset": 41, "endOffset": 68}, {"referenceID": 8, "context": "For example, in Response Surface Bandits (Ginebra and Clayton, 1995), there is a known prior over the parameters of the reward curve and the metric on the policy space is known. More recently, Mersereau et al. (2009) present a greedy policy which takes advantage of the correlation between the arms in their reward functions, assuming a linear form with one parameter, with a known prior.", "startOffset": 42, "endOffset": 217}, {"referenceID": 18, "context": "In another thread, Dependent Bandits (Pandey et al., 2007) assume that the arms in a multi-armed bandit can be clustered into different groups, such that the members of each have correlated reward distribution parameters.", "startOffset": 37, "endOffset": 58}, {"referenceID": 1, "context": "Then, each cluster is represented with one representative arm, and the algorithm proceeds in two steps: a cluster is first chosen by a variant of UCB1 (Auer et al., 2002) applied to the set of representative arms, and then the same method is used again to choose between the arms of the chosen cluster.", "startOffset": 151, "endOffset": 170}, {"referenceID": 1, "context": "Then, each cluster is represented with one representative arm, and the algorithm proceeds in two steps: a cluster is first chosen by a variant of UCB1 (Auer et al., 2002) applied to the set of representative arms, and then the same method is used again to choose between the arms of the chosen cluster. We assume in our work that the set of previously-solved tasks span and represent the space well, but we do not dwell on how this set of tasks can be selected. Clustering is one good candidate for that, and one particular example of identifying the important types in a task space can be seen in the work of Mahmud et al. (2013).", "startOffset": 152, "endOffset": 631}, {"referenceID": 23, "context": "In Contextual Bandits (Strehl et al., 2006; Langford and Zhang, 2008), the agent is able to observe side information (or context labels) that are related to the nature of the bandit machine, and the question becomes one of selecting the best arm for each possible context.", "startOffset": 22, "endOffset": 69}, {"referenceID": 11, "context": "In Contextual Bandits (Strehl et al., 2006; Langford and Zhang, 2008), the agent is able to observe side information (or context labels) that are related to the nature of the bandit machine, and the question becomes one of selecting the best arm for each possible context.", "startOffset": 22, "endOffset": 69}, {"referenceID": 15, "context": "Another related treatment is that of latent bandits (Maillard and Mannor, 2014) where, in the single-cluster arrival case, the experienced bandit machine is drawn from a single cluster with known reward distributions, and in the agnostic case the instances are drawn from many unknown clusters with unknown reward", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "Bayesian Reinforcement Learning (BRL) is a paradigm of Reinforcement Learning that handles the uncertainty in an unknown MDP in a Bayesian manner by maintaining a probability distribution over the space of possible MDPs, and updating that distribution using the observations generated from the MDP as the interaction continues (Dearden et al., 1999).", "startOffset": 327, "endOffset": 349}, {"referenceID": 5, "context": "Bayesian Reinforcement Learning (BRL) is a paradigm of Reinforcement Learning that handles the uncertainty in an unknown MDP in a Bayesian manner by maintaining a probability distribution over the space of possible MDPs, and updating that distribution using the observations generated from the MDP as the interaction continues (Dearden et al., 1999). In work by Wilson et al. (2007), the problem of Multi-task Reinforcement Learning of a possibly-infinite stream of MDPs is handled in a Bayesian framework.", "startOffset": 328, "endOffset": 383}, {"referenceID": 26, "context": "When no gradient information is available to guide the search, Wingate et al. (2011) propose to use MCMC to search in the space of policies which is endowed with a prior.", "startOffset": 63, "endOffset": 85}], "year": 2015, "abstractText": "A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires \u2018fast\u2019 responses, in terms of rapid convergence, especially when the task instance has a short duration such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library in contrast to policy learning. In policy reuse, the agent has prior experience from the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse and present an algorithm for efficiently responding to a novel task instance by reusing a policy from this library of existing policies, where the choice is based on observed \u2018signals\u2019 which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations. ? The first two authors contributed equally to this paper. Benjamin Rosman Mobile Intelligent Autonomous Systems (MIAS), Council for Scientific and Industrial Research (CSIR), South Africa, and the School of Computer Science and Applied Mathematics, University of the Witwatersrand, South Africa. E-mail: BRosman@csir.co.za. Majd Hawasly School of Informatics, University of Edinburgh, UK. E-mail: M.Hawasly@ed.ac.uk. Subramanian Ramamoorthy School of Informatics, University of Edinburgh, UK. E-mail: S.Ramamoorthy@ed.ac.uk. ar X iv :1 50 5. 00 28 4v 2 [ cs .A I] 1 4 D ec 2 01 5 2 Rosman, Hawasly & Ramamoorthy", "creator": "LaTeX with hyperref package"}}}