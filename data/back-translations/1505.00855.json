{"id": "1505.00855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature", "abstract": "In recent years, the number of digitized and publicly accessible art collections has rapidly increased. With the availability of such large collections of digitized artworks, it is necessary to develop multimedia systems to archive and retrieve this pool of data. Measuring visual similarity between artistic objects is an essential step for such multimedia systems that can benefit from multimedia tasks at a higher level. To model this similarity between images, we should extract the appropriate visual characteristics for paintings and find the best approach to learning the similarity metric based on these characteristics. We will examine a comprehensive list of visual characteristics and metric learning approaches to learn an optimized measure of similarity between images. We will develop a machine that is capable of making aesthetically related semantic assessments at the level, such as predicting the style, genre and artist of a painting, and providing optimized similarity measures based on the available knowledge in the field of art interpretation.", "histories": [["v1", "Tue, 5 May 2015 01:25:26 GMT  (1438kb,D)", "http://arxiv.org/abs/1505.00855v1", "21 pages"]], "COMMENTS": "21 pages", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG cs.MM", "authors": ["babak saleh", "ahmed elgammal"], "accepted": false, "id": "1505.00855"}, "pdf": {"name": "1505.00855.pdf", "metadata": {"source": "CRF", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature", "authors": ["Babak Saleh", "Ahmed Elgammal"], "emails": ["babaks@cs.rutgers.edu", "elgammal@cs.rutgers.edu"], "sections": [{"heading": "1 Introduction", "text": "In the past few years, the number of fine-art collections that are digitized and publicly available has been growing rapidly. Such collections span classical 1 and modern and contemporary artworks 2. With the availability of such large collections of digitized artworks comes the need to develop multimedia systems to archive and retrieve this pool of data. Typically these collections, in particular early modern ones, come with metadata in the form of annotations by art historians and curators, including information about each painting\u2019s artist, style, date, genre, etc. For online galleries displaying contemporary artwork, there is a need to develop automated recommendation systems that can retrieve \u201csimilar\u201d paintings that the user might like to buy. This highlights the need to investigate metrics of visual similarity among digitized paintings that are optimized for the domain of painting.\nThe field of computer vision has made significant leaps in getting digital systems to recognize and categorize objects and scenes in images and videos. These advances have been driven by a wide spread need for the technology, since cameras are everywhere now. However a person looking at a painting can make sophisticated inferences\n1 Examples: Wikiart; Arkyves; BBC Yourpainting 2 Examples: Artsy; Behance; Artnet\nar X\niv :1\n50 5.\n00 85\n5v 1\n[ cs\n.C V\nbeyond just recognizing a tree, a chair, or the figure of Christ. Even individuals without specific art historical training can make assumptions about a painting\u2019s genre (portrait or landscape), its style (impressionist or abstract), what century it was created, the artists who likely created the work and so on. Obviously, the accuracy of such assumptions depends on the viewer\u2019s level of knowledge and exposure to art history. Learning and judging such complex visual concepts is an impressive ability of human perception [2].\nThe ultimate goal of our research is to develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting\u2019s style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Immediate questions that arise include, but are not limited to: What visual features should be used to encode information in images of paintings? How does one weigh different visual features to achieve a useful similarity measure? What type of art historical knowledge should be used to optimize such similarity measures? In this paper we address these questions and aim to provide answers that can benefit researchers in the area of computer-based analysis of art. Our work is based on a systematic methodology and a comprehensive evaluation on one of the largest available digitized art datasets.\nArtists use different concepts to describe paintings. In particular, stylistic elements, such as space, texture, form, shape, color, tone and line are used. Other principles include movement, unity, harmony, variety, balance, contrast, proportion, and pattern. To this might be added physical attributes, like brush strokes as well as subject matter and other descriptive concepts [13].\nFor the task of computer analyses of art, researchers have engineered and investigated various visual features3 that encode some of these artistic concepts, in particular brush strokes and color, which are encoded as low-level features such as texture statistics and color histograms (e.g. [19, 20]). Color and texture are highly prone to variations\n3 In contrast to art disciplines, in the fields of computer vision and machine learning, researchers use the term\u201cvisual features\u201d to denote statistical measurements that are extracted from images for the task of classification. In this paper we stick to this typical terminology.\nduring the digitization of paintings; color is also affected by a painting\u2019s age. The effect of digitization on the computational analysis of paintings is investigated in great depth by Polatkan et al. [24]. This highlights the need to carefully design visual features that are suitable for the analysis of paintings.\nClearly, it would be a cumbersome process to engineer visual features that encode all the aforementioned artistic concepts. Recent advances in computer vision, using deep neural networks, showed the advantage of \u201clearning\u201d the features from data instead of engineering such features. However, It would also be impractical to learn visual features that encode such artistic concepts, since that would require extensive annotation of these concepts in each image within a large training and testing dataset. Obtaining such annotations require expertise in the field of art history that can not be achieved with typical crowed-sourcing annotators.\nGiven the aforementioned challenges to engineering or learning suitable visual features for painting, in this paper we follow an alternative strategy. We mainly investigate different state-of-the-art visual elements, ranging from low-level elements to semanticlevel elements. We then use metric learning to achieve optimal similarity metrics between paintings that are optimized for specific prediction tasks, namely style, genre, and artist classification. We chose these tasks to optimize and evaluate the metrics since, ultimately, the goal of any art recommendation system would be to retrieve artworks that are similar along the directions of these high-level semantic concepts. Moreover, annotations for these tasks are widely available and more often agreed-upon by art historians and critics, which facilitates training and testing the metrics.\nIn this paper we investigate a large space of visual features and learning methodologies for the aforementioned prediction tasks. We propose and compare three learning methodologies to optimize such tasks. We present results of a comprehensive comparative study that spans four state-of-the-art visual features, five metric learning approaches and the proposed three learning methodologies, evaluated on the aforementioned three artistic prediction tasks."}, {"heading": "2 Related Work", "text": "On the subject of painting, computers have been used for a diverse set of tasks. Traditionally, image processing techniques have been used to provide art historians with quantification tools, such as pigmentation analysis, statistical quantification of brush strokes, etc. We refer the reader to [28, 5] for comprehensive surveys on this subject.\nSeveral studies have addressed the question of which features should be used to encode information in paintings. Most of the research concerning the classification of paintings utilizes low-level features encoding color, shadow, texture, and edges. For example Lombardi [20] has presented a study of the performance of these types of features for the task of artist classification among a small set of artists using several supervised and unsupervised learning methodologies. In that paper the style of the painting was identified as a result of recognizing the artist.\nSince brushstrokes provide a signature that can help identify the artist, designing visual features that encode brushstrokes has been widely adapted.(e.g. [25, 18, 22, 15, 6, 19]). Typically, texture statistics are used for that purpose. However, as mentioned earlier, texture features are highly affected by the digitization resolution. Researchers also investigated the use of features based on local edge orientation histograms, such as SIFT [21] and HOG [10]. For example, [12] used SIFT features within a Bag-of-words pipeline to discriminate among a set of eight artists.\nArora et al. [3] presented a comparative study for the task of style classification, which evaluated low-level features, such as SIFT and Color SIFT [1], versus semanticlevel features, namely Classemes [29], which encodes object presence in the image. It was found that semantic-level features significantly outperform low-level features for this task. However the evaluation was conducted on a small dataset of 7 styles, with 70 paintings in each style. Carneiro et al [9] also concluded that low-level texture and color features are not effective because of inconsistent color and texture patterns that describe the visual classes in paintings.\nMore recently, Saleh et al [26] used metric learning approaches for finding influence paths between painters based on their paintings. They evaluated three metric learning approaches to optimize a metric over low-level HOG features. In contrast to that work, the evaluation presented in this paper is much wider in scope since we address three tasks (style, genre and artist prediction), we cover features spanning from low-level to semantic-level and we evaluate five metric learning approaches. Moreover, The dataset of [26] has only 1710 images from 66 artists, while we conducted our experiments on 81,449 images painted by 1119 artists. Bar et al [4] proposed an approach for style classification based on features obtained from a convolution neural network pre-trained on an image categorization task. In contrast we show that we can achieve better results with much lower dimensional features that are directly optimized for style and genre classification. Lower dimensionality of the features is preferred for indexing large image collections."}, {"heading": "3 Methodology", "text": "In this section we explain the methodology that we follow to find the most appropriate combination of visual features and metrics that produce accurate similarity measure-\nments. We acquire these measurements to mimic the art historian\u2019s ability to categorize paintings based on their style, genre and the artist who made it. In the first step, we extract visual features from the image. These visual features range from low-level (e.g. edges) to high-level (e.g. objects in the painting). More importantly, in the next step we learn how to adjust these features for different classification tasks by learning the appropriate metrics. Given the learned metric we are able to project paintings from a high dimensional space of raw visual information to a meaningful space with much lower dimensionality. Additionally, learning a classifier in this low-dimensional space can be easily scaled up for large collections.\nIn the rest of this section: First, we introduce our collection of fine-art paintings and explain what are the tasks that we target in this work. Later, we explore methodologies that we consider in this work to find the most accurate system for aforementioned tasks. Finally, we explain different types of visual features that we use to represent images of paintings and discuss metric learning approaches that we applied to find the proper notion of similarity between paintings."}, {"heading": "3.1 Dataset and Proposed Tasks", "text": "In order to gather our collection of fine-art paintings, we used the publicly available dataset of \u201dWikiart paintings\u201d4; which, to the best of our knowledge, is the largest online public collection of digitized artworks. This collection has images of 81,449 fineart paintings from 1,119 artists ranging from fifteen centuries to contemporary artists. These paintings are from 27 different styles (Abstract, Byzantine, Baroque, etc.) and 45 different genres (Interior, Landscape, etc.) Previous work [26, 9] used different resources and made smaller collections with limited variability in terms of style, genre and artists. The work of [4] is the closest to our work in terms of data collection procedure, but the number of images in their collection is half of ours.\nWe target automatic classification of paintings based on their style, genre and artist using visual features that are automatically extracted using computer vision algorithms. Each of these tasks has its own challenges and limitations. For example, there are large\n4 http://www.wikiart.org/\nvariations in terms of visual appearances in paintings from one specific style. However, this variation is much more limited for paintings by one artist. These larger intra-class variations suggests that style classification based on visual features is more challenging than artist classification. For each of the tasks we selected a subset of the data that ensure enough samples for training and testing. In particular for style classification we use a subset of the date with 27 styles where each style has at least 1500 paintings with no restriction on genre or artists, with a total of 78,449 images. For genre classification we use a subset with 10 genre classes, where each genre has at least 1500 paintings with no restriction of style or genre, with a total of 63,691 images. Similarly for artist classification we use a subset of 23 artists, where each of them has at least 500 paintings, with a total of 18,599 images. Table 1 lists the set of style, genre, and artist labels."}, {"heading": "3.2 Classification Methodology", "text": "In order to classify paintings based on their style, genre or artist we followed three methodologies.\nMetric Learning: First, as depicted in figure 1, we extract visual features from images of paintings. For each of these prediction tasks, we learn a similarity metric optimized for it, i.e. style-optimized metric, genre-optimized metric and artist-optimized metric. Each metric induces a projector to a corresponding feature space optimized for the corresponding task. Having the metric learned, we project the raw visual features into the new optimized feature space and learn classifiers for the corresponding prediction task. For that purpose we learn a set of one-vs-all SVM classifiers for each of the labels in table 1 for each of the tasks.\nWhile our first strategy focuses on classification based on combinations of a metric and a visual feature, the next two methodologies that we followed fuse different features or different metrics.\nFeature fusion: The second methodology that we used for classification is depicted in figure 2. In this case, we extract different types of visual features (four types of features as will explained next). Based on the prediction task (e.g. style) we learn the metric for each type of feature as before. After projecting these features separately, we concatenate them to make the final feature vector. The classification will be based on training classifiers using these final features. This feature fusion is important as we want to capture different types of visual information by using different types of features. Also concatenating all features together and learn a metric on top of this huge feature vector is computationally intractable. Because of this issue, we learn metrics on feature separately and after projecting features by these metrics, we can concatenate them for classification purposes.\nMetric-fusion: The third methodology (figure 3) projects each visual features using multiple metrics (in our experiment we used five metrics as will be explained next) and then fuses the resulting optimized feature spaces to obtain a final feature vector for classification. This is an important strategy, because each one of the metric learning approaches use a different criteria to learn the similarity measurement. By learning all metrics individually (on the same type of feature), we make sure that we took into account all criteria (e.g. information theory along with neighbor hood analysis)."}, {"heading": "3.3 Visual Features", "text": "Visual features in computer vision literature are either engineered and extracted in an unsupervised way (e.g. HOG, GIST) or learned based on optimizing a specific task, typically categorization of objects or scenes (e.g. CNN-based features). This results in high-dimensional feature vectors that might not necessary correspond to nameable (semantic-level) characteristics of an image. Based on the ability to find a meaning, visual features can be categorized into low-level and high-level. Low-level features are visual descriptors that there is no explicit meaning for each dimension of them, while high-level visual features are designed to capture some notions (usually objects). For this work, we investigated some state-of-the-art representatives of these two categories:\nLow-level Features: On one hand, in order to capture low-level visual information we extracted GIST features [23], which are holistic features that are designed for scene categorization. GIST features provide a 512 real-valued representation that implicitly captures the dominant spatial structure of the image.\nLearned Semantic-level Features: On the other hand, for the purpose of semantic representation of the images, we extracted three object-based representation of the images: Classeme [29], Picodes [8], and CNN-based features [16]. In all these three features, each element of the feature vector represents the confidence of the presence of an object-category in the image, therefore they provide a semantic encoding of the images. However, for learning these features, the object-categories are generic and are not art-specific. First two features are designed to capture the presence of a set of basiclevel object categories as following: a list of entry-level categories (e.g. horse and cross) is used for downloading a large collection of images from the web. For each image a comprehensive set of low-level visual features are extracted and one classifier is learned for each category. For a given test image, these classifiers are applied on the image and the responses (confidences) make the final feature vector. We followed the implementation of [7] and for each image extracted a 2659 dimensional real-valued Classeme feature vector and a 2048 dimensional binary-value Picodes feature.\nConvolutional Neural Networks(CNN) [17] showed a remarkable performance for the task of large-scale image categorization [16]. CNNs have four convolutional layers followed by three fully connected layers. Bar et al [4] showed that a combination of the\n0 100 200 300 400 500 600 700 800 900 1000\noutput of these fully connected layers achieve a superior performance for the task of style classification of paintings. Following this observation we used the last layer of a pre-trained CNN [16] (1000 dimensional real-valued vectors) as another feature vector."}, {"heading": "3.4 Metric Learning", "text": "The purpose of Metric Learning is to find some pair-wise real-valued function dM (x, x\u2032) which is non-negative, symmetric, obeys the triangle inequality and returns zero if and only if x and x\u2032 are the same point. Training such a function in a general form can be seen as the following optimization problem:\nmin M l(M,D) + \u03bbR(M) (1)\nThis optimization has two sides, first it tries to minimize the amount of loss l(M,D) by using metric M over data samples D while trying to adjust the model by the regularization term R(M). The first term shows the accuracy of the trained metric and second one estimates its capability over new data and avoids overfitting. Based on the enforced constraints, the resulted metric can be linear or non-linear and depending on the amount of labels used for training, it can be supervised or unsupervised.\nFor consistency over the metric learning algorithms, we need to fix the notation first. We learn the matrix M that will be used in Generalized Mahalanobis Distance: dM (x, x \u2032) = \u221a\n(x\u2212 x\u2032)\u2032M(x\u2212 x\u2032), where M by definition is a positive semi-definite matrix and can be decomposed as M = GTG. We use this matrix G to project raw visual features. Measuring similarity in this projection space is simply computing the euclidean distance between two item.\nIt is interesting that we can reduce the dimension of features during learning the metric whenM is a low rank matrix. More importantly, there are significantly important information in the ground truth annotation associated with paintings that we use to learn a more reliable metric in a supervised fashion for both the linear and non-linear cases. We consider following approaches that differ based on the form of M or the amount of regularization.\nConfusion matrix for Style classification\n5 10 15 20 25\nNeighborhood Component Analysis (NCA) The objective function of NCA [14] is related to analyzing the nearest neighbors. The idea starts with projecting the data by matrix M and training a leave-one-out classifier. Then the probability of correctly classifying xi is Pi = \u2211 j:yi=yj\nPij , where Pij is the mean expected loss of classifying xi as a member of class j.\nThen this metric is learned by optimizing the following term: maxM \u2211\ni Pi. We can decompose M as L\u2032 \u2217 L and choosing a rectangular L will result in a low-rank matrix M . Although this method is easy to understand and implement, it is subject to local minimums. This happens due to the non-convexity of the proposed optimization problem. The next approach has the advantage of solving a convex optimization.\nLarge Margin Nearest Neighbors (LMNN) LMNN [32] is an approach for learning a Mahalanobis distance, which is widely used because of its global optimum solution and superior performance in practice. The learning of this metric involves a set of constrains, all of which are defined locally. This means that LMNN enforces the k nearest neighbor of any training instance belonging to the same class (these instances are called \u201ctarget neighbors\u201d). This should be done while all the instances of other classes, referred as \u201cimpostors\u201d, should be far from this point. For finding the target neighbors, Euclidean distance has been applied to each pair of samples, resulting in the following formulation:\nmin M\n(1\u2212 \u00b5) \u2211\n(xi,xj)\u2208T\nd2M (xi, xj) + \u00b5 \u2211 i,j,k \u03b7i,j,k\ns.t. : d2M (xi, xk)\u2212 d2M (xi, xj) \u2265 1\u2212 \u03b7i,j,k\u2200(xi, xj , xk) \u2208 I.\nWhere T stands for the set of Target neighbors and I represents Impostors. Since these constrains are locally defined, this optimization leads to a convex formulation and a global solution. This metric learning approach is related to Support Vector Machines (SVM) in principle, which theoretically engages its usage along with SVM for the task of classification.\nDue to the popularity of LMNN, different variations of it have been introduced, including a non-linear version called gb-LMNN [32] which we used in our experiments\nConfusion matrix for Genre classification\n1 2 3 4 5 6 7 8 9 10\nas well. However its performance for classification tasks was worse that linear LMNN. We assume this poor performance is rooted in the nature of visual features that we extract for paintings.\nBoost Metric This approach is based on the fact that a positive semi-definite matrix can be decomposed into a linear combination of trace-one rank-one matrices. Shen et al [27] use this fact and instead of learning M , finds a set of weaker metrics that can be combined and give the final metric. They treat each of these matrices as a Weak Learner, which is used in the literature of Boosting methods. The resulting algorithm applies the idea of AdaBoost to Mahalanobis distance, which has been shown to be quiet efficient in practice.\nThis method is particularly of our interest, because we can learn an individual metric for each style of paintings and finally merge these metrics to get a unique final metric. Theoretically the final metric can perform well to find similarities inside each style/genre of paintings as well.\nInformation Theory Metric Learning (ITML) This metric learning algorithm is based on Information theory rather than Mahalanobis distances. In other words the optimization problem of learning a metric involves an information measure.\nDavis et al [11] introduce the measure of LogDet divergence regularization between two matrices M,M \u2032(can be interpreted as metrics). By using this measure, learning the metric can be represented by:\nmin M \u2032\u2208PSD\nDld(M,M \u2032) + \u03b3 \u2211 i,j i,j\ns.t. : d2M \u2032(xi, xj) \u2264 u+ i,j\u2200(xi, xj) \u2208 S. d2M \u2032(xi, xj) \u2265 v \u2212 i,j\u2200(xi, xj) \u2208 D.\nLearning ITML via this formulation aims to satisfy a set of Similarity(S) and Dissimilarity(D) constrains while keeping the new metric M \u2032 close to the initial metric\nConfusion matrix for artist classification\n2 4 6 8 10 12 14 16 18 20 22\nM . There are two key features of the LogDet divergence: 1) It is finite if and only if matrices are positive semi-definite(PSD), 2) This function is rank-preserving.\nThese properties indicate that if we start learning the metricM \u2032 by setting the initial matrix M as identity matrix(I), ITML returns a metric that is from the same rank and is very similar to the Euclidean distance.\nAlthough this iterative process converges to a global minimum which performs well in practice, it is very sensitive to the choice of initialization of metric(M ).\nMetric Learning for Kernel Regression (MLKR) Similar to NCA objective function, which minimizes the classification error; Weinberger and Tesauro [31] learn a metric by optimizing the leave-one-out error for the task of kernel regression. In kernel regression, there is an essential need for proper distances between points that will be used for weighting sample data. MLKR learn this distance by minimizing the leave-one-out error for regression on training data. Although this metric learning method is designed for kernel regression, the resulted distance function can be used in variety of tasks."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experimental Setting", "text": "Visual Features As we explained in section 3, we extract GIST features as low-level visual features and Classeme, Picodes and CNN-based features as the high-level semantic features. We followed the original implementation of Oliva and Torralba [23] to get a 512 dimensional feature vector. For Classeme and Picodes we used the implementation of Bergamo et al [29], resulting in 2659 dimensional Classeme features and 2048 dimensional Picodes features. We used the implementation of Vedaldi and Lenc [30] to extract 1000 dimensional feature vectors of the last layer of CNN.\nObject-based representations of the images produce feature vectors that are much higher in dimensionality than GIST descriptors. In the sake of a fair comparison of all types of features for the task of metric learning, we transformed all feature vectors to have the same size as GIST (512 dimensional). We did this by applying Principle Component Analysis (PCA) for each type and projecting the original features onto the first\n512 eigenvectors (with biggest eigenvalues). In order to verify the quality of projection, we looked at the corresponding coefficients of eigenvalues for PCA projections. Independent of feature type, the value of these coefficients drops significantly after the first 500 eigenvectors. For example, figure 4 plots these coefficients of PCA projection for CNN features. Summation of the first 500 coefficients is 95.88% of the total summation. This shows that our projections (with 512 eigenvectors) captures the true underlying space of the original features. Using these reduced features speeds up the metric learning process as well.\nMetric Learning We used implementation of [32] to learn LMNN metric(both version of linear and non-linear) and MLKR 5. For the BoostMetric we slightly adjusted the implementation of [27]. For NCA we adopted its implementation by Fowlkes6 to work on large scale feature vectors smoothly. For the case of ITML metric learning, we followed the original implementation of authors with the default setting. For the rest of methods, parameters are chosen through a grid search that finds the minimum nearest neighbor classification. Regarding the training time, learning the ITML metric was the fastest and learning NCA and LMNN were the slowest ones. Due to computational constrains we set the parameters of LMNN metric to reduce the size of features to 100. NCA metric reduces the dimension of features to the number of categories for each tasks: 27 for style classification, 23 for artist classification and 10 for genre classification. We randomly picked 3000 samples, which we used for metric learning. These samples follow the same distribution as original data and are not used for classification experiments."}, {"heading": "4.2 Classification Experiments", "text": "For the purpose of metric learning, we conducted experiments with labels for three different tasks of style, genre and artist prediction. In following sections we investigate the performance of these metrics on different features for classification of aforementioned concepts.\nWe learned all the metrics in section3 for all 27 styles of paintings in our dataset (e.g. Expressionism, Realism, etc.). However, we did not use all the genres for learning metrics. In fact in our dataset we have 45 genres, some of which have less than 20 images. This makes the metric learning impractical and highly biased toward genres\n5 http://www.cse.wustl.edu/ kilian/index.html 6 http://www.ics.uci.edu/ fowlkes/\nwith larger number of paintings. Because of this issue, we focus on 10 genres with more than 1500 paintings. These genres are listed in table 1. In all experiments we conducted 3 fold cross validation and reported the average accuracy over all partitions. We found the best value for penalty term in SVM (which is equal to 10) by three fold cross validation. In the next three sections, we explain settings and findings for each task independently.\nStyle Classification Table 2 contains the result (accuracy percentage) of style classification (SVM) after applying different metrics on a set of features. Columns correspond to different features and rows are different metrics that are used for projecting features before learning style classifiers. In order to quantify the improvement by learning similarity metrics, we conducted a baseline experiment (first row in the table) as the following: For each type of features, we learn a set of one-vs-all classifiers on raw feature vectors. Generally Boost metric learning and ITML approaches give the highest in accuracy for the task of style classification over different visual features. However the greatest improvement over the baseline is gained by application of Boost metric on Classeme features. We visualized the confusion matrix for the task of style classification, when we learn Boost metric on Classeme features.\nFigure 5 shows this matrix, where red represents higher values. Further analysis of some confusions that are captured in this matrix result in interesting findings. In the rest of this paragraph we explain some of these cases. First, we found that there is a big confusion between \u201cAbstract expressionism\u201d (first row) and \u201cAction paintings\u201d (second column). Art historians verify the fact that this confusion is meaningful and somehow expected. \u201cAction painting\u201d is a type or subgenre of \u201cabstract expressionism\u201d and are characterized by paintings created through a much more active process\u2013 drips, flung paint, stepping on the canvas.\nAnother confusion happens between \u201cExpressionism\u201d (column 10) and \u201cFauvism\u201d (row 11), which is actually expected based on art history literature. \u201cMannerism\u201d (row 14) is a style of art during the (late)\u201cRenaissance\u201d (column 12), where they show unusual effect in scale and are less naturalistic than \u201cEarly Renaissance\u201d. This similarity between \u201cMannerism\u201d (row 14) and \u201cRenaissance\u201d (column 12) is captured by our system as well where results in confusion during style classification. \u201cMinimalism\u201d (column 15) and \u201cColor field paintings\u201d(6th row) are mostly confused with each other. We can agree on this finding as we look at members of these styles and figure out the similarity in terms of simple form and distribution of colors. Lastly some of the confusions are completely acceptable based on the origins of these styles (art movements) that are\nnoted in art history literature. For example, \u201cRenaissance\u201d(column 18) and \u201cEarly Renaissance\u201d(row 9); \u201cPost Impressionism\u201d (column 21) and \u201cImpressionism\u201d(row 13); \u201cCubism\u201d (8th row) and \u201cSynthetic Cubism\u201d (column 26). Synthetic cubism is the later act of cubism with more color continued usage of collage and pasted papers, but less linear perspective than cubism.\nGenre Classification We narrowed down the list of all genres in our dataset (45 in total) to get a reasonable number of samples for each genre (10 selected genres are listed in table 1). We trained ten one-vs-all SVM classifiers and compare their performance in Table 3. In this table columns represent different features and rows are different metric that we used to compute the distance. As table 3 shows we achieved the best performance for genre classification by learning Boost metric on top of Classeme features. Generally the performance of these classifiers are better than classifiers that we trained for style classification. This is expected as the number of genres is less than the number of styles in our dataset.\nFigure 6 shows the confusion matrix for classification of genre by learning Boost metric, when we used Classeme features. Investigating the confusions that we find in this matrix, reveals interesting results. For example, our system confuses \u201cLandscape\u201d (5th row) with \u201cCityspace\u201d (2nd column) and \u201cGenre paintings\u201d (3rd column). However, this confusion is expected as art historians can find common elements in these genres. On one hand \u201cLandscape\u201d paintings usually show rivers, mountains and valleys and there is no significant figure in them; frequently very similar to \u201cGenre paintings\u201d as they capture daily life. The difference appears in the fact that despite the \u201cGenre paintings\u201d, \u201cLandscape\u201d paintings are idealized. On the other hand, \u201cLandscape\u201d and \u201cCityspace\u201d paintings are very similar as both have open space and use realistic color tonalities.\nArtist Classification For the task of the artist classification, we trained one-vs-all SVM classifiers for each of 23 artists. For each test image, we determine its artist by finding the classifier that produces the maximum confidence. Table 4 shows the performance of different combinations of features and metrics for this task. In general learning Boost metric improves artist classification better than all other metrics, except the case of CNN features where learning ITML metric gained the best performance. We plotted the confusion matrix of this classification task in figure 7. In this plot, some confusions between artists are clearly reasonable. We investigated two cases:\nFirst case, \u201cClaude Monet\u201d(5th row) and \u201cCamille Pissaro\u201d(3rd column). Both of these Impressionist artists who lived in the late nineteen and early twentieth centuries. Interestingly, based on art history literature Monet and Pissaro became friends when they both attended the \u201dAcade\u0301mie Suisse\u201d in Paris. This friendship lasted for a long time and resulted in some noticeable interactions between them. Second case, paintings of \u201cChilde Hassam\u201d(4th row) are mostly confused with ones from \u201cMonet\u201d(5th column). This confusion is acceptable as Hassam is an American Impressionist, who declared himself as being influenced by French Impressionists. Hassam called himself an \u201cExtreme Impressionist\u201d, who painted some flag-themed artworks similar to Monet.\nBy looking at reported performances in tables 2- 4, we conclude that, all three classification tasks can benefit from learning the appropriate metric. This means that we can improve the accuracy of baseline classification by learning metrics independent of the type of visual feature or the concept that we are classifying painting based on. Experimental results show that, independent of the task, NCA and MLKR approaches are performing worse than other metrics. Additionally, Boost metric always gives the best or the second best results for all classification tasks.\nRegarding analysis of importance of features, we can verify that Classeme and Picode features are better image representations for classification purposes. Based on these classification experiments, we claim that Classemes and Picodes features perform better than CNN features. This is rooted in the fact that amount of supervision for training Classeme and Picodes is more than CNN training. Also, unlike Classeme and Picodes, CNN feature is designed to categorize the object insides a given bounding box. However, in the case of paintings we cannot assume that all the bounding boxes around the objects are given.\nIntegration of Features and Metrics We investigated the performance of different metric learning approaches and visual features individually. In the next step, we find out the best performance for aforementioned classification tasks by combining different visual features. Toward this goal, we followed two strategies. First, for a given metric, we project visual features by applying the metric and concatenate these projected visual features together. Second, we fixed the type of visual feature that we use and project it with the application of different metrics and concatenate these projections all together. Having this larger feature vectors (either of two strategies), we train SVM classifiers for three tasks of Style, Genre and Artist classification. Table 6 shows the results of these experiments where we followed the earlier strategy and table 5 shows the results of the later case. In general we get better results by fixing the metric and concatenating the projected feature vectors (first strategy).\nThe work of Bar et al [4] is the most similar to ours and we compare our final results of these experiments with their reported performance. [4] only performed the task\nof style classification on half of the images in our dataset and achieved the accuracy of 43% by using two variations of PiCoDes features and two layers of CNN. However we outperform their approach by achieving 45.97 % accuracy for the task of style classification when we used LMNN metric to project GIST, Classeme, PiCoDes and CNN features and concatenate them all together as it is reported in the third column of table 6.\nOur contribution goes beyond outperforming state-of-the-art by learning a more compact feature representation. In this work, our best performance for style classification happens when we concatenate four 100-dimensional feature vectors. This results in a 400 dimensional feature vectors that we train SVM classifiers on top of them. However [4] extract a 3882 dimensional feature vector to their best reported performance. As a result we not only outperform the state-of-the-art, but presented a better image representation that reduces the amount of space by 90%. Our efficient feature vector is an extremely useful image representation that gains the best classification accuracy and we consider its application for the task of image retrieval as future work.\nTo qualitatively evaluate extracted visual features and learned metrics, we did a prototype image search task. As the feature fusion with application of LMNN metric gives the best performance for style classification, we used this setting as our similarity measurement model. Figure 8 shows some sample output of this image search task. For each pair, the image on the left is the query image, which we find the closest match(image on the right) to it based on LMNN and feature fusion. However we force the system to pick the closest match that does not belong to the same style as the query image. This verifies that although we learn the metric based on style labels, the learned projection can find similarity across styles."}, {"heading": "5 Conclusion and Future Works", "text": "In this paper we investigated the applicability of metric learning approaches and performance of different visual features for learning similarity in a collection of fine-art paintings. We implemented meaningful metrics for measuring similarity between paintings. These metrics are learned in a supervised manner to put paintings from one concept close to each other and far from others. In this work we used three concepts: Style, Genre and Artist. We used these learned metrics to transform raw visual features into another space that we can significantly improve the performance for three important tasks of Style, Genre and Artist classification. We conducted our comparative experiments on the largest publicly available dataset of fine-art paintings to evaluate the performance for the aforementioned tasks.\nWe conclude that:\n\u2013 Classeme features show the superior performance for all three tasks of Style, Genre or Artist classification. This superior performance is independent of the type of metric that has been learned. \u2013 In the case of working on individual type of visual features, Boost metric and Information Theoretic Metric Learning(ITML) approaches improve the accuracy of classification tasks across all features. \u2013 For the case of using different types of features all together(feature fusion), LargeMargin Nearest-Neighbor(LMNN) metric learning achieves the best performance for all classification experiments. \u2013 By learning LMNN metric on Classeme features, we find an optimized representation that not only outperforms state-of-the art for the task of style classification, but reduce the size of feature vector by 90%.\nWe consider verification of applicability of this representation for the task of image retrieval and recommendation systems as future work. As other future works we would like to learn metrics based on other annotation(e.g. time period)."}], "references": [{"title": "Csift: A sift descriptor with color invariant characteristics", "author": ["A.E. Abdel-Hakim", "A.A. Farag"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Visual thinking", "author": ["R. Arnheim"], "venue": "Univ of California Press,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1969}, {"title": "Towards automated classification of fine-art painting style: A comparative study", "author": ["R.S. Arora", "A.M. Elgammal"], "venue": "ICPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification of artistic styles using binarized features derived from a deep neural network", "author": ["Y. Bar", "N. Levy", "L. Wolf"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Computer Vision and Image Analysis of Art: Proceedings of the SPIE Electronic Imaging Symposium, San Jose Convention Center, 18-22 January 2010", "author": ["A. Bentkowska-Kafel", "J. Coddington"], "venue": "PROCEEDINGS OF SPIE.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic extraction of brushstroke orientation from paintings", "author": ["I.E. Berezhnoy", "E.O. Postma", "H.J. van den Herik"], "venue": "Machine Vision and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Classemes and other classifier-based features for efficient object categorization", "author": ["A. Bergamo", "L. Torresani"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, page 1,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Picodes: Learning a compact code for novel-category recognition", "author": ["A. Bergamo", "L. Torresani", "A.W. Fitzgibbon"], "venue": "Advances in Neural Information Processing Systems, pages 2088\u20132096,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Artistic image classification: An analysis on the printart database", "author": ["G. Carneiro", "N.P. da Silva", "A.D. Bue", "J.P. Costeira"], "venue": "In ECCV,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "International Conference on Computer Vision & Pattern Recognition, volume 2, pages 886\u2013893, June", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Foundations of Art and Design", "author": ["L. Fichner-Rathus"], "venue": "Clark Baxter,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "NIPS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Image processing for artist identification", "author": ["C.R. Johnson", "E. Hendriks", "I.J. Berezhnoy", "E. Brevdo", "S.M. Hughes", "I. Daubechies", "J. Li", "E. Postma", "J.Z. Wang"], "venue": "Signal Processing Magazine, IEEE, 25(4):37\u201348,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Studying digital imagery of ancient paintings by mixtures of stochastic models", "author": ["J. Li", "J.Z. Wang"], "venue": "Image Processing, IEEE Transactions on, 13(3):340\u2013353,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Rhythmic brushstrokes distinguish van gogh from his contemporaries: Findings via automated brushstroke extraction", "author": ["J. Li", "L. Yao", "E. Hendriks", "J.Z. Wang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "The classification of style in fine-art painting", "author": ["T.E. Lombardi"], "venue": "ETD Collection for Pace University. Paper AAI3189084.,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A digital technique for art authentication", "author": ["S. Lyu", "D. Rockmore", "H. Farid"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 101(49):17006\u201317010,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Detection of forgery in paintings using supervised learning", "author": ["G. Polatkan", "S. Jafarpour", "A. Brasoveanu", "S. Hughes", "I. Daubechies"], "venue": "16th IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical classification of paintings using face- and brush stroke models", "author": ["R. Sablatnig", "P. Kammerer", "E. Zolda"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Knowledge discovery of artistic influences: A metric learning approach", "author": ["B. Saleh", "K. Abe", "A. Elgammal"], "venue": "ICCC,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Computer vision and computer graphics analysis of paintings and drawings: An introduction to the literature", "author": ["D.G. Stork"], "venue": "Computer Analysis of Images and Patterns, pages 9\u201324. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "ECCV,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "CoRR, abs/1412.4564,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Metric learning for kernel regression", "author": ["K. Weinberger", "G. Tesauro"], "venue": "Eleventh international conference on artificial intelligence and statistics, pages 608\u2013615,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "JMLR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Learning and judging such complex visual concepts is an impressive ability of human perception [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "To this might be added physical attributes, like brush strokes as well as subject matter and other descriptive concepts [13].", "startOffset": 120, "endOffset": 124}, {"referenceID": 17, "context": "[19, 20]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[19, 20]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "We refer the reader to [28, 5] for comprehensive surveys on this subject.", "startOffset": 23, "endOffset": 30}, {"referenceID": 4, "context": "We refer the reader to [28, 5] for comprehensive surveys on this subject.", "startOffset": 23, "endOffset": 30}, {"referenceID": 18, "context": "For example Lombardi [20] has presented a study of the performance of these types of features for the task of artist classification among a small set of artists using several supervised and unsupervised learning methodologies.", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "[25, 18, 22, 15, 6, 19]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "[25, 18, 22, 15, 6, 19]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "[25, 18, 22, 15, 6, 19]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "[25, 18, 22, 15, 6, 19]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "[25, 18, 22, 15, 6, 19]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "[25, 18, 22, 15, 6, 19]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Researchers also investigated the use of features based on local edge orientation histograms, such as SIFT [21] and HOG [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "Researchers also investigated the use of features based on local edge orientation histograms, such as SIFT [21] and HOG [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "[3] presented a comparative study for the task of style classification, which evaluated low-level features, such as SIFT and Color SIFT [1], versus semanticlevel features, namely Classemes [29], which encodes object presence in the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[3] presented a comparative study for the task of style classification, which evaluated low-level features, such as SIFT and Color SIFT [1], versus semanticlevel features, namely Classemes [29], which encodes object presence in the image.", "startOffset": 136, "endOffset": 139}, {"referenceID": 27, "context": "[3] presented a comparative study for the task of style classification, which evaluated low-level features, such as SIFT and Color SIFT [1], versus semanticlevel features, namely Classemes [29], which encodes object presence in the image.", "startOffset": 189, "endOffset": 193}, {"referenceID": 8, "context": "Carneiro et al [9] also concluded that low-level texture and color features are not effective because of inconsistent color and texture patterns that describe the visual classes in paintings.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "More recently, Saleh et al [26] used metric learning approaches for finding influence paths between painters based on their paintings.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "Moreover, The dataset of [26] has only 1710 images from 66 artists, while we conducted our experiments on 81,449 images painted by 1119 artists.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "Bar et al [4] proposed an approach for style classification based on features obtained from a convolution neural network pre-trained on an image categorization task.", "startOffset": 10, "endOffset": 13}, {"referenceID": 24, "context": ") Previous work [26, 9] used different resources and made smaller collections with limited variability in terms of style, genre and artists.", "startOffset": 16, "endOffset": 23}, {"referenceID": 8, "context": ") Previous work [26, 9] used different resources and made smaller collections with limited variability in terms of style, genre and artists.", "startOffset": 16, "endOffset": 23}, {"referenceID": 3, "context": "The work of [4] is the closest to our work in terms of data collection procedure, but the number of images in their collection is half of ours.", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "For this work, we investigated some state-of-the-art representatives of these two categories: Low-level Features: On one hand, in order to capture low-level visual information we extracted GIST features [23], which are holistic features that are designed for scene categorization.", "startOffset": 203, "endOffset": 207}, {"referenceID": 27, "context": "Learned Semantic-level Features: On the other hand, for the purpose of semantic representation of the images, we extracted three object-based representation of the images: Classeme [29], Picodes [8], and CNN-based features [16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "Learned Semantic-level Features: On the other hand, for the purpose of semantic representation of the images, we extracted three object-based representation of the images: Classeme [29], Picodes [8], and CNN-based features [16].", "startOffset": 195, "endOffset": 198}, {"referenceID": 14, "context": "Learned Semantic-level Features: On the other hand, for the purpose of semantic representation of the images, we extracted three object-based representation of the images: Classeme [29], Picodes [8], and CNN-based features [16].", "startOffset": 223, "endOffset": 227}, {"referenceID": 6, "context": "We followed the implementation of [7] and for each image extracted a 2659 dimensional real-valued Classeme feature vector and a 2048 dimensional binary-value Picodes feature.", "startOffset": 34, "endOffset": 37}, {"referenceID": 15, "context": "Convolutional Neural Networks(CNN) [17] showed a remarkable performance for the task of large-scale image categorization [16].", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "Convolutional Neural Networks(CNN) [17] showed a remarkable performance for the task of large-scale image categorization [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "Bar et al [4] showed that a combination of the", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "Following this observation we used the last layer of a pre-trained CNN [16] (1000 dimensional real-valued vectors) as another feature vector.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Neighborhood Component Analysis (NCA) The objective function of NCA [14] is related to analyzing the nearest neighbors.", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "Large Margin Nearest Neighbors (LMNN) LMNN [32] is an approach for learning a Mahalanobis distance, which is widely used because of its global optimum solution and superior performance in practice.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Due to the popularity of LMNN, different variations of it have been introduced, including a non-linear version called gb-LMNN [32] which we used in our experiments", "startOffset": 126, "endOffset": 130}, {"referenceID": 25, "context": "Shen et al [27] use this fact and instead of learning M , finds a set of weaker metrics that can be combined and give the final metric.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "Davis et al [11] introduce the measure of LogDet divergence regularization between two matrices M,M \u2032(can be interpreted as metrics).", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "Metric Learning for Kernel Regression (MLKR) Similar to NCA objective function, which minimizes the classification error; Weinberger and Tesauro [31] learn a metric by optimizing the leave-one-out error for the task of kernel regression.", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "We followed the original implementation of Oliva and Torralba [23] to get a 512 dimensional feature vector.", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "For Classeme and Picodes we used the implementation of Bergamo et al [29], resulting in 2659 dimensional Classeme features and 2048 dimensional Picodes features.", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "We used the implementation of Vedaldi and Lenc [30] to extract 1000 dimensional feature vectors of the last layer of CNN.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "Metric Learning We used implementation of [32] to learn LMNN metric(both version of linear and non-linear) and MLKR 5.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "For the BoostMetric we slightly adjusted the implementation of [27].", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "The work of Bar et al [4] is the most similar to ours and we compare our final results of these experiments with their reported performance.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "[4] only performed the task", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "However [4] extract a 3882 dimensional feature vector to their best reported performance.", "startOffset": 8, "endOffset": 11}], "year": 2015, "abstractText": "In the past few years, the number of fine-art collections that are digitized and publicly available has been growing rapidly. With the availability of such large collections of digitized artworks comes the need to develop multimedia systems to archive and retrieve this pool of data. Measuring the visual similarity between artistic items is an essential step for such multimedia systems, which can benefit more high-level multimedia tasks. In order to model this similarity between paintings, we should extract the appropriate visual features for paintings and find out the best approach to learn the similarity metric based on these features. We investigate a comprehensive list of visual features and metric learning approaches to learn an optimized similarity measure between paintings. We develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting\u2019s style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks.", "creator": "LaTeX with hyperref package"}}}