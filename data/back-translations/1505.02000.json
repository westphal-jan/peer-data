{"id": "1505.02000", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2015", "title": "Deep Learning for Medical Image Segmentation", "abstract": "This report provides an overview of the current state of the art in deep learning architectures and optimization techniques, using the ADNI hippocampus MRI dataset as an example of comparing the effectiveness and efficiency of various convolutionary architectures to solve patch-based 3-dimensional hippocampus segmentation, which is important for diagnosing Alzheimer's disease. We found that a slightly unconventional \"stacked 2D\" approach provides much better classification performance than simple 2D patches without requiring much more computing power. We also examined the popular \"triplanar\" approach used in some recently published studies, and found that it delivers much better results than the 2D approach, but also has a moderate increase in computing power demand. Finally, we examined a full 3D architecture of the revolutionary architecture and found that it delivers slightly better results than the triplanar approach, but at the cost of significantly increasing computing performance.", "histories": [["v1", "Fri, 8 May 2015 11:35:53 GMT  (271kb,D)", "http://arxiv.org/abs/1505.02000v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["matthew lai"], "accepted": false, "id": "1505.02000"}, "pdf": {"name": "1505.02000.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Medical Image Segmentation", "authors": ["Matthew Lai"], "emails": [], "sections": [{"heading": null, "text": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3- dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer\u2019s Disease. We found that a slightly unconventional \u201dstacked 2D\u201d approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \u201dtri-planar\u201d approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement. ar X\niv :1\n50 5.\nContents"}, {"heading": "1 Introduction 5", "text": ""}, {"heading": "2 Traditional Neural Networks 5", "text": "2.1 Network Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Network Nodes and Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Training Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.4 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"}, {"heading": "3 Deep Learning 9", "text": "3.1 Why Build Deep Networks? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Vanishing Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2.1 Solution 1: Layer-wise Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.2 Solution 2: Rectified Linear Activation Units . . . . . . . . . . . . . . . . . . . 10\n3.3 DropOut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.4 Model Compression and Training Set Augmentation . . . . . . . . . . . . . . . . . . . 12\n3.5 Making Deep Nets Shallow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.6 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"}, {"heading": "4 Hippocampus Segmentation 14", "text": "4.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.1.1 Pre-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.1.2 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.1.3 Convolutional Method 1: Stacked 2D Patches . . . . . . . . . . . . . . . . . . . 15\n4.1.4 Convolutional Method 2: Tri-planar Patches . . . . . . . . . . . . . . . . . . . 16\n4.1.5 Convolutional Method 3: 3D Patches . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.1.6 Image Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.1.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.1.8 Post-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5 Conclusion and Future Work 19\nList of Tables\n1 Performance using different numbers of layers . . . . . . . . . . . . . . . . . . . . . . . 18\n2 Performance using different 2D patch sizes . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3 Performance using different tri-planar patch sizes . . . . . . . . . . . . . . . . . . . . . 18\n4 Performance using different 3D patch sizes . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5 Multiple runs of best configurations with random initialization (2D 24x24 7 layers, tri-planar 24x24, 3D 20x20x20) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nList of Figures\n1 Activation functions - red: ReLU, blue: tanh, green: logistic . . . . . . . . . . . . . . . 11\n2 Example of different norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 Hippocampus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4 Before and after post-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "1 Introduction", "text": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4]. In many of these applications, algorithms based on deep learning has surpassed the previous state-of-art performance. At the heart of all deep learning algorithms is the domain-independent idea of using hierarchical layers of learned abstraction to efficiently accomplish high level tasks [1].\nThis report first provide an overview of traditional artificial neural network concepts introduced in the 1980s, before introducing more recent discoveries that made training deep networks practical and effective. Finally, we present the results of applying multiple deep architectures to the ADNI hippocampus segmentation problem, and comparing their classification performances and computational power requirements."}, {"heading": "2 Traditional Neural Networks", "text": "Artificial neural networks (ANN) are a machine learning technique inspired by and loosely based on biological neural networks (BNN). While they are similar in the sense that they both use a large number of identical and linked simple computational units to achieve high performance on complex tasks, modern ANNs have been so heavily optimized for efficient implementation on electronic computers that they bear little resemblance to their biological counterpart. In particular, time-dependent integrateand-fire mechanism in BNNs have been replaced by steady state values representing frequency of firing, and most ANNs also have vastly simplified connection architectures that allow for efficient propagation. Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]). Unless otherwise specified, all further references to neural networks (NN) in this report refer to artificial neural networks."}, {"heading": "2.1 Network Architectures", "text": "In a typical neural network, nodes are placed in layers, with the first layer being the input layer, and the last layer being the output layer. The input nodes are special in that their outputs are simply the value of the corresponding features in the input vector.\nFor example, in a classification task that has a 3-dimensional input (x, y, z) and a binary output, one possible network design is to have 3 input nodes, and 1 output node. The input and output layers are usually considered fixed in network design.\nWith only an input layer and an output layer, with all input nodes connected to all output nodes, the network essentially implements a matrix multiply, or a linear transformation. This type of networks can solve simple problems where the feature space is linearly-separable. However, for linearly-separable problems, simpler techniques such as linear regression or logistic regression can usually achieve similar performance, with the only difference being training methods.\nMost modern applications of neural networks use one or more hidden layers \u2013 layers that sit between the input layer and the output layers, to allow the network to model non-linearity in the feature space. The number of hidden layers and the number of hidden nodes in each layer are hyper-parameters that are not always easy to determine. While some rules-of-thumb have been proposed, they are still, for the most part, determined by trial-and-error. The risk of using too-small a network is that it may not have enough representative power to model all useful patterns in the input (high bias), while the risk\nof using too-large a network is that it may overfit the data, and start modeling noise in the training set (high variance). It is usually better to err on the side of larger networks, because many effective techniques exist to combat overfitting, as will be detailed in later sections of the report. Using network size to limit overfitting is error-prone, time-consuming, and not very effective.\nMethods have been proposed for automatic hyperparameter tuning, such as evolving cascade networks [9], which trains a large network through an iterative process, by first starting with a minimal network, and in each iteration, train a few \u201dcandidate\u201d networks that have more nodes in different layers, and keeping the best. There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8]. However, these methods have not seen widespread adoption, due to the large increase in training time, and marginal benefits when overfitting is avoided using other methods than limiting network size.\nIt has been proven that a network with 1 hidden layer can approximate any continuous (in feature space) function to any accuracy, and a network with 2 hidden layers can approximate any function to any accuracy [10, 11]. Given infinite computational power, memory, and training set, there is theoretically no reason to go above 2 hidden layers. However, as will be explained later in the report, it is much more efficient to solve complex problems using a deeper network than one with only 2 hidden layers."}, {"heading": "2.2 Network Nodes and Activation Functions", "text": "In a neural network, each node (besides input nodes) has one or more scalar inputs, and one output. Each link between nodes have a scalar weight, and each node has a bias to shift the point of activation.\nf( \u2211 wi \u2217 xi + b) (1)\nThe output of each node is computed as shown in Equation 1, where xi\u2019s are inputs to the node, wi\u2019s are the weights of the associated link, b is a bias associated with the node, and f(x) is a function associated with the node, known as the activation function.\nThere are a few activation functions in widespread use. For output nodes in regression networks, a linear activation function (eg. y = x) is most commonly used to give these networks a range of all real numbers. For output nodes in classification networks, the softmax function (exponential normalization) is often used to transform the outputs into something that can be interpreted as a probability distribution.\nFor hidden nodes, the traditional choices are hyperbolic tangent (y = tanh(x)) and the logistic function (y = 1\n1+e\u2212k(x\u2212x0) ). Both these functions are designed to satisfy 3 conditions -\n\u2022 Differentiable everywhere\n\u2022 Monotonic\n\u2022 Non-linear\nIt was believed that these properties are essential for an activation function.\nThe differentiability property is important because we must be able to take the derivative of the function at any point during training using gradient-based methods. It is not necessary for networks trained using non-gradient-based methods such as genetic algorithm.\nThe monotonicity property is important because if the activation function is not monotonic, it will introduce additional local minimums in the parameter space, and impede training efforts.\nNon-linearity is important because otherwise the network will lose the ability to model non-linear patterns in the training set. Non-linearity is achieved using saturation in this case, with the hyperbolic tangent function saturating at y = \u22121 and y = 1, and the logistic function saturating at y = 0 and y = 1.\nIn practice, although the logistic function is more biologically plausible, hyperbolic tangent usually allows faster training since being linear around 0 means nodes will not start training in saturation (which would make training much slower) even if inputs are zero or negative [12]."}, {"heading": "2.3 Training Neural Networks", "text": "Training algorithms for neural networks fall into two major categories - gradient-based and nongradient-based. This report focuses on gradient-based methods as it is much more commonly used in recent times, and usually converges much faster as well.\nAs mentioned in Section 2.2, each node in the network has a weight associated with each incoming link, and a scalar bias. The weight and bias of a node are the parameters of the node. If we concatenate the weights and biases of all nodes in a network into one vector \u03b8, it completely defines the behaviour of a network (for a given set of hyper-parameters, ie. network architecture).\ny = f(\u03b8, x) (2)\nIf the set of hyper-parameters (network architecture) is encoded into a function f(), we can define the output of the network as shown in Equation 2, where y and x are the output and input vectors respectively.\nThe goal of the training process, therefore, is to find a \u03b8 such that f(\u03b8, x) approximates the function we are trying to model. In other words, given a set of inputs and their desired outputs, we are trying to find a \u03b8 that minimizes the difference between the desired outputs and network outputs, for all entries in the training set. For that, a measurement of error is needed.\nE(\u03b8, Ts) = 1\n2 \u2211 (xi,yi)\u2208Ts (yi \u2212 f(\u03b8, xi))2 (3)\nOne such error measure is mean-squared-error (MSE), and it is the most commonly used error measure. This is given in Equation 3, where Ts is the training set, xi and yi are the input and desired output of a training pair, N is the number of entries in the training set, and g(xi) is the network output.\nOur goal is to minimize E(\u03b8, Ts) given Ts. For very small networks, it may be feasible to do an exhaustive search to find the point in parameter space where the mean-squared-error is minimized, but for networks of reasonable sizes, an exhaustive search is not practical. Gradient descent is the most commonly used optimisation algorithm for neural networks.\nIn gradient descent, \u03b8 is first initialized to a random point in the parameter space. Weights and biases are typically drawn in a way that keeps most nodes in the linear region at the beginning of training. One popular method is to draw from the uniform distribution (\u2212 a\u221a\ndin , a\u221a din ), where a is chosen based\non the shape of the activation function (where it starts to saturate), and din is the number of inputs to the node [13].\n\u2206\u03b8 = \u2212L\u2202E(\u03b8, Ts) \u2202\u03b8\n(4)\nAfter initialization, the gradient descent algorithm performs a walk in the parameter space, guided by the gradient of the error surface. In its simplest form, in each iteration, the algorithm takes a step in the opposite direction of the gradient, with the step size proportional to the magnitude of the gradient, and a fixed learning rate. This is shown in Equation 4, where L is the learning rate, and all other symbols are as previously defined.\n\u2202E(\u03b8, Ts) \u2202wkj = \u2202E \u2202yj \u2202yj \u2202xj \u2202xj \u2202wkj\n(5)\nThe partial derivative of the error function for each parameter is as shown in Equation 5 after applying chain rule, where xj is the result of the summation (input to activation function), yj is the output of the activation function, and wkj is the weight we are examining.\nThe only difference between the output layer and hidden layers is that hidden layers do not really have an error. However, we can still derive error terms for them by calculating its contribution to the input to the activation function of the next node. This is equivalent to another application of chain rule, since the input to the activation function is simply the sum of the contributions from each node in the previous layer. It is convenient to do this propagation backwards, from the final error, multiplying by the derivative of the activation function each time, before \u201dassigning blame\u201d of the error proportionally to the weights connecting previous layer nodes to the node we are examining. This is the basis of back-propagation, and this is why we require the activation function to be differentiable. A detailed derivation is omitted here for brevity.\nMany variants of gradient descent have been proposed, each with different performance characteristics. The one in most popular use is gradient descent with momentum, where instead of calculating the gradient each time and use that as the step, we combine it with a fraction of the weight update of the previous iteration. This allows faster convergence in situations where the gradient is much larger in some dimensions than others (eg. down the bottom of a valley) [14].\nAnother common variation is learning rate scheduling - changing the learning rate as training progresses. The goal is to get to somewhere close to a local minimum quickly, then slow down to avoid overshooting. This idea is taken further in resilient back-propagation (RPROP), where only the sign of the gradient is used. In RPROP, each weight has an independent learning rate, that is increased (usually multiplied by 1.2) if the sign of the gradient has not changed from the previous iteration, and reduced (usually by a factor of 0.5) if the gradient has changed signs [15]. This allows all weights to train at close to their optimal learning rate, and eliminates the learning rate parameter that must be manually tuned in other gradient descent variants. An initial learning rate still needs to be chosen, but it doesn\u2019t significantly affect training time or result [15]."}, {"heading": "2.4 Regularization", "text": "When the training set is limited in size as it is usually, it is dangerous to train without any constraint since the network will eventually start to model noise in the training set, and be too specialized to generalize beyond the training set.\nE(\u03b8, Ts) = 1\n2 \u2211 (xi,yi)\u2208Ts (yi \u2212 f(\u03b8, xi))2 + \u03bb|\u03b8|2 (6)\nOne popular way to combat overfitting is regularization - the idea of encouraging weights to have smaller values. The most common form of regularization is L2 regularization, where the L2 norm of \u03b8 (the parameter vector) is added to the error function, as shown in Equation 6, where \u03bb is a parameter that controls the strength of regularization, and it needs to be tuned. If \u03bb is too low, the network would overfit. If \u03bb is too high, the network would underfit.\nOther norms such as L1 and L12 are also used, with different effects. They will be explored in the discussion of deep networks later in the report."}, {"heading": "3 Deep Learning", "text": ""}, {"heading": "3.1 Why Build Deep Networks?", "text": "As mentioned earlier in Section 2.1, a neural network with 2 hidden layers is already theoretically a universal function approximator capable of approximating any function, continuous or not, to any arbitrary accuracy. In light of that, it may seem pointless to pursue networks with more hidden layers.\nThe main benefit of using deep networks is node-efficiency - it is often possible to approximate complex functions to the same accuracy using a deeper network with much fewer total nodes compared to a 2-hidden-layer network with very large hidden layers. Besides computational benefits, a model with a smaller degree of freedom (number of parameters) requires a smaller dataset to train [16], and size of the training set is often a limiting factor in neural network training.\nIntuitively, the reason for a smaller and deeper network to be more effective than an equally sized (in total nodes) shallower network is that a deep network reduces the amount of redundant work done.\nAs an example, suppose we are given bitmap images containing triangles, rectangles, pentagons, and hexagons, and our task is to count the number of each shape in each image. In the case of a deep network, the first layers can perform the low level task of calculating image gradients and identifying lines in the image, and transform it to a more convenient form. Higher layers can then perform classification and counting from the simpler representation.\nOn the other hand, if we have a shallow network, the low level tasks would have to be performed multiple times, since there is little cross-feeding of intermediate results. This would result in a lot of redundant work done.\nWith very deep networks, it is possible to model functions that work with many layers of abstraction - for example, classifying the gender of images of faces, or the breed of dogs. It is not practical to perform these tasks using shallow networks, because the redundant work done is exponential to the number of layers, and an equivalent shallow network would require exponentially more computational power, and exponentially larger training sets, neither of which are usually available."}, {"heading": "3.2 Vanishing Gradients", "text": "The fact that deeper networks are more computationally efficient has been known for a very long time, and deep networks have been attempted as early as 1980 [17]. In 1989, LeCun et al. successfully applied a 3-hidden-layer network to ZIP code recognition. However, they were not able to scale to higher number of layers or more complex problems due to very slow training, the reason of which was not understood.\nIn 1991, Hochreiter identified the problem and called it \u201dvanishing gradients\u201d [18, 19]. Essentially,\nwhat\u2019s happening is as errors are propagated back from the output layer, it\u2019s multiplied by derivatives of the activation function at the point of activation. As soon as the propagation gets to a node in saturation (where derivative is close to 0), the error is reduced to the level of noise, and nodes behind the saturated node train extremely slowly. No effective solution to the problem was found for many years.\nResearchers continued to try to build deep networks, but with often disappointing performance."}, {"heading": "3.2.1 Solution 1: Layer-wise Pre-training", "text": "In 2007, Hinton proposed a solution to the problem, and started the current wave of renewed interest in deep learning. The idea is to first train each layer in an unsupervised and greedy fashion, to try to identify application-independent features from each layer, before finally training the entire network on labeled data [20].\nThe way he implemented the idea is with a generative model, in the form of a restricted Boltzmann machine (RBM), where each layer contains a set of binary variables with associated probability distributions, and each layer is trained to predict the previous layer using an algorithm known as contrastive divergence [20].\nAnother idea in the same vein is to use a neural network to reproduce the original input [21], which allows the reuse of all the neural network techniques that have already been developed (unlike for RBM).\nThe idea is to first start with just the input layer and one hidden layer, and train the network (using standard back-propagation gradient descent) to produce the original input, essentially training the network to model the identity function [21]. If the hidden layer has fewer nodes than the input layer, the output of the hidden layer is then a compressed and more abstract representation of the input. This process is repeated to train each hidden layer, with the output of the previous layer as the input [21]. The final network can then be trained with the actual output layer using standard back-propagation from labeled data. This works around the vanishing gradient problem because when the final back-propagation is performed, most of the earlier layers are already trained to provide application-independent abstractions.\nIn modern implementations, some artificial noise is often injected into the input of autoencoders to encourage robustness in the autoencoders, by testing their ability to reconstruct clean input from partially-corrupted input [21]. These autoencoders are known as denoising autoencoders. Denoising autoencoders perform similarly to systems based on RBMs and their stacked variant - Deep Belief Networks (DBN) [21]."}, {"heading": "3.2.2 Solution 2: Rectified Linear Activation Units", "text": "In 2011, a simpler solution to the vanishing gradients problem was proposed by Glorot et al. Their solution to the vanishing gradients problem is to simply use an activation function that does not reduce the error as it\u2019s propagated back [12].\nThe proposed function is the rectified linear activation function (ReLU), y = max(0, x) [12]. See Figure 1 for a comparison of the three activation functions.\nIn the activated region (x > 0), the derivative is 1, and in the deactivated region (x <= 0), the derivative is 0. It doesn\u2019t change error signals as they are passed through.\nAt first glance it seems like a strange choice for an activation function, for 2 reasons -\n\u2022 It is not differentiable at zero\n\u2022 It is unbounded on the positive side\nThe non-differentiability at zero is inconsequential, since neural networks work in real numbers, it is highly unlikely that it will be at exactly x = 0 at any time. In practice, we define the derivative at zero to either be 0 or 1, and it doesn\u2019t have any real world effect.\nThe unbounded nature of the function on the positive side is more problematic, because they can result in very large activations in later layers, which may cause numerical problems. This can be solved using L1 regularization, which not only limits the magnitude of weights, but also enforces sparsity [12].\nA sparse network is one where for any given input, only a small subset of nodes will be activated. The fact that L1 and L12 regularization encourage sparsity is in contrast to L2 regularization which discourages sparsity. L2 regularization encourages the desired output to be constructed from many small weights (which would have a lower L2 norm) instead of one large weight, which would have a higher L2 norm, but equal L1 norm, and lower L12 norm, as shown in Figure 2. In essence, L1 and L 1 2 regularization encourage nodes to be independent from one another, and not co-evolve with others. This can result in more efficient usage of available nodes, and also have computational advantages in networks using activation functions with a hard 0 saturation (such as the rectified linear activation) [12], since if a node\u2019s output is 0, it does not need to be broadcasted to nodes in the next layer.\nWith rectified linear activation, L1 regularization, and no pre-training, Glorot et al. reports achieving slightly superior or similar performance to previous results based on pre-training [12].\nOne possible reason for the increase in performance is more efficient use of nodes. With a pre-training\nsystem, nodes are trained to extract all discriminating patterns from the input data, even if some of those patterns are irrelevant for the task at hand. Without pre-training, those irrelevant patterns would not be encoded, thus freeing up the nodes for more important patterns for the task.\nHowever, he also identified one situation where pre-training is still advantageous - for semi-supervised problems. In a semi-supervised problem, a large training set is available, but only a small subset is labeled. In this case, starting with pre-training using the entire dataset before training the whole network with the labeled subset can result in a network that generalizes better [12].\nFor purely-supervised problems, most researchers have abandoned the pre-training idea, and adopted ReLU + L1 regularization instead.\nThere is not sufficient research to decide whether ReLU is also superior to traditional activation functions in shallower networks. Most shallow networks still use hyperbolic tangent or logistic activation, because they are not affected by the vanishing gradient problem."}, {"heading": "3.3 DropOut", "text": "In 2012, Hinton et al. proposed a technique that greatly improves network performance in cases where there is limited training data [22]. The idea stems from the observation that when the training set is small, there will be many possible models that will perform well on the training set, but only some will perform well on the testing set.\nThe traditional solution to this is to train an ensemble of networks from different initialization and/or subsets of the training set, then combine their outputs to produce the final output (often by averaging, or voting) [23]. This approach is known to work well in improving model performance, but it is often not computationally feasible in deep learning, where training a single network can take many hours or days even on a fast GPU. Many trained neural networks are also used in real-time applications, and using an ensemble would make the model many times slower to evaluate.\nHinton et al.\u2019s proposal, termed \u201dDropOut\u201d, is to randomly de-activate 50% of the nodes of a network on each training iteration. A disabled node would not participate in forward propagation (where they would output 0), and would block any error signal from propagating through the node during backpropagation. When training is done, all nodes are re-enabled, but all weights are halved to maintain the same output range [22].\nThey were able to achieve similar results as ensembles of large numbers of networks, with only about 2\u00d7 the computational power requirement of a single network. They hypothesized that this is because disabling 50% of nodes at random on each iteration forces nodes to independently evolve, instead of co-evolving with others. Co-evolution is not optimal because, for example, some nodes can evolve to correct mistakes made by other nodes, instead of modeling useful patterns. In DropOut, nodes cannot assume other nodes exist, and are forced to be \u201dindependently useful\u201d [22]."}, {"heading": "3.4 Model Compression and Training Set Augmentation", "text": "As mentioned above in Section 3.3, ensembles of networks often achieve higher performance than any constituent network, but are computationally infeasible in most real-world applications. This is especially true for problems with small labeled datasets, where single networks are likely to overfit.\nBucilua et al. proposed that for semi-supervised problems, where a large dataset is available but only a small subset is labeled, it may be beneficial to train an ensemble (or another slow and accurate model) on the labeled subset, and use it to label all data, before finally training a single network (or\nanother fast model) on the entire dataset, to reproduce both the original labels and the labels added by the ensemble [24].\nIn applications where there is a shortage of both labeled and unlabeled data, the training set can be artificially augmented. Training set augmentation is application-specific. For example, in computer vision applications where the network should be translationally and rotationally invariant, additional training entries can be formed by translating or rotating images from the original training set [25]."}, {"heading": "3.5 Making Deep Nets Shallow", "text": "Continuing on the theme of model compression, in 2014, Ba and Caruana showed that with the help of a high performance deep network, a shallow network can be trained to perform much better than a similar shallow network that is trained directly on the training set [26].\nTheir algorithm first trains a deep net (or an ensemble of deep nets) on the original training set, and use it to provide \u201dextended labels\u201d for all entries in the training set [26]. In case of classification problems where the output layer is often softmax, the \u201dextended labels\u201d are inputs to the softmax layer [26]. The inputs to the softmax layer are log probabilities of each class.\nFinally, a shallow network can be trained to predict the log probabilities, instead of the original single-class label. This makes training much easier for the shallow network, because multi-class log probabilities labels provide much more information than a single-class label. By modelling the log probabilities, the shallow network is also mimic-ing how the deep net (or ensemble) will generalize to unseen data, which mostly depend on the relative values of log probabilities for classes that are not the highest [26]. The performances of these shallow networks are much higher than networks trained on the single-class labels.\nThis result is significant because it proves that the reason why shallow networks perform worse than deep networks is not entirely due to the increase in representative power and flexibility of deep networks. It is also due to our current training algorithms being sub-optimal for shallow networks, and if we can develop better training algorithms, we can potentially significantly improve the performance of shallow networks.\nHowever, the performance of these mimic-ing shallow networks are still not quite as good as the deep networks or ensembles they are mimic-ing [26]. Therefore, the option of creating a mimic-ing shallow network allows a tradeoff to be made between accuracy and speed."}, {"heading": "3.6 Convolutional Neural Networks", "text": "Convolutional neural networks are a neural network architecture that uses extensive weight-sharing to reduce the degrees of freedom of models that operate on features that are spatially-correlated [17]. This includes 2D and 3D images (and 2D videos, which can be seen as 3D images), but it has also very recently been successfully applied to natural language processing [27].\nConvolutional neural networks are inspired by the observation that for inputs like images (with each pixel being an input dimension), many low level operations are local, and they are not positiondependent [17]. For example, one operation that is useful in many computer vision applications is edge detection. In a fully-connected deep network, the edge detector would have to be separately trained for each part of the image, even though they would most likely all arrive at similar results. It would be better if a kernel can be trained to do edge detection for the entire image at the same time.\nIn its current iteration, convolutional neural networks are composed of 3 different types of layers -\nconvolutional, max-pooling, and fully-connected [2]. One typical arrangement is alternating between convolutional and maxpooling layers, before finishing off with 2 fully-connected hidden layers.\nEach convolutional layer has the same dimensions as the input, but each pixel is only activated by a region of pixels centered around the pixels at the same location in the input images. The weights are also shared for each output pixel. In effect, each map in a convolutional layer performs a convolution of the input images, with a learned kernel.\nMax-pooling layers perform downsampling on the images. One typical downsample factor is 2x2 (dividing both width and height by 2). While averaging can also be used, empirical results suggest that downsampling by taking the maximum in each sub-region gives the best performance in most cases [28]. Max-pooling is responsible for summarizing each sub-region, and it gives the network some translational and rotational invariance.\nFully-connected layers are often used as final layers to encode position-dependent information and more global patterns.\nMost existing applications of convolutional neural networks are on 2D images, but the idea can also be extended to 3D, with 3D images and 3D kernels. It can be used to process actual 3D images (eg. MR Images), or videos, using time as the third dimension."}, {"heading": "4 Hippocampus Segmentation", "text": "The hippocampus is a component of human brains responsible for committing short-term episodic and declarative memory into long-term memory, as well as navigation [29]. Hippocampus segmentation is important in the diagnosis of Alzheimer\u2019s disease (AD), as it is one of the components first affected by the disease. A reduction in hippocampal volume can be used as a marker for AD diagnosis [30].\nHumans have 2 hippocampi, shaped like seahorses, as shown in Figure 3. Our goal is to classify each voxel in an MR Image as non-hippocampus, left hippocampus, or right hippocampus. We are using this problem to evaluate different deep learning techniques for patch-based segmentation. All images are labeled by one human expert. Unfortunately, none of the images have been labeled by more than 1 human expert to determine variances in human labeling."}, {"heading": "4.1 Methodology", "text": "We explore 3 convolutional neural network architectures for patch-based segmentation on the ADNI Alzheimer\u2019s MRI dataset [31].\nFor all 3 cases, the pre-processing and post-processing done are identical.\nFor all 3 cases, 60% (120) of the images are used as the training set, 20% (40) as the validation set, and 20% (40) as the testing set. Patches from the same image are always only used in one of the sets."}, {"heading": "4.1.1 Pre-Processing", "text": "Before we begin labeling an image, we first crop it down to a rectangular bounding box, so we can perform masking in normalized coordinates. In case of the ADNI dataset, all images are already in the same orientation, so no rotation is needed.\nFrom going through all images in the training set, we determined that the hippocampi are always in the region (0.42 < x < 0.81, 0.30 < y < 0.67, 0.22 < z < 0.80), relative to each dimension of the bounding box of their respective brains. We enlarged the region by 0.03 on each side, and use (0.39 < x < 0.84, 0.27 < y < 0.70, 0.19 < z < 0.83) as the mask. All voxels outside of the mask are automatically classified as non-hippocampus. All training patches are drawn from within the mask."}, {"heading": "4.1.2 Sampling", "text": "It would be dangerous to draw training voxels uniformly randomly from within the mask, because even within the mask, the vast majority of voxels are non-hippocampus, and hence there would be very few positive samples. Another problem is that edge voxels (voxels at the edges between positive and negative voxels) would be severely under-represented, even though they will most likely be the most difficult voxels to classify.\nTherefore, we draw samples as follows -\n\u2022 For 50% of the samples, we keep drawing randomly until we get a voxel where the 5x5x5 bounding box around the voxel contains more than 1 class\n\u2022 For 25% of the samples, we keep drawing randomly until we get a positive voxel\n\u2022 For the remaining 25% of the samples, we keep drawing randomly until we get a negative voxel\nThis drawing scheme ensures that none of the important types of voxels are under-represented. The biggest downside of this scheme is that it distorts the prior probabilities of each class, possible solutions to which are discussed later in the report."}, {"heading": "4.1.3 Convolutional Method 1: Stacked 2D Patches", "text": "The first method we tried is to use a stack of 2D patches around each voxel we want to sample. For example, for a patch size of 24 and a layer count of 3, we would extract three 24x24 patches - one around the voxel in question, one in parallel and above, and one in parallel and below.\nEach of the layers are given to a 2D convolutional neural network as different channels.\nThis method gives the network some 3D context around the voxel (in case of stack sizes greater than 1), at a relatively low space overhead. However, the network is not convolutional in the third dimension.\nNetwork architecture is 20 5x5 kernels in first convolutional layer, 50 5x5 kernels in the second convolutional layer, a 1000 nodes fully-connected layer, then finally a softmax layer for exponential normalization. No max-pooling is used, since network performs slightly worse with any max-pooling."}, {"heading": "4.1.4 Convolutional Method 2: Tri-planar Patches", "text": "The second method we tried is the tri-planar method used by Prasoon et al. and Roth et al. in other medical imaging applications [32, 33].\nFor each voxel, we extract 3 square patches around the voxel, perpendicular to each axis. For example, if we want a patch size of 24, we would extract a 24x24 patch on the x-y plane centered around the voxel in question, and a 24x24 patch on the x-z plane, and another 24x24 patch on the y-z plane.\nSince the corresponding pixels from the 3 patches are not spatially correlated in this case, we use a network architecture that consists of 2 convolutional layers (20 5x5 kernels and 50 5x5 kernels) for each of the 3 patches with no connections between them until the very end, where we feed all their outputs into a 1000 nodes fully-connected layer for final classification. No max-pooling is used."}, {"heading": "4.1.5 Convolutional Method 3: 3D Patches", "text": "This approach is an intuitive extension of the 2D approach into 3D. For each voxel we want to sample, we take a 3D patch with equal length on each side, around the voxel. For a patch size of 24, we would extract 24x24x24 patches.\nNetwork architecture is 20 5x5x5 kernels in the first convolutional layer, 50 5x5x5 kernels in the second convolutional layer, then a 1000 nodes fully-connected layer as before. No max-pooling is used."}, {"heading": "4.1.6 Image Labeling", "text": "After the network is trained, to label an image, patches are extracted for every voxel in the mask region (in the correct format for the network architecture in use), and the result is used to label the voxel. Any voxel outside of the mask region is automatically classified as negative."}, {"heading": "4.1.7 Training", "text": "All network training are done with standard stochastic gradient descent with a batch size of 50 and a fixed learning rate of 0.01.\nAt the beginning of training, termination iteration is set to 1 validation period. Validation is done after every pass through the training set (24,000 patches). Every time a validation score improves the current best validation score by more than 1% (in error, not classification rate), the terminating iteration is set to twice the current iteration count. This means training will only be terminated if there is no significant improvement for at least the second half of the elapsed time."}, {"heading": "4.1.8 Post-Processing", "text": "Besides comparing raw output after labeling by convolutional neural networks, we also want to see what kind of performance can we get after some simple post-processing to clean up the results. The post-processing applied is the same for all 3 convolutional architectures.\nFor each labeled image, we first calculate the centroid of all voxels labeled left-hippocampus, and the centroid of all voxels labeled right-hippocampus.\nThen, we divide up the image into blobs (connected voxels with the same classification), and for each blob, we check their size.\nIf a blob is smaller than a certain threshold (500 voxels in our case), and the labeling is negative (non-hippocampus), it is re-labeled to be the nearest hippocampus (based on centroid).\nIf a blob is smaller than the threshold, and the labeling is positive (hippocampus), it is re-labeled as negative (non-hippocampus).\nWe find that these simple post-processing steps clean up most of the obviously mis-classified voxels, as shown in Figure 4."}, {"heading": "4.2 Results", "text": "All timing results in this section are obtained on a single NVIDIA GeForce GTX Titan Black GPU, using Theano\u2019s CUDA implementation of convolutional neural networks [34].\nOur first experiment is to determine the effect of number of layers on the performance of the 2D convolutional architecture. As shown in Table 1, there are very clear improvements going from 1 to 3 layers, but there are no clear improvements beyond 3 layers. However, we also note that number of layers has little impact on speed in terms of time per iteration.\nAll labeling results are on one of the images in the testing set, with 1755 positive voxels, and 1393947 negative voxels. All false positive and false negative values are after post-processing, and since the total number of positive and negative voxels are the same, the false positive and false negative values are directly comparable between runs. They are proportional to (100%\u2212 recall) and (100%\u2212 precision)\nrespectively.\n# Layers Best Val Perf Test Perf False Pos (vxl) False Neg (vxl) Iter T (mins) 1 21.55% 21.38% 666 426 18240 22.15 2 13.29% 14.56% 1173 413 15360 27.67 3 8.76% 9.79% 513 283 25920 30.39 4 11.69% 11.51% 706 446 19680 25.33 5 8.95% 9.64% 490 306 18720 21.99 7 9.34% 9.65% 479 312 23040 27.65 9 9.25% 10.03% 577 292 22080 27.37 11 10.04% 11.23% 583 331 8640 20.37 13 9.23% 10.01% 761 283 28320 34.52\nTable 1: Performance using different numbers of layers\nThe next experiment is to test different patch sizes, also with the 2D architecture. As shown in Table 2, there are little benefits in going beyond 24x24. However, in this case, training becomes much slower for larger patch sizes (in time per iteration). Therefore, the optimal patch size seems to be 24x24.\nNext, we experiment with patch size for the tri-planar architecture, and see similar results, with the optimal patch size being 24x24, as shown in Table 3. There are also very significant reductions in training speed as patch size is increased.\nFinally, we look at patch sizes for the 3D architecture. Unfortunately, we are constrained by available GPU memory in this case, and can only use up to 20x20x20 patches. We find that unlike the previous two architectures, the 3D architecture performs well even at a very small patch size of 12x12x12, and it is not clear whether it actually benefits from having larger patches, as shown in Table 4.\nFrom the above experiments, we selected 3 configurations to investigate further -\n\u2022 2D 24x24, 7 layers\n\u2022 Tri-planar 24x24\n\u2022 3D 20x20x20\nWe run each configuration five times to see how consistent they are. The networks are initialized with different random seeds each time, but trained on the same training sets. Results are presented in Table 5.\nWhile the results on the validation and testing sets are reasonably consistent between each run of the same configuration, the actual labeling performance on an entire image is much less consistent. This can be due to the fact that the class distribution in the patches used for the training/validation/testing sets are not the same as the distribution in an actual image. The discrepancy in image labeling performance could be because some models do better at classifying some classes than others, and those classes are more common in an actual image.\nFrom the perspective of speed-accuracy tradeoffs, the 2D architecture is clearly the least accurate, but also the fastest to train at approximately 754 iterations per minute.\nThe tri-planar architecture has clearly better performance than 2D, and can be trained at approximately 221 iterations per minute.\nThe 3D architecture performs consistently better than the tri-planar architecture in classifying patches, but that advantage does not seem to translate well to image labeling performance, where it seems to perform slightly worse. It is also the slowest to train at approximately 95 iterations per minute, with the longest run taking more than 5 hours to train."}, {"heading": "5 Conclusion and Future Work", "text": "In this project we investigated the use of three different convolutional network architectures for patchbased segmentation of the hippocampi region in MRI images. We discovered that the popular triplanar approach offers a good tradeoff between accuracy and training time. While the 3D approach performs marginally better at patch classification, it does not seem to perform as well at labeling an entire image. This is most likely due to the sampling method altering prior probabilities of the classes\npresented to the training algorithm, and if this problem is solved, the 3D approach should perform marginally better than the tri-planar approach in whole-image labeling as well, but with a much higher computational power requirement.\nThere are many possible avenues for future investigation. For example, there are learning rate scheduling algorithms available that may significantly shorten training time without affecting quality of results, such as Zeiler\u2019s famous ADADELTA algorithm [35], which assigns an independent learning rate to each weight depending on how often they are activated, so that while some oft-used connections can have slower learning rates to achieve higher precision, rarely-used connections can still be trained at a higher learning rate to reduce bias.\nIt may also be beneficial to give the coordinate of each patch (within the mask) to the fully-connected layers of the neural networks, along with the prior probability of each class at that coordinate, determined by statistical analysis on the training set.\nOne possible extension to the tri-planar architecture is to include images at multiple scales for each plane, similarly to how it was applied to traffic sign recognition by Sermanet and LeCun [36]. This would allow the networks to have more global context, and if the largest scales are big enough to include boundaries of the brain, it may compensate for the lack of positional information in the triplanar architecture. This can either be an alternative to, or complement, the idea of giving statistical coordinate-based prior probabilities to the network."}], "references": [{"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["Itamar Arel", "Derek C Rose", "Thomas P Karnowski"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Honglak Lee", "Peter Pham", "Yan Largman", "Andrew Y Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Tuning of the structure and parameters of a neural network using an improved genetic algorithm", "author": ["Frank Hung-Fat Leung", "Hak-Keung Lam", "Sai-Ho Ling", "Peter Kwong-Shun Tam"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Evolving networks: Using the genetic algorithm with connectionist learning", "author": ["Richard K Belew", "John McInerney", "Nicol N Schraudolph"], "venue": "In In. Citeseer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "Combinations of genetic algorithms and neural networks: A survey of the state of the art", "author": ["J David Schaffer", "Darrell Whitley", "Larry J Eshelman"], "venue": "In Combinations of Genetic Algorithms and Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "A learning algorithm for evolving cascade neural networks", "author": ["Vitaly Schetinin"], "venue": "Neural Processing Letters,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken"], "venue": "Neural networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A weight initialization method for improving training speed in feedforward neural network", "author": ["Jim YF Yam", "Tommy WS Chow"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Gradient descent: Second-order momentum and saturating error", "author": ["Barak A Pearlmutter"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Rprop-a fast adaptive learning algorithm", "author": ["Martin Riedmiller", "Heinrich Braun"], "venue": "In Proc. of ISCIS VII), Universitat. Citeseer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Estimating the dimension of a model", "author": ["Gideon Schwarz"], "venue": "The annals of statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1978}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological cybernetics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1980}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Sepp Hochreiter"], "venue": "Master\u2019s thesis, Institut fur Informatik, Technische Universitat,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Learning multiple layers of representation", "author": ["Geoffrey E Hinton"], "venue": "Trends in cognitive sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Ensembling neural networks: many could be better than all", "author": ["Zhi-Hua Zhou", "Jianxin Wu", "Wei Tang"], "venue": "Artificial intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "arXiv preprint arXiv:1503.01838,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Episodic and declarative memory: role of the hippocampus. Hippocampus", "author": ["Endel Tulving", "Hans J Markowitsch"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "Atlas-based hippocampus segmentation in alzheimer\u2019s disease and mild cognitive impairment", "author": ["Owen T Carmichael", "Howard A Aizenstein", "Simon W Davis", "James T Becker", "Paul M Thompson", "Carolyn Cidis Meltzer", "Yanxi Liu"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Standardization of analysis sets for reporting results from adni mri data", "author": ["Bradley T Wyman", "Danielle J Harvey", "Karen Crawford", "Matt A Bernstein", "Owen Carmichael", "Patricia E Cole", "Paul K Crane", "Charles DeCarli", "Nick C Fox", "Jeffrey L Gunter"], "venue": "Alzheimer\u2019s & Dementia,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network", "author": ["Adhish Prasoon", "Kersten Petersen", "Christian Igel", "Fran\u00e7ois Lauze", "Erik Dam", "Mads Nielsen"], "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "A new 2.5 d representation for lymph node detection using random sets of deep convolutional neural network observations", "author": ["Holger R Roth", "Le Lu", "Ari Seff", "Kevin M Cherry", "Joanne Hoffman", "Shijun Wang", "Jiamin Liu", "Evrim Turkbey", "Ronald M Summers"], "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["Pierre Sermanet", "Yann LeCun"], "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "At the heart of all deep learning algorithms is the domain-independent idea of using hierarchical layers of learned abstraction to efficiently accomplish high level tasks [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 169, "endOffset": 172}, {"referenceID": 5, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 328, "endOffset": 337}, {"referenceID": 6, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 328, "endOffset": 337}, {"referenceID": 7, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 328, "endOffset": 337}, {"referenceID": 8, "context": "Methods have been proposed for automatic hyperparameter tuning, such as evolving cascade networks [9], which trains a large network through an iterative process, by first starting with a minimal network, and in each iteration, train a few \u201dcandidate\u201d networks that have more nodes in different layers, and keeping the best.", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8].", "startOffset": 112, "endOffset": 121}, {"referenceID": 6, "context": "There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8].", "startOffset": 112, "endOffset": 121}, {"referenceID": 7, "context": "There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8].", "startOffset": 112, "endOffset": 121}, {"referenceID": 9, "context": "It has been proven that a network with 1 hidden layer can approximate any continuous (in feature space) function to any accuracy, and a network with 2 hidden layers can approximate any function to any accuracy [10, 11].", "startOffset": 210, "endOffset": 218}, {"referenceID": 10, "context": "It has been proven that a network with 1 hidden layer can approximate any continuous (in feature space) function to any accuracy, and a network with 2 hidden layers can approximate any function to any accuracy [10, 11].", "startOffset": 210, "endOffset": 218}, {"referenceID": 11, "context": "In practice, although the logistic function is more biologically plausible, hyperbolic tangent usually allows faster training since being linear around 0 means nodes will not start training in saturation (which would make training much slower) even if inputs are zero or negative [12].", "startOffset": 280, "endOffset": 284}, {"referenceID": 12, "context": "One popular method is to draw from the uniform distribution (\u2212 a \u221a din , a \u221a din ), where a is chosen based on the shape of the activation function (where it starts to saturate), and din is the number of inputs to the node [13].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "down the bottom of a valley) [14].", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "5) if the gradient has changed signs [15].", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "An initial learning rate still needs to be chosen, but it doesn\u2019t significantly affect training time or result [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Besides computational benefits, a model with a smaller degree of freedom (number of parameters) requires a smaller dataset to train [16], and size of the training set is often a limiting factor in neural network training.", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The fact that deeper networks are more computationally efficient has been known for a very long time, and deep networks have been attempted as early as 1980 [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "In 1991, Hochreiter identified the problem and called it \u201dvanishing gradients\u201d [18, 19].", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "In 1991, Hochreiter identified the problem and called it \u201dvanishing gradients\u201d [18, 19].", "startOffset": 79, "endOffset": 87}, {"referenceID": 19, "context": "The idea is to first train each layer in an unsupervised and greedy fashion, to try to identify application-independent features from each layer, before finally training the entire network on labeled data [20].", "startOffset": 205, "endOffset": 209}, {"referenceID": 19, "context": "The way he implemented the idea is with a generative model, in the form of a restricted Boltzmann machine (RBM), where each layer contains a set of binary variables with associated probability distributions, and each layer is trained to predict the previous layer using an algorithm known as contrastive divergence [20].", "startOffset": 315, "endOffset": 319}, {"referenceID": 20, "context": "Another idea in the same vein is to use a neural network to reproduce the original input [21], which allows the reuse of all the neural network techniques that have already been developed (unlike for RBM).", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "The idea is to first start with just the input layer and one hidden layer, and train the network (using standard back-propagation gradient descent) to produce the original input, essentially training the network to model the identity function [21].", "startOffset": 243, "endOffset": 247}, {"referenceID": 20, "context": "This process is repeated to train each hidden layer, with the output of the previous layer as the input [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "In modern implementations, some artificial noise is often injected into the input of autoencoders to encourage robustness in the autoencoders, by testing their ability to reconstruct clean input from partially-corrupted input [21].", "startOffset": 226, "endOffset": 230}, {"referenceID": 20, "context": "Denoising autoencoders perform similarly to systems based on RBMs and their stacked variant - Deep Belief Networks (DBN) [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "Their solution to the vanishing gradients problem is to simply use an activation function that does not reduce the error as it\u2019s propagated back [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "The proposed function is the rectified linear activation function (ReLU), y = max(0, x) [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "This can be solved using L1 regularization, which not only limits the magnitude of weights, but also enforces sparsity [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "This can result in more efficient usage of available nodes, and also have computational advantages in networks using activation functions with a hard 0 saturation (such as the rectified linear activation) [12], since if a node\u2019s output is 0, it does not need to be broadcasted to nodes in the next layer.", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "reports achieving slightly superior or similar performance to previous results based on pre-training [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "In this case, starting with pre-training using the entire dataset before training the whole network with the labeled subset can result in a network that generalizes better [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "proposed a technique that greatly improves network performance in cases where there is limited training data [22].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "The traditional solution to this is to train an ensemble of networks from different initialization and/or subsets of the training set, then combine their outputs to produce the final output (often by averaging, or voting) [23].", "startOffset": 222, "endOffset": 226}, {"referenceID": 21, "context": "When training is done, all nodes are re-enabled, but all weights are halved to maintain the same output range [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "In DropOut, nodes cannot assume other nodes exist, and are forced to be \u201dindependently useful\u201d [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "For example, in computer vision applications where the network should be translationally and rotationally invariant, additional training entries can be formed by translating or rotating images from the original training set [25].", "startOffset": 224, "endOffset": 228}, {"referenceID": 24, "context": "Continuing on the theme of model compression, in 2014, Ba and Caruana showed that with the help of a high performance deep network, a shallow network can be trained to perform much better than a similar shallow network that is trained directly on the training set [26].", "startOffset": 264, "endOffset": 268}, {"referenceID": 24, "context": "Their algorithm first trains a deep net (or an ensemble of deep nets) on the original training set, and use it to provide \u201dextended labels\u201d for all entries in the training set [26].", "startOffset": 176, "endOffset": 180}, {"referenceID": 24, "context": "In case of classification problems where the output layer is often softmax, the \u201dextended labels\u201d are inputs to the softmax layer [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "By modelling the log probabilities, the shallow network is also mimic-ing how the deep net (or ensemble) will generalize to unseen data, which mostly depend on the relative values of log probabilities for classes that are not the highest [26].", "startOffset": 238, "endOffset": 242}, {"referenceID": 24, "context": "However, the performance of these mimic-ing shallow networks are still not quite as good as the deep networks or ensembles they are mimic-ing [26].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "Convolutional neural networks are a neural network architecture that uses extensive weight-sharing to reduce the degrees of freedom of models that operate on features that are spatially-correlated [17].", "startOffset": 197, "endOffset": 201}, {"referenceID": 25, "context": "This includes 2D and 3D images (and 2D videos, which can be seen as 3D images), but it has also very recently been successfully applied to natural language processing [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "Convolutional neural networks are inspired by the observation that for inputs like images (with each pixel being an input dimension), many low level operations are local, and they are not positiondependent [17].", "startOffset": 206, "endOffset": 210}, {"referenceID": 1, "context": "convolutional, max-pooling, and fully-connected [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 26, "context": "While averaging can also be used, empirical results suggest that downsampling by taking the maximum in each sub-region gives the best performance in most cases [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 27, "context": "The hippocampus is a component of human brains responsible for committing short-term episodic and declarative memory into long-term memory, as well as navigation [29].", "startOffset": 162, "endOffset": 166}, {"referenceID": 28, "context": "A reduction in hippocampal volume can be used as a marker for AD diagnosis [30].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "We explore 3 convolutional neural network architectures for patch-based segmentation on the ADNI Alzheimer\u2019s MRI dataset [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "in other medical imaging applications [32, 33].", "startOffset": 38, "endOffset": 46}, {"referenceID": 31, "context": "in other medical imaging applications [32, 33].", "startOffset": 38, "endOffset": 46}, {"referenceID": 32, "context": "All timing results in this section are obtained on a single NVIDIA GeForce GTX Titan Black GPU, using Theano\u2019s CUDA implementation of convolutional neural networks [34].", "startOffset": 164, "endOffset": 168}, {"referenceID": 33, "context": "For example, there are learning rate scheduling algorithms available that may significantly shorten training time without affecting quality of results, such as Zeiler\u2019s famous ADADELTA algorithm [35], which assigns an independent learning rate to each weight depending on how often they are activated, so that while some oft-used connections can have slower learning rates to achieve higher precision, rarely-used connections can still be trained at a higher learning rate to reduce bias.", "startOffset": 195, "endOffset": 199}, {"referenceID": 34, "context": "One possible extension to the tri-planar architecture is to include images at multiple scales for each plane, similarly to how it was applied to traffic sign recognition by Sermanet and LeCun [36].", "startOffset": 192, "endOffset": 196}], "year": 2015, "abstractText": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer\u2019s Disease. We found that a slightly unconventional \u201dstacked 2D\u201d approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \u201dtri-planar\u201d approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement. ar X iv :1 50 5. 02 00 0v 1 [ cs .L G ] 8 M ay 2 01 5", "creator": "LaTeX with hyperref package"}}}