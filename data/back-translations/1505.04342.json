{"id": "1505.04342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2015", "title": "Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter", "abstract": "Twitter, a popular social media outlet, has become a huge source of linguistic data, rich in opinions, feelings and discussions. Due to the growing popularity of Twitter, its perceived potential for exerting social influence has led to the rise of a diverse community of automatons commonly referred to as bots. These inorganic and semi-organic Twitter entities can range from benevolent (e.g. weather update bots, help-wanted-alert bots) to malicious (e.g. spam messages, advertising or radical opinions). Existing detection algorithms use metadata (time between tweets, number of followers, etc.) to identify robotic accounts.", "histories": [["v1", "Sun, 17 May 2015 01:22:00 GMT  (603kb,D)", "https://arxiv.org/abs/1505.04342v1", null], ["v2", "Tue, 19 May 2015 13:32:43 GMT  (603kb,D)", "http://arxiv.org/abs/1505.04342v2", null], ["v3", "Tue, 2 Jun 2015 16:14:38 GMT  (603kb,D)", "http://arxiv.org/abs/1505.04342v3", null], ["v4", "Wed, 11 Nov 2015 23:02:05 GMT  (5696kb,D)", "http://arxiv.org/abs/1505.04342v4", null], ["v5", "Wed, 24 Feb 2016 07:42:59 GMT  (8619kb,D)", "http://arxiv.org/abs/1505.04342v5", null], ["v6", "Tue, 14 Jun 2016 13:44:56 GMT  (8619kb,D)", "http://arxiv.org/abs/1505.04342v6", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eric m clark", "jake ryland williams", "chris a jones", "richard a galbraith", "christopher m danforth", "peter sheridan dodds"], "accepted": false, "id": "1505.04342"}, "pdf": {"name": "1505.04342.pdf", "metadata": {"source": "CRF", "title": "Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter", "authors": ["Eric M. Clark", "Jake Ryland Williams", "Chris A. Jones", "Richard A. Galbraith", "Christopher M. Danforth", "Peter Sheridan Dodds"], "emails": ["eclark@uvm.edu"], "sections": [{"heading": null, "text": "Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter\nEric M. Clark,1, 2, 3, 4, 5, \u2217 Jake Ryland Williams,1, 2, 3, 4 Chris A. Jones,5, 6, 7 Richard\nA. Galbraith,8, 9 Christopher M. Danforth,1, 2, 3, 4 and Peter Sheridan Dodds1, 2, 3, 4\n1Department of Mathematics & Statistics 2Vermont Complex Systems Center\n3Vermont Advanced Computing Core 4Computational Story Lab 5Department of Surgery\n6Global Health Economics Unit of the Vermont Center for Clinical and Translational Science 7Vermont Center for Behavior and Health\n8Department of Medicine 9Vermont Center for Clinical and Translational Science\n(Dated: June 15, 2016)\nTwitter, a popular social media outlet, has evolved into a vast source of linguistic data, rich with opinion, sentiment, and discussion. Due to the increasing popularity of Twitter, its perceived potential for exerting social influence has led to the rise of a diverse community of automatons, commonly referred to as bots. These inorganic and semi-organic Twitter entities can range from the benevolent (e.g., weather-update bots, help-wanted-alert bots) to the malevolent (e.g., spamming messages, advertisements, or radical opinions). Existing detection algorithms typically leverage metadata (time between tweets, number of followers, etc.) to identify robotic accounts. Here, we present a powerful classification scheme that exclusively uses the natural language text from organic users to provide a criterion for identifying accounts posting automated messages. Since the classifier operates on text alone, it is flexible and may be applied to any textual data beyond the Twittersphere.\nPACS numbers:\nI. INTRODUCTION\nTwitter has become a mainstream social outlet for the discussion of a myriad of topics through microblogging interactions. Members chiefly communicate via short text-based public messages restricted to 140 characters, called tweets. As Twitter has evolved from a simple microblogging social media interface into a mainstream source of communication for the discussion of current events, politics, consumer goods/services, it has become increasingly enticing for parties to gameify the system by creating automated software to send messages to organic (human) accounts as a means for personal gain and for influence manipulation [1, 2]. The results of sentiment and topical analyses can be skewed by robotic accounts that dilute legitimate public opinion by algorithmically generating vast amounts of inorganic content. Nevertheless, data from Twitter is becoming a source of interest in public health and economic research in monitoring the spread of disease [3, 4] and gaining insight into public health trends [5].\nIn related work [6\u20139], researchers have built classification algorithms using metadata idiosyncratic to Twitter, including the number of followers, posting frequency, account age, number of user mentions/replies, username\n\u2217Electronic address: eclark@uvm.edu\nlength, and number of retweets. However, relying on metadata can be problematic: sophisticated spam algorithms now emulate the daily cycle of human activity and author borrowed content to appear human [6]. Another problematic spam tactic is the renting of accounts of legitimate users (called sponsored accounts), to introduce short bursts of spam and hide under the user\u2019s organic metadata to mask the attack [10].\nA content based classifier proposed by [18] measures the entropy between Twitter time intervals along with user meta data to classify Twitter accounts, and requires a comparable number of tweets (\u2265 60) for adequate classification accuracy as our proposed method. SentiBot, another content based classifier [19], utilizes latent Dirichlet allocation (LDA) for topical categorization combined with sentiment analysis techniques to classify individuals as either bots or humans. We note that as these automated entities evolve their strategies, combinations of our proposed methods and studies previously mentioned may be required to achieve reasonable standards for classification accuracy. Our method classifies accounts solely based upon their linguistic attributes and hence can easily be integrated into these other proposed strategies.\nWe introduce a classification algorithm that operates using three linguistic attributes of a user\u2019s text. The algorithm analyzes:\n1. the average URL count per tweet\n2. the average pairwise lexical dissimilarity between a\nTypeset by REVTEX\nar X\niv :1\n50 5.\n04 34\n2v 6\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n16\n2 user\u2019s tweets, and\n3. the word introduction rate decay parameter of the user for various proportions of time-ordered tweets\nWe provide detailed descriptions of each attribute in the next section. We then test and validate our algorithm on 1 000 accounts which were hand coded as automated or human.\nWe find that for organic users, these three attributes are densely clustered, but can vary greatly for automatons. We compute the average and standard deviation of each of these dimensions for various numbers of tweets from the human coded organic users in the dataset. We classify accounts by their distance from the averages from each of these attributes. The accuracy of the classifier increases with the number of tweets collected per user. Since this algorithm operates independently from user metadata, robotic accounts do not have the ability to adaptively conceal their identities by manipulating their user attributes algorithmically. Also, since the classifier is built from time ordered tweets, it can determine if a once legitimate user begins demonstrating dubious behavior and spam tactics. This allows for social media data-miners to dampen a noisy dataset by weeding out suspicious accounts and focus on purely organic tweets."}, {"heading": "II. DATA HANDLING", "text": ""}, {"heading": "A. Data-Collection", "text": "We filtered a 1% sample of Twitter\u2019s streaming API (the spritzer feed) for tweets containing geo-spatial metadata spanning the months of April through July in 2014. Since roughly 1% of tweets provided GPS located spatial coordinates, our sample represents nearly all of the tweets from users who enable geotagging. This allows for much more complete coverage of each user\u2019s account. From this sample, we collected all of the geo-tweets from the most active 1 000 users for classification as human or robot and call this the Geo-Tweet dataset."}, {"heading": "B. Social HoneyPots", "text": "To place our classifier in the context of recent work, we applied our algorithm to another set of accounts collected from the Social HoneyPot Experiment [11]. This work exacted a more elaborate approach to find automated accounts on Twitter by creating a network of fake accounts (called Devils [12]) that would tweet about trending topics amongst themselves in order to tempt robotic interactions. The experiment was analyzed and compiled into a dataset containing the tweets of \u201clegitimate users\u201d and those classified as \u201ccontent polluters\u201d. We note that the users in this dataset were not hand coded. Accounts that followed the Devil honeypot accounts were deemed robots. Their organic users were compiled\nfrom a random sample of Twitter, and were only deemed organic because these accounts were not suspended by Twitter at the time. Hence the full HoneyPot dataset can only serve as an estimate of the capability of this classification scheme."}, {"heading": "C. Human Classification of Geo-Tweets", "text": "Each of the 1 000 users were hand classified separately by two evaluators. All collected tweets from each user were reviewed until the evaluator noticed the presence of automation. If no subsample of tweets appeared to be algorithmically generated, the user was classified as human. The results were merged, and conflicting entries were resolved to produce a final list of user ids and codings. See Figure 1 for histograms and violin plots summarizing the distributions of each user class. We note that any form of perceived automation was sufficient to deem the account as automated. See SI for samples of each of these types of tweets from each user class and a more thorough description of the annotation process."}, {"heading": "D. Types of Users", "text": "We consider organic content, i.e. from human accounts, as those that have not tweeted in an algorithmic fashion. We focused on three distinct classes of automated tweeting:\nRobots: Tweets from these accounts draw on a strictly limited vocabulary. The messages follow a very structured pattern, many of which are in the form\n3 of automated updates. Examples include Weather Condition Update Accounts, Police Scanner Update Accounts, Help Wanted Update Accounts, etc.\nCyborgs: The most covert of the three, these automatons exhibit human-like behavior and messages through loosely structured, generic, automated messages and from borrowed content copied from other sources. Since many malicious cyborgs on Twitter try to market an idea or product, a high proportion of their tweets contain URLs, analogous to spam campaigns studied on Facebook [13]. Messages range from the backdoor advertising of goods and services [14] to those trying to influence social opinion or even censor political conversations [15]. These accounts act like puppets from a central algorithmic puppeteer to push their product on organic users while trying to appear like an organic user [16]. Since these accounts tend to borrow content, they have a much larger vocabulary in comparison to ordinary robots. Due to Twitter\u2019s 140 character-pertweet restriction, some of the borrowed content being posted must be truncated. A notable attribute of many cyborgs is the presence of incomplete messages followed by an ellipsis and a URL. Included in this category are \u2018malicious promoter\u2019 accounts [11] that are radically promoting a business or an idea systematically.\nHuman Spammers: These are legitimate accounts that abuse an algorithm to post a burst of almost indistinguishable tweets that may differ by a character in order to fool Twitter\u2019s spam detection protocols. These messages are directed at a particular user, commonly for a follow request to attempt to increase their social reach and influence.\nAlthough we restrict our focus to the aforementioned classes, we did notice the presence of other subclasses, which we have named \u201clisters\u201d, and \u201cquoters\u201d, that have both organic and automaton features. Listers are accounts that send their messages to large groups of individuals at once. Quoters are dedicated accounts that are referencing distant passages from literature or song lyrics. Most of the tweets from these accounts are all encased in quotations. These accounts also separately tweet organic content. We classified these accounts as human because there was not sufficient evidence suggesting these behaviors were indeed automated."}, {"heading": "III. METHODS", "text": ""}, {"heading": "A. Classification Algorithm", "text": "The classifier, C, takes ordinal samples of tweets from each user, \u00b5, of varying number, s, to determine if the user is a human posting strictly organic content or is algorithmically automating tweets:\nC : \u00b5s \u2192 {0, 1} = {Organic, Automaton}.\nAlthough we have classified each automaton into three distinct classes, the classifier is built more simply to detect and separate organic content from automated. To classify the tweets from a user, we measure three distinct linguistic attributes:\n1. Average Pairwise Tweet Dissimilarity,\n2. Word Introduction Rate Decay Parameter,\n3. Average number of URLs (hyperlinks) per tweet."}, {"heading": "B. Average Pairwise Tweet Dissimilarity", "text": "Many algorithmically generated tweets contain similar structures with minor character replacements and long chains of common substrings. Purely organic accounts have tweets that are very dissimilar on average. The length of a tweet, t, is defined as the number of characters in the tweet and is denoted |t|. Each tweet is cleaned by truncating multiple whitespace characters and the metric is performed case insensitively. A sample of s tweets from a particular user is denoted T s\u00b5. Given a pair of tweets from a particular user, ti, tj \u2208 T s\u00b5, the pairwise tweet dissimilarity, D(ti, tj), is given by subtracting the length of the longest common subsequence of both tweets, |LCS(ti, tj)| and then weighting by the sum of the lengths of both tweets:\nD(ti, tj) = |ti|+ |tj | \u2212 2 \u00b7 |LCS(ti, tj)|\n|ti|+ |tj | .\nThe average tweet dissimilarity of user \u00b5 for sample size of s tweets is calculated as:\n\u00b5slcs = 1( s 2 ) \u00b7 \u2211 ti,tj\u2208T s\u00b5 D(ti, tj).\nFor example, given the two tweets: (t1, t2) = (I love Twitter, I love to spam). Then |t1| = |t2| = 14, LCS(t1, t2) = |I love t| = 8 (including whitespaces) and we calculate the pairwise tweet dissimilarity as:\nD(t1, t2) = 14 + 14\u2212 2 \u00b7 8\n14 + 14 =\n12 28 = 3 7 ."}, {"heading": "C. Word Introduction Decay Rate", "text": "Since social robots automate messages, they have a limited and crystalline vocabulary in comparison to organic accounts. Even cyborgs that mask their automations with borrowed content cannot fully mimic the rate at which organic users introduce unique words into their text over time. The word introduction rate is a measure of the number of unique word types introduced\nover time from a given sample of text [17]. The rate at which unique words are introduced naturally decays over time, and is observably different between automated and organic text. By testing many random word shufflings of a text, we define mn as the average number of words between the nth and n + 1st initial unique word type appearances. From [17], the word introduction decay rate, \u03b1(n), is given as\n\u03b1(n) = 1/mn \u221d n\u2212\u03b3 for \u03b3 > 0.\nFor each user, the scaling exponent of the word introduction decay rate, \u03b1, is approximated by performing standard linear regression on the last third of the logtransformed tail of the average gap size distribution as a function of word introduction number, n [17]. In figure 2 below, the log transformed rank-unique word gap distribution is given for each individual in the data set. Here the human population (green) is distinctly distributed in comparison to the automatons."}, {"heading": "D. Average URLs per Tweet", "text": "Hyperlinks (URLs) help automatons spread spam and malware [10, 20, 21] . A high fraction of tweets from spammers tend to contain some type of URL in comparison to organic individuals, making the average URLs per tweet a valuable attribute for bot classification algorithms [8, 22, 23]. For each user, the average URL rate is measured by the total number of occurrences of the substring \u2018http:\u2019 within tweets, and then divided by the total number of tweets authored by the user in the sample of\nsize s:\n\u00b5surl = #Occurrences of \u2018http:\u2019\n#Sampled Tweets ."}, {"heading": "E. Cross Validation Experiment", "text": "We perform a standard 10-fold Cross Validation procedure on the 2014 Geo-Tweet data set to measure the accuracy of using each linguistic feature for classifying Organic accounts. We divided individuals into 10 equally sized groups. Then 10 trials are performed where 9 of the 10 groups are used to train the algorithm to classify the final group.\nDuring the Calibration phase, we measure each of the three features for every human coded account in the training set. We sequentially collect tweets from each user from a random starting position in time. We record the arithmetic mean and standard deviation of the Organic attributes to classify the remaining group. The classifier distinguishes Human from Automaton by using a varying threshold, n, from the average attribute value computed from the training set. For each attribute, we classify each user as an automaton if their feature falls further than n standard deviations away from the organic mean, for varying n.\nFor each trial, the False Positives and True Positives for a varying window size, n, are recorded. To compare to other bot-detection strategies, we rate True Positives as the success at which the classifier identifies automatons by exclusion, and False Positives as humans that are incorrectly classified as automatons. The results of the trials for varying tweet sizes are averaged and visualized with a Receiver Operator Characteristic curve (ROC) (see Figure 3). The accuracy of each experiment is measured as the area under the ROC, or AUC. To benchmark the classifier, a 10-fold cross validation was also performed on the HoneyPot tweet-set which we describe in the following section."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": ""}, {"heading": "A. Geo-Tweet Classification Validation", "text": "The ROC curves for the Geo-Tweet 10 fold Cross Validation Experiment for varying tweet bins in Figure 3 show that the accuracy increases as a function of number of tweets.\nAlthough the accuracy of the classifier increases with the number of collected tweets, we see in Figure 4 that within 50 tweets the accuracy of the average of 10 random trials is only slightly higher than a 500 tweet user sample. While this is very beneficial to our task (isolating humans), we note that larger samples see greater returns when one instead wants to isolate spammers, that tweet random bursts of automation.\n5"}, {"heading": "B. HoneyPot External Validation", "text": "The classifier was tested on the Social Honeypot Twitter-bot dataset provided by [11]. Results are visualized with a ROC curve in Figure 5. The averaged optimal threshold for the full English user dataset (blue curve) had a high true positive rate (correctly classified automatons: 86%), but also had a large false positive rate (misclassified humans: 22%).\nThe Honeypot Dataset relied on Twitter\u2019s spam detection protocols to label their randomly collected \u201clegitimate users\u201d. Some forms of automation (weather-bots, help-wanted bots) are permitted by Twitter. Other cyborgs that are posting borrowed organic content can fool Twitter\u2019s automation criterion. This ill formation of the training set greatly reduces the ability of the classifier to distinguish humans from automatons, since the classifier gets the wrong information about what constitutes a human. To see this, a random sample of 1 000 English Honeypot users was hand-coded to mirror the previous experiment. On this smaller sample (black curve in Figure 4), the averaged optimal threshold accuracy increased to 96%."}, {"heading": "C. Calibrated Classifier Performance", "text": "We created the thresholding window of final calibrated classifier using the results from the calibration experiment. We average the optimal parameters from the 10 fold cross validation on the Geo-Tweet dataset from each\nof the 10 calibration trials for tweet bins ranging from 25 to 500 in increments of 25 tweets. We also average and record the optimal parameter windows, nopt and their standard deviations, \u03c3opt. The standard deviations serve as a tuning parameter to increase the sensitivity of the classifier, by increasing the feature cutoff window (n). The results from applying the calibrated classifier to the\n6\nfull set of 1 000 users, using 400 tweet bags is given in Figure 6. The feature cutoff window (black lines) estimates if the user\u2019s content is organic or automated. Human feature sets (True Negatives: 716) are densely distributed with a 4.79% False Positive Rate (i.e., humans classified as robots). The classifier accurately classified 90.32% of the automated accounts and 95.21% of the Organic accounts. See Figure S1 for a cross sectional comparison of each feature set. We note that future work may apply different methods in statistical classification to optimize these feature sets, and that using these simple cutoffs already leads to a high level of accuracy."}, {"heading": "V. CONCLUSION", "text": "Using a flexible and transparent classification scheme, we have demonstrated the potential of using linguistic features as a means of classifying automated activity on Twitter. Since these features do not use the metadata provided by Twitter, our classification scheme may be applicable outside of the Twittersphere. Future work can extend this analysis multilingually and incorporate additional feature sets with an analogous classification scheme. URL content can also be more deeply analyzed to identify organic versus SPAM related hyperlinks.\nWe note the potential for future research to investigate and to distinguish between each sub-class of automa-\nton. We formed our taxonomy according to the different modes of text production. Our efforts were primarily focused in separating any form of automation from organic,human content. In doing so we recognized three distinct classes of these types of automated accounts. However, boundary cases (e.g. cyborg-spammers, robotspammers, robotic-cyborgs, etc.) along with other potential aforementioned subclasses (e.g. listers, quoters, etc.) can limit the prowess of our current classification scheme tailored towards these subclasses. We have shown that human content is distinctly different from these forms of automation, and that for a binary classification of automated or human, these features have a very reasonable performance with our proposed algorithm.\nOur study distinguishes itself by focusing on automated behavior that is tolerated by Twitter, since both types of inorganic content can skew the results of sociolinguistic analyses. This is particularly important, since Twitter has become a possible outlet for health economics [4] research including monitoring patient satisfaction and modeling disease spread [3, 24]. Monitoring excessive social media marketing of electronic nicotine delivery systems (also known as e-cigarettes), discussed in [25, 26], makes classifying organic and automated activity relevant for research that can benefit policy-makers regarding public health agendas. Isolating organic content on Twitter can help dampen noisy data-sets and is pertinent for research involving social media data and other linguistic data sources where a mixture of humans and automatons exist.\nIn health care, a cardinal problem with the use of electronic medical records is their lack of interoperability. This is compounded by a lack of standardization and use of data dictionaries which results in a lack of precision concerning our ability to collate signs, symptoms, and diagnoses. The use of millions or billions of tweets concerning a given symptom or diagnosis might help to improve that precision. But it would be a major setback if the insertion of data tweeted from automatons would obscure useful interpretation of such data. We hope that the approaches we have outlined in the present manuscript will help alleviate such problems."}, {"heading": "VI. ACKNOWLEDGMENTS", "text": "The authors wish to acknowledge the Vermont Advanced Computing Core which provided High Performance Computing resources contributing to the research results. EMC and JRW was supported by the UVM Complex Systems Center, PSD was supported by NSF Career Award # 0846668. CMD and PSD were also supported by a grant from the MITRE Corporation and NSF grant #1447634. CJ is supported in part by the National Institute of Health (NIH) Research wards R01DA014028 & R01HD075669, and by the Center of Biomedical Research Excellence Award P20GM103644\n7 from the National Institute of General Medical Sciences.\n[1] V.S. Subrahmanian, A. Azaria, S. Durst, V. Kagan, A. Galstyan, K. Lerman, L. Zhu, E. Ferrara, A. Flammini, and F. Menczer, arXiv arXiv:1601.05140, (2016), URL http://arxiv.org/pdf/1601.05140v1.pdf. [2] D. Harris, Can evil data scientists fool us all with the world\u2019s best spam?, goo.gl/psEguf (2013). [3] A. Sadilek, H. A. Kautz, and V. Silenzio, in ICWSM (2012). [4] A. Wagstaff and A. J. Culyer, Journal of health economics 31, 406 (2012). [5] L. Mitchell, M. R. Frank, K. D. Harris, P. S. Dodds, and C. M. Danforth, PloS one 8, e64417 (2013). [6] E. Ferrara, O. Varol, C. Davis, F. Menczer, and A. Flammini, CoRR abs/1407.5225 (2014), URL http:// arxiv.org/abs/1407.5225. [7] F. Benevenuto, G. Magno, T. Rodrigues, and V. Almeida, in In Collaboration, Electronic messaging, AntiAbuse and Spam Conference (CEAS (2010). [8] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia, in Proceedings of the 26th Annual Computer Security Applications Conference (ACM, New York, NY, USA, 2010), ACSAC \u201910, pp. 21\u201330, ISBN 978-1-4503-0133-6, URL http://doi.acm.org/10.1145/1920261.1920265. [9] C. M. Zhang and V. Paxson, in Proceedings of the 12th International Conference on Passive and Active Measurement (Springer-Verlag, Berlin, Heidelberg, 2011), PAM\u201911, pp. 102\u2013111, ISBN 978-3-642- 19259-3, URL http://dl.acm.org/citation.cfm?id= 1987510.1987521. [10] K. Thomas, C. Grier, D. Song, and V. Paxson, in Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference (ACM, New York, NY, USA, 2011), IMC \u201911, pp. 243\u2013258, ISBN 978-1-4503-1013-0, URL http://doi.acm.org/10.1145/ 2068816.2068840. [11] K. Lee, B. D. Eoff, and J. Caverlee, in In AAAI Int?l Conference on Weblogs and Social Media (ICWSM (2011). [12] K. Lee, B. D. Eoff, and J. Caverlee, in ICWSM, edited by W. W. Cohen and S. Gosling (The AAAI Press, 2010), URL http://dblp.uni-trier.de/db/ conf/icwsm/icwsm2010.html#LeeEC10. [13] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Y. Zhao, in Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement (ACM, New York, NY, USA, 2010), IMC \u201910, pp. 35\u201347, ISBN 978-1-4503-0483-2, URL http://doi.acm.org/10.1145/1879141.1879147. [14] J. Huang, R. Kornfield, G. Szczypka, and S. L. Emery, Tobacco control 23, iii26 (2014). [15] K. Thomas, C. Grier, and V. Paxson, in Presented as part of the 5th USENIX Workshop on Large-Scale Exploits and Emergent Threats (USENIX, Berkeley, CA, 2012), URL https://www.usenix.org/conference/leet12/ adapting-social-spam-infrastructure-political-censorship. [16] X. Wu, Z. Feng, W. Fan, J. Gao, and Y. Yu, in Machine Learning and Knowledge Discovery in Databases, edited by H. Blockeel, K. Kersting, S. Nijssen, and F. elezn (Springer Berlin Heidelberg, 2013), vol. 8190 of Lecture Notes in Computer Science, pp. 483\u2013498, ISBN 978-3-642-40993-6, URL http://dx.doi.org/10.1007/ 978-3-642-40994-3_31. [17] J. R. Williams, J. P. Bagrow, C. M. Danforth, and P. S. Dodds, Text mixing shapes the anatomy of rank-frequency distributions: A modern Zipfian mechanics for natural language, Physical Review E (in press) (Accepted 17 March 2015). [18] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia, Detecting automation of twitter accounts: Are you a human, bot, or cyborg?, Dependable and Secure Computing, IEEE Transactions on v9, n6 pp 811-824 (2012). [19] J.P. Dickerson, V. Kagan, H. Wang, and VS. Subrahmanian, Using sentiment to detect bots on Twitter: Are humans more opinionated than bots?, Advances in Social Networks Analysis and Mining (ASONAM), 2014 IEEE/ACM International Conference on 620\u2013627 (2014). [20] G. Brown, T. Howe, M. Ihbe, A. Prakash, and K. Borders, in Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work (ACM, New York, NY, USA, 2008), CSCW \u201908, pp. 403\u2013412, ISBN 978-1-60558-007-4, URL http://doi.acm.org/10.1145/ 1460563.1460628. [21] C. Wagner, S. Mitter, M. Strohmaier, and C. Krner, When social bots attack: Modeling susceptibility of users in online social networks. [22] K. Lee, J. Caverlee, and S. Webb, in Proceedings of the 19th International Conference on World Wide Web (ACM, New York, NY, USA, 2010), WWW \u201910, pp. 1139\u20131140, ISBN 978-1-60558-799-8, URL http://doi. acm.org/10.1145/1772690.1772843. [23] K. Lee, J. Caverlee, and S. Webb, in Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (ACM, New York, NY, USA, 2010), SIGIR \u201910, pp. 435\u2013442, ISBN 978-1-4503-0153-4, URL http://doi.acm.org/10.1145/ 1835449.1835522. [24] D. A. Broniatowski, M. J. Paul, and M. Dredze, PloS one 8, e83672 (2013). [25] E.M. Clark, C Jones, J.R. Williams, A.N. Kurti, M.C. Norotsky, C.M. Danforth, and P.S. Dodds, arXiv arXiv:1508.01843, (2015), URL http://arxiv.org/ abs/1508.01843. [26] J. Huang, R. Kornfield, G. Szczypka, and S. L. Emery, Tobacco control 23, iii26 (2014).\n8 Supplementary materials\nS1: Cross Sectional Classifier Performance\nFIG. S1 Calibrated Classifier Performance on 1 000 User Geo Tweet Dataset. Correctly classified humans (True Negatives), are coded in Green, while correctly identified automatons (True Positives) are coded in red. The 400 tweet average optimal thresholds from the cross validation experiment designate the thresholding for each feature. The black lines demonstrate each feature cutoff.\n9 S2: Model Comparisons\nReceiver Operator Characteristic (ROC) Curves of the performance of each individual feature are given in FIG. S2 below. We see each feature set performs comparably with accuracies (measured as AUC) ranging from 80% to 91% depending on the number of tweets compiled in the analysis. Combinations of each metric greatly increases the classification accuracy, the apparent most accurate model uses all three features. However, it is notable that combinations of two of these features perform strongly in comparison. It is also notable that the word introduction decay parameter coupled with the average URL rate performs as well as the Dissimilarity-URL model. The Dissimilarity metric requires determining the Longest Common Substring between many sets of tweets which is computationally expensive compared to analytically calculating the Word Introduction Decay Parameter.\n0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT ru\ne P\no si\nti v e R\na te\nAverage URL Rate\n25 Tweets, AUC = 0.82 (FP,TP,N) = (0.25,0.73,0.85) 50 Tweets, AUC = 0.84 (FP,TP,N) = (0.24,0.76,0.94) 100 Tweets, AUC = 0.84 (FP,TP,N) = (0.2,0.79,0.99) 300 Tweets, AUC = 0.87 (FP,TP,N) = (0.15,0.77,1.1) 400 Tweets, AUC = 0.87 (FP,TP,N) = (0.15,0.79,1.11)\n0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT ru\ne P\no si\nti v e R\na te\nAverage Dissimilarity\n25 Tweets, AUC = 0.82 (FP,TP,N) = (0.1,0.74,0.94) 50 Tweets, AUC = 0.87 (FP,TP,N) = (0.08,0.77,1.32) 100 Tweets, AUC = 0.88 (FP,TP,N) = (0.13,0.79,1.12) 300 Tweets, AUC = 0.88 (FP,TP,N) = (0.06,0.78,1.59) 400 Tweets, AUC = 0.89 (FP,TP,N) = (0.07,0.79,1.6)\n0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT ru\ne P\no si\nti v e R\na te\nWord Introduction Decay Rate\n25 Tweets, AUC = 0.83 (FP,TP,N) = (0.12,0.7,1.45) 50 Tweets, AUC = 0.86 (FP,TP,N) = (0.15,0.77,1.27) 100 Tweets, AUC = 0.88 (FP,TP,N) = (0.11,0.79,1.38) 300 Tweets, AUC = 0.91 (FP,TP,N) = (0.09,0.8,1.63) 400 Tweets, AUC = 0.91 (FP,TP,N) = (0.11,0.83,1.5)\n0.00 0.05 0.10 0.15 0.20 0.25 False Positive Rate\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nT ru\ne P\no si\nti v e R\na te\nCombinations\nAll: 400 Tweets, AUC = 0.97 (FP,TP,N) = (0.06,0.91,2.53) URLxDISS: 400 Tweets, AUC = 0.96 (FP,TP,N) = (0.06,0.89,2.24) URLxWI: 400 Tweets, AUC = 0.96 (FP,TP,N) = (0.05,0.89,2.5) DISSxWI: 400 Tweets, AUC = 0.94 (FP,TP,N) = (0.11,0.88,1.72)\n10\nS3: Word Introduction Decay Parameter\nHere, we expand upon our description of the Word Introduction Decay Parameter. This parameter is based upon random word shufflings of a text, but is computed via the analytic formula given in Eq 8. of [16]. To determine the decay rate parameter, we: (1) compute the word introduction rate as a function of word number, n, and (2) regress in log-log space for a power law decay rate parameter measuring in the final third for the tail, where the decay rate assumes the form of a power law. While this heuristic is crude and could certainly be refined to more precisely measure the power law region, which can vary with corpus size, the tightness of organic-user clustering afforded by this parameter, coupled with its computationally cheap cost when compared to the pairwise tweet dissimilarity metric affords us great power for bot discrimination.\nIn Figure S3 below the unique word introduction gaps are plotted in log-space as a function of unique word introduction number (rank) for each individual in our data set for various numbers of tweets. We see the distribution growing with the number of tweets. However, at each resolution, the human class is very distinctly distributed in comparison to each form of automation. Even within 25 tweets, the human clustering is visually apparent versus their automated counterparts.\nIn Figure S4 below, we visualize the stability of this parameter between an individual\u2019s set of tweets. The tweets from each account in our data set were resampled 100 times to recompute the word introduction decay parameter for 25 tweets (top) and 400 tweets (bottom). The standard deviation between each account\u2019s 100 decay parameters is given in the histogram. The average standard deviation across all individuals of each set, \u00b5, is given in the title of each histogram. Notably, the humans have very little deviation (i.e. within the \u2019window of forgiveness\u2019) for both sets of tweets. Automated classes, in particular spammers, can vary quite wildly depending on the sample of tweets that are analyzed. In particular, spammers look similar to (and usually are) humans and if the spamming event is not captured in the sampled data they will be misclassified. This decay parameter for human text is robust for varying sets of tweets and is quite distinguishable from automated accounts.\n0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10\nDeviation\n0\n50\n100\n150\n200\n250\n300\n350\n400\nF re\nq u\ne n\nc y\nHumans: \u00b5 = 0.02, N = 752\n0.00 0.05 0.10 0.15 0.20 0.25\nDeviation\n0\n5\n10\n15\n20\n25\n30 Cyborgs: \u00b5 = 0.06, N = 91\n0.00 0.05 0.10 0.15 0.20 0.25\nDeviation\n0\n5\n10\n15\n20\n25 Robots: \u00b5 = 0.07, N = 54\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nDeviation\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18 Spammers: \u00b5 = 0.25, N = 103\nWord Introduction Decay Standard Deviation Distribution: 25 Tweets\n0.02 0.03 0.04 0.05 0.06 0.07 0.08\nDeviation\n0\n50\n100\n150\n200\n250\nF re\nq u\ne n\nc y\nHumans: \u00b5 = 0.04, N = 752\n0.00 0.05 0.10 0.15 0.20 0.25\nDeviation\n0\n5\n10\n15\n20\n25\n30\n35 Cyborgs: \u00b5 = 0.06, N = 91\n0.00 0.05 0.10 0.15 0.20 0.25 0.30\nDeviation\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18 Robots: \u00b5 = 0.11, N = 54\n0.0 0.1 0.2 0.3 0.4 0.5\nDeviation\n0\n5\n10\n15\n20\n25 Spammers: \u00b5 = 0.23, N = 103\nWord Introduction Decay Standard Deviation Distribution: 400 Tweets\n11\nS4: Human Annotation of Twitter Accounts\nIn this section we describe the annotation process for classifying user accounts. Each of the one-thousand accounts were separately classified by two evaluators. All of the collected tweets from each account were assessed until the presence of automation was uncovered. An account was coded as \u2018human\u2019 if no automated posting presence was detected (i.e. an algorithm posting on the individual\u2019s behalf). The inter-rater reliability is summarized in the table below by listing the classification discrepancies between account classes. For each class, the counts of the type of rating are displayed. For example, the Human class had a total of 6 account discrepancies which is composed of 12 different scores (6 from each rater) - 5 human codings, 3 robot, 0 cyborg, and 4 spammers.\nThe reliability between raters was favorable (91.49%) for the entire dataset. The largest source of discrepancies were from the \u2018Robot\u2019 and \u2018Spammer\u2019 classes. The \u2019Spammer\u2019 class was most confused with \u2019Humans\u2019which is intuitive because many of these individuals were humans that utilized a algorithm to SPAM a particular message. \u2018Robots\u2019 were commonly confused with \u2019cyborgs\u2019. This is most likely due to boundary cases regarding both classes. The boundaries between these classes can at time be ambiguous. We classified cyborgs as automatons that were posting \u2018borrowed content\u2018 from another source or an account that used human assisted automation, i.e. a human that could be overseeing an automated account. Robots were defined as strictly posting structured automated messages in the forms of updates. Perhaps future work can work to sub classify different types of robots and cyborgs to investigate the ecology of these automatons.\nEach discrepancy was revisited by both annotators and discussed until a class was determined. For extreme boundary cases, the account ID was searched via the hyperlink: https://twitter.com/intent/user?user_ id=###. This helped observe other user features (screen name, description, etc.) to make a better decision about the user. This was especially helpful for identifying promotional accounts or news sources.\nScreen shots of particular accounts are given below to help describe the annotation process. Each annotator scrolled through a terminal interface containing each individual\u2019s tweets. Scrolling through \u2019human\u2018\ntext appears un-ordered and chaotic with very little structure. Automated accounts have very structured messages, hence these patterns become very apparent in comparison to human accounts.\nCyborg Account Example: A canonical cyborg\u2019s tweets are given below. This particular automaton is a news promotional account that is tweeting links to articles. Notice the description tailors off when it reaches the character limit and shows this with an ellipses (...) next to a URL.\nRobot Account Example: Robots tweet generically structured messages, usually as a form of update. These automatons have a very limited vocabulary and in general only change a few characters per tweet. This robot (below) is an example of a weather update bot that is tweeting statistics about the weather at regular intervals.\nSpammer Account Example: Tweets from a spamming human account are given below. This individual has utilized an algorithm to tweet at a musical celebrity. Many of these spam algorithms try to fool Twitter\u2019s detector by including a different number or symbol at the end of the tweet.\nNo. Tweet\n1 #CallMeCam n#CallMeCam n @USER n nIf Cameron called me it\u2019ll seriously make my day I love you please call me! 100\n2 #CallMeCam n#CallMeCam n @USER n nIf Cameron called me it\u2019ll seriously make my day I love you please call me! 321\n3 #CallMeCam n#CallMeCam n @USER n nIf Cameron called me it\u2019ll seriously make my day I love you please call me! 167\n4 S/o to @USER thanks for the support. Check out my music @USER URL I promise u won\u2019t be disappointed.\n5 S/o to @USER destiiny thanks for the support. Check out my music @USER URL I promise u won\u2019t be disappointed.\n6 S/o to @USER thanks for the support. Check out my music @USER URL I promise u won\u2019t be disappointed.\n7 nAshton Irwin from 5SOS n nMy birthday is in 11 days, nAnd it would be an amazing gift, nIf you could follow me. Ily n @USER n nX3126\n8 nAshton Irwin from 5SOS n nMy birthday is today, nAnd it would be an amazing gift, nIf you could follow me. Ily n @USER n nX5408\n9 nAshton Irwin from 5SOS n nMy birthday is in 22 days, nAnd it would be an amazing gift, nIf you could follow me. Ily n @USER n nX765\n10 nAshton Irwin from 5SOS n nMy birthday is in 8 days, nAnd it would be an amazing gift, nIf you could follow me. Ily n @USER n nX3422\n13"}], "references": [{"title": "and F", "author": ["V.S. Subrahmanian", "A. Azaria", "S. Durst", "V. Kagan", "A. Galstyan", "K. Lerman", "L. Zhu", "E. Ferrara", "A. Flammini"], "venue": "Menczer, arXiv arXiv:1601.05140, ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Can evil data scientists fool us all with the world\u2019s best spam", "author": ["D. Harris"], "venue": "goo.gl/psEguf ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "and V", "author": ["A. Sadilek", "H.A. Kautz"], "venue": "Silenzio, in ICWSM ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Journal of health economics 31", "author": ["A. Wagstaff", "A.J. Culyer"], "venue": "406 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "PloS one 8", "author": ["L. Mitchell", "M.R. Frank", "K.D. Harris", "P.S. Dodds", "C.M. Danforth"], "venue": "e64417 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "and A", "author": ["E. Ferrara", "O. Varol", "C. Davis", "F. Menczer"], "venue": "Flammini, CoRR abs/1407.5225 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "in In Collaboration", "author": ["F. Benevenuto", "G. Magno", "T. Rodrigues", "V. Almeida"], "venue": "Electronic messaging, Anti- Abuse and Spam Conference (CEAS ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "and S", "author": ["Z. Chu", "S. Gianvecchio", "H. Wang"], "venue": "Jajodia, in Proceedings of the 26th Annual Computer Security Applications Conference ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "and V", "author": ["K. Thomas", "C. Grier", "D. Song"], "venue": "Paxson, in Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "and J", "author": ["K. Lee", "B.D. Eoff"], "venue": "Caverlee, in In AAAI Int?l Conference on Weblogs and Social Media (ICWSM ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "in ICWSM", "author": ["K. Lee", "B.D. Eoff", "J. Caverlee"], "venue": "edited by W. W. Cohen and S. Gosling ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "and B", "author": ["H. Gao", "J. Hu", "C. Wilson", "Z. Li", "Y. Chen"], "venue": "Y. Zhao, in Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Tobacco control 23", "author": ["J. Huang", "R. Kornfield", "G. Szczypka", "S.L. Emery"], "venue": "iii26 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "and V", "author": ["K. Thomas", "C. Grier"], "venue": "Paxson, in Presented as part of the 5th USENIX Workshop on Large-Scale Exploits and Emergent Threats ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "in Machine Learning and Knowledge Discovery in Databases", "author": ["X. Wu", "Z. Feng", "W. Fan", "J. Gao", "Y. Yu"], "venue": "edited by H. Blockeel, K. Kersting, S. Nijssen, and F. elezn ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Text mixing shapes the anatomy of rank-frequency distributions: A modern Zipfian mechanics for natural language", "author": ["J.R. Williams", "J.P. Bagrow", "C.M. Danforth", "P.S. Dodds"], "venue": "Physical Review E (in press) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting automation of twitter accounts: Are you a human", "author": ["Z. Chu", "S. Gianvecchio", "H. Wang", "S. Jajodia"], "venue": "bot, or cyborg?, Dependable and Secure Computing, IEEE Transactions on v9, n6 pp 811-824 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Using sentiment to detect bots on Twitter: Are humans more opinionated than bots", "author": ["J.P. Dickerson", "V. Kagan", "H. Wang", "VS. Subrahmanian"], "venue": "Advances in Social Networks Analysis and Mining (ASONAM), 2014 IEEE/ACM International Conference on 620\u2013627 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "and K", "author": ["G. Brown", "T. Howe", "M. Ihbe", "A. Prakash"], "venue": "Borders, in Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "and S", "author": ["K. Lee", "J. Caverlee"], "venue": "Webb, in Proceedings of the 19th International Conference on World Wide Web ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "and S", "author": ["K. Lee", "J. Caverlee"], "venue": "Webb, in Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "PloS one 8", "author": ["D.A. Broniatowski", "M.J. Paul", "M. Dredze"], "venue": "e83672 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "C Jones", "author": ["E.M. Clark"], "venue": "J.R. Williams, A.N. Kurti, M.C. Norotsky, C.M. Danforth, and P.S. Dodds, arXiv arXiv:1508.01843, ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "As Twitter has evolved from a simple microblogging social media interface into a mainstream source of communication for the discussion of current events, politics, consumer goods/services, it has become increasingly enticing for parties to gameify the system by creating automated software to send messages to organic (human) accounts as a means for personal gain and for influence manipulation [1, 2].", "startOffset": 395, "endOffset": 401}, {"referenceID": 1, "context": "As Twitter has evolved from a simple microblogging social media interface into a mainstream source of communication for the discussion of current events, politics, consumer goods/services, it has become increasingly enticing for parties to gameify the system by creating automated software to send messages to organic (human) accounts as a means for personal gain and for influence manipulation [1, 2].", "startOffset": 395, "endOffset": 401}, {"referenceID": 2, "context": "Nevertheless, data from Twitter is becoming a source of interest in public health and economic research in monitoring the spread of disease [3, 4] and gaining insight into public health trends [5].", "startOffset": 140, "endOffset": 146}, {"referenceID": 3, "context": "Nevertheless, data from Twitter is becoming a source of interest in public health and economic research in monitoring the spread of disease [3, 4] and gaining insight into public health trends [5].", "startOffset": 140, "endOffset": 146}, {"referenceID": 4, "context": "Nevertheless, data from Twitter is becoming a source of interest in public health and economic research in monitoring the spread of disease [3, 4] and gaining insight into public health trends [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 5, "context": "In related work [6\u20139], researchers have built classification algorithms using metadata idiosyncratic to Twitter, including the number of followers, posting frequency, account age, number of user mentions/replies, username", "startOffset": 16, "endOffset": 21}, {"referenceID": 6, "context": "In related work [6\u20139], researchers have built classification algorithms using metadata idiosyncratic to Twitter, including the number of followers, posting frequency, account age, number of user mentions/replies, username", "startOffset": 16, "endOffset": 21}, {"referenceID": 7, "context": "In related work [6\u20139], researchers have built classification algorithms using metadata idiosyncratic to Twitter, including the number of followers, posting frequency, account age, number of user mentions/replies, username", "startOffset": 16, "endOffset": 21}, {"referenceID": 5, "context": "However, relying on metadata can be problematic: sophisticated spam algorithms now emulate the daily cycle of human activity and author borrowed content to appear human [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Another problematic spam tactic is the renting of accounts of legitimate users (called sponsored accounts), to introduce short bursts of spam and hide under the user\u2019s organic metadata to mask the attack [10].", "startOffset": 204, "endOffset": 208}, {"referenceID": 16, "context": "A content based classifier proposed by [18] measures the entropy between Twitter time intervals along with user meta data to classify Twitter accounts, and requires a comparable number of tweets (\u2265 60) for adequate classification accuracy as our proposed method.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "SentiBot, another content based classifier [19], utilizes latent Dirichlet allocation (LDA) for topical categorization combined with sentiment analysis techniques to classify individuals as either bots or humans.", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "To place our classifier in the context of recent work, we applied our algorithm to another set of accounts collected from the Social HoneyPot Experiment [11].", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "This work exacted a more elaborate approach to find automated accounts on Twitter by creating a network of fake accounts (called Devils [12]) that would tweet about trending topics amongst themselves in order to tempt robotic interactions.", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Since many malicious cyborgs on Twitter try to market an idea or product, a high proportion of their tweets contain URLs, analogous to spam campaigns studied on Facebook [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "Messages range from the backdoor advertising of goods and services [14] to those trying to influence social opinion or even censor political conversations [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "Messages range from the backdoor advertising of goods and services [14] to those trying to influence social opinion or even censor political conversations [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "These accounts act like puppets from a central algorithmic puppeteer to push their product on organic users while trying to appear like an organic user [16].", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "Included in this category are \u2018malicious promoter\u2019 accounts [11] that are radically promoting a business or an idea systematically.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "over time from a given sample of text [17].", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "From [17], the word introduction decay rate, \u03b1(n), is given as", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "For each user, the scaling exponent of the word introduction decay rate, \u03b1, is approximated by performing standard linear regression on the last third of the logtransformed tail of the average gap size distribution as a function of word introduction number, n [17].", "startOffset": 260, "endOffset": 264}, {"referenceID": 8, "context": "Hyperlinks (URLs) help automatons spread spam and malware [10, 20, 21] .", "startOffset": 58, "endOffset": 70}, {"referenceID": 18, "context": "Hyperlinks (URLs) help automatons spread spam and malware [10, 20, 21] .", "startOffset": 58, "endOffset": 70}, {"referenceID": 7, "context": "A high fraction of tweets from spammers tend to contain some type of URL in comparison to organic individuals, making the average URLs per tweet a valuable attribute for bot classification algorithms [8, 22, 23].", "startOffset": 200, "endOffset": 211}, {"referenceID": 19, "context": "A high fraction of tweets from spammers tend to contain some type of URL in comparison to organic individuals, making the average URLs per tweet a valuable attribute for bot classification algorithms [8, 22, 23].", "startOffset": 200, "endOffset": 211}, {"referenceID": 20, "context": "A high fraction of tweets from spammers tend to contain some type of URL in comparison to organic individuals, making the average URLs per tweet a valuable attribute for bot classification algorithms [8, 22, 23].", "startOffset": 200, "endOffset": 211}, {"referenceID": 9, "context": "The classifier was tested on the Social Honeypot Twitter-bot dataset provided by [11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "This is particularly important, since Twitter has become a possible outlet for health economics [4] research including monitoring patient satisfaction and modeling disease spread [3, 24].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "This is particularly important, since Twitter has become a possible outlet for health economics [4] research including monitoring patient satisfaction and modeling disease spread [3, 24].", "startOffset": 179, "endOffset": 186}, {"referenceID": 21, "context": "This is particularly important, since Twitter has become a possible outlet for health economics [4] research including monitoring patient satisfaction and modeling disease spread [3, 24].", "startOffset": 179, "endOffset": 186}, {"referenceID": 22, "context": "Monitoring excessive social media marketing of electronic nicotine delivery systems (also known as e-cigarettes), discussed in [25, 26], makes classifying organic and automated activity relevant for research that can benefit policy-makers regarding public health agendas.", "startOffset": 127, "endOffset": 135}, {"referenceID": 0, "context": "[1] V.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "of [16].", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Eric M. Clark, 2, 3, 4, 5, \u2217 Jake Ryland Williams, 2, 3, 4 Chris A. Jones, 6, 7 Richard A. Galbraith, 9 Christopher M. Danforth, 2, 3, 4 and Peter Sheridan Dodds 2, 3, 4 Department of Mathematics & Statistics Vermont Complex Systems Center Vermont Advanced Computing Core Computational Story Lab Department of Surgery Global Health Economics Unit of the Vermont Center for Clinical and Translational Science Vermont Center for Behavior and Health Department of Medicine Vermont Center for Clinical and Translational Science (Dated: June 15, 2016)", "creator": "LaTeX with hyperref package"}}}