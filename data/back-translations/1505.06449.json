{"id": "1505.06449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2015", "title": "Efficient Elastic Net Regularization for Sparse Linear Models", "abstract": "We extend previous work on efficient training of linear models by applying stochastic updates only to features that are not zero, and conveniently bring weights up to date. So far, only closed form updates have been described for the popular $\\ ell _ 1 $, $\\ ell _ {\\ infty} $and the rarely used $\\ ell _ 2 $norm. We extend this work by showing the correct closed form updates for the popular $\\ ell ^ 2 _ 2 $and elastic grid-regulated models. We show a dynamic programming algorithm for calculating the correct elastic network update with only one time-constant substance compression per update. Our algorithm handles both fixed and decreasing learning rates, and we derive the result both for the introduction of herchastic grade drops (SGD) and for backward division (FoBoS). We validate the algorithm per non-actualized net zero set, and otherwise show a fast sack of 260.12 attributes per non-actualized algorithm.", "histories": [["v1", "Sun, 24 May 2015 15:42:58 GMT  (12kb)", "http://arxiv.org/abs/1505.06449v1", null], ["v2", "Tue, 26 May 2015 07:28:50 GMT  (12kb)", "http://arxiv.org/abs/1505.06449v2", null], ["v3", "Thu, 2 Jul 2015 20:44:57 GMT  (13kb)", "http://arxiv.org/abs/1505.06449v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zachary c lipton", "charles elkan"], "accepted": false, "id": "1505.06449"}, "pdf": {"name": "1505.06449.pdf", "metadata": {"source": "CRF", "title": "Efficient Elastic Net Regularization for Sparse Linear Models", "authors": ["Zachary C. Lipton", "Charles Elkan"], "emails": ["elkan}@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n06 44\n9v 1\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "Machine learning practitioners are often tasked with learning over large datasets. An abundance of affordable storage, network bandwidth, and the proliferation of datagenerating devices, have combined to saddle many organizations with big data problems. Document auto-tagging problems frequently contain millions of documents, hundreds of thousands of features, and thousands of labels. When the number of labels and features is large, even the weight matrix of a dense linear model may not fit in main memory on a single workstation. However, for many applications features are sparse and good regularized models can be made compact. Given such sparsity, it is desirable train machine learning models using algorithms that require time that scales only with the number of non-zero features.\nOnline algorithms, such as stochastic gradient descent, are widely used to train high-dimensional models on large-scale data. In these methods, each example is processed one at a time, updating the model on the fly. When a dataset is sparse and the\nobjective function is not regularized, this sparsity can be exploited by updating only the weights corresponding to non-zero features. However, to prevent overfitting models to high-dimensional data, it is often necessary to apply a regularization penalty, imposing a prior belief that the model parameters should be sparse and small in magnitude. Problematically, widely-used regularizers such as \u21131 (lasso), \u211322 (ridge), and elastic net (\u21131 + \u211322) destroy the sparsity of the gradient, seemingly requiring an update for each nonzero weight for each example.\nIn this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6]. As each example is processed, we update only those weights corresponding to non-zero features. The model is brought current as needed by lazily applying closed-form constant-time updates whenever a feature becomes non-zero. For sparse data sets, the algorithm runs in time independent of the nominal dimensionality, scaling linearly with the number of non-zero features per example.\nTo date, only the closed form updates for the \u21131, \u2113\u221e, and the rarely used \u21132 norm have been described. We extend this work by showing the proper closed form updates for the popular \u211322 and elastic net regularized models. When the learning rate varies (typically decreasing as a function of time), we show that the proper elastic net update can be computed with a dynamic programming algorithm, requiring only one constanttime subproblem computation per update. 1\nAdditionally, we implement the system, showing that on a widely studied dataset containing the abstracts of millions of articles from biomedical literature, we can train a logistic regression classifier with elastic net regularization 612 times faster than an otherwise identical model with dense updates."}, {"heading": "2 Background and Definitions", "text": "We consider a data matrix X \u2208 Rn\u00d7d where each row xi corresponds to one of n examples and each column, indexed by j corresponds to one of d features. We desire a linear model, parametrized by a vector w \u2208 Rd, which minimizes a convex objective function F (w), expressible as \u2211n\ni=1 Fi(w), where F returns the loss with respect to the entire corpus X and Fi returns the loss calculated on example xi.\nIn many datasets, the vast majority of entries xij are zero-valued. The-bag-ofwords representation of text is one such case. We say that such datasets are sparse. When features correspond to counts or binary values, as in bag-of-words, we sometimes say say that a zero-valued feature xij is absent. We use p to refer to the average number of nonzero features per example. Naturally, when a dataset is sparse, we prefer algorithms which take time O(p) to those O(d).\n1Our dynamic programming algorithms for constant-time updates with varying learning rates add time O(1) per stochastic gradient update and have space complexity O(T ), where T is the total number of stochastic gradient updates. If this space complexity is too great, that problem can be solved by alotting a space budget and bringing all weights current when the budget is filled. As this cost is amortized across many iterations, it adds negligibly to the total running time."}, {"heading": "2.1 Regularization", "text": "To prevent overfitting, regularization restricts the freedom of a model\u2019s parameters, penalizing their distance from some prior belief. Most widely used regularizers penalize large weights with an objective function of the form\nF (w) = L(w) +R(w) (1)\nMany commonly used regularizers R(w) are of the form \u03bb||w|| where \u03bb determines the strength of regularization and \u21130, \u21131, \u211322, and \u2113\u221e are common choices for the norm. The \u21131 regularizer is popular owing to its tendency to produce sparse models. In this paper, we focus on elastic net, a linear combination of \u21131 and \u211322 regularization that has been shown to produce comparably sparse models to \u21131 while often achieving superior accuracy [12]."}, {"heading": "2.2 Stochastic Gradient Descent", "text": "Gradient descent is a common strategy to find the optimal parametersw. Any gradient descent method requires an initial learning rate \u03b7. Then to minimize F (w), a number of steps T , indexed by t, are taken in the direction of the negative gradient:\nw(t+1) := w(t) \u2212 \u03b7\nn \u2211\ni=1\n\u2207Fi(w).\nAn appropriately attenuated learning rate ensures both that the optimal parameters will eventually be reached and that the algorithm will yield a value within a distance \u01eb of the optimal value for any small value \u01eb [2] .\nTraditional (or \u201dbatch\u201d) gradient descent requires a pass through the entire dataset for each update. Stochastic gradient descent (SGD) circumvents this problem by updating the model once after visiting each example. With SGD, one determines a number of epochs E indexed by e. Within each epoch, examples are randomly selected one at a time (or in \u201dmini-batches\u201d). For simplicity of notation, w.l.o.g., we will assume the examples are selected one at a time. The gradient is calculated with respect to that example, and the model is updated according to the rule wt+1 := wt \u2212 \u03b7\u2207w(t)Fi. Because the examples are chosen randomly, the expected value of this noisy gradient is identical to the true value of the gradient taken with respect to the entire corpus.\nGiven a continuously differentiable convex objective function F (w), stochastic gradient descent is known to converge for learning rates \u03b7 that satisfy \u2211\nt \u03b7t = \u221e and \u2211\nt \u03b7 2 t < \u221e [1]. Learning rates \u03b7i \u221d 1 t and \u03b7i \u221d 1\u221at both satisfy these properties. 2 For many objective functions, such as linear or logistic regression without regularization, the noisy gradient \u2207wFi is sparse when the input is sparse. In these cases, one needs only to update the weights corresponding to non-zero features in the current example xi. This runs in time O(p), where p \u226a d is the average number of nonzero features in an example.\n2 Some common objective functions as those with \u21131 regularization, are not differentiable when weights are equal to zero. However, forward backward splitting (FoBoS) [10], offers a principled approach to this problem.\nRegularization, however, can ruin the sparsity of the gradient. Consider an objective function of form (Equation 1), where ||w|| for some norm ||\u00b7||. In these cases, even when the feature xij = 0, the gradient \u2207wjFi is nonzero owing to the regularization penalty. A simple and correct solution is to update weights when either the weight or feature is nonzero, but to ignore weights when both are zero. Given feature sparsity and persistent model sparsity throughout training, ignoring all weights wj = 0 corresponding to features xij = 0 provides a substantial benefit. But such an algorithm would still scale with the size of the model, which may be far larger than p. Our algorithm scales in time complexity O(p)."}, {"heading": "2.3 Forward Backward Splitting", "text": "Proximal algorithms are an approach to optimization in which each update consists of solving a convex optimization problem [8]. Forward Backward Splitting (FoBoS) [10] uses proximal algorithms to provide a principled approach to online optimization with non-smooth regularizers. We first step in the direction of the negative gradient of the differentiable loss function. We then update the weight by solving a convex optimization problem, which simultaneously penalizes distance from the new parameters and minimizes the regularization term. Duchi and Singer established the convergence of the Forward Backward method in [10].\nFirst an standard unregularized stochastic gradient step is applied\nw(t+ 1 2 ) = w(t) \u2212 \u03b7(t)\u2207wLi (2)\nThen a convex optimization is solved which applies the regularization penalty. For elastic net the problem to be solved is\nw(t+1) = argminw\n(\n1 2 ||w \u2212w(t+ 1 2 )||22 + \u03b7 t\u03bb1||w||1 + \u03b7t\u03bb2 2 ||w||22\n)\n(3)\nNote that when \u2207wjL = 0, w (t+ 12 ) j = w (t) j . The problems corresponding to \u21131 or \u2113 2 2 separately can easily be derived by setting the corresponding \u03bb to 0."}, {"heading": "3 Lazy Updates", "text": "We extend the method for lazy updates introduced in [2], [6], and [10] to the case of \u21132 and elastic net regularization. The essence of the algorithm for sparse lazy updates is as follows (Algorithm 1). We maintain an array \u03c8 \u2208 Rd in which each \u03c8j stores the index of the last iteration at which each feature was nonzero and a counter k, which stores the index of the current iteration. When processing each example xi, we iterate through the nonzero features xij , comparing the current iteration index k to \u03c8j . For each feature with nonzero weight wj , we lazily apply the k\u2212\u03c8j successive updates in constant time, bringing those weights current. Using the updated weights, we compute the prediction y\u0302(k) with the current parameters w(k). We then compute the gradient and update the weights. When training is complete, we pass once through all nonzero weights to apply the lazy updates to bring the model current. Provided that we can\napply all lazy updates in O(1) time, our algorithm processes each example in O(p) time regardless of the dimension d.\nAlgorithm 1 Lazy Updates\nRequire: \u03c8 \u2208 Rd\nfor t \u2208 1, ..., T do Sample xi randomly from the training set for j s.t. xij 6= 0 do\nwj \u2190 Lazy(wj, t, \u03c8j) \u03c8j \u2190 t\nend for w \u2190 w \u2212 \u03b7(t)\u2207wFi\nend for\nTo use the algorithm with a chosen regularizer, it remains only to demonstrate the existence of constant time updates. In the following subsections, we derive the closed form updates for \u21131, \u211322 and elastic net regularization, starting with the simple case when the learning rate \u03b7 is fixed during each epoch, and extending to the more complicated case when the learning rate is decreased between iterations as a function of time. These results hold for schedule of weight decrease that depend on time, but cannot be directly applied to Adagrad, an algorithm for which each weight has a separate learning rate which is decreased with the inverse of the accumulated sum of squared gradients with respect to that weight."}, {"heading": "4 Prior Work", "text": "Over the last several years, a large body of work has advanced the field of online learning. Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].\nIn 2008 Carpenter described an idea for performing lazy updates for stochastic gradient descent [2]. In his method, they maintain a vector \u03c8 \u2208 Nd, where each \u03c8i stores the index of the last epoch in which each weight was last regularized. They then perform periodic batch updates. However, as they acknowledge, the system they describe results in updates that do not produce the same result as applying an update after each time step.\nLangford et al. concurrently developed a similar approach for lazily updating \u21131 regularized linear models [6]. They restrict their attention to \u21131 models. Additionally, they only describe the closed form update when the learning rate \u03b7 is constant, although they do suggest that an update can be derived when \u03b7i decays as i grows large. We extend this work, deriving the closed form updates for \u211322, and elastic net regularization. Our algorithms work for both fixed and varying learning rates.\nIn 2009, Duchi and Singer describe a method they call Forward Backward Splitting (FoBoS) for online optimization of a regularized loss function [4]. Here, after each gradient step to minimize the loss function, the weight is regularized by solving a convex optimization problem. They derive regularization updates for \u21131, \u211322, \u2113 2 2, and \u2113\u221e norms. Further, they share the insight of applying lazy updates, when training on sparse high dimensional data. Their lazy updates hold for all norms \u2113q for q \u2208 1, 2,\u221e, However they do not hold for the commonly used \u211322 norm. Consequently it also does not hold for mixed norms involving the \u211322 norm such as the widely used elastic net (\u21131 + \u2113 2 2)."}, {"heading": "5 Closed Form Lazy Updates for SGD", "text": "In this section, we will derive constant-time stochastic gradient updates when processing examples from sparse datasets. Using these, the lazy update algorithm can train linear models with time complexity O(p). For brevity, we will describe the more general case where the learning rate is varied. When the learning rate is constant the algorithm can be easily modified to have O(1) space complexity."}, {"heading": "5.1 Lazy SGD Update \u21131 Regularization with Attenuated Learning Rate", "text": "The closed form update for \u21131 regularized models was derived by Singer and Duchi [10]. The constant time lazy update can be applied with:\nw (k) j = sgn(w (\u03c8j) j )\n[\n|w (\u03c8j) j | \u2212 \u03bb1 (S(k \u2212 1)\u2212 S(\u03c8j \u2212 1))\n]\n+ . (4)\nwhere S(t) is a function that returns the partial sum \u2211t \u03c4=0 \u03b7 (\u03c4).\nThe sum \u2211t+n\u22121 \u03c4=t \u03b7 (\u03c4) can be computed in constant time, using a dynamic programming scheme. On each iteration t, we compute S(t) in constant time given its predecessor as S(t) = \u03b7(t)+S(t\u2212 1). The base case for this recursion is S(0) = \u03b7(0). We then cache this value in an array for subsequent constant time lookup.\nWhen the learning rate decays with 1/t, the terms \u03b7(\u03c4) follow the harmonic series, and we do not require dynamic programming to compute the lazy update for \u21131regularization. Each partial sum of the harmonic series is a harmonic number H(t) = \u2211t\ni=1 1 t . Clearly \u2211t+n\u22121 \u03c4=t \u03b7 (\u03c4) = \u03b7(0) (H(t+ n)\u2212H(t)), where H\u03c4 is the \u03c4th harmonic number. While there is no instantly computable analytic expression to calculate the \u03c4th harmonic number, there exist good approximations.\nThe O(T ) space complexity of this algorithm may seem problematic. However, this easily dealt with by bringing all weights current after each epoch. The cost to do this is amortized across all iterations and is thus negligible."}, {"heading": "5.2 Lazy SGD Update for \u211322 Regularization with Attenuated Learning Rate", "text": "We now consider the case of \u211322 regression with attenuated learning rate. For a given examplexi, if a feature xij = 0 and the learning rate is varying, the stochastic gradient update rule for an \u211322 regularized objective is\nw (t+1) j = w (t) j \u2212 \u03b7 (t)\u03bb2 \u00b7 w (t) j\n(5)\nConsidering successive updates, the decreasing learning rate prevents a collecting lazy updates as terms in a geometric series as we could when if learning rate was fixed. However, we can employ a dynamic programming strategy, similar to the system we demonstrated for \u21131 with attenuated learning rate.\nLemma 1. For SGD with \u211322 regularization, the constant time lazy update to bring a weight current from iteration \u03c8j to k is given by\nw (k) j = w (\u03c8j) j\nP (k \u2212 1)\nP (\u03c8j \u2212 1) (6)\nwhere P(t) is the partial product \u220ft \u03c4=0(1\u2212 \u03b7 (\u03c4)\u03bb2).\nProof. Rewriting the above expressions for lazy updates:\nw (t+1) j = w (t) j (1\u2212 \u03b7 (t)\u03bb2)\nw (t+n) j = w (t) j (1\u2212 \u03b7 (t)\u03bb2)(1 \u2212 \u03b7 (t+1)\u03bb2) \u00b7 ... \u00b7 (1 \u2212 \u03b7 (t+n\u22121)\u03bb2) (7)\nThe products P (t) = \u220ft \u03c4=0(1 \u2212 \u03b7 (\u03c4)\u03bb2) can be cached on each iteration in constant time using the recursive relation\nP (t) = (1 \u2212 \u03b7(t)\u03bb2)P (t\u2212 1)\nThe base case is P (0) = a0 = (1\u2212 \u03b70\u03bb2). Given cached values P (0), ..., P (t+ n), it is then easy to calculate the exact lazy update in constant time:\nw (t+n) j = w (t) j\nP (t+ n\u2212 1)\nP (t\u2212 1) . (8)\nOur claim follows.\nAs in the case of \u211322 regularization with fixed learning rate, we needn\u2019t worry that the regularization update will flip the sign of our weightwj , owing to \u2200t \u2265 0, P (t) > 0."}, {"heading": "5.3 Lazy SGD Update for Elastic Net Regularization with Attenuated Learning Rate", "text": "Finally, we derive the constant time lazy update for SGD with elastic net regularization. recalling that a model regularized by elastic net has an objective function of the form\nF (w) = L(w) + \u03bb1||w||1 + \u03bb2 2 ||w||22.\nWhen a feature xj = 0, the SGD update rule is\nw (t+1) j = sgn(w (t) j )\n[\n|w (t) j | \u2212 \u03b7 (t)\u03bb1 \u2212 \u03b7 (t)\u03bb2|w (t) j |\n]\n+\n= sgn(w (t) j )\n[\n(1 \u2212 \u03b7(t)\u03bb2)|w (t) j | \u2212 \u03b7 (t)\u03bb1\n]\n+\n(9)\nTheorem 1. To bring a weight w\u03c8jj current to w k j using update (Equation 9), the constant time update is\nw (k) j = sgn(w \u03c8j j )\n[\n|wj | \u03c8j\nP (k \u2212 1)\nP (\u03c8j \u2212 1) \u2212 P (k \u2212 1) \u00b7 (B(k \u2212 1)\u2212B(\u03c8j \u2212 1))\n]\n+ (10)\nProof. The attenuated learning rate prevents us from working out a simple expansion like ??. Instead, we can write the following inductive expressions for a few consecutive terms in the sequence\nw (t+1) j = sgn(w (t) j )\n[\n(1\u2212 \u03b7(t)\u03bb2)|w (t) j | \u2212 \u03b7 (t)\u03bb1\n]\n+ (11)\nSubstituting a\u03c4 = (1 \u2212 \u03b7(\u03c4)\u03bb2) and b\u03c4 = \u2212\u03b7(\u03c4)\u03bb1 gives\nw(t+1) = sgn(w (t) j )\n[\nat|w (t)|+ bt\n]\n+\n...\nw(t+n) = sgn(w (t) j ) [ a(t+n\u22121)(...a(t+1) ( atw t \u2212 bt ) \u2212 b(t+1)...)\u2212 b(t+n\u22121) ]\n+\n= sgn(w (t) j )\n[\n|w (t) j |\nt+n\u22121 \u220f\n\u03c4=t\na\u03c4 +\nt+n\u22122 \u2211\n\u03c4=t\nbi\n(\nt+n\u22122 \u220f\nq=\u03c4\naq\n)\n+ b(t+n\u22121)\n]\n+\n(12)\nThe leftmost term, \u220ft+n\u22121 \u03c4=t a\u03c4 can be calculated in constant time as P (t+n\u22121)/P (t\u2212 1) using cached values from the dynamic programming scheme discussed in the previous section.\nTo cache the remaining terms, we group the center and right-most terms and apply the following simplifications\nt+n\u22122 \u2211\n\u03c4=t\nbi\n(\nt+n\u22122 \u220f\nq=\u03c4\naq\n)\n+ bt+n\u22121\n= bt P (t+ n\u2212 2)\nP (t\u2212 1) + bt+1\nP (t+ n\u2212 2)\nP (t) + ...+ bt+n\u22121\nP (t+ n\u2212 2)\nP (t+ n\u2212 2)\n= \u2212\u03bb1P (t+ n\u2212 2)\n(\n\u03b7(t)\nP (t\u2212 1) +\n\u03b7(t+1)\nP (t) + ...+\n\u03b7(t+n\u22121)\nP (t+ n\u2212 2)\n) (13)\nWe now add a new layer to our dynamic programming formulation. In addition to pre-calculating all values P (t) as we go, we define a partial sum over inverses of partial products\nB(t) = t \u2211\n\u03c4=0\n\u03b7(\u03c4)\nP (\u03c4 \u2212 1) .\nGiven that P (t \u2212 1) can be accessed in O(1) at time t, B(t) can now be cached in constant time. The dynamic programming here depends upon the simple recurrence relation\nB(t) = B(t\u2212 1) + \u03b7(t)\nP (t\u2212 1)\nwith the base case B(\u22121) = 0 Thus B(0) = 0 + \u03b7(0)/1 = \u03b7(0). Thus, for SGD elastic net with heuristic clipping and attenuated learning rate, the update rule to lazily apply any number n of successive regularization updates in constant time to weight wj is:\nw(t+n) = sgn(w (t) j )\n[\n|w (t) j |\nP (t+ n\u2212 1)\nP (t\u2212 1) \u2212 \u03bb1P (t+ n\u2212 1) (B(t+ n\u2212 1)\u2212B(t\u2212 1))\n]\n+\n(14)"}, {"heading": "6 Lazy Updates for Forward Backward Splitting", "text": "Recall the optimization problems for \u21131, \u211322 and elastic net regularization (Section 2.3). They also described the lazy update for \u21131 regularization which is the same as the truncated gradient update in (Equation 4). Thus we turn our attention to FoBoS updates for \u211322 and elastic net regularization."}, {"heading": "6.1 Lazy FoBoS Update for \u211322 Regularization", "text": "For \u211322 regularization, to apply the regularization update we solve the problem from Equation 3 with \u03bb1 set to 0. Solving forw\u2217 gives the simple updatew (t+1) j = w (t) j /(1+ \u03b7(t)\u03bb2) whenever xij = 0. Note that this differs from the standard stochastic gradient descent step.\nWe can store the values P \u2032(t) = \u220ft \u03c4=0 1 1+\u03b7t\u03bb2 . Thus constant time lazy update for\nFoBoS with \u211322 regularization to bring a weight current at time k from time \u03c8j is\nwkj = w (\u03c8j) j\nP \u2032(k \u2212 1)\nP \u2032(\u03c8j \u2212 1) (15)\nwhere P \u2032(t) = (1 + \u03b7(t)\u03bb2)1 \u00b7 P \u2032(t\u2212 1) with base case P \u2032(\u22121) = 1."}, {"heading": "6.2 Lazy FoboS Update for Elastic Net Regularization", "text": "Finally, in the case of elastic net regularization via forward backward splitting, we solve the convex optimization problem from Equation 3. This problem also comes apart and can be optimized for each wj separately. Setting the derivative wrt wj yields the following optimal solution:\nw (t+1) j = sgn(w (t) j )\n[\n|w (t) j | \u2212 \u03b7 (t)\u03bb1\n\u03b7t\u03bb2 + 1\n]\n+\nTheorem 2. The exact constant time lazy update for FoBoS with elastic net regularization and attenuated learning rate to bring a weight current at time k from time \u03c8j is\nw (k) j = sgn(w (\u03c8j) j )\n[\n|w (\u03c8j) j |\nP \u2032(k \u2212 1)\nP \u2032(\u03c8j \u2212 1) \u2212 P \u2032(k \u2212 1) \u00b7 \u03bb1 (B \u2032(k \u2212 1)\u2212B\u2032(\u03c8j \u2212 1))\n]\n+ (16)\nwhere B\u2032(t) = B\u2032(t\u2212 1) + \u03b7 (t)\nP (t\u22121) with base cases B \u2032(0) = \u03b7(0) and B\u2032(\u22121) = 0.\nProof. We substitute at = (\u03b7(t)\u03bb2 + 1)\u22121 and bt = \u2212\u03b7(t)\u03bb1. Note that neither at nor bt depends upon wj . Consider successive updates:\nwt+1j = sgn(w t j) [ at(|w (t) j |+ bt) ]\n+\nw (t+n) j = sgn(w t j)\n\n|wtj |\nt+n\u22121 \u220f\n\u03b2=t\na\u03b2 +\nt+n\u22121 \u2211\n\u03c4=t\n(\nb\u03c4\nt+n\u22121 \u220f\n\u03b1=\u03c4\na\u03b1\n)\n\n\n+\n. (17)\nIn the first term, P \u2032(t+n\u22121) P \u2032(t\u22121) can be substituted for \u220ft+n\u22121 \u03b2=t a\u03b2 . The second term can be expanded\nt+n\u22121 \u2211\n\u03c4=t\n(\nb\u03c4\nt+n\u22121 \u220f\n\u03b1=\u03c4\na\u03b1\n)\n=\nt+n\u22121 \u2211\n\u03c4=t\n(\nb\u03c4 P (t+ n\u2212 1)\nP (\u03c4 \u2212 1)\n)\n= \u2212P (t+ n\u2212 1) \u00b7 \u03bb1\nt+n\u22121 \u2211\n\u03c4=t\n(\n\u03b7(\u03c4)\nP (\u03c4 \u2212 1)\n)\n(18)\nUsing the dynamic programming approach, for each iteration t, we can calculate\nB\u2032(t) = B\u2032(t\u2212 1) + \u03b7(t)\nP (t\u2212 1)\nwith the base cases B\u2032(0) = \u03b7(0) and B\u2032(\u22121) = 0. Thus\nw (t+n) j = sgn(w t j)\n[\n|wtj | P \u2032(t+ n\u2212 1)\nP \u2032(t\u2212 1) \u2212 P \u2032(t+ n\u2212 1) \u00b7 \u03bb1 (B \u2032(t+ n\u2212 1)\u2212B\u2032(t\u2212 1))\n]\n+ (19)"}, {"heading": "7 Experiments", "text": "We do not need justify the usefulness of logistic regression with elastic net regularization. However, to confirm the correctness and speed of our dynamic programming solution, we tested the system on a bag-of-words representation of abstracts from biomedical articles indexed in Medline. Our dataset contained 1, 000, 000 examples, 260,941 features and 88.54 nonzero features per document\nWe prototyped the system in Python. Sparse datasets are represented as lists of dictionaries, where keys correspond to feature indices and values correspond to word counts. We implemented both standard and lazy FoBoS for logistic regression regularized with elastic net. We confirmed on a synthetic dataset that the standard FoBoS updates and lazy updates output identical weights up to 4 significant figures. To make a fair comparison, both systems exploit sparsity when calculating predictions.\nLazy update FoBos performed 612.2 times faster than FoBoS with dense regularization updates. It is important to consider that a pure speedup proportional to the number of zeros to non-zeros in the training data would be 2947.1528\u00d7. Our additional calculations provide a constant factor slowdown. But overall, when considerably less than one quarter of all features have nonzero values, our algorithm provides a tremendous speedup. While our dynamic programming strategy consumes space linear in the number of iterations, it does not present a significant time penalty. Further, when training one epoch, storing two floating point numbers per example is a modest use of space compared to the data itself.\nFoBos Elastic Net w/ Lazy Updates FoBos Elastic Net w/ Dense Updates 1893 Examples / Second 3.086 Examples / Second"}, {"heading": "8 Discussion", "text": "Many interesting datasets are high-dimensional, and many high-dimensional datasets are sparse. To be fast, algorithms should scale with the number of non-zeros per example, not the nominal dimensionality. We have demonstrated an algorithm for fast learning on linear models with \u211322 and elastic net regularization, verifying its correctness analytically and performance empirically."}, {"heading": "Acknowledgments", "text": "This research was conducted with generous support from the Division of Biomedical Informatics at the University of California, San Diego, which has funded the first author via a training grant from the National Library of Medicine.Galen Andrew began evaluating lazy updates for multilabel classification with Charles Elkan in the summer of 2014. His notes provided an insightful starting point for this research. Sharad Vikram provided invaluable help in checking the derivations of closed form updates."}], "references": [{"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "author": ["Bob Carpenter"], "venue": "Alias-i, Inc., Tech. Rep, pages", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Efficient learning using forward-backward splitting", "author": ["Yoram Singer", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "In this paper, we focus on elastic net, a linear combination of l1 and l2 regularization that has been shown to produce comparably sparse models to l1 while often achieving superior accuracy [12].", "startOffset": 191, "endOffset": 195}, {"referenceID": 1, "context": "An appropriately attenuated learning rate ensures both that the optimal parameters will eventually be reached and that the algorithm will yield a value within a distance \u01eb of the optimal value for any small value \u01eb [2] .", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "t \u03b7t = \u221e and \u2211 t \u03b7 2 t < \u221e [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "However, forward backward splitting (FoBoS) [10], offers a principled approach to this problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "Forward Backward Splitting (FoBoS) [10] uses proximal algorithms to provide a principled approach to online optimization with non-smooth regularizers.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "Duchi and Singer established the convergence of the Forward Backward method in [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 9, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 215, "endOffset": 218}, {"referenceID": 7, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 294, "endOffset": 297}, {"referenceID": 4, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 347, "endOffset": 350}, {"referenceID": 1, "context": "In 2008 Carpenter described an idea for performing lazy updates for stochastic gradient descent [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "concurrently developed a similar approach for lazily updating l1 regularized linear models [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In 2009, Duchi and Singer describe a method they call Forward Backward Splitting (FoBoS) for online optimization of a regularized loss function [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "1 Lazy SGD Update l1 Regularization with Attenuated Learning Rate The closed form update for l1 regularized models was derived by Singer and Duchi [10].", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "We extend previous work on efficiently training linear models by applying stochastic updates to non-zero features only, lazily bringing weights current as needed. To date, only the closed form updates for the l1, l\u221e, and the rarely used l2 norm have been described. We extend this work by showing the proper closed form updates for the popular l22 and elastic net regularized models. We show a dynamic programming algorithm to calculate the proper elastic net update with only one constant-time subproblem computation per update. Our algorithm handles both fixed and decreasing learning rates and we derive the result for both stochastic gradient descent (SGD) and forward backward splitting (FoBoS). We empirically validate the algorithm, showing that on a bag-of-words dataset with 260, 941 features and 88 nonzero features on average per example, our method trains a logistic regression classifier with elastic net regularization 612 times faster than an otherwise identical implementation with dense updates.", "creator": "LaTeX with hyperref package"}}}