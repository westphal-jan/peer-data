{"id": "1505.06816", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Representing Meaning with a Combination of Logical and Distributional Models", "abstract": "NLP tasks differ in the semantic information they require, and at this point not a single semantic representation meets all requirements. Logic-based representations characterize the sentence structure, but do not capture the tiered aspect of meaning. Distribution models give graduated similarity assessments for words and phrases, but do not sufficiently capture the general sentence structure. Thus, it was argued that the two are complementary. In this paper, we use a hybrid approach that combines logic-based and distribution-related semantics by probabilistic logical conclusions in Markov logic networks (MLNs). We focus on textual entailment (RTE), a task that can utilize the strengths of both representations. Our system consists of three components, 1) parses and task representation, in which RTE problems are represented in probability theory logic (MLNs). This is significantly different from the representation in standardized first order logic (RTE), 2) we construct the knowledge-based form of parasistic constituent sources as if we parse the following constructions in a rasiena.", "histories": [["v1", "Tue, 26 May 2015 06:19:18 GMT  (126kb,D)", "http://arxiv.org/abs/1505.06816v1", null], ["v2", "Sun, 29 Nov 2015 03:51:26 GMT  (184kb,D)", "http://arxiv.org/abs/1505.06816v2", null], ["v3", "Tue, 23 Feb 2016 03:46:07 GMT  (184kb,D)", "http://arxiv.org/abs/1505.06816v3", null], ["v4", "Tue, 7 Jun 2016 13:30:01 GMT  (180kb,D)", "http://arxiv.org/abs/1505.06816v4", "Special issue of Computational Linguistics on Formal Distributional Semantics, 2016"], ["v5", "Wed, 8 Jun 2016 15:07:47 GMT  (180kb,D)", "http://arxiv.org/abs/1505.06816v5", "Special issue of Computational Linguistics on Formal Distributional Semantics, 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["i beltagy", "stephen roller", "pengxiang cheng", "katrin erk", "raymond j mooney"], "accepted": false, "id": "1505.06816"}, "pdf": {"name": "1505.06816.pdf", "metadata": {"source": "CRF", "title": "Representing Meaning with a Combination of Logical Form and Vectors", "authors": ["Islam Beltagy", "Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J. Mooney"], "emails": [], "sections": [{"heading": null, "text": "Representing Meaning with a Combination of Logical Form and Vectors\nIslam Beltagy Computer Science Department The University of Texas at Austin\nStephen Roller Computer Science Department The University of Texas at Austin\nPengxiang Cheng Computer Science Department The University of Texas at Austin\nKatrin Erk Linguistics Department The University of Texas at Austin\nRaymond J. Mooney Computer Science Department The University of Texas at Austin\nNLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not adequately capture overall sentence structure. So it has been argued that the two are complementary.\nIn this paper, we adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). We focus on textual entailment (RTE), a task that can utilize the strengths of both representations. Our system is three components, 1) parsing and task representation, where input RTE problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) knowledge base construction in the form of weighted inference rules from different sources like WordNet, paraphrase collections, and lexical and phrasal distributional rules generated on the fly. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that counteract scaling differences between resources. 3) inference, where we show how to solve the inference problems efficiently. In this paper we focus on the SICK dataset, and we achieve a state-of-the-art result.\nOur system handles overall sentence structure and phenomena like negation in the logic, then uses our Robinson resolution variant to query distributional systems about words and short phrases. Therefor, we use our system to evaluate distributional lexical entailment approaches. We also publish the set of rules queried from the SICK dataset, which can be a good resource to evaluate them."}, {"heading": "1. Introduction", "text": "Computational semantics studies how to encode the meaning of natural language in a machine-friendly representation that supports automated reasoning, and that, ideally, can be automatically acquired from large text corpora. Effective semantic repre-\n\u00a9 2005 Association for Computational Linguistics\nar X\niv :1\n50 5.\n06 81\n6v 1\n[ cs\n.C L\n] 2\n6 M\nay 2\n01 5\nsentations and reasoning tools give computers the power to perform useful complex applications like Question Answering, Automatic Grading and Machine Translation. But tasks in computational semantics are very diverse and pose different requirements on the underlying formalism for representing meaning. Some tasks require a detailed representation of the structure of complex sentences. Some tasks require the ability to recognize near-paraphrases or degrees of similarity between sentences. Some tasks require logical inference, either exact or approximate. Often it is necessary to handle ambiguity and vagueness in meaning. Finally, we frequently want to be able to learn relevant knowledge automatically from corpus data.\nThere is no single representation for natural language meaning at this time that fulfills all requirements, but there are representations that meet some of the criteria. Logic-based representations (Montague 1970; Dowty, Wall, and Peters 1981; Kamp and Reyle 1993) like first-order logic provide an expressive and flexible formalism that represents many of the linguistic phenomena like conjunctions, disjunctions, negations and quantifiers, and in addition there are standardized inference mechanisms for them. But first-order logic fails to capture the graded aspect of meaning in languages because it is binary by nature. Distributional models (Turney and Pantel 2010) use contextual similarity to predict the graded semantic similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010), and to model polysemy (Sch\u00fctze 1998; Erk and Pad\u00f3 2008; Thater, F\u00fcrstenau, and Pinkal 2010), but they do not adequately capture logical structure (Grefenstette 2013). This suggests that distributional models and logic-based representations of natural language meaning are complementary in their strengths, as has frequently been remarked (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Grefenstette and Sadrzadeh 2011; Baroni, Bernardi, and Zamparelli 2014). So it may be advantageous to combine the two frameworks. There are now two hybrid approaches that combine logic and distributional semantics (Beltagy et al. 2013; Lewis and Steedman 2013), which both use logic-based semantics as a basis and add in distributional information to help with inference tasks.\nWhat is the status of such hybrid approaches? One possibility is to say that what we really want is a uniform framework that encompasses the abilities of both logic-based and distributional semantics, but until we have that, we will have to use hybrid systems. Another possibility \u2013 and this is the one that we will argue for \u2013 is that hybrid models are actually the right way to represent meaning. We follow Stokhof (2013) in assuming that meaning is a heterogenous phenomenon that is about truth conditions and grounding and observed contexts (among other things). We further follow Stokhof in assuming that meaning may not be characterizeable in terms of a single unified theory. In that case, the best way to go about characterizing meaning is to identify coherent component theories \u2013 like logic-based semantics and distributional semantics \u2013 and to think about the best way to integrate them.\nIn our framework (Garrette, Erk, and Mooney 2011; Beltagy et al. 2013), we use logical form as the primary meaning representation for a sentence. Distributional information is encoded in the form of weighted logical rules. In the simplest form of this idea, we can use, say, the distributional similarity of the words fix and correct as the weight on a rule that says that any fixing event tends to be a correcting event:\n\u2200x.fix(x)\u2192 correct(x) | f(sim( ~fix, ~correct))\nTo draw inferences over such weighted rules, a probabilistic logic framework is necessary. We use Markov Logic Networks (MLN) (Richardson and Domingos 2006), a\nStatistical Relational Learning (SRL) technique (Getoor and Taskar 2007) that combines logical and statistical knowledge in one uniform framework, and provides a mechanism for coherent probabilistic inference. Markov Logic Networks represent uncertainty in terms of weights on the logical rules as in the example below.\n\u2200x. Smoke(x)\u21d2 Cancer(x) | 1.5 \u2200x.y Friend(x, y)\u21d2 (Smoke(x)\u21d4 Smoke(y)) | 1.1 (1)\nThe example states that if someone smokes, there is a chance that they get cancer, and the smoking behaviour of friends is usually similar. Markov logic uses such weighted rules to derive a probability distribution over possible worlds through an undirected graphical model. This probability distribution over possible worlds is then used to draw inferences.\nWe focus on the Recognizing Textual Entailment (RTE) task (Dagan et al. 2013). Given two sentences, called the Text and Hypothesis, RTE is the task of finding out if the Text entails, contradicts, or is not related to the Hypothesis, where \u201centailment\u201d here does not mean logical entailment: The Hypothesis is entailed if a human annotator judges that it plausibly follows from the Text. When using naturally occurring sentences, this is a very challenging task that should be able to utilize the unique strengths of both logic-based and distributional semantics: To draw the correct conclusions, a deep understanding of sentence meaning is necessary, including structural issues like modals and negation and including paraphrasing at the lexical and phrasal level. RTE has many applications like Question Answering, Information Retrieval, Automatic Grading and Machine Translation. We focus in particular on the SICK dataset, as it was designed as a test specifically for compositional distributional models (Marelli et al. 2014b), which compose distributional phrase representations from the distributional representations of their components.\nOur framework is ideally suited for testing distributional approaches (as well as lexical and phrasal paraphrase repositories) for their potential as signaling lexical entailment (Geffet and Dagan 2005) and phrasal entailment. The system handles negation, logical connectives, and quantifiers at the level of logical form, and only queries the distributional model (or resource) for words and short phrases, for which such models are much more suited at this time than sentences of arbitrary length and complexity.\nWe publish a dataset of all the lexical and phrasal distributional rules that our system queries when running on SICK, along with gold standard annotations 1. The training and testing sets are extracted from the SICK training and testing sets respectively. Total number of rules (training + testing) is 12,510, only 10,213 are unique with 3,106 entailing rules, 177 contradictions and 6,928 Neutral. This is a valuable resource especially for testing lexical entailment systems, as they contain a variety of entailment relations (hypernymy, synonymy, antonymy, etc.), and are actually useful in an end-toend RTE system.\nIn this paper, we discuss the system proposed in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013) in more detail, including improvements that allow MLN inference to scale more effectively (Beltagy and Mooney 2014) and to adapt logical constructs to work appropriately in a probabilistic inference setting (Beltagy and Erk 2015). In addition, this paper makes the following new contributions:\n1 Available at: https://github.com/ibeltagy/rrr\n\u2022 Previously, our system did not use any lexical alignment of the Text and the Hypothesis in the RTE task, but instead generated distributional inference rules linking any word in the Text to any word in the Hypothesis. We now use the logical form to guide alignment through a variant of Robinson resolution (Robinson 1983), such that the only distributional rules constructed are those that are needed for a successful inference. These rules can be annotated almost automatically with gold standard annotations. We publish a dataset of all the lexical and phrasal distributional rules collected from the SICK dataset using our variant of the Robinson resolution algorithm (12,510 rules), along with gold standard annotations of entailment or nonentailment. This is a valuable resource especially for testing lexical entailment systems, as they contain a variety of entailment relations, and are actually useful in an end-to-end RTE system. \u2022 Lexical entailment was defined by Geffet and Dagan (2005) as a relation that holds between two words if \u201cthere are some contexts in which one of the words can be substituted by the other, such that the meaning of the original word can be inferred from the new one.\u201d At this point, it is unclear to what extent distributional information actually contains the information needed for this task. A recent paper title asks: \u201cDo supervised distributional methods really learn lexical inference relations?\u201d (Levy et al. 2015). We test this for the case of word pairs in the SICK dataset. Previous datasets for this task came from a variety of sources; we perform this task for the first time on data from an actual RTE dataset. Testing the ability of distributional similarity ratings on their ability to indicate lexical entailment, we confirm that distributional information contains some information on lexical entailment, though high cosine similarity often indicates co-hyponymy, and that the difference between two vectors is a better indicator of hypernymy than cosine, or than the supposed hypernym vector alone. \u2022 We also evaluate a compositional distributional approach on the task of phrasal entailment. Compositional distributional approaches have typically been evaluated on tasks of phrase or sentence similarity (Mitchell and Lapata 2010; Paperno, Pham, and Baroni 2014). To our knowledge, it has not been tested before to what extent phrase similarity can be effectively used to determine entailment. The approach that we test is the state-of-the-art approach by Paperno, Pham, and Baroni (2014). We find that this approach is effective at flagging phrase pairs that are not entailing, for example because of prepositions that change sentence meaning or because of a difference in semantic roles (\u201cman eats near kitten\u201d/\u201dkitten eats\u201d), but not so much at identifying entailing phrase pairs. Our experiments on lexical and phrasal entailments rely on the Robinson resolution based alignment, which removes large amounts of irrelevant rules and allows the system to focus on the relevant ones. \u2022 Marelli et al. report that for the SICK dataset used in a SemEval 2014 shared task (Marelli et al. 2014a), purely compositional models showed lower performance than non-compositional models. In this paper, we show that it is possible for a model that performs deep compositional semantic analysis to reach state-ofthe-art performance. \u2022 In addition to distributional rules, we add rules from existing databases, in particular WordNet (Princeton University 2010) and the paraphrase collection PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013). We use a rule-based technique to translate entries from the paraphrase collection to logical rules.\n\u2022 Rules from different sources come with different weights. We use weight learning to map these weights to MLN weights. We learn one weights scaling factor per rules source. We use simple grid-search to learn the scaling factors. \u2022 We show how to represent the RTE task as a probabilistic logic inference problem, and the inferences needed for the threeway classification. We also argue for using the closed-word assumption (everything has very low prior probability) and show how to implement it for different forms of hypothesis. \u2022 Contradictory RTE sentence pairs are often only contradictory given some assumption about entity coreference. For example, A jet is not flying and A jet is flying are not contradictory unless we assume that the two jets are the same. Handling such coreferences is important to detecting many cases of contradiction. \u2022 To reduce the impact of mis-parsing, we combine results from two different CCG parsers. Experiments show that this improves accuracy.\nThe rest of this paper is organized as follows. Section 2 provides some necessary background. Section 3 gives an overview of our system that is fleshed out in Sections 4, 5 and 6. Section 7 reports an evaluation on the SICK dataset, and Section 8 discusses future work."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1 Logic-based Semantics", "text": "Logic-based representations of meaning have a long tradition in semantics (Montague 1970; Dowty, Wall, and Peters 1981; Kamp and Reyle 1993) as well as computational semantics (Blackburn and Bos 2005; van Eijck and Unger 2010). They handle many complex semantic phenomena such as negation and quantifiers, they identify discourse referents along with the predicates that apply to them and the relations that hold between them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: \u201cThe case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.\u201d\nRecent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). In our system, we use Boxer (Bos 2008), a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG parser (Clark and Curran 2004). which maps the input sentences into a lexically-based logical form, in which the predicates are words in the sentence. For example, the sentence A man is driving a car would be translated to\n\u2203x, y, z. man(x) \u2227 agent(y, x) \u2227 drive(y) \u2227 patient(y, z) \u2227 car(z) (2)\nAs can be seen, Boxer uses a Neo-Davidsonian framework (Parsons 1990): y is an event variable, and the semantic roles agent and patient are turned into predicates linking y to the agent x and patient z."}, {"heading": "2.2 Distributional Semantics", "text": "Distributional models (Turney and Pantel 2010) use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010). They are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais 1997; Lund and Burgess 1996). Therefore, distributional models are relatively easier to build than logical representations, automatically acquire knowledge from \u201cbig data\u201d, and capture the graded nature of linguistic meaning, but they do not adequately capture logical structure (Grefenstette 2013).\nDistributional models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais 1997) or by a component-wise product of word vectors (Mitchell and Lapata 2008, 2010), or through more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli 2010; Grefenstette and Sadrzadeh 2011)."}, {"heading": "2.3 Integrating logic-based and distributional semantics", "text": "It has been noted repeatedly that logic-based and distributional approaches seem to be complementary in their strengths and weaknesses (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Baroni, Bernardi, and Zamparelli 2014). This suggests that it may be useful to combine the two frameworks, and in fact there are now multiple hybrid systems that do so.\nBeltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both. This is the approach that we build on in this paper. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical forms. The main difference between the two approaches lies in the role of gradience. Lewis and Steedman view weights and probabilities as a problem to be avoided. We believe that the uncertainty inherent in both language processing and world knowledge should be front and center in the inference that we do. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang, Jordan, and Klein 2011). They construct phrasal entailment rules based on a logic-based alignment, and use distributional similarity of aligned words to filter rules that do not surpass a given threshold.\nWe should also mention distributional models where the dimensions of the vectors encode model-theoretic model structures rather than observed co-occurrences (Clark 2012; Sadrzadeh, Clark, and Coecke 2013; Grefenstette 2013), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular quantifiers.\nIf we follow Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012) and Bruni et al. (2012) (among others) in also considering perceptual context as part of distributional models, then Cooper et al. (2014) also qualifies as a hybrid logic-based and distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics."}, {"heading": "2.4 Hybrid representations for a heterogeneous semantics", "text": "But what is the status of hybrid logic-based and distributional approaches? One possibility is to say that they are preliminary systems that we need to resort to until we finally develop a uniform framework that encompasses the advantages of both logicbased and distributional semantics. Another possibility is that hybrid systems are the right way to represent meaning \u2013 and it is this second possibility that we will argue for. In an article on \u201cformal semantics and Wittgenstein\u201d, Stokhof (2013) suggests that meaning is a heterogeneous phenonemon. He writes: \u201cThis means acknowledging that what we call \u2019meaning\u2019 is both individual and social; internal and external; natural and socio-cultural; and so on. If we follow the close association between meaning and use that Wittgenstein\u2019s work suggests, we can not but conclude that some aspects of meaning reside in the individual whereas others are determined by the community (or communities) to which the individual belongs; that there are aspects of meaning that are closely connected with mental content in the narrow sense, whereas others are intrinsically related to facts about the external environment; that there biological and psychological determinants of meaning, but also defining influences from the sociocultural environment.\u201d Stokhof then makes his idea of meaning as a heterogeneous concept concrete by characterizing it as Marr\u2019s type 2 (Marr 1977). Marr distinguished two types of problems in artificial intelligence, those that can be characterized in terms of a uniform mathematical theory \u2013 his type 1 \u2013 and those that cannot. Type 2 theories are not really theories but messy algorithms without explanatory value. They are all that is possible, Marr writes, \u201cwhen a problem is solved by the simultaneous action of a considerable number of processes, whose interaction is its own simplest description.\u201d Of course it is not possible to prove that any problem only allows for type 2 theories; it can only be surmised when enough attempts at type 1 theories have failed. Marr\u2019s example of a problem that probably falls into this category is computer vision. Stokhof adds meaning as another problem that may be in this category, and one has to admit that judging from the complexity of the phenomenon, he may be right. What does that mean for meaning representation? Marr himself seems not to assume that messy algorithms are all that can be done for type 2 cases. Instead, for the case of computer vision he seems to propose a combination of type 1 theories for subproblems. This is also the hypothesis that we follow for meaning representation.\nWe assume that meaning is a heterogeneous phenomenon. and think that this means that it is important to think about which type 1 theories must form part of meaning representation, and how they should be combined. Our current answer is that components of such a hybrid system will include both logic-based and distributional semantics. As a method for integrating components, we use probabilistic inference implemented through Markov Logic Networks (Richardson and Domingos 2006). We discuss probabilistic inference and Markov Logic Networks now, and present our hybrid semantic model in detail in Sections 3 through 6."}, {"heading": "2.5 Probabilistic Logic with Markov Logic Networks", "text": "In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd 2009), which employ weighted formulas in first-order logic to compactly encode undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning\nweights to first-order logical rules, combining a diverse set of inference rules and performing sound probabilistic inference.\nWeighting the rules is a way of softening them compared to hard logical constraints and thereby allowing situations in which not all clauses are satisfied. Equation 1 shows an example of weighted logical rules. With the weighted rules, a set of constants need to be specified. For the rules in equation 1, we can add constants representing two persons, Anna (A) and Bob (B). Probabilistic logic uses the constants to \u201cground\u201d atoms with variables, so we get \u201cground atoms\u201d like Smoke(A), Smoke(B),Cancer(A),Cancer(B), Friend(A,A), Friend(A,B), Friend(B,A), Friend(B,B). Rules are also grounded by replacing each atom with variables by all its possible ground atoms.\nMLNs take as input a set of weighted first-order formulas F = F1, . . . , Fn. They then compute a set of ground literals by grounding all predicates occurring in F with all possible constants in the system. Next, they define a probability distribution over possible worlds, where a world is a truth assignment to the set of all ground literals. The probability of a world depends on the weights of the input formulas F as follows: The probability of a world increases exponentially with the total weight of the ground clauses that it satisfies. The probability of a given world x is defined as:\nP (X = x) = 1\nZ exp (\u2211 i wini (x) ) (3)\nwhere Z is the partition function, i ranges over all formulas Fi in F , wi is the weight of Fi, and ni(x) is the number of groundings of Fi that are true in the world x. This probability distribution P (X = x) over possible worlds is computed using a Markov network (Pearl 1988), an undirected graphical model (hence the name Markov Logic Networks). In this Markov network, the nodes are the ground literals, and two nodes are connected by an edge if they co-occur in a ground clause, such that the cliques in the network correspond to ground clauses. A joint assignment of values to all nodes in the graph is a possible world, a truth assignment to the ground literals. In addition to the set R of weighted formulas, an MLN takes an evidence set E asserting some truth values about some of the random variables, e.g.Cancer(A) means that Anna has cancer. Marginal inference for MLNs calculates the probability P (Q|E,R) for a query formula Q.\nAlchemy (Kok et al. 2005) is the most widely used MLN implementation. It is a software package that contains implementations of a variety of MLN inference and learning algorithms. However, developing a scalable, general-purpose, accurate inference method for complex MLNs is an open problem."}, {"heading": "2.6 Recognizing Textual Entailment", "text": "The task that we focus on in this paper is Recognizing Textual Entailment (RTE) (Dagan et al. 2013), the task of determining whether one natural language text, the Text T , entails, contradicts, or is not related (neutral) to another, the Hypothesis H . Here are examples from the SICK dataset (Marelli et al. 2014c):\n\u2022 Entailment T: A man and a woman are walking together through the woods.\nH: A man and a woman are walking through a wooded area.\n\u2022 Contradiction\nT: A man is jumping into an empty pool H: A man is jumping into a full pool\n\u2022 Neutral T: A young girl is dancing\nH: A young girl is standing on one leg\nThe SICK (\u201cSentences Involving Compositional Knowledge\u201d) dataset, which we use for evaluation in this paper, was designed to foreground particular linguistic phenomena but to eliminate the need for world knowledge beyond linguistic knowledge. It was constructed from sentences from two image description datasets, ImageFlickr2 and the SemEval 2012 STS MSR-Video Description data3. Randomly selected sentences from these two sources were first simplified to remove some linguistic phenomena that the dataset was not aiming to cover. Then additional sentences were created as variations over these sentences, by paraphrasing, negation, and reordering. RTE pairs were then created that consisted of a simplified original sentence paired with one of the transformed sentences (generated from either the same or a different original sentence)."}, {"heading": "3. System Overview", "text": "Our approach has three main components:\n1. Parsing and Task Representation, where input natural sentences are mapped into logic then used to represent the RTE task as a probabilistic inference problem. 2. Knowledge Base Construction, where the background knowledge is collected from different sources, encoded as first-order logic rules and weighted. This is where the distributional information is integrated into our system. 3. Inference, which solves the generated probabilistic logic problem using Markov Logic Networks.\nFigure 1 shows the high-level system architecture. Input sentences are mapped to logic using Boxer, the knowledge base KB is collected, then the KB and the sentence representations are passed to the inference engine to solve the inference problem.\nOne powerful advantage of relying on a general-purpose probabilistic logic as a semantic representation is that it allows for a highly modular system. This means the\n2 http://nlp.cs.illinois.edu/HockenmaierGroup/data.html 3 http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=data\nmost recent advancements in any of the system components, in parsing, in knowledge base resources and distributional semantics, and in inference algorithms, can be easily incorporated into the system.\nIn the Parsing and Task Representation step, we map input sentences to logic. As RTE is a three-way classification task (entailing, neutral, or contradicting), we perform probabilistic inference on two problems. The mapping of sentences to logic differs from standard first order logic in several respects because of properties of the probabilistic inference system. First, Markov Logic Networks make the Domain Closure Assumption (DCA), which states that there are no objects in the universe other than the named constants (Richardson and Domingos 2006). This means that constants need to be explicitly introduced in the domain in order to makes probabilistic logic produce the expected inferences. Another representational issue that we discuss is why we should make the closed-world assumption, and its implications on the task representation.\nIn the Knowledge Base Construction step, we collect inference rules from a variety of sources. We add rules from existing databases, in particular WordNet (Princeton University 2010) and PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013). For the integration of distributional semantics, we use a variant of Robinson resolution to align the Text T and the Hypothesis H , and to find the difference between them, which we formulate as an entailment rule. We then train a lexical and phrasal entailment classifier to assess this rule. This is where distributional information is incorporated into the system. Ideally, rules need be contextualized but we leave that to future work.\nIn the Inference step, automated reasoning for MLNs is used to perform the RTE task. We implement an MLN inference algorithm that directly supports querying complex logical formula, which is not supported in the available MLN tools (Beltagy and Mooney 2014). We exploit the closed-world assumption to help reduce the size of the inference problem in order to make it tractable (Beltagy and Mooney 2014). We also discuss weight learning for the rules in the knowledge base."}, {"heading": "4. Parsing and Task Representation", "text": "Our system maps sentences to logical formulas for inference. In this section, we discuss this process, and in particular how it differs from semantics construction for standard first-order theorem proving due to the probabilistic inference setting we use. This section draws on Beltagy and Erk (2015)."}, {"heading": "4.1 Parsing using Boxer", "text": "Natural-language sentences are mapped to logical form using Boxer (Bos 2008). Boxer is a rule-based semantic analysis system that translates a CCG parse into a logical form as in Equation 2. We call Boxer\u2019s output alone an \u201cuninterpreted logical form\u201d because the predicate symbols are simply words and do not have meaning by themselves. Their semantics derives from the knowledge base KB we build in Section 5.\nThe default CCG parser that Boxer uses is C&C (Clark and Curran 2004). To reduce errors due to parsing, we want to use multiple parses; however, we found that the top parses we get from C&C are usually not diverse enough and map to the same logical form. Therefore, in addition to the top C&C parse, we use the top parse from another recent CCG parser, EasyCCG (Lewis and Steedman 2014). In Section 7, we evaluate using C&C alone and using an ensemble of both parsers. Assume sentences T and H each have two parses, T1, T2 and H1, H2. To utilize both parsers, we compute four textual entailments: T1 \u21d2 H1, T1 \u21d2 H2,T2 \u21d2 H1, T2 \u21d2 H2."}, {"heading": "4.2 The RTE Task", "text": "We are given two sentences T and H , and we want to determine if T entails, contradicts or is neutral toH . Checking for entailment in the standard logic is checking if T \u2227KB \u21d2 H , where KB is the knowledge base we build in Section 5. Its probabilistic version is calculating the probability P (H|T,KB,Wt,h), where H is the probabilistic logic query and Wt,h is the world configuration, which includes the number of constants in the domain, and the prior probability of each ground atom. Wt,h is a function of T and H , and Sections 4.3 and 4.4 discuss how it is constructed.\nDifferentiating between Contradiction and Neutral requires one more inference, namely to calculate the probability P (\u00acH|T,KB,Wt,\u00acH). If P (H|T,KB,Wt,H) is high while P (\u00acH|T,KB,Wt,\u00acH) is low, this indicates entailment. The opposite case indicates contradiction. If the two probabilities values are close, this means T does not significantly affect the probability of H , indicating a neutral case. To learn the thresholds for these decisions, we train an SVM classifier with LibSVM\u2019s default parameters (Chang and Lin 2001) to map the two probabilities to the final decision."}, {"heading": "4.3 The Domain Closure Assumption and its Consequences", "text": "We need to diverge from the standard logical form representations of natural language sentences because MLNs make the Domain Closure Assumption (DCA, (Genesereth and Nilsson 1987; Richardson and Domingos 2006)): The only models considered for a set F of formulas are those for which the following three conditions hold. (a) Different constants refer to different objects in the domain, (b) the only objects in the domain are those that can be represented using the constant and function symbols in F , and (c) for each function f appearing in F , the value of f applied to every possible tuple of arguments is known, and is a constant appearing in F . Together, these three conditions entail that there is a one-to-one relation between objects in the domain and the named constants of F . When the set of all constants is known, it can be used to ground predicates to generate the set of all ground literals, which then become the nodes in the graphical model. Different constant sets result in different graphical models. If no constants are explicitly introduced, the graphical model is empty (no random variables). This means that when we work with MLNs to reason over natural language sentences, we need to introduce a sufficient number of constants explicitly into the formula.\nSkolemization. We introduce some of the necessary constants simply through the wellknown technique of Skolemization [Skolem 1920]. It transforms a formula \u2200x1 . . . xn\u2203y.F to \u2200x1 . . . xn.F \u2217, where F \u2217 is formed from F by replacing all free occurrences of y by a term f(x1, . . . , xn) for a new function symbol f . If n = 0, f is called a Skolem constant, otherwise a Skolem function. Although Skolemization is a widely used technique in firstorder logic, it is not frequently employed in probabilistic logic since many applications do not require existential quantifiers.\nWe use Skolemization on the text T (but not the HypothesisH , as we cannot assume a priori that what it says is a fact). For example, the logical expression in Equation 2, which represents the sentence T: A man is driving a car, will be Skolemized to:\nman(M) \u2227 agent(D,M) \u2227 drive(D) \u2227 patient(D,C) \u2227 car(C) (4)\nwhere M,D,C are constants introduced into the domain.\nStandard Skolemization transforms existential quantifiers embedded under universal quantifiers to Skolem functions. For example, for the text T: All birds fly and its logical form \u2200x. bird(x)\u21d2 \u2203y. agent(y, x) \u2227 fly(y) the standard Skolemization is \u2200x. bird(x)\u21d2 agent(f(x), x) \u2227 fly(f(x)). Per condition (c) of the DCA above, if a Skolem function appeared in a formula, we would have to know its value for any constant in the domain, and this value would have to be another constant. To achieve this, we introduce a new predicate skolemf instead of each Skolem function f , and for every constant that is a bird, we add an extra constant that is a flying event. The example above then becomes:\nT : \u2200x. bird(x)\u21d2 \u2200y. skolemf (x, y)\u21d2 agent(y, x) \u2227 fly(y)\nIf the domain contains a single bird B1, then we introduce a new constant C1 and an atom skolemf (B1, C1) to state that the Skolem function f maps the constant B1 to the constant C1.\nExistence. But how would the domain contain a bird B1 in the case of the Text T: All birds fly, \u2200x.bird(x)\u21d2 \u2203y.fly(y) \u2227 agent(y, x)? Skolemization does not introduce any variables for the universally quantified x. We still introduce a constant B1 that is a bird. This can be justified by pragmatics since the sentence presupposes that there are, in fact, birds [Strawson 1950; Geurts 2007]. By using this existential presupposition, we avoid the problem of empty graphical models. The sentence T: All birds fly is changed to T: All birds fly, and there is a bird. At this point, Skolemization takes over to generate a constant that is a bird. Sentences like T: There are no birds constitute a special case: For such sentences, we do not generate evidence of a bird. In this case, the nonemptiness of the domain is not assumed because the sentence explicitly negates it.\nUniversal quantifiers in the Hypothesis. The most serious problem with the DCA is that it affects the behavior of universal quantifiers in the Hypothesis. Suppose we know that T: Tweety is a yellow bird, represented with Skolemization as bird(B) \u2227 yellow(B). Then we can conclude that H: All birds are yellow, because by the DCA we are only considering models with this single constant which we know is both a bird and yellow. To address this problem, we again introduce new constants.\nWe want a hypothesis H: All birds are yellow to be judged true iff there is evidence that all birds will be yellow, no matter how many birds there are in the domain. So H should follow from T2: All birds are yellow but not from T1: There is a yellow bird. Therefore we introduce a new constant D for the Hypothesis and assert bird(D) to test if we can then conclude that yellow(D). The new evidence bird(D) prevents the hypothesis from being judged true given T1. Given T2, the new bird D will be inferred to be yellow, in which case we take the hypothesis to be true. Again, with a hypothesis such as H: There are no birds, we do not generate any evidence for the existence of a bird."}, {"heading": "4.4 The Closed-World Assumption", "text": "Section 4.2 suggests using the conditional probability P (H|T,KB,Wt,h) as an indicationon whether T entails H . However, how useful the conditional probability is as an indication of entailment depends on P (H|KB,Wt,h) which is the prior probability of H . For example, if H has a high prior probability, then a high conditional probability P (H|T,KB,Wt,h) does not add much information because it is not clear if the probability is high because T really entails H , or because of the high prior probability of H .\nWe discuss two suggestions on how to solve this problem and make the probability P (H|T,KB,Wt,h) less sensitive to P (H|KB,Wt,h). The first is to use the ratio P (H|T,KB,Wt,h) P (H|KB,Wt,h) , with the intuition that the absolute value of P (H|T,KB,Wt,h) does not really matter, but what matters is how much adding T changes the probability of H positively (indicating entailment) or negatively (indicating contradiction). The second option is to pick a particular Wt,h such that the prior probability of H is approximately zero, P (H|KB,Wt,h) \u2248 0, so that we know that any increase in the conditional probability is an effect of adding T . This inference alone does not account for contradictions, which is why an additional inference P (\u00acH|T,KB,Wt,\u00acH) is needed, as mentioned in section 4.2.\nFor the rest of this section, we argue why we believe the first option is not a good fit for the RTE task while the second is a better fit. Then we show how to set the worlds configurationsWt,h such that P (H|KB,Wt,h) \u2248 0 by enforcing the closed-world assumption (CWA) which is the assumption that all ground atoms have very low prior probability (or false by default).\n4.4.1 Problems with the ratio. The first problem with the ratio approach is that its motivation does not fit the definition of the RTE task. For example for T: A person is driving a red car, and H: A person is driving a new red car, T should not be entailing H because there is no evidence that the car is new. However, the probability of H conditioned on T increases dramatically compared to the prior probability ofH because T has evidence for a large part of H . So this means that a high ratio is not always an indication of entailment.\nThere are also cases of entailment with a not very high ratio. Consider for example T: No one is driving, and H: No one is driving a car. T entails H , and P (H|T,KB,Wt,h) is greater than P (H|KB,Wt,h), but not too much greater because P (H|KB,Wt,h) is already a high value. The above two examples show that the intuition behind taking the ratio, which is how much conditioning on T changes the probability of H , does not really fit the RTE task.\nThe last problem with the ratio is that it is very sensitive to the problem size (length of T and H). That is, entailing pairs of different sizes have different ratios. This makes reasoning with the ratio tricky. It could be possible to normalize the ratios given the problem size, but we did not explore this direction.\n4.4.2 Using the CWA to set the prior probability of H to zero. The closed-world assumption (CWA) is the assumption that everything is false by default, or that all ground atoms have very low prior probability. For most Hs, setting the worlds configurations Wt,h such that all ground atoms have low prior probability is enough to achieve that P (H|KB,Wt,h) \u2248 0 (not for negatedHs, and this case is discussed below). For example, H: A young girl is dancing, in logic is:\nH : \u2203x, y. young(x) \u2227 girl(x) \u2227 agent(y, x) \u2227 dance(y)\nHaving low prior probability on all ground atoms means that the prior probability of this existentially quantified H is close to zero.\nWe believe that this setup is more appropriate for the RTE task for the following reasons. First, this aligns with how the RTE task is defined, that H should be entailed by T not from general world knowledge. For example, if T: A man is walking in the rain, and H: Texas is in the USA, then although H is true in the real world, T does not entail\nH . Even though H is true in the real world, for the purpose of RTE, we need to assume that it is not. This also means that anything that is not explicitly stated in T should be assumed to be false by default. Another example: T: A man is driving a car, H: A man is driving a new car, again, T does not entail H because there is no evidence that the car is new, in other words, the ground atom new(C) has very low prior probability.\nThe second reason is that with the CWA, the inference result is less sensitive to the domain size (number of constants in the domain). In the RTE task, most variables in the hypothesis are existentially quantified. Without the CWA, the probability of an existentially quantified hypothesis increases as the domain size increases, regardless of the evidence. This makes sense in the MLN setting, because in larger domains the probability that something exists increases. However, this is not what we need for the RTE task, as the probability of the hypothesis should depend on T and KB, not the domain size. With the CWA, what affects the probability of H is the non-zero evidence that T provides and KB, regardless of the domain size.\nThe third reason is computational efficiency. As discussed in Section 2, Markov Logic Networks perform probabilistic inference by first computing all possible groundings of a given set of weighted formulas, and then using a Markov network to compute probabilities for truth assignments over this set of ground literals. The grounding step can require significant amounts of memory. This is particularly striking for problems in natural language semantics because they usually have mostly existentially quantified variables, which, as discussed above, results in a model with many constants due to Skolemization. Beltagy and Mooney (2014) show how to utilize the CWA to address this problem by reducing the number of ground literals that the system generates. They determine, based onKB, which ground atoms are useful to model (probability does not equal prior probability) and which are not (probability remains at the prior probability). They use an algorithm that tracks the propagation of evidence from T through KB, which finds (without running inference) the ground atoms whose probabilities will remain at their prior probability. These ground atoms can be assumed to be false and can be dropped from the inference problem without significantly changing the computed probability of H . This algorithm is discussed in details in Section 6.2.\n4.4.3 Setting the prior probability of negated H to zero. While using the CWA is enough to set P (H|KB,Wt,h) \u2248 0 for most Hs, it does not work for negated H . Assuming that everything is false by default and that all ground atoms have very low prior probability (CWA) means that all negated Hs are true by default. The result is that all negated H are judged entailed regardless of T . For example, T: A dog is sleeping would entail H: There is no young girl dancing. This H in first-order logic is:\nH : \u2200x, y. young(x) \u2227 girl(x)\u21d2 \u00ac(agent(y, x) \u2227 dance(y))\nAs both x and y are universally quantified variables in H , we generate evidence of a young girl young(G) \u2227 girl(G) as described in section 4.3. Because of the CWA, G is assumed to be not dancing, and H ends up being true regardless of T .\nTo set the prior probability of H to \u2248 0 and prevent it from being assumed true when T is just uninformative, we construct a new ruleA that implements a kind of antiCWA. A is formed as a conjunction of all the predicates that were not used to generate evidence before, and are negated in H . This rule A gets a positive weight indicating that its ground atoms have high prior probability. As the rule A together with the evidence generated from H states the opposite of the negated parts of H , the prior probability of H is low, and H cannot become true unless T explicitly negates A. T is translated into\nunweighted rule, which are taken to have infinite weight, and which thus can overcome the finite positive weight of A. Here is a Neutral RTE example adapted from the SICK dataset, T: A young girl is standing on one leg, and H: There is no young girl dancing. Their representations are:\nT : \u2203x, y, z. young(x) \u2227 girl(x) \u2227 agent(y, x) \u2227 stand(y) \u2227 on(y, z) \u2227 one(z) \u2227 leg(z) H : \u2200x, y. young(x) \u2227 girl(x)\u21d2 \u00ac(agent(y, x) \u2227 dance(y)) E: young(G) \u2227 girl(G) A: agent(D,G) \u2227 dance(D)|w = 1.5\nE is the evidence generated for the universally quantified variables in H , and A is the weighted rule for the remaining negated predicates. The relation between T and H is Neutral, as T does not entail H . This means, we want P (H|T,KB,Wt,h) \u2248 0, but because of the CWA, P (H|T,KB,Wt,h) \u2248 1. Adding A solves this problem and P (H|T,A,KB,Wt,h) \u2248 0 because H is not explicitly entailed by T .\nIn case H contains existentially quantified variables that occur in negated predicates, they need to be universally quantified in A for H to have a low prior probability. For example, H: There is some bird that is not black:\nH : \u2203x. bird(x) \u2227 \u00acblack(x)\nA : \u2200x. black(x)|w = 1.5\nIf one variable is universally quantified and the other is existentially quantified, we need to do something more complex. Here is an example, H: The young girl is not dancing:\nH : \u2203x. young(x) \u2227 girl(x) \u2227 \u00ac( \u2203y. agent(y, x) \u2227 dance(y) )\nA : \u2200v. agent(D, v) \u2227 dance(D)|w = 1.5\nNotes about how inference proceeds with the rule A added. If H is a negated formula that is entailed by T , then T (which has infinite weight) will contradict A, allowing H to be true. Any weighted inference rules in the knowledge base KB will need weights high enough to overcome A. So the weight of A is taken into account when computing inference rule weights.\nIn addition, adding the ruleA introduces constants in the domain that are necessary for making the inference. For example, take T: It is not the case that a person is walking, and H: It is not the case that a man is walking, which in logic are:\nT : \u00ac\u2203x, y. person(x) \u2227 agent(y, x) \u2227 walk(y) H : \u00ac\u2203x, y. man(x) \u2227 agent(y, x) \u2227 walk(y) A: man(M) \u2227 agent(W,M) \u2227 walk(W )|w = 1.5\nKB: \u2200x. man(x)\u21d2 person(x)\nWithout the constants M and W added by the rule A, the domain would have been empty and the inference output would have been wrong. The ruleA prevents this problem. In addition, the introduced evidence in A fit the idea of \u201cevidence propagation\u201d mentioned above, (detailed in Section 6.2). For entailing sentences that are negated, like in the example above, the evidence propagates from H to T (not from T to H as in nonnegated examples). In the example, the rule A introduces an evidence for man(M) that then propagates from the LHS to the RHS of the KB rule."}, {"heading": "4.5 Coreference Resolution for Contradiction", "text": "In the SICK dataset, many of the contradictions can not be captured by checking if T \u2227 KB entails \u00acH or its probabilistic counterpart P (\u00acH|T,KB), because the annotators make additional assumptions to reach the conclusion of Contradiction. For example, if we have T: A jet is not flying and H: A jet is flying, then strictly speaking T and H are not contradictory because it is possible that the two sentence are referring to different jets. Although the sentence uses a jet not the jet, the annotators make the assumption that the jet in H refers to the jet in T . We need to enforce a similar coreferencing assumption to get many of the contradictions right. Here are the logical formulas for the example above after coreferencing is implemented:\nT : \u2203x. jet(x) \u2227 \u00ac(\u2203y. agent(y, x) \u2227 fly(y))\nskolemized T : jet(J) \u2227 \u00ac(\u2203y. agent(y, J) \u2227 fly(y))\nH : \u2203x, y. jet(x) \u2227 agent(y, x) \u2227 fly(y)\n\u00acH : \u00ac\u2203x, y. jet(x) \u2227 agent(y, x) \u2227 fly(y)\nupdated \u00acH : \u00ac\u2203y. jet(J) \u2227 agent(y, J) \u2227 fly(y)\nNotice how the constant J representing the jet in T is used in the updated \u00acH instead of the quantified variable x.\nThe system must first determine the entities that corefer. For two entities to corefer, they should have different polarities (one negated and the other not) in T and \u00acH . For all pairs of entities that fulfill this condition, they are determined to corefer iff they have the same lemma. For the example above, jet in T is not negated, and jet in \u00acH is negated, and both words are the same, so they are coreferring. Here is another example: T: The surfer is riding a small wave, H: The surfer is riding a big wave, all entities in \u00acH are negated, so the surfers, waves and the riding events are all coreferring. Of course we still need an inference rule small \u21d4 \u00ac big, which we obtain from WordNet as explained in Section 5.1.1.\nUsing word lemmas is not enough. Consider the example: T: A person is not playing the keyboard, H: A man is playing a keyboard, we need person and man to corefer. For cases like this, we rely on the alignments found using the modified Robinson resolution method discussed in Section 5.2. In this case, it determines that person and man should be aligned, so they are marked as coreferring. Here is another example: T: The man is denying an interview, H: The man is granting an interview. In this case, denying and granting are coreferring."}, {"heading": "5. Knowledge Base Construction", "text": "This section discusses how we build the weighted knowledge base KB for a given sentence pair T and H . First, we discuss the existing rule sets we add to KB and how we translate them to logical rules. Then, we discuss how we integrate distributional information in the KB. We use a variant of Robinson resolution to align T with H , and\nfind the difference between them in the form of an inference rule. Then we use different distributional semantic techniques to give a weight to this inference rule.4"}, {"heading": "5.1 Precompiled Rules", "text": "The first set of rules are collected by translating existing rule databases. We collect rules from WordNet (Princeton University 2010) the paraphrase collection PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013). We use simple string matching to find the set of rules that are relevant to a given RTE problem T and H . If the LHS of the rule is a substring of T (or H) and the RHS is substring of H (or T ), the rule is added. Rules that go from H to T are important in case T and H are negated, e.g. T : There is no one driving a vehicle, H : There is no one driving a car. The rule needed is car\u21d2 vehicle which goes from H to T .\n5.1.1 WordNet. WordNet (Princeton University 2010) is a lexical database of words grouped into sets of synonyms. In addition to grouping synonyms, it lists semantic relations connecting groups. We represent the information on WordNet as \u201chard\u201d logical rules and add them to the system\u2019s KB. The semantic relations we use are:\n\u2022 Synonyms: \u2200x. man(x)\u21d4 guy(x) \u2022 Hypernyms: \u2200x. car(x)\u21d2 vehicle(x) \u2022 Antonyms: \u2200x. tall(x)\u21d4 \u00acshort(x)\nOne advantage of using logic for semantic representation is that it is a powerful representation that can effectively represent different semantic relations.\n5.1.2 Paraphrase collections. Paraphrase collections are precompiled sets of rules, e.g: a person riding a bike \u21d2 a biker. We translate paraphrase collections, in this case PPDB, to logical rules and add them to KB. We use a simple rule-based approach to do the translation. Given two sentences S1 and S2, their parsed logical forms L(S1) and L(S2) and a rule LHS \u21d2 RHS, we want to determine the logical rule L(R). First, we assume that L(LHS) and L(RHS) are conjunctions of atoms. If L(LHS) or L(RHS) contain negations or disjunctions, the rule is dropped, as we assume that negations and disjunctions are already represented in L(S1) or L(S2). Given this assumption, L(LHS) (and similarly L(RHS)) can be constructed by collecting a subset of atoms from L(S1) (L(S2)). For each word in LHS, we find a predicate in L(S1) for this word and add it to L(LHS). For the example above, the words person, riding and bike correspond to literals person(p), riding(r) and bike(b) (for variables p, r, b). In addition, we add to L(LHS) all semantic role predicates in L(L1) that connect the predicates found in the first step. For the example, the literal agent(r, p) connects person(p) and riding(r), and the literal patient(r, b) connects riding(r) and bike(b), and they are also added to L(LHS). This gives L(LHS) = person(p) \u2227 agent(r, p) \u2227 riding(r) \u2227 patient(r, b) \u2227 bike(b) and L(RHS) = biker(k).\nThe next step is appropriately binding the variables in L(LHS) to those in L(RHS). In the example above, the variable k in the RHS should be matched with the variable p in the LHS. We determine these bindings using a simple rule-based\n4 Ideally, this weighted KB should be contextualized, that is, the weight of each rule is a function of the rule in addition to its context (T and H), not just the rule. This is how we can take the different meaning of a word/phrase into account. The KB presented in this section is not contextualized, and we leave contextualization to future work.\napproach: We manually define paraphrase rule templates, which specify the variable bindings specific to each template. A template of a rule lists the part of speech tag corresponding to each variable in both sides of the rule. For our example, LHS has the variables p, r and b and they correspond to the part of speech tags N , V and N respectively. Similarly for the RHS, the variable k corresponds to the part of speech tag N . This makes the template for this paraphrase rule: NVN \u21d2 N . Our variable binding rule for this template binds the variable k with the variable p. The final paraphrase rule is: \u2200p, r, b. person(p) \u2227 agent(r, p) \u2227 riding(r) \u2227 patient(r, b) \u2227 bike(b)\u21d2 biker(p). In case some variables in the RHS remain unbound, they are existentially quantified, e.g.: \u2200p. pizza(p)\u21d2 \u2203q. slice(p) \u2227 of(p, q) \u2227 pizza(q).\nWeight Mapping. Each PPDB rule comes with a set of similarity scores, and we need to map these scores to a single MLN weight. We use the simple log-linear equation suggested by Ganitkevitch, Van Durme, and Callison-Burch (2013) to map the scores into a single value:\nweight(r) = \u2212 N\u2211 i=1 \u03bbi log\u03d5i (5)\nwhere, r is the rule,N is number of the similarity scores provided for the rule r, \u03d5i is the value of the ith score, and \u03bbi is its scaling factor. For simplicity, following Ganitkevitch, Van Durme, and Callison-Burch (2013), we set all \u03bbi to 1. To map this weight to a final MLN rule weight, we use the weight-learning method discussed in Section 6.3.\n5.1.3 Handcoded rules. We can also add a few handcoded rules to the KB that we do not get from other resources. For the SICK dataset, we only add few lexical rules where one side of the rule is the word nobody, e.g: nobody\u21d4 \u00ac somebody and nobody\u21d4 \u00ac person."}, {"heading": "5.2 Robinson Resolution for Alignment and Rule Extraction", "text": "Which distributional rules should be generated for a pair of a Text T and Hypothesis H? Earlier versions of our system generated distributional rules matching any word or short phrase in T with any word or short phrase in H . This includes many rules that would not be necessary, for example for T: A man is cutting a cucumber and H: a guy is slicing a zucchini, the system also generated weighted rules linking man to zucchini. Instead, we use a novel method to generate only the necessary rules relevant to T and H . We assume that T entailsH , and ask what the missing rule setKB is that is necessary to prove this entailment. We use a variant of Robinson resolution (Robinson 1983) to generate this KB. This gives us very specific rules that are tailored to a particular T and H . Below, we will use these rules as training data for an entailment rule classifier. Another way of viewing our variant of Robinson resolution is that it is generating an alignment between words and phrases in T and words or phrases in H guided by the logic.\n5.2.1 Modified Robinson Resolution. Robinson resolution is a theorem proving method for testing unsatisfiability. It assumes a formula in conjunctive normal form (CNF), a conjunction of disjunctions of literals, or more formally a formula of the form \u2200x1, . . . , xn ( C1 \u2227 . . . \u2227 Cm), where theCj are disjunctions of positive or negative literals. The resolution rule takes two clauses containing complementary literals, and produces a\nnew clause implied by them. Writing a clauseC as the set of its literals, we can formulate the rule as\nC1 \u222a {L1} C2 \u222a {L2} (C1 \u222a C2)\u03b8\nwhere \u03b8 is a most general unifier of L1 and \u00acL2. In our case, we use a variant of Robinson resolution to remove the parts of Text T and HypothesisH that the two sentences have in common. Instead of one set of clauses, we use two: one is the CNF of T , the other is the CNF of \u00acH . The resolution rule is only applied to pairs of clauses where one clause is from T , the other from H . When no further applications of the resolution rule are possible, we are left with remainder formulas rT and rH . If rH is the empty clause, thenH follows from T without inference rules from KB. Otherwise, inference rules need to be generated. In the simplest case, we form a single inference rule as follows. All variables occurring in rT or rH are existentially quantified, all constants occurring in rT or rH are un-Skolemized to new universally quantified variables, and we infer the negation of rH from rT . That is, we form the inference rule\n\u2200x1 . . . xn\u2203y1 . . . ym. rT\u03b8 \u21d2 \u00acrH\u03b8\nwhere {y1 . . . ym} is the set of all variables occurring in rT or rH , {a1, . . . an} is the set of all constants occurring in rT or rH and \u03b8 is the inverse of a substitution \u03b8 : {a1 \u2192 x1, . . . , an \u2192 xn} for distinct variables x1, . . . , xn.\nFor example, consider T: A groundhog sat on a hill and H: A woodchuck sat on a hill. This gives us the following two clause sets. Note that all existential quantifiers have been eliminated through Skolemization. The Hypothesis is negated, so we get five clauses for T but only one for H .\nT : {groundhog(A)}, {hill(B)}, {sit(C}, {agent(C,A)}, {on(C,B} \u00acH : {\u00acwoodchuck(x),\u00achill(y),\u00acsit(z),\u00acagent(z, x),\u00acon(z, y)}\nThe resolution rule can be applied 4 times. After that,C has been unified with z (because we have resolved sit(C) with sit(z)), B with y (because we have resolved hill(B) with hill(y)), and A with x (because we have resolved agent(C,A) with agent(z, x)). The formula rT is groundhog(A), and rH is \u00acwoodchuck(A). So the rule that we generate is\n\u2200x.groundhog(x)\u21d2 woodchuck(x)\nThe modified Robinson resolution thus does two things at once: It removes words that T and H have in common, leaving the words for which inference rules are needed, and it aligns words and phrases in T with words and phrases in H through unification.\nOne important refinement to this general idea is that we need to distinguish metapredicates introduced by Boxer, such as agent(X,Y ), from content predicates that correspond to words in the sentences. Resolving on meta-predicates can result in incorrect rules, for example in the case of T: A person solves a problem and H: A person finds a solution\nto a problem, in CNF\nT : {person(A)}, {solve(B)}, {problem(C)}, {agent(B,A)}, {patient(B,C)} \u00acH : {\u00acperson(x),\u00acfind(y),\u00acsolution(z),\u00acproblem(u),\u00acagent(y, x),\u00acpatient(y, z),\n\u00acto(z, u)}\nIf we resolve patient(B,C) with patient(y, z), we identify the problem C with the solution z, leading to a wrong alignment. We avoid this problem by resolving on metapredicates only when they are fully grounded (that is, when the substitution of variables with constants has already been done by some other resolution step involving content predicates).\nIn this variant of Robinson resolution, we currently do not do search, but unify two literals only if they are fully grounded or if the literal in T has a unique literal in H that it can be resolved with, and vice versa. This works for most pairs in the SICK dataset. In future work, we would like to add searching to our algorithm, which will help get better rules for sentences with duplicate words.\n5.2.2 Rules Refinements. The rules we get from the modified Robinson resolution need to be refined in various ways for them to work for the entailment rules subsystem.\nExtending rules. In many cases, a rule that only shows the difference between Text and Hypothesis is too short and needs context to be usable as a distributional rule, for example: T: A dog is running in the snow, H: A dog is running through the snow, the rule we get is \u2200x, y. in(x, y)\u21d2 through(x, y). Although this rule is correct, it does not carry enough information to compute a vector representation for each side. Remember that the variables x and y were Skolem constants in rT and rH , for example rT : in(R,S) and rH : through(R,S). We extend the rule by adding the content words that contain the constants R and S. In this case, we add the running event and the snow back in. The final rule is: \u2200x, y. run(x) \u2227 in(x, y) \u2227 snow(y)\u21d2 run(x) \u2227 through(x, y) \u2227 snow(y). Here is another example: T: A person is pouring olive oil into a pot , H: A person is pouring cooking oil into a pot, and the rule is \u2200x. olive(x)\u21d2 cooking(x) which we extend to \u2200x. olive(x) \u2227 oil(x)\u21d2 cooking(x) \u2227 oil(x)\nIn some cases however, extending the rule adds unnecessary complexity, for example: T: A man is jumping into an empty pool, H: A man is jumping into a full pool and the rule is \u2200x. empty(x)\u21d2 full(x), and extending it gives \u2200x. empty(x) \u2227 pool(x)\u21d2 full(x) \u2227 pool(x) which makes the rule unnecessary complex. At the moment, we have no general algorithm for when to extend the rule and when not to. Such an algorithm would have to take lexical meaning, and in particular context-dependence, into account. At this time, we always extend the rule. As discussed below, the entailment rules subsystem can itself choose to split long rules, and it may choose to split these extended rules again.\nSplitting long rules. Conversely, the formulas rT and rH often need to be split into multiple inference rules. Splitting long rules into shorter ones makes it easier for the entailment rules subsystem to represent them and evaluate their similarities. We do this splitting based on the structure of the formulas rT and rH .\nFirst, we split each formula into disconnected sets of predicates. For example, consider T: The doctors are healing a man, H: The doctor is helping the patient which leads to the rule \u2200x, y heal(x) \u2227man(y)\u21d2 help(x) \u2227 patient(y). The formula rT is split into\nheal(x) and man(y) because the two literals do not have any variable in common and there is no relation (such as agent()) to link them. Similarly, rH is split into help(x) and patient(y). If any of the splits has more than one verb, we split it again, where each new split contains one verb and its arguments.\nAfter that, we create new rules that link any part of rT and any part of rH that have at least one variable in common. So for our example we get \u2200x heal(x)\u21d2 help(x) and \u2200y man(y)\u21d2 patient(y).\nIt is important to note that many rules could in principle be split even more, for example a rule like a man and two women are facing a camera\u21d2 a group of people are looking at the camera can be split into a man and two women\u21d2 a group of people and facing a camera \u21d2 looking at the camera . However, there are cases where splitting the rule does not work, for example: A person, who is riding a bike\u21d2 A biker . Here, splitting the rule and using person\u21d2 biker loses crucial context information. So we do not perform those additional splits at the level of the logical form, though the entailment rules subsystem may choose to do further splits.\n5.2.3 Translating the logical rule into text. The output of our modified Robinson resolution is a logical formula. We map this formula to a text before passing it to the entailment rules subsystem. Each Boxer predicate or relation (except meta predicates and relations) comes with an index pointing to the source word. For each predicate or relation in the logical formula, we replace it with its corresponding word from the original sentence. This yields a simple readable rule text that the entailment rules subsystem can handle.\n5.2.4 Annotating Rules. The output from the previous steps is a set of rules {r1, ..., rn} for each pair T and H . We want to use the gold standard annotations of RTE training pairs to annotate the rules as being either entailing, non-entailing or contradictory. This way, we can build a training set to train the entailment rule classifier.\nRTE pairs are annotated with one of three classes, Entailment, Contradiction and Neutral. The Entailment cases are the most straightforward. Knowing that T \u2227 r1 \u2227 ... \u2227 rn \u21d2 H , then it must be that all ri are entailing. We automatically label all ri of the entailing pairs as entailing rules.\nFor Neutral pairs, we know that T \u2227 r1 \u2227 ... \u2227 rn ; H , so at least one of the ri is non-entailing. We experimented with automatically labeling all ri as non-entailing, but that adds a lot of noise in the training data. For example, if T: A man is eating an apple and H: A guy is eating an orange, then the rule man \u21d2 guy is entailing, but the rule apple \u21d2 orange is non-entailing. So we compare the ri from a Neutral pair to the entailing rules derived from entailing pairs. All rules ri found among the entailing rules from entailing pairs are assumed to be entailing (unless n = 1, that is, unless we only have one rule), and all other rules are assumed to be non-entailing. We found that this step improved the accuracy of our system. To further improve the accuracy, we performed a manual annotation of the rules derived from Neutral pairs, labeling them as either entailing or non-entailing. From around 5,900 unique rules, we found 737 to be entailing. In future work, we plan to use multiple instance learning (Dietterich, Lathrop, and Lozano-Perez 1997; Bunescu and Mooney 2007) to avoid manual annotation; we discuss this further in Section 8.\nFor Contradicting pairs, we make a few simplification assumptions that fit almost all such pairs in the SICK dataset. In most of the contradiction sentence pairs in the SICK dataset, one of the two sentences T or H is negated. For pairs where T or H has a negation, we assume that this negation is negating the whole sentence, not just a part of\nit. We first consider the case where T is not negated, and H = \u00acSh. As T contradicts H , it must hold that T \u21d2 \u00acH , so T \u21d2 \u00ac\u00acSh, and hence T \u21d2 Sh. This means that we just need to run our modified Robinson resolution with the sentences T and Sh and label all resulting ri as entailing.\nNext we consider the case where T = \u00acSt while H is not negated. As T contradicts H , it must hold that \u00acSt \u21d2 \u00acH , so H \u21d2 St. Again, this means that we just need to run the modified Robinson resolution with H as the \u201cText\u201d and St as the \u201cHypothesis\u201d and label all resulting ri as entailing. Because of quantifier polarity, the inferences here need to go from H to T , not the other way around.\nThe last case of contradiction is when both T andH are not negated, for example: T: A man is jumping into an empty pool, H: A man is jumping into a full pool, where empty and full are antonyms. As before, we run the modified Robinson resolution with T and H and get the resulting ri. Similar to the Neutral pairs, at least one of the ri is a contradictory rule, while the rest could be entailing or contradictory rules. As for the Neutral pairs, we take a rule ri to be entailing if it is among the entailing rules derived so far. All other rules are taken to be contradictory rules. We did not do the manual annotation for these rules because they are few."}, {"heading": "5.3 The Lexical and Phrasal Entailment Rule Classifier", "text": "After extracting lexical and phrasal rules using our modified Robinson resolution (described in Section 5.2), we use several combinations of lexical resources to build a lexical and phrasal entailment rule classifier, or for short, entailment rule classifier (we use the short name throughout the rest of the paper) for weighting the rules appropriately. These extracted rules create an especially valuable resource for testing lexical entailment systems, as they contain a variety of entailment relations (hypernymy, synonymy, antonymy, etc.), and are actually useful in an end-to-end RTE system.\nWe describe the entailment rule classifier in multiple parts. In Section 5.3.1, we overview a lexical entailment rule classifier, which only handles single words. Section 5.3.2 describes the lexical resources used. In Section 5.3.3, we describe how our previous work in supervised hypernymy detection is used in the system. In Section 5.3.4, we describe the approaches for extending the classifier to handle phrases.\n5.3.1 Lexical Entailment Rule Classifier. We begin by describing the lexical entailment rule classifier, which only predicts entailment between single words, treating the task as a supervised classification problem given the lexical rules constructed from the modified Robinson resolution as input. We use numerous features which we expect to be predictive of lexical entailment. Many were previously shown to be successful for the SemEval 2014 Shared Task on lexical entailment (Marelli et al. 2014a; Bjerva et al. 2014; Lai and Hockenmaier 2014). Altogether, we use four major groups of features for the initial lexical entailment rule classifier. A summary of the features is contained in Table 1, and they are described in greater detail below.\nWordform Features. We extract a number of simple features based on the simple usage of the LHS and RHS in their original sentences. We extract features for whether the LHS and RHS have the same lemma, same surface form, same POS, which POS tags they have, and whether they are singular or plural. Plurality is determined from the POS tags.\nWordNet Features. We use WordNet 3.0 to determine whether the LHS and RHS have known synonymy, antonymy, hypernymy, or hyponomy relations. We disambiguate between multiple synsets for a lemma by selecting the synsets for the LHS and RHS which minimize their path distance. If no path exists, we choose the most common synset for the lemma. Path similarity, as implemented in the Natural Language Toolkit (Bird, Klein, and Loper 2009), is also used as a feature.\nDistributional Features. We measure distributional semantic similarity in two different distributional spaces, one which models general word similarity (BoW), and one which models only syntactic similarity (Dep). We use the cosine similarity of the LHS and RHS in both of these spaces as features.\nOne very important feature set used from distributional similarity is the histogram binning of the cosines. We create 12 additional binary, mutually-exclusive features, which mark whether the distributional similarity is within a given range. We use the ranges of exactly 0, exactly 1, 0.01-0.09, 0.10-0.19, . . . , 0.90-0.99. Figure 2 shows the importance of these histogram features: words that are very similar (0.90-0.99) are much less likely to be lexically entailing than words which are moderately similar (0.70-0.89). This is because the most highly similar words are more likely to be co-hyponyms than hypernymy or other entailing relationships.\n5.3.2 Preparing Distributional Spaces. As described in the previous section, we use distributional semantic similarity as features for the classifier. Here we describe the preprocessing steps to create these distributional resources.\nCorpus and Preprocessing:. We use the BNC, ukWaC and a 2014-01-07 copy of Wikipedia. All corpora are tokenized, POS tagged, lemmatized, and dependency parsed using Stanford CoreNLP. We collapse particle verbs into a single token, and all tokens are annotated with a simplified POS tag (NN, VB, JJ, RB), so that the same lemma with a different POS is modeled separately. We only create distributional representations for content lemmas (nouns, verbs, adjectives, and adverbs) appearing at least 1000 times in the concatenated corpus. The final distributional spaces model 50,984 unique lemma/POS types, and are based on roughly 1.5B tokens.\nBag-of-Words vectors:. We filter all but the 51k chosen lemmas from the corpus, and create one sentence per line. We use Word2Vec\u2019s skip-gram algorithm to create vectors (Mikolov et al. 2013). We use 300 latent dimensions, subsampling set to 1e-5, a window size of 20, and 15 negative samples. These parameters were not tuned, but rather chosen as reasonable expected defaults for the task. We use the relatively large window size to ensure the BoW vectors captured more topical and general word similarity, rather than syntactic similarity, which is modeled explicitly by the dependency vectors.\nDependency vectors:. We extract (lemma/POS, relation, context/POS) tuples from each of the Stanford Collapsed CC Dependency graphs obtained in preprocessing. We filter all tuples with lemmas not in our 51k chosen types. Keeping in line with prior work in syntactic distributional spaces (Baroni and Lenci 2010), we model inverse relations as well, but mark them separately. For example, \u201cred/JJ car/NN\u201d will generate tuples for both (car/NN, amod, red/JJ) and (red/JJ, amod\u22121, car/NN). After extracting tuples, we discard all but the top 100k (relation, context/POS) pairs and build a vector space using\nlemma/POS as rows, and (relation, context/POS) as columns. The matrix is transformed using Positive Pointwise Mutual Information (PPMI), and reduced to 300 dimensions using Singular Value Decomposition (SVD). We do not vary these parameters, but chose them as they performed best in previous work on identifying lexical relations (Roller, Erk, and Boleda 2014).\n5.3.3 Asymmetric Entailment Features. As an additional set of features, we also use the representation previously employed by the asymmetric, supervised entailment rule classifier described by Roller, Erk, and Boleda (2014). Previously, this classifier was only used on artificial datasets, which encoded specific lexical relations, like hypernymy, co-hyponomy, and meronomy. Here, we use its representation to encode just the three general relations: entailment, neutral, and contradiction.\nThe asymmetric features take inspiration from Mikolov, Yih, and Zweig (2013), who found that differences between distributional vectors often encode certain linguistic regularities, like ~king \u2212 ~man+ ~woman \u2248 ~queen. In particular the asymmetric classifier uses two sets of features, < f, g >, where\nfi(LHS,RHS) = ~LHSi \u2212 ~RHSi\ngi(LHS,RHS) = f 2 i ,\nthat is, the vector difference between the LHS and the RHS, and this difference vector squared. Both feature sets are extremely important to strong performance.\nFor these asymmetric features, we use the Dependency space described earlier. We choose the Dep space because we previously found that spaces reduced using SVD outperform word embeddings generated by the Skip-gram procedure. We do not use both spaces, because of the large number of features this creates.\n5.3.4 Extending Lexical Entailment to Phrases. The lexical entailment rule classifier described in previous sections is limited to only simple rules, where the LHS and RHS are both single words. Many of the rules generated by the modified Robinson resolution are actually phrasal rules, such as little boy\u2192 child, or running\u2192 moving quickly. In order to model these phrases, we use two general approaches: first, we use a state-of-the-art compositional distributional model, in order to create vector representations of phrases, and then include the same cosine and cosine histogram features described in the previous section. The full details of the compositional distributional model are described in Section 5.3.5. In addition to a compositional distributional model, we also used a simple, greedy word aligner, similar to the one described by Lai and Hockenmaier (2014). This aligner works by finding the pair of words on the LHS and RHS which are most similar in a distributional space, and marking them as \u201caligned\u201d. The process is repeated until at least one side is completely exhausted.\nAfter performing the phrasal alignment, we compute a number of base features, based on just the results of the alignment procedure. These include values like the length of the rule, the percent of words unaligned, etc. We also compute all of the same features used in the lexical entailment rule classifier (Wordform, WordNet, Distributional) and compute their min/mean/max across all the alignments. We do not include the asymmetric entailment features as the feature space then becomes extremely large, and it is unclear how these features should be useful. Table 2 contains a listing of all phrasal features used.\n5.3.5 Phrasal Distributional Semantics. We build phrasal distributional space based on the practical lexical function model of Paperno, Pham, and Baroni (2014). We again use as the corpus a concatenation of BNC, ukWaC and English Wikipedia, parsed with the Stanford CoreNLP parser. We focus on 5 types of dependency labels, \u201camod\u201d, \u201cnsubj\u201d, \u201cdobj\u201d, \u201cpobj\u201d, \u201cacomp\u201d, and combine the governor and dependent words of these dependencies to form adjective-noun, subject-verb, verb-object, preposition-noun and verb-complement phrases respectively.We only retain phrases where both the governor and the dependent are among the 50K most frequent words in the corpus, which results in roughly 1.9 million unique phrases. The co-occurrence counts of the 1.9 million phrases with the 20K most frequent neighbor words within a 2-word window are converted to a positive Pointwise Mutual Information (PPMI) matrix, further reduced to 300 dimensions by performing SVD on a lexical vector space and applying the resulting SVD representation to the phrase vectors (as performing SVD directly on the phrasal vector space would have been too memory-intensive), and normalized to length 1.\nPaperno et al. represent a word as a vector, which represents the contexts in which the word can appear, along with a number of matrices, one for each type of dependent that the word can take. For a transitive verb like chase, this would be one matrix for subjects, and one for direct objects. The representation of the phrase chases dog is then\n~chase+ o chase\u00d7 ~dog\nwhere \u00d7 is matrix multiplication, and when the phrase is extended with cat to form cat chases dog, the representation is\n~chase+ s chase\u00d7 ~cat+ ( ~chase+ o chase\u00d7 ~dog)\nFor verbs, the practical lexical function model trains a matrix for each of the relations nsubj, dobj and acomp, for adjectives a matrix for amod, and for prepositions a matrix for pobj. For example, the amod matrix of the adjective \u201cred/JJ\u201d is trained as follows. We collect all phrases in which \u201cred/JJ\u201d serves as adjective modifier (assuming the number of such phrases is N ), like \u201cred/JJ car/NN\u201d, \u201cred/JJ house/NN\u201d etc., and construct two 300\u00d7N matrices Marg and Mph, where the ith column of Marg is the vector of the noun modified by \u201cred/JJ\u201d in the ith phrase (\u2212\u2192car, \u2212\u2212\u2212\u2192 house, etc.), and the ith column of Mph is vector of phrase i minus the vector of \u201cred/JJ\u201d ( \u2212\u2212\u2212\u2212\u2192 red car \u2212 \u2212\u2192 red, \u2212\u2212\u2212\u2212\u2212\u2212\u2192 red house\u2212 \u2212\u2192 red, etc.), normalized to length 1. Then the amod matrix red (amod) \u2208 R300\u00d7300 of \u201cred/JJ\u201d can be computed via ridge regression. Given trained matrices, we compute the composition vectors by applying the lexical functions recursively starting from the lowest dependency.\nAs discussed above, some of the logical rules from Section 5.2 need to be split into multiple rules. We use the dependency parse to split long rules by iteratively searching for the highest nodes in the dependency tree that occur in the logical rule, and identifying the logical rule words that are its descendants in phrases that the practical lexical functional model can handle. After splitting, we perform greedy alignment on phrasal vectors to pair up rule parts. Similar to Section 5.3.4, we iteratively identify the pair of phrasal vectors on the LHS and RHS which have the highest cosine similarity until one side has no more phrases."}, {"heading": "6. Probabilistic Logical Inference", "text": "The last component is probabilistic logical inference. We showed in Section 4 how to represent the task as probabilistic inference problems of the form P (Q|E,R,W ), where Q is the query formula, E is the evidence set, R is a set of rules, and W is the world configuration. This section shows how to solve this inference problem.\nMLN inference is usually intractable, and using MLN implementations \u201cout of the box\u201d does not work for our application. This section discusses an MLN implementation that supports complex queries Q. It also shows how to use the CWA to decrease the problem size, hence making inference more efficient. Finally, this section discusses a simple weight learning scheme to learn global scaling factors for weighted rules in KB from different sources. Other than for the weight learning, this section mostly draws on the previous paper by Beltagy and Mooney (2014)."}, {"heading": "6.1 Query Formula", "text": "Current implementations of MLNs like Alchemy (Kok et al. 2005) do not allow queries to be complex formulas, they can only calculate probabilities of ground atoms. This section discusses an inference algorithm for arbitrary query formulas.\nStandard Work-Around. Although current MLN implementations can only calculate probabilities of ground atoms, they can be used to calculate the probability of a complex formula through a simple work-around. The complex query formula Q is added to the MLN using the hard formula:\nQ\u21d4 result(D) | \u221e (6)\nwhere result(D) is a new ground atom that is not used anywhere else in the MLN. Then, inference is run to calculate the probability of result(D), which is equal to the probability of the formula Q. However, this approach can be very inefficient for some queries. For example, consider the query Q,\nQ : \u2203x, y, z. man(x) \u2227 agent(y, x) \u2227 drive(y) \u2227 patient(y, z) \u2227 car(z) (7)\nThis form of an existentially quantified formula with a list of conjunctively joined atoms is very common in the inference problems we are addressing, so it is important to have efficient inference for such queries. However, using thisQ in Equation 6 results in a very inefficient MLN. The direction Q\u21d0 result(D) of the double-implication in Equation 6 is very inefficient because the existentially quantified formula is replaced with a large disjunction over all possible combinations of constants for variables x, y and z (Gogate and Domingos 2011). Generating this disjunction, converting it to clausal form, and running inference on the resulting ground network becomes increasingly intractable as the number of variables and constants grow.\nNew Inference Method. Instead, we propose an inference algorithm to directly calculate the probability of complex query formulas. The probability of a formula is the sum of the probabilities of the possible worlds that satisfy it. Gogate and Domingos (2011) show that to calculate the probability of a formula Q given a probabilistic knowledge base K, it is enough to compute the partition function Z of K with and without Q added as a hard formula:\nP (Q | K) = Z(K \u222a {(Q,\u221e)}) Z(K)\n(8)\nTherefore, all we need is an appropriate algorithm to estimate the partition function Z of a Markov network. Then, we construct two ground networks, one with the query and one without, and estimate their Zs using that estimator. The ratio between the two Zs is the probability of Q.\nWe tried to estimate Z using a harmonic-mean estimator on the samples generated by MC-SAT (Poon and Domingos 2006), a popular and generally effective MLN inference algorithm, but we found that the estimates are highly inaccurate as shown by Venugopal and Gogate (2013). Instead we use SampleSearch (Gogate and Dechter 2011) to estimate the partition function. SampleSearch is an importance sampling algorithm that has been shown to be effective when there is a mix of probabilistic and deterministic (hard) constraints, a fundamental property of the inference problems we address. Importance sampling in general is problematic in the presence of determinism, because many of the generated samples violate the deterministic constraints, and they get rejected. Instead, SampleSearch uses a base sampler to generate samples then uses backtracking search with a SAT solver to modify the generated sample if it violates the deterministic constraints. We use an implementation of SampleSearch that uses a generalized belief propagation algorithm called Iterative Join-Graph Propagation (IJGP) (Dechter, Kask, and Mateescu 2002) as a base sampler. This version is available online (Gogate 2014).\nFor cases like the example Q in Equation 7, we need to avoid generating a large disjunction because of the existentially quantified variables. So we replace Q with its negation \u00acQ, replacing the existential quantifiers with universals, which are easier to ground and perform inference upon. Finally, we compute the probability of the\nquery P (Q) = 1\u2212 P (\u00acQ). Note that replacing Q with \u00acQ cannot make inference with the standard work-around faster, because with \u00acQ, the direction \u00acQ\u21d2 result(D) suffers from the same problem of existential quantifiers that we previously had with Q\u21d0 result(D)."}, {"heading": "6.2 Inference Optimization using Closed-World Assumption", "text": "This section explains why our MLN inference problems are computationally difficult, then explains how the closed-world assumption (CWA) can be used to reduce the problem size and speed up inference. For more details, see Beltagy and Mooney (2014).\nIn the inference problems we address, formulas are typically long, especially the query formula. The number of ground clauses of a first-order formula is exponential in the number of variables in the formula, it is O(cv), where c is number of constants in the domain and v is number of variables in the formula. For any moderately long formula, the number of resulting ground clauses is infeasible to process using available inference algorithms.\nSection 4.4 concludes that we should make the CWA when formulating the RTE task as a probabilistic logic inference problem. This is to say, that all ground atoms have low prior probabilities, unless they can be inferred from the evidence and knowledge base. However, we found that a large fraction of the ground atoms cannot be inferred from the evidence and knowledge base, and their probabilities remain very low. This suggests that these ground atoms can be identified and removed in advance with very little impact on the approximate nature of the inference. As the number of such ground atoms is large, this has the potential to dramatically decrease the size of the ground network and speed up inference.\nFor a ground atom to be inferred (its marginal probability does not equal the prior probability), the ground atom needs to be reachable from the evidence. A ground atom is said to be reachable if there is a way to propagate the evidence through the knowledge base and reach this ground atom. For example,\nPredicates : man(x), person(x), dog(x)\nE : man(M)\nKB : \u2200x. man(x)\u21d2 person(x)\nThe constantM propagates from the evidenceman(M) throughKB to reach person(x). Therefore, person(M) is a reachable ground atom and its marginal probability does not equal its prior probability. However, for the predicate dog(x), there is no way for the constant M to reach it, and the probability of ground atom dog(M) equals its prior probability. The ground atom dog(M) can be removed from the ground network without significantly affecting the probability of the query. Another way of looking at this notion of reachability is as a form of automatic type checking, where man and person are of compatible types, while dog is incompatible."}, {"heading": "6.3 Weight Learning", "text": "The KB is a set of weighted rules. These weights come from different sources, in our case PPDB weighst (Section 5.1.2) and the confidence of the entailments rule classifier (Section 5.3). These weights need to be mapped to MLN weights. We use weight\nlearning to do the mapping. Similar to the work of Zirn et al. (2011), we learn a single mapping parameter for each source of rules that functions as a scaling factor:\nMLNweight = scalingFactor \u00d7 ruleWeight (9)\nWe use a simple grid search to learn the scaling factors that optimize performance on the RTE training data.\nAssuming that all rule weights are in [0, 1] (this is the case for classification confidence scores, and PPDB weights can be scaled), we also try the following mapping function:\nMLNweight = scalingFactor \u00d7 log( ruleWeight 1\u2212 ruleWeight ) (10)\nThis function assures that for an MLN with a single rule LHS \u21d2 RHS |MLNweight, it is the case that P (RHS|LHS) = ruleWeight, given that scalingFactor = 1."}, {"heading": "7. Evaluation", "text": "This section evaluates our system. First, we evaluate several lexical and phrasal distributional systems on the rules we collected using modified Robinson resolution. Second, we use the best configuration we find in the first step as a knowledge base and evaluate our system on the RTE task using the SICK dataset.\nDataset. The SICK dataset, which is described in Section 2, consists of 5,000 pairs for training and 4,927 for testing. Pairs are annotated for RTE and STS (Semantic Textual Similarity) tasks. We use the RTE part of the dataset."}, {"heading": "7.1 Evaluating the Entailment Rule Classifier", "text": "We evaluate the entailment rule classifier described in Section 5.3. Evaluation is broken into four parts: first, we overview performance of the entire entailment rule classifier on all rules, both lexical and phrasal. We then break down these results into performance on only lexical rules and only phrasal rules. Finally, we look at only the asymmetric features to address concerns raised by Levy et al. (2015).\n7.1.1 Experimental Setup. We use the gold standard annotations described in Section 5.2.4. We perform 10 fold cross-validation on the annotated training set, using the same folds in all settings. Since some RTE sentence pairs require multiple lexical rules, we ensure that CV folds are stratified across the sentences, so that the same sentence cannot appear in both training and testing. We use a Logistic Regression classifier with\nan L2 regularizer.5 Since we perform three-way classification, we train models using one-vs-all.\nPerformance is measured in two main metrics. Intrinsic accuracy measures how the classifier performs in the cross-validation setting on the training data. This corresponds to treating lexical and phrasal entailment as a basic supervised learning problem. RTE accuracy is accuracy on the end task of textual entailment using the predictions of the entailment rule classifier. For RTE accuracy, the predictions of the entailment rule classifier were used as the only knowledge base in the RTE system. RTE training accuracy uses the predictions from the cross-validation experiment, and for RTE test accuracy the entailment rule classifier was trained on the whole training set.\n7.1.2 Overall Lexical and Phrasal Entailment Evaluation. Table 3 shows the results of the Entailment experiments on all rules, both lexical and phrasal. In order to give bounds on our system\u2019s performance, we present baseline score (entailment rule classifier always predicts non-entailing) and ceiling score (entailment rule classifier always predicts gold standard annotation).\nWe see that the ceiling score (entailment rule classifier always predicts gold standard annotation) does not achieve perfect performance. This is due to a number of different issues including parsing problems, imperfect rules generated by the modified Robinson resolution, a few system inference timeouts, and various idiosyncracies of the SICK dataset.\nAnother point to note is that WordNet is by far the strongest set of features for the task. This is unsurprising, as synonomy and hypernymy information from WordNet gives nearly perfect information for much of the task. There are some exceptions, such as woman 9 man, or black 9 white, which WordNet lists as antonyms, but which are\n5 We experimented with multiple classifiers, including Logistic Regression, Decision Trees, and SVM (with polynomial, RBF, and linear kernels). We found that Logistic Regression and Linear SVMs performed best, but chose Logistic Regression, since it was also used in Roller, Erk, and Boleda (2014). We also determined an L2 regularizer slightly outperformed an L1 regularizer, but did not vary the regularization parameter. It is not immediately obvious why a linear classifier should perform best on the task, but we believe it is because a great deal of nonlinearity is already encoded in the features themselves. For example, Same POS and the Histogram and Min/Mean/Max features all capture simple nonlinearities, without giving the classifier freedom to search for interaction between unrelated variables. It is worth noting, however, that Lai and Hockenmaier (2014) use similar features and also found success with a Logistic Regression classifier.\nnot considered contradictions in the SICK dataset. However, even though WordNet has extremely high coverage on this particular dataset, it still is far from exhaustive: about a quarter of the rules have at least one pair of words for which WordNet relations could not be determined.\nWe notice that the lexical distributional features do surprisingly well on the task. This indicates that, even with only distributional similarity, we do well enough to score in the upper half of systems in the original SemEval shared task (Marelli et al. 2014a). Two components were critical to our increased performance over our prior work: first, the use of multiple distributional spaces (one topical, one syntactic); second, the binning of cosine values.\nThe phrasal distributional similarity features, which are based on the state-of-theart (Paperno, Pham, and Baroni 2014) compositional vector space, perform somewhat disappointingly on the task. We discuss possible reasons for this below in Section 7.1.4.\nWe also note that the Basic Alignment features and WordForm features (described in Tables 1 and 2) do not do particularly well on their own. This is encouraging, as it means the dataset cannot be handled by simply expecting the same words to appear on the LHS and RHS. Finally, we note that the features are highly complementary, and the combination of all features gives a substantial boost to performance.\n7.1.3 Evaluating the Lexical Entailment Rule Classifier. Table 4 shows performance of the classifier on only the lexical rules, which have single words on the LHS and RHS. In these experiments we use the same procedure as before, but omit the phrasal rules from the dataset. On the RTE tasks, we compute accuracy over only the SICK pairs which require at least one lexical rule. Note that a new ceiling score is needed, as some rules require both lexical and phrasal predictions, but we do not predict any phrasal rules.\nAgain we see that WordNet features have the highest contribution. Distributional rules still perform dramatically better than the baseline, but the gap between distributional features and WordNet is much more apparent. Perhaps most encouraging is the very high performance of the Asymmetric features: by themselves, they perform substantially better than just the distributional features. We investigate this further below in Section 7.1.5.\nAs with the entire dataset, we once again see that all the features are highly complementary, and intrinsic accuracy is greatly improved by using all the features together. It may be surprising that these significant gains in intrinsic accuracy do not translate to improvements on the RTE tasks; in fact, there is a minor drop from using all features compared to only using WordNet. This most likely depends on which pairs\nthe system gets right or wrong. For sentences involving multiple lexical rules, errors become disproportionately costly. As such, the high-precision WordNet predictions are slightly better on the RTE task.\n7.1.4 Evaluating the Phrasal Entailment Rule Classifier. Table 5 shows performance when looking at only the phrasal rules. As with the evaluation of lexical rules, we evaluate the RTE tasks only on sentence pairs that use phrasal rules, and do not provide any lexical inferences. As such, the ceiling score must again be recomputed.\nWe first notice that the phrasal subset is generally harder than the lexical subset: none of the features sets on their own provide dramatic improvements over the baseline, or come particularly close to the ceiling score. On the other hand, using all features together does dramatically better than any of the feature groups by themselves, indicating again that the feature groups are highly complementary.\nDistributional features perform rather close to the Wordform features, suggesting that possibly the Distributional features may simply be proxies for the same lemma and same POS features. A qualitative analysis comparing the predictions of Wordform and Distributional features shows otherwise though: the Wordform features are best at correctly identifying nonentailing phrases (higher precision), while the distributional features are best at correctly identifying entailing phrases (higher recall).\nAs with the full dataset, we see that the features based on Paperno, Pham, and Baroni (2014) do not perform as well as just the alignment-based distributional lexical features; in fact, they do not perform even as well as features which make predictions using only Wordform features. We qualitatively compare the Paperno et al. features (or phrasal features for short) to the features based on word similarity of greedily aligned words (or alignment features). We generally find the phrase features are much more likely to predict neutral, while the alignment-based features are much more likely to predict entailing. In particular, the phrasal vectors seem to be much better at capturing non-entailment based on differences in prepositions (walk inside building 9 walk outside building), additional modifiers on the RHS (man 9 old man, room 9 darkened room), and changing semantic roles (man eats near kitten 9 kitten eats). Surprisingly, we found the lexical distributional features were better at capturing complex paraphrases, such as teenage\u2192 in teens, ride bike\u2192 biker, or young lady\u2192 teenage girl.\n7.1.5 Evaluating the Asymmetric Classifier. Levy et al. (2015) show several experiments suggesting that asymmetric classifiers do not perform substantially better at the task of\nidentifying hypernyms than when the RHS vectors alone are used as features. That is, they find that the asymmetric classifier and variants frequently learn to identify prototypical hypernyms rather than the hypernymy relation itself. We look at our data in the light of the Levy et al. study, in particular as none of the entailment problem sets used by Levy et al. were derived from an existing RTE dataset like our entailment problems.\nIn a qualitative analysis comparing the predictions of a classifier using only Asymmetric features with a classifier using only cosine similarity, we found that the Asymmetric classifier does substantially better at distinguishing hypernymy from co-hyponomy. This is what we had hoped to find, as we had previously found an Asymmetric feature-based classifier to perform well at identifying hypernymy in other data sets (Roller, Erk, and Boleda 2014), and cosine is known to heavily favor cohyponymy (Baroni and Lenci 2011). On the other hand, we find that cosine features are much better at discovering synonomy, and that Asymmetric frequently mistakes antonomy as an entailing relation.\nWe further did a qualitative analysis comparing the predictions of a classifier using only Asymmetric features to a classifier that tries to learn typical hyponyms or hypernyms by using only the LHS vectors, or the RHS vectors, or both. Table 6 shows the results of these experiments. We only consider rules with single words on the LHS and RHS, so our ceiling and baseline are the same as in Table 4.\nCounter to the main findings of Levy et al. (2015), we find that there is at least some learning of the entailment relationship by the asymmetric classifier (in particular on the intrinsic evaluation), as opposed to the prototypical hypernym hypothesis. We believe this is because the dataset is too varied to allow the classifier to learn what an entailing RHS looks like. Indeed, a qualitative analysis shows that the asymmetric features successfully predict many hypernyms that RHS vectors miss. On the other hand, the RHS do manage to capture particular semantic classes, especially on words that appear many times in the dataset, like cut, slice, man, cliff, and weight.\nThe classifier given both the LHS and RHS vectors dramatically outperforms its components: it is given freedom to nearly memorize rules that appear commonly in the data. Still, using all three sets of features (Asym + LHS + RHS) is most powerful by a substantial margin. This feature set is able to capture the frequently occuring items, while also allowing some power to generalize to novel entailments."}, {"heading": "7.2 RTE Task Evaluation", "text": "This section evaluates different components of the system, and finds a configuration of our system that achieves state-of-the-art results on the SICK RTE dataset.\nWe evaluate the following system components. The component logic is our basic MLN-based logic system that computes two inference probabilities (Section 4.2). This includes the changes to the logical form to handle the domain closure assumption (Section 4.3), the inference algorithm for query formulas (Section 6.1), and the inference optimization (Section 6.2). The component cws deals with the problem that the closed-world assumption raises for negation in the hypothesis (Section 4.4), and coref is coreference resolution to identify contradictions (Section 4.5). The component multiparse signals the use of two parsers, the top C&C parse and the top EasyCCG parse (Section 4.1).\nThe remaining components add entailment rules. The component eclassif adds the rules from the best performing entailment rule classifier trained in Section 7.1. This is the system with all features included. The ppdb component adds rules from PPDB paraphrase collection (Section 5.1.2). The wlearn component learns a scaling factor for ppdb rules, and another scaling factor for the eclassif rules that maps the classification confidence scores to MLN weights (Section 6.3). Without weight learning, the scaling factor for ppdb is set to 1, and all eclassif rules are used as hard rules (infinite weight). The wlearn_log component is similar to wlearn but uses equation 10, which first transforms a rule weight to its log odds. The wn component adds rules from WordNet (Section 5.1.1). In addition, we have a few handcoded rules (Section 5.1.3). Like wn, the components hyp and mem repeat information that is used as features for entailment rules classification but is not always picked up by the classifier. As the classifier sometimes misses hypernyms, hyp marks all hypernymy rules as entailing (so this component is subsumed by wn), as well as all rules where the left-hand side and the right-hand side are the same. (The latter step becomes necessary after splitting long rules derived by our modified Robinson resolution; some of the pieces may have equal left-hand and right-hand sides.) The mem component memorizes all entailing rules seen in the training set of eclassif.\n7.2.1 Ablation Experiment without eclassif. Because eclassif has the most impact on the system\u2019s accuracy, and when enabled suppresses the contribution of the other components, we evaluate the other components first without eclassif. In the following section, we add the eclassif rules. Table 7 summarizes the results of this experiment.\nThe results show that each component plays a role in improving the system accuracy. Our best accuracy without eclassif is 80.37%. Without handling the problem of negated hypotheses (logic alone), P (\u00acH|T ) is almost always 1 and this additional inference becomes useless, resulting in an inability to distinguish between Neutral and Contradiction. Adding cwa significantly improves accuracy because the resulting system has P (\u00acH|T ) equal to 1 only for Contradictions.\nEach rule set (ppdb, wn, handcoded) improves accuracy by reducing the number of false negatives. We also note that applying weight learning (wlearn) to find a global scaling factor for PPDB rules makes them more useful. The learned scaling factor is 3.0. When the knowledge base is lacking other sources, weight learning assigns a high scaling factor to PPDB, giving it more influence throughout. When eclassif is added in the following section, weight learning assigns PPDB a low scaling factor because eclassif already includes a large set of useful rules, such that only the highest weighted PPDB rules contribute significantly to the final inference.\nThe last component tested is the use of multiple parses (multiparse). Many of the false negatives are due to misparses. Using two different parses reduces the impact of the misparses, improving the system accuracy.\n7.2.2 Ablation Experiment with eclassif. In this experiment, we first use eclassif as a knowledge base, then incrementally add the other system components. Table 8 summarizes the results. First, we note that adding eclassif to the knowledge base KB significantly improves the accuracy from 73.37% to 82.99%. This is higher than what ppdb and wn achieved without eclassif. Adding handcoded still improves the accuracy somewhat.\nAdding multiparse improves accuracy, but interestingly, not as much as in the previous experiment (without eclassif). The improvement on the test set decreases from 1.58% to just 0.18%. Therefore, the rules in eclassif help reduce the impact of misparses. Here is an example to show how: T: A deer is jumping over a wall, H: The deer is jumping over the fence which in logic are:\nT : \u2203x, y, z. deer(x) \u2227 agent(y, x) \u2227 jump(y) \u2227 over(y, z) \u2227 wall(z) H : \u2203x, y, z. deer(x) \u2227 agent(y, x) \u2227 jump(y) \u2227 over(y) \u2227 patient(y, z) \u2227 wall(z)\nThe modified Robinson resolution yields the following rule:\nF : \u2200x, y. jump(x) \u2227 over(x, y) \u2227 wall(y)\u21d2 jump(x) \u2227 over(x) \u2227 patient(x, y) \u2227 wall(y)\nNote that in T , the parser treats over as a preposition, while in H , jump over is treated as a particle verb. A lexical rule wall \u21d2 fence is not enough to get the right inference because of this inconsistency in the parsing. The rule F reflects this parsing inconsistency. When F is translated to text for the entailment classifier, we obtain jump over wall\u21d2 jump over fence, which is a simple phrase that the entailment classifier addresses without dealing with the complexities of the logic. Without the modified Robinson resolution, we would have had to resort to collecting \u201cstructural\u201d inference rules like \u2200x, y. over(x, y)\u21d2 over(x) \u2227 patient(x, y).\nThe next two components added are hyp and mem, two components that in principle should not add anything over eclassif, but they do add some accuracy due to noise in the training data of eclassif.\nThe next components are wlearn and wlearn_log. Both weight learning components help improve the system\u2019s accuracy. It is interesting to see that even though the SICK dataset is not designed to evaluate \"degree of entailment\", it is still useful to keep the rules uncertain (as opposed to using hard rules) and use probabilistic inference. Results also show that wlearn_log performs slightly better than wlearn.\nFinally, adding ppdb does not improve the accuracy. Apparently, eclassif already captures all the useful rules that we were getting from ppdb. It is interesting to see that simple distributional information can subsume a large paraphrase database like PPDB. Adding wn (not shown in the table) leads to a slight decrease in accuracy.\nThe system comprising logic, cwa, coref, multiparse, eclassif, handcoded, hyp, wlearn_log, and mem achieves an accuracy of 85.06% on the SICK test set, achieving a state-of-the-art score. The entailment rule classifier eclassif plays a vital role in achieving this result."}, {"heading": "8. Future Work", "text": "Contextualization. The evaluation of the entailment rule classifier showed that some of the entailments are context-specific, like put/pour (which are entailing only for liquids) or push/knock (which is entailing in the context of \u201cpushing a toddler into a puddle\u201d). Cosine-based distributional features were able to identify some of these cases when all other features did not. We would like to explore whether contextualized distributional word representations, which take the sentence context into account (Erk and Pad\u00f3 2008; Thater, F\u00fcrstenau, and Pinkal 2010; Dinu, Thater, and Laue 2012), can identify such context-specific lexical entailments more reliably.\nDistributional entailment. It is well-known that cosine similarity gives particularly high ratings to co-hyponyms (Baroni and Lenci 2011), and our evaluation confirmed that this is a problem for lexical entailment judgments, as co-hyponyms are usually not entailing. However, co-hyponymy judgments can be used to position unknown terms in the WordNet hierarchy (Snow, Jurafsky, and Ng 2006). This could be a new way of using distributional information in lexical entailment: using cosine similarity to position a term in an existing hierarchy, and then using the relations in the hierarchy for lexical entailment. While distributional similarity is usually used only on individual word pairs as if nothing else was known about the language, this technique would use\ndistributional similarity to learn the meaning of unknown terms given that many other terms are already known.\nMultiple instance learning. To obtain training data for the entailment rule classifier, we annotated aligned pairs of words and phrases in the SICK dataset. However, this laborintensive approach does not scale up to other datasets, and it is not clear that the entailment examples from the SICK training set would be equally helpful for other RTE datasets. We will instead experiment with multiple instance learning (Dietterich, Lathrop, and Lozano-Perez 1997; Bunescu and Mooney 2007): A classifier learns from bags of items. In our case, a positive bag (entailing RTE pair) will contain only positive datapoints, while a negative bag (neutral or contradiction) will contain at least one negative datapoint.\nQuestion Answering. Our semantic representation is a deep flexible semantic representation that can be used to perform various types of tasks. We are interested in applying our semantic representation to the question answering task. Question answering is the task of finding an answer of a WH question from large text corpus. This task is interesting because it may offer a wider variety of tasks to the distributional subsystem, including context-specific matches and the need to learn domain-specific distributional knowledge. In our framework, all the text would be translated to logic, and the question would be translated to a logical expression with an existentially quantified variable representing the questioned part. Then the probabilistic logic inference tool would aim to find the best entities in the text that fill in that existential quantifier in the question. Existing logic-based systems are usually applied to limited domains, such as querying a specific database (Kwiatkowski et al. 2013; Berant et al. 2013), but with our system, we have the potential to query a large corpus because we are using Boxer for widecoverage semantic analysis. The interesting bottleneck is the inference. It would be very challenging to scale probabilistic logic inference to such large inference problems.\nGeneralized Quantifiers. One important extension to this work is to support generalized quantifiers in probabilistic logic. Some determiners, such as \u201cfew\u201d and \u201cmost\u201d, cannot be represented in standard first-order logic, and are usually addressed using higherorder logics. But it could be possible to represent them using the probabilistic aspect of probabilistic logic, sidestepping the need for higher-order logic."}, {"heading": "9. Conclusion", "text": "Being able to effectively represent natural languages semantics is important and has many important applications. We have introduced an approach that uses probabilistic logic to combine the expressivity and automated inference provided by logical representations, with the ability to capture graded aspects of natural language captured by distributional semantics. We evaluated this semantic representation on the RTE task which requires deep semantic understanding. Our system maps natural-language sentences to logical formulas, uses them to build probabilistic logic inference problems, builds a knowledge base from precompiled resources and on-the-fly distributional resources, then performs inference using Markov Logic. Experiments demonstrated state-of-theart performance on the recently introduced SICK RTE task."}, {"heading": "Acknowledgments", "text": "This research was supported by the DARPA DEFT program under AFRL grant FA8750-13-2-0026 and by the NSF CAREER grant IIS 0845925. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the view of DARPA, DoD or the US government. Some experiments were run on the Mastodon Cluster supported by NSF Grant EIA-0303609. Some experiments were run at the Texas Advanced Computing Center (TACC)6 at The University of Texas at Austin."}], "references": [{"title": "Integrating experiential and distributional data to learn semantic representations", "author": ["Andrews", "Vigliocco", "Vinson2009]Andrews", "Mark", "Gabriella Vigliocco", "David Vinson"], "venue": "Psychological Review,", "citeRegEx": "Andrews et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2009}, {"title": "Frege in space: A program for compositional distributional semantics", "author": ["Baroni", "Bernardi", "Zamparelli2014]Baroni", "Marco", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010]Baroni", "Marco", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "How we BLESSed distributional semantic evaluation", "author": ["Baroni", "Lenci2011]Baroni", "Marco", "Alessandro Lenci"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010]Baroni", "Marco", "Roberto Zamparelli"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2010)", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Montague meets Markov: Deep semantics with probabilistic logical form", "author": ["Beltagy et al.2013]Beltagy", "Islam", "Cuong Chau", "Gemma Boleda", "Dan Garrette", "Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of the Second Joint Conference on Lexical and Computational Semantics", "citeRegEx": "al.2013.Beltagy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Beltagy et al\\.", "year": 2013}, {"title": "On the proper treatment of quantifiers in probabilistic logic semantics", "author": ["Beltagy", "Erk2015]Beltagy", "Islam", "Katrin Erk"], "venue": "In Proceedings of the International Conference on Computational Semantics", "citeRegEx": "Beltagy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2015}, {"title": "Efficient Markov logic inference for natural language semantics", "author": ["Beltagy", "Mooney2014]Beltagy", "Islam", "Raymond J. Mooney"], "venue": "In Proceedings of AAAI 2014 Workshop on Statistical Relational AI (StarAI-2014)", "citeRegEx": "Beltagy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Berant et al.2013]Berant", "Jonathan", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2013)", "citeRegEx": "al.2013.Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Berant et al\\.", "year": 2013}, {"title": "Natural language processing with Python. \"O\u2019Reilly Media, Inc.", "author": ["Bird", "Klein", "Loper2009]Bird", "Steven", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Bjerva et al.2014]Bjerva", "Johannes", "Johan Bos", "Rob van der Goot", "Malvina Nissim"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "al.2014.Bjerva et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014.Bjerva et al\\.", "year": 2014}, {"title": "Representation and Inference for Natural Language: A First Course in Computational Semantics", "author": ["Blackburn", "Bos2005]Blackburn", "Patrick", "Johan Bos"], "venue": "CSLI Publications,", "citeRegEx": "Blackburn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blackburn et al\\.", "year": 2005}, {"title": "Wide-coverage semantic analysis with Boxer", "author": ["Bos2008]Bos", "Johan"], "venue": "In Proceedings of Semantics in Text Processing", "citeRegEx": "Bos2008.Bos and Johan.,? \\Q2008\\E", "shortCiteRegEx": "Bos2008.Bos and Johan.", "year": 2008}, {"title": "Choosing sense distinctions for WSD: Psycholinguistic evidence", "author": ["Susan Windisch"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Windisch.,? \\Q2008\\E", "shortCiteRegEx": "Windisch.", "year": 2008}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012]Bruni", "Elia", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Comptuational Linguistics (ACL", "citeRegEx": "al.2012.Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al.2012.Bruni et al\\.", "year": 2012}, {"title": "Multiple instance learning for sparse positive bags", "author": ["Bunescu", "Mooney2007]Bunescu", "Razvan", "Ray Mooney"], "venue": "In Proceedings of the 24th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm", "author": ["Chang", "Lin2001]Chang", "Chih-Chung", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Vector space models of lexical meaning", "author": ["Clark2012]Clark", "Stephen"], "venue": "In Handbook of Contemporary Semantics. Wiley-Blackwell,", "citeRegEx": "Clark2012.Clark and Stephen.,? \\Q2012\\E", "shortCiteRegEx": "Clark2012.Clark and Stephen.", "year": 2012}, {"title": "Parsing the WSJ using CCG and log-linear models", "author": ["Clark", "Curran2004]Clark", "Stephen", "James R. Curran"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Clark et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2004}, {"title": "Mathematical foundations for a compositional distributed model of meaning. Linguistic Analysis, 36(1-4):345\u2013384", "author": ["Coecke", "Sadrzadeh", "Clark2011]Coecke", "Bob", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": null, "citeRegEx": "Coecke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "A probabilistic rich type theory for semantic interpretation", "author": ["Cooper et al.2014]Cooper", "Robin", "Simon Dobnik", "Shalom Lappin", "Staffan Larsson"], "venue": "In Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS),", "citeRegEx": "al.2014.Cooper et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014.Cooper et al\\.", "year": 2014}, {"title": "An open-source grammar development environment and broad-coverage english grammar using HPSG", "author": ["Copestake", "Flickinger2000]Copestake", "Ann", "Dan Flickinger"], "venue": "In Proceedings of Language Resources and Evaluation Conference (LREC),", "citeRegEx": "Copestake et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2000}, {"title": "Recognizing textual entailment: Models and applications", "author": ["Dagan et al.2013]Dagan", "Ido", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "al.2013.Dagan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Dagan et al\\.", "year": 2013}, {"title": "Iterative join-graph propagation", "author": ["Dechter", "Kask", "Mateescu2002]Dechter", "Rina", "Kalev Kask", "Robert Mateescu"], "venue": "In Proceedings of 18th Conference on Uncertainty in Artificial Intelligence (UAI-2002)", "citeRegEx": "Dechter et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 2002}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Dietterich", "Lathrop", "Lozano-Perez1997]Dietterich", "Thomas G", "Richard H. Lathrop", "Tomas Lozano-Perez"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "A comparison of models of word meaning in context", "author": ["Dinu", "Thater", "Laue2012]Dinu", "Georgiana", "Stefan Thater", "S\u00f6ren Laue"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL", "citeRegEx": "Dinu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2012}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Domingos", "Lowd2009]Domingos", "Pedro", "Daniel Lowd"], "venue": null, "citeRegEx": "Domingos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 2009}, {"title": "Introduction to Montague Semantics. D", "author": ["Dowty", "Wall", "Peters1981]Dowty", "David R", "Robert E. Wall", "Stanley Peters"], "venue": null, "citeRegEx": "Dowty et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Dowty et al\\.", "year": 1981}, {"title": "Reconciling fine-grained lexical knowledge and coarse-grained ontologies in the representation of near-synonyms", "author": ["Edmonds", "Hirst2000]Edmonds", "Philip", "Graeme Hirst"], "venue": "In Proceedings of the Workshop on Semantic Approximation,", "citeRegEx": "Edmonds et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Edmonds et al\\.", "year": 2000}, {"title": "A structured vector space model for word meaning in context", "author": ["Erk", "Pad\u00f32008]Erk", "Katrin", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2008)", "citeRegEx": "Erk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2008}, {"title": "PPDB: The paraphrase database", "author": ["Ganitkevitch", "Van Durme", "Callison-Burch2013]Ganitkevitch", "Juri", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Integrating logical representations with probabilistic information using Markov logic", "author": ["Garrette", "Erk", "Mooney2011]Garrette", "Dan", "Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of International Conference on Computational Semantics", "citeRegEx": "Garrette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Geffet", "Dagan2005]Geffet", "Maayan", "Ido Dagan"], "venue": "In Proceedings of the 43rd Annual meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Geffet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Geffet et al\\.", "year": 2005}, {"title": "Logical foundations of artificial intelligence", "author": ["Genesereth", "M.R. Nilsson1987]Genesereth", "N.J. Nilsson"], "venue": null, "citeRegEx": "Genesereth et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Genesereth et al\\.", "year": 1987}, {"title": "Introduction to Statistical Relational Learning", "author": ["Getoor", "L. Taskar2007]Getoor", "B. Taskar"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "SampleSearch: Importance sampling in presence of determinism", "author": ["Gogate", "Dechter2011]Gogate", "Vibhav", "Rina Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Gogate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2011}, {"title": "Probabilistic theorem proving", "author": ["Gogate", "Domingos2011]Gogate", "Vibhav", "Pedro Domingos"], "venue": "In Proceedings of 27th Conference on Uncertainty in Artificial Intelligence (UAI-2011)", "citeRegEx": "Gogate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2011}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Grefenstette2013]Grefenstette", "Edward"], "venue": "In Proceedings of Second Joint Conference on Lexical and Computational Semantics", "citeRegEx": "Grefenstette2013.Grefenstette and Edward.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette2013.Grefenstette and Edward.", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Grefenstette", "Sadrzadeh2011]Grefenstette", "Edward", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2011)", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "The Alchemy system for statistical relational AI. http://www.cs.washington.edu/ai/alchemy", "author": ["Kok et al.2005]Kok", "Stanley", "Parag Singla", "Matthew Richardson", "Pedro Domingos"], "venue": null, "citeRegEx": "al.2005.Kok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al.2005.Kok et al\\.", "year": 2005}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2013)", "citeRegEx": "al.2013.Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Kwiatkowski et al\\.", "year": 2013}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Lai", "Hockenmaier2014]Lai", "Alice", "Julia Hockenmaier"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Lai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2014}, {"title": "A solution to Plato\u2019s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997]Landauer", "Thomas", "Susan Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015]Levy", "Omer", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics a\u0302A\u0306S\u0327 Human Language Technologies (NAACL HLT 2015),", "citeRegEx": "al.2015.Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al.2015.Levy et al\\.", "year": 2015}, {"title": "Combined distributional and logical semantics. Transactions of the Association for Computational Linguistics (TACL-2013), 1:179\u2013192", "author": ["Lewis", "Steedman2013]Lewis", "Mike", "Mark Steedman"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2013}, {"title": "A* ccg parsing with a supertag-factored model", "author": ["Lewis", "Steedman2014]Lewis", "Mike", "Mark Steedman"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2014)", "citeRegEx": "Lewis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2014}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang", "Jordan", "Klein2011]Liang", "Percy", "Michael Jordan", "Dan Klein"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers, 28(2):203\u2013208", "author": ["Lund", "Burgess1996]Lund", "Kevin", "Curt Burgess"], "venue": null, "citeRegEx": "Lund et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lund et al\\.", "year": 1996}, {"title": "SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marelli et al.2014a]Marelli", "Marco", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": "In Proceedings of the International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "al.2014a.Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014a.Marelli et al\\.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marelli et al.2014b]Marelli", "Marco", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In Proceedings of the 9th Edition of the Language Resources and Evaluation Conference (LREC", "citeRegEx": "al.2014b.Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014b.Marelli et al\\.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marelli et al.2014c]Marelli", "Marco", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014),", "citeRegEx": "al.2014c.Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014c.Marelli et al\\.", "year": 2014}, {"title": "Artificial intelligence \u2013 a personal view", "author": ["Marr1977]Marr", "David"], "venue": "Artificial Intelligence,", "citeRegEx": "Marr1977.Marr and David.,? \\Q1977\\E", "shortCiteRegEx": "Marr1977.Marr and David.", "year": 1977}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov et al.2013]Mikolov", "Tomas", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of International Conference on", "citeRegEx": "al.2013.Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Yih", "Zweig2013]Mikolov", "Tomas", "Wentau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008]Mitchell", "Jeff", "Mirella Lapata"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010]Mitchell", "Jeff", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Paperno", "Pham", "Baroni2014]Paperno", "Denis", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Events in the semantics of English", "author": ["Parsons1990]Parsons", "Terry"], "venue": "MIT press,", "citeRegEx": "Parsons1990.Parsons and Terry.,? \\Q1990\\E", "shortCiteRegEx": "Parsons1990.Parsons and Terry.", "year": 1990}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["Poon", "Domingos2006]Poon", "Hoifung", "Pedro Domingos"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI-06),", "citeRegEx": "Poon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2006}, {"title": "A machine-oriented logic based on the resolution principle", "author": ["A. J"], "venue": null, "citeRegEx": "J.,? \\Q1983\\E", "shortCiteRegEx": "J.", "year": 1983}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Roller", "Erk", "Boleda2014]Roller", "Stephen", "Katrin Erk", "Gemma Boleda"], "venue": "In Proceedings of the Twenty Fifth International Conference on Computational Linguistics", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "The Frobenius anatomy of word meanings I: subject and object relative pronouns", "author": ["Sadrzadeh", "Clark", "Coecke2013]Sadrzadeh", "Mehrnoosh", "Stephen Clark", "Bob Coecke"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Sadrzadeh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sadrzadeh et al\\.", "year": 2013}, {"title": "Grounded models of semantic representation", "author": ["Silberer", "Lapata2012]Silberer", "Carina", "Mirella Lapata"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL", "citeRegEx": "Silberer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2012}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Snow", "Jurafsky", "Ng2006]Snow", "Rion", "Daniel Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL", "citeRegEx": "Snow et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Contextualizing semantic representations using syntactically enriched vector models", "author": ["Thater", "F\u00fcrstenau", "Pinkal2010]Thater", "Stefan", "Hagen F\u00fcrstenau", "Manfred Pinkal"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Thater et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thater et al\\.", "year": 2010}, {"title": "Formal Philosophy. Selected Papers of Richard Montague", "author": ["H. Richmond"], "venue": null, "citeRegEx": "Richmond,? \\Q1974\\E", "shortCiteRegEx": "Richmond", "year": 1974}, {"title": "Logical inference on dependency-based compositional semantics", "author": ["Tian", "Miyao", "Takuya2014]Tian", "Ran", "Yusuke Miyao", "Matsuzaki Takuya"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010]Turney", "Peter", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Probabilistic semantics for natural language. In Logic and interactive rationality (LIRA) yearbook", "author": ["van Eijck", "Lappin2012]van Eijck", "Jan", "Shalom Lappin"], "venue": null, "citeRegEx": "Eijck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Eijck et al\\.", "year": 2012}, {"title": "Computational Semantics with Functional Programming", "author": ["van Eijck", "Unger2010]van Eijck", "Jan", "Christina Unger"], "venue": null, "citeRegEx": "Eijck et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eijck et al\\.", "year": 2010}, {"title": "GiSS: Combining SampleSearch and Importance Sampling for inference in mixed probabilistic and deterministic graphical models", "author": ["Venugopal", "Gogate2013]Venugopal", "Deepak", "Vibhav Gogate"], "venue": "In Proceedings of Association for the Advancement of Artificial Intelligence(AAAI-13)", "citeRegEx": "Venugopal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Venugopal et al\\.", "year": 2013}, {"title": "Fine-grained sentiment analysis with structural features", "author": ["Zirn et al.2011]Zirn", "C\u00e4cilia", "Mathias Niepert", "Heiner Stuckenschmidt", "Michael Strube"], "venue": "In Proceedings of the The 5th International Joint Conference on Natural Language Processing (IJCNLP", "citeRegEx": "al.2011.Zirn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Zirn et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "There are now two hybrid approaches that combine logic and distributional semantics (Beltagy et al. 2013; Lewis and Steedman 2013), which both use logic-based semantics as a basis and add in distributional information to help with inference tasks. What is the status of such hybrid approaches? One possibility is to say that what we really want is a uniform framework that encompasses the abilities of both logic-based and distributional semantics, but until we have that, we will have to use hybrid systems. Another possibility \u2013 and this is the one that we will argue for \u2013 is that hybrid models are actually the right way to represent meaning. We follow Stokhof (2013) in assuming that meaning is a heterogenous phenomenon that is about truth conditions and grounding and observed contexts (among other things).", "startOffset": 85, "endOffset": 672}, {"referenceID": 6, "context": "In this paper, we discuss the system proposed in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013) in more detail, including improvements that allow MLN inference to scale more effectively (Beltagy and Mooney 2014) and to adapt logical", "startOffset": 86, "endOffset": 108}, {"referenceID": 6, "context": "Beltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Beltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both. This is the approach that we build on in this paper. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical forms.", "startOffset": 0, "endOffset": 284}, {"referenceID": 6, "context": "Beltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both. This is the approach that we build on in this paper. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical forms. The main difference between the two approaches lies in the role of gradience. Lewis and Steedman view weights and probabilities as a problem to be avoided. We believe that the uncertainty inherent in both language processing and world knowledge should be front and center in the inference that we do. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang,", "startOffset": 0, "endOffset": 771}, {"referenceID": 53, "context": "We use Word2Vec\u2019s skip-gram algorithm to create vectors (Mikolov et al. 2013).", "startOffset": 56, "endOffset": 77}], "year": 2017, "abstractText": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not adequately capture overall sentence structure. So it has been argued that the two are complementary. In this paper, we adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). We focus on textual entailment (RTE), a task that can utilize the strengths of both representations. Our system is three components, 1) parsing and task representation, where input RTE problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) knowledge base construction in the form of weighted inference rules from different sources like WordNet, paraphrase collections, and lexical and phrasal distributional rules generated on the fly. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that counteract scaling differences between resources. 3) inference, where we show how to solve the inference problems efficiently. In this paper we focus on the SICK dataset, and we achieve a state-of-the-art result. Our system handles overall sentence structure and phenomena like negation in the logic, then uses our Robinson resolution variant to query distributional systems about words and short phrases. Therefor, we use our system to evaluate distributional lexical entailment approaches. We also publish the set of rules queried from the SICK dataset, which can be a good resource to evaluate them.", "creator": "LaTeX with hyperref package"}}}