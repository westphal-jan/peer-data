{"id": "1506.00196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion", "abstract": "Sequence-to-sequence translation methods based on generating with a page-by-page language model have recently shown promising results in several tasks. Machine translation has used models based on source-side words to generate target-language text, and caption models of conditioned images have been used to generate captions. Previous work with this approach has focused on large vocabulary tasks and measured quality with respect to BLEU. In this paper, we examine the applicability of such models to the qualitatively different task of graph-to-phoneme. Here, the vocabularies on the input and output side are small, simple n-gram models perform well, and credits are only granted when the output is accurate. We find that the simple page-by-page generation approach is able to compete with the state of the art, and we are able to use the stat-of-the-short-time memory (STD) clearly.", "histories": [["v1", "Sun, 31 May 2015 05:14:06 GMT  (62kb,D)", "https://arxiv.org/abs/1506.00196v1", "submitted to Interspeech 2015"], ["v2", "Fri, 12 Jun 2015 12:47:57 GMT  (58kb,D)", "http://arxiv.org/abs/1506.00196v2", null], ["v3", "Thu, 20 Aug 2015 06:27:07 GMT  (62kb,D)", "http://arxiv.org/abs/1506.00196v3", "Published in INTERSPEECH 2015, Dresden, Germany"]], "COMMENTS": "submitted to Interspeech 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kaisheng yao", "geoffrey zweig"], "accepted": false, "id": "1506.00196"}, "pdf": {"name": "1506.00196.pdf", "metadata": {"source": "CRF", "title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion", "authors": ["Kaisheng Yao", "Geoffrey Zweig"], "emails": ["gzweig}@microsoft.com"], "sections": [{"heading": null, "text": "eration with a side-conditioned language model have recently shown promising results in several tasks. In machine translation, models conditioned on source side words have been used to produce target-language text, and in image captioning, models conditioned images have been used to generate caption text. Past work with this approach has focused on large vocabulary tasks, and measured quality in terms of BLEU. In this paper, we explore the applicability of such models to the qualitatively different grapheme-to-phoneme task. Here, the input and output side vocabularies are small, plain n-gram models do well, and credit is only given when the output is exactly correct. We find that the simple side-conditioned generation approach is able to rival the state-of-the-art, and we are able to significantly advance the stat-of-the-art with bi-directional long short-term memory (LSTM) neural networks that use the same alignment information that is used in conventional approaches. Index Terms: neural networks, grapheme-to-phoneme conversion, sequence-to-sequence neural networks"}, {"heading": "1. Introduction", "text": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310]. The use of a side-conditioned language model [11] is attractive for its simplicity, and apparent performance, and these successes complement other recent work in which neural networks have advanced the state-of-the-art, for example in language modeling [12, 13], language understanding [14], and parsing [15].\nIn these previously studied tasks, the input vocabulary size is large, and the statistics for many words must be sparsely estimated. To alleviate this problem, neural network based approaches use continuous-space representations of words, in which words that occur in similar contexts tend to be close to each other in representational space. Therefore, data that benefits one word in a particular context causes the model to generalize to similar words in similar contexts. The benefits of using neural networks, in particular, both simple recurrent neural networks [16] and long short-term memory (LSTM) neural networks [17\u201319], to deal with sparse statistics are very apparent.\nHowever, to our best knowledge, the top performing methods for the grapheme-to-phoneme (G2P) task have been based on the use of Kneser-Ney n-gram models [20]. Because of the relatively small cardinality of letters and phones, n-gram statistics, even with long context windows, can be reliably trained. On G2P tasks, maximum entropy models [21] also perform well. The G2P task is distinguished in another important way:\nwhereas the machine translation and image captioning tasks are scored with the relatively forgiving BLEU metric, in the G2P task, a phonetic sequence must be exactly correct in order to get credit when scored.\nIn this paper, we study the open question of whether side-conditioned generation approaches are competitive on the grapheme-to-phoneme task. We find that LSTM approach proposed by [5] performs well and is very close to the state-ofthe-art. While the side-conditioned LSTM approach does not require any alignment information, the state-of-the-art \u201cgraphone\u201d method of [20] is based on the use of alignments. We find that when we allow the neural network approaches to also use alignment information, we significantly advance the stateof-the-art.\nThe remainder of the paper is structured as follows. We review previous methods in Sec. 2. We then present sideconditioned generation models in Sec. 3, and models that leverage alignment information in Sec. 4. We present experimental results in Sec. 5 and provide a further comparison with past work in Sec. 6. We conclude in Sec. 7."}, {"heading": "2. Background", "text": "This section summarizes the state-of-the-art solution for G2P conversion. The G2P conversion can be viewed as translating an input sequence of graphemes (letters) to an output sequence of phonemes. Often, the grapheme and phoneme sequences have been aligned to form joint grapheme-phoneme units. In these alignments, a grapheme may correspond to a null phoneme with no pronunciation, a single phoneme, or a compound phoneme. The compound phoneme is a concatenation of two phonemes. An example is given in Table 1.\nGiven a grapheme sequence L = l1, \u00b7 \u00b7 \u00b7 , lT , a corresponding phoneme sequence P = p1, \u00b7 \u00b7 \u00b7 , pT , and an alignment A, the posterior probability p(P |L,A) is approximated as:\np(P |A,L) \u2248 T\u220f\nt=1\np(pt|pt\u22121t\u2212k, l t+k t\u2212k) (1)\nwhere k is the size of a context window, and t indexes the positions in the alignment.\nar X\niv :1\n50 6.\n00 19\n6v 3\n[ cs\n.C L\n] 2\n0 A\nug 2\n01 5\nFollowing [21,22], Eq. (1) can be estimated using an exponential (or maximum entropy) model in the form of\np(pt|x = (pt\u22121t\u2212k, l t+k t\u2212k)) =\nexp( \u2211\ni \u03bbifi(x, pt))\u2211 q exp( \u2211 i \u03bbifi(x, q))\n(2)\nwhere features fi(\u00b7) are usually 0 or 1 indicating the identities of phones and letters in specific contexts.\nJoint modeling has been proposed for grapheme-tophoneme conversion [20, 21, 23]. In these models, one has a vocabulary of grapheme and phoneme pairs, which are called graphones. The probability of a graphone sequence is\np(C = c1 \u00b7 \u00b7 \u00b7 cT ) = T\u220f\nt=1\np(ct|c1 \u00b7 \u00b7 \u00b7 ct\u22121), (3)\nwhere each c is a graphone unit. The conditional probability p(ct|c1 \u00b7 \u00b7 \u00b7 ct\u22121) is estimated using an n-gram language model.\nTo date, these models have produced the best performance on common benchmark datasets, and are used for comparison with the architectures in the following sections."}, {"heading": "3. Side-conditioned Generation Models", "text": "In this section, we explore the use of side-conditioned language models for generation. This approach is appealing for its simplicity, and especially because no explicit alignment information is needed."}, {"heading": "3.1. Encoder-decoder LSTM", "text": "In the context of general sequence to sequence learning, the concept of encoder and decoder networks has recently been proposed [3, 5, 19, 24, 25]. The main idea is mapping the entire input sequence to a vector, and then using a recurrent neural network (RNN) to generate the output sequence conditioned on the encoding vector. Our implementation follows the method in [5], which we denote as encoder-decoder LSTM. Figure 1 depicts a model of this method. As in [5], we use an LSTM [19] as the basic recurrent network unit because it has shown better performance than simple RNNs on language understanding [26] and acoustic modeling [27] tasks.\nIn this method, there are two sets of LSTMs: one is an encoder that reads the source-side input sequence and the other\nis a decoder that functions as a language model and generates the output. The encoder is used to represent the entire input sequence in the last-time hidden layer activities. These activities are used as the initial activities of the decoder network. The decoder is a language model that uses past phoneme sequence \u03c6t\u221211 to predict the next phoneme \u03c6t, with its hidden state initialized as described. It stops predicting after outputting \u3008/os\u3009, the output-side end-of-sentence symbol. Note that in our models, we use \u3008s\u3009 and \u3008/s\u3009 as input-side begin-of-sentence and end-of-sentence tokens, and \u3008os\u3009 and \u3008/os\u3009 for corresponding output symbols.\nTo train these encoder and decoder networks, we used backpropagation through time (BPTT) [28,29], with the error signal originating in the decoder network.\nWe use a beam search decoder to generate phoneme sequence during the decoding phase. The hypothesis sequence with the highest posterior probability is selected as the decoding result."}, {"heading": "4. Alignment Based Models", "text": "In this section, we relax the earlier constraint that the model translates directly from the source-side letters to the target-side phonemes without the benefit of an explicit alignment."}, {"heading": "4.1. Uni-directional LSTM", "text": "A model of the uni-directional LSTM is in Figure 2. Given a pair of source-side input and target-side output sequences and an alignment A, the posterior probability of output sequence given the input sequence is\np(\u03c6T1 |A, lT1 ) = T\u220f\nt=1\np(\u03c6t|\u03c6t\u221211 , l t 1) (4)\nwhere the current phoneme prediction \u03c6t depends both on its past prediction \u03c6t\u22121 and the input letter sequence lt. Because of the recurrence in the LSTM, prediction of the current phoneme depends on the phoneme predictions and letter sequence from the sentence beginning. Decoding uses the same beam search decoder described in Sec. 3."}, {"heading": "4.2. Bi-directional LSTM", "text": "The bi-directional recurrent neural network was proposed in [30]. In this architecture, one RNN processes the input from\nleft-to-right, while another processes it right-to-left. The outputs of the two sub-networks are then combined, for example being fed into a third RNN. The idea has been used for speech recognition [30] and more recently for language understanding [31]. Bi-directional LSTMs have been applied to speech recognition [19] and machine translation [6].\nIn the bi-directional model, the phoneme prediction depends on the whole source-side letter sequence as follows\np(\u03c6T1 |A, lT1 ) = T\u220f\nt=1\np(\u03c6t|\u03c6t\u221211 l T 1 ) (5)\nFigure 3 illustrates this model. Focusing on the third set of inputs, for example, letter lt = A is projected to a hidden layer, together with the past phoneme prediction \u03c6t\u22121 = K. The letter lt = A is also projected to a hidden layer in the network that runs in the backward direction. The hidden layer activation from the forward and backward networks is then used as the input to a final network running in the forward direction. The output of the topmost recurrent layer is used to predict the current phoneme \u03c6t = AE.\nWe found that performance is better when feeding the past phoneme prediction to the bottom LSTM layer, instead of other layers such as the softmax layer. However, this architecture can be further extended, e.g., by feeding the past phoneme predictions to both the top and bottom layers, which we may investigate in future work.\nIn the figure, we draw one layer of bi-directional LSTMs. In Section 5, we also report results for deeper networks, in which the forward and backward layers are duplicated several times; each layer in the stack takes the concatenated outputs of the forward-backward networks below as its input.\nNote that the backward direction LSTM is independent of the past phoneme predictions. Therefore, during decoding, we first pre-compute its activities. We then treat the output from the backward direction LSTM as additional input to the top-layer LSTM that also has input from the lower layer forward direction LSTM. The same beam search decoder described before can then be used."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Datasets", "text": "Our experiments were conducted on the three US English datasets1: the CMUDict, NetTalk, and Pronlex datasets that have been evaluated in [20, 21]. We report phoneme error rate (PER) and word error rate (WER) 2. In the phoneme error rate computation, following [20, 21], in the case of multiple reference pronunciations, the variant with the smallest edit distance is used. Similarly, if there are multiple reference pronunciations for a word, a word error occurs only if the predicted pronunciation doesn\u2019t match any of the references.\nThe CMUDict contains 107877 training words, 5401 validation words, and 12753 words for testing. The Pronlex data contains 83182 words for training, 1000 words for validation, and 4800 words for testing. The NetTalk data contains 14985 words for training and 5002 words for testing, and does not have a validation set."}, {"heading": "5.2. Training details", "text": "For the CMUDict and Pronlex experiments, all meta-parameters were set via experimentation with the validation set. For the NetTalk experiments, we used the same model structures as with the Pronlex experiments.\nTo generate the alignments used for training the alignmentbased methods of Sec. 4, we used the alignment package of [32]. We used BPTT to train the LSTMs. We used sentence level minibatches without truncation. To speed-up training, we used data parallelism with 100 sentences per minibatch, except for the CMUDict data, where one sentence per minibatch gave the best performance on the development data. For the alignmentbased methods, we sorted sentences according to their lengths, and each minibatch had sentences with the same length. For encoder-decoder LSTMs, we didn\u2019t sort sentences in the same lengths as done in the alignment-based methods, and instead, followed [5].\nFor the encoder-decoder LSTM in Sec. 3, we used 500 dimensional projection and hidden layers. When increasing the depth of the encoder-decoder LSTMs, we increased the depth of both encoder and decoder networks. For the bi-directional LSTMs, we used a 50 dimensional projection layer and 300 dimensional hidden layer. For the uni-directional LSTM experiments on CMUDict, we used a 400 dimensional projection layer, 400 dimensional hidden layer, and the above described data parallelism.\nFor both encoder-decoder LSTMs and the alignment-based methods, we randomly permuted the order of the training sentences in each epoch. We found that the encoder-decoder LSTM needed to start from a small learning rate, approximately 0.007 per sample. For bi-directional LSTMs, we used initial learning rates of 0.1 or 0.2. For the uni-directional LSTM, the initial learning rate was 0.05. The learning rate was controlled by monitoring the improvement of cross-entropy scores on validation sets. If there was no improvement of the cross-entropy score, we halved the learning rate. NetTalk dataset doesn\u2019t have a validation set. Therefore, on NetTalk, we first ran 10 iterations with a fixed per-sample learning rate of 0.1, reduced the learning rate by half for 2 more iterations, and finally used 0.01 for 70 iterations.\n1We thank Stanley F. Chen who kindly shared the data set partition he used in [21].\n2We observed a strong correlation of BLEU and WER scores on these tasks. Therefore we didn\u2019t report BLEU scores in this paper.\nThe models of Secs. 3 and 4 require using a beam search decoder. Based on validation results, we report results with beam width of 1.0 in likelihood. We did not observe an improvement with larger beams. Unless otherwise noted, we used a window of 3 letters in the models. We plan to release our training recipes to public through computation network toolkit (CNTK) [33]."}, {"heading": "5.3. Results", "text": "We first report results for all our models on the CMUDict dataset [21]. The first two lines of Table 2 show results for the encoder-decoder models. While the error rates are reasonable, the best previously reported results of 24.53% WER [20] are somewhat better. It is possible that combining multiple systems as in [5] would achieve the same result, we have chosen not to engage in system combination.\nThe effect of using alignment based models is shown at the bottom of Table 2. Here, the bi-directional models produce an unambiguous improvement over the earlier models, and by training a three-layer bi-directional LSTM, we are able to significantly exceed the previous state-of-the-art.\nWe noticed that the uni-directional LSTM with default window size had the highest WER, perhaps because one does not observe the entire input sequence as is the case with both the encoder-decoder and bi-directional LSTMs. To validate this claim, we increased the window size to 6 to include the current and five future letters as its source-side input. Because the average number of letters is 7.5 on CMUDict dataset, the uni-directional model in many cases thus sees the entire letter sequences. With a window size of 6 and additional information from the alignments, the uni-directional model was able to perform better than the encoder-decoder LSTM."}, {"heading": "5.4. Comparison with past results", "text": "We now present additional results for the NetTalk and Pronlex datasets, and compare with the best previous results. The method of [20] uses 9-gram graphone models, and [21] uses 8-gram maximum entropy model.\nChanges in WER of 0.77, 1.30, and 1.27 for CMUDict, NetTalk and Pronlex datasets respectively are significant at the 95% confidence level. For PER, the corresponding values are 0.15, 0.29, and 0.28. On both the CMUDict and NetTalk datasets, the bi-directional LSTM outperforms the previous results at the 95% significance level."}, {"heading": "6. Related Work", "text": "Grapheme-to-phoneme has important applications in text-tospeech and speech recognition. It has been well studied in the past decades. Although many methods have been proposed in the past, the best performance on the standard dataset so far\nwas achieved using a joint sequence model [20] of graphemephoneme joint multi-gram or graphone, and a maximum entropy model [21].\nTo our best knowledge, our methods are the first single neural-network-based system that outperform the previous state-of-the-art methods [20,21] on these common datasets. It is possible to improve performances by combining multiple systems and methods [34, 35], we have chosen not to engage in building hybrid models.\nOur work can be cast in the general sequence to sequence translation category, which includes tasks such as machine translation and speech recognition. Therefore, perhaps the most closely related work is [6]. However, instead of the marginal gains in their bi-direction models, our model obtained significant gains from using bi-direction information. Also, their work doesn\u2019t include experimenting with deeper structures, which we found beneficial. We plan to conduct machine translation tasks to compare our models and theirs."}, {"heading": "7. Conclusion", "text": "In this paper, we have applied both encoder-decoder neural networks and alignment based models to the grapheme-tophoneme task. The encoder-decoder models have the significant advantage of not requiring a separate alignment step. Performance with these models comes close to the best previous alignment-based results. When we go further, and inform a bidirectional neural network models with alignment information, we are able to make significant advances over previous methods."}, {"heading": "8. References", "text": "[1] L. H. Son, A. Allauzen, and F. Yvon, \u201cContinuous space\ntranslation models with neural networks,\u201d in Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies. Association for Computational Linguistics, 2012, pp. 39\u201348.\n[2] M. Auli, M. Galley, C. Quirk, and G. Zweig, \u201cJoint language and translation modeling with recurrent neural networks.,\u201d in EMNLP, 2013, pp. 1044\u20131054.\n[3] N. Kalchbrenner and P. Blunsom, \u201cRecurrent continuous translation nodels,\u201d in EMNLP, 2013.\n[4] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul, \u201cFast and robust neural network joint models for statistical machine translation,\u201d in ACL, 2014.\n[5] H. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in NIPS, 2014.\n[6] M. Sundermeyer, T. Alkhouli, J. Wuebker, and H. Ney, \u201cTranslation modeling with bidirectional recurrent neural networks,\u201d in EMNLP, 2014.\n[7] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dolla\u0301r, J. Gao, X. He, M. Mitchell, J. Platt, L. Zitnick, and G. Zweig, \u201cFrom captions to visual concepts and back,\u201d preprint arXiv:1411.4952, 2014.\n[8] A. Karpathy and F.-F. Li, \u201cDeep visual-semantic alignments for generating image descriptions,\u201d arXiv preprint arXiv:1412.2306, 2014.\n[9] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, \u201cShow and tell: A neural image caption generator,\u201d arXiv preprint arXiv:1411.4555, 2014.\n[10] J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, \u201cLong-term recurrent convolutional networks for visual recognition and description,\u201d arXiv preprint arXiv:1411.4389, 2014.\n[11] T. Mikolov and G. Zweig, \u201cContext dependent recurrent neural network language model,\u201d in SLT, 2012.\n[12] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, \u201cA neural probabilistic language model,\u201d Journal of Machine Learning Research, 2003.\n[13] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernocky, \u201cStrategies for training large scale neural network language models,\u201d in ASRU, 2011.\n[14] K. Yao, G. Zweig, M. Hwang, Y. Shi, and Dong Yu, \u201cRecurrent neural networks for language understanding,\u201d in INTERSPEECH, 2013.\n[15] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton, \u201cGrammar as a foreign language,\u201d in submitted to ICLR, 2015.\n[16] T. Mikolov, \u201cStatistical language models based on neural networks,\u201d in PhD Thesis, Brno University of Technology, 2012.\n[17] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Computation, , no. 9, 1997.\n[18] F. A. Gers, J. Schmidhuber, and F. Cummins, \u201cLearning to forget: Continual prediction with LSTM,\u201d Neural Computation, vol. 12, pp. 2451\u20132471, 1999.\n[19] A. Graves, \u201cGenerating sequences with recurrent neural networks,\u201d Tech. Rep. arxiv.org/pdf/1308.0850v2.pdf, 2013.\n[20] M. Bisani and H. Ney, \u201cJoint-sequence models for grapheme-to-phoneme conversion,\u201d Speech communication, vol. 50, no. 5, pp. 434\u2013451, 2008.\n[21] S. Chen, \u201cConditional and joint models for grapheme-tophoneme conversion,\u201d in EUROSPEECH, 2003.\n[22] A. Berger, S. Della Pietra, and V. Della Pietra, \u201cA maximum entropy approach to natural language processing,\u201d Computational Ling., vol. 22, no. 1, pp. 39\u201371.\n[23] L. Galescu and J. F. Allen, \u201cBi-directional conversion beween graphemes and phonemes using a joint n-gram model,\u201d in ISCA Tutorial and Research Workshop on Speech Synthesis, 2001.\n[24] K. Cho, B. v. Merrienboer, C. Gulcchre, F. Bougares, H. Schwenk, and Y. Bengjo, \u201cLearning phrase representation using RNN encoder-decoder for statistical machine translation,\u201d in arxiv:1406.1072v2, 2014.\n[25] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d in arxiv.org/abs/1409.0473, 2014.\n[26] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, \u201cSpoken language understanding using long short-term memory neural networks,\u201d in SLT, 2014.\n[27] H. Sak, A. Senior, and F. Beaufays, \u201cLong shortterm memory based recurrent neural network architectures for large vocabulary speech recognition,\u201d Tech. Rep. arxiv.org/pdf/1402.1128v1.pdf, 2014.\n[28] M.C. Mozer, Y. Chauvin, and D. Rumelhart, \u201cThe utility driven dynamic error propagation network,\u201d Tech. Rep.\n[29] R. Williams and J. Peng, \u201cAn efficient gradient-based algorithm for online training of recurrent network trajectories,\u201d Neural Computation, vol. 2, pp. 490\u2013501, 1990.\n[30] M. Schuster and K. Paliwal, \u201cBidirectional recurrent neural networks,\u201d IEEE Trans. on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.\n[31] G. Mesnil, X. He, L. Deng, and Y. Bengio, \u201cInvestigation of recurrent-neural-network architectures and learning methods for language understanding,\u201d in INTERSPEECH, 2013.\n[32] S. Jiampojamarn, G. Kondrak, and T. Sherif, \u201cApplying many-to-many alignments and hidden markov models to letter-to-phoneme conversion,\u201d in Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, Rochester, New York, April 2007, pp. 372\u2013379, Association for Computational Linguistics.\n[33] D. Yu, A. Eversole, M. Seltzer, K. Yao, Z. Huang, B. Guenter, O. Kuchaiev, Y. Zhang, F. Seide, H. Wang, J. Droppo, G. Zweig, C. Rossbach, J. Currey, J. Gao, A. May, B. Peng, A. Stolcke, and M. Slaney, \u201cAn introduction to computational networks and the computational network toolkit,\u201d Tech. Rep. MSR, Microsoft Research, 2014, https://cntk.codeplex.com.\n[34] K. Wu and et al, \u201cEncoding linear models as weighted finite-state transducers,\u201d in Interspeech, 2014.\n[35] K. Rao, F. Peng, H. Sak, and F. Beaufays, \u201cGraphemeto-phoneme conversion using long short-term memory recurrent neural networks,\u201d in ICASSP, 2015."}], "references": [{"title": "Continuous space translation models with neural networks", "author": ["L.H. Son", "A. Allauzen", "F. Yvon"], "venue": "Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies. Association for Computational Linguistics, 2012, pp. 39\u201348.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "EMNLP, 2013, pp. 1044\u20131054.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent continuous translation nodels", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "ACL, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["H. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Translation modeling with bidirectional recurrent neural networks", "author": ["M. Sundermeyer", "T. Alkhouli", "J. Wuebker", "H. Ney"], "venue": "EMNLP, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "L. Zitnick", "G. Zweig"], "venue": "preprint arXiv:1411.4952, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F.-F. Li"], "venue": "arXiv preprint arXiv:1412.2306, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. Cernocky"], "venue": "ASRU, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M. Hwang", "Y. Shi", "Dong Yu"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "submitted to ICLR, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "PhD Thesis, Brno University of Technology, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, , no. 9, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, pp. 2451\u20132471, 1999.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "Tech. Rep. arxiv.org/pdf/1308.0850v2.pdf, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint-sequence models for grapheme-to-phoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech communication, vol. 50, no. 5, pp. 434\u2013451, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Conditional and joint models for grapheme-tophoneme conversion", "author": ["S. Chen"], "venue": "EUROSPEECH, 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["A. Berger", "S. Della Pietra", "V. Della Pietra"], "venue": "Computational Ling., vol. 22, no. 1, pp. 39\u201371.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 0}, {"title": "Bi-directional conversion beween graphemes and phonemes using a joint n-gram model", "author": ["L. Galescu", "J.F. Allen"], "venue": "ISCA Tutorial and Research Workshop on Speech Synthesis, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning phrase representation using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. v. Merrienboer", "C. Gulcchre", "F. Bougares", "H. Schwenk", "Y. Bengjo"], "venue": "arxiv:1406.1072v2, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arxiv.org/abs/1409.0473, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "SLT, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Long shortterm memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Tech. Rep. arxiv.org/pdf/1402.1128v1.pdf, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The utility driven dynamic error propagation network", "author": ["M.C. Mozer", "Y. Chauvin", "D. Rumelhart"], "venue": "Tech. Rep.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 0}, {"title": "An efficient gradient-based algorithm for online training of recurrent network trajectories", "author": ["R. Williams", "J. Peng"], "venue": "Neural Computation, vol. 2, pp. 490\u2013501, 1990.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1990}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "IEEE Trans. on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "INTER- SPEECH, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion", "author": ["S. Jiampojamarn", "G. Kondrak", "T. Sherif"], "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, Rochester, New York, April 2007, pp. 372\u2013379, Association for Computational Linguistics.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "H. Wang", "J. Droppo", "G. Zweig", "C. Rossbach", "J. Currey", "J. Gao", "A. May", "B. Peng", "A. Stolcke", "M. Slaney"], "venue": "Tech. Rep. MSR, Microsoft Research, 2014, https://cntk.codeplex.com.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Encoding linear models as weighted finite-state transducers", "author": ["K. Wu"], "venue": "Interspeech, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Graphemeto-phoneme conversion using long short-term memory recurrent neural networks", "author": ["K. Rao", "F. Peng", "H. Sak", "F. Beaufays"], "venue": "ICASSP, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 161, "endOffset": 166}, {"referenceID": 1, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 161, "endOffset": 166}, {"referenceID": 2, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 161, "endOffset": 166}, {"referenceID": 3, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 161, "endOffset": 166}, {"referenceID": 4, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 161, "endOffset": 166}, {"referenceID": 5, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 161, "endOffset": 166}, {"referenceID": 6, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 188, "endOffset": 194}, {"referenceID": 7, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 188, "endOffset": 194}, {"referenceID": 8, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 188, "endOffset": 194}, {"referenceID": 9, "context": "In recent work on sequence to sequence translation, it has been shown that side-conditioned neural networks can be effectively used for both machine translation [1\u20136] and image captioning [7\u201310].", "startOffset": 188, "endOffset": 194}, {"referenceID": 10, "context": "The use of a side-conditioned language model [11] is attractive for its simplicity, and apparent performance, and these successes complement other recent work in which neural networks have advanced the state-of-the-art, for example in language modeling [12, 13], language understanding [14], and parsing [15].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "The use of a side-conditioned language model [11] is attractive for its simplicity, and apparent performance, and these successes complement other recent work in which neural networks have advanced the state-of-the-art, for example in language modeling [12, 13], language understanding [14], and parsing [15].", "startOffset": 253, "endOffset": 261}, {"referenceID": 12, "context": "The use of a side-conditioned language model [11] is attractive for its simplicity, and apparent performance, and these successes complement other recent work in which neural networks have advanced the state-of-the-art, for example in language modeling [12, 13], language understanding [14], and parsing [15].", "startOffset": 253, "endOffset": 261}, {"referenceID": 13, "context": "The use of a side-conditioned language model [11] is attractive for its simplicity, and apparent performance, and these successes complement other recent work in which neural networks have advanced the state-of-the-art, for example in language modeling [12, 13], language understanding [14], and parsing [15].", "startOffset": 286, "endOffset": 290}, {"referenceID": 14, "context": "The use of a side-conditioned language model [11] is attractive for its simplicity, and apparent performance, and these successes complement other recent work in which neural networks have advanced the state-of-the-art, for example in language modeling [12, 13], language understanding [14], and parsing [15].", "startOffset": 304, "endOffset": 308}, {"referenceID": 15, "context": "The benefits of using neural networks, in particular, both simple recurrent neural networks [16] and long short-term memory (LSTM) neural networks [17\u201319], to deal with sparse statistics are very apparent.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "The benefits of using neural networks, in particular, both simple recurrent neural networks [16] and long short-term memory (LSTM) neural networks [17\u201319], to deal with sparse statistics are very apparent.", "startOffset": 147, "endOffset": 154}, {"referenceID": 17, "context": "The benefits of using neural networks, in particular, both simple recurrent neural networks [16] and long short-term memory (LSTM) neural networks [17\u201319], to deal with sparse statistics are very apparent.", "startOffset": 147, "endOffset": 154}, {"referenceID": 18, "context": "The benefits of using neural networks, in particular, both simple recurrent neural networks [16] and long short-term memory (LSTM) neural networks [17\u201319], to deal with sparse statistics are very apparent.", "startOffset": 147, "endOffset": 154}, {"referenceID": 19, "context": "However, to our best knowledge, the top performing methods for the grapheme-to-phoneme (G2P) task have been based on the use of Kneser-Ney n-gram models [20].", "startOffset": 153, "endOffset": 157}, {"referenceID": 20, "context": "On G2P tasks, maximum entropy models [21] also perform well.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "We find that LSTM approach proposed by [5] performs well and is very close to the state-ofthe-art.", "startOffset": 39, "endOffset": 42}, {"referenceID": 19, "context": "While the side-conditioned LSTM approach does not require any alignment information, the state-of-the-art \u201cgraphone\u201d method of [20] is based on the use of alignments.", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Notice that the input sequence for encoder LSTM is time reversed, as in [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 20, "context": "Following [21,22], Eq.", "startOffset": 10, "endOffset": 17}, {"referenceID": 21, "context": "Following [21,22], Eq.", "startOffset": 10, "endOffset": 17}, {"referenceID": 19, "context": "Joint modeling has been proposed for grapheme-tophoneme conversion [20, 21, 23].", "startOffset": 67, "endOffset": 79}, {"referenceID": 20, "context": "Joint modeling has been proposed for grapheme-tophoneme conversion [20, 21, 23].", "startOffset": 67, "endOffset": 79}, {"referenceID": 22, "context": "Joint modeling has been proposed for grapheme-tophoneme conversion [20, 21, 23].", "startOffset": 67, "endOffset": 79}, {"referenceID": 2, "context": "In the context of general sequence to sequence learning, the concept of encoder and decoder networks has recently been proposed [3, 5, 19, 24, 25].", "startOffset": 128, "endOffset": 146}, {"referenceID": 4, "context": "In the context of general sequence to sequence learning, the concept of encoder and decoder networks has recently been proposed [3, 5, 19, 24, 25].", "startOffset": 128, "endOffset": 146}, {"referenceID": 18, "context": "In the context of general sequence to sequence learning, the concept of encoder and decoder networks has recently been proposed [3, 5, 19, 24, 25].", "startOffset": 128, "endOffset": 146}, {"referenceID": 23, "context": "In the context of general sequence to sequence learning, the concept of encoder and decoder networks has recently been proposed [3, 5, 19, 24, 25].", "startOffset": 128, "endOffset": 146}, {"referenceID": 24, "context": "In the context of general sequence to sequence learning, the concept of encoder and decoder networks has recently been proposed [3, 5, 19, 24, 25].", "startOffset": 128, "endOffset": 146}, {"referenceID": 4, "context": "Our implementation follows the method in [5], which we denote as encoder-decoder LSTM.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "As in [5], we use an LSTM [19] as the basic recurrent network unit because it has shown better performance than simple RNNs on language understanding [26] and acoustic modeling [27] tasks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 18, "context": "As in [5], we use an LSTM [19] as the basic recurrent network unit because it has shown better performance than simple RNNs on language understanding [26] and acoustic modeling [27] tasks.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "As in [5], we use an LSTM [19] as the basic recurrent network unit because it has shown better performance than simple RNNs on language understanding [26] and acoustic modeling [27] tasks.", "startOffset": 150, "endOffset": 154}, {"referenceID": 26, "context": "As in [5], we use an LSTM [19] as the basic recurrent network unit because it has shown better performance than simple RNNs on language understanding [26] and acoustic modeling [27] tasks.", "startOffset": 177, "endOffset": 181}, {"referenceID": 27, "context": "To train these encoder and decoder networks, we used backpropagation through time (BPTT) [28,29], with the error signal originating in the decoder network.", "startOffset": 89, "endOffset": 96}, {"referenceID": 28, "context": "To train these encoder and decoder networks, we used backpropagation through time (BPTT) [28,29], with the error signal originating in the decoder network.", "startOffset": 89, "endOffset": 96}, {"referenceID": 29, "context": "The bi-directional recurrent neural network was proposed in [30].", "startOffset": 60, "endOffset": 64}, {"referenceID": 29, "context": "The idea has been used for speech recognition [30] and more recently for language understanding [31].", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "The idea has been used for speech recognition [30] and more recently for language understanding [31].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "Bi-directional LSTMs have been applied to speech recognition [19] and machine translation [6].", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "Bi-directional LSTMs have been applied to speech recognition [19] and machine translation [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 19, "context": "Our experiments were conducted on the three US English datasets: the CMUDict, NetTalk, and Pronlex datasets that have been evaluated in [20, 21].", "startOffset": 136, "endOffset": 144}, {"referenceID": 20, "context": "Our experiments were conducted on the three US English datasets: the CMUDict, NetTalk, and Pronlex datasets that have been evaluated in [20, 21].", "startOffset": 136, "endOffset": 144}, {"referenceID": 19, "context": "In the phoneme error rate computation, following [20, 21], in the case of multiple reference pronunciations, the variant with the smallest edit distance is used.", "startOffset": 49, "endOffset": 57}, {"referenceID": 20, "context": "In the phoneme error rate computation, following [20, 21], in the case of multiple reference pronunciations, the variant with the smallest edit distance is used.", "startOffset": 49, "endOffset": 57}, {"referenceID": 31, "context": "4, we used the alignment package of [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "For encoder-decoder LSTMs, we didn\u2019t sort sentences in the same lengths as done in the alignment-based methods, and instead, followed [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 20, "context": "Chen who kindly shared the data set partition he used in [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "We plan to release our training recipes to public through computation network toolkit (CNTK) [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "We first report results for all our models on the CMUDict dataset [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "53% WER [20] are somewhat better.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "It is possible that combining multiple systems as in [5] would achieve the same result, we have chosen not to engage in system combination.", "startOffset": 53, "endOffset": 56}, {"referenceID": 19, "context": "The method of [20] uses 9-gram graphone models, and [21] uses 8-gram maximum entropy model.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "The method of [20] uses 9-gram graphone models, and [21] uses 8-gram maximum entropy model.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "CMUDict past results [20] 5.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "NetTalk past results [20] 8.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "Pronlex past results [20, 21] 6.", "startOffset": 21, "endOffset": 29}, {"referenceID": 20, "context": "Pronlex past results [20, 21] 6.", "startOffset": 21, "endOffset": 29}, {"referenceID": 19, "context": "was achieved using a joint sequence model [20] of graphemephoneme joint multi-gram or graphone, and a maximum entropy model [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "was achieved using a joint sequence model [20] of graphemephoneme joint multi-gram or graphone, and a maximum entropy model [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "To our best knowledge, our methods are the first single neural-network-based system that outperform the previous state-of-the-art methods [20,21] on these common datasets.", "startOffset": 138, "endOffset": 145}, {"referenceID": 20, "context": "To our best knowledge, our methods are the first single neural-network-based system that outperform the previous state-of-the-art methods [20,21] on these common datasets.", "startOffset": 138, "endOffset": 145}, {"referenceID": 33, "context": "It is possible to improve performances by combining multiple systems and methods [34, 35], we have chosen not to engage in building hybrid models.", "startOffset": 81, "endOffset": 89}, {"referenceID": 34, "context": "It is possible to improve performances by combining multiple systems and methods [34, 35], we have chosen not to engage in building hybrid models.", "startOffset": 81, "endOffset": 89}, {"referenceID": 5, "context": "Therefore, perhaps the most closely related work is [6].", "startOffset": 52, "endOffset": 55}], "year": 2015, "abstractText": "Sequence-to-sequence translation methods based on generation with a side-conditioned language model have recently shown promising results in several tasks. In machine translation, models conditioned on source side words have been used to produce target-language text, and in image captioning, models conditioned images have been used to generate caption text. Past work with this approach has focused on large vocabulary tasks, and measured quality in terms of BLEU. In this paper, we explore the applicability of such models to the qualitatively different grapheme-to-phoneme task. Here, the input and output side vocabularies are small, plain n-gram models do well, and credit is only given when the output is exactly correct. We find that the simple side-conditioned generation approach is able to rival the state-of-the-art, and we are able to significantly advance the stat-of-the-art with bi-directional long short-term memory (LSTM) neural networks that use the same alignment information that is used in conventional approaches.", "creator": "LaTeX with hyperref package"}}}