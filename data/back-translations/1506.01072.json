{"id": "1506.01072", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition", "abstract": "A neuromorphic chip that combines analog CMOS spiking neurons and memristive synapses offers a promising solution for brain-inspired computing because it can provide massive parallelism and density of neural networks. Previous hybrid analog CMOS memristor approaches required extensive CMOS circuits for training, eliminating most of the density benefits achieved by introducing memristor synapses. In addition, they used different waveforms for pre- and post-synaptic spikes that added unwanted circuit overheads. Here, we describe a hardware architecture that can feature a large number of memristor synapses to learn patterns in the real world. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors, and enables competitive learning in a compact circuit module implemented in and in the memristors.", "histories": [["v1", "Tue, 2 Jun 2015 21:35:51 GMT  (1580kb)", "http://arxiv.org/abs/1506.01072v1", "This is a preprint of an article accepted for publication in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no. 2, June 2015, Emerging and Selected Topics in Circuits and Systems, IEEE Journal on, vol. 5, no.2, June 2015"], ["v2", "Mon, 8 Jun 2015 20:32:49 GMT  (1190kb)", "http://arxiv.org/abs/1506.01072v2", "This is a preprint of an article accepted for publication in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no. 2, June 2015"]], "COMMENTS": "This is a preprint of an article accepted for publication in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no. 2, June 2015, Emerging and Selected Topics in Circuits and Systems, IEEE Journal on, vol. 5, no.2, June 2015", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV cs.ET", "authors": ["xinyu wu", "vishal saxena", "kehan zhu"], "accepted": false, "id": "1506.01072"}, "pdf": {"name": "1506.01072.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xinyuwu@u.boisestate.", "vishalsaxena@boisestate.edu;", "kehanzhu@u.boisestate.edu)."], "sections": [{"heading": null, "text": "spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different waveforms for pre and post-synaptic spikes that added undesirable circuit overhead. Here we describe a hardware architecture that can feature a large number of memristor synapses to learn real-world patterns. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors and implements competitive learning in a compact circuit module, and enables in-situ plasticity in the memristor synapses. We demonstrate handwritten-digits recognition using the proposed architecture using transistor-level circuit simulations. As the described neuromorphic architecture is homogeneous, it realizes a fundamental building block for large-scale energy-efficient brain-inspired silicon chips that could lead to next-generation cognitive computing.\nIndex Terms\u2014Neuromorphic, Silicon Neuron, Memristor, Resistive Memory, Spike-Timing Dependent Plasticity, Spiking Neural Network, Machine Learning, Brain-Inspired Computing\nI. INTRODUCTION\nHE HUMAN brain is a very energy-efficient computing machine: tasks like perception, object recognition, speech\nrecognition and language translation are trivial to a human\nbrain; whereas modern machines can do such tasks, but require orders of magnitude more energy, as well as specialized\nprogramming. Massive parallelism is one of the reasons our brains are so effective in the above mentioned decision-making\ntasks. Radically different from today\u2019s predominant von\nNeumann computers (memories and processing elements are separated), a biological brain stores memory and computes\nThis is a preprint of an article accepted for publication in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no. 2, June 2015. Personal use is permitted, but republication/redistribution requires IEEE permission. Copyright \u00a9 2015 IEEE.\nThis work is supported in part by the National Science Foundation under the Grant CCF-1320987. The work of X. Wu and K. Zhu are supported in part by the graduate fellowship of Boise State University.\nThe authors are with the Electrical and Computer Engineering Department, Boise State University, Boise, ID 83725 USA (e-mail: xinyuwu@u.boisestate. edu; vishalsaxena@boisestate.edu; kehanzhu@u.boisestate.edu).\nDigital Object Identifier 10.1109/JETCAS.2015.2433552.\nusing similar motifs. Neurons perform computation by\npropagating spikes and storing memories in the relative strengths of their synapses as well as their interconnectivities.\nBy repeating such a simple structure of neurons and synapses, a biological brain realizes a very energy-efficient computer.\nInspired by such architecture, artificial neural networks\n(ANNs) have been developed and achieved remarkable success in a few specific applications, but historically require hardware\nresource intensive training methods (such as the gradient-based back-propagation algorithms) on conventional computers, and\ntherefore making them inefficient computationally and in\nenergy use. By exploiting parallel graphical processing units (GPUs) or field programmable gate arrays (FPGAs), power\nconsumption of neural networks has been reduced by several\norders of magnitude [1], which yet remains far higher than their biological counterparts.\nIn the past decade, the discovery of spike-timing-dependentplasticity (STDP) [2]\u2013[8] has opened new avenues in neural\nnetwork research. Theoretical studies have suggested STDP\ncan be used to train spiking neural networks (SNNs) in-situ without trading-off their parallelism [9]\u2013[12]. Further,\nnano-scale memristive devices have demonstrated biologically\nplausible STDP behavior in several experiments [13]\u2013[17], and therefore have emerged as an ideal candidate for electrical\nsynapses. To this end, hybrid CMOS-memristor analog very-large-scale integrated (VLSI) circuits have been proposed\n[18]\u2013[22] to achieve dense integration of CMOS neurons and\nmemristors for brain-inspired computing chips by leveraging the contemporary nanometer silicon processing technology.\nResearchers have recently demonstrated pattern recognition applications on spiking neuromorphic systems (with memristor\nsynapses) [23]\u2013[32] using leaky integrate-and-fire neurons\n(IFNs). Most of these systems either require extra training\ncircuitry attached to the synapses (thus eliminating most of the\ndensity advantages gained by using memristors), or different\nwaveforms for pre- and post-synaptic spikes (thus introducing undesirable circuit - overhead which significantly limit power\nand area budget of a large-scale neuromorphic system). There have been a few CMOS IFN designs that attempt to\naccommodate memristor synapses and in-situ synaptic\nplasticity together. An asynchronous IFN architecture was proposed in [33], [34], which provided current summing nodes,\nand propagated same-shape spikes in both the forward and backward directions. Another CMOS IFN with a current\nconveyor was implemented to drive the memristor as excitatory\nT\n2156-3357 \u00a9 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nor inhibitory synapse [35], [36]. However, none of them\nsupports pattern classification directly owing to the lack of a mechanism for making decisions when employed in a neural\nnetwork. Moreover, the consideration of large current drive\ncapability for a massive number of passive memristor synapses was absent in these designs.\nIn this paper, we describe a neuromorphic architecture that\ncan scale to a large number of memristor synapses to learn real-world patterns. To do so, a versatile CMOS spiking IFN\nwas developed. A winner-takes-all (WTA) interface is embedded to empower competitive learning with a shared\nWTA bus topology among local neurons. A dynamic powering\nscheme is used to achieve large current drive capability without compromising the energy-efficiency. By exploiting a\nreconfigurable architecture inspired by [34], the neuron accommodates symmetric forward and backward propagation\nof spikes for online STDP. With a new tri-mode operation, the\nneuron encapsulates all functions with a single OpAmp in a very compact circuit, while allowing one-terminal connectivity\nbetween the neuron and a synapse. Consequently, it enables a\nsimple repeating homogenous structure with a fully asynchronous communication protocol, and thus facilitates\nscaling-up to large-scale neuromorphic chips. Employing an\nindustry-standard circuit simulator, we show online STDP\nlearning in memristors and large current drive capability with\nhigh energy-efficiency of the proposed neuron, and demonstrate a handwritten-digits recognition application using\nthe proposed architecture.\nThe rest of this article is organized as follows: Section II introduces the system architecture and building blocks needed\nto realize a homogeneous neuromorphic system; Section III proposes the CMOS neuron topology and explains how it\nworks as a fundamental information processing unit; Section\nIV presents a pattern recognition application using the proposed system; Section V demonstrates operations of the\nproposed CMOS neuron, STDP learning in memristors and an\n8\u00d78 handwritten-digits recognition; finally, Section VI\ndiscusses the limitations and future challenges."}, {"heading": "II. HOMOGENEOUS NEUROMORPHIC SYSTEM", "text": "Fig. 1B shows a basic neuromorphic unit which comprises several synapses and a neuron block. It mimics a biological\nneuron as shown in Fig. 1A, where the synapse receives spikes from other neurons and converts them into currents according\nto their synaptic strength. The neuron block performs\nspatio-temporal integration of the spikes and generates output spikes (or action potentials) similar to the operation of a neuron\nsoma (Fig. 1C). Further, the dendrites and axons are\nimplemented using interconnect circuits which model the spiking-signal propagation through neuronal fibers and used to\nrealize larger signal processing networks [37]."}, {"heading": "A. Memristor as Synapse", "text": "The memristor was first conceptually conceived in 1971 by\nLeon Chua [30] from a circuit theory perspective. In theory, a memristor is a two-terminal device that can retain an internal analog state by the value of its resistance1, or conductance, that\ndepends upon on the history of the applied voltage and thus the current flowing through the device. Since the conductance of a\nmemristor can be incrementally increased or decreased by\ncontrolling the flux through it, it is a potential candidate for realizing electronic equivalent of biological synapses.\nHowever, memristor based neural networks have only begun to be explored due to the recent emergence of nano-scale\nmemristor devices.\nMemristance has recently been demonstrated in nano-scale\n1 Memristance, resistance, conductance, synaptic weight and synaptic strength are the different descriptions for the same character of a memristor synapse. For convenience, we use conductance, which is proportional to synaptic weight as used in computer science or synaptic strength as used in neuroscience, when we refer to memristor device.\ntwo-terminal devices using various material systems [13]\u2013[19], [38]\u2013[43]. Fig. 2A schematically shows a highly simplified\nmodel of thin-film memristors, in which a memristor is composed of two resistors in series, one is un-doped with high\nresistanceand the other is doped thus having low resistance.\nThe total thickness of the film L is separated into doped and un-doped regions, and the total resistance is the sum of the two\nregions. The average length of the doped region is taken as a state variable d. To increase the depth of the doped region, ions\nare forced into the film with a potential over the threshold Vp across two electrodes; on the contrary, to reduce the depth of\nthe doped region-, ions are removed from the film with an\nopposite potential which exceeds the erasing threshold Vn. This modulation of the doping depth allows the control of the conductance of a memristor. It should be noted that the above\ntwo-resistor model is a simple and convenient way of describing a memristor. In the dielectric region of a physical\nmemristor device, the doping depth is typically represented by\ncomplex metallic filament structures. There exist a multitude of models that aim to correspond to the physics/chemistry behind\nthe conductance change in memristors of various types [18], [19], [44], [45]. In this work, a much more sophisticated device\nmodel pertinent to physical memristors, from [45], was used for\ncircuit simulation.\nSeveral nano-scale memristors in literature have shown that\ntheir conductance modification characteristics are similar to the\nSTDP rule [13]\u2013[17], [46], and therefore act as ideal electrical synapses for brain-inspired computing. STDP states that the\nsynaptic weight w is modulated according to the relative timing of the pre- and post-synaptic neuron firing. As illustrated in Fig.\n2B, a spike pair with the pre-synaptic spike arrives before the\npost-synaptic spike results in increasing the synaptic strength (or potentiation); a pre-synaptic spike after a post-synaptic\nspike results in decreasing the synaptic strength (or depression). Changes of the synaptic weight plotted as a function of the\nrelative arrival timing of the post-synaptic spike with respect to\nthe pre-synaptic spike is called the STDP function or learning window. A popular choice for the STDP function \u0394w is shown\nin Eq. 1, and the corresponding plot is shown in Fig 2C\n\u2206\ud835\udc64 = { \ud835\udc34+\ud835\udc52\n\u2212\u2206\ud835\udc61 /\ud835\udf0f+ \ud835\udc53\ud835\udc5c\ud835\udc5f \u2206\ud835\udc61 > 0 \ud835\udc34\u2212\ud835\udc52 \u2206\ud835\udc61 /\ud835\udf0f\u2212 \ud835\udc53\ud835\udc5c\ud835\udc5f \u2206\ud835\udc61 < 0\n(1)\nA theoretical analysis in [33] illustrated a method to relate \u0394w\nand memristor characteristics, by mapping the over-threshold portion of Vnet (the shaded area of the shaded regions in Fig 2B) to the change in memristance through an ideal memristor\nmodel. However, physical devices have complicated physical and/or electro-chemical mechanisms. Consequently,\nresearchers typically plot a memristor\u2019s conductance \u0394Gmr versus \u0394t either from simulations or experimental results to\nshow the STDP learning function.\nNano-scale memristors have shown low-energy consumption to change their states and very compact layout\nfootprint [18], [19], [47]. Recent advances even reported these\ntwo merits in sub-pJ order [48], and 10-nm range [49] respectively. Thus, it is possible to yield a brain-inspired\nmachine by cohesively packing millions of memristor synapses and thousands of CMOS neurons on a stamp-size silicon chip\nwhile consuming power density which is of the same order as a\nhuman brain (for a nominal 1kHz spiking rate)."}, {"heading": "B. Silicon Neuron", "text": "Since neuromorphic engineering emerged in 1980s [50],\nseveral silicon neuron design styles have appeared in literature. These designs model certain aspects of biological neurons\n[51]\u2013[58]. However, most of them focus on faithfully modeling\nthe ionic channel dynamics in biological spiking neurons, and require the synapses to act as controlled current sources. As a\nresult, they consume large silicon area, and therefore are not amenable for large-scale neuromorphic networks with a\nmassive number of silicon neurons.\nThe emergence of nano-scale memristors has triggered a growing interest in integrating these devices with silicon\nneurons to realize novel neuromorphic systems [23]\u2013[32]. In these systems, researchers have used bio-inspired leaky\nintegrate-and-fire neuron (IFN) models as an alternative to the\ncomplex bio-mimetic neuron models to implement large networks of interconnected spiking neurons. The IFN model is\na single-compartment model, wherein the entire cell is\nabstracted as a single membrane capacitance Cm which sums each current Ii(t) flowing into the neuron through the ith synapse, and a membrane resistance Rm which causes passive leakage of a membrane current Vm(t) / Rm as\n\ud835\udc36\ud835\udc5a \ud835\udc51\ud835\udc49\ud835\udc5a\n\ud835\udc51\ud835\udc61 = \u2211 \ud835\udc3c\ud835\udc56(\ud835\udc61) \u2212\n\ud835\udc49\ud835\udc5a(\ud835\udc61)\n\ud835\udc45\ud835\udc5a \ud835\udc56 . (2)\nThe IFN model captures the transient spiking behavior of the\nneuron with reasonable accuracy for use in learning while\nrequiring a relative low number of transistors for its implementation. Currently, the IFNs used in memristor\nneuromorphic systems need either extra training circuitry attached to memristor synapses (thus eliminating most of the\ndensity advantages gained by using memristor synapses) or\nemploy different waveforms for pre- and post-synaptic spikes (thus introducing undesirable circuit overhead which limits\npower and area budget of a large-scale neuromorphic system).\nThere have been a very few CMOS IFN designs attempting to address above problems in order to accommodate memristor synapses with in-situ synaptic plasticity ability. In [33], the authors proposed a reconfigurable IFN architecture which provided a current summing node to accommodate memristors. In [34], an architecture with a STDP-compatible spike generator was proposed, which enables online STDP by propagating same-shape spikes in both the forward and backward directions. In [35] a CMOS IFN with a current conveyor was designed to drive memristor as either an excitatory or an inhibitory synapse, and [36] shows the measurement results from a ferroelectric memristor. However, none of them can be directly employed to form a learning system because a decision making ability (e.g. competitive learning) was absent in these neurons. They require extra decision circuitry which may need a large silicon area and\ndoesn\u2019t correspond to its biological counterparts. Moreover, these neurons don\u2019t provide an energy-efficient driving capability to interface with a large number of memristor synapses, which is generally desired in mimicking biological neural networks, e.g. a cerebellar Purkinje cell needs to form up to 200,000 synaptic connections [59], or for real-world pattern recognition applications, e.g. MNIST patterns have 784 pixels [60]. For instance, when a neuron drives 1000 memristor synapses, each of them having 1M\u2126 resistance, it requires 1mA current to sustain a 1V spike amplitude resulting in 1mW instantaneous power consumption. Therefore, a highly-scalable driver circuit solution for memristor synapses while avoiding large circuit overhead is truly desired [22].\nA silicon neuron amenable to build large-scale brain-inspired neuromorphic system with massive memristor synapses should:\n(1) Connect to a synapse at one terminal only; (2) Sustain a fixed voltage across the synapse in the absence\nof spikes;\n(3) Provide a current summing node to sense incoming spikes; (4) Provide large current flowing into synapses when firing; (5) Fire a suitable waveform to enable STDP in the synapse; (6) Enable pattern learning through decision-making\nability;\n(7) Be compact and energy-efficient.\nFig. 3A shows the schematic of our proposed CMOS neuron that fulfills all of the above criteria. This circuit effectively combines an OpAmp-based integrator, an STDP-compatible spike generator, a WTA interface and a control circuit for reconfiguration. By employing tri-mode operation, it provides a unique port, Vden, to sum the incoming currents and to propagate post-synaptic spikes, and another port Vaxon to propagate pre-synaptic spikes. These two ports also sustain a fixed voltage Vrefr during integration and membrane capacitor discharge, while driving a specific STDP-compatible waveform with a large current to enable online synaptic\nplasticity in the large number of memristor synapses connected in parallel. Moreover, an inhibitive discharge mode with a shared WTA bus enables competitive learning among local neurons. All of these functions are assembled around a single CMOS OpAmp that is dynamically biased to supply large current only when driving the synapses while maintaining low power consumption during the rest of the time. Further, the neuron functions in a fully asynchronous manner consuming dynamic power only when computation is occurring. The details of the neuron circuit and its operation will be discussed in section III."}, {"heading": "C. Local Competitive Learning", "text": "STDP enables online synaptic weight modification, but it\ndoesn\u2019t automatically lead to network learning behavior.\nConventional ANNs employ a gradient-based back-propagation algorithm to train a network. Although the\nsame technique can be applied to SNNs as well [61], a gradient computation requires very sophisticated hardware and therefore\nis infeasible for a massively parallel system. In neuroscience\nstudies, competitive learning has been observed and used to demonstrate synaptic plasticity directly together with STDP\n[12], [62]\u2013[64], whereas no extra training circuitry is required.\nCompetitive learning is also known as the winner-takes-all\n(WTA) algorithm whereby when a neuron fires, it inhibits its\nneighbors\u2019 from firing to prevent from changing their weight.\nWTA uses a topology where an inhibit signal can be\ncommunicated to every other neuron in the network once it\nfires, at the same time, each neuron \u201clistens\u201d the inhibit signal from other neurons, as shown in Fig. 3B. However, such an\nexplicit inhibition is resource hungry and difficult to scale-up in neuromorphic hardware, especially if the number of competing\nneuron units is large. Instead, an implicit inhibition with a\nbus-like operation is very efficient: several local neurons are connected to one shared bus together, and every neuron can\nmonitor the bus status before its firing. In this scheme, a neuron\nis allowed to present an inhibitive signal only if there is no\nspike event on the shared bus; otherwise, it discharges and\nsuppresses potential firing. The detailed circuit realization of the WTA bus will be discussed in section III.\nIt is worth noting that the proposed global reset mechanism differs from the dynamics of traditional neural networks, in\nwhich, typically, the firing of one neuron in a WTA network\nwill either reduce the membrane potential (and thus spiking probability) of other neurons or prevent firing in a short time\nwindow. The implications to the computational aspects of the\nnetwork dynamics with this global reset scheme can be investigated in further theoretical studies."}, {"heading": "D. Crossbar Networks", "text": "To build our proposed neuromorphic system, CMOS neurons and memristor synapses are organized in a crossbar\nnetwork [65], [66], as shown in Fig. 3C. In this architecture, each input neuron is connected to another output neuron with a\ntwo terminal memristor to form a matrix-like connection for\neach crossbar layer. By cascading and/or stacking crossbars, a large-scale system can be constructed. Semiconductor\ntechnologies now offer vertical integration capability using\nthrough silicon via (TSV) for multiple chips and 3D packages\n[67].\nAs discussed, the proposed neuromorphic system architecture uses only two basic building blocks; a\ntwo-terminal memristor and a versatile CMOS neuron, which\nworks in fully asynchronous manner. As they form a simple one-node contact, a large-scale neuromorphic system for\nbrain-inspired computing can be potentially realized by\nspatially repeating and/or hierarchically stacking the proposed WTA circuit motif of neurons and crossbar synapses."}, {"heading": "III. THE DESIGN OF CMOS NEURON", "text": "A silicon neuron is the most critical component needed to\nrealize a neural network on a chip, while the synapses and\ncrossbar structure are relatively simple in terms of architectural complexity. In our proposed neuron, the tri-mode operation,\nWTA bus, dynamic powering and STDP-compatible spike\ngeneration make up the key roles to realize a cohesive architecture."}, {"heading": "A. Tri-mode Operation", "text": "A spiking silicon neuron for competitive learning should perform three major functions: (1) current summing and\nintegration, (2) firing when membrane potential crosses a\nthreshold and driving resistive loads, and (3) providing an inhibitive discharge. These three functions are performed with\na single OpAmp which is a key advantage of our neuron.\n(1) The integration mode\nAs shown in Fig. 4A, in this mode, switch SW1 connects the \u201cmembrane\u201d capacitor Cmem with the output of the OpAmp, SW2 is open, and SW3 connects post-synapses to a resting voltage Vrest which can be either equal to Vrefr or can be floated. \u03a6d and \u03a6f are asynchronous phase signals to control the switches. As the spike generator is designed to hold a voltage to\nthe refractory potential Vrefr during the non-firing time, the OpAmp\u2019s positive port is set to Vrefr. Under this configuration, the OpAmp realizes a leaky integrator; currents flowing from\nthe pre-synapses are summed at Vden and charge the capacitor Cmem resulting in \u201cmembrane potential\u201d Vmem, with the voltage leak-rate controlled by a triode transistor Mleaky. Vmem moves down as more charge is stored on Cmem, and triggers a reconfiguration event of the neuron upon reaching the firing\nthreshold Vthr.\n(2) The firing mode\nAs shown in Fig. 4B, in this mode, switch SW2 is closed and the switch SW3 bridges the OpAmp output to post-synapses. The OpAmp is now reconfigured as a voltage buffer. The\nSTDP-compatible spike generator creates the required action potential waveform Vspk and relays it to the positive port of the OpAmp. Then, both the pre-synapses and post-synapses are\nshorted to the buffer\u2019s output. The neuron propagates spikes in the backward direction from Vden which is the same port of current summing. The pre-synaptic spikes are driven in the forward direction on Vaxon to the post-synapses. This firing-mode occurs either when the neuron wins the first-to-fire\ncompetition among the local neurons connected to a WTA bus, or during supervised learning. In the former scenario, the\nwinning neuron presents a firing signal on the WTA bus noted\nas Vwtab, and forces other neurons on the same bus into \u201cdischarge mode\u201d. In the latter scenario, Vmode indicates a supervised learning procedure and disables competition among\nthe neurons. Then, with a teaching signal Vtch, the neuron is forced to fire a spike and drives it into pre-synapses, and\nconsequently modulates the synaptic weights under the STDP\nlearning rule. For stable operation, only one Vtch of a neuron is active at a time in order to avoid conflict.\n(3) The inhibitive discharge mode As shown in Fig. 4C, in this mode, switch SW1 is closed, SW2 connects Vrefr to discharge Cmem, and SW3 is disconnected from the OpAmp output to isolate the neuron from the post-synapses."}, {"heading": "B. Dynamic Powering", "text": "The energy-efficiency of the neuron is tied to the above\ndiscussed tri-mode operation. For dynamic powering, a\ntwo-stage OpAmp is designed with the output stage split into a\nmajor branch and a minor branch. The major branch provides large current driving capability; while the minor low-power\nbranch works with the first stage to provide the desired gain. Two complementary signals \u03a6i and \u03a6f are used to bias the OpAmp in low-power configuration by disabling the major\nbranch during integration and discharging modes, while enabling it to drive large currents in the firing mode. In this\nwork, we modified a compact folded-cascode topology [68] with an embedded split class-AB driver to realize a\ndynamically powered OpAmp."}, {"heading": "C. WTA Bus Interface", "text": "Fig. 5A shows a proposed WTA bus interface that can be embedded in the neuron with a compact implementation, and is amenable to scale-up. The bus interface works in an asynchronous manner. A tri-state buffer is employed to isolate the neuron output from the bus during the non-firing state, and a pulled-up bus when a neuron fires. During normal operation, the interface circuit monitors the bus status. A firing event presented as logic high on the bus activates \u03a6d and forces the neuron to switch to the discharge mode. When a potential firing is triggered by either the comparator output Vcpr or the supervised learning signal Vtch, the D-flip-flop (DFF) locks-in the instant bus state and passes it to \u03a6f. The logic low of \u03a6f, implying an existing firing event of another neuron, will consequently suppress neuron from firing; on the contrary, the\nlogic high of \u03a6f gives a green-light to switch the local neuron to the firing mode, and broadcasts an inhibitive signal via the shared bus. When the firing is finished, the DFF state is cleared."}, {"heading": "D. STDP-Compatible Spike Generator", "text": "The shape of the action potential Vspk strongly influences the STDP learning function. A biological-like STDP pulse with exponential rising edges is very difficult to realize in circuits.\nHowever, a bio-inspired STDP pulse can be achieved with a\nsimpler action potential shape: a short narrow positive pulse of\nlarge amplitude followed by a longer slowly decreasing\nnegative tail as plotted in Fig. 5C. This leads to a simple implementation, and yet realizes a STDP learning function\nsimilar to the biological counterpart [20]. The detailed spike\ngenerator circuit, shown in Fig. 5A, employs a voltage selector and RC charging circuit for the positive tail and the negative\ntail, respectively."}, {"heading": "IV. PATTERN RECOGNITION APPLICATION", "text": "As an important application of machine learning, optical character recognition (OCR) is widely used to demonstrate and evaluate pattern recognition performance. An electronic OCR system is designed to convert the images of printed text into computer-readable text to be used for electronic storage, pre-processing for machine learning, text-to-speech, and data mining, etc.\nFig. 6 illustrates a single-layer OCR system with the proposed architecture: the text image is read by an input sensory matrix where each pixel maps to a neuron and is converted into spikes. All spikes from input neurons propagate through a synaptic memristor network to the output neurons. Summing of the input spikes causes a spike from a winning output neuron under WTA competition, which then back-propagates and locally updates weights of the synapses via a STDP learning rule.\nTo effectively train this network, a supervised method is used. The teaching signal Vtch is provided to the assigned output neuron as shown in Fig. 3A. The signal Vtch forces the neuron to\nspike immediately after input pattern is received. Thus, the learning algorithm is tightly embedded in hardware in the proposed implementation.\nIn a trained network, test patterns can be classified without a teaching signal Vtch. Output neurons sum the currents flowing into them and fire according to the WTA competition to indicate the class of an input pattern. Such a pattern recognition system realizes real-time performance thanks to its straightforward event-driven parallel operation.\nThe proposed system is compatible with the spiking neural network model as described in [12], [62], [63]. Unsupervised learning of patterns can also be realized with the same circuit."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Simulation Setup", "text": "The circuits were designed using the Cadence analog design environment and the simulations were carried out with the Spectre circuit simulator.\nWe employed a device model in [45] that has been matched to multiple physical memristors [13], [39]\u2013[42], and resistive random access memory characterization results [69].\nThe silicon neuron was realized with an IBM 180nm standard CMOS process. A two-stage OpAmp was used with folded-cascode topology for the first stage followed by a dynamically biased class-AB output stage. With an equivalent load of 1k\u2126 in parallel with 20pF, the OpAmp has 39 dB DC gain, 3V/\u00b5s slew rate and 5MHz unity-gain frequency in integration mode; and 60dB DC gain, 15MHz unit gain frequency and 15V/\u00b5s slew rate in firing mode. The STDP-compatible pulse generator circuit was designed with digital configurability to allow interfacing with a broad range of memristors. Such tunability may be also useful in the circuit implementation to compensate for the memristor parameter variations. For instance, spike parameters Va+ = 140mV, Va- = 30mV, tail+ = 1\u03bcs and tail- = 3\u03bcs were chosen for a device with Vp = 0.16V and Vn = 0.15V, where Va+ and Va- were small enough to avoid perturbing the memristor, and large enough to create net potentials across the memristor with a potential above the memristor programming thresholds Vp and Vn."}, {"heading": "B. CMOS Neuron Behaviors and STDP in Memristors", "text": "Functionality of the proposed neuron was first simulated in a small neural circuit with two memristor synapses connected between two input neurons (pre-synaptic neurons) and one output neuron (post-synaptic neuron) as shown in Fig. 7A.\nFig. 7B shows the integration and firing operations of the neuron and the STDP learning in the memristors. In this simulation, one of the pre-synaptic neurons was forced to spike regularly with output Vpre1 (solid line), while the other spikes randomly with output Vpre2 (dash line). The post-synaptic neuron summed the currents that were converted from Vpre1 and Vpre2 by the two synapses, and yielded Vmem. Post-synaptic spikes Vpost were generated once Vmem crossed the firing threshold voltage Vthr = 0.3V. The bottom subplot shows potentiation and depression of the memristor synapses when a post-synaptic spike overlapped with the latest pre-synaptic spike, and created a net potential Va+ + Va- = 170mV over the memristors which was exceed their programming thresholds Vp\n= 160mV or Vn = 150mV. Quantitatively, a post/pre-synaptic spike pair with 1\u03bcs arriving time difference \u0394t resulted in a 0.2\u03bcS conductance increase or decrease depending on late or earlier arrival of Vpost relative to Vpre respectively. Fig. 7C summarizes the STDP learning in memristor conductance change \u0394Gmr versus \u00b15\u00b5s range of \u0394t. The asymmetric curve shape with more depression peak value than potentiation was caused by the lower memristor negative threshold Vn than Vp.\nTo evaluate energy-efficiency, the neurons were designed to have the capability to drive up to 10,000 memristor synapses with an assumption that the distribution of resistive states is tightly arranged around 1M\u2126 resistance. This yields a 100\u03a9 equivalent resistive load. Fig. 7D shows the neuron consumed 13\u03bcA baseline current in the integration mode. When firing, the dynamically biased output stage consumed around 56\u03bcA current in the class-AB stage, and drove the remaining current to memristor synapses: a 1.4mA peak current for 10,000 memristor synapses sustained a spike voltage amplitude of 140mV. The current sunk by the synapses follows Ohm\u2019s law due to the nature of the memristor synapse as a resistive-type load. Insufficient current supplied to the memristors will cause a lower spike voltage amplitude that may fail STDP learning. Here, the widely used energy-efficiency figure-of-merit for silicon neuron, pJ/spike/synapse, becomes dependent on the resistance of synapses, and therefore, is not an appropriate descriptor of neuron\u2019s efficiency. Instead, the power efficiency \u03b7 during the maximum driving condition (at equivalent resistive load) should be used, i.e.\n\ud835\udf02 = \ud835\udc3cmr\n\ud835\udc3cmr+\ud835\udc3cIFN . (3)\nHere Imr is the current consumed by a memristor and IIFN is the current consumed by a silicon neuron. Our simulation demonstrated \u03b7 = 97% with 100 \u03a9 for the selected memristor, and a baseline power consumption of 22\u03bcW with a 1.8V power supply voltage. This baseline power consumption doesn\u2019t change with the neuron\u2019s driving capability thanks to the\ntri-mode operation. As a comparison, a neuron without dynamical biasing consumes a 5-fold baseline current; a neuron based on dual-OpAmp architecture may consume a 10-fold static current. It should be noted these power consumption values are for a neuron design that targets a broad range of memristors, without optimizing for a specific device, and therefore have a significant room for improvement in power efficiency when designed for specific memristor characteristics."}, {"heading": "C. Handwritten Digits Recognition", "text": "We employed handwritten digits obtained from the UCI Machine Learning Repository [70] to demonstrate real-world pattern learning and classification with the proposed system. Fig. 8A shows the pattern examples in this dataset. These images include handwritten digits from a total of 43 individuals, 30 included the training set and a separate 13 to the test set. 32\u00d732 bitmaps are divided into non-overlapping blocks of 4\u00d74 and the number of \u2018on\u2019 pixels are counted in each block. This generates an input matrix of 8\u00d78 where each element is an integer in the range of 0 to 15.\nIn our simulations, digits \u201c0\u201d, \u201c1\u201d, \u201c2\u201d and \u201c7\u201d were selected from the training dataset, in which there are 376, 389, 380 and 387 samples of each digit respectively. In the testing dataset, the samples number are 178, 182, 177 and 179, respectively. Samples in the testing dataset are different from the samples in the training dataset. These images were mapped onto an 8\u00d78 sensory neuron matrix consists of 64 IFNs, and pixel values were converted into currents flowing to IFNs, with a threshold of seven or greater for \u201con\u201d values used. This results in the input spike trains are shown in Fig. 8D. Each dot represents a spike and corresponds to an image pixel in binary form.\nDuring the training phase, the training mode Vmode signal was sent to the output neurons. Digit samples were presented to the system in their original sequence in the dataset. Corresponding labels were read into the simulator to activate the teaching signal Vtch to the corresponding output neuron, and forced a\npost-synaptic spike Vpost at 1\u03bcs after each pattern was presented. All samples of the four digits in the training dataset were presented.\nFig. 8B plots conductance changes in the memristor synapses connecting to each of the four output neurons. Before training, all synapses were initialized with Gaussian randomly distributed conductances (\u03bc = 8.5nS, \u03c3 = 4nS). During training, their conductances were gradually increased and separated to different values, due to the STDP learning of the memristors. Because of computing resource restrictions on circuit-level simulations, we have limited the training demonstration to only one epoch here. However, the weights stabilize eventually after several epochs of training based on Matlab simulations as shown later using the IFN model of Eq. (2) instead of a transistor-level circuit.\nFig. 8C is a rearrangement of the conductance into an 8\u00d78 bitmap with each pixel corresponding to an input image. It is remarkable that the synaptic networks abstracted several distinctive features of the digits: The loop of the digit \u201c0\u201d, the vertical line of the \u201c1\u201d, and the bone of \u201c2\u201d and \u201c7\u201d.\nFig. 8D shows a testing simulation with 20 samples from each digit (out of four) and presented to the system for recognition in a class-by-class fashion. With an untrained synaptic network, the four output neurons responded to the inputs with random spiking. After training, each output neuron responds to the input patterns in the same class most of time showing clear selectivity, and only one neuron fired under the local competition rule.\nFig. 9A zooms into the details of currents and membrane voltages during testing. Due to the modulation of the synaptic network (causing different integration speeds), the total current\nflowing into the output neurons were separated; the neuron with the largest current (I0) had its membrane voltage Vmem0 cross the firing threshold Vth first winning the competition to fire first; whereas the current flowing into neuron \u201c7\u201d (I7) was too small to make its Vmem7 reach the firing threshold. The other two neurons had their Vmem reach the firing threshold, but their potential firing events were suppressed by the winner neuron. Membrane voltages of all neurons were reset by the WTA signal on the shared bus (not shown), and the actual circuit behavior introduced a 50ns delay from Vth crossing to Vmem resetting.\nTo illustrate this competitive learning in another way, we define spiking \u2018opportunities\u2019 of the output neurons based on the total currents flowing into them,\n\ud835\udc5d\ud835\udc5b = \u2211 \ud835\udc3c\ud835\udc5b,\ud835\udc56(\ud835\udc61) \ud835\udc56 / \u2211 \u2211 \ud835\udc3c\ud835\udc5b,\ud835\udc56(\ud835\udc61) \ud835\udc56\ud835\udc5b (4)\nwhere pn is the relative spiking opportunity of the nth output neuron and In,i is the current flowing into the nth output neuron by the ith input. With the same synaptic weights and the all In,i equal, it follows that pn = 1/n, which means the same chance to fire and no winner (for this reason, the synapses can\u2019t be initialized to all zero values. And such a condition doesn\u2019t exist in a real-world environment too). Once the synaptic weights are well modulated, they create different currents flowing into neurons. With a larger current, a neuron has the higher opportunity to spike in the same timeslot, which distinguishes the winner neuron from the others.\nIn this pattern recognition example, a 96% correction rate was achieved with the selected 4 digits. Matlab simulations with the IFN mathematical model show 83% correction rate with all 10 digits. These results are encouraging especially considering the system is a simple single-layer network, and no input encoding was applied. Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class."}, {"heading": "VI. DISCUSSION", "text": "The described CMOS spiking neuron architecture is generalized for memristor synapses. By selecting appropriate CMOS technology with sufficient supply voltage, online STDP learning can be achieved with the memristors, but not limited\nto, as reported in [39]\u2013[42], [69]. However, the memristor in [13], with its Vp = 1.5V and Vn = 0.5V, would be difficult to fit into this architecture. With these threshold voltages, it is impossible to find a STDP pulse that can produce both potentiation and depression while not disturbing the memristor. In other words, for generalized STDP learning, assuming symmetric the pre- and post-synaptic spikes, a memristor is\nexpected to have its thresholds satisfy the condition: |\ud835\udc49\ud835\udc5d \u2212 \ud835\udc49\ud835\udc5b| < \ud835\udc5a\ud835\udc56\ud835\udc5b (\ud835\udc49\ud835\udc5d , \ud835\udc49\ud835\udc5b).\nIn terms of energy-efficiency, an optimized design is the one with driving capability tailored according to the desired application and the memristor used. In the presented simulations, the neuron was tailored to support up to 1.5mA current in order to sustain Va+ = 140mV to a memristor network which has a peak average resistance around 93\u2126. With MNIST patterns, each output neuron would have 784 input synaptic connections, thus the average resistive loading of these 784 synapses should be evaluated for both training and testing scenarios. The neuron driving capability is selected to sustain the least spike voltage amplitudes on the lowest equivalent resistive load while achieving the highest power efficiency. If the resistance of the memristor in its low resistance state (LRS) is 1k\u2126 and (say) 1% of the memristors are in their LRS, 7,840\u00b5A current is required to maintain a 1V spike voltage. For VGA (480640 pixels) images, this number skyrockets to 32,700\u00b5A. It can be concluded that to implement low-power brain-inspired computing chip, the memristor synapses should have fairly high resistances (more than a M\u2126) in their LRS, or a mechanism to isolate non-active synapses from the network during neurons\u2019 firing without large overheads becomes necessary.\nOn physical device side, a memristor passive crossbar architecture generally suffers from sneak paths (undesired paths parallel to the intended path for current sensing) [18], [66], [73]\u2013[75]. The sneak-paths problem is caused by directly connecting resistive-type cells on sensing grid to the high-impedance terminations of the unselected lines. As stated in section II. B, a fixed voltage across a memristor is required for brain-inspired computing. Therefore, every path without a spike in the crossbar is tied to Vrefr, and so the above discussed large current pouring into memristor networks becomes costly in terms of power consumption. Theoretically, a non-firing neuron could have a floating output thus reducing the current,\nbut consequently sneak paths may bridge spiking neurons to other neurons and cause malfunction. So far, none of the existing solutions for sneak-paths work for memristor synapses, and thus further studies are required.\nDevice variability is another challenge when using nano-scale memristors as synapses. Large variations in time and space of memristor synapses could cause unpredictable dynamics in the network, or simply fail to do learning. Although a spiking neural network offers some tolerance to device variation [76], the memristor threshold variations can easily fail network training especially when a low voltage spike is applied. There is a careful design trade-off between the low-voltage amplitudes of a spike required for energy-efficiency, and the high net potential margin over the memristor\u2019s characteristics required for reliable STDP learning. For instance, a memristor with Vp = 160mV and Vn = 150mV requires the spike voltage must higher than 80mV while a practical value typically in the range of 100 to 140mV to minimize the impact from device variations and spike noise. Some recent works have tried to address device variability by combining binary memristors to form a multi-level memristor cell for stochastic computing [32], [77]. Our proposed architecture works for stochastic computing as well, however, a stochastic firing mechanism is needed for the silicon neuron implementation instead of deterministic firing. Leveraging the stochastic behavior of nano-devices, a solution was proposed in [78] but its hardware realization feasibility still needs evaluation. Finally, it should be noted that the circuit-level simulations with faithful modeling of electrical behavior consumes significant amount of time as well as computing resources. Due to these restrictions, we limited the training demonstration to one epoch in the circuit-level simulations in shown this work. Based on the behavioral Matlab simulation results (see Fig. 10) with the IFN mathematical model of Eq. (2), the network optimally trains for the desired patterns and the weights eventually stabilize. This is expected if the circuit-level simulations were continued for several training intervals. Moreover, in our Matlab simulation, one has the flexibility to randomly initialize the weights. However, in a circuits approach, the memristors are expected to \u2018pre-formed\u2019 using a voltage pulse (or a photo-induced pre-forming step) which sets them in a high-resistance initial state. Therefore, the circuit simulations presented in this paper were initialized with all the\nmemristors in their high-resistance state (low conductance) and then were potentiated to their final weights."}, {"heading": "VII. CONCLUSION", "text": "This paper describes a homogenous spiking neuromorphic system. It combines standard CMOS design of a novel silicon integrate-and-fire neuron with a memristor crossbar which can be realized in contemporary nano-scale semiconductor technology. This system naturally embeds localized online learning and computing by employing STDP learning in the memristor synapses with a winner-takes-all strategy among the local neurons. The CMOS neuron combines its circuit functions in a compact manner based on a single OpAmp, using a tri-mode operation. It also enables one-terminal connectivity between a neuron and a synapse, this fully exploits the synaptic density gain obtained by using memristor crossbar synapses. Supported by its reconfigurable architecture, a dynamic powering scheme allows the neuron to interface with a large number of memristor synapses without compromising energy-efficiency. Circuit simulations verified the functionality of the proposed neuron, and demonstrated an application of real-world pattern recognition with handwriting digits. In conclusion, the described system is homogenous, fully asynchronous, energy-efficient, and compact. Thus, it realizes a fundamental building block for a large-scale brain-inspired computing architecture."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank the anonymous reviewers for their help in improving this work with their comments on the manuscript\nwriting and results presentation. The authors also thank Dr.\nJohn Chiasson for his comments on the manuscript."}], "references": [{"title": "Power analysis of large-scale, real-time neural networks on SpiNNaker", "author": ["E. Stromatias", "F. Galluppi", "C. Patterson", "S. Furber"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "A neuronal learning rule for sub-millisecond temporal coding", "author": ["W. Gerstner", "R. Kempter", "J. van Hemmen", "H. Wagner"], "venue": "Nature, vol. 383, num. 6595, pp. 76-78, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Synaptic modification by correlated activity: Hebb\u2019s postulate revisited", "author": ["G. Bi", "M. Poo"], "venue": "Annual Review of Neuroscience, vol. 24, pp. 139\u2013166, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Rate, timing, and cooperativity jointly determine cortical synaptic plasticity", "author": ["P.J. Sj\u00f6str\u00f6m", "G.G. Turrigiano", "S.B. Nelson"], "venue": "Neuron, vol. 32, no. 6, pp. 1149\u20131164, Dec. 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Dendritic spikes as a mechanism for cooperative long-term potentiation", "author": ["N. Golding", "N. Staff", "N. Spruston"], "venue": "Nature, vol. 418, no. July, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Spike timing-dependent plasticity of neural circuits", "author": ["Y. Dan", "M.-M. Poo"], "venue": "Neuron, vol. 44, no. 1, pp. 23\u201330, Sep. 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Spike timing-dependent plasticity: from synapse to perception", "author": ["Y. Dan", "M. Poo"], "venue": "Physiological Reviews, pp. 1033\u20131048, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised learning of visual features through spike timing dependent plasticity", "author": ["T. Masquelier", "S.J.S. Thorpe"], "venue": "PLOS Computational Biology, vol. 3, no. 2, Feb. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised learning of head pose through spike-timing dependent plasticity", "author": ["U. Weidenbacher", "H. Neumann"], "venue": "Perception in Multimodal Dialogue Systems, vol. 5078 LNCS, Springer Berlin Heidelberg, 2008, pp. 123\u2013131.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised Feature Learning Via Sparse Hierarchical Representations", "author": ["H. Lee"], "venue": "Stanford University", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity", "author": ["B. Nessler", "M. Pfeiffer", "L. Buesing", "W. Maass"], "venue": "PLOS Computational Biology, vol. 9, no. 4, Apr. 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Nanoscale memristor device as synapse in neuromorphic systems", "author": ["S.H. Jo", "T. Chang", "I. Ebong", "B.B. Bhadviya", "P. Mazumder", "W. Lu"], "venue": "Nano Letters, vol. 10, no. 4, pp. 1297\u2013301, Apr. 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Nanoelectronic programmable synapses based on phase change materials for brain-inspired computing", "author": ["D. Kuzum", "R. Jeyasingh", "B. Lee", "H. Wong"], "venue": "Nano Letters, pp. 2179\u20132186, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "An electronic synapse device based on metal oxide resistive switching memory for neuromorphic computation", "author": ["S. Yu", "Y. Wu", "R. Jeyasingh"], "venue": "IEEE Transations on Electron Devices, vol. 58, no. 8, pp. 2729\u20132737, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Analog memory and spike-timing-dependent plasticity characteristics of a nanoscale titanium oxide bilayer resistive switching device", "author": ["K. Seo", "I. Kim", "S. Jung", "M. Jo", "S. Park"], "venue": "Nanotechnology, vol. 22, no. 25, p. 254023, Jun. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Ultrafast synaptic events in a chalcogenide memristor", "author": ["Y. Li", "Y. Zhong", "L. Xu", "J. Zhang", "X. Xu", "H. Sun", "X. Miao"], "venue": "Scientific Reports, vol. 3, p. 1619, Jan. 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Memristive devices for computing", "author": ["J.J. Yang", "D.B. Strukov", "D.R. Stewart"], "venue": "Nature Nanotechnology, vol. 8, no. 1, pp. 13\u201324, Jan. 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Building Neuromorphic Circuits with Memristive Devices", "author": ["T. Chang", "Y. Yang", "W. Lu"], "venue": "IEEE Circuits and Systems Magazine, vol. 13, no. 2, pp. 56\u201373, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A proposal for hybrid memristor-CMOS spiking neuromorphic learning systems", "author": ["T. Serrano-Gotarredona"], "venue": "IEEE Circuits and Systems Magazine, vol. 13, no. 2, pp. 74\u201388, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Integration of nanoscale memristor synapses in neuromorphic computing architectures", "author": ["G. Indiveri", "B. Linares-Barranco", "R. Legenstein", "G. Deligeorgis", "T. Prodromakis"], "venue": "Nanotechnology, vol. 24, no. 38, p. 384010, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Plasticity in memristive devices for spiking neural networks", "author": ["S. Sa\u00efghi", "C.G. Mayr", "T. Serrano-Gotarredona", "H. Schmidt", "G. Lecerf", "J. Tomas", "J. Grollier", "S. Boyn", "A.F. Vincent", "D. Querlioz", "S. La Barbera", "F. Alibart", "D. Vuillaume", "O. Bichler", "C. Gamrat", "B. Linares-Barranco"], "venue": "Frontiers in Neuroscience, vol. 9, no. March, pp. 1\u201316, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Bioinspired networks with nanoscale memristive devices that combine the unsupervised and supervised learning approaches", "author": ["D. Querlioz", "W. Zhao", "P. Dollfus"], "venue": "International Symposium on Nanoscale Architectures (NANOARCH), 2012, pp. 203\u2013210.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern classification by memristive crossbar circuits using ex situ and in situ training", "author": ["F. Alibart", "E. Zamanidoost", "D.B. Strukov"], "venue": "Nature Communications, vol. 4, no. May, p. 2072, Jan. 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Neuromorphic Hardware System for Visual Pattern Recognition with Memristor Array and CMOS Neuron", "author": ["M. Chu", "B. Kim", "S. Park", "H. Hwang", "M.-G. Jeon", "B.H. Lee", "B.-G. Lee"], "venue": "IEEE Transactions on Industrial Electronics, vol. 62, no. 4, pp. 2410 - 2419, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Orientation Classification by a Winner-Take-All Network with Oxide RRAM based Synaptic Devices", "author": ["S. Yu"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2014, pp. 1058\u20131061.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "On-chip supervised learning rule for ultra high density neural crossbar using memristor for synapse and neuron", "author": ["D. Chabi", "Z. Wang", "W. Zhao", "J.-O. Klein"], "venue": "International Symposium on Nanoscale Architectures (NANOARCH), 2014, pp. 7\u201312.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuromorphic character recognition system with two PCMO-Memristors as a synapse", "author": ["A. Sheri", "H. Hwang", "M. Jeon", "B. Lee"], "venue": "IEEE Transactions on Industrial Electronics, vol. 61, no. 6, pp. 2933\u20132941, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern Recognition with Memristor Networks", "author": ["P. Sheridan", "W. Ma", "W. Lu"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2014, pp. 1078\u20131081.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A CMOS-memristive self-learning neural network for pattern classification applications", "author": ["M. Payvand", "J. Rofeh", "A. Sodhi", "L. Theogarajan"], "venue": "International Symposium on Nanoscale Architectures (NANOARCH), 2014, no. 1, pp. 92\u201397.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Hardware implementation of associative memory characteristics with analogue-type resistive-switching device", "author": ["K. Moon", "S. Park", "J. Jang", "D. Lee", "J. Woo"], "venue": "Nanotechnology, vol. 25, no. 49, p. 495204.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 0}, {"title": "A compound memristive synapse model for statistical learning through STDP in spiking neural networks", "author": ["J. Bill", "R. Legenstein"], "venue": "Frontiers in Neuroscience, vol. 8, no. December, pp. 1\u201318, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "On spike-timing-dependent-plasticity, memristive devices, and building a self-learning visual cortex", "author": ["C. Zamarre\u00f1o-Ramos", "L.A. Camu\u00f1as-Mesa", "J.A. P\u00e9rez-Carrasco", "T. Masquelier", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Frontiers in Neuroscience, vol. 5, no. 26, Jan. 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of adaptive nano/CMOS neural architectures", "author": ["T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "IEEE International Conference on Electronics, Circuits, and Systems (ICECS), 2012, pp. 949\u2013952.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Excitatory and Inhibitory Memristive Synapses for Spiking Neural Networks", "author": ["G. Lecerf", "J. Tomas", "S. Saighi"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2013, pp. 1616\u20131619.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Silicon Neuron dedicated to Memristive Spiking Neural Networks", "author": ["G. Lecerf", "J. Tomas"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2014, pp. 1568\u20131571.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Biophysics of Computation: Information Processing in Single Neurons", "author": ["C. Koch"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Self-organized computation with unreliable, memristive nanodevices", "author": ["G.S. Snider"], "venue": "Nanotechnology, vol. 18, no. 36, p. 365202, 2007.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Cortical computing with memristive nanodevices", "author": ["G. Snider"], "venue": "SciDAC Review, pp. 58\u201365, 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Memristive switching mechanism for metal/oxide/metal nanodevices", "author": ["J. Yang", "M. Pickett", "X. Li"], "venue": "Nature nanotechnology, vol.3, no.7, pp. 429-433, Jul. 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Silver chalcogenide based memristor devices", "author": ["A. Oblea", "A. Timilsina", "D. Moore", "K. Campbell"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2010, vol. 3, pp. 4\u20136.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Anatomy of a Nanoscale Conduction Channel Reveals the Mechanism of a High\u2010Performance Memristor", "author": ["F. Miao", "J. Strachan", "J. Yang"], "venue": "Advanced Materials, vol. 23, no. 47, pp. 5633\u20135640, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive Neuromorphic Architecture (ANA)", "author": ["F.Z. Wang", "L.O. Chua", "X. Yang", "N. Helian", "R. Tetzlaff", "T. Schmidt", "C. Li", "J.M.G. Carrasco", "W. Chen", "D. Chu"], "venue": "Neural Networks, vol. 45, pp. 111\u2013116, Sep. 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Memristor Model Comparison", "author": ["A. Ascoli", "F. Corinto", "V. Senger", "R. Tetzlaff"], "venue": "IEEE Circuits and Systems Magazine, vol. 13, no. 2, pp. 89\u2013105, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized Memristive Device SPICE Model and its Application in Circuit Design", "author": ["C. Yakopcic"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2013, vol. 32, no. 8, pp. 1201\u20131214.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Memristance can explain spike-time-dependent-plasticity in neural synapses", "author": ["B. Linares-barranco", "T. Serrano-gotarredona"], "venue": "Nature Precedings, pp. 1\u20134, 2009.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Design Considerations of Synaptic Device for Neuromorphic Computing", "author": ["S. Yu", "D. Kuzum"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2014, pp. 1062\u20131065.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "A low energy oxide-based electronic synaptic device for neuromorphic visual systems with tolerance to device variation", "author": ["S. Yu", "B. Gao", "Z. Fang", "H. Yu", "J. Kang", "H.-S.S.P. Wong"], "venue": "Advanced Materials, vol. 25, no. 12, pp. 1774\u20131779, Mar. 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "10\u00d710nm2 Hf/HfOx crossbar resistive RAM with excellent performance, reliability and low-energy operatio", "author": ["B. Govoreanu", "G.S. Kar", "Y. Chen", "V. Paraschiv", "S. Kubicek", "A. Fantini", "M. Jurczak"], "venue": "IEEE International Electron Devices Meeting (IEDM), 2011, pp. 31\u201336.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Analog VLSI and Neural Systems", "author": ["C. Mead"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1989}, {"title": "Neuromorphic silicon neuron circuits", "author": ["G. Indiveri", "R. Etienne-Cummings", "J. Schemmel", "G. Cauwenberghs", "J. Arthur", "S. Saighi", "T. Serrano-Gotarredona", "J. Wijekoon", "Y. Wang", "K. Boahen", "B. Linares-Barranco", "T.J. Hamilton", "A. van Schaik", "R. Etienne-Cummings", "T. Delbruck", "S.-C. Liu", "P. Dudek", "P. H\u00e4fliger", "S. Renaud", "J. Schemmel", "G. Cauwenberghs", "J. Arthur", "K. Hynna", "F. Folowosele", "S. Saighi", "T.  13 Serrano-Gotarredona", "J. Wijekoon", "Y. Wang", "K. Boahen"], "venue": "Frontiers in Neuroscience, vol. 5, no. 5, p. 73, Jan. 2011.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "A robust and compact 65 nm LIF analog neuron for computational purposes", "author": ["A. Joubert", "B. Belhadj", "R. Heliot"], "venue": "International New Circuits and systems conference (NEWCAS), 2011, pp. 9\u201312.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "A combinational digital logic approach to STDP", "author": ["A. Cassidy", "A.G. Andreou", "J. Georgiou"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2011, pp. 673\u2013676.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "CMOS and memristor-based neural network design for position detection", "author": ["I. Ebong", "P. Mazumder"], "venue": "Proceedings of the IEEE, pp. 1\u201311, 2012.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "Energy-efficient neuron, synapse and STDP integrated circuits", "author": ["J.M. Cruz-Albrecht", "M.W. Yung", "N. Srinivasa"], "venue": "IEEE Transaction on Biomedical Circuits and Systems, vol. 6, no. 3, pp. 246\u201356, Jun. 2012.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Silicon spiking neurons for hardware implementation of extreme learning machines", "author": ["A. Basu", "S. Shuo", "H. Zhou", "M. Hiot Lim", "G.-B. Huang"], "venue": "Neurocomputing, vol. 102, pp. 125\u2013134, Feb. 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "A generalised conductance-based silicon neuron for large-scale spiking neural networks", "author": ["R. Wang", "T.J. Hamilton", "J. Tapson", "A. Van Schaik", "G. Dp"], "venue": "International Symposium on Circuits and Systems (ISCAS), 2014, pp. 1564\u20131567.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Izhikevich neuron circuit using stochastic logic", "author": ["H. Akima", "S. Sato", "K. Nakajima", "M. Sakuraba"], "venue": "Electronics Letters, vol. 50, no. 24, pp. 1795\u20131797, Nov. 2014.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Cerebellar cortex: its simulation and the relevance of Marr\u2019s theory", "author": ["T. Tyrrell", "D. Willshaw"], "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences, vol. 336, no. 1277, pp. 239\u2013257, 1992.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1992}, {"title": "Enabling back propagation training of memristor crossbar neuromorphic processors", "author": ["R. Hasan", "T.M. Taha"], "venue": "International Joint Conference on Neural Networks (IJCNN), pp. 21\u201328, Jul. 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains", "author": ["T. Masquelier", "R. Guyonneau", "S.J. Thorpe"], "venue": "PLoS ONE, vol. 3, no. 1, p. e1377, Jan. 2008.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "Simulation of a memristor-based spiking neural network immune to device variations", "author": ["D. Querlioz", "O. Bichler", "C. Gamrat"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2011, pp. 1775\u20131781.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Precise-Spike-Driven Synaptic Plasticity: Learning Hetero-Association of Spatiotemporal Spike Patterns", "author": ["Q. Yu", "H. Tang", "K. Tan", "H. Li"], "venue": "PLoS ONE, 2013.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2013}, {"title": "A scalable neural chip with synaptic electronics using CMOS integrated memristors", "author": ["J.M. Cruz-Albrecht", "T. Derosier", "N. Srinivasa"], "venue": "Nanotechnology, vol. 24, no. 38, p. 384011, Sep. 2013.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "Complementary resistive switches for passive nanocrossbar memories", "author": ["E. Linn", "R. Rosezin", "C. K\u00fcgeler", "R. Waser"], "venue": "Nature Materials, vol. 9, no. 5, pp. 403\u2013406, 2010.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2010}, {"title": "Through-Silicon Via (TSV)", "author": ["M. Motoyoshi"], "venue": "Proceedings of the IEEE, vol. 97, pp. 43\u201348, 2009.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2009}, {"title": "Compact power-efficient 3 V CMOS rail-to-rail input/output operational amplifier for VLSI cell libraries", "author": ["R. Hogervorst", "J.P. Tero", "R.G.H. Eschauzier", "J.H. Huijsing"], "venue": "IEEE Journal of Solid-State Circuits, vol. 29, no. I, pp. 1505\u20131513, 1994.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1994}, {"title": "CMOS compatible nanoscale nonvolatile resistance switching memory", "author": ["S. Jo", "W. Lu"], "venue": "Nano Letters, vol. 8, no. 2, pp. 392\u2013397, 2008.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Lichman, \u201cUCI Machine Learning Repository [http://archive.ics.uci.edu/ml].", "author": ["B.M. K"], "venue": "University of California, Irvine, School of Information and Computer Sciences,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}, {"title": "Memristor-based neural logic blocks for non-linearly separable functions", "author": ["M. Soltiz", "S.S.S.S. Member", "D. Kudithipudi", "C. Merkel", "G.S. Rose", "R.E. Pino"], "venue": "IEEE Transaction on Computers, vol. 62, no. 8, pp. 1597\u20131606, 2013.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2013}, {"title": "Memristor Crossbar-Based Neuromorphic Computing System: A Case Study", "author": ["M. Hu", "H. Li", "Y. Chen", "Q. Wu", "G.G.S. Rose", "R.R.W. Linderman"], "venue": "IEEE Transaction on Neural Networks and Learning Systems, vol. 25, no. 99, pp. 1\u20131, Oct. 2014.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2014}, {"title": "Memristor crossbar-based unsupervised image learning", "author": ["L. Chen", "C. Li", "T. Huang", "Y. Chen", "X. Wang"], "venue": "Neural Computing and Applications, Nov. 2013.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "Review of Nanostructured Resistive Switching Memristor and Its Applications", "author": ["S.G. Hu", "S.Y. Wu", "W.W. Jia", "Q. Yu", "L.J. Deng", "Y.Q. Fu", "Y. Liu", "T.P. Chen"], "venue": "Nanoscience and Nanotechnology Letters, vol. 6, no. 9, pp. 729\u2013757, Sep. 2014.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2014}, {"title": "Memristor-based memory: The sneak paths problem and solutions", "author": ["M.A. Zidan", "H.A.H. Fahmy", "M.M. Hussain", "K.N. Salama"], "venue": "Microelectronics Journal, vol. 44, no. 2, pp. 176\u2013183, Feb. 2013.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Immunity to Device Variations in a Spiking Neural Network With Memristive Nanodevices", "author": ["D. Querlioz", "O. Bichler", "P. Dollfus", "C. Gamrat"], "venue": "IEEE Transactions on Nanotechnology, 2013, vol. 12, no. 3, pp. 288\u2013295.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic memristive devices for computing and neuromorphic applications", "author": ["S. Gaba", "P. Sheridan", "J. Zhou", "S. Choi", "W. Lu"], "venue": "Nanoscale, vol. 5, pp. 5872\u20138, 2013.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic neuron design using conductive bridge RAM", "author": ["G. Palma", "M. Suri", "D. Querlioz", "E. Vianello", "B. De Salvo"], "venue": "International Symposium on Nanoscale Architectures (NANOARCH), 2013, pp. 95\u2013100.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "By exploiting parallel graphical processing units (GPUs) or field programmable gate arrays (FPGAs), power consumption of neural networks has been reduced by several orders of magnitude [1], which yet remains far higher than their biological counterparts.", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "In the past decade, the discovery of spike-timing-dependentplasticity (STDP) [2]\u2013[8] has opened new avenues in neural network research.", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "In the past decade, the discovery of spike-timing-dependentplasticity (STDP) [2]\u2013[8] has opened new avenues in neural network research.", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "can be used to train spiking neural networks (SNNs) in-situ without trading-off their parallelism [9]\u2013[12].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "can be used to train spiking neural networks (SNNs) in-situ without trading-off their parallelism [9]\u2013[12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "Further, nano-scale memristive devices have demonstrated biologically plausible STDP behavior in several experiments [13]\u2013[17], and therefore have emerged as an ideal candidate for electrical synapses.", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "Further, nano-scale memristive devices have demonstrated biologically plausible STDP behavior in several experiments [13]\u2013[17], and therefore have emerged as an ideal candidate for electrical synapses.", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "To this end, hybrid CMOS-memristor analog very-large-scale integrated (VLSI) circuits have been proposed [18]\u2013[22] to achieve dense integration of CMOS neurons and", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "To this end, hybrid CMOS-memristor analog very-large-scale integrated (VLSI) circuits have been proposed [18]\u2013[22] to achieve dense integration of CMOS neurons and", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Researchers have recently demonstrated pattern recognition applications on spiking neuromorphic systems (with memristor synapses) [23]\u2013[32] using leaky integrate-and-fire neurons (IFNs).", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "Researchers have recently demonstrated pattern recognition applications on spiking neuromorphic systems (with memristor synapses) [23]\u2013[32] using leaky integrate-and-fire neurons (IFNs).", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "An asynchronous IFN architecture was proposed in [33], [34], which provided current summing nodes, and propagated same-shape spikes in both the forward and backward directions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "An asynchronous IFN architecture was proposed in [33], [34], which provided current summing nodes, and propagated same-shape spikes in both the forward and backward directions.", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "or inhibitory synapse [35], [36].", "startOffset": 22, "endOffset": 26}, {"referenceID": 34, "context": "or inhibitory synapse [35], [36].", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "By exploiting a reconfigurable architecture inspired by [34], the neuron accommodates symmetric forward and backward propagation of spikes for online STDP.", "startOffset": 56, "endOffset": 60}, {"referenceID": 35, "context": "Further, the dendrites and axons are implemented using interconnect circuits which model the spiking-signal propagation through neuronal fibers and used to realize larger signal processing networks [37].", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "The memristor was first conceptually conceived in 1971 by Leon Chua [30] from a circuit theory perspective.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "two-terminal devices using various material systems [13]\u2013[19], [38]\u2013[43].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "two-terminal devices using various material systems [13]\u2013[19], [38]\u2013[43].", "startOffset": 57, "endOffset": 61}, {"referenceID": 36, "context": "two-terminal devices using various material systems [13]\u2013[19], [38]\u2013[43].", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "two-terminal devices using various material systems [13]\u2013[19], [38]\u2013[43].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "There exist a multitude of models that aim to correspond to the physics/chemistry behind the conductance change in memristors of various types [18], [19], [44], [45].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "There exist a multitude of models that aim to correspond to the physics/chemistry behind the conductance change in memristors of various types [18], [19], [44], [45].", "startOffset": 149, "endOffset": 153}, {"referenceID": 42, "context": "There exist a multitude of models that aim to correspond to the physics/chemistry behind the conductance change in memristors of various types [18], [19], [44], [45].", "startOffset": 155, "endOffset": 159}, {"referenceID": 43, "context": "There exist a multitude of models that aim to correspond to the physics/chemistry behind the conductance change in memristors of various types [18], [19], [44], [45].", "startOffset": 161, "endOffset": 165}, {"referenceID": 43, "context": "In this work, a much more sophisticated device model pertinent to physical memristors, from [45], was used for", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "their conductance modification characteristics are similar to the STDP rule [13]\u2013[17], [46], and therefore act as ideal electrical synapses for brain-inspired computing.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "their conductance modification characteristics are similar to the STDP rule [13]\u2013[17], [46], and therefore act as ideal electrical synapses for brain-inspired computing.", "startOffset": 81, "endOffset": 85}, {"referenceID": 44, "context": "their conductance modification characteristics are similar to the STDP rule [13]\u2013[17], [46], and therefore act as ideal electrical synapses for brain-inspired computing.", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "A theoretical analysis in [33] illustrated a method to relate \u0394w and memristor characteristics, by mapping the over-threshold portion of Vnet (the shaded area of the shaded regions in Fig 2B) to the change in memristance through an ideal memristor model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "Nano-scale memristors have shown low-energy consumption to change their states and very compact layout footprint [18], [19], [47].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Nano-scale memristors have shown low-energy consumption to change their states and very compact layout footprint [18], [19], [47].", "startOffset": 119, "endOffset": 123}, {"referenceID": 45, "context": "Nano-scale memristors have shown low-energy consumption to change their states and very compact layout footprint [18], [19], [47].", "startOffset": 125, "endOffset": 129}, {"referenceID": 46, "context": "Recent advances even reported these two merits in sub-pJ order [48], and 10-nm range [49] respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 47, "context": "Recent advances even reported these two merits in sub-pJ order [48], and 10-nm range [49] respectively.", "startOffset": 85, "endOffset": 89}, {"referenceID": 48, "context": "Since neuromorphic engineering emerged in 1980s [50],", "startOffset": 48, "endOffset": 52}, {"referenceID": 49, "context": "These designs model certain aspects of biological neurons [51]\u2013[58].", "startOffset": 58, "endOffset": 62}, {"referenceID": 56, "context": "These designs model certain aspects of biological neurons [51]\u2013[58].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "The emergence of nano-scale memristors has triggered a growing interest in integrating these devices with silicon neurons to realize novel neuromorphic systems [23]\u2013[32].", "startOffset": 160, "endOffset": 164}, {"referenceID": 30, "context": "The emergence of nano-scale memristors has triggered a growing interest in integrating these devices with silicon neurons to realize novel neuromorphic systems [23]\u2013[32].", "startOffset": 165, "endOffset": 169}, {"referenceID": 31, "context": "In [33], the authors proposed a reconfigurable IFN architecture which provided a current summing node to accommodate memristors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "In [34], an architecture with a STDP-compatible spike generator was proposed, which enables online STDP by propagating same-shape spikes in both the forward and backward directions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [35] a CMOS IFN with a current conveyor was designed to drive memristor as either an excitatory or an inhibitory synapse, and [36] shows the measurement results from a ferroelectric memristor.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "In [35] a CMOS IFN with a current conveyor was designed to drive memristor as either an excitatory or an inhibitory synapse, and [36] shows the measurement results from a ferroelectric memristor.", "startOffset": 129, "endOffset": 133}, {"referenceID": 57, "context": "a cerebellar Purkinje cell needs to form up to 200,000 synaptic connections [59], or for real-world pattern recognition applications, e.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "Therefore, a highly-scalable driver circuit solution for memristor synapses while avoiding large circuit overhead is truly desired [22].", "startOffset": 131, "endOffset": 135}, {"referenceID": 58, "context": "Although the same technique can be applied to SNNs as well [61], a gradient computation requires very sophisticated hardware and therefore", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "In neuroscience studies, competitive learning has been observed and used to demonstrate synaptic plasticity directly together with STDP [12], [62]\u2013[64], whereas no extra training circuitry is required.", "startOffset": 136, "endOffset": 140}, {"referenceID": 59, "context": "In neuroscience studies, competitive learning has been observed and used to demonstrate synaptic plasticity directly together with STDP [12], [62]\u2013[64], whereas no extra training circuitry is required.", "startOffset": 142, "endOffset": 146}, {"referenceID": 61, "context": "In neuroscience studies, competitive learning has been observed and used to demonstrate synaptic plasticity directly together with STDP [12], [62]\u2013[64], whereas no extra training circuitry is required.", "startOffset": 147, "endOffset": 151}, {"referenceID": 62, "context": "To build our proposed neuromorphic system, CMOS neurons and memristor synapses are organized in a crossbar network [65], [66], as shown in Fig.", "startOffset": 115, "endOffset": 119}, {"referenceID": 63, "context": "To build our proposed neuromorphic system, CMOS neurons and memristor synapses are organized in a crossbar network [65], [66], as shown in Fig.", "startOffset": 121, "endOffset": 125}, {"referenceID": 64, "context": "Semiconductor technologies now offer vertical integration capability using through silicon via (TSV) for multiple chips and 3D packages [67].", "startOffset": 136, "endOffset": 140}, {"referenceID": 65, "context": "In this work, we modified a compact folded-cascode topology [68] with an embedded split class-AB driver to realize a dynamically powered OpAmp.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "This leads to a simple implementation, and yet realizes a STDP learning function similar to the biological counterpart [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "The proposed system is compatible with the spiking neural network model as described in [12], [62], [63].", "startOffset": 88, "endOffset": 92}, {"referenceID": 59, "context": "The proposed system is compatible with the spiking neural network model as described in [12], [62], [63].", "startOffset": 94, "endOffset": 98}, {"referenceID": 60, "context": "The proposed system is compatible with the spiking neural network model as described in [12], [62], [63].", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "We employed a device model in [45] that has been matched to multiple physical memristors [13], [39]\u2013[42], and resistive random access memory characterization results [69].", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "We employed a device model in [45] that has been matched to multiple physical memristors [13], [39]\u2013[42], and resistive random access memory characterization results [69].", "startOffset": 89, "endOffset": 93}, {"referenceID": 37, "context": "We employed a device model in [45] that has been matched to multiple physical memristors [13], [39]\u2013[42], and resistive random access memory characterization results [69].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "We employed a device model in [45] that has been matched to multiple physical memristors [13], [39]\u2013[42], and resistive random access memory characterization results [69].", "startOffset": 100, "endOffset": 104}, {"referenceID": 66, "context": "We employed a device model in [45] that has been matched to multiple physical memristors [13], [39]\u2013[42], and resistive random access memory characterization results [69].", "startOffset": 166, "endOffset": 170}, {"referenceID": 67, "context": "We employed handwritten digits obtained from the UCI Machine Learning Repository [70] to demonstrate real-world pattern learning and classification with the proposed system.", "startOffset": 81, "endOffset": 85}, {"referenceID": 43, "context": "A memristor model in [45] was employed.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class.", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class.", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class.", "startOffset": 63, "endOffset": 67}, {"referenceID": 68, "context": "Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class.", "startOffset": 69, "endOffset": 73}, {"referenceID": 69, "context": "Applying symbolic patterns that were used in [24], [25], [28], [29], [71], [72], 100% correction rates were achieved simply because each pattern produced a unique synaptic network with their weights having exactly the same shape as the identical pattern of each class.", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "By selecting appropriate CMOS technology with sufficient supply voltage, online STDP learning can be achieved with the memristors, but not limited to, as reported in [39]\u2013[42], [69].", "startOffset": 166, "endOffset": 170}, {"referenceID": 40, "context": "By selecting appropriate CMOS technology with sufficient supply voltage, online STDP learning can be achieved with the memristors, but not limited to, as reported in [39]\u2013[42], [69].", "startOffset": 171, "endOffset": 175}, {"referenceID": 66, "context": "By selecting appropriate CMOS technology with sufficient supply voltage, online STDP learning can be achieved with the memristors, but not limited to, as reported in [39]\u2013[42], [69].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "However, the memristor in [13], with its Vp = 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "On physical device side, a memristor passive crossbar architecture generally suffers from sneak paths (undesired paths parallel to the intended path for current sensing) [18], [66], [73]\u2013[75].", "startOffset": 170, "endOffset": 174}, {"referenceID": 63, "context": "On physical device side, a memristor passive crossbar architecture generally suffers from sneak paths (undesired paths parallel to the intended path for current sensing) [18], [66], [73]\u2013[75].", "startOffset": 176, "endOffset": 180}, {"referenceID": 70, "context": "On physical device side, a memristor passive crossbar architecture generally suffers from sneak paths (undesired paths parallel to the intended path for current sensing) [18], [66], [73]\u2013[75].", "startOffset": 182, "endOffset": 186}, {"referenceID": 72, "context": "On physical device side, a memristor passive crossbar architecture generally suffers from sneak paths (undesired paths parallel to the intended path for current sensing) [18], [66], [73]\u2013[75].", "startOffset": 187, "endOffset": 191}, {"referenceID": 73, "context": "Although a spiking neural network offers some tolerance to device variation [76], the memristor threshold variations can easily fail network training especially when a low voltage spike is applied.", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "Some recent works have tried to address device variability by combining binary memristors to form a multi-level memristor cell for stochastic computing [32], [77].", "startOffset": 152, "endOffset": 156}, {"referenceID": 74, "context": "Some recent works have tried to address device variability by combining binary memristors to form a multi-level memristor cell for stochastic computing [32], [77].", "startOffset": 158, "endOffset": 162}, {"referenceID": 75, "context": "Leveraging the stochastic behavior of nano-devices, a solution was proposed in [78] but its hardware realization feasibility still needs evaluation.", "startOffset": 79, "endOffset": 83}], "year": 2015, "abstractText": "A neuromorphic chip that combines CMOS analog spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different waveforms for pre and post-synaptic spikes that added undesirable circuit overhead. Here we describe a hardware architecture that can feature a large number of memristor synapses to learn real-world patterns. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors and implements competitive learning in a compact circuit module, and enables in-situ plasticity in the memristor synapses. We demonstrate handwritten-digits recognition using the proposed architecture using transistor-level circuit simulations. As the described neuromorphic architecture is homogeneous, it realizes a fundamental building block for large-scale energy-efficient brain-inspired silicon chips that could lead to next-generation cognitive computing.", "creator": "Microsoft\u00ae Word 2013"}}}