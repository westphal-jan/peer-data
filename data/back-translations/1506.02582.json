{"id": "1506.02582", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "On Convergence of Emphatic Temporal-Difference Learning", "abstract": "Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of the divergence of non-political temporal difference learning processes with linear function approximation. In this paper, we present the first convergence evidence for two emphatic algorithms, ETD ($\\ lambda $) and ELSTD ($\\ lambda $). We demonstrate, under general non-political conditions, that convergence iterates in $L ^ 1 $for ELSTD ($\\ lambda $) and the almost certain convergence of the approximation value functions calculated by both algorithms using a single, infinitely long trajectory. Our analysis includes new techniques with applications beyond emphatic algorithms that, for example, provide the first evidence that standard TD ($\\ lambda $) converge sufficiently large $even during training for $lambda $.", "histories": [["v1", "Mon, 8 Jun 2015 16:42:10 GMT  (75kb,D)", "http://arxiv.org/abs/1506.02582v1", "44 pages; a shorter version to appear at the 28th Annual Conference on Learning Theory (COLT), 2015"], ["v2", "Wed, 12 Aug 2015 23:06:08 GMT  (75kb,D)", "http://arxiv.org/abs/1506.02582v2", "45 pages; an oversight in a proof in Appendix C of the first version has been corrected. A shorter article based on the first version appeared at the 28th Annual Conference on Learning Theory (COLT), 2015"]], "COMMENTS": "44 pages; a shorter version to appear at the 28th Annual Conference on Learning Theory (COLT), 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["huizhen yu"], "accepted": false, "id": "1506.02582"}, "pdf": {"name": "1506.02582.pdf", "metadata": {"source": "CRF", "title": "On Convergence of Emphatic Temporal-Difference Learning\u2217", "authors": ["Huizhen Yu"], "emails": ["JANEY.HZYU@GMAIL.COM"], "sections": [{"heading": null, "text": "Keywords: Markov decision processes; approximate policy evaluation; reinforcement learning; temporal difference methods; importance sampling; stochastic approximation; convergence\n\u2217 This research was supported by a grant from Alberta Innovates \u2013 Technology Futures. A shorter version of this paper is to appear at the 28th Annual Conference on Learning Theory (COLT), Paris, France, 2015.\nar X\niv :1\n50 6.\n02 58\n2v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\n5"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Emphatic TD Algorithms: ETD(\u03bb) and ELSTD(\u03bb) 4", "text": "2.1 A Policy Evaluation Problem in Off-Policy Learning . . . . . . . . . . . . . . . . 4 2.2 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Convergence Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"}, {"heading": "3 Properties of Trace Iterates and Convergence Analysis of ELSTD(\u03bb) 8", "text": "3.1 Properties of Trace Iterates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2 Main Results on L1 and Almost Sure Convergence . . . . . . . . . . . . . . . . . 10"}, {"heading": "4 Convergence Analysis of ETD(\u03bb) 11", "text": "4.1 Convergence of Constrained ETD(\u03bb) . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2 Convergence of ETD(\u03bb) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nReferences 14\nAppendices 17"}, {"heading": "A Proof Details for Section 3 17", "text": "A.1 Some Basic Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 Properties of the Trace Iterates {(et, Ft)} . . . . . . . . . . . . . . . . . . . . . . 19 A.3 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.4 Handling Noisy Rewards: Proof of Prop. 3.1 . . . . . . . . . . . . . . . . . . . . . 25 A.5 Proof of Theorem 2.1 on the Convergence of ELSTD(\u03bb) . . . . . . . . . . . . . . 28 A.6 Related Result: Alternative Proof of Existence of an Invariant Probability Measure 32"}, {"heading": "B Proofs for Section 4 35", "text": ""}, {"heading": "C Negative Definiteness of the Matrix C 38", "text": "C.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 C.2 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39"}, {"heading": "1. Introduction", "text": "We consider discounted finite-spaces Markov decision processes (MDPs) and the problem of learning an approximate value function for a given policy from off-policy data, that is, from data due to a different policy. The first policy is called the target policy and the second is called the behavior policy. For example, one may want to learn value functions for many target policies in parallel from one (exploratory) behavior; this requires off-policy learning.\nWe focus on temporal-difference (TD) methods with linear function approximation (Sutton, 1988). Such methods are typically convergent when the target and behavior policies are the same (the on-policy case), but not in the off-policy case (Tsitsiklis and Van Roy, 1997). This difficulty is intrinsic to sampling states according to an arbitrary policy.1 Gradient-based or least squares-based approaches have been used to avoid this difficulty.2\nRecently, Sutton, Mahmood, and White (2015) proposed a new approach to address this issue more directly. They introduced an emphatic TD(\u03bb) algorithm, or ETD(\u03bb) as we call it here. The approach is related to the early work on episodic off-policy TD(\u03bb) (Precup et al., 2001), and is based on the idea of re-weighting the states when forming the eligibility traces in TD(\u03bb), so that the weights reflect the occupation frequencies of the target policy rather than the behavior policy. The result of this weighting scheme is that the \u201cmean updates\u201d associated with ETD(\u03bb) now involve a negative definite matrix, similar to the convergent on-policy TD algorithms. This is a salient feature of the emphatic TD method.\nThe purpose of this paper is to investigate the convergence properties of ETD(\u03bb) and its leastsquares version, ELSTD(\u03bb). Under general conditions, we show that (see Theorems 2.1, 2.2):\n(i) for stepsizes decreasing as t\u2212c, c \u2208 (1/2, 1], the matrix and vector iterates generated by ELSTD(\u03bb) converge in L1 to the desired limits, which define a projected Bellman equation; (ii) for stepsizes decreasing as t\u22121, both algorithms generate approximate value functions that converge almost surely to the desired solution of an associated projected Bellman equation.\nThese results show that the new emphatic TD algorithms are sound for off-policy learning. Regarding proof techniques, we note that although the \u201cmean updates\u201d of ETD(\u03bb) involve a negative definite matrix, it is still difficult to directly apply results from stochastic approximation theory to establish rigorously the association between the \u201cmean updates\u201d and the ETD(\u03bb) iterates, thereby obtaining the desired convergence. The stability criterion of (Borkar and Meyn, 2000) (see also (Borkar, 2008, Chap. 3)) and the \u201cnatural averaging\u201d argument in (Borkar, 2008, Chap. 6) seem suitable, but they require a certain tightness condition that is hard to verify in the general off-policy learning setting where the variances of the trace iterates can grow to infinity with time.3 The analysis of (Tsitsiklis and Van Roy, 1997) has a strong condition (Condition (6), p. 683, in particular), which is difficult to satisfy unless the trace iterates are uniformly bounded. But in general, this would impose a strong restriction on the behavior policy (cf. Yu, 2012, Prop. 3.1, Footnote 3, and the discussion in p. 3320-3322).\nFor regular off-policy LSTD(\u03bb) and TD(\u03bb) (Bertsekas and Yu, 2009), it has been shown by Yu (2012) that the associated joint process of states and trace iterates exhibit useful properties, by which convergence results for LSTD(\u03bb) can be derived. Subsequently, the results can be used to furnish\n1. See the papers (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2015) and the books (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) for related examples and discussion. 2. See e.g., (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014). 3. Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).\nthe conditions of a convergence theorem from stochastic approximation theory (Kushner and Yin, 2003) and yield convergence results for TD(\u03bb). In this paper we will take the proof approach used in (Yu, 2012). We note, however, that most of the intermediate results needed in our case require different and more involved proofs, due to the complexity of the emphatic TD method. Furthermore, we will give a new argument to prove the almost sure convergence of ETD(\u03bb), which applies also to the regular off-policy TD(\u03bb) of (Bertsekas and Yu, 2009) for \u03bb near 1. This improves a result of (Yu, 2012), which only dealt with a constrained version of TD(\u03bb) that restricts the iterates to lie in a bounded set.\nThis paper is organized as follows. In Section 2 we formulate the approximate policy evaluation problem, and we describe the ETD(\u03bb) and ELSTD(\u03bb) algorithms, and the approximate Bellman equations they aim to solve. We also state our main convergence results in this section. In Section 3 we prove our convergence theorem for ELSTD(\u03bb), and prepare results needed for analyzing ETD(\u03bb) with a \u201cmean ODE\u201d4 method. In Section 4 we prove our convergence theorem for ETD(\u03bb). We collect long proofs, technical lemmas and other related results in Appendices A-C."}, {"heading": "2. Emphatic TD Algorithms: ETD(\u03bb) and ELSTD(\u03bb)", "text": ""}, {"heading": "2.1. A Policy Evaluation Problem in Off-Policy Learning", "text": "Let S = {1, . . . , N} be the state space, and letA be a finite set of actions. We assume, without loss of generality, that for every state, all actions are feasible. If we take action a \u2208 A at state s \u2208 S, the system moves from state s to state s\u2032 with probability p(s\u2032 | s, a), and we receive a random reward with mean r(s, a, s\u2032) and bounded variance, according to a probability distribution q(\u00b7 | s, a, s\u2032).\nWe are interested in evaluating the performance of a given stationary policy5 \u03c0, the target policy, without knowledge of the MDP model. The evaluation is to be done by using just observations of state transitions and rewards, while following a stationary policy \u03c0o 6= \u03c0, the behavior policy.\nStarting from time t = 0, applying \u03c0 would generate a sequence of rewards R0, R1, . . .. The performance of \u03c0 will be measured in terms of the expected total rewards attained under \u03c0 up to a random termination time \u03c4 \u2265 1 that depends on the states in a Markovian way. In particular, if at time t \u2265 1, the state is s and termination has not occurred yet, then the probability of \u03c4 = t (terminating at time t) is 1\u2212 \u03b3(s), for a given parameter \u03b3(s) \u2208 [0, 1].\nLet P\u03c0 denote the transition matrix of the Markov chain on S induced by \u03c0. Let \u0393 denote the N \u00d7 N diagonal matrix with diagonal entries \u03b3(s), s \u2208 S. Denote by \u03c0(a | s) and \u03c0o(a | s) the probability of taking action a at state s under the policy \u03c0 and \u03c0o, respectively.\nAssumption 2.1 (Conditions on the target and behavior policies) (i) The target policy \u03c0 is such that (I \u2212 P\u03c0\u0393)\u22121 exists (equivalently, termination occurs with\nprobability 1 under \u03c0, for any initial state). (ii) The behavior policy \u03c0o induces an irreducible Markov chain on S , and moreover, for all\n(s, a) \u2208 S \u00d7A, \u03c0o(a | s) > 0 if \u03c0(a | s) > 0.\nUnder Assumption 2.1(i), we define the value function of the target policy \u03c0 by v\u03c0 : S \u2192 R, v\u03c0(s) = E\u03c0 [\u2211\u03c4\u22121 t=0 Rt \u2223\u2223\u2223S0 = s], where E\u03c0 denotes expectation with respect to the probability 4. ODE stands for ordinary differential equation. 5. A stationary policy is a decision rule that specifies the probability of taking action a at state s for every s \u2208 S.\ndistribution of the process of states, actions and rewards, (St, At, Rt), t \u2265 0, induced by the policy \u03c0. Let r\u03c0 be the expected one-stage reward function under \u03c0; i.e., r\u03c0(s) = E\u03c0 [ R0 | S0 = s ] for s \u2208 S. Then the desired function v\u03c0 can be seen to satisfy uniquely the Bellman equation6\nv\u03c0 = r\u03c0 + P\u03c0\u0393 v\u03c0, i.e., v\u03c0 = (I \u2212 P\u03c0\u0393)\u22121r\u03c0."}, {"heading": "2.2. Algorithms", "text": "We consider computing v\u03c0 with the ETD(\u03bb) algorithm (Sutton et al., 2015) and its least-squares version, ELSTD(\u03bb), using linear function approximation, while following the behavior policy \u03c0o. Let E \u2282 RN be the approximation subspace of dimension n, and let \u03a6 be an N \u00d7 n matrix whose columns form a basis of E. The approximation problem is to find a parameter vector \u03b8 \u2208 Rn such that v = \u03a6\u03b8 \u2208 E approximates v\u03c0 well.\nWe express v = \u03a6\u03b8 as v(s) = \u03c6(s)>\u03b8, s \u2208 S, where the superscript > stands for transpose, and \u03c6(s) \u2208 Rn is the transposed s-th row of \u03a6 and represents the \u201cfeatures\u201d of state s. Like standard TD(\u03bb), if a transition (s, s\u2032) occurs with reward r\u2032, ETD(\u03bb) and ELSTD(\u03bb) use the \u201ctemporal difference\u201d term, r\u2032 + \u03b3(s\u2032)\u03c6(s\u2032)>\u03b8 \u2212 \u03c6(s)>\u03b8, to adjust the parameter \u03b8 for the approximate value function. Also like standard TD(\u03bb), these algorithms aim to solve a projected (single-step or multistep) Bellman equation; but we shall defer the discussion of this until after describing the ETD(\u03bb) algorithm.\nWe focus on a general form of the ETD(\u03bb) algorithm, which uses state-dependent \u03bb values specified by a function \u03bb : S \u2192 [0, 1]. Inputs to the algorithm are the states, actions and rewards, {(St, At, Rt), t \u2265 0}, generated under the behavior policy \u03c0o, where Rt is the random reward received upon the transition from state St to St+1 with action At. The algorithm can access the following functions, in addition to the features \u03c6(s):\n(i) \u03b3 : S \u2192 [0, 1], which specifies the termination probabilities (or equivalently, the statedependent discount factors) that define v\u03c0, as described earlier; (ii) \u03bb : S \u2192 [0, 1], which determines the single or multi-step Bellman equation for the algorithm [cf. the subsequent Eqs. (2.5)-(2.6)]; (iii) \u03c1 : S \u00d7 A \u2192 R+ given by \u03c1(s, a) = \u03c0(a | s)/\u03c0o(a | s) (with 0/0 = 0), which gives the likelihood ratios for action probabilities that can be used to compensate for sampling states and actions according to the behavior policy \u03c0o instead of the target policy \u03c0;\n(iv) i : S \u2192 R+, which gives the algorithm additional flexibility to weigh states according to the degree of \u201cinterest\u201d indicated by i(s).\nThe ETD(\u03bb) algorithm does the following. For each t \u2265 0, let \u03b1t \u2208 (0, 1] be a stepsize parameter, and to simplify notation, let\n\u03c1t = \u03c1(St, At), \u03b3t = \u03b3(St), \u03bbt = \u03bb(St).\nETD(\u03bb) calculates recursively \u03b8t \u2208 Rn, t \u2265 0, according to\n\u03b8t+1 = \u03b8t + \u03b1t et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) >\u03b8t \u2212 \u03c6(St)>\u03b8t ) , (2.1)\n6. One can verify this Bellman equation directly. It also follows from the standard MDP theory (see e.g., Puterman, 1994), as by definition v\u03c0 here can be related to a value function in a discounted MDP where the discount factors depend on state transitions, similar to discounted semi-Markov decision processes.\nwhere et \u2208 Rn (called the \u201celigibility trace\u201d) is calculated together with two nonnegative scalar iterates (Ft,Mt) according to:7\nFt = \u03b3t \u03c1t\u22121 Ft\u22121 + i(St), (2.2) Mt = \u03bbt i(St) + (1\u2212 \u03bbt)Ft, (2.3) et = \u03bbt \u03b3t \u03c1t\u22121 et\u22121 +Mt \u03c6(St). (2.4)\nFor t = 0, (e0, F0, \u03b80) are given as an initial condition of the algorithm. We recognize that the iteration (2.1) has the same form as standard TD, but the trace et is calculated differently, involving an \u201cemphasis\u201d weight Mt on the state St, which itself evolves along with the iterate Ft, called the \u201cfollow-on\u201d trace. If Mt is always set to 1 regardless of Ft and i(\u00b7), then the iteration (2.1) reduces to the standard TD(\u03bb) in the case where \u03b3 and \u03bb are constants.\nTo explain at a high level what ETD(\u03bb) aims to achieve with the weighting scheme (2.2)-(2.4), let us discuss the approximate Bellman equation it aims to solve. Associated with ETD(\u03bb) is a generalized Bellman equation of which v\u03c0 is the unique solution (Sutton, 1995):8\nv = r\u03bb\u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v. (2.5)\nHere P \u03bb\u03c0,\u03b3 is an N \u00d7 N substochastic matrix, and r\u03bb\u03c0,\u03b3 \u2208 RN is a vector of expected total rewards attained by \u03c0 up to some random time depending on the functions \u03b3 and \u03bb, given by\nP \u03bb\u03c0,\u03b3 = I \u2212 (I \u2212 P\u03c0\u0393\u039b)\u22121 (I \u2212 P\u03c0\u0393), r\u03bb\u03c0,\u03b3 = (I \u2212 P\u03c0\u0393\u039b)\u22121 r\u03c0, (2.6)\nwhere \u039b is a diagonal matrix with diagonal entries \u03bb(s), s \u2208 S. ETD(\u03bb) aims to solve a projected version of the Bellman equation (2.5) (see Sutton et al., 2015):\nv = \u03a0 ( r\u03bb\u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v ) , v \u2208 E, \u21d0\u21d2 C\u03b8 + b = 0, \u03b8 \u2208 Rn. (2.7)\nIn the above, \u03a0 is the projection onto E with respect to a weighted Euclidean norm or seminorm. The weights that define this norm also define the diagonal entries of a diagonal matrix M\u0304 , and are given by\ndiag(M\u0304) = d>\u03c0o,i(I \u2212 P \u03bb\u03c0,\u03b3)\u22121, with d\u03c0o,i \u2208 RN , d\u03c0o,i(s) = d\u03c0o(s) \u00b7 i(s), s \u2208 S, (2.8)\nwhere d\u03c0o(s) > 0 denotes the steady state probability of state s for the behavior policy \u03c0o, under Assumption 2.1(ii). For the corresponding linear equation in the \u03b8-space in Eq. (2.7),\nC = \u2212\u03a6>M\u0304 (I \u2212 P \u03bb\u03c0,\u03b3) \u03a6, b = \u03a6>M\u0304 r\u03bb\u03c0,\u03b3 . (2.9)\nImportant for the convergence of ETD(\u03bb) is the negative definiteness of C. It can be shown that under Assumption 2.1, C is negative definite whenever C is nonsingular.9 By comparison, if we set Mt = 1 regardless of Ft and i(\u00b7), the weights that define the projection norm and diag(M\u0304)\n7. For insights about ETD(\u03bb), see (Sutton et al., 2015; Mahmood et al., 2015). Our definition (2.4) of {et} differs slightly from its original definition, but the two are equivalent; ours appears to be more convenient for our analysis. 8. For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al., 2015). 9. The negative definiteness of C is proved for positive i(\u00b7) under Assumption 2.1 by Sutton et al. (2015), and their result extends to nonnegative i(\u00b7), as long as C is nonsingular (see our Prop. C.1 in Appendix C).\nwould simply become d\u03c0o , the same as in the regular off-policy TD(\u03bb). If we set Mt = i(s), then the weights are given by d\u03c0o,i. Neither of these cases guarantees C to be negative definite, unless \u03bb is sufficiently close to 1. Having the desirable negative definiteness property of C is one of the motivations for introducing the weighting scheme (2.2)-(2.4) in ETD(\u03bb) (Sutton et al., 2015).\nFor the convergence analysis in this paper, we shall assume:\nAssumption 2.2 (Nonsingularity condition) The matrix C given in Eq. (2.9) is nonsingular.\nWe remark that for ETD(\u03bb) under Assumption 2.1, C is always negative semidefinite (Sutton et al., 2015) (cf. our Prop. C.1, Appendix C), so the nonsingularity condition above is equivalent to C being negative definite, as noted earlier. This condition is fairly mild and allows i(s) = 0 for some states s. Specifically, as we prove in Appendix C, Assumption 2.2 is equivalent to a condition on the approximation subspace (Prop. C.2), which requires merely that the set of feature vectors of those states with positive emphasis weights contains n linearly independent vectors (cf. Remark C.2). Moreover, this requirement can be fulfilled easily without knowledge of the model (see Cor. C.1, Remark C.2). We also note that when C is negative definite, the projection \u03a0 in Eq. (2.7) is welldefined (with respect to a seminorm if in Eq. (2.8) some diagonal entries of M\u0304 equal zero), the projected Bellman equation (2.7) has a unique solution, and bounds on the approximation error of ETD(\u03bb) can be derived using the approach of Scherrer (2010). (For details of this discussion, see Remark C.1 in Appendix C.)\nThe ELSTD(\u03bb) algorithm aims to solve the same projected Bellman equation (2.7) as ETD(\u03bb). ELSTD(\u03bb) calculates iteratively an n\u00d7 n matrix Ct and a vector bt \u2208 Rn according to\nCt+1 = (1\u2212 \u03b1t)Ct + \u03b1t et \u00b7 \u03c1t ( \u03b3t+1\u03c6(St+1) > \u2212 \u03c6(St)> ) , (2.10)\nbt+1 = (1\u2212 \u03b1t) bt + \u03b1t et \u00b7 \u03c1tRt, (2.11)\nwhere the trace et is calculated according to Eqs. (2.2)-(2.4) as in ETD(\u03bb). ELSTD(\u03bb) sets \u03b8t = \u2212C\u22121t bt, the solution to Ct\u03b8 + bt = 0, when Ct is invertible.\nLike ETD(\u03bb), without the weighting scheme (2.2)-(2.4), ELSTD(\u03bb) would reduce essentially to the regular LSTD(\u03bb) (see e.g., (Boyan, 1999; Yu, 2012) for on-policy and off-policy LSTD(\u03bb))."}, {"heading": "2.3. Convergence Results", "text": "We analyze ETD(\u03bb) and ELSTD(\u03bb) with diminishing stepsizes. Summarized below are their convergence properties, which we will establish in the rest of this paper. In what follows, we denote by \u2016 \u00b7 \u2016 the infinity norm for both vectors and matrices (viewed as vectors). For different stepsize conditions, our results will involve different convergence modes: convergence in L1,10 in probability, or almost sure (a.s.) convergence (we write a.s.\u2192 for \u201cconverges almost surely\u201d). First, we state a general stepsize condition that we will use.\nAssumption 2.3 (Stepsize condition) The stepsize sequence {\u03b1t} is deterministic and eventually nonincreasing, and satisfies \u03b1t \u2208 (0, 1], \u2211 t \u03b1t =\u221e, \u2211 t \u03b1 2 t <\u221e.\nUnder the above condition we may take \u03b1t = t\u2212c, c \u2208 (1/2, 1]. However, stepsizes decreasing as t\u22121 will be required in our almost sure convergence results; some cases will require \u03b1t = O(1/t) with \u03b1t\u2212\u03b1t+1\u03b1t = O(1/t). 11 (For instance, \u03b1t = c1/(c2 + t) for some constants c1, c2 > 0.)\n10. For vector-valued random variables X , Xt, t \u2265 0, by \u201c{Xt} converges to X in L1\u201d we mean E[\u2016Xt \u2212X\u2016] t\u2192\u221e\u2192 0. 11. We write \u03b4t = O(1/t) for a scalar sequence {\u03b4t}, if for some c > 0, 0 \u2264 \u03b4t \u2264 c/t for all t.\nOur results are as follows. Let \u03b8\u2217 denote the desired limit for ETD(\u03bb):\n\u03b8\u2217 = \u2212C\u22121b, for C, b defined by Eq. (2.9) under Assumptions 2.1, 2.2.\nTheorem 2.1 (L1 and almost sure convergence of ELSTD(\u03bb) Iterates) Under Assumptions 2.1, 2.3, for any given initial (e0, F0, C0, b0), the sequence {(Ct, bt)} generated by the ELSTD(\u03bb) algorithm (2.10)-(2.11) converges in L1:\nlim t\u2192\u221e\nE [\u2225\u2225Ct \u2212 C\u2225\u2225] = 0, lim t\u2192\u221e E [\u2225\u2225bt \u2212 b\u2225\u2225] = 0.\nIf in addition the stepsize is given by \u03b1t = 1/(t+ 1), then Ct a.s.\u2192 C, bt a.s.\u2192 b.\nThe preceding theorem yields immediately the convergence of the parameter sequence {\u03b8t} generated by ELSTD(\u03bb):\nCorollary 2.1 (Convergence of ELSTD(\u03bb)) Let Assumptions 2.1-2.3 hold. Let {\u03b8t} be generated by the ELSTD(\u03bb) algorithm (2.10)-(2.11) as \u03b8t = \u2212C\u22121t bt. Then for any given initial (e0, F0, C0, b0), {\u03b8t} converges to \u03b8\u2217 in probability; if in addition \u03b1t = 1/(t+ 1), then \u03b8t a.s.\u2192 \u03b8\u2217.\nTheorem 2.2 (Almost sure convergence of ETD(\u03bb)) Let Assumptions 2.1-2.3 hold. Let {\u03b8t} be generated by the ETD(\u03bb) algorithm (2.1) with stepsizes satisfying \u03b1t = O(1/t) and\n\u03b1t\u2212\u03b1t+1 \u03b1t =\nO(1/t). Then for any given initial (e0, F0, \u03b80), \u03b8t a.s.\u2192 \u03b8\u2217.\nRemark 2.1 (On stepsizes) We believe that the range of stepsizes for the a.s. convergence of ELSTD(\u03bb) can be enlarged. If additional conditions on the behavior policy are imposed to restrict the variances of the trace iterates, it should also be possible to enlarge the range of stepsizes for ETD(\u03bb). These topics, as well as the use of random stepsizes, are under active investigation.\nRemark 2.2 (On variances) The preceding convergence results hold under almost minimal conditions on the behavior policy (Assumption 2.1(ii)). However, unless we restrict sufficiently the behavior policy (which is difficult to do without knowledge of the model, when \u03b3 6< 1), the variances of the trace iterates can grow unboundedly (cf. Remark A.1), significantly affecting the speed of convergence. This is a main difficulty in off-policy methods in general. Further research is required to overcome it. For a recent work in this direction, see (Mahmood et al., 2014)."}, {"heading": "3. Properties of Trace Iterates and Convergence Analysis of ELSTD(\u03bb)", "text": "In this section we analyze the trace iterates and convergence properties of ELSTD(\u03bb) iterates. The analysis not only leads to Theorem 2.1 on the convergence of ELSTD(\u03bb), but also prepares the stage for the subsequent ODE-based convergence proof for ETD(\u03bb), by ensuring that \u201clocal averaging\u201d gives the desired \u201cmean dynamics,\u201d as will be seen in Section 4.\nThe structure of our analysis will be similar to that of (Yu, 2012) for regular off-policy LSTD(\u03bb), but the proofs at intermediate steps are new and more involved. We will explain the key proof arguments in this section, and give the proof details and related results in Appendix A."}, {"heading": "3.1. Properties of Trace Iterates", "text": "Let Zt = (St, At, et, Ft) for t \u2265 0; they form a Markov chain on S \u00d7 A \u00d7 Rn+1. First, we observe several important properties of the trace iterates {(et, Ft)} and the Markov chain {Zt}, under Assumption 2.1:\n(i) For any given initial (e0, F0), supt\u22650 E [\u2225\u2225(et, Ft)\u2225\u2225] <\u221e. (See Prop. A.1.)\n(ii) Let {(et, Ft)} and {(e\u0302t, F\u0302t)} be defined by the same recursion (2.2)-(2.4), using the same state and action random variables, but with different initial conditions (e0, F0) 6= (e\u03020, F\u03020). Then Ft \u2212 F\u0302t a.s.\u2192 0 and et \u2212 e\u0302t a.s.\u2192 0 (the zero vector in Rn). (See Prop. A.2.) (iii) We can approximate the traces (et, Ft), which depend on the entire history of past states and actions, by similarly defined \u201ctruncated traces\u201d (e\u0303t,K , F\u0303t,K) which depend on the most recent 2K states and actions only [cf. Eqs. (A.13)-(A.15)]. The expected approximation \u201cerror\u201d can be bounded uniformly in t, by a constant LK which decreases to 0 as K \u2192 \u221e. (See Prop. A.3.) (iv) {Zt} is a weak Feller Markov chain12 and bounded in probability,13 and hence it has at least one invariant probability measure.14 Furthermore, as we will show in Theorem 3.2 below, {Zt} has a unique invariant probability measure and is ergodic.\nThese properties suggest that despite the growing variances, the trace iterates are well-behaved. Figure 1 shows how the convergence results of this section, to be introduced next, will depend on these properties.\n12. A Markov chain {Xt} on a metric space is weak Feller if E[f(X1) | X0 = x] is continuous in x for every bounded continuous function f on the state space (Meyn and Tweedie, 2009, Prop. 6.1.1(i)). Using this and the fact that (e1, F1) depends continuously on (e0, F0) [cf. Eqs. (2.2)-(2.4)], the weak Feller property of {Zt} can be seen. 13. A Markov chain {Xt} on a topological space is bounded in probability if, for each initial state x and each > 0, there exists a compact subset D of the state space such that lim inft\u2192\u221ePx(Xt \u2208 D) \u2265 1 \u2212 , where Px denotes the probability of events conditional on X0 = x (Meyn and Tweedie, 2009, p. 142). In our case, since S and A are finite, the property (i) above together with the Markov inequality implies that {Zt} is bounded in probability (cf. Yu, 2012, Lemma 3.4). 14. By (Meyn and Tweedie, 2009, Theorem 12.1.2(ii)), a weak Feller Markov chain bounded in probability has at least one invariant probability measure. We mention that there is also an alternative, direct proof of the existence of an invariant probability measure for {Zt}, which does not rely on the weak Feller property (see Appendix A.6)."}, {"heading": "3.2. Main Results on L1 and Almost Sure Convergence", "text": "We formulate our convergence results in terms of a general recursion that can be specialized to the ELSTD(\u03bb) iteration. This generality is needed in order to make the results useful for other proofs, specifically, for proving the uniqueness of the invariant probability measure of {Zt}, and for establishing convergence conditions required by an ODE-based analysis for ETD(\u03bb), as those proofs will rely on the convergence properties of certain iterates that are different from ELSTD(\u03bb).\nWe define the general recursion just mentioned as follows. Denote y = (e, F ); thus y \u2208 Rn+1. Consider a vector-valued function h : Rn+1\u00d7S \u00d7A\u00d7S \u2192 Rm such that h(y, s, a, s\u2032) is Lipschitz continuous in y for each (s, a, s\u2032); i.e., there exists some constant Lh such that for any y, y\u0302 \u2208 Rn+1,\u2225\u2225h(y, s, a, s\u2032)\u2212 h(y\u0302, s, a, s\u2032)\u2225\u2225 \u2264 Lh\u2016y \u2212 y\u0302\u2016, \u2200 (s, a, s\u2032) \u2208 S \u00d7A\u00d7 S. (3.1) Given h, {Zt} and the stepsizes {\u03b1t}, we define a recursion as follows:\nGt+1 = (1\u2212 \u03b1t)Gt + \u03b1t h(Yt, St, At, St+1). (3.2)\nThe ELSTD(\u03bb) iterates Ct and bt correspond to the following choices of h, respectively:\nh1(y, s, a, s \u2032) = e \u00b7 \u03c1(s, a) ( \u03b3(s\u2032)\u03c6(s\u2032)> \u2212 \u03c6(s)> ) , h2(y, s, a, s \u2032) = e \u00b7 \u03c1(s, a) r(s, a, s\u2032). (3.3)\nHere h1 is matrix-valued (we view it as an Rm-valued function with m = n \u00d7 n), and h2 is Rnvalued. As just mentioned, we will also need to consider other choices of h in our proofs later.\nWe first show that {Gt} converges in L1 to some constant vector. The proof (given in Appendix A.3) exploits the property (iii) of truncated traces mentioned earlier: this property allows us to obtain the desired result by working with simple finite-state Markov chains.\nTheorem 3.1 (L1-convergence of {Gt}) Let h be a vector-valued function satisfying the Lipschitz condition (3.1), and let {Gt} be defined by the recursion (3.2), using the process {Zt}. Then under Assumptions 2.1, 2.3, there exists a constant vector G\u2217 (independent of the stepsizes) such that for any given initial Y0 = (e0, F0) and G0, limt\u2192\u221e E\n[\u2225\u2225Gt \u2212G\u2217\u2225\u2225] = 0. Next we analyze the a.s. convergence of {Gt}, by using ergodicity properties of the infinitespace Markov chain {Zt} that we establish first. For each initial condition Z0 = z, define the occupation probability measures \u00b5z,t for t \u2265 1, by \u00b5z,t(B) = 1t \u2211t k=1 1B(Zk) for any Borel subset B of S\u00d7A\u00d7Rn+1, where 1B denotes the indicator function for the setB (i.e., 1B(x) = 1 if x \u2208 B, and 1B(x) = 0 otherwise). Let E\u00b5 denote expectation with respect to the probability distribution of the process {Zt} with \u00b5 as the initial distribution of Z0.\nTheorem 3.2 (Ergodicity of {Zt}) Under Assumption 2.1, the Markov chain {Zt} has a unique invariant probability measure \u03b6, and moreover, the following hold: (i) For each initial condition Z0 = z, the sequence {\u00b5z,t} of occupation measures converges weakly15 to \u03b6, almost surely. (ii) E\u03b6\n[\u2225\u2225h(Z0, S1)\u2225\u2225] <\u221e for any function h satisfying the Lipschitz condition (3.1). 15. For probability measures \u00b5, \u00b5t, t \u2265 0, on a metric space X , {\u00b5t} is said to converge weakly to \u00b5 if for all bounded\ncontinuous functions f on X , \u222b fd\u00b5t \u2192 \u222b fd\u00b5 as t\u2192\u221e (Dudley, 2002, Chap. 9.3).\nThe preceding theorem follows from the properties of trace iterates given earlier and Theorem 3.1 (cf. Figure 1). The proof is the same as the corresponding proofs of (Yu, 2012, Theorem 3.2 and Prop. 3.2) for the case of off-policy LSTD. In particular, to prove the uniqueness of the invariant probability measure (which is not as easy to prove as the existence given in the property (iv) earlier), we use the property (ii) and the convergence in L1 result given in Theorem 3.1.16\nWe can now show that {Gt} converges a.s. for stepsize \u03b1t = 1/(t+ 1), by using the preceding results (cf. Figure 1), together with a strong law of large numbers for stationary processes (Doob, 1953, Chap. X, Theorem 2.1) (see also Meyn and Tweedie, 2009, Theorem 17.1.2). The proof is a verbatim repetition of the proof of (Yu, 2012, Theorem 3.3) and is therefore omitted.\nTheorem 3.3 (Almost sure convergence of {Gt}) Let h and {Gt} be as in Theorem 3.1, and let the stepsize be \u03b1t = 1/(t+ 1). Then, under Assumption 2.1, for any given initial Y0 = (e0, F0) and G0, Gt a.s.\u2192 G\u2217, where G\u2217 = E\u03b6 [ h(Y0, S0, A0, S1) ] is the constant vector in Theorem 3.1.\nFinally, we also need to analyze the cumulative effects of noise in the observed rewards Rt and show that they diminish asymptotically. To this end, consider the following recursion: W0 = 0 and\nWt+1 = (1\u2212 \u03b1t)Wt + \u03b1t et \u03c1t \u00b7 \u03c9t+1, t \u2265 0, (3.4)\nwhere \u03c9t+1 = Rt \u2212 r(St, At, St+1) are noise variables.\nProposition 3.1 (Effects of noise in random rewards) Under Assumptions 2.1, 2.3, for any given initial (e0, F0), we have (i) E [ \u2016Wt\u2016 ] \u2192 0; and (ii) if, in addition, the stepsize is \u03b1t = 1/(t + 1), then Wt a.s.\u2192 0.\nThe proof of the preceding proposition is given in Appendix A.4. The proof of part (i) uses the property (iii) of truncated traces, similarly to the proof of Theorem 3.1, and the proof of part (ii) is similar to that of Theorem 3.3 (cf. Figure 1).\nThe convergence of ELSTD(\u03bb) stated in Theorem 2.1 now follows from the preceding results (cf. Figure 1). Specifically, we calculate the limit G\u2217 in Theorem 3.1 for the two functions h1, h2 in Eq. (3.3), which are associated with the ELSTD(\u03bb) iterates {Ct}, {bt}, respectively, and we show that G\u2217 = C for h = h1 and G\u2217 = b for h = h2. We also write the iterates {bt} equivalently as bt+1 = Gt+1 + Wt+1 with h = h2 in the definition of {Gt}. Then, the L1-convergence part of Theorem 2.1 follows from Theorem 3.1 and Prop. 3.1(i), and the a.s. convergence part of Theorem 2.1 follows from Theorem 3.3 and Prop. 3.1(ii). The complete proof with all the details is given in Appendix A.5."}, {"heading": "4. Convergence Analysis of ETD(\u03bb)", "text": "Recall that ETD(\u03bb) calculates iteratively \u03b8t, t \u2265 0, according to \u03b8t+1 = \u03b8t + \u03b1t et \u00b7 \u03c1t ( Rt + \u03b3t+1\u03c6(St+1) >\u03b8t \u2212 \u03c6(St)>\u03b8t ) . (4.1)\nUsing the results of Section 3, we can now analyze its convergence by applying a \u201cmean ODE\u201d method from stochastic approximation theory (Kushner and Yin, 2003).\n16. Theorem 3.1 is useful here because on the separable metric space S\u00d7A\u00d7Rn+1, bounded Lipschitz continuous functions are convergence-determining for weak convergence of probability measures (Dudley, 2002, Theorem 11.3.3).\nDenoting \u03c9\u0303t+1 = \u03c1t (Rt \u2212 r(St, At, St+1)), let us write the iteration (4.1) equivalently as\n\u03b8t+1 = \u03b8t + \u03b1t h(\u03b8t, \u03bet) + \u03b1t et \u00b7 \u03c9\u0303t+1, (4.2)\nwhere \u03bet = (et, St, At, St+1) and h : Rn \u00d7 Rn \u00d7 S \u00d7A\u00d7 S \u2192 Rn is given by h(\u03b8, \u03be) = e \u00b7 \u03c1(s, a) ( r(s, a, s\u2032) + \u03b3(s\u2032)\u03c6(s\u2032)>\u03b8 \u2212 \u03c6(s)>\u03b8 ) , for \u03be = (e, s, a, s\u2032). (4.3)\nWe will apply (Kushner and Yin, 2003, Theorem 6.1.1) to analyze the convergence of {\u03b8t} generated by (4.1). The \u201cmean ODE\u201d associated with ETD(\u03bb) (4.1) is\nx\u0307 = h\u0304(x), where h\u0304(x) = Cx+ b. (4.4)\nWhen C is negative definite, the above ODE has a unique bounded (constant) solution x(\u00b7) \u2261 \u03b8\u2217 = \u2212C\u22121b on the time interval (\u2212\u221e,+\u221e), and \u03b8\u2217 is globally asymptotically stable for (4.4) in the sense of Liapunov (cf. Kushner and Clark, 1978, p. 23-24). (A Liapunov function in this case is given by \u2016\u03b8 \u2212 \u03b8\u2217\u201622, where \u2016 \u00b7 \u20162 denotes the Euclidean norm.)\nHowever, the a.s. boundedness of {\u03b8t} is not easy to prove directly, which has prevented us from getting the desired convergence \u03b8t\na.s.\u2192 \u03b8\u2217 from (Kushner and Yin, 2003, Theorem 6.1.1) directly. For this reason, we analyze first a constrained version of (4.1) and establish its convergence. The result will then help the convergence analysis of the unconstrained algorithm (4.1) in Section 4.2."}, {"heading": "4.1. Convergence of Constrained ETD(\u03bb)", "text": "Consider the following constrained ETD(\u03bb) algorithm:\n\u03b8t+1 = \u03a0B ( \u03b8t + \u03b1t h(\u03b8t, \u03bet) + \u03b1t et \u00b7 \u03c9\u0303t+1 ) , (4.5)\nwhere B is a closed ball in Rn with a sufficiently large radius r: B = {\u03b8 \u2208 Rn | \u2016\u03b8\u20162 \u2264 r}, and \u03a0B is the Euclidean projection onto B. The \u201cmean ODE\u201d associated with the constrained algorithm (4.5) is the projected ODE\nx\u0307 = h\u0304(x) + z, z \u2208 \u2212NB(x), (4.6)\nwhereNB(x) is the normal cone ofB at x, and z is the boundary reflection term that cancels out the component of h\u0304(x) inNB(x) and is the \u201cminimal force\u201d needed to keep the solution in B (Kushner and Yin, 2003, Chap. 4.3). The negative definiteness of the matrixC implies that the projected ODE (4.6) has no stationary points other than \u03b8\u2217 if the radius of B is sufficiently large:\nLemma 4.1 Let c > 0 be such that x>Cx \u2264 \u2212c\u2016x\u201622 for all x \u2208 Rn. Suppose B has a radius r > \u2016b\u20162/c. Then \u03b8\u2217 lies in the interior of B, and the only solution x(t), t \u2208 (\u2212\u221e,+\u221e), of the projected ODE (4.6) in B is x(\u00b7) \u2261 \u03b8\u2217.\nThe proof of Lemma 4.1 is given in Appendix B. We now apply (Kushner and Yin, 2003, Theorem 6.1.1) and Lemma 4.1 to prove the a.s. convergence of the constrained ETD(\u03bb) as stated in the theorem below. The proof is given in Appendix B, and it uses the results of Section 3 to verify the conditions required by (Kushner and Yin, 2003, Theorem 6.1.1).\nTheorem 4.1 (Almost sure convergence of constrained ETD(\u03bb)) Let Assumptions 2.1-2.3 hold. Let {\u03b8t} be the sequence generated by the constrained ETD(\u03bb) algorithm (4.5) with stepsizes satisfying \u03b1t = O(1/t) and\n\u03b1t\u2212\u03b1t+1 \u03b1t = O(1/t), and with the radius r of B exceeding the threshold\ngiven in Lemma 4.1. Then, for any given initial (e0, F0, \u03b80), \u03b8t a.s.\u2192 \u03b8\u2217."}, {"heading": "4.2. Convergence of ETD(\u03bb)", "text": "We now prove the convergence theorem, Theorem 2.2, for the unconstrained ETD(\u03bb) algorithm by using the convergence of the constrained algorithm we just established. In particular, we shall compare the iterates generated by the unconstrained algorithm with those generated by the constrained one, and show that the difference between them diminishes asymptotically with probability one.\nLet B = { \u03b8 \u2208 Rn | \u2016\u03b8\u20162 \u2264 r } with its radius r satisfying the condition of Lemma 4.1. Note that to project \u03b8 onto B is simply to scale \u03b8: \u03a0B\u03b8 = \u03b8 if \u2016\u03b8\u20162 \u2264 r; and \u03a0B\u03b8 = r \u00b7 \u03b8/\u2016\u03b8\u20162 if \u2016\u03b8\u20162 > r. More concisely,\n\u03a0B\u03b8 = \u03b7 \u03b8, where \u03b7 = min{1, r/\u2016\u03b8\u20162}.\nTo simplify notation, define matrix Ht and vector gt by\nHt = et \u00b7 \u03c1t ( \u03b3t+1 \u03c6(St+1)\u2212 \u03c6(St) )> , gt = et \u00b7 \u03c1tRt.\nLet us write the constrained algorithm (4.5) equivalently as\n\u03b8\u0303t+1 = (I + \u03b1tHt) \u00b7 \u03b7t \u03b8\u0303t + \u03b1t gt, (4.7)\nwhere \u03b70 = 1 and \u03b7t = min{1, r/\u2016\u03b8\u0303t\u20162} for t \u2265 1. (For t \u2265 1, \u03b7t \u03b8\u0303t corresponds to the projected iterate in (4.5), and \u03b8\u0303t the iterate just before the projection.) The unconstrained algorithm (4.1) can be equivalently written as\n\u03b8t+1 = (I + \u03b1tHt) \u00b7 \u03b8t + \u03b1t gt. (4.8)\nLemma 4.2 Under the conditions of Theorem 4.1, for any given initial (e0, F0), almost surely, the sequence of matrices, \u220ft k\u2265t\u0304 (I + \u03b1kHk), t = t\u0304, t\u0304 + 1, . . ., converges to the n \u00d7 n zero matrix as t\u2192\u221e, for all t\u0304 \u2265 0.\nProof It is sufficient to consider a given (arbitrary) vector y \u2208 Rn and prove that for each initial (e0, F0) and each t\u0304 \u2265 0, \u220ft k\u2265t\u0304 (I + \u03b1kHk) y\na.s.\u2192 0. To this end, consider generating the iterates \u03b8\u0303t\u0304, \u03b8\u0303t\u0304+1, . . . , starting from time t\u0304 and \u03b8\u0303t\u0304 = y, by using the constrained algorithm (4.7) as follows:\n\u03b8\u0303k+1 = (I + \u03b1kHk) \u00b7 \u03b7k \u03b8\u0303k, k \u2265 t\u0304.\nIn the above, we calculate (ek, Fk) and Hk as before starting from time 0 and the given initial condition (e0, F0), and we have set gk = Rk = 0 for all k. Notice that since the stepsize sequence {\u03b1t} satisfies the condition of Theorem 4.1, so does the stepsize sequence, \u03b1t\u0304+1, \u03b1t\u0304+2, . . .. Then, in view of the Markovian property of {(St, At, et, Ft)}, we can apply Theorem 4.1 to the above iteration starting from time t\u0304 for each possible value of (et\u0304, Ft\u0304), thereby concluding that for the given (e0, F0) and t\u0304, \u03b8\u0303t\na.s.\u2192 0 (because Rk = 0 for all k and the solution to C\u03b8 = 0 is 0). On the other hand,\n\u03b8\u0303t+1 = (\u220ft k\u2265t\u0304 (I + \u03b1kHk) ) \u00b7 (\u220ft k\u2265t\u0304 \u03b7k ) \u00b7 y. (4.9)\nSince the solution 0 lies in the interior of B, if \u03b8\u0303t \u2192 0, then \u03b7k = 1 for all k sufficiently large. Thus the convergence \u03b8\u0303t a.s.\u2192 0 implies that as t \u2192 \u221e, \u220ft k\u2265t\u0304 \u03b7k converges a.s. to a strictly positive number that depends on the sample path and the vector y. Consequently, from Eq. (4.9) and the\nconvergence \u03b8\u0303t a.s.\u2192 0, we obtain that (\u220ft k\u2265t\u0304 (I + \u03b1kHk) ) y a.s.\u2192 0 as t \u2192 \u221e. Now this holds for any given vector y, so by letting y be each column of the identity matrix, it follows that as t\u2192\u221e, the matrix \u220ft k\u2265t\u0304 (I + \u03b1kHk) converges a.s. to the zero matrix.\nFinally, we prove the a.s. convergence of the unconstrained ETD(\u03bb) as stated by Theorem 2.2: Proof of Theorem 2.2 Let {\u03b8\u0303t} be the iterates generated by the constrained algorithm (4.7) using the same trajectory of states, actions and rewards that are used by the unconstrained algorithm (4.1) to generate {\u03b8t}. By Theorem 4.1 and Lemma 4.2, there exists a set \u21261 of sample paths such that \u21261 has probability one and on \u21261,\n\u03b8\u0303t \u2192 \u03b8\u2217 and lim t\u21920\n\u220ft k\u2265t\u0304 (I + \u03b1kHk) = 0n\u00d7n, \u2200 t\u0304 \u2265 0,\nwhere 0n\u00d7n denotes the n\u00d7n zero matrix. Consider each path in \u21261. By our choice of the constraint set B, \u03b8\u2217 lies in the interior of B (Lemma 4.1), so the convergence \u03b8\u0303t \u2192 \u03b8\u2217 implies the existence of a path-dependent time t\u2032 <\u221e such that \u03b7k = 1 for all k \u2265 t\u2032. Then\n\u03b8\u0303k+1 = (I + \u03b1kHk) \u00b7 \u03b8\u0303k + \u03b1k gk, \u2200 k \u2265 t\u2032,\nand consequently, \u03b8k+1 \u2212 \u03b8\u0303k+1 = (I + \u03b1kHk) \u00b7 ( \u03b8k \u2212 \u03b8\u0303k ) , \u2200 k \u2265 t\u2032,\n\u03b8t+1 \u2212 \u03b8\u0303t+1 = (\u220ft k\u2265t\u2032 (I + \u03b1kHk) ) \u00b7 ( \u03b8t\u2032 \u2212 \u03b8\u0303t\u2032 ) , \u2200 t \u2265 t\u2032. (4.10)\nAs t\u2192\u221e, the matrix \u220ft k\u2265t\u2032 (I + \u03b1kHk)\u2192 0n\u00d7n for the sample path under consideration. Thus, from Eq. (4.10) we obtain \u03b8t \u2212 \u03b8\u0303t \u2192 0; since \u03b8\u0303t \u2192 \u03b8\u2217, this implies \u03b8t \u2192 \u03b8\u2217.\nRemark 4.1 (Almost sure convergence of regular off-policy TD(\u03bb)) If \u03bb is a constant sufficiently close to 1, the matrix associated with the \u201cmean updates\u201d of the regular off-policy TD(\u03bb) algorithm is also negative definite (Bertsekas and Yu, 2009). In that case, (Yu, 2012, Prop. 4.1) established the a.s. convergence but only for a constrained version of the algorithm, similar to our Theorem 4.1. The proofs given in this subsection, combined with (Yu, 2012, Prop. 4.1), can be used to establish the desired a.s. convergence for the unconstrained off-policy TD(\u03bb) in that case."}, {"heading": "Acknowledgments", "text": "I thank Prof. Richard Sutton for discussions on the ETD algorithm, and Dr. Joseph Modayil, Ashique Mahmood, and several anonymous reviewers for helpful comments on the first version of this paper. This research was supported by a grant from Alberta Innovates \u2013 Technology Futures."}, {"heading": "Appendix A. Proof Details for Section 3", "text": "In this appendix we give proof details and related results for Section 3. Assumption 2.1 on the target and behavior policies will be in force throughout, so it will not be mentioned explicitly in intermediate technical results."}, {"heading": "A.1. Some Basic Technical Lemmas", "text": "We prove three basic lemmas that will be useful later. First, recall that \u0393 and \u039b are diagonal matrices with \u03b3(s) (discount factors) and \u03bb(s), s \u2208 S , on their diagonals, respectively. Note also that under Assumption 2.1, the inverse (I \u2212P\u03c0\u0393)\u22121 exists. This implies that (I \u2212P\u03c0\u0393\u039b)\u22121 also exists. Then, since\n(I \u2212 P\u03c0\u0393)\u22121 = \u221e\u2211 t=0 (P\u03c0\u0393) t, (I \u2212 P\u03c0\u0393\u039b)\u22121 = \u221e\u2211 t=0 (P\u03c0\u0393\u039b) t,\nboth (P\u03c0\u0393)t and (P\u03c0\u0393\u039b)t converge to the zero matrix as t\u2192\u221e. We now specify some notation. In what follows, let 1 denote the vector of all ones. For an expression H that results in a vector in RN , we will write (H)(s) for the s-th entry of the resulting vector. (For example, (P\u03c01)(s) and (1>P\u03c0)(s) represent the s-th entry of the vector P\u03c01 and 1>P\u03c0, respectively.)\nLet Ft = \u03c3 ( S0, A0, . . . , St ) be the \u03c3-algebra generated by the states and actions up to time t, including the state St but excluding the action At. Recall some shorthand notation we defined earlier:\n\u03c1t = \u03c1(St, At) = \u03c0(At|St) \u03c0o(At|St) , \u03b3t = \u03b3(St), \u03bbt = \u03bb(St).\nTo simplify notation, let us also define for t \u2265 1,\n\u03b2t = \u03c1t\u22121 \u03b3t \u03bbt.\nLemma A.1 For all t > k \u2265 0,\nE [ \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t | Fk ] = ( (P\u03c0\u0393) t\u2212k1 ) (Sk) \u2264 1, (A.1)\nE [ \u03b2k+1\u03b2k+2 \u00b7 \u00b7 \u00b7\u03b2t | Fk ] = ( (P\u03c0\u0393\u039b) t\u2212k1 ) (Sk) \u2264 1. (A.2)\nFurthermore, as t\u2192\u221e, \u220ft k=1 ( \u03c1k\u22121\u03b3k ) a.s.\u2192 0 and\u220ftk=1 \u03b2k a.s.\u2192 0. Proof The first two equations follow simply from a direct calculation.\nLet \u2206t = \u220ft k=1 ( \u03c1k\u22121\u03b3k ) . To prove \u2206t a.s.\u2192 0, consider equivalently the iterates\n\u22060 = 1, \u2206t = (\u03c1t\u22121\u03b3t)\u2206t\u22121, t \u2265 1.\nClearly \u2206t is Ft-measurable, and by Eq. (A.1) with k = t\u2212 1,\nE [ \u2206t | Ft\u22121 ] = \u2206t\u22121 \u00b7 E [ \u03c1t\u22121\u03b3t | Ft\u22121 ] \u2264 \u2206t\u22121.\nSo {(\u2206t,Ft)} is a nonnegative supermartingale with E[\u22060] = 1 < \u221e. By a convergence theorem for nonnegative supermartingales (Neveu, 1975, Theorem II-2-9), \u2206t\na.s.\u2192 \u2206\u221e for some nonnegative random variable \u2206\u221e satisfying E [ \u2206\u221e ] \u2264 lim inft\u2192\u221e E [ \u2206t ] . From Eq. (A.1) with k = 0, we have\nE [ \u2206t ] \u2264 1>(P\u03c0\u0393)t1 \u2192 0 as t \u2192 \u221e; therefore, E [ \u2206\u221e ] = 0. This implies \u2206\u221e = 0 a.s., i.e., \u2206t a.s.\u2192 0. The assertion \u220ft k=1 \u03b2k\na.s.\u2192 0 follows similarly by considering the iterates \u2206t = \u03b2t\u2206t\u22121 with \u22060 = 1, and by using Eq. (A.2) together with the nonnegative supermartingale convergence argument.\nLemma A.2 For k \u2265 0, let Yk be anFk-measurable nonnegative random variable. Then for t > k,\nE [ Yk \u00b7 (\u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t )] \u2264 E[Yk] \u00b7 ( 1>(P\u03c0\u0393) t\u2212k1 ) , (A.3)\nE [ Yk \u00b7 ( \u03b2k+1\u03b2k+2 \u00b7 \u00b7 \u00b7\u03b2t )] \u2264 E[Yk] \u00b7 ( 1>(P\u03c0\u0393\u039b) t\u2212k1 ) . (A.4)\nHence, if for some constant L <\u221e, E[Yk] \u2264 L for all k, then\nE\n[ t\u2211\nk=0\nYk \u00b7 (\u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t )] \u2264 L \u00b7 1> ( t\u2211\nk=0\n(P\u03c0\u0393) k ) 1 <\u221e, (A.5)\nE\n[ t\u2211\nk=0\nYk \u00b7 ( \u03b2k+1\u03b2k+2 \u00b7 \u00b7 \u00b7\u03b2t )] \u2264 L \u00b7 1> ( t\u2211\nk=0\n(P\u03c0\u0393\u039b) k ) 1 <\u221e. (A.6)\nProof For any k \u2265 0, ( (P\u03c0\u0393) t\u2212k1 ) (Sk) \u2264 1>(P\u03c0\u0393)t\u2212k1. Using this, Eq. (A.1) in Lemma A.1, and the assumption that Yk is Fk-measurable, we have\nE [ Yk \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t )] = E [ Yk \u00b7 E [ (\u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) | Fk ] ] \u2264 E[Yk] \u00b7 ( 1>(P\u03c0\u0393) t\u2212k1 ) .\nThis proves Eqs. (A.3), (A.5). Similarly, Eqs. (A.4), (A.6) are obtained by using Eq. (A.2) in Lemma A.1 and a direct calculation.\nLemma A.3 Let {ak} and {ck} be two sequences of nonnegative numbers with \u2211\u221e\nk=1 ak < \u221e and \u2211\u221e k=1 ck <\u221e. Then limt\u2192\u221e \u2211t k=1 ak ct\u2212k = 0.\nProof For any m \u2264 t,\nt\u2211 k=1 ak ct\u2212k = m\u2211 k=1 ak ct\u2212k + t\u2211 k=m+1 ak ct\u2212k \u2264 ( max k ak ) \u00b7 t\u22121\u2211 k=t\u2212m ck + (max k ck) \u00b7 t\u2211 k=m+1 ak.\nSince {ak} and {ck} are summable by assumption, we have maxk ak < \u221e, maxk ck < \u221e, and limt\u2192\u221e \u2211t\u22121 k=t\u2212m ck = 0. So if we fix m and let t go to infinity in the preceding inequality, we have\nlim sup t\u2192\u221e t\u2211 k=1 ak ct\u2212k \u2264 (max k ck) \u00b7 \u221e\u2211 k=m+1 ak.\nSince limm\u2192\u221e \u2211\u221e\nk=m+1 ak = 0 by the summable assumption, by letting m go to infinity in the right-hand side above, we obtain limt\u2192\u221e \u2211t k=1 ak ct\u2212k = 0.\nA.2. Properties of the Trace Iterates {(et, Ft)}\nIn this subsection we state formally the properties of trace iterates which we mentioned in Section 3.1, and we give their proofs. These properties will be used frequently in obtaining some of our main convergence theorems.\nThe following proposition is the property (i) mentioned in Section 3.1. First, let us express the traces et, Ft, by using their definitions [cf. Eqs. (2.2)-(2.4)], as\nFt = F0 \u00b7 ( \u03c10\u03b31 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) + t\u2211 k=1 i(Sk) \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) , (A.7)\net = e0 \u00b7 ( \u03b21 \u00b7 \u00b7 \u00b7\u03b2t ) + t\u2211 k=1 Mk \u00b7 \u03c6(Sk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) , (A.8)\nwhere \u03b2k = \u03c1k\u22121\u03b3k\u03bbk as defined in the previous subsection, and\nMk = \u03bbk i(Sk) + (1\u2212 \u03bbk)Fk. Let Ft = \u03c3 ( S0, A0, . . . , St ) for t \u2265 0, throughout this subsection.\nProposition A.1 For any given initial (e0, F0), supt\u22650 E [\u2225\u2225(et, Ft)\u2225\u2225] <\u221e.\nProof Let us calculate E [ Ft ] and E [ \u2016et\u2016 ] . Since the number of states is finite, there exists a finite constant L > 0 such that L \u2265 \u2016(e0, F0)\u2016 and L \u2265 i(s), L \u2265 \u2016\u03c6(s)\u2016 for all states s. Using the expression (A.7) for Ft and applying Eq. (A.5) in Lemma A.2 (with Y0 = F0, Yk = i(Sk), k \u2265 1), we have the bound\nE [ Ft ] \u2264 L \u00b7 1>\n( t\u2211\nk=0\n(P\u03c0\u0393) k ) 1 \u2264 L \u00b7 1>(I \u2212 P\u03c0\u0393)\u221211.\nThus supt\u22650 E [ Ft ] < \u221e. We now calculate E [ \u2016et\u2016 ] . Using the expression (A.8) for et, and using also the fact Mk \u2264 L+ Fk, we can bound \u2016et\u2016 by\n\u2016et\u2016 \u2264 L \u00b7 ( \u03b21 \u00b7 \u00b7 \u00b7\u03b2t) + L \u00b7 t\u2211 k=1 ( L+ Fk ) \u00b7 ( \u03b2k+1\u03b2k+2 \u00b7 \u00b7 \u00b7\u03b2t ) .\nUsing the fact supk\u22650 E [ Fk ] \u2264 L\u2032 for some finite constant L\u2032 as we just proved, and using also Eq. (A.6) in Lemma A.2 (with Y0 = L, Yk = L(L+ Fk), k \u2265 1), we obtain\nE [ \u2016et\u2016 ] \u2264 L(L+ L\u2032 + 1) \u00b7 1>\n( t\u2211\nk=0\n(P\u03c0\u0393\u039b) k ) 1 \u2264 L(L+ L\u2032 + 1) \u00b7 1>(I \u2212 P\u03c0\u0393\u039b)\u221211.\nHence supt\u22650 E [ \u2016et\u2016 ] <\u221e. Since \u2225\u2225(et, Ft)\u2225\u2225 \u2264 \u2016et\u2016+ Ft, this shows that sup t\u22650 E [\u2225\u2225(et, Ft)\u2225\u2225] \u2264 sup t\u22650 E [ \u2016et\u2016 ] + sup t\u22650 E [ Ft ] <\u221e.\nThe proof is complete.\nRecall that {Zt} with Zt = (St, At, et, Ft) denotes the Markov chain on the joint space S \u00d7 A \u00d7 Rn+1 of states, actions and traces, and it is a weak Feller Markov chain (cf. Footnote 12). As explained in Section 3.1, since S and A are finite, the preceding proposition implies that {Zt} is bounded in probability and hence, by its weak Feller property, has at least one invariant probability measure. We will need the following result (which is the property (ii) in Section 3.1) to prove that {Zt} has a unique invariant probability measure.\nLet (e\u0302t, F\u0302t), t \u2265 1, be defined by the same recursion (2.2)-(2.4) that defines (et, Ft), using the same state and action random variables, but with a different initial condition (e\u03020, F\u03020). We write a zero vector in any Euclidean space as 0.\nProposition A.2 For any two given initial conditions (e0, F0) and (e\u03020, F\u03020),\nFt \u2212 F\u0302t a.s.\u2192 0, et \u2212 e\u0302t a.s.\u2192 0.\nProof Using the expression (A.7) for Ft and F\u0302t, we have Ft \u2212 F\u0302t = (F0 \u2212 F\u03020) \u00b7 \u220ft k=1(\u03c1k\u22121\u03b3k).\nSince \u220ft k=1(\u03c1k\u22121\u03b3k) a.s.\u2192 0 by Lemma A.1, it follows that Ft \u2212 F\u0302t a.s.\u2192 0.\nThe proof of Lemma A.1 also shows E [\u2223\u2223Ft \u2212 F\u0302t\u2223\u2223] \u2264 \u2223\u2223F0 \u2212 F\u03020\u2223\u2223 \u00b7 1>(P\u03c0\u0393)t1. (A.9)\nWe will need this inequality below. We now prove et\u2212 e\u0302t\na.s.\u2192 0. To simplify the derivation, we first observe that if e\u03020 6= e0 but F\u03020 = F0, then F\u0302t = Ft for all t, so the expression (A.8) for et and e\u0302t gives \u2016et\u2212e\u0302t\u2016 = \u2016e0\u2212e\u03020\u2016\u00b7 \u220ft k=1 \u03b2k.\nSince \u220ft k=1 \u03b2k a.s.\u2192 0 by Lemma A.1, it follows immediately that in this case \u2016et \u2212 e\u0302t\u2016 a.s.\u2192 0.\nThus, for the general case, we can focus on the difference between et and e\u0302t that is due to the difference between the initial F0 and F\u03020. In particular, define another sequence of iterates (e\u0303t, F\u0303t) using the same recursion (2.2)-(2.4) but with the initial condition (e\u03030, F\u03030) = (e0, F\u03020). Since\u2225\u2225et\u2212 e\u0302t\u2225\u2225 \u2264 \u2225\u2225et\u2212 e\u0303t\u2225\u2225+\u2225\u2225e\u0303t\u2212 e\u0302t\u2225\u2225 and \u2225\u2225e\u0303t\u2212 e\u0302t\u2225\u2225 a.s.\u2192 0 by what we just proved, to show et\u2212 e\u0302t a.s.\u2192 0, it is sufficient to prove \u2016et \u2212 e\u0303t\u2016\na.s.\u2192 0. Since F\u03030 = F\u03020, the sequence {F\u0303t} coincides with {F\u0302t}. Then by the definition of et and e\u0303t,\net \u2212 e\u0303t = \u03b2t ( et\u22121 \u2212 e\u0303t\u22121 ) + (1\u2212 \u03bbt) ( Ft \u2212 F\u0302t ) \u00b7 \u03c6(St).\nSince 0 \u2264 E [ \u03b2t | Ft\u22121 ] \u2264 1 (Lemma A.1) and 0 \u2264 1\u2212 \u03bbt \u2264 1, it follows that\nE [\u2225\u2225et \u2212 e\u0303t\u2225\u2225 | Ft\u22121] \u2264 \u2225\u2225et\u22121 \u2212 e\u0303t\u22121\u2225\u2225+ Yt\u22121, where Yt\u22121 = E[\u2223\u2223Ft \u2212 F\u0302t\u2223\u2223 \u00b7 \u2016\u03c6(St)\u2016 | Ft\u22121].\n(A.10) Let us show \u2211\u221e t=0 Yt <\u221e a.s. In view of Eq. (A.10), this will then imply, by a convergence theorem in (Neveu, 1975, Ex. II-4, p. 33-34) for nonnegative random processes (which is a consequence of the nonnegative supermartingale convergence theorem), that\n\u2225\u2225et\u2212e\u0303t\u2225\u2225 converges a.s. to a finite limit. To prove \u2211\u221e t=0 Yt <\u221e a.s., it is sufficient to show E [\u2211\u221e t=0 Yt ] <\u221e. LetL = maxs\u2208S \u2016\u03c6(s)\u2016.\nBy Eq. (A.9), for each t, E [ Yt ] = E [\u2223\u2223Ft+1 \u2212 F\u0302t+1\u2223\u2223 \u00b7 \u2016\u03c6(St+1)\u2016 ] \u2264 L\u2223\u2223F0 \u2212 F\u03020\u2223\u2223 \u00b7 1>(P\u03c0\u0393)t+11.\nSince \u2211\u221e\nt=0(1 >(P\u03c0\u0393) t+11) \u2264 1>(I \u2212 P\u03c0\u0393)\u221211 < \u221e, we obtain E [\u2211\u221e t=0 Yt ] = \u2211\u221e t=0 E [ Yt ] <\n\u221e. Hence \u2211\u221e\nt=0 Yt <\u221e a.s., and as discussed earlier, this implies that\u2225\u2225et \u2212 e\u0303t\u2225\u2225 a.s.\u2192 \u2206\u221e\nfor a nonnegative real-valued random variable \u2206\u221e. What remains to be proved is \u2206\u221e = 0 a.s. By Fatou\u2019s lemma (Dudley, 2002, Theorem 4.3.3),\nE[\u2206\u221e] \u2264 lim inf t\u2192\u221e\nE [\u2225\u2225et \u2212 e\u0303t\u2225\u2225]. (A.11)\nWe show that the right-hand side equals 0. By a direct calculation (using Eq. (A.8) and the fact e\u03030 = e0), we can write\net \u2212 e\u0303t = t\u2211\nk=1\n\u03c6(Sk) \u00b7 (Fk \u2212 F\u0302k) \u00b7 (1\u2212 \u03bbk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) .\nFor each k \u2265 1, using Lemma A.2 and Eq. (A.9), we have E [\u2223\u2223Fk \u2212 F\u0302k\u2223\u2223 \u00b7 (1\u2212 \u03bbk) \u00b7 (\u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t)] \u2264 E[\u2223\u2223Fk \u2212 F\u0302k\u2223\u2223] \u00b7 (1>(P\u03c0\u0393\u039b)t\u2212k1)\n\u2264 \u2223\u2223F0 \u2212 F\u03020\u2223\u2223 \u00b7 (1>(P\u03c0\u0393)k1) \u00b7 (1>(P\u03c0\u0393\u039b)t\u2212k1).\nFrom the preceding two relations, it follows that\nE [\u2225\u2225et \u2212 e\u0302t\u2225\u2225] \u2264 L\u2223\u2223F0 \u2212 F\u03020\u2223\u2223 \u00b7 t\u2211\nk=1\n( 1>(P\u03c0\u0393) k1 ) \u00b7 ( 1>(P\u03c0\u0393\u039b) t\u2212kP\u03c01 ) . (A.12)\nFrom Lemma A.3 with {ak} and {ck} defined as ak = 1>(P\u03c0\u0393)k1 and ck = 1>(P\u03c0\u0393\u039b)k1 for k \u2265 1, we have\nlim t\u2192\u221e t\u2211 k=1 ( 1>(P\u03c0\u0393) k1 ) \u00b7 ( 1>(P\u03c0\u0393\u039b) t\u2212k1 ) = 0.\nCombining this with Eq. (A.12) gives lim inft\u2192\u221e E [\u2225\u2225et\u2212 e\u0303t\u2225\u2225] = 0, and consequently, E[\u2206\u221e] = 0\nby Eq. (A.11). This implies \u2206\u221e = 0 a.s., i.e., \u2225\u2225et \u2212 e\u0303t\u2225\u2225 a.s.\u2192 0.\nThe next proposition is the property (iii) mentioned in Section 3.1, which concerns approximating the trace iterates (et, Ft) by truncated traces that depend on a fixed number of the most recent states and actions only. We will use this proposition subsequently to prove Theorem 3.1: it allows us to work with simple finite-space Markov chains, instead of working with the infinite-space Markov chain {Zt} directly.\nFor each integer K \u2265 1, we define the truncated traces Yt,K = (e\u0303t,K , F\u0303t,K) as follows:\nYt,K = (et, Ft) for t \u2264 K,\nand for t \u2265 K + 1,\nF\u0303t,K = t\u2211\nk=t\u2212K i(Sk) \u00b7\n( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) , (A.13)\nM\u0303t,K = \u03bbt i(St) + (1\u2212 \u03bbt)F\u0303t,K , (A.14)\ne\u0303t,K = t\u2211\nk=t\u2212K M\u0303k,K \u00b7 \u03c6(Sk) \u00b7\n( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) . (A.15)\nDenote the original traces by Yt = (et, Ft); recall that they can be expressed as in Eqs. (A.7)(A.8). We have the following result, in which the notation \u201cLK \u2193 0\u201d means that LK decreases monotonically to 0 as K \u2192\u221e, and in which Z0 = (S0, A0, e0, F0) as we recall:"}, {"heading": "Proposition A.3", "text": "(i) For any given initial Y0 = (e0, F0), there exist constants LK ,K \u2265 1, with LK \u2193 0, such that\nE [\u2225\u2225Yt \u2212 Yt,K\u2225\u2225] \u2264 LK , \u2200 t \u2265 0.\n(ii) There exist constants LK ,K \u2265 1, independent of the given initial value of Z0, such that LK \u2193 0 and\nE [\u2225\u2225Yt,K\u2032 \u2212 Yt,K\u2225\u2225] \u2264 LK , \u2200K \u2032 \u2265 K, t > 2K \u2032.\nProof Let L = max{F0, maxs\u2208S i(s)}. We first calculate Ft \u2212 F\u0303t,K . By definition Ft \u2212 F\u0303t,K = 0 for t \u2264 K. For t \u2265 K + 1, using the expressions (A.7), (A.13) of Ft and F\u0303t,K , we have\nFt \u2212 F\u0303t,K = F0 \u00b7 ( \u03c10\u03b31 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) + t\u2212K\u22121\u2211 k=1 i(Sk) \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) ,\nfrom which it follows by applying Eq. (A.3) in Lemma A.2 that E [\u2223\u2223Ft \u2212 F\u0303t,K\u2223\u2223] \u2264 L \u00b7 1>( t\u2211\nk=K+1\n(P\u03c0\u0393) k ) 1 \u2264 L \u00b7 1> ( \u221e\u2211 k=K+1 (P\u03c0\u0393) k ) 1 def = L (1) K . (A.16)\nSimilarly we bound et \u2212 e\u0303t,K . By definition et = e\u0303t,K for t \u2264 K. For t \u2265 K + 1, using the expressions (A.8), (A.15) of et and e\u0303t,K , and using also the expressions (2.3), (A.14) of Mt and M\u0303t,K , we have\net \u2212 e\u0303t,K = e0 \u00b7 ( \u03b21 \u00b7 \u00b7 \u00b7\u03b2t ) + t\u2212K\u22121\u2211 k=1 Mk \u00b7 \u03c6(Sk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) +\nt\u2211 k=t\u2212K \u03c6(Sk) \u00b7 ( Fk \u2212 F\u0303k,K ) \u00b7 (1\u2212 \u03bbk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) .\nBy Prop. A.1, supk\u22650 E[Mk] < \u221e, so we can find a constant L\u2032 < \u221e that is greater than \u2016e0\u2016, maxs\u2208S \u2016\u03c6(s)\u2016 and ( maxs\u2208S \u2016\u03c6(s)\u2016 ) \u00b7 supk\u22650 E[Mk]. Then applying Eq. (A.4) in Lemma A.2, and using also Eq. (A.16), we obtain\nE [\u2225\u2225et \u2212 e\u0303t,K\u2225\u2225] \u2264 L\u2032 \u00b7 1>(t\u2212K\u22121\u2211\nk=0\n(P\u03c0\u0393\u039b) t\u2212k ) 1 + L\u2032 \u00b7\nt\u2211 k=t\u2212K E [\u2223\u2223Fk \u2212 F\u0303k,K\u2223\u2223] \u00b7 (1>(P\u03c0\u0393\u039b)t\u2212k1)\n\u2264 L\u2032 \u00b7 1> (\nt\u2211 k=K+1 (P\u03c0\u0393\u039b) k ) 1 + L\u2032 \u00b7 L(1)K \u00b7 ( 1> K\u2211 k=0 (P\u03c0\u0393\u039b) k1 )\n\u2264 L\u2032 \u00b7 1> ( \u221e\u2211 k=K+1 (P\u03c0\u0393\u039b) k ) 1 + L\u2032 \u00b7 L(1)K \u00b7 ( 1>(I \u2212 P\u03c0\u0393\u039b)\u221211 ) def = L (2) K .\nNow let LK = L (1) K +L (2) K . From the expressions of L (1) K and L (2) K given above, clearly, LK \u2193 0 as K \u2192\u221e. Then, in view of the relation \u2225\u2225Yt \u2212 Yt,K\u2225\u2225 \u2264 \u2223\u2223Ft \u2212 F\u0303t,K\u2223\u2223+ \u2225\u2225et \u2212 e\u0303t,K\u2225\u2225, we obtain the\ndesired bound E [\u2225\u2225Yt \u2212 Yt,K\u2225\u2225] \u2264 E [\u2223\u2223Ft \u2212 F\u0303t,K\u2223\u2223]+ E [\u2225\u2225et \u2212 e\u0303t,K\u2225\u2225] \u2264 LK . This proves part (i).\nPart (ii) is proved similarly. By the definition of the truncated traces, for t > 2K \u2032 \u2265 2K,\nF\u0303t,K\u2032 \u2212 F\u0303t,K = t\u2212K\u22121\u2211 k=t\u2212K\u2032 i(Sk) \u00b7 ( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) ,\nand\ne\u0303t,K\u2032 \u2212 e\u0303t,K = t\u2212K\u22121\u2211 k=t\u2212K\u2032 M\u0303k,K\u2032 \u00b7 \u03c6(Sk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) +\nt\u2211 k=t\u2212K \u03c6(Sk) \u00b7 ( F\u0303k,K\u2032 \u2212 F\u0303k,K ) \u00b7 (1\u2212 \u03bbk) \u00b7 ( \u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t ) .\nWe then apply the same calculation as in the proof of part (i). When t > 2K \u2032, the truncated traces do not depend on the initial condition (e0, F0). Since the state and action spaces are finite, we can set the constants L,L\u2032 to be independent of the initial condition of Z0. Part (ii) then follows.\nRemark A.1 (On the behavior of trace iterates) From the properties of {(et, Ft)} given above and the ergodicity of the Markov chain {(St, At, et, Ft)} shown in Theorem 3.2, we see that these trace iterates are well-behaved. On the other hand, like in regular off-policy algorithms, these iterates can be unbounded almost surely and their variances can grow to infinity with time. There are no contradictions here. To illustrate this point, let us consider a simple example with just 1 state and 2 actions, S = {1},A = {a1, a2}, where all actions result in a self-transition at state 1. Let \u03c0(a1 | 1) = 1 for the target policy \u03c0, and let \u03c0o(a1 | 1) = q < 1 for the behavior policy \u03c0o. Let the discount factor be a constant \u03b3 < 1. Then for all t,\nE[ \u03b32t \u03c1 2 t\u22121 | Ft\u22121 ] = \u03b32/q.\nSuppose \u03b32/q > 1. Then even with i(1) = 0, if F0 > 0, the definition Ft = \u03b3t\u03c1t\u22121Ft\u22121 implies that\nE[F 2t ] = E [ E[ \u03b32t \u03c1 2 t\u22121 | Ft\u22121 ] \u00b7 F 2t\u22121 ] = (\u03b32/q)t \u00b7 F 20 \u2192\u221e,\nyet since i(1) = 0, {Ft} is also a supermartingale converging to 0 a.s. (cf. the proof of Lemma A.1). For the case i(1) > 0, again E[F 2t ] \u2192 \u221e if \u03b32/q > 1, and by (Yu, 2012, Prop. 3.1) the sequence {Ft} is almost surely unbounded if \u03b3/q > 1, yet {Ft} is bounded in probability in the sense described by Prop. A.1.\nAs mentioned earlier in Remark 2.2, it can be desirable to restrict the behavior policy so that the variances of the trace iterates do not grow to infinity. In the simple example above, this can be easily arranged. In the general case, however, if the state-dependent discount factor \u03b3(\u00b7) can take the value 1 for some states, then without knowledge of the MDP model, to sufficiently restrict the behavior policy seems to be a difficult task."}, {"heading": "A.3. Proof of Theorem 3.1", "text": "For convenience, we restate Theorem 3.1 here. Recall that the theorem concerns the recursion\nGt+1 = (1\u2212 \u03b1t)Gt + \u03b1t h(Yt, St, At, St+1),\nwhere Yt = (et, Ft), and the function h is Lipschitz continuous in y: for some constant Lh,\u2225\u2225h(y, s, a, s\u2032)\u2212 h(y\u0302, s, a, s\u2032)\u2225\u2225 \u2264 Lh\u2016y \u2212 y\u0302\u2016, \u2200 y, y\u0302 \u2208 Rn+1, \u2200 (s, a, s\u2032) \u2208 S \u00d7A\u00d7 S. Theorem 3.1 (L1-convergence of {Gt}) Let h be a vector-valued function satisfying the Lipschitz condition (3.1), and let {Gt} be defined by the recursion (3.2), using the process {Zt}. Then under Assumptions 2.1, 2.3, there exists a constant vector G\u2217 (independent of the stepsizes) such that for any given initial Y0 = (e0, F0) and G0, limt\u2192\u221e E\n[\u2225\u2225Gt \u2212G\u2217\u2225\u2225] = 0. Proof The proof proceeds in three steps: (i) For each K \u2265 1, we consider the truncated traces Yt,K = (e\u0303t,K , F\u0303t,K), t = 0, 1, . . ., defined by Eqs. (A.13)-(A.15). Correspondingly, we define iterates G\u03030,K = G0 and\nG\u0303t+1,K = (1\u2212 \u03b1t) G\u0303t,K + \u03b1t h(Yt,K , St, At, St+1).\nFor each t, Yt,K is a function of (St\u22122K , At\u22122K , . . . , St), so h(Yt,K , St, At, St+1) can be viewed as a function of Xt = (St\u22122K , At\u22122K , . . . , St+1), where {Xt} is a finite state Markov chain with a single recurrent class by Assumption 2.1(ii). Then, with E0 denoting the expectation under the stationary distribution of the Markov chain {(St, At)}, we have, by a result from stochastic approximation theory (Borkar, 2008, Chap. 6, Theorem 7 and Cor. 8), that under Assumption 2.3 on the stepsizes,\nG\u0303t,K a.s.\u2192 G\u2217K , where G\u2217K = E0 [ h(Yk,K , Sk, Ak, Sk+1) ] \u2200 k > 2K. (A.17)\nClearly, the vector G\u2217K does not depend on the initial condition (Y0, G0) and the stepsizes {\u03b1t}. Since for all t, \u2016G\u0303t,K\u2016 \u2264 L for some constant L < \u221e, we also have by the bounded convergence theorem\nlim t\u2192\u221e\nE [\u2225\u2225G\u0303t,K \u2212G\u2217K\u2225\u2225] = 0. (A.18)\n(ii) We show that as K \u2192 \u221e, G\u2217K converges to some vector G\u2217. For any K \u2032 > K, using the Lipschitz property of h and Prop. A.3(ii), we have that for k > 2K \u2032,\u2225\u2225G\u2217K\u2032 \u2212G\u2217K\u2225\u2225 = \u2225\u2225E0[h(Yk,K\u2032 , Sk, Ak, Sk+1)\u2212 h(Yk,K , Sk, Ak, Sk+1)]\u2225\u2225\n\u2264 Lh E0 [\u2225\u2225Yk,K\u2032 \u2212 Yk,K\u2225\u2225] \u2264 Lh LK ,\nwhere LK is some constant with LK \u2193 0 as K \u2192\u221e. This shows that {G\u2217K} is a Cauchy sequence and hence converges to some G\u2217. (iii) We establish the theorem by bounding the differences between Gt and G\u0303t,K for an increasing K. For each K,\nlim sup t\u2192\u221e\nE [\u2225\u2225Gt \u2212G\u2217\u2225\u2225] \u2264 lim sup t\u2192\u221e E [\u2225\u2225Gt \u2212 G\u0303t,K\u2225\u2225]+ lim sup t\u2192\u221e E [\u2225\u2225G\u0303t,K \u2212G\u2217K\u2225\u2225]+ \u2225\u2225G\u2217K \u2212G\u2217\u2225\u2225.\nIn the right-hand side, the second term equals 0 by Eq. (A.18), and the last term converges to 0 as K \u2192\u221e, as we just showed in step (ii). Consider now the first term. Since\nGt+1 \u2212 G\u0303t+1,K = (1\u2212 \u03b1t) ( Gt \u2212 G\u0303t,K ) + \u03b1t ( h(Yt, St, At, St+1)\u2212 h(Yt,K , St, At, St+1) ) and\n\u2225\u2225h(Yt, St, At, St+1)\u2212h(Yt,K , St, At, St+1)\u2225\u2225 \u2264 Lh\u2016Yt\u2212Yt,K\u2016 by the Lipschitz property of h, we have\nE [\u2225\u2225Gt+1 \u2212 G\u0303t+1,K\u2225\u2225] \u2264 (1\u2212 \u03b1t)E[\u2225\u2225Gt \u2212 G\u0303t,K\u2225\u2225]+ \u03b1tLhE[\u2016Yt \u2212 Yt,K\u2016]\n\u2264 (1\u2212 \u03b1t)E [\u2225\u2225Gt \u2212 G\u0303t,K\u2225\u2225]+ \u03b1tLhLK , (A.19)\nwhere the second inequality follows from Prop. A.3(i), which gives the constants LK ,K \u2265 1, with LK \u2193 0. For each K, in view of Assumption 2.3 on the stepsize, the inequality (A.19) implies that\nlim sup t\u2192\u221e\nE [\u2225\u2225Gt \u2212 G\u0303t,K\u2225\u2225] \u2264 LhLK .\nThen, since LK \u2193 0, letting K go to infinity in the right-hand side of the preceding inequality, it follows that limt\u2192\u221e E [\u2225\u2225Gt \u2212G\u2217\u2225\u2225] = 0."}, {"heading": "A.4. Handling Noisy Rewards: Proof of Prop. 3.1", "text": "For convenience, we restate Prop. 3.1 below. For each t \u2265 0, let \u03c9t+1 = Rt \u2212 r(St, At, St+1), the noise in the observed reward Rt. We consider the recursion (3.4): W0 = 0 and\nWt+1 = (1\u2212 \u03b1t)Wt + \u03b1t et \u03c1t \u00b7 \u03c9t+1, t \u2265 0. (A.20)\nRecall that it is assumed in our MDP model thatRt has mean r(St, At, St+1) and bounded variance; specifically, let Ft = \u03c3(S0, A0, . . . , St+1) in what follows, and we have that for some constant L <\u221e,\nE [ \u03c9t+1 | Ft ] = 0, E [ \u03c92t+1 | Ft ] < L. (A.21)\nProposition 3.1 (Effects of noise in random rewards) Under Assumptions 2.1, 2.3, for any given initial (e0, F0), we have (i) E [ \u2016Wt\u2016 ] \u2192 0; and (ii) if, in addition, the stepsize is \u03b1t = 1/(t + 1), then Wt a.s.\u2192 0.\nBecause the proofs of part (i) and part (ii) use quite different arguments, we give them separately below. Proof of Prop. 3.1(i) To simplify notation, denote \u03c9\u0303t+1 = \u03c1t \u03c9t+1. Similarly to the proof of Theorem 3.1, we first consider for each K \u2265 1, the truncated traces {e\u0303t,K , t \u2265 0} given by Eq. (A.15), and we replace {et} in the recursion (A.20) by {e\u0303t,K} to define iterates W\u03030,K = 0 and\nW\u0303t+1,K = (1\u2212 \u03b1t) W\u0303t,K + \u03b1t e\u0303t,K \u00b7 \u03c9\u0303t+1, t \u2265 0.\nSince the number of states and actions is finite, we can bound \u2016e\u0303t,K\u2016 by some finite constant for all t. Then, using Eq. (A.21), we have that for all t \u2265 0,\nE [ e\u0303t,K \u00b7 \u03c9\u0303t+1 | Ft] = 0, E [ \u2016e\u0303t,K\u20162 \u00b7 \u03c9\u03032t+1 | Ft ] \u2264 L\u2032 for some constant L\u2032 <\u221e.\nUnder Assumption 2.3 on the stepsize {\u03b1t}, this implies by (Tsitsiklis, 1994, Lemma 1) that W\u0303t,K\na.s.\u2192 0. Next we show limt\u2192\u221e E [ \u2016W\u0303t,K\u2016 ] = 0. Since \u03b1t \u2208 (0, 1] for all t and W\u03030,K = 0, for each t \u2265 0, W\u0303t+1,K can be expressed as a convex combination of ej,K \u03c9\u0303j+1, j \u2264 t, with coefficients ct,j (each ct,j is a function of (\u03b10, . . . , \u03b1t)). Consequently, \u2016W\u0303t+1,K\u2016 \u2264 \u2211t j=0 ct,j\u2016ej,K\u2016 \u00b7 |\u03c9\u0303j+1|, and by the convexity of the function x2,\n\u2016W\u0303t+1,K\u20162 \u2264 \u2211t j=0ct,j\u2016ej,K\u20162 \u00b7 |\u03c9\u0303j+1|2.\nAs discussed earlier, the variance of ej,K \u00b7 \u03c9\u0303j+1 can be bounded uniformly for all j, so the preceding inequality implies that there exists some constant L\u2032 <\u221e with\nE [ \u2016W\u0303t+1,K\u20162 ] \u2264 L\u2032, \u2200 t \u2265 0. (A.22)\nThis in turn implies that the sequence {\u2016W\u0303t,K\u2016, t \u2265 0} is uniformly integrable (see e.g., Billingsley, 1968, p. 32); i.e.,\nsup t\u22650\nE [ \u2016W\u0303t,K\u2016 \u00b7 1 ( \u2016W\u0303t,K\u2016 \u2265 a )] \u2192 0 as a\u2192 +\u221e,\n(where 1 ( \u2016W\u0303t,K\u2016 \u2265 a ) is the indicator for the event \u2016W\u0303t,K\u2016 \u2265 a). By (Neveu, 1975, Lemma IV-25, p. 66), every uniformly integrable sequence of random variables which converges almost surely also converges in L1. Therefore, since W\u0303t,K a.s.\u2192 0 as proved earlier, we have limt\u2192\u221e E [ \u2016W\u0303t,K\u2016 ] = 0. We now prove limt\u2192\u221e E [ \u2016Wt\u2016 ] = 0 similarly to the proof step (iii) for Theorem 3.1. For each\nK \u2265 1, since limt\u2192\u221e E [ \u2016W\u0303t,K\u2016 ] = 0, we have\nlim sup t\u2192\u221e\nE [\u2225\u2225Wt\u2225\u2225] \u2264 lim sup t\u2192\u221e E [\u2225\u2225Wt \u2212 W\u0303t,K\u2225\u2225]+lim sup t\u2192\u221e E [\u2225\u2225W\u0303t,K\u2225\u2225] = lim sup t\u2192\u221e E [\u2225\u2225Wt \u2212 W\u0303t,K\u2225\u2225] .\nThus it is sufficient to prove that\nlim K\u2192\u221e lim sup t\u2192\u221e\nE [\u2225\u2225Wt \u2212 W\u0303t,K\u2225\u2225] = 0. (A.23)\nTo this end, let us write Wt+1 \u2212 W\u0303t+1,K = (1\u2212 \u03b1t) ( Wt \u2212 W\u0303t,K ) + \u03b1t ( et \u2212 e\u0303t,K ) \u00b7 \u03c9\u0303t+1.\nSince the number of states and actions is finite, Eq. (A.21) implies that for all t, E [ |\u03c9\u0303t+1| | Ft ] \u2264 L\u2032 for some constant L\u2032 <\u221e. Consequently,\nE [\u2225\u2225Wt+1 \u2212 W\u0303t+1,K\u2225\u2225] \u2264 (1\u2212 \u03b1t) E [\u2225\u2225Wt \u2212 W\u0303t,K\u2225\u2225]+ \u03b1t L\u2032 \u00b7 E [\u2225\u2225et \u2212 e\u0303t,K\u2225\u2225]\n\u2264 (1\u2212 \u03b1t) E [\u2225\u2225Wt \u2212 W\u0303t,K\u2225\u2225]+ \u03b1t L\u2032 \u00b7 LK ,\nwhere the second inequality follows from Prop. A.3(i), and LK ,K \u2265 1, are constants with the property that LK \u2193 0 as K \u2192 \u221e. By Assumption 2.3 on the stepsize, the preceding inequality implies that for each K,\nlim sup t\u2192\u221e\nE [\u2225\u2225Wt \u2212 W\u0303t,K\u2225\u2225] \u2264 L\u2032 \u00b7 LK .\nLetting K go to infinity and using the fact LK \u2193 0, we obtain the desired equality (A.23), which implies limt\u2192\u221e E [\u2016Wt\u2016] = 0 as discussed earlier.\nProof of Prop. 3.1(ii) Note that with \u03b1t = 1/(t+ 1), the convergence Wt a.s.\u2192 0 we want to prove is equivalent to the convergence of the time average, 1t+1 \u2211t k=0 ek \u00b7 \u03c1k\u03c9k+1 a.s.\u2192 0, where each term in the sum is a function of (ek, Sk, Ak, Sk+1, Rk):\nek \u00b7 \u03c1k\u03c9k+1 = ek \u00b7 \u03c1(Sk, Ak) \u00b7 ( Rk \u2212 r(Sk, Ak, SK+1) ) .\nBy Theorem 3.2, the Markov chain {Zt} = {(St, At, et, Ft)} has a unique invariant probability measure \u03b6. Consequently, the Markov chain {Z \u2032t} := {(St, At, et, Ft, St+1, Rt)} has a unique invariant probability measure \u03b6 \u2032, determined by \u03b6 together with the probabilities of the successor state St+1 given (St, At) and the conditional distribution of the rewardRt given (St, At, St+1), which are specified by the MDP model. Let E\u03b6\u2032 denote expectation with respect to the probability distribution of the stationary Markov chain {Z \u2032t} with the initial distribution being \u03b6 \u2032. From Theorem 3.2(ii) and the relation between \u03b6 \u2032 and \u03b6, we have\nE\u03b6\u2032 [ \u2016e0\u2016 \u00b7 \u03c10 |\u03c91| ] \u2264 L\u2032E\u03b6 [ \u2016e0\u2016 ] <\u221e (A.24)\nfor some constant L\u2032 <\u221e. Specifically, in the above, we obtain the first inequality by bounding the conditional expectation of |\u03c91|, conditioned on (e0, S0, A0), by some finite constant [cf. Eq. (A.21)], and we then obtain the second inequality by applying Theorem 3.2(ii).\nGiven the finite expectation in Eq. (A.24), we can apply to the stationary process {Z \u2032t} with initial distribution \u03b6 \u2032 a strong law of large numbers for stationary processes [(Doob, 1953, Chap. X, Theorem 2.1); see also (Meyn and Tweedie, 2009, Theorem 17.1.2)]. By this theorem, there exists a nonempty subset D1 of the state space of {Z \u2032t} such that:\n(i) D1 has \u03b6 \u2032-measure 1, and (ii) for each initial condition Z \u20320 = z\n\u2032 \u2208 D1, {Wt} converges a.s. (with respect to the probability measure induced by the initial condition Z \u20320 = z\n\u2032 for the process {Z \u2032t}). In view of the dependence relations between the variables (St, At, St+1, Rt) given by the MDP model, and also in view of the finiteness of the state-action space S \u00d7 A and the irreducibility property of the behavior policy \u03c0o (Assumption 2.1(ii)), the preceding properties of the set D1 imply that there exists a nonempty subsetD2 of the space Rn+1 (which is the space of (et, Ft)) such that:\n(i) D2 has measure 1 with respect to the marginal on Rn+1 of the invariant probability measure \u03b6, and\n(ii) for each initial condition (e0, F0, S0) = (e, F, s) \u2208 D2 \u00d7 S , {Wt} converges a.s. But the limit of {Wt} cannot differ from 0 by Prop. 3.1(i) proved earlier (since for the given initial condition, E[\u2016Wt\u2016] \u2192 0 implies the existence of a subsequence of {Wt} converging to 0 a.s. (Dudley, 2002, Theorem 9.2.1)). Thus, we conclude that for each initial condition (e, F, s) \u2208 D2 \u00d7 S , Wt\na.s.\u2192 0. Now to establish Prop. 3.1(ii), we only need to show that for any given initial condition (e0, F0) =\n(e, F ) 6\u2208 D2, Wt a.s.\u2192 0 as well. To prove this, let s \u2208 S be an arbitrary given state. Consider the sequence {(et, Ft,Wt)} with (e0, F0) = (e, F ) and S0 = s. Consider also a second sequence {(e\u0303t, F\u0303t, W\u0303t)} which is generated by the same recursion and the same trajectory of states, actions and rewards that define the first sequence {(et, Ft,Wt)}, but with a pair of initial traces\n(e\u03030, F\u03030) = (e\u0303, F\u0303 ) \u2208 D2, possibly different from (e, F ). By what we proved earlier, W\u0303t a.s.\u2192 0. On the other hand, by definition\nWt+1 \u2212 W\u0303t+1 = 1\nt+ 1 t\u2211 k=0 ( ek \u2212 e\u0303k ) \u00b7 \u03c1k\u03c9k+1.\nIn the right-hand side, we have ek \u2212 e\u0303k a.s.\u2192 0 (as k \u2192 \u221e) by Prop. A.2, and we also have\n1 t+1 \u2211t k=0 \u03c1k\u03c9k+1\na.s.\u2192 0 by the properties of {\u03c9t} and (Tsitsiklis, 1994, Lemma 1).17 Consequently, Wt+1 \u2212 W\u0303t+1 a.s.\u2192 0; since W\u0303t a.s.\u2192 0, this implies Wt a.s.\u2192 0. The proof is now complete."}, {"heading": "A.5. Proof of Theorem 2.1 on the Convergence of ELSTD(\u03bb)", "text": "The proof proceeds by calculating the limit G\u2217 in Theorem 3.1 for the two functions h1, h2 in Eq. (3.3): with y = (e, F ) \u2208 Rn+1,\nh1(y, s, a, s \u2032) = e \u00b7\u03c1(s, a) ( \u03b3(s\u2032)\u03c6(s\u2032)>\u2212\u03c6(s)> ) , h2(y, s, a, s \u2032) = e \u00b7\u03c1(s, a) r(s, a, s\u2032), (A.25)\nwhich are associated with the ELSTD(\u03bb) iterates Ct, bt, respectively. Specifically, based on the proof of Theorem 3.1, we first calculate for each K, the limit G\u2217K given in Eq. (A.17), which is associated with the truncated traces (e\u0303t,K , F\u0303t,K). We then take K to\u221e to get the expression of G\u2217 since G\u2217 = limK\u2192\u221eG\u2217K , as shown in the step (ii) of the proof of Theorem 3.1. The details of this calculation are given below, and the subsequent Lemma A.4 establishes that\nG\u2217 = C for h = h1; G\u2217 = b for h = h2. (A.26)\nLet us give now the rest of the proof of Theorem 2.1, assuming for the moment that Eq. (A.26) has been proved. Then, with h = h1, Theorem 3.1 yields the L1-convergence of {Ct} to C, and Theorem 3.3 yields Ct\na.s.\u2192 C for stepsizes \u03b1t = 1/(t+ 1). For the iterates {bt} [cf. Eq. (2.11)], we also need to take care of the noise in the rewards Rt, by\nusing Prop. 3.1. Specifically, with W0 = 0, let\n\u03c9t+1 = Rt \u2212 r(St, At, St+1), Wt+1 = (1\u2212 \u03b1t)Wt + \u03b1t et \u03c1t \u00b7 \u03c9t+1, t \u2265 0,\n[cf. Eq. (3.4)]. By definition, bt+1 = (1\u2212 \u03b1t) bt + \u03b1t et \u00b7 \u03c1tRt = (1\u2212 \u03b1t) bt + \u03b1t et \u00b7 \u03c1t ( r(St, At, St+1) + \u03c9t+1 ) ,\nso the iteration for {bt} can be equivalently expressed as\nbt+1 = Gt+1 +Wt+1,\n17. Specifically, we write Xt+1 := 1t+1 \u2211t k=0 \u03c1k\u03c9k+1 equivalently as the recursion,\nXt+1 = (1\u2212 1t+1 )Xt + 1 t+1 \u03c1t\u03c9t+1 with X0 = 0,\nand apply (Tsitsiklis, 1994, Lemma 1) to obtain Xt a.s.\u2192 0. To apply the latter lemma, observe that since S andA are finite spaces, \u03c1t is bounded for all t, and the weighted noise variables {\u03c1t\u03c9t+1} thus have conditional zero mean and uniformly bounded variances, conditioned on Ft, similar to the properties of {\u03c9t} shown in Eq. (A.21). Then, the conditions of (Tsitsiklis, 1994, Lemma 1) are satisfied and its conclusion applies.\nwhereGt+1 is given by the recursion (3.2) with h = h2 andG0 = b0, andWt+1 is as defined above. Then by Theorem 3.1, Eq. (A.26) and Prop. 3.1(i), we have\nlim t\u2192\u221e\nE [\u2225\u2225bt \u2212 b\u2225\u2225] \u2264 lim t\u2192\u221e E [\u2225\u2225Gt \u2212G\u2217\u2225\u2225]+ lim t\u2192\u221e E [\u2225\u2225Wt\u2225\u2225] = 0.\nThis proves the L1-convergence of {bt} to b. Similarly, its a.s. convergence in the second part of Theorem 2.1 follows from Theorem 3.3, Eq. (A.26) and Prop. 3.1(ii) as\nGt a.s.\u2192 G\u2217 = b and Wt a.s.\u2192 0 =\u21d2 bt = Gt +Wt a.s.\u2192 b.\nThus Theorem 2.1 is proved. In the rest of this subsection, we verify Eq. (A.26), which we used in the proof above."}, {"heading": "Computing the Limiting Matrix and Vector for ELSTD(\u03bb)", "text": "The desired limits for ELSTD(\u03bb) are the matrix C and vector b given in Eqs. (2.6)-(2.9), Section 2:\nC = \u2212\u03a6>M\u0304 (I \u2212 P \u03bb\u03c0,\u03b3) \u03a6 = \u2212\u03a6>M\u0304 (I \u2212 P\u03c0\u0393\u039b)\u22121 (I \u2212 P\u03c0\u0393) \u03a6, (A.27)\nb = \u03a6>M\u0304 r\u03bb\u03c0,\u03b3 = \u03a6 >M\u0304 (I \u2212 P\u03c0\u0393\u039b)\u22121 r\u03c0, (A.28)\nwhere M\u0304 is a diagonal matrix with\ndiag(M\u0304) = d>\u03c0o,i (I \u2212 P \u03bb\u03c0,\u03b3)\u22121, d\u03c0o,i \u2208 RN , d\u03c0o,i(s) = d\u03c0o(s) \u00b7 i(s), s \u2208 S,\nand d\u03c0o(s) is the steady state probability of state s under the behavior policy \u03c0o (cf. Assumption 2.1(ii)), and i(s) is the \u201cinterest\u201d weight for state s. By the definition (2.6) of P \u03bb\u03c0,\u03b3 , we can also write\ndiag(M\u0304) = d>\u03c0o,i (I \u2212 P\u03c0\u0393)\u22121 (I \u2212 P\u03c0\u0393\u039b). (A.29)\nRecall that E\u03b6 denotes expectation with respect to the stationary Markov chain {Zt}, where Zt = (St, At, et, Ft), with its unique invariant probability measure \u03b6 as the initial distribution (cf. Theorem 3.2). We denote Yt = (et, Ft).\nLemma A.4 Under Assumption 2.1,\nE\u03b6 [ h1(Y0, S0, A0, S1) ] = C, E\u03b6 [ h2(Y0, S0, A0, S1) ] = b.\nProof The proof proceeds as follows. We first calculate the limit vectorG\u2217K defined in Eq. (A.17) in the proof of Theorem 3.1, for the two choices of the function h given in Eq. (A.25): h = h1, h = h2. We then calculate G\u2217 by its definition given in the proof of Theorem 3.1: G\u2217 = limK\u2192\u221eG\u2217K . Theorem 3.3 shows E\u03b6 [ h(Y0, S0, A0, S1) ] = G\u2217, so the lemma follows if we prove that G\u2217 = C for h = h1, and G\u2217 = b for h = h2. Let E0 denotes expectation with respect to the probability measure of the stationary Markov chain {(St, At)}. Recall that for each K \u2265 1, G\u2217K is defined by Eq. (A.17) as\nG\u2217K = E0 [ h(Yt,K , St, At, St+1) ] , \u2200 t > 2K, (A.30)\nwhere Yt,K = (e\u0303t,K , F\u0303t,K) are the truncated traces defined together with M\u0303t,K in Eqs. (A.13)(A.15). To simplify the calculation of G\u2217K , we first calculate E0 [ M\u0303t,K\u03a8(St) ] for any given t > K and any matrix or vector-valued function \u03a8 on S. By Eqs. (A.13)-(A.14),\nM\u0303t,K = \u03bbt i(St) + (1\u2212 \u03bbt) F\u0303t,K , F\u0303t,K = t\u2211\nk=t\u2212K i(Sk) \u00b7\n( \u03c1k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) .\nThus, M\u0303t,K can be equivalently expressed as\nM\u0303t,K = i(St) + K\u2211 k=1 i(St\u2212k) \u00b7 ( \u03c1t\u2212k\u03b3t\u2212k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) \u00b7 (1\u2212 \u03bbt).\nTo calculate E0 [ M\u0303t,K\u03a8(St) ] , we calculate the expectation for each term in the above summation separately. In what follows, for each s \u2208 S , let 1s(\u00b7) denote the indicator for state s. For an expression H that results in an N -dimensional vector, we write (H)(s) for the s-th entry of the resulting vector. Under Assumption 2.1(ii), we have\nE0 [ i(St) \u00b7 1s(St) ] = d\u03c0o(s) i(s) = ( d > \u03c0o,i I)(s),\nand for k = 1, 2, . . . ,K,\nE0 [ i(St\u2212k) \u00b7 ( \u03c1t\u2212k\u03b3t\u2212k+1 \u00b7 \u00b7 \u00b7 \u03c1t\u22121\u03b3t ) \u00b7 (1\u2212 \u03bbt) \u00b7 1s(St) ] = ( d>\u03c0o,i(P\u03c0\u0393) k(I \u2212 \u039b) ) (s).\nHence\nE0 [ M\u0303t,K \u00b7 1s(St) ] = ( d>\u03c0o,i ( I + K\u2211 k=1 (P\u03c0\u0393) k(I \u2212 \u039b) )) (s),\nand consequently,\nE0 [ M\u0303t,K \u00b7\u03a8(St) ] = \u2211 s\u2208S E0 [ M\u0303t,K \u00b7 1s(St) ] \u00b7\u03a8(s)\n= \u2211 s\u2208S ( d>\u03c0o,i ( I + K\u2211 k=1 (P\u03c0\u0393) k(I \u2212 \u039b) )) (s) \u00b7\u03a8(s). (A.31)\nLet us now calculateG\u2217K for h = h1 or h2 simultaneously, using Eq. (A.30) and the expressions of h1, h2 given in Eq. (A.25). Let t > 2K, and let Ft = \u03c3(S0, A0, . . . , St). For the term appearing after e in the expression of h1, we have\nE0 [ \u03c1t ( \u03b3t+1\u03c6(St+1) > \u2212 \u03c6(St)> ) | Ft ] = \u03a81(St),\nwhere \u03a81 maps each s to the s-th row of the matrix (P\u03c0\u0393\u2212 I)\u03a6. For the term appearing after e in the expression of h2, we have\nE0 [ \u03c1t r(St, At, St+1) | Ft ] = \u03a82(St),\nwhere \u03a82 maps each s to the s-th entry of the vector r\u03c0. Denote \u03a8o1 = (P\u03c0\u0393\u2212 I)\u03a6,\u03a8o2 = r\u03c0. Corresponding to h = h1 or h2, let \u03a8 = \u03a81 or \u03a82, and let \u03a8o = \u03a8o1 or \u03a8 o 2, respectively. Then, by Eqs. (A.30) and (A.25), we have\nG\u2217K = E0 [ E0[h(Yt,K , St, At, St+1) | Ft ] ]\n= E0 [ e\u0303t,K \u00b7\u03a8(St) ]\n= E0\n[ t\u2211\nk=t\u2212K M\u0303k,K \u00b7 \u03c6(Sk) \u00b7 (\u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t) \u00b7\u03a8(St)\n] (A.32)\n= t\u2211 k=t\u2212K E0 [ M\u0303k,K \u00b7 \u03c6(Sk) \u00b7 E0 [ (\u03b2k+1 \u00b7 \u00b7 \u00b7\u03b2t) \u00b7\u03a8(St) | Fk ] ]\n= t\u2211 k=t\u2212K E0 [ M\u0303k,K \u00b7 \u03c6(Sk) \u00b7 ( (P\u03c0\u0393\u039b) t\u2212k\u03a8o ) r (Sk) ] . (A.33)\nIn the above we used the definition (A.15) of e\u0303t,K in Eq. (A.32), and in Eq. (A.33), the term (\u00b7 \u00b7 \u00b7 )r(Sk) inside the expectation denotes the Sk-th row of the matrix or vector given by the expression inside the parentheses (\u00b7 \u00b7 \u00b7 ). We shall also use this notational convention in the proof below.\nFrom Eq. (A.33), using the fact that the expectation is with respect to the stationary Markov chain {(St, At)}, we obtain\nG\u2217K = t\u2211 k=t\u2212K E0 [ M\u0303t,K \u00b7 \u03c6(St) \u00b7 ( (P\u03c0\u0393\u039b) t\u2212k\u03a8o ) r (St) ]\n= E0 [ M\u0303t,K \u00b7 \u03c6(St) \u00b7 ( K\u2211 k=0 (P\u03c0\u0393\u039b) k \u00b7\u03a8o ) r (St) ] . (A.34)\nCorresponding to the last two terms inside the expectation above, define a function \u03a8K on S by\n\u03a8K(s) = \u03c6(s) \u00b7 ( K\u2211 k=0 (P\u03c0\u0393\u039b) k \u00b7\u03a8o ) r (s), s \u2208 S.\nThen by combining Eq. (A.34) with (A.31), we have\nG\u2217K = E0 [ M\u0303t,K \u00b7\u03a8K(St) ] = \u2211 s\u2208S ( d>\u03c0o,i ( I + K\u2211 k=1 (P\u03c0\u0393) k(I \u2212 \u039b) )) (s) \u00b7\u03a8K(s). (A.35)\nWe now take K to infinity to get the expression of G\u2217. For h = h1, \u03a8o in the definition of \u03a8K is given by \u03a8o = (P\u03c0\u0393\u2212 I)\u03a6. Using the equality relations,\nI + \u221e\u2211 k=1 (P\u03c0\u0393) k(I \u2212 \u039b) = (I \u2212 P\u03c0\u0393)\u22121(I \u2212 P\u03c0\u0393\u039b),\n\u221e\u2211 k=0 (P\u03c0\u0393\u039b) k = (I \u2212 P\u03c0\u0393\u039b)\u22121,\nwe obtain from the expression (A.35) of G\u2217K that\nG\u2217 = \u2211 s\u2208S ( d>\u03c0o,i(I \u2212 P\u03c0\u0393)\u22121(I \u2212 P\u03c0\u0393\u039b) ) (s) \u00b7 \u03c6(s) \u00b7 ( (I \u2212 P\u03c0\u0393\u039b)\u22121 \u00b7 (P\u03c0\u0393\u2212 I)\u03a6 ) r (s)\n= \u03a6>M\u0304 (I \u2212 P\u03c0\u0393\u039b)\u22121 (P\u03c0\u0393\u2212 I) \u03a6 = C,\nwhere the second equality follows from the expression (A.29) for diag(M\u0304), and the third equality follows from Eq. (A.27). For h = h2, \u03a8o in the definition of \u03a8K is given by \u03a8o = r\u03c0, and a similar calculation gives\nG\u2217 = \u2211 s\u2208S ( d>\u03c0o,i(I \u2212 P\u03c0\u0393)\u22121(I \u2212 P\u03c0\u0393\u039b) ) (s) \u00b7 \u03c6(s) \u00b7 ( (I \u2212 P\u03c0\u0393\u039b)\u22121 \u00b7 r\u03c0 ) r (s)\n= \u03a6>M\u0304 (I \u2212 P\u03c0\u0393\u039b)\u22121 r\u03c0 = b,\nwhere the last equality follows from Eq. (A.28)."}, {"heading": "A.6. Related Result: Alternative Proof of Existence of an Invariant Probability Measure", "text": "Consider the Markov chain {Zt}, where Zt = (St, At, et, Ft). In Section 3, we used, among others, the weak Feller property of the Markov chain {Zt} to establish the existence of at least one invariant probability measure for {Zt} (the property (iv) in Section 3.1). We now give an alternative proof for this statement, by constructing directly an invariant probability measure. This proof is similar to that of (Yu, 2012, Lemma 4.2), and it was motivated by an analysis of the LSTD(1) algorithm by Meyn (2008, Chap. 11.5.2). The proof will also yield directly that under that invariant probability measure,\n\u2225\u2225(e0, F0)\u2225\u2225 has a finite expectation, which was established in Theorem 3.2(ii) earlier by using different arguments.\nProposition A.4 Under Assumption 2.1, the Markov chain {Zt} has at least one invariant probability measure \u03b6 with E\u03b6 [\u2225\u2225(e0, F0)\u2225\u2225] <\u221e. Proof Consider a double-ended stationary Markov chain {(St, At) | \u2212\u221e < t <\u221e} with transition probability matrix P\u03c0o and probability distribution Po. In this proof, let E0 denote expectation with respect to Po. Let Xt = ( (St, At), (St\u22121, At\u22121), . . . ) , and denote by PX the probability distribution of Xt, which is a probability measure on (S \u00d7 A)\u221e and is the same for all t due to stationarity.\nWe will first define two functions, f : (S \u00d7A)\u221e \u2192 R+ and \u03c8 : (S \u00d7A)\u221e \u2192 Rn, which relate to the traces F, e, respectively. We will then show that the distribution of ( S0, A0, \u03c8(X0), f(X0) ) is an invariant probability measure of {Zt}.\nLet us introduce some notation. For x \u2208 (S \u00d7 A)\u221e, we index the components of x as x = ( (s0, a0), (s\u22121, a\u22121), . . . ) , and we denote by x(\u2212k) the tail of x starting from s\u2212k, i.e.,\nx(\u2212k) = ( (s\u2212k, a\u2212k), (s\u2212k\u22121, a\u2212k\u22121), . . . ) . Recall that \u03b2t = \u03c1t\u22121\u03b3t\u03bbt. Correspondingly, we define a function \u03b2 : S \u00d7A\u00d7 S \u2192 R+ by\n\u03b2(s, a, s\u2032) = \u03c1(s, a) \u03b3(s\u2032)\u03bb(s\u2032).\nFor an expression H that results in a vector in RN , we write (H)(s) for the s-th entry of that vector.\nWe now define the function f .18 Since\n\u221e\u2211 k=0 E0 [ i(S\u2212k) \u00b7 ( \u03c1\u2212k\u03b3\u2212k+1 \u00b7 \u00b7 \u00b7 \u03c1\u22121\u03b30 )] = \u221e\u2211 k=0 E0 [ i(S\u2212k) \u00b7 ( (P\u03c0\u0393) k1 ) (S\u2212k) ] = d>\u03c0o,i\n\u221e\u2211 k=0 (P\u03c0\u0393) k1 <\u221e, (A.36)\nwe can define a nonnegative real-valued measurable function f on (S \u00d7A)\u221e such that\nf(x) =\n{\u2211\u221e k=0 i(s\u2212k) \u00b7 ( \u03c1(s\u2212k, a\u2212k)\u03b3(s\u2212k+1) \u00b7 \u00b7 \u00b7 \u03c1(s\u22121, a\u22121)\u03b3(s0) ) , if x \u2208 D1,\n0, otherwise,\nwhere D1 is a subset of (S \u00d7A)\u221e with PX(D1) = 1. By Eq. (A.36),\u222b f(x)PX(dx) = E0 [ f(X0)\n] = \u221e\u2211 k=0 E0 [ i(S\u2212k) \u00b7 ( \u03c1\u2212k\u03b3\u2212k+1 \u00b7 \u00b7 \u00b7 \u03c1\u22121\u03b30 )] <\u221e.\nWe now define the function \u03c8. Define two constants L\u2032, L as follows. Let L\u2032 = E0 [ f(X0) ] ;\nequivalently, L\u2032 = E0 [ f(X\u2212k) ] for all k by stationarity. Let L \u2265 max { i(s), \u2016\u03c6(s)\u2016 } for all s \u2208 S. By taking conditional expectation similarly to the proof of Lemma A.2, we have the following bound:\n\u221e\u2211 k=0 E0 [\u2225\u2225\u2225(\u03bb\u2212k \u00b7 i(S\u2212k) + (1\u2212 \u03bb\u2212k)f(X\u2212k)) \u00b7 \u03c6(S\u2212k) \u00b7 (\u03b2\u2212k+1 \u00b7 \u00b7 \u00b7\u03b20)\u2225\u2225\u2225]\n= \u221e\u2211 k=0 E0 [( \u03bb\u2212k \u00b7 i(S\u2212k) + (1\u2212 \u03bb\u2212k)f(X\u2212k) ) \u00b7 \u2225\u2225\u03c6(S\u2212k)\u2225\u2225 \u00b7 (\u03b2\u2212k+1 \u00b7 \u00b7 \u00b7\u03b20)]\n\u2264 (L+ L\u2032) \u00b7 L \u00b7 \u221e\u2211 k=0 1>(P\u03c0\u0393\u039b) k1 <\u221e.\nTherefore, by a theorem on integration (Rudin, 1966, Theorem 1.38, p. 28-29), we can define a measurable function \u03c8 on (S \u00d7A)\u221e such that the following hold:\n(i) on a set D2 \u2282 (S \u00d7A)\u221e with PX(D2) = 1,\n\u03c8(x) = \u221e\u2211 k=0 ( \u03bb(s\u2212k) i(s\u2212k) + (1\u2212 \u03bb(s\u2212k)) f(x(\u2212k)) ) \u00b7 \u03c6(s\u2212k)\n\u00b7 ( \u03b2(s\u2212k, a\u2212k, s\u2212k+1) \u00b7 \u00b7 \u00b7\u03b2(s\u22121, a\u22121, s0) ) ,\nwhere the infinite series on the right-hand side converges to a vector in Rn; (ii) outside D2, \u03c8(x) = 0; and\n18. We note that to gain intuition about the proof, it will be helpful to compare our definition of f with the expression of Ft in Eq. (A.7), and compare our subsequent definition of \u03c8 with the expression of et in Eq. (A.8).\n(iii) \u03c8(x) is integrable with\nE0 [ \u2016\u03c8(X0)\u2016 ] = \u222b \u2016\u03c8(x)\u2016PX(dx) <\u221e\nand\u222b \u03c8(x)PX(dx) = \u221e\u2211 k=0 E0 [( \u03bb\u2212ki(S\u2212k) + (1\u2212 \u03bb\u2212k)f(X\u2212k) ) \u00b7 \u03c6(S\u2212k) \u00b7 ( \u03b2\u2212k+1 \u00b7 \u00b7 \u00b7\u03b20 )] .\nLet Y o0 = (\u03c8(X0), f(X0)). We now show that the probability distribution of (S0, A0, Y o 0 ) is an invariant probability measure of the Markov chain {Zt}. To this end, considerX1 = ( (S1, A1), X0 ) , and let us define Y o1 = (e o 1, F o 1 ) based on Y o 0 and (S0, A0, S1), using the same recursion that defines (et, Ft) [cf. Eqs. (2.2)-(2.4)]:\nF o1 = \u03b31 \u03c10 \u00b7 f(X0) + i(S1), eo1 = \u03b21 \u03c8(X0) + ( \u03bb1 i(S1) + (1\u2212 \u03bb1)F o1 ) \u00b7 \u03c6(S1).\nIf (S0, A0, Y o0 ) and (S1, A1, Y o 1 ) have the same distribution, then this distribution must be an invariant probability measure of {Zt} because the stochastic kernel that governs the transition from (S0, A0, Y o0 ) to (S1, A1, Y o 1 ) is the same as that from Z0 = ( S0, A0, (e0, F0) ) to Z1 =(\nS1, A1, (e1, F1) ) .\nNow define a set D \u2282 (S \u00d7A)\u221e by D = D1 \u2229D2 \u2229 ( S \u00d7A\u00d7 (D1 \u2229D2) ) , where D1, D2 are the sets in the definitions of the functions f and \u03c8, respectively. Since PX(D1) = PX(D2) = 1, we have\nPX(D) = P o(X1 \u2208 D) = Po(X1 \u2208 D1 \u2229D2, X0 \u2208 D1 \u2229D2) = 1.\nConsider the case X1 \u2208 D. Then both X0, X1 \u2208 D1 \u2229D2. By the definition of f on D1, it follows that\nF o1 = i(S1) + \u221e\u2211 k=0 i(S\u2212k) \u00b7 ( \u03c1\u2212k\u03b3\u2212k+1 \u00b7 \u00b7 \u00b7 \u03c1\u22121\u03b30 ) \u00b7 \u03c10\u03b31 = f(X1),\nand from this and the definition of \u03c8 on D2, it also follows that eo1 = ( \u03bb1 i(S1) + (1\u2212 \u03bb1) f(X1) ) \u00b7 \u03c6(S1)\n+ \u221e\u2211 k=0 ( \u03bb\u2212k \u00b7 i(S\u2212k) + (1\u2212 \u03bb\u2212k)f(X\u2212k) ) \u00b7 \u03c6(S\u2212k) \u00b7 ( \u03b2\u2212k+1 \u00b7 \u00b7 \u00b7\u03b20 ) \u00b7 \u03b21\n= \u03c8(X1). By stationarity, ( S0, A0, \u03c8(X0), f(X0) ) and (S1, A1, \u03c8(X1), f(X1) ) have the same distribution. Denote this distribution by \u03b6. Since (eo1, F o 1 ) differs from ( \u03c8(X1), f(X1) ) only when X1 6\u2208 D, an event with Po-probability 0, we conclude that (S0, A0, Y o0 ) and (S1, A1, Y o\n1 ) have the same distribution \u03b6, which is an invariant probability measure of {Zt} as discussed earlier. Then, from the integrability property of \u03c8 and f shown earlier, we have\nE\u03b6 [\u2225\u2225(e0, F0)\u2225\u2225] \u2264 E\u03b6[\u2225\u2225e0\u2225\u2225]+ E\u03b6[F0] = E0[\u2225\u2225\u03c8(X0)\u2225\u2225]+ E0[f(X0)] <\u221e.\nThis completes the proof."}, {"heading": "Appendix B. Proofs for Section 4", "text": "In this appendix we prove Lemma 4.1 and Theorem 4.1 for the constrained ETD(\u03bb) algorithm (4.5). We will restate both theorems for convenience.\nRecall that the constrained ETD(\u03bb) calculates \u03b8t, t \u2265 0, all restricted to be in a closed ball with radius r, B = {\u03b8 \u2208 Rn | \u2016\u03b8\u20162 \u2264 r}, according to\n\u03b8t+1 = \u03a0B ( \u03b8t + \u03b1t h(\u03b8t, \u03bet) + \u03b1t et \u00b7 \u03c9\u0303t+1 ) ,\nwhere \u03c9\u0303t+1 = \u03c1t ( Rt\u2212r(St, At, St+1) ) is noise, \u03bet = (et, St, At, St+1), and the function h is given by Eq. (4.3) as\nh(\u03b8, \u03be) = e \u00b7 \u03c1(s, a) ( r(s, a, s\u2032) + \u03b3(s\u2032)\u03c6(s\u2032)>\u03b8 \u2212 \u03c6(s)>\u03b8 ) , for \u03be = (e, s, a, s\u2032).\nThe \u201cmean ODE\u201d associated with this algorithm is the projected ODE (4.6):\nx\u0307 = h\u0304(x) + z, z \u2208 \u2212NB(x),\nwhere h\u0304(x) = Cx + b, NB(x) is the normal cone of B at x, and z is the boundary reflection term that keeps the solution in B (Kushner and Yin, 2003). The solution of h\u0304(x) = 0 is denoted \u03b8\u2217; i.e., \u03b8\u2217 = \u2212C\u22121b.\nLemma 4.1 Let c > 0 be such that x>Cx \u2264 \u2212c\u2016x\u201622 for all x \u2208 Rn. Suppose B has a radius r > \u2016b\u20162/c. Then \u03b8\u2217 lies in the interior of B, and the only solution x(t), t \u2208 (\u2212\u221e,+\u221e), of the projected ODE (4.6) in B is x(\u00b7) \u2261 \u03b8\u2217.\nProof By the definition of \u03b8\u2217, C\u03b8\u2217 + b = 0. Therefore,\n0 = \u3008\u03b8\u2217, C\u03b8\u2217 + b\u3009 = \u3008\u03b8\u2217, C\u03b8\u2217\u3009+ \u3008\u03b8\u2217, b\u3009 \u2264 \u2212c\u2016\u03b8\u2217\u201622 + \u2016b\u20162\u2016\u03b8\u2217\u20162,\nwhich implies \u2016\u03b8\u2217\u20162 \u2264 b\u20162/c < r, i.e., \u03b8\u2217 lies in the interior of B. For a point x on the boundary of B, \u2016x\u20162 = r and the normal cone NB(x) = {ax | a \u2265 0}. Since r > \u2016b\u20162/c, we have\n\u3008x, h\u0304(x)\u3009 = \u3008x,Cx\u3009+ \u3008x, b\u3009 \u2264 \u2212c\u2016x\u201622 + \u2016x\u20162\u2016b\u20162 = r (\u2212c r + \u2016b\u20162) < 0.\nThis shows that for any x on the boundary of B, h\u0304(x) points inside B and hence at x, the boundary reflection term z \u2208 \u2212NB(x) that keeps the solution in B is the zero vector. Consequently, any solution of the projected ODE (4.6) in B is a solution of the ODE (4.4), which is x(\u00b7) \u2261 \u03b8\u2217.\nNext we prove Theorem 4.1.\nTheorem 4.1 (Almost sure convergence of constrained ETD(\u03bb)) Let Assumptions 2.1-2.3 hold. Let {\u03b8t} be the sequence generated by the constrained ETD(\u03bb) algorithm (4.5) with stepsizes satisfying \u03b1t = O(1/t) and\n\u03b1t\u2212\u03b1t+1 \u03b1t = O(1/t), and with the radius r of B exceeding the threshold\ngiven in Lemma 4.1. Then, for any given initial (e0, F0, \u03b80), \u03b8t a.s.\u2192 \u03b8\u2217.\nProof The desired conclusions will follow immediately from (Kushner and Yin, 2003, Theorem 6.1.1) and Lemma 4.1, if we can show that the conditions of (Kushner and Yin, 2003, Theorem 6.1.1) are met. Relevant here are the conditions A.6.1.1-A.6.1.4 and A.6.1.6-A.6.1.7 in (Kushner and Yin, 2003, p. 165). We first adapt these six conditions to our problem, and by using stronger forms of the conditions A.6.1.6-A.6.1.7 given in (Kushner and Yin, 2003, Eq. (6.1.10), p. 166), we obtain the conditions (i)-(vi) below.\nThe first two conditions are for the functions h, h\u0304 [cf. Eqs. (4.3), (4.4)] and the noise {\u03c9\u0303t}: (i) supt\u22650 E [ \u2016h(\u03b8t, \u03bet) + et \u00b7 \u03c9\u0303t+1\u2016 ] <\u221e.\n(ii) h\u0304(\u03b8) is continuous, and h(\u03b8, \u03be) is continuous in \u03b8 for each \u03be. Condition (i) is satisfied here. Indeed, we have supt\u22650 E [ \u2016h(\u03b8t, \u03bet)\u2016 ] < \u221e, in view of Prop. A.1, the Lipschitz continuity of h in e, and the fact that \u2016\u03b8t\u20162 \u2264 r for all t by the definition of the constrained algorithm. Since the rewards Rt have bounded variances by assumption and the noise variable \u03c9\u0303t+1 = \u03c1t ( Rt \u2212 r(St, At, St+1) ) by definition, we can bound E [ |\u03c9\u0303t+1| | Ft ] by some\nconstant for all t, where Ft = \u03c3(S0, A0, . . . , St+1), and consequently, we also have supt\u22650 E [ \u2016et \u00b7\n\u03c9\u0303t+1\u2016 ] <\u221e by Prop. A.1. Hence condition (i) holds. Condition (ii) is also clearly satisfied here.\nThe four remaining conditions to be introduced are of the same type and relate to the asymptotic rate of change conditions introduced by (Kushner and Clark, 1978). These conditions can guarantee that the effects caused by the noises \u03c9\u0303t+1 or by the discrepancies between h and h\u0304 asymptotically \u201caverage out\u201d so that the desired convergence can take place.\nFor any real T \u2032 > 0, define integerm(T \u2032) = min{t \u2265 0 | \u2211t\nk=0 \u03b1k > T \u2032}. Conditions (iii)-(vi)\nbelow are required to hold for each a \u2265 0 and some T > 0 (here a and T are real numbers): (iii) For each \u03b8,\nlim t\u2192\u221e P supj\u2265t max0\u2264T \u2032\u2264T \u2225\u2225\u2225\u2225\u2225\u2225 m(jT+T \u2032)\u22121\u2211 k=m(jT ) \u03b1k ( h(\u03b8, \u03bek)\u2212 h\u0304(\u03b8) )\u2225\u2225\u2225\u2225\u2225\u2225 \u2265 a  = 0. (B.1)\n(iv)\nlim t\u2192\u221e P supj\u2265t max0\u2264T \u2032\u2264T \u2225\u2225\u2225\u2225\u2225\u2225 m(jT+T \u2032)\u22121\u2211 k=m(jT ) \u03b1k ek \u00b7 \u03c9\u0303k+1 \u2225\u2225\u2225\u2225\u2225\u2225 \u2265 a  = 0. (B.2)\n(v) There exist nonnegative measurable functions g1(\u03b8), g2(\u03be) such that\n\u2016h(\u03b8, \u03be)\u2016 \u2264 g1(\u03b8) g2(\u03be),\nwhere g1 is bounded on each bounded set of \u03b8, and g2 satisfies that supt\u22650 E [ g2(\u03bet) ] < \u221e and\nlim t\u2192\u221e P supj\u2265t max0\u2264T \u2032\u2264T \u2223\u2223\u2223\u2223\u2223\u2223 m(jT+T \u2032)\u22121\u2211 k=m(jT ) \u03b1k ( g2(\u03bek)\u2212 E [ g2(\u03bek) ])\u2223\u2223\u2223\u2223\u2223\u2223 \u2265 a  = 0. (B.3)\n(vi) There exist nonnegative measurable functions g3(\u03b8), g4(\u03be) such that for each \u03b8, \u03b8\u2032,\n\u2016h(\u03b8, \u03be)\u2212 h(\u03b8\u2032, \u03be)\u2016 \u2264 g3(\u03b8 \u2212 \u03b8\u2032) g4(\u03be),\nwhere g3 is bounded on each bounded set of \u03b8, with g3(\u03b8)\u2192 0 as \u03b8 \u2192 0, and g4 satisfies that supt\u22650 E [ g4(\u03bet) ] <\u221e and\nlim t\u2192\u221e P supj\u2265t max0\u2264T \u2032\u2264T \u2223\u2223\u2223\u2223\u2223\u2223 m(jT+T \u2032)\u22121\u2211 k=m(jT ) \u03b1k ( g4(\u03bek)\u2212 E [ g4(\u03bek) ])\u2223\u2223\u2223\u2223\u2223\u2223 \u2265 a  = 0. (B.4)\nOne method given in (Kushner and Yin, 2003, Chap. 6.2, p. 170-171) of verifying the conditions (B.1)-(B.4) above is to show that a strong law of large numbers hold for the processes involved. In particular, let \u03c8k represent h(\u03b8, \u03bek)\u2212 h\u0304(\u03b8) for condition (iii), ek \u00b7 \u03c9\u0303k+1 for condition (iv), g2(\u03bek)\u2212 E [ g2(\u03bek) ] for condition (v), and g4(\u03bek)\u2212 E [ g4(\u03bek) ] for condition (vi). If\n1\nt+ 1 t\u2211 k=0 \u03c8k a.s.\u2192 0 (B.5)\nfor the respective {\u03c8k}, then the conditions (B.1)-(B.4) hold for stepsizes satisfying \u03b1t = O(1/t) and \u03b1t\u2212\u03b1t+1\u03b1t = O(1/t) (see Kushner and Yin, 2003, Example 6.1, p. 171).\nWe now apply the convergence results given earlier in this paper to show that the desired convergence (B.5) holds for the processes involved in conditions (iii)-(vi). In particular, for each fixed \u03b8, the almost sure convergence part of Theorem 2.1 implies that\n1\nt+ 1 t\u2211 k=0 h(\u03b8, \u03bek) a.s.\u2192 E\u03b6 [ h(\u03b8, \u03be0) ] = h\u0304(\u03b8).\nThus, condition (iii) holds, as just discussed. By Prop. 3.1(ii), 1t+1 \u2211t k=0 ek \u00b7 \u03c9\u0303k+1 a.s.\u2192 0, so condition (iv) is also met. We verify now conditions (v)-(vi). For condition (v), we take g1(\u03b8) = \u2016\u03b8\u2016 + 1, and we bound the function h by\n\u2016h(\u03b8, \u03be)\u2016 \u2264 ( \u2016\u03b8\u2016+ 1 ) g2(\u03be), where g2(\u03be) = L\u2016e\u2016,\nand L > 0 is some constant. (This bound can be verified directly using the expression of h and the fact that the sets S and A are finite.) Similarly, for condition (vi), we take g3(\u03b8) = \u2016\u03b8\u2016, and we bound the change in h(\u03b8, \u03be) in terms of the change in \u03b8 as follows: for any \u03b8, \u03b8\u2032 \u2208 Rn,\u2225\u2225h(\u03b8, \u03be)\u2212 h(\u03b8\u2032, \u03be)\u2225\u2225 \u2264 \u2016\u03b8 \u2212 \u03b8\u2032\u2016 g4(\u03be), where g4(\u03be) = L\u2032\u2016e\u2016, and L\u2032 > 0 is some constant. Now the functions g2, g4 are Lipschitz continuous in e. Hence, for j = 2, 4, it follows from Prop. A.1 that supt\u22650 E [ gj(\u03bet) ] < \u221e, and it follows from Theorems 3.3 and 3.1 that\n1\nt+ 1 t\u2211 k=0 gj(\u03bek) a.s.\u2192 E\u03b6 [ gj(\u03be0) ] , and\n1\nt+ 1 t\u2211 k=0 E [ gj(\u03bek) ] \u2192 E\u03b6 [ gj(\u03be0) ] , as t\u2192\u221e.\nThe preceding two relations imply the desired convergence:\n1\nt+ 1 t\u2211 k=0 ( gj(\u03bek)\u2212 E [ gj(\u03bek) ]) a.s.\u2192 0, j = 2, 4.\nThis shows that conditions (v)-(vi) are met. The theorem now follows by combining (Kushner and Yin, 2003, Theorem 6.1.1) with the characterization of the solution of the projected ODE (4.6) given by Lemma 4.1, using the fact that under Assumptions 2.1 and 2.2, the matrix C is negative definite (Prop. C.1)."}, {"heading": "Appendix C. Negative Definiteness of the Matrix C", "text": "In this appendix we prove a necessary and sufficient condition (Prop. C.2 below) for the matrix C associated with ETD(\u03bb) to be negative definite. Recall from Eqs. (2.8)-(2.9) that\nC = \u2212\u03a6> M\u0304(I \u2212 P \u03bb\u03c0,\u03b3) \u03a6\nwhere \u03a6 is the feature matrix with full column rank, P \u03bb\u03c0,\u03b3 is a substochatic matrix, and M\u0304 is a nonnegative diagonal matrix with its diagonal, diag(M\u0304), given by\ndiag(M\u0304) = d>\u03c0o,i(I \u2212 P \u03bb\u03c0,\u03b3)\u22121, d>\u03c0o,i = ( d\u03c0o(1) i(1), . . . , d\u03c0o(N) i(N) ) .\nHere Assumption 2.1 is in force and ensures that (I \u2212 P \u03bb\u03c0,\u03b3)\u22121 exists and d\u03c0o(s) > 0 for all s \u2208 S. The negative definiteness of C is important for the a.s. convergence of ETD(\u03bb). It is known to hold if i(s) > 0 for all s \u2208 S (Sutton et al., 2015). In general, C is always negative semidefinite for nonnegative i(\u00b7), and thus C is negative definite whenever it is nonsingular.\nIn what follows, we first include a proof of the fact just mentioned, for completeness (see Prop. C.1). We then give explicitly a condition on the approximation subspace which we will prove to be equivalent to the nonsingularity/negative definiteness of C (Prop. C.2). We also show, by specializing this subspace condition, that if those states s of interest (i.e., i(s) > 0) are represented by features \u03c6(s) that are rich enough, then C can be made negative definite, without knowledge of the model (See Cor. C.1, Remark C.2). In addition, we discuss the connection of this subspace condition to seminorm projections, and show that when C is nonsingular, the ETD(\u03bb) solution can be viewed as the solution of a projected Bellman equation involving a seminorm projection (see Remark C.1)."}, {"heading": "C.1. Preliminaries", "text": "First, recall that the matrix C is said to be negative definite if there exists c > 0 such that\ny>Cy \u2264 \u2212c \u2016y\u201622, \u2200 y \u2208 Rn,\nand negative semidefinite if c = 0 in the preceding inequality. The negative definiteness of C is equivalent to that of the symmetric matrix\nC + C> = \u2212\u03a6> ( M\u0304(I \u2212 P \u03bb\u03c0,\u03b3) + (I \u2212 P \u03bb\u03c0,\u03b3)>M\u0304 ) \u03a6.\nSimilarly to (Sutton, 1988; Sutton et al., 2015), our analysis will focus on the N \u00d7 N symmetric matrix\nG = M\u0304(I \u2212Q) + (I \u2212Q)>M\u0304\nfor the substochastic matrix Q = P \u03bb\u03c0,\u03b3 and the nonnegative diagonal matrix M\u0304 as given above. We will use a theorem from (Varga, 2000, Cor. 1.22, p. 23), according to which a symmetric real matrix\nwith positive diagonal entries is positive definite if it is strictly diagonally dominant or irreducibly diagonally dominant. Note that by definition, G is irreducibly diagonally dominant if G is irreducible19 and satisfies the following diagonally dominant conditions for every row of G, with strict inequality holding for at least one row:\n|Gss| \u2265 \u2211 s\u0304 6=s |Gss\u0304|, s = 1, . . . , N,\nwhereas G is strictly diagonally dominant if it satisfies the above inequalities strictly for all rows. We now give a proof of the fact about the relation between the nonsingularity and the negative definiteness of C mentioned at the beginning. This result is due to (Sutton et al., 2015). Regarding notation, in the proofs below, for v \u2208 RN , we write v(s) for the s-th entry of v, and for an expression H that results in a vector in RN , we write (H)(s) for the s-th entry of that vector. For an expression H that results in an N \u00d7N matrix, we write [H]ss\u0304 for its (s, s\u0304)-th element. We write 0 for a zero vector in any Euclidean space.\nProposition C.1 Let Assumption 2.1 hold. Then, C is negative definite if C is nonsingular.\nProof We show first that if i(s) > 0 for all s \u2208 S, then G is strictly diagonally dominant, and hence positive definite; and that if i(s) \u2265 0 for all s \u2208 S, then G is positive semidefinite.\nLet J = {s \u2208 S | i(s) = 0}. Suppose J = \u2205. By definition M\u0304ss = ( d>\u03c0o,i(I \u2212 Q)\u22121 ) (s). Using this together with the fact that Q is substochastic, by a direct calculation as in (Sutton et al., 2015), we have that for each s \u2208 S,\nGss \u2212 \u2211 s\u0304 6=s |Gss\u0304| = M\u0304ss \u00b7\n( 1\u2212\nN\u2211 s\u0304=1 Qss\u0304\n) +\nN\u2211 s\u0304=1 M\u0304s\u0304s\u0304 \u00b7 [ I \u2212Q ] s\u0304s\n(C.1)\n\u2265 0 + ( d>\u03c0o,i(I \u2212Q)\u22121 \u00b7 (I \u2212Q) ) (s)\n= 0 + d\u03c0o,i(s) (C.2)\n> 0,\nwhere in the last strict inequality, we used the fact that i(s) > 0 implies d\u03c0o,i(s) > 0 under Assumption 2.1(ii). This shows thatG is strictly diagonally dominant with positive diagonal entries, and hence positive definite by (Varga, 2000, Cor. 1.22).\nConsider now the case J 6= \u2205. For all s \u2208 J , perturb i(s) to \u03b4 > 0, and denote by G\u03b4 the matrix G corresponding to the perturbed i(\u00b7). Then G\u03b4 is positive definite by the preceding proof. So for the original G, by continuity, G = lim\u03b4\u21920G\u03b4 is positive semidefinite. It then follows that the matrix \u03a6>G\u03a6 = \u2212C \u2212C> is positive semidefinite. Hence C is negative semidefinite; but C is nonsingular by assumption, so C must be negative definite."}, {"heading": "C.2. Main Result", "text": "We now give the main result of this section. It expresses the nonsingularity condition onC explicitly in terms of a condition on the approximation subspace E (the column space of \u03a6).\n19. A symmetric matrix G is irreducible if it corresponds to a connected (undirected) graph when the indices are viewed as the nodes of the graph, and the nonzero entries of G are viewed as edges of the graph.\nProposition C.2 Let Assumption 2.1 hold, and let J0 = {s \u2208 S | M\u0304ss = 0}. Suppose the approximation subspace E \u2282 RN is such that\nv \u2208 E and v(s) = 0, \u2200 s 6\u2208 J0 =\u21d2 v = 0. (C.3)\nThen the matrix C is negative definite. Furthermore, C is nonsingular if and only if the condition (C.3) holds.\nThe corollary below gives a sufficient condition (C.4) for C being negative definite, which can be fulfilled without knowledge of the model, as we will elaborate in Remark C.2. This corollary is a direct consequence of the preceding proposition, and follows from the observation that since i(s) > 0 implies M\u0304ss > 0, the condition (C.4) implies the condition (C.3) in Prop. C.2.\nCorollary C.1 Let Assumption 2.1 hold, and let J = {s \u2208 S | i(s) = 0}. Suppose the approximation subspace E \u2282 RN is such that\nv \u2208 E and v(s) = 0, \u2200 s 6\u2208 J =\u21d2 v = 0. (C.4)\nThen the matrix C is negative definite.\nWe now proceed to prove Prop. C.2. Roughly speaking, the method of proof is to decompose the matrixG into irreducible diagonal blocks and use, among others, the theorem (Varga, 2000, Cor. 1.22, p. 23) on irreducibly diagonally dominant matrices mentioned earlier.\nIn the two technical lemmas that follow, we let the matrixG and the nonnegative diagonal matrix M\u0304 take a slightly more general form:\nG = M\u0304(I \u2212Q) + ( M\u0304(I \u2212Q) )> , diag(M\u0304) = d>\u03c0o,i (I \u2212Q)\u22121,\nwhere Q is a substochastic matrix (not necessarily P \u03bb\u03c0,\u03b3), and d\u03c0o,i is a nonnegative vector (for notational simplicity, we keep using d\u03c0o,i instead of introducing new notation).\nLemma C.1 Suppose the matrix (I \u2212 Q) is invertible. Then the s-th diagonal entry M\u0304ss = 0 if and only if the s-th row and s-th column of G contain all zeros. Proof We have G = M\u0304(I \u2212Q) + ( M\u0304(I \u2212Q)\n)>. Suppose s is a state with M\u0304ss 6= 0. Then the s-th row of the matrix M(I \u2212Q) is nonzero (because the s-th row of I \u2212Q is nonzero, given that (I \u2212 Q)\u22121 exists). The nonzero entries of this row cannot be canceled out by the corresponding entries from the s-th row of ( M\u0304(I \u2212 Q)\n)>, because Q is a substochastic matrix and M\u0304 is nonnegative. Therefore, the s-th row of G must also be nonzero. This proves the \u201cif\u201d part.\nFor the \u201conly if\u201d part, suppose s is a state with M\u0304ss = 0. Then the s-th row of the matrix M\u0304(I \u2212Q) contains all zeros, so, since G = M\u0304(I \u2212Q) + ( M\u0304(I \u2212Q) )> and is symmetric, to prove the \u201conly if\u201d part, we only need to show that the s-th column of M\u0304(I \u2212 Q) is a zero column. We prove this by contradiction.\nSuppose for some state s\u0304 6= s, the (s\u0304, s)-entry of the matrix M\u0304(I \u2212Q) is nonzero. Then using the definition of M\u0304s\u0304s\u0304, this entry can be expressed as\nMs\u0304s\u0304 \u00b7 [ I \u2212Q ] s\u0304s = \u2212 ( d>\u03c0o,i (I \u2212Q)\u22121 ) (s\u0304) \u00b7Qs\u0304s 6= 0,\nwhich, in view of the equality (I \u2212Q)\u22121 = \u2211\nk\u22650Q k and the nonnegativity of Q, implies that(\nd>\u03c0o,iQ k ) (s\u0304) \u00b7Qs\u0304s > 0 for some k \u2265 0.\nThis in turn implies that for the state s,( d>\u03c0o,iQ k ) (s) > 0 for some k \u2265 0,\nand hence M\u0304ss = ( d>\u03c0o,i (I \u2212Q)\u22121 ) (s) \u2265 ( d>\u03c0o,iQ k ) (s) > 0,\ncontradicting the assumption M\u0304ss = 0. Thus the s-th column of M\u0304(I\u2212Q) must be a zero column.\nLemma C.2 Suppose that the matrix (I \u2212 Q) is invertible and the matrix G is irreducible. Then the diagonal entries of M\u0304 must be positive, and G is irreducibly diagonally dominant with positive diagonal entries, and hence positive definite.\nProof If s is a state with M\u0304ss = 0, by Lemma C.1, the s-th row and s-th column ofG would contain all zeros, which cannot happen if G is irreducible. Thus M\u0304ss > 0 for all s \u2208 S.\nWe have calculated in the proof of Prop. C.1 [cf. Eqs. (C.1)-(C.2)] that for nonnegative i(\u00b7),\nGss \u2212 \u2211 s\u0304 6=s |Gss\u0304| = M\u0304ss \u00b7\n( 1\u2212\nN\u2211 s\u0304=1 Qss\u0304\n) +\nN\u2211 s\u0304=1 M\u0304s\u0304s\u0304 \u00b7 [ I \u2212Q ] s\u0304s \u2265 0\nfor all rows s. The strict inequality Gss \u2212 \u2211\ns\u0304 6=s |Gss\u0304| > 0 must hold for some s. To see this, note that the invertibility of (I \u2212 Q) implies that 1 \u2212 \u2211N s\u0304=1Qss\u0304 > 0 for some s, which together\nwith M\u0304ss > 0 implies that the first term in the right-hand side above, M\u0304ss \u00b7 ( 1\u2212 \u2211N s\u0304=1Qss\u0304 ) , must be positive for at least one row s, whereas the second term in the right-hand side above equals d\u03c0o,i(s) \u2265 0 [cf. Eqs. (C.1)-(C.2)]. Since G is irreducible by assumption, this proves that G is irreducibly diagonally dominant.\nFinally, since Q is substochastic and (I \u2212Q)\u22121 exists, the diagonals of I \u2212Q must be positive. The diagonals of M\u0304 are also positive, as proved earlier. Thus the diagonal entries Gss > 0 for all rows s. It then follows from (Varga, 2000, Cor. 1.22) that G is positive definite.\nWe are now ready to prove Prop. C.2. Regarding notation, in the proof, if G1, G2, . . . , GL are L square matrices (which can have different sizes), we will write diag ( G1, G2, . . . , GL ) for the block-diagonal matrix that has Gk as its k-th diagonal block. However, for a single square matrix G1, we will keep using diag(G1) to mean the diagonal of G1.\nProof of Prop. C.2 By Assumption 2.1(i), (I \u2212 P\u03c0\u0393)\u22121 exists, which implies that for the substochastic matrix Q = P \u03bb\u03c0,\u03b3 [cf. Eq. (2.6)], (I \u2212Q)\u22121 also exists. So the matrices M\u0304 , C and G are well defined. By reordering the states if necessary, we can arrange G into a block-diagonal matrix with L blocks,\nG = diag ( G(1), . . . , G(L\u22121), G(L) ) (C.5)\nsuch that:\n(i) for each ` = 1, . . . , L\u2212 1, the `th-block G(`) is irreducible; and (ii) the L-th block G(L) is a zero matrix (if G does not have a zero block, we will treat G(L) as a\nmatrix of size zero, and this will not affect the proof below). Note that by Lemma C.1, the row/column indices associated with the zero block G(L) are exactly those in the set\nJ0 = {s \u2208 S | M\u0304ss = 0}.\nSince the condition (C.3) rules out the case J0 = S, G cannot be a zero matrix, so it must have at least one irreducible block.\nWe prove next that the matrix Q has the following structure, matching the block-diagonal structure of G:\nQ =  Q(1) Q(2) HH Q(L\u22121)\n\u2217 \u2217 \u00b7 \u00b7 \u00b7 \u2217 \u2217  (C.6) where the blocks Q(`), ` \u2264 L \u2212 1, on the diagonal correspond to the blocks G(`), ` \u2264 L \u2212 1, on the diagonal of G, the unmarked blocks contain all zeros, and the \u2217-blocks can have both zeros and positive entries.\nTo prove Eq. (C.6) by contradiction, suppose it does not hold. This means that there must exist two states s 6= s\u0304 with Qss\u0304 > 0, but the entry Qss\u0304 lies inside an unmarked block of the matrix on the right-hand side of Eq. (C.6). This position of Qss\u0304 implies Gss\u0304 = 0, which is possible only if M\u0304ss = 0 (otherwise, Qss\u0304 6= 0 would force Gss\u0304 6= 0). But if M\u0304ss = 0, s \u2208 J0, which is the set of indices associated with the last zero block, as shown earlier. Consequently, the entry Qss\u0304 cannot lie inside an unmarked block as we assumed. This contradiction proves that Eq. (C.6) must hold.\nFrom the structure of Q shown in (C.6), it follows that (I \u2212Q)\u22121 has the same structure:\n( I \u2212Q )\u22121 =  ( I \u2212Q(1) )\u22121 ( I \u2212Q(2) )\u22121 HH ( I \u2212Q(L\u22121) )\u22121\n\u2217 \u2217 \u00b7 \u00b7 \u00b7 \u2217 \u2217\n . (C.7)\nSince G = M\u0304(I \u2212 Q) + (I \u2212 Q)>M\u0304 , Eqs. (C.5), (C.6) and (C.7) together imply that for each ` \u2264 L\u2212 1, the matrix G(`) can be expressed as\nG(`) = M\u0304 (`) ( I \u2212Q(`) ) + ( I \u2212Q(`) )> M\u0304 (`),\nwhere M\u0304 (`) is the `-th diagonal block in the corresponding decomposition of M\u0304 as\nM\u0304 = diag ( M\u0304 (1), . . . , M\u0304 (L) ) ,\nand if we decompose the vector d\u03c0o,i similarly as d\u03c0o,i = ( d (1) \u03c0o,i, . . . , d (L) \u03c0o,i ) , then for each ` \u2264 L\u22121, the diagonal block M\u0304 (`) has its diagonal entries given by\ndiag ( M\u0304 (`) ) = ( d\n(`) \u03c0o,i\n)>( I \u2212Q(`) )\u22121 , ` \u2264 L\u2212 1.\nIn the above expression, we also used the fact d(L)\u03c0o,i = 0, which is implied by M\u0304 (L) being a zero matrix (which we showed at the beginning of this proof).20\nWe now apply Lemma C.2 to each irreducible block G(`), ` \u2264 L \u2212 1 (with M\u0304 = M\u0304 (`) and Q = Q(`), a substochastic matrix). This yields that each of these G(`) is positive definite, and consequently, the block-diagonal matrix\nG\u0302 = diag ( G(1), . . . , G(L\u22121) ) is positive definite.\nFinally, we prove the statement of the proposition. For the block-diagonal decomposition of G as G = diag(G\u0302,G(L)), write a point y \u2208 RN correspondingly as y = (y1, y0). I.e., the indices of the components of y0 are those in J0 = {s \u2208 S | M\u0304ss = 0}, and the dimension of y1 is N\u0302 = N \u2212 |J0|.\nSince G\u0302 is positive definite, there exists some c > 0 such that\ny1 >G\u0302 y1 \u2265 c \u2016y1\u201622, \u2200 y1 \u2208 RN\u0302 . (C.8)\nConsider a point y = (y1, y0) \u2208 E with y1 = 0. Then y0 = 0 by the assumption (C.3). Since E is a subspace, this implies that there exists some constant \u03b4 > 0 such that\ninf y\u2208E, \u2016y\u20162=1\n\u2016y1\u20162 \u2265 \u03b4. (C.9)\nUsing Eqs. (C.8)-(C.9), we have\ninf y\u2208E, \u2016y\u20162=1 y>Gy = inf y\u2208E, \u2016y\u20162=1 y1 >G\u0302 y1 \u2265 inf y\u2208E, \u2016y\u20162=1 c \u2016y1\u201622 \u2265 c \u03b42 > 0. (C.10)\nSinceE is the column space of \u03a6 and \u03a6 has linearly independent columns by definition, the inequality (C.10) establishes that the matrix \u03a6>G\u03a6 = \u2212C \u2212 C> is positive definite, and consequently, C is negative definite.\nThe preceding proof also shows that C is nonsingular if the condition (C.3) holds. To complete the proof, let us assume that the condition (C.3) does not hold and show that C must be singular. Let y = (y1, y0) \u2208 E be such that y1 = 0 and y0 6= 0. Then since G(L) is a zero block, y>Gy = 0, which implies that the matrix \u03a6>G\u03a6 = \u2212C \u2212 C> is singular. If C were nonsingular, then by Prop. C.1, \u2212C \u2212 C> would be positive definite and hence nonsingular, a contradiction. Therefore, C must be singular.\nFinally, we make two remarks on the conditions (C.3) and (C.4) in Prop. C.2 and Cor. C.1.\nRemark C.1 (Seminorm projection) Using seminorm projections to formulate the projected Bellman equations associated with TD methods is introduced in (Yu and Bertsekas, 2012). There, conditions of the form (C.3) or (C.4) are used to define a projection on the approximation subspace with respect to a seminorm. We can use this formulation here to interpret the solution of ETD(\u03bb) 20. Using the expression (I \u2212 Q)\u22121 = \u2211 k\u22650Q\nk, it can be seen from the definition of M\u0304ss that M\u0304ss \u2265 d\u03c0o,i(s). Therefore, M\u0304ss = 0 implies that d\u03c0o,i(s) = 0.\nand ELSTD(\u03bb). Specifically, define a weighted Euclidean seminorm \u2016 \u00b7 \u2016M\u0304 on RN , using diag(M\u0304) as the weights, as\n\u2016v\u20162M\u0304 = \u2211 s\u2208S M\u0304ss \u00b7 v(s)2.\nCondition (C.3) ensures that the projection \u03a0M\u0304 onto E with respect to the seminorm \u2016 \u00b7 \u2016M\u0304 is well-defined and has the matrix representation\n\u03a0M\u0304 = \u03a6 ( \u03a6>M\u0304\u03a6 )\u22121 \u03a6>M\u0304\n(cf. Yu and Bertsekas, 2012, Sec. 2.1). So by Prop. C.2 and the convergence results of this paper, when C is nonsingular, ETD(\u03bb) and ELSTD(\u03bb) solve in the limit the projected Bellman equation\nv = \u03a0M\u0304 ( r\u03bb\u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v ) .\nThe relation between the solution v = \u03a6\u03b8\u2217 of this equation and the desired value function v\u03c0, in particular, the approximation error, can be analyzed then, using the oblique projection viewpoint (Scherrer, 2010) (for details, see also (Yu and Bertsekas, 2012)).\nRemark C.2 (Equivalent conditions in terms of features) The condition (C.3) can be paraphrased in terms of the features \u03c6(s) as follows:\n\u2200 s \u2208 S with M\u0304ss = 0, \u03c6(s) \u2208 span { \u03c6(s\u0304) \u2223\u2223 s\u0304 \u2208 S and M\u0304s\u0304s\u0304 > 0}; (C.11) namely, from those states with positive emphasis weights M\u0304s\u0304s\u0304 > 0, n linearly independent feature vectors can be found. Similarly, the condition (C.4) can be paraphrased as:\n\u2200 s \u2208 S with i(s) = 0, \u03c6(s) \u2208 span { \u03c6(s\u0304) \u2223\u2223 s\u0304 \u2208 S and i(s\u0304) > 0}; (C.12) namely, from the states with positive interest weights, n linearly independent feature vectors can be found. This shows that even without knowing P\u03c0 and M\u0304 , by designing a rich enough set of features for states of interest beforehand, we can ensure the sufficient condition (C.4) for the nonsingularity and negative definiteness of the matrix C.\nConditions like (C.11), (C.12) [or equivalently, (C.3), (C.4)] are naturally satisfied in the case where the approximate values of the policy \u03c0 at certain states (e.g., those states s with M\u0304ss = 0 or i(s) = 0) are interpolated or extrapolated from the approximate values of \u03c0 at some other \u201crepresentative\u201d states, based on the \u201cproximity\u201d of the former states to the representative ones."}], "references": [{"title": "Projected equation methods for approximate solution of large linear systems", "author": ["D.P. Bertsekas", "H. Yu"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Bertsekas and Yu,? \\Q2009\\E", "shortCiteRegEx": "Bertsekas and Yu", "year": 2009}, {"title": "Convergence of Probability Measures", "author": ["P. Billingsley"], "venue": null, "citeRegEx": "Billingsley,? \\Q1968\\E", "shortCiteRegEx": "Billingsley", "year": 1968}, {"title": "Stochastic Approximation: A Dynamic Viewpoint", "author": ["V.S. Borkar"], "venue": null, "citeRegEx": "Borkar,? \\Q2008\\E", "shortCiteRegEx": "Borkar", "year": 2008}, {"title": "The O.D.E. method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S.P. Meyn"], "venue": "SIAM J. Control Optim.,", "citeRegEx": "Borkar and Meyn,? \\Q2000\\E", "shortCiteRegEx": "Borkar and Meyn", "year": 2000}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "In Proc. The 16th Int. Conf. Machine Learning,", "citeRegEx": "Boyan,? \\Q1999\\E", "shortCiteRegEx": "Boyan", "year": 1999}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Res.,", "citeRegEx": "Dann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2014}, {"title": "Stochastic Processes", "author": ["J.L. Doob"], "venue": null, "citeRegEx": "Doob,? \\Q1953\\E", "shortCiteRegEx": "Doob", "year": 1953}, {"title": "Real Analysis and Probability", "author": ["R.M. Dudley"], "venue": null, "citeRegEx": "Dudley,? \\Q2002\\E", "shortCiteRegEx": "Dudley", "year": 2002}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "Journal of Machine Learning Res.,", "citeRegEx": "Geist and Scherrer,? \\Q2014\\E", "shortCiteRegEx": "Geist and Scherrer", "year": 2014}, {"title": "Importance sampling for stochastic simulations", "author": ["P.W. Glynn", "D.L. Iglehart"], "venue": "Management Science,", "citeRegEx": "Glynn and Iglehart,? \\Q1989\\E", "shortCiteRegEx": "Glynn and Iglehart", "year": 1989}, {"title": "Stochastic Approximation Methods for Constrained and Unconstrained Systems", "author": ["H.J. Kushner", "D.S. Clark"], "venue": null, "citeRegEx": "Kushner and Clark,? \\Q1978\\E", "shortCiteRegEx": "Kushner and Clark", "year": 1978}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["H.J. Kushner", "G.G. Yin"], "venue": null, "citeRegEx": "Kushner and Yin,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "Maei,? \\Q2011\\E", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A.R. Mahmood", "H. van Hasselt", "R.S. Sutton"], "venue": "In Proc. Conf. Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "In European Workshops on Reinforcement Learning, Lille, France,", "citeRegEx": "Mahmood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Control Techniques for Complex Networks", "author": ["S. Meyn"], "venue": null, "citeRegEx": "Meyn,? \\Q2008\\E", "shortCiteRegEx": "Meyn", "year": 2008}, {"title": "Markov Chains and Stochastic Stability", "author": ["S. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "Meyn and Tweedie,? \\Q2009\\E", "shortCiteRegEx": "Meyn and Tweedie", "year": 2009}, {"title": "Discrete-Parameter Martingales", "author": ["J. Neveu"], "venue": null, "citeRegEx": "Neveu,? \\Q1975\\E", "shortCiteRegEx": "Neveu", "year": 1975}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "In Proc. The 18th Int. Conf. Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Combining importance sampling and temporal difference control variates to simulate Markov chains", "author": ["R.S. Randhawa", "S. Juneja"], "venue": "ACM Trans. Modeling and Computer Simulation,", "citeRegEx": "Randhawa and Juneja,? \\Q2004\\E", "shortCiteRegEx": "Randhawa and Juneja", "year": 2004}, {"title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view", "author": ["B. Scherrer"], "venue": "In Proc. The 27th Int. Conf. Machine Learning,", "citeRegEx": "Scherrer,? \\Q2010\\E", "shortCiteRegEx": "Scherrer", "year": 2010}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In Proc. The 12th Int. Conf. Machine Learning,", "citeRegEx": "Sutton,? \\Q1995\\E", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "An emphatic approach to the problem of off-policy", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "temporal-difference learning,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Asynchronous stochastic approximation and Q-learning", "author": ["J.N. Tsitsiklis"], "venue": "Machine Learning,", "citeRegEx": "Tsitsiklis,? \\Q1994\\E", "shortCiteRegEx": "Tsitsiklis", "year": 1994}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Trans. Automat. Contr.,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1997}, {"title": "Matrix Iterative Analysis", "author": ["R.S. Varga"], "venue": "Springer-Verlag, Berlin, 2nd edition,", "citeRegEx": "Varga,? \\Q2000\\E", "shortCiteRegEx": "Varga", "year": 2000}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM J. Control Optim.,", "citeRegEx": "Yu,? \\Q2012\\E", "shortCiteRegEx": "Yu", "year": 2012}, {"title": "Weighted Bellman equations and their applications in approximate dynamic programming", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "LIDS Technical Report 2876,", "citeRegEx": "Yu and Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Yu and Bertsekas", "year": 2012}, {"title": "imply, by a convergence theorem in (Neveu, 1975, Ex. II-4, p. 33-34) for nonnegative random processes (which is a consequence of the nonnegative supermartingale convergence theorem)", "author": ["\u221e a.s"], "venue": null, "citeRegEx": "a.s.,? \\Q1975\\E", "shortCiteRegEx": "a.s.", "year": 1975}, {"title": "Roughly speaking, the method of proof is to decompose the matrixG into irreducible diagonal blocks and use, among others, the theorem (Varga, 2000, Cor", "author": ["Prop. C"], "venue": null, "citeRegEx": "C.2.,? \\Q2000\\E", "shortCiteRegEx": "C.2.", "year": 2000}], "referenceMentions": [{"referenceID": 22, "context": "Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation.", "startOffset": 42, "endOffset": 76}, {"referenceID": 22, "context": "We focus on temporal-difference (TD) methods with linear function approximation (Sutton, 1988).", "startOffset": 80, "endOffset": 94}, {"referenceID": 18, "context": "The approach is related to the early work on episodic off-policy TD(\u03bb) (Precup et al., 2001), and is based on the idea of re-weighting the states when forming the eligibility traces in TD(\u03bb), so that the weights reflect the occupation frequencies of the target policy rather than the behavior policy.", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "The stability criterion of (Borkar and Meyn, 2000) (see also (Borkar, 2008, Chap.", "startOffset": 27, "endOffset": 50}, {"referenceID": 0, "context": "For regular off-policy LSTD(\u03bb) and TD(\u03bb) (Bertsekas and Yu, 2009), it has been shown by Yu (2012) that the associated joint process of states and trace iterates exhibit useful properties, by which convergence results for LSTD(\u03bb) can be derived.", "startOffset": 41, "endOffset": 65}, {"referenceID": 17, "context": "We focus on temporal-difference (TD) methods with linear function approximation (Sutton, 1988). Such methods are typically convergent when the target and behavior policies are the same (the on-policy case), but not in the off-policy case (Tsitsiklis and Van Roy, 1997). This difficulty is intrinsic to sampling states according to an arbitrary policy.1 Gradient-based or least squares-based approaches have been used to avoid this difficulty.2 Recently, Sutton, Mahmood, and White (2015) proposed a new approach to address this issue more directly.", "startOffset": 81, "endOffset": 488}, {"referenceID": 0, "context": "For regular off-policy LSTD(\u03bb) and TD(\u03bb) (Bertsekas and Yu, 2009), it has been shown by Yu (2012) that the associated joint process of states and trace iterates exhibit useful properties, by which convergence results for LSTD(\u03bb) can be derived.", "startOffset": 42, "endOffset": 98}, {"referenceID": 25, "context": "See the papers (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2015) and the books (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) for related examples and discussion.", "startOffset": 15, "endOffset": 79}, {"referenceID": 24, "context": ", 2015) and the books (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) for related examples and discussion.", "startOffset": 22, "endOffset": 78}, {"referenceID": 12, "context": ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).", "startOffset": 2, "endOffset": 83}, {"referenceID": 0, "context": ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).", "startOffset": 2, "endOffset": 83}, {"referenceID": 8, "context": ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).", "startOffset": 2, "endOffset": 83}, {"referenceID": 5, "context": ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).", "startOffset": 2, "endOffset": 83}, {"referenceID": 9, "context": "Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).", "startOffset": 33, "endOffset": 107}, {"referenceID": 20, "context": "Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).", "startOffset": 33, "endOffset": 107}, {"referenceID": 25, "context": "Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).", "startOffset": 33, "endOffset": 107}, {"referenceID": 11, "context": "the conditions of a convergence theorem from stochastic approximation theory (Kushner and Yin, 2003) and yield convergence results for TD(\u03bb).", "startOffset": 77, "endOffset": 100}, {"referenceID": 29, "context": "In this paper we will take the proof approach used in (Yu, 2012).", "startOffset": 54, "endOffset": 64}, {"referenceID": 0, "context": "Furthermore, we will give a new argument to prove the almost sure convergence of ETD(\u03bb), which applies also to the regular off-policy TD(\u03bb) of (Bertsekas and Yu, 2009) for \u03bb near 1.", "startOffset": 143, "endOffset": 167}, {"referenceID": 29, "context": "This improves a result of (Yu, 2012), which only dealt with a constrained version of TD(\u03bb) that restricts the iterates to lie in a bounded set.", "startOffset": 26, "endOffset": 36}, {"referenceID": 25, "context": "Algorithms We consider computing v\u03c0 with the ETD(\u03bb) algorithm (Sutton et al., 2015) and its least-squares version, ELSTD(\u03bb), using linear function approximation, while following the behavior policy \u03c0o.", "startOffset": 62, "endOffset": 83}, {"referenceID": 23, "context": "Associated with ETD(\u03bb) is a generalized Bellman equation of which v\u03c0 is the unique solution (Sutton, 1995):8 v = r \u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v.", "startOffset": 92, "endOffset": 106}, {"referenceID": 25, "context": "For insights about ETD(\u03bb), see (Sutton et al., 2015; Mahmood et al., 2015).", "startOffset": 31, "endOffset": 74}, {"referenceID": 14, "context": "For insights about ETD(\u03bb), see (Sutton et al., 2015; Mahmood et al., 2015).", "startOffset": 31, "endOffset": 74}, {"referenceID": 23, "context": "For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al.", "startOffset": 81, "endOffset": 119}, {"referenceID": 24, "context": "For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al.", "startOffset": 81, "endOffset": 119}, {"referenceID": 25, "context": "For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al., 2015).", "startOffset": 140, "endOffset": 161}, {"referenceID": 13, "context": ", 2015; Mahmood et al., 2015). Our definition (2.4) of {et} differs slightly from its original definition, but the two are equivalent; ours appears to be more convenient for our analysis. 8. For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al., 2015). 9. The negative definiteness of C is proved for positive i(\u00b7) under Assumption 2.1 by Sutton et al. (2015), and their result extends to nonnegative i(\u00b7), as long as C is nonsingular (see our Prop.", "startOffset": 8, "endOffset": 461}, {"referenceID": 25, "context": "4) in ETD(\u03bb) (Sutton et al., 2015).", "startOffset": 13, "endOffset": 34}, {"referenceID": 25, "context": "1, C is always negative semidefinite (Sutton et al., 2015) (cf.", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": ", (Boyan, 1999; Yu, 2012) for on-policy and off-policy LSTD(\u03bb)).", "startOffset": 2, "endOffset": 25}, {"referenceID": 29, "context": ", (Boyan, 1999; Yu, 2012) for on-policy and off-policy LSTD(\u03bb)).", "startOffset": 2, "endOffset": 25}, {"referenceID": 20, "context": "7) has a unique solution, and bounds on the approximation error of ETD(\u03bb) can be derived using the approach of Scherrer (2010). (For details of this discussion, see Remark C.", "startOffset": 111, "endOffset": 127}, {"referenceID": 13, "context": "For a recent work in this direction, see (Mahmood et al., 2014).", "startOffset": 41, "endOffset": 63}, {"referenceID": 29, "context": "The structure of our analysis will be similar to that of (Yu, 2012) for regular off-policy LSTD(\u03bb), but the proofs at intermediate steps are new and more involved.", "startOffset": 57, "endOffset": 67}, {"referenceID": 11, "context": "Using the results of Section 3, we can now analyze its convergence by applying a \u201cmean ODE\u201d method from stochastic approximation theory (Kushner and Yin, 2003).", "startOffset": 136, "endOffset": 159}, {"referenceID": 0, "context": "1 (Almost sure convergence of regular off-policy TD(\u03bb)) If \u03bb is a constant sufficiently close to 1, the matrix associated with the \u201cmean updates\u201d of the regular off-policy TD(\u03bb) algorithm is also negative definite (Bertsekas and Yu, 2009).", "startOffset": 214, "endOffset": 238}, {"referenceID": 11, "context": "\u1e8b = h\u0304(x) + z, z \u2208 \u2212NB(x), where h\u0304(x) = Cx + b, NB(x) is the normal cone of B at x, and z is the boundary reflection term that keeps the solution in B (Kushner and Yin, 2003).", "startOffset": 152, "endOffset": 175}, {"referenceID": 10, "context": "The four remaining conditions to be introduced are of the same type and relate to the asymptotic rate of change conditions introduced by (Kushner and Clark, 1978).", "startOffset": 137, "endOffset": 162}, {"referenceID": 25, "context": "It is known to hold if i(s) > 0 for all s \u2208 S (Sutton et al., 2015).", "startOffset": 46, "endOffset": 67}, {"referenceID": 22, "context": "Similarly to (Sutton, 1988; Sutton et al., 2015), our analysis will focus on the N \u00d7 N symmetric matrix G = M\u0304(I \u2212Q) + (I \u2212Q)>M\u0304 for the substochastic matrix Q = P \u03bb \u03c0,\u03b3 and the nonnegative diagonal matrix M\u0304 as given above.", "startOffset": 13, "endOffset": 48}, {"referenceID": 25, "context": "Similarly to (Sutton, 1988; Sutton et al., 2015), our analysis will focus on the N \u00d7 N symmetric matrix G = M\u0304(I \u2212Q) + (I \u2212Q)>M\u0304 for the substochastic matrix Q = P \u03bb \u03c0,\u03b3 and the nonnegative diagonal matrix M\u0304 as given above.", "startOffset": 13, "endOffset": 48}, {"referenceID": 25, "context": "This result is due to (Sutton et al., 2015).", "startOffset": 22, "endOffset": 43}, {"referenceID": 25, "context": "Using this together with the fact that Q is substochastic, by a direct calculation as in (Sutton et al., 2015), we have that for each s \u2208 S,", "startOffset": 89, "endOffset": 110}, {"referenceID": 30, "context": "1 (Seminorm projection) Using seminorm projections to formulate the projected Bellman equations associated with TD methods is introduced in (Yu and Bertsekas, 2012).", "startOffset": 140, "endOffset": 164}, {"referenceID": 21, "context": "The relation between the solution v = \u03a6\u03b8\u2217 of this equation and the desired value function v\u03c0, in particular, the approximation error, can be analyzed then, using the oblique projection viewpoint (Scherrer, 2010) (for details, see also (Yu and Bertsekas, 2012)).", "startOffset": 195, "endOffset": 211}, {"referenceID": 30, "context": "The relation between the solution v = \u03a6\u03b8\u2217 of this equation and the desired value function v\u03c0, in particular, the approximation error, can be analyzed then, using the oblique projection viewpoint (Scherrer, 2010) (for details, see also (Yu and Bertsekas, 2012)).", "startOffset": 235, "endOffset": 259}], "year": 2015, "abstractText": "We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD(\u03bb) and ELSTD(\u03bb). We prove, under general off-policy conditions, the convergence in L for ELSTD(\u03bb) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD(\u03bb) also converges under off-policy training for \u03bb sufficiently large.", "creator": "LaTeX with hyperref package"}}}