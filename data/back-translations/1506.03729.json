{"id": "1506.03729", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Recovering Communities in the General Stochastic Block Model Without Knowing the Parameters", "abstract": "Most recent developments of the stochastic block model (SBM) are based on knowledge of the model parameters or at least the number of communities. In this paper, efficient algorithms are presented that do not require such knowledge and yet achieve the optimal information-theoretical trade-offs identified in [AS15] for communities of linear size. Results are threefold: (i) In the constant-degree regime, an algorithm is developed that requires only a lower limit for the relative size of communities and recognizes communities with optimal precision scaling for large degrees; (ii) in the regime in which the degrees are scaled by $\\ omega (1) $(divergent degrees), this algorithm is expanded to a fully agnostic algorithm that takes into account only the graph in question and simultaneously learns the model parameters (including the number of communities) and communities with an accuracy of $\\ omega (1) $(1) $(1) (divergent algorithm) that simultaneously recognizes the number of optimum communities in the model parameters (including the number of optimum algorithms) and the complexity of the most recent algorithms.", "histories": [["v1", "Thu, 11 Jun 2015 16:09:28 GMT  (100kb,D)", "http://arxiv.org/abs/1506.03729v1", "arXiv admin note: substantial text overlap witharXiv:1503.00609"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1503.00609", "reviews": [], "SUBJECTS": "math.PR cs.IT cs.LG cs.SI math.IT", "authors": ["emmanuel abbe", "colin sandon"], "accepted": true, "id": "1506.03729"}, "pdf": {"name": "1506.03729.pdf", "metadata": {"source": "CRF", "title": "Recovering communities in the general stochastic block model without knowing the parameters", "authors": ["Emmanuel Abbe", "Colin Sandon"], "emails": ["eabbe@princeton.edu.", "sandon@princeton.edu."], "sections": [{"heading": null, "text": "\u2217Program in Applied and Computational Mathematics, and EE department, Princeton University, Princeton, USA, eabbe@princeton.edu. This research was partially supported by the 2014 Bell Labs Prize.\n\u2020Department of Mathematics, Princeton University, USA, sandon@princeton.edu.\nar X\niv :1\n50 6.\n03 72\n9v 1\n[ m\nat h.\nPR ]\n1 1\nJu n\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Related results on the general SBM with known parameters . . . . . . . . . 2 1.2 Estimating the parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"}, {"heading": "2 Results 4", "text": "2.1 Definitions and terminologies . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Partial recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Exact recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "3 Proof Techniques and Algorithms 6", "text": "3.1 Partial recovery and the Agnostic-sphere-comparison algorithm . . . . . 6 3.2 Exact recovery and the Agnostic-degree-profiling algorithm . . . . . . 10"}, {"heading": "4 An example with real data 11", "text": ""}, {"heading": "5 Open problems 12", "text": "6 The Agnostic-sphere-comparison algorithm in details 16"}, {"heading": "7 Appendix 24", "text": "7.1 Partial Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n7.1.1 Formal results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 7.1.2 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7.2 Exact recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 7.2.1 Formal results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 7.2.2 Testing degree profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 7.2.3 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . 57"}, {"heading": "1 Introduction", "text": "This paper studies the problem of recovering communities in the general stochastic block model with linear size communities, for constant and slowly diverging degree regimes. In contrast to [AS15], this paper does not require knowledge of the SBM parameters. In particular, the problem of learning the model parameters is solved when average degrees are diverging. We next provide some motivations on the problem and further background on the model.\nDetecting communities (or clusters) in graphs is a fundamental problem in networks, computer science and machine learning. This applies to a large variety of complex networks (e.g., social and biological networks) as well as to data sets engineered as networks via similarly graphs, where one often attempts to get a first impression on the data by trying to identify groups with similar behavior. In particular, finding communities allows one to find like-minded people in social networks [GN02, NWS], to improve recommendation systems [LSY03, XWZ+14], to segment or classify images [SM97, SHB07], to detect protein complexes [CY06, MPN+99], to find genetically related sub-populations [PSD00, JTZ04], or discover new tumor subclasses [SPT+01].\nWhile a large variety of community detection algorithms have been deployed in the past decades, the understanding of the fundamental limits of community detection has only appeared more recently, in particular for the SBM [Co10, DKMZ11, Mas14, MNS14, ABH14, MNSb, YC14, AS15]. The SBM is a canonical model for community detection [HLL83, WBB76, FMW85, WW87, BC09, KN11, BCLS87, DF89, Bop87, JS98, CK99, CI01, SN97, McS01, BC09, RCY11, CWA12, CSX12], where n vertices are partitioned into k communities of relative size pi, i \u2208 [k], and pairs of nodes in communities i and j connect independently with probability Wi,j .\nRecently the SBM came back to the center of the attention at both the practical level, due to extensions allowing overlapping communities [ABFX08] that have proved to fit well real data sets in massive networks [GB13], and at the theoretical level due to new phase transition phenomena [Co10, DKMZ11, Mas14, MNS14, ABH14, MNSb]. The latter works focus exclusively on the SBM with two symmetric communities, i.e., each community is of the same size and the connectivity in each community is identical. Denoting by p the intra- and q the extra-cluster probabilities, most of the results are concerned with two figure of merits: (i) recovery (also called exact recovery or strong consistency), which investigates the regimes of p and q for which there exists an algorithm that recovers with high probability the two communities completely [BCLS87, DF89, Bop87, JS98, CK99, CI01, SN97, McS01, BC09, RCY11, CWA12, CSX12, Vu14, YC14], (ii) detection, which investigates the regimes for which there exists an algorithm that recovers with high probability a positively correlated partition [Co10, DKMZ11, MNS12, Mas14, MNS14].\nThe sharp threshold for exact recovery was obtained in [ABH14, MNSb], showing1 that for p = a log(n)/n, q = b log(n)/n, a, b > 0, exact recovery is solvable if and only if\u221a a\u2212 \u221a b \u2265 2, with efficient algorithms achieving the threshold provided in [ABH14, MNSb]. In addition, [ABH14] introduces an SDP proved to achieve the threshold in [BH14, Ban15], while [YP14] shows that a spectral algorithm also achieves the threshold. Prior to these,\n1[MNSb] generalizes this to a, b = \u0398(1).\nthe sharp threshold for detection was obtained in [Mas14, MNS14], showing that detection is solvable (and so efficiently) if and only if (a \u2212 b)2 > 2(a + b), when p = a/n, q = b/n, settling a conjecture made in [DKMZ11] and improving on [Co10].\nBesides the detection and the recovery properties, one may ask about the partial recovery of the communities, studied in [MNSa, GV14, Vu14, CRV15, AS15]. Of particular interest to this paper is the case of almost exact recovery (also called weak consistency), where only a vanishing fraction of the nodes is allowed to be misclassified. For two-symmetric communities, [MNSb] shows that almost exact recovery is possible if and only if n(p\u2212 q)2/(p+ q) diverges, generalized in [AS15] for general SBMs.\nIn the next section, we discuss the results for the general SBM of interest in this paper and the problem of learning the model parameters. We conclude this section by providing motivations on the problem of achieving the threshold with an efficient and universal algorithm.\nThreshold phenomena have long been studied in fields such as information theory (e.g., Shannon\u2019s capacity) and constraint satisfaction problems (e.g., the SAT threshold). In particular, the quest of achieving the threshold has generated major algorithmic developments in these fields (e.g., LDPC codes, polar codes, survey propagation to name a few). Likewise, identifying thresholds in community detection models is key to benchmark and guide the development of clustering algorithms. Most reasonable algorithms may succeed in some regimes, while in others they may be doomed to fail due to computational barriers. However, it is particularly crucial to develop benchmarks that do not depend sensitively on the knowledge of the model parameters. A natural question is hence whether one can solve the various recovery problems in the SBM without having access to the parameters. This paper answers this question by the affirmative for the exact and almost exact recovery of the communities."}, {"heading": "1.1 Related results on the general SBM with known parameters", "text": "Most of the previous works are concerned with the SBM having symmetric communities (mainly 2 or sometimes k), with the exception of [Vu14] which provides some achievability results for the general SBM.2 Recently, [AS15] studied the fundamental limits for the general SBM, with results as follows (where SBM(n, p,W ) is the SBM with community prior p and connectivity matrix W ).\nI. Partial and almost exact recovery in the general SBM. The first result of [AS15] concerns the regime where the connectivity matrix scales as Q/n for a positive symmetric matrix Q (i.e., the node average degree is constant). The following notion of SNR is introduced3\nSNR = |\u03bbmin|2/\u03bbmax (1)\n2[GV14] also study variations of the k-symmetric model.\n3Note that this in a sense the \u201cworst-case\u201d notion of SNR, which ensures that all of the communities can be separated (when amplified); one could consider other ratios of the kind |\u03bbj |2/\u03bbmax, for subsequent eigenvalues (j = 2, 3, . . . ), if interested in separating only subset of the communities.\nwhere \u03bbmin and \u03bbmax are respectively the smallest 4 and largest eigenvalue of diag(p)Q.\nThe algorithm Sphere-comparison is proposed that solves partial recovery with exponential accuracy and quasi-linear complexity when the SNR diverges, solving in particular almost exact recovery.\nTheorem 1. [AS15] Given any k \u2208 Z, p \u2208 (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal, let \u03bb be the largest eigenvalue of PQ, and \u03bb\u2032 be the eigenvalue of PQ with the smallest nonzero magnitude. If \u03c1 := |\u03bb \u2032|2 \u03bb > 4, \u03bb\n7 < (\u03bb\u2032)8, and 4\u03bb3 < (\u03bb\u2032)4, then for some \u03b5 = \u03b5(\u03bb, \u03bb\u2032) and C = C(p,Q) > 0, Sphere-comparison (see Section 3.1) detects with high probability communities in graphs drawn from SBM(n, p,Q/n) with accuracy 1\u22124ke\u2212 C\u03c1 16k /(1\u2212exp(\u2212 C\u03c116k ( (\u03bb\u2032)4 \u03bb3 \u2212 1 )\n)), provided that the above is larger than 1\u2212 mini pi2 ln(4k) , and runs in O(n1+ ) time. Moreover, \u03b5 can be made arbitrarily small with 8 ln(\u03bb \u221a 2/|\u03bb\u2032|)/ ln(\u03bb), and C(p, \u03b1Q) is independent of \u03b1.\nNote that for k symmetric clusters, SNR reduces to (a\u2212b) 2\nk(a+(k\u22121)b) , which is the quantity of\ninterest for detection [DKMZ11, MNS12]. Moreover, the SNR must diverge to ensure almost exact recovery in the symmetric case [AS15]. The following is an important consequence of the previous theorem, as it shows that Sphere-comparison achieves almost exact recovery when the entries of Q are scaled.\nCorollary 1. [AS15] For any k \u2208 Z, p \u2208 (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal, there exists (\u03b4) = O(1/ ln(\u03b4)) such that for all sufficiently large \u03b4 there exists an algorithm (Sphere-comparison) that detects communities in graphs drawn from SBM(n, p, \u03b4Q) with accuracy 1\u2212 e\u2212\u2126(\u03b4) and complexity On(n1+ (\u03b4)). II. Exact recovery in the general SBM. The second result in [AS15] is for the regime where the connectivity matrix scales as log(n)Q/n, Q fixed, where it is shown that exact recovery has a sharp threshold characterized by the divergence function\nD+(f, g) = max t\u2208[0,1] \u2211 x\u2208[k] ( tf(x) + (1\u2212 t)g(x)\u2212 f(x)tg(x)1\u2212t ) ,\nnamed the CH-divergence in [AS15]. Specifically, if all pairs of columns in diag(p)Q are at D+-distance at least 1 from each other, then exact recovery is solvable in the general SBM. This provides in particular an operational meaning to a new divergence function analog to the KL-divergence in the channel coding theorem (see Section 2.3 in [AS15]). Moreover, an algorithm (Degree-profiling) is developed that solves exact recovery down to the D+ limit in quasi-linear time, showing that exact recovery has no informational to computational gap (as opposed to the conjectures made for detection with more than 4 communities [DKMZ11]). The following gives a more general statement characterizing which subset of communities can be extracted \u2014 see Definition 3 for formal definitions.\nTheorem 2. [AS15] (i) Exact recovery is solvable in the stochastic block model G2(n, p,Q) for a partition [k] = tts=1As if and only if for all i and j in different subsets of the partition,5\nD+((PQ)i, (PQ)j) \u2265 1, (2)\n4The smallest eigenvalue of diag(p)Q is the one with least magnitude.\n5The entries of Q are assumed to be non-zero.\nIn particular, exact recovery is information-theoretically solvable in SBM(n, p,Q log(n)/n) if and only if mini,j\u2208[k],i 6=j D+((PQ)i||(PQ)j) \u2265 1. (ii) The Degree-profiling algorithm (see [AS15]) recovers the finest partition that can be recovered with probability 1 \u2212 on(1) and runs in o(n1+ ) time for all > 0. In particular, exact recovery is efficiently solvable whenever it is information-theoretically solvable.\nIn summary, exact or almost exact recovery is closed for the general SBM (and detection is closed for 2 symmetric communities). However this is for the case where the parameters of the SBM are assumed to be known, and with linear-size communities."}, {"heading": "1.2 Estimating the parameters", "text": "For the estimation of the parameter, some results are known for two-symmetric communities. In the logarithmic degree regime, since the SDP is agnostic to the parameters (it is a relaxation of the min-bisection), and the parameters can be estimated by recovering the communities [ABH14, BH14, Ban15]. For the constant-degree regime, [MNS12] shows that the parameters can be estimated above the threshold by counting cycles (which is efficiently approximated by counting non-backtracking walks). These are however for a fixed number of communities, namely 2. We also became recently aware of a parallel work [BCS15], which considers private graphon estimation (including SBMs). In particular, for the logarithmic degree regime, [BCS15] obtains a procedure to estimate parameters of graphons in an appropriate version of the L2 norm. This procedure is however not efficient.\nFor the general SBM, the results of [AS15] allow to find communities efficiently, however these rely on the knowledge of the parameters. Hence, a major open problem is to understand if these results can be extended without such a knowledge."}, {"heading": "2 Results", "text": "Agnostic algorithms are developed for the constant and diverging node degrees. These afford optimal accuracy scaling for large node degrees and achieve the CH-divergence limit for logarithmic node degrees in quasi-linear time. In particular, these solve the parameter estimation problems for SBM(n, p, \u03c9(1)Q) without knowing the number of communities. An example with real data is provided in Section 4."}, {"heading": "2.1 Definitions and terminologies", "text": "The general stochastic block model SBM(n, p,W ) is a random graph ensemble defined on the vertex-set V = [n], where each vertex v \u2208 V is assigned independently a hidden (or planted) label \u03c3v in [k] under a probability distribution p = (p1, . . . , pk) on [k], and each (unordered) pair of nodes (u, v) \u2208 V \u00d7V is connected independently with probability W\u03c3u,\u03c3v , where W\u03c3u,\u03c3v is specified by a symmetric k \u00d7 k matrix W with entries in [0, 1]. Note that G \u223c SBM(n, p,W ) denotes a random graph drawn under this model, without the hidden (or planted) clusters (i.e., the labels \u03c3v ) revealed. The goal is to recover these labels by observing only the graph.\nThis paper focuses on p independent of n (the communities have linear size), W dependent on n such that the average node degrees are either constant or logarithmically growing, and\nk fixed. These assumptions on p and k could be relaxed, for example to slowly growing k, but we leave this for future work. As discussed in the introduction, the above regimes for W are both motivated by applications, as networks are typically sparse [LLDM08, Str01] though the average degrees may not be small, and by the fact that interesting mathematical phenomena take place in these regimes. For convenience, we attribute specific notations for the model in these regimes:\nDefinition 1. For a symmetric matrix Q \u2208 Rk\u00d7k+ , G1(n, p,Q) denotes SBM(n, p,Q/n), and G2(n, p,Q) denotes SBM(n, p, ln(n)Q/n).\nDefinition 2. (Partial recovery.) An algorithm recovers or detects communities in SBM(n, p,W ) with an accuracy of \u03b1 \u2208 [0, 1], if it outputs a labelling of the nodes {\u03c3\u2032(v), v \u2208 V }, which agrees with the true labelling \u03c3 on a fraction \u03b1 of the nodes with probability 1\u2212 on(1). The agreement is maximized over relabellings of the communities.\nDefinition 3. (Exact recovery.) Exact recovery is solvable in SBM(n, p,W ) for a community partition [k] = tts=1As, where As is a subset of [k], if there exists an algorithm that takes G \u223c SBM(n, p,W ) and assigns to each node in G an element of {A1, . . . , At} that contains its true community6 with probability 1\u2212 on(1). Exact recovery is solvable in SBM(n, p,W ) if it is solvable for the partition of [k] into k singletons, i.e., all communities can be recovered.\nThe problem is solvable information-theoretically if there exists an algorithm that solves it, and efficiently if the algorithm runs in polynomial-time in n. Note that exact recovery for the partition [k] = {i} t ([k] \\ {i}) is equivalent to extracting community i. In general, recovering a partition [k] = tts=1As is equivalent to merging the communities that are in a common subset As and recovering the merged communities. Note also that exact recovery in SBM(n, p,W ) requires the graph not to have vertices of degree 0 in multiple communities with high probability (i.e., connectivity in the symmetric case). Therefore, for exact recovery, we focus below on W = ln(n)n Q where Q is fixed."}, {"heading": "2.2 Partial recovery", "text": "Our main result in the Appendix (Theorme 6) applies to SBM(n, p,Q/n) with arbitrary Q. We provided here a specific instance which is easier to parse.\nTheorem 3 (See Theorem 6). Given \u03b4 > 0 and for any k \u2208 Z, p \u2208 (0, 1)k with \u2211 pi = 1 and 0 < \u03b4 \u2264 min pi, and any symmetric matrix Q with no two rows equal such that every entry in Qk is positive (in other words, Q such that there is a nonzero probability of a path between vertices in any two communities in a graph drawn from G1(n, p, cQ)), there exists (c) = O(1/ ln(c)) such that for all sufficiently large c, Agnostic-sphere-comparison(G, \u03b4) detects communities in graphs drawn from G1(n, p, cQ) with accuracy at least 1\u2212 e\u2212\u2126(c) in On(n 1+ (c)) time.\nNote that a vertex in community i has degree 0 with probability exponential in c, and there is no way to differentiate between vertices of degree 0 from different communities. So,\n6This is again up to relabellings of the communities.\nan error rate that decreases exponentially with c is optimal. The above gives in particular the parameter estimation in the case c = \u03c9(1) (see also Lemma 17 in the Appendix).\nThe general result in the Appendix yields the following refined results in the k-block symmetric case.\nTheorem 4. Consider the k-block symmetric case. In other words, pi = 1 k for all i, and Qi,j is \u03b1 if i = j and \u03b2 otherwise. The vector whose entries are all 1s is an eigenvector of PQ with eigenvalue \u03b1+(k\u22121)\u03b2k , and every vector whose entries add up to 0 is an eigenvector of PQ with eigenvalue \u03b1\u2212\u03b2k . So, \u03bb = \u03b1+(k\u22121)\u03b2 k and \u03bb \u2032 = \u03b1\u2212\u03b2k and (\u03bb\u2032)2 \u03bb = (a\u2212b)2 k(a+(k\u22121)\u03b2) . Then, as long as 1609 k(\u03b1+ (k \u2212 1)\u03b2) 7 < (\u03b1\u2212 \u03b2)8 and 4k(\u03b1+ (k \u2212 1)\u03b2)3 < (\u03b1\u2212 \u03b2)4, there exist a constant c > 0 such that Agnostic-sphere-comparison(G, 1/k) detects communities with an accuracy of 1\u2212O(e\u2212c(\u03b1\u2212\u03b2)2/(\u03b1+(k\u22121)\u03b2)) for sufficiently large (\u03b1\u2212 \u03b2)2/(\u03b1+ (k \u2212 1)\u03b2).\nWe refer to Section 4 for an example of implementation with real data."}, {"heading": "2.3 Exact recovery", "text": "Recall that from [AS15], exact recovery is information-theoretically solvable in the stochastic block model G2(n, p,Q) for a partition [k] = tts=1As if and only if for all i and j in different subsets of the partition,\nD+((PQ)i, (PQ)j) \u2265 1. (3)\nWe next show that this can be achieved without knowing the parameters. Recall that the finest partition is the largest partition of [k] that ensure (19).\nTheorem 5. (See Theorem 7) The Agnostic-degree-profiling algorithm (see Section 3.2) recovers the finest partition in any G2(n, p,Q), it uses no input except the graph in question, and runs in o(n1+ ) time for all > 0. In particular, exact recovery is efficiently and universally solvable whenever it is information-theoretically solvable.\nThe proof assumes that the entries of Q are non-zero, see Remark 1 for zero entries. To achieve this result we rely on a two step procedure. First an algorithm is developed to recover all but a vanishing fraction of nodes \u2014 this is the main focus of our partial recovery result \u2014 and then a procedure is used to \u201cclean up\u201d the leftover graphs using the node degrees of the preliminary classification. This turns out to be much more efficient than aiming for an algorithm that directly achieves exact recovery. We already used this technique in [AS15], but here we also deal with the difficulties resulting from not knowing the SBM\u2019s parameters."}, {"heading": "3 Proof Techniques and Algorithms", "text": "3.1 Partial recovery and the Agnostic-sphere-comparison algorithm\nThe first key observation used to classify graphs\u2019 vertices is that if v is a vertex in a graph drawn from G1(n, p,Q) then for all small r the expected number of vertices in community i that are r edges away from v is approximately ei \u00b7 (PQ)re\u03c3v . So, we define:\nDefinition 4. For any vertex v, let Nr(v) be the set of all vertices with shortest path to v of length r. We also refer to the vector with i-th entry equal to the number of vertices in Nr(v) that are in community i as Nr(v). If there are multiple graphs that v could be considered a vertex in, let Nr[G](v) be the set of all vertices with shortest paths in G to v of length r.\nOne could probably determine PQ and e\u03c3 given the values of (PQ) re\u03c3v for a few different r, but using Nr(v) to approximate that would require knowing how many of the vertices in Nr(v) are in each community. So, we attempt to get information relating to how many vertices in Nr(v) are in each community by checking how it connects to Nr\u2032(v\n\u2032) for some vertex v\u2032 and integer r\u2032. The obvious way to do this would be to compute the cardinality of their intersection. Unfortunately, whether a given vertex in community i is in Nr(v) is not independent of whether it is in Nr\u2032(v\n\u2032), which causes the cardinality of |Nr(v) \u2229Nr\u2032(v\u2032)| to differ from what one would expect badly enough to disrupt plans to use it for approximations.\nIn order to get around this, we randomly assign every edge in G to a set E with probability c. We hence define the following. Definition 5. For any vertices v, v\u2032 \u2208 G, r, r\u2032 \u2208 Z, and subset of G\u2019s edges E, let Nr,r\u2032[E](v \u00b7 v\u2032) be the number of pairs of vertices (v1, v2) such that v1 \u2208 Nr[G\\E](v), v2 \u2208 Nr\u2032[G\\E](v\u2032), and (v1, v2) \u2208 E.\nNote that E and G\\E are disjoint; however, G is sparse enough that even if they were generated independently a given pair of vertices would have an edge between them in both with probability O( 1\nn2 ). So, E is approximately independent of G\\E. Thus, for any\nv1 \u2208 Nr[G/E](v) and v2 \u2208 Nr\u2032[G/E](v\u2032), (v1, v2) \u2208 E with a probability of approximately cQ\u03c3v1 ,\u03c3v2/n. As a result,\nNr,r\u2032 [E](v \u00b7 v\u2032) \u2248 Nr[G\\E](v) \u00b7 cQ\nn Nr\u2032[G\\E](v\n\u2032)\n\u2248 ((1\u2212 c)PQ)re\u03c3v \u00b7 cQ n ((1\u2212 c)PQ)r\u2032e\u03c3v\u2032 = c(1\u2212 c)r+r\u2032e\u03c3v \u00b7Q(PQ)r+r \u2032 e\u03c3v\u2032/n\nLet \u03bb1, ..., \u03bbh be the distinct eigenvalues of PQ, ordered so that |\u03bb1| \u2265 |\u03bb2| \u2265 ... \u2265 |\u03bbh| \u2265 0. Also define h\u2032 so that h\u2032 = h if \u03bbh 6= 0 and h\u2032 = h\u2212 1 if \u03bbh = 0. If Wi is the eigenspace of PQ corresponding to the eigenvalue \u03bbi, and PWi is the projection operator on to Wi, then\nNr,r\u2032[E](v \u00b7 v\u2032) \u2248 c(1\u2212 c)r+r \u2032 e\u03c3v \u00b7Q(PQ)r+r \u2032 e\u03c3v\u2032/n (4)\n= c(1\u2212 c)r+r\u2032\nn\n(\u2211 i PWi(e\u03c3v) ) \u00b7Q(PQ)r+r\u2032 \u2211 j PWj (e\u03c3v\u2032 )  (5) = c(1\u2212 c)r+r\u2032\nn\n\u2211 i,j PWi(e\u03c3v) \u00b7Q(PQ)r+r \u2032 PWj (e\u03c3v\u2032 ) (6)\n= c(1\u2212 c)r+r\u2032\nn\n\u2211 i,j PWi(e\u03c3v) \u00b7 P\u22121(\u03bbj)r+r \u2032+1PWj (e\u03c3v\u2032 ) (7)\n= c(1\u2212 c)r+r\u2032\nn\n\u2211 i \u03bbr+r \u2032+1 i PWi(e\u03c3v) \u00b7 P \u22121PWi(e\u03c3v\u2032 ) (8)\nwhere the final equality holds because for all i 6= j,\n\u03bbiPWi(e\u03c3v) \u00b7 P\u22121PWj (e\u03c3v\u2032 ) = (PQPWi(e\u03c3v)) \u00b7 P \u22121PWj (e\u03c3v\u2032 )\n= PWi(e\u03c3v) \u00b7QPWj (e\u03c3v\u2032 ) = PWi(e\u03c3v) \u00b7 P\u22121\u03bbjPWj (e\u03c3v\u2032 ),\nand since \u03bbi 6= \u03bbj , this implies that PWi(e\u03c3v) \u00b7 P\u22121PWj (e\u03c3v\u2032 ) = 0. In order to simplify the terminology,\nDefinition 6. Let \u03b6i(v \u00b7 v\u2032) = PWi(e\u03c3v) \u00b7 P\u22121PWi(e\u03c3v\u2032 ) for all i, v, and v \u2032.\nEquation (14) is dominated by the \u03bbr+r \u2032+1 1 term, so getting good estimate of the \u03bb r+r\u2032+1 2\nthrough \u03bbr+r \u2032+1\nh\u2032 terms requires cancelling it out somehow. As a start, if \u03bb1 > \u03bb2 > \u03bb3 then\nNr+2,r\u2032[E](v \u00b7 v\u2032) \u00b7Nr,r\u2032[E](v \u00b7 v\u2032)\u2212N2r+1,r\u2032[E](v \u00b7 v \u2032)\n\u2248 c 2(1\u2212 c)2r+2r\u2032+2\nn2 (\u03bb21 + \u03bb 2 2 \u2212 2\u03bb1\u03bb2)\u03bbr+r \u2032+1 1 \u03bb r+r\u2032+1 2 \u03b61(v \u00b7 v \u2032)\u03b62(v \u00b7 v\u2032)\nNote that the left hand side of this expression is equal to det \u2223\u2223\u2223\u2223 Nr,r\u2032[E](v \u00b7 v\u2032) Nr+1,r\u2032[E](v \u00b7 v\u2032)Nr+1,r\u2032[E](v \u00b7 v\u2032) Nr+2,r\u2032[E](v \u00b7 v\u2032) \u2223\u2223\u2223\u2223.\nMore generally, in order to get an expression that can be used to estimate the \u03bbi and \u03b6i(v \u00b7v\u2032), we consider the determinant of the following.\nDefinition 7. Let Mm,r,r\u2032[E](v \u00b7 v\u2032) be the m \u00d7 m matrix such that Mm,r,r\u2032[E](v \u00b7 v\u2032)i,j = Nr+i+j,r\u2032[E](v \u00b7 v\u2032) for each i and j.\nTo the degree that approximation 8 holds and c is small, each column of Mm,r,r\u2032[E](v \u00b7 v\u2032) is a linear combination of the vectors\nc(1\u2212 c)r+r\u2032\nn \u03b6i(v \u00b7 v\u2032)\u03bbr+r\n\u2032\ni [1, \u03bbi, \u03bb 2 i , ..., \u03bb m\u22121 i ] t\nwith coefficients that depend only on {\u03bb1, ..., \u03bbh}. So, by linearity of the determinant in one column, det(Mm,r,r\u2032[E](v \u00b7 v\u2032)) is a linear combination of these vector\u2019s wedge products with coefficients that are independent of r and r\u2032. By antisymmetry of wedge products, only the products that use m different such vectors contribute to the determinant, and the products involving the eigenvalues of highest magnitude will dominate. As a result, there exist constants \u03b3(\u03bb1, ..., \u03bbm) and \u03b3 \u2032(\u03bb1, ..., \u03bbm) such that\ndet(Mm,r,r\u2032[E](v \u00b7 v\u2032)) \u2248 cm(1\u2212 c)m(r+r\u2032)\nnm \u03b3(\u03bb1, ..., \u03bbm) m\u220f i=1 \u03bbr+r \u2032+1 i \u03b6i(v \u00b7 v \u2032)\nif |\u03bbm| > |\u03bbm+1|, and\ndet(Mm,r,r\u2032[E](v \u00b7 v\u2032)) \u2248 c m(1\u2212 c)m(r+r\u2032) nm \u00b7 m\u22121\u220f i=1 \u03bbr+r \u2032+1 i \u03b6i(v \u00b7 v \u2032)\n\u00b7 ( \u03b3(\u03bb1, ..., \u03bbm)\u03bb r+r\u2032+1 m \u03b6m(v \u00b7 v\u2032) + \u03b3\u2032(\u03bb1, ..., \u03bbm)\u03bbr+r \u2032+1 m+1 \u03b6m+1(v \u00b7 v \u2032) )\nif |\u03bbm| = |\u03bbm+1|. These facts suggest the following plan for estimating the eigenvalues corresponding to a graph. First, pick several vertices at random. Then, use the fact that |Nr[G\\E](v)| \u2248 ((1 \u2212 c)\u03bb1)r for any good vertex v to estimate \u03bb1. Next, use the formulas above about det(Mm,r,r\u2032[E](v \u00b7 v)) to get an approximation of h\u2032 and all of PQ\u2019s eigenvalues for each selected vertex. Finally, take the median of these estimates.\nNow, note that whether or not |\u03bbm| = |\u03bbm+1|, we have\ndet(Mm,r+1,r\u2032[E](v \u00b7 v\u2032))\u2212 (1\u2212 c)m\u03bbm+1 m\u22121\u220f i=1 \u03bbi det(Mm,r,r\u2032[E](v \u00b7 v\u2032)) \u2248 c m\nnm \u03b3(\u03bb1, ..., \u03bbm) \u03bbm \u2212 \u03bbm+1 (1\u2212 c)m\u03bbm m\u220f i=1 ((1\u2212 c)\u03bbi)r+r \u2032+2\u03b6i(v \u00b7 v\u2032)\nThat means that det(Mm,r+1,r\u2032[E](v \u00b7 v\u2032))\u2212 (1\u2212 c)m\u03bbm+1 \u220fm\u22121 i=1 \u03bbi det(Mm,r,r\u2032[E](v \u00b7 v\u2032))\ndet(Mm\u22121,r+1,r\u2032[E](v \u00b7 v\u2032))\u2212 (1\u2212 c)m\u22121\u03bbm \u220fm\u22122 i=1 \u03bbi det(Mm\u22121,r,r\u2032[E](v \u00b7 v\u2032)) \u2248 c (1\u2212 c)n \u03b3(\u03bb1, ..., \u03bbm)\n\u03b3(\u03bb1, ..., \u03bbm\u22121) \u03bbm\u22121(\u03bbm \u2212 \u03bbm+1) \u03bbm(\u03bbm\u22121 \u2212 \u03bbm) ((1\u2212 c)\u03bbm)r+r \u2032+2\u03b6m(v \u00b7 v\u2032)\nThis fact can be used in combination with estimates of PQ\u2019s eigenvalues to approximate \u03b6i(v \u00b7 v\u2032) for arbitrary v, v\u2032, and i.\nOf course, this requires r and r\u2032 to be large enough that\nc(1\u2212 c)r+r\u2032\nn \u03bbr+r \u2032+1 i \u03b6i(v \u00b7 v \u2032)\nis large relative to the error terms for all i \u2264 h\u2032. At a minimum, that requires that |(1\u2212 c)\u03bbi|r+r\n\u2032+1 = \u03c9(n). On a different note, for any v and v\u2032,\n0 \u2264 PWi(e\u03c3v \u2212 e\u03c3v\u2032 ) \u00b7 P \u22121PWi(e\u03c3v \u2212 e\u03c3v\u2032 ) = \u03b6i(v \u00b7 v)\u2212 2\u03b6i(v \u00b7 v \u2032) + \u03b6i(v \u2032 \u00b7 v\u2032)\nwith equality for all i if and only if \u03c3v = \u03c3v\u2032 , so sufficiently good approximations of \u03b6i(v \u00b7 v), \u03b6i(v \u00b7 v\u2032) and \u03b6i(v\u2032 \u00b7 v\u2032) can be used to determine which pairs of vertices are in the same community.\nOne could generate a reasonable classification based solely on this method of comparing vertices. However, that would require computing Nr,r\u2032[E](v \u00b7 v) for every vertex in the graph with fairly large r + r\u2032, which would be slow. Instead, we use the fact that for any vertices v, v\u2032, and v\u2032\u2032 with \u03c3v = \u03c3v\u2032 6= \u03c3v\u2032\u2032 ,\n\u03b6i(v \u2032 \u00b7 v\u2032)\u2212 2\u03b6i(v \u00b7 v\u2032) + \u03b6i(v \u00b7 v) = 0 \u2264 \u03b6i(v\u2032\u2032 \u00b7 v\u2032\u2032)\u2212 2\u03b6i(v \u00b7 v\u2032\u2032) + \u03b6i(v \u00b7 v)\nfor all i, and the inequality is strict for at least one i. So, subtracting \u03b6i(v \u00b7 v) from both sides gives us that\n\u03b6i(v \u2032 \u00b7 v\u2032)\u2212 2\u03b6i(v \u00b7 v\u2032) \u2264 \u03b6i(v\u2032\u2032 \u00b7 v\u2032\u2032)\u2212 2\u03b6i(v \u00b7 v\u2032\u2032)\nfor all i, and the inequality is still strict for at least one i. So, given a representative vertex in each community, we can determine which of them a given vertex, v, is in the same community as without needing to know the value of \u03b6i(v \u00b7 v). This runs fairly quickly if \u03b6i(v \u00b7 v\u2032) is approximated using Nr,r\u2032[E](v\u2032 \u00b7 v) such that r is large and r\u2032 is small because the algorithm only requires focusing on |Nr\u2032(v)| vertices. This leads to the following plan for partial recovery. First, randomly select a set of vertices that is large enough to contain at least one vertex from each community with high probability. Next, compare all of the selected vertices in an attempt to determine which of them are in the same communities. Then, pick one in each community. After that, use the algorithm referred to above to attempt to determine which community each of the remaining vertices is in. As long as there actually was at least one vertex from each community in the initial set and none of the approximations were particularly bad, this should give a reasonably accurate classification.\nThe risk that this randomly gives a bad classification due to a bad set of initial vertices can be mitigated as follows. First, repeat the previous classification procedure several times. Assuming that the procedure gives a good classification more often than not, the good classifications should comprise a set that contains more than half the classifications and which has fairly little difference between any two elements of the set. Furthermore, any such set would have to contain at least one good classification, so none of its elements could be too bad. So, find such a set and average its classifications together. This completes the Agnostic-Sphere-comparison-algorithm. We refer to Section 6 for a detailed version.\n3.2 Exact recovery and the Agnostic-degree-profiling algorithm\nThe exact recovery part is similar to [AS15] and uses the fact that once a good enough clustering has been obtained from Agnostic-sphere-comparison, the classification can be finished by making local improvements based on the nodes\u2019 neighborhoods. The key result here is that, when testing between two multivariate Poisson distributions of means log(n)\u03bb1 and log(n)\u03bb2 respectively, where \u03bb1, \u03bb2 \u2208 Zk+, the probability of error (of say maximum a posteriori decoding) is\n\u0398 ( n\u2212D+(\u03bb1,\u03bb2)\u2212o(1) ) . (9)\nThis is proved in [AS15]. In the case of unknown parameters, the algorithmic approach is largely unchanged, adding a step where the best known classification is used to estimate P and Q prior to any step in which vertices are classified based on their neighbors. The analysis of the algorithm requires however some careful handling.\nFirst, it is necessary to prove that given a labelling of the graph\u2019s vertices with an error rate of x, one can compute approximations of P and Q that are within O(x+ log(n)/ \u221a n) of their true values with probability 1 \u2212 o(1). Secondly, one needs to modify the robust degree profiling lemma to show that attempting to determine vertices\u2019 communities based on estimates of p and Q that are off by at most \u03b4, p\u2032 and Q\u2032, and a classification of its neighbors that has an error rate of \u03b4 classifies the vertices with an error rate only eO(\u03b4 logn) times higher than it would be given accurate values of p and Q and accurate classifications of the vertices\u2019 neighbors. Combining these yields the conclusion that any errors in the\nestimates of the SBM\u2019s parameters do not disrupt vertex classification any worse than the errors in the preliminary classifications already were.\nThe Agnostic-degree-profiling algorithm. The inputs are (G, \u03b3), where G is a graph, and \u03b3 \u2208 [0, 1] (see Theorem 7 for how to set \u03b3).\nThe algorithm outputs an assignment of each vertex to one of the groups of communities {A1, . . . , At}, where A1, . . . , At is the partition of [k] in to the largest number of subsets such that D+((pQ)i, (pQ)j) \u2265 1 for all i, j in [k] that are in different subsets (this is called the \u201cfinest partition\u201d). It does the following:\n(1) Define the graph g\u2032 on the vertex set [n] by selecting each edge in g independently with probability \u03b3, and define the graph g\u2032\u2032 that contains the edges in g that are not in g\u2032.\n(2) Run Agnostic-sphere-comparison on g\u2032 to obtain the preliminary classification \u03c3\u2032 \u2208 [k]n (see Section 7.1.)\n(3) Determine the size of each alleged community, and the edge density between each pair of alleged communities.\n(4) For each node v \u2208 [n], determine in which community node v is most likely to belong to based on its degree profile computed from the preliminary classification \u03c3\u2032 (see Section 7.2.2), and call it \u03c3\u2032\u2032v\n(5) Use \u03c3\u2032\u2032v to get new estimates of p and Q. (6) For each node v \u2208 [n], determine in which group A1, . . . , At node v is most likely to belong to based on its degree profile computed from the preliminary classification \u03c3\u2032\u2032 (see Section 7.2.2)."}, {"heading": "4 An example with real data", "text": "We have tested a simplified version of our algorithm on the data from \u201cThe political blogosphere and the 2004 US Election\u201d [AG05], which contains a list of political blogs that were classified as liberal or conservative, and links between the blogs.\nThe algorithm we used has a few major modifications relative to our standard algorithm. First of all, instead of using Nr,r\u2032(v \u00b7 v\u2032) as its basic tool for comparing vertices, it uses a different measure, N \u2032r,r\u2032(v \u00b7 v\u2032) which is defined as the fraction of pairs of an edge leaving the ball of radius r centered on v and an edge leaving the ball of radius r\u2032 centered on v\u2032 which hit the same vertex but are not the same edge. Making the measure a fraction of the pairs rather than a count of pairs was necessary to prevent N \u2032r,r\u2032(v \u00b7 v\u2032) from being massively dependent on the degrees of v and v\u2032, which would have resulted in the increased variance in vertex degree obscuring the effects of \u03c3v and \u03c3v\u2032 on N \u2032 r,r\u2032(v \u00b7 v\u2032). The other changes to the definition make the measure somewhat less reliable, but it is still useable as long as the average degree is fairly high and v 6= v\u2032.\nSecondly, the version of Vertex-comparison-algorithm we used simply concludes that two vertices, v and v\u2032, are in different communities if N \u2032r,r\u2032(v \u00b7 v\u2032) is below average and the same community otherwise. This is reasonable because of the following facts. For one thing, because the normalization converts the dominant term to a constant, N \u2032r,r\u2032(v \u00b7 v\u2032) is approximately affine in (\u03bb2/\u03bb1)\nr+r\u2032\u03b62(v \u00b7 v\u2032)/n. Also, as a result of the symmetry between communities, \u03b62(v \u00b7 v) is the same for all v. So, \u03b62(v \u00b7 v) \u2212 2\u03b62(v \u00b7 v\u2032) + \u03b62(v\u2032 \u00b7 v\u2032) is also affine in \u03b62(v \u00b7 v\u2032). Furthermore, there are only two possible values of \u03b62(v \u00b7 v\u2032) by symmetry,\nand \u03bb2 > 0, so \u03b62(v \u00b7 v) \u2212 2\u03b62(v \u00b7 v\u2032) + \u03b62(v\u2032 \u00b7 v\u2032) > 0 iff (\u03bb2/\u03bb1)r+r \u2032 \u03b62(v \u00b7 v\u2032)/n is below average. Finally, because both communities have the same average degree, \u03b61(v, v \u2032) is independent of v and v\u2032 so \u03b61(v \u00b7 v) \u2212 2\u03b61(v \u00b7 v\u2032) + \u03b61(v\u2032 \u00b7 v\u2032) is always 0. The version of Vertex-classification-algorithm we used is comparably modified.\nFinally, our algorithm generates reference vertices by repeatedly picking two vertices at random and comparing them. If it concludes that they are in different communities and they both have above-average degree, it accepts them as reference vertices; otherwise it tries again. Requiring above-average degree is useful because a higher degree vertex is less likely to have its neighborhood distorted by a couple of atypical neighbors.\nOut of 40 trials, the resulting algorithm gave a reasonably good classification 37 times. Each of these classified all but 56 to 67 of the 1222 vertices in the graph\u2019s main component correctly. The state-of-the-art described in [CG15] gives a lowest value at 58, with the best algorithms around 60, while algorithms regularized spectral methods such as the one in [QR13] obtain about 80 errors."}, {"heading": "5 Open problems", "text": "The current result should also extend directly to a slowly growing number of communities (e.g., up to logarithmic). It would be interesting to extend the current approach to smaller sized communities or larger numbers of communities (watching the complexity scaling with the number of communities), as well as more general models with corrected-degrees, labeled-edges, or overlapping communities (though linear-sized overlapping communities can be treated with the approach of [AS15])."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 Partial Recovery", "text": ""}, {"heading": "7.1.1 Formal results", "text": "Theorem 6. For any \u03b4 > 0, there exists an algorithm (Agnostic-sphere-comparison) such that the following holds. Given any k \u2208 Z, p \u2208 (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal, let \u03bb be the largest eigenvalue of PQ, and \u03bb\u2032 be the eigenvalue of PQ with the smallest nonzero magnitude. For any x, x\u2032, and such that x is either a unit reciprocal or an integer, is a rational number of the form 1z or 1\u2212 1 z , and all of the following hold:\n2ke \u2212 .9x 2\u03bb\u20322 min pi 16\u03bbk3/2((min pi) \u22121/2+x) / ( 1\u2212 e \u2212 .9x 2\u03bb\u20322 min pi 16\u03bbk3/2((min pi) \u22121/2+x) \u00b7(( .9\u03bb \u20324 4\u03bb3 )\u22121) ) < 1\n2\n.9(\u03bb\u20322/2)4 > \u03bb7\n0 < x \u2264 x\u2032 < \u03bbk \u03bb\u2032min pi (2\u03bb3/\u03bb\u20322)1\u2212 /3 < \u03bb (1 + /3) > log(\u03bb)/ log(\u03bb\u20322/2\u03bb) 13(2x\u2032(min pj) \u22121/2 + (x\u2032)2) < min\n6=0 (wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032}))\nEvery entry of Qk is positive\n\u2203w \u2208 Rk such that QPw = \u03bbw,w \u00b7 Pw = 1, and x \u2264 minwi/2. \u03b4 \u2264 min pi\n8 ln(4b1/\u03b4c)b1/\u03b4ce \u2212 x 2\u03bb\u20322\u03b4 16\u03bbb1/\u03b4c3/2(\u03b4\u22121/2+x) / ( 1\u2212 e \u2212 x 2\u03bb\u20322\u03b4 16\u03bbb1/\u03b4c3/2(\u03b4\u22121/2+x) \u00b7(( \u03bb \u20324 4\u03bb3 )\u22121) ) < \u03b4\nmin pi > 8ke \u2212 .9x \u20322\u03bb\u20322 min pi 16\u03bbk3/2((min pi) \u22121/2+x\u2032) / ( 1\u2212 e \u2212.9 x \u20322\u03bb\u20322 min pi 16\u03bbk3/2((min pi) \u22121/2+x\u2032) \u00b7(( .9\u03bb \u20324 4\u03bb3 )\u22121) )\nWith probability 1 \u2212 o(1), the algorithm runs in O(n1+ 2 3 log n) time and detects communities in graphs drawn from G1(n, p,Q) with accuracy at least 1\u2212 3y\u2032 without any input beyond \u03b4 and the graph, where\ny\u2032 = 2ke \u2212 .9x \u20322\u03bb\u20322 min pi 16\u03bbk3/2((min pi) \u22121/2+x\u2032) / ( 1\u2212 e \u2212.9 x \u20322\u03bb\u20322 min pi 16\u03bbk3/2((min pi) \u22121/2+x\u2032) \u00b7(( .9\u03bb \u20324 4\u03bb3 )\u22121) )\nConsidering the way \u03b4, , x, and x\u2032 scale when Q is multiplied by a scalar yields the following corollary.\nCorollary 2. For any k \u2208 Z, p \u2208 (0, 1)k with |p| = 1, and symmetric matrix Q with no two rows equal such that Qk has all positive entries, there exist (c) = O(1/ ln(c)) such that for all sufficiently large c, Agnostic-sphere-comparison detects communities in graphs drawn from G1(n, p, cQ) with accuracy at least 1\u2212 e\u2212\u2126(c) in On(n1+ (c)).\nIf instead of having constant average degree, one has an average degree which increases as n increases, one can slowly reduce b, \u03b4, and as n increases, leading to the following corollary.\nCorollary 3. For any k \u2208 Z, p \u2208 [0, 1]k with |p| = 1, symmetric matrix Q with no two rows equa such that Qm has all positive entries for sufficiently large ml, and c(n) such that c = \u03c9(1), Agnostic-sphere-comparison detects the communities with accuracy 1\u2212 o(1) in G1(n, p, c(n)Q) and runs in o(n1+ ) time for all > 0.\nThese corollaries are important as they show that if the entries of the connectivity matrix Q are amplified by a coefficient growing with n, almost exact recovery is achieved by (Agnostic-sphere-comparison) without parameter knowledge."}, {"heading": "7.1.2 Proof of Theorem 6", "text": "Proving Theorem 6 will require establishing some terminology. First, let \u03bb1, ..., \u03bbh be the distinct eigenvalues of PQ, ordered so that |\u03bb1| \u2265 |\u03bb2| \u2265 ... \u2265 |\u03bbh| \u2265 0 and if |\u03bbi| = |\u03bbi+1| then \u03bbi > 0 > \u03bbi+1. Also define h\n\u2032 so that h\u2032 = h if \u03bbh 6= 0 and h\u2032 = h \u2212 1 if \u03bbh = 0. In addition to this, let d be the largest sum of a column of PQ.\nDefinition 12. For any graph G drawn from G1(n, p,Q) and any set of vertices in G, V , let \u2212\u2192 V be the vector such that \u2212\u2192 V i is the number of vertices in V that are in community i. Define w1(V ), w2(V ), ..., wh(V ) such that \u2212\u2192 V = \u2211 wi(V ) and wi(V ) is an eigenvector of PQ with eigenvalue \u03bbi for each i.\nw1(V ), ..., wh(V ) are well defined because Rk is the direct sum of PQ\u2019s eigenspaces. The key intuition behind their importance is that if V \u2032 is the set of vertices adjacent to vertices in V then \u2212\u2192 V \u2032 \u2248 PQ \u2212\u2192 V , so wi(V \u2032) \u2248 PQ \u00b7 wi(V ) = \u03bbiwi(V ).\nDefinition 13. For any vertex v, let Nr(v) be the set of all vertices with shortest path to v of length r. If there are multiple graphs that v could be considered a vertex in, let Nr[G\u2032](v) be the set of all vertices with shortest paths in G\u2032 to v of length r.\nWe also typically refer to \u2212\u2212\u2212\u2212\u2212\u2212\u2192 Nr[G\u2032](v) as simply Nr[G\u2032](v), as the context will make it clear\nwhether the expression refers to a set or vector.\nDefinition 14. A vertex v of a graph drawn from G1(n, p,Q) is (R, x)-good if for all 0 \u2264 r < R and w \u2208 Rk with w \u00b7 Pw = 1\n|w \u00b7Nr+1(v)\u2212 w \u00b7 PQNr(v)| \u2264 x\u03bbh\u2032\n2\n( \u03bb2h\u2032\n2\u03bb1 )r and (R, x)-bad otherwise.\nNote that since any such w can be written as a linear combination of the ei, v is\n(R, x)-good if |ei \u00b7 Nr+1(v) \u2212 ei \u00b7 PQNr(v)| \u2264 x\u03bbh\u20322\n( \u03bb2 h\u2032\n2\u03bb1\n)r\u221a pi/k for all 1 \u2264 i \u2264 k and\n0 \u2264 r < R.\nLemma 1. If v is a (R, x)-good vertex of a graph drawn from G1(n, p,Q), then for every 0 \u2264 r \u2264 R, |Nr(v)| \u2264 \u03bbr1 \u221a k((min pi) \u22121/2 + x).\nProof. First, note that for any eigenvector of PQ, w, and r < R,\n|(P\u22121w) \u00b7Nr+1(v)\u2212 (P\u22121w) \u00b7 PQNr(v)| \u2264 x\u03bbh\u2032\n2\n( \u03bb2h\u2032\n2\u03bb1\n)r\u221a w \u00b7 P\u22121w\nSo, by the triangle inequality,\n|(P\u22121w) \u00b7Nr+1(v)| \u2264 |(P\u22121PQw) \u00b7Nr(v)|+ x\u03bbh\u2032\n2\n( \u03bb2h\u2032\n2\u03bb1\n)r\u221a w \u00b7 P\u22121w\n\u2264 \u03bb1|(P\u22121w) \u00b7Nr(v)|+ x ( \u03bb1 2 )r+1\u221a w \u00b7 P\u22121w\nThus, for any r \u2264 R, it must be the case that\n|(P\u22121w) \u00b7Nr(v)| \u2264 \u03bbr1|(P\u22121w) \u00b7N0(v)|+ r\u2211\nr\u2032=1\n\u03bbr\u2212r \u2032 1 \u00b7 x ( \u03bb1 2 )r\u2032 \u221a w \u00b7 P\u22121w\n\u2264 \u03bbr1 ( |w\u03c3v/p\u03c3v |+ x \u221a w \u00b7 P\u22121w ) Now, define w1,..., wh such that PQwi = \u03bbiwi for each i and p = \u2211h i=1wi. For any i, j,\n\u03bbiwi \u00b7 P\u22121wj = (PQwi) \u00b7 P\u22121wj = wi \u00b7 P\u22121PQwj = \u03bbjwi \u00b7 P\u22121wj\nIf i 6= j, then \u03bbi 6= \u03bbj , so this implies that wi \u00b7 P\u22121wj = 0. It follows from this that\u2211 i wi \u00b7 P\u22121wi = \u2211 i,j wi \u00b7 P\u22121wj\n= (\u2211 i wi ) \u00b7 P\u22121 \u2211 j wj  = p \u00b7 P\u22121p = 1\nAlso, for any i, it is the case that |(wi)\u03c3v/p\u03c3v | \u2264 \u221a (wi)\u03c3v \u00b7 p\u22121\u03c3v \u00b7 (wi)\u03c3v/ \u221a p\u03c3v \u2264 (min pi)\u22121/2 \u221a wi \u00b7 P\u22121wi\nTherefore, for any r \u2264 R, we have that\n|Nr(v)| = |(P\u22121p) \u00b7Nr(v)| \u2264 \u2211 i |(P\u22121wi) \u00b7Nr(v)|\n\u2264 \u03bbr1 \u2211 i |(wi)\u03c3v/p\u03c3v |+ \u03bbr1x \u2211 i \u221a wi \u00b7 P\u22121wi \u2264 \u03bbr1 \u221a k((min pi) \u22121/2 + x)\nThe following two lemmas are proved in [AS15].\nLemma 2. Let k \u2208 Z, p \u2208 (0, 1)k with |p| = 1, Q be a symmetric matrix such that \u03bb4h\u2032 > 4\u03bb31, and 0 < x < \u03bb1k\u03bbh\u2032 min pi . Then there exists\ny < 2ke \u2212\nx2\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x) / 1\u2212 e\u2212 x2\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x) \u00b7(( \u03bb4h\u20324\u03bb31 )\u22121) \nand R(n) = \u03c9(1) such that at least 1\u2212 y of the vertices of a graph drawn from G1(n, p,Q) are (R(n), x)-good with probability 1\u2212 o(1).\nLemma 3. Let k \u2208 Z, p \u2208 (0, 1)k with |p| = 1, Q be a symmetric matrix such that \u03bb4h\u2032 > 4\u03bb31, R(n) = \u03c9(1), and > 0 such that (2\u03bb31/\u03bb 2 h\u2032) 1\u2212 /3 < \u03bb1. A vertex of a graph drawn from G(p,Q, n) is (R(n), x)-good but (1\u2212 /3ln\u03bb1 lnn, x)-bad with probability o(1). Definition 15. For any vertices v, v\u2032 \u2208 G, r, r\u2032 \u2208 Z, and subset of G\u2019s edges E, let Nr,r\u2032[E](v \u00b7 v\u2032) be the number of pairs of vertices (v1, v2) such that v1 \u2208 Nr[G\\E](v), v2 \u2208 Nr\u2032[G\\E](v \u2032), and (v1, v2) \u2208 E.\nNote that if Nr[G\\E](v) and Nr\u2032[G\\E](v \u2032) have already been computed, Nr,r\u2032[E](v \u00b7 v\u2032) can\nbe computed by means of the following algorithm, where E[v] = {v\u2032 : (v, v\u2032) \u2208 E}\nCompute-Nr,r\u2032[E](v \u00b7 v\u2032): for v1 \u2208 Nr\u2032[G\\E](v\u2032):\nfor v2 \u2208 E[v1] : if v2 \u2208 Nr[G\\E](v) :\ncount=count+1 return count\nNote that this runs in O((d+1)|Nr\u2032[G\\E](v\u2032)|) average time. The plan is to independently put each edge in G in E with probability c. Then the probability distribution of G\\E will\nbe G1(n, p, (1\u2212 c)Q), so Nr[G\\E](v) \u2248 ((1\u2212 c)PQ)re\u03c3v and Nr\u2032[G\\E](v\u2032) \u2248 ((1\u2212 c)PQ)r \u2032 e\u03c3v\u2032 . So, it will hopefully be the case that\nNr,r\u2032[E](v \u00b7v\u2032) \u2248 ((1\u2212c)PQ)re\u03c3v \u00b7cQ((1\u2212c)PQ)r \u2032 e\u03c3v\u2032/n = c(1\u2212c) r+r\u2032e\u03c3v \u00b7Q(PQ)r+r \u2032 e\u03c3v\u2032/n.\nMore rigorously, we have that:\nLemma 4. Choose p, Q, G drawn from G1(n, p,Q), E randomly selected from G\u2019s edges such that each of G\u2019s edges is independently assigned to E with probability c, and v, v\u2032 \u2208 G chosen independently from G\u2019s vertices. Then with probability 1\u2212 o(1),\n|Nr,r\u2032[E](v \u00b7 v\u2032)\u2212Nr[G\\E](v) \u00b7 cQNr\u2032[G\\E](v\u2032)/n| < (1 + \u221a |Nr[G\\E](v)| \u00b7 |Nr\u2032[G\\E](v\u2032)|/n) log n\nProof. Roughly speaking, for each v1 \u2208 Nr[G\\E](v) and v2 \u2208 Nr\u2032[G\\E](v\u2032), (v1, v2) \u2208 E with probability cQ\u03c3v1 ,\u03c3v2/n. This is complicated by the facts that (v1, v1) is never in E and no edge is in G\\E and E. However, this changes the expected value of Nr,r\u2032[E](v \u00b7 v\u2032) given G\\E by at most a constant unless G has more than double its expected number of edges, something that happens with probability o(1). Furthermore, whether (v1, v2) is in E is independent of whether (v\u20321, v \u2032 2) is in E unless (v \u2032 1, v \u2032 2) = (v1, v2) or (v \u2032 1, v \u2032 2) = (v2, v1). So, the variance of Nr,r\u2032[E](v \u00b7 v\u2032) is proportional to its expected value, which is\nO(|Nr[G\\E](v)| \u00b7 |Nr\u2032[G\\E](v\u2032)|/n).\nNr,r\u2032[E](v \u00b7 v\u2032) is within log n standard deviations of its expected value with probability 1\u2212 o(1), which completes the proof.\nNote that if \u2212\u2192v is an eigenvector of (1\u2212 c)PQ, \u221a PQ\u2212\u2192v is an eigenvector of the symmetric matrix (1\u2212c) \u221a PQ \u221a P . So, since eigenvectors of a symmetric matrix with different eigenvalues are orthogonal, we have\nNr[G\\E](v) \u00b7 cQNr\u2032[G\\E](v\u2032)/n = c\nn \u2211 i wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\nLemma 5 (Determinant Lemma). Let 0 < c < 1, x > 0, G be drawn from G1(n, p,Q), E be a subset of G\u2019s edges that independently contains each edge with probability c, and m \u2208 Z+. For any v, v\u2032 \u2208 G and r \u2265 r\u2032 \u2208 Z+, such that ((1\u2212 c)\u03bb2h\u2032/2)r+r \u2032 > \u03bbr+r \u2032\n1 n let Mm,r,r\u2032[E](v \u00b7 v\u2032) be the m \u00d7 m matrix such that Mm,r,r\u2032[E](v \u00b7 v\u2032)i,j = Nr+i+j,r\u2032[E](v \u00b7 v\u2032) for each i and j. There exist \u03b3 = \u03b3((1\u2212 c)\u03bbi,m) and \u03b3\u2032 = \u03b3\u2032((1\u2212 c)\u03bbi,m) such that \u03b3 is nonzero and for any r, r\u2032, and vertices v, v\u2032 \u2208 G, then with probability 1\u2212 o(1), either v is (r + 2m+ 1, x)-bad, v\u2032 is (r\u2032 + 1, x)-bad, or\n|det(Mm,r,r\u2032[E](v \u00b7 v\u2032))\u2212 cm\nnm m\u22121\u220f i=1 wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\n\u00b7 (\u03b3wm(Nr[G\\E](v)) \u00b7Qwm(Nr\u2032[G\\E](v\u2032)) + \u03b3\u2032wm+1(Nr[G\\E](v)) \u00b7Qwm+1(Nr\u2032[G\\E](v\u2032)))|\n\u2264 c m\nnm lnm+1 n(1\u2212 c)m(r+r\u2032)|\u03bbm+2|r+r \u2032 m\u22121\u220f i=1 |\u03bbi|r+r \u2032\n+ cm\nnm lnm+1 n(1\u2212 c)m(r+r\u2032)|\u03bbm|r+r \u2032 |\u03bbm+1|r+r \u2032 m\u22122\u220f i=1 |\u03bbi|r+r \u2032\nwhere we temporarily adopt the convention that if i > h\u2032, \u03bbi = \u03bbh\u2032/ \u221a\n2 and wi(S) = 0 for all S.\nAlternately, if m = h\u2032 + 1 then with probability 1\u2212 o(1), either v is (r + 2m+ 1, x)-bad, v\u2032 is (r\u2032 + 1, x)-bad, or\n|det(Mm,r,r\u2032[E](v \u00b7 v\u2032))|\n\u2264 c h\u2032+1\nnh\u2032+1 log2(n)(1\u2212 c)h\u2032(r+r\u2032) h\u2032\u220f i=1 |\u03bbi|r+r \u2032 ( ((1\u2212 c)\u03bb1)2r n + ((1\u2212 c)\u03bb1)r/2 ) \u00b7 (1\u2212 c)r\u2032\u03bbr\u20321\nProof. For each 1 \u2264 l \u2264 2m and 1 \u2264 i \u2264 h, let\nxl(i) = c n (1\u2212 c)l\u03bbliwi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\nNext, for each 1 \u2264 i \u2264 h and 0 \u2264 l \u2264 m, let ul(i) be the column vector thats jth entry is xl+j(i) for 1 \u2264 j \u2264 m. Also, for each 1 \u2264 l \u2264 m, let ul(h + 1) be the length m column vector thats jth entry is Nr+l+j,r\u2032[E](v \u00b7 v\u2032)\u2212 \u2211h i=1 xl+j(i) for 1 \u2264 j \u2264 m. Note that for each\n1 \u2264 l \u2264 m, the lth column of Mm,r,r\u2032[E](v \u00b7 v\u2032) is \u2211h+1 i=1 ul(i). So,\ndet(Mm,r,r\u2032[E](v \u00b7 v\u2032)) = \u2211\ni\u2208(Z\u2229[1,h+1])m det([u1(i1), u2(i2), ..., um(im)])\nFor any i \u2208 (Z \u2229 [1, h + 1])m, if there exist j 6= j\u2032 such that ij = ij\u2032 \u2264 h\u2032, then uj(ij) = (1\u2212 c)j\u2212j\u2032\u03bbj\u2212j \u2032\nij uj\u2032(ij\u2032), which implies that det([u1(i1), u2(i2), ..., um(im)]) = 0.\nIf m \u2264 h and i is some permutation of the integers from 1 to m, then\ndet([u1(i1), u2(i2), ..., um(im)]) =  m\u220f j=1 (1\u2212 c)j\u03bbjij  sgn(i) det([u0(1), u0(2), ..., u0(m)]) The jth column of this matrix is proportional to cnwj(Nr[G\\E](v)) \u00b7Qwj(Nr\u2032[G\\E](v\n\u2032)), so there exists some \u03b3 = \u03b3({(1\u2212 c)\u03bbj},m) such that the sum of all such terms is\ncm nm \u03b3 m\u220f j=1 wj(Nr[G\\E](v)) \u00b7Qwj(Nr\u2032[G\\E](v\u2032))\nAlternately, the sum of all such terms is equal to\ndet  m\u2211 j=1 u1(j), m\u2211 j=1 u2(j), ..., m\u2211 j=1 um(j)  If xl(0) 6= 0 for each 1 \u2264 l \u2264 m and u\u2032 \u2208 (R)m such that\n([\u2211m j=1 u1(j), \u2211m j=1 u2(j), ..., \u2211m j=1 um(j) ]) u\u2032 = 0, then for each 1 \u2264 i \u2264 m,\nm\u2211 l=1 u\u2032l m\u2211 j=1 xl+i(j) = 0\nm\u2211 j=1 m\u2211 l=1 u\u2032l c n (1\u2212 c)l+i\u03bbl+ij wj(Nr[G\\E](v)) \u00b7Qwj(Nr\u2032[G\\E](v \u2032)) = 0\nm\u2211 j=1 wj(Nr[G\\E](v)) \u00b7Qwj(Nr\u2032[G\\E](v\u2032))(1\u2212 c)i\u03bbij m\u2211 l=1 u\u2032l \u00b7 (1\u2212 c)l\u03bblj = 0\nThat can only hold for all such i if \u2211m\nl=1 u \u2032 l(1\u2212 c)l\u03bblj = 0 for all 1 \u2264 j \u2264 m, and that can\nonly be the case if u\u2032 = 0. Therefore, the determinant is nonzero unless xl(0) = 0 for some l, which implies that \u03b3 6= 0. If m < h then by similar logic, there exists \u03b3\u2032 = \u03b3\u2032({(1\u2212 c)\u03bbi},m) such that the sum of all terms for which i is a permutation of the integers from 1 to m\u2212 1 and m+ 1 is\ncm nm \u03b3\u2032 \u00b7 wm+1(Nr[G\\E](v)) \u00b7Qwm+1(Nr\u2032[G\\E](v\u2032)) m\u22121\u220f i=1 wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\nThat accounts for all i \u2208 (Z \u2229 [1, h+ 1])m except for some of those such that there exists j such that ij \u2265 min(m+ 2, h+ 1) or there exist j, j\u2032 such that ij = m and ij\u2032 = m+ 1.\nIf v is (r + 2m+ 1, x)-good, then\nwi(Nr[G/E](v))P \u22121wi(Nr[G/E](v)) \u2264 ((min pj)\u22121/2 + x)2(1\u2212 c)2r\u03bb2ri\nfor all i. Similarly, if v\u2032 is (r\u2032 + 1, x)-good then\nwi(Nr\u2032[G/E](v \u2032))P\u22121wi(Nr\u2032[G/E](v \u2032)) \u2264 ((min pj)\u22121/2 + x)2(1\u2212 c)2r \u2032 \u03bb2r \u2032 i\nfor all i. If both hold, then |xl(i)| \u2264 cn(1\u2212 c) r+r\u2032+l|\u03bbi|r+r \u2032+l+1((min pj) \u22121/2 + x)2 for all\ni. Furthermore, for any l and j,\n|ul(h+ 1)j | = |Nr+l+j,r\u2032[E](v \u00b7 v\u2032)\u2212 h\u2211 i=1 xl+j(i)|\n\u2264 |Nr+l+j,r\u2032[E](v \u00b7 v\u2032)\u2212 c n Nr+l+j[G\\E](v) \u00b7QNr\u2032[G\\E](v\u2032)|\n+ | c n Nr+l+j[G\\E](v) \u00b7QNr\u2032[G\\E](v\u2032)\u2212 h\u2211 i=1 xl+j(i)|\n\u2264 (1 + \u221a |Nr+l+j[G\\E](v)| \u00b7 |Nr\u2032[G\\E](v\u2032)|/n) log n\n+ c\nn h\u2211 i=1 |wi(Nr+l+j[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\n\u2212 (1\u2212 c)l+j\u03bbl+ji wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v \u2032))|\nhence\n|ul(h+ 1)j | \u2264 (1 + ((1\u2212 c)\u03bb1)(r+r \u2032+l+j)/2) \u221a k((min pi) \u22121/2 + x)/ \u221a n) log n\n+ c\nn h\u2211 i=1 (1\u2212 c)r+l+j\u22121x\u03bbh\u2032 ( \u03bb2h\u2032 2\u03bb1 )r |\u03bbi|l+j\u22121\n\u00b7 \u221a wi(Nr\u2032[G\\E](v\u2032)) \u00b7QPQwi(Nr\u2032[G\\E](v\u2032))\n\u2264 (1 + ((1\u2212 c)\u03bb1)(r+r \u2032+l+j)/2) \u221a k((min pi) \u22121/2 + x)/ \u221a n) log n\n+ c\nn h\u2211 i=1 (1\u2212 c)r+l+j\u22121x\u03bbh\u2032 ( \u03bb2h\u2032 2\u03bb1 )r |\u03bbi|l+j\u22121((min pj)\u22121/2 + x)(1\u2212 c)r \u2032 |\u03bbi|r \u2032+1\n\u2264 (1 + ((1\u2212 c)\u03bb1)(r+r \u2032+l+j)/2) \u221a k((min pi) \u22121/2 + x)/ \u221a n) log n\n+ ch\nn (1\u2212 c)l+j\u22121x\u03bbh\u2032\n( (1\u2212 c)2\u03bb2h\u2032\n2\n)(r+r\u2032)/2 \u03bbl+j1 ((min pj) \u22121/2 + x)\nwith probability 1\u2212 o(1). In other words, under these circumstances |xl(i)| is upper bounded by a constant multiple of cn((1\u2212 c)|\u03bbi|) r+r\u2032 if v and v\u2032 are both good, and every entry of ul(h+ 1) has a magnitude that is upper bounded by a constant multiple of ((1 \u2212 c)\u03bb1)(r+r \u2032)/2 log n/ \u221a n + cn((1 \u2212 c)2\u03bb2h\u2032/2) (r+r\u2032)/2. Either way, every entry of ul(i) is upper bounded by a constant multiple of cn((1\u2212 c)|\u03bbi|) r+r\u2032 log n.\nThat means that for any i \u2208 (Z \u2229 [1, h + 1])m such that ij \u2265 m + 1 for some i, then det([u1(i1), u2(i2), ..., um(im)]) is upper bounded by a constant multiple of cm nm log m(n)(1\u2212 c)m(r+r \u2032)|\u03bbm+2|r+r \u2032\u220fm\u22121 i=1 |\u03bbi|r+r \u2032 Similarly, for i \u2208 (Z \u2229 [1, h + 1])m such that ij \u2265 m\u2212 1 and ij\u2032 \u2265 m\u2212 1 with j 6= j\u2032, det([u1(i1), u2(i2), ..., um(im)]) is upper bounded by a constant multiple of c m\nnm log m(n)(1 \u2212 c)m(r+r\u2032)|\u03bbm|r+r \u2032 |\u03bbm+1|r+r \u2032\u220fm\u22122 i=1 |\u03bbi|r+r \u2032 . There are at most\nmm such i; therefore,\n| det(Mm,r,r\u2032[E](v \u00b7 v\u2032))\u2212 cm\nnm m\u22121\u220f i=1 wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\n\u00b7 (\u03b3wm(Nr[G\\E](v)) \u00b7Qwm(Nr\u2032[G\\E](v\u2032)) + \u03b3\u2032wm+1(Nr[G\\E](v)) \u00b7Qwm+1(Nr\u2032[G\\E](v\u2032)))|\n\u2264 c m\nnm lnm+1(n)(1\u2212 c)m(r+r\u2032)|\u03bbm+2|r+r \u2032 m\u22121\u220f i=1 |\u03bbi|r+r \u2032\n+ cm\nnm lnm+1(n)(1\u2212 c)m(r+r\u2032)|\u03bbm|r+r \u2032 |\u03bbm+1|r+r \u2032 m\u22122\u220f i=1 |\u03bbi|r+r \u2032\nwith probability 1\u2212 o(1), as desired. Alternately, recall that if v is (r + 2m + 1, x)-good then for any r\u2032\u2032 < r + 2m + 1 and\ni \u2264 h,\n||E[Nr\u2032\u2032+1[G\\E](v))\u2212 (1\u2212 c)PQNr\u2032\u2032[G\\E](v)|| = O\n( |Nr\u2032\u2032[G\\E](v)|2\nn\n) = O ( ((1\u2212 c)\u03bb1)2r \u2032\u2032\nn\n)\nAlso, for fixed values of Nr\u2032\u2032\u2032(v) for all r \u2032\u2032\u2032 \u2264 r\u2032\u2032 \u2264 r+2m+1, each element of Nr\u2032\u2032+1[G\\E](v)\nhas a variance of O(|Nr\u2032\u2032[G\\E](v)|) = O((1\u2212 c)\u03bbr \u2032\u2032 1 ). So,\n||wi(Nr\u2032\u2032+l+j[G\\E](v))\u2212 (1\u2212 c)l+j\u03bb l+j i wi(Nr\u2032\u2032 [G\\E](v))||\n\u2264\n( ((1\u2212 c)\u03bb1)2r \u2032\u2032\nn + ((1\u2212 c)\u03bb1)r\n\u2032\u2032/2 ) lnn\nwith probability 1\u2212o(1) for all l, j \u2264 m and i \u2264 h. This implies that if v is (r+2m+1, x)-good and v\u2032 is (r\u2032 + 1, x)-good then |ul(h+ 1)j | = |Nr+l+j,r\u2032[E](v \u00b7 v\u2032)\u2212 h\u2211 i=1 xl+j(i)|\n\u2264 |Nr+l+j,r\u2032[E](v \u00b7 v\u2032)\u2212 c n Nr+l+j[G\\E](v) \u00b7QNr\u2032[G\\E](v\u2032)|\n+ | c n Nr+l+j[G\\E](v) \u00b7QNr\u2032[G\\E](v\u2032)\u2212 h\u2211 i=1 xl+j(i)|\n\u2264 (1 + \u221a |Nr+l+j[G\\E](v)| \u00b7 |Nr\u2032[G\\E](v\u2032)|/n) log n\n+ c\nn h\u2211 i=1 |wi(Nr+l+j[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032))\n\u2212 (1\u2212 c)l+j\u03bbl+ji wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v \u2032))|\n\u2264 (1 + ((1\u2212 c)\u03bb1)(r+r \u2032+l+j)/2) \u221a k(((min pi) \u22121/2 + x)/ \u221a n) log n\n+ c\nn h\u2211 i=1 ( ((1\u2212 c)\u03bb1)2r n + ((1\u2212 c)\u03bb1)r/2 ) log n \u00b7 ||Qwi(Nr\u2032[G\\E](v\u2032))||\n\u2264 (1 + ((1\u2212 c)\u03bb1)(r+r \u2032+l+j)/2) \u221a k(((min pi) \u22121/2 + x)/ \u221a n) log n\n+ c\nn h\u2211 i=1 ( ((1\u2212 c)\u03bb1)2r n + ((1\u2212 c)\u03bb1)r/2 ) log n((min pj) \u22121/2 + x)(1\u2212 c)r\u2032 |\u03bbi|r \u2032+1\n\u2264 (1 + ((1\u2212 c)\u03bb1)(r+r \u2032+l+j)/2) \u221a k(((min pi) \u22121/2 + x)/ \u221a n) log n\n+ ch\nn\n( ((1\u2212 c)\u03bb1)2r\nn + ((1\u2212 c)\u03bb1)r/2\n) log n((min pj) \u22121/2 + x)(1\u2212 c)r\u2032\u03bbr\u2032+11\nfor any l and j with probability 1\u2212 o(1). If m = h\u2032 + 1 then for any i \u2208 (Z \u2229 [1, h+ 1])m, either there exist j 6= j\u2032 such that ij = ij\u2032 \u2264 h\u2032, or there exists j such that ij > h\u2032. Either\nway, det([u1(i1), u2(i2), ..., um(im)]) is upper bounded by a constant multiple of\nch \u2032+1\nnh\u2032+1 log(n)(1\u2212 c)h\u2032(r+r\u2032) h\u2032\u220f i=1 |\u03bbi|r+r \u2032 ( ((1\u2212 c)\u03bb1)2r n + ((1\u2212 c)\u03bb1)r/2 ) \u00b7 (1\u2212 c)r\u2032\u03bbr\u20321\nwith probability 1 \u2212 o(1). There are only mm possible choices of i, so with probability 1\u2212 o(1), either v is (r + 2m+ 1, x)-bad, v\u2032 is (r\u2032 + 1, x)-bad, or\n|det(Mm,r,r\u2032[E](v \u00b7 v\u2032))|\n\u2264 c h\u2032+1\nnh\u2032+1 log2(n)(1\u2212 c)h\u2032(r+r\u2032) h\u2032\u220f i=1 |\u03bbi|r+r \u2032 ( ((1\u2212 c)\u03bb1)2r n + ((1\u2212 c)\u03bb1)r/2 ) \u00b7 (1\u2212 c)r\u2032\u03bbr\u20321\nWhile this is a helpful result, it turns out to be more useful to have an expression that links the determinant to wi(Nr[G\\E](v)) \u00b7Qwi(Nr\u2032[G\\E](v\u2032)) for fixed values of r and r\u2032. So, we have the following:\nLemma 6. Let 0 < c < 1, x > 0, G be drawn from G1(n, p,Q), E be a subset of G\u2019s edges that independently contains each edge with probability c, and m \u2264 h\u2032. Now, for any v, v\u2032 \u2208 G and \u221a\nlnn \u2264 r\u2032 \u2264 r \u2208 Z+, such that ((1\u2212 c)\u03bb2h\u2032/2)r+r \u2032 > \u03bbr+r \u2032\n1 n, with probability 1\u2212 o(1), either v is (r + 2m+ 1, x)-bad, v\u2032 is (r\u2032 + 1, x)-bad, or\n|det(Mm,r,r\u2032[E](v \u00b7 v\u2032))\u2212 cm\nnm m\u22121\u220f i=1 ((1\u2212 c)\u03bbi)r+r \u2032\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))\n\u00b7 (\u03b3((1\u2212 c)\u03bbm)r+r \u2032\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v \u2032)) + \u03b3\u2032((1\u2212 c)\u03bbm+1)r+r \u2032\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v \u2032)))|\n\u2264 1 ln2m+2 n \u00b7 c m nm m\u220f i=1 |(1\u2212 c)\u03bbi|r+r \u2032\nProof. First, note that\n|wi(Nr[G/E](v)) \u00b7Qwi(Nr\u2032[G/E](v\u2032))\n\u2212 ((1\u2212 c)\u03bbi)r+r \u2032\u22122 \u221a lnnwi(N\u221alnn[G/E](v)) \u00b7Qwi(N\u221alnn[G/E](v \u2032))|\n\u2264 |\u03bbi[wi(Nr[G/E](v)) \u00b7 P\u22121(wi(Nr\u2032[G/E](v\u2032))\u2212 ((1\u2212 c)\u03bbi)r \u2032\u2212 \u221a lnnwi(N\u221alnn[G/E](v \u2032)))\n+ (wi(Nr[G/E](v))\u2212 ((1\u2212 c)\u03bbi)r\u2212 \u221a lnnwi(N\u221alnn[G/E](v)))\n\u00b7 P\u22121((1\u2212 c)\u03bbi)r \u2032\u2212 \u221a lnnwi(N\u221alnn[G/E](v \u2032))]|\n\u2264 |\u03bbi| \u221a wi(Nr[G/E](v)) \u00b7 P\u22121wi(Nr[G/E](v)) \u00b7 x|(1\u2212 c)\u03bbi|r \u2032\n2 \u221a lnn\n+ |\u03bbi| x|(1\u2212 c)\u03bbi|r\n2 \u221a lnn \u00b7 |(1\u2212 c)\u03bbi|r\n\u2032\u2212 \u221a lnn \u221a wi(N\u221alnn[G/E](v \u2032)) \u00b7 P\u22121wi(N\u221alnn[G/E](v\u2032))\n\u2264 2x(1\u2212 c) r+r\u2032 |\u03bbi|r+r \u2032+1\n2 \u221a lnn ((min pj)\n\u22121/2 + x) = o(|(1\u2212 c)\u03bbi|r+r \u2032 / ln2m+2(n))\nAlso,\n|\u03bbm+2|r+r \u2032 /|\u03bbm|r+r \u2032 \u2264 n\u2212 ln(|\u03bbm/\u03bbm+2|)/ ln((1\u2212c)\u03bb 2 h\u2032/(2\u03bb1)) = o(1/ ln3m+3 n)\nand\n|\u03bbm+1|r+r \u2032 /|\u03bbm\u22121|r+r \u2032 \u2264 n\u2212 ln(|\u03bbm\u22121/\u03bbm+1|)/ ln((1\u2212c)\u03bb 2 h\u2032/(2\u03bb1)) = o(1/ ln3m+3 n)\nCombining these inequalities with the determinant lemma yields the desired result.\nIn some sense, this establishes that\ndet(Mm,r,r\u2032[E](v \u00b7 v\u2032)) \u2248 cm\nnm m\u22121\u220f i=1 ((1\u2212 c)\u03bbi)r+r \u2032\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))\n\u00b7 (\u03b3((1\u2212 c)\u03bbm)r+r \u2032\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v \u2032))\n+ \u03b3\u2032((1\u2212 c)\u03bbm+1)r+r \u2032\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v \u2032)))\nHowever, in order to know this in a useful sense it is necessary to prove that these terms are large relative to the error terms. In order to do that, we need the following:\nLemma 7. For any p \u2208 (0, 1)k with \u2211 pi = 1 and k \u00d7 k matrix Q with nonnegative entries such that every entry of Qk is positive, there exists a unique w \u2208 (0,\u221e)k such that w is an eigenvector of PQ with eigenvalue \u03bb1 and w\u00b7P\u22121w = 1. Now, let G be drawn from G1(n, p,Q), v \u2208 G, and w\u2032 be an eigenvector of PQ with eigenvalue \u03bbi such that \u03bb2i > \u03bb1. With probability 1\u2212 o(1) either v is ( \u221a lnn,min(P\u22121w)i/2)-bad, or |w\u2032 \u00b7 P\u22121N\u221alnn(v)| \u2265 \u03bb \u221a lnn i / lnn.\nProof. Every entry in (PQ)k is positive, so its eigenvector of largest eigenvalue is unique up to multiplication, and its entries all have the same sign. That means that it has a unique multiple, w, such that w \u00b7 P\u22121w = 1 and w1 > 0. In this case all of w\u2019s entries must be positive, and since (PQ)kw = \u03bbk1w, it must be the case that PQw = \u03bb1w.\nGiven any ( \u221a lnn,min(P\u22121w)j/2)-good vertex v, and any r < \u221a lnn,\nw \u00b7 P\u22121Nr(v) \u2265 \u03bbr1w \u00b7 P\u22121{v} \u2212 min(P\u22121w)j\n2\nr\u22121\u2211 j=0 2\u2212j\u22121\u03bbr1 \u2265 \u03bbr1 min(P\u22121w)j/2\nFor any eigenvector w\u2032\u2032 with eigenvalue \u03bbi\u2032 and w \u2032\u2032 \u00b7 P\u22121w\u2032\u2032 = 1,\n|w\u2032\u2032\u00b7P\u22121Nr(v)| \u2264 |\u03bbi\u2032 |rw\u2032\u2032\u00b7P\u22121{v}+ min(P\u22121w)j\n2\nr\u22121\u2211 j=0 2\u2212j\u22121|\u03bbi\u2032 |r \u2264 |\u03bbi\u2032 |r(min p \u22121/2 j +min(P \u22121w)j/2)\n\u03bb1 > \u03bb2, so there exists a constant r0 such that for any r > r0,\nNr(v)j \u2265 min(P\u22121w)j\n4 \u03bbr1wj\nfor all j. For r0 < r \u2264 \u221a\nlnn, 1 \u2264 j \u2264 k, and a fixed value of Nr(v), the probability distribution of Nr+1(v)j is within o(1) of a poisson distribution with expected value\n(PQNr(v))j . Furthermore, Nr+1(v)j\u2032 has negligible dependence on Nr+1(v)j for all j \u2032 6= j. So, w\u2032 \u00b7P\u22121Nr+1(v) has an expected value of \u03bbiw\u2032 \u00b7P\u22121Nr(v)+o(1) and a standard deviation of \u221a\u221a\u221a\u221a k\u2211\nj=1\nw\u20322j p \u22122 j (PQNr(v))j + o(1) \u2264 \u221a\u221a\u221a\u221a2\u03bbr+11 min(P\u22121w)j k\u2211 j=1 w\u20322j p \u22122 j wj + o(1)\nThis implies that E [ |w\u2032 \u00b7 P\u22121N\u221alnn(v)\u2212 \u03bb \u221a lnn\u2212r i w \u2032 \u00b7 P\u22121Nr(v)| ] \u2264\n\u221a lnn\u22121\u2211 r\u2032=r E [ |\u03bb \u221a lnn\u22121\u2212r\u2032 i w \u2032 \u00b7 P\u22121Nr\u2032+1(v)\u2212 \u03bb \u221a lnn\u2212r\u2032 i w \u2032 \u00b7 P\u22121Nr\u2032(v)| ]\n\u2264 \u221a lnn\u22121\u2211 r\u2032=r \u03bb \u221a lnn\u22121\u2212r\u2032 i \u221a\u221a\u221a\u221a2\u03bbr\u2032+11 min(P\u22121w)j k\u2211 j=1 w\u20322j p \u22122 j wj + o(1)  \u2264 \u03bb \u221a lnn\u2212r\ni \u03bb r/2 1 \u00b7 1\u221a \u03bb2i /\u03bb1 \u2212 1 \u221a\u221a\u221a\u221a2 min(P\u22121w)j k\u2211 j=1 w\u20322j p \u22122 j wj + o(1) \nIn particular, this means that if there exists r0 < r < 2 ln lnn/ ln(\u03bb 2 i /\u03bb1) such that |w\u2032 \u00b7 P\u22121Nr(v)| > \u03bbr/21 ln ln ln lnn then with probability 1\u2212 o(1),\n|w\u2032 \u00b7 P\u22121N\u221alnn(v)| \u2265 \u03bb \u221a lnn\u2212r i \u03bb r/2 1 ln ln ln lnn\u2212 \u03bb \u221a lnn\u2212r i \u03bb r/2 1 \u221a ln ln ln lnn\n\u2265 \u03bb \u221a\nlnn i \u00b7 (\u03bb 2 i /\u03bb1)\n\u2212r/2\n\u2265 \u03bb \u221a\nlnn i / lnn\nFurthermore, since the probability distribution of w\u2032 \u00b7 P\u22121Nr(v) for a fixed value of Nr\u22121(v) is a sum of constant multiples of poisson distributions with expected values of \u2126(\u03bb r 1), it must be the case that |w\u2032 \u00b7P\u22121Nr(v)| > \u03bbr/21 ln ln ln lnn with probability e\u2212O(ln 2 ln ln lnn) = \u03c9(1/ ln lnn). Therefore, there exists r0 < r < 2 ln lnn/ ln(\u03bb 2 i /\u03bb1) such that this holds with probability 1\u2212 o(1), and the lemma follows.\nThis also implies that for any v, v\u2032, and i, either v is ( \u221a lnn,min(P\u22121w)i/2)-bad, v \u2032\nis ( \u221a\nlnn,min(P\u22121w)i/2)-bad, or |wi(N\u221alnn(v)) \u00b7 P \u22121wi(N\u221alnn(v\n\u2032))| \u2265 \u03bb2 \u221a\nlnn i / ln 2 n with probability 1 \u2212 o(1), since the degree of dependence between N\u221alnn(v) and N\u221alnn(v\n\u2032) is negligible. This allows me to attempt to approximate PQ\u2019s eigenvalues as follows.\nLemma 8. Let 0 < c < 1, min(P\u22121w)i/2 > x > 0, G be drawn from G1(n, p,Q), E be a subset of G\u2019s edges that independently contains each edge with probability c, v \u2208 G, such that (1\u2212 c)(\u03bb2h\u2032/2)4 > \u03bb71.\nWith probability 1\u2212 o(1), either v is (23 \u00b7 log n/ log((1\u2212 c)\u03bb1), x)-bad or Basic-Eigenvalueapproximation-algorithm(E,c,v) runs in O(n) time and returns (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032) such that h \u2032 = h\u2032\u2032 and |\u03bb\u2032i \u2212 \u03bbi| < ln \u22123/2(n) for all i.\nBasic-Eigenvalue-approximation-algorithm(E,c,v):\nCompute Nr[G\\E](v) for each r until |Nr[G\\E](v)| > \u221a n, and then set \u03bb\u2032\u20321 = 2r \u221a n/(1\u2212 c).\nSet r = r\u2032 = 23 log n/ log((1\u2212 c)\u03bb \u2032\u2032 1)\u2212\n\u221a lnn. Then, compute\n2r\n\u221a nmax(| detMm,r,r[E](v \u00b7 v)|, | detMm,r+1,r[E](v \u00b7 v)|)\ncmax(|detMm\u22121,r,r[E](v \u00b7 v)|, |detMm\u22121,r+1,r[E](v \u00b7 v)|)\nuntil an m is found for which this expression is less than ((1\u2212 c)\u03bb\u2032\u20321)3/4 + 1\u221alnn . Then, set h\u2032\u2032 = m\u2212 1.\nThen, set\n|\u03bb\u2032i| = 1\n1\u2212 c\n\u221a det(Mi,r+3,r\u2032[E](v \u00b7 v\u2032))/det(Mi,r+1,r\u2032[E](v \u00b7 v\u2032))/ i\u22121\u220f j=1 (1\u2212 c)|\u03bb\u2032j |\nunless | det(Mi,r+1,r\u2032[E](v \u00b7 v\u2032))| < \u221a | det(Mi,r,r\u2032[E](v \u00b7 v\u2032))| \u00b7 | det(Mi,r+2,r\u2032[E](v \u00b7 v\u2032))|, in\nwhich case set |\u03bb\u2032i| = 11\u2212c \u221a det(Mi,r+2,r\u2032[E](v \u00b7 v\u2032))/ det(Mi,r,r\u2032[E](v \u00b7 v\u2032))/ \u220fi\u22121 j=1(1 \u2212 c)|\u03bb\u2032j |. Repeat this for each i \u2264 h\u2032\u2032\nNext, for each i < h\u2032\u2032, if ||\u03bb\u2032i| \u2212 |\u03bb\u2032i+1|| < 1lnn then set \u03bb \u2032 i = |\u03bb\u2032i| and \u03bb\u2032i+1 = \u2212|\u03bb\u2032i+1|. For each i \u2264 h\u2032\u2032 such that ||\u03bb\u2032i| \u2212 |\u03bb\u2032i+1|| \u2265 1lnn and ||\u03bb \u2032 i\u22121| \u2212 |\u03bb\u2032i|| \u2265 1lnn set\n\u03bb\u2032i = 1 1\u2212 c det(Mi,r+1,r\u2032[E](v \u00b7 v\u2032))/ det(Mi,r,r\u2032[E](v \u00b7 v\u2032))/ i\u22121\u220f j=1 (1\u2212 c)\u03bb\u2032j\nReturn (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)\nProof. If v is (23 \u00b7 log n/ log((1\u2212 c)\u03bb1, x)-good, there exists a constant r such that for any r < r\u2032\u2032 < 23 \u00b7 log n/ log((1\u2212 c)\u03bb1),\nmin(P\u22121w)j 4 ((1\u2212 c)\u03bb1)r \u2032\u2032 k\u2211 j=1 wj \u2264 |Nr\u2032\u2032[G\\E](v[0])| \u2264 ((1\u2212 c)\u03bb1)r \u2032\u2032\u221a k((min pi) \u22121/2 + x)\nSo, the minimum r\u2032\u2032 such that |Nr\u2032\u2032[G\\E](v)| > \u221a n is within a constant of log n/2 log((1\u2212c)\u03bb1). That means that |\u03bb1 \u2212 \u03bb\u2032\u20321| is upper bounded by a constant multiple of 1/ log n, and that r = r\u2032 is within a constant of the value it would have if \u03bb\u2032\u20321 were \u03bb1. This also implies that if n is sufficiently large it is less than 23 \u00b7 log n/ log((1\u2212 c)\u03bb1)\u2212 2h\n\u2032 \u2212 3. For m \u2264 h\u2032, r\u2032\u2032 \u2208 {r, r + 1}, and v \u2208 G if |\u03bbi| 6= |\u03bbi+1| then\n| det(Mm,r\u2032\u2032,r\u2032\u2032[E](v \u00b7 v))\n\u03b3({(1\u2212c)\u03bbj},m)cm nm \u220fm i=1((1\u2212 c)\u03bbi)2r \u2032\u2032\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v)) \u22121| = o(1)\nwith probability 1\u2212 o(1). If |\u03bbi| = |\u03bbi+1| then \u03bbi = \u2212\u03bbi+1, so either \u03b3((1\u2212 c)\u03bbm)2r\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v)) has the same sign as \u03b3\n\u2032((1\u2212 c)\u03bbm+1) 2r\u22122 \u221a\nlnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v)) or \u03b3((1\u2212c)\u03bbm)2r+1\u22122 \u221a lnnwm(N\u221alnn[G\\E](v))\u00b7Qwm(N\u221alnn[G\\E](v)) has the same sign as \u03b3\n\u2032((1\u2212 c)\u03bbm+1) 2r+1\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v))). Either way, we have that\n(1\u2212 o(1))|\u03b3({(1\u2212 c)\u03bbj},m)c m\nnm\nm\u220f i=1 ((1\u2212 c)\u03bbi)2r\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v))|\n\u2264 max(|det(Mm,r,r[E](v \u00b7 v))|, |det(Mm,r+1,r[E](v \u00b7 v))|)\n\u2264 (1 + o(1))| c m\nnm m\u22121\u220f i=1 ((1\u2212 c)\u03bbi)2r+1\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v))|\n\u00b7 (|\u03b3((1\u2212 c)\u03bbm)2r+1\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v))| + |\u03b3\u2032((1\u2212 c)\u03bbm+1)2r+1\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v))|)\nwith probability 1\u2212 o(1). Furthermore, by lemma 8, these bounds are within a factor of O(ln2 n) of each other with probability 1\u2212 o(1).\nThat means that\nnmax(| detMm,r,r[E](v \u00b7 v)|, | detMm,r+1,r[E](v \u00b7 v)|) cmax(| detMm\u22121,r,r[E](v \u00b7 v)|, |detMm\u22121,r+1,r[E](v \u00b7 v)|)\nis within a factor of ln3(n) of |((1\u2212c)\u03bbm)2r1\u22122 \u221a\nlnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v))| with probability 1\u2212 o(1), and thus that\n2r1\n\u221a nmax(| detMm,r,r[E](v \u00b7 v)|, |detMm,r+1,r[E](v \u00b7 v)|)\ncmax(| detMm\u22121,r,r[E](v \u00b7 v)|, |detMm\u22121,r+1,r[E](v \u00b7 v)|) = |(1\u2212 c)\u03bbm| \u00b1 o(1)\nwith probability 1\u2212 o(1). |(1\u2212 c)\u03bbm| \u2265 4 \u221a\n(4(1\u2212 c)3\u03bb31, so with probability 1\u2212 o(1), this expression is not less than ((1\u2212 c)\u03bb\u2032\u20321)3/4 + 1\u221alnn .\nIf m = h\u2032 + 1, r\u2032\u2032 \u2208 {r, r + 1}, and v \u2208 G, then with probability 1 \u2212 o(1) either v is (r\u2032\u2032 + 2m+ 1, x)-bad or\n|det(Mm,r,r\u2032[E](v \u00b7 v))|\n\u2264 c h\u2032+1\nnh\u2032+1 ln2(n)(1\u2212 c)2h\u2032\u00b7r\u2032\u2032 h\u2032\u220f i=1 |\u03bbi|2r \u2032\u2032\n( ((1\u2212 c)\u03bb1)2r \u2032\u2032\nn + ((1\u2212 c)\u03bb1)r\n\u2032\u2032/2 ) \u00b7 (1\u2212 c)r\u2032\u2032\u03bbr\u2032\u20321\n\u2264 2 c h\u2032+1\nnh\u2032+1 ln2(n)(1\u2212 c)2h\u2032\u00b7r\u2032\u2032 h\u2032\u220f i=1 |\u03bbi|2r \u2032\u2032 \u00b7 ((1\u2212 c)\u03bb1)3r \u2032\u2032/2\nIf v falls under the later case,\nnmax(| detMm,r,r[E](v \u00b7 v)|, | detMm,r+1,r[E](v \u00b7 v)|) cmax(|detMm\u22121,r,r[E](v \u00b7 v)|, |detMm\u22121,r+1,r[E](v \u00b7 v)|) = O(ln2(n)((1\u2212 c)\u03bb1)3r/2)\n, so\n2r\n\u221a nmax(|detMm,r,r[E](v \u00b7 v)|, |detMm,r+1,r[E](v \u00b7 v)|)\ncmax(|detMm\u22121,r,r[E](v \u00b7 v)|, | detMm\u22121,r+1,r[E](v \u00b7 v)|) = ((1\u2212c)\u03bb1)3/4+O(ln(ln(n))/ ln(n))\nTherefore, with probability 1\u2212 o(1), either v is (23 \u00b7 log n/ log((1\u2212 c)\u03bb1), x)-bad or h \u2032\u2032 = h\u2032.\nBy the previous two lemmas, with probability 1\u2212 o(1) either v is (r+ 2m+ 4, x)-bad, or\nwi(N\u221alnn(v)) \u00b7 P \u22121wi(N\u221alnn(v\n\u2032))| \u2265 \u03bb2 \u221a\nlnn i / ln 2 n\nfor all i \u2264 h and\n|det(Mm,r\u2032\u2032,r\u2032[E](v \u00b7 v\u2032))\n\u2212 c m\nnm m\u22121\u220f i=1 ((1\u2212 c)\u03bbi)r \u2032\u2032+r\u2032\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))\n\u00b7 (\u03b3((1\u2212 c)\u03bbm)r \u2032\u2032+r\u2032\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v \u2032)) + \u03b3\u2032((1\u2212 c)\u03bbm+1)r \u2032\u2032+r\u2032\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v \u2032)))|\n\u2264 1 ln2m+2 n \u00b7 c m nm m\u220f i=1 |(1\u2212 c)\u03bbi|r \u2032\u2032+r\u2032\nfor all m \u2264 h\u2032 + 1 and r \u2264 r\u2032\u2032 < r + 4. Assume that the later holds. If |\u03bbm+1| < |\u03bbm| then if n is sufficiently large the \u03b3\u2032 term will be negligable relative to the \u03b3 term, while if |\u03bbm+1| = |\u03bbm| increasing r\u2032\u2032 by two would multiply both of these terms by (1\u2212 c)2\u03bb2m. Either way, we have that for any k \u2264 h\u2032 and r \u2264 r\u2032\u2032 \u2264 r + 1,\n| det(Mm,r\u2032\u2032+2,r\u2032[E](v \u00b7 v\u2032))/det(Mm,r\u2032\u2032,r\u2032[E](v \u00b7 v\u2032))\u2212 k\u220f j=1 ((1\u2212 c)\u03bbj)2| \u2264 1 ln7/8 n\nas long as n is sufficiently large, and\n|\u03b3((1\u2212 c)\u03bbm)r \u2032\u2032+r\u2032\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v \u2032))\n+ \u03b3\u2032((1\u2212 c)\u03bbm+1)r \u2032\u2032+r\u2032\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v \u2032))|\n\u2265 1 2 |\u03b3((1\u2212 c)\u03bbm)r\n\u2032\u2032+r\u2032\u22122 \u221a\nlnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v \u2032))|\nWe assumed that n is large, so that can only fail if |\u03bbm| = |\u03bbm+1|. In this case, \u03bbm = \u2212\u03bbm+1, so increasing m by one would result in both terms having the same sign, at which point the condition is satisfied. So,\n||\u03bb\u2032i|2 \u2212 |\u03bbi|2| \u2264 4 ln\u22127/8 n\nfor any i \u2264 h\u2032. For any i < h\u2032, if |\u03bbi| > |\u03bbi+1|, then for sufficiently large n, |\u03bb\u2032i|\u2212|\u03bb\u2032i+1| will be greater than 1lnn . On the other hand, if |\u03bbi| = |\u03bbi+1| and n is sufficiently large, |\u03bb \u2032 i| \u2212 |\u03bb\u2032i+1| will be less than 1lnn . So, the algorithm will suceed at determining which eigenvalues have the same absolute values and assign \u03bb\u2032i the same sign as \u03bbi for each i. So, |\u03bb\u2032i\u2212\u03bbi| < ln \u22123/2(n) as desired. Assuming this all works, the slowest part of the algorithm is computing expressions of the form Nr\u2032\u2032,r\u2032\u2032\u2032[E](v \u00b7 v), each such expression can be computed in O(n) time, and only a constant number of them need to be computed. So, the algorithm runs in O(n) time.\nSo, this algorithm sometimes works, but it fails if v is bad. This risk can be mitigated by using multiple vertices as follows.\nImproved-Eigenvalue-approximation-algorithm(c):\nCreate a set of edges E, that each of G\u2019s edges is independently assigned to with probability c.\nRandomly select \u221a lnn of G\u2019s vertices, v[1], v[2],..., v[ \u221a lnn].\nRun Basic-Eigenvalue-approximation-algorithm(E,c,v[i]) for each i \u2264 \u221a\nlnn, stopping the algorithm prematurely if it takes more than O(n \u221a lnn) time.\nReturn (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032) where h \u2032\u2032 and \u03bb\u2032i are the median outputs of the executions of BasicEigenvalue-approximation-algorithm for each i.\nLemma 9. Let 0 < c < 1, min(P\u22121w)i/2 > x > 0, and G be drawn from G1(n, p,Q). Improved-Eigenvalue-approximation-algorithm(c) runs in O(n log n) time. Furthermore, if (1\u2212 c)(\u03bb2h\u2032/2)4 > \u03bb71, x < \u03bb1k \u03bbh\u2032 min pi , and\n2ke \u2212\nx2(1\u2212c)\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x) / 1\u2212 e\u2212 x2(1\u2212c)\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x) \u00b7(( (1\u2212c)\u03bb4h\u20324\u03bb31 )\u22121)  < 1\n2\nthen Improved-Eigenvalue-approximation-algorithm(c) returns (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032) such that h \u2032 = h\u2032\u2032 and |\u03bb\u2032i \u2212 \u03bbi| < ln \u22123/2(n) for all i with probability 1\u2212 o(1).\nProof. Generating E takes O(n) time, picking v[i] takes o(n) time, each execution of Basic-Eigenvalue-approximation-algorithm(c) runs in O(n \u221a log n) time, and combining their outputs takes o(n) time. So, this algorithm runs in O(n log n) time. Assuming the conditions are satisfied, there exists y < 12 such that 1\u2212 y of G\u2019s vertices are (23 \u00b7 log n/ log((1\u2212 c)\u03bb1, x)-good with probability 1\u2212 o(1). So, with probability 1\u2212 o(1), the majority of the selected vertices of G are (23 \u00b7 log n/ log((1 \u2212 c)\u03bb1), x)-good and the majority of the executions of Basic-Eigenvalue-approximation-algorithm give good output. If this happens, then the median value of h\u2032\u2032 is h\u2032, and for each 1 \u2264 i \u2264 h\u2032, the median value of \u03bb\u2032i is within ln \u22123/2(n) of \u03bbi for each i, as desired.\nGiven approximations of PQ\u2019s eigenvalues, one can attempt to approximate Ni({v}) \u00b7 P\u22121Ni({v\u2032}) as follows.\nVertex-product-approximation-algorithm(v,v\u2019,r,r\u2019,E,c,(\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)): (Assumes that Nr\u2032\u2032[G\\E](v) has already been computed for r \u2032\u2032 \u2264 r + 2h\u2032\u2032 + 3 and that Nr\u2032\u2032[G\\E](v \u2032) has already been computed for r\u2032\u2032 \u2264 r\u2032)\nFor each i \u2264 h\u2032\u2032, set\nzi(v \u00b7 v\u2032) = det(Mi,r+1,r\u2032[E](v \u00b7 v\u2032)\u2212 (1\u2212 c)i\u03bb\u2032i+1\n\u220fi\u22121 j=1 \u03bb \u2032 j det(Mi,r,r\u2032[E](v \u00b7 v\u2032)\ndet(Mi\u22121,r+1,r\u2032[E](v \u00b7 v\u2032)\u2212 (1\u2212 c)i\u22121\u03bb\u2032i \u220fi\u22122 j=1 \u03bb \u2032 j det(Mi\u22121,r,r\u2032[E](v \u00b7 v\u2032)\n\u00b7 n(\u03bb\u2032i\u22121 \u2212 \u03bb\u2032i)\u03b3({(1\u2212 c)\u03bb\u2032j}, i\u2212 1) c\u03bb\u2032i\u22121(\u03bb \u2032 i \u2212 \u03bb\u2032i+1)\u03b3({(1\u2212 c)\u03bb\u2032j}, i) ((1\u2212 c)\u03bb\u2032i)\u2212r\u2212r \u2032\u22121\nReturn (z1(v \u00b7 v\u2032), ..., zh\u2032\u2032(v \u00b7 v\u2032)).\nLemma 10. Let 0 < c < 1, min(P\u22121w)i/2 > x > 0, G be drawn from G1(n, p,Q), E be a subset of G\u2019s edges that independently contains each edge with probability c, v, v\u2032 \u2208 G and \u221a lnn \u2264 r\u2032 \u2264 r \u2208 Z+, such that ((1 \u2212 c)\u03bb2h\u2032/2)r+r \u2032 > \u03bbr+r \u2032 1 n. Also let (\u03bb \u2032 1, ..., \u03bb \u2032 h\u2032\u2032) be a h\u2032\u2032-tuple which may depend on n such that h\u2032\u2032 = h\u2032 and |\u03bb\u2032i \u2212 \u03bbi| \u2264 ln \u22123/2(n) for all i. Assume that Nr\u2032\u2032[G\\E](v) has already been computed for r \u2032\u2032 \u2264 r + 2m + 3 and that Nr\u2032\u2032[G\\E](v \u2032) has already been computed for r\u2032\u2032 \u2264 r\u2032. Vertex-product-approximationalgorithm(v, v\u2032, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) runs in O(((1\u2212 c)\u03bb1)r \u2032 ) average time. Furthermore, with probability 1 \u2212 o(1), either v is (r + 2m + 4, x)-bad, v\u2032 is (r\u2032 + 1, x)-bad, or Vertexproduct-approximation-algorithm(v, v\u2032, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) returns (z1(v \u00b7 v\u2032), ..., zh\u2032(v \u00b7 v\u2032)) such that |zi(v \u00b7 v\u2032)\u2212 wi({v}) \u00b7 P\u22121wi({v\u2032})| < 2x(min pj)\u22121/2 + x2 + o(1) for all i.\nProof. The slowest part of the algorithm is computing the expressions of the form Nr\u2032\u2032,r\u2032[E](v \u00b7 v\u2032). Each of these can be computed in O(E[|Nr\u2032[G\\E](v\u2032)|]) = O(((1\u2212 c)\u03bb1)r \u2032 ) average time, and the number of these that need to be computed is constant in n. So, this algorithm runs in O(((1\u2212 c)\u03bb1)r \u2032 ) average time.\nWith probability 1\u2212 o(1) either v is (r + 2m+ 4, x)-bad, v\u2032 is (r\u2032 + 1, x)-bad, or\nwi(N\u221alnn(v)) \u00b7 P \u22121wi(N\u221alnn(v\n\u2032))| \u2265 \u03bb2 \u221a\nlnn i / ln 2 n\nfor all i \u2264 h and\n|det(Mm,r\u2032\u2032,r\u2032[E](v \u00b7 v\u2032))\u2212 cm\nnm m\u22121\u220f i=1 ((1\u2212 c)\u03bbi)r \u2032\u2032+r\u2032\u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))\n\u00b7 (\u03b3((1\u2212 c)\u03bbm)r \u2032\u2032+r\u2032\u22122 \u221a lnnwm(N\u221alnn[G\\E](v)) \u00b7Qwm(N\u221alnn[G\\E](v \u2032)) + \u03b3\u2032((1\u2212 c)\u03bbm+1)r \u2032\u2032+r\u2032\u22122 \u221a lnnwm+1(N\u221alnn[G\\E](v)) \u00b7Qwm+1(N\u221alnn[G\\E](v \u2032)))|\n\u2264 1 ln2m+2 n \u00b7 c m nm m\u220f i=1 |(1\u2212 c)\u03bbi|r \u2032\u2032+r\u2032\nfor all m \u2264 h\u2032 + 1 and r \u2264 r\u2032\u2032 < r + 4. Assume that the later holds.\n|det(Mi,r+1,r\u2032[E](v \u00b7 v\u2032))\u2212 (1\u2212 c)i\u03bb\u2032i+1 i\u22121\u220f j=1 \u03bb\u2032j det(Mi,r,r\u2032[E](v \u00b7 v\u2032))\n\u2212 \u03b3\u03bbi \u2212 \u03bbi+1 \u03bbi \u00b7 c i ni i\u220f j=1 ((1\u2212 c)\u03bbj)r+1+r \u2032\u22122 \u221a lnnwj(N\u221alnn[G\\E](v)) \u00b7Qwj(N\u221alnn[G\\E](v \u2032))|\n\u2264 1 lnn \u00b7 | c\ni\nni i\u220f j=1 ((1\u2212 c)\u03bbj)r+1+r \u2032\u22122 \u221a lnnwj(N\u221alnn[G\\E](v)) \u00b7Qwj(N\u221alnn[G\\E](v \u2032))|\nfor any i \u2264 h with probability 1\u2212 o(1). So,\n| det(Mi,r+1,r\u2032[E](v \u00b7 v\u2032)\u2212 (1\u2212 c)i\u03bb\u2032i+1\n\u220fi\u22121 j=1 \u03bb \u2032 j det(Mi,r,r\u2032[E](v \u00b7 v\u2032)\ndet(Mi\u22121,r+1,r\u2032[E](v \u00b7 v\u2032)\u2212 (1\u2212 c)i\u22121\u03bb\u2032i \u220fi\u22122 j=1 \u03bb \u2032 j det(Mi\u22121,r,r\u2032[E](v \u00b7 v\u2032) \u2212 \u03b3({(1\u2212 c)\u03bbj}, i) \u03b3({(1\u2212 c)\u03bbj}, i\u2212 1) \u00b7 \u03bbi\u22121(\u03bbi \u2212 \u03bbi+1) \u03bbi(\u03bbi\u22121 \u2212 \u03bbi)\n\u00b7 c n\n((1\u2212 c)\u03bbi)r+r \u2032\u22122 \u221a lnn+1wi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))|\n\u2264 1 n \u221a lnn \u00b7 |((1\u2212 c)\u03bbi)r+r \u2032+1|\nwith probability 1\u2212 o(1). Therefore,\n|zi(v \u00b7 v\u2032)\u2212 \u03bb\u22121i ((1\u2212 c)\u03bbi) \u22122 \u221a lnnwi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))| = o(1)\nwith probability 1\u2212 o(1). That implies that\n|zi \u2212 wi({v}) \u00b7 P\u22121wi({v\u2032})|\n\u2264 |zi \u2212 (1\u2212 c)((1\u2212 c)\u03bbi)\u22122 \u221a lnn\u22121wi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))|\n+ (1\u2212 c) \u00b7 |((1\u2212 c)\u03bbi)\u22122 \u221a lnn\u22121wi(N\u221alnn[G\\E](v)) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))\n\u2212 ((1\u2212 c)\u03bbi)\u2212 \u221a lnn\u22121wi({v}) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))|\n+ |(1\u2212 c) \u00b7 ((1\u2212 c)\u03bbi)\u2212 \u221a lnn\u22121wi({v}) \u00b7Qwi(N\u221alnn[G\\E](v \u2032))\u2212 wi({v}) \u00b7 P\u22121wi({v\u2032}))|\n\u2264 |((1\u2212 c)\u03bbi)\u2212 \u221a lnnwi(N\u221alnn[G\\E](v \u2032)) \u00b7 P\u22121[((1\u2212 c)\u03bbi)\u2212 \u221a lnnwi(N\u221alnn[G\\E](v))\u2212 wi({v})]|\n+ |wi({v}) \u00b7 P\u22121[((1\u2212 c)\u03bbi)\u2212 \u221a lnnwi(N\u221alnn[G\\E](v \u2032))\u2212 wi({v\u2032})] + o(1)\nBy goodness of v and v\u2032, this is less than or equal to ((1\u2212 c)\u03bbi)\u2212 \u221a lnn \u221a wi(N\u221alnn[G\\E](v \u2032)) \u00b7 P\u22121wi(N\u221alnn[G\\E](v\u2032)) \u00b7 x\n+ \u221a wi({v}) \u00b7 P\u22121wi({v}) \u00b7 x+ o(1)\n\u2264 \u221a wi({v\u2032}) \u00b7 P\u22121wi({v\u2032}) + 2wi({v\u2032}) \u00b7 P\u22121[((1\u2212 c)\u03bbi)\u2212 \u221a lnnwi(N\u221alnn[G\\E](v \u2032))\u2212 wi({v\u2032})]+\n[((1\u2212 c)\u03bbi)\u2212 \u221a lnnwi(N\u221alnn[G\\E](v \u2032))\u2212 wi({v\u2032})] \u00b7P\u22121[((1\u2212 c)\u03bbi)\u2212 \u221a lnnwi(N\u221alnn[G\\E](v \u2032))\u2212 wi({v\u2032})] \u00b7 x\n+ x \u221a\n1/min pj + o(1) \u2264 \u221a 1/min pj + 2x/ \u221a min pj + x2x+ x/ \u221a min pj + o(1) = (x2 + 2x(min pj) \u22121/2) + o(1)\nwith probability 1\u2212 o(1), as desired.\nFor any two vertices in different communities, v and v\u2032, the fact that Q\u2019s rows are distinct\nimplies that Q( \u2212\u2192 {v} \u2212 \u2212\u2212\u2192 {v\u2032}) 6= 0. So, wi({v}) 6= wi({v\u2032}) for some 1 \u2264 i \u2264 h\u2032. That means that for any two vertices v and v\u2032,\n(wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032})) = wi({v}) \u00b7 P\u22121wi({v})\u2212 2wi({v}) \u00b7 P\u22121wi({v\u2032}) + wi({v\u2032}) \u00b7 P\u22121wi({v\u2032}) \u2265 0\nfor all 1 \u2264 i \u2264 h\u2032, with equality for all i if and only if v and v\u2032 are in the same community. This also implies that given a vertex v, another vertex in the same community v\u2032, and a vertex in a different community v\u2032\u2032,\n2wi({v}) \u00b7 P\u22121wi({v\u2032})\u2212 wi({v\u2032}) \u00b7 P\u22121wi({v\u2032}) \u2265 2wi({v}) \u00b7 P\u22121wi({v\u2032\u2032})\u2212 wi({v\u2032\u2032}) \u00b7 P\u22121wi({v\u2032\u2032})\nfor all 1 \u2264 i \u2264 h\u2032 and the inequality is strict for at least one i. This suggests the following algorithms for classifying vertices.\nVertex-comparison-algorithm(v,v\u2019, r,r\u2019,E,x,c,(\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)): (Assumes that Nr\u2032\u2032[G\\E](v) and Nr\u2032\u2032[G\\E](v \u2032) have already been computed for r\u2032\u2032 \u2264 r+2h\u2032\u2032+3)\nRun Vertex-product-approximation-algorithm(v, v\u2032, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)), Vertex-productapproximation-algorithm(v, v, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)), and Vertex-product-approximationalgorithm(v\u2032, v\u2032, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)).\nIf \u2203i : zi(v \u00b7 v)\u2212 2zi(v \u00b7 v\u2032) + zi(v\u2032 \u00b7 v\u2032) > 5(2x(min pj)\u22121/2 + x2) then conclude that v and v\u2032 are in different communities.\nOtherwise, conclude that v and v\u2032 are in the same community.\nLemma 11. Assuming that each of G\u2019s edges was independently assigned to E with probability c, this algorithm runs in O(((1\u2212 c)\u03bb1)max(r,r\n\u2032)) average time. Furthermore, if each execution of Vertex-product-approximation-algorithm succeeds and 13(2x(min pj)\n\u22121/2 + x2) is less than the minimum nonzero value of (wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032})) then the algorithm returns the correct result with probability 1\u2212 o(1).\nProof. The slowest step of the algorithm is using Vertex-product-approximation-algorithm. This runs in an average time of O(((1\u2212 c)\u03bb1)max(r,r\n\u2032)) and must be done 3 times. If each execution of Vertex-product-approximation-algorithm succeeds then with probability 1\u2212 o(1) the zi are all within 6 5(2x(min pj)\n\u22121/2 + x2) of the products they seek to approximate, in which case\nzi(v \u00b7 v)\u2212 2zi(v \u00b7 v\u2032) + zi(v\u2032 \u00b7 v\u2032) > 5(2x(min pj)\u22121/2 + x2)\nif and only if (wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032})) 6= 0,\nwhich is true for some i if and only if v and v\u2032 are in different communities.\nVertex-classification-algorithm(v[],v\u2019, r,r\u2019,E,c,(\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)): (Assumes that Nr\u2032\u2032[G\\E](v[\u03c3])have already been computed for 0 \u2264 \u03c3 < k and r\u2032\u2032 \u2264 r+2h\u2032\u2032+3, that Nr\u2032\u2032[G\\E](v\n\u2032) has already been computed for all r\u2032\u2032 \u2264 r\u2032, and that zi(v[\u03c3] \u00b7 v[\u03c3]) has already been computed for each i and \u03c3)\nRun Vertex-product-approximation-algorithm(v[\u03c3], v\u2032, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) for each \u03c3.\nFind a \u03c3 that minimizes the value of\nmax \u03c3\u2032 6=\u03c3,i\u2264h\u2032\u2032\nzi(v[\u03c3] \u00b7 v[\u03c3])\u2212 2zi(v[\u03c3] \u00b7 v\u2032)\u2212 [zi(v[\u03c3\u2032] \u00b7 v[\u03c3\u2032])\u2212 2zi(v[\u03c3\u2032] \u00b7 v\u2032)]\nand conclude that v\u2032 is in the same community as v[\u03c3].\nLemma 12. Assuming that E was generated properly, this algorithm runs in O(((1\u2212c)\u03bb1)r \u2032 ) average time. Let x > 0 and assume that each execution of Vertex-product-approximationalgorithm succeeds (including the previous ones to compute zi(v[\u03c3] \u00b7 v[\u03c3])), and that v[] contains exactly one vertex from each community. Also, assume that 13(2x(min pj)\n\u22121/2 +x2) is less than the minimum nonzero value of (wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032})). Then this algorithm classifies v\u2032 correctly with probability 1\u2212 o(1).\nProof. Again, the slowest step of the algorithm is running Vertex-product-approximationalgorithm. This runs in an average time of O(((1\u2212 c)\u03bb1)r \u2032 ) and must be done k times. If the conditions given above are satisifed, then each zi is within 21 20(2x(min pj)\n\u22121/2 + x2) of the product it seeks to approximate with probability 1\u2212 o(1). If this is the case, then\n2wi({v\u2032}) \u00b7 P\u22121wi({v[\u03c3]})\u2212 wi({v[\u03c3]}) \u00b7 P\u22121wi({v[\u03c3]}) \u2265 2wi({v\u2032}) \u00b7 P\u22121wi({v[\u03c3\u2032]})\u2212 wi({v[\u03c3\u2032]}) \u00b7 P\u22121wi({v[\u03c3\u2032]})\nfor all i and \u03c3\u2032 if v\u2032 is in the same community as v[\u03c3], and\n2wi({v\u2032}) \u00b7 P\u22121wi({v[\u03c3]})\u2212 wi({v[\u03c3]}) \u00b7 P\u22121wi({v[\u03c3]}) \u2264 2wi({v\u2032}) \u00b7 P\u22121wi({v[\u03c3\u2032]})\u2212 wi({v[\u03c3\u2032]}) \u00b7 P\u22121wi({v[\u03c3\u2032]})\u2212 13(2x(min pj)\u22121/2 + x2)\nfor some i and \u03c3 otherwise. So,\nzi(v[\u03c3] \u00b7 v[\u03c3])\u2212 2zi(v[\u03c3] \u00b7 v\u2032) \u2264 zi(v[\u03c3\u2032] \u00b7 v[\u03c3])\u2212 2zi(v[\u03c3\u2032] \u00b7 v\u2032) + 19 3 \u00b7 (2x(min pj)\u22121/2 + x2)\nfor all i and \u03c3 iff v\u2032 is in the same community as v[\u03c3]. So,\nmax \u03c3\u2032 6=\u03c3,i\u2264h\u2032\u2032\nzi(v[\u03c3]\u00b7v[\u03c3])\u22122zi(v[\u03c3]\u00b7v\u2032)\u2212[zi(v[\u03c3\u2032]\u00b7v[\u03c3\u2032])\u22122zi(v[\u03c3\u2032]\u00b7v\u2032)] \u2264 19 3 \u00b7(2x(min pj)\u22121/2+x2)\niff v\u2032 is in the same community as v[\u03c3]. Therefore, the algorithm returns the correct result with probability 1\u2212 o(1).\nAt this point, we can finally start giving algorithms for classifying a graph\u2019s vertices.\nLemma 13. Let x\u2032 > 0, and assume that all of the following hold:\n< 1\n(1\u2212 c)\u03bb4h\u2032 > 4\u03bb31 0 < x \u2264 x\u2032 < \u03bb1k \u03bbh\u2032 min pi (2(1\u2212 c)\u03bb31/\u03bb2h\u2032)1\u2212 /3 < (1\u2212 c)\u03bb1 (1 + /3) > log((1\u2212 c)\u03bb1)/ log((1\u2212 c)\u03bb2h\u2032/2\u03bb1) 13(2x\u2032(min pj)\n\u22121/2 + (x\u2032)2) < min 6=0 (wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032}))\n\u2203k such that every entry of Qk is positive \u2203w \u2208 Rk such that QPw = \u03bb1w,w \u00b7 Pw = 1, and x \u2264 minwi/2. h\u2032\u2032 = h\u2032 |\u03bbi \u2212 \u03bb\u2032i| \u2264 ln\u22123/2(n) for all i\nUnreliable-graph-classification-algorithm(G,c,m, ,x,(\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)):\nRandomly assign each edge in G to E independently with probability c.\nRandomly select m vertices in G, v[0], ..., v[m\u2212 1].\nLet r = (1\u2212 3) log n/ log((1\u2212 c)\u03bb \u2032 1)\u2212 \u221a lnn and r\u2032 = 2 3 \u00b7 log n/ log((1\u2212 c)\u03bb \u2032 1)\nCompute Nr\u2032\u2032[G\\E](v[i]) for each r \u2032\u2032 \u2264 r + 2h\u2032\u2032 + 3 and 0 \u2264 i < m.\nRun vertex-comparison-algorithm(v[i], v[j], r, r\u2032, E, x, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) for every i and j\nIf these give consistent results, randomly select one alleged member of each community v\u2032[\u03c3]. Otherwise, fail.\nFor every v\u2032\u2032 in the graph, compute Nr\u2032\u2032[G\\E](v \u2032\u2032) for each r\u2032\u2032 \u2264 r\u2032. Then, run vertex-classification-algorithm(v\u2032[], v\u2032\u2032, r, r\u2032, E, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) in order to get a hypothesized classification of v\u2032\u2032\nReturn the resulting classification.\nLet y = 2ke \u2212\nx2(1\u2212c)\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x) / 1\u2212 e\u2212 x2(1\u2212c)\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x) \u00b7(( (1\u2212c)\u03bb4h\u20324\u03bb31 )\u22121) \nand y\u2032 = 2ke \u2212\nx\u20322(1\u2212c)\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x\u2032) / 1\u2212 e\u2212 x\u20322(1\u2212c)\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x\u2032) \u00b7(( (1\u2212c)\u03bb4h\u20324\u03bb31 )\u22121) \nThis algorithm runs in O(m2n1\u2212 3 + n1+ 2 3 ) time. Furthermore, with probability 1\u2212 o(1), G is such that Unreliable-graph-classification-algorithm(G, c,m, , x, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) has at least a 1\u2212 k(1\u2212min pi)m \u2212my\nchance of classifying at least 1\u2212 y\u2032 of G\u2019s vertices correctly.\nProof. Let r0 = (1\u2212 3) log n/ log((1\u2212c)\u03bb1). There exists y \u2032\u2032 < y such that if these conditions hold, then with probability 1\u2212 o(1), at least 1\u2212 y\u2032\u2032 of G\u2019s vertices are (r0, x)-good and the number of vertices in G in community \u03c3 is within \u221a n log n of p\u03c3n for all \u03c3. If this is the case, then for sufficiently large n, it is at least 1\u2212 k(1\u2212min pi)m \u2212my likely that every one of the m randomly selected vertices is (r0, x)-good and at least one is selected from each community.\nIf v[i] is (r0, x)-good for all i, then with probability 1 \u2212 o(1), vertex-comparisonalgorithm(v[i], v[j], r, r\u2032, E, x, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) determines whether or not v[i] and v[j] are in the same community correctly for every i and j, allowing the algorithm to pick one member of each community. If that happens, then the algorithm will classify each (r\u2032+h\u2032, x\u2032)-good\nvertex correctly with probability 1\u2212o(1). So, as long as the initial selection of v[] is good, the algorithm classifies at least 1\u2212 y\u2032 of the graph\u2019s vertices correctly with probability 1\u2212 o(1).\nGenerating E and v[] takes O(n) time. Computing Nr\u2032\u2032[G\\E](v[i]) for all r \u2032\u2032 \u2264 r+ 2h\u2032 + 3 takes O(m| \u222ar\u2032\u2032 Nr\u2032\u2032[G\\E](v[i])) = O(mn) time, and computing Nr\u2032\u2032[G\\E](v\u2032) for all r\u2032\u2032 \u2264 r\u2032 and v\u2032 \u2208 G takes\nO(n| \u222ar\u2032\u2032\u2264r\u2032 Nr\u2032\u2032[G\\E]) = O(n \u00b7 ((1\u2212 c)\u03bb1)r \u2032 ) = O(n1+ 2 3 )\ntime. Once these have been computed, running Vertex-comparisonalgorithm(v[i], v[j], r, r\u2032, E, x, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) for every i and j takes O(m\n2 \u00b7 ((1 \u2212 c)\u03bb1)r) = O(m2n1\u2212 3 ) time, at which point an alleged member of each community can be found in O(m2) time. Running Vertex-classification-algorithm(v\u2032[], v\u2032\u2032, r, r\u2032, E, c, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) for every v\u2032\u2032 \u2208 G takes O(n \u00b7 ((1\u2212 c)\u03bb1)r \u2032 ) = O(n1+ 2 3 ) time. So, the overall algorithm runs in O(m2n1\u2212 3 + n1+ 2 3 ) average time.\nSo, this algorithm can sometimes give a vertex classification that is nontrivially better than that obtained by guessing. However, it has an assymptotically nonzero failure rate and requires too much information about the graph\u2019s parameters. In order to get around that, we combine the results of multiple executions of the algorithm and add in a parameter analysis procedure as follows.\nLemma 14. Assume that there exist x, x\u2032, and such that x is either a unit reciprocal or\nReliable-graph-classification-algorithm(G,m,\u03b4, T(n)) (i.e., Agnostic-sphere-comparison):\nRun Improved-Eigenvalue-approximation-algorithm(.1) in order to compute (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)\nLet \u03bb\u2032\u20321 = \u03bb \u2032 1 + 2 ln \u22123/2(n), \u03bb\u2032\u2032h\u2032\u2032 = \u03bb \u2032 h\u2032\u2032 \u2212 2 ln \u22123/2(n), and k\u2032 = b1/\u03b4c\nLet x be the smallest rational number of minimal numerator such that\nk\u2032(1\u2212 \u03b4)m +m \u00b7 2k\u2032e \u2212\nx2(\u03bb\u2032\u2032 h\u2032\u2032 ) 2\u03b4\n16\u03bb\u2032\u20321 (k \u2032)3/2((\u03b4)\u22121/2+x) / 1\u2212 e\u2212 x2(\u03bb\u2032\u2032h\u2032\u2032 )2\u03b416\u03bb\u2032\u20321 (k\u2032)3/2((\u03b4)\u22121/2+x) \u00b7(( (\u03bb\u2032\u2032h\u2032\u2032 )44(\u03bb\u2032\u20321 )3 )\u22121)  < 1\n2\nLet be the smallest rational number of the form 1z or 1\u2212 1 z such that (2(\u03bb \u2032\u2032 1) 3/(\u03bb\u2032\u2032h\u2032\u2032) 2)1\u2212 /3 < \u03bb\u2032\u20321 and (1 + /3) > log(\u03bb \u2032\u2032 1)/ log((\u03bb \u2032\u2032 h\u2032\u2032) 2/2\u03bb\u2032\u20321)\nLet c be the largest unit reciprocal less than 1/9 such that all of the following hold:\n(1\u2212 c)(\u03bb\u2032\u2032h\u2032\u2032)4 > 4(\u03bb\u2032\u20321)3 (2(1\u2212 c)(\u03bb\u2032\u20321)3/(\u03bb\u2032\u2032h\u2032)2)1\u2212 /3 < (1\u2212 c)\u03bb\u2032\u20321 (1 + /3) > log((1\u2212 c)\u03bb\u2032\u20321)/ log((1\u2212 c)(\u03bb\u2032\u2032h\u2032)2/2\u03bb\u2032\u20321)\nk\u2032(1\u2212 \u03b4)m +m \u00b7 2k\u2032e \u2212\nx2(1\u2212c)(\u03bb\u2032\u2032 h\u2032 ) 2\u03b4\n16\u03bb\u2032\u20321 (k \u2032)3/2((\u03b4)\u22121/2+x) / 1\u2212 e\u2212 x2(1\u2212c)(\u03bb\u2032\u2032h\u2032\u2032 )2\u03b416\u03bb\u2032\u20321 (k\u2032)3/2((\u03b4)\u22121/2+x) \u00b7(( (1\u2212c)(\u03bb\u2032\u2032h\u2032\u2032 )44(\u03bb\u2032\u20321 )3 )\u22121)  < 1\n2\nRun Unreliable-graph-classification-algorithm(G, c,m, , x, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) T (n) times and record the resulting classifications.\nFind the smallest y\u2032\u2032 such that there exists a set of more than half of the classifications no two of which have more than y\u2032\u2032 disagreement, and discard all classifications not in the set. In this step, define the disagreement between two classifications as the minimum disagreement over all bijections between their communities.\nFor every vertex in G, randomly pick one of the remaining classifications and assert that it is in the community claimed by that classification, where a community from one classification is assumed to correspond to the community it has the greatest overlap with in each other classification.\nReturn the resulting combined classification.\nan integer, is a rational number of the form 1z or 1\u2212 1 z , and all of the following hold:\n2ke \u2212\n.9x2\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x) / 1\u2212 e\u2212 .9x2\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x) \u00b7(( .9\u03bb4h\u20324\u03bb31 )\u22121)  < 1\n2\n.9(\u03bb2h\u2032/2) 4 > \u03bb71 \u03bb4h\u2032 > 4\u03bb 3 1 0 < x \u2264 x\u2032 < \u03bb1k \u03bbh\u2032 min pi (2\u03bb31/\u03bb 2 h\u2032) 1\u2212 /3 < \u03bb1 (1 + /3) > log(\u03bb1)/ log(\u03bb 2 h\u2032/2\u03bb1) 13(2x\u2032(min pj) \u22121/2 + (x\u2032)2) < min\n6=0 (wi({v})\u2212 wi({v\u2032})) \u00b7 P\u22121(wi({v})\u2212 wi({v\u2032}))\nevery entry of Qk is positive\n\u2203w \u2208 Rk such that QPw = \u03bb1w,w \u00b7 Pw = 1, and x \u2264 minwi/2. \u03b4 \u2264 min pi\nb1/\u03b4c \u00b7 (1\u2212 \u03b4)m +m \u00b7 2b1/\u03b4ce \u2212\nx2\u03bb2 h\u2032\u03b4\n16\u03bb1b1/\u03b4c3/2(\u03b4\u22121/2+x) / 1\u2212 e\u2212 x2\u03bb2h\u2032\u03b416\u03bb1b1/\u03b4c3/2(\u03b4\u22121/2+x) \u00b7(( \u03bb4h\u20324\u03bb31 )\u22121)  < 1\n2\nT (n) = w(1)\nT (n) \u2264 ln(n)\nmin pi > 8ke \u2212\n.9x\u20322\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x\u2032) / 1\u2212 e\u2212.9 x\u20322\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x\u2032) \u00b7(( .9\u03bb4h\u20324\u03bb31 )\u22121) \nWith probability 1\u2212 o(1), Reliable-graph-classification-algorithm(G,m,\u03b4, T(n)) runs in O(m2n1\u2212 3T (n) + n1+ 2 3 T (n)) time and classifies at least 1\u2212 3y\u2032 of G\u2019s vertices correctly, where\ny\u2032 = 2ke \u2212\n.9x\u20322\u03bb2 h\u2032 min pi\n16\u03bb1k 3/2((min pi) \u22121/2+x\u2032) / 1\u2212 e\u2212.9 x\u20322\u03bb2h\u2032 min pi16\u03bb1k3/2((min pi)\u22121/2+x\u2032) \u00b7(( .9\u03bb4h\u20324\u03bb31 )\u22121) \nProof. With probability 1\u2212o(1), Improved-Eigenvalue-approximation-algorithm gives output such that h\u2032\u2032 = h\u2032 and |\u03bbi \u2212 \u03bb\u2032i| \u2264 ln\n\u22123/2(n) for each i. Assuming that this holds, the algorithm finds the largest x and that satisfy the conditions above and the largest unit reciprocal less than 1/9, c, that satisifes the conditions for Unreliable-graph-classificationalgorithm(G, c,m, , x, (\u03bb\u20321, ..., \u03bb \u2032 h\u2032\u2032)) to have a greater than 1/2 success rate for any k \u2264 b1/\u03b4c with probability 1\u2212 o(1). Since its success rate is greater than 1/2 and T (n) = \u03c9(1), more than half of the executions of Unreliable-graph-classification-algorithm give classifications with error y\u2032 or less with probability 1\u2212 o(1). That means that y\u2032\u2032 \u2264 2y\u2032 and at least one of the classifications in the selected set must have error y\u2032 or less. Thus, all of the selected classifications have error 3y\u2032 or less. The requirement that min pi > 4y\n\u2032 ensures that the bijection between any two of these classifications\u2019 communities that minimizes disagreeement\nis the identity. Therefore, this algorithm classifies at least 1\u2212 3y\u2032 of the vertices correctly, as desired.\nAssuming this all works correctly, Improved-Eigenvalue-approximation-algorithm runs in O(n ln(n)) time. Finding x, , and c takes constant time, and running Unreliable-graphclassification-algorithm T (n) times takes O(m2n1\u2212 3T (n) + n1+ 2 3 T (n)). Computing the degree of agreement between each pair of classifications takes O(n ln2(n)) time. T (n) < lnn, so the brute force algorithm finds y\u2032\u2032 and the corresponding set in O(n) time. Combining the classifications takes O(n) time. Therefore, this whole algorithm runs in O(m2n1\u2212 3T (n) + n1+ 2 3 T (n)) time with probability 1\u2212 o(1), as desired.\nProof of Theorem 6. If the conditions hold, Reliable-graph-classificationalgorithm(G, ln(4b1/\u03b4c)/\u03b4, \u03b4, ln(n)) has the desired properties by the previous lemma."}, {"heading": "7.2 Exact recovery", "text": "Recall that p is a probability vector of dimension k, Q is a k \u00d7 k symmetric matrix with positive entries, and G2(n, p,Q) denotes the stochastic block model with community prior p and connectivity matrix ln(n)Q/n. A random graph G drawn under G2(n, p,Q) has a planted community assignment, which we denote by \u03c3 \u2208 [k]n and call sometime the true community assignment.\nRecall also that exact recovery is solvable for a community partition [k] = tts=1As, if there exists an algorithm that assigns to each node in G an element of {A1, . . . , At} that contains its true community7 with probability 1 \u2212 on(1). Exact recovery is solvable in SBM(n, p,W ) if it is solvable for the partition of [k] into k singletons, i.e., all communities can be recovered."}, {"heading": "7.2.1 Formal results", "text": "Definition 16. Let \u00b5, \u03bd be two positive measures on a discrete set X , i.e., two functions from X to R+. We define the CH-divergence between \u00b5 and \u03bd by\nD+(\u00b5, \u03bd) := max t\u2208[0,1] \u2211 x\u2208X ( t\u00b5(x) + (1\u2212 t)\u03bd(x)\u2212 \u00b5(x)t\u03bd(x)1\u2212t ) . (16)\nNote that for a fixed t,\u2211 x\u2208X ( t\u00b5(x) + (1\u2212 t)\u03bd(x)\u2212 \u00b5(x)t\u03bd(x)1\u2212t ) is an f -divergence. For t = 1/2, i.e., the gap between the arithmetic and geometric means, we have \u2211\nx\u2208X t\u00b5(x) + (1\u2212 t)\u03bd(x)\u2212 \u00b5(x)t\u03bd(x)1\u2212t = 1 2 \u2016\u221a\u00b5\u2212 \u221a \u03bd\u201622 (17)\n7Up to a relabelling of the communities.\nwhich is the Hellinger divergence (or distance), and the maximization over t of the part\u2211 x \u00b5(x)\nt\u03bd(x)1\u2212t is the exponential of the Chernoff divergence. We refer to Section 8.3 for further discussions on D+. Note also that we will often evaluate D+ as D+(x, y) where x, y are vectors instead of measures.\nDefinition 17. For the SBM G2(n, p,Q), where p has dimension k (i.e., there are k communities), the finest partition of [k] is the partition of [k] in to the largest number of subsets such that D+((PQ)i, (PQ)j) \u2265 1 for all i, j that are in different subsets.\nWe next present our main theorem for exact recovery. We first provide necessary and sufficient conditions for exact recovery of partitions, and then provide an agnostic algorithm that solves exact recovery efficiently, more precisely, in quasi-linear time.\nRecall that from [AS15] exact recovery is solvable in the stochastic block model G2(n, p,Q) for a partition [k] = tts=1As if and only if for all i and j in different subsets of the partition,\nD+((PQ)i, (PQ)j) \u2265 1, (18)\nwhere (PQ)i denotes the i-th row of the matrix PQ. In particular, exact recovery is solvable in G2(n, p,Q) if and only if mini,j\u2208[k],i 6=j D+((PQ)i, (PQ)j) \u2265 1.\nTheorem 7. Let k \u2208 Z+ denote the number of communities, p \u2208 (0, 1)k with |p| = 1 denote the community prior, P = diag(p), and let Q \u2208 (0,\u221e)k\u00d7k symmetric with no two rows equal. For G \u223c G2(n, p,Q), the algorithm Agnostic-degree-profiling(G, ln lnn4 lnn ) recovers the finest partition, runs in o(n1+ ) time for all > 0, and does not need to know the parameters (it uses no input except the graph in question).\nNote that the second item in the theorem implies that Agnostic-degree-profiling solves exact recovery efficiently whenever the parameters p and Q allow for exact recovery to be solvable.\nRemark 1. If Qij = 0 for some i and j then the results above still hold, except that if for all i and j in different subsets of the partition,\nD+((PQ)i, (PQ)j) \u2265 1, (19)\nbut there exist i and j in different subsets of the partition such that D+((PQ)i, (PQ)j) = 1 and ((PQ)i,k \u00b7(PQ)j,k \u00b7((PQ)i,k\u2212(PQ)j,k) = 0 for all k, then the optimal algorithm will have an assymptotically constant failure rate. The recovery algorithm also needs to be modified to accomodate 0\u2019s in Q.\nThe algorithm Agnostic-degree-profiling is given in Section 3.1 and replicated below. The idea is to recover the communities with a two-step procedure, similarly to one of the algorithms used in [ABH14] for the two-community case. In the first step, we run Agnostic-sphere-comparison on a sparsified version of G2(n, p,Q) which has a slowly growing average degree. Hence, from Corollary 2, Agnostic-sphere-comparison recovers correctly a fraction of nodes that is arbitrarily close to 1 (w.h.p.). In the second step, we proceed to an improvement of the first step classification by making local checks for each node in the residue graph and deciding whether the node should be moved to another\ncommunity or not. This step requires solving a hypothesis testing problem for deciding the local degree profile of vertices in the SBM. The CH-divergence appears when resolving this problem, as the mis-classification error exponent. We present this result of self-interest in Section 7.2.2. The proof of Theorem 7 is given in Section 7.2.3.\nAgnostic-degree-profiling algorithm. Inputs: a graph G = ([n], E), and a splitting parameter \u03b3 \u2208 [0, 1] (see Theorem 7 for the choice of \u03b3). Output: Each node v \u2208 [n] is assigned a community-list A(v) \u2208 {A1, . . . , At}, where A1, . . . , At is intended to be the partition of [k] in to the largest number of subsets such that D+((pQ)i, (pQ)j) \u2265 1 for all i, j in [k] that are in different subsets. Algorithm: (1) Define the graph G\u2032 on the vertex set [n] by selecting each edge in G independently with probability \u03b3, and define the graph G\u2032\u2032 that contains the edges in G that are not in G\u2032. (2) Run Agnostic-sphere-comparison on G\u2032 to obtain the preliminary classification \u03c3\u2032 \u2208 [k]n (see Section 7.1 and Corollary 2.) (3) Estimate p and Q based on the alleged communities\u2019 sizes and the edge densities between them. (4) For each node v \u2208 [n], determine in which community node v is most likely to belong to based on its degree profile computed from the preliminary classification \u03c3\u2032 and the estimates of p and Q (see Section 7.2.2), and call it \u03c3\u2032\u2032v (5) Re-estimate p and Q based on the sizes of the communities claimed by \u03c3\u2032\u2032 and the edge densities between them. (6) For each node v \u2208 [n], determine in which group A1, . . . , At node v is most likely to belong to based on its degree profile computed from the preliminary classification \u03c3\u2032\u2032 and the new estimates of P and Q (see Section 7.2.2)."}, {"heading": "7.2.2 Testing degree profiles", "text": "In this section, we consider the problem of deciding which community a node in the SBM belongs to based on its degree profile. The notions are the same as in [AS15], we repeat them to ease the reading and explain how the Agnostic-degree-profiling algorithm works.\nDefinition 18. The degree profile of a node v \u2208 [n] for a partition of the graph\u2019s vertices into k communities is the vector d(v) \u2208 Zk+, where the j-th component dj(v) counts the number of edges between v and the vertices in community j. Note that d(v) is equal to N1(v) as defined in Definition 8.\nFor G \u223c G2(n, p,Q), community i \u2208 [k] has a relative size that concentrates exponentially fast to pi. Hence, for a node v in community j, d(v) is approximately given by \u2211 i\u2208[k]Xijei, where Xij are independent and distributed as Bin(npi, ln(n)Qi,j/n), and where Bin(a, b) denotes8 the binomial distribution with a trials and success probability b. Moreover, the Binomial is well-enough approximated by a Poisson distribution of the same mean in this\n8Bin(a, b) refers to Bin(bac, b) if a is not an integer.\nregime. In particular, Le Cam\u2019s inequality gives\u2225\u2225\u2225\u2225Bin(na, ln(n)n b ) \u2212 P (ab ln(n)) \u2225\u2225\u2225\u2225 TV \u2264 2ab 2 ln2(n) n , (20)\nhence, by the additivity of Poisson distribution and the triangular inequality,\n\u2016\u00b5d(v) \u2212 P(ln(n) \u2211 i\u2208[k] piQi,jei)\u2016TV = O ( ln2(n) n ) . (21)\nWe will rely on a simple one-sided bound (see (44)) to approximate our events under the Poisson measure.\nConsider now the following problem. Let G be drawn under the G2(n, p,Q) SBM and assume that the planted partition is revealed except for a given vertex. Based on the degree profile of that vertex, is it possible to classify the vertex correctly with high probability? We have to resolve a hypothesis testing problem, which involves multivariate Poisson distributions in view of the previous observations. We next study this problem.\nTesting multivariate Poisson distributions. Consider the following Bayesian hypothesis testing problem with k hypotheses. The random variable H takes values in [k] with P{H = j} = pj (this is the a priori distribution of H). Under H = j, an observed random variable D is drawn from a multivariate Poisson distribution with mean \u03bb(j) \u2208 Rk+, i.e.,\nP{D = d|H = j} = P\u03bb(j)(d), d \u2208 Zk+, (22)\nwhere\nP\u03bb(j)(d) = \u220f i\u2208[k] P\u03bbi(j)(di), (23)\nand\nP\u03bbi(j)(di) = \u03bbi(j)\ndi\ndi! e\u2212\u03bbi(j). (24)\nIn other words, D has independent Poisson entries with different means. We use the following notation to summarize the above setting:\nD|H = j \u223c P(\u03bb(j)), j \u2208 [k]. (25)\nOur goal is to infer the value of H by observing a realization of D. To minimize the error probability given a realization of D, we must pick the most likely hypothesis conditioned on this realization, i.e.,\nargmaxj\u2208[k]P{D = d|H = j}pj , (26)\nwhich is the Maximum A Posteriori (MAP) decoding rule.9 To resolve this maximization, we can proceed to a tournament of k \u2212 1 pairwise comparisons of the hypotheses. Each comparison allows us to eliminate one candidate for the maxima, i.e.,\nP{D = d|H = i}pi > P{D = d|H = j}pj \u21d2 H 6= j. (27)\n9Ties can be broken arbitrarily.\nThe error probability Pe of this decoding rule is then given by, Pe = \u2211 i\u2208[k] P{D \u2208 Bad(i)|H = i}pi, (28)\nwhere Bad(i) is the region in Zk+ where i is not maximizing (26). Moreover, for any i \u2208 [k], P{D \u2208 Bad(i)|H = i} \u2264 \u2211 j 6=i P{D \u2208 Badj(i)|H = i} (29)\nwhere Badj(i) is the region in Zk+ where P{D = x|H = i}pi \u2264 P{D = x|H = j}pj . Note that with this upper-bound, we are counting the overlap regions where P{D = x|H = i}pi \u2264 P{D = x|H = j}pj for different j\u2019s multiple times, but no more than k \u2212 1 times. Hence,\u2211\nj 6=i P{D \u2208 Badj(i)|H = i} \u2264 (k \u2212 1)P{D \u2208 Bad(i)|H = i}. (30)\nPutting (28) and (29) together, we have Pe \u2264 \u2211 i 6=j P{D \u2208 Badj(i)|H = i}pi, (31)\n= \u2211 i<j \u2211 d\u2208Zk+ min(P{D = d|H = i}pi,P{D = d|H = j}pj) (32)\nand from (30),\nPe \u2265 1 k \u2212 1 \u2211 i<j \u2211 d\u2208Zk+ min(P{D = d|H = i}pi,P{D = d|H = j}pj). (33)\nTherefore the error probability Pe can be controlled by estimating the terms \u2211\nd\u2208Zk+ min(P{D =\nd|H = i}pi,P{D = d|H = j}pj). In our case, recall that\nP{D = d|H = i} = P\u03bb(i)(d), (34)\nwhich is a multivariate Poisson distribution. In particular, we are interested in the regime where k is constant and \u03bb(i) = ln(n)ci, ci \u2208 Rk+, and n diverges. Due to (32), (33), we can then control the error probability by controlling \u2211 x\u2208Zk+\nmin(Pln(n)ci(x)pi,Pln(n)cj (x)pj), which we will want to be o(1/n) to classify vertices in the SBM correctly with high probability based on their degree profiles (see next section). The following lemma provides the relevant estimates.\nLemma 15. For any c1, c2 \u2208 (R+ \\ {0})k with c1 6= c2 and p1, p2 \u2208 R+ \\ {0},\u2211 x\u2208Zk+ min(Pln(n)c1(x)p1,Pln(n)c2(x)p2) = O ( n \u2212D+(c1,c2)\u2212 ln ln(n)2 ln(n) ) , (35)\n\u2211 x\u2208Zk+ min(Pln(n)c1(x)p1,Pln(n)c2(x)p2) = \u2126 ( n \u2212D+(c1,c2)\u2212 k ln ln(n)2 ln(n) ) , (36)\nwhere D+(c1, c2) is the CH-divergence as defined in (16).\nIn other words, the CH-divergence provides the error exponent for deciding among multivariate Poisson distributions. We did not find this result in the literature, but found a similar result obtained by Verdu\u0301 [Ver86], who shows that the Hellinger distance (the special case with t = 1/2 instead of the maximization over t) appears in the error exponent for testing Poisson point-processes, although [Ver86] does not investigate the exact error exponent. This lemma was proven in [AS15].\nThis lemma together with previous bounds on Pe imply that if D+(ci, cj) > 1 for all i 6= j, the true hypothesis is correctly recovered with probability o(1/n). However, it may be that D+(ci, cj) > 1 only for a subset of (i, j)-pairs. What can we then infer? While we may not recover the true value of H with probability o(1/n), we may narrow down the search within a subset of possible hypotheses with that probability of error.\nTesting composite multivariate Poisson distributions. We now consider the previous setting, but we are no longer interested in determining the true hypothesis, but in deciding between two (or more) disjoint subsets of hypotheses. Under hypothesis 1, the distribution of D belongs to a set of possible distributions, namely P(\u03bbi) where i \u2208 A, and under hypothesis 2, the distribution of D belongs to another set of distributions, namely P(\u03bbi) where i \u2208 B. Note that A and B are disjoint subsets such that A \u222aB = [k]. In short,\nD|H\u0303 = 1 \u223c P(\u03bbi), for some i \u2208 A, (37) D|H\u0303 = 2 \u223c P(\u03bbi), for some i \u2208 B, (38)\nand as before the prior on \u03bbi is pi. To minimize the probability of deciding the wrong hypothesis upon observing a realization of D, we must pick the hypothesis which leads to the larger probability between P{H\u0303 \u2208 A|D = d} and P{H\u0303 \u2208 B|D = d}, or equivalently,\u2211\ni\u2208A P\u03bb(i)(d)pi \u2265 \u2211 i\u2208B P\u03bb(i)(d)pi \u21d2 H\u0303 = 1, (39)\u2211 i\u2208A P\u03bb(i)(d)pi < \u2211 i\u2208B P\u03bb(i)(d)pi \u21d2 H\u0303 = 2. (40)\nIn other words, the problem is similar to the previous one, using the above mixture distributions. If we denote by P\u0303e the probability of making an error with this test, we have\nP\u0303e = \u2211 x\u2208Zk+ min (\u2211 i\u2208A P\u03bb(i)(x)pi, \u2211 i\u2208B P\u03bb(i)(x)pi ) . (41)\nMoreover, applying bounds on the minima of two sums, P\u0303e \u2264 \u2211 x\u2208Zk+ \u2211 i\u2208A,j\u2208B min ( P\u03bb(i)(x)pi,P\u03bb(j)(x)pj ) , (42)\nP\u0303e \u2265 1 |A||B| \u2211 x\u2208Zk+ \u2211 i\u2208A,j\u2208B min ( P\u03bb(i)(x)pi,P\u03bb(j).(x)pj ) . (43)\nTherefore, for constant k and \u03bb(i) = ln(n)ci, ci \u2208 Rk+, with n diverging, it suffices to control the decay of \u2211 x\u2208Zk+ min(P\u03bb(i)(x)pi,P\u03bb(j)(x)pj) when i \u2208 A and j \u2208 B, in order to bound\nthe error probability of deciding whether a vertex degree profile belongs to a group of communities or not.\nThe same reasoning can be applied to the problem of deciding whether a given node belongs to a group of communities, with more than two groups. Also, for any p and p\u2032 such that |pj \u2212 p\u2032j | < lnn/ \u221a n for each j, Q, \u03b3(n), and i,\n\u2211 x\u2208Zk+ max ( Bin( np\u2032, (1\u2212\u03b3(n)) ln(n) n Qi )(x)\u2212 2PPQi(1\u2212\u03b3(n)) ln(n)/n(x), 0 ) = O(1/n2). (44)\nSo, the error rate for any algorithm that classifies vertices based on their degree profile in a graph drawn from a sparse SBM is at most O(1/n2) more than twice what it would be if the probability distribution of degree profiles really was the poisson distribution.\nIn summary, we have proved the following.\nLemma 16. Let k \u2208 Z+ and let A1, . . . , At be disjoint subsets of [k] such that \u222ati=1Ai = [k]. Let G be a random graph drawn under G2(n, p, (1 \u2212 \u03b3(n))Q). Assigning the most likely community subset Ai to a node v based on its degree profile d(v) gives the correct assignment with probability\n1\u2212O ( n\u2212(1\u2212\u03b3(n))\u2206\u2212 1 2 ln((1\u2212\u03b3(n)) lnn)/ lnn + 1\nn2\n) ,\nwhere\n\u2206 = min r,s\u2208[t] r 6=s min i\u2208Ar,j\u2208As D+((pQ)i, (pQ)j). (45)\nMoreover, in order to prove Theorem 7, we will need a version of this lemma that still holds when some of our information is inaccurate. First of all, consider attempting the previous testing procedure when one thinks the distributions are \u03bb\u2032 with probability p\u2032 instead of \u03bb with probability p. Assume that there exists such that for all i and j, |\u03bb\u2032(i)j\u2212\u03bb(i)j | \u2264 min(\u03bb\u2032(i)j , \u03bb(i)j) and |p\u2032i\u2212pi| \u2264 min(p\u2032i, pi). Then for any i \u2208 A, x \u2208 Zk+, the previous hypothesis testing procedure will not classify x as being in B unless\u2211\nj\u2208B (1 + )|x|+1P\u03bb(j)(x)pj \u2265 (1 + )\u2212|x|\u22121P\u03bb(i)(x)pi\nThat means that for any i \u2208 A, x \u2208 Zk+, the probability that x arises from P\u03bb(i) and is then misclassified as being in B is at most\nP\u03bb(i)(x)pi \u2264 \u2211 j\u2208B (1 + )2|x|+2P\u03bb(j)(x)pj\nSo, this hypothesis testing procedure has an error probability of at most\u2211 x\u2208Zk+ (1 + )2|x|+2 \u2211 i\u2208A,j\u2208B min ( P\u03bb(i)(x)pi,P\u03bb(j)(x)pj ) Furthermore,\nLemma 17. For any p and Q there exists c such that the following holds for all \u03b4 < min pi/2. Let G be a random graph drawn from G2(n, p,Q), and \u03c3\u2032 be a classification of G\u2019s vertices that misclassifies \u03b4n vertices. Also, let p\u2032 be the frequencies with which vertices are classified as being in the communities, and Q\u2032 be n/ lnn times the edge densities between the alleged communities. Then with probability 1\u2212o(1), every element of p\u2032 or Q\u2032 is within c\u03b4+ \u221a lnn/n of the corresponding element of p or Q.\nProof. With probability 1 \u2212 o(1), the size of every community is within \u221a\nlnn/n of its expected value, and the edge density between each pair of communities is within \u221a n lnn of its expected value. Also, there exists a constant c\u2032 such that with probability 1\u2212 o(1), no vertex has degree more than c\u2032 lnn. Assuming this is the case, the misclassification of vertices can not change the apparent number of edges between any two communities by more than c\u2032\u03b4n lnn, and it can not change the apparent size of any community by more than \u03b4n. Thus, |p\u2032i \u2212 pi| \u2264 \u221a lnn/n+ \u03b4 and\n|Q\u2032i,j \u2212Qi,j | \u2264 \u221a lnn/n+ 4 c\u2032\u03b4\npi \u00b7 pj + 4Qi,j\n\u03b4pi + \u03b4pj + \u03b4 2\npipj\nThis lets us prove the following \u201crobust\u201d version of this lemma to prove Theorem 7.\nLemma 18. Let k \u2208 Z+ and let A1, . . . , At be disjoint subsets of [k] such that \u222ati=1Ai = [k]. Let G be a random graph drawn under G2(n, p, (1 \u2212 \u03b3(n))Q). There exist c1, c2, and c3 such that with probability 1\u2212 o(1) G is such that for any sufficiently small \u03b4, assigning the most likely community subset Ai to a node v based on a distortion of its degree profile that independently gets each node\u2019s community wrong with probability at most \u03b4 and estimates of p and Q based on a classification of the graphs vertices that misclassifies each vertex with probability \u03b4 gives the correct assignment with probability at least\n1\u2212 c2 \u00b7 (1 + c1\u03b4)c3 lnn \u00b7 ( n\u2212(1\u2212\u03b3(n))\u2206\u2212 1 2 ln((1\u2212\u03b3(n)) lnn)/ lnn ) \u2212 c2 n2 ,\nwhere\n\u2206 = min r,s\u2208[t] r 6=s min i\u2208Ar,j\u2208As D+((pQ)i, (pQ)j). (46)\nProof. First, note that by the previous lemma, with probability 1 \u2212 o(1), G is such that every element of the estimates of p and Q is within c\u03b4 + \u221a lnn/n of its true value. Choose c3 such that every vertex in the graph has degree less than c3 lnn with probability 1\u2212 o(1). If this holds, then the probability of misclassifying a vertex based on its true degree profile and the estimates of its parameters is at most\n2(1 + 2c\u03b4/min(pi, (PQ)i,j)) 2c3 lnn+2 \u00b7 ( n\u2212(1\u2212\u03b3(n))\u2206\u2212 1 2 ln((1\u2212\u03b3(n)) lnn)/ lnn ) +O ( 1\nn2 ) based on the previous bounds on the probability that classifying a vertex based on its degree profile fails.\nNow, let\nc\u20321 = max i,j\n\u2211 pi\u2032qi\u2032,j/(piqi,j).\nThe key observation is that v\u2019s mth neighbor had at least a mini,j(piqi,j)/ \u2211 pi\u2032qi\u2032,j chance of actually being in community \u03c3 for each \u03c3, so its probability of being reported as being in community \u03c3 is at most 1 + c\u20321\u03b4 times the probability that it actually is. So, the probability that its reported degree profile is bad is at most (1 + c\u20321\u03b4)\n|N1(v)| times the probability that its actual degree profile is bad. The conclusion follows."}, {"heading": "7.2.3 Proof of Theorem 7", "text": "We prove the possibility result here. The converse was proven in [AS15] for known parameters, hence also applies here.\nLet G \u223c G2(n, p,Q) and \u03b3 = ln lnn4 lnn , and A1, . . . , At be a partition of [k]. Agnostic-degreeprofiling(G, p,Q, \u03b3) recovers the partition [k] = tts=1As with probability 1\u2212 on(1) if for all i, j in [k] that are in different subsets,\nD+((PQ)i, (PQ)j) \u2265 1. (47)\nThe idea behind Claim 1 is contained in Lemma 16. However, there are several technical steps that need to be handled:\n1. The graphs G\u2032 and G\u2032\u2032 obtained in step 1 of the algorithm are correlated, since an edge cannot be both in G\u2032 and G\u2032\u2032. However, this effect can be discarded since two independent versions would share edges with low enough probability.\n2. The classification in step 2 using Agnostic-sphere-comparison has a vanishing fraction of vertices which are wrongly labeled, and the SBM parameters are unknown. This requires using the robust version of Lemma 16, namely Lemma 18.\n3. In the case where D+((PQ)i, (PQ)j) = 1 a more careful classification is needed as carried in steps 3 and 4 of the algorithm.\nProof. With probability 1\u2212O(1/n), no vertex in the graph has degree greater than c3 lnn. Assuming that this holds, no vertex\u2019s set of neighbors in G\u2032\u2032 is more than\n(1\u2212max qi,j lnn/n)\u2212c3 lnn \u00b7 (n/(n\u2212 c3 lnn))c3 lnn = 1 + o(1)\ntimes as likely to occur as it would be if G\u2032\u2032 were independent of G\u2032. So, the fact that they are not has negligible impact on the algorithm\u2019s error rate. Now, let\n\u03b4 = (e (1\u2212\u03b3) 2c3 mini6=j D+((PQ)i,(PQ)j) \u2212 1)/c1.\nBy Lemma 18, if the classification in step 2 has an error rate of at most \u03b4, then the classification in step 3 has an error rate of\nO(n\u2212(1\u2212\u03b3) mini 6=j D+((PQ)i,(PQ)j)/2 + 1/n2),\nobserving that if \u03c3\u2032v 6= \u03c3v the error rate of \u03c3\u2032\u2032v\u2032 for v\u2032 adjacent to v is at worst multiplied by a constant. That in turn ensures that the final classification has an error rate of at most\nO ( (1 +O(n\u2212(1\u2212\u03b3) mini 6=j D+((PQ)i,(PQ)j)/2 + 1/n2))c3 lnn 1\nn lnn\u22121/4\n) = O ( 1\nn lnn\u22121/4\n) ."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Airoldi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2008}, {"title": "Exact recovery in the stochastic block model, Available at ArXiv:1405.3267", "author": ["E. Abbe", "A.S. Bandeira", "G. Hall"], "venue": null, "citeRegEx": "Abbe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abbe et al\\.", "year": 2014}, {"title": "The political blogosphere and the 2004 u.s. election: Divided they blog", "author": ["Lada A. Adamic", "Natalie Glance"], "venue": "Proceedings of the 3rd International Workshop on Link Discovery (New York, NY, USA), LinkKDD \u201905,", "citeRegEx": "Adamic and Glance,? \\Q2005\\E", "shortCiteRegEx": "Adamic and Glance", "year": 2005}, {"title": "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms", "author": ["E. Abbe", "C. Sandon"], "venue": null, "citeRegEx": "Abbe and Sandon,? \\Q2015\\E", "shortCiteRegEx": "Abbe and Sandon", "year": 2015}, {"title": "Random laplacian matrices and convex relaxations", "author": ["A.S. Bandeira"], "venue": null, "citeRegEx": "Bandeira,? \\Q2015\\E", "shortCiteRegEx": "Bandeira", "year": 2015}, {"title": "A nonparametric view of network models and newmangirvan and other modularities", "author": ["P.J. Bickel", "A. Chen"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Bickel and Chen,? \\Q2009\\E", "shortCiteRegEx": "Bickel and Chen", "year": 2009}, {"title": "Graph bisection algorithms with good average case", "author": ["T.N. Bui", "S. Chaudhuri", "F.T. Leighton", "M. Sipser"], "venue": "behavior, Combinatorica", "citeRegEx": "Bui et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Bui et al\\.", "year": 1987}, {"title": "Private graphon estimation for sparse graphs, In preparation", "author": ["C. Borgs", "J. Chayes", "A. Smith"], "venue": null, "citeRegEx": "Borgs et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Borgs et al\\.", "year": 2015}, {"title": "Achieving exact cluster recovery threshold via semidefinite programming, arXiv:1412.6156", "author": ["J. Xu B. Hajek", "Y. Wu"], "venue": null, "citeRegEx": "Hajek and Wu,? \\Q2014\\E", "shortCiteRegEx": "Hajek and Wu", "year": 2014}, {"title": "Eigenvalues and graph bisection: An average-case analysis", "author": ["R.B. Boppana"], "venue": "In 28th Annual Symposium on Foundations of Computer Science", "citeRegEx": "Boppana,? \\Q1987\\E", "shortCiteRegEx": "Boppana", "year": 1987}, {"title": "Achieving optimal misclassification proportion in stochastic block", "author": ["A.Y. Zhang H.H. Zhou C. Gao", "Z. Ma"], "venue": null, "citeRegEx": "Gao and Ma,? \\Q2015\\E", "shortCiteRegEx": "Gao and Ma", "year": 2015}, {"title": "Hill-climbing finds random planted bisections", "author": ["T. Carson", "R. Impagliazzo"], "venue": "Proc. 12th Symposium on Discrete Algorithms (SODA 01), ACM press,", "citeRegEx": "Carson and Impagliazzo,? \\Q2001\\E", "shortCiteRegEx": "Carson and Impagliazzo", "year": 2001}, {"title": "Algorithms for graph partitioning on the planted partition model, Lecture", "author": ["A. Condon", "R.M. Karp"], "venue": "Notes in Computer Science", "citeRegEx": "Condon and Karp,? \\Q1999\\E", "shortCiteRegEx": "Condon and Karp", "year": 1999}, {"title": "Graph partitioning via adaptive spectral techniques, Comb", "author": ["A. Coja-oghlan"], "venue": "Probab. Comput", "citeRegEx": "Coja.oghlan,? \\Q2010\\E", "shortCiteRegEx": "Coja.oghlan", "year": 2010}, {"title": "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery", "author": ["P. Chin", "A. Rao", "V. Vu"], "venue": null, "citeRegEx": "Chin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chin et al\\.", "year": 2015}, {"title": "Stochastic blockmodels with a growing number of classes", "author": ["D.S. Choi", "P.J. Wolfe", "E.M. Airoldi"], "venue": null, "citeRegEx": "Choi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2012}, {"title": "Detecting functional modules in the yeast proteinprotein interaction", "author": ["J. Chen", "B. Yuan"], "venue": "network, Bioinformatics", "citeRegEx": "Chen and Yuan,? \\Q2006\\E", "shortCiteRegEx": "Chen and Yuan", "year": 2006}, {"title": "The solution of some random NP-hard problems in polynomial expected time", "author": ["M.E. Dyer", "A.M. Frieze"], "venue": "Journal of Algorithms", "citeRegEx": "Dyer and Frieze,? \\Q1989\\E", "shortCiteRegEx": "Dyer and Frieze", "year": 1989}, {"title": "Statistical analysis of multiple sociometric relations, Journal of The American Statistical Association", "author": ["S.E. Fienberg", "M.M. Meyer", "S.S. Wasserman"], "venue": null, "citeRegEx": "Fienberg et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Fienberg et al\\.", "year": 1985}, {"title": "Efficient discovery of overlapping communities in massive networks", "author": ["P.K. Gopalan", "D.M. Blei"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Gopalan and Blei,? \\Q2013\\E", "shortCiteRegEx": "Gopalan and Blei", "year": 2013}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Girvan and Newman,? \\Q2002\\E", "shortCiteRegEx": "Girvan and Newman", "year": 2002}, {"title": "Community detection in sparse networks via Grothendieck\u2019s inequality", "author": ["O. Gu\u00e9don", "R. Vershynin"], "venue": null, "citeRegEx": "Gu\u00e9don and Vershynin,? \\Q2014\\E", "shortCiteRegEx": "Gu\u00e9don and Vershynin", "year": 2014}, {"title": "The metropolis algorithm for graph bisection", "author": ["Mark Jerrum", "Gregory B. Sorkin"], "venue": "Discrete Applied Mathematics", "citeRegEx": "Jerrum and Sorkin,? \\Q1998\\E", "shortCiteRegEx": "Jerrum and Sorkin", "year": 1998}, {"title": "Cluster analysis for gene expression data: a survey, Knowledge and Data Engineering", "author": ["D. Jiang", "C. Tang", "A. Zhang"], "venue": "IEEE Transactions on", "citeRegEx": "Jiang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2004}, {"title": "Stochastic blockmodels and community structure in networks", "author": ["B. Karrer", "M.E.J. Newman"], "venue": "Phys. Rev. E", "citeRegEx": "Karrer and Newman,? \\Q2011\\E", "shortCiteRegEx": "Karrer and Newman", "year": 2011}, {"title": "Statistical properties of community structure in large social and information networks", "author": ["J. Leskovec", "K.J. Lang", "A. Dasgupta", "M.W. Mahoney"], "venue": "Proceedings of the 17th international conference on World Wide Web (New York, NY, USA),", "citeRegEx": "Leskovec et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2008}, {"title": "Community detection thresholds and the weak Ramanujan property", "author": ["L. Massouli\u00e9"], "venue": "STOC 2014: 46th Annual Symposium on the Theory of Computing (New York, United States),", "citeRegEx": "Massouli\u00e9,? \\Q2014\\E", "shortCiteRegEx": "Massouli\u00e9", "year": 2014}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "Annual Symposium on Foundations of Computer Science", "citeRegEx": "McSherry,? \\Q2001\\E", "shortCiteRegEx": "McSherry", "year": 2001}, {"title": "Stochastic block models and reconstruction, Available online at arXiv:1202.1499", "author": ["E. Mossel", "J. Neeman", "A. Sly"], "venue": null, "citeRegEx": "Mossel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mossel et al\\.", "year": 2012}, {"title": "Detecting protein function and protein-protein interactions from genome", "author": ["E.M. Marcotte", "M. Pellegrini", "H.-L. Ng", "D.W. Rice", "T.O. Yeates", "D. Eisenberg"], "venue": "sequences, Science", "citeRegEx": "Marcotte et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Marcotte et al\\.", "year": 1999}, {"title": "Inference of Population Structure Using Multilocus Genotype", "author": ["J.K. Pritchard", "M. Stephens", "P. Donnelly"], "venue": "Data, Genetics", "citeRegEx": "Pritchard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pritchard et al\\.", "year": 2000}, {"title": "Regularized spectral clustering under the degree-corrected stochastic blockmodel, Advances in Neural Information Processing Systems", "author": ["T. Qin", "K. Rohe"], "venue": null, "citeRegEx": "Qin and Rohe,? \\Q2013\\E", "shortCiteRegEx": "Qin and Rohe", "year": 2013}, {"title": "Spectral clustering and the high-dimensional stochastic blockmodel", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "The Annals of Statistics", "citeRegEx": "Rohe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohe et al\\.", "year": 2011}, {"title": "Image processing, analysis, and machine vision, Thomson-Engineering", "author": ["M. Sonka", "V. Hlavac", "R. Boyle"], "venue": null, "citeRegEx": "Sonka et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sonka et al\\.", "year": 2007}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Shi and Malik,? \\Q1997\\E", "shortCiteRegEx": "Shi and Malik", "year": 1997}, {"title": "Estimation and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure", "author": ["T.A.B. Snijders", "K. Nowicki"], "venue": "Journal of Classification", "citeRegEx": "Snijders and Nowicki,? \\Q1997\\E", "shortCiteRegEx": "Snijders and Nowicki", "year": 1997}, {"title": "Exploring complex networks, Nature", "author": ["S.H. Strogatz"], "venue": null, "citeRegEx": "Strogatz,? \\Q2001\\E", "shortCiteRegEx": "Strogatz", "year": 2001}, {"title": "Asymptotic error probability of binary hypothesis testing for poisson point-process observations (corresp.)", "author": ["S. Verd\u00fa"], "venue": "Information Theory, IEEE Transactions on", "citeRegEx": "Verd\u00fa,? \\Q1986\\E", "shortCiteRegEx": "Verd\u00fa", "year": 1986}, {"title": "A simple svd algorithm for finding hidden partitions, Available online at arXiv:1404.3918", "author": ["V. Vu"], "venue": null, "citeRegEx": "Vu,? \\Q2014\\E", "shortCiteRegEx": "Vu", "year": 2014}, {"title": "Stochastic blockmodels for directed graphs", "author": ["Y.J. Wang", "G.Y. Wong"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Wang and Wong,? \\Q1987\\E", "shortCiteRegEx": "Wang and Wong", "year": 1987}, {"title": "Jointly clustering rows and columns of binary matrices: Algorithms and trade-offs, SIGMETRICS Perform", "author": ["J. Xu", "R. Wu", "K. Zhu", "B. Hajek", "R. Srikant", "L. Ying"], "venue": "Eval. Rev", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices", "author": ["J. Xu Y. Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "Accurate community detection in the stochastic block model via spectral algorithms", "author": ["S. Yun", "A. Proutiere"], "venue": null, "citeRegEx": "Yun and Proutiere,? \\Q2014\\E", "shortCiteRegEx": "Yun and Proutiere", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Bandeira, and G. Hall, Exact recovery in the stochastic block model, Available at ArXiv:1405.3267. (2014). 1, 4, 50", "startOffset": 0, "endOffset": 106}, {"referenceID": 4, "context": "Bandeira, Random laplacian matrices and convex relaxations, arXiv:1504.03987 (2015). 1, 4", "startOffset": 0, "endOffset": 84}, {"referenceID": 41, "context": "Chen, A nonparametric view of network models and newmangirvan and other modularities, Proceedings of the National Academy of Sciences (2009). 1", "startOffset": 0, "endOffset": 141}, {"referenceID": 9, "context": "Boppana, Eigenvalues and graph bisection: An average-case analysis, In 28th Annual Symposium on Foundations of Computer Science (1987), 280\u2013285.", "startOffset": 0, "endOffset": 135}, {"referenceID": 13, "context": "Coja-oghlan, Graph partitioning via adaptive spectral techniques, Comb. Probab. Comput. 19 (2010), no.", "startOffset": 0, "endOffset": 98}, {"referenceID": 38, "context": "Vu, Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery, arXiv:1501.05021 (2015). 2", "startOffset": 0, "endOffset": 149}, {"referenceID": 41, "context": "Chen, S. Sanghavi, and H. Xu, Clustering Sparse Graphs, arXiv:1210.3335 (2012). 1", "startOffset": 0, "endOffset": 79}, {"referenceID": 41, "context": "Chen and B. Yuan, Detecting functional modules in the yeast proteinprotein interaction network, Bioinformatics 22 (2006), no.", "startOffset": 0, "endOffset": 121}, {"referenceID": 27, "context": "McSherry, Spectral partitioning of random graphs, In 42nd Annual Symposium on Foundations of Computer Science (2001), 529\u2013537.", "startOffset": 0, "endOffset": 117}, {"referenceID": 36, "context": "Strogatz, Exploring complex networks, Nature 410 (2001), no.", "startOffset": 0, "endOffset": 56}, {"referenceID": 37, "context": "Verd\u00fa, Asymptotic error probability of binary hypothesis testing for poisson point-process observations (corresp.), Information Theory, IEEE Transactions on 32 (1986), no.", "startOffset": 0, "endOffset": 167}, {"referenceID": 38, "context": "[Vu14] V. Vu, A simple svd algorithm for finding hidden partitions, Available online at arXiv:1404.3918 (2014). 1, 2", "startOffset": 1, "endOffset": 111}, {"referenceID": 41, "context": "Chen, Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices, arXiv:1402.1267 (2014). 1", "startOffset": 0, "endOffset": 163}], "year": 2015, "abstractText": "Most recent developments on the stochastic block model (SBM) rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in [AS15] for linear size communities. The results are three-fold: (i) in the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and detects communities with an optimal accuracy scaling for large degrees; (ii) in the regime where degrees are scaled by \u03c9(1) (diverging degrees), this is enhanced into a fully agnostic algorithm that only takes the graph in question and simultaneously learns the model parameters (including the number of communities) and detects communities with accuracy 1\u2212 o(1), with an overall quasi-linear complexity; (iii) in the logarithmic degree regime, an agnostic algorithm is developed that learns the parameters and achieves the optimal CH-limit for exact recovery, in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the general SBM with linear size communities. \u2217Program in Applied and Computational Mathematics, and EE department, Princeton University, Princeton, USA, eabbe@princeton.edu. This research was partially supported by the 2014 Bell Labs Prize. \u2020Department of Mathematics, Princeton University, USA, sandon@princeton.edu. ar X iv :1 50 6. 03 72 9v 1 [ m at h. PR ] 1 1 Ju n 20 15", "creator": "LaTeX with hyperref package"}}}