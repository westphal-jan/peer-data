{"id": "1506.04720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Latent Regression Bayesian Network for Data Representation", "abstract": "A major difficulty in learning directed models with many latent variables is the insoluble conclusion. To solve this problem, most existing algorithms assume that the latent variables are made independent of each other, either by developing specific priorities or by approaching the actual posterior by means of a factorized distribution. We believe that the correlations between latent variables are crucial for faithful data representation. Driven by this idea, we propose an inference method based on conditional pseudo-probability and true dependencies between the latent variables. To learn, we propose applying the Hard Expectation Maximization (EM) algorithm that avoids the intractability of traditional EM by maximum probability, rather than by summing up the data probability. Qualitative and quantitative calculations of our model against the state-of-the-art evaluation of benchmark models and the proposed effectiveness of data reconstruction demonstrate.", "histories": [["v1", "Mon, 15 Jun 2015 19:34:59 GMT  (197kb,D)", "http://arxiv.org/abs/1506.04720v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siqi nie", "qiang ji"], "accepted": false, "id": "1506.04720"}, "pdf": {"name": "1506.04720.pdf", "metadata": {"source": "CRF", "title": "Latent Regression Bayesian Network for Data Representation", "authors": ["Siqi Nie", "Qiang Ji"], "emails": ["nies@rpi.edu.", "jiq@rpi.edu."], "sections": [{"heading": "1 Introduction", "text": "Deep directed generative models have received increasing attention recently, because the top-down connections explicitly model the data generating process. Different levels of latent variables capture features (or abstractions [15]) in a coarse-to-fine manner. Compared with undirected models such as restricted Boltzmann machines (RBMs) and deep Boltzmann machines (DBMs) [17], directed generative models have their own advantages. First, samples can be easily obtained by straightforward ancestral sampling without the need for Markov chain Monte Carlo (MCMC) methods. Second, there is no partition function issue since the joint distribution is obtained by multiplying all local conditional probabilities, which requires no further normalization. Last but most importantly, the latent variables are dependent on each other given the observations \u2217Email: nies@rpi.edu. Affiliation: Rensselaer Polytechnic Institute, USA. \u2020Email: jiq@rpi.edu. Affiliation: Rensselaer Polytechnic Institute, USA.\nar X\niv :1\n50 6.\n04 72\n0v 1\n[ cs\n.L G\n] 1\nthrough the so-called \u201cexplain-away\u201d principle. Through their inter dependency, latent variables coordinate with each other to better explain the patterns in the visible layer.\nLearning directed models with many latent variables is challenging, mainly due to the intractable computation of the posterior probability. Although Markov Chain Monte Carlo (MCMC) method is a straightforward solution, the mixing stage is often too slow. To simplify the inference for deep belief networks, Hinton et al. [8] introduced a complementary prior for the latent variables which makes the posterior fully factorized. Some recent efforts for learning generative models have focused on variational methods [12, 13, 16], by introducing another distribution to approximate the true posterior and maximize a variational lower bound of the data likelihood. The approximating distribution is typically fully factorized for computational efficiency. However, the assumption of the factorized distribution sacrifices the \u201c explain-away\u201d effect for efficient inference, which inevitably enlarges the distance to the true posterior, and weakens the representation power of the model. This defeats a major advantage of directed graphical models.\nIn this work, we address the problem of learning deep directed models in a different direction. We propose to use the EM algorithm with two approximations in the inference and learning phases. First, we approximate the true posterior distribution during inference by the conditional pseudo-likelihood, which preserves to certain degree the dependencies among latent variables. Second, we approximate the data likelihood using a max-out setting during the E-step of the learning to overcome the exponential number of configurations of the latent variables. As a result, the E-step requires the maximum a posteriori (MAP) inference, which is efficiently solved based on the pseudo-likelihood. It can also be seen as the application of iterated conditional modes (ICM) [2] to directed graphical models. In the M-step, the problem is transferred into parameter learning with complete data, which is much easier to handle."}, {"heading": "2 Related Work", "text": "The research on learning directed model with latent variables can be dated back to 1990s. A standard approach is the Expectation Maximization (EM) algorithm, which maximizes the expected data log-likelihood for parameter learning. EM algorithm and its variants have been used for learning latent mixture of factor analyzers [6], probabilistic latent semantic indexing [10], probabilistic latent semantic analysis [11] and latent Dirichlet allocation (LDA) [3]. Such models have a few latent variables so that the posterior probability of latent variables can be exactly computed in the E-step.\nIn the case of many latent variables, exact computation of the posterior is intractable because of the exponential number of the latent variable configurations. Patel et al. [15] make one latent variable connecting to a small patch of the input data. Therefore each patch and the corresponding latent variable form a small model, which allows exact maximum a posteriori (MAP) inference. Many other approaches have been proposed to approximate the posterior probability of latent variables along two directions.\nOne approach is to replace the true posterior distribution with a factorized distribution as an approximation. This approach was first proposed by Saul et al. [20], known as the mean field theory for learning sigmoid belief networks. A fully fac-\ntorized variational posterior is introduced to approximate the true posterior distribution of latent variables. Recently, Gan et al. [5] extended the mean field method, and proposed a Bayesian approach to learn deep sigmoid belief networks by introducing sparsity-encoraging priors on the model parameters. Alternatively, the posterior distribution can be approximated using a feed-forward network. The wake-sleep algorithm [9] augments the multi-layer belief networks with feed-forward recognition networks. Wake-sleep alternates between updating the model parameters in the wake phase and the recognition network parameters in the sleep phase. Inspired by this idea, many approaches have been proposed recently for learning directed graphical models by maximizing a variational lower bound on the data log-probability. Mnih and Gregor [13] introduced the neural variational inference and learning (NVIL) algorithm for sigmoid belief networks. A feed-forward inference network is used to obtain exact samples from the variational posterior. A neural network is introduced to reduce the variance of the samples. Kingma and Welling [12] proposed the auto-encoding variational Bayes method for continuous latent variables, in which a reparameterization is employed to efficiently generate samples from the Gaussian distribution. Similarly, Rezende et al. [16] propose a stochastic backpropagation algorithm for learning deep generative models with continuous latent variables. Gregor et al. [7] augment the directed model with an encoder, which is also kind of inference network.\nAnother direction is to make the posterior probability factorized by specifically designing a prior distribution of latent variables. Hinton et al. [8] proposed a complementary prior to ensure a factorized posterior, and proposed a fast learning algorithm for deep belief networks (DBNs), which is basically a hybrid network with a single undirected layer and several directed layers.\nIn all the above-mentioned methods, the inference typically assumes independency among latent variables due to special prior or factorized approximation in order to accelerate the inference. Because of this assumption, the inference network is not able to capture the correlations among the latent variables. Therefore the approximate posterior might differ significantly from the underlying true posterior. In this work, we intend to preserve the latent variable dependencies for better data representation. We approximate the posterior probability by the conditional pseudo-likelihood, and employ a hard version of the EM algorithm for parameter estimation."}, {"heading": "3 Latent Regression Bayesian Network", "text": "We propose a generalized directed graphical model, called latent regression Bayesian network (LRBN), as shown in Fig. 1 (a). The latent variables in LRBN are binary, and the visible variables can be continuous or discrete. Each latent variable is connected to all visible variables. We discuss the parameterization of both cases in the sequel. The case of continuous latent variables can be referred to as factor analyzers [6, 21] or deep latent Gaussian models [12, 16]."}, {"heading": "3.1 Discrete LRBN", "text": "For discrete LRBN, both latent and observation variables are discrete. For brevity, we discuss the binary case with observation variables x \u2208 Bnd and latent variables h \u2208 Bnh . We assume that the latent variables h determine the patterns in data x, therefore directed links are used to model their relationships, as in a Bayesian network.\nPrior probability for latent variables is represented as a log-liner model,\nP (hj = 1) = \u03c3(dj) , (1)\nwhere dj is the parameter defining the prior distribution for node hj ; \u03c3(\u00b7) is the sigmoid function \u03c3(z) = 1/(1 + e\u2212z).\nThe conditional probability given the latent variables is, P (xi = 1|h) = \u03c3( \u2211 j wijhj + bi) , (2)\nwhere wij is the weight of the link connecting node hj and xi; bi is the offset for node xi. The joint probability is,\nP\u03b8(x, h)= exp (\u2211 i,jwijxihj+ \u2211 ibixi+ \u2211 jdjhj\u2212 \u2211 i log ( 1 + exp( \u2211 j wijhj + bi) )) \u220f j(1+exp(dj)) . (3) In this case, the model becomes a sigmoid belief network (SBN) [14] with one latent layer. If more layers are added on top, the conditional probability is defined in the same way as Eq. 2. The model is named a regression model based on the nature of the conditional probability. For discrete visible node, the input to the sigmoid function is a linear combination of the latent variables; for continuous visible node, the mean of a visible node is a linear combination of its parent nodes.\nAs a similar model with undirected links, RBMs (Fig. 1 (b)) have been widely used in the literature for feature learning and data representation. The joint probability defined by a discrete RBM is,\nPRBM(x, h)= 1\nZ exp \u2211 i bixi+ \u2211 i,j wijxihj+ \u2211 j djhj  . (4)\nComparing Eq. 3 and 4, the additional terms in the numerator captures the correlations among them. This is the reason why P (h|x) is not factorized over individual latent nodes hj in LRBN, which is the major difference from RBM. An advantage of Eq. 3 is that every term can be computed given the values of all variables, without the issue of the intractable partition function Z.\nThe discussion of hybrid LRBN is moved to a supplementary material due to the limited space."}, {"heading": "3.2 Hybrid LRBN", "text": "For hybrid LRBN, the observation variables x \u2208 Rnd are continuous while the latent variables are binary h \u2208 Bnh . The prior distribution of the latent variables is the same as Eq. 1. Given the latent variables, the visible variable is assumed to follow Gaussian distribution, whose mean is a linear combination of the latent variables,\nP (xi|h) \u223c N \u2211 j wijhj + bi, \u03c3i  , i = 1, . . . , nd , (5) where wij is the weight of the link connecting node hj and xi; bi is the offset of the mean for node xi; \u03c3i is the standard deviation. To simplify the learning process, each component of the data is normalized to have zero mean and unit variance, therefore \u03c3i is set to 1. From the prior distribution and conditional distribution, the joint distribution for x and h is,\nP\u03b8(x, h) = exp\n( \u2212 12 ||x\u2212Wh\u2212 b|| 2 + dTh )\n(2\u03c0)nd/2 \u220f j (1 + exp(dj))\n, (6)\nor,\nP\u03b8(x, h) = 1 (2\u03c0)nd/2 \u220f j (1 + exp(dj))\nexp\n( 1\n2 (x\u2212b)T (x\u2212b)\u2212xTWh+1 2 hTWTWh\u2212dTh\n) .\n(7)\nFor brevity, vector and matrix forms are used, W = {wij}, b = {bi}, d = {dj}. \u03b8 = {W, b, d} represent all the parameters.\nFor real-valued input data and binary latent variables, Gaussian-Bernoulli RBM defines the joint probability of visible and latent layer,\nPRBM(x, h)= 1\nZ exp ( \u22121 2 (x\u2212b)T (x\u2212b)+xTWh+dTh ) , (8)\nwhere Z is the partition function to make PRBM(x, h) a valid probability distribution. The input data is assumed to be normalized to have unit variance.\nComparing LRBN (Eq. 7) and RBM (Eq. 8), with the same dimensionality of the visible and latent layer, the two models have the same amount of parameters. However,\nthe directed model has a quadratic term hTWTWh = \u2211 i (\u2211 j wijhj )2 , which does\nnot exist in the joint distribution of RBM. This term explicitly captures the correlations among the latent variables h. It also explains why given the visible layer, the latent variables are dependent on each other."}, {"heading": "4 LRBN Inference", "text": "In this section, we introduce an efficient inference method for LRBN based on conditional pseudo-likelihood. Given a LRBN model with known parameters, the goal of inference is to compute the posterior probability of the latent variables given input data, i.e., computing P (h|x).\nIn this work, we are interested in the maximum a posteriori (MAP) inference, which is to find the configuration of latent variables that maximizes the posterior probability given observations,\nh\u2217 = argmax h P (h|x) . (9)\nThe MAP inference is motivated by the observation that from the data generating point of view, the variables in one latent layer take values according to the conditional probability given its upper layer. Therefore this configuration dominates all the others in explaining each data sample. In addition, the goal of feature learning is to learn a feature h that best explains x. In this regard, we only care about the most probable states of the latent variables given the observation.\nBecause of the dependencies among elements of h, direct computing P (h|x) is computationally intractable, in particular when the dimension of h is high. According to the chain rule, the posterior probability of h is,\nP (h|x) = \u220f j P (hj |h1, . . . , hj\u22121, x) , (10)\nThe pseudo-likelihood replaces the conditional likelihood by a more tractable objective, i.e., P (h|x) \u2248 \u220f j P (hj |h\u2212j , x) , (11)\nwhere h\u2212j = {h1, . . . , hj\u22121, hj+1, . . . , hnh} is the set of all latent variables except hj . In this approximation, we add conditioning over additional variables.\nThe conditional pseudo-likelihood can be factorized into local conditional probabilities, which can be estimated in parallel. To optimize over the pseudo-likelihood, one latent variable is updated by fixing all other variables,\nht+1j = argmax hj P (hj |x, ht\u2212j) , 1 \u2264 j \u2264 nh , (12)\nwhere t denotes the tth iteration.\nTheorem 1 The updating rule (Eq. 12) guarantees that the posterior probabilityP (h|x) will only increase or stay the same after each iteration.\nP (ht+1j , h t \u2212j |x) \u2265 P (ht|x) . (13)\nThe conditional probability of one latent variable by fixing all other variables is easy to compute, since it involves the computation of the joint distribution twice.\nP (hj |x, h\u2212j)= P (hj , x, h\u2212j)\u2211 h\u2032j P (h\u2032j , x, h\u2212j) . (14)\nIn general, computing the joint probability P (x, h) has complexity O(ndnh). If each latent variable is updated t times to get h\u2217, the overall complexity for the LRBN inference is O(tndn2h), which is much lower than the O(2\nnhndnh) complexity when computing P (h|x) directly.\nThe updating method can be seen as a coordinate ascent algorithm or the iterated conditional modes (ICM) as in inference of Markov random fields. Typically in MRF the number of neighbors for one node is limited. In the case of LRBN, one latent variable is related to all the other variables, due to the rich dependencies encoded in the structure. As discussed above, existing methods to address the inference intractability problem makes the posterior probability completely factorized, therefore sacrificing the dependencies among the latent variables. In contrast, through pseudo-likelihood, we can preserve the dependencies to certain degree.\nThe inference method requires an initialization for the hidden variables. Different initializations will end up with different local optimal points. To obtain consistent initialization, we drop the direction of the links, and treat the directed model as an undirected one. Therefore, the latent variables are independent of each other given the observations. Specifically, for binary input,\nP (hj = 1|x) = \u03c3( \u2211 i wijxi + dj) . (15)\nFor continuous input, based on Eq. 7, we drop the off-diagonal terms of matrix WTW for the sake of efficiency, resulting in a factorized distribution of the latent variables,\nP (hj = 1|x) = \u03c3( \u2211 i wijxi + dj \u2212 sj) , (16)\nwhere (s1, . . . , snh) = diag( 1 2W TW ). For the initialization, the dependency among the latent variables is ignored, and then through coordinate ascent, it is recovered by updating a subset of variables with others fixed."}, {"heading": "5 LRBN Learning", "text": "In this section, we introduce an efficient LRBN learning method based on the hard Expectation Maximization (EM) algorithm. The conventional EM algorithm is not an option here due to the intractability of computing posterior probability in the E-step. The hard version of EM algorithm has been explored in [22] for learning a deep Gaussian mixture model. This model has a deep structure in terms of linear transformations, but only has two layers of variables."}, {"heading": "5.1 Learning One Latent Layer", "text": "Consider the model P\u03b8(x, h) defined in section 3. The goal of parameter learning is to estimate the parameters \u03b8 = {w, b, d} given a set of data samples D={x(m)}Mm=1. The conventional maximum likelihood (ML) parameter estimation is to maximize the following objective function,\n\u03b8\u2217 = argmax \u03b8 \u2211 m log \u2211 h P\u03b8(x (m), h) . (17)\nThe second summation in Eq. 17 is intractable due to the exponentially many configurations of h. In this work, we employ a max-out estimation of the data log-probability, with the following objective function,\n\u03b8\u2217 = argmax \u03b8 \u2211 m logmax h P\u03b8(x (m), h) . (18)\nNote that the max-out approximation of the data likelihood is not equivalent to approximating P (h|x) with a delta function. A delta function must be avoided since it is also factorized, which defeats our motivation of preserving the dependency.\nWith objective function Eq. 18, the learning method becomes a hard version of the EM algorithm, which iteratively fills in the latent variables and update the parameters. In the E-step, h\u2217 = argmaxh P (x, h) is effectively estimated using the proposed inference method. In the M-step, the problem of parameter estimation is straightforward because now we are dealing with complete data.\nFor binary observations, the parameter learning can be decomposed into learning local CPD for each variable, by solving multiple logistic regression problems. The gradient of the parameters is,\n\u2202 logP (x, h)\n\u2202wij = hj (xi \u2212 P (xi = 1|h)) , (19)\n\u2202 logP (x, h)\n\u2202bi = xi \u2212 P (xi = 1|h) . (20)\nIn hybrid LRBN, the objective function is convex, and the maximization likelihood solution can be obtained by setting,\u2211\nm\n\u2202 logP (x(m), h(m))\n\u2202\u03b8 = 0 . (21)\nThe solution of parameters has a closed form,\nW = (\u2211 m (x(m) \u2212 b)(h(m))T )(\u2211 m h(m)(h(m))T )\u22121 . (22)\nIn case of large datasets, it is time consuming because all the training instances are used to compute the gradient. Stochastic gradient ascent algorithm can be used to address this issue. The true gradient is approximated by the gradient at a minibatch of training samples. As the algorithm sweeps through the entire training set, it performs the gradient updating for each training sample. Several passes is made over the training set until the algorithm converges. The learning method is summarized in Algorithm 1.\nAlgorithm 1 Unsupervised Parameter learning of an LRBN with one latent layer. Input training data X = {x(m)} Output parameters \u03b8 of an LRBN\n1: Initial parameters \u03b8; 2: Initialize the states h0 for all the latent variables using some feed-forward model\n(Section 4); 3: while parameters not converging, do 4: Random select a minibatch of data instances x \u2208 X 5: Update the corresponding h for x by maximizing the posterior probability, using\ncurrent parameters, h\u2217 = argmax\nh P\u03b8(x, h) . (23)\n6: Compute the gradients using Eq. 19 and 20. Update the parameters,\n\u03b8 = \u03b8 + \u03bbO\u03b8 logP (x, h \u2217) (24)\n7: end while"}, {"heading": "5.2 Learning Deep Layers", "text": "The learning method of a two-layer LRBN not only provides the parameter \u03b8, but also perform the MAP inference to obtain h\u2217=argmaxh P (h|x). If we denote the features as h1 and treat them as the input to another LRBN, the same learning procedure can be repeated to learn another layer of features h2. By doing this we stack another LRBN on top of the first one to build a deep model.\nIn general, let hl denote the variables in the lth latent layer (0 \u2264 l \u2264 L, h0 = x), and \u03b8l be the parameters involved between layer l and l+1.\nThe parameter \u03b8l\u2217 is estimated as,\n\u03b8l\u2217 = argmax \u03b8l \u2211 m logmax hl+1 P\u03b8(h l,(m), hl+1) , 1 \u2264 l \u2264 L . (25)\nTo optimize the objective function, we use the stochastic gradient ascent method, and replace the input X by hl in Algorithm 1. By performing the layer-wise learning procedure from the first latent layer to the Lth, we learn a deep model from bottom-up sequentially. Each time the MAP estimation of one latent layer is treated as input to the next two-layer model. For data instance x(m),\nhl,(m) = argmax hl P (hl|hl\u22121,(m)) , 1 \u2264 l \u2264 L , (26)\nwhere h0,(m) = x(m). The layer-wise pre-training procedure extracts different levels of features from the input data, and also provides an initial estimation of the parameters."}, {"heading": "5.3 Fine Tuning", "text": "The layer-wise training ignores the parameters of other layers when training a model for each layer from bottom-up. To improve the model performance globally, we em-\nploy a fine tuning procedure from top-down after the layer-wise pre-training phase. Depending on whether the labels are available or not, the fine tuning can be done in either supervised or unsupervised manner as discussed below."}, {"heading": "5.3.1 Unsupervised Fine Tuning", "text": "By extending Eq. 18, the objective function for learning with multiple latent layers is,\n\u03b8\u2217 = argmax \u03b8 \u2211 m logmax {hl} P\u03b8(x (m), h1, . . . , hL) . (27)\nGiven the states in layer l, the variables in layer l \u2212 1 is independent of the variables in layer l + 1. Therefore, the unsupervised fine-tuning is performed for every three consecutive layers. The variables in the middle layer is updated with its upper and lower layers fixed,\nhl\u2217 = argmax hl P (hl|hl\u22121, hl+1) , 1 \u2264 l \u2264 L\u2212 1 . (28)\nThe conditional probability P (hl|hl\u22121, hl+1) is also approximated by the conditional pseudo-likelihood,\nP (hl|hl\u22121, hl+1) \u2248 \u220f j P (hlj |hl\u2212j , hl\u22121, hl+1) . (29)\nMAP inference is performed by updating one variable with all others fixed,\nht+1j = argmax hj P (hlj |hl\u2212j , hl\u22121, hl+1) , 1 \u2264 j \u2264 nh . (30)\nTo be consistent with bottom-up training, the initialization of the latent variable hl always follows Eq. 15 and 16. With the updated layer hl, we are able to update the parameters \u03b8l\u22121 and \u03b8l through,\n\u03b8l\u22121\u2217 = argmax \u03b8 \u2211 m logP (hl\u22121,(m), hl,(m)) . (31)\nand \u03b8l\u2217 = argmax\n\u03b8 \u2211 m logP (hl,(m), hl+1,(m)) . (32)\nThis process alternates between parameters updating and latent states updating. Therefore, the information is able to propagate among different layers, and the overall quality of the model will increase. The fine tuning proceeds in a top-down manner starting from layers {L\u2212 2, L\u2212 1, L} and ending with layers {0, 1, 2}. The bottom-up pre-training and top-down fine-tuning procedures are performed iteratively until the parameters converge."}, {"heading": "5.3.2 Supervised Fine Tuning", "text": "The parameter of the model can be fine-tuned discriminatively if the label information is available. Define a set of target variables t = (t1, ..., tC), with C being the total number of classes. tc = 1 if a sample belongs to class c, and tk = 0,\u2200k 6= c. The supervised fine-tuning contains three steps. First, a layer-wise pre-training is performed with L\u22121 latent layers (excluding the top layer) using the method discussed in Section 5.2, obtaining \u03b8l and hl, 1\u2264 l\u2264L\u22121.\nSecond, the parameter \u03b8L for the top two layers is estimated as,\n\u03b8L\u22121\u2217 = argmax \u03b8 \u2211 m logP (hL\u22121,(m), t(m)) . (33)\nThis step works with complete data, which does not require inference because t is always observed. Thees two steps form the bottom-up pre-training procedure.\nThird, the top-down fine tuning starts with layers {L\u22122, L\u22121, L} with hL = t and ends with layers {0, 1, 2}. Latent states updating follows Eq. 28, and parameter updating follows Eq. 31 and 32."}, {"heading": "6 Experiments", "text": "In this section, we evaluate the performance of LRBN and compare against other methods on three binary datasets: MNIST, Caltech 101 Silhouettes and OCR letters. Binary datasets are chosen to compare with other models. The extension to real-value datasets is straightforward. The experiments will evaluate representation and reconstruction power of the proposed model."}, {"heading": "6.1 Experimental protocol", "text": "We trained the LRBN model using stochastic gradient ascent algorithm with learning rate 0.25. The size of the minibatches is set to 20. Two different structures are studied: one hidden layer with 200 variables, and two hidden layers with each layer containing 200 variables, consistent with the configurations in [5, 13]. For each dataset, we randomly selected 100 samples from the training set to form a validation set. The joint probability on the validation set is a criterion for early stopping.\nIn this section, we first evaluate the MAP configuration of the latent variables through reconstruction. Reconstruction is performed as follows: given a data vector x, perform a MAP inference to get h\u2217=argmaxh P (h|x). Then x\u0303=argmaxx P (x|h\u2217) is the reconstructed data. The reconstruction error |x\u0303 \u2212 x|2 can evaluate how well the model fits the data.\nThe second criterion is the widely used test data log-probability. Directly computing probability P (x) is intractable due to the exponentially many terms in the summation P (x) = \u2211 h P (x, h). In this work, we estimate the log-probability using the conservative sampling-based log-likelihood (CSL) method [1],\nlog P\u0302 (x) = logmeanh\u2208SP (x|h) , (34)\nwhere S is a set of samples h of the latent variables collected from a Markov chain. The expectation of the estimator is a lower bound on the true log-likelihood [1]. Because of the nature of directed models, samples can be collected from the ancestral sampling procedure. Specifically, the top layer is sampled from the prior probability P (hL), and the lower layers are sampled from the conditional probabilities P (hl\u22121|hl). One million samples are used to reach convergence of the estimation of log-probability, and the average of ten repetitions is reported.\nThe reconstruction error evaluates the quality of the most probable explanation of the latent variables given observations, while the data log-probability evaluates the overall quality of all configurations of latent variables. They are two complementary criteria for model evaluation.\nIn the experiment, we compare with published results if they are available. For reconstruction we implement the NVIL, RBM, DBN and DBM models following [4, 8, 13, 17] (denoted by (*) in the following tables). Similar log-likelihood achieved by our implementation indicates the correctness of the implementation."}, {"heading": "6.2 MNIST dataset", "text": "The first experiment is performed on the binary version of the MNIST dataset (thresholding at 0.5). The dataset consists of 70,000 handwritten digits with dimension 28\u00d728. It is partitioned into a training set of 60,000 images and a testing set of 10,000 images.\nThe average reconstruction errors of different learning models are reported in Table 1. The MAP inference of neural variational inference and learning (NVIL) [13] is through the inference network. For deep belief network (DBN) [19] and deep Boltzmann machine (DBM) [17], the posterior P (h|x) is already factorized, so that the inference is performed individually for each latent variable. The average reconstruction error of the proposed model is 4.56 pixels, which significantly outperforms the other competing methods. This is consistent with our objective function, indicating the most probable explanation contains most information in the input data, which is effectively captured in the proposed model. Some examples of the reconstruction are shown in Fig. 2.\nIn Table 2 we report the average log-probability on the test set. With the same dimensionality, LRBN outperforms variational Bayes [5], and is similar to that learned using NVIL [13]. Even though our objective function does not explicitly maximize the data likelihood, the learned model achieves comparable performance compared with state of the art learning methods, which indicates that the proposed method is also\neffective in capturing the distribution of the training data. In all algorithms, introducing a second hidden layer improves the performance. Our method achieves almost 8 nats improvement when additional latent layer is used. Some samples from the generative model are given in Fig. 3.\nThere is still a gap between the proposed learning method and DBN or DBM. One reason is that we do not have as many latent nodes as in DBN and DBM. Moreover, the max-out approximation of the data likelihood during learning drops all the nondominant configurations of the latent variables. Therefore it does not necessarily perform well on the task of likelihood comparison. However, it still achieves comparable or even better performance compared to other learning methods."}, {"heading": "6.3 Caltech 101 Silhouettes dataset", "text": "The second experiment is performed on the Caltech 101 Silhouettes dataset. The dataset contains 6364 training images and 2307 testing images. The reconstruction error is reported in Table 3. The proposed learning method outperforms all the competing methods by a large margin, indicating the effectiveness of the max-out approximation.\nThe test data log-probability is reported in Table 4. With the same dimensionality, the model learned by the proposed algorithm outperforms the one learned by variational Bayes [5], which is considered as one of the state of the art methods of training sigmoid belief networks. With one hidden layer of size 200, the improvement is 34 nats; with a second hidden layer of size 200, the improvement is 28 nats. Moreover, compared to an RBM with much more parameters, our model also achieves better performance, indicating the importance of the underlying dependency of the latent variables. Examples are shown in Fig. 4."}, {"heading": "6.4 OCR letters dataset", "text": "The last experiment is performed on the OCR letters dataset, which contains 42,152 training images and 10,000 testing images of English letters. The images have the dimensionality of 16\u00d78.\nThe reconstruction error is reported in Table 5. The proposed method shows superior performance compared to all the competing methods. The average reconstruction error on the test set is 5.95 pixels, which is at least 17 pixels better than the other methods.\nThe test data log-probability is reported in Table 6. Our model obtains a variational lower bound of -35.02, which outperforms the variational Bayes learning method, and is slightly worse than DBM [18], which has 100 times more parameters. Samples from the LRBN are shown in Fig. 5. We display the samples of letter \u2019g\u2019. For the same letter, the learned model is able to capture the different handwriting styles, while preserving the key information."}, {"heading": "7 Conclusion", "text": "In this work, we introduce a directed deep model based on the latent regression Bayesian network to explicitly capture the dependencies among the latent variables for data representation. We introduce an efficient inference method based on pseudo-likelihood and coordinate ascent. A hard EM learning method is proposed for efficient parameter learning. The proposed inference method solve the inference intractability, while preserving the dependencies among latent variables. We theoretically and empirically compare different models and learning methods. We point out that the latent variables in regression Bayesian network have strong dependencies, which can better explain the patterns in the input layer. Experiments on benchmark datasets shows the proposed model significantly outperforms the existing models in data reconstruction and achieves comparable performance for data representation."}], "references": [{"title": "Bounding the test log-likelihood of generative models", "author": ["Yoshua Bengio", "Li Yao", "Kyunghyun Cho"], "venue": "In International Conference on Learning Representations (Conference Track),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "On the statistical analysis of dirty pictures", "author": ["Julian Besag"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Enhanced gradient for training restricted boltzmann machines", "author": ["K Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning deep sigmoid belief networks with data augmentation", "author": ["Zhe Gan", "Ricardo Henao", "David Carlson", "Lawrence Carin"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "The em algorithm for mixtures of factor analyzers", "author": ["Zoubin Ghahramani", "Geoffrey E Hinton"], "venue": "Technical report, Technical Report CRG-TR-96-1,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Andriy Mnih", "Daan Wierstra"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "The\u201d wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Geoffrey E Hinton", "Peter Dayan", "Brendan J Frey", "Radford M Neal"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Probabilistic latent semantic analysis", "author": ["Thomas Hofmann"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["Radford M Neal"], "venue": "Artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "A probabilistic theory of deep learning", "author": ["Ankit B Patel", "Tan Nguyen", "Richard G Baraniuk"], "venue": "arXiv preprint arXiv:1504.00641,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Lawrence K Saul", "Tommi Jaakkola", "Michael I Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Deep mixtures of factor analysers", "author": ["Yichuan Tang", "Geoffrey E Hinton", "Ruslan Salakhutdinov"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Factoring variations in natural images with deep gaussian mixture models", "author": ["Aaron van den Oord", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "Different levels of latent variables capture features (or abstractions [15]) in a coarse-to-fine manner.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Compared with undirected models such as restricted Boltzmann machines (RBMs) and deep Boltzmann machines (DBMs) [17], directed generative models have their own advantages.", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "[8] introduced a complementary prior for the latent variables which makes the posterior fully factorized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Some recent efforts for learning generative models have focused on variational methods [12, 13, 16], by introducing another distribution to approximate the true posterior and maximize a variational lower bound of the data likelihood.", "startOffset": 87, "endOffset": 99}, {"referenceID": 12, "context": "Some recent efforts for learning generative models have focused on variational methods [12, 13, 16], by introducing another distribution to approximate the true posterior and maximize a variational lower bound of the data likelihood.", "startOffset": 87, "endOffset": 99}, {"referenceID": 15, "context": "Some recent efforts for learning generative models have focused on variational methods [12, 13, 16], by introducing another distribution to approximate the true posterior and maximize a variational lower bound of the data likelihood.", "startOffset": 87, "endOffset": 99}, {"referenceID": 1, "context": "It can also be seen as the application of iterated conditional modes (ICM) [2] to directed graphical models.", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "EM algorithm and its variants have been used for learning latent mixture of factor analyzers [6], probabilistic latent semantic indexing [10], probabilistic latent semantic analysis [11] and latent Dirichlet allocation (LDA) [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "EM algorithm and its variants have been used for learning latent mixture of factor analyzers [6], probabilistic latent semantic indexing [10], probabilistic latent semantic analysis [11] and latent Dirichlet allocation (LDA) [3].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "EM algorithm and its variants have been used for learning latent mixture of factor analyzers [6], probabilistic latent semantic indexing [10], probabilistic latent semantic analysis [11] and latent Dirichlet allocation (LDA) [3].", "startOffset": 182, "endOffset": 186}, {"referenceID": 2, "context": "EM algorithm and its variants have been used for learning latent mixture of factor analyzers [6], probabilistic latent semantic indexing [10], probabilistic latent semantic analysis [11] and latent Dirichlet allocation (LDA) [3].", "startOffset": 225, "endOffset": 228}, {"referenceID": 14, "context": "[15] make one latent variable connecting to a small patch of the input data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], known as the mean field theory for learning sigmoid belief networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] extended the mean field method, and proposed a Bayesian approach to learn deep sigmoid belief networks by introducing sparsity-encoraging priors on the model parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The wake-sleep algorithm [9] augments the multi-layer belief networks with feed-forward recognition networks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "Mnih and Gregor [13] introduced the neural variational inference and learning (NVIL) algorithm for sigmoid belief networks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Kingma and Welling [12] proposed the auto-encoding variational Bayes method for continuous latent variables, in which a reparameterization is employed to efficiently generate samples from the Gaussian distribution.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "[16] propose a stochastic backpropagation algorithm for learning deep generative models with continuous latent variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] augment the directed model with an encoder, which is also kind of inference network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed a complementary prior to ensure a factorized posterior, and proposed a fast learning algorithm for deep belief networks (DBNs), which is basically a hybrid network with a single undirected layer and several directed layers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "The case of continuous latent variables can be referred to as factor analyzers [6, 21] or deep latent Gaussian models [12, 16].", "startOffset": 79, "endOffset": 86}, {"referenceID": 20, "context": "The case of continuous latent variables can be referred to as factor analyzers [6, 21] or deep latent Gaussian models [12, 16].", "startOffset": 79, "endOffset": 86}, {"referenceID": 11, "context": "The case of continuous latent variables can be referred to as factor analyzers [6, 21] or deep latent Gaussian models [12, 16].", "startOffset": 118, "endOffset": 126}, {"referenceID": 15, "context": "The case of continuous latent variables can be referred to as factor analyzers [6, 21] or deep latent Gaussian models [12, 16].", "startOffset": 118, "endOffset": 126}, {"referenceID": 13, "context": "(3) In this case, the model becomes a sigmoid belief network (SBN) [14] with one latent layer.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "The hard version of EM algorithm has been explored in [22] for learning a deep Gaussian mixture model.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Two different structures are studied: one hidden layer with 200 variables, and two hidden layers with each layer containing 200 variables, consistent with the configurations in [5, 13].", "startOffset": 177, "endOffset": 184}, {"referenceID": 12, "context": "Two different structures are studied: one hidden layer with 200 variables, and two hidden layers with each layer containing 200 variables, consistent with the configurations in [5, 13].", "startOffset": 177, "endOffset": 184}, {"referenceID": 0, "context": "In this work, we estimate the log-probability using the conservative sampling-based log-likelihood (CSL) method [1],", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "The expectation of the estimator is a lower bound on the true log-likelihood [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "For reconstruction we implement the NVIL, RBM, DBN and DBM models following [4, 8, 13, 17] (denoted by (*) in the following tables).", "startOffset": 76, "endOffset": 90}, {"referenceID": 7, "context": "For reconstruction we implement the NVIL, RBM, DBN and DBM models following [4, 8, 13, 17] (denoted by (*) in the following tables).", "startOffset": 76, "endOffset": 90}, {"referenceID": 12, "context": "For reconstruction we implement the NVIL, RBM, DBN and DBM models following [4, 8, 13, 17] (denoted by (*) in the following tables).", "startOffset": 76, "endOffset": 90}, {"referenceID": 16, "context": "For reconstruction we implement the NVIL, RBM, DBN and DBM models following [4, 8, 13, 17] (denoted by (*) in the following tables).", "startOffset": 76, "endOffset": 90}, {"referenceID": 12, "context": "The MAP inference of neural variational inference and learning (NVIL) [13] is through the inference network.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "For deep belief network (DBN) [19] and deep Boltzmann machine (DBM) [17], the posterior P (h|x) is already factorized, so that the inference is performed individually for each latent variable.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "For deep belief network (DBN) [19] and deep Boltzmann machine (DBM) [17], the posterior P (h|x) is already factorized, so that the inference is performed individually for each latent variable.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Method DIM Recon Error NVIL* [13] 200 - 200 35.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "52 DBN* [19] 200 - 200 29.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "78 DBM* [17] 200 - 200 23.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "With the same dimensionality, LRBN outperforms variational Bayes [5], and is similar to that learned using NVIL [13].", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "With the same dimensionality, LRBN outperforms variational Bayes [5], and is similar to that learned using NVIL [13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "Method DIM 10k VB [5] 200 -116.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "91 VB [5] 200 - 200 -110.", "startOffset": 6, "endOffset": 9}, {"referenceID": 12, "context": "74 NVIL [13] 200 -113.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "1 NVIL [13] 200 - 200 -99.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "8 DBN [19] 500 - 2000 -86.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "22 DBM [17] 500 - 1000 -84.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "With the same dimensionality, the model learned by the proposed algorithm outperforms the one learned by variational Bayes [5], which is considered as one of the state of the art methods of training sigmoid belief networks.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": "Method DIM Recon Error NVIL* [13] 200 - 200 29.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "78 RBM* [4] 200 32.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "47 DBN* [19] 200 - 200 28.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "17 DBM* [17] 200 - 200 24.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "Method DIM Log-prob VB [5] 200 -136.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "84 VB [5] 200 - 200 -125.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "60 RBM [4] 4000 -107.", "startOffset": 7, "endOffset": 10}, {"referenceID": 18, "context": "78 DBN* [19] 200 - 200 -120.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "46 DBM* [17] 200 - 200 -118.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "02, which outperforms the variational Bayes learning method, and is slightly worse than DBM [18], which has 100 times more parameters.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "Method DIM Recon Error NVIL* [13] 200 - 200 14.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "79 RBM* [4] 200 16.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "83 DBN* [19] 200 - 200 12.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "47 DBM* [17] 200 - 200 11.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "Method DIM Log-prob VB [5] 200 -48.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "20 VB [5] 200 - 200 -47.", "startOffset": 6, "endOffset": 9}, {"referenceID": 18, "context": "84 DBN* [19] 200 - 200 40.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "75 DBM [18] 2000 - 2000 -34.", "startOffset": 7, "endOffset": 11}], "year": 2015, "abstractText": "Deep directed generative models have attracted much attention recently due to their expressive representation power and the ability of ancestral sampling. One major difficulty of learning directed models with many latent variables is the intractable inference. To address this problem, most existing algorithms make assumptions to render the latent variables independent of each other, either by designing specific priors, or by approximating the true posterior using a factorized distribution. We believe the correlations among latent variables are crucial for faithful data representation. Driven by this idea, we propose an inference method based on the conditional pseudo-likelihood that preserves the dependencies among the latent variables. For learning, we propose to employ the hard Expectation Maximization (EM) algorithm, which avoids the intractability of the traditional EM by max-out instead of sum-out to compute the data likelihood. Qualitative and quantitative evaluations of our model against state of the art deep models on benchmark datasets demonstrate the effectiveness of the proposed algorithm in data representation and reconstruction.", "creator": "LaTeX with hyperref package"}}}