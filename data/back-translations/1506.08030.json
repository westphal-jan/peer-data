{"id": "1506.08030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2015", "title": "Dynamic Bayesian Ontology Languages", "abstract": "Most of these formalities, however, assume that the likely structure of knowledge remains static over time. We present a general approach to expanding ontological language to include the time-evolving uncertainty represented by a dynamic Bayesian network. We show how original language reasoning and dynamic Bayesian conclusions can be used for effective thinking within our framework.", "histories": [["v1", "Fri, 26 Jun 2015 11:32:46 GMT  (113kb,D)", "http://arxiv.org/abs/1506.08030v1", "Fifth International Workshop on Statistical Relational AI (StarAI'2015)"]], "COMMENTS": "Fifth International Workshop on Statistical Relational AI (StarAI'2015)", "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["\\ ismail \\ ilkan ceylan", "rafael pe\\~naloza"], "accepted": false, "id": "1506.08030"}, "pdf": {"name": "1506.08030.pdf", "metadata": {"source": "CRF", "title": "Dynamic Bayesian Ontology Languages", "authors": ["\u0130smail \u0130lkan Ceylan", "Rafael Pe\u00f1aloza"], "emails": ["ceylan@tcs.inf.tu-dresden.de", "rafael.penaloza@unibz.it"], "sections": [{"heading": "Introduction", "text": "Description Logics (DLs) (Baader et al. 2007) are a wellknown family of knowledge representation formalisms that have been successfully employed for encoding the knowledge of many application domains. In DLs, knowledge is represented through a finite set of axioms, usually called an ontology or knowledge base. In essence, these axioms are atomic pieces of knowledge that provide explicit information of the domain. When mixed together in an ontology, these axioms may imply some additional knowledge that is not explicitly encoded. Reasoning is the act of making this implicit knowledge explicit through an entailment relation.\nSome of the largest and best-maintained DL ontologies represent knowledge from the bio-medical domains. For instance, the NCBO Bioportal1 contains 420 ontologies of various sizes. In the bio-medical fields it is very common to have only uncertain knowledge. The certainty that an expert has on an atomic piece of knowledge may have arisen from a statistical test, or from possibly imprecise measurements, for example. It thus becomes relevant to extend DLs to represent and reason with uncertainty.\nThe need for probabilistic extensions of DLs has been observed for over two decades already. To cover it, many different formalisms have been introduced (Jaeger 1994; Lukasiewicz and Straccia 2008; Lutz and Schro\u0308der 2010; Klinov and Parsia 2011). The differences in these logics range from the underlying classical DL used, to the semantics, to the assumptions made on the probabilistic component. One of the main issues that these logics need to handle\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1http://bioportal.bioontology.org/\nis the representation of joint probabilities, in particular when the different axioms are not required to be probabilistically independent. A recent approach solves this issue by dividing the ontology into contexts, which intuitively represent axioms that must appear together. The probabilistic knowledge is expressed through a Bayesian network that encodes the joint probability distribution of these contexts. Although originally developed as an extension of the DL EL (Ceylan and Pen\u0303aloza 2014b), the framework has been extended to arbitrary ontology languages with a monotone entailment relation (Ceylan and Pen\u0303aloza 2014a).\nOne common feature of the probabilistic extensions of DLs existing in the literature is that they consider the probability distribution to be static. For many applications, this assumption does not hold: the probability of a person to have gray hair increases as time passes, as does the probability of a computer component to fail. To the best of our knowledge, there is so far no extension of DLs that can handle evolving probabilities effectively.\nIn this paper, we describe a general approach for extending ontology languages to handle evolving probabilities. By extension, our method covers all DLs, but is not limited to them. The main idea is to adapt the formalism from (Ceylan and Pen\u0303aloza 2014a) to use dynamic Bayesian networks (DBNs) (Murphy 2002) as the underlying uncertainty structure to compactly encode evolving probability distributions.\nGiven an arbitrary ontology language L, we define its dynamic Bayesian extension DBL. We show that reasoning in DBL can be seamlessly divided into the probabilistic computation over the DBN, and the logical component with its underlying entailment relation. In order to reduce the number of entailment checks, we compile a so-called context formula, which encodes all contexts in which a given consequence holds.\nRelated to our work are relational BNs (Jaeger 1997) and their extensions. In contrast to relational BNs, we provide a tight coupling between the logical formalism and the DBN, which allows us to describe evolving probabilities while keeping the intuitive representations of each individual component. Additionally, restricting the logical formalism to specific ontology languages provides an opportunity for finding effective reasoning algorithms.\nar X\niv :1\n50 6.\n08 03\n0v 1\n[ cs\n.A I]\n2 6\nJu n\n20 15"}, {"heading": "Bayesian Ontology Languages", "text": "To remain as general as possible, we do not fix a specific logic, but consider an arbitrary ontology languageL consisting of two infinite sets A and C of axioms and consequences, respectively, and a class O \u2286 \u2118fin(A) of finite sets of axioms, called ontologies, such that ifO \u2208 O, thenO\u2032 \u2208 O for all O\u2032 \u2286 O. The language L is associated to a class I of interpretations and an entailment relation |= \u2286 I\u00d7(A\u222aC). An interpretation I \u2208 I is a model of the ontology O (I |= O) if I |= \u03b1 for all \u03b1 \u2208 O. O entails c \u2208 C (O |= c) if every model of O entails c. Notice that the entailment relation is monotonic; i.e., if O |= c and O \u2286 O\u2032 \u2208 O, then O\u2032 |= c. Any standard description logic (DL) (Baader et al. 2007) is an ontology language of this kind; consequences in these languages are e.g. concept unsatisfiability, concept subsumption, or query entailment. However, many other ontology languages of varying expressivity and complexity properties exist. For the rest of this paper, L is an arbitrary but fixed ontology language, with axioms A, ontologies O, consequences C, and interpretations I.\nAs an example language we use the DL EL (Baader et al. 2005), which we briefly introduce here. Given two disjoint sets NC and NR, EL concepts are built by the grammar rule C ::= A | > | C u C | \u2203r.C where A \u2208 NC and r \u2208 NR. EL axioms and consequences are expressions of the form C v D, where C and D are concepts. An interpretation is a pair (\u2206I , \u00b7I) where \u2206I is a non-empty set and \u00b7I maps every A \u2208 NC to AI \u2286 \u2206I and every r \u2208 NR to rI \u2286 \u2206I \u00d7 \u2206I . This function is extended to concepts by >I := \u2206I , (C uD)I := CI \u2229DI , and (\u2203r.C)I := {d | \u2203e \u2208 CI .(d, e) \u2208 rI}. This interpretation entails the axiom (or consequence) C v D iff CI \u2286 DI .\nThe Bayesian ontology language BL extends L by associating each axiom in an ontology with a context, which intuitively describes the situation in which the axiom is required to hold. The knowledge of which context applies is uncertain, and expressed through a Bayesian network (Ceylan and Pen\u0303aloza 2014a).\nBriefly, a Bayesian network (Darwiche 2009) is a pair B = (G,\u03a6), where G = (V,E) is a finite directed acyclic graph (DAG) whose nodes represent Boolean random variables, and \u03a6 contains, for every x \u2208 V , a conditional probability distribution PB(x | \u03c0(x)) of x given its parents \u03c0(x). Every variable x \u2208 V is conditionally independent of its non-descendants given its parents. Thus, the BN B defines a unique joint probability distribution (JPD) over V :\nPB(V ) = \u220f x\u2208V PB(x | \u03c0(x)).\nLet V be a finite set of variables. A V -context is a consistent set of literals over V . A V -axiom is an expression of the form \u3008\u03b1 : \u03ba\u3009 where \u03b1 \u2208 A is an axiom and \u03ba is a V -context. A V -ontology is a finite set O of V -axioms, such that {\u03b1 | \u3008\u03b1 : \u03ba\u3009 \u2208 O} \u2208 O. A BL knowledge base (KB) over V is a pair K = (O,B) where B is a BN over V and O is a V -ontology.\nWe briefly illustrate these notions over the language BEL, an extension of the DL EL, in the following Example.\nExample 1. Consider the BEL KB K1 = (B1,O1) where O1={ \u3008Comp v \u2203use.Mem u \u2203use.CPU : \u2205\u3009 ,\n\u3008\u2203use.FailMem v FailComp : {x}\u3009 , \u3008\u2203use.FailCPU v FailComp : {x}\u3009 , \u3008\u2203use.FailMem u \u2203use.FailCPU v FailComp:{\u00acx}\u3009 , \u3008Mem v FailMem : {y}\u3009 , \u3008CPU v FailCPU : {z}\u3009},\nand B1 is the BN shown in Figure 1. This KB represents a computer failure scenario, where x stands for a critical situation, y represents the memory failing, and z the CPU failing.\nThe contextual semantics is defined by extending interpretations to evaluate also the variables from V . A V -interpretation is a pair I = (I,V I) where I \u2208 I and V I is a propositional interpretation over the variables V . The V -interpretation I = (I,V I) is a model of \u3008\u03b1 : \u03ba\u3009 (I |= \u3008\u03b1 : \u03ba\u3009), where \u03b1 \u2208 A, iff (V I 6|=p \u03ba) 2 or (I |= \u03b1).\nIt is a model of the V -ontology O iff it is a model of all the V -axioms in O. It entails c \u2208 C if I |= c. The intuition behind this semantics is that an axiom is evaluated to true by all models provided it is in the right context.\nGiven a V -ontology O, every propositional interpretation, or world, W on V defines an ontology OW := {\u03b1 | \u3008\u03b1 : \u03ba\u3009 \u2208 O,W |=p \u03ba}. Consider the KB K1 provided in Example 1: The worldW = {x,\u00acy, z} defines the ontology\nOW={Comp v \u2203use.Mem u \u2203use.CPU, \u2203use.FailMem v FailComp, \u2203use.FailCPU v FailComp, CPU v FailCPU}.\nIntuitively, a contextual ontology is a compact representation of exponentially many ontologies from L; one for each worldW . The uncertainty in BL is expressed by the BN B, which is interpreted using multiple world semantics. Definition 2 (probabilistic interpretation). A probabilistic interpretation is a pair P = (I, PI), where I is a set of V -interpretations and PI is a probability distribution over I such that PI(I) > 0 only for finitely many interpretations I \u2208 I. It is a model of the V -ontology O if every I \u2208 I is a model of O. P is consistent with the BN B if for every valuationW of the variables in V it holds that\u2211\nI\u2208I,V I=W PI(I) = PB(W).\n2We use |=p to distinguish propositional entailment from |=.\nThe probabilistic interpretation P is a model of the KB (B,O) iff it is a model of O and consistent with B.\nThe fundamental reasoning task in BL, probabilistic entailment, consists in finding the probability of observing a consequence c; that is, the probability of being at a context where c holds.\nDefinition 3 (probabilistic entailment). Let c \u2208 C, and K a BL KB. The probability of c w.r.t. the probabilistic interpretation P = (I, PI) is PP(c) := \u2211 (I,W)\u2208I,I|=c PI(I,W). The probability of c w.r.t. K is PK(c) := infP|=K PP(c). It has been shown that to compute the conditional probability of a consequence c, it suffices to test, for each world W , whether OW entails c (Ceylan and Pen\u0303aloza 2014b). Proposition 4. Let K = (B,O) be a BL KB and c \u2208 C. Then PK(c) = \u2211 OW |=c PB(W).\nThis means that reasoning in BL can be reduced to exponentially many entailment tests in the classical language L. For some logics, this exponential enumeration of worlds can be avoided (Ceylan and Pen\u0303aloza 2014c). However, this depends on the properties of the ontological language and its entailment relation, and cannot be guaranteed for arbitrary languages.\nAnother relevant problem is to compute the probability of a consequence given some partial information about the context. Given a context \u03ba, the conditional probability PK(c | \u03ba) is defined via the rule PK(c, \u03ba) = PK(c | \u03ba)PB(\u03ba), where\nPK(c, \u03ba) = \u2211\nOW |=c,W|=p\u03ba PB(W).\nFor simplicity, in the rest of this paper we consider only unconditional consequences. However, it should be noted that all results can be transferred to the conditional case.\nExample 5. Consider again the KB K1 = (B1,O1) from Example 1 and the consequence Comp v FailComp. We are interested in finding the probability of the computer to fail, i.e. PK1(Comp v FailComp). This can be computed by enumerating all worldsW for which OW |= Comp v FailComp, which yields the probability 0.238.\nAs seen, it is possible to extend any ontological language to allow for probabilistic reasoning based on a Bayesian network. We now further extend this formalism to be able to cope with controlled updates of the probabilities over time."}, {"heading": "Dynamic Bayesian Ontology Languages", "text": "WithBL, one is able to represent and reason about the uncertainty of the current context, and the consequences that follow from it. In that setting, the joint probability distribution of the contexts, expressed by the BN, is known and fixed. In some applications, see especially (Sadilek and Kautz 2010) this probability distribution may change over time. For example, as the components of a computer age, their probability of failing increases. The new probability depends on how likely it was for the component to fail previously, and the ageing factors to which it is exposed. We now extend BL to\nhandle these cases, by considering dynamic BNs as the underlying formalism for managing uncertainty over contexts.\nDynamic BNs (DBNs) (Dean and Kanazawa 1989; Murphy 2002) extend BNs to provide a compact representation of evolving joint probability distributions for a fixed set of random variables. The update of the JPD is expressed through a two-slice BN, which expresses the probabilities at the next point in time, given the current context. Definition 6 (DBN). Let V be a finite set of Boolean random variables. A two-slice BN (TBN) over V is a pair (G,\u03a6), where G = (V \u222a V \u2032, E) is a DAG containing no edges between elements of V , V \u2032 = {x\u2032 | x \u2208 V }, and \u03a6 contains, for every x\u2032 \u2208 V \u2032 a conditional probability distribution P (x\u2032 | \u03c0(x\u2032)) of x\u2032 given its parents \u03c0(x\u2032). A dynamic Bayesian network (DBN) over V is a pair D = (B1,B\u2192) where B1 is a BN over V , and B\u2192 is a TBN over V .\nA TBN over V = {x, y, z} is depicted in Figure 2. The set of nodes of the graph can be thought of as containing two disjoint copies of the random variables in V . Then, the probability distribution at time t+ 1 depends on the distribution at time t. In the following we will use Vt and xt for x \u2208 V , to denote the variables in V at time t.\nAs standard in BNs, the graph structure of a TBN encodes the conditional dependencies among the nodes: every node is independent of all its non-descendants given its parents. Thus, for a TBN B, the conditional probability distribution at time t+ 1 given time t is\nPB(Vt+1 | Vt) = \u220f x\u2032\u2208V \u2032 PB(x \u2032 | \u03c0(x\u2032)).\nWe further assume the Markov property: the probability of the future state is independent from the past, given the present state.\nIn addition to the TBN, a DBN contains a BN B1 that encodes the JPD of V at the beginning of the evolution. Thus, the DBND = (B1,B\u2192) defines, for every t \u2265 1, the unique probability distribution\nPB(Vt) = PB1(V1) t\u220f i=2 \u220f x\u2208V PB\u2192(xi | \u03c0(xi)).\nIntuitively, the distribution at time t is defined by unraveling the DBN starting from B1, using the two-slice structure of B\u2192 until t copies of V have been created. This produces a\nnew BN B1:t encoding the distribution over time of the different variables. Figure 3 depicts the unraveling to t = 3 of the DBN (B1,B\u2192) where B1 and B\u2192 are the networks depicted in Figures 1 and 2, respectively. The conditional probability tables of each node given its parents (not depicted) are those of B1 for the nodes in V1, and of B\u2192 for nodes in V2 \u222a V3. Notice that B1:t has t copies of each random variable in V . For a given t \u2265 1, we call Bt the BN obtained from the unraveling B1:t of the DBN to time t, and eliminating all variables not in Vt. In particular, we have that PBt(V ) = PB1:t(Vt).\nThe dynamic Bayesian ontology language DBL is very similar to BL, except that the probability distribution of the contexts evolves accordingly to a DBN.\nDefinition 7 (DBL KB). A DBL knowledge base (KB) is a pairK = (D,O) whereD = (B1,B\u2192) is a DBN over V and O is a V -ontology. Let K = (D,O) be a DBL KB over V . A timed probabilistic interpretation is an infinite sequence P = (Pt)t\u22651 of probabilistic interpretations. P is a model of K if for every t, Pt is a model of the BL KB (Bt,O).\nIn a nutshell, a DBN can be thought of as a compact representation of an infinite sequence of BNs B1,B2, . . . over V . Following this idea, a DBL KB expresses an infinite sequence of BL KBs, where the ontological component remains unchanged, and only the probability distribution of the contexts evolves over time. A timed probabilistic interpretation P simply interprets each of these BL KBs, at the corresponding point in time. To be a model of a DBL KB, P must then be a model of all the associated BL KBs.\nBefore describing the reasoning tasks forDBL and methods for solving them, we show how the computation of all the contexts that entail a consequence can be reduced to the enumeration of the worlds satisfying a propositional formula."}, {"heading": "Compiling Contextual Knowledge", "text": "From Proposition 4, we see that reasoning in BL can be reduced to checking, for every world W , whether OW |= c. This reduces probabilistic reasoning to a sequence of standard entailment tests over the original languageL. However, each of these entailments might be very expensive. For example, in the very expressive DL SHOIQ, deciding an entailment is already NEXPTIME-hard (Tobies 2000). Rather than repeating this reasoning step for every world, it makes sense to try to identify the relevant worlds a priori. We do this through the computation of a context formula.\nDefinition 8 (context formula). Let O be a V -ontology, and c \u2208 C. A context formula for c w.r.t. O is a propositional formula \u03c6 such that for every interpretationW of the variables in V , it holds that OW |= c iffW |=p \u03c6.\nThe idea behind this formula is that, for finding whether OW |= c, it suffices to check whether the valuationW satisfies \u03c6. This test requires only linear time on the length of the context formula. The context formula can be seen as a generalization of the pinpointing formula (Baader and Pen\u0303aloza 2010b) and the boundary (Baader et al. 2012), defined originally for classical ontology languages.\nExample 9. Consider again the V -ontology O1 from Example 1. The formula \u03c61 := (x \u2227 (y \u2228 z)) \u2228 (\u00acx \u2227 y \u2227 z) is a context formula for Comp v FailComp w.r.t. O1. In fact, the valuation {x,\u00acy, z} satisfies this formula.\nClearly, computing the context formula must be at least as hard as deciding an entailment in L: if we label every axiom in a classical L-ontology O with the same propositional variable x, then the boundary formula of c w.r.t. this {x}-ontology is x iff O |= c. On the other hand, the algorithm used for deciding the entailment relation can usually be modified to compute the context formula. Using arguments similar to those developed for axiom pinpointing (Baader and Pen\u0303aloza 2010a; Kalyanpur et al. 2007), it can be shown that for most description logics, computing the context formula is not harder, in terms of computational complexity, than standard reasoning. In particular this holds for any arbitrary ontology language whose entailment relation is EXPTIME-hard. This formula can also be compiled into a more efficient data structure like binary decision diagrams (Lee 1959). Intuitively, this means that we can compute this formula using the same amount of resources needed for only one entailment test, and then use it for verifying whether the sub-ontology defined by a world entails the consequence in an efficient way.\nReasoning in DBL Rather than merely computing the probability of currently observing a consequence, we are interested in computing the probability of a consequence to follow after some fixed number of time steps t.\nDefinition 10 (probabilistic entailment with time). Let K = (D,O) be a DBL KB and c \u2208 C. Given a timed interpretation P and t \u2265 1, the probability of c at time t w.r.t. P is PP(c[t]) := PPt(c). The probability of c at time t w.r.t. K is PK(c[t]) := infP|=K PP(c[t]).\nWe show that probabilistic entailment over a fixed time bound can be reduced to probabilistic entailment defined for BOLs by unravelling the DBN.\nLemma 11. Let K = (D,O) be a DBL KB, c \u2208 C, and t \u2265 1. Then the probability of c at time t w.r.t. K is given by\nPK(c[t]) = \u2211\nOW |=c PBt(W)\nProof. (Sketch) A timed model P of K is a sequence of probabilistic interpretations P1,P2, . . ., where each Pi is a\nmodel of the BL KB Ki := (Bi,O). We use this fact to show that\nPK(c[t]) = inf P|=K PP(c[t]) = inf P|=K PPt(c) (1)\n= inf Pt|=Kt PPt(c) = PKt(c) (2) = \u2211\nOW |=c PBt(W), (3)\nwhere (1) follows from Definition 10, (2) holds by definition, and (3) follows from Proposition 4.\nLemma 11 provides a method for computing the probability of an entailment at a fixed time t. One can first generate the BN Bt, and then compute the probability w.r.t. Bt of all the worlds that entail c. Moreover, using a context formula we can compile away the ontology and reduce reasoning to standard inferences in BNs, only.\nTheorem 12. Let K = (D,O) be a DBL KB, c \u2208 C, \u03c6 a context formula for c w.r.t. O, and t \u2265 1. Then the probability of c at time t w.r.t. K is given by PK(c[t]) = PBt(\u03c6).\nProof. By Lemma 11 and the definition of a context formula, we have\nPK(c[t]) = \u2211\nOW |=c PBt(W) = \u2211 W|=p\u03c6 PBt(W) = PBt(\u03c6),\nwhich proves the result.\nThis means that one can first compute a context formula for c and then do probabilistic inferences over the DBN to detect the probability of satisfying \u03c6 at time t. For this, we can exploit any existing DBN inference method. One option is to do variable elimination over the t-step unraveled DBN B1:t, to compute Bt. Assuming that t is fixed, it suffices to make 2|V | inferences (one for each world) over Bt and the same number of propositional entailment tests over the context formula. If entailment in L is already exponential, then computing the probability of c at time t is as hard as deciding entailments.\nThe previous argument only works assuming a fixed time point t. Since it depends heavily on computing Bt (e.g., via variable elimination), it does not scale well as t increases. Other methods have been proposed for exploiting the recursive structure of the DBN. For instance, one can use the algorithm described in (Vlasselaer et al. 2014) that provides linear scalability over time. The main idea is to compile the structure into an arithmetic circuit (Darwiche 2009) and use forward and backward message passing (Murphy 2002).\nWhile computing the probability of a consequence at a fixed point in time t is a relevant task, it is usually more important to know whether the consequence can be observed within a given time limit. In our computer example, we would be interested in finding the probability of the system failing within, say, the following twenty steps.\nAbusing of the notation, we use the expression\u223c c, c \u2208 C, to denote that the consequence c does not hold; i.e., I |= \u223c c iff I 6|= c. Thus, for example, PK(\u223c c[t]) is the probability\nof c not holding at time t. To find the probability of observing c in the first t time steps, one can alternatively compute the probability of not observing c in any of those steps. Formally, for a timed interpretation P and t \u2208 N, we define\nPP(c[1 : t]) := 1\u2212 PP(\u223c c[1], . . . ,\u223c c[t]). Definition 13 (time bounded probabilistic entailment). The probability of observing c in at most t steps w.r.t. the DBL KB K is PK(c[1 : t]) := infP|=K PP(c[1 : t]).\nJust as before, given a constant t \u2265 1, it is possible to compute PK(c[1 : t]) by looking at the t-step unraveling of D. More precisely, to compute PP(\u223c c[1], . . . ,\u223c c[t]), it suffices to look at all the valuationsW of \u22c3ti=1 Vi such that for all i, 1 \u2264 i \u2264 t, it holds that OW(i) |= \u223c c. These valuations correspond to an evolution of the system where the consequence c is not observed in the first t steps. The probability of these valuations w.r.t. B1:t then yields the probability of not observing this consequence. We thus get the following result. Theorem 14. Let K = (D,O) be a DBL KB, c \u2208 C, \u03c6 a context formula for c w.r.t. O, and t \u2265 1. Then PK(c[1 : t]) = \u2211 W|\u2203i.W(i)|=p\u03c6 PB1:t(W).\nProof (Sketch). Using the pithy interpretations of the crisp ontologies OW(i), we can build a timed interpretation P0 such that PP0(c[1 : t]) = \u2211 W|\u2203i.W(i)|=p\u03c6 PB1:t(W), in a way similar to Theorem 12 of (Ceylan and Pen\u0303aloza 2014b). The existence of another timed interpretation P such that PP(c[1 : t]) < PP0(c[1 : t]) contradicts the properties of the pithy interpretations. Thus, we obtain that PP0(c[1 : t]) = infP|=K PP(c[1 : t]) = PK(c[1 : t]).\nThis means that the probability of observing a consequence within a fixed time-bound t can be computed by simply computing the context formula and then performing probabilistic a posteriori computations over the unraveled BN. In our running example, the probability of observing a computer failure in the next 20 steps is simply\nPK(Comp v FailComp[1 : 20]) = \u2211\nW|\u2203i.W(i)|=p\u03c6 PBi(\u03c6).\nThus, the computational complexity of reasoning is not affected by introducing the dynamic evolution of the BN, as long as the time bound is constant. Notice, however, that the number of possible valuations grows exponentially on the time bound t. Thus, for large intervals, this approach becomes unfeasible.\nBy extending the time limit indefinitely, we can also find the probability of eventually observing the consequence c (e.g., the probability of the system ever failing). The probability of eventually observing c w.r.t. K is given by PK(c[\u221e]) := limt\u2192\u221e PK(c[1 : t]). Notice that PK(c[1 : t]) is monotonic on t and bounded by 1; hence PK(c[\u221e]) is well defined.\nObserve that Theorem 14 cannot be used to compute the probability of eventually observing c since one cannot necessarily predict the changes in probabilities of finding worlds that entail the consequence c. Rather than considering these\nincreasingly large BNs separately, we can exploit methods developed for probability distributions that evolve over time. This will also allow us to extract more information from DBL KBs.\nIt is easy to see that every TBN defines a timehomogeneous Markov chain over a finite state space. More precisely, if B is a TBN over V , then MB is the Markov chain, where every valuation W of the variables in V is a state and the transition probability distribution given the current stateW is described by the BN obtained from addingW as evidence to the first slice of B. For example, the TBN B\u2192 from Figure 2 yields the conditional probability distribution given that {x, y, z} was observed at time t depicted in Figure 4. From this, we can derive the probability of observing {x, y, z} at time t + 1 given that it was observed at time t, which is P ({x, y, z}t+1 | {x, y, z}t) = 0.252.\nWe extend the notions from Markov chains to TBNs in the obvious way. In particular, the TBN B is irreducible if for every two worlds V,W , the probability of eventually reaching W given V is greater than 0. It is aperiodic if for every world W there is an nW such that for all n \u2265 nW , it holds that P (Wn | W) > 0. A distribution PW over the worlds is stationary if \u2211 W P (V | W)PW (W) = PW (V) holds for every world V . It follows immediately that if B is irreducible and aperiodic, then it has a unique stationary distribution (Harris 1956).\nGiven a TBN B over V , let now \u2206B be the set of all stationary distributions over the worlds of V . For a worldW , define \u03b4B(W) := minP\u2208\u2206B P (W) to be the smallest probability assigned by any stationary distribution of B toW . If \u03b4B(W) > 0, then we know that, regardless of the initial distribution, in the limit we will always be able to observe the worldW with a constant positive probability. In particular, this means that the probability of eventually observing W equals 1. Notice moreover that this results is independent of the initial distribution used.\nWe can take this idea one step forward, and consider sets of worlds. For a propositional formula \u03c6, let\n\u03b4B(\u03c6) := min P\u2208\u2206B \u2211 W|=p\u03c6 P (W).\nIn other words, \u03b4B(\u03c6) expresses the minimum probability of satisfying \u03c6 in any stationary distribution of B. From the arguments above, we obtain the following theorem.\nTheorem 15. Let K = (D,O) be a DBL KB over V with"}, {"heading": "D = (B1,B\u2192), c \u2208 C, and \u03c6 a context formula for c w.r.t.", "text": "O. If \u03b4B\u2192(\u03c6) > 0, then PK(c[\u221e]) = 1.\nIn particular, if B\u2192 is irreducible and aperiodic, \u2206B contains only one stationary distribution, which simplifies the computation of the function \u03b4. Unfortunately, such a simple characterization of PK(c[\u221e]) cannot be given when \u03b4B\u2192(\u03c6) = 0. In fact, in this case the result may depend strongly on the initial distribution. Example 16. Let V = {x}, O2 = {\u3008A v B : {x}\u3009}, and consider the TBN B\u2032\u2192 over V defined by P (x\u2032 | x) = 1 and P (x\u2032 | \u00acx) = 0. It is easy to see that any distribution over the valuations of V is stationary. For every initial distribution B, if K = (D,O2) where D = (B,B\u2032\u2192), then PK(A v B[\u221e]) = PB(x).\nSo far, our reasoning services have focused on predicting the outcome at future time steps, given the current knowledge of the system. Based on our model of evolving probabilities, the distribution at any time t + 1 depends only on time t, if it is known. However, for many applications it makes sense to consider evidence that is observed throughout several time steps. For instance, in our computer failure scenario, the DBN B\u2192 ensures that, if at some point a critical situation is observed (x is true), then the probability of observing a memory or CPU failure in the next step is higher. That is, the evolution of the probability distribution is affected by the observed value of the variable x.\nSuppose that we have observed over the first t time steps that no critical situation has occurred, and we want to know the probability of a computer failure. Formally, let E be a consistent set of literals over \u22c3t i=1 Vi. We want to compute the probability PK(c[t] | E) of observing c at time t given the evidence E. This is just a special case of bounded probabilistic entailment, where the worlds are not only restricted w.r.t. the context formula but also w.r.t. the evidence E.\nThe efficiency of this approach depends strongly on the time bound t, but also on the structure of the TBN B\u2192. Recall that the complexity of reasoning in a BN depends on the tree-width of its underlying DAG (Pan et al. 1998). The unraveling of B\u2192 produces a new DAG whose tree-width might increase with each unraveling step, thus impacting the reasoning methods negatively."}, {"heading": "Conclusions", "text": "We have introduced a general approach for extending ontology languages to handle time-evolving probabilities with the help of a DBN. Our framework can be instantiated to any language with a monotonic entailment relation including, but not limited to, all the members of the description logic family of knowledge representation formalisms.\nOur approach extends on ideas originally introduced for static probabilistic reasoning. The essence of the method is to divide an ontology into different contexts, which are identified by a consistent set of propositional variables from a previously chosen finite set of variables V . The probabilistic knowledge is expressed through a probability distribution over the valuations of V which is encoded by a DBN.\nInterestingly, our formalism allows for reasoning methods that exploit the properties of both, the ontological, and\nthe probabilistic components. From the ontological point of view, we can use suplemental reasoning to produce a context formula that encodes all the possible worlds from which a wanted consequence can be derived. We can then use standard DBN methods to compute the probability of satisfying this formula.\nThis work represents first steps towards the development of a formalism combining well-known ontology languages with time-evolving probabilities. First of all, we have introduced only the most fundamental reasoning tasks. It is possible to think of many other problems like finding the most plausible explanation for an observed event, or computing the expected time until a consequence is derived, among many others.\nFinally, the current methods developed for handling DBNs, although effective, are not adequate for our problems. To find out the probability of satisfying the context formula \u03c6, we need compute the probability of each of the valuations that satisfy \u03c6 at different points in time. Even using methods that exploit the structure of the DBN directly, the information of the context formula is not considered. Additionally, with the most efficient methods to-date, it is unclear how to handle the evidence over time effectively. Dealing with these, and other related problems, will be the main focus of our future work."}, {"heading": "Acknowledgments", "text": "I\u0307smail I\u0307lkan Ceylan is supported by DFG within the Research Training Group \u201cRoSI\u201d (GRK 1907). Rafael Pen\u0303aloza was partially supported by DFG through the Cluster of Excellence \u2018cfAED,\u2019 while he was still affiliated with TU Dresden and the Center for Advancing Electronics Dresden, Germany."}], "references": [{"title": "Automata-based Axiom Pinpointing", "author": ["Baader", "Pe\u00f1aloza 2010a] Franz Baader", "Rafael Pe\u00f1aloza"], "venue": "J. of Automated Reasoning,", "citeRegEx": "Baader et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2010}, {"title": "Axiom Pinpointing in General Tableaux", "author": ["Baader", "Pe\u00f1aloza 2010b] Franz Baader", "Rafael Pe\u00f1aloza"], "venue": "J. of Logic and Computation,", "citeRegEx": "Baader et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2010}, {"title": "Pushing the EL envelope", "author": ["Baader"], "venue": "In Proc. of IJCAI\u201905,", "citeRegEx": "Baader,? \\Q2005\\E", "shortCiteRegEx": "Baader", "year": 2005}, {"title": "PatelSchneider, editors. The Description Logic Handbook: Theory, Implementation, and Applications", "author": ["Baader"], "venue": null, "citeRegEx": "Baader,? \\Q2007\\E", "shortCiteRegEx": "Baader", "year": 2007}, {"title": "Context-Dependent Views to Axioms and Consequences of Semantic Web Ontologies", "author": ["Baader"], "venue": "J. of Web Semantics,", "citeRegEx": "Baader,? \\Q2012\\E", "shortCiteRegEx": "Baader", "year": 2012}, {"title": "The Bayesian Description Logic BEL", "author": ["Ceylan", "Rafael Pe\u00f1aloza"], "venue": "In Proc. of IJCAR\u201914,", "citeRegEx": "Ceylan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ceylan et al\\.", "year": 2014}, {"title": "Tight Complexity Bounds for Reasoning in the Description Logic BEL", "author": ["Ceylan", "Rafael Pe\u00f1aloza"], "venue": "In Proc. of JELIA\u201914,", "citeRegEx": "Ceylan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ceylan et al\\.", "year": 2014}, {"title": "Modeling and Reasoning with Bayesian Networks", "author": ["Adnan Darwiche"], "venue": null, "citeRegEx": "Darwiche.,? \\Q2009\\E", "shortCiteRegEx": "Darwiche.", "year": 2009}, {"title": "A model for reasoning about persistence and causation", "author": ["Dean", "Kanazawa 1989] Thoma Dean", "Keiji Kanazawa"], "venue": "Computational intelligence,", "citeRegEx": "Dean et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1989}, {"title": "The existence of stationary measures for certain Markov processes", "author": [], "venue": "In Proc. of Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "Harris.,? \\Q1956\\E", "shortCiteRegEx": "Harris.", "year": 1956}, {"title": "Probabilistic Reasoning in Terminological Logics", "author": ["Manfred Jaeger"], "venue": "In Proc. of KR\u201994,", "citeRegEx": "Jaeger.,? \\Q1994\\E", "shortCiteRegEx": "Jaeger.", "year": 1994}, {"title": "Finding All Justifications of OWL DL Entailments", "author": ["Kalyanpur"], "venue": "In Proc. of ISWC\u201907,", "citeRegEx": "Kalyanpur,? \\Q2007\\E", "shortCiteRegEx": "Kalyanpur", "year": 2007}, {"title": "Representing sampling distributions in P-SROIQ", "author": ["Klinov", "Parsia 2011] Pavel Klinov", "Bijan Parsia"], "venue": "In Proc. of URSW\u201911,", "citeRegEx": "Klinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Klinov et al\\.", "year": 2011}, {"title": "Representation of Switching Circuits by Binary-Decision Programs", "author": ["Chang-Yeong Lee"], "venue": "Bell System Technical Journal,", "citeRegEx": "Lee.,? \\Q1959\\E", "shortCiteRegEx": "Lee.", "year": 1959}, {"title": "Managing uncertainty and vagueness in description logics for the Semantic Web", "author": ["Lukasiewicz", "Umberto Straccia"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Lukasiewicz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lukasiewicz et al\\.", "year": 2008}, {"title": "Probabilistic Description Logics for Subjective Uncertainty", "author": ["Lutz", "Schr\u00f6der 2010] Carsten Lutz", "Lutz Schr\u00f6der"], "venue": "In Proc. of KR\u201910,", "citeRegEx": "Lutz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lutz et al\\.", "year": 2010}, {"title": "Dynamic bayesian networks: representation, inference and learning", "author": ["Kevin Patrick Murphy"], "venue": "PhD thesis,", "citeRegEx": "Murphy.,? \\Q2002\\E", "shortCiteRegEx": "Murphy.", "year": 2002}, {"title": "Inference Algorithms in Bayesian Networks and the Probanet System", "author": ["Pan"], "venue": "Digital Signal Processing,", "citeRegEx": "Pan,? \\Q1998\\E", "shortCiteRegEx": "Pan", "year": 1998}, {"title": "Recognizing multi-agent activities from GPS data", "author": ["Sadilek", "Kautz 2010] Adam Sadilek", "Henry Kautz"], "venue": "In Proc. of AAAI\u201910,", "citeRegEx": "Sadilek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sadilek et al\\.", "year": 2010}, {"title": "The Complexity of Reasoning with Cardinality Restrictions and Nominals in Expressive Description Logics", "author": ["Stephan Tobies"], "venue": "J. of Artificial Intel. Research,", "citeRegEx": "Tobies.,? \\Q2000\\E", "shortCiteRegEx": "Tobies.", "year": 2000}, {"title": "Efficient Probabilistic Inference for Dynamic Relational Models", "author": ["Vlasselaer"], "venue": "In Proc. of StarAI\u201914, volume WS-14-13 of AAAI Workshops. AAAI Press,", "citeRegEx": "Vlasselaer,? \\Q2014\\E", "shortCiteRegEx": "Vlasselaer", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Many formalisms combining ontology languages with uncertainty, usually in the form of probabilities, have been studied over the years. Most of these formalisms, however, assume that the probabilistic structure of the knowledge remains static over time. We present a general approach for extending ontology languages to handle time-evolving uncertainty represented by a dynamic Bayesian network. We show how reasoning in the original language and dynamic Bayesian inferences can be exploited for effective reasoning", "creator": "LaTeX with hyperref package"}}}