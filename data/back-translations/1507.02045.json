{"id": "1507.02045", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2015", "title": "What Your Username Says About You", "abstract": "Usernames are ubiquitous on the Internet and often indicate the demographic evolution of users. In this paper, it is investigated to what extent gender and language can be derived from a username alone by using unattended morphology to break user names down into subunits. Experimental results for both tasks show the effectiveness of the proposed morphological characteristics compared to a character base of n-grams.", "histories": [["v1", "Wed, 8 Jul 2015 06:52:50 GMT  (17kb)", "https://arxiv.org/abs/1507.02045v1", null], ["v2", "Sun, 16 Aug 2015 18:06:17 GMT  (18kb)", "http://arxiv.org/abs/1507.02045v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron jaech", "mari ostendorf"], "accepted": true, "id": "1507.02045"}, "pdf": {"name": "1507.02045.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ajaech@uw.edu", "ostendor@uw.edu", "@taylorswift13"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 7.\n02 04\n5v 2\n[ cs\n.C L\n] 1\n6 A\nug 2\n01 5"}, {"heading": "1 Introduction", "text": "There is much interest in automatic recognition of demographic information of Internet users to improve the quality of online interactions. Researchers have looked into identifying a variety of factors about users, including age, gender, language, religious beliefs and political views. Most work leverages multiple sources of information, such as search query history, Twitter feeds, Facebook likes, social network links, and user profiles. However, in many situations, little of this information is available. Conversely, usernames are almost always available.\nIn this work, we look specifically at classifying gender and language based only on the username. Prior work by sociologists has established a link between usernames and gender (Cornetto and Nowak, 2006), and studies have linked usernames to other attributes, such as individual beliefs (Crabill, 2007; Hassa, 2012) and shown how usernames shape perceptions of gender and ethnicity in the absence of common nonverbal cues (Pelletier, 2014). The connections to ethnicity motivate the exploration of language identification.\nGender identification based on given names is very effective for English (Liu and Ruths, 2013),\nsince many names are strongly associated with a particular gender, like \u201cEmily\u201d or \u201cMark\u201d. Unfortunately, the requirement that each username be unique precludes use of given names alone. Instead, usernames are typically a combination of component words, names and numbers. For example, the Twitter name @taylorswift13 might decompose into \u201ctaylor\u201d, \u201cswift\u201d and \u201c13\u201d. The sub-units carry meaning and, importantly, they are shared with many other individuals. Thus, our approach is to leverage automatic decomposition of usernames into sub-units for use in classification.\nWe use the Morfessor algorithm (Creutz and Lagus, 2006; Virpioja et al., 2013) for unsupervised morphology induction to learn the decomposition of the usernames into subunits. Morfessor has been used successfully in a variety of language modeling frameworks applied to a number of languages, particularly for learning concatenative morphological structure. The usernames that we analyze are a good match to the Morfessor framework, which allows us to push the boundary of how much can be done with only a username.\nThe classifier design is described in the next section, followed by a description of experiments on gender and language recognition that demonstrate the utility of morph-based features compared to character n-gram features. The paper closes with a discussion of related work and a summary of key findings."}, {"heading": "2 General Classification Approach", "text": ""}, {"heading": "2.1 Unsupervised Morphology Learning", "text": "In linguistics, a morpheme is the \u201cminimal linguistic unit with lexical or grammatical meaning\u201d (Booij, 2012). Morphemes are combined in various ways to create longer words. Similarly, usernames are frequently made up of a concatenated sequence of smaller units. These sub-units will be\nreferred to as u-morphs to highlight the fact that they play an analogous role to morphemes but for purposes of encoding usernames rather than standard words in a language. The u-morphs are subunits that are small enough to be shared across different usernames but retain some meaning.\nUnsupervised morphology induction using Morfessor (Creutz and Lagus, 2006) is based on a minimum description length (MDL) objective, which balances two competing goals: maximizing both the likelihood of the data and of the model. The likelihood of the data is maximized by longer tokens and a bigger lexicon whereas the likelihood of the model is maximized by a smaller lexicon with shorter tokens. A parameter controls the trade-off between the two parts of the objective function, which alters the average u-morph length. We tune this parameter on held-out data to optimize the classification performance of the demographic tasks.\nMaximizing the Morfessor objective exactly is computationally intractable. The Morfessor algorithm searches for the optimal lexicon using an iterative approach. First, the highest probability decomposition for each training token is found given the current model. Then, the model is updated with the counts of the u-morphs. A umorph is added to the lexicon when it increases the weighted likelihood of the data by more than the cost of increasing the size of the lexicon.\nUsernames can be mixed-case, e.g. \u201cJohnDoe\u201d. The case change gives information about a likely u-morph boundary, but at the cost of doubling the size of the character set. To more effectively leverage this cue, all characters are made lowercase but each change from lower to uppercase is marked with a special token, e.g. \u201cjohn$doe\u201d. Using this encoding reduces the u-morph inventory size, and we found it to give slightly better results in language identification.\nCharacter 3-grams and 4-grams are used as baseline features. Before extracting the n-grams a \u201c#\u201d token is placed at the start and end of each username. The n-grams are overlapping to give them the best chance of finding a semantically meaningful sub-unit."}, {"heading": "2.2 Classifier Design", "text": "Given a decomposition of the username into a sequence of u-morphs (or character n-grams), we represent the relationship between the observed\nfeatures and each class with a unigram language model. If a username u has decomposition m1, . . . ,mn then it is assigned to the class ci for which the unigram model gives it the highest posterior probability, or equivalently:\nargmaxi pC(ci) n \u220f\nk=1\np(mk|ci),\nwhere pC(ci) is the class prior and p(mk|ci) is the class-dependent unigram.1\nFor some demographics, the class prior can be very skewed, as in the case of language detection where English is the dominant language. The choice of smoothing algorithm can be important in such cases, since minority classes have much less training data for estimating the language model and benefit from having more probability mass assigned to unseen words. Here, we follow the approach proposed in (Frank and Bouckaert, 2006) that normalizes the token count vectors for each class to have the same L1 norm, specifically:\np(mk|ci) = 1\nZ\n(\n1 + \u03b2 \u00b7 n(mk, ci)\nn(ci)\n)\n,\nwhere n(\u00b7) indicates counts and \u03b2 controls the strength of the smoothing. Setting \u03b2 equal to the number of training examples approximately matches the strength of the smoothing to the addone-smoothing algorithm. Z = \u03b2 + |M | is a constant to make the probabilities sum to one.\nOnly a small portion of usernames on the Internet come with gender labels. In these situations, semi-supervised learning algorithms can use the unlabeled data to improve the performance of the classifier. We use a self-training expectationmaximization (EM) algorithm similar to that described in (Nigam et al., 2000). The algorithm first learns a classifier on the labeled data. In the E-step, the classifier assigns probabilistic labels to the unlabeled data. In the M-step, the labeled data and the probabilistic labels are combined to learn\n1Note that the unigram model used here, which considers only the observed u-morphs or n-grams, is not the same as using a Naive Bayes (NB) classifier based on a vector of u-morph counts. In the former, unobserved u-morphs do not impact the class-dependent probability, whereas the zero counts do impact the probability for the NB classifier. Since the vast majority of possible u-morphs are unobserved in a username, it is better to base the decision only on the observed u-morphs. The n-gram model is actually a unigram with an n-gram \u201cvocabulary\u201d rather than an n-gram language model.\na new classifier. These steps are iterated until convergence, which usually requires three iterations for our tasks.\nNigam et al. (2000) call their method EM-\u03bb because it uses a parameter \u03bb to reduce the weight of the unlabeled examples relative to the labeled data. This is important because the independence assumptions of the unigram model lead to overconfident predictions. We used another method that directly corrects the estimated posterior probabilities. Using a small validation set, we binned the probability estimates and calculated the true class probability for each bin. The EM algorithm used the corrected probabilities for each bin for the unlabeled data during the maximization step. Samples with a prediction confidence of less than 60% are not used for training."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Gender Identification", "text": "Data was collected from the OkCupid dating site by downloading up to 1,000 profiles from 27 cities in the United States, first for men seeking women and again for women seeking men to obtain a balanced set of 44,000 usernames. The data is partitioned into three sets with 80% assigned to training and 10% each to validation and test. We also use 3.5M usernames from the photo messaging app Snapchat (McCormick, 2014): 1.5M are used for u-morph learning and 2M are for self-training. All names in this task used only lower case, due to the nature of the available data.\nThe top features ranked by likelihood ratios are given in Table 1. The u-morphs clearly carry semantic meaning, and the trigram features appear to be substrings of the top u-morph features. The trigram features have an advantage when the u-morphs are under-segmented such as if the u-morph \u201cniceguy\u201d or \u201cthatguy\u201d is included in the lexicon. Conversely, the n-grams can suffer from over-segmentation. For example, the trigram \u201cguy\u201d is inside the surname \u201cNguyen\u201d even though it is better to ignore that substring in this\ncontext. Many other tokens suffer from this problem, e.g. \u201cmiss\u201d is in \u201cmission\u201d.\nThe variable-length u-morphs are longer on average than the character n-grams (4.9 characters). The u-morph inventory size is similar to that for 3-grams but 5-10 times smaller than the 4-gram inventory, depending on the amount of data used since the inventory is expanded in semi-supervised training. By using the MDL criterion in unsupervised morphology learning, the u-morphs provide a more efficient representation of usernames than n-grams and make it easier to control the tradeoff between vocabulary size and average segment length. The smaller inventory is less sensitive to sparse data in language model training.\nThe experiment results are presented in Table 2. For the supervised learning method, the character 3-gram and 4-gram features give equivalent performance, and the u-morph features give the lowest error rate by a small amount (3% relative). More significantly, the character n-gram systems do not benefit from semi-supervised learning, but the u-morph features do. The semi-supervised u-morph features obtain an error rate of 25.8%, which represents a 10% relative reduction over the baseline character n-gram results."}, {"heading": "3.2 Language Identification on Twitter", "text": "This experiment takes usernames from the Twitter streaming API. Each username is associated with a tweet, for which the Twitter API identifies a language. The language labels are noisy, so we remove approximately 35% of the tweets where the Twitter API does not agree with the langid.py classifier (Lui and Baldwin, 2012). Both training and test sets are restricted to the nine languages that comprise at least 1% of the training set. These languages cover 96% of the observed tweets (see Table 4). About 110,000 usernames were reserved for testing and 430,000 were used for training both u-morphs and the classifier. Semi-supervised methods are not used because of the abundant labeled data. For each language, we train a one-vs.all classifier. The mixed case encoding technique\n(see sec. 2.1) gives a small increase (0.5%) in the accuracy of the model and reduces the u-morph model size by 5%.\nThe results in Tables 3 and 4 contrast systems using 4-grams, u-morphs, and a combination model, showing precision-recall trade-offs for all users together and F1 scores broken down by specific languages, respectively. The combination system simply uses the average of the posterior log-probabilities for each class giving equal weight to each model. While the overall F1 scores are similar for the 4-gram and u-morph systems, their precision and recall trade-offs are quite different, making them effective in combination. The 4-gram system has higher recall, and the u-morph system has higher precision. With the combination, we obtain a substantial gain in precision over the 4-gram system with a modest loss in recall, resulting in a 3% absolute improvement in average F1 score.\nLooking at performance on the different languages, we find that the F1 score for the combination model is higher than the 4-gram for every language, with precision always improving. For the dominant languages, the difference in recall is negligible. The infrequent languages have a 4- 8% drop in recall, but the gains in precision are substantial for these languages, ranging from 50- 100% relative. The greatest contrast between the 4-gram and the combination system can be seen for the least frequent languages, i.e. the languages with the least amount of training data. In particular, for French, the precision of the combination system (0.36) is double that of the 4-gram model (0.18) with only a 34% loss in recall (0.24 to 0.16).\nLooking at the most important features from the classifier highlights the ability of the morphemes to capture relevant meaning. The presence of the morpheme \u201cjuan\u201d, \u201cjose\u201d or \u201cflor\u201d increase the probability of a Spanish language tweet by five times. The same is true for Portuguese\nand the morpheme \u201cbieber\u201d. The morpheme \u201cq8\u201d increases the odds of an Arabic language tweet by thirteen times due to its phonetic similarity to the name of the Arabic speaking country Kuwait. Other features may simply reflect cultural norms. For example, having an underscore in the username makes it five percent less likely to observe an English tweet. These highly discriminative morphemes are both long and short. It is hard for the fixed-length n-grams to capture this information as well as the morphemes do."}, {"heading": "4 Related Work", "text": "Of the many studies on automatic classification of online user demographics, few have leveraged names or usernames at all, and the few that do mainly explore their use in combination with other features. The work presented here differs in its use of usernames alone, but more importantly in the introduction of morphological analysis to handle a large number of usernames.\nTwo studies on gender recognition are particularly relevant. Burger et al. (2011) use the Twitter username (or screen name) in combination with other profile and text features to predict gender, but they also look at the use of username features alone. The results are not directly comparable to ours, because of differences in the data set used (150k Twitter users) and the classifier framework (Winnow), but the character n-gram performance is similar to ours (21-22% different from the majority baseline). The study uses over 400k character n-grams (n=1-5) for screen names alone; our study indicatess that the u-morphs can reduce this number by a factor of 10. Burger et al. (2011)\nused the same strategy with the self-identified full name of the user as entered into their profile, obtaining 89% gender recognition (vs. 77% for screen names). Later, Liu and Ruths (2013) use the full first name from a user\u2019s profile for gender detection, finding that for the names that are highly predictive of gender, performance improves by relying on this feature alone. However, more than half of the users have a name that has an unknown gender association. Manual inspection of these cases indicated that the majority includes strings formed like usernames, nicknames or other types of word concatenations. These examples are precisely what the u-morph approach tries to address.\nLanguage identification is an active area of research (Bergsma et al., 2012; Zubiaga et al., 2014), but the username has not been used as a feature. Again, results are difficult to compare due to the lack of a common test set, but it is notable that the average F1 score for the combination model approaches the scores obtained on a similar Twitter language identification task where the algorithm has access to the full text of the tweet (Lui and Baldwin, 2014): 73% vs. 77% .\nA study that is potentially relevant to our work is automatic classification of ethnicity of Twitter users, specifically whether a user is AfricanAmerican (Pennacchiotti and Popescu, 2011). Again, a variety of content, profile and behavioral features are used. Orthographic features of the username are used (e.g. length, number of numeric/alpha characters), and names of users that a person retweets or replies to. The profile name features do not appear to be useful, but examples of related usernames point to the utility of our approach for analysis of names in other fields."}, {"heading": "5 Conclusions", "text": "In summary, this paper has introduced the use of unsupervised morphological analysis of usernames to extract features (u-morphs) for identifying user demographics, particularly gender and language. The experimental results demonstrate that usernames contain useful personal information, and that the u-morphs provide a more efficient and complementary representation than character n-grams.2 The result for language identifi-\n2In order to allow the replicability of the experiments, software and data for building and evaluating our classifiers using pre-trained Morfessor models is available at http://github.com/ajaech/username_analytics.\ncation is particularly remarkable because it comes close to matching the performance achieved by using the full text of a tweet. The work is complementary to other demographic studies in that the username prediction can be used together with other features, both for the user and members of his/her social network.\nThe methods proposed here could be extended in different directions. The unsupervised morphology learning algorithm could incorporate priors related to capitalization and non-alphabetic characters to better model these phenomena than our simple text normalization approach. More sophisticated classifiers could also be used, such as variable-length n-grams or neural-network-based n-gram language models, as opposed to the unigram model used here. Of course the sophistication of the classifier will be limited by the amount of training data available.\nA large amount of data is not necessary to build a high precision username classifier. For example, less than 7,000 training examples were available for Turkish in the language identification experiment and the classifier had a precision of 76%. Since little data is required, there may be many more applications of this type of model.\nPrior work on unsupervised morphological induction focused on applying the algorithm to natural language input. By using those techniques with a new type of input, this paper shows that there are other applications of morphology learning."}], "references": [{"title": "Language identification for creating language-specific twitter collections", "author": ["Paul McNamee", "Mossaab Bagdouri", "Clayton Fink", "Theresa Wilson"], "venue": "In Proc. second workshop on language in social media,", "citeRegEx": "Bergsma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2012}, {"title": "The Grammar of Words: An Introduction to Linguistic Morphology", "author": ["G. Booij"], "venue": "Oxford Textbooks in Linguistics. OUP Oxford", "citeRegEx": "Booij.,? \\Q2012\\E", "shortCiteRegEx": "Booij.", "year": 2012}, {"title": "Discriminating gender on Twitter", "author": ["Burger et al.2011] John D Burger", "John Henderson", "George Kim", "Guido Zarrella"], "venue": "In Proc. Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Burger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Burger et al\\.", "year": 2011}, {"title": "Utilizing usernames for sex categorization in computer-mediated communication: Examining perceptions and accuracy", "author": ["Cornetto", "Nowak2006] Karen M. Cornetto", "Kristine L. Nowak"], "venue": "CyberPsychology & Behavior", "citeRegEx": "Cornetto et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cornetto et al\\.", "year": 2006}, {"title": "Comparative content analysis of social identity cues within a white supremacist discussion board and a social activist discussion board", "author": ["Scott L Crabill"], "venue": "Ph.D. thesis,", "citeRegEx": "Crabill.,? \\Q2007\\E", "shortCiteRegEx": "Crabill.", "year": 2007}, {"title": "Morfessor in the morpho challenge", "author": ["Creutz", "Lagus2006] Mathias Creutz", "Krista Lagus"], "venue": "In Proc. PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes", "citeRegEx": "Creutz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2006}, {"title": "Naive Bayes for text classification with unbalanced classes", "author": ["Frank", "Bouckaert2006] Eibe Frank", "Remco R Bouckaert"], "venue": "In Proc. KDD,", "citeRegEx": "Frank et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Frank et al\\.", "year": 2006}, {"title": "Projecting, exposing, revealing self in the digital world: Usernames as a social practice in a Moroccan chatroom", "author": ["Samira Hassa"], "venue": null, "citeRegEx": "Hassa.,? \\Q2012\\E", "shortCiteRegEx": "Hassa.", "year": 2012}, {"title": "What\u2019s in a name? using first names as features for gender inference in Twitter", "author": ["Liu", "Ruths2013] Wendy Liu", "Derek Ruths"], "venue": "In Proc. AAAI Spring Symposium,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "langid. py: An off-the-shelf language identification tool", "author": ["Lui", "Baldwin2012] Marco Lui", "Timothy Baldwin"], "venue": "In Proc. ACL 2012 System Demonstrations,", "citeRegEx": "Lui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2012}, {"title": "Accurate language identification of Twitter messages", "author": ["Lui", "Baldwin2014] Marco Lui", "Timothy Baldwin"], "venue": "Proc. 5th Workshop on Language Analysis for Social Media (LASM)@ EACL,", "citeRegEx": "Lui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2014}, {"title": "4.6 million snapchat phone numbers and usernames leaked, January", "author": ["Rich McCormick"], "venue": null, "citeRegEx": "McCormick.,? \\Q2014\\E", "shortCiteRegEx": "McCormick.", "year": 2014}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["Nigam et al.2000] Kamal Nigam", "Andrew Kachites McCallum", "Sebastian Thrun", "Tom Mitchell"], "venue": "Machine learning,", "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "You\u2019ve got mail: Identity perceptions based on email usernames", "author": ["Laura Pelletier"], "venue": "Journal of Undergraduate Research at Minnesota State University,", "citeRegEx": "Pelletier.,? \\Q2014\\E", "shortCiteRegEx": "Pelletier.", "year": 2014}, {"title": "A machine learning approach to Twitter user classification", "author": ["Pennacchiotti", "Popescu2011] Marco Pennacchiotti", "Ana-Maria Popescu"], "venue": "In Proc. AAAI Conference on Weblogs and Social Media,", "citeRegEx": "Pennacchiotti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pennacchiotti et al\\.", "year": 2011}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline", "author": ["Peter Smit", "StigArne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Overview of TweetLID: Tweet language identification at SEPLN 2014", "author": ["Fresno."], "venue": "Proceedings of the Tweet Language Identification Workshop.", "citeRegEx": "Fresno.,? 2014", "shortCiteRegEx": "Fresno.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Prior work by sociologists has established a link between usernames and gender (Cornetto and Nowak, 2006), and studies have linked usernames to other attributes, such as individual beliefs (Crabill, 2007; Hassa, 2012) and shown how usernames shape perceptions of gender and ethnicity in the absence of common nonverbal cues (Pelletier, 2014).", "startOffset": 189, "endOffset": 217}, {"referenceID": 7, "context": "Prior work by sociologists has established a link between usernames and gender (Cornetto and Nowak, 2006), and studies have linked usernames to other attributes, such as individual beliefs (Crabill, 2007; Hassa, 2012) and shown how usernames shape perceptions of gender and ethnicity in the absence of common nonverbal cues (Pelletier, 2014).", "startOffset": 189, "endOffset": 217}, {"referenceID": 13, "context": "Prior work by sociologists has established a link between usernames and gender (Cornetto and Nowak, 2006), and studies have linked usernames to other attributes, such as individual beliefs (Crabill, 2007; Hassa, 2012) and shown how usernames shape perceptions of gender and ethnicity in the absence of common nonverbal cues (Pelletier, 2014).", "startOffset": 324, "endOffset": 341}, {"referenceID": 15, "context": "We use the Morfessor algorithm (Creutz and Lagus, 2006; Virpioja et al., 2013) for unsupervised morphology induction to learn the decomposition of the usernames into subunits.", "startOffset": 31, "endOffset": 78}, {"referenceID": 1, "context": "guistic unit with lexical or grammatical meaning\u201d (Booij, 2012).", "startOffset": 50, "endOffset": 63}, {"referenceID": 12, "context": "maximization (EM) algorithm similar to that described in (Nigam et al., 2000).", "startOffset": 57, "endOffset": 77}, {"referenceID": 12, "context": "Nigam et al. (2000) call their method EM-\u03bb because it uses a parameter \u03bb to reduce the weight of the unlabeled examples relative to the labeled data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "5M usernames from the photo messaging app Snapchat (McCormick, 2014): 1.", "startOffset": 51, "endOffset": 68}, {"referenceID": 2, "context": "Burger et al. (2011) use the Twitter username (or screen name) in combination with other profile and text features to predict gender, but they also look at the use of username features alone.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Burger et al. (2011)", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Language identification is an active area of research (Bergsma et al., 2012; Zubiaga et al., 2014), but the username has not been used as a feature.", "startOffset": 54, "endOffset": 98}], "year": 2015, "abstractText": "Usernames are ubiquitous on the Internet, and they are often suggestive of user demographics. This work looks at the degree to which gender and language can be inferred from a username alone by making use of unsupervised morphology induction to decompose usernames into sub-units. Experimental results on the two tasks demonstrate the effectiveness of the proposed morphological features compared to a character n-gram baseline.", "creator": "LaTeX with hyperref package"}}}