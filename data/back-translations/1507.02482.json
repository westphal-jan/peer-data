{"id": "1507.02482", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jul-2015", "title": "Differentially Private Ordinary Least Squares", "abstract": "Linear regression is one of the most widely used techniques in data analysis. In the face of a collection of samples composed of characteristics $x $and a designation $y $, linear regression is used to find the best prediction of the characteristic as a linear combination of the characteristics. However, it is also common to use linear regression for its explanatory skills rather than for its characterization prediction. Ordinary smallest squares (OLS) are often used in statistics to establish a correlation between an attribute (e.g. gender) and a designation (e.g. income) in the presence of other characteristics. OLS uses linear regression to estimate the correlation between the characteristic $x _ j $and a characteristic $x _ j $on a given dataset; and then, assuming a certain generative model for the data, OLS gives an interval that probably contains the correlation between $y $and $x _ j $in the underlying distribution (a confidence interval).", "histories": [["v1", "Thu, 9 Jul 2015 12:32:19 GMT  (41kb)", "https://arxiv.org/abs/1507.02482v1", null], ["v2", "Mon, 2 Nov 2015 02:19:03 GMT  (46kb)", "http://arxiv.org/abs/1507.02482v2", null], ["v3", "Wed, 25 Nov 2015 00:24:42 GMT  (46kb)", "http://arxiv.org/abs/1507.02482v3", null], ["v4", "Mon, 21 Aug 2017 21:30:27 GMT  (605kb,D)", "http://arxiv.org/abs/1507.02482v4", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.LG", "authors": ["or sheffet"], "accepted": true, "id": "1507.02482"}, "pdf": {"name": "1507.02482.pdf", "metadata": {"source": "META", "title": "Differentially Private Ordinary Least Squares", "authors": ["Or Sheffet"], "emails": ["<osheffet@ualberta.ca>."], "sections": [{"heading": "1. Introduction", "text": "Since the early days of differential privacy, its main goal was to design privacy preserving versions of existing techniques for data analysis. It is therefore no surprise that several of the first differentially private algorithms were machine learning algorithms, with a special emphasis on the ubiquitous problem of linear regression (Kasiviswanathan et al., 2008; Chaudhuri et al., 2011; Kifer et al., 2012; Bass-\n1Computing Science Dept., University of Alberta, Edmonton AB, Canada. This work was done when the author was at Harvard University, supported by NSF grant CNS-123723. Correspondence to: Or Sheffet <osheffet@ualberta.ca>.\nily et al., 2014). However, all existing body of work on differentially private linear regression measures utility by bounding the distance between the linear regressor found by the standard non-private algorithm and the regressor found by the privacy-preserving algorithm. This is motivated from a machine-learning perspective, since bounds on the difference in the estimators translate to error bounds on prediction (or on the loss function). Such bounds are (highly) interesting and non-trivial, yet they are of little use in situations where one uses linear regression to establish correlations rather than predict labels.\nIn the statistics literature, Ordinary Least Squares (OLS) is a technique that uses linear regression in order to infer the correlation between a variable and an outcome, especially in the presence of other factors. And so, in this paper, we draw a distinction between \u201clinear regression,\u201d by which we refer to the machine learning technique of finding a specific estimator for a specific loss function; and \u201cOrdinary Least Squares,\u201d by which we refer to the statistical inference done assuming a specific model for generating the data and that uses linear regression. Many argue that OLS is the most prevalent technique in social sciences (Agresti & Finlay, 2009). Such works make no claim as to the labels of a new unlabeled batch of samples. Rather they aim to establish the existence of a strong correlation between the label and some feature. Needless to say, in such works, the privacy of individuals\u2019 data is a concern.\nIn order to determine that a certain variable xj is positively (resp. negatively) correlated with an outcome y, OLS assumes a model where the outcome y is a noisy version of a linear mapping of all variables: y = \u03b2 \u00b7 x + e (with e denoting random Gaussian noise) for some predetermined and unknown \u03b2 . Then, given many samples (xi, yi) OLS establishes two things: (i) when fitting a linear function to best predict y from x over the sample (via computing \u03b2\u0302 = (\u2211 i xix T i )\u22121 ( \u2211 i yixi)) the coefficient \u03b2\u0302j is positive (resp. negative); and (ii) inferring, based on \u03b2\u0302j , that the true \u03b2j is likely to reside in R>0 (resp. R<0). In fact, the crux in OLS is by describing \u03b2j using a probability distribution over the reals, indicating where \u03b2j is likely to fall, derived by computing t-values. These values take into account both the variance in the data as well as the variance of the noise e.1 Based on this probability distribution one can\n1For example, imagine we run linear regression on a certain (X,y) which results in a vector \u03b2\u0302 with coordinates \u03b2\u03021 = \u03b2\u03022 =\nar X\niv :1\n50 7.\n02 48\n2v 4\n[ cs\n.D S]\n2 1\nA ug\n2 01\n7\ndefine the \u03b1-confidence interval \u2014 an interval I centered at \u03b2\u0302j whose likelihood to contain \u03b2j is 1\u2212\u03b1. Of particular importance is the notion of rejecting the null-hypothesis, where the interval I does not contain the origin, and so one is able to say with high confidence that \u03b2j is positive (resp. negative). Further details regarding OLS appear in Section 2.\nIn this work we give the first analysis of statistical inference for OLS using differentially private estimators. We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014). Instead, the novelty of our work lies in the analyses of the algorithms and in proving that the output of the algorithms is useful for statistical inference.\nThe Algorithms. Our first algorithm (Algorithm 1) is an adaptation of Gaussian JLT. Proving that this adaptation remains ( , \u03b4)-differentially private is straightforward (the proof appears in Appendix A.1). As described, the algorithm takes as input a parameter r (in addition to the other parameters of the problem) that indicates the number of rows in the JL-matrix. Later, we analyze what should one set as the value of r. Our second algorithm is taken\nAlgorithm 1 Outputting a private Johnson-Lindenstrauss projection of a matrix.\nInput: A matrix A \u2208 Rn\u00d7d and a bound B > 0 on the l2-norm of any row in A. Privacy parameters: , \u03b4 > 0. Parameter r indicating the number of rows in the resulting matrix.\nSet w s.t. w2 = 8B 2 (\u221a 2r ln(8/\u03b4) + 2 ln(8/\u03b4) ) .\nSample Z \u223c Lap(4B2/ ) and let \u03c3min(A) denote the smallest singular value of A. if \u03c3min(A)2 > w2 + Z + 4B 2 ln(1/\u03b4) then\nSample a (r\u00d7n)-matrixRwhose entries are i.i.d samples from a normal Gaussian. return RA and \u201cmatrix unaltered\u201d. else LetA\u2032 denote the result of appendingAwith the d\u00d7dmatrix wId\u00d7d. Sample a (r \u00d7 (n + d))-matrix R whose entries are i.i.d samples from a normal Gaussian. returnRA\u2032 and \u201cmatrix altered\u201d. end if\nverbatim from the work of Dwork et al (2014). We de-\n0.1. Yet while the column X1 contains many 1s and (\u22121)s, the column X2 is mostly populated with zeros. In such a setting, OLS gives that it is likely to have \u03b21 \u2248 0.1, whereas no such guarantees can be given for \u03b22.\nliberately focus on algorithms that approximate the 2ndmoment matrix of the data and then run hypothesis-testing by post-processing the output, for two reasons. First, they enable sharing of data2 and running unboundedly many hypothesis-tests. Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function \u2014 but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private. This means that differentially-private OLS based on these ERM algorithms requires us to devise new versions of these algorithms, making this a second step in this line of work... (After first understanding what we can do using existing algorithms.) We leave this approach \u2014 as well as performing private hypothesis testing using a PTR-type algorithm (Dwork & Lei, 2009) (output merely reject / don\u2019t-reject decision without justification), or releasing only relevant tests judging by their p-values (Dwork et al., 2015) \u2014 for future work.\nOur Contribution and Organization. We analyze the performances of our algorithms on a matrix A of the form A = [X;y], where each coordinate yi is generated according to the homoscedastic model with Gaussian noise, which is a classical model in statistics. We assume the existence of a vector \u03b2 s.t. for every i we have yi = \u03b2Txi + ei and ei is sampled i.i.d from N (0, \u03c32).3\nWe study the result of running Algorithm 1 on such data in the two cases: where A wasn\u2019t altered by the algorithm and when A was appended by the algorithm. In the former case, Algorithm 1 boils down to projecting the data under a Gaussian JLT. Sarlos (2006) has already shown that the JLT is useful for linear regression, yet his work bounds the l2-norm of the difference between the estimated regression before and after the projection. Following Sarlos\u2019 work, other works in statistics have analyzed compressed\n2Researcher A collects the data and uses the approximation of the 2nd-moment matrix to test some OLS hypothesis; but once the approximation is published researcher B can use it to test for a completely different hypothesis.\n3This model may seem objectionable. Assumptions like the noise independence, 0-meaned or sampled from a Gaussian distribution have all been called into question in the past. Yet due to the prevalence of this model we see fit to initiate the line of work on differentially private Least Squares with this Ordinary model.\nAlgorithm 2 \u201cAnalyze Gauss\u201d Algorithm of Dwork et al (2014).\nInput: A matrix A \u2208 Rn\u00d7d and a bound B > 0 on the l2-norm of any row in A. Privacy parameters: , \u03b4 > 0. N \u2190 symmetric (d \u00d7 d)-matrix with upper triangle entries sampled i.i.d from N ( 0, 2B 4 ln(2/\u03b4) 2 ) .\nreturn ATA+N .\nlinear regression (Zhou et al., 2007; Pilanci & Wainwright, 2014a;b). However, none of these works give confidence intervals based on the projected data, presumably for three reasons. Firstly, these works are motivated by computational speedups, and so they use fast JLT as opposed to our analysis which leverages on the fact that our JL-matrix is composed of i.i.d Gaussians. Secondly, the focus of these works is not on OLS but rather on newer versions of linear regression, such as Lasso or when \u03b2 lies in some convex set. Lastly, it is evident that the smallest confidence interval is derived from the data itself. Since these works do not consider privacy applications, (actually, (Zhou et al., 2007; Pilanci & Wainwright, 2014a) do consider privacy applications of the JLT, but quite different than differential privacy) they assume the analyst has access to the data itself, and so there was no need to give confidence intervals for the projected data. Our analysis is therefore the first, to the best of our knowledge, to derive t-values \u2014 and therefore achieve all of the rich expressivity one infers from tvalues, such as confidence bounds and null-hypotheses rejection \u2014 for OLS estimations without having access to X itself. We also show that, under certain conditions, the sample complexity for correctly rejecting the null-hypothesis increases from a certain bound N0 (without privacy) to a bound ofN0 + O\u0303( \u221a N0 \u00b7\u03ba( 1nA\nTA)/ ) with privacy (where \u03ba(M) denotes the condition number of the matrixM .) This appears in Section 3.\nIn Section 4 we analyze the case Algorithm 1 does append the data and the JLT is applied to A\u2032. In this case, solving the linear regression problem on the projected A\u2032 approximates the solution for Ridge Regression (Tikhonov, 1963; Hoerl & Kennard, 1970). In Ridge Regression we aim to solve minz (\u2211 i(yi \u2212 zTxi)2 + w2\u2016z\u20162 ) , which means we penalize vectors whose l2-norm is large. In general, it is not known how to derive t-values from Ridge regression, and the literature on deriving confidence intervals solely from Ridge regression is virtually non-existent. Indeed, prior to our work there was no need for such calculations, as access to the data was (in general) freely given, and so deriving confidence intervals could be done by appealing back to OLS. We too are unable to derive approximated t-values in the general case, but under additional assumptions about the data \u2014 which admittedly depend in part on \u2016\u03b2\u2016 and so cannot be verified solely from the data \u2014 we show that solving the linear regression problem on RA\u2032 allows us to give confidence intervals for \u03b2j , thus correctly determining the correlation\u2019s sign.\nIn Section 5 we discuss the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014) that outputs a noisy version of a covariance of a given matrix using additive noise rather than multiplicative noise. Empirical work (Xi et al., 2011) shows that Analyze Gauss\u2019s output might be non-PSD if the input has small singular values, and this results in truly bad regressors. Nonetheless, under additional conditions (that imply that the output is PSD), we derive confidence\nbounds for Dwork et al\u2019s \u201cAnalyze Gauss\u201d algorithm. Finally, in Section 6 we experiment with the heuristic of computing the t-values directly from the outputs of Algorithms 1 and 2. We show that Algorithm 1 is more \u201cconservative\u201d than Algorithm 2 in the sense that it tends to not reject the null-hypothesis until the number of examples is large enough to give a very strong indication of rejection. In contrast, Algorithm 2 may wrongly rejects the null-hypothesis even when it is true.\nDiscussion. Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence). But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016). These works are experimentally promising, yet they (i) focus on different statistical tests (mostly Goodness-of-Fit and Independence testing), (ii) are only able to prove results for the case of simple hypothesis-testing (a single hypothesis) with an efficient data-generation procedure through repeated simulations \u2014 a cumbersome and time consuming approach. In contrast, we deal with a composite hypothesis (we simultaneously reject all \u03b2s with sign(\u03b2j) 6= sign(\u03b2\u0302j)) by altering the confidence interval (or the critical region).\nOne potential reason for avoiding confidence-interval analysis for differentially private hypotheses testing is that it does involve re-visiting existing results. Typically, in statistical inference the sole source of randomness lies in the underlying model of data generation, whereas the estimators themselves are a deterministic function of the dataset. In contrast, differentially private estimators are inherently random in their computation. Statistical inference that considers both the randomness in the data and the randomness in the computation is highly uncommon, and this work, to the best of our knowledge, is the first to deal with randomness in OLS hypothesis testing. We therefore strive in our analysis to separate the two sources of randomness \u2014 as in classic hypothesis testing, we use \u03b1 to denote the bound on any bad event that depends solely on the homoscedastic model, and use \u03bd to bound any bad event that depends on the randomized algorithm.4 (Thus, any result which is originally of the form \u201c\u03b1-reject the null-hypothesis\u201d is now converted into a result \u201c(\u03b1+\u03bd)-reject the null hypothesis\u201d.)\n4Or any randomness in generating the feature matrixX which standard OLS theory assumes to be fixed, see Theorems 2.2 and 3.3."}, {"heading": "2. Preliminaries and OLS Background", "text": "Notation. Throughout this paper, we use lower-case letters to denote scalars (e.g., yi or ei); bold characters to denote vectors; and UPPER-case letters to denote matrices. The l-dimensional all zero vector is denoted 0l, and the l \u00d7 m-matrix of all zeros is denoted 0l\u00d7m. We use e to denote the specific vector y \u2212 X\u03b2 in our model; and though the reader may find it a bit confusing but hopefully clear from the context \u2014 we also use ej and ek to denote elements of the natural basis (unit length vector in the direction of coordinate j or k). We use , \u03b4 to denote the privacy parameters of Algorithms 1 and 2, and use \u03b1 and \u03bd to denote confidence parameters (referring to bad events that hold w.p. \u2264 \u03b1 and \u2264 \u03bd resp.) based on the homoscedastic model or the randomized algorithm resp. We also stick to the notation from Algorithm 1 and usew to denote the positive scalar for which w2 = 8B 2 (\u221a 2r ln(8/\u03b4) + ln(8/\u03b4)\n) throughout this paper. We use standard notation for SVD composition of a matrix (M = U\u03a3V T), its singular values and its Moore-Penrose inverse (M+).\nThe Gaussian distribution. A univariate Gaussian N (\u00b5, \u03c32) denotes the Gaussian distribution whose mean is \u00b5 and variance \u03c32. Standard concentration bounds on Gaussians give that Pr[x > \u00b5 + 2\u03c3 \u221a ln(2/\u03bd)] < \u03bd for any \u03bd \u2208 (0, 1e ). A multivariate Gaussian N (\u00b5,\u03a3) for some positive semi-definite \u03a3 denotes the multivariate Gaussian distribution where the mean of the j-th coordinate is the \u00b5j and the covariance between coordinates j and k is \u03a3j,k. The PDF of such Gaussian is defined only on the subspace colspan(\u03a3). A matrix Gaussian distribution, denoted N (Ma\u00d7b, Ia\u00d7a, V ) has mean M , independence among its rows and variance V for each of its columns. We also require the following property of Gaussian random variables: Let X and Y be two random Gaussians s.t. X \u223c N (0, \u03c32) and Y \u223c N (0, \u03bb2) where 1 \u2264 \u03c3 2\n\u03bb2 \u2264 c 2 for some c, then for any S \u2282 R we have\n1 cPrx\u2190Y [x \u2208 S] \u2264 Prx\u2190X [x \u2208 S] \u2264 cPrx\u2190Y [x \u2208 S/c] (see Proposition A.2).\nAdditional Distributions. We denote by Lap(\u03c3) the Laplace distribution whose mean is 0 and variance is 2\u03c32. The \u03c72k-distribution, where k is referred to as the degrees of freedom of the distribution, is the distribution over the l2-norm squared of the sum of k independent normal Gaussians. That is, given i.i.d X1, . . . , Xk \u223c N (0, 1) it holds that \u03b6 def= (X1, X2, . . . , Xk) \u223c N (0k, Ik\u00d7k), and \u2016\u03b6\u20162 \u223c \u03c72k. Existing tail bounds on the \u03c72k distribution (Laurent & Massart, 2000) give that\nPr [ \u2016\u03b6\u20162 \u2208 ( \u221a k \u00b1 \u221a 2 ln(2/\u03bd))2 ] \u2265 1 \u2212 \u03bd. The Tk-\ndistribution, where k is referred to as the degrees of freedom of the distribution, denotes the distribution over the reals created by independently sampling Z \u223c N (0, 1) and\n\u2016\u03b6\u20162 \u223c \u03c72k, and taking the quantity Z/ \u221a \u2016\u03b6\u20162/k. It is a known fact that Tk k\u2192\u221e\u2192 N (0, 1), thus it is a common practice to apply Gaussian tail bounds to the Tk-distribution when k is sufficiently large.\nDifferential Privacy. In this work, we deal with input in the form of a n \u00d7 d-matrix with each row bounded by a l2-norm of B. Two inputs A and A\u2032 are called neighbors if they differ on a single row. Definition 2.1 ((Dwork et al., 2006a)). An algorithm ALG which maps (n \u00d7 d)-matrices into some range R is ( , \u03b4)differential privacy it holds that Pr[ALG(A) \u2208 S] \u2264 e Pr[ALG(A\u2032) \u2208 S] + \u03b4 for all neighboring inputs A and A\u2032 and all subsets S \u2282 R.\nBackground on OLS. For the unfamiliar reader, we give here a very brief overview of the main points in OLS. Further details, explanations and proofs appear in Section A.3.\nWe are given n observations {(xi, yi)}ni=1 where \u2200i,xi \u2208 Rp and yi \u2208 R. We assume the existence of \u03b2 \u2208 Rp s.t. the label yi was derived by yi = \u03b2Txi + ei where ei \u223c N (0, \u03c32) independently (also known as the homoscedastic Gaussian model). We use the matrix notation where X denotes the (n \u00d7 p)- feature matrix and y denotes the labels. We assume X has full rank.\nThe parameters of the model are therefore \u03b2 and \u03c32, which we set to discover. To that end, we minimize minz \u2016y \u2212 Xz\u20162 and have\n\u03b2\u0302 = (XTX)\u22121XTy = (XTX)\u22121XT(X\u03b2 + e) = \u03b2 +X+e (1) \u03b6 = y \u2212X\u03b2\u0302 = (X\u03b2 + e)\u2212X(\u03b2 +X+e) = (I \u2212XX+)e (2)\nAnd then for any coordinate j the t-value, which is the quantity t(\u03b2j) def =\n\u03b2\u0302j\u2212\u03b2j\u221a (XTX)\u22121j,j \u00b7 \u2016\u03b6\u2016\u221a n\u2212p , is\ndistributed according to Tn\u2212p-distribution. I.e., Pr [ \u03b2\u0302 and \u03b6 satisfying t(\u03b2j) \u2208 S ] = \u222b S PDFTn\u2212p(x)dx for any measurable S \u2282 R. Thus t(\u03b2j) describes the likelihood of any \u03b2j \u2014 for any z \u2208 R we can now give an estimation of how likely it is to have \u03b2j = z (which is PDFTn\u2212p(t(z))), and this is known as t-test for the value z. In particular, given 0 < \u03b1 < 1, we denote c\u03b1 as the number for which the interval (\u2212c\u03b1, c\u03b1) contains a probability mass of 1 \u2212 \u03b1 from the Tn\u2212p-distribution. And so we derive a corresponding confidence interval I\u03b1 centered at \u03b2\u0302j where \u03b2j \u2208 I\u03b1 with confidence of level of 1\u2212 \u03b1.\nOf particular importance is the quantity t0 def = t(0) =\n\u03b2\u0302j \u221a n\u2212p \u2016\u03b6\u2016 \u221a\n(XTX)\u22121j,j\n,since if there is no correlation between xj\nand y then the likelihood of seeing \u03b2\u0302j depends on the ratio of its magnitude to its standard deviation. As mentioned earlier, since Tk k\u2192\u221e\u2192 N (0, 1), then rather than viewing\nthis t0 as sampled from a Tn\u2212p-distribution, it is common to think of t0 as a sample from a normal GaussianN (0, 1). This allows us to associate t0 with a p-value, estimating the event \u201c\u03b2j and \u03b2\u0302j have different signs.\u201d Specifically, given \u03b1 \u2208 (0, 1/2), we \u03b1-reject the null hypothesis if p0 < \u03b1. Let \u03c4\u03b1 be the number s.t. \u03a6(\u03c4\u03b1) = \u222b\u221e \u03c4\u03b1 1\u221a 2\u03c0 e\u2212x\n2/2dx = \u03b1. This means we \u03b1-reject the null hypothesis when |t0| > \u03c4\u03b1. We now lower bound the number of i.i.d sample points needed in order to \u03b1-reject the null hypothesis. This bound is our basis for comparison between standard OLS and the differentially private version.5\nTheorem 2.2. Fix any positive definite matrix \u03a3 \u2208 Rp\u00d7p and any \u03bd \u2208 (0, 12 ). Fix parameters \u03b2 \u2208 R p and \u03c32 and a coordinate j s.t. \u03b2j 6= 0. Let X be a matrix whose n rows are i.i.d samples from N (0,\u03a3), and y be a vector where yi \u2212 (X\u03b2)i is sampled i.i.d from N (0, \u03c32). Fix \u03b1 \u2208 (0, 1). Then w.p. \u2265 1 \u2212 \u03b1 \u2212 \u03bd we have that OLS\u2019s (1 \u2212 \u03b1)-confidence interval has length O(c\u03b1 \u221a \u03c32/(n\u03c3min(\u03a3))) provided n \u2265 C1(p + ln(1/\u03bd)) for some sufficiently large constant C1. Furthermore, there exists a constant C2 such that w.p. \u2265 1 \u2212 \u03b1 \u2212 \u03bd OLS (correctly) rejects the null hypothesis provided n \u2265 max { C1(p+ ln(1/\u03bd)), p+ C2 \u03c32\n\u03b22j \u00b7 c\n2 \u03b1+\u03c4 2 \u03b1\n\u03c3min(\u03a3)\n} , where c\u03b1\nis the number for which \u222b c\u03b1 \u2212c\u03b1 PDFTn\u2212p(x)dx = 1\u2212 \u03b1."}, {"heading": "3. OLS over Projected Data", "text": "In this section we deal with the output of Algorithm 1 in the special case where Algorithm 1 outputs matrix unaltered and so we work with RA.\nTo clarify, the setting is as follows. We denote A = [X;y] the column-wise concatenation of the (n\u00d7 (d\u2212 1))-matrix X with the n-length vector y . (Clearly, we can denote any column of A as y and any subset of the remaining columns as the matrix X .) We therefore denote the output RA = [RX;Ry] and for simplicity we denote M = RX and p = d \u2212 1. We denote the SVD decomposition of X = U\u03a3V T. So U is an orthonormal basis for the columnspan of X and as X is full-rank V is an orthonormal basis for Rp. Finally, in our work we examine the linear regression problem derived from the projected data. That is, we denote\n\u03b2\u0303 = (XTRTRX)\u22121(RX)T(Ry) = \u03b2 + (RX)+Re (3)\n\u03c3\u03032 = r\nr \u2212 p \u2016\u03b6\u0303\u20162 , with \u03b6\u0303 = 1\u221a r Ry \u2212 1\u221a r (RX)\u03b2\u0303 (4)\nWe now give our main theorem, for estimating the t-values based on \u03b2\u0303 and \u03c3\u0303.\n5Theorem 2.2 also illustrates how we \u201cseparate\u201d the two sources of privacy. In this case, \u03bd bounds the probability of bad events that depend to sampling the rows of X , and \u03b1 bounds the probability of a bad event that depends on the sampling of the y coordinates.\nTheorem 3.1. Let X be a (n \u00d7 p)-matrix, and parameters \u03b2 \u2208 Rp and \u03c32 are such that we generate the vector y = X\u03b2 + e with each coordinate of e sampled independently from N (0, \u03c32). Assume Algorithm 1 projects the matrix A = [X;y] without altering it. Fix \u03bd \u2208 (0, 1/2) and r = p + \u2126(ln(1/\u03bd)). Fix coordinate j. Then we have that w.p. \u2265 1 \u2212 \u03bd deriving \u03b2\u0303 and \u03c3\u03032 as in Equations (3) and (4), the pivot quantity t\u0303(\u03b2j) =\n\u03b2\u0303j\u2212\u03b2j \u03c3\u0303 \u221a\n(XTRTRX)\u22121j,j\nhas a\ndistribution D satisfying e\u2212aPDFTr\u2212p(x) \u2264 PDFD(x) \u2264 eaPDFTr\u2212p(e\n\u2212ax) for any x \u2208 R, where we denote a = r\u2212p n\u2212p .\nThe implications of Theorem 3.1 are immediate: all estimations one can do based on the t-values from the true data X,y , we can now do based on t\u0303 modulo an approximation factor of exp( r\u2212pn\u2212p ). In particular, Theorem 3.1 enables us to deduce a corresponding confidence interval based on \u03b2\u0303 .\nCorollary 3.2. In the same setting as in Theorem 3.1, w.p. \u2265 1\u2212 \u03bd we have the following. Fix any \u03b1 \u2208 (0, 12 ). Let c\u0303\u03b1 denote the number s.t. the interval (c\u0303\u03b1,\u221e) contains \u03b12 e \u2212a\nprobability mass of the Tr\u2212p-distribution. Then Pr[\u03b2j \u2208( \u03b2\u0303j \u00b1 ea \u00b7 c\u0303\u03b1 \u00b7 \u03c3\u0303 \u221a (XTRTRX)\u22121j,j ) ] \u2265 1\u2212 \u03b1. 6\nWe compare the confidence interval of Corollary 3.2 to the confidence interval of the standard OLS model, whose length is c\u03b1 \u2016\u03b6\u2016\u221a n\u2212p \u221a (XTX)\u22121j,j . As R is a JLmatrix, known results regarding the JL transform give\nthat \u2016\u03b6\u0303\u2016 = \u0398 (\u2016\u03b6\u2016), and that \u221a\n(r \u2212 p)(XTRTRX)\u22121j,j = \u0398 (\u221a\n(XTX)\u22121j,j\n) . We therefore have that\n\u03c3\u0303 \u221a (XTRTRX)\u22121j,j = \u2016\u03b6\u0303\u2016 \u221a r\u221a\nr\u2212p \u221a (XTRTRX)\u22121j,j =\u221a\nr\u00b7(n\u2212p) (r\u2212p)2 \u00b7 \u0398 ( \u2016\u03b6\u2016\u221a n\u2212p \u221a (XTX)\u22121j,j ) . So for values of r for which rr\u2212p = \u0398(1) we get that the confidence interval\nof Theorem 3.1 is a factor of \u0398 ( c\u0303\u03b1 c\u03b1 \u221a n\u2212p r\u2212p ) -larger than the standard OLS confidence interval. Observe that when \u03b1 = \u0398(1), which is the common case, the dominating factor is \u221a (n\u2212 p)/(r \u2212 p). This bound intuitively makes sense: we have contracted n observations to r observations, hence our model is based on confidence intervals derived from Tr\u2212p rather than Tn\u2212p.\nIn the supplementary material we give further discussion, in which we compare our work to the more straight-forward bounds one gets by \u201cplugging in\u201d Sarlos\u2019 work (2006); and we also compare ourselves to the bounds derived from alternative works in differentially private linear regression.\n6Moreover, this interval is essentially optimal: denote d\u0303\u03b1 s.t the interval (d\u0303\u03b1,\u221e) contains \u03b12 e r\u2212p n\u2212p prob-\nability mass of the Tr\u2212p-distribution. Then Pr[\u03b2j \u2208( \u03b2\u0303j \u00b1 d\u0303\u03b1 \u00b7 \u03c3\u0303 \u221a (XTRTRX)\u22121j,j ) ] \u2264 1\u2212 \u03b1.\nRejecting the Null Hypothesis. Due to Theorem 3.1, we can mimic OLS\u2019 technique for rejecting the null hypothesis. I.e., we denote t\u03030 = \u03b2\u0303j\n\u03c3\u0303 \u221a\n(XTRTRX)\u22121j,j\nand re-\nject the null-hypothesis if indeed the associated p\u03030, denoting p-value of the slightly truncated e\u2212 r\u2212p n\u2212p t\u03030, is below \u03b1 \u00b7 e\u2212 r\u2212p n\u2212p . Much like Theorem 2.2 we now establish a lower bound on n so that w.h.p we end up (correctly) rejecting the null-hypothesis.\nTheorem 3.3. Fix a positive definite matrix \u03a3 \u2208 Rp\u00d7p. Fix parameters \u03b2 \u2208 Rp and \u03c32 > 0 and a coordinate j s.t. \u03b2j 6= 0. Let X be a matrix whose n rows are sampled i.i.d from N (0p,\u03a3). Let y be a vector s.t. yi \u2212 (X\u03b2)i is sampled i.i.d from N (0, \u03c32). Fix \u03bd \u2208 (0, 1/2) and \u03b1 \u2208 (0, 1/2). Then there exist constants C1, C2, C3 and C4 such that when we run Algorithm 1 over [X;y] with parameter r w.p. \u2265 1 \u2212 \u03b1 \u2212 \u03bd we (correctly) reject the null hypothesis using p\u03030 (i.e., Algorithm 1 returns matrix unaltered and we can estimate t\u03030 and verify that indeed p\u03030 < \u03b1 \u00b7 e \u2212 r\u2212pn\u2212p ) provided\nr \u2265 p + max { C1 \u03c32(c\u03032\u03b1+\u03c4\u0303 2 \u03b1)\n\u03b22j\u03c3min(\u03a3) , C2 ln(1/\u03bd)\n} , and n \u2265\nmax { r, C3\nw2 min{\u03c3min(\u03a3),\u03c32} , C4p ln(1/\u03bd) }\nwhere\nc\u0303\u03b1, \u03c4\u0303\u03b1 defined s.t. PrX\u223cTr\u2212p [X > c\u0303\u03b1/e r\u2212p n\u2212p ] = PrX\u223cN (0,1)[X > \u03c4\u0303\u03b1/e r\u2212p n\u2212p ] = \u03b12 e \u2212 r\u2212pn\u2212p ."}, {"heading": "3.1. Setting the Value of r, Deriving a Bound on n", "text": "Comparing the lower bound on n given by Theorem 3.3 to the bound of Theorem 2.2, we have that the data-dependent bound of \u2126 ( (c\u0303\u03b1+\u03c4\u0303\u03b1) 2\u03c32\n\u03b22j\u03c3min(\u03a3)\n) should now hold for r rather than\nn. Yet, Theorem 3.3 also introduces an additional dependency between n and r: we require n = \u2126(w 2\n\u03c32 + w2 \u03c3min(\u03a3) )\n(since otherwise we do not have \u03c3min(A) w and Algorithm 1 might alterA before projecting it) and by definition w2 is proportional to \u221a r ln(1/\u03b4)/ . This is precisely the focus of our discussion in this subsection. We would like to set r\u2019s value as high as possible \u2014 the larger r is, the more observations we have in RA and the better our confidence bounds (that depend on Tr\u2212p) are \u2014 while satisfying n = \u2126( \u221a r\n\u00b7min{\u03c32,\u03c3min(\u03a3)} ).\nRecall that if each sample point is drawn i.i.d x \u223c N (0p,\u03a3), then each sample (xi \u25e6 yi) is sampled from N (0p+1,\u03a3A) for \u03a3A defined in the proof of Theorem 3.3,\nthat is: \u03a3A = (\n\u03a3 \u03a3\u03b2\n\u03b2T\u03a3 \u03c32+\u03b2T\u03a3\u03b2\n) . So, Theo-\nrem 3.3 gives the lower bound r \u2212 p = \u2126 ( \u03c32(c\u0303\u03b1+\u03c4\u0303\u03b1) 2\n\u03b22j\u03c3min(\u03a3) ) and the following lower bounds on n: n \u2265 r and\nn = \u2126\n( B2( \u221a r ln(1/\u03b4)+ln(1/\u03b4))\n\u03c3min(\u03a3A)\n) , which means r =\nmin { n,\n2\u03c32min(\u03a3A) B4 ln(1/\u03b4) (n\u2212 ln(1/\u03b4))\n2 }\n. This discussion culminates in the following corollary.\nCorollary 3.4. Denoting L\u0303B2.2 = \u03c3 2(c\u0303\u03b1+\u03c4\u0303\u03b1) 2\n\u03b22j\u03c3min(\u03a3) , we thus conclude that if n \u2212 p \u2265 \u2126 ( L\u0303B2.2 ) and n =\n\u2126 ( B2 ln(1/\u03b4) \u03c3min(\u03a3A) \u00b7 \u221a L\u0303B2.2 ) , then the result of Theorem 3.3\nholds by setting r = min { n,\n2\u03c32min(\u03a3A) B4 ln(1/\u03b4) (n\u2212 ln(1/\u03b4))\n2 }\n.\nIt is interesting to note that when we know \u03a3A, we also have a bound on B. Recall \u03a3A, the variance of the Gaussian (x \u25e6 y). Since every sample is an independent draw from N (0p+1,\u03a3A) then we have an upper bound of B2 \u2264 log(np)\u03c3max(\u03a3A). So our lower bound on n (using \u03ba(\u03a3A) to denote the condition number of \u03a3A) is given by\nn \u2265 max { \u2126 ( L\u0303B2.2 ) , \u2126\u0303 ( \u03ba(\u03a3A) ln(1/\u03b4) \u00b7 \u221a L\u0303B2.2 )} .\nObserve, overall this result is similar in nature to many other results in differentially private learning (Bassily et al., 2014) which are of the form \u201cwithout privacy, in order to achieve a total loss of \u2264 \u03b7 we have a sample complexity bound of some N\u03b7; and with differential privacy the sample complexity increases to N\u03b7 + \u2126( \u221a N\u03b7/ ).\u201d However,\nthere\u2019s a subtlety here worth noting. L\u0303B2.2 is proportional to 1\u03c3min(\u03a3A) but not to \u03ba(\u03a3A) = \u03c3max(\u03a3A) \u03c3min(\u03a3A)\n. The additional dependence on \u03c3max follows from the fact that differential privacy adds noise proportional to the upper bound on the norm of each row."}, {"heading": "4. Projected Ridge Regression", "text": "We now turn to deal with the case that our matrix does not pass the if-condition of Algorithm 1. In this case, the matrix is appended with a d \u00d7 d-matrix which is wId\u00d7d. De-\nnoting A\u2032 = [\nA w \u00b7 Id\u00d7d\n] we have that the algorithm\u2019s\noutput is RA\u2032. Similarly to before, we are going to denote d = p+ 1 and decompose A = [X;y] with X \u2208 Rn\u00d7p and y \u2208 Rn, with the standard assumption of y = X\u03b2 + e and ei sampled i.i.d from N (0, \u03c32). We now need to introduce some additional notation. We denote the appended matrix and vectors X \u2032 and y \u2032 s.t. A\u2032 = [X \u2032;y \u2032]. And so, using the output RA\u2032 of Algorithm 1, we solve the linear regression problem derived from 1\u221a\nr RX \u2032 and 1\u221a r Ry \u2032. I.e., we set\n\u03b2 \u2032 = (X \u2032TRTRX \u2032)\u22121(RX \u2032)T(Ry \u2032) \u03b6 \u2032 = 1\u221a\nr (Ry \u2032 \u2212RX \u2032\u03b2 \u2032) (5)\nSarlos\u2019 results (2006) regarding the Johnson Lindenstrauss transform give that, when R has sufficiently many rows, solving the latter optimization problem gives a good approximation for the solution of the optimization problem \u03b2R = arg minz \u2016y \u2032 \u2212 X \u2032z\u20162 = arg minz ( \u2016y \u2212Xz\u20162 + w2\u2016z\u20162 ) . The latter problem is\nknown as the Ridge Regression problem. Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large. It is also often applied in the case where X doesn\u2019t have full rank or is close to not having full-rank: one can show that the minimizer \u03b2R = (XTX + w2Ip\u00d7p)\u22121XTy is the unique solution of the Ridge Regression problem and that the RHS is always well-defined.\nWhile the solution of the Ridge Regression problem might have smaller risk than the OLS solution, it is not known how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate \u03b2R back into \u03b2\u0302 = (XTX)\u22121XTy and relying on OLS). In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard OLS, because access to X and y was given.\nTherefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.7 Clearly, there are situations where such confidence bounds simply cannot be derived.Nonetheless, under additional assumptions about the data, our work can give confidence intervals for \u03b2j , and in the case where the interval doesn\u2019t intersect the origin \u2014 assure us that sign(\u03b2\u2032j) = sign(\u03b2j) w.h.p. This is detailed in the supplementary material.\nTo give an overview of our analysis, we first discuss a model where e = y \u2212 X\u03b2 is fixed (i.e., the data is fixed and the algorithm is the sole source of randomness), and prove that in this model \u03b2\u2032 is as an approximation to \u03b2\u0302 .\nTheorem 4.1. Fix X \u2208 Rn\u00d7p and y \u2208 R. Define \u03b2\u0302 = X+y and \u03b6 = (I \u2212 XX+)y . Let RX \u2032 = M \u2032 and Ry \u2032 denote the result of applying Algorithm 1 to the matrixA = [X;y] when the algorithm appends the data with a w \u00b7 I matrix. Fix a coordinate j and any \u03b1 \u2208 (0, 1/2). When computing \u03b2 \u2032 and \u03b6 \u2032 as in (5), we have that w.p. \u2265 1 \u2212 \u03b1 it holds that \u03b2\u0302j \u2208 ( \u03b2\u2032j \u00b1 c\u2032\u03b1\u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7 (M \u2032TM \u2032) \u22121 j,j\n) where c\u2032\u03b1 denotes the number such that (\u2212c\u2032\u03b1, c\u2032\u03b1) contains 1\u2212 \u03b1 mass of the Tr\u2212p-distribution.\nHowever, our goal remains to argue that \u03b2\u2032j serves as a good approximation for \u03b2j . To that end, we combine the standard OLS confidence interval \u2014 which says that w.p. \u2265 1\u2212\u03b1 over the randomness of picking e in the homoscedas-\ntic model we have |\u03b2j \u2212 \u03b2\u0302j | \u2264 c\u03b1\u2016\u03b6\u2016 \u221a (XTX)\u22121j,j n\u2212p \u2014 with the confidence interval of Theorem 4.1 above, and denoting I = c\u03b1 \u2016\u03b6\u2016\u221a n\u2212p \u221a (XTX)\u22121j,j + c \u2032 \u03b1 \u2016\u03b6 \u2032\u2016\u221a r\u2212p \u221a r(M \u2032TM \u2032)\u22121j,j we have that Pr[|\u03b2\u2032j \u2212 \u03b2j | = O(I)] \u2265 1 \u2212 \u03b1. And 7Note: The na\u0131\u0308ve approach of using RX \u2032 and Ry \u2032 to interpolate RX and Ry and then apply Theorem 3.1 using these estimations of RX and Ry ignores the noise added from appending the matrix A into A\u2032, and therefore leads to inaccurate estimations of the t-values.\nso, in summary, in Section C we give conditions under which the length of the interval I is dominated by the c\u2032\u03b1 \u2016\u03b6 \u2032\u2016\u221a r\u2212p \u221a r(M \u2032TM \u2032)\u22121j,j factor derived from Theorem 4.1."}, {"heading": "5. Confidence Intervals for \u201cAnalyze Gauss\u201d", "text": "In this section we analyze the \u201cAnalyze Gauss\u201d algorithm of Dwork et al (2014). Algorithm 2 works by adding random Gaussian noise to ATA, where the noise is symmetric with each coordinate above the diagonal sampled i.i.d from N (0,\u22062) with \u22062 = O ( B4 log(1/\u03b4) 2 ) . Using the same no-\ntation for a sub-matrix of A as [X;y] as before, we denote\nthe output of Algorithm 2 as  X\u0303TX X\u0303Ty y\u0303TX y\u0303Ty . Thus, we approximate \u03b2 and \u2016\u03b6\u2016 by \u03b2\u0303 = ( X\u0303TX )\u22121 X\u0303Ty and\n\u2016\u0303\u03b6\u20162 = y\u0303Ty \u2212 2 y\u0303TX \u03b2\u0303 + \u03b2\u0303 T X\u0303TX \u03b2\u0303 resp. We now argue\nthat it is possible to use \u03b2\u0303j and \u2016\u0303\u03b6\u20162 to get a confidence interval for \u03b2j under certain conditions. Theorem 5.1. Fix \u03b1, \u03bd \u2208 (0, 12 ). Assume that there exists \u03b7 \u2208 (0, 12 ) s.t. \u03c3min(X TX) > \u2206 \u221a p ln(1/\u03bd)/\u03b7. Under the homoscedastic model, given \u03b2 and \u03c32, if we assume also that \u2016\u03b2\u2016 \u2264 B and \u2016\u03b2\u0302\u2016 = \u2016(XTX)\u22121XTy\u2016 \u2264 B, then w.p. \u2265 1\u2212 \u03b1\u2212 \u03bd it holds that\n\u2223\u2223\u2223\u03b2j \u2212 \u03b2\u0303j\u2223\u2223\u2223 is at most O ( \u03c1 \u00b7 \u221a( X\u0303TX \u22121 j,j + \u2206 \u221a p ln(1/\u03bd) \u00b7 X\u0303TX \u22122 j,j ) ln(1/\u03b1)\n+ \u2206 \u221a X\u0303TX \u22122 j,j \u00b7 ln(1/\u03bd) \u00b7 (B \u221a p+ 1) ) where \u03c1 is w.h.p an upper bound on \u03c3 (details appear in the Supplementary material).\nNote that the assumptions that \u2016\u03b2\u2016 \u2264 B and \u2016\u03b2\u0302\u2016 \u2264 B are fairly benign once we assume each row has bounded l2-norm. The key assumption is that XTX is well-spread. Yet in the model where each row in X is sampled i.i.d fromN (0,\u03a3), this assumption merely means that n is large enough \u2014 namely, that n = \u2126\u0303(\u2206 \u221a p ln(1/\u03bd)\n\u03b7\u00b7\u03c3min(\u03a3) ).\n6. Experiment: t-Values of Output\nGoal. We set to experiment with the outputs of Algorithms 1 and 2. While Theorem 3.1 guarantees that computing the t-value from the output of Algorithm 1 in the matrix unaltered case does give a good approximation of the t-value \u2013 we were wondering if by computing the t-value directly from the output we can (a) get a good approximation of the true (non-private) t-value and (b) get the same \u201chigher-level conclusion\u201d of rejecting the nullhypothesis. The answers are, as ever, mixed. The two main\nobservations we do notice is that both algorithms improve as the number of examples increases, and that Algorithm 1 is more conservative then Algorithm 2.\nSetting. We tested both algorithms in two settings. The first is over synthetic data. Much like the setting in Theorems 2.2 and 3.3, X was generated using p = 3 independent normal Gaussian features, and y was generated using the homoscedastic model. We chose \u03b2 = (0.5,\u22120.25, 0) so the first coordinate is twice as big a the second but of opposite sign, and moreover, y is independent of the 3rd feature. The variance of the label is also set to 1, and so the variance of the homosedastic noise equals to \u03c32 = 1\u2212 (0.5)2 \u2212 (\u22120.25)2. The number of observations n ranges from n = 1000 to n = 100000.\nThe second setting is over real-life data. We ran the two algorithms over diabetes dataset collected over ten years (1999-2008) taken from the UCI repository (Strack et al., 2014). We truncated the data to 4 attributes: sex (binary), age (in buckets of 10 years), number medications (numeric, 0-100), and a diagnosis (numeric, 0-1000). Naturally, we added a 5th column of all-1 (intercept). Omitting any entry with missing or non-numeric values on these nine attributes we were left with N = 91842 entries, which we shuffled and fed to the algorithm in varying sizes \u2014 from n = 30, 000 to n = 90, 000. Running OLS over the entire N observation yields \u03b2 \u2248 (14.07, 0.54,\u22120.22, 482.59), and t-Values of (10.48, 1.25,\u22122.66, 157.55).\nThe Algorithms. We ran a version of Algorithm 1 that uses a DP-estimation of \u03c3min, and finds the largest r the we can use without altering the input, yet if this r is below 25 then it does alter the input and approximates Ridge regression. We ran Algorithm 2 verbatim. We set = 0.25 and \u03b4 = 10\u22126. We repeated each algorithm 100 times.\nResults. We plot the t-values we get from Algorithms 1 and 2 and decide to reject the null-hypothesis based on tvalue larger than 2.8 (which corresponds to a fairly conservative p-value of 0.005). Not surprisingly, as n increases, the t-values become closer to their expected value \u2013 the tvalue of Analyze Gauss is close to the non-private t-value and the t-value from Algorithm 1 is a factor of \u221a r n smaller as detailed above (see after Corollary 3.2). As a result, when the null-hypothesis is false, Analyze Gauss tends to produce larger t-values (and thus reject the null-hypothesis) for values of n under which Algorithm 1 still does not reject, as shown in Figure 1a. This is exacerbated in real data setting, where its actual least singular value (\u2248 500) is fairly small in comparison to its size (N = 91842).\nHowever, what is fairly surprising is the case where the null-hypothesis should not be rejected \u2014 since \u03b2j = 0 (in the synthetic case) or its non-private t-value is close to 0 (in the real-data case). Here, the Analyze Gauss\u2019 tvalue approximation has fairly large variance, and we still\nget fairly high (in magnitude) t-values. As the result, we falsely reject the null-hypothesis based on the t-value of Analyze Gauss quite often, even for large values of n. This is shown in Figure 1b. Additional figures (including plotting the distribution of the t-value approximations) appear in the supplementary material.\nThe results show that t-value approximations that do not take into account the inherent randomness in the DPalgorithms lead to erroneous conclusions. One approach would be to follow the more conservative approach we advocate in this paper, where Algorithm 1 may allow you to get true approximation of the t-values and otherwise reject the null-hypothesis only based on the confidence interval (of Algorithm 1 or 2) not intersecting the origin. Another approach, which we leave as future work, is to replace the T -distribution with a new distribution, one that takes into account the randomness in the estimator as well. This, however, has been an open and long-standing challenge since the first works on DP and statistics (see (Vu & Slavkovic, 2009; Dwork & Lei, 2009)) and requires we move into non-asymptotic hypothesis testing."}, {"heading": "Acknowledgements", "text": "The bulk of this work was done when the author was a postdoctoral fellow at Harvard University, supported by NSF grant CNS-123723; and also an unpaid collaborator on NSF grant 1565387. The author wishes to wholeheartedly thank Prof. Salil Vadhan, for his tremendous help in shaping this paper. The author would also like to thank Prof. Jelani Nelson and the members of the \u201cPrivacy Tools for Sharing Research Data\u201d project at Harvard University (especially James Honaker, Vito D\u2019Orazio, Vishesh Karwa, Prof. Kobbi Nissim and Prof. Gary King) for many helpful discussions and suggestions; as well as Abhradeep Thakurta for clarifying the similarity between our result. Lastly the author thanks the anonymous referees for many helpful suggestions in general and for a reference to (Ullman, 2015) in particular."}, {"heading": "A. Extended Introductory Discussion", "text": "Due to space constraint, a few details from the introductory parts (Sections 1,2) were omitted. We bring them in this appendix. We especially recommend the uninformed reader to go over the extended OLS background we provide in Appendix A.3."}, {"heading": "A.1. Proof Of Privacy of Algorithm 1", "text": "Theorem A.1. Algorithm 1 is ( , \u03b4)-differentially private.\nProof. The proof of the theorem is based on the fact the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork & Lei, 2009) with the differentially private analysis of the Johnson-Lindenstrauss transform of (Sheffet, 2015).\nMore specifically, we use Theorem B.1 from (Sheffet, 2015) that states that given a matrix A whose all of its singular values at greater than T ( , \u03b4) where T ( , \u03b4)2 = 2B2 (\u221a 2r ln(4/\u03b4) + 2 ln(4/\u03b4) ) , publishing RA is ( , \u03b4)-\ndifferentially private for a r-row matrix R whose entries sampled are i.i.d normal Gaussians. Since we have that all of the singular values of A\u2032 are greater than w (as specified in Algorithm 1), outputtingRA\u2032 is ( /2, \u03b4/2)-differentially private. The rest of the proof boils down to showing that (i) the if-else-condition is ( /2, 0)-differentially private and that (ii) w.p. \u2264 \u03b4/2 any matrix A whose smallest singular value is smaller than w passes the if-condition (step 3). If both these facts hold, then knowing whether we pass the if-condition or not is ( /2)-differentially private and the output of the algorithm is ( /2, \u03b4)-differentially private, hence basic composition gives the overall bound of ( , \u03b4)differential privacy.\nTo prove (i) we have that for any pair of neighboring matricesA andB that differ only on the i-th row, denoted ai and bi resp., we have BTB \u2212 bibTi = ATA \u2212 aiaTi . Applying Weyl\u2019s inequality we have\n\u03c3min(B TB) \u2264 \u03c3min(BTB \u2212 bibTi ) + \u03c3max(bibTi )\n\u2264 \u03c3min(ATA) + \u03c3max(aiaTi ) + \u03c3max(bibTi ) \u2264 \u03c3min(ATA) + 2B2\nhence |\u03c3min(A)2\u2212\u03c3min(B)2| \u2264 2B2, so addingLap( 4B 2\n) is ( /2)-differentially private.\nTo prove (ii), note that by standard tail-bounds on the Laplace distribution we have that Pr[Z < \u2212 4B2 ln(1/\u03b4)\n] \u2264 \u03b4 2 . Therefore, w.p. 1 \u2212 \u03b4/2 it holds that\nany matrix A that passes the if-test of the algorithm must have \u03c3min(A)2 > w2. Also note that a similar argument shows that for any 0 < \u03b2 < 1, any matrix A s.t. \u03c3min(A) 2 > w2 + 4B 2 ln(1/\u03b2) passes the if-condition of the algorithm w.p. 1\u2212 \u03b2."}, {"heading": "A.2. Omitted Preliminary Details", "text": "Linear Algebra and Pseudo-Inverses. Given a matrix M we denote its SVD as M = USV T with U and V being orthonormal matrices and S being a non-negative diagonal matrix whose entries are the singular values of M . We use \u03c3max(M) and \u03c3min(M) to denote the largest and smallest singular value resp. Despite the risk of confusion, we stick to the standard notation of using \u03c32 to denote the variance of a Gaussian, and use \u03c3j(M) to denote the j-th singular value of M . We use M+ to denote the Moore-Penrose inverse of M , defined as M+ = V S\u22121UT where S\u22121 is a matrix with S\u22121j,j = 1/Sj,j for any j s.t. Sj,j > 0.\nThe Gaussian Distribution. A univariate Gaussian N (\u00b5, \u03c32) denotes the Gaussian distribution whose mean is \u00b5 and variance \u03c32, with PDF(x) = ( \u221a\n2\u03c0\u03c32)\u22121 exp(\u2212x\u2212\u00b52\u03c32 ). Standard concentration bounds on Gaussians give that Pr[x > \u00b5 + 2\u03c3 \u221a ln(1/\u03bd)] < \u03bd for any \u03bd \u2208 (0, 1e ). A multivariate Gaussian N (\u00b5,\u03a3) for some positive semi-definite \u03a3 denotes the multivariate Gaussian distribution where the mean of the j-th coordinate is the \u00b5j and the co-variance between coordinates j and k is \u03a3j,k. The PDF of such Gaussian is defined only on the subspace colspan(\u03a3), where for every x \u2208 colspan(\u03a3) we have PDF(x) =(\n(2\u03c0)rank(\u03a3) \u00b7 d\u0303et(\u03a3) )\u22121/2 exp ( \u2212 12 (x \u2212\u00b5) T\u03a3+(x \u2212\u00b5) )\nand d\u0303et(\u03a3) is the multiplication of all non-zero singular values of \u03a3. A matrix Gaussian distribution denoted N (Ma\u00d7b, U, V ) has mean M , variance U on its rows and variance V on its columns. For full rank U and V it holds that PDFN (M,U,V )(X) = (2\u03c0)\u2212ab/2(det(U))\u2212b/2(det(V ))\u2212a/2 \u00b7 exp(\u2212 12 trace ( V \u22121(X \u2212M)TU\u22121(X \u2212M) ) ). In our case, we will only use matrix Gaussian distributions with N (Ma\u00d7b, Ia\u00d7a, V ) and so each row in this matrix is an i.i.d sample from a b-dimensional multivariate Gaussian N ((M)j\u2192, V ).\nWe will repeatedly use the rules regarding linear operations on Gaussians. That in, for any c, it holds that cN (\u00b5, \u03c32) = N (c \u00b7 \u00b5, c2\u03c32). For any C it holds that C \u00b7 N (\u00b5,\u03a3) = N (C\u00b5,C\u03a3CT). And for any C is holds thatN (M,U, V ) \u00b7 C = N (MC,U,CTV C). In particular, for any c (which can be viewed as a b\u00d71-matrix) it holds thatN (M,U, V ) \u00b7 c = N (Mc,U,cTV c) = N (Mc,cTV c \u00b7 U).\nWe will also require the following proposition.\nProposition A.2. Given \u03c32, \u03bb2 s.t. 1 \u2264 \u03c3 2\n\u03bb2 \u2264 c 2 for some\nconstant c, letX and Y be two random Gaussians s.t. X \u223c N (0, \u03c32) and Y \u223c N (0, \u03bb2). It follows that 1cPDFY (x) \u2264 PDFX(x) \u2264 cPDFcY (x) for any x. Corollary A.3. Under the same notation as in Proposition A.2, for any set S \u2282 R it holds that 1cPrx\u2190Y [x \u2208 S] \u2264 Prx\u2190X [x \u2208 S] \u2264 cPrx\u2190cY [x \u2208 S] =\ncPrx\u2190Y [x \u2208 S/c]\nProof. The proof is mere calculation.\nPDFX(x) PDFcY (x) =\n\u221a c2\u03bb2\n\u03c32 \u00b7\nexp(\u2212 x 2\n2\u03c32 )\nexp(\u2212 x 2\n2c2\u03bb2 )\n\u2264 c \u00b7 exp(x 2\n2 (\n1 c2\u03bb2 \u2212 1 \u03c32 )) \u2264 c \u00b7 exp(0) = c\nPDFX(x) PDFY (x) =\n\u221a \u03bb2\n\u03c32 \u00b7\nexp(\u2212 x 2\n2\u03c32 )\nexp(\u2212 x 2\n2\u03bb2 )\n\u2265 c\u22121 exp(x 2\n2 ( 1 \u03bb2 \u2212 1 \u03c32 )) \u2265 exp(0) c = c \u22121\nThe Tk-Distribution. The Tk-distribution, where k is referred to as the degrees of freedom of the distribution, denotes the distribution over the reals created by independently sampling Z \u223c N (0, 1) and \u2016\u03b6\u20162 \u223c \u03c72k, and taking the quantity Z\u221a\n\u2016\u03b6\u20162/k . Its PDF is given by\nPDFTk(x) \u221d ( 1 + x 2\nk )\u2212k+12 . It is a known fact that as k increases, Tk becomes closer and closer to a normal Gaussian. The T -distribution is often used to determine suitable bounds on the rate of converges, as we illustrate in Section A.3. As the T -distribution is heavy-tailed, existing tail bounds on the T -distribution (which are of the form: if \u03c4\u03bd = C \u221a k((1/\u03bd)2/k \u2212 1) for some constant C\nthen \u222b\u221e \u03c4\u03bd\nPDFTk(x)dx < \u03bd) are often cumbersome to work with. Therefore, in many cases in practice, it common to assume \u03bd = \u0398(1) (most commonly, \u03bd = 0.05) and use existing tail-bounds on normal Gaussians.\nDifferential Privacy facts. It is known (Dwork et al., 2006b) that if ALG outputs a vector in Rd such that for any A and A\u2032 it holds that \u2016ALG(A) \u2212 ALG(A\u2032)\u20161 \u2264 B, then adding Laplace noise Lap(1/ ) to each coordinate of the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and A\u2032 it holds that \u2016ALG(A)\u2212ALG(A\u2032)\u201622 \u2264 \u22062 then adding Gaussian noise N (0,\u22062 \u00b7 2 ln(2/\u03b4) 2 ) to each coordinate of the output of ALG(A) satisfies ( , \u03b4)-differential privacy.\nAnother standard result (Dwork et al., 2006a) gives that the composition of the output of a ( 1, \u03b41)-differentially private algorithm with the output of a ( 2, \u03b42)-differentially private algorithm results in a ( 1+ 2, \u03b41+\u03b42)-differentially private algorithm."}, {"heading": "A.3. Detailed Background on Ordinary Least Squares", "text": "For the unfamiliar reader, we give a short description of the model under which OLS operates as well as the confidence bounds one derives using OLS. This is by no means an ex-\nhaustive account of OLS and we refer the interested reader to (Rao, 1973; Muller & Stewart, 2006).\nGiven n observations {(xi, yi)}ni=1 where for all i we have xi \u2208 Rp and yi \u2208 R, we assume the existence of a pdimensional vector \u03b2 \u2208 Rp s.t. the label yi was derived by yi = \u03b2\nTxi + ei where ei \u223c N (0, \u03c32) independently (also known as the homoscedastic Gaussian model). We use the matrix notation whereX denotes the (n\u00d7p)-matrix whose rows are xi, and use y,e \u2208 Rn to denote the vectors whose i-th entry is yi and ei resp. To simplify the discussion, we assume X has full rank.\nThe parameters of the model are therefore \u03b2 and \u03c32, which we set to discover. To that end, we minimize minz \u2016y \u2212 Xz\u20162 and solve\n\u03b2\u0302 = (XTX)\u22121XTy = (XTX)\u22121XT(X\u03b2+e) = \u03b2+X+e\nAs e \u223c N (0n, \u03c32In\u00d7n), it holds that \u03b2\u0302 \u223c N (\u03b2, \u03c32(XTX)\u22121), or alternatively, that for every coordinate j it holds that \u03b2\u0302j = eTj \u03b2\u0302 \u223c N (\u03b2j , \u03c32(XTX) \u22121 j,j ). Hence we get \u03b2\u0302j\u2212\u03b2j \u03c3 \u221a\n(XTX)\u22121j,j\n\u223c N (0, 1). In addition, we de-\nnote the vector\n\u03b6 = y\u2212X\u03b2\u0302 = (X\u03b2 +e)\u2212X(\u03b2 +X+e) = (I \u2212XX+)e\nand since XX+ is a rank-p (symmetric) projection matrix, we have \u03b6 \u223c N (0, \u03c32(I \u2212 XX+)). Therefore, \u2016\u03b6\u20162 is equivalent to summing the squares of (n\u2212 p) i.i.d samples from N (0, \u03c32). In other words, the quantity \u2016\u03b6\u20162/\u03c32 is sampled from a \u03c72-distribution with (n \u2212 p) degrees of freedom.\nWe sidetrack from the OLS discussion to give the following bounds on the l2-distance between \u03b2 and \u03b2\u0302 , as the next claim shows.\nClaim A.4. For any 0 < \u03bd < 1/2, the following holds w.p. \u2265 1\u2212\u03bd over the randomness of the model (the randomness over e)\n\u2016\u03b2 \u2212 \u03b2\u0302\u20162 = \u2016X+e\u20162 = O ( \u03c32 log(p/\u03bd) \u00b7 \u2016X+\u20162F ) (6)\n\u2016\u03b2\u0302\u20162 = \u2016\u03b2 +X+e\u20162 = O( ( \u2016\u03b2\u2016+ \u03c3 \u00b7 \u2016X+\u2016F \u00b7 \u221a log(p/\u03bd) )2 )\u2223\u2223\u2223 1n\u2212p\u2016\u03b6\u20162 \u2212 \u03c32\u2223\u2223\u2223 = O(\u221a ln(1/\u03bd)n\u2212p )\nProof. Since e \u223c N (0n, \u03c32In\u00d7n) then X+e \u223c N (0n, \u03c32(XTX)\u22121). Denoting the SVD decomposition (XTX)\u22121 = V SV T with S denoting the diagonal matrix whose entries are \u03c3\u22122max(X), . . . , \u03c3 \u22122 min(X), we have that V TX+e \u223c N (0n, \u03c32S). And so, each coordinate of V TX+e is distributed like an i.i.d Gaussian. So\nw.p. \u2265 1 \u2212 \u03bd/2 non of these Gaussians is a factor of O(\u03c3 \u221a ln(p/\u03bd)) greater than its standard deviation. And so w.p. \u2265 1 \u2212 \u03bd/2 it holds that \u2016X+e\u20162 = \u2016V TX+e\u20162 \u2264 O(\u03c32 log(p/\u03bd) (\u2211 i \u03c3 \u22122 i (X) ) ). Since \u2211 i \u03c3 \u22122 i (X) = trace((XTX)\u22121) = trace(X+(X+)T) = \u2016X+\u20162F , the bound of (6) is proven.\nThe bound on \u2016\u03b2\u0302\u20162 is an immediate corollary of (6) using the triangle inequality.8 The bound on \u2016\u03b6\u20162 follows from tail bounds on the \u03c72n\u2212p distribution, as detailed in Section 2.\nReturning to OLS, it is important to note that \u03b2\u0302 and \u03b6 are independent of one another. (Note, \u03b2\u0302 depends solely on X+e = (X+X)X+e = X+PUe, whereas \u03b6 depends on (I \u2212XX+)e = PU\u22a5e. As e is spherically symmetric, the two projections are independent of one another and so \u03b2\u0302 is independent of \u03b6 .) As a result of the above two calculations, we have that the quantity\nt\u03b2\u0302j (\u03b2j) def = \u03b2\u0302j\u2212\u03b2j\u221a (XTX)\u22121j,j \u00b7 \u2016\u03b6\u2016\u221a n\u2212p = \u03b2\u0302j\u2212\u03b2j \u03c3 \u221a (XTX)\u22121j,j\n/ \u2016\u03b6\u2016\n\u03c3 \u221a n\u2212p\nis distributed like a T -distribution with (n \u2212 p) degrees of freedom. Therefore, we can compute an exact probability estimation for this quantity. That is, for any measurable S \u2282 R we have Pr [ \u03b2\u0302 and \u03b6 satisfying t\u03b2\u0302j (\u03b2j) \u2208 S ] = \u222b S PDFTn\u2212p(x)dx\nThe importance of the t-value t(\u03b2j) lies in the fact that it can be fully estimated from the observed data X and y (for any value of \u03b2j), which makes it a pivotal quantity. Therefore, given X and y , we can use t(\u03b2j) to describe the likelihood of any \u03b2j \u2014 for any z \u2208 R we can now give an estimation of how likely it is to have \u03b2j = z (which is PDFTn\u2212p(t(z))). The t-values enable us to perform multitude of statistical inferences. For example, we can say which of two hypotheses is more likely and by how much (e.g., we are 5-times more likely that the hypothesis \u03b2j = 3 is true than the hypothesis \u03b2j = 14 is true); we can compare between two coordinates j and j\u2032 and report we are more confident that \u03b2j > 0 than \u03b2j\u2032 > 0; or even compare among the t-values we get across multiple datasets (such as the datasets we get from subsampling rows from a single dataset).\nIn particular, we can use t(\u03b2j) to \u03b1-reject unlikely values of \u03b2j . Given 0 < \u03b1 < 1, we denote c\u03b1 as the number for which the interval (\u2212c\u03b1, c\u03b1) contains a probability mass of 1 \u2212 \u03b1 from the Tn\u2212p-distribution. And so we derive a\n8Observe, though e is spherically symmetric, and is likely to be approximately-orthogonal to \u03b2 , this does not necessarily hold for X+e which isn\u2019t spherically symmetric. Therefore, we result to bounding the l2-norm of \u03b2\u0302 using the triangle bound.\ncorresponding confidence interval I\u03b1 centered at \u03b2\u0302j where \u03b2j \u2208 I\u03b1 with confidence of level of 1\u2212 \u03b1.\nWe comment as to the actual meaning of this confidence interval. Our analysis thus far applied w.h.p to a vector y derived according to this model. Such X and y will result in the quantity t\u03b2\u0302j (\u03b2j) being distributed like a Tn\u2212pdistribution \u2014 where \u03b2j is given as the model parameters and \u03b2\u0302j is the random variable. We therefore have that guarantee that for X and y derived according to this model, the\nevent E\u03b1 def = \u03b2\u0302j \u2208 ( \u03b2j \u00b1 c\u03b1 \u00b7 \u221a (XTX)\u22121j,j \u00b7 \u2016\u03b6\u20162 n\u2212p ) hap-\npens w.p. 1 \u2212 \u03b1. However, the analysis done over a given dataset X and y (once y has been drawn) views the quantity t\u03b2\u0302j (\u03b2j) with \u03b2\u0302j given and \u03b2j unknown. Therefore the event E\u03b1 either holds or does not hold. That is why the alternative terms of likelihood or confidence are used, instead of probability. We have a confidence level of 1 \u2212 \u03b1 that indeed \u03b2j \u2208 \u03b2\u0302j\u00b1c\u03b1 \u00b7 \u221a (XTX)\u22121j,j \u00b7 \u2016\u03b6\u20162 n\u2212p , because this event does happen in 1\u2212\u03b1 fraction of all datasets generated according to our model.\nRejecting the Null Hypothesis. One important implication of the quantity t(\u03b2j) is that we can refer specifically to the hypothesis that \u03b2j = 0, called the null hypothesis. This quantity, t0 def = t\u03b2\u0302j (0) = \u03b2\u0302j \u221a n\u2212p \u2016\u03b6\u2016 \u221a\n(XTX)\u22121j,j\n, represents how\nlarge is \u03b2\u0302j relatively to the empirical estimation of standard deviation \u03c3. Since it is known that as the number of degrees of freedom of a T -distribution tends to infinity then the T - distribution becomes a normal Gaussian, it is common to think of t0 as a sample from a normal Gaussian N (0, 1). This allows us to associate t0 with a p-value, estimating the event \u201c\u03b2j and \u03b2\u0302j have different signs.\u201d Formally, we define p0 = \u222b\u221e |t0| 1\u221a 2\u03c0 e\u2212x\n2/2dx. It is common to reject the null hypothesis when p0 is sufficiently small (typically, below 0.05).9\nSpecifically, given \u03b1 \u2208 (0, 1/2), we say we \u03b1-reject the null hypothesis if p0 < \u03b1. Let \u03c4\u03b1 be the number s.t. \u03a6(\u03c4\u03b1) = \u222b\u221e \u03c4\u03b1 1\u221a 2\u03c0 e\u2212x 2/2dx = \u03b1. (Standard bounds\ngive that \u03c4\u03b1 < 2 \u221a\nln(1/\u03b1).) This means we \u03b1-reject the null hypothesis if t0 > \u03c4\u03b1 or t0 < \u2212\u03c4\u03b1, meaning if |\u03b2\u0302j | > \u03c4\u03b1 \u221a (XTX)\u22121j,j \u2016\u03b6\u2016\u221a n\u2212p .\nWe can now lower bound the number of i.i.d sample points needed in order to \u03b1-reject the null hypothesis. This bound will be our basis for comparison \u2014 between standard OLS and the differentially private version.10 9Indeed, it is more accurate to associate with t0 the value\u222b\u221e |t0|\nPDFTn\u2212p(x)dx and check that this value is < \u03b1. However, as most uses take \u03b1 to be a constant (often \u03b1 = 0.05), asymptotically the threshold we get for rejecting the null hypothesis are the same.\n10This theorem is far from being new (except for maybe fo-\nTheorem A.5 (Theorem 2.2 restated.). Fix any positive definite matrix \u03a3 \u2208 Rp\u00d7p and any \u03bd \u2208 (0, 12 ). Fix parameters \u03b2 \u2208 Rp and \u03c32 and a coordinate j s.t. \u03b2j 6= 0. Let X be a matrix whose n rows are i.i.d samples from N (0,\u03a3), and y be a vector where yi \u2212 (X\u03b2)i is sampled i.i.d from N (0, \u03c32). Fix \u03b1 \u2208 (0, 1). Then w.p. \u2265 1 \u2212 \u03bd we have that the (1 \u2212 \u03b1)-confidence interval is of length O(c\u03b1 \u221a \u03c32/(n\u03c3min(\u03a3))) provided n \u2265 C1(p + ln(1/\u03bd)) for some sufficiently large constant C1. Furthermore, there exists a constant C2 such that w.p. \u2265 1 \u2212 \u03b1 \u2212 \u03bd we (correctly) reject the null hypothesis provided\nn \u2265 max { C1(p+ ln(1/\u03bd)), C2 \u03c32\n\u03b22j \u00b7 c\n2 \u03b1 + \u03c4 2 \u03b1\n\u03c3min(\u03a3)\n}"}, {"heading": "Here c\u03b1 denotes the number for which\u222b c\u03b1", "text": "\u2212c\u03b1 PDFTn\u2212p(x)dx = 1 \u2212 \u03b1. (If we are content with approximating Tn\u2212p with a normal Gaussian than one can set c\u03b1 \u2248 \u03c4\u03b1 < 2 \u221a ln(1/\u03b1).)\nProof. The discussion above shows that w.p. \u2265 1 \u2212 \u03b1 we have |\u03b2j \u2212 \u03b2\u0302j | \u2264 c\u03b1 \u221a (XTX)\u22121j,j \u2016\u03b6\u20162 n\u2212p ; and in order to \u03b1-reject the null hypothesis we must have |\u03b2\u0302j | > \u03c4\u03b1 \u221a (XTX)\u22121j,j \u2016\u03b6\u20162 n\u2212p . Therefore, a sufficient condition for OLS to \u03b1-reject the null-hypothesis is to have n large\nenough s.t. |\u03b2j | > (c\u03b1 + \u03c4\u03b1) \u221a (XTX)\u22121j,j \u2016\u03b6\u20162 n\u2212p . We therefore argue that w.p.\u2265 1\u2212 \u03bd this inequality indeed holds.\nWe assume each row of X i.i.d vector xi \u223c N (0p,\u03a3), and recall that according to the model \u2016\u03b6\u20162 \u223c \u03c32\u03c72(n \u2212 p). Straightforward concentration bounds on Gaussians and on the \u03c72-distribution give: (i) W.p. \u2264 \u03b1 it holds that \u2016\u03b6\u2016 > \u03c3 ( \u221a n\u2212 p+ 2 ln(2/\u03b1))). (This is part of the standard OLS analysis.) (ii) W.p. \u2264 \u03bd it holds that \u03c3min(XTX) \u2264 \u03c3min(\u03a3)( \u221a n \u2212 ( \u221a p+ \u221a 2 ln(2/\u03bd)))2. (Rudelson & Vershynin, 2009) Therefore, due to the lower bound n = \u2126(p + ln(1/\u03bd)), w.p.\u2265 1 \u2212 \u03bd \u2212 \u03b1 we have that none of these events hold. In such a case we have\u221a\n(XTX)\u22121j,j \u2264 \u221a \u03c3max((XTX)\u22121) = O(\n1\u221a n\u03c3min(\u03a3) )\nand \u2016\u03b6\u2016 = O(\u03c3 \u221a n\u2212 p). This implies that the confidence interval of level 1 \u2212 \u03b1 has length of c\u03b1 \u221a (XTX)\u22121j,j \u00b7 \u2016\u03b6\u20162 n\u2212p = O ( c\u03b1 \u221a \u03c32\nn\u03c3min(\u03a3)\n) ; and that in\norder to \u03b1-reject that null-hypothesis it suffices to have |\u03b2j | = \u2126 ( (c\u03b1 + \u03c4\u03b1) \u221a \u03c32\nn\u03c3min(\u03a3)\n) . Plugging in the lower\nbound on n, we see that this inequality holds.\nWe comment that for sufficiently large constants C1, C2,\ncusing on the setting where every row in X is sampled from an i.i.d multivariate Gaussians), it is just stated in a non-standard way, discussing solely the power of the t-test in OLS. For further discussions on sample size calculations see (Muller & Stewart, 2006).\nit holds that all the constants hidden in the O- and \u2126notations of the proof are close to 1. I.e., they are all within the interval (1 \u00b1 \u03b7) for some small \u03b7 > 0 given C1, C2 \u2208 \u2126(\u03b7\u22122)."}, {"heading": "B. Projecting the Data using Gaussian Johnson-Lindenstrauss Transform", "text": ""}, {"heading": "B.1. Main Theorem Restated and Further Discussion", "text": "Theorem B.1 (Theorem 3.1 restated.). Let X be a n \u00d7 p matrix, and parameters \u03b2 \u2208 Rp and \u03c32 are such that we generate the vector y = X\u03b2 + e with each coordinate of e sampled independently from N (0, \u03c32). Assume \u03c3min(X) \u2265 C \u00b7 w and that n is sufficiently large s.t. all of the singular values of the matrix [X;y] are greater than C \u00b7 w for some large constant C, and so Algorithm 1 projects the matrixA = [X;y] without altering it, and publishes [RX;Ry]. Fix \u03bd \u2208 (0, 1/2) and r = p + \u2126(ln(1/\u03bd)). Fix coordinate j. Then w.p. \u2265 1\u2212 \u03bd we have that deriving \u03b2\u0303 , \u03b6\u0303 and \u03c3\u03032 as follows\n\u03b2\u0303 = (XTRTRX)\u22121(RX)T(Ry) = \u03b2 + (RX)+Re\n\u03b6\u0303 = 1\u221a r Ry \u2212 1\u221a r (RX)\u03b2\u0303\n= 1\u221a r\n( I \u2212 (RX)(XTRTRX)\u22121(RX)T) ) Re\n\u03c3\u03032 = r\nr \u2212 p \u2016\u03b6\u0303\u20162\nthen the pivot quantity\nt\u0303(\u03b2j) = \u03b2\u0303j \u2212 \u03b2j \u03c3\u0303 \u221a\n(XTRTRX)\u22121j,j\nhas a distribution D satisfying e\u2212aPDFTr\u2212p(x) \u2264 PDFD(x) \u2264 eaPDFTr\u2212p(e\u2212ax) for any x \u2208 R, where we denote a = r\u2212pn\u2212p .\nComparison with Existing Bounds. Sarlos\u2019 work (2006) utilizes the fact that when r, the numbers of rows in R, is large enough, then 1\u221a\nr R is a Johnson-Lindenstrauss\nmatrix. Specifically, given r and \u03bd \u2208 (0, 1) we denote \u03b7 = \u2126(\n\u221a p ln(p) ln(1/\u03bd)\nr ), and so r = O( p ln(p) ln(1/\u03bd) \u03b72 ).\nLet us denote \u03b2\u0303 = arg minz 1r\u2016RXz \u2212 Ry\u2016 2. In this setting, Sarlos\u2019 work (Sarlo\u0301s, 2006) (Theorem 12(3)) guarantees that w.p. \u2265 1 \u2212 \u03bd we have \u2016\u03b2\u0302 \u2212 \u03b2\u0303\u20162 \u2264 \u03b7\u2016\u03b6\u2016/\u03c3min(X) = O (\u221a p log(p) log(1/\u03bd) r\u03c3min(XTX) \u2016\u03b6\u2016 ) . Na\u0131\u0308vely bounding |\u03b2\u0302j \u2212 \u03b2\u0303j | \u2264 \u2016\u03b2\u0302 \u2212 \u03b2\u0303\u2016 and using the confidence interval for \u03b2\u0302j \u2212 \u03b2j from Section A.311\n11Where we approximate c\u03b1, the tail bound of the Tn\u2212pdistribution with the tail bound on a Gaussian, i.e., use the approximation c\u03b1 \u2248 O( \u221a ln(1/\u03b1)).\ngives a confidence interval of level 1 \u2212 (\u03b1 + \u03bd) centered at \u03b2\u0303j with length of O (\u221a p ln(p) log(1/\u03bd) r\u03c3min(XTX) \u2016\u03b6\u2016 ) +\nO (\u221a (XTX)\u22121j,j log(1/\u03b1) n\u2212p \u2016\u03b6\u2016 ) =\nO (\u221a\np ln(p) log(1/\u03bd)+log(1/\u03b1) r\u03c3min(XTX)\n\u2016\u03b6\u2016 )\n. This implies that our confidence interval has decreased its degrees of freedom from n\u2212 p to roughly r/p ln(p), and furthermore, that it no longer depends on (XTX)\u22121j,j but rather on 1/\u03c3min(X\nTX). It is only due to the fact that we rely on Gaussians and by mimicking carefully the original proof that we can deduce that the t\u0303-value has (roughly) r \u2212 p degrees of freedom and depends solely on (XTX)\u22121j,j .\n(In the worst case, we have that (XTX)\u22121j,j is proportional to \u03c3min(XTX)\u22121, but it is not uncommon to have matrices where the former is much larger than the latter.) As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2dp of the linear regression give a data-independent12 bound of \u2016\u03b2dp \u2212 \u03b2\u0302\u2016 = O\u0303(p/ ). Such bounds are harder to compare with the interval length given by Corollary 3.2. Indeed, as we discuss in Section 3 under \u201cRejecting the null-hypothesis,\u201d enough samples from a multivariate Gaussian whose covariance-matrix is well conditioned give a bound which is well below the worstupper bound of O(p/ ). (Yet, it is possible that these techniques also do much better on such \u201cwell-behaved\u201d data.) What the works of Sarlos and alternative works regrading differentially private linear regression do not take into account are questions such as generating a likelihood for \u03b2j nor do they discuss rejecting the null hypothesis."}, {"heading": "B.2. Proof of Theorem 3.1", "text": "We now turn to our analysis of \u03b2\u0303 and \u03b6\u0303 , where our goal is to show that the distribution of the t\u0303-values as specified in Theorem 3.1 is well-approximated by the Tr\u2212pdistribution. For now, we assume the existence of fixed vectors \u03b2 \u2208 Rp and e \u2208 Rn s.t. y = X\u03b2 + e. (Later, we will return to the homoscedastic model where each coordinate of e is sampled i.i.d from N (0, \u03c32) for some \u03c32.) In other words, we first examine the case where R is the sole source of randomness in our estimation. Based on the assumption that e is fixed, we argue the following.\nClaim B.2. In our model, given X and the output M = RX , we have that \u03b2\u0303 \u223c N ( \u03b2 +X+e, \u2016PU\u22a5e\u20162(MTM)\u22121 ) and \u03b6\u0303 \u223c\nN ( 0n, \u2016P U\u22a5e\u2016 2 r (Ir\u00d7r \u2212M(M TM)\u22121MT) ) . Where PU\u22a5 denotes the projection operator onto the subspace orthogonal to colspan(X); i.e., PU = XX+ and PU\u22a5 = (Ir\u00d7r \u2212XX+).\n12In other words, independent of X,\u03b6 .\nProof. The matrix R is sampled from N (0r\u00d7p, Ir\u00d7r, Ip\u00d7p). Given X and RX = M , we learn the projection of each row in R onto the subspace spanned by the columns of X . That is, denoting uT as the i-th row of R and vT as the i-th row of M , we have that XTu = v . Recall, initially u \u223c N (0n, In\u00d7n) \u2013 a spherically symmetric Gaussian. As a result, we can denote u = PUu \u00d7 PU\u22a5u where the two projections are independent samples from N (0n, PU ) and N (0n, PU\u22a5) resp. However, once we know that v = XTu we have that PUu = X(X\nTX)\u22121XTu = X(XTX)\u22121v so we learn PUu exactly, whereas we get no information about PU\u22a5 so PU\u22a5u is still sampled from a Gaussian N (0n, PU\u22a5). As we know for each row of R that uTPU = vTX+, we therefore have that\nR = RPU +RPU\u22a5 = MX + +RPU\u22a5\nwhere RPU\u22a5 \u223c N (0r\u00d7n, Ir\u00d7r, PU\u22a5). From here on, we just rely on the existing results about the linearity of Gaussians.\nR \u223c N (MX+, Ir\u00d7r, PU\u22a5) \u21d2 Re \u223c N (MX+e, \u2016PU\u22a5e\u20162Ir\u00d7r) \u21d2M+Re \u223c N (X+e, \u2016PU\u22a5e\u20162(MTM)\u22121)\nso \u03b2\u0303 = \u03b2 + M+Re implies \u03b2\u0303 \u223c N (\u03b2 + X+e, \u2016PU\u22a5e\u20162(MTM)\u22121). And as \u03b6\u0303 = 1\u221a r (Ir\u00d7r \u2212 M(MTM)\u22121MT)Re then we have \u03b6\u0303 \u223c N (0r, \u2016P U\u22a5e\u2016 2 r (Ir\u00d7r \u2212 MM +)) as (Ir\u00d7r \u2212MM+)M = 0r\u00d7p.\nClaim B.2 was based on the assumption that e is fixed. However, given X and y there are many different ways to assign vectors \u03b2 and e s.t. y = X\u03b2 + e. However, the distributions we get in Claim B.2 are unique. To see that, recall Equations (1) and (2): \u03b2 + X+e = X+y = \u03b2\u0302 and PU\u22a5e = PU\u22a5y = (I \u2212 XX+)y = \u03b6 . We therefore have \u03b2\u0303 \u223c N (\u03b2\u0302 , \u2016\u03b6\u20162(MTM)\u22121) and \u03b6\u0303 \u223c N (0n, \u2016\u03b6\u2016 2\nr (I \u2212 MM+)). We will discuss this further, in Section 4, where we will not be able to better analyze the explicit distributions of our estimators. But in this section, we are able to argue more about the distributions of \u03b2\u0303 and \u03b6\u0303 .\nSo far we have considered the case that e is fixed, whereas our goal is to argue about the case where each coordinate of e is sampled i.i.d from N (0, \u03c32). To that end, we now switch to an intermediate model, in which PUe is sampled from a multivariate Gaussian while PU\u22a5e is fixed as some arbitrary vector of length l. Formally, let Dl denote the distribution where PUe \u223c N (0, \u03c32PU ) and PU\u22a5e is fixed as some specific vector whose length is denoted by \u2016PU\u22a5e\u2016 = l. Claim B.3. Under the same assumptions as in Claim B.2, given that e \u223c Dl, we have that\n\u03b2\u0303 \u223c N ( \u03b2, \u03c32(XTX)\u22121 + l2(MTM)\u22121 ) and\n\u03b6\u0303 \u223c N ( 0n, l2 r (I \u2212MM +) ) .\nProof. Recall, \u03b2\u0303 = \u03b2 + M+Re = \u03b2 + M+(MX+ + RPU\u22a5)e = \u03b2 + X\n+e + M+R(PU\u22a5e). Now, under the assumption e \u223c Dl we have that \u03b2 is the sum of two independent Gaussians:\n\u03b2 +X+e \u223c N (\u03b2, \u03c32 ( X+ \u00b7 PU \u00b7 (X+)T ) ) = N (\u03b2, \u03c32(XTX)\u22121) RPU\u22a5e \u223c N (0r, \u2016PU\u22a5e\u20162Ir\u00d7r) \u21d2M+Re \u223c N (0p, \u2016PU\u22a5e\u20162(MTM)\u22121)\nSumming the two independent Gaussians\u2019 means and variances gives the distribution of \u03b2\u0303 . Furthermore, in Claim B.2 we have already established that for any fixed e we have \u03b6\u0303 \u223c N ( 0n, \u2016P U\u22a5e\u2016 2 r (I \u2212MM +) ) . Hence, for\ne \u223c Dl we still have \u03b6\u0303 \u223c N ( 0n, l2 r (I \u2212MM +) )\n. (It is easy to verify that the same chain of derivations is applicable when e \u223c Dl.)\nCorollary B.4. Given that e \u223c Dl we have that \u03b2\u0303j \u223c N (\u03b2j , \u03c32(XTX)\u22121j,j + l2(MTM) \u22121 j,j ) for any coordinate j, and that \u2016\u03b6\u0303\u20162 \u223c l 2\nr \u00b7 \u03c7 2 r\u2212p.\nProof. The corollary follows immediately from the fact that \u03b2j = eTj \u03b2\u0303 , and from the definition of the \u03c7\n2- distribution, as \u03b6\u0303 is a spherically symmetric Gaussian defined on the subspace colspan(M)\u22a5 of dimension r \u2212 p.\nTo continue, we need the following claim.\nClaim B.5. GivenX andM = RX , and given that e \u223c Dl we have that \u03b2\u0303 and \u03b6\u0303 are independent.\nProof. Recall, \u03b2\u0303 = \u03b2 + X+e + M+R(PU\u22a5e). And so, given X , M and a specific vector PU\u22a5e we have that the distribution of \u03b2\u0303 depends on (i) the projection of e on U = colspan(X) and on (ii) the projection of each row in R onto U\u0303 = colspan(M). The distribution of \u03b6\u0303 = 1\u221a\nr PU\u0303\u22a5Re = 1\u221a r PU\u0303\u22a5(MX + + RPU\u22a5)e = 1\u221a r PU\u0303\u22a5RPU\u22a5e depends on (i) the projection of e onto U \u22a5 (which for the time being is fix to some specific vector of length l) and on (ii) the projection of each row in R onto U\u0303\u22a5. Since PUe is independent from PU\u22a5e, and since for any rowuT ofRwe have thatPU\u0303u is independent ofPU\u0303\u22a5u, and since e and R are chosen independently, we have that \u03b2\u0303 and \u03b6\u0303 are independent.\nFormally, consider any pair of coordinates \u03b2\u0303j and \u03b6\u0303k, and we have\n\u03b2\u0303j \u2212 \u03b2j = eTjX+e + eTjM+(RPU\u22a5e)\n\u03b6\u0303k = e T kPU\u0303\u22a5(RPU\u22a5e)\nRecall, we are givenX andM = RX . Therefore, we know PU and PU\u0303 . And so\nCov[\u03b2\u0303j , \u03b6\u0303k]\n= E[(\u03b2\u0303j \u2212 \u03b2j)(\u03b6\u0303k \u2212 0)] = E[eTjX +e(RPU\u22a5e) TPU\u0303\u22a5ek]\n+ E[eTjM +(RPU\u22a5e)(RPU\u22a5e) TPU\u0303\u22a5ek]\n= eTjX +E[eeTPU\u22a5 ]E[R T]PU\u0303\u22a5ek\n+ eTjM +E[(RPU\u22a5e)(RPU\u22a5e) T]PU\u0303\u22a5ek\n= eTjX +E[eeTPU\u22a5 ] ( (MX+)T + E[(RPU\u22a5) T] ) PU\u0303\u22a5ek\n+ eTjM + ( \u2016PU\u22a5e\u20162Ir\u00d7r ) PU\u0303\u22a5ek\n= eTjX +E[eeTPU\u22a5 ](X +)T ( MTPU\u0303\u22a5 ) ek + 0\n+ l2 \u00b7 eTj ( M+PU\u0303\u22a5 ) ek\n= 0 + 0 + 0 = 0\nAnd as \u03b2\u0303 and \u03b6\u0303 are Gaussians, having their covariance = 0 implies independence.\nHaving established that \u03b2\u0303 and \u03b6\u0303 are independent Gaussians and specified their distributions, we continue with the proof of Theorem 3.1. We assume for now that there exists some small a > 0 s.t.\nl2(MTM)\u22121j,j \u2264 \u03c3 2(XTX)\u22121j,j + l 2(MTM)\u22121j,j \u2264 e2a \u00b7 l2(MTM)\u22121j,j (7)\nThen, due to Corollary A.3, denoting the distributions N1 = N (0, l2(MTM)\u22121j,j ) and N2 = N (0, \u03c32(XTX)\u22121j,j + l2(MTM) \u22121 j,j ), we have that for any S \u2282 R it holds that13\ne\u2212aPr\u03b2\u0303j\u223cN1 [S] \u2264 Pr\u03b2\u0303j\u223cN2 [S] \u2264 e aPr\u03b2\u0303j\u223cN1 [S/e a]\n(8)\nMore specifically, denote the function\nt\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) = \u03c8 \u2212 \u03b2j \u2016\u03be\u2016 \u221a\nr r\u2212p (M TM)\u22121j,j\n= \u03c8 \u2212 \u03b2j l \u221a\n(MTM)\u22121j,j\n/\u2016\u03be\u2016\u221a rr\u2212p l\nand observe that when we sample \u03c8,\u03be independently s.t. \u03c8 \u223c N (\u03b2j , l2(MTM)\u22121j,j ) and \u2016\u03be\u20162 \u223c l 2 r \u03c7 2 r\u2212p then t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) is distributed like a T -distribution with r \u2212 p 13In fact, it is possible to use standard techniques from differential privacy, and argue a similar result \u2014 that the probabilities of any event that depends on some function f(\u03b2j) under \u03b2j \u223c N1 and under \u03b2j \u223c N2 are close in the differential privacy sense.\ndegrees of freedom. And so, for any \u03c4 > 0 we have that under such way to sample \u03c8,\u03be we have Pr[t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) > \u03c4 ] = 1\u2212 CDFTr\u2212p(\u03c4).\nFor any \u03c4 \u2265 0 and for any non-negative real value z let S\u03c4z denote the suitable set of values s.t.\nPr\u03c8\u223cN (\u03b2j , l 2(MTM)\u22121j,j )\n\u2016\u03be\u20162\u223c l 2\nr \u03c7 2 r\u2212p\n [t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) > \u03c4 ]\n= \u221e\u222b 0 PDF l2 r \u03c7 2 r\u2212p (z) \u00b7 Pr {\u03c8\u2212\u03b2j\u223cN (0, l2(MTM)\u22121j,j )} [S\u03c4z ] dz\nThat is, S\u03c4z = ( \u03c4 \u00b7 z \u221a r r\u2212p (M TM)\u22121j,j , \u221e ) .\nWe now use Equation (8) (Since N (0, l2(MTM)\u22121j,j ) is precisely N1) to deduce that Pr\u03c8\u223cN (\u03b2j , l 2(MTM)\u22121j,j+\u03c3 2(XTX)\u22121j,j )\n\u2016\u03be\u20162\u223c l 2\nr \u03c7 2 r\u2212p\n [t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) > \u03c4 ]\n= \u222b \u221e 0 PDF l2 r \u03c7 2 r\u2212p (z) Pr \u03c8 \u2212 \u03b2j \u223c N (0, l2(MTM)\u22121j,j + \u03c32(XTX) \u22121 j,j ) [S\u03c4z ]dz\n\u2264 ea \u222b \u221e\n0 PDF l2 r \u03c7 2 r\u2212p (z) Pr \u03c8\u2212\u03b2j\u223cN (0, l2(MTM)\u22121j,j )\n[S\u03c4z /ea]dz\n(\u2217) = ea \u222b \u221e 0 PDF l2 r \u03c7 2 r\u2212p (z) Pr \u03c8\u2212\u03b2j\u223cN (0, l2(MTM)\u22121j,j ) [S\u03c4/e a z ]dz = eaPr\u03c8\u223cN (\u03b2j , l 2(MTM)\u22121j,j )\n\u2016\u03be\u20162\u223c l 2\nr \u03c7 2 r\u2212p\n [t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) > \u03c4/ea]\n= ea ( 1\u2212 CDFTr\u2212p(\u03c4/ea) ) where the equality (\u2217) follows from the fact that S\u03c4z /c = S \u03c4/c z for any c > 0, since it is a non-negative interval. Analogously, we can also show that Pr\u03c8\u223cN (\u03b2j , l 2(MTM)\u22121j,j+\u03c3 2(XTX)\u22121j,j )\n\u2016\u03be\u20162\u223c l 2\nr \u03c7 2 r\u2212p\n [t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) > \u03c4 ]\n\u2265 e\u2212aPr\u03c8\u223cN (\u03b2j , l 2(MTM)\u22121j,j )\n\u2016\u03be\u20162\u223c l 2\nr \u03c7 2 r\u2212p\n [t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) > \u03c4 ]\n= e\u2212a ( 1\u2212 CDFTr\u2212p(\u03c4) ) In other words, we have just shown that for any interval I = (\u03c4,\u221e) with \u03c4 \u2265 0 we have that Pr\u03c8\u223cN (\u03b2j , l 2(MTM)\u22121j,j+\u03c3 2(XTX)\u22121j,j )\n\u2016\u03be\u20162\u223c l 2\nr \u03c7 2 r\u2212p\n [t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) \u2208 I]\nis lower bounded by ea \u222b I PDFTr\u2212p(z)dz and upper\nbounded by ea \u222b\nI/ea PDFTr\u2212p(z)dz. We can now repeat\nthe same argument for I = (\u03c41, \u03c42) with 0 \u2264 \u03c41 < \u03c42 (using an analogous definition of S\u03c41,\u03c42z ), and again\nfor any I = (\u03c41, \u03c42) with \u03c41 < \u03c42 \u2264 0, and deduce that the PDF of the function t\u0303(\u03c8, \u2016\u03be\u2016, \u03b2j) at x \u2014 where we sample \u03c8 \u223c N (\u03b2j , l2(MTM)\u22121j,j + \u03c32(XTX) \u22121 j,j ) and \u2016\u03be\u20162 \u223c l 2\nr \u03c7 2 r\u2212p independently \u2014 lies in the range(\ne\u2212aPDFTr\u2212p(x), e aPDFTr\u2212p(x/e\na) ) . And so, using\nCorollary B.4 and Claim B.5, we have that when e \u223c Dl, the distributions of \u03b2\u0303j and \u2016\u03b6\u0303\u20162 are precisely as stated above, and so we have that the distribution of t\u0303(\u03b2j) def = t\u0303(\u03b2\u0303j , \u2016\u03b6\u0303\u2016, \u03b2j) has a PDF that at the point x is \u201csandwiched\u201d between e\u2212aPDFTr\u2212p(x) and e aPDFTr\u2212p(x/e a).\nNext, we aim to argue that this characterization of the PDF of t\u0303(\u03b2j) still holds when e \u223c N (0n, \u03c32In\u00d7n). It would be convenient to think of e as a sample in N (0n, \u03c32PU ) \u00d7N (0n, \u03c32PU\u22a5). (So while in Dl we have PUe \u223c N (0n, \u03c32PU ) butPU\u22a5e is fixed, now bothPUe and PU\u22a5e are sampled from spherical Gaussians.) The reason why the above still holds lies in the fact that t\u0303(\u03b2j) does not depend on l. In more details:\nPre\u223cN (0n,\u03c32In\u00d7n) [ t\u0303(\u03b2j) \u2208 I ] =\n\u222b v Pre\u223cN (0n,\u03c32In\u00d7n) [ t\u0303(\u03b2j) \u2208 I | PU\u22a5e = v ] PDFP U\u22a5e (v)dv\n= \u222b v Pr e\u223cDl [ t\u0303(\u03b2j) \u2208 I | l = \u2016v\u2016 ] PDFP U\u22a5e (v)dv\n\u2264 \u222b v ( ea \u222b I/ea PDFTr\u2212p(z)dz ) PDFP U\u22a5e (v)dv\n= ( ea \u222b I/ea PDFTr\u2212p(z)dz )\u222b v PDFP U\u22a5e (v)dv\n= ea \u222b I/ea PDFTr\u2212p(z)dz\nwhere the last transition is possible precisely because t\u0303 is independent of l (or \u2016v\u2016) \u2014 which is precisely what makes this t-value a pivot quantity. The proof of the lower bound is symmetric.\nTo conclude, we have shown that if Equation (7) holds, then for every interval I \u2282 R we have that Pre\u223cN (0n,\u03c32In\u00d7n) [ t\u0303(\u03b2j) \u2208 I ] is lower bounded by e\u2212aPrz\u223cTr\u2212p [z \u2208 I] and upper bounded by eaPrz\u223cTr\u2212p [z \u2208 (I/ea)]. So to conclude the proof of Theorem 3.1, we need to show that w.h.p such a as in Equation (7) exists. Claim B.6. In the homoscedastic model with Gaussian noise, if both n and r satisfy n, r \u2265 p + \u2126(log(1/\u03bd)), then we have that \u03c32(XTX)\u22121j,j +l 2(MTM)\u22121j,j \u2265 l2(MTM) \u22121 j,j and\n\u03c32(XTX)\u22121j,j +l 2(MTM)\u22121j,j \u2264 (1+ 2(r\u2212p) n\u2212p )\u00b7l 2(MTM)\u22121j,j\nUsing (1 + 2(r\u2212p)n\u2212p ) \u2264 e 2(r\u2212p) n\u2212p , Theorem 3.1 now follows from plugging a = r\u2212pn\u2212p to our above discussion.\nProof. The lower bound is immediate from non-negativity of \u03c32 and of (XTX)\u22121j,j = \u2016(XTX)\u22121/2ej\u20162. We therefore prove the upper bound.\nFirst, observe that l2 = \u2016PU\u22a5e\u20162 is sampled from \u03c32\u00b7\u03c72n\u2212p as U\u22a5 is of dimension n \u2212 p. Therefore, it holds that w.p. \u2265 1\u2212 \u03bd/2 that\n\u03c32 (\u221a n\u2212 p\u2212 \u221a 2 ln(2/\u03bd) )2 \u2264 l2\nand assuming n > p+100 ln(2/\u03bd) we therefore have \u03c32 \u2264 4 3(n\u2212p) l 2.\nSecondly, we argue that when r > p + 300 ln(4/\u03bd) we have that w.p. \u2265 1 \u2212 \u03bd/2 it holds that 3 4 (X TX)\u22121j,j \u2264 (r \u2212 p)(XTRTRX) \u22121 j,j . To see this, first observe that by picking R \u223c N (0r\u00d7n, Ir\u00d7r, In\u00d7n) the distribution of the product RX \u223c N (0r\u00d7d, Ir\u00d7r, XTX) is identical to picking Q \u223c N (0r\u00d7d, Ir\u00d7r, Id\u00d7d) and taking the product Q(XTX)1/2. Therefore, the distribution of (XTRTRX)\u22121 is identical to ( (XTX)1/2QTQ(XTX)1/2 )\u22121 = (XTX)\u22121/2(QTQ)\u22121(XTX)\u22121/2. Denoting v = (XTX)\u22121/2ej we have \u2016v\u20162 = (XTX)\u22121j,j . Claim A.1 from (Sheffet, 2015) gives that w.p. \u2265 1 \u2212 \u03bd/2 we have\n(r \u2212 p) \u00b7 eTj ( (XTX)1/2QTQ(XTX)1/2 )\u22121 ej\n= vT( 1r\u2212pQ TQ)\u22121v \u2265 34v Tv = 34 (X TX)\u22121j,j\nwhich implies the required.\nCombining the two inequalities we get:\n\u03c32(XTX)\u22121j,j \u2264 16l2(r\u2212p) n\u2212p (X TRTRX)\u22121j,j\n\u2264 2(r\u2212p)n\u2212p l 2(XTRTRX)\u22121j,j\nand as we denote M = RX we are done.\nWe comment that our analysis in the proof of Claim B.6 implicitly assumes r n (as we do think of the projection R as dimensionality reduction), and so the ratio r\u2212p n\u2212p is small. However, a similar analysis holds for r which is comparable to n \u2014 in which we would argue that \u03c32(XTX)\u22121j,j+l\n2(MTM)\u22121j,j \u03c32(XTX)\u22121 \u2208 [1, 1 + \u03b7] for some small \u03b7."}, {"heading": "B.3. Proof of Theorem 3.3", "text": "Theorem B.7 (Theorem 3.3 restated.). Fix a positive definite matrix \u03a3 \u2208 Rp\u00d7p. Fix parameters \u03b2 \u2208 Rp and \u03c32 > 0 and a coordinate j s.t. \u03b2j 6= 0. Let X be a matrix whose n rows are sampled i.i.d fromN (0p,\u03a3). Let y be a vector s.t. yi\u2212(X\u03b2)i is sampled i.i.d fromN (0, \u03c32). Fix \u03bd \u2208 (0, 1/2) and \u03b1 \u2208 (0, 1/2). Then there exist constants C1, C2, C3 and C4 such that when we run Algorithm 1 over [X;y] with\nparameter r w.p. \u2265 1\u2212\u03bd we correctly \u03b1-reject the null hypothesis using p\u03030 (i.e., w.p. \u2265 1 \u2212 \u03bd Algorithm 1 returns matrix unaltered and we can estimate t\u03030 and verify that indeed p\u03030 < \u03b1 \u00b7 e \u2212 r\u2212pn\u2212p ) provided\nr \u2265 p+ max { C1 \u03c32(c\u03032\u03b1 + \u03c4\u0303 2 \u03b1)\n\u03b22j\u03c3min(\u03a3) , C2 ln(1/\u03bd)\n}\nand n \u2265 max { r, C3\nw2\nmin{\u03c3min(\u03a3), \u03c32} , C4(p+ ln(1/\u03bd)) } where c\u0303\u03b1, \u03c4\u0303\u03b1 denote the numbers s.t. \u221e\u222b\nc\u0303\u03b1/e\nr\u2212p n\u2212p\nPDFTr\u2212p(x)dx = \u03b1 2 e \u2212 r\u2212pn\u2212p and\n\u221e\u222b \u03c4\u0303\u03b1/e r\u2212p n\u2212p PDFN (0,1)(x)dx = \u03b1 2 e \u2212 r\u2212pn\u2212p resp.\nProof. First we need to use the lower bound on n to show that indeed Algorithm 1 does not alter A, and that various quantities are not far from their expected values. Formally, we claim the following.\nProposition B.8. Under the same lower bounds on n and r as in Theorem 3.3, w.p. 1\u2212\u03b1\u2212\u03bd we have that Theorem 3.1 holds and also that\n\u03b6\u0303\u20162 = \u0398( r\u2212pr \u2016PU\u22a5e\u2016 2) = \u0398( r\u2212pr (n\u2212 p)\u03c3 2)\nand (XTRTRX)\u22121j,j = \u0398( 1 r\u2212p (X TX)\u22121j,j )\nProof of Proposition B.8. First, we need to argue that we have enough samples as to have the gap \u03c32min([X; y])\u2212w2 sufficiently large.\nSince xi \u223c N (0,\u03a3), and yi = \u03b2Txi + ei with ei \u223c N (0, \u03c32), we have that the concatenation (xi \u25e6 yi) is also sampled from a Gaussian. Clearly, E[yi] = \u03b2TE[xi] + E[ei] = 0. Similarly, E[xi,jyi] = E[xi,j \u00b7 (\u03b2Txi + ei)] = (\u03a3\u03b2)j and E[y2i ] = E[e 2 i ] + E[\u2016X\u03b2\u20162] = \u03c32 + E[\u03b2TXTX\u03b2 ] = \u03c32 + \u03b2T\u03a3\u03b2 . Therefore, each row of A is an i.i.d sample of N (0p+1,\u03a3A), with\n\u03a3A =\n( \u03a3 \u03a3\u03b2\n\u03b2T\u03a3 \u03c32+\u03b2T\u03a3\u03b2 ) Denote \u03bb2 = \u03c3min(\u03a3). Then, to argue that \u03c3min(\u03a3A) is large we use the lower bound from (Ma & Zarowski, 1995) (Theorem 3.1) combining with some simple arithmetic manipulations to deduce that \u03c3min(\u03a3A) \u2265 min{\u03c3min(\u03a3), \u03c32}.\nHaving established a lower bound on \u03c3min(\u03a3A), it follows that with n = \u2126(p ln(1/\u03bd)) i.i.d draws from N (0p+1,\u03a3A) we have w.p. \u2264 \u03bd/4 that \u03c3min(ATA) = o(n) \u00b7 min{\u03c3min(\u03a3), \u03c32}. Conditioned on \u03c3min(ATA) = \u2126(n\u03c3min(\u03a3A)) = \u2126(w\n2) being large enough, we have that w.p. \u2264 \u03bd/4 over the randomness of Algorithm 1 the matrix A does not pass the if-condition and the output of the algorithm is not RA. Conditioned on Algorithm 1 outputting RA, and due to the lower bound r = p + \u2126(ln(1/\u03bd)), we have that the result of Theorem 3.1 does not hold w.p. \u2264 \u03b1+ \u03bd/4. All in all we deduce that w.p. \u2265 1\u2212\u03b1\u2212 3\u03bd/4 the result of Theorem 3.1 holds. And since we argue Theorem 3.1 holds, then the following two bounds that are used in the proof14 also hold:\n(XTRTRX)\u22121j,j = \u0398( 1 r\u2212p (X TX)\u22121j,j )\n\u2016PU\u22a5e\u20162 = \u0398((n\u2212 p)\u03c32)\nLastly, in the proof of Theorem 3.1 we argue that for a given PU\u22a5e the length \u2016\u03b6\u0303\u20162 is distributed like \u2016P U\u22a5e\u2016 2\nr \u03c7 2 r\u2212p. Appealing again to the fact that r = p + \u2126(ln(1/\u03bd) we have that w.p. \u2265 \u03bd/4 it holds that \u2016\u03b6\u0303\u20162 > 2(r \u2212 p)\u2016PU\u22a5e\u2016 2\nr . Plugging in the value of \u2016PU\u22a5e\u2016 2 con-\ncludes the proof of the proposition.\nBased on Proposition B.8, we now show that we indeed reject the null-hypothesis (as we should). When Theorem 3.1 holds, reject the null-hypothesis iff p\u03030 < \u03b1 \u00b7 e \u2212 r\u2212pn\u2212p which holds iff |t\u03030| > e r\u2212p n\u2212p \u03c4\u0303\u03b1. This implies we reject that null-hypothesis when |\u03b2\u0303j | > e r\u2212p n\u2212p \u03c4\u0303\u03b1 \u00b7\n\u03c3\u0303 \u221a\n(XTRTRX)\u22121j,j ). Note that this bound is based\non Corollary 3.2 that determines that |\u03b2\u0303j \u2212 \u03b2j | =\nO ( e r\u2212p n\u2212p c\u0303\u03b1 \u00b7 \u03c3\u0303 \u221a (XTRTRX)\u22121j,j ) ) . And so we have that\nw.p. \u2265 1\u2212 \u03bd we \u03b1-reject the null hypothesis when it holds that |\u03b2j | > 3(c\u0303\u03b1+ \u03c4\u0303\u03b1) \u00b7 \u03c3\u0303 \u221a (XTRTRX)\u22121j,j ) \u2265 e r\u2212p n\u2212p (c\u0303\u03b1+\n\u03c4\u0303\u03b1)\u03c3\u0303 \u221a (XTRTRX)\u22121j,j ) (due to the lower bound n \u2265 r).\nBased on the bounds stated above we have that \u03c3\u0303 = \u2016\u03b6\u0303\u2016 \u221a\nr r\u2212p = \u0398(\u03c3\n\u221a n\u2212 p \u221a r\u2212p r \u221a r r\u2212p ) = \u0398(\u03c3 \u221a n\u2212 p)\nand that\n(XTRTRX)\u22121j,j = \u0398( 1 r\u2212p (X TX)\u22121j,j ) = O ( 1 r\u2212p \u00b7 1 n\u03c3min(\u03a3) ) And so, a sufficient condition for rejecting the nullhypothesis is to have\n|\u03b2j | = \u2126 ( (c\u0303\u03b1 + \u03c4\u0303\u03b1)\u03c3 \u221a n\u2212 p r \u2212 p \u00b7 \u221a 1 n\u03c3min(\u03a3) ) 14More accurately, both are bounds shown in Claim B.6.\n= \u2126(e r\u2212p n\u2212p (c\u0303\u03b1 + \u03c4\u0303\u03b1)\u03c3\u0303 \u221a (XTRTRX)\u22121j,j ))\nwhich, given the lower bound r = p + \u2126 ( (c\u0303\u03b1+\u03c4\u0303\u03b1) 2\u03c32\n\u03b22j\u03c3min(\u03a3) ) indeed holds."}, {"heading": "C. Projected Ridge Regression", "text": "In this section we deal with the case that our matrix does not pass the if-condition of Algorithm 1. In this case, the matrix is appended with a d \u00d7 d-matrix which is wId\u00d7d.\nDenoting A\u2032 = [\nA w \u00b7 Id\u00d7d\n] we have that the algorithm\u2019s\noutput is RA\u2032.\nSimilarly to before, we are going to denote d = p + 1 and decompose A = [X;y] with X \u2208 Rn\u00d7p and y \u2208 Rn, with the standard assumption of y = X\u03b2 + e and ei sampled i.i.d from N (0, \u03c32).15 We now need to introduce some additional notation. We denote the appended matrix and vectors X \u2032 and y \u2032 s.t. A\u2032 = [X \u2032;y \u2032]. Meaning:\nX \u2032 =  XwIp\u00d7p 0Tp  and\ny \u2032 =  y0p w  = X \u2032\u03b2 +  e\u2212w\u03b2 w  def= X \u2032\u03b2 + e\u2032 And so we respectively denote R = [R1;R2;R3] with R1 \u2208 Rr\u00d7n, R2 \u2208 Rr\u00d7p and R3 \u2208 Rr\u00d71 (so R3 is a vector denoted as a matrix). Hence:\nM \u2032 = RX \u2032 = R1X + wR2\nand\nRy \u2032 = RX \u2032\u03b2+Re\u2032 = R1y+wR3 = R1X\u03b2+R1e+wR3\nAnd so, using the output RA\u2032 of Algorithm 1, we solve the linear regression problem derived from 1\u221a\nr RX \u2032 and\n1\u221a r Ry \u2032. I.e., we set\n\u03b2 \u2032 = arg min z 1 r\u2016Ry \u2032 \u2212RX \u2032z\u20162\n= (X \u2032TRTRX \u2032)\u22121(RX \u2032)T(Ry \u2032)\nSarlos\u2019 results (2006) regarding the Johnson Lindenstrauss transform give that, when R has sufficiently many rows, solving the latter optimization problem gives a good approximation for the solution of the optimization problem \u03b2R = arg minz \u2016y \u2032 \u2212X \u2032z\u20162 = arg minz ( \u2016y \u2212Xz\u20162 + w2\u2016z\u20162\n) 15Just as before, it is possible to denote any single column as y\nand any subset of the remaining columns as X .\nThe latter problem is known as the Ridge Regression problem. Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large. It is also often applied in the case where X doesn\u2019t have full rank or is close to not having full-rank. That is because the Ridge Regression problem is always solvable. One can show that the minimizer \u03b2R = (XTX + w2Ip\u00d7p)\n\u22121XTy is the unique solution of the Ridge Regression problem and that the RHS is always defined (even when X is singular).\nThe original focus of Ridge Regression is on penalizing \u03b2R for having large coefficients. Therefore, Ridge Regression actually poses a family of linear regression problems: minz \u2016y\u2212Xz\u2016+ \u03bb\u2016z\u20162, where one may set \u03bb to be any non-negative scalar. And so, much of the literature on Ridge Regression is devoted to the art of fine-tuning this penalty term \u2014 either empirically or based on the \u03bb that yields the best risk: \u2016E[\u03b2R] \u2212 \u03b2\u20162 + Var(\u03b2R).16 Here we propose a fundamentally different approach for the choice of the normalization factor \u2014 we set it so that solution of the regression problem would satisfy ( , \u03b4)-differential privacy (by projecting the problem onto a lower dimension).\nWhile the solution of the Ridge Regression problem might have smaller risk than the OLS solution, it is not known how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate \u03b2R back into \u03b2\u0302 = (XTX)\u22121XTy and relying on OLS). In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard OLS, because access to X and y was given.\nTherefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.17 Clearly, there are situations where such confidence bounds simply cannot be derived. (Consider for example the case where X = 0n\u00d7p and y is just i.i.d draws from N (0, \u03c32), so obviously [X; y] gives no information about \u03b2 .) Nonetheless, under additional assumptions about the data, our work can give confidence intervals for \u03b2j , and in the case where the interval doesn\u2019t intersect the origin \u2014 assure us that sign(\u03b2\u2032j) = sign(\u03b2j) w.h.p.\nClearly, Sarlos\u2019 work (2006) gives an upper bound on the distance \u2016\u03b2 \u2032\u2212\u03b2R\u2016. However, such distance bound doesn\u2019t come with the coordinate by coordinate confidence guarantee we would like to have. In fact, it is not even clear from Sarlos\u2019 work that E[\u03b2 \u2032] = \u03b2R (though it is obvious to see that E[(X \u2032TRTRX \u2032)]\u03b2R = E[(RX \u2032)TRy \u2032]). Here,\n16Ridge Regression, as opposed to OLS, does not yield an unbiased estimator. I.e., E[\u03b2R] 6= \u03b2 .\n17Note: The na\u0131\u0308ve approach of using RX \u2032 and Ry \u2032 to interpolate RX and Ry and then apply Theorem 3.1 using these estimations of RX and Ry ignores the noise added from appending the matrix A into A\u2032, and it is therefore bound to produce inaccurate estimations of the t-values.\nwe show that E[\u03b2 \u2032] = \u03b2\u0302 which, more often than not, does not equal \u03b2R.\nComment about notation. Throughout this section we assume X is of full rank and so (XTX)\u22121 is well-defined. If X isn\u2019t full-rank, then one can simply replace any occurrence of (XTX)\u22121 with X+(X+)T. This makes all our formulas well-defined in the general case."}, {"heading": "C.1. Running OLS on the Projected Data", "text": "In this section, we analyze the projected Ridge Regression, under the assumption (for now) that e is fixed. That is, for now we assume that the only source of randomness comes from picking the matrix R = [R1;R2;R3]. As before, we analyze the distribution over \u03b2 \u2032 (see Equation (9)), and the value of the function we optimize at \u03b2 \u2032. Denoting M \u2032 = RX \u2032, we can formally express the estimators:\n\u03b2 \u2032 = (M \u2032TM \u2032)\u22121M \u2032TRy \u2032 (9) \u03b6 \u2032 = 1\u221a\nr (Ry \u2032 \u2212RX \u2032\u03b2 \u2032) (10)\nClaim C.1. Given that y = X\u03b2+e for a fixed e, and given X and M \u2032 = RX \u2032 = R1X + wR2 we have that \u03b2 \u2032 \u223c N ( \u03b2 +X+e,\n(w2(\u2016\u03b2 +X+e\u20162 + 1) + \u2016PU\u22a5e\u20162)(M \u2032TM \u2032)\u22121 )\n\u03b6 \u2032 \u223c N ( 0r,\nw2(\u2016\u03b2+X+e\u20162+1)+\u2016P U\u22a5e\u2016 2\nr (Ir\u00d7r \u2212M \u2032M \u2032+) ) and furthermore, \u03b2 \u2032 and \u03b6 \u2032 are independent of one another.\nProof. First, we write \u03b2 \u2032 and \u03b6 \u2032 explicitly, based on e and projection matrices:\n\u03b2 \u2032 = (M \u2032TM \u2032)\u22121M \u2032TRy \u2032\n= M \u2032+(R1X)\u03b2 +M \u2032+(R1e + wR3)\n\u03b6 \u2032 = 1\u221a r (Ry \u2032 \u2212RX \u2032\u03b2 \u2032)\n= 1\u221a r (Ir\u00d7r \u2212M \u2032M \u2032+)Re\u2032 = 1\u221a r PU \u2032\u22a5(R1e \u2212 wR2\u03b2 + wR3)\nwith U \u2032 denoting colspan(M \u2032) and PU \u2032\u22a5 denoting the projection onto the subspace U \u2032\u22a5.\nAgain, we break e into an orthogonal composition: e = PUe + PU\u22a5e with U = colspan(X) (hence PU = XX+) and U\u22a5 = colspan(X)\u22a5. Therefore,\n\u03b2 \u2032 = M \u2032+(R1X)\u03b2 +M \u2032+(R1XX +e +R1PU\u22a5e + wR3) = M \u2032+(R1X)(\u03b2 +X +e) +M \u2032+(R1PU\u22a5e + wR3)\nwhereas \u03b6 \u2032 is essentially 1\u221a r (Ir\u00d7r \u2212M \u2032M \u2032+)(R1XX+e +R1PU\u22a5e \u2212 wR2\u03b2 + wR3)\n(\u2217) = 1\u221a\nr (Ir\u00d7r \u2212M \u2032M \u2032+)\u00b7\n(R1XX +e +R1PU\u22a5e + (M \u2032 \u2212 wR2)\u03b2 + wR3) = 1\u221a\nr (Ir\u00d7r \u2212M \u2032M \u2032+)\u00b7\n(R1X(\u03b2 +X +e) +R1PU\u22a5e + wR3)\nwhere equality (\u2217) holds because (I \u2212M \u2032M \u2032+)M \u2032v = 0 for any v .\nWe now aim to describe the distribution of R given that we know X \u2032 and M \u2032 = RX \u2032. Since\nM \u2032 = R1X + wR2 + 0 \u00b7R3 = R1X(X+X) + wR2 = (R1PU )X + wR2\nthen M \u2032 is independent of R3 and independent of R1PU\u22a5 . Therefore, given X and M \u2032 the induced distribution over R3 remainsR3 \u223c N (0r, Ir\u00d7r), and similarly, givenX and M \u2032 we have R1PU\u22a5 \u223c N (0r\u00d7n, Ir\u00d7r, PU\u22a5) (rows remain independent from one another, and each row is distributed like a spherical Gaussian in colspan(X)\u22a5). And so, we have that R1X = R1PUX = M \u2032 \u2212 wR2, which in turn implies:\nR1X \u223c N ( M \u2032, Ir\u00d7r, w 2 \u00b7 Ip\u00d7p )\nmultiplying this random matrix with a vector, we get\nR1X(\u03b2+X +e) \u223c N (M \u2032\u03b2 +M \u2032X+e, w2\u2016\u03b2 +X+e\u20162Ir\u00d7r)\nand multiplying this random vector with a matrix we get\nM \u2032+R1X(\u03b2+X +e) \u223c N (\u03b2 +X+e, w2\u2016\u03b2 +X+e\u20162(M \u2032TM)\u22121)\nI.e.,\nM \u2032+R1X(\u03b2+X +e) \u223c \u2016\u03b2+X+e\u2016\u00b7N (u,w2(M \u2032TM)\u22121)\nwhere u denotes a unit-length vector in the direction of \u03b2+ X+e.\nSimilar to before we have\nRPU\u22a5 \u223c N (0r\u00d7n, Ir\u00d7r, PU\u22a5) \u21d2M \u2032+(RPU\u22a5e) \u223c N (0d, \u2016PU\u22a5e\u20162(M \u2032TM \u2032)\u22121)\nwR3 \u223c N (0r, w2Ir\u00d7r) \u21d2M \u2032+(wR3) \u223c N (0d, w2(M \u2032+M \u2032)\u22121)\nTherefore, the distribution of \u03b2 \u2032, which is the sum of the 3 independent Gaussians, is as required.\nAlso, \u03b6 \u2032 = 1\u221a r PU \u2032\u22a5 (R1X(\u03b2 +X +e) +R1PU\u22a5e + wR3) is the sum of 3 independent Gaussians, which implies its distribution is\nN (\n1\u221a r PU \u2032\u22a5M \u2032(\u03b2 +X+e),\n1 r (w 2(\u2016\u03b2 +X+e\u20162 + 1) + \u2016PU\u22a5e\u20162)PU \u2032\u22a5 )\nI.e., N ( 0r, 1 r (w 2(\u2016\u03b2 +X+e\u20162 + 1) + \u2016PU\u22a5e\u20162)PU \u2032\u22a5 ) as PU \u2032\u22a5M \u2032 = 0r\u00d7r.\nFinally, observe that \u03b2 \u2032 and \u03b6 \u2032 are independent as the former depends on the projection of the spherical Gaussian R1X(\u03b2 + X\n+e) + R1PU\u22a5e + wR3 on U \u2032, and the latter depends on the projection of the same multivariate Gaussian on U \u2032\u22a5.\nObserve that Claim C.1 assumes e is given. This may seem somewhat strange, since without assuming anything about e there can be many combinations of \u03b2 and e for which y = X\u03b2 + e. However, we always have that \u03b2 + X+e = X+y = \u03b2\u0302 . Similarly, it is always the case the PU\u22a5e = (I \u2212 XX+)y = \u03b6 . (Recall OLS definitions of \u03b2\u0302 and \u03b6 in Equation (1) and (2).) Therefore, the distribution of \u03b2 \u2032 and \u03b6 \u2032 is unique (once y is set):\n\u03b2 \u2032 \u223c N ( \u03b2\u0302 , (w2(\u2016\u03b2\u0302\u20162 + 1) + \u2016\u03b6\u20162)(M \u2032TM \u2032)\u22121 ) \u03b6 \u2032 \u223c N ( 0r, w2(\u2016\u03b2\u0302\u20162 + 1) + \u2016\u03b6\u20162\nr (Ir\u00d7r \u2212M \u2032M \u2032+)\n)\nAnd so for a given dataset [X;y] we have that \u03b2 \u2032 serves as an approximation for \u03b2\u0302 .\nAn immediate corollary of Claim C.1 is that for any fixed e it holds that the quantity t\u2032(\u03b2j) =\n\u03b2\u2032j\u2212(\u03b2j+(X +e)j) \u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7(M \u2032TM \u2032)\u22121j,j = \u03b2\u2032j\u2212\u03b2\u0302j \u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7(M \u2032TM \u2032)\u22121j,j is distributed like a Tr\u2212p-distribution. Therefore, the following theorem follows immediately.\nTheorem C.2. Fix X \u2208 Rn\u00d7p and y \u2208 R. Define \u03b2\u0302 = X+y and \u03b6 = (I \u2212 XX+)y . Let RX \u2032 and Ry \u2032 denote the result of applying Algorithm 1 to the matrix A = [X;y] when the algorithm appends the data with a w \u00b7 I matrix. Fix a coordinate j and any \u03b1 \u2208 (0, 1/2). When computing \u03b2 \u2032 and \u03b6 \u2032 as in Equations (9) it and (10), we have that w.p. \u2265 1\u2212 \u03b1 it holds that\n\u03b2\u0302j \u2208 ( \u03b2\u2032j \u00b1 c\u2032\u03b1\u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7 (M \u2032TM \u2032) \u22121 j,j ) where c\u2032\u03b1 denotes the number such that (\u2212c\u2032\u03b1, c\u2032\u03b1) contains 1\u2212 \u03b1 mass of the Tr\u2212p-distribution.\nNote that Theorem C.2, much like the rest of the discussion in this Section, builds on y being fixed, which means \u03b2\u2032j serves as an approximation for \u03b2\u0302j . Yet our goal is to argue about similarity (or proximity) between \u03b2\u2032j and \u03b2j . To that end, we combine the standard OLS confidence interval \u2014 which says that w.p. \u2265 1 \u2212 \u03b1 over the randomness of picking e in the homoscedastic model we have\n|\u03b2j \u2212 \u03b2\u0302j | \u2264 c\u03b1\u2016\u03b6\u2016 \u221a (XTX)\u22121j,j n\u2212p \u2014 with the confidence interval of Theorem C.2 above, and deduce that w.p. \u2265 1\u2212\u03b1\nwe have that |\u03b2\u2032j \u2212 \u03b2j | is at most\nO c\u03b1 \u2016\u03b6\u2016 \u221a\n(XTX)\u22121j,j \u221a n\u2212 p + c\u2032\u03b1 \u2016\u03b6 \u2032\u2016 \u221a r(M \u2032TM \u2032)\u22121j,j \u221a r \u2212 p  (11)\n18And so, in the next section, our goal is to give conditions under which the interval of Equation (11) isn\u2019t much larger in comparison to the interval length of c\u2032\u03b1 \u2016\u03b6 \u2032\u2016\u221a r\u2212p \u221a r(M \u2032TM \u2032)\u22121j,j we get from Theorem C.2; and more importantly \u2014 conditions that make the interval of Theorem C.2 useful and not too large. (Note, in expectation \u2016\u03b6 \u2032\u2016\u221a r\u2212p is about \u221a (w2 + w2\u2016\u03b2\u0302\u20162 + \u2016\u03b6\u20162)/r. So, for example, in situations where \u2016\u03b2\u0302\u2016 is very large, this interval isn\u2019t likely to inform us as to the sign of \u03b2j .)\nMotivating Example. A good motivating example for the discussion in the following section is when [X;y] is a strict submatrix of the dataset A. That is, our data contains many variables for each entry (i.e., the dimensionality d of each entry is large), yet our regression is made only over a modest subset of variables out of the d. In this case, the least singular value of A might be too small, causing the algorithm to alter A; however, \u03c3min(XTX) could be sufficiently large so that had we run Algorithm 1 only on [X;y] we would not alter the input. (Indeed, a differentially private way for finding a subset of the variables that induce a submatrix with high \u03c3min is an interesting open question, partially answered \u2014 for a single regression \u2014 in the work of Thakurta and Smith (Thakurta & Smith, 2013).) Indeed, the conditions we specify in the following section depend on \u03c3min( 1nX\nTX), which, for a zero-mean data, the minimal variance of the data in any direction. For this motivating example, indeed such variance isn\u2019t necessarily small."}, {"heading": "C.2. Conditions for Deriving a Confidence Interval for Ridge Regression", "text": "Looking at the interval specified in Equation (11), we now give an upper bound on the the random quantities in this interval: \u2016\u03b6\u2016, \u2016\u03b6 \u2032\u2016, and (M \u2032TM \u2032)\u22121j,j . First, we give bound that are dependent on the randomness in R (i.e., we continue to view e as fixed).\nProposition C.3. For any \u03bd \u2208 (0, 1/2), if we have r = p + \u2126(ln(1/\u03bd)) then with probability \u2265\n18Observe that w.p. \u2265 1 \u2212 \u03b1 over the randomness of e we have that |\u03b2j \u2212 \u03b2\u0302j | \u2264 c\u03b1\u2016\u03b6\u2016 \u221a (XTX)\u22121j,j n\u2212p , and w.p. \u2265 1 \u2212 \u03b1 over the randomness of R we have that |\u03b2\u2032j \u2212 \u03b2\u0302j | \u2264 c\u2032\u03b1\u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7 (M \u2032TM \u2032) \u22121 j,j . So technically, to give a (1 \u2212 \u03b1)confidence interval around \u03b2\u2032j that contains \u03b2j w.p. \u2265 1\u2212 \u03b1, we need to use c\u03b1/2 and c\u2032\u03b1/2 instead of c\u03b1 and c \u2032 \u03b1 resp. To avoid overburdening the reader with what we already see as too many parameters, we switch to asymptotic notation.\n1 \u2212 \u03bd over the randomness of R we have (r \u2212 p)(M \u2032TM)\u22121j,j = \u0398 ( (w2Ip\u00d7p +X TX)\u22121j,j ) and \u2016\u03b6 \u2032\u20162 r\u2212p = \u0398(w 2+w2\u2016\u03b2\u0302\u20162+\u2016\u03b6\u20162\nr ).\nProof. The former bound follows from known results on the Johnson-Lindenstrauss transform (as were shown in the proof of Claim B.6). The latter bound follows from standard concentration bounds of the \u03c72-distribution.\nPlugging in the result of Proposition C.3 to Equation (11) we get that w.p. \u2265 1\u2212 \u03bd the difference |\u03b2\u2032j \u2212 \u03b2j | is at most O ( c\u03b1 \u2016\u03b6\u2016\u221a n\u2212 p \u221a (XTX)\u22121j,j\n+ c\u2032\u03b1\n\u221a w2 + w2\u2016\u03b2\u0302\u20162 + \u2016\u03b6\u20162\nr \u2212 p\n\u221a (w2Ip\u00d7p +XTX) \u22121 j,j ) (12)\nWe will also use the following proposition.\nProposition C.4.\n(XTX)\u22121j,j \u2264 ( 1 + w2\n\u03c3min(XTX)\n) (w2Ip\u00d7p +X TX)\u22121j,j\nProof. We have that\n(XTX)\u22121\n= (XTX)\u22121(XTX + w2Ip\u00d7p)(X TX + w2Ip\u00d7p) \u22121\n= (XTX + w2Ip\u00d7p) \u22121 + w2(XTX)\u22121(XTX + w2Ip\u00d7p) \u22121\n= (Ip\u00d7p + w 2(XTX)\u22121)(XTX + w2Ip\u00d7p) \u22121\n= (XTX + w2Ip\u00d7p) \u22121/2\u00b7\n(Ip\u00d7p + w 2(XTX)\u22121)\u00b7\n(XTX + w2Ip\u00d7p) \u22121/2\nwhere the latter holds because (Ip\u00d7p + w2(XTX)\u22121) and (XTX + w2Ip\u00d7p)\n\u22121 are diagonalizable by the same matrix V (the same matrix for which (XTX) = V S\u22121V T). Since we have \u2016Ip\u00d7p +w2(XTX)\u22121\u2016 = 1 + w 2\n\u03c32min(X) , it is\nclear that (Ip\u00d7p + w2(XTX)\u22121) (1 + w 2\n\u03c32min(X) )Ip\u00d7p.\nWe deduce that (XTX)\u22121j,j = e T j (X TX)\u22121ej \u2264 (1 + w2 \u03c32min(X) )(XTX + w2Ip\u00d7p) \u22121 j,j .\nBased on Proposition C.4 we get from Equation (12) that\n|\u03b2\u2032j \u2212 \u03b2j | is at most\nO( ( c\u03b1 \u221a\u221a\u221a\u221a\u2016\u03b6\u20162(1 + w2\u03c3min(XTX) ) n\u2212 p +\nc\u2032\u03b1\n\u221a w2 + w2\u2016\u03b2\u0302\u20162 + \u2016\u03b6\u20162\nr \u2212 p\n)\u221a (w2Ip\u00d7p +XTX) \u22121 j,j )\n(13)\nAnd so, if it happens to be the case that exists some small \u03b7 > 0 for which \u03b2\u0302 , \u03b6 and w2 satisfy\n\u2016\u03b6\u20162(1 + w 2\n\u03c3min(XTX) )\nn\u2212 p \u2264 \u03b72\n( w2 + w2\u2016\u03b2\u0302\u20162 + \u2016\u03b6\u20162\nr \u2212 p\n) (14)\nthen we have that Pr[\u03b2j \u2208( \u03b2\u2032j \u00b1O((1 + \u03b7) \u00b7 c\u2032\u03b1\u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7 (M \u2032TM \u2032) \u22121 j,j ) )\n] \u2265 1 \u2212 \u03b1.19 Moreover, if in this case |\u03b2j | >\nc\u2032\u03b1(1 + \u03b7)\n\u221a w2+w2\u2016\u03b2\u0302\u20162+\u2016\u03b6\u20162\nr\u2212p\n\u221a (w2Ip\u00d7p +XTX) \u22121 j,j\nthen Pr[sign(\u03b2\u2032j) = sign(\u03b2j)] \u2265 1\u2212 \u03b1. This is precisely what Claims C.5 and C.6 below do. Claim C.5. If there exists \u03b7 > 0 s.t. n \u2212 p \u2265 2\u03b72 (r \u2212 p) and n 2 =\n\u2126 ( r3/2 \u00b7 B 2 ln(1/\u03b4) \u00b7\n1\n\u03b72\u03c3min( 1 nX TX) ) , then Pr[\u03b2j \u2208(\n\u03b2\u2032j \u00b1O((1 + \u03b7) \u00b7 c\u2032\u03b1\u2016\u03b6 \u2032\u2016 \u221a r r\u2212p \u00b7 (M \u2032TM \u2032) \u22121 j,j ) )\n] \u2265 1\u2212 \u03b1.\nProof. Based on the above discussion, it is enough to argue that under the conditions of the claim, the constraint of Equation (14) holds. Since we require \u03b7 2\n2 \u2265 r\u2212p n\u2212p then\nit is evident that \u2016\u03b6\u2016 2 n\u2212p \u2264 \u03b72\u2016\u03b6\u20162 2(r\u2212p) . So we now show that \u2016\u03b6\u20162 n\u2212p \u00b7 w2 \u03c3min(XTX) \u2264 \u03b7 2\u2016\u03b6\u20162 2(r\u2212p) under the conditions of the claim, and this will show the required. All that is left is some algebraic manipulations. It suffices to have:\n\u03b72 2 \u00b7 n\u2212p r\u2212p\u03c3min(X TX) \u2265 \u03b7 2 2 \u00b7 n2 r \u03c3min( 1 nX TX)\n\u2265 32B 2 \u221a r ln(8/\u03b4) \u2265 w2\nwhich holds for n2 \u2265 r3/2 \u00b7 64B 2 ln(1/\u03b4) \u03b72 \u03c3min( 1 nX TX)\u22121, as we assume to hold.\nClaim C.6. Fix \u03bd \u2208 (0, 12 ). If (i) n = p + \u2126(ln(1/\u03bd)), (ii) \u2016\u03b2\u20162 = \u2126(\u03c32\u2016X+\u20162F ln( p \u03bd )) and (iii) r \u2212 p =\n\u2126\n( (c\u2032\u03b1) 2(1+\u03b7)2\n\u03b22j\n( 1 + \u2016\u03b2\u20162 + \u03c3 2\n\u03c3min( 1 nX TX)\n)) , then in the\nhomoscedastic model, with probability\u2265 1\u2212\u03bd\u2212\u03b1 we have that sign(\u03b2j) = sign(\u03b2\u2032j).\n19We assume n \u2265 r so c\u03b1 < c\u2032\u03b1 as the Tn\u2212p-distribution is closer to a normal Gaussian than the Tr\u2212p-distribution.\nProof. Based on the above discussion, we aim to show that in the homoscedastic model (where each coordinate ei \u223c N (0, \u03c32) independently) w.p. \u2265 1 \u2212 \u03bd it holds that the magnitude of \u03b2j is greater than\nc\u2032\u03b1(1+\u03b7)\n\u221a w2 + w2\u2016\u03b2\u0302\u20162 + \u2016\u03b6\u20162\nr \u2212 p\n\u221a (w2Ip\u00d7p +XTX) \u22121 j,j\nTo show this, we invoke Claim A.4 to argue that w.p. \u2265 1 \u2212 \u03bd we have (i) \u2016\u03b6\u20162 \u2264 2\u03c32(n \u2212 p) (since n = p + \u2126(ln(1/\u03bd))), and (ii) \u2016\u03b2\u0302\u20162 \u2264 2\u2016\u03b2\u20162 (since \u2016\u03b2 \u2212 \u03b2\u0302\u20162 \u2264 \u03c32\u2016X+\u20162F ln( p \u03bd ) whereas \u2016\u03b2\u2016 2 = \u2126(\u03c32\u2016X+\u20162F ln( p \u03bd ))). We also use the fact that (w2Ip\u00d7p + XTX)\u22121j,j \u2264 (w2 + \u03c3\u22121min(X TX)), and then deduce that\n(1 + \u03b7)c\u2032\u03b1\n\u221a w2 + w2\u2016\u03b2\u0302\u20162 + \u2016\u03b6\u20162\nr \u2212 p\n\u221a (w2Ip\u00d7p +XTX) \u22121 j,j\n\u2264 (1 + \u03b7)c \u2032 \u03b1\u221a\nr \u2212 p\n\u221a 2 w2(1 + \u2016\u03b2\u20162) + \u03c32(n\u2212 p)\nw2 + \u03c3min(XTX)\n\u2264 (1 + \u03b7)c \u2032 \u03b1\u221a\nr \u2212 p\n\u221a 2(1 + \u2016\u03b2\u20162) + 2\u03c3\n2(n\u2212 p) \u03c3min(XTX) \u2264 |\u03b2j |\ndue to our requirement on r \u2212 p.\nObserve, out of the 3 conditions specified in Claim C.6, condition (i) merely guarantees that the sample is large enough to argue that estimations are close to their expect value; and condition (ii) is there merely to guarantee that \u2016\u03b2\u0302\u2016 \u2248 \u2016\u03b2\u2016. It is condition (iii) which is non-trivial to hold, especially together with the conditions of Claim C.5 that pose other constraints in regards to r, n, \u03b7 and the various other parameters in play. It is interesting to compare the requirements on r to the lower bound we get in Theorem 3.3 \u2014 especially the latter bound. The two bounds are strikingly similar, with the exception that here we also require r \u2212 p to be greater than 1+\u2016\u03b2\u2016 2\n\u03b22j . This is part of the\nunfortunate effect of altering the matrix A: we cannot give confidence bounds only for the coordinates j for which \u03b22j is very small relative to \u2016\u03b2\u20162.\nIn summary, we require to have n = p + \u2126(ln(1/\u03bd)) and that X contains enough sample points to have \u2016\u03b2\u0302\u2016 comparable to \u2016\u03b2\u2016, and then set r and \u03b7 such that (it is convenient to think of \u03b7 as a small constant, say, \u03b7 = 0.1)\n\u2022 r \u2212 p = O(\u03b72(n\u2212 p)) (which implies r = O(n)) \u2022 r = O( ( \u03b72 n 2\nB2 ln(1/\u03b4)\u03c3min( 1 nX\nTX) ) 2 3 )\n\u2022 r \u2212 p = \u2126( 1+\u2016\u03b2\u2016 2\n\u03b22j + \u03c3\n2 \u03b22j \u00b7 \u03c3\u22121min( 1nX TX))\nto have that the (1 \u2212 \u03b1)-confidence interval around \u03b2\u2032j does not intersect the origin. Once again, we comment that these conditions are sufficient but not necessary, and furthermore \u2014 even with these conditions holding \u2014 we do not make any claims of optimality of our confidence bound. That is because from Proposition C.4 onwards our discussion uses upper bounds that do not have corresponding lower bounds, to the best of our knowledge."}, {"heading": "D. Confidence Intervals for \u201cAnalyze Gauss\u201d Algorithm", "text": "To complete the picture, we now analyze the \u201cAnalyze Gauss\u201d algorithm of Dwork et al (Dwork et al., 2014). Algorithm 2 works by adding random Gaussian noise to ATA, where the noise is symmetric with each coordinate above the diagonal sampled i.i.d from N (0,\u22062) with \u22062 = O ( B4 log(1/\u03b4) 2 ) .20 Using the same notation for a sub-matrix of A as [X;y] as before, with X \u2208 Rn\u00d7p and y \u2208 Rn, we denote the output of Algorithm 2 as X\u0303TX X\u0303Ty\ny\u0303TX y\u0303Ty\n =  XTX +N XTy +n\nyTX +nT yTy +m  (15)\nwhere N is a symmetric p\u00d7 p-matrix, n is a p-dimensional vector and m is a scalar, whose coordinates are sampled i.i.d from N (0,\u22062).\nUsing the output of Algorithm 2, it is simple to derive analogues of \u03b2\u0302 and \u2016\u03b6\u20162 (Equations (1) and (2))\n\u03b2\u0303 = ( X\u0303TX )\u22121 X\u0303Ty = ( XTX +N )\u22121 (XTy +n)\n(16)\n\u2016\u0303\u03b6\u20162 = y\u0303Ty \u2212 2 y\u0303TX \u03b2\u0303 + \u03b2\u0303 T X\u0303TX \u03b2\u0303\n= y\u0303Ty \u2212 y\u0303TX X\u0303TX \u22121 X\u0303Ty (17)\nWe now argue that it is possible to use \u03b2\u0303j and \u2016\u0303\u03b6\u20162 to get a confidence interval for \u03b2j under certain conditions. Theorem D.1. Fix \u03b1, \u03bd \u2208 (0, 12 ). Assume that there exists \u03b7 \u2208 (0, 12 ) s.t. \u03c3min(X TX) > \u2206 \u221a p ln(1/\u03bd)/\u03b7. Under the homoscedastic model, given \u03b2 and \u03c32, if we assume also that \u2016\u03b2\u2016 \u2264 B and \u2016\u03b2\u0302\u2016 = \u2016(XTX)\u22121XTy\u2016 \u2264 B, then w.p. \u2265 1\u2212 \u03b1\u2212 \u03bd it holds that |\u03b2j \u2212 \u03b2\u0303j | it at most O ( \u03c1 \u00b7 \u221a( X\u0303TX \u22121 j,j + \u2206 \u221a p ln(1/\u03bd) \u00b7 X\u0303TX \u22122 j,j ) ln(1/\u03b1)\n20It is easy to see that the l2-global sensitivity of the mapping A 7\u2192 ATA is \u221d B4. Fix any A1, A2 that differ on one row which is some vector v with \u2016v\u2016 = B in A1 and the all zero vector in A2. Then GS22 = \u2016AT1A1 \u2212 AT2A2\u20162F = \u2016vvT \u20162F = trace(vvT \u00b7 vvT) = (vTv)2 = B4.\n+ \u2206 \u221a X\u0303TX \u22122 j,j \u00b7 ln(1/\u03bd) \u00b7 (B \u221a p+ 1) ) where \u03c1 is such that \u03c12 is w.h.p an upper bound on \u03c32, defined as\n\u03c12 def = ( 1\u221a\nn\u2212p\u22122 \u221a ln(4/\u03b1) )2 \u00b7(\n\u2016\u0303\u03b6\u20162 \u2212 C \u00b7 ( \u2206 B2 \u221a p\n1\u2212\u03b7\n\u221a ln(1/\u03bd) + \u22062\u2016X\u0303TX \u22121 \u2016F \u00b7 ln(p/\u03bd) )) for some large constant C.\nWe comment that in practice, instead of using \u03c1, it might be better to use the MLE of \u03c32, namely:\n\u03c32 def = 1n\u2212p\n( \u2016\u0303\u03b6\u20162 + \u22062\u2016X\u0303TX \u22121 \u2016F )\ninstead of \u03c12, the upper bound we derived for \u03c32. (Replacing an unknown variable with its MLE estimator is a common approach in applied statistics.) Note that the assumption that \u2016\u03b2\u2016 \u2264 B is fairly benign once we assume each row has bounded l2-norm. The assumption \u2016\u03b2\u0302\u2016 \u2264 B simply assumes that \u03b2\u0302 is a reasonable estimation of \u03b2 , which is likely to hold if we assume that XTX is well-spread. The assumption about the magnitude of the least singular value of XTX is therefore the major one. Nonetheless, in the case we considered before where each row inX is sampled i.i.d from N (0,\u03a3), this assumption merely means that n is large enough s.t. n = \u2126\u0303(\u2206 \u221a p ln(1/\u03bd)\n\u03b7\u00b7\u03c3min(\u03a3) ).\nIn order to prove Theorem D.1, we require the following proposition.\nProposition D.2. Fix any \u03bd \u2208 (0, 12 ). Fix any matrix M \u2208 Rp\u00d7p. Let v \u2208 Rp be a vector with each coordinate sampled independently from a Gaussian N (0,\u22062). Then we have that Pr [ \u2016Mv\u2016 > \u2206 \u00b7 \u2016M\u2016F \u221a 2 ln(2p/\u03bd) ] < \u03bd.\nProof. Given M , we have that Mv \u223c N (0,\u22062 \u00b7MMT). Denoting M \u2019s singular values as sv1, . . . , svp, we can rotate Mv without affecting its l2-norm and infer that \u2016Mv|2 is distributed like a sum on p independent Gaussians, each sampled fromN (0,\u22062 \u00b7 sv2i ). Standard union bound gives that w.p. \u2265 1 \u2212 \u03bd non of the p Gaussians exceeds its standard deviation by a factor of \u221a 2 ln(2p/\u03bd). Hence, w.p.\n\u2265 1 \u2212 \u03bd it holds that \u2016Mv\u20162 \u2264 2\u22062 \u2211 i sv 2 i ln(2p/\u03bd) = 2\u22062 \u00b7 trace(MMT) \u00b7 ln(2p/\u03bd).\nOur proof also requires the use of the following equality, that holds for any invertible A and any matrix B s.t. I + B \u00b7A\u22121 is invertible:\n(A+B) \u22121 = A\u22121 \u2212A\u22121 ( I +BA\u22121 )\u22121 BA\u22121\nIn our case, we have\nX\u0303TX \u22121\n= (XTX +N)\u22121\n= (XTX)\u22121 \u2212 (XTX)\u22121 ( I +N(XTX)\u22121 )\u22121 N(XTX)\u22121\n= (XTX)\u22121 ( I \u2212 ( I +N(XTX)\u22121 )\u22121 N(XTX)\u22121 ) def = (XTX)\u22121 ( I \u2212 Z \u00b7 (XTX)\u22121 ) (18)\nProof of Theorem D.1. Fix \u03bd > 0. First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al., 2014) in their analysis), to see that w.p. \u2265 1 \u2212 \u03bd/6 we have \u2016N\u2016 = O(\u2206 \u221a p ln(1/\u03bd)). And so, for the remainder of the proof we fix N subject to having bounded operator norm. Note that by fixing N we\nfix X\u0303TX .\nRecall that in the homoscedastic model, y = X\u03b2 + e with each coordinate of e sampled i.i.d from N (0, \u03c32). We therefore have that\n\u03b2\u0303 = X\u0303TX \u22121 (XTy +n) = X\u0303TX \u22121 (XTX\u03b2 +XTe +n)\n= X\u0303TX \u22121 (X\u0303TX \u2212N)\u03b2 + X\u0303TX \u22121 XTe + X\u0303TX \u22121 n\n= \u03b2 \u2212 X\u0303TX \u22121 N\u03b2 + X\u0303TX \u22121 XTe + X\u0303TX \u22121 n\nDenoting the j-th row of X\u0303TX \u22121 as X\u0303TX \u22121\nj\u2192 we deduce:\n\u03b2\u0303j = \u03b2j \u2212 X\u0303TX \u22121 j\u2192N\u03b2 + X\u0303 TX \u22121 j\u2192X Te + X\u0303TX \u22121 j\u2192n\n(19)\nWe na\u0131\u0308vely bound the size of the term X\u0303TX \u22121 j\u2192N\u03b2 by \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u2225 \u2016N\u2016\u2016\u03b2\u2016 =\nO (\u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u2225 \u00b7B\u2206\u221ap ln(1/\u03bd)). To bound X\u0303TX \u22121\nj\u2192X Te note that e is cho-\nsen independently of X\u0303TX and since e \u223c N (0, \u03c32I) we have X\u0303TX \u22121\nj\u2192X Te \u223c N ( 0, \u03c32 \u00b7 eTj X\u0303TX \u22121 \u00b7XTX \u00b7 X\u0303TX \u22121 ej ) . Since\nwe have\nX\u0303TX \u22121 \u00b7XTX \u00b7 X\u0303TX \u22121\n= X\u0303TX \u22121 \u00b7 (X\u0303TX \u2212N) \u00b7 X\u0303TX \u22121\n= X\u0303TX \u22121 \u2212 X\u0303TX \u22121 \u00b7N \u00b7 X\u0303TX \u22121\nwe can bound the variance of X\u0303TX \u22121\nj\u2192X Te by\n\u03c32 ( X\u0303TX \u22121 j,j + \u2016N\u2016 \u00b7 \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u22252 ) . Appealing to\nGaussian concentration bounds, we have that w.p. \u2265 1 \u2212 \u03b1/2 the absolute value of this Gaussian is at most\nO \u221a\u221a\u221a\u221a(X\u0303TX \u22121j,j + \u2206\u221ap ln(1/\u03bd) \u00b7 \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u22252 ) \u03c32 ln(1/\u03b1) . To bound X\u0303TX \u22121\nj\u2192n note that n \u223c N (0,\u22062I) is sampled independently of X\u0303TX . We therefore have that X\u0303TX \u22121 j\u2192n \u223c N (0,\u22062 \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u22252). Gaussian concentra-\ntion bounds give that w.p\u2265 1\u2212\u03bd/6 we have |X\u0303TX \u22121\nj\u2192n| =\nO ( \u2206 \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u2225\u221aln(1/\u03bd)). Plugging this into our above bounds on all terms that appear in Equation (19) we have that w.p. \u2265 1 \u2212 \u03bd/2 \u2212 \u03b1/2 we have that\n\u2223\u2223\u2223\u03b2\u0303j \u2212 \u03b2j\u2223\u2223\u2223 is at most O\n(\u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u2225 \u00b7B\u2206\u221ap ln(1/\u03bd)) +O \u03c3 \u221a\u221a\u221a\u221a(X\u0303TX \u22121j,j + \u2206\u221ap ln(1/\u03bd) \u00b7 \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u22252 ) ln(1/\u03b1)\n +O ( \u2206 \u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u2225\u221aln(1/\u03bd))\nNote that due to the symmetry of X\u0303TX we have\u2225\u2225\u2225\u2225X\u0303TX \u22121j\u2192\u2225\u2225\u2225\u22252 = X\u0303TX \u22122j,j (the (j, j)-coordinate of the matrix X\u0303TX \u22122 ), thus |\u03b2\u0303j \u2212 \u03b2j | is at most\nO ( \u03c3 \u00b7 \u221a( X\u0303TX \u22121 j,j + \u2206 \u221a p ln(1/\u03bd) \u00b7 X\u0303TX \u22122 j,j ) ln(1/\u03b1)\n+ \u2206 \u221a X\u0303TX \u22122 j,j \u00b7 ln(1/\u03bd) \u00b7 (B \u221a p+ 1) ) (20)\nAll of the terms appearing in Equation (20) are known\ngiven X\u0303TX , except for \u03c3 \u2014 which is a parameter of the model. Next, we derive an upper bound on \u03c3 which we can then plug into Equation (20) to complete the proof of the theorem and derive a confidence interval for \u03b2j .\nRecall Equation (17), according to which we have\n\u2016\u0303\u03b6\u20162 = y\u0303Ty \u2212 y\u0303TX X\u0303TX \u22121 X\u0303Ty\n(18) = yTy +m\n\u2212 (yTX +nT)(XTX)\u22121(I \u2212 Z \u00b7 (XTX)\u22121)(XTy +n) = yTy +m\n\u2212 yTX(XTX)\u22121XTy + yTX(XTX)\u22121Z(XTX)\u22121XTy\n\u2212 2yTX(XTX)\u22121n + 2yTX(XTX)\u22121Z(XTX)\u22121n\n\u2212nT(XTX)\u22121(I \u2212 Z \u00b7 (XTX)\u22121)n\nRecall that \u03b2\u0302 = (XTX)\u22121XTy , and so we have\n= yT ( I \u2212X(XTX)\u22121XT ) y +m\u2212 \u03b2\u0302 T Z\u03b2\u0302\n\u2212 2\u03b2\u0302 T (I \u2212 Z(XTX)\u22121)n \u2212nTX\u0303TX \u22121 n (21)\nand of course, both n and m are chosen independently of\nX\u0303TX and y .\nBefore we bound each term in Equation (21), we first give a bound on \u2016Z\u2016. Recall, Z = ( I +N(XTX)\u22121 )\u22121 N . Recall our assumption (given in the statement of Theorem D.1) that \u03c3min(XTX) \u2265 \u2206\u03b7 \u221a p ln(1/\u03bd). This implies that \u2016N(XTX)\u22121\u2016 \u2264 \u2016N\u2016\u00b7\u03c3min(XTX)\u22121 = O(\u03b7). Hence\n\u2016Z\u2016 \u2264 (\u2016I+N(XTX)\u22121\u2016)\u22121 \u00b7\u2016N\u2016 = O ( \u2206 \u221a p ln(1/\u03bd)\n1\u2212\u03b7 ) Moreover, this implies that \u2016Z(XTX)\u22121\u2016 \u2264 O ( \u03b7\n1\u2212\u03b7 ) and that \u2016I \u2212 Z(XTX)\u22121\u2016 \u2264 O ( 1\n1\u2212\u03b7\n) .\nArmed with these bounds on the operator norms of Z and (I\u2212Z(XTX)\u22121) we bound the magnitude of the different terms in Equation (21).\n\u2022 The term yT (I \u2212XX+)y is the exact term from the standard OLS, and we know it is distributed like \u03c32 \u00b7 \u03c72n\u2212p distribution. Therefore, it is greater than \u03c32( \u221a n\u2212 p \u2212 2 \u221a ln(4/\u03b1))2 w.p. \u2265 1\u2212 \u03b1/2.\n\u2022 The scalar m sampled from m \u223c N (0,\u22062) is bounded by O(\u2206 \u221a ln(1/\u03bd)) w.p. \u2265 1\u2212 \u03bd/8.\n\u2022 Since we assume \u2016\u03b2\u0302\u2016 \u2264 B, the term \u03b2\u0302 T Z\u03b2\u0302 is upper bounded by B2\u2016Z\u2016 = O ( B2\u2206 \u221a p ln(1/\u03bd)\n1\u2212\u03b7\n) .\n\u2022 Denote zTn = 2\u03b2\u0302 T\n(I\u2212Z(XTX)\u22121)n. We thus have that zTn \u223c N (0,\u22062\u2016z\u20162) and that its magnitude is at\nmost O(\u2206 \u00b7 \u2016z\u2016 \u221a\nln(1/\u03bd)) w.p. \u2265 1 \u2212 \u03bd/8. We can upper bound \u2016z\u2016 \u2264 2\u2016\u03b2\u0302\u2016 \u2016I \u2212 Z(XTX)\u22121\u2016 = O( B1\u2212\u03b7 ), and so this term\u2019s magnitude is upper\nbounded by O ( \u2206\u00b7B \u221a ln(1/\u03bd)\n1\u2212\u03b7\n) .\n\u2022 Given our assumption about the least singular value of XTX and with the bound on \u2016N\u2016, we have that \u03c3min(X\u0303TX) \u2265 \u03c3min(XTX) \u2212 \u2016N\u2016 > 0 and so the symmetric matrix X\u0303TX is a PSD. Therefore,\nthe term nTX\u0303TX \u22121 n = \u2016X\u0303TX \u22121/2 n\u20162 is strictly positive. Applying Proposition D.2 we have that w.p. \u2265 1 \u2212 \u03bd/8 it holds that nTX\u0303TX \u22121 n \u2264\nO ( \u22062\u2016X\u0303TX \u22121 \u2016F \u00b7 ln(p/\u03bd) ) .\nPlugging all of the above bounds into Equation (21) we get that w.p. \u2265 1\u2212 \u03bd/2\u2212 \u03b1/2 it holds that \u03c32 \u2264 (\n1\u221a n\u2212p\u22122 \u221a ln(4/\u03b1) )2 \u00b7(\n\u2016\u0303\u03b6\u20162 +O ( (1 + B2 \u221a p+B 1\u2212\u03b7 )\u2206 \u221a ln(1/\u03bd) + \u22062\u2016X\u0303TX \u22121 \u2016F \u00b7 ln(p/\u03bd) ))\nand indeed, the RHS is the definition of \u03c12 in the statement of Theorem D.1."}, {"heading": "E. Experiment: Additional Figures", "text": "To complete our discussion about the experiments we have conducted, we attach here additional figures, plotting both the t-value approximations we get from both algorithms, and the \u201chigh-level decision\u201d of whether correctly reject or not-reject the null hypothesis (and with what sign). First, we show the distribution of the t-value approximation for coordinates that should be rejected, in Figure 2, and then the decision of whether to reject or not based on this t-value \u2014 and whether it was right, conservative (we didn\u2019t reject while we needed to) or wrong (we rejected with the wrong sign, or rejected when we shouldn\u2019t have rejected) in Figure 3. As one can see, Algorithm 1 has far lower t-values (as expected) and therefore is much more conservative. In fact, it tends to not-reject coordinate 1 of the real-data even on the largest value of n (Figure 3c).\nHowever, because Algorithm 1 also has much smaller variance, it also does not reject when it ought to notreject, whereas Algorithm 2 erroneiously rejects the nullhypotheses. This can be seen in Figures 4 and 5."}], "references": [{"title": "Statistical Methods for the Social Sciences", "author": ["A. Agresti", "B. Finlay"], "venue": null, "citeRegEx": "Agresti and Finlay,? \\Q2009\\E", "shortCiteRegEx": "Agresti and Finlay", "year": 2009}, {"title": "Private empirical risk minimization: Efficient algorithms and tight error bounds", "author": ["R. Bassily", "A. Smith", "A. Thakurta"], "venue": "In FOCS,", "citeRegEx": "Bassily et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2014}, {"title": "The Johnson-Lindenstrauss transform itself preserves differential privacy", "author": ["J. Blocki", "A. Blum", "A. Datta", "O. Sheffet"], "venue": "In FOCS,", "citeRegEx": "Blocki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blocki et al\\.", "year": 2012}, {"title": "Convergence rates for differentially private statistical estimation", "author": ["Chaudhuri", "Kamalika", "Hsu", "Daniel J"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "Differentially private empirical risk minimization", "author": ["Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Local privacy and statistical minimax rates", "author": ["Duchi", "John C", "Jordan", "Michael I", "Wainwright", "Martin J"], "venue": "In FOCS, pp", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Differential privacy and robust statistics", "author": ["C. Dwork", "J. Lei"], "venue": "In STOC,", "citeRegEx": "Dwork and Lei,? \\Q2009\\E", "shortCiteRegEx": "Dwork and Lei", "year": 2009}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Dwork", "Cynthia", "Kenthapadi", "Krishnaram", "McSherry", "Frank", "Mironov", "Ilya", "Naor", "Moni"], "venue": "In EUROCRYPT,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Dwork", "Cynthia", "Mcsherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"], "venue": "In TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Analyze gauss - optimal bounds for privacy preserving principal component analysis", "author": ["Dwork", "Cynthia", "Talwar", "Kunal", "Thakurta", "Abhradeep", "Zhang", "Li"], "venue": "In STOC,", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Private false discovery rate control", "author": ["Dwork", "Cynthia", "Su", "Weijie", "Zhang", "Li"], "venue": "CoRR, abs/1511.03803,", "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics, 12:55\u201367,", "citeRegEx": "Hoerl and Kennard,? \\Q1970\\E", "shortCiteRegEx": "Hoerl and Kennard", "year": 1970}, {"title": "What can we learn privately", "author": ["S. Kasiviswanathan", "H. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "In FOCS,", "citeRegEx": "Kasiviswanathan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kasiviswanathan et al\\.", "year": 2008}, {"title": "Private convex optimization for empirical risk minimization with applications to high-dimensional regression", "author": ["Kifer", "Daniel", "Smith", "Adam D", "Thakurta", "Abhradeep"], "venue": "In COLT,", "citeRegEx": "Kifer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kifer et al\\.", "year": 2012}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["B. Laurent", "P. Massart"], "venue": "The Annals of Statistics, 28(5),", "citeRegEx": "Laurent and Massart,? \\Q2000\\E", "shortCiteRegEx": "Laurent and Massart", "year": 2000}, {"title": "On lower bounds for the smallest eigenvalue of a hermitian positivedefinite matrix", "author": ["E.M. Ma", "Zarowski", "Christopher J"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ma et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ma et al\\.", "year": 1995}, {"title": "Linear Model Theory: Univariate, Multivariate, and Mixed Models", "author": ["Muller", "Keith E", "Stewart", "Paul W"], "venue": null, "citeRegEx": "Muller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Muller et al\\.", "year": 2006}, {"title": "Randomized sketches of convex programs with sharp guarantees", "author": ["M. Pilanci", "M. Wainwright"], "venue": "In ISIT,", "citeRegEx": "Pilanci and Wainwright,? \\Q2014\\E", "shortCiteRegEx": "Pilanci and Wainwright", "year": 2014}, {"title": "Iterative hessian sketch: Fast and accurate solution approximation for constrained least-squares", "author": ["Pilanci", "Mert", "Wainwright", "Martin J"], "venue": "CoRR, abs/1411.0347,", "citeRegEx": "Pilanci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pilanci et al\\.", "year": 2014}, {"title": "Linear statistical inference and its applications", "author": ["Rao", "C. Radhakrishna"], "venue": null, "citeRegEx": "Rao and Radhakrishna.,? \\Q1973\\E", "shortCiteRegEx": "Rao and Radhakrishna.", "year": 1973}, {"title": "Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing", "author": ["Rogers", "Ryan M", "Vadhan", "Salil P", "Lim", "Hyun-Woo", "Gaboardi", "Marco"], "venue": "In ICML,", "citeRegEx": "Rogers et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2016}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Rudelson", "Mark", "Vershynin", "Roman"], "venue": "Comm. Pure Appl. Math, pp", "citeRegEx": "Rudelson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rudelson et al\\.", "year": 2009}, {"title": "Improved approx. algs for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "In FOCS,", "citeRegEx": "Sarl\u00f3s,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s", "year": 2006}, {"title": "Private approximations of the 2nd-moment matrix using existing techniques in linear regression", "author": ["O. Sheffet"], "venue": "CoRR, abs/1507.00056,", "citeRegEx": "Sheffet,? \\Q2015\\E", "shortCiteRegEx": "Sheffet", "year": 2015}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["Smith", "Adam D"], "venue": "In STOC, pp", "citeRegEx": "Smith and D.,? \\Q2011\\E", "shortCiteRegEx": "Smith and D.", "year": 2011}, {"title": "Impact of HbA1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient", "author": ["B. Strack", "J. DeShazo", "C. Gennings", "J. Olmo", "S. Ventura", "K. Cios", "J. Clore"], "venue": "records. BioMed Research International,", "citeRegEx": "Strack et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Strack et al\\.", "year": 2014}, {"title": "Topics in Random Matrix Theory", "author": ["T. Tao"], "venue": "American Mathematical Soc.,", "citeRegEx": "Tao,? \\Q2012\\E", "shortCiteRegEx": "Tao", "year": 2012}, {"title": "Differentially private feature selection via stability arguments, and the robustness of the lasso", "author": ["Thakurta", "Abhradeep", "Smith", "Adam"], "venue": "In COLT,", "citeRegEx": "Thakurta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Thakurta et al\\.", "year": 2013}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["A.N. Tikhonov"], "venue": "Soviet Math. Dokl.,", "citeRegEx": "Tikhonov,? \\Q1963\\E", "shortCiteRegEx": "Tikhonov", "year": 1963}, {"title": "Privacy-preserving data sharing for genome-wide association studies", "author": ["Uhler", "Caroline", "Slavkovic", "Aleksandra B", "Fienberg", "Stephen E"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "Uhler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uhler et al\\.", "year": 2013}, {"title": "Private multiplicative weights beyond linear queries", "author": ["J. Ullman"], "venue": "In PODS,", "citeRegEx": "Ullman,? \\Q2015\\E", "shortCiteRegEx": "Ullman", "year": 2015}, {"title": "Differential privacy for clinical trial data: Preliminary evaluations", "author": ["D. Vu", "A. Slavkovic"], "venue": "In ICDM,", "citeRegEx": "Vu and Slavkovic,? \\Q2009\\E", "shortCiteRegEx": "Vu and Slavkovic", "year": 2009}, {"title": "Differentially private hypothesis testing, revisited", "author": ["Wang", "Yue", "Lee", "Jaewoo", "Kifer", "Daniel"], "venue": "CoRR, abs/1511.03376,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Mixture of gaussian models and bayes error under differential privacy", "author": ["B. Xi", "M. Kantarcioglu", "A. Inan"], "venue": "In CODASPY. ACM,", "citeRegEx": "Xi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xi et al\\.", "year": 2011}, {"title": "Comparison with Existing Bounds. Sarlos\u2019 work (2006) utilizes the fact that when r, the numbers of rows in R, is large enough", "author": ["n\u2212p"], "venue": null, "citeRegEx": ".,? \\Q2006\\E", "shortCiteRegEx": ".", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014).", "startOffset": 415, "endOffset": 435}, {"referenceID": 2, "context": "We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014).", "startOffset": 261, "endOffset": 302}, {"referenceID": 9, "context": "We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014).", "startOffset": 261, "endOffset": 302}, {"referenceID": 4, "context": "Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function \u2014 but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private.", "startOffset": 85, "endOffset": 131}, {"referenceID": 1, "context": "Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function \u2014 but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private.", "startOffset": 85, "endOffset": 131}, {"referenceID": 10, "context": ") We leave this approach \u2014 as well as performing private hypothesis testing using a PTR-type algorithm (Dwork & Lei, 2009) (output merely reject / don\u2019t-reject decision without justification), or releasing only relevant tests judging by their p-values (Dwork et al., 2015) \u2014 for future work.", "startOffset": 252, "endOffset": 272}, {"referenceID": 28, "context": "In this case, solving the linear regression problem on the projected A\u2032 approximates the solution for Ridge Regression (Tikhonov, 1963; Hoerl & Kennard, 1970).", "startOffset": 119, "endOffset": 158}, {"referenceID": 9, "context": "In Section 5 we discuss the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014) that outputs a noisy version of a covariance of a given matrix using additive noise rather than multiplicative noise.", "startOffset": 54, "endOffset": 74}, {"referenceID": 33, "context": "Empirical work (Xi et al., 2011) shows that Analyze Gauss\u2019s output might be non-PSD if the input has small singular values, and this results in truly bad regressors.", "startOffset": 15, "endOffset": 32}, {"referenceID": 5, "context": "Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence).", "startOffset": 92, "endOffset": 187}, {"referenceID": 10, "context": "Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence).", "startOffset": 92, "endOffset": 187}, {"referenceID": 29, "context": "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).", "startOffset": 203, "endOffset": 285}, {"referenceID": 32, "context": "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).", "startOffset": 203, "endOffset": 285}, {"referenceID": 20, "context": "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).", "startOffset": 203, "endOffset": 285}, {"referenceID": 1, "context": "Observe, overall this result is similar in nature to many other results in differentially private learning (Bassily et al., 2014) which are of the form \u201cwithout privacy, in order to achieve a total loss of \u2264 \u03b7 we have a sample complexity bound of some N\u03b7; and with differential privacy the sample complexity increases to N\u03b7 + \u03a9( \u221a N\u03b7/ ).", "startOffset": 107, "endOffset": 129}, {"referenceID": 28, "context": "Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large.", "startOffset": 20, "endOffset": 59}, {"referenceID": 25, "context": "We ran the two algorithms over diabetes dataset collected over ten years (1999-2008) taken from the UCI repository (Strack et al., 2014).", "startOffset": 115, "endOffset": 136}, {"referenceID": 30, "context": "Lastly the author thanks the anonymous referees for many helpful suggestions in general and for a reference to (Ullman, 2015) in particular.", "startOffset": 111, "endOffset": 125}, {"referenceID": 23, "context": "The proof of the theorem is based on the fact the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork & Lei, 2009) with the differentially private analysis of the Johnson-Lindenstrauss transform of (Sheffet, 2015).", "startOffset": 253, "endOffset": 268}, {"referenceID": 23, "context": "1 from (Sheffet, 2015) that states that given a matrix A whose all of its singular values at greater than T ( , \u03b4) where T ( , \u03b4) = 2B (\u221a 2r ln(4/\u03b4) + 2 ln(4/\u03b4) ) , publishing RA is ( , \u03b4)differentially private for a r-row matrix R whose entries sampled are i.", "startOffset": 7, "endOffset": 22}, {"referenceID": 7, "context": "It is known (Dwork et al., 2006b) that if ALG outputs a vector in R such that for any A and A\u2032 it holds that \u2016ALG(A) \u2212 ALG(A)\u20161 \u2264 B, then adding Laplace noise Lap(1/ ) to each coordinate of the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and A\u2032 it holds that \u2016ALG(A)\u2212ALG(A)\u20162 \u2264 \u2206 then adding Gaussian noise N (0,\u2206 \u00b7 2 ln(2/\u03b4) 2 ) to each coordinate of the output of ALG(A) satisfies ( , \u03b4)-differential privacy.", "startOffset": 13, "endOffset": 263}, {"referenceID": 22, "context": "In this setting, Sarlos\u2019 work (Sarl\u00f3s, 2006) (Theorem 12(3)) guarantees that w.", "startOffset": 30, "endOffset": 44}, {"referenceID": 4, "context": ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2 of the linear regression give a data-independent12 bound of \u2016\u03b2 \u2212 \u03b2\u0302\u2016 = \u00d5(p/ ).", "startOffset": 60, "endOffset": 120}, {"referenceID": 1, "context": ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2 of the linear regression give a data-independent12 bound of \u2016\u03b2 \u2212 \u03b2\u0302\u2016 = \u00d5(p/ ).", "startOffset": 60, "endOffset": 120}, {"referenceID": 30, "context": ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator \u03b2 of the linear regression give a data-independent12 bound of \u2016\u03b2 \u2212 \u03b2\u0302\u2016 = \u00d5(p/ ).", "startOffset": 60, "endOffset": 120}, {"referenceID": 23, "context": "1 from (Sheffet, 2015) gives that w.", "startOffset": 7, "endOffset": 22}, {"referenceID": 28, "context": "Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large.", "startOffset": 20, "endOffset": 59}, {"referenceID": 9, "context": "To complete the picture, we now analyze the \u201cAnalyze Gauss\u201d algorithm of Dwork et al (Dwork et al., 2014).", "startOffset": 85, "endOffset": 105}, {"referenceID": 26, "context": "First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al.", "startOffset": 69, "endOffset": 80}, {"referenceID": 9, "context": "First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al., 2014) in their analysis), to see that w.", "startOffset": 95, "endOffset": 115}], "year": 2017, "abstractText": "Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives tvalues \u2014 representing the likelihood of each real value to be the true correlation. Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the \u201cAnalyze Gauss\u201d algorithm (Dwork et al., 2014).", "creator": "LaTeX with hyperref package"}}}