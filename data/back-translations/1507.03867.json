{"id": "1507.03867", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jul-2015", "title": "Rich Component Analysis", "abstract": "In many cases, we have multiple datasets (also called views) that capture different and overlapping aspects of the same phenomenon. Often, we are interested in finding patterns that are unique to one or a subset of views. For example, we might have a set of molecular observations and a set of physiological observations for the same group of individuals, and we want to quantify molecular patterns that are not correlated with physiology. Although this is a common problem, this is a major challenge when the correlations come from complex distributions. In this paper, we are developing the general framework of Rich Component Analysis (RCA) to learn settings in which observations from different views are driven by different groups of latent components, and each component can be a complex, high-dimensional distribution. We are introducing algorithms based on cumulative extraction that have been shown to learn each of the components without showing the other components to be modulated by different groups of latent components, and each component can also be integrated with suchastic samples, like RCA.", "histories": [["v1", "Tue, 14 Jul 2015 14:38:23 GMT  (82kb,D)", "http://arxiv.org/abs/1507.03867v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["rong ge 0001", "james zou"], "accepted": true, "id": "1507.03867"}, "pdf": {"name": "1507.03867.pdf", "metadata": {"source": "CRF", "title": "Rich Component Analysis", "authors": ["Rong Ge", "James Zou"], "emails": ["rongge@microsoft.com", "jazo@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "A hallmark of modern data deluge is the prevalence of complex data that capture different aspects of some common phenomena. For example, for a set of patients, it\u2019s common to have multiple modalities of molecular measurements for each individual (gene expression, genotyping, etc.) as well as physiological attributes. Each set of measurements corresponds to a view on the samples. The complexity and the heterogeneity of the data is such that it\u2019s often not feasible to build a joint model for all the data. Moreover, if we are particularly interested in one aspect of the problem (e.g. patterns that are specific to a subset of genes that are not shared across all genes), it would be wasteful of computational and modeling resources to model the interactions across all the data.\nMore concretely, suppose we have two sets (views) of data, U and V , on a common collection of samples. We model this as U = S1 +S2 and V = AS2 +S3, where S1 captures the latent component specific to U , S3 is specific to V , and S2 is common to both U and V and is related in the two views by an unknown linear transformation A. Each component Si can be a complex, high-dimensional distribution. The observed samples from U and V are componentwise linear combinations of the unobserved samples from Si. To model all the data, we would need to jointly model all three Si, which can have prohibitive sample/computation complexity and also prone to model misspecification. Ideally, if we are only interested in the component that\u2019s unique to the first view, we would simply write down a model for S1 without making any parametric assumptions about S2 and S3, except that they are independent.\nIn this paper, we develop a general framework of Rich Component Analysis (RCA) to explore such multi-component, multi-view datasets. Our framework allows for learning an arbitrarily complex model of a specific component of the data, Si, without having to make parametric assumptions about other components Sj . This allows the analyst to focus on the most salient aspect of data analysis. The main conceptual contribution is the development of new algorithms to learn parameters of complex distributions without any samples from that distribution. In the two-view example, we do not observe samples from our model of interest, S1. Instead the observations from U are compositions of true samples from S1 with complex signal from another process S2 which is shared with V . Our approach performs consistent parameter estimation of S1 without modeling S2, S3.\nar X\niv :1\n50 7.\n03 86\n7v 1\n[ cs\n.L G\n] 1\n4 Ju\nl 2 01\n5\nOutline. RCA consists of two stages: 1) from the observed data, extract all the cumulants of the component that we want to model; 2) using the cumulants, perform method-of-moments or maximum likelihood estimation (MLE) of model parameters via polynomial approximations to gradient descent. We introduce the relevant properties of cumulants and tensors in Section 2. In Section 3, we develop the formal models for Rich Component Analysis (RCA) and the cumulant extraction algorithms. We discuss how RCA differs from existing models. Section 4 shows how to integrate the extracted cumulants with method-of-moments or stochastic gradient descent for MLE inference. We show the performance gains of RCA in Section 5. All the proofs are in the Appendix."}, {"heading": "2 Preliminaries", "text": "In this section we introduce the basics of cumulants. For more information please refer to Appendix A. Cumulants provide an alternative way to describe the correlations of random variables. Unlike moments, cumulants have the nice property that the cumulant of sum of independent random variables equals to the sum of cumulants. For a random variable X \u2208 R the cumulant is defined to be the coefficients of the cumulant generating function logE[etX ].\nWe can also define cross-cumulants which are cumulants for different variables (e.g. covariance). For n variables X1, ..., Xn, their cross-cumulant can be computed using the following formula:\n\u03bat(X1, ..., Xt) = \u2211 \u03c0 (|\u03c0| \u2212 1)!(\u22121)|\u03c0|\u22121 \u220f B\u2208\u03c0 E[ \u220f i\u2208B Xi].\nIn this formula, \u03c0 is enumerated over all partitions of [t], |\u03c0| is the number of parts andB runs through the list of parts. We also use \u03bat(X) \u2261 \u03bat(X, ...,X) when it\u2019s the same random variable.\nWe can similarly define cumulants for multivariate distributions. For random vector X \u2208 Rd, the t-th order cumulant (and t-th order moment) is an object in Rdt (a t-th order tensor). The (i1, ..., it)-th coordinate of cumulant tensor is \u03bat(Xi1 , Xi2 , ..., Xit). We often unfold tensors into matrices. Tensor T \u2208 Rd t\nunfolds into matrix M = unfold(T ) \u2208 Rdt\u22121\u00d7d: M(i1,...,it\u22121),it = Ti1,...,it . Cumulants have several nice properties that we summarize below.\nFact Suppose X1, ..., Xt are random variables in Rd. The t-th order cumulant \u03bat(X1, ..., Xt) is a tensor in Rd t\nthat have the following properties:\n1. (Independence) If (X1, ..., Xt) and (Y1, ..., Yt) are independent, then \u03bat(X1+Y1, ..., Xt+Yt) = \u03bat(X1, ..., Xt)+ \u03bat(Y1, ..., Yt).\n2. (Linearity) \u03bat(c1X1, ..., ctXt) = c1c2 \u00b7 \u00b7 \u00b7 ct\u03bat(X1, ..., Xt), more generally we can apply arbitrary linear transformations to multi-variate cumulants (see Appendix A).\n3. (Computation) The cumulant \u03bat(X1, ..., Xt)can be computed in O((td)t) time.\nThe second order cross-cumulant, \u03ba2(X,Y ) is equal to the covariance E[(X \u2212 E[X])(Y \u2212 E[Y ])]. Higher cumulants measures higher-order correlations and also provide a measure of the deviation from Gaussianity\u2013all 3rd and higher order cumulants of Gaussian random variables are zero."}, {"heading": "3 Rich Component Analysis", "text": "In this section, we show how to use cumulant to disentangle complex latent components. The key ideas and applications of RCA are captured in the contrastive learning setting when there are two views. We introduce this model next and then show how to extend it to general settings."}, {"heading": "3.1 RCA for contrastive learning", "text": "Recall the example in the introduction where we have two views of the data, formally,\nU = S1 + S2, V = AS2 + S3. (1)\nHere, S1, S2, S3 \u2208 Rd are independent random variables that can have complicated distributions; A \u2208 Rd\u00d7d is an unknown linear transformation. The observations consist of pairs of samples (u, v). Each pair is generated by drawing independent samples si \u223c Si, i = 1, 2, 3 and adding these samples component-wise to obtain u = s1 + s2 and v = As2 + s3. Note that the same s2 shows up in both u and v, introducing correlation between the two views. We are interested in learning properties about Si, for example learning its maximum likelihood (MLE) parameters. For concreteness, we focus our discussion on learning S1 although our techniques also apply to S2 and S3. We don\u2019t have any samples from S1. The observations of U involves a potentially complicated perturbation by S2. Our hope is to remove this perturbation by utilizing the second view V , and we would like to do this without assuming a particular model for S2 or S3.\nNote that the problem is inherently under-determined: it is impossible to find the means of S1, S2, S3 without any additional information. This is in some sense the only ambiguity, as we will see if we know the mean of one distribution it is possible to extract all order cumulants of S1, S2, S3. For simplicity throughout this section we assume the means of S1, S2, S3 are 0 (given the mean of any of S1, S2, S3, we can always use the means of U and V to compute the means of other distributions, and shift them to have mean 0).\nDetermining linear transformation First we can find A by the following formula:\nA> = unfold(\u03ba4(V,U, U, U)) \u2020unfold(\u03ba4(V,U, U, V )). (2)\nLemma 3.1. Suppose the unfolding of the 4-th order cumulant unfold(\u03ba4(AS2, S2, S2, S2)) has full rank, given the exact cumulants \u03ba4(V,U, U, U) and \u03ba4(V,U, U, V ), the above algorithm finds the correct linear transformation A in time O(d5).\nIntuitively, since only S2 appears in both U and V , the cross-cumulants \u03ba4(V,U, U, U) and \u03ba4(V,U, U, V ) depend only on S2. Also, by linearity of cumulants we must have unfold(\u03ba4(V,U, U, V )) = unfold(\u03ba4(V,U, U, U))A> (see Appendix B.1). In the lemma we could have used third order cumulants, however for many distributions (e.g. all symmetric distributions) the third order cumulant is 0. Most distributions satisfy the condition that unfold(\u03ba4(AS2, S2, S2, S2)) is full rank, the only natural distribution that does not satisfy this constraint is the Gaussian distribution (where \u03ba4 is 0).\nExtracting cumulants Even when the linear transformation A is known, in most cases it is still information theoretically impossible to find the values of the samples s1, s2, s3 as we only have two views. However, we can still hope to learn useful information about the distributions S1, S2, S3. In particular, we derived the following formulas to estimate the cumulants of the distributions:\n\u03bat(S1) = \u03bat(U)\u2212 \u03bat(U,U, ..., U,A\u22121V ), (3) \u03bat(S2) = \u03bat(U,U, ..., U,A\n\u22121V ), (4) \u03bat(S3) = \u03bat(V )\u2212 \u03bat(AU, V, V, ..., V ). (5)\nTheorem 3.2. For all t > 1, Equations (3)-(5) compute the t-th order cumulants for S1, S2, S3 in time O((td)t+2)\nProof of Theorem 3.2 relies on the fact that since only S2 appears in bothU and V , the cross-cumulant \u03bat(U,U, ..., U,A\u22121V ) captures the cumulant of S2. Moreover, by independence, \u03bat(U) = \u03bat(S1) + \u03bat(S2), so we can recover \u03bat(S1) by subtracting off the estimated \u03ba(S2) (and similarly for \u03bat(S3)). When the dimension of U is smaller than the dimension of V and A \u2208 RdV \u00d7dU has full column rank, the above formula with pseudo-inverse A\u2020 in place of A\u22121 still recovers all cumulants. In Appendix B.1, we prove that both the formulas for computing A and for extracting the cumulants are robust to noise. In particular, we give the sample complexity for learning A and \u03bat(S1) from samples of U and V , both are polynomial in relevant quantities.\nGiven \u03bat(S1), we can use standard algorithms to compute moments of S1. Many learning algorithms are based on method-of-moments and can be directly applied (see Section 4.1). Other optimization-based algorithms can also be adapted (Section 4.2)."}, {"heading": "3.2 General model of Rich Component Analysis", "text": "We can extend the cumulant extraction algorithm in contrastive learning to general settings with more views and components. The ideas are very similar, but the algorithm is more technical in order to keep track of all the components. We present the intuition and the main results here and defer the details to Appendix B.2. Consider a set of observations U1, U2, . . . , Uk \u2208 Rd, each is linearly related to a subset of variables S1, S2, . . . , Sp \u2208 Rd, the variable Sj appears in a subset Qj \u2282 [k] of the observations. That is,\n\u2200i \u2208 [k] Ui = p\u2211 j=1 A(i,j)Sj , (6)\nwhere A(i,j) \u2208 Rd\u00d7d are unknown linear transformations, and A(i,j) = 0 if i 6\u2208 Qj . For simplicity we assume all the linear transformations are invertible. The variable Sj models the latent source of signal that is common to the subset of observations {Ui|i \u2208 Qj}. The matrix A(i,j) models the transformation of latent signal Sj in view i. In order for the model to be identifiable, it is necessary that all the subsets Qj\u2019s are distinct (otherwise the latent sources with identical Qj can be collapsed into one Sj). In the most general setting, we have a latent signal that is uniquely associated with every subset of observations. In this case, p = 2k \u2212 1 and {Qj} corresponds to all the non-empty subsets of [k]. In some settings, only specific subset of views Ui share common signals and {Qj} can be a small set. We measure the complexity of the set system using the following notion:\nDefinition 3.1 (L-distinguishable). We say a set system {Qj} is L-distinguishable, if for every set Qj , there exists a subset T \u2282 Qj of size at most L (called the distinguishing set) such that for any other set Qj\u2032(j\u2032 6= j), either Qj \u2282 Qj\u2032 or T 6\u2282 Qj\u2032 .\nFor example, the set system of the contrastive model is {{1}, {1, 2}, {2}} and it is 2-distinguishable. Intuitively, for any set Qj in the set system, there is a subset T of size at most L that distinguishes Qj from all the other sets (except the supersets of Qj). We use Algorithm 1 to recover all the linear transformations A(i,j) (for more details of the algorithm see Algorithm 2 in Appendix). Algorithm 1 takes as input a set system {Qj} that captures our prior belief about how the datasets are related. When we don\u2019t have any prior belief, we can input the most general {Qj} of size 2k \u2212 1, which is k-distinguishable. The algorithm automatically determines if certain variable Sj = 0. In the algorithm, min(Qj) is the smallest element of Qj .\nAlgorithm 1 FindLinear Require: set system {Qj} that is L-distinguishable, L+ 1-th order moments\nrepeat Pick a set Qj that is not a subset of any remaining sets Let T = {w1, w2, ..., wL} be the distinguishing set for Qj Compute cumulants for all i \u2208 Qj : Mi = unfold(\u03baL+1(Uw1 , ..., UwL , Ui). If MminQj = 0 then the variable Sj = 0; continue the loop. Let A(i,j) = (M\u2020minQjMi)\n> for all i \u2208 Qj , A(i,j) = 0 for all i 6\u2208 Qj . Mark Qj as processed, subtract all the cumulants of Qj .\nuntil all sets are processed\nLemma 3.3. Given observations Ui\u2019s as defined in Equation 6, suppose the sets Qj\u2019s are L-distinguishable, all the unknown linear transformations A(i,j)\u2019s are invertible, unfoldings unfold(\u03baL+1(Sj)) is either 0 (if Sj = 0) or have full rank, then given the exact L + 1-th order cumulants, Algorithm 1 outputs all the correct linear transformations A(i,j) in time poly(L!, (dk)L).\nOnce all the linear transformations A(i,j) are recovered, we follow the same strategy as in the contrastive analysis case 3.1.\nTheorem 3.4. Under the same assumption as Lemma 3.3, for any t \u2265 L Algorithm 3 computes the correct t-th order cumulants for all the variables in time poly((L+ t)!, (dk)L+t).\nNote that in the most general case it is impossible to find cumulants with order t < L, because there can be many different variables Sj\u2019s but not enough views. Both Algorithms 1 and 3 are robust to noise, with sample complexity that depends polynomially on the relevant condition numbers, and exponential in the order of cumulant considered. For more details see Appendix B.2."}, {"heading": "3.3 Related models", "text": "Independent component analysis (ICA)[4] may appear similar to our model, but it is actually quite different. In ICA, let s = [s1, ..., sn] be a vector of latent sources, where si\u2019s are one dimensional independent, non-Gaussian random variables. There is an unknown mixture matrix A and the observations are x = As. Given many samples x(t), the goal is to deconvolve and recover each sample si. In our setting, each si can be a high-dimensional vector with complex correlations. It is information-theoretically not possible to deconvolve and recover the individual samples si. Instead we aim to learn the distribution Si(\u03b8) without having explicit samples from it.\nAnother related model is canonical correlation analysis (CCA)[6]. The generative model interpretation of CCA is: there is a common signal z \u223c N(0, I), and view-specific signals z(m) \u223c N(0, I). Each view x(m) is then sampled according to N(A(m)z + B(m)z(m),\u03a3(m)), where m index the view. CCA is equivalent to maximum likelihood estimation of A(m) in this generative model. In our framework, CCA corresponds to the very restricted setting where S1, S2, S3 are all Gaussians. RCA learns S1 without making such parametric assumptions about S2 and S3. Moreover, using CCA, it is not clear how to learn the distribution S1 if it is not orthogonal to the shared subspace S2. In our experiments, we show that the naive approach of performing CCA (or kernel CCA) followed by taking the orthogonal projection leads to very poor performance. Factor analysis (FA)[5] also corresponds to a multivariate Gaussian model, and hence does not address the general problem that we solve. In FA, latent variables are sampled z \u223c N(0, I) and the observations are x|z \u223c N(\u00b5+ \u039bz,\u03a8).\nA different notion of contrastive learning was introduced in [14]. They focused on settings where there are two mixture models with overlapping mixture components. The method there applies only for Latent Dirichlet allocation and Hidden Markov Models and requires explicit parametric models for each component."}, {"heading": "4 Using Cumulants in learning applications", "text": "The cumulant extraction techniques of Section 3 constructs unbiased estimators for the cumulants of Si. In this section we show how to use the estimated cumulants/moments to perform maximum likelihood learning of Si. For concreteness, we frame the discussion on the contrastive learning setting, where we want to learn S1. For general RCA the method works when L (see Definition 3.1) is small or the distributions have specific relationship between lower and higher cumulants."}, {"heading": "4.1 Method-of-Moments", "text": "RCA recovers the cumulants of S1, from which we can construct all the moments of S1 in time O((td)t). This makes it possible to directly combine RCA with any estimation algorithm based on the method-of-moments. Methodof-moments have numerous applications in machine learning. The simplest (and most commonly used) example is arguably principal component analysis, where we want to find the maximum variance directions in S1. This is only related to the covariance matrix E[S1S>1 ]. RCA removes the covariance due to S2 and constructs an unbiased estimator of E[S1S>1 ], from which we can extract the top eigen-space.\nThe next simplest model is least squares regression (LSR). Suppose the distribution S1 contains samples and labels (X,Y ) \u2208 Rd \u00d7R, and only the samples are corrupted by perturbations, i.e. Y is independent of S2. LSR tries to find a parameter \u03b2 that minimizes E[(Y \u2212 \u03b2>X)2]. The optimal solution again only depends on the moments of (X,Y ): \u03b2\u2217 = (E[XX>])\u22121E[Y X]. Using the second-order cumulants/moments extracted from RCA , we can efficiently estimate \u03b2\u2217.\nMethod-of-moment estimators, especially together with tensor decomposition algorithms have been successfully applied to learning many latent variable models, including Mixture of Gaussians (GMM), Hidden Markov Model,\nLatent Dirichlet Allocation and many others (see [1]). RCA can be used in conjunction with all these methods. We\u2019ll consider learning GMM in Section 5."}, {"heading": "4.2 Approximating Gradients", "text": "There are many machine learning models where it\u2019s not clear how to apply method-of-moments. Gradient descent (GD) and stochastic gradient descent (SGD) are general purpose techniques for parameter estimation across many models. Here we show how to combine RCA with gradient descent. The key idea is that the extracted cumulants/moments of S1 forms a polynomial basis. If the gradient of the log-likelihood can be approximated by a lowdegree polynomial in S1, then the extracted cumulants from RCA can be used to approximate this gradient.\nConsider the general setting where we have a model D with parameter \u03b8, and for any sample s1 the likelihood is L(\u03b8, s1). The maximum likelihood estimator tries to find the parameter that maximizes the likelihood of observed samples: \u03b8\u2217 = arg maxE[logL(\u03b8, s1)]. In many applications, this is solved using stochastic gradient descent, where we pick a random sample and move the current guess to the corresponding gradient direction: \u03b8(t+1) = \u03b8(t) + \u03b7t\u2207\u03b8 logL(\u03b8, s(t)1 ), where \u03b7t is a step size and s (t) 1 is the t-th sample. For convex functions this is known to converge to the optimal solution [12]. Even for non-convex functions this is often used as a heuristic. If the gradient of log-likelihood \u2207\u03b8 logL(\u03b8, s1) is a low degree polynomial in s1, then using the lower order moments we can obtain an unbiased estimator for E[\u2207\u03b8 logL(\u03b8, S1)] with bounded variance, which is sufficient for stochastic gradient to work. This is the case for linear least-squares regression, and its regularized forms using either `1 or `2 regularizer.\nIn the case when log-likelihood is not a low degree polynomial in S1, we approximate the gradient by a low degree polynomial, either through simple Taylor\u2019s expansion or other polynomial approximations (e.g. Chebyshev polynomials, see more in [10]). This will give us a biased estimator for the gradient whose bias decreases with the degree we use. In general, when the (negative) log-likelihood function is strongly convex we can still hope to find an approximate solution:\nLemma 4.1. Suppose the negative log-likelihood function F (\u03b8) = \u2212E[logL(\u03b8, S1)] is \u00b5-strongly convex and Hsmooth, given an estimator G(\u03b8) for the gradient such that \u2016G(\u03b8) \u2212\u2207F (\u03b8)\u2016 \u2264 , gradient descent using G(\u03b8) with step size 12H converges to a solution \u03b8 such that \u2016\u03b8 \u2212 \u03b8\u2217\u2016 2 \u2264 8 2 \u00b52 .\nWhen high degree polynomials are needed to approximate the gradient, our algorithm requires number of samples that grows exponentially in the degree.\nLogistic Regression We give a specific example to illustrate using RCA and low degree polynomials to simulate gradient descent. Consider the basic logistic regression setting, where the samples s1 = (x, y) \u2208 Rd \u00d7 {0, 1}, and the log-likelihood function is logL(\u03b8, s1) = log e y\u03b8>x\n1+e\u03b8>x . The gradient of the log-likelihood is: \u2207\u03b8 logL(\u03b8, s1) =\n(y \u2212 e \u03b8>x\n1+e\u03b8>x )x.\nWe can then approximate the function e \u03b8>x\n1+e\u03b8>x using a low degree polynomial in \u03b8>x. As an example, we use 3rd\ndegree Chebychev: e \u03b8>x\n1+e\u03b8>x \u2248 0.5 + 0.245\u03b8>x\u2212 0.014(\u03b8>x)3. The gradient we take in each step is\nE[\u2207\u03b8 logL(\u03b8, S1)] \u2248 E[Y X]\u2212 0.5E[X]\u2212 0.245E[X(\u03b8>X)] + 0.014E[X(\u03b8>X)3].\nTo estimate this approximation, we only need quadratic terms E[X(\u03b8>X)] and a projection of the 4-th order moment E[X(\u03b8>X)3]. These terms are computed from the projected 2nd and 4-th order cumulants of X that are extracted from the cumulants of U and V via Section 3. Because of the projection these quantities are much easier to compute (in fact, they can be estimated in linear time)."}, {"heading": "5 Experiments", "text": "In the experiments, we focus on the contrastive learning setting where we are given observations of U = S1 + S2 and V = AS2 + S3 and the goal is to estimate the parameters for the S1 distribution. Our approach can also learn the shared component S2 as well as S3. We tested our method in five settings, where S1 corresponds to: low rank Gaussian (PCA), linear regression, mixture of Gaussians (GMM), logistic regression and the Ising model. The first three settings illustrate combining RCA with method-of-moments and the latter two settings requires RCA with polynomial approximation to stochastic gradient descent. In each setting, we compared the following four algorithms:\n1. The standard learning algorithm using the actual samples s1 \u223c S1(\u03b8) to learn the parameters \u03b8. This is the gold-standard, denoted as \u2018true samples\u2019.\n2. Our contrastive RCA algorithm using paired samples from U and V to learn S1(\u03b8).\n3. The naive approach that ignores S2 and uses U to learn S1(\u03b8) directly, denoted as \u2018naive\u2019.\n4. First perform Canonical Correlation Analysis (CCA) on U and V , and project the samples from U onto the subspace orthogonal to the canonical correlation subspace. Then learn S1 from the projected samples of U . We denote this as \u2018CCA\u2019. In all five settings, we let S3 be sampled uniformly from [\u22121, 1]d, where d is the appropriate dimension of S3. The empirical results are robust to other choices of S3 that we have tried, e.g. multivariate Gaussian or mixture of Gaussians.\nContrastive PCA. S1 was set to have a principal component along direction v1, i.e. s1 \u223c N (0, v1v>1 +\u03c32I). S2 was sampled from Unif([\u22121, 1]d) + v2v>2 and v1, v2 are random unit vectors in Rd. RCA constructs an unbiased estimator of E[S1S>1 ] from the samples of U and V . We then report the top eigenvector of this estimator as the estimated v\u03021. We evaluate each algorithm by the mean squared error (MSE) of the inferred v\u03021 to the true v1.\nContrastive regression. S1 is the uniform distribution, s1 \u223c Unif([\u22121, 1]d) and y = \u03b2>s1 + N (0, 1). S2 was sampled from Unif([\u22121, 1]d) + v2v>2 and \u03b2, v2 are random unit vectors in Rd. Our approach gives unbiased estimator of E[S1S>1 ] from which we estimate \u03b2\u0302 = (E[S1S>1 ])\u22121E[Y S1]. All algorithms are evaluated by the MSE between the inferred \u03b2\u0302 and the true \u03b2. Contrastive mixture of Gaussians. S1 is a mixture of d spherical Gaussians in Rd, s1 \u223c \u2211d k=1 1 dN (\u00b5 (1) k , \u03c3\n2). S2 is also a mixture of spherical Gaussians, s2 \u223c \u2211d k=1 1 dN (\u00b5 (2) k , \u03c3\n2). RCA gives unbiased estimators of the thirdorder moment tensor, E[s1 \u2297 s1 \u2297 s1]. We then use the estimator in [7] to get a low rank tensor whose components correspond to center vectors, and apply alternating minimization (see [9]) to infer \u00b5\u0302(1)k . Algorithms are evaluated by the MSE between the inferred centers {\u00b5\u0302(1)k } and the true centers {\u00b5 (1) k }.\nContrastive logistic regression. Let s1 \u223c Unif([\u22121, 1]d) and y = 1 with probability 1 1+e\u2212\u03b2 >s1 . S2 was sampled from Unif([\u22121, 1]d) + v2v>2 , and \u03b2, v2 are unit vectors in Rd. We use the 4-th order Chebychev polynomial approximation to the SGD of logistic regression as in Section 4.2. Evaluation is the MSE error between the inferred \u03b2\u0302 and the true \u03b2.\nContrastive Ising model. Let S1 be a mean-zero Ising model on d-by-d grid with periodic boundary conditions. Each of the d2 vertices are connected to four neighbors and can take on values {\u00b11}. The edge between vertices i and j is associated with a coupling Jij \u223c Unif[\u22121, 1]. The state of the Ising model, s1, has probability 1 Z e \u2211 (i,j)\u2208E Jijs1(i)s1(j), where Z is the partition function. We let S2 also be a d-by-d grid of spins where half of the spins are independent Bernoulli random variables and the other half are correlated, i.e. they are all 1 or all -1 with probability 0.5. We use composite likelihood to estimate the couplings Jij of S1, which is asymptotically consistent with MLE of the true likelihood [13]. For the gold-standard baseline (which uses the true samples s1), we use the exact gradient of the composite likelihood. For RCA , we used the 4-th order Taylor approximation to the gradient. Evaluation is the MSE between the true Jij and the estimated J\u0302ij .\nResults. For the method-of-moment applications\u2013PCA, linear regression, GMM\u2013we used 10 dimensional samples for U and V . The tradeoff between inference accuracy (measured in MSE) and sample size is shown in the top row of Figure 1. Even with just 100 samples, RCA performs significantly better than the naive approach and CCA. With 1000 samples, the accuracy of RCA approaches that of the algorithm using the true samples from S1. It is interesting to note that projecting onto the subspace orthogonal to CCA can perform much worse than even the naive algorithm. In the linear regression setting, for example, when the signal of S2 happens to align with \u03b2, the direction of prediction, projecting onto the subspace orthogonal to S2 loses much of the predictive signal.\nIn the SGD settings, we used a 10 dimensional logistic model and a 5-by-5 Ising model (50 Jij parameters to infer). RCA also performed substantially better than the two benchmarks (Figure 1 d, e). In all the cases, the accuracy of RCA improved monotonically with increasing sample size. This was not the case for the Naive and CCA algorithms, which were unable to take advantage of larger data due to model-misspecification. In Figure 1 f and g, we plot the learning trajectory of RCA over the SGD steps for representative runs of the algorithm with 1000 samples. RCA converges to the final state at a rate similar to the true-sample case. The residual error of RCA is due to the bias introduced by approximating the sigmoid with low-degree polynomial. When many samples are available, a higher-degree polynomial approximation can be used to reduce this bias.\nWe also explored how the algorithms perform as the magnitude of the signal in S2 is increased compared to S1 (Figure 1 h-j) with fixed 1000 samples. In these plots the x-axis measures the ratio of standard deviations of S2 and S1.At close to 0, most of the signal of U comes from S1, and all the algorithms are fairly accurate. As the strength\nof the perturbation increases, RCA performs significantly better than the benchmarks, especially in the Ising model. Finally we empirically explored the sample complexity of the subroutine to recover the A matrix from the 4th order cumulants. Figure 1 k shows the MSE between the trueA (sampled\u223c Unif[\u22121, 1]d\u00d7d) and the inferred A\u0302 as a function of the sample size. Even with 1000 samples, we can obtain reasonable estimates of A \u2208 R30\u00d730.\nBiomarkers experiment. We applied RCA to a real dataset of DNA methylation biomarkers. Twenty biomarkers (10 test and 10 control) measured the DNA methylation level (a real number between 0 and 1) at twenty genomic loci across 686 individuals [15]. Each individual was associated with a binary disease status Y . Logistic regression on the ten test biomarkers was used to determine the weight vector, \u03b2, which quantifies the contribution of the methylation at each of these ten locus to the disease risk. The other ten independent loci are control markers. Getting accurate estimates for the values of \u03b2 is important for understanding the biological roles of these loci. In this dataset, all the samples were measured on one platform, leading to relatively accurate estimate of \u03b2. In many cases samples are collected from multiple facilities (or by different labs). We simulated this within our RCA framework. We let S1 be the original data matrix of the ten test markers across the 686 samples. We let S3 be the original data matrix of the ten control markers in these same samples. We modeled S2 as a mixture model, where samples are randomly assigned to different components that capture lab specific biases. The perturbed observations are U = S1+S2 and V = AS2+S3, i.e. U and V simulate the measurements for the test and control markers, respectively, when the true signal has been perturbed by this mixtures distribution of lab biases. We assume that we can only access U and V and do not know S2, i.e. where each sample is generated. Running logistic regression directly on U and the phenotype Y obtained a MSE of 0.24 (std 0.03) between the inferred \u03b2\u0302 and the true \u03b2 measured from directly regressing S1 on Y . Directly using CCA also introduce significant errors with MSE of 0.25 (std 0.02). Using all the control markers as covariates in the logistic regression, the MSE of the test markers\u2019 \u03b2 was 0.14 (std 0.03). In general, adding V as covariates to the regression can eliminate S2 at the expense of adding S3, and can reduce accuracy when S3 is larger than S2. Using our RCA logistic regression on U and V , we obtained significantly more accurate estimates of \u03b8, with MSE 0.1 (std 0.03). See Appendix for more analysis of this experiment."}, {"heading": "A More Tensor and Cumulant Notations", "text": "In this section we introduce the notations and basics for tensors and cumulants.\nMatrix Notations For a matrix M \u2208 Rn\u00d7m we use \u2016M\u2016 to denote its spectral norm sup\u2016x\u2016=1 \u2016Mx\u2016, \u2016M\u2016F to denote its Frobenius norm \u2016M\u2016F = \u221a\u2211 i,jM 2 i,j , and \u03c3min(M) to denote its smallest singular value.\nWhen n \u2265 m and the matrix M has full column rank, we use M\u2020 to denote its Moore-Penrose pseudoinverse which in particular satisfy M\u2020M = I .\nWe also sometimes use the Kronecker product of matrices, for A \u2208 Rm\u00d7n and B \u2208 Rp\u00d7q , A \u2297 B is a matrix in Rmp\u00d7nq that has the following block structure:\nA\u2297B =  A1,1B A1,2B \u00b7 \u00b7 \u00b7 A1,nB A2,1B A2,2B \u00b7 \u00b7 \u00b7 A2,nB ... ...\n... Am,1B Am,2B \u00b7 \u00b7 \u00b7 Am,nB  The singular values of A\u2297B is just the product of singular values of A and B.\nTensor Notations A tensor T \u2208 Rdt is a t-dimensional array, and is frequently used to represent higher order moments or cumulants. We index the elements in the tensor using a t-tuple (i1, i2, ..., it) \u2208 [d]t. The entries of tensor product [u1\u2297u2\u2297\u00b7 \u00b7 \u00b7ut](i1,i2,...,it) is simply the product of corresponding entries \u220ft j=1 uj(ij). We use u\n\u2297t to denote u\u2297 u\u2297 \u00b7 \u00b7 \u00b7 \u2297 u t times.\nFor a distribution X \u2208 Rd, the t-th order moment is a tensor E[X\u22974], whose (i1, i2, ..., it)-th entry is equal to E[Xi1Xi2 \u00b7 \u00b7 \u00b7Xit ]. Later we shall see cumulants can also be conveniently represented as tensors.\nA tensor can be viewed as a multi-linear form (just as a matrix M can be viewed as a bilinear form uTMv). For a tensor T we define T (M1,M2, . . . ,Mt) to be\nT (M1,M2, . . . ,Mt)(i1,...,it) = \u2211\n(j1,...,jt)\u2208[d]t T(j1,...,jt) t\u220f l=1 Ml(jl, il).\nThis multi-linear form works well with the moment tensors, especially for matrices M1, ...,Mt we always have\nE[X\u2297t](M1, ...,Mt) = E[(M>1 X)\u2297 (M>2 X)\u2297 \u00b7 \u00b7 \u00b7 \u2297 (M>t X)].\nOften to simplify operations tensors are unfolded to become matrices. There can be many ways to unfold a tensor, but in this paper we mostly use a particular unfolding which makes the tensor into a Rdt\u22121\u00d7d matrix:\nunfold(T )(i1,...,it\u22121),it = T(i1,...,it).\nSimilar to matrices, we also define the Frobenius norm of tensors to be the `2 norm of all its entries, in particular \u2016T\u2016F = \u2016unfold(T )\u2016F = \u221a \u2211 i1,...,it T 2(i1,...,it).\nCumulants Cumulants provide an alternative way to describe the lower order correlations of a random variable. Unlike moments, cumulants have the nice property that the cumulant of sum of independent random variables equals to the sum of cumulants. Formally, for a random variable X \u2208 R the cumulant is defined to be the coefficients of the cumulant generating function logE[etX ] (\u03bat(X) is just t! times the coefficient in front of Xt). When the variables are different the cross-cumulants (similar to covariance) can similarly be defined, and it can be computed as:\n\u03bat(X1, ..., Xt) = \u2211 \u03c0 (|\u03c0| \u2212 1)!(\u22121)|\u03c0|\u22121 \u220f B\u2208\u03c0 E[ \u220f i\u2208B Xi]. (7)\nIn this formula, \u03c0 is enumerated over all partitions of [t], |\u03c0| is the number of parts in partition and B runs through the list of all parts.\nSimilarly, it is possible to define cumulants for multivariate distributions. For random variableX \u2208 Rd \u03bat(X)(i1,...,it) = \u03bat(Xi1 , ..., Xit). This cross cumulant can be computed in a similar way as Equation (7), however the products should be replaced by tensor products and the ordering of coordinates is important when doing the tensor product.\nFact Suppose X1, ..., Xt are random variables in Rd. The t-th order cumulant \u03bat(X1, ..., Xt) is a tensor in Rd t\nthat have the following properties:\n1. (Independence) If (X1, ..., Xt) and (Y1, ..., Yt) are independent, then \u03bat(X1+Y1, ..., Xt+Yt) = \u03bat(X1, ..., Xt)+ \u03bat(Y1, ..., Yt).\n2. (Linearity) \u03bat(M>1 X1, ...,M > t Xt) = \u03bat(X1, ..., Xt)(M1, ...,Mt).\n3. (Relation to Moments) The t-th order cumulant is a polynomial over the first t-th order moments. Similarly the tth order moment is a polynomial over the first t-th order cumulants. Further both polynomials can be computed in O(t!) time. Converting between first t-th order moments and cumulants for d-dimensional variables takes O((td)d) time.\nIntuitively, cumulants can measure how correlated two distributions are. The simplest case is \u03ba2(X,Y ) which is equal to the covariance E[(X\u2212E[X])(Y \u2212E[Y ])], and is 0 only if the two variables are not correlated in second order. For more detailed introductions to cumulants see books like [8]."}, {"heading": "B Details for Section 3", "text": "In this section, we prove the equations and algorithms in Section 3 indeed compute the desirable quantity, and further we give sample complexity bounds.\nB.1 Contrastive Learning We first prove Equation (2) computes the correct linear transformation.\nLemma B.1 (Lemma 3.1 restated). Suppose the unfolding of the 4-th order cumulant unfold(\u03ba4(AS2, S2, S2, S2)) has full rank, given the exact cumulants \u03ba4(V,U, U, U) and \u03ba4(V,U, U, V ), Equation (2) finds the correct linear transformation in time O(d5).\nProof. Since U = S1 + S2 and V = AS2 + S3, we know\n\u03ba4(V,U, U, U) = \u03ba4(AS2 + S3, S1 + S2, S1 + S2, S1 + S2)\n= \u03ba4(0, S1, S1, S1) + \u03ba4(AS2, S2, S2, S2) + \u03ba4(S3, 0, 0, 0)\n= \u03ba4(AS2, S2, S2, S2).\nHere the second step uses the fact that cumulants are additive for independent variables, and third step uses the linearity of cumulants.\nSimilarly, we know \u03ba4(V,U, U, V ) = \u03ba4(AS2, S2, S2, AS2) = \u03ba4(AS2, S2, S2, S2)(I, I, I, A>). For the unfoldings of these cumulants, we have\nunfold(cum4(V,U, U, V )) = unfold(cum4(V,U, U, U))A >.\nTherefore when unfold(cum4(V,U, U, V )) has full rank we can compute A using pseudo-inverse. For the running time, the main computation is a pseudo-inverse and a matrix product for d3\u00d7d matrices, both take\nO(d5) time.\nNext we show given the linear transformation, it is possible to estimate the cumulants using Equations (3 - 5). In fact, we can also avoid computing the cross-cumulants and work with just the cumulants of variables:\n\u03bat(S1) = \u03bat(U)\u2212 \u03bat(U +A \u22121V )\u2212 \u03bat(U)\u2212 \u03bat(A\u22121V ) 2t \u2212 2 , (8)\n\u03bat(S2) = \u03bat(U +A \u22121V )\u2212 \u03bat(U)\u2212 \u03bat(A\u22121V ) 2t \u2212 2 , (9) \u03bat(S3) = \u03bat(V )\u2212 \u03bat(AU + V )\u2212 \u03bat(AU)\u2212 \u03bat(V )\n2t \u2212 2 . (10)\nTheorem B.2 (Theorem 3.2 restated). For all t > 1, Equations (3)-(5) or (8)-(10) compute the correct cumulants for S1, S2, S3 in time O((td)t+2). Moreover, if V has dimension higher than U and A has full column rank, replacing A\u22121 by A\u2020 still gives correct cumulants.\nProof. The proof of Equations (3)-(5) is very similar to the previous lemma. Note that\n\u03bat(U,U, ..., U,A \u22121V ) = \u03bat(S1 + S2, ..., S1 + S2, S2 +A \u22121S3)\n= \u03bat(S1, ..., S1, 0) + \u03bat(S2, S2, S2, S2) + \u03bat(0, ..., 0, A \u22121S3)\n= \u03bat(S2).\nSo we have Equation (4), and using the fact that \u03bat(U) = \u03bat(S1) + \u03bat(S2) we get Equation (3). Equation (5) follows similarly.\nIn order to get Equations (8)-(10), first note that by the linearity of cumulants, we can write \u03bat(U +A\u22121V ) as the sum of 2t terms:\n\u03bat(U +A \u22121V ) = \u2211 z\u2208{0,1}t \u03bat(z1U + (1\u2212 z1)A\u22121V, z2U + (1\u2212 z2)A\u22121V, ..., ztU + (1\u2212 zt)A\u22121V ).\nAmong all these terms, one is equal to \u03bat(U), one is equal to \u03bat(A\u22121V ), and all the other 2t \u2212 2 terms are crosscumulants that involve both U and V . Since S2 is the only variable that appears in both U and A\u22121V , all the 2t \u2212 2 terms are equal to \u03bat(S2), therefore we have Equation (9). Equation (8) again follows from the fact that \u03bat(U) = \u03bat(S1) + \u03bat(S2), and Equation (10) is very similar.\nThe moreover part follows by directly replacing A\u22121 with A\u2020 in the above argument. Note that in this case we can still find A because unfold(cum4(AS2, S2, S2, S2)) still has full rank as long as unfold(cum4(S2, S2, S2, S2)) has full rank.\nFor running time, the main bottleneck is computing the cumulants (which takes O((td)t) time), and then applying the matrix A to the cumulants (which takes O(dt+2) time).\nFinally, we show the equations are robust under sampling noise. For that we use the following bounds on cumulants\nFact ([3]) For any cross-cumulant \u03bat(U1, ..., Ut), if all the variables have bounded norm \u2016Ui\u2016 \u2264 R, then the cumulant has Frobenius norm bounded by (tR)R.\nIn practice we use k-statistics [11] to estimate the cumulants, the standard deviation of k-statistics is bounded by a similar formula.\nLemma B.3. Suppose the distributions S1, S2, S3 have bounded radiusR, the 4-th order cumulant unfold(\u03ba4(V,U, U, U)) has smallest singular value \u03c34, matrixA has smallest singular value \u03c3A and \u2016A\u2016 \u2265 1, given 4-th order cumulants that are -close in Frobenius norm (and R4), the linear transformation A is recovered with accuracy \u2016A\u20162R4/\u03c324 . Given t-th order cross-cumulants of U ,V that are t-close in Frobenius norm, the cumulants of S1 can be computed with accuracy O ( \u2016A\u20163R4(tR)t\n\u03c324\u03c3 2 A\n+ t\u03c3\n) using (3). In particular, to estimate the cumulants of S1 with accuracy \u03b7 the\nnumber of samples required is \u2126((tR)2t\u2016A\u201610R16/\u03c344\u03c34A\u03b72).\nProof. First we show the algorithms are robust under perturbation. For that we need the fact that any 4-th order crosscumulant with bounded variables always have Frobenius norm of order at most O(R4). As a corollary we know the cross-cumulant \u03ba4(V,U, U, U) has norm at most O(\u2016A\u2016R4) and \u03ba4(V,U, U, V ) has norm at most O(\u2016A\u20162R4) Let M\u0302 be the noisy version of M = unfold(\u03ba4(V,U, U, U)), by assumption and by standard matrix perturbation bounds, we know \u2016M\u0302\u2020 \u2212M\u2020\u2016F \u2264 O( /\u03c324). On the other hand, let N\u0302 be the noisy version of N = unfold(\u03ba4(V,U, U, V )), we know \u2016N\u0302 \u2212N\u2016F \u2264 , therefore\n\u2016M\u0302\u2020N\u0302 \u2212M\u2020N\u2016F \u2264 O(\u2016M\u0302\u2020 \u2212M\u2020\u2016\u2016N\u2016F + \u2016M\u2016\u2016N\u0302 \u2212N\u2016F ) \u2264 O( \u2016A\u20162R4/\u03c324).\nFor computing the t-th order cumulant, the main source of error is applyingA\u22121 to the cross cumulant \u03bat(U, ..., U, V ) to get \u03bat(U, ..., U,A\u22121V ), as we don\u2019t have the matrix A exactly. Since the norm cross-cumulant is always bounded by (tR)t\u2016A\u2016, we know when is small enough the error is roughly (ignoring lower order terms)\n\u2016A\u0302\u22121 \u2212A\u22121\u2016\u2016\u03bat(U, ..., U, V )\u2016F + \u2016A\u22121\u2016\u2016\u03ba\u0302t(U, ..., U, V )\u2212 \u03bat(U, ..., U, V )\u2016F\nwhich is bounded by\nO\n( \u2016A\u20163R4(tR)t\n\u03c324\u03c3 2 A\n+ t \u03c3A\n) .\nAlso, by the variance bounds for cumulants we know withZ samples, t \u2264 (tR)t\u2016A\u2016/ \u221a Z and \u2264 O(R4\u2016A\u20162/ \u221a Z),\ntherefore when Z = \u2126((tR)2t\u2016A\u201610R16/\u03c344\u03c34A\u03b72), the estimation of S1 has desirable error.\nB.2 Rich component analysis We first give the algorithm for computing the linear transformations and then show it computes the correct quantities.\nAlgorithm 2 FindLinear Require: set system {Qj} that is L-distinguishable, L+ 1-th order moments\nrepeat Pick a set Qj that is not a subset of any remaining sets Let T = {w1, w2, ..., wL} be the distinguishing set for Qj Compute cumulants for all i \u2208 Qj :\nMi = unfold(\u03baL+1(Uw1 , ..., UwL , Ui) \u2212 \u2211\nl:Qj\u2282Ql\n\u03baL+1(Sl)((A (w1,l))>, ..., (A(wL,l))>, (A(i,l))>))\nIf MminQj = 0 (or \u03c3min(MminQj ) is too small), then Sj = 0; continue the loop. Let A(i,j) = (M\u2020minQiMi)\n> for all i \u2208 Qj , A(i,j) = 0 for all i 6\u2208 Qj . Mark Qj as processed, and let\n\u03baL+1(Sj) = \u03baL+1((A (w1,j))\u22121Uw1 , ..., (A (wL,j))\u22121UwL , UminQj ) \u2212 \u2211\nl:Qj\u2282Ql\n\u03baL+1(Sl)((A (w1,l))>(A(w1,j))\u2212>, ..., (A(wL,l))>(A(wL,j))\u2212>, (A(minQj ,l))>).\nuntil all sets are processed\nThe main idea behind this algorithm is that Since we know the sets are L-distinguishable, if we start from maximal set Qj , there must be a distinguishing subset of size L that is only contained in Qj . Similar to the contrastive setting, if we consider a cross-cumulant that contains all the variables in this distinguishing set, then the resulting cumulant must only depend on this particular variable Sj . Further, using different last variable in the cross-cumulants (similar\nto using \u03ba4(V,U, U, U) and \u03ba4(V,U, U, V )) and exploit the linearity of the cumulants, we can recover the linear transformations.\nNote that without loss of generality we can assume A(min(Qj),j) = I , because otherwise we can replace Sj with the distribution S\u2032j = A (min(Qj),j)Sj .\nLemma B.4 (Lemma 3.3 restated). Given observations Ui\u2019s as defined in Equation 6, suppose the sets Qj\u2019s are Ldistinguishable, all the unknown linear transformations A(i,j)\u2019s are invertible, unfoldings unfold(cumL+1(Sj)) is either 0 (when Sj = 0) or have full rank, then given the exact L + 1-th order cumulants, Algorithm 2 outputs the correct linear transformations A(i,j) in time poly(L!, (dk)L).\nProof. We prove this by induction using the following hypothesis: For all the processed variables, Algorithm 2 finds the correct linear transformationsA(i,j) and cumulant \u03baL+1(Sj). This hypothesis is clearly true at the beginning of the algorithm (as no variables are processed). We now show the algorithm will compute the correct quantities for the next variable Sj . By the algorithm, we know the setQj is not a subset of any remaining sets, and T is a distinguishing set. Therefore, for other remaining set, we know it cannot contain all the elements in T . Therefore, by linearity and additivity of cumulants, we know \u03baL+1(Uw1 , ..., Uwt , Ui)(i \u2208 Qj) will only depend on the variable Sj and some of the previously processed variables. In particular,\n\u03baL+1(Uw1 , ..., Uwt , Ui) = \u03baL+1(A (w1,j)Sj , A (w2,j)Sj , ..., A (wL,j)Sj , A (j,i)Sj) + \u2211\nl:Qj\u2282Ql\n\u03baL+1(A (w1,l)Sl, A (w2,l)Sl, ..., A (wL,l)Sl, A (i,l)Sl).\nBy induction hypothesis, we have already processed all the other terms related to Ql (l 6= j and Qj \u2282 Ql), so we have the correct cumulants \u03baL+1(Sl) and linear transformations A(i,l)\u2019s. Those terms will be subtracted out during the algorithm. Therefore we know\nMi = unfold(\u03baL+1(A (w1,j)Sj , A (w2,j)Sj , ..., A (wt,j)Sj , A (i,j)Sj))\n= unfold(\u03baL+1(A (w1,j)Sj , A (w2,j)Sj , ..., A (wt,j)Sj , Sj))(A (i,j))>\nIn particular MminQj = unfold(\u03baL+1(A (w1,j)Sj , A (w2,j)Sj , ..., A (wt,j)Sj , Sj)). Therefore, the algorithm computes the correct linear transformations if the matrix MminQj has full rank. The fact that MminQj has full rank is implied by assumptions, because we can write this matrix as\nunfold(\u03baL+1(A (w1,j)Sj , A (w2,j)Sj , ..., A (wt,j)Sj , Sj)) = (A (w1,j) \u2297 \u00b7 \u00b7 \u00b7 \u2297A(wL,j))unfold(\u03baL+1(Sj)).\nHere\u2297 is the Kronecker product of matrices, and it is well-known that the Kronecker product of invertible matrices are still invertible. Since unfold(\u03baL+1(Sj)) is either 0 or has full rank by assumption, we know we can either detect there is no component corresponding to set Qj , or have a matrix MminQj with full rank. In the latter case the correctness of the L+ 1-th order cumulant calculation then simply follows from the linearity of cumulants.\nFinally, we estimate the running time of the algorithm. Computing any cumulant can be done in poly(L!, dL) time. Finding the distinguishing set (by exhaustive search) takes no more than poly(kL) time. The algorithm runs in at most p \u2264 2L iterations, each iteration computes a small number of cumulants and does small number of linear-algebraic calculations (which are all poly in (kd)L), so the total running time is at most poly(L!, (kd)L)\nNow we are ready to give the algorithm for computing cumulants and prove that it works.\nTheorem B.5 (Theorem 3.4 restated). Under the same assumption as Lemma 3.3, for any t \u2265 L Algorithm 3 computes the correct t-th order cumulants for all the variables in time poly((L+ t)!, (dk)L+t).\nProof. The proof of this lemma is very similar to the previous one. Again we prove the lemma by induction, under the following induction hypothesis:\nFor all the processed variables, Algorithm 2 finds the correct cumulant \u03bat(Sj).\nAlgorithm 3 ComputeCumulant Require: set system {Qj} that is L-distinguishable, order t \u2265 L Ensure: t-th order cumulant for all the variables\nApply Algorithm 2 to find A(i,j)\u2019s, remove all sets whose variables do not appear. repeat\nPick a set Qj that is not a subset of any remaining sets Let T = {w1, w2, ..., wL} be the distinguishing set for Qj , let wL+1 = wL+2 = \u00b7 \u00b7 \u00b7 = wt = wL. Mark Qj as processed, let\n\u03bat(Sj) = \u03bat((A (w1,j))\u22121Uw1 , ..., (A (wt,j))\u22121Uwt) \u2212 \u2211\nl:Qj\u2282Ql\n\u03bat(Sl)((A (w1,l))>(A(w1,j))\u2212>, ..., (A(wt,l))>(A(wt,j))\u2212>)\nuntil all sets are processed\nThis is clearly true before the main loop. We now show that the algorithm successfully compute the cumulant of the next variable.\nSimilar as before, since w1, ..., wt contains all the elements of a distinguishing set T , we know\n\u03bat((A (w1,j))\u22121Uw1 , ..., (A (wt,j))\u22121Uwt) = \u03bat(Sj) + \u2211\nl:Qj\u2282Ql\n\u03bat((A (w1,j))\u22121A(w1,l)Sl, (A (w2,j))\u22121, A(w2,l)Sl, ..., (A (wt,j))\u22121A(wt,l)Sl)\n= \u03bat(Sj) + \u2211\nl:Qj\u2282Ql\n\u03bat(Sl)((A (w1,l))>(A(w1,j))\u2212>, ..., (A(wt,l))>(A(wt,j))\u2212>).\nBy induction hypothesis all the other terms are computed in previous iterations of the algorithm, so they are subtracted out. Therefore we get the first term which is equal to \u03bat(Sj).\nFinally we prove the sample complexity bounds.\nLemma B.6. Suppose the distributions Sj\u2019s have bounded radius R, the L+ 1-th order cumulant unfold(\u03baL+1(Sj)) has smallest singular value \u03c3\u03ba, nonzero matricesA(i,j) has smallest singular value \u03c3A and spectral norm at most \u2016A\u2016. Also, suppose the longest chain of subsets Qj1 \u2282 Qj2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 Qjq has length q. Given L + 1-th order cumulants that are -close in Frobenius norm, the linear transformation A is recovered with accuracy (pLR\u2016A\u2016/\u03c3A\u03c3\u03ba)O(qL). Given t-th order cumulants that are -close in Frobenius norm, the cumulants of Sj can be computed with accuracy (pLR\u2016A\u2016/\u03c3A\u03c3\u03ba)O(qL+qt). In particular, to estimate the cumulants of S1 with accuracy \u03b7 the number of samples required is \u2126((pLR\u2016A\u2016/\u03c3A\u03c3\u03ba)O(qL+qt)/\u03b72).\nProof. We prove this by induction. For each variable Sj , let depth qj be the length of the longest chain such that Qj \u2282 Qj1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 Qqj\u22121. We shall prove that theA(i,j)\u2019s are recovered with accuracy A,qj = O( (p\u2016A\u20162L+2(LR)t+1/\u03c32LA \u03c32\u03ba)qj\u22121\u2016A\u2016t+1(tR)L+1/\u03c32LA \u03c32\u03ba) and the L+ 1-th cumulant is recovered with accuracy \u03ba,qj = O(L A,qj\u2016MminQj\u2016F /\u03c32A).\nFirst we show the base case, when qj = 1 and therefore there is no other set that contains this set. In this case,A(i,j) is just equal to (M\u2020minQjMi) > where the M \u2019s are the unfoldings of L+ 1-th order cross-cumulants (so we have them with accuracy ). By standard matrix perturbation bounds we know the error is bounded by \u2016Mi\u2016F /\u03c3min(MminQj )2 and we just need to bound the smallest singular value and Frobenius norm for the M \u2019s. For MminQj , we know it is equal to a linear transformation of the unfolding of \u03baL+1(Sj), therefore \u03c3min(MminQj ) \u2265 \u03c3LA\u03c3c. Similarly we have \u2016Mi\u2016F \u2264 O(\u2016A\u2016L+1(LR)L+1). Therefore A,1 = O( \u2016A\u2016L+1(LR)L+1/\u03c32LA \u03c32\u03ba). When we compute the L + 1-th order cumulant, the dominating term is applying the inverses of the A matrices we estimated, and we know \u03ba,1 = O(L A,1\u2016MminQj\u2016F /\u03c32A) = O( L\u2016A\u20162L+1(LR)2L+2/\u03c3 2L+2 A \u03c3 2 \u03ba).\nSuppose we have shown this for all of Qj\u2019s with small depth qj \u2264 u. For a set Qj with qj = u + 1, when we compute the matrices M we need to subtract the cumulants of the previously computed variables. The number of such variables is at most p, and each variable has an additional error of O( \u03ba,qj\u22121\u2016A\u2016L+1 + L A,qj\u22121\u2016A\u2016L(LR)L+1) \u2264 O( \u03ba,qj\u22121\u2016A\u2016L+1). This (O( \u03ba,qj\u22121p\u2016A\u2016L+1)) is our new error in estimating the cumulants. Therefore, by the same argument we have A,qj = O( \u03ba,qj\u22121p\u2016A\u20162L+2(LR)L+1/\u03c32LA \u03c32\u03ba) = O( (p\u2016A\u20162L+2(LR)L+1/\u03c32LA \u03c32\u03ba)qj\u22121\u2016A\u2016L+1(LR)L+1/\u03c32LA \u03c32\u03ba)\nThe rest of the proof follows from very similar induction on Algorithm 3."}, {"heading": "C Details for Section 4", "text": "In this section we prove Lemma 4.1, which shows for a strongly convex function, given a biased estimator for the gradient we can still hope to get close to its optimal solution.\nLemma C.1 (Lemma 4.1 restated). Suppose the negative log-likelihood function F (\u03b8) = \u2212E[logH(\u03b8, S1)] is \u00b5strongly convex and H-smooth, given an estimator G(\u03b8) for the gradient such that \u2016G(\u03b8) \u2212 \u2207F (\u03b8)\u2016 \u2264 , gradient descent using G(\u03b8) with step size 12H converges to a solution \u03b8 such that \u2016\u03b8 \u2212 \u03b8\u2217\u2016 2 \u2264 8 2 \u00b52 .\nBefore proving this lemma we first introduce basic definitions for strongly convex functions.\nDefinition C.1 (\u00b5-strongly convex). A function F (\u03b8) (whose second order derivatives exist) is \u00b5-strongly convex if for any two points \u03b8, \u03c4 we have\nF (\u03b8) \u2265 F (\u03c4) + \u3008\u2207F (\u03c4), \u03b8 \u2212 \u03c4\u3009+ \u00b5 2 \u2016\u03b8 \u2212 \u03c4\u20162.\nDefinition C.2 (H-smooth). A function F (\u03b8) (whose second order derivatives exist) isH-smooth if for any two points \u03b8, \u03c4 we have\nF (\u03b8) \u2264 F (\u03c4) + \u3008\u2207F (\u03c4), \u03b8 \u2212 \u03c4\u3009+ H 2 \u2016\u03b8 \u2212 \u03c4\u20162.\nThe proof of Lemma 4.1 mostly follows from the approximate gradient framework in [2]. For completeness we also give the proof here.\nProof. Let \u03b8\u2217 be the optimal point. First, by \u00b5-strongly convexity we know\n\u3008\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2265 F (\u03b8)\u2212 F (\u03b8\u2217) + \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2217\u20162.\nOn the other hand, by H-smoothness we know\nF (\u03b8\u2217) \u2264 min \u03b7 F (\u03b8 \u2212 \u03b7\u2207F (\u03b8)) \u2264 min \u03b7 F (x)\u2212 \u03b7\u2016\u2207F (\u03b8)\u20162 + H\u03b7\n2\n2 \u2016\u2207F (\u03b8)\u20162 = F (x)\u2212 1 2H \u2016\u2207F (\u03b8)\u20162.\nTherefore \u3008\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2265 1\n2H \u2016\u2207F (\u03b8)\u20162 + \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2217\u20162. (11)\nNow we prove even when the gradientG(\u03b8) is only an approximation, the above equation still holds approximately.\nClaim C.1. \u3008G(\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2265 1\n4H \u2016G(\u03b8)\u20162 + \u00b5 4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 \u2212 2 2/\u00b5.\nProof. We know\n\u3008G(\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 = \u3008\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009+ \u3008G(\u03b8)\u2212\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2265 \u3008\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2212 \u2016\u03b8 \u2212 \u03b8\u2217\u2016\n\u2265 \u3008\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2212 \u00b5\n4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 \u2212\n2\n\u00b5 .\nAlso, \u2016G(\u03b8)\u20162 \u2264 2\u2016G(\u03b8)\u2212\u2207F (\u03b8)\u20162 + 2\u2016\u2207F (\u03b8)\u20162. Using these two inequalities in Equation (11), we get\n\u3008G(\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2265 \u3008\u2207F (\u03b8), \u03b8 \u2212 \u03b8\u2217\u3009 \u2212 \u00b5\n4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 \u2212\n2\n\u00b5\n\u2265 1 2H \u2016\u2207F (\u03b8)\u20162 + \u00b5 4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 \u2212\n2\n\u00b5\n\u2265 1 4H \u2016\u2207F (\u03b8)\u20162 + \u00b5 4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 \u2212\n2\n2H \u2212\n2\n\u00b5\n\u2265 1 4H \u2016\u2207F (\u03b8)\u20162 + \u00b5 4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 \u2212\n2 2\n\u00b5\nThe above Claim essentially matches the (\u03b1, \u03b2, )-approximate condition in [2]. Now suppose the update rule is\n\u03b8(t+1) = \u03b8(t) \u2212 1 2H G(\u03b8(t)).\nWe can then prove convergence result:\nClaim C.2. \u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 \u2264 (1\u2212 \u00b5\n4H )t\u2016\u03b8(0) \u2212 \u03b8\u2217\u20162 +\n8 2 \u00b52 .\nProof. We prove this by induction. Assume this is true for step t (the base case t = 0 is trivial), then for the next step we have\n\u2016\u03b8(t+1) \u2212 \u03b8\u2217\u20162 = \u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 \u2212 1 H \u3008G(\u03b8(t)), \u03b8(t) \u2212 \u03b8\u2217\u3009+ 1 4H2 \u2016G(\u03b8(t))\u20162\n= \u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 \u2212 1 2H (2\u3008G(\u03b8(t)), \u03b8(t)\u03b8\u2217\u3009 \u2212 1 2H \u2016G(\u03b8(t))\u20162)\n\u2264 \u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 \u2212 1 2H ( \u00b5 2 \u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 \u2212 4\n2\n\u00b5 )\n\u2264 (1\u2212 \u00b5 4H )\u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 + 2 2 \u00b5H .\nSubstituting in the bound for \u2016\u03b8(t) \u2212 \u03b8\u2217\u20162 we get the exact claim.\nTherefore by carefully choosing the step size gradient descent quickly converges to a nearby point (in fact similar argument works as long as the learning rate is upper bounded by 12H ). Similar arguments can be proved for stochastic gradient with a small enough step size (depending on the variance)."}, {"heading": "D Details for Section 5", "text": "Ising model inference. Let \u03b8 \u2261 {Jij}, the composite log-likelihood lcl of this Ising model can be written as\nlcl(\u03b8) = ES  \u2211 i\u2208Vert log P (S|\u03b8) P (S, S(i) = 1|\u03b8) + P (S, S(i) = \u22121|\u03b8)  .\nThe gradient of the composite log-likelihood with respect to a particular Jij is\n5Jij lcl = ES\n[ 2S(i)S(j)\n1 + exp(2S(i) \u2211 k\u2208neigh(i) JikS(k)) + 2S(i)S(j) 1 + exp(2S(j) \u2211 k\u2208neigh(j)JjkS(k))\n]\n\u2248 ES 2S(i)S(j)\u2212 S(i)2S(j) \u2211 k\u2208neigh(i) JikS(k)\u2212 S(i)S(j)2 \u2211 k\u2208neigh(j) JjkS(k)  where we have used the 4-th order Taylor expansion. We then use the 2nd and 4th order cumulant tensors from U and V to obtain unbiased estimator of terms ES [S(i)S(j)] and ES [S(i)2S(j)S(k)]. This gives the approximate gradient used in SGD. In the experiments, we used batch size of 100 samples to approximate each step of the gradient.\nBiomarkers experiment. In the simulation for having samples from multiple facilities, we get two views of the data, where U = S1 + S2 and V = S\u20322 + S3. Here S1 represents the values for test markers, S3 represents the values for control markers, and (S2, S\u20322) jointly represents the perturbation caused by different labs.\nIn our set up, we assume the samples come from two different labs, each lab has a bias on all the 20 markers (we use (p1, q1), (p2, q2) \u2208 R10 \u00d7 R10 to denote the biases). That is\n(S2, S \u2032 2) = { (p1, q1) sample from lab 1 (p2, q2) sample from lab 2\nIn this case, when the vectors p, q are in general positions, it is easy to see that there is a rank-2 matrix A such that S\u20322 = AS2. In particular, let P \u2208 R10\u00d72 be the matrix whose columns are p1, p2, Q \u2208 R10\u00d72 be the matrix whose columns are q1, q2, then we know A = QP \u2020.\nNote that this does not fit directly in our framework as the distribution S2 has low rank, and therefore the 4-th order cumulant cannot have full column rank. However, we can consider S2 = PX2 and S\u20322 = QX2 (where X2 = (1, 0) for samples from lab 1 and X2 = (0, 1) for samples from lab 2). We show that the algorithm still makes sense in this setting.\nLet W = unfold(\u03ba4(QX2, PX2, PX2, X2)) \u2208 R1000\u00d72, by the linearity of cumulants, we know\nM1 = unfold(\u03ba4(V,U, U, U)) = WP >, M2 = unfold(\u03ba4(V,U, U, V )) = WQ >.\nIn this case, M1 does not have full column rank, so the usual definition of pseudo-inverse does not work. However, we can write P = ZR where Z \u2208 R10\u00d72 is an orthonormal matrix (Z>Z = I), and R \u2208 R2\u00d72 and then hope to find M\u20201 such that M \u2020 1M1 = ZZ > (this is possible because we can let M\u20201 := Z(WR >)\u2020).\nWhen we use this definition of pseudo-inverse, it is easy to check that (M\u20201M2) > = QR\u22121Z> = QP \u2020, therefore\nour algorithm can still recover the correct rank-2 A matrix."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Simple, efficient, neural algorithms for dictionary learning", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Bounds for mixed cumulants and higher-order covariances of bounded random variables", "author": ["AV Bulinskii"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Handbook of Blind Source Separation: Independent component analysis and applications", "author": ["Pierre Comon", "Christian Jutten"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Modern factor analysis", "author": ["Harry H Harman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1976}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pages 321\u2013377,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1936}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Tensor decompositions and applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Approximation theory and methods", "author": ["Michael James David Powell"], "venue": "Cambridge university press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1981}, {"title": "Mathematical statistics with Mathematica, volume 1", "author": ["Colin Rose", "Murray D Smith"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Stochastic Convex Optimization", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A overview of composite likelihood methods", "author": ["Cristiano Varin", "Nancy Reid", "David Firth"], "venue": "Statistica Sinica,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Contrastive Learning Using Spectral Methods", "author": ["James Zou", "Daniel Hsu", "David Parkes", "Ryan Adam"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "3 Related models Independent component analysis (ICA)[4] may appear similar to our model, but it is actually quite different.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "Another related model is canonical correlation analysis (CCA)[6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Factor analysis (FA)[5] also corresponds to a multivariate Gaussian model, and hence does not address the general problem that we solve.", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "A different notion of contrastive learning was introduced in [14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Latent Dirichlet Allocation and many others (see [1]).", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "For convex functions this is known to converge to the optimal solution [12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "Chebyshev polynomials, see more in [10]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "We then use the estimator in [7] to get a low rank tensor whose components correspond to center vectors, and apply alternating minimization (see [9]) to infer \u03bc\u0302 k .", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "We then use the estimator in [7] to get a low rank tensor whose components correspond to center vectors, and apply alternating minimization (see [9]) to infer \u03bc\u0302 k .", "startOffset": 145, "endOffset": 148}, {"referenceID": 11, "context": "We use composite likelihood to estimate the couplings Jij of S1, which is asymptotically consistent with MLE of the true likelihood [13].", "startOffset": 132, "endOffset": 136}], "year": 2015, "abstractText": "In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, highdimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don\u2019t have samples from the true model but only samples after complex perturbations.", "creator": "LaTeX with hyperref package"}}}