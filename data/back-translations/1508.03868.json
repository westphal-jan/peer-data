{"id": "1508.03868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology", "abstract": "Each culture and language is unique, and our work explicitly focuses on the uniqueness of culture and language in relation to human emotions, especially sentiment and emotion semantics, and how they manifest themselves in social multimedia. We develop sentences of sentimental and emotion-driven visual concepts by adapting semantic structures, adjective-noun pairs, that were originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for the automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform to create a large-scale multilingual visual conceptionology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is hierarchically organized through multilingual clusters of visually detectable nouns and clusters of visual biasial versions of these nouns.", "histories": [["v1", "Sun, 16 Aug 2015 21:43:59 GMT  (8952kb,D)", "http://arxiv.org/abs/1508.03868v1", "11 pages, to appear at ACM MM'15"], ["v2", "Sat, 22 Aug 2015 16:33:13 GMT  (8952kb,D)", "http://arxiv.org/abs/1508.03868v2", "11 pages, to appear at ACM MM'15"], ["v3", "Wed, 7 Oct 2015 19:07:14 GMT  (8952kb,D)", "http://arxiv.org/abs/1508.03868v3", "11 pages, to appear at ACM MM'15"]], "COMMENTS": "11 pages, to appear at ACM MM'15", "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.CV cs.IR", "authors": ["brendan jou", "tao chen", "nikolaos pappas", "miriam redi", "mercan topkara", "shih-fu chang"], "accepted": false, "id": "1508.03868"}, "pdf": {"name": "1508.03868.pdf", "metadata": {"source": "CRF", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology", "authors": ["Brendan Jou", "Tao Chen", "Nikolaos Pappas", "Miriam Redi", "Mercan Topkara", "Shih-Fu Chang"], "emails": ["bjou@ee.columbia.edu", "taochen@ee.columbia.edu", "npappas@idiap.ch", "redi@yahoo-inc.com", "mercan@jwplayer.com", "sfchang@ee.columbia.edu", "Permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.5.4 [Information Interfaces and Presentation]: Hypertext/Hypermedia; I.2.10 [Artificial Intelligence]: Vision and Scene Understanding\nKeywords Multilingual; Language; Cultures; Cross-cultural; Emotion; Sentiment; Ontology; Concept Detection; Social Multimedia\n\u2217Denotes equal contribution.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. To Appear at MM\u201915, October 26\u201330, 2015, Brisbane, Australia. c\u00a9 2015 ACM. ISBN 978-1-4503-3459-4/15/10 ...$15.00. DOI: http://dx.doi.org/10.1145/2733373.2806246."}, {"heading": "1. INTRODUCTION", "text": "If you scoured the world and took several people at random from major countries and asked them to fill in the blank \u201c love\u201d in their native tongue, how many unique adjectives would you expect to find? Would people from some cultures tend to fill it with twisted, while others pure or unconditional or false? All over the world, we daily express our thoughts and feelings in culturally isolated contexts; and when we travel abroad, we know that to cross a physical border also means to cross into the unique behaviors and interactions of that people group \u2013 its cultural border. How similar or different are our sentiments and feelings from this other culture? Or the thoughts and objects we tend to talk about most? Motivated by questions like this, our work explores the computational understanding of human affect along cultural lines, with focus on visual content. In particular, we seek to answer the following important questions: (1) how are images in various languages used to express affective visual concepts, e.g. beautiful place or delicious food? And (2) how are such affective visual concepts used to convey different emotions and sentiment across languages? ar X iv :1\n50 8.\n03 86\n8v 1\n[ cs\n.M M\n] 1\n6 A\nug 2\nIn Psychology, there are two major schools-of-thought on the connection between cultural context and human affect, i.e. our experiential feelings via our sentiments and emotions. Some believe emotion to be culture-specific [29], that is, emotion is dependent on one\u2019s cultural context, while others believe emotion to be universal [16], that is, emotion and culture are independent mechanisms. For example, while this paper is written in English, there are emotion words/phrases in other languages for which there is no exact translation in English, e.g., Schadenfreude in German refers to the pleasure at someone else\u2019s expense. Do Englishspeakers not feel those same emotions or do they simply refer to them in a different way? Or even if the reference is the same, perhaps the underlying emotion is different?\nIn Affective Computing [33] and Multimedia, we often refer to the affective gap as the conceptual divide between the low-level visual stimuli, like images and features, and the high-level, abstracted semantics of human affect, e.g. happy or sad. In one attempt to bridge sentiment and visual media, Borth et al. [5] developed a visual sentiment ontology (VSO), a set of 1,200 mid-level concepts using structured semantics called adjective-noun pairs (ANPs). The noun portion of the ANP allows for computer vision detectability and the adjective serves to polarize the noun toward a positive or negative sentiment, or emotion, e.g. so instead of having visual concepts like sky or dog, we have beautiful sky or scary dog. Many works like this that have built algorithms, models and datasets on the assumption of the psychology theory that emotions are universal. However, while such works provide great research contributions in that native language, their applicability and generalization to other languages and cultures remains largely unexplored.\nWe present a large-scale multilingual visual sentiment ontology (MVSO) and dataset including adjective-noun pairs from 12 languages of diverse origins: Arabic, Chinese, Dutch, English, French, German, Italian, Persian, Polish, Russian, Spanish, and Turkish. We make the following contributions: (1) a principled, context-aware pipeline for designing a multilingual visual sentiment ontology, (2) a Multilingual Visual Sentiment Ontology mined from social multimedia data end-to-end, (3) a MVSO organized hierarchically into noun-\nbased clusters and sentiment-biased adjective-noun pair subclusters, (4) a multilingual, sentiment-driven visual concept detector bank, and (5) the release of a dataset containing MVSO and large-scale image collection with benchmark cross-lingual sentiment prediction1."}, {"heading": "2. RELATED WORK", "text": "We address the general challenge of affective image understanding, aiming at both recognition and analysis of sentiment and emotions in visual data, but from a multilingual and cross-cultural perspective. Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35]. Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].\nProgressive research in\u201cvisual affect\u201drecognition was done in [42] and [27] where image features were designed based on art and psychology principles for emotion prediction. And such works were later improved in [18] by adding social media data in semi-supervised frameworks. From this research effort in visual affect understanding, several affective image datasets were released to the public. The International Affective Picture System (IAPS) dataset [25] is a seminal dataset of \u223c1,000 images, focused on induced emotions in humans for biometric measurement. The Geneva Affective PicturE Database (GAPED) [8] consists of 730 pictures meant to supplement IAPS and tries to narrow the themes across images. And recently, in [5], a visual sentiment ontology (VSO) and dataset was created from Flickr image data, resulting in a collection of adjective-noun pairs along with corresponding images, tags and sentiment. One major issue with these datasets and existing methods is that they do not consider the context in which emotions are felt and perceived. Instead, they assume that visual affect is universal, and do not account for the influence of culture or language. We explicitly tackle visual affect understanding from a multi-cultural, multilingual perspective. In addition,\n1mvso.cs.columbia.edu\nwhile existing works often use handpicked data, we gather our data \u201cin the wild\u201d on a popular, multilingual social multimedia platform.\nThe study of emotions across language and culture has long been a topic of research in Psychology. A main contention in the area concerns whether emotions are culturespecific [29], i.e. their perception and elicitation varies with the context, or universal [16]. In [36], a survey of crosscultural work on semantics surrounding emotion elicitation and perception is given, showing that there are still competing views as to whether emotion is pan-cultural, culturespecific, or some hybrid of both. Inspired by research in this domain, we are the first to investigate the relationship between visual affect and culture2 from a multimedia-driven and computational perspective, as far as we know.\nOther work in cross-lingual research comes from text sentiment analysis and music information retrieval. In [3] and [31], they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively. In [26] and [17], they presented approaches to indexing digital music libraries with music from multiple languages. Specific to emotion, [17] tried to highlight differences between languages by building models for predicting the musical mood and then cross-predicting in other languages. Unlike these works, we propose a multimediadriven approach for cross-cultural visual sentiment analysis in the context of online image collections.\nIt is important to distinguish our work from that of Borth et al. on VSO [5] and its associated detector bank, SentiBank [4]. Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc. However, in addition to lack of multilingual support, there are several technical challenges with VSO [4, 5] that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to \u201cconstructed\u201d pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.. Our proposed MVSO discovery method can be easily extended to any language, while achieving greater coverage and diversity than VSO."}, {"heading": "3. ONTOLOGY CONSTRUCTION", "text": "An overview of the proposed method for multilingual visual sentiment concept ontology construction is shown in\n2Note that we use language and culture interchangeably often. We define language as the \u201clens\u201d through which we can observe culture. So while the two can be distinguished, for simplicity, we use them interchangeably.\nFigure 2. In the first stage, we obtain a set of images and their tags using seed emotion keyword queries, selected according to emotion ontologies from psychology such as [34] or [11]. Next, each image tag is labeled automatically by a language-specific part-of-speech tagger and adjective-noun combinations are discovered from words in the tags. Then, the combinations are filtered based on language, semantics, sentiment, frequency and diversity filters to ensure that the final set of ANPs have the following properties: (a) are written in the target language, (b) they do not refer to named entities or technical terms, (c) reflect a non-neutral sentiment, (d) are frequently used, and (e) are used by a non-trivial number of users of the target language.\nThe discovery of affective visual concepts for these languages using adjective-noun pairs poses several challenges in lexical, structural and semantic ambiguities, which are well-known problems in natural language processing. Lexical ambiguity is when a word has multiple meanings which depend on the context, e.g. sport jaguar or forest jaguar. Structural ambiguity is when a word might have different grammatical interpretation depending on the position in the context, e.g. ambient light or light room. Semantic ambiguity is when a combination of words with the same syntactic structure have different semantic interpretation, e.g. big apple. We selected languages in our MVSO according to the availability of public natural language processing tools and sentiment ontologies per language so that automatic processing was feasible. In addition, we sought to cover a wide range of geographic regions from the Americas to Europe and to Asia. We settled on 12 languages: Arabic, Chinese, Dutch, English, French, German, Italian, Persian, Polish, Russian, Spanish, and Turkish.\nWe applied our proposed data collection pipeline to a popular social multimedia sharing platform, Yahoo! Flickr3, and collected public data from November 2014 to February 2015 using the Flickr API. We selected Flickr because there is an existing body of multimedia research using it in the past, and in particular, [20] describes how Flickr satisfies two conditions for making use of the \u201cwisdom of the social multimedia\u201d: popularity and availability. We do not repeat the argument in [20], but note that in addition to those benefits, Flickr has multilingual support and the use of Flickr facilitates a natural comparison to the seminal VSO [5] work."}, {"heading": "3.1 Adjective-Noun Pair Discovery", "text": "As our seed emotion ontology, we selected the Plutchik\u2019s Wheel of Emotions [34]. This psychology ontology was selected because it consists of graded intensities for multiple basic emotions providing a richer set of emotional valences compared to alternatives like [11]; it has also been shown\n3www.flickr.com\nto be useful for VSO [5]. The Plutchik emotions are organized by eight basic emotions, each with three valences: ecstasy > joy > serenity; admiration > trust > acceptance; terror > fear > apprehension; amazement > surprise > distraction; grief > sadness > pensiveness; loathing > disgust > boredom; rage > anger > annoyance; and, vigilance > anticipation > interest.\nMultilingual Query Construction: To obtain seeds for each language, we recruited 12 native and proficient language speakers to provide a set of translated or synonymous keywords to those of the 24 Plutchik emotions. Speakers were allowed to use any number of keywords per emotion since the possible synonyms per emotion and language can vary, but they were asked to rank their chosen keywords along each emotion seed. They were also allowed to use tools like Google Translate4 or other resources to enrich their emotion keywords. Table 1 lists top ranked keywords according to speakers for 7 out of 12 languages in each emotion.\nGiven the set of keywords E(l) = {e(l)ij | i = 1 . . . 24, j = 1 . . . ni} describing each emotion i per language l, where ni is the number of keywords per emotion i, we performed tagbased queries on tags with the Flickr API to retrieve images and their related tags. Like [5], for each emotion, we chose to sample only the top 50K images ranked by Flickr relevance to simply limit the size of our results, but if an emotion had less than 50K images, we extended the search to additional metadata, i.e. title and description.\nPart-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13]. Though not all the tags contained multiple words, the average number of words was always greater than the average number of tags for all languages, so word context was almost always taken into account. From the full set of part-of-speech labels, we retained identified nouns, adjectives and other part-of-\n4translate.google.com\nspeech types which can be used as adjectives, such as simple or past participle (e.g. smiling face) in English.\nDiscovery Strategy: We based our discovery strategy for ANPs on co-occurrence in image tags, that is, if an adjective-noun pair is relevant to the specific emotion it should appear at least once as that exact pair phrase in the crawled images for that emotion. To validate the completeness of our strategy we compared with VSO and found that \u223c86% of ANPs discovered by VSO [5] overlap with the English ANPs discovered by our method."}, {"heading": "3.2 Filtering Candidate Adjective-Noun Pairs", "text": "From these discovered ANPs, we applied several filters to ensure they satisfied the following criteria: (a) written in the target language, (b) do not refer to named entities, (c) reflect a non-neutral sentiment, (d) frequently used and (e) used by multiple speakers of the language.\nLanguage & Semantics: We used a combination of language dictionaries5 instead of language classifiers to verify the correctness of the ANP as the performance of using the latter was low for short-length text, especially for Romance languages which share characters. All of the English ANPs were classified as indeed English by the dictionary, while for other languages, ANPs were removed if they passed the English dictionary filter but not the target language dictionary. The intuition for this was that most candidate ANPs in other languages were mixed mostly with English. We removed candidate pairs which referred to named entities or technical terms, where named entities were detected using several public knowledge bases such as Wikipedia and dictionaries for names6, cities, regions and countries7, and technical terms were removed via a manually created list of words specific to our source domain, Flickr, containing photography-related (e.g. macro, exposure) and camera-related words (e.g. DSLR).\nNon-neutral Sentiment: To filter out neutral candidate adjective-noun pairs, each ANP was scored in sentiment using two publicly available sentiment ontologies: SentiStrength [38] and SentiWordnet [12]. SentiStrength ontology supported all the languages we considered, but since SentiWordnet could only be used directly for English, we passed in automatic translations in English from all other languages to it, following previous research on multilingual sentiment analysis in machine translation [1, 2]8. We computed the ANP sentiment score S(anp) \u2208 [\u22122,+2] as:\nS(anp) = { S(a) : sgn {S(a)} 6= sgn {S(n)} S(a) + S(n) : otherwise (1)\nwhere S(a) \u2208 [\u22121,+1] and S(n) \u2208 [\u22121,+1] are the sentiment scores of the individual adjective and noun words, respectively, each of which are given by the arithmetic mean of SentiStrength and SentiWordnet scores on the word, and sgn is the sign of the scores. The piecewise condition essentially says that if the signs of the sentiment scores of the adjective and noun differ, then we ignore the noun. This\n5www.winedt.org 6www.wikipedia.org and www.ssa.gov, respectively 7www.geobytes.com 8For four non-English languages with the highest ANP counts, we have verified only a small percentage of nonneutral ANPs (less than 2%) reverse sentiment polarity after translation, confirming similar observations in the previous work.\nhighlights our belief that adjectives are the dominant sentiment modifiers in an adjective-noun pair, so for example, even if a noun is positive, like wedding, an adjective such as horrible would completely change the sentiment of the combined pair. And so, for these sign mismatch cases, we chose the adjective\u2019s sentiment alone. In the other case, when the sign of the adjective and noun were the same, whether both positive (e.g. happy wedding) or both negative (e.g. scary spider), we simply allowed the ANP sentiment score to be the unweighted sum of its parts. ANP candidates with zero sentiment score were filtered out.\nFrequency: Good ANPs are those which are actually used together. Here, we loosely defined an ANP\u2019s \u201cfrequency\u201d of usage as its number of occurrences as an image tag on Flickr. When computing counts for each pair, we accounted for language-specific syntax like the ordering of adjectives and nouns. Following anthropology research [10], we followed two dominant orderings (91.5% of the languages worldwide): adj-noun and noun-adj. We also \u201cmerged\u201d simplified and traditional forms in Chinese by considering them to be from the same language pool but distinct characters sets. In addition, we considered the possible intermediate Chinese character \u7684 during our frequency counting. For all non-English languages, we retained all ANPs that occurred at least once as an image tag; but for English, since Flickr\u2019s most dominant number of users are English-speaking, we set a higher frequency threshold of 40.\nDiversity: The shear frequency of an adjective-noun pair occurrence alone was not sufficient to ensure a pair\u2019s pervasive use in a language. We also checked if the ANP was used by a non-trivial number of distinct Flickr users for a given language. We identified the number of users contributing to uploads of images for each ANP and found a power law distribution in every language. To avoid this uploader bias, we removed all ANPs with less than three uploaders. Many removed candidate pairs came from companies and merchants for advertising and branding.\nTo further ensure diversity in our MVSO, we subsampled nouns in every language by limiting to the 100 most frequent ANPs per adjective so that we do not have, for example, the adjective surprising modifying every possible noun in our corpus. In addition, we performed stem unification by checking and including only the inflected form (e.g. singular/plural) of an ANP that was most popular in usage as a tag on Flickr. This unification did also filter some candidate ANPs as some \u201cduplicates\u201d were present but simply in different inflected forms."}, {"heading": "3.3 Crowdsourcing Validation", "text": "A further inspection of the corpus after the automatic filtering process showed that some issues could not completely be solved in an automatic fashion. Common errors included many fundamental natural language processing challenges like confusions in named entity recognition (e.g. big apple), language mixing (e.g. adjective in English + noun in Turkish), grammar inconsistency (e.g. adj-adj, or verb-noun) and semantic incongruity (e.g. happy happiness). So to refine our multilingual visual sentiment ontology, we crowdsourced a validation task. For each language, we asked native speaking workers to evaluate the correctness of ANPs post automatic filtering. We collected judgements using CrowdFlower9, a crowdsourcing platform that distributes small\n9www.crowdflower.com\ntasks to a large number of workers, where we limited workers by their language expertise. We note that while we elected to perform this additional stage of crowdsourcing, other researchers may find a fully automatic pipeline more desirable, so in our public release, we also release the pre-crowdsourced version of our MVSO.\n3.3.1 Crowdsourcing Setup We required that each ANP was evaluated by at least\nthree independent workers. To ensure high quality results, we also required workers to be (1) native speakers of the language, for which CrowdFlower had its own language competency and expertise test for workers, and (2) have a good reputation according to the crowdsourcing platform, measured by workers\u2019 performance on other annotation jobs. For whatever reasons, for three languages (Persian, Polish and Dutch), the CrowdFlower platform does not evaluate workers based on their language expertise, so we filtered them by provenience, selecting the countries according to the official language spoken (e.g. Netherlands, Belgium, Aruba and Suriname for Dutch).\nTask Interface: The verification task for workers consisted of simply evaluating the correctness of adjective-noun pairs. At the top of each page, we gave a short summary of the job and tasked workers: \u201cVerify that a word pair in <Language> is a valid adjective-noun pair.\u201d Workers were provided with a detailed definition of what an adjectivenoun pair is and a summary of the criteria for evaluating ANPs, i.e. it (1) is grammatically correct (adjective + noun), (2) shows language consistency, (3) shows generality, that is, commonly used and does not refer to a named entity, and (4) is semantically logical. To guide workers, examples of correct and incorrect ANPs were provided for each criteria, where these ground truths were carefully judged and selected by four independent expert annotators. In the interface, aside from instructions, workers were shown five ANPs and simply chose between \u201cyes\u201d or \u201cno\u201d to validate ANPs.\nQuality Control: Like some other crowdsourcing platforms, CrowdFlower provides a quality control mechanism called test questions to evaluate and track the performance of workers. These test questions come from pre-annotated ground truth, which in our case, correspond to ANPs with binary validation decisions for correctness. To access our task at all, workers were first required to correctly answer at least seven out of ten such test questions. In addition\nthough, worker performance was tracked throughout the course of the task where these test questions were randomly inserted at certain points, disguised as normal units. For each language, we asked language experts to select ten correct and ten incorrect adjective-noun pairs from each language corpus to serve as the test questions.\n3.3.2 Crowdsourcing Results To measure the quality of our crowdsourcing, we looked\nat the annotator agreement along each validation task. For all languages, the agreement was very strong with an average annotator agreement of 87%, where workers agreed on either the correctness or incorrectness of ANPs. We found that workers tended to agree more that ANPs were correct than that they were incorrect. This was likely due to the wide range of possible criteria for rejecting an ANP where some criteria are easy to evaluate (e.g. language consistency), while others, such as general usage versus named entity, may cause disagreement among users due to the cultural background of the worker. For example, not all workers may agree that an ANP like big eyes or big apple refers to a named entity. However, for languages where the agreement on the incorrect ANPs was high, namely Arabic, German, and Polish, the average annotator agreement as a percentage of all ANP for that language were greater than 90%.\nOn average, our crowdsourcing validated that a vast number of the input candidate ANPs from our automatic ANP discovery and filtering process were indeed correct ANPs. English, Spanish and Russian were the top three for which the automatic pipeline performed the best, where every three in five ANPs were approved by the crowd judgements. However, for certain languages, including German, Dutch, Persian and French, the number of ANPs rejected by the crowd was actually greater than accepted ANPs due to a higher occurrence of mixed language pairs, e.g. witzig humor. In Table 3, we summarize statistics from our crowdsourcing experiments according to the number of ANPs, percentage of correct/incorrect ANPs by worker majority vote, and average agreement."}, {"heading": "4. DATASET ANALYSIS & STATISTICS", "text": "Having acquired a final set of adjective-noun pairs for each of the 12 languages, we downloaded images by querying the Flickr API with ANPs using a mix of tag and metadata search. To limit the size of our dataset, we downloaded no more than 1,000 images per ANP query and also enforced a limit of no more than 20 images from any given uploader on Flickr for increased visual diversity. The selected 1,000 images were selected from the pool of retrieved image tag search results, but in the event that this pool is less than 1,000, we also enlarged the pool to include searches on the image title and description, or metadata. Selections from the pool of results were always randomized and a small number of images which Flickr or uploaders removed or changed privacy settings midway were removed. In total, we downloaded 7,368,364 images across 15,630 ANPs for the 12 languages, where English (4,049,507), Spanish (1,417,781) and Italian (845,664) contributed the most images."}, {"heading": "4.1 Comparison with VSO [5]", "text": "To verify and test the efficacy of our MVSO, we provide a comparison of our extracted English visual sentiment ontology with that of VSO [5] along dimensions of size (num-\nber of ANPs) and diversity of nouns and adjectives (Figure 3). In Figure 3a, the overlap of English MVSO with VSO is compared with VSO alone after applying all filtering criteria except subsampling which might exclude ANPs belonging to VSO. As mentioned previously, about 86% overlaps between them. As we vary a frequency threshold t (as described in Sec. 3.2) over image tag counts, the overlap converges to 100%. This confirms that the popular ANPs covered by VSO are also covered by MVSO, an interesting finding given the difference in the crawling time periods and approaches. In Figure 3b, we show that there are far greater number of ANPs in our English MVSO compared to VSO ANPs throughout all the possible values of frequency threshold, after applying all filtering criteria. Similarly, as shown in Figure 3c, given there are more adjectives and nouns in our English MVSO, we also achieve greater diversity than VSO.\nIn Figure 3d, we compare the number of ANPs for the remaining languages in MVSO with VSO after applying all filtering criteria. The curves show that VSO has more ANPs than all the languages for most of the languages over all values of t, except from Spanish, Italian and French in the low values of t. Our intuition is that this is due to the popularity of English on Flickr compared to other languages. In Figures 3e and 3d, we observe that these three languages have greater diversity of adjectives and nouns than VSO for t \u2264 103, German and Dutch have greater diversity than\nVSO for smaller values of threshold t, while the rest of the languages have smaller diversity over most values of t."}, {"heading": "4.2 Sentiment Distributions", "text": "Returning to our research motivation from the Introduction, an interesting question to ask is which languages tend to be more positive or negative in their visual content. To answer this, we computed the median sentiment value across all ANPs of each language and ranked languages as in Fig. 4. Here, to take into account the popularity difference among ANPs, we replicated each ANP k times, with k equal to the number of images tagged with the ANP, up to an upper limit L = \u03b1 \u00d7 Avgi, where Avgi is the average image count per ANP in the ith language. Varying \u03b1 value will result in different medians and distributions, but the trend in differentiating positive languages from negative ones was quite stable. We show the case when \u03b1 = 3 in Fig. 4, indicating that there is an overall tendency toward positive sentiment across all languages, where Spanish demonstrates the highest positive sentiment, followed by Italian. This surprising observation is in fact compatible with previous research showing that there is a universal positivity bias over languages with Spanish being the most relatively positive language [9]. The languages with the lowest sentiment were Persian and Arabic, followed by English.\nThe sentiment distributions (Fig. 4: right) also showed interesting phenomena: Spanish being the most positive language also has the highest variation in sentiment, while German has the most concentrated sentiment distribution. Even for languages that have the lowest median sentiment values, the range of sentiment was concentrated in a small range near zero (between 0 and -0.1)."}, {"heading": "4.3 Emotion Distributions", "text": "Another interesting question arises when considering cooccurrence of ANPs with the emotions in different languages. While our adjective-noun pair concepts were selected to be sentiment-biased, emotions still represent the root of our framework since we built MVSO out from seed emotion terms. So aside from sentiment, which focuses on only positivity/negativity, what are probable mappings of ANPs to emotions for each language? What emotions are most frequently occurring across languages? Given the set of keywords E(l) = {e(l)ij | i = 1 . . . 24, j = 1 . . . ni} describing each emotion i per language l, where ni is the number of keywords per emotion i, the set of ANPs belonging to language l, noted as x \u2208 X(l), and the number of images tagged with both ANP x and emotion keyword eij , C (x) = {c(x)ij | i =\n1 . . . 24, j = 1 . . . ni}, we define the probabilities of emotion for each ANP x in language l as:\nemoi(x) =\n1 ni \u2211ni j=1 c\n(x) ij\u221124\ni=1 1 ni \u2211ni j=1 c (x) ij \u2208 [0, 1] (2)\nNote the model in (2) does not take into account correlation among emotions, where for example, by an image tagged with \u201cecstasy,\u201d users may also imply \u201cjoy\u201d even though the latter is not explicitly tagged. These correlations can be easily accounted for by smoothing co-occurrence counts cij over correlated emotions, e.g. the co-occurrence counts of an ANP tagged with \u201cecstasy\u201d can be included partially in the co-occurrence count of \u201cjoy.\u201d Regardless, still based on (2), we compute a normalized emotion score per language l and emotion i as:\nscorei(l) =\n\u2211|X(l)| x=1 emo\ni(x) \u00b7 count(x)\u221124 i=1 \u2211|X(l)| x=1 emo i(x) \u00b7 count(x) \u2208 [0, 1] (3)\nFigure 5 shows these scores per language and Plutchik emotion [34] on a heatmap diagram. Scores in each row sum to 1 (over 24 emotions). The emotions are ordered by the sum of their scores across languages. The top-5 emotions across all languages are joy, serenity, interest, grief and fear. And the highest ranked emotion is joy in Russian, Chinese and Arabic. Two other emotions in the top-5 were also positive: serenity, being high ranked emotion for Dutch, Italian, Chinese and Persian, and interest for English, Turkish and Dutch. The remaining two emotions in the top-5 were negative: grief for Persian and Turkish, and fear, which was high ranked in German and Polish. We also observed that pensiveness was top ranked for Persian and Polish, vigilance for French, rage for German, while apprehension and distraction for Spanish. We note that these results are more concrete for languages with many ANPs (>1000) and less conclusive for those with few ANPs like Arabic and Persian."}, {"heading": "5. CROSS-LINGUAL MATCHING", "text": "To get a gauge on the topics commonly mentioned across different cultures and languages, we analyzed alignments of translations for each ANP to English as a basis. Two approaches were taken to study this: exact and approximate alignment. We ensured that translations of ANPs also passed all our validation filters described in Sec. 3.2 for this analysis.\nExact Alignment: We grouped ANPs from each language that have the exact same translation. For example, old books was the translation for one or more ANPs from seven languages, including \u8001\u66f8 (Chinese), livres anciens (French), vecchi libri (Italian), \u0421\u0442\u0430\u0440\u044b\u0435 \u043a\u043d\u0438\u0433\u0438 (Russian), libros antiguos (Spanish), eski kitaplar (Turkish). The translation covered by the greatest number of languages was beautiful girl with ANPs from ten languages. Figure 6 (left) shows a correlation matrix of the number of times ANPs from pairs of languages appeared together in a set with the exact same translation, e.g. out of all the translations that German ANPs were translated to (782), more of them were translated to the same phrase with the ANPs used by Dutch speakers (39) than with the ANPs used by Chinese speakers (23). This was striking given that there were less (340) translation phrases from Dutch than from Chinese (473).\nApproximate Alignment: Translations can be inaccurate, especially when capturing underlying semantics where context is not provided. And so, we relaxed the strict condition for exact matches by approximately matching using a hierarchical two-stage clustering approach instead. First, we extracted nouns using TreeTagger [37] from the list of translated phrases and discovered 3,099 total nouns. We then extracted word2vec [32] features, a word representation trained on a Google News10 corpus, for these translated nouns (188 nouns were out-of-vocabulary), and performed kmeans clustering (k=200) to get groups of nouns with similar meaning. The number of clusters was picked based on the coherence of clusters; and we picked the number where the inertia value of the clustering started saturating while gradually increasing k. In the second stage of our hierarchical clustering, we split phrases from the translations into different groups based on the clusters their nouns belonged to. We extracted word2vec [32] features from the full translated phrase in each cluster and ran another round of k-means clustering (adjusting k based on the number of phrases in each cluster, where phrases in each noun-cluster ranged from 3 to 253). This two-stage clustering enables us to create a hierarchical organization of our ANPs across languages and form a multilingual ontology over visual sentiment concepts (MVSO), unlike the flat structure in VSO [5]. We discovered 3,329 sub-clusters of ANP concepts, e.g. resulting in clusters containing little pony and little horse as in Figure 7. This approach also yielded a larger intersection between languages, where German and Dutch share 118 clusters, and German and Chinese intersect over 101 ANP clusters.\n10news.google.com\nThe correlation matrix from this approximate matching is shown in Figure 6, along with one subtree from our ontology by hierarchical clustering in Figure 7. For Figure 7, we projected data to R2 using t-SNE dimensionality reduction [40]. On the left, six clusters composed of different sets of nouns are shown with clusters of sunlight-rays-glow and dog-catpony. On the right, we show the sub-clustering of ANPs for the dog-cat-pony cluster in A, giving us noun groupings modified by sentiment-biasing adjectives to get ANPs like funny dog-funny cats and adopted dog-abondoned puppy."}, {"heading": "6. VISUAL SENTIMENT PREDICTION", "text": "To test the effectiveness of a vision-based approach for visual affect understanding when crossing languages, we designed and built language-specific sentiment predictors using the data collected with MVSO. Inspired by work in [17], we studied the extent to which the visual sentiments of a given language can be predicted by sentiment models of other languages. We chose to focus on a sentiment prediction task, i.e. predicting whether an image is of positive or negative sentiment, because there is a large body of work expressly focused on sentiment (e.g. [5, 42, 43]) for its simplicity, compared to emotion prediction. More importantly, we wanted to reduce the number of variables to be analyzed since our primary goal was to uncover cross-lingual differences.\nWe first constructed a bank of visual concept detectors like in [4] for our final MVSO adjective-noun pairs. For simplicity, we focused on the six languages with the most ANPs and associated images in our dataset: in decreasing order, English, Spanish, Italian, French, German and Chinese. Combined these six languages account for 94.7% of the ANPs in MVSO and 98.4% of the images in our dataset. However, to ensure that there were enough training images for each ANP, only the ANPs with no less than 125 images were selected for model training and prediction. This reduced the combined ANP coverage to 63.5% but still ensured 92.0% coverage for images. For each ANP, the images were split randomly 80/20% train/test, respectively."}, {"heading": "6.1 Visual Sentiment Concept Detectors", "text": "To construct our bank of visual concept detectors of ANPs, we used convolutional neural networks (CNNs), in particular, adopting an AlexNet-styled architecture [24] for its good performance on large-scale vision recognition and detection tasks. To train our detector bank, we fined-tuned six models, one for each language, where network weights were initial-\nized with DeepSentiBank [6], an AlexNet model trained on the VSO [5] dataset. This fine-tuning approach ensures that each network begins with weights that are already somewhat \u201caffectively\u201d biased. The base learning rates were set to 0.001 and the number of output neurons in the last fully connected layer were set to the number of training ANPs of each language. Step sizes for reducing the learning rate in the second stage were set proportional to the number of training images per language. For a single language, finetuning took between 12 and 40 hours for convergence on a single NVIDIA GTX 980 GPU implemented with Caffe [19]. From Table 4, as expected we achieve higher top-1 and top-5 accuracies than DeepSentiBank [6], even when the numbers of output neurons in English and Spanish are higher than those in [6]. Top-k accuracy refers to the percentage of classifications that the true class is in the top k predicted ranks."}, {"heading": "6.2 Sentiment Prediction on Flickr", "text": "We used the CNN-based visual concept models trained for each language to extract image features and use the sentiment scores of ANPs as supervised labels to learn sentiment prediction models. We compared different layers of the CNN models as image features. To simplify the process, we bi-\nnarized the ANP sentiment scores computed with Eq. (1), i.e. into positive and negative classes, and learned a binary classifier using linear SVMs, one for each language. The training images are those associated with ANPs of strong sentiment scores (absolute values higher than 0.05). Splits of training and test sets were stratified across all languages so that the amount of training and testing for positive and negative sentiment classes was the same for fair cross-lingual experiment comparison.\nWe found that the softmax output features from the penultimate layer outputs of each language\u2019s CNN model performed the best for all languages, and we show resulting sentiment prediction results in Figure 9. Each language expectedly did better in predicting test samples from its own language, but in addition, Chinese generally was the most difficult to predict by models trained from other languages; and using a sentiment model trained over Chinese images to predict the sentiment in other languages was also the worst in average. We speculate that this is due to the difference in the visual sentiment portrayal from Eastern and Western cultures. Interestingly, the classification of French and Italian sentiments was the most consistent using models from all languages. We also observed good performance in crosslingual prediction for Latin languages, i.e. Spanish, Italian and French, where Italian was the best cross-lingual classifier for Spanish and French sentiment, and Spanish was best for Italian sentiment, followed by French. Despite not performing as well as others in average, the English-specific sentiment model had the least variance in its accuracy across all languages, likely from the pervasiveness of English worldwide and across cultures.\nIn Figure 10, we show three classification example results from our cross-lingual sentiment prediction. On the left, an image from the Italian test set representing the costumi tradizionali concept was labeled as positive via sentiment scoring, but was predicted by the German model to be negative; this may be due to differences in cultural perceptions of traditional clothing. In the center, the Chinese model wrongly predicted an image from the English test set of foggy morning as positive, possibly for its resemblance to a Chinese painting. And on the right, an image of a beau village from the French test set was successfully classified as\npositive with the Spanish sentiment predictor. These examples and preliminary experiments highlight some similarities and differences in how visual sentiment is expressed and perceived by various cultures."}, {"heading": "7. CONCLUSION & FUTURE WORK", "text": "We proposed a new multilingual discovery method for visual sentiment concepts and showed its efficacy on a social multimedia platform for 12 languages. We based our approach on the psychology theory that emotions are culturespecific and carry inherent linguistic context, and so we showed how to use language-specific part-of-speech labeling along with progressive filtering to achieve coverage and diversity of visual affect concepts in multiple languages. In addition, we presented a two-stage hierarchical clustering approach to unify our ontology across languages. And we make our Multilingual Visual Sentiment Ontology (MVSO), pre-crowdsourcing as well as post, and image dataset, available to the public. A cross-lingual analysis of our large-scale MVSO and image dataset using semantic matching and visual sentiment prediction hint that emotions are not necessarily culturally universal. Our preliminary results show that there are indeed commonalities, but also distinct separations, in how visual affect is expressed and perceived, where other works assumed only commonalities. We believe these point to the colorful diversity of our world, rather than our inability to understand one another.\nIn the future, we plan to explore differences along other human factors which can be collected from self-reported user metadata like age group, gender, profession, etc. We will also adopt our approach to other language-specific social multimedia platforms to counter the insufficient data for some languages like Arabic, Persian and Chinese. In addition, while we discussed culture and languages in this work, we have not yet performed an in-depth study on geo-location data in MVSO, often provided along with uploaded images on Flickr. While such information could be useful to distinguish between sub-cultures speaking the same languages (e.g. Spanish vs. South-Americans), we omitted such a study here because of the noise that geo-location data can add. For example, an American traveling in China uploading pictures is still more likely to use their native tongue to tag and sentimentally describe their content. The trade-off is that while their semantics are culturally American, the uploaded visual content is now from another culture, so there is still much to be explored from geo-location and user metadata."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "Research was sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program, Agreement Number W911NF-12-C-0028. The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the of official policies, either expressed or implied, of the U.S. Defense Advanced Research Projects Agency or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here-on. Co-author B. Jou was supported by the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. And co-author N. Pappas was supported by the InEvent (FP7-ICT n. 287872) and MODERN Sinergia (CRSII2 147653) projects."}, {"heading": "9. REFERENCES", "text": "[1] A. Balahur and M. Turchi. Multilingual sentiment analysis using machine translation? In WASSA, 2012.\n[2] C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. Multilingual subjectivity analysis using machine translation. In EMNLP, 2008.\n[3] M. Bautin, L. Vijayarenu, and S. Skiena. International sentiment analysis for news and blogs. In ICWSM, 2008.\n[4] D. Borth, T. Chen, R. Ji, and S.-F. Chang. SentiBank: Large-scale ontology and classifiers for detecting sentiment and emotions in visual content. In ACM MM, 2013.\n[5] D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In ACM MM, 2013.\n[6] T. Chen, D. Borth, T. Darrell, and S.-F. Chang. DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks. arXiv preprint arXiv:1410.8586, 2014.\n[7] Y.-Y. Chen, T. Chen, W. H. Hsu, H.-Y. M. Liao, and S.-F. Chang. Predicting viewer affective comments based on image content in social media. In ICMR, 2014.\n[8] E. S. Dan-Glauser and K. Scherer. The Geneva affective picture database: A new 730-picture database focusing on valence and normative significance. Behav. Res. Meth., 43(2), 2011.\n[9] S. Dodds and et al. Human language reveals a universal positivity bias. PNAS, 112(8), 2015.\n[10] M. S. Dryer and M. Haspelmath, editors. WALS Online. Max Planck Institute for Evolutionary Anthropology, 2013. http://wals.info/chapter/87.\n[11] P. Ekman. Facial expression and emotion. American Psychologist, 48(4), 1993.\n[12] A. Esuli and F. Sebastiani. SENTIWORDNET: A publicly available lexical resource for opinion mining. In LREC, 2006.\n[13] Z. Gu\u0308ngo\u0308rdu\u0308 and K. Oflazer. Parsing Turkish using the lexical functional grammar formalism. In ACL, 1994.\n[14] M. Gygli, H. Grabner, H. Riemenschneider, F. Nater, and L. V. Gool. The interestingness of images. In ICCV, 2013.\n[15] P. Hala\u0301csy, A. Kornai, and C. Oravecz. HunPos: An open source trigram tagger. In ACL, 2007.\n[16] M. G. Haselton and T. Ketelaar. Irrational emotions or emotional wisdom? The evolutionary psychology of affect and social behavior. Affect in Soc. Think. and Behav., 8(21), 2006.\n[17] X. Hu and Y.-H. Yang. Cross-cultural mood regression for music digital libraries. In JCDL, 2014.\n[18] J. Jia, S. Wu, X. Wang, P. Hu, L. Cai, and J. Tang. Can we understand van Gogh\u2019s mood?: Learning to infer affects from images in social networks. In ACM MM, 2012.\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM MM, 2014.\n[20] X. Jin, A. Gallagher, L. Cao, J. Luo, and J. Han. The wisdom of social multimedia: Using Flickr for prediction and forecast. In ACM MM, 2010.\n[21] B. Jou, S. Bhattacharya, and S.-F. Chang. Predicting viewer perceived emotions in animated GIFs. In ACM MM, 2014.\n[22] Y. Ke, X. Tang, and F. Jing. The design of high-level features for photo quality assessment. In CVPR, 2006.\n[23] A. Khosla, A. Das Sarma, and R. Hamid. What makes an image popular? In WWW, 2014.\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.\n[25] P. Lang, M. Bradley, and B. Cuthbert. International Affective Picture System (IAPS): Technical manual and affective ratings. Technical report, NIMH CSEA, 1997.\n[26] J. H. Lee, J. S. Downie, and S. J. Cunningham. Challenges in cross-cultural/multilingual music information seeking. In ISMIR, 2005.\n[27] J. Machajdik and A. Hanbury. Affective image classification using features inspired by psychology and art theory. In ACM MM, 2010.\n[28] H. R. Markus and S. Kitayama. Culture and the self: Implications for cognition, emotion, and motivation. Psychological Review, 98(2), 1991.\n[29] E. D. McCarthy. The social construction of emotions: New directions from culture theory. Social Perspectives on Emotion, 2, 1994.\n[30] B. Mesquita, N. H. Frijda, and K. Scherer. Culture and emotion. In J. W. Berry, P. R. Dasen, and T. S. Saraswathi, editors, Handbook of Cross-cultural Psychology, volume 2. Allyn & Bacon, 1997.\n[31] R. Mihalcea, C. Banea, and J. Wiebe. Learning multilingual subjective language via cross-lingual projections. In ACL, 2007.\n[32] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.\n[33] R. W. Picard. Affective Computing. MIT Press, 1997.\n[34] R. Plutchik. Emotion: A Psychoevolutionary Synthesis. Harper & Row, 1980.\n[35] M. Redi, N. O\u2019Hare, R. Schifanella, M. Trevisiol, and A. Jaimes. 6 Seconds of sound and vision: Creativity in micro-videos. In CVPR, 2014.\n[36] J. A. Russell. Culture and the categorization of emotions. Psychological Bulletin, 110(3), 1991.\n[37] H. Schmid. Probabilistic part-of-speech tagging using decision trees. In Intl Conf. on New Methods in Language Proc., 1994.\n[38] M. Thelwall, K. Buckley, G. Paltoglou, and D. Cai. Sentiment strength detection in short informal text. Jour. Ameri. Soci. for Info. Sci. & Tech., 61(12), 2010.\n[39] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL, 2003.\n[40] L. van der Maaten and G. E. Hinton. Visualizing high-dimensional data using t-SNE. JMLR, 9, 2008.\n[41] E. A. Vessel, J. Stahl, N. Maurer, A. Denker, and G. G. Starr. Personalized visual aesthetics. In SPIE-IS&T Electronic Imaging, 2014.\n[42] V. Yanulevskaya, J. van Gemert, K. Roth, A. Herbold, N. Sebe, and J. M. Geusebroek. Emotional valence categorization using holistic image features. In ICIP, 2008.\n[43] Q. You, J. Luo, H. Jin, and J. Yang. Robust image sentiment analysis using progressively trained and domain transferred deep networks. In AAAI, 2014.\n[44] R. B. Zajonc. Feeling and thinking: Preferences need no inferences. American Psychologist, 35(2), 1980."}], "references": [{"title": "Multilingual sentiment analysis using machine translation", "author": ["A. Balahur", "M. Turchi"], "venue": "WASSA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Multilingual subjectivity analysis using machine translation", "author": ["C. Banea", "R. Mihalcea", "J. Wiebe", "S. Hassan"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "International sentiment analysis for news and blogs", "author": ["M. Bautin", "L. Vijayarenu", "S. Skiena"], "venue": "In ICWSM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "SentiBank: Large-scale ontology and classifiers for detecting sentiment and emotions in visual content", "author": ["D. Borth", "T. Chen", "R. Ji", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["T. Chen", "D. Borth", "T. Darrell", "S.-F. Chang"], "venue": "arXiv preprint arXiv:1410.8586,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Predicting viewer affective comments based on image content in social media", "author": ["Y.-Y. Chen", "T. Chen", "W.H. Hsu", "H.-Y.M. Liao", "S.-F. Chang"], "venue": "In ICMR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The Geneva affective picture database: A new 730-picture database focusing on valence and normative significance", "author": ["E.S. Dan-Glauser", "K. Scherer"], "venue": "Behav. Res. Meth.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Human language reveals a universal positivity bias", "author": ["S. Dodds"], "venue": "PNAS, 112(8),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Facial expression and emotion", "author": ["P. Ekman"], "venue": "American Psychologist,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "SENTIWORDNET: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "In LREC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Parsing Turkish using the lexical functional grammar formalism", "author": ["Z. G\u00fcng\u00f6rd\u00fc", "K. Oflazer"], "venue": "In ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "The interestingness of images", "author": ["M. Gygli", "H. Grabner", "H. Riemenschneider", "F. Nater", "L.V. Gool"], "venue": "In ICCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "HunPos: An open source trigram tagger", "author": ["P. Hal\u00e1csy", "A. Kornai", "C. Oravecz"], "venue": "In ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Irrational emotions or emotional wisdom? The evolutionary psychology of affect and social behavior", "author": ["M.G. Haselton", "T. Ketelaar"], "venue": "Affect in Soc. Think. and Behav.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Cross-cultural mood regression for music digital libraries", "author": ["X. Hu", "Y.-H. Yang"], "venue": "In JCDL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Can we understand van Gogh\u2019s mood?: Learning to infer affects from images in social networks", "author": ["J. Jia", "S. Wu", "X. Wang", "P. Hu", "L. Cai", "J. Tang"], "venue": "In ACM MM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM MM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The wisdom of social multimedia: Using Flickr for prediction and forecast", "author": ["X. Jin", "A. Gallagher", "L. Cao", "J. Luo", "J. Han"], "venue": "In ACM MM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Predicting viewer perceived emotions in animated GIFs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "The design of high-level features for photo quality assessment", "author": ["Y. Ke", "X. Tang", "F. Jing"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "What makes an image popular", "author": ["A. Khosla", "A. Das Sarma", "R. Hamid"], "venue": "In WWW,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "International Affective Picture System (IAPS): Technical manual and affective ratings", "author": ["P. Lang", "M. Bradley", "B. Cuthbert"], "venue": "Technical report, NIMH CSEA,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Challenges in cross-cultural/multilingual music information seeking", "author": ["J.H. Lee", "J.S. Downie", "S.J. Cunningham"], "venue": "In ISMIR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "In ACM MM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Culture and the self: Implications for cognition, emotion, and motivation", "author": ["H.R. Markus", "S. Kitayama"], "venue": "Psychological Review,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1991}, {"title": "The social construction of emotions: New directions from culture theory", "author": ["E.D. McCarthy"], "venue": "Social Perspectives on Emotion,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Culture and emotion", "author": ["B. Mesquita", "N.H. Frijda", "K. Scherer"], "venue": "Handbook of Cross-cultural Psychology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["R. Mihalcea", "C. Banea", "J. Wiebe"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Affective Computing", "author": ["R.W. Picard"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Emotion: A Psychoevolutionary Synthesis", "author": ["R. Plutchik"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1980}, {"title": "Seconds of sound and vision: Creativity in micro-videos", "author": ["M. Redi", "N. O\u2019Hare", "R. Schifanella", "M. Trevisiol", "A. Jaimes"], "venue": "In CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Culture and the categorization of emotions", "author": ["J.A. Russell"], "venue": "Psychological Bulletin,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1991}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "In Intl Conf. on New Methods in Language Proc.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Sentiment strength detection in short informal text", "author": ["M. Thelwall", "K. Buckley", "G. Paltoglou", "D. Cai"], "venue": "Jour. Ameri. Soci. for Info. Sci. & Tech.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "In NAACL,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["L. van der Maaten", "G.E. Hinton"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Personalized visual aesthetics", "author": ["E.A. Vessel", "J. Stahl", "N. Maurer", "A. Denker", "G.G. Starr"], "venue": "In SPIE-IS&T Electronic Imaging,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Emotional valence categorization using holistic image features", "author": ["V. Yanulevskaya", "J. van Gemert", "K. Roth", "A. Herbold", "N. Sebe", "J.M. Geusebroek"], "venue": "In ICIP,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "In AAAI,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Feeling and thinking: Preferences need no inferences", "author": ["R.B. Zajonc"], "venue": "American Psychologist,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1980}], "referenceMentions": [{"referenceID": 4, "context": "[5], but in a multilingual context.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Unlike the flat structure in [5], our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": ", t5) are labeled with part-of-speech tags, and adjectives and nouns are used to form candidate adjective-noun pair (ANP) combinations [5], while others are ignored (in red).", "startOffset": 135, "endOffset": 138}, {"referenceID": 27, "context": "Some believe emotion to be culture-specific [29], that is, emotion is dependent on one\u2019s cultural context, while others believe emotion to be universal [16], that is, emotion and culture are independent mechanisms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Some believe emotion to be culture-specific [29], that is, emotion is dependent on one\u2019s cultural context, while others believe emotion to be universal [16], that is, emotion and culture are independent mechanisms.", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "Do Englishspeakers not feel those same emotions or do they simply refer to them in a different way? Or even if the reference is the same, perhaps the underlying emotion is different? In Affective Computing [33] and Multimedia, we often refer to the affective gap as the conceptual divide between the low-level visual stimuli, like images and features, and the high-level, abstracted semantics of human affect, e.", "startOffset": 206, "endOffset": 210}, {"referenceID": 4, "context": "[5] developed a visual sentiment ontology (VSO), a set of 1,200 mid-level concepts using structured semantics called adjective-noun pairs (ANPs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 133, "endOffset": 137}, {"referenceID": 33, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 110, "endOffset": 122}, {"referenceID": 28, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 110, "endOffset": 122}, {"referenceID": 32, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 110, "endOffset": 122}, {"referenceID": 39, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 39, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 173, "endOffset": 181}, {"referenceID": 42, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 173, "endOffset": 181}, {"referenceID": 26, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 206, "endOffset": 210}, {"referenceID": 40, "context": "Progressive research in\u201cvisual affect\u201drecognition was done in [42] and [27] where image features were designed based on art and psychology principles for emotion prediction.", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "Progressive research in\u201cvisual affect\u201drecognition was done in [42] and [27] where image features were designed based on art and psychology principles for emotion prediction.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "And such works were later improved in [18] by adding social media data in semi-supervised frameworks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "The International Affective Picture System (IAPS) dataset [25] is a seminal dataset of \u223c1,000 images, focused on induced emotions in humans for biometric measurement.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "The Geneva Affective PicturE Database (GAPED) [8] consists of 730 pictures meant to supplement IAPS and tries to narrow the themes across images.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "And recently, in [5], a visual sentiment ontology (VSO) and dataset was created from Flickr image data, resulting in a collection of adjective-noun pairs along with corresponding images, tags and sentiment.", "startOffset": 17, "endOffset": 20}, {"referenceID": 27, "context": "A main contention in the area concerns whether emotions are culturespecific [29], i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "their perception and elicitation varies with the context, or universal [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "In [36], a survey of crosscultural work on semantics surrounding emotion elicitation and perception is given, showing that there are still competing views as to whether emotion is pan-cultural, culturespecific, or some hybrid of both.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [3] and [31], they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively.", "startOffset": 3, "endOffset": 6}, {"referenceID": 29, "context": "In [3] and [31], they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "In [26] and [17], they presented approaches to indexing digital music libraries with music from multiple languages.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [26] and [17], they presented approaches to indexing digital music libraries with music from multiple languages.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Specific to emotion, [17] tried to highlight differences between languages by building models for predicting the musical mood and then cross-predicting in other languages.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "on VSO [5] and its associated detector bank, SentiBank [4].", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "on VSO [5] and its associated detector bank, SentiBank [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc.", "startOffset": 124, "endOffset": 131}, {"referenceID": 19, "context": "Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc.", "startOffset": 124, "endOffset": 131}, {"referenceID": 6, "context": "Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "However, in addition to lack of multilingual support, there are several technical challenges with VSO [4, 5] that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to \u201cconstructed\u201d pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.", "startOffset": 102, "endOffset": 108}, {"referenceID": 4, "context": "However, in addition to lack of multilingual support, there are several technical challenges with VSO [4, 5] that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to \u201cconstructed\u201d pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.", "startOffset": 102, "endOffset": 108}, {"referenceID": 32, "context": "In the first stage, we obtain a set of images and their tags using seed emotion keyword queries, selected according to emotion ontologies from psychology such as [34] or [11].", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "In the first stage, we obtain a set of images and their tags using seed emotion keyword queries, selected according to emotion ontologies from psychology such as [34] or [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "We selected Flickr because there is an existing body of multimedia research using it in the past, and in particular, [20] describes how Flickr satisfies two conditions for making use of the \u201cwisdom of the social multimedia\u201d: popularity and availability.", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "We do not repeat the argument in [20], but note that in addition to those benefits, Flickr has multilingual support and the use of Flickr facilitates a natural comparison to the seminal VSO [5] work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "We do not repeat the argument in [20], but note that in addition to those benefits, Flickr has multilingual support and the use of Flickr facilitates a natural comparison to the seminal VSO [5] work.", "startOffset": 190, "endOffset": 193}, {"referenceID": 32, "context": "As our seed emotion ontology, we selected the Plutchik\u2019s Wheel of Emotions [34].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "This psychology ontology was selected because it consists of graded intensities for multiple basic emotions providing a richer set of emotional valences compared to alternatives like [11]; it has also been shown", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "to be useful for VSO [5].", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Like [5], for each emotion, we chose to sample only the top 50K images ranked by Flickr relevance to simply limit the size of our results, but if an emotion had less than 50K images, we extended the search to additional metadata, i.", "startOffset": 5, "endOffset": 8}, {"referenceID": 35, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 240, "endOffset": 244}, {"referenceID": 37, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 262, "endOffset": 266}, {"referenceID": 13, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 11, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 328, "endOffset": 332}, {"referenceID": 4, "context": "To validate the completeness of our strategy we compared with VSO and found that \u223c86% of ANPs discovered by VSO [5] overlap with the English ANPs discovered by our method.", "startOffset": 112, "endOffset": 115}, {"referenceID": 36, "context": "Non-neutral Sentiment: To filter out neutral candidate adjective-noun pairs, each ANP was scored in sentiment using two publicly available sentiment ontologies: SentiStrength [38] and SentiWordnet [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 10, "context": "Non-neutral Sentiment: To filter out neutral candidate adjective-noun pairs, each ANP was scored in sentiment using two publicly available sentiment ontologies: SentiStrength [38] and SentiWordnet [12].", "startOffset": 197, "endOffset": 201}, {"referenceID": 0, "context": "SentiStrength ontology supported all the languages we considered, but since SentiWordnet could only be used directly for English, we passed in automatic translations in English from all other languages to it, following previous research on multilingual sentiment analysis in machine translation [1, 2].", "startOffset": 295, "endOffset": 301}, {"referenceID": 1, "context": "SentiStrength ontology supported all the languages we considered, but since SentiWordnet could only be used directly for English, we passed in automatic translations in English from all other languages to it, following previous research on multilingual sentiment analysis in machine translation [1, 2].", "startOffset": 295, "endOffset": 301}, {"referenceID": 4, "context": "1 Comparison with VSO [5]", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "To verify and test the efficacy of our MVSO, we provide a comparison of our extracted English visual sentiment ontology with that of VSO [5] along dimensions of size (numFigure 3: Comparison of our English MVSO and VSO [5] in Figures (a), (b) and (c), in terms of ANP overlap, no.", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "To verify and test the efficacy of our MVSO, we provide a comparison of our extracted English visual sentiment ontology with that of VSO [5] along dimensions of size (numFigure 3: Comparison of our English MVSO and VSO [5] in Figures (a), (b) and (c), in terms of ANP overlap, no.", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "This surprising observation is in fact compatible with previous research showing that there is a universal positivity bias over languages with Spanish being the most relatively positive language [9].", "startOffset": 195, "endOffset": 198}, {"referenceID": 0, "context": "i=1 1 ni \u2211ni j=1 c (x) ij \u2208 [0, 1] (2)", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "i=1 \u2211|X(l)| x=1 emo i(x) \u00b7 count(x) \u2208 [0, 1] (3)", "startOffset": 38, "endOffset": 44}, {"referenceID": 32, "context": "Figure 5 shows these scores per language and Plutchik emotion [34] on a heatmap diagram.", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "First, we extracted nouns using TreeTagger [37] from the list of translated phrases and discovered 3,099 total nouns.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "We then extracted word2vec [32] features, a word representation trained on a Google News corpus, for these translated nouns (188 nouns were out-of-vocabulary), and performed kmeans clustering (k=200) to get groups of nouns with similar meaning.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "We extracted word2vec [32] features from the full translated phrase in each cluster and ran another round of k-means clustering (adjusting k based on the number of phrases in each cluster, where phrases in each noun-cluster ranged from 3 to 253).", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "This two-stage clustering enables us to create a hierarchical organization of our ANPs across languages and form a multilingual ontology over visual sentiment concepts (MVSO), unlike the flat structure in VSO [5].", "startOffset": 209, "endOffset": 212}, {"referenceID": 30, "context": "For visualization, word2vec [32] vectors were projected to R using t-SNE [40].", "startOffset": 28, "endOffset": 32}, {"referenceID": 38, "context": "For visualization, word2vec [32] vectors were projected to R using t-SNE [40].", "startOffset": 73, "endOffset": 77}, {"referenceID": 38, "context": "For Figure 7, we projected data to R using t-SNE dimensionality reduction [40].", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Inspired by work in [17], we studied the extent to which the visual sentiments of a given language can be predicted by sentiment models of other languages.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "[5, 42, 43]) for its simplicity, compared to emotion prediction.", "startOffset": 0, "endOffset": 11}, {"referenceID": 40, "context": "[5, 42, 43]) for its simplicity, compared to emotion prediction.", "startOffset": 0, "endOffset": 11}, {"referenceID": 41, "context": "[5, 42, 43]) for its simplicity, compared to emotion prediction.", "startOffset": 0, "endOffset": 11}, {"referenceID": 3, "context": "We first constructed a bank of visual concept detectors like in [4] for our final MVSO adjective-noun pairs.", "startOffset": 64, "endOffset": 67}, {"referenceID": 22, "context": "To construct our bank of visual concept detectors of ANPs, we used convolutional neural networks (CNNs), in particular, adopting an AlexNet-styled architecture [24] for its good performance on large-scale vision recognition and detection tasks.", "startOffset": 160, "endOffset": 164}, {"referenceID": 5, "context": "ized with DeepSentiBank [6], an AlexNet model trained on the VSO [5] dataset.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "ized with DeepSentiBank [6], an AlexNet model trained on the VSO [5] dataset.", "startOffset": 65, "endOffset": 68}, {"referenceID": 17, "context": "For a single language, finetuning took between 12 and 40 hours for convergence on a single NVIDIA GTX 980 GPU implemented with Caffe [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "From Table 4, as expected we achieve higher top-1 and top-5 accuracies than DeepSentiBank [6], even when the numbers of output neurons in English and Spanish are higher than those in [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "From Table 4, as expected we achieve higher top-1 and top-5 accuracies than DeepSentiBank [6], even when the numbers of output neurons in English and Spanish are higher than those in [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "0% DSB [6] 2,089 826,806 41,113 - - 8.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "Table 4: Adjective-noun pair (ANP) classification performance on Flickr images for six major languages in MVSO and compared to DeepSentiBank (DSB) [6].", "startOffset": 147, "endOffset": 150}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}