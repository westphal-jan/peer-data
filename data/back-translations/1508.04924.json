{"id": "1508.04924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Distributed Compressive Sensing: A Deep Learning Approach", "abstract": "We deal with the problem of compressed sampling with Multiple Measurement Vectors (MMVs), when the structure of sparse vectors in different channels depends on each other. \"The sparse vectors are not necessarily sparse.\" We capture this dependence by calculating the conditional probability of each input of sparse vectors that the \"residuals\" of all previous sparse vectors are not zero. To calculate these probabilities, we propose to use Long Short-Term Memory (LSTM) [1], a data-driven bottom-up model for sequence modeling. To force model parameters, we minimize a cross-entropy cost function. We propose a greedy solver working on the decoder over probabilities. By conducting extensive experiments with two sets of real-world data, we show that the proposed method clearly meets the general MV simultaneous solution of SOGI (Pursuit) and Oryesian method.", "histories": [["v1", "Thu, 20 Aug 2015 08:57:29 GMT  (1290kb,D)", "http://arxiv.org/abs/1508.04924v1", null], ["v2", "Mon, 7 Sep 2015 01:15:11 GMT  (1508kb,D)", "http://arxiv.org/abs/1508.04924v2", null], ["v3", "Wed, 11 May 2016 22:18:13 GMT  (2747kb,D)", "http://arxiv.org/abs/1508.04924v3", "To appear in IEEE Transactions on Signal Processing"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["hamid palangi", "rabab ward", "li deng"], "accepted": false, "id": "1508.04924"}, "pdf": {"name": "1508.04924.pdf", "metadata": {"source": "CRF", "title": "Distributed Compressive Sensing: A Deep Learning Approach", "authors": ["Hamid Palangi", "Rabab Ward", "Li Deng"], "emails": ["hamidp@ece.ubc.ca)", "rababw@ece.ubc.ca)", "deng@microsoft.com)"], "sections": [{"heading": null, "text": "Index Terms\u2014Compressive Sensing, Deep Learning, Long Short-Term Memory.\nI. INTRODUCTION\nCOMPRESSIVE Sensing (CS) [4],[5],[6] is an ef-fective approach for acquiring sparse signals by which both sensing and compression are performed at the same time. Since there are numerous examples of natural and artificial signals that are sparse in the time, spatial or a transform domain, CS has found numerous applications. These include medical imaging, geophysical data analysis, computational biology, remote sensing and communications.\nIn the general CS framework, instead of acquiring N samples of a signal x \u2208 <N\u00d71, M random measurements are acquired where M < N . This is expressed by the underdetermined system of linear equations:\ny = \u03a6x (1)\nH. Palangi and R. Ward are with the Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, V6T 1Z4 Canada (e-mail: {hamidp,rababw}@ece.ubc.ca)\nL. Deng is with Microsoft Research, Redmond, WA 98052 USA (e-mail: {deng}@microsoft.com)\nwhere y \u2208 <M\u00d71 is the known measured vector and \u03a6 \u2208 <M\u00d7N is a random measurement matrix. An important assumption needed by the decoder to uniquely recover x given y and \u03a6, is that x is sparse in a given basis \u03a8. This means that\nx = \u03a8s (2)\nwhere s is K\u2212sparse, i.e., s has at most K non-zero elements. The basis \u03a8 can be complete; i.e., \u03a8 \u2208 <N\u00d7N , or over-complete; i.e., \u03a8 \u2208 <N\u00d7N1 where N < N1 (compressed sensing for over-complete dictionaries is introduced in [7]). Therefore from (1) and (2):\ny = As (3)\nwhere A = \u03a6\u03a8. Since there is just one measurement vector, the above problem is also called Single Measurement Vector (SMV) problem in compressive sensing.\nIn the distributed compressive sensing , also known as Multiple Measurement Vectors (MMV), a set of L sparse vectors {si}i=1,2,...,L is to be jointly recovered from a set of L measurement vectors {yi}i=1,2,...,L. Different application areas of MMV include magnetoencephalography, array processing, equalization of sparse communication channels and cognitive radio [8].\nSuppose that the L sparse vectors and measurement vectors are stacked as columns of matrices S = [s1, s2, . . . , sL] and Y = [y1,y2, . . . ,yL] respectively. In the MMV problem we want to reconstruct S given Y:\nY = AS (4)\nIn (4), S is assumed to be jointly sparse, i.e., non-zero entries of each vector occur at the same locations as that of others which means that sparse vectors have the same support. Assume that S is jointly sparse. Then, the necessary and sufficient condition to obtain a unique S given Y is [9]:\n|supp(S)| < spark(A)\u2212 1 + rank(S) 2\n(5)\nwhere |supp(S)| is the number of rows in S with nonzero energy and spark of a given matrix is the smallest possible number of linearly dependent columns of that matrix which gives a measure of linear dependency in the system modelled by a given matrix. In the SMV problem,\nar X\niv :1\n50 8.\n04 92\n4v 1\n[ cs\n.L G\n] 2\n0 A\nug 2\n01 5\n2 no rank information exists. In the MMV problem, rank information exists and affects the uniqueness bounds. Generally, on average, solving the MMV problem jointly can lead to better uniqueness guarantees than solving the SMV problem for each vector independently [10].\nIn the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.g., Y is observed and S should be sparse in basis \u03a8, create a posterior density function for the values of S. Depending on the requirements imposed by a specific application, each of above methods might be selected to find S from Y in (4)."}, {"heading": "A. Problem Statement", "text": "As mentioned above, many MMV reconstruction methods do not rely on training data. However, for many applications, a huge amount of data similar to the data to be compressed using CS is available. Examples are camera recordings of the same environment, images of the same class (e.g., flowers, buildings, ....), electroencephalogram (EEG) of different parts of the brain, etc. In this paper, we address the following questions in the MMV problem that arise when we have training data:\n1) Can we use a data driven bottom up approach to learn the structure of the sparse vectors in S using the already available data? And then use this structure to design a better algorithm for the MMV problem reconstructing the new data? 2) Most of the reconstruction algorithms for MMV rely on joint sparsity of S. However, in some practical applications, the sparse vectors in S are not exactly jointly sparse. This can be because of noise or sources that create different sparsity patterns. Examples are cameras capturing different scenes, images of different classes, etc. Although S is not jointly sparse, but there is a possible dependency among columns of S. In these situations, the common approach is to find norm of each row of S to get a sparse vector and then using SMV methods to reconstruct this vector. However, due to lack of joint sparsity, this method will not give satisfactory performance. The question is, can we design the aforementioned data driven method in a way that it captures the dependency among sparse vectors in S as well? A dependency that is not necessarily joint sparsity. And then use it in the reconstruction algorithm at the decoder?\nPlease note that we want to address above questions \u201cwithout adding any complexity or adaptability\u201d to the\nencoder. In other words, we do not want to design an optimal encoder, i.e., optimal sensing matrix \u03a6 or sparsifying basis \u03a8, for the given training data. The encoder is as simple and general as possible. This is specially important when we want to build sensors with low power consumption and limited battery life. However, decoder can be much more complex than encoder. For example, decoder can be a powerful data processing machine."}, {"heading": "B. Proposed Method", "text": "To address above questions, we propose to use a two step greedy reconstruction algorithm, 1) at each iteration of the reconstruction algorithm, and for each column of S represented as si, we find the conditional probability of each entry of si to be non-zero given residuals of all previous sparse vectors (columns) at that iteration. Then we pick up the most probable entry and add it to the support of si. Definition of residual matrix at j\u2212th iteration is Rj = Y \u2212ASj where Sj is the estimation of sparse matrix S at j\u2212th iteration. 2) In the first step we have found the location of non-zero entries. In the second step we want to find the values of them. This can be done by solving a least squares problem to find si given yi and A\u2126i . A\u2126i is a matrix that includes only those atoms (columns) of A that are a member of support of si.\nWe propose to use a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells and a softmax layer on the top of it to find these conditional probabilities at each iteration. To find the model parameters, we minimize a cross entropy cost function between the probability given by the model and the known probabilities in the training data. Details of how to generate training data and training data probabilities is explained in the subsequent sections. Please note that this training is done just once on the whole training data. After that, the resulting model is used in the reconstruction algorithm for any test data that has not been observed by the model before. Therefore, the proposed reconstruction algorithm is almost as fast as greedy methods. The block diagram of the proposed method is presented in Fig. 1 and Fig. 2. We will explain these figures in detail in subsequent sections.\nTo the best of our knowledge, this is the first model based method in MMV problem sparse reconstruction that is based on a deep learning bottom up model. Similar to all deep learning methods, it has the important feature of learning the structure of S from the raw data automatically. Although it is based on a greedy method that selects subsets that are not necessarily optimal, we experimentally show that with properly trained model and with just one layer of LSTM, it significantly outperforms well known MMV baselines (e.g., SOMP) as well as well known Bayesian methods for MMV problem\n\ud835\udc97(\ud835\udc61 \u2212 1)\n(e.g., Multitask Bayesian Compressive Sensing (MTBCS)[2] and Sparse Bayesian Learning for temporally correlated sources (T-SBL)[3]). We show this on two real world datasets.\nWe want to emphasize that the computations carried at the encoder are mainly multiplication by a random\nmatrix. The extra computations are added at the decoder. Therefore an important feature of compressive sensing (low power encoding) is preserved."}, {"heading": "C. Related Work", "text": "Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17]. In [14], it has theoretically been shown that using signal models that exploit these structures will result in a decrease in the number of measurements, i.e., M . In [8], a thorough review on the CS methods that use structure presented in the sparse signal or in the measurements is presented. In [15], a Bayesian framework for CS is presented. It uses the prior information about the sparsity of s to provide a posterior density function for the entries of s (assuming y is observed). It then uses a Relevance Vector Machine (RVM) [18] to estimate the entries of the sparse vector. The method is called Bayesian Compressive Sensing (BCS). In [2], a Bayesian framework is presented for the MMV problem. It assumes that the L \u201ctasks\u201d in the MMV problem in (4), are not statistically independent. By imposing a shared prior on the L tasks, an empirical method is presented to estimate hyperparameters and extensions of RVM are used for the inference step. The method is called Multitask Compressive Sensing (MTBCS). In [2], it is experimentally shown that the MT-\n4 BCS outperforms applying Orthogonal Matching Pursuit (OMP) on each task, Simultaneous Orthogonal Matching Pursuit (SOMP) which is straightforward extension of OMP for the MMV problem, and applying BCS for each task. In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem. It was shown that the global minimum of the proposed method is always the sparsest one. The authors in [3], address the MMV problem when the entries in each row of S are correlated. An algorithm based on SBL is proposed and it is shown that the proposed algorithm outperforms mixed norm (`1,2) optimization as well as the method proposed in [13]. The proposed method is called T-SBL. In [16], a greedy algorithm aided by a neural network is proposed to address the SMV problem in (3). The neural network parameters are calculated by solving a regression problem and used to select the appropriate column of A at each iteration of OMP. The main modification to OMP is replacing the correlation step with a neural network. They have experimentally shown that the proposed method outperforms OMP and `1 optimization. The method is called Neural Network OMP (NNOMP). In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem. The joint sparsity of S is an important assumption in the proposed method. It has been experimentally shown that it outperforms SOMP and `1,2 in the MMV problem. The proposed methods are called Nonlinear Weighted SOMP (NWSOMP) for one layer model and DSN-WSOMP for multilayer model.\nThe rest of the paper is organized as follows: In section II, the basics of Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) cells is briefly explained. The proposed method and the learning algorithm are presented in section III. Experimental results on two real world datasets are presented in section IV. Conclusions and future work directions are presented in section V. Details of the final gradient expressions for the learning section of the proposed method are presented in Appendix A."}, {"heading": "II. RNN WITH LSTM CELLS", "text": "The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31]. If we look at the sparse vectors (columns) in S as a sequence, the main idea of using RNN for the MMV problem is to predict the sparsity patterns over different sparse vectors in S.\nAlthough RNN performs the sequence modelling in a principled manner, it is generally difficult to learn the long term dependency within the sequence due to vanishing gradients problem. One of the effective solutions for this problem in RNNs is using memory cells instead\nof neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.\nWe use the architecture of LSTM illustrated in Fig. 2 for the proposed sequence modelling method for the MMV problem. In this figure, i(t), f(t) ,o(t) , c(t) are input gate, forget gate, output gate and cell state vector respectively, Wp1, Wp2 and Wp3 are peephole connections, Wi, Wreci and bi, i = 1, 2, 3, 4 are input connections, recurrent connections and bias values, respectively, g(\u00b7) and h(\u00b7) are tanh(\u00b7) function and \u03c3(\u00b7) is the sigmoid function. We use this architecture to find v for each channel and then use the proposed method in Fig. 1 to find the entries that are more probable to be non-zero. Considering Fig. 2, the forward pass for LSTM model is as follows:\nyg(t) = g(W4r(t) + Wrec4v(t\u2212 1) + b4) i(t) = \u03c3(W3r(t) + Wrec3v(t\u2212 1) + Wp3c(t\u2212 1) + b3) f(t) = \u03c3(W2r(t) + Wrec2v(t\u2212 1) + Wp2c(t\u2212 1) + b2) c(t) = f(t) \u25e6 c(t\u2212 1) + i(t) \u25e6 yg(t) o(t) = \u03c3(W1r(t) + Wrec1v(t\u2212 1) + Wp1c(t) + b1) v(t) = o(t) \u25e6 h(c(t)) (6)\nwhere \u25e6 denotes Hadamard (element-wise) product."}, {"heading": "III. PROPOSED METHOD", "text": ""}, {"heading": "A. High Level Picture", "text": "The summary of the proposed method is presented in Fig. 1. We initialize the residual vector, r, for each channel by the measurement vector, y, of that channel. These residual vectors serve as the input to the LSTM model which captures their features using input weight matrices (W1,W2,W3,W4) as well as their dependency using recurrent weight matrices (Wrec1,Wrec2,Wrec3,Wrec4) and central memory unit shown in Fig. 2. A transformation matrix U is then used to transform output of each memory cell after gating, i.e., v \u2208 <ncell\u00d71, into the sparse vectors space, i.e., s \u2208 <N\u00d71. Then a softmax layer is used for each channel to find the probability values for each entry of each sparse vector to be nonzero. For example, for channel 1, the j-th output of the softmax layer is:\nP (s1(j)|r1) = ez(j)\u2211n k=1 e z(k) (7)\nThen for each channel, the entry with maximum probability value is selected and added to the support set of the channel. After that, given the new support set, the following least squares problem is solved to find an estimation of the sparse vector for the j-th channel:\ns\u0302j = argmin sj\n\u2016yj \u2212A\u2126jsj\u201622 (8)\n5 Using s\u0302j , the new residual value for the j-th channel is calculated as follows:\nrj = yj \u2212A\u2126j s\u0302j (9)\nThis residual serves as the input to the LSTM model at the next iteration of the algorithm. The stopping criteria for the algorithm is when the residual values are small enough or when it has performed N iterations where N is the dimension of the sparse vector. Since we have used LSTM cells for the proposed method, we call it LSTMCS algorithm. The pseudo-code of the proposed method is presented in Algorithm 1.\nAlgorithm 1 Distributed Compressive Sensing using Long Short-Term Memory (LSTM-CS) Inputs: CS measurement matrix A \u2208 <M\u00d7N ; matrix of measurements Y \u2208 <M\u00d7L; minimum `2 norm of residual matrix \u201cresMin\u201d as stopping criterion; Trained \u201clstm\u201d model Output: Matrix of sparse vectors S\u0302 \u2208 <N\u00d7L Initialization: S\u0302 = 0; j = 1; i = 1; \u2126 = \u2205; R = Y. 1: procedure LSTM-CS(A,Y, lstm) 2: while i \u2264 N and \u2016R\u20162 \u2264 resMin do 3: i\u2190 i+ 1 4: for j = 1\u2192 L do 5: R(:, j)i \u2190\nR(:,j)i\u22121 max(|R(:,j)i\u22121|)\n6: vj \u2190 lstm(R(:, j)i,vj\u22121, cj\u22121) . LSTM 7: zj \u2190 Uvj 8: c\u2190 softmax(zj) 9: idx\u2190 Support(max(c))\n10: \u2126i \u2190 \u2126i\u22121 \u222a idx 11: S\u0302\u2126i (:, j)\u2190 (A\u2126i )\u2020Y(:, j) . Least Squares 12: S\u0302\u2126 C i (:, j)\u2190 0 13: R(:, j)i \u2190 Y(:, j)\u2212A\u2126i S\u0302\u2126i (:, j) 14: end for 15: end while 16: end procedure\nWe continue by explaining how the training data is prepared from off-line dataset and then we present the details of the learning method. Please note that all the computations explained in the subsequent two sections are performed just once and does not affect the run time of the proposed solver in Fig. 1. It is almost as fast as greedy algorithms for sparse reconstruction."}, {"heading": "B. Training Data Generation", "text": "The main viewpoint of the proposed method is to look at the sparse reconstruction problem as a two step task: a classification at the first step and a subsequent least squares at the second step. In classification step, we want to find the atom of the dictionary, i.e., the column of A, that is most relevant to the given residual of the current channel and the residuals of the previous channels in the MMV problem. Therefore we need a set of residual vectors and corresponding sparse vectors for supervised training. Since the off-line data and A are given, we can imitate steps explained in the previous section to generate residuals. This means that, given a sparse vector s with k non-zero entries, we calculate y using (3). Then\nwe find the entry with maximum value in s and set it to zero. Assume that the index of this entry is k0. This gives us the new sparse vector snew with k\u22121 non-zero entries. Now we calculate residual vector from:\nr = y \u2212Asnew (10)\nIt is obvious that this residual value is only because of removing k0-th entry of s. Therefore, this residual corresponds to the k0-th atom of the dictionary. Output of this procedure is the training pair (r, s0) where s0 is a one hot vector that has value 1 at k0 entry and zero at other entries.\nAbove procedure is continued upto the point that s does not have any non-zero entry. Then the same procedure is used for the next training sample. Since we do not know the number of non-zero entries, k, in advance, we assume a maximum number of non-zero entries per channel during experiments."}, {"heading": "C. Learning Method", "text": "To calculate the proposed model parameters, i.e., matrices in Fig. 2 and U in Fig.1, we minimize a cross entropy cost function over the off-line training data. Assuming s is the output vector of softmax given by the model in Fig. 1 (output of the softmax layer is represented as conditional probabilities in Fig. 1) and s0 is the one hot vector explained in the previous section, the following optimization problem is solved:\nL(\u039b) = min \u039b  nB\u2211 i=1 Bsize\u2211 r=1 L\u2211 \u03c4=1 N\u2211 j=1 Lr,i,\u03c4,j(\u039b)  Lr,i,\u03c4,j(\u039b) = \u2212s0,r,i,\u03c4 (j)log(sr,i,\u03c4 (j)) (11)\nwhere nB is the number of mini-batches in the training data, Bsize is the number of training data pairs, (r, s0), in each mini-batch, L is the number of channels in the MMV problem, i.e., number of columns of S, and N is the length of vector s and s0. \u039b denotes the collection of the model parameters that includes W1, W2, W3, W4, Wrec1, Wrec2, Wrec3, Wrec4, Wp1, Wp2, Wp3, b1, b2, b3 and b4 in Fig. 2 and U in Fig. 1.\nTo solve optimization problem in (11), we use Backpropagation through time (BPTT) with Nesterov method. The update equations for parameter \u039b at epoch k are as follows:\n4\u039bk = \u039bk \u2212\u039bk\u22121 4\u039bk = \u00b5k\u221214\u039bk\u22121 \u2212 k\u22121\u2207L(\u039bk\u22121 + \u00b5k\u221214\u039bk\u22121)\n(12)\nwhere \u2207L(\u00b7) is the gradient of the cost function in (11), is the learning rate and \u00b5k is a momentum parameter determined by the scheduling scheme used for training. Above equations are equivalent to Nesterov\n6 method in [34]. To see why, please refer to appendix A.1 of [35] where Nesterov method is derived as a momentum method. The gradient of the cost function, \u2207L(\u039b), is:\n\u2207L(\u039b) = nB\u2211 i=1 Bsize\u2211 r=1 L\u2211 \u03c4=1 N\u2211 j=1 \u2202Lr,i,\u03c4,j(\u039b)\n\u2202\u039b\ufe38 \ufe37\ufe37 \ufe38 one large update\n(13)\nAs it is obvious from (13), since we have unfolded the LSTM over channels in S, we fold it back when we want to calculate gradients over the whole sequence of channels.\n\u2202Lr,i,\u03c4,j(\u039b) \u2202\u039b in (13) and error signals for different parameters of the proposed model that are necessary for training are presented in Appendix A. Due to lack of space, we omit the presentation of full derivation of the gradients.\nWe have used mini-batch training to accelerate training and one large update instead of incremental updates during back propagation through time. To resolve the gradient explosion problem we have used gradient clipping. To accelerate the convergence, we have used Nesterov method [34] and found it effective in training the proposed model for the MMV problem.\nWe have used a simple yet effective scheduling for \u00b5k in (12), in the first and last 10% of all parameter updates \u00b5k = 0.9 and for the other 80% of all parameter updates \u00b5k = 0.995. We have used a fixed step size for training LSTM. Please note that since we are using mini-batch training, all parameters are updated for each mini-batch in (13).\nA summary of training method for LSTM-CS is presented in Algorithm 2."}, {"heading": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "text": "This section presents the results of the different reconstruction algorithms for the MMV problem including the proposed method in this work. We have performed experiments on two real world datasets, MNIST dataset of handwritten digits [36] and three different classes of images from natural image dataset of Microsoft Research in Cambridge [37].\nIn this section, we would like to answer the following questions: (i) How is the performance of different reconstruction algorithms for the MMV problem, including the proposed method, when different channels, i.e., different columns in S, have different sparsity patterns? (ii) Does the proposed method perform well enough when there is correlation among different sparse vectors? E.g., when sparse vectors are DCT or Wavelet transform of different blocks of an image? (iii) How fast is the proposed method compared to other reconstruction algorithms for the MMV problem?\nAlgorithm 2 Training the proposed model for Distributed Compressive Sensing\nInputs: Fixed step size \u201c \u201d, Scheduling for \u201c\u00b5\u201d, Gradient clip threshold \u201cthG\u201d, Maximum number of Epochs \u201cnEpoch\u201d, Total number of training pairs in each mini-batch \u201cBsize\u201d, Number of channels for the MMV problem \u201cL\u201d. Outputs: LSTM-CS trained model for distributed compressive sensing \u201c\u039b\u201d. Initialization: Set all parameters in \u039b to small random numbers, i = 0, k = 1. procedure LSTM-CS(\u039b)\nwhile i \u2264 nEpoch do for \u201cfirst minibatch\u201d \u2192 \u201clast minibatch\u201d do r \u2190 1 while r \u2264 Bsize do\nCompute \u2211L \u03c4=1 \u2202Lr,\u03c4 \u2202\u039bk . use (19) to (50) in appendix A r \u2190 r + 1\nend while Compute \u2207L(\u039bk)\u2190 \u201csum above terms over r\u201d if \u2207L(\u039bk) > thG then \u2207L(\u039bk)\u2190 thG . For each entry of the gradient matrix \u2207L(\u039bk) end if Compute 4\u039bk . use (12) Update: \u039bk \u2190 4\u039bk + \u039bk\u22121 k \u2190 k + 1\nend for i\u2190 i+ 1\nend while end procedure\nFor all the results presented in this section, the reconstruction error is defined as:\nMSE = \u2016S\u0302\u2212 S\u2016 \u2016S\u2016\n(14)\nwhere S is the actual sparse matrix and S\u0302 is the recovered sparse matrix from random measurements by the reconstruction algorithm. The machine used to perform the experiments has an Intel(R) Core(TM) i7 CPU with clock 2.93 GHz and with 16 GB RAM. The MATLAB codes for the proposed LSTM-CS method are available online at \u201c*I will upload the codes and put the link here!* \u201d."}, {"heading": "A. MNIST Dataset", "text": "MNIST is a dataset of handwritten digits where the images of the digits are normalized in size and centred to have fixed size images. The task is to encode 4 images of size 24\u00d724 at the same time, i.e., we have 4 channels and L = 4 in (4). The encoder is typical compressive sensing encoder, a randomly generated matrix A. We have renormalized each column of A to have unit norm. Since the images are already sparse, i.e., have a few number of non-zero pixels, no transform, \u03a8 in (2), is used. To simulate measurement noise, we have added a Gaussian noise with standard deviation 0.005 to the measurement matrix Y in (4). This results in measurements with signal to noise ratio (SNR) of 40dB. We have divided each image into four 12 \u00d7 12 blocks. This means that the length of each sparse vector is N = 144. We have taken 50% random measurements from each sparse vector, i.e.,\n7\nM = 72. After receiving all blocks in the decoder, we compute the reconstruction error defined in (14) for the full image. We have randomly selected 10 images for each digit from the set {0, 1, 2, 3}, i.e., 40 images in total for the test. This means that first column of S is image of digit 0, the second column for digit 1, the third column for digit 2 and the fourth column for digit 3. Test images are represented in Fig. 3.\nWe have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.\nFor BCS we have set the initial noise variance of i-th channel to the suggested value by the authors, std(yi)\n2/100 where i \u2208 {1, 2, 3, 4} and std(.) calculates the standard deviation. We have set the threshold for stopping the algorithm to 10\u22128. For MT-BCS we have set parameters of Gamma prior on noise variance to a = 100/0.1 and b = 1 which are the suggested values by the authors. We have set the stopping threshold to 10\u22128 as well. For T-SBL we have used the default values proposed by the authors. For NWSOMP, during training, we have used one layer, 512 neurons and 25 epochs of parameters update. For LSTM-CS, during training, we have used one layer, 512 cells and 25 epochs of parameter updates. We have used just 200 images for training set. The training set does not include any of 40 images used for test. To monitor overfitting and prevent\nit, we have used 3 images per channel as validation set and early stopping if necessary. Please note that images used for validation are not used neither in training set nor in test set. Results are presented in Fig. 4.\nIn Fig. 4, vertical axis is MSE defined in (14) and horizontal axis is the number of non-zero entries in the sparse vector. Number of measurements, M , is fixed to 72. Each point on the curves of the figure is the average of MSE over 40 reconstructed test images at the decoder.\nWe observe from Fig. 4 that LSTM-CS significantly outperforms the reconstruction algorithms for the MMV problem discussed in this paper for the MNIST dataset. One important reason for this is that many existing MMV solvers rely on joint sparsity in S, while the proposed method does not rely on this assumption. The reconstructed images using different MMV reconstruction algorithms for 4 test images are presented in Fig. 5. An interesting observation from Fig. 5 is that the accuracy of reconstruction depends on the complexity of sparsity pattern. For example when the sparsity pattern is simple, e.g., image of digit 1 in Fig. 5, all the algorithms perform well. But when the sparsity pattern is more complex, e.g., image of digit 0 in Fig. 5, their reconstruction accuracy degrades significantly.\nWe have repeated the experiments on the MNIST dataset with 25% random measurements, i.e., M = 36. The results are presented in Fig. 6."}, {"heading": "B. Natural Images Dataset", "text": "For experiments on natural images we have used the MSR Cambridge dataset [37]. 10 randomly selected test\nimages of three classes of this dataset are used for experiments. The images are shown in Fig. 7. We have used 64\u00d7 64 images. Each image is divided into 8\u00d7 8 blocks. After reconstructing all blocks of an image in the decoder, the MSE for the reconstructed image is calculated. The task is to encode 4 blocks (L = 4) of an image and reconstruct them in the decoder. This means that S in (4) has 4 columns each one having N = 64 entries. We have used 50% measurements, i.e., Y in (4) have 4 columns each one having M = 32 entries.\nWe have compared the performance of the proposed algorithm, LSTM-CS, with SOMP, T-SBL, MT-BCS and NWSOMP. We have not included results of applying BCS per channel due its weak performance compared to other methods (this is shown in the experiments for MNIST dataset). We have used the same setting as the settings for the MNIST dataset for different methods which is explained in the previous section. The only differences here are: (i) For each class of images, we have used just 55 images for training set and 5 images for validation set which do not include any of 10 images used for test. (ii) We have used 15 epochs for training LSTM-CS which is enough for this dataset, compared to 25 epochs for the MNIST dataset. We have performed experiments for two popular transforms, DCT and Wavelet, for all aforementioned reconstruction algorithms. For the wavelet transform we have used Haar wavelet transform with 3 levels of decomposition. Results for DCT transform are presented in Fig. 8.\nResults for wavelet transform are presented in Fig. 9.\nTo conclude the experiments section, the CPU time for different reconstruction algorithms for the MMV problem discussed in this paper are presented in Fig. 10. Each point on the curves in Fig. 10 is the time spent to reconstruct each sparse vector averaged over all the 8 \u00d7 8 blocks in 10 test images. We observed from this figure that the proposed algorithm is almost as fast as greedy algorithms."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "In this paper we presented a method to reconstruct sparse vectors for the MMV problem. We showed that the proposed method does not rely on common joint sparsity assumption. Through experiments on two real world datasets, we showed that the proposed method outperforms general MMV baseline SOMP as well as Bayesian model based methods MT-BCS and T-SBL for the MMV problem. Please note that we have not used multiple layers of LSTM or advanced deep learning methods for training, e.g., regularization using drop out, etc which will improve the performance of LSTM-CS. This paper is a proof of concept that deep learning methods and specifically sequence modelling methods, e.g., LSTM, can improve the performance of the MMV solvers significantly. This is the case specially when the sparsity patterns are more complicated than simple DCT or Wavelet transform. We showed this on the MNIST dataset. Our future work includes extending the LSTMCS to bidirectional LSTM-CS and using the proposed method for compressive sensing of video where there is correlation among frames, and compressive sensing of EEG signals where there is significant correlation among different EEG channels."}, {"heading": "VI. ACKNOWLEDGEMENT", "text": "We want to thank the authors of [3] and [15] and [2] for making the code of their work available. It helped us a lot to perform comparisons."}, {"heading": "APPENDIX A EXPRESSIONS FOR THE GRADIENTS", "text": "In this appendix we present the final gradient expressions that are necessary to use for training the proposed model for the MMV problem. Due to lack of space, we omit the presentation of full derivations of these gradients.\nStarting with the cost function in (11), we use the Nesterov method described in (12) to update LSTM-CS model parameters. Here, \u039b is one of the weight matrices or bias vectors {W1,W2,W3,W4,Wrec1,Wrec2,Wrec3,Wrec4 ,Wp1,Wp2,Wp3,b1,b2,b3,b4} in the LSTM-CS architecture. The general format of the gradient of the cost function, \u2207L(\u039b), is the same as (13). To calculate \u2202Lr,i,\u03c4 (\u039b)\n\u2202\u039b from (11) we have:\n\u2202Lr,i,\u03c4 (\u039b)\n\u2202\u039b = \u2212 N\u2211 j=1 s0,r,i,\u03c4 (j) \u2202log(sr,i,\u03c4 (j)) \u2202\u039b (15)\nAfter a straightforward derivation of derivatives we will have:\n\u2202Lr,i,\u03c4 (\u039b)\n\u2202\u039b = (\u03b2sr,i,\u03c4 \u2212 s0,r,i,\u03c4 ) \u2202z\u03c4 \u2202\u039b\n(16)\nwhere z\u03c4 is the vector z for \u03c4 -th channel in Fig. 1 and \u03b2 is a scalar defined as:\n\u03b2 = N\u2211 j=1 s0,r,i,\u03c4 (j) (17)\nSince during training data generation we have generated one hot vectors for s0, \u03b2 always equals to 1. Since we are looking at different channels as a sequence, for a more\n10\nclear presentation we show any vector corresponding to t-th channel with (t) instead of index \u03c4 . For example, z\u03c4 is represented by z(t).\nSince z(t) = Uv(t) we have:\n\u2202z(t)\n\u2202\u039b = UT\n\u2202v(t)\n\u2202\u039b (18)\nCombining (16), (17) and (18) we will have:\n\u2202Lr,i,t(\u039b)\n\u2202\u039b = UT (sr,i(t)\u2212 s0,r,i(t))\n\u2202v(t)\n\u2202\u039b (19)\nStarting from \u201ct = L\u201d-th channel, we define e(t) as:\ne(t) = UT (sr,i(t)\u2212 s0,r,i(t)) (20)\nThe expressions for the gradients for different parameters of LSTM-CS model are presented in the subsequent sections. We omit the subscripts r and i for simplicity of presentation. Please note that the final value of the\ngradient is sum of gradient values over the mini-batch samples and number of channels as represented by summations in (13).\nA. Output Weights U\n\u2202Lt \u2202U = (s(t)\u2212 s0(t)).v(t)T (21)"}, {"heading": "B. Output Gate", "text": "For recurrent connections we have:\n\u2202Lt \u2202Wrec1 = \u03b4rec1(t).v(t\u2212 1)T (22)\nwhere\n\u03b4rec1(t) = o(t) \u25e6 (1\u2212 o(t)) \u25e6 h(c(t)) \u25e6 e(t) (23)\n11\nFor input connections, W1, and peephole connections, Wp1, we will have:\n\u2202Lt \u2202W1 = \u03b4rec1(t).r(t)T (24)\n\u2202Lt \u2202Wp1 = \u03b4rec1(t).c(t)T (25)\nThe derivative for output gate bias values will be:\n\u2202Lt \u2202b1 = \u03b4rec1(t) (26)\nC. Input Gate\nFor the recurrent connections we have:\n\u2202Lt \u2202Wrec3 = diag(\u03b4rec3(t)). \u2202c(t) \u2202Wrec3 (27)\nwhere\n\u03b4rec3(t) = (1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) \u25e6 o(t) \u25e6 e(t) \u2202c(t) \u2202Wrec3 = diag(f(t)). \u2202c(t\u2212 1) \u2202Wrec3 + bi(t).v(t\u2212 1)T bi(t) = yg(t) \u25e6 i(t) \u25e6 (1\u2212 i(t)) (28)\nFor the input connections we will have the following:\n\u2202Lt \u2202W3 = diag(\u03b4rec3(t)). \u2202c(t) \u2202W3 (29)\nwhere \u2202c(t)\n\u2202W3 = diag(f(t)). \u2202c(t\u2212 1) \u2202W3 + bi(t).r(t) T (30)\nFor the peephole connections we will have:\n\u2202Lt \u2202Wp3 = diag(\u03b4rec3y (t)). \u2202c(t) \u2202Wp3 (31)\nwhere \u2202c(t)\n\u2202Wp3 = diag(f(t)). \u2202c(t\u2212 1) \u2202Wp3 +bi(t).c(t\u22121)T (32)\nFor bias values, b3, we will have:\n\u2202Lt \u2202b3 = diag(\u03b4rec3(t)). \u2202c(t) \u2202b3 (33)\nwhere \u2202c(t)\n\u2202b3 = diag(f(t)). \u2202c(t\u2212 1) \u2202b3 + bi(t) (34)"}, {"heading": "D. Forget Gate", "text": "For the recurrent connections we will have: \u2202Lt\n\u2202Wrec2 = diag(\u03b4rec2(t)).\n\u2202c(t)\n\u2202Wrec2 (35)\nwhere\n\u03b4rec2(t) = (1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) \u25e6 o(t) \u25e6 e(t) \u2202c(t) \u2202Wrec2 = diag(f(t)). \u2202c(t\u2212 1) \u2202Wrec2 + bf (t).v(t\u2212 1)T bf (t) = c(t\u2212 1) \u25e6 f(t) \u25e6 (1\u2212 f(t)) (36)\nFor input connections to forget gate we will have:\n\u2202Lt \u2202W2 = diag(\u03b4rec2(t)). \u2202c(t) \u2202W2 (37)\nwhere \u2202c(t)\n\u2202W2 = diag(f(t)). \u2202c(t\u2212 1) \u2202W2 + bf (t).r(t) T (38)\nFor peephole connections we have:\n\u2202Lt \u2202Wp2 = diag(\u03b4rec2(t)). \u2202c(t) \u2202Wp2 (39)\nwhere \u2202c(t)\n\u2202Wp2 = diag(f(t)). \u2202c(t\u2212 1) \u2202Wp2 +bf (t).c(t\u22121)T (40)\nFor forget gate\u2019s bias values we will have:\n\u2202Lt \u2202b2 = diag(\u03b4rec2(t)). \u2202c(t) \u2202b2 (41)\nwhere \u2202c(t)\n\u2202b2 = diag(f(t)). \u2202c(t\u2212 1) \u2202b3 + bf (t) (42)\n12\nE. Input without Gating (yg(t))\nFor recurrent connections we will have:\n\u2202Lt \u2202Wrec4 = diag(\u03b4rec4(t)). \u2202c(t) \u2202Wrec4 (43)\nwhere\n\u03b4rec4(t) = (1\u2212 h(c(t))) \u25e6 (1 + h(c(t))) \u25e6 o(t) \u25e6 e(t) \u2202c(t) \u2202Wrec4 = diag(f(t)). \u2202c(t\u2212 1) \u2202Wrec4 + bg(t).v(t\u2212 1)T bg(t) = i(t) \u25e6 (1\u2212 yg(t)) \u25e6 (1 + yg(t)) (44)\nFor input connections we have:\n\u2202Lt \u2202W4 = diag(\u03b4rec4(t)). \u2202c(t) \u2202W4 (45)\nwhere\n\u2202c(t) \u2202W4 = diag(f(t)). \u2202c(t\u2212 1) \u2202W4 + bg(t).r(t) T (46)\nFor bias values we will have:\n\u2202Lt \u2202b4 = diag(\u03b4rec4(t)). \u2202c(t) \u2202b4 (47)\nwhere\n\u2202c(t)\n\u2202b4 = diag(f(t)). \u2202c(t\u2212 1) \u2202b4 + bg(t) (48)"}, {"heading": "F. Error signal backpropagation", "text": "Error signals are back propagated through time using following equations:\n\u03b4rec1(t\u2212 1) = [o(t\u2212 1) \u25e6 (1\u2212 o(t\u2212 1)) \u25e6 h(c(t\u2212 1))] \u25e6 [WTrec1.\u03b4rec1(t) + e(t\u2212 1)] (49)\n\u03b4reci(t\u2212 1) = [(1\u2212 h(c(t\u2212 1))) \u25e6 (1 + h(c(t\u2212 1))) \u25e6 o(t\u2212 1)] \u25e6 [WTreci .\u03b4\nreci(t) + e(t\u2212 1)], for i \u2208 {2, 3, 4} (50)"}], "references": [{"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Multitask compressive sensing", "author": ["S. Ji", "D. Dunson", "L. Carin"], "venue": "Signal Processing, IEEE Transactions on, vol. 57, no. 1, pp. 92\u2013 106, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse signal recovery with temporally correlated source vectors using sparse bayesian learning", "author": ["Z. Zhang", "B.D. Rao"], "venue": "Selected Topics in Signal Processing, IEEE Journal of, vol. 5, no. 5, pp. 912\u2013926, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289 \u20131306, april 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Candes", "J. Romberg", "T. Tao"], "venue": "Communications on Pure and Applied Mathematics, vol. 59, no. 8, pp. 1207\u20131223, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sensing [lecture notes", "author": ["R. Baraniuk"], "venue": "IEEE Signal Processing Magazine, vol. 24, no. 4, pp. 118 \u2013121, july 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Compressed sensing with coherent and redundant dictionaries", "author": ["E.J. Cands", "Y.C. Eldar", "D. Needell", "P. Randall"], "venue": "Applied and Computational Harmonic Analysis, vol. 31, no. 1, pp. 59 \u2013 73, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Structured compressed sensing: From theory to applications", "author": ["M. Duarte", "Y. Eldar"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 9, pp. 4053 \u20134085, sept. 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Rank awareness in joint sparse recovery", "author": ["M. Davies", "Y. Eldar"], "venue": "IEEE Transactions on Information Theory, vol. 58, no. 2, pp. 1135 \u20131146, Feb. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Average case analysis of multichannel sparse recovery using convex relaxation", "author": ["Y. Eldar", "H. Rauhut"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 1, pp. 505 \u2013519, Jan. 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for simultaneous sparse approximation. part I: Greedy pursuit", "author": ["J. Tropp", "A. Gilbert", "M. Strauss"], "venue": "Signal Processing, vol. 86, no. 3, pp. 572 \u2013 588, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for simultaneous sparse approximation. part II: Convex relaxation", "author": ["J. Tropp"], "venue": "Signal Processing, vol. 86, no. 3, pp. 589 \u2013 602, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "An empirical bayesian strategy for solving the simultaneous sparse approximation problem", "author": ["D.P. Wipf", "B.D. Rao"], "venue": "Signal Processing, IEEE Transactions on, vol. 55, no. 7, pp. 3704\u20133716, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Model-based compressive sensing", "author": ["R. Baraniuk", "V. Cevher", "M. Duarte", "C. Hegde"], "venue": "Information Theory, IEEE Transactions on, vol. 56, no. 4, pp. 1982\u20132001, April 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Bayesian compressive sensing", "author": ["S. Ji", "Y. Xue", "L. Carin"], "venue": "Signal Processing, IEEE Transactions on, vol. 56, no. 6, pp. 2346\u20132356, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Embedding prior knowledge within compressed sensing by neural networks", "author": ["D. Merhej", "C. Diab", "M. Khalil", "R. Prost"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 10, pp. 1638 \u2013 1649, oct. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Using deep stacking network to improve structured compressed sensing with multiple measurement vectors", "author": ["H. Palangi", "R. Ward", "L. Deng"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "J. Mach. Learn. Res., vol. 1, pp. 211\u2013244, Sep. 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Analysis of sparse bayesian learning", "author": ["A.C. Faul", "M.E. Tipping"], "venue": "Advances in Neural Information Processing Systems (NIPS) 14. MIT Press, 2001, pp. 383\u2013389.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["L. Deng", "D. Yu", "J. Platt"], "venue": "Proc. ICASSP, march 2012, pp. 2133 \u20132136.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30 \u201342, jan. 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["A.J. Robinson"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 298\u2013305, August 1994.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Analysis of the correlation structure for a neural predictive model with application to speech recognition", "author": ["L. Deng", "K. Hassanein", "M. Elmasry"], "venue": "Neural Networks, vol. 7, no. 2, pp. 331\u2013339, 1994.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in Proc. INTERSPEECH, Makuhari,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "Representation Learning Workshp, ICML, 2012.  13", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "Proc. ICASSP, Vancouver, Canada, May 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "Proc. INTERSPEECH, Lyon, France, August 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent deep-stacking networks for sequence classification", "author": ["H. Palangi", "L. Deng", "R. Ward"], "venue": "Signal and Information Processing (ChinaSIP), 2014 IEEE China Summit International Conference on, July 2014, pp. 510\u2013514.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning input and recurrent weight matrices in echo state networks", "author": ["H. Palangi", "L. Deng", "R.K. Ward"], "venue": "NIPS Workshop on Deep Learning, December 2013. [Online]. Available: http://research.microsoft.com/apps/pubs/default.aspx? id=204701", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, pp. 2451\u20132471, 1999.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 115\u2013143, Mar. 2003.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, vol. 27, pp. 372\u2013376, 1983.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1983}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "ICML (3)\u201913, 2013, pp. 1139\u20131147.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov 1998. [Online]. Available: http://yann.lecun.com/exdb/mnist/", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "To compute these probabilities, we propose to use Long Short-Term Memory (LSTM) [1], a bottom up data driven model for sequence modelling.", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3].", "startOffset": 263, "endOffset": 266}, {"referenceID": 2, "context": "By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3].", "startOffset": 330, "endOffset": 333}, {"referenceID": 3, "context": "COMPRESSIVE Sensing (CS) [4],[5],[6] is an effective approach for acquiring sparse signals by which both sensing and compression are performed at the same time.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "COMPRESSIVE Sensing (CS) [4],[5],[6] is an effective approach for acquiring sparse signals by which both sensing and compression are performed at the same time.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "COMPRESSIVE Sensing (CS) [4],[5],[6] is an effective approach for acquiring sparse signals by which both sensing and compression are performed at the same time.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": ", \u03a8 \u2208 <N\u00d7N1 where N < N1 (compressed sensing for over-complete dictionaries is introduced in [7]).", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Different application areas of MMV include magnetoencephalography, array processing, equalization of sparse communication channels and cognitive radio [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "Then, the necessary and sufficient condition to obtain a unique S given Y is [9]:", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "Generally, on average, solving the MMV problem jointly can lead to better uniqueness guarantees than solving the SMV problem for each vector independently [10].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 275, "endOffset": 279}, {"referenceID": 12, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 309, "endOffset": 313}, {"referenceID": 1, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 315, "endOffset": 318}, {"referenceID": 2, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 320, "endOffset": 323}, {"referenceID": 1, "context": ", Multitask Bayesian Compressive Sensing (MTBCS)[2] and Sparse Bayesian Learning for temporally correlated sources (T-SBL)[3]).", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": ", Multitask Bayesian Compressive Sensing (MTBCS)[2] and Sparse Bayesian Learning for temporally correlated sources (T-SBL)[3]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 129, "endOffset": 132}, {"referenceID": 14, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 1, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 151, "endOffset": 154}, {"referenceID": 15, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 16, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "In [14], it has theoretically been shown that using signal models that exploit these structures will result in a decrease in the number of measurements, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8], a thorough review on the CS methods that use structure presented in the sparse signal or in the measurements is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [15], a Bayesian framework for CS is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "It then uses a Relevance Vector Machine (RVM) [18] to estimate the entries of the sparse vector.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "In [2], a Bayesian framework is presented for the MMV problem.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2], it is experimentally shown that the MT-", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "The authors in [3], address the MMV problem when the entries in each row of S are correlated.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "An algorithm based on SBL is proposed and it is shown that the proposed algorithm outperforms mixed norm (`1,2) optimization as well as the method proposed in [13].", "startOffset": 159, "endOffset": 163}, {"referenceID": 15, "context": "In [16], a greedy algorithm aided by a neural network is proposed to address the SMV problem in (3).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 177, "endOffset": 181}, {"referenceID": 24, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 183, "endOffset": 187}, {"referenceID": 25, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 189, "endOffset": 193}, {"referenceID": 26, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 201, "endOffset": 205}, {"referenceID": 28, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 207, "endOffset": 211}, {"referenceID": 29, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 219, "endOffset": 223}, {"referenceID": 0, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.", "startOffset": 120, "endOffset": 123}, {"referenceID": 31, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.", "startOffset": 173, "endOffset": 177}, {"referenceID": 32, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.", "startOffset": 182, "endOffset": 186}, {"referenceID": 33, "context": "method in [34].", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "1 of [35] where Nesterov method is derived as a momentum method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 33, "context": "To accelerate the convergence, we have used Nesterov method [34] and found it effective in training the proposed model for the MMV problem.", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "We have performed experiments on two real world datasets, MNIST dataset of handwritten digits [36] and three different classes of images from natural image dataset of Microsoft Research in Cambridge [37].", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 272, "endOffset": 276}, {"referenceID": 1, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 356, "endOffset": 359}, {"referenceID": 2, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 502, "endOffset": 505}, {"referenceID": 16, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 613, "endOffset": 617}, {"referenceID": 2, "context": "ACKNOWLEDGEMENT We want to thank the authors of [3] and [15] and [2] for making the code of their work available.", "startOffset": 48, "endOffset": 51}, {"referenceID": 14, "context": "ACKNOWLEDGEMENT We want to thank the authors of [3] and [15] and [2] for making the code of their work available.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "ACKNOWLEDGEMENT We want to thank the authors of [3] and [15] and [2] for making the code of their work available.", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "We address the problem of compressed sensing with Multiple Measurement Vectors (MMVs) when the structure of sparse vectors in different channels depend on each other. The sparse vectors are not necessarily joint sparse. We capture this dependency by computing the conditional probability of each entry of each sparse vector to be non-zero given \u201cresiduals\u201d of all previous sparse vectors. To compute these probabilities, we propose to use Long Short-Term Memory (LSTM) [1], a bottom up data driven model for sequence modelling. To compute model parameters we minimize a cross entropy cost function. We propose a greedy solver that uses above probabilities at the decoder. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3]. Nevertheless, we emphasize that the proposed method is a data driven method where availability of training data is important. However, in many applications, train data is indeed available, e.g., recorded images or video.", "creator": "LaTeX with hyperref package"}}}