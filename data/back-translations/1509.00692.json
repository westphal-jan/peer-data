{"id": "1509.00692", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2015", "title": "Discovery of Web Usage Profiles Using Various Clustering Techniques", "abstract": "The explosive growth of the World Wide Web (WWW) requires the development of web personalization systems to understand the preferences of users, which deliver dynamically tailored content to individual users. To reveal information about user preferences from web usage data, Web Usage Mining (WUM) techniques are widely applied to web log data. Cluster techniques are widely used within the WUM to capture similar interests and trends among users who access a website. Clustering aims to divide a data set into groups or clusters where cluster similarities are minimized and similarities within a cluster are maximized. In this paper, four commonly used cluster techniques are presented and compared: k-Means, k-Medoids, Leader and DBSCAN. These techniques are implemented and tested against web user navigation data. Performance and validity of individual techniques are presented and compared.", "histories": [["v1", "Tue, 1 Sep 2015 09:31:37 GMT  (1103kb)", "http://arxiv.org/abs/1509.00692v1", "arXiv admin note: substantial text overlap witharXiv:1507.03340"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1507.03340", "reviews": [], "SUBJECTS": "cs.DB cs.IR cs.LG", "authors": ["zahid ansari", "waseem ahmed", "m f azeem", "a vinaya babu"], "accepted": false, "id": "1509.00692"}, "pdf": {"name": "1509.00692.pdf", "metadata": {"source": "CRF", "title": "Discovery of Web Usage Profiles Using Various Clustering Techniques", "authors": ["Zahid Ansari", "Waseem Ahmed", "M.F. Azeem"], "emails": ["zahid.ansari@acm.org"], "sections": [{"heading": null, "text": "has necessitated the development of Web personalization systems in order to understand the user preferences to dynamically serve customized content to individual users. To reveal information about user preferences from Web usage data, Web Usage Mining (WUM) techniques are extensively being applied to the Web log data. Clustering techniques are widely used in WUM to capture similar interests and trends among users accessing a Web site. Clustering aims to divide a data set into groups or clusters where inter-cluster similarities are minimized while the intra cluster similarities are maximized. This paper reviews four of the popularly used clustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques are implemented and tested against the Web user navigational data. Performance and validity results of each technique are presented and compared. (Abstract)\nKeywords-component; web usage mining; k-means clustering; kmedoids clustering; leader clustering; DBSCAN\nI. INTRODUCTION\nWeb Usage Mining [1] is described as the automatic\ndiscovery and analysis of patterns in web logs and associated\ndata collected as a result of user interactions with Web\nresources on one or more Web sites. The goal of Web usage\nmining is to capture, model, and analyse the behavioural\npatterns and profiles of users interacting with a Web site. The\ndiscovered patterns are usually represented as collections of\nURLs that are frequently accessed by groups of users with\ncommon interests. Web usage mining has been used in a\nvariety of applications such as i) Web Personalization systems\n[2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence\n[5], iv) System Improvement to understand the web traffic\nbehaviour which can be utilized to decide strategies for web\ncaching [6], load balancing and data distribution [7], iv) Fraud\ndetection: detection of unusual accesses to the secured data [8],\netc. Clustering techniques are widely used in WUM to capture similar interests and trends among users accessing a Web site. Clustering aims to divide a data set into groups or clusters where inter-cluster similarities are minimized while the intra cluster similarities are maximized. Details of various clustering techniques can be found in survey articles [9]-[11]. The ultimate goal of clustering is to assign data points to a finite system of k clusters. Union of these clusters is equal to a full dataset with the possible exception of outliers. Clustering groups the data objects based only on the information found in the data which describes the data objects and the relationships between them.\nSome of the main categories of the clustering methods are [12]: i) Partitioning methods, that create k partitions of a given data set, each representing a cluster. Typical partitioning methods include k-means, k-medoids etc. In k-means algorithm each cluster is represented by the mean value of the data points in the cluster called centroid of the cluster. On the other hand and in k-medoids algorithm, each cluster is represented by one of the data point located near the center of the cluster called medoid of the cluster. Leader clustering is also a partitioning based clustering techniques which generates the clusters based on an initially specified dissimilarity measure, ii) Hierarchical methods create a hierarchical decomposition of the given set of data objects. A hierarchical method can be classified as being either agglomerative or divisive, based on how the hierarchical decomposition is formed. iii) Density- based methods form the clusters based on the notion of density. They can discover the clusters of arbitrary shapes. These methods continue growing the given cluster as long as the number of objects or data points in the \u201cneighborhood\u201d exceeds some threshold. They can also filter out noise and outliers. DBSCAN is a typical densitybased method that grows clusters according to a density-based connectivity analysis. iv) Grid-based methods quantize the data\nJuly Issue Page 18 of 69 ISSN 2229 5208\nVol. 1, No. 3, 2011 object space into a finite number of cells that form a grid structure. All the clustering operations are performed on the grid structure. v) Model-based methods, that discover the best fit between data points given a mathematical model. Mathematical model is usually specified as a probability distribution.\nThe remainder of the paper is organized as follows. Section\nII presents a overview of web usage mining using clustering\ntechniques and the underlying concepts. Section III presents\neach of the k-Means, k-Medoids, Leader and DBSCAN\nclustering techniques in detail along with the underlying\nmathematical foundations. Section IV describes the\nexperimental results of each technique, followed by a\ncomparison of the results. A brief conclusion and future work\nare presented in Section V.\nII. WEB USAGE MINING USING CLUSTERING\nA number of clustering algorithms have been used in Web usage mining where the data items are user sessions consisting of sequence of page URLs accessed and interest scores on each URL page based on the characteristics of user behaviour such as time elapsed on a page or the bytes downloaded [2]. In this context, clustering can be used in two ways, either to cluster users or to cluster items. In user-based clustering, users are grouped together based on the similarity of their web page navigational patterns. In item based clustering, items are clustered based on the similarity of the interest scores for these items across all users. Mobasher et. al. [13], [14] have used both user-based clustering as well as item-based clustering in a personalization framework based on Web usage mining.\nA typical user-based clustering starts with the matrix representing the user sessions or user profiles and partitions this multi-dimensional space into k groups of profiles that are close to each other based on a measure of distance or similarity among the vectors (such as Euclidean or Manhattan distance). Clusters obtained in this way can represent user segments based on their common navigational behaviour or interest shown in various URL items. In order to determine similarity between a target user and a user segment represented by the user session clusters, the centroid vector corresponding to each cluster is computed which is the representation of that user segment. To make a recommendation for a target user u and target URL item i, a neighbourhood of user segments that have a interest scores for i and whose aggregate profile is most similar to u are selected. This neighbourhood represents the set of user segments of which the target user is most likely to be a member. Given that the aggregate profile of a user segment that contains the average interest scores for each item within the segment, a prediction can be made for item i using k-nearestneighbor approach [15].\nWe map the user sessions as vectors of URL references in a n-dimensional space. Let } , , , 21 nuuuU  be a set of n unique URLs appearing in the preprocessed log and let } , , , 21 msssS  be a set of m user sessions discovered by preprocessing the web log data, where each user session Ss i can be represented as } , , , 21 muuu wwws  . Each\niu w may be either a binary or non-binary value depending on whether it represents presence and absence of the URL in the\nsession or some other feature of the URL. If iu w represents presence of absence of the URL in the session, then each user session is represented as a bit vector where\n(1) otherwise 0;\n; if ;1      su w iui\nInstead of binary weights, feature weights can also be used to represent a user session. These feature weights may be based on frequency of occurrence of a URL reference within the user session, the time a user spends on a particular page or the number of bytes downloaded by the user from a page.\nIII. DATA CLUSTERING TECHNIQUES\nIn this section a detailed discussion of each clustering technique and its underlying mathematical model is presented."}, {"heading": "A. k-Means Clustering Algorithm:", "text": "The k-Means clustering or Hard c-Means clustering algorithm [16] is one of the most commonly used methods for partitioning the data. Given a set of m data points  mixX i 1|  , where each data point is a n-dimensional vector, k-means clustering algorithm aims to partition the m data points into k clusters (k \u2264 m) C = {c1, c2, \u2026, ck} so as to minimize an objective function (or a cost function) J(V, X) of dissimilarity [17], which is the within-cluster sum of squares. In most cases the dissimilarity measure is chosen as the Euclidean distance. The objective function is an indicator of the distance of the n data points from their respective cluster centers. The objective function J, based on the Euclidean distance between a data point vector xi in cluster j and the corresponding cluster center vj, is defined in (2).\nj v i xvxd\ncxu\nc\nvxduvxJ\nvxduvxJVXJ\nji\njiij\ni\nm\ni\njiijjii\nk\nj\nm\ni\njiij\nk\nj\njii\nand between disatnce theis ),(\notherwise. 0 and if ,1\n, cluster ithin function w objective theis\n, ),(.),( , where\n(2) , ),(.),(),(\n2\n1\n2\n1 1\n2\n1\n\n\n  \n\n  \n \n\n \n\n \nEuclidian distance between various data points and cluster centers can be calculated using (3).\nJuly Issue Page 19 of 69 ISSN 2229 5208\nj thj k\ni thi k\nj k\nn\nk\ni\nkji\nvkv\nxkx\nn\nvxvxd (3) ),(\nof dimensionsof value theis\nof dimensionsof value theis\npoint dataeach of dimensions ofnumber theis , where\n2\n1 2   \nThe k-means clustering first initializes the cluster centers randomly. Then each data point xi is assigned to some cluster vj which has the minimum distance with this data point. Once all the data points have been assigned to clusters, cluster centers are updated by taking the weighted average of all data points in that cluster. This recalculation of cluster centers results in better cluster center set. The process is continued until there is no change in cluster centers.\nThe partitioned clusters are defined by a mk binary membership matrix U, where the element uij is 1, if the ith data point xi belongs to the cluster j, and 0 otherwise. Once the cluster centers V = {v1, v2, \u2026, vk}, are fixed, the membership function uij that minimizes (2) can be derived as follows:\n     (4) otherwise 0; ,,1* *, ),(),( if ;1 * 22 kjjjvxdvxd u jijiij \nThe equation (4) specifies that assign each data point xi to the cluster cj with the closest cluster center vj. Once the membership matrix U=[uij ] is fixed, the optimal center vj that minimizes (2) is the mean of all the data point vectors in cluster j :\n\n\n\n\n\n\nm\ni ijjjj\nm\ncxi i j\nj\nuccc\nx c v\nji\n1\n,\nalso and cluster of size theis ,\n(5) 1\n, where\nGiven an initial set of k means or cluster centers, V = {v1, v2, \u2026, vk}, the algorithm proceeds by alternating between two steps: i) Assignment step: Assign each data point to the cluster with the closest cluster center. ii) Update step: Update the cluster center as the mean of all the data points in that cluster. The input to the algorithm is a set of m data points  mixX i 1|  , where each data point is a n-dimensional vector, it then determines the cluster centers vj and the membership matrix U iteratively as explained in Fig. 1.\nThe k-means algorithm provides locally optimal solutions\nwith respect to the sum of squared errors represented by the\nerror objective function. Since it is a fast iterative algorithm, it\nhas been applied to a variety of areas [18]-[20].\nThe attractiveness of the k-means lies in its simplicity and\nflexibility. However, it suffers from major shortcomings that\nhave been a cause for it not being implemented on large\ndatasets. The most important among these are i) k-Means\nscales poorly with respect to the time it takes for large number\nof points; ii) The algorithm might converge to a solution that\nis a local minimum of the objective function. The main\ndisadvantage of this algorithm lies in its sensitivity to initial\npositions of the cluster centroids [21].\nSince the performance of the k-Means algorithm depends\non the initial positions of the cluster centeroids, it is\nrecommended to execute the algorithm multiple times, each\nwith a different set of initial centroids."}, {"heading": "B. K-Medoids Clustering Algorthm:", "text": "k-Medoid is a classical partitioning technique of clustering that clusters the data set of m data points into k clusters. It attempts to minimize the squared error, which is the distance between data points within a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-Medoids algorithm selects data points as cluster centers (or medoids). A medoid is a data point of a cluster, whose average dissimilarity to all the other data points in the cluster is minimal i.e. it is a most centrally located data point in the cluster [20],[22].\nGiven a set of m data points  mixX i 1|  , where each data point is a n-dimensional vector, k-mdoids clustering algorithm aims to partition the m data points into k clusters (k \u2264 m) C = {c1, c2, \u2026, ck} so as to minimize an objective function representing the sum of the dissimilarities between each of the data points and its corresponding cluster medoid. Let M = {m1, m2, \u2026, mk} be the set of medoids corresponding to C. The objective function J(X, M) is defined in (7)\nJuly Issue Page 20 of 69 ISSN 2229 5208\njm th k j k m\nix th k i kx\nn\nj k\nm n\nk\ni\nk xjmixd\njmi xjmixd\njcixiju\njcjm\ni i x\nk\nj\nm\ni\njiij mxduMXJ\nof dimensions of value theis\nof dimensions of value theis\npoint dataeach of dimensions ofnumber theis , where\n(8)\n2\n1 ),(\n2\nand between disatnceEuclidean theis ),( 2\notherwise. 0 and if ,1\ncluster of medoid theis\npoint data th theis\n, where\n(7) , ),(.),(\n1 1\n2\n  \n\n   \n  \n\n  \n \nThe partitioned clusters are defined by a mk binary membership matrix U, where the element uij is 1, if the ith data point xi belongs to the cluster j, and 0 otherwise. Once the cluster medoids M = {m1, m2, \u2026, mk}, are fixed, the membership function uij that minimizes (7) can be derived as follows:\n     (9) otherwise 0; ,,1* *, ),(),( if ;1 * 22 kjjjmxdmxd u jijiij \nThe equation (9) specifies that assign each data point xi to the cluster medoid mj. Once the membership matrix U=[uij] is fixed, the new cluster medoids mj that minimizes (7) can be found using (10)\n(10) ),(minarg   \njl j cx\nli cxi\njm xxd\nThe basic strategy of k-Medoids clustering algorithms is to discover k clusters in m objects by first arbitrarily selecting a representative data point (the Medoid) as the center for each cluster. Each remaining data point is clustered with the medoid to which it is the most similar. The algorithm takes the input parameter k, the number of clusters to be partitioned among a set of m objects.\nThe most common realisation of k-medoid clustering is the Partitioning Around Medoids (PAM) algorithm and is as described in Fig.\nIt is more robust to noise and outliers as compared to kmeans because because a medoid is less influenced by outliers or other extreme values than a mean. It minimizes the sum of pair-wise dissimilarities instead of a sum of squared Euclidean distances as in case of k-means. Both methods require the user to specify k, the number of clusters."}, {"heading": "C. Leader Clustering Algorthm:", "text": "The leader clustering algorithm [23],[24] is based on a predefined dissimilarity threshold. Initially, a random data point from the input data set is selected as leader. Subsequently, distance of every other data point with the selected leader is computed. If the distance of a data point is less than the dissimilarity threshold, that data point falls in the cluster with the initial leader. Otherwise, the data point is identified as a new leader. The computation of leaders is continued till all the data points are considered. It should be noted that the result of the clustering depends on the chosen distance threshold. The number of leaders is inversely proportional to the selected threshold.\nGiven a set of m data points  mixX i 1|  , where each data point is a n-dimensional vector. The Euclidean distance between the ith data point Xxi  and j th leader Ll j  (where\nJuly Issue Page 21 of 69 ISSN 2229 5208\nAlgorithm: Leader Clustering\nInput: i) Set of m data points X={x1, \u2026, xm},\nii) \u03b1, the dissimilarity threshold.\nOutput: Set of clusters C = {c1, \u2026, ck}, Steps:\n1) 1,,  jLC  // Initilize the cluster and leader sets\n2) 1xl j  // Initialize 1x as the first leader\n3) jlLL \n4) 1xcc jj \n5) jcCC \n6) for each Xxi  where i = 2, \u2026 m 7) begin\n8) ),(minarg , ji Lljj lxdj  \n9) if ),(2 ji lxd then\n10) ijj xcc \n11) else 12) j = j + 1\n13) ij xl \n14) jlLL \n15) ijj xcc \n16) jcCC \n17) endif 18) end"}, {"heading": "D. DBSCAN Clustering Algorthm:", "text": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) [25] is a density-based data clustering algorithm because it finds a number of clusters starting from the estimated density distribution of corresponding nodes.\nFigure 3. Leader Clustering Algorithm\nGiven a set of m data points  mixX i 1|  , where each data point is a n-dimensional vector. The Euclidean distance between the two data points Xxp  and Xxq  is given by\nq\nq p\nx th k q k m\npx th k p k x\nn\nk x\nn\nk k xqxpxd\nof dimensions of value theis\nof dimensions of value theis\npoint dataeach of dimensions ofnumber theis , where\n(12)\n2\n1 ),(\n2 \n \nIn this algorithm concept of a cluster is based on the notion of \u201c\u03b5-neighborhood\u201d and \u201cdensity reachability\u201d. Let the \u03b5-\nneighborhood of a data point xp , denoted as )( pxN is defined as below:\ndistance odneighborh theis , where\n(13) ),(2 X)(\n\n  \n      q x p xd q xpxN\nLet  be the minimum number of points required to form a\ncluster. A point xq is directly density-reachable from a point xp, if xq is part of \u03b5-neighborhood of xp and if the number of points in the \u03b5-neighborhood of xp are greater than or equal to  as\nspecified in (13)\ncluster afor required points ofnumber minimum theis\n)(\n(14) )(\n\n \n\nwhere\nx\npxN\npxNq\n\n\nxq is called density-reachable from xp if there is a sequence x1 , \u2026 , xn of points with x1 = xp and xn = xq where each xi + 1 is directly density-reachable from xi. Two points xp and xq are said to be density-connected if there is a point xo such that xo and xp as well as xo and xq are density-reachable.\nA cluster of data points satisfies two properties: i) All the data points within the cluster are mutually density-connected. ii) If a data point is density-connected to any data point of the cluster, it is part of the cluster as well.\nInput to DBSCAN algorithm are i) \u03b5 (epsilon) and ii)  ,\nthe minimum number of points required to form a cluster. The algorithm starts by randomly selecting a starting data point that has not been visited. If the \u03b5-neighborhood of this data point contains sufficiently many points, a cluster is started. Otherwise, the data point is labeled as noise. Later this point might be found in a sufficiently sized \u03b5-neighborhood of a different data point and hence could become part of a cluster. If a data point is found to be part of a cluster, all the data points in its \u03b5-neighborhood are also part of that cluster and hence added to the cluster. This process continues until the cluster is completely found. Then, a new unvisited point is selected and processed, leading to the discovery of a next cluster or noise. Fig. 4 describes the DBSCAN algorithm.\nAlthough DBSCAN can cluster objects given input\nparameters such as and \u03b7, but it is the responsibility of the user to select these parameter values. Such parameter settings are usually empirically set and difficult to determine, especially for high-dimensional data sets.\nJuly Issue Page 22 of 69 ISSN 2229 5208\nIV. EXPERIMENTAL RESULTS\nIn order to discover the clusters that exist in user accesses sessions of a web site, we carried out a number of experiments using various clustering techniques. The Web access logs were taken from the P.A. College of Engineering, Mangalore web site, at URL http://www.pace.edu.in. The site hosts a variety of information, including departments, faculty members, research areas, and course information. The Web access logs covered a period of one month, from February 1, 2011 to February 8, 2011. There were 12744 logged requests in total."}, {"heading": "A. Preprocessing the Web Log Data:", "text": "After performing the cleaning operation the output file contained 12744 entries. Total numbers of unique users identified are 16 and the number of user sessions discovered are 206. Table II depicts the results of cleaning and user identification and user session identification steps of preprocessing. Further details of our preprocessing approaches can be found from our previous work [26]."}, {"heading": "RESULTS OF CLEANING AND USER IDENTIFICATION", "text": "Figure 4 shows the result of user session identification. It depicts the percentage of user sessions accessing the specified number of URLs."}, {"heading": "B. Clustering of User Navigational Sessions:", "text": "Once the user sessions are discovered, user session data is presented to the four different clustering algorithms in order to discover session clusters that represent similar URL access patterns. These algorithms are i) k-Means ii) k-Medoids iii) Leader and iv) DBSCAN. Since the above clustering algorithms result in different clusters it is important to perform an evaluation of the results to assess their quality. We evaluated our results based on DB index and C Index which are two quality measures to evaluate the quality of the discovered clusters. These validity measures a described below:\nDavies-Bouldin Validity Index: This index attempts to\nminimize the average distance between each cluster and the\none most similar to it. It is defined as:\n      (16) , max 1\n1 ,1      \n\n  \n  \nk\ni ji\nji\nijkj ccdis\ncdiamcdiam\nk DB\nJuly Issue Page 23 of 69 ISSN 2229 5208\nAn optimal value of the k is the one that minimizes this\nindex.\nC Index: It is defined as [28]:\n(17) ,\nminmax\nmin\nSS\nSS C\n\n \nHere S is the sum of distances over all pairs of objects form the same cluster. Let m be the number of those pairs and Smin is the sum of the m smallest distances if all pairs of objects are considered. Similarly Smax is the sum of the m largest distances out of all pairs. The interval of the C-index values is [0, 1] and this value should be minimized.\nThe results of application of various clustering algorithms are presented in the following subsections.\n1) k-Means Algorithm: We conducted multiple runs of k-Means algorithm by selecting the input parameter k (number of clusters) ranging from k = 2, \u2026, 67. (The value 67 for the number of clusters is one third of total number of the discovered user sessions). For each of these runs we computed the value of the clustering error function (J) using (2) which represents the sum of the squared error. We also computed the execution timings, Dunn\u2019s index, DB index and C index for all of the above runs. Table III describes the results after the application of k-Means clustering algorithm."}, {"heading": "K-MEANS CLUSTERING RESULTS", "text": "One of the problems associated with the k-Means algorithm is\nthat it may produce empty clusters depending on the initial\ncentroids chosen. Graph in Fig. 6 Describes the number of\nempty clusters generated for different values of k. M.K.\nPakhira [29] has proposed a modified k-means algorithm to\navoid the empty clusters. K-medoids algoritm also rectifies\nthis problem.\n2) k-Medoids Algorithm: We conducted the multiple runs of k-Medoids algorithm by selecting the input parameter k (number of clusters) ranging from k = 2, \u2026, 67. (The value 67 for the number of clusters is one third of total number of the discovered user sessions). For each of these runs we computed the value of the clustering error function (J) using (7), which represents the sum of the squared error. We also computed the execution timings, Dunn\u2019s index and DB index and C index for all of the above runs. Table IV describes the results after the application of kMeans clustering algorithm."}, {"heading": "K-MEDOIDS CLUSTERING RESULTS", "text": "We compared the k-Means and k-Medoids algorithms based on clustering error (J as defined in equations (2) and (7)), cluster validity using C index and the execution time.\nJuly Issue Page 24 of 69 ISSN 2229 5208\nOur results (Fig. 7) show that the k-Means algorithm minimizes the clustering error (J) slightly better than the kMedoids algorithm. C index values in graph plot of Fig. 8 indicates that the clusters of k-Means algorithm have better validity index than that of k-Medoids algorithm. On the other the execution timings of k-Medoids algorithms are faster than the that of k-Means algorithm as show in Fig.9.\n3) Leader Algorithm: We conducted the multiple runs of Leader algorithm by selecting the input parameter \u03b5 (Dissimilarity Threshold) ranging from \u03b5 = 0.5, \u2026, 3.5 in steps of 0.5. For each of these runs we computed the value of the clustering error. We also computed the execution timings, DB index and C index for all of the above runs. Table V describes the results after the application of Leader clustering algorithm.\nTABLE V"}, {"heading": "LEADER CLUSTERING RESULTS", "text": "Epsilon\n(\u03b5)\nError\n(J)\nDB\nIndex C Index\nExecution Time(ms)\nNo. of\nClusters\n1\n1.5\n2\n2.5\n3\n3.5\n26.19\n76.81\n216.62\n398.81\n467.07\n624.87\n0.3623\n0.5061\n0.5578\n0.7200\n0.9084\n0.8801\n0.0021\n0.0348\n0.0588\n0.0801\n0.1878\n0.2407\n3\n2\n2\n1\n2\n1\n115\n86\n56\n33\n26\n14\nFig. 10 shows the results of Leader clustering. From the graph it is very clear that the number of discovered clusters is inversely proportional to the dissimilarity threshold \u03b5.\nFigure 10. Number of clusters formed Vs. Dissimilarity Threshold \u03b5\n4) DBSCAN Algorithm: We conducted the multiple runs of DBSCAN algorithm by selecting the input parameter \u03b5 (neighborhood disatnace) ranging from \u03b5 = 0.5, \u2026, 3.5 in steps of 0.5. The other parameter \u03b7 which indicates the minimum no. of points in a cluster is set in a range from \u03b7 = 2, \u2026, 10. For each of these runs we computed the value of the clustering error. We also computed the execution timings, DB index and C index for all of the above runs. Table VI describes the results after the application of DBSCAN algorithm for the value of \u03b7 = 2.\nJuly Issue Page 25 of 69 ISSN 2229 5208\nThe graph plot in Fig. 11 displays the C index as a function of the neighbourhood distance \u03b5, for different values of \u03b7 (the minimum number of points in a cluster). The graph shows that the C index value improves as we increase the neighbourhood distance \u03b5. It also improves if we decrease the value of \u03b7 .\nThe graph plot in Fig. 12 displays the number of clusters\nformed as a function of the neighbourhood distance \u03b5, for\ndifferent values of \u03b7 (the minimum number of points in a\ncluster). The graph shows that the number of clusters formed\ndecreases as we increase the neighbourhood distance \u03b5. It also\ndecreases if we increase the value of \u03b7 .\nThe next two graphs compare the results of the Leader and\nDBSCAN techniques.\nThe graph plot in Fig. 13 displays the C validity index value\nas a function of Epsilon (\u03b5). Here \u03b5 is the dissimilarity\nthreshold in case of Leader clustering and neighbourhood\ndistance in case of DBSCAN. Our results show that in case of\nLeader clustering, validity index improves for lower values of\ndissimilarity distance \u03b5. In case of DBSCAN, the validity\nindex improves as increase the value of neighbourhood\ndistance \u03b5. Note that we have set the value of \u03b7 to 1.\nThe graph plot in Fig.14 displays the Execution Time as a\nfunction of Epsilon (\u03b5). It is clear from the graph that the\nLeader algorithm performs much faster than the DBSCAN if\nwe keep the Leader dissimilarity threshold and DBSCAN\nneighbourhood distance same. Note that we have set the value\nof \u03b7 to 1."}, {"heading": "I. CONCLUSION AND FUTURE WORK", "text": "In this paper we have presented our framework for web\nusage data clustering for users\u2019 navigational sessions using k-\nMeans, k-Medoids, Leader and DBSCAN clustering\nalgorithms. We provided a detailed overview of these\ntechniques. We also described the mathematical model and\nalgorithm details related to the implementation of these\nclustering algorithms in order to discover the user sessions\nclusters. From the results presented in the previous section, we\nconclude the following points.\n K-means clustering produces fairly higher accuracy and lower clustering error as compared with k-medoids clustering algorithm.\n K-means algorithm may result in the formation of empty cluster while it is not the case with k-medoids algorithm.\n Our result shows that k-medoids algorithm gives reasonably better time performance than that of the kmeans algorithm. The reason behind this is we are using a large data set. The k-Medoids algorithm requires to compute the distance between every pair of data objects only once and uses this distance at every stage of iteration. On the other for an optimal solution k-Means algorithm performs multiple runs and computes the distance between every data object and it\u2019s corresponding cluster center.\n Although Leader clustering algorithm does not require estimating the value of k at the beginning, it does require estimating the dissimilarity threshold \u03b5.\n Number of clusters formed in Leader clustering is inversely proportional to the value of dissimilarity threshold \u03b5.\n Leader clustering validity index (C index) improves as we increase the value of the dissimilarity threshold \u03b5.\n DBSCAN algorithm can identify a data point as a noise or outlier.\nJuly Issue Page 26 of 69 ISSN 2229 5208\n DBSCAN validity index (C index) improves as we decrease the value of the neighborhood distance \u03b5.\n If we choose the same value for dissimilarity threshold in Leader clustering and neighbor distance in DBSCAN (while keeping \u03b7 constant), the time performance of Leader clustering much faster than that of DBSCAN.\nAnother direction of future work is related with the use of\nfuzzy c-Mean clustering technique to discover the user session\nclusters. The reason behind this is, although the several\nclustering algorithms described are suitable in handling the\ncrisp data which have clear cut boundaries, but in reality web\nusage data is semi-structured and contains the outliers and\nincomplete navigational data, due to a wide variety of reasons\ninherent to web browsing and logging. Therefore, Web Usage\nMining requires modelling of multiple overlapping sets in the\npresence of significant noise and outliers. Soft Computing\nbased techniques such as Fuzzy Clustering can be very useful\nfor mining such semi structured, noisy and incomplete data."}], "references": [{"title": "Web usage mining: Discovery and applications of usage patterns from web data", "author": ["J. Srivastava", "R. Cooley", "M. Deshpande", "P.N. Tan"], "venue": "SIGKDD explorations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Data mining for web personalization", "author": ["B. Mobasher"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Adaptive web sites: Automatically synthesizing web pages", "author": ["M. Etzioni O. Perkowitz"], "venue": "In Proceedings of the 15th National Conference on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Adaptive web sites", "author": ["M. Etzioni O. Perkowitz"], "venue": "Communications of ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Business intelligence from web usage mining", "author": ["Ajith Abraham"], "venue": "Journal of Information & Knowledge Management,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Improving end-to-end performance of the web using server volumes and proxy filters", "author": ["Edith Cohen", "Balachander Krishnamurthy", "Jennifer Rexford"], "venue": "SIGCOMM Comput. Commun. Rev.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Exploiting web log mining for web cache enhancement", "author": ["Alexandros Nanopoulos", "Dimitrios Katsaros", "Yannis Manolopoulos"], "venue": "WEBKDD", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "A stateful intrusion detection system for world-wide web servers", "author": ["G. Vigna", "W. Robertson", "Vishal Kher", "R.A. Kemmerer"], "venue": "In Computer Security Applications Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "Springer, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "A survey of clustering data mining techniques", "author": ["B. Pavel"], "venue": "Grouping Multidimensional Data. Springer Berlin Heidelberg, 2006, pp. 25\u201371.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D.I. Wunsch"], "venue": "Neural Networks, IEEE Transactions on, vol. 16, no. 3, pp. 645\u2013678, May 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Data Mining: Concepts and Techniques", "author": ["M.K. Jiawei Han"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Discovery and evaluation of aggregate usage profiles for web personalization", "author": ["B. Mobasher", "H. Dai", "T. Luo", "M. Nakagawa"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Automatic personalization based on web usage mining", "author": ["Bamshad Mobasher", "Robert Cooley", "Jaideep Srivastava"], "venue": "Commun. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["Jonathan L. Herlocker", "Joseph A. Konstan", "Al Borchers", "John Riedl"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1979}, {"title": "Neuro- Fuzzy and Soft Computing \u2013 A Computational Approach to Learning and Machine Intelligence", "author": ["Jang", "J.-S.R.", "Sun", "C.-T.", "E. Mizutani"], "venue": "Prentice Hall.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 0}, {"title": "Pattern Classification and Scene", "author": ["R.O. Duda", "P.E. Hart"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1973}, {"title": "Bubes, Algorithm for Clustering Data, Prentice-Hall", "author": ["R.C.A.K. Jain"], "venue": "Englewood Cli s, NJ,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1988}, {"title": "Finding Groups in Data. An Introduction to Cluster", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1990}, {"title": "Refining initial points for k-means clustering", "author": ["P.S. Bradley", "Usama M. Fayyad"], "venue": "In Proceedings Fifteenth International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Cluster Analysis * Algorithms for Data Reduction and Classi\"cation of Objects", "author": ["H. Spath"], "venue": "Ellis Horwood Limited,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1980}, {"title": "Comparison ofGenetic algorithmbased prototype selection scheme, Pattern Recognition", "author": ["T.R. Babu", "M.N. Murty"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["Martin Ester", "Hans-Peter Kriegel", "J\u00f6rg Sander", "Xiaowei Xu"], "venue": "Proceedings of the Second International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Preprocessing users web page navigational data to discover usage patterns", "author": ["Zahid Ansari", "Mohammed Fazle Azeem", "A. Vinaya Babu", "Waseem Ahmed"], "venue": "In The Seventh International Conference on Computing and Information", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "A cluster separation measure", "author": ["D.L. Davies", "D.W. Bouldin"], "venue": "IEEE Trans. Pattern Anal. Machine Intell", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1979}, {"title": "Quadratic assignment as a general dataanalysis strategy", "author": ["L. Hubert", "J. Schultz"], "venue": "British Journal of Mathematical and Statistical Psychology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1976}, {"title": "A Modified k-means Algorithm to Avoid Empty Clusters, International Journal of Recent Trends in Engineering", "author": ["M.K. Pakhira"], "venue": "Vol 1, No", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Web Usage Mining [1] is described as the automatic discovery and analysis of patterns in web logs and associated data collected as a result of user interactions with Web resources on one or more Web sites.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 130, "endOffset": 133}, {"referenceID": 4, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 289, "endOffset": 292}, {"referenceID": 6, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 331, "endOffset": 334}, {"referenceID": 7, "context": "Web usage mining has been used in a variety of applications such as i) Web Personalization systems [2], ii) Adaptive Web Sites [3][4], iii) Business Intelligence [5], iv) System Improvement to understand the web traffic behaviour which can be utilized to decide strategies for web caching [6], load balancing and data distribution [7], iv) Fraud detection: detection of unusual accesses to the secured data [8], etc.", "startOffset": 407, "endOffset": 410}, {"referenceID": 8, "context": "Details of various clustering techniques can be found in survey articles [9]-[11].", "startOffset": 73, "endOffset": 76}, {"referenceID": 10, "context": "Details of various clustering techniques can be found in survey articles [9]-[11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Some of the main categories of the clustering methods are [12]: i) Partitioning methods, that create k partitions of a given data set, each representing a cluster.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "WEB USAGE MINING USING CLUSTERING A number of clustering algorithms have been used in Web usage mining where the data items are user sessions consisting of sequence of page URLs accessed and interest scores on each URL page based on the characteristics of user behaviour such as time elapsed on a page or the bytes downloaded [2].", "startOffset": 326, "endOffset": 329}, {"referenceID": 12, "context": "[13], [14] have used both user-based clustering as well as item-based clustering in a personalization framework based on Web usage mining.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14] have used both user-based clustering as well as item-based clustering in a personalization framework based on Web usage mining.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "Given that the aggregate profile of a user segment that contains the average interest scores for each item within the segment, a prediction can be made for item i using k-nearestneighbor approach [15].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "k-Means Clustering Algorithm: The k-Means clustering or Hard c-Means clustering algorithm [16] is one of the most commonly used methods for partitioning the data.", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": ", ck} so as to minimize an objective function (or a cost function) J(V, X) of dissimilarity [17], which is the within-cluster sum of squares.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "Since it is a fast iterative algorithm, it has been applied to a variety of areas [18]-[20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "Since it is a fast iterative algorithm, it has been applied to a variety of areas [18]-[20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "The main disadvantage of this algorithm lies in its sensitivity to initial positions of the cluster centroids [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "it is a most centrally located data point in the cluster [20],[22].", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Leader Clustering Algorthm: The leader clustering algorithm [23],[24] is based on a predefined dissimilarity threshold.", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "Leader Clustering Algorthm: The leader clustering algorithm [23],[24] is based on a predefined dissimilarity threshold.", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "DBSCAN Clustering Algorthm: DBSCAN (Density-Based Spatial Clustering of Applications with Noise) [25] is a density-based data clustering algorithm because it finds a number of clusters starting from the estimated density distribution of corresponding nodes.", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "Further details of our preprocessing approaches can be found from our previous work [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 26, "context": "C Index: It is defined as [28]: (17) , min max min S S S S C \uf02d \uf02d \uf03d", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "The interval of the C-index values is [0, 1] and this value should be minimized.", "startOffset": 38, "endOffset": 44}, {"referenceID": 27, "context": "Pakhira [29] has proposed a modified k-means algorithm to avoid the empty clusters.", "startOffset": 8, "endOffset": 12}], "year": 2011, "abstractText": "The explosive growth of World Wide Web (WWW) has necessitated the development of Web personalization systems in order to understand the user preferences to dynamically serve customized content to individual users. To reveal information about user preferences from Web usage data, Web Usage Mining (WUM) techniques are extensively being applied to the Web log data. Clustering techniques are widely used in WUM to capture similar interests and trends among users accessing a Web site. Clustering aims to divide a data set into groups or clusters where inter-cluster similarities are minimized while the intra cluster similarities are maximized. This paper reviews four of the popularly used clustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques are implemented and tested against the Web user navigational data. Performance and validity results of each technique are presented and compared. (Abstract) Keywords-component; web usage mining; k-means clustering; kmedoids clustering; leader clustering; DBSCAN", "creator": "Adobe Acrobat 8.0 Combine Files"}}}