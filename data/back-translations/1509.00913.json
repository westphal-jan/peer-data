{"id": "1509.00913", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a perpetual learning machine; a new type of DNN capable of enabling brain-like dynamic \"on-the-fly\" learning because it exists in a self-monitored state of perpetual stochastic gradient descent.", "histories": [["v1", "Thu, 3 Sep 2015 01:30:29 GMT  (473kb)", "http://arxiv.org/abs/1509.00913v1", null], ["v2", "Tue, 8 Sep 2015 07:54:41 GMT  (467kb)", "http://arxiv.org/abs/1509.00913v2", null], ["v3", "Tue, 29 Sep 2015 16:57:42 GMT  (590kb)", "http://arxiv.org/abs/1509.00913v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1509.00913"}, "pdf": {"name": "1509.00913.pdf", "metadata": {"source": "CRF", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic \u2018on the fly\u2019 learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework.\nIndex terms\u2014Perpetual Learning Machine, Perpetual Stochastic Gradient Descent, self-supervised learning, parallel dither.\nI. INTRODUCTION\nIt is an embarassing fact that while deep neural networks (DNN) are frequently compared to the brain, and even their performance found to be similar in specific static tasks, there remains a critical difference; DNN do not exhibit the fluid and dynamic learning of the brain but are static once trained. For example, to add a new class of data to a trained DNN it is necessary to add the respective new training data to the preexisting training data and re-train (probably from scratch) to account for the new class. By contrast, learning is essentially additive in the brain \u2013 if we want to learn a new thing, we do.\nThus, whilst there is little doubt that the learning of the brain and machine learning are essentially the same, the learning of the brain involves the emergent phenomenon of memory which has failed to emerge from machine learning. Indeed, recent machine-inspired approaches to \u2018memory\u2019 have involved explicit add-on storage facilities [e.g., 1] which explicitly discriminate between learning (training \u2013 i.e., of weights) and memory (storage \u2013 i.e., of data). Thus, the problem has been brushed under the carpet.\nIn this article, we describe a novel form of supervised learning model, which we call a Perpetual Learning Machine, which gives rise to the basic properties of memory. Our model involves two DNNs, one for storage and the other for recall. The storage DNN learns the classes of some training images. The recall DNN learns to synthesise the same images from the same classes. Together, the two networks hold, encoded, the training set. We then place these pair of DNNs in a selfsupervised and homeostatic state of Perpetual Stochastic Gradient Descent (PSGD). During each step of PSGD, a random class is chosen and an image synthesised from the recall DNN. This randomly synthesised image is then used in\ncombination with the random class to train both DNNs via non-batch SGD. I.e., the PSGD is driven by training data that is synthesised from memory according to random classes. We next demonstrate that new classes may be learned on the fly by introducing them, via \u2018new experience\u2019 SGD steps, into the path of PSGD. Over time, new classes are assimilated without disruption of earlier learning and hence we demonstrate a machine which both learns and remembers.\nII. METHOD\nWe chose the well-known MNIST hand-written digit dataset [2]. First, we unpacked the images of 28x28 pixels into vectors of length 784. Example digits are given in Fig. 1. Pixel intensities were normalized to zero mean.\nPerpetual Memory. In order to test the idea of perpetual memory, through perpetual learning, we required our model to learn to identify a collection of images. We took the first 75 of the MNIST digits and assigned each to an arbitrary class (this is arbitrary associative learning). This gave 75 unique classes, each associated with a single, specific digit. The task of the model was to recognise the images and assign to them the correct (arbitrary) classes. We split the 75 digits into 50 \u2018learn during training\u2019 examples and 25 \u2018learn later on the fly\u2019 test\nexamples. The first 50 training examples were learned with typical SGD and discarded. Hence, they were not available for later use during assimilation of additional classes. The latter 25 examples were held back for insertion during PSGD.\nStorage and Recall. We instantiated two DNN; the storage DNN was a typical classifier of size 784x100x75, with the softmax output layer corresponding to the 75-way classification problem. In addition to the 50 training classes, the 75 possible classes provides for 25 redundant (unused) classes to be learned later. The storage DNN took images as input and produced classes as output. The recall DNN was of size 75x100x784, took classes as input and synthesised the training images at output. Both DNNs used biased sigmoids [3] throughout (with zero bias in the output layer).\nThe storage and recall DNNs were trained, independently, using only the first 50 images for 100 full-sweep iterations of typical non-batch SGD [4,5]. Training was performed (regularised) using parallel (100x) dither w/ dropout [as in 5]. After 100 iterations, classification error was converged at 0.02% (1 mistake in 50) for the storage DNN, and at 0.02% for the storage DNN fed with the output of the recall DNN charged with synthesising the images of the respective test classes. Hence, the recall was suitably robust and was more or less visually indistinguishable from the original training images. Fig. 1 plots some example digits recalled (synthesised) using the recall DNN.\nPerpetual Stochastic Gradient Descent. Once the pair of DNN were independently trained on the 50 training images, the training images were discarded and the DNN were placed in a mutually recurrent and perpetual circuit. Fig. 3 shows a schematic diagram of PSGD; For each iteration of the PSGD, a random class was chosen (from the total 75 possible). Next, using this random class, a respective image was synthesised using the recall DNN. This synthetic image was then\ncombined with the random class and used together to train both DNNs in parallel (via non-batch SGD [5]). I.e., given the random seed, the recall DNN synthesised \u2013 from memory \u2013 the relevant training image and used it for self-supervision. This step of non-batch SGD also employed parallel dither w/dropout (100x). As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.\nIntroduction and learning of new classes. After 1000 iterations of PSGD, during which time homeostatic convergence had occurred, we enter a 2000-iteration on-thefly-learning epoch. In this epoch, new classes (of the 25) were introduced at random. At each iteration of PSGD, one of the 25 new digits was chosen (at random) and both DNN were trained for an additional single step (using 100x parallel dither w/ dropout) using the respective training data. Thus, the new training data were introduced into the path of the PSGD.\nFinally, at the end of the 2000-iterations on-the-fly-learning epoch, the introduction of new classes was stopped and the PSGD continued for a further 2000 iterations, during which time homeostatic convergence was reinstated (i.e., emerged).\nTo test the efficacy, on-the-fly dynamics and robustness of this perpetual memory, the classification accuracy of the storage DNN was tested at each iteration of PSGD. To do this, first the test classes were passed through the recall DNN to synthesise the \u2018remembered\u2019 images. These images were then classified using the storage DNN and the accuracy computed. At each PSGD iteration, to test retention of the pre-learned training set, test error was computed for the 50 training examples. In addition, to test the assimilation of the new classes, test error was computed on the 25 new classes. Finally, test error was computed over the whole set. This gave three dynamic measures of memory and recall that could be plotted as a function of time.\nIII. RESULTS\nFig. 3 plots the recall accuracy (classification error rate) of the storage DNN (via the recall DNN, as described above) as a function of PSGD iterations. Separate error functions are plotted for the training set of 50 (blue), the on-the-fly test set of 25 (green) and the entire set of 75 (red). In the case of the training set error function (blue), the first 1000 iterations demonstrate the homeostasis of perpetual learning \u2013 memory. The first 1000 iterations also demonstrate that classification accuracy in the on-the-fly test set (green) is around chance level and the combined full-set error rate is a weighted average of the two (red).\nAt 1000 iterations begins the 2000-iteration on-the-flylearning epoch, where new classes are learned. During this epoch, each step of PSGD additionally involves a SGD step taken with a randomly selected element of the new test set (25)\nconcurrently with the continuing self-supervised PSGD (random steps of self-supervision). Initially, the training set (blue) and full set (red) error functions deflect upwards as the impact of the new classes causes adjustment of the paths of the pre-established training set. At the same time, classification accuracy on the on-the-fly test set (green) begins to fall rapidly until it converges. Within a few hundred iterations of PSGD homeostasis is reinstated, as is reflected across the various error functions. At the 3000 iterations point (the end of the on-the-fly-learning epoch) the introduction of new data ceases with no obvious effect (even for the new test set error), illustrating a homeostasis which results from all 75 classes being fully assimilated into the perpetual memory. Essentially, 25 new images classes were learned on the fly, and retained after the on-the-fly-learning epoch, with no loss in accuracy for the original, pre-learned 50-strong training set.\nWe note that (data not shown) no aspect of the current model trains successfully without regularisation via parallel dither (w/ dropout) [5]. The attempt to train on the training set without parallel dither resulted in sufficiently poor performance that any form of PSGD did not succeed at all. Even if the initial training was done with parallel dither, if the PSGD was undertaken without parallel dither (i.e., either without any regularisation or with dropout) the result was a rapid explosion of error that did not recover. Hence, it appears that the regularisation provided by parallel dither (w/ dropout) is critical.\nIV. DISCUSSION AND CONCLUSION\nWe have demonstrated a powerful new class of selfsupervised deep neural network \u2013 a Perpetual Learning Machine -, capable of both maintaining perpetual memory and assimilating new classes into perpetual memory without the\nuse of the original training set. To achieve this we have placed two feed-forward DNNs \u2013 one for storage (classification) and the other for recall (synthesis) - in a recurrent and selfsupervised state of Perpetual Stochastic Gradient Descent. Each iteration of PSGD involves parallel backpropagated SGD driven by random recall of the training set. We have demonstrated that new classes can be assimilated into the perpetual memory by inserting concurrent SGD steps training for new image classes. I.e., we have demonstrated both onthe-fly learning and the persistence of what is learned on-thefly.\nIn summary, we have shown how memory emerges from learning and hence we have provided a means to unify learning and memory within a machine learning framework. Unlike conventional \u2018memory\u2019 (storage) devices attached to neural networks (e.g., LSTM [1]), this new class of Perpetual Learning Machine is able to store, retain, recall and add memories in perpetuity. Furthermore, unlike the conventional\nstorage-based \u2018memory\u2019 devices (e.g., [1]), the present unified architecture represents both learning and memory through the states of its weights. Hence, it seems possible that a similar principle may be responsible for both learning and memory within the brain.\nACKNOWLEDGMENT\nAJRS did this work on the weekends and was supported by his wife and children.\nREFERENCES [1] Hochreiter S, Schmidhuber J (1997) \u201cLong short-term memory\u201d,\nNeural computation 9: 1735-1780. [2] LeCun Y, Bottou L, Bengio Y, Haffner P (1998) \u201cGradient-based\nlearning applied to document recognition\u201d, Proc. IEEE 86: 2278\u20132324. [3] Simpson AJR (2015) \u201cAbstract Learning via Demodulation in a Deep\nNeural Network\u201d, arxiv.org abs/1502.04042. [4] Simpson AJR (2015) \u201cDither is Better than Dropout for Regularising\nDeep Neural Networks\u201d, arxiv.org abs/1508.04826. [5] Simpson AJR (2015) \u201cParallel Dither and Dropout for Regularising\nDeep Neural Networks\u201d, arxiv.org abs/1508.07130. [6] Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov R\n(2012) \u201cImproving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580."}], "references": [{"title": "Long short-term memory", "author": ["S Hochreiter", "J Schmidhuber"], "venue": "Neural computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org abs/1502.04042", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Dither is Better than Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.04826", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Parallel Dither and Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.07130", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "We chose the well-known MNIST hand-written digit dataset [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Both DNNs used biased sigmoids [3] throughout (with zero bias in the output layer).", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "The storage and recall DNNs were trained, independently, using only the first 50 images for 100 full-sweep iterations of typical non-batch SGD [4,5].", "startOffset": 143, "endOffset": 148}, {"referenceID": 4, "context": "The storage and recall DNNs were trained, independently, using only the first 50 images for 100 full-sweep iterations of typical non-batch SGD [4,5].", "startOffset": 143, "endOffset": 148}, {"referenceID": 4, "context": "This synthetic image was then combined with the random class and used together to train both DNNs in parallel (via non-batch SGD [5]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 6, "endOffset": 11}, {"referenceID": 4, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 6, "endOffset": 11}, {"referenceID": 5, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "We note that (data not shown) no aspect of the current model trains successfully without regularisation via parallel dither (w/ dropout) [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": ", LSTM [1]), this new class of Perpetual Learning Machine is able to store, retain, recall and add memories in perpetuity.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": ", [1]), the present unified architecture represents both learning and memory through the states of its weights.", "startOffset": 2, "endOffset": 5}], "year": 2015, "abstractText": "Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic \u2018on the fly\u2019 learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework.", "creator": "PDFCreator Version 1.7.1"}}}