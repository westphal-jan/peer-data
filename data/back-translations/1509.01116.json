{"id": "1509.01116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "A tree-based kernel for graphs with continuous attributes", "abstract": "The availability of graph data with node attributes that can be either discrete or real is constantly increasing. While existing kernel methods are effective techniques for handling graphs with discrete node attributes, their adaptation to non-discrete or continuous node attributes has been limited, mainly for computational reasons. Recently, some specifically tailored nuclei have been proposed for this area. To alleviate computational problems, the size of the attribute space of such nuclei tends to be smaller than that of the nuclei for discrete node attributes. However, such a selection could have a negative impact on predictive capability. In this paper, we propose a graph kernel for complex and continuous node attributes whose characteristics are tree structures extracted from specific graph visits. Experimental results from real data sets show that the (approximate version of) proposed kernel is comparable to the current state of the art.", "histories": [["v1", "Thu, 3 Sep 2015 14:59:10 GMT  (1289kb)", "http://arxiv.org/abs/1509.01116v1", "Paper submitted to IEEE Transactions on Neural Networks and Learning Systems"], ["v2", "Tue, 20 Dec 2016 16:54:02 GMT  (1029kb)", "http://arxiv.org/abs/1509.01116v2", "This work has been submitted to the IEEE Transactions on Neural Networks and Learning Systems for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"]], "COMMENTS": "Paper submitted to IEEE Transactions on Neural Networks and Learning Systems", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["giovanni da san martino", "nicol\\`o navarin", "alessandro sperduti"], "accepted": false, "id": "1509.01116"}, "pdf": {"name": "1509.01116.pdf", "metadata": {"source": "CRF", "title": "A tree-based kernel for graphs with continuous attributes", "authors": ["Giovanni Da San Martino"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n01 11\n6v 1\n[ cs\n.L G\n] 3\nS ep\n2 01\n5 1\nIn this paper, we propose a graph kernel for complex and continuous nodes\u2019 attributes, whose features are tree structures extracted from specific graph visits. Experimental results obtained on real-world datasets show that the (approximated version of the) proposed kernel is comparable with current state-of-the-art kernels in terms of classification accuracy while requiring shorter running times.\nI. INTRODUCTION\nThere is an increasing availability of data in the form of attributed graphs, i.e. graphs where some information is attached to nodes and edges (and to the graph itself). For computational reasons, available machine learning techniques for graph-structured data have been focusing on problems whose data can be modelled as graphs with discrete attributes. However, in many application domains, such as bioinformatics and action recognition, non-discrete node attributes are available [1], [2]. For example, many bioinformatics problems deal with proteins. It is possible to represent a protein as a graph, where nodes represent secondary structure elements. Two nodes are connected whenever they are neighbors either in the amino acid sequence or in space [1]. Each node has a discrete-valued attribute indicating the structure it belongs to (helix, sheet or turn). Moreover, several chemical and physical measurements can be associated with each node, such as the length of the secondary structure element in A\u030a, its hydrophobicity, polarity, polarizability etc. In some tasks, discarding such type of information has a significantly negative impact on the predictive performance (see section VII).\nMost of the graph kernels in the literature are not suited for non-discrete node labels since their computational efficiency hinges on avoiding to consider matches between distinct discrete labels. Of course, this strategy cannot work for nondiscrete labels, which in general are all distinct. An alternative would be to define a kernel function between graph nodes,\nG. Da San Martino is with the ALT research group, Qatar Computing Research Institute, HBKU, P.O. Box 5825 Doha, Qatar.\nN. Navarin and A. Sperduti are with the Department of Mahematics, University of Padova, via trieste 63, Padova, Italy.\nhowever the resulting computational times become unfeasible, as in the case of [3]. For this reason, recently there has been an increasing interest in the definition of graph kernels that can efficiently deal with continuous-valued attributed graphs. The problem is challenging because both fast and expressive (in terms of discrimination) kernels are looked for.\nIn this paper, we present a new kernel inspired by the graph kernel framework proposed in [4]. The features induced by the kernel are tree structures extracted from breadth-first visits of a graph (contrary to [1] an edge is only traversed once per visit). The main contribution of this paper is to extend the definition of tree kernels, and consequently derive a graph kernel, which is able to deal with complex and continuous node labels. As the experimental results show, the richer feature space allows to reach state-of-the-art classification performances on real world datasets. While the computational complexity of our kernel is the same as competing ones in literature, we describe an approximated computation of the kernel between graph nodes in order to reach fastest running times, while keeping state-of-the-art results."}, {"heading": "II. RELATED WORK", "text": "In the last few years, several different graph kernels for discrete-valued graphs have been proposed in literature. Early works presented kernels that have to be computed in closed form, such as the random walk kernel [5] or the shortest path kernel [3]. These kernels suffer from a relatively high computational complexity: O(n3) and O(n4) respectively. More recently, research focused on the efficiency of kernel calculation. State-of-the-art kernels use explicit feature mapping techniques [4], [6], [7], with computational complexities almost linear in the size of the graphs. If we consider graphs with continuous-valued labels, this last class of kernels cannot be easily modified to deal with them because their efficiency hinges on the ability to perform computation only for discrete labels that match. Of course, this is not possibile when considering continuos-valued labels. Between the two obvious possible solutions, i.e. adopt slower kernels or discretizing/ignoring the continuous attributes of the graphs, the latter approach was usually the preferred one [8]. In [9] a kernel for graphs with continuous-valued labels has been presented. The kernel matches common subgraphs up to a fixed size k, and has complexity O(nk).\nIn [10] another more efficient kernel has been presented. This kernel is a sum of path kernels, that in turn are a sum of node kernels. The computational complexity of the kernel is O(n2(m+ logn+ d+ \u03c32)) where n and m are the\n2 number of nodes and edges in the graph, respectively, \u03c3 is the depth of the graph and d is the dimension of the vectors associated to nodes. However, experimental results show that this kernel cannot achieve the same predictive performance as other computationally more demanding graph kernels, e.g. the shortest path kernel.\nVery recently, two kernel frameworks able to deal with continuous and vectorial labels have been proposed: in [11] authors propose to use Locality Sensitive Hashing to discretize continuous and vectorial labels, while in [12] a very general framework of graph kernels is proposed.\nThe experience on discrete-labeled graphs teaches us that path-features are not the most expressive ones. In fact, in [4], [7] it is shown that tree-features can express a more suitable similarity measure for many tasks. The framework presented in [4] is especially interesting since it allows to easily define a kernel for graphs from a vast class of tree kernels, and it constitutes the starting point of our proposal."}, {"heading": "III. ORDERED DECOMPOSITION DAG KERNELS FOR GRAPHS WITH DISCRETE LABELS", "text": "This section briefly recalls the procedure, described in [4], for extracting, given a graph, the tree structures which the kernel we propose is based on.\nLet us first introduce some notation. A graph G = (VG, EG, LG) is a triplet where VG is the set of vertices, EG the set of edges and LG() a function mapping nodes to discrete labels. A graph is undirected if (vi, vj) \u2208 E \u21d4 (vj , vi) \u2208 E, otherwise it is directed. A path p(vi, vj) of length n in a graph G is a sequence of nodes v1, . . . , vn, where v1 = vi, vn = vj and (vk, vk+1) \u2208 E for 1 \u2264 k < n. A cycle is a path for which v1 = vn. A graph is acyclic if it has no cycles. A tree is a directed acyclic graph where each node has exactly one incoming edge, except the root node which has no incoming edge. The root of a tree T is represented by r(T ). The i-th child of a node v \u2208 VT is referred to as chv[i]. The number of children of a node v is referred to as \u03c1(v) (\u03c1 is the maximum out-degree of a tree or graph). A proper subtree rooted at node v comprises v and all its descendants.\nIn order to map the graphs into trees, two intermediate steps are needed:\n1) map the graph G into a multiset of Decomposition DAGs DDG = {DD vi G |vi \u2208 VG}, where DD vi G is formed by\nthe nodes and the directed edges in the shortest path(s) between vi and any vj \u2208 VG (each edge direction is determined according to the visit). Figure 1-1) shows an example of DDG. The decomposition we have defined ensures that isomorphic graphs are represented exactly by the same multiset of DAGs [4]. The computation of the multiset DDG for a graph G requires O(nm) time. 2) Since the kernel we are going to describe in this paper requires the DAG nodes to be ordered, a strict partial order between nodes in DDviG has been defined yielding a Ordered Decomposition DAG ODDviG . Let \u03ba() be a perfect hash function and #, \u2308, \u230b be symbols never appearing in any node label, then let define the function over nodes \u03c0(v) as\ns\nb\ne\nd\nInput Graph\n1 s\ne\nd\nb\ne\ns b d\nb\ns e d\nd\nb\ns\ne\nDAG visits\n\u03c0(v) = L(v) if v is a leaf node and \u03c0(v) = \u03ba ( L(v) \u2308 \u03c0(chv[1])#\u03c0(chv[2]) . . .#\u03c0(chv[\u03c1(v)]) \u230b)\notherwise. The nodes of DDviG can be efficiently sorted according to \u03c0(v) values. The ordering function is defined in such a way to ensure that the swapping of nodes with the same \u03c0() value does not change the feature space representation of the examples [4]. Figure 1-2) shows an example of such ordering. Once \u03ba() values are computed, ordering the sibling of each node in a DD requires \u2211\nv\u2208DD \u03c1(v) log \u03c1(v) \u2264 log \u03c1 \u2211\nv\u2208DD \u03c1(v) = m log \u03c1 steps. Ordering all n DAGs in a DDG requires O(nm log \u03c1) time. 3) Finally, any Ordered DAG (ODD) is mapped into a multiset of trees. Let us define T (vi) as the tree resulting from the visit of ODDviG starting from node vi: the visit returns the nodes reachable from vi in ODD vi G . If a node\nvj can be reached more than once, more occurrences of vj will appear in T (vi). In the following, the notation Tl(vi) indicates a tree visit of depth l. Notice that any node vj of the DAG having l > 1 incoming edges will be duplicated l times in T (vi) (together with all the nodes that are reached from vj in the same visit). Thus, given a DAG ODDviG1 , the total number of nodes of all the tree visits, i.e. \u2211\nv\u2208ODD vi G1\n|T (v)|,\ncan be exponential with respect to |VG1 |. However, such observation does not imply that the complexity of the resulting kernel is exponential since tree visits need not to be explicitly computed to evaluate the kernel function.\nThe Ordered Decomposition DAGs (ODD) kernel is finally defined as:\nK(G1, G2) = \u2211\nOD1\u2208ODD(G1) OD2\u2208ODD(G2)\n\u2211\nv1\u2208VOD1 v2\u2208VOD2\nh \u2211\nj=1\nC(r(Tj(v1)), r(Tj(v2)))\n(1) where C() is a function defining a tree kernel. Among the kernels for trees defined in literature, the one employed in\n3 the paper is the Subtree Kernel (ST) [13], which counts the number of shared proper subtrees between the two input trees and has O(t log t) complexity (here t is the number of nodes of a tree). We recall that the tree visits can be limited to a depth h and that, consequently, the size of the tree visits is constant if we assume \u03c1 and h constant: in this case the ODD kernel with ST as tree kernel has complexity O(n log n) [4]."}, {"heading": "IV. GRAPH KERNELS FOR CONTINUOUS NODE LABELS", "text": "This section shows an easy way to define novel kernels for graphs able to deal with non-discrete node labels. Our strategy is to extend tree kernels to deal with complex node labels. However, the kernel we describe is also able to deal with continuous labels only. We start by describing a straightforward extension of the kernel for discrete labels presented in Section III. We analyze the computational complexity of the resulting kernel, showing that it is not competitive. We then propose an alternative kernel definition. Moreover, in Section V we provide an efficient algorithm for the computation of such kernel.\nWe focus on the ST kernel in the following, but similar extensions can be easily applied to other tree kernels. First a few definitions need to be extended to the continuous domain. In order to simplify the presentation, we will also cast the notation and the following function definitions to the tasks\u2019 domains of the experimental setting. Let us define a graph with continuous attributes as G = (VG, EG, LG, AG) where AG() is a function associating with each node a real-valued vector in Rd. In the following, we will assume the DAGs to be ordered as described in Section III.\nLet us assume kernels on discrete labels KL(v1, v2) and on continuous attributes KA(v1, v2) are given as parameters. Let us first recall the definition of the C() function of ST [13]:\nCST (v1, v2) = \u03bb \u00b7KL(v1, v2) (2)\nif v1 and v2 are leaves, and\nCST (v1, v2) = \u03bb \u00b7KL(v1, v2)\n\u03c1(v1) \u220f\nj=1\nCST (chj [v1], chj [v2]) (3)\nif v1 and v2 have the same number of children, 0 otherwise. Here \u03bb is a kernel parameter.\nA natural way to extend the ST kernel to deal with continuous labels is to introduce the KA() kernel on continuous labels wherever the KL() kernel on discrete labels appears in the formulation. Namely, we can substitute KL(v1, v2) with KL(v1, v2) \u00b7 KA(v1, v2) in eq. (2) and (3). A drawback of this approach is that the kernel value between v1 and v2 may be influenced by the particular ordering. For example, assume that v1 and v2 have the same number of children, each one with the same discrete label, but with a different continuous label. In this case the pairs for which KA is computed, and consequently the value of the kernel evaluation, depends on how identical discrete labels are ordered. Even if we extend the ordering function to consider continuous labels, the selection of pairs would be biased by the ordering function. In fact, ideally we would like to compute the kernel for all those nodes whose discrete labels are identical. In addition, using\neq. (2) and (3) to compute the kernel would result in O(n4) computational complexity for the final graph kernel. One of the advantages of the original ST kernel for trees is that there is an algorithm to compute it in O(n logn) time. Such algorithm relies on the fact that the kernel between two subtrees is computed in constant time, which is no more true when nondiscrete labels are involved.\nIn the remaining of this section, we will define another possibility of extension for the ST kernel that can be computed in a more efficient way. Moreover, the resulting kernel will not depend on the adopted ordering (as long as it is well-defined), making its interpretation more intuitive.\nThe way we propose to extend the C() function of the ST kernel to handle complex node labels is the following: i) if v1 and v2 are leaf nodes then CCST (v1, v2) = \u03bb \u00b7 KL(v1, v2) \u00b7 KA(v1, v2); ii) if v1 and v2 are not leaf nodes and \u03c1(v1) = \u03c1(v2) then\nCCST (v1, v2) = KA(v1, v2) \u00b7 CST (v1, v2), (4)\n0 otherwise. Note that the rightmost quantity on the right-hand side of the equation is the original CST function, i.e. it does not consider continuous labels. Thus, the kernel for complex node labels is applied only to the root node of the tree-features. However, if CST (v1, v2) > 0 then CST (v\u20321, v \u2032 2) > 0 for any v \u2032 1 descendant of v1 and v\u20322 descendant of v2. As a consequence, the kernel KA() is evaluated on all the descendants of v1 and v2. The kernel in eq. (1), instantiated with the tree kernel in eq. (4), is positive semidefinite since it is a finite product of positive semidefinite kernels, and the ordering function adopted for the children ordering is well-defined. We call it ODDCLST kernel. In order to instantiate to the kernel employed in our experiments, we define the kernel on discrete node labels as KL(v1, v2) = \u03b4(L(v1), L(v2)) (where \u03b4 is Kronecker\u2019s delta funcion)1 and the kernel on vectorial attributes as the gaussian kernel: KA(v, v\u2032) = e\u2212\u03b2||A(v)\u2212A(v\n\u2032)||2 , where \u03b2 is a parameter. Eq.(4) has the only purpose to show how the computation of the ST kernel changes from the discrete to the complex node label domain. The algorithm we are going to use to compute the kernel is more efficient than the direct computation of the closed form, and is shown in the following section."}, {"heading": "V. EFFICIENT IMPLEMENTATION WITH FEATUREMAPS", "text": "In this section, we provide an efficient algorithm to compute the ODDCLST kernel presented in Section IV. The kernel has been implemented in Python, thus the algorithms are (simplified) snippets of the code. Algorithm 1 computes the FeatureMap of a graph G. FeatureMap is an HashMap that indicizes all the proper subtrees that appear in the graph considering only the discrete node labels. Each subtree encoded by a value x in the FeatureMap, has associated another HashMap containing all the continuos attribute vectors of each subtree encoded by the same value x in the FeatureMap. We recall that the subtree features do not consider the continuous labels, thus the same subtree may appear multiple times in the same graph.\n1In the case of domains of graphs where discrete labels are not defined, the degree of each node can be considered as discrete node label.\n4 1 def computeFeatureMap(G,h) Data: G= a graph Data: h= maximum depth of the considered structures Result: FeatureMap={subtreeID:{veclabels:freq}} Result: SizeMap={subtreeID:size} 2 DDs=computeDecompositionDAGs(G,h); 3 ODDs=order(DDs); 4 FeatureMap={}; 5 for ODD in ODDs do 6 for v in topologicalSort(ODD) do 7 for j in 0. . .h do 8 subtreeID=encode(Tj (v)); 9 SizeMap[subtreeID]=|Tj (v)|;\n10 if subtreeID not in FeatureMap then 11 FeatureMap[subtreeID]={v:1}; 12 else 13 if v not in FeatureMap[subtreeID] then 14 FeatureMap[subtreeID][v]=1; 15 else 16 FeatureMap[subtreeID][v]+=1; 17 return FeatureMap, SizeMap\nAlgorithm 1: Sketch of an algorithm to compute the FeatureMap of a graph. The notation is Python-style: {} is an HashMap, and the in operator applied to an HashMap performs the lookup of the element in it.\nEach attribute vector has associated its frequency. Aside from hash collisions, only subtrees with identical structures and discrete labels are encoded by the same hash value. The value of CST () for two subtrees encoded by the same hash value is CST (v1, v2) = \u03bb|T (v1)| [13], thus we only need to know the size of the subtree. Let us now analyze the computational complexity of the algorithm. Lines 2-3 require O(mn) and O(nm log \u03c1) time, respectively (see Section 3). Since the nodes are sorted in inverse topological order, the encoding of line 8 can be computed with a time complexity of O(\u03c1) [4].Let H \u2264 \u230a\u03c1\nh+1+1 \u03c1\u22121 \u230b \u2264 n be the average number of nodes in a\nDD (equivalently, nH is the notal number of nodes in all the DDs). Lines 8-16 insert a single feature in the FeatureMap. The total number of features generated from a graph G is then nH(h+1) \u2264 (h+1)n2 (lines 5-7).The (amortized) cost of inserting all such features in the FeatureMap is O(nHh). The overall computational complexity of Algorithm 1 is then O(n(m log \u03c1+hH)). Note that Algorithm 1 has to be executed only once per example.\nAlgorithm 2 sketches the code to compute the kernel value between two graphs. Once the FeatureMaps have been computed with Algorithm 1, to calculate the kernel we need to search for matching subtree features in the two FeatureMaps. For any matching subtree feature subtreeID, we need to compute the kernel KA(vi, vj), vi \u2208 VG1 , vj \u2208 VG2 for each pair of vertices that generate the feature subtreeID in the two graphs. The complexity of Algorithm 2 is linear in the number of discrete features (lines 5-6) and quadratic in the lists of vectorial labels associated to each discrete feature. Note that there are at most n different attribute vectors in the original graph (one associated to each node), so each discrete feature can be associated with at most n different vectorial labels. Thus, the distribution of the O(hHn) elements in a FeatureMap (FM1 or FM2) that maximizes the computational complexity is the one\n1 def ODDCLkernel(G1,G2,\u03bb,h) Data: G1,G2=two graphs; \u03bb= a weighting parameter Result: k=the kernel value 2 FM1,SM1=computeFeatureMap(G1 ,h); 3 FM2,SM2=computeFeatureMap(G2 ,h); 4 k=0; 5 for subtreeID in FM1 do 6 if subtreeID in FM2 then\n/* subtreeID is a feature generated\nfrom vertices vi \u2208 G1 and vj \u2208 G2 where CST (vi, vj) 6= 0 */\n7 for vi in FM1[subtreeID] do 8 for vj in FM2[subtreeID] do 9 freq1=FM1[subtreeID][vi];\n10 freq2=FM2[subtreeID][vj ]; 11 size=SM1[subtreeID]; 12 k+=freq1\u00b7freq2\u00b7\u03bbsize \u00b7kA(vi,vj )\n/* CST (vi, vj) = \u03bb size\n*/\n13 return k Algorithm 2: Sketch of an algorithm for the computation of ODDCLST kernel. \u03bb is a kernel parameter.\nhaving O(hH) different discrete features, each one with an associated list of vectorial labels of size O(n). The complexity of Algorithm 2 is then O(hHn2Q(KA)) where Q(KA) is the complexity of the node kernel."}, {"heading": "VI. COMPUTATION SPEEDUP WITH RBF KERNEL APPROXIMATION", "text": "Profiling the execution of Algorithm 2, the most expensive step is the computation of KA() for all the pairs of vectorial labels associated to a subtree feature (lines 7-12). In order to speed-up the kernel computation, we propose to approximate this step. Recently, [14] proposed a method to generate an (approximated) explicit feature space representation for the RBF kernel by Monte Carlo approximation of its Fourier transform. This procedure depends on a parameter D fixing the dimensionality of the resulting approximation, where the higher this number the better the approximation. Note that the resulting kernel, even if it is an approximation, is positive semidefinite.\nHaving this explicit representation, we can substitute the RBF kernel on the original vectorial labels with a linear kernel calculated on the explicit RBF approximation, that we will refer to as \u03c6\u0302DRBF (), i.e.\n\u2211\nvi\n\u2211\nvj\nkRBF (vi, vj) \u2243 \u3008 \u2211\nvi\n\u03c6\u0302DRBF (vi), \u2211\nvj\n\u03c6\u0302DRBF (vj)\u3009.\n(5) It is worth to notice that, with D \u2192 \u221e, (5) becomes an equality. Thus, we can substitute lines 11-17 of Algorithm 1 in order to associate to subtreeID just the sum of the explicit RBF vectors generated from the vectorial labels. The resulting complexity of Algorithm 1 is O(n(m log \u03c1+hHD),while the complexity of Algorithm 2 drops to O(nhHD). Note that, in practice, the computational gain due to approximation might be higher than what the worst-case analysis suggests (See Section VII), because the worst-case scenarios of the two versions of the algorithm are different. In the approximated case, the complexity is independent from the number of\n5\ncontinuous attributes associated with a discrete feature, and the worst case is the one that maximizes the number of discrete features."}, {"heading": "VII. EXPERIMENTS", "text": "In this section, we compare the ODDCLST kernel presented in Section IV and its approximated version presented in Section VI with several state-of-the-art kernels for graphs with continuous labels. After the description of the experimental setup in Section VII-A, we discuss the predictive performance of the different kernels in Section VII-B. In Section VII-C we compare the computational times required by the different kernel calculations."}, {"heading": "A. Experimental setup", "text": "The results presented in this section are referred to an SVM classifier in a process of nested 10-fold cross validation, in which the kernel (and the classifier) parameters are validated using the training dataset only. The experiments have been repeated 10 times (with different cross validation splits), and the average accuracy results with standard deviation are reported. The proposed kernels parameters values have been cross validated from the following sets: h={0, 1, 2, 3}, \u03bb={0.1, 0.3, 0.5, 0.7, 1, 1.2}. for ODDCLApprox the D parameter has been set to 1000 after preliminary experiments, because it seems a good tradeoff between the computational requirements and the quality of the resulting approximation. Note that, for higher valued of D, one can expect to reproduce almost exactly the results of ODDCLST kernel. The SVM C parameter have been selected in the set {0.01, 0.1, 1, 10, 100, 1000, 10000}. All the considered kernels depend on node kernels for continuous vectors and/or discrete node labels. The kernel for continuous attributes has been fixed for all the kernels as KA(v1, v2) = e\u2212\u03bb||A(v1)\u2212A(v2)|| 2 with \u03bb = 1/d and d the size of the vector of attributes A(vi). The kernel for discrete node labels has been fixed to KL(v1, v2) = \u03b4(L(v), L(v\u2032)) where \u03b4 is the Dirac\u2019s delta function. Where discrete labels were not available, the degree of each node has been considered as the node label. Note that this step is not strictly necessary, since if the graphs have no discrete label information, we can just assume all the nodes having the same label. However, agreement on out-degree is an effective way to speed-up computation and increase the discriminativeness of kernels. Following [10], the kernel matrices have been normalized.\nWe tested our method on the (publicly available) datasets from [10]: ENZYMES, PROTEINS and SYNTHETIC. ENZYMES is a set of proteins from the BRENDA database [15]. Each protein is represented as a graph, where nodes correspond to secondary structure elements. Two nodes are connected whenever they are neighbors either in the amino acid sequence or in the 3D space of the protein tertiary structure [1]. Each node has a discrete attribute indicating the structure it belongs to (helix, sheet or turn). Moreover, several chemical and physical measurements can be associated with each node, obtaining a vector-valued attribute associated to each node. Examples of these measurements are the length\nof the secondary structure element in A\u030a, its hydrophobicity, polarity, polarizability etc. The task is the classification of enzymes into one out of 6 EC top-level classes. There are 100 graphs per class in the dataset. The average number of nodes and edges of the graphs is 32.6 and 46.7, respectively. The size of the vectors associated with the nodes is 18. PROTEINS is the dataset from [16]. The proteins are represented as graphs as described above. The task is to distinguish between enzymes and non-enzymes. There are 1113 graphs in the dataset, each one with an average of 39.1 nodes and 72.8 edges. The dimension of the continuous node attributes is 1. SYNTHETIC is a dataset presented in [10]. A random graph with 100 nodes and 196 edges has been generated. Each node has a corresponding continuous label sampled from N (0, 1). Then 2 classes of graphs have been generated with 150 graphs each. Each graph in the first class was generated rewiring 5 edges and permuting 10 node attributes from the original graph, while for each graph in the second class 10 edges and 5 node attributes has been modified. Finally, noise from N (0, 0.452) has been added to each node attribute in both classes."}, {"heading": "B. Experimental results", "text": "In Table I we report the experimental results of the proposed ODDCLST kernel, its approximated version ODDCLApprox presented in Section VI, the base kernel ODDST , and the results from the paper [10], corrected according to the erratum. As a note for the interested reader, for the ENZYMES dataset, we reported the results relative to the symmetrized version available in the erratum. The reported kernels are: the GraphHopper kernel (GH) [10], the connected subgraph matching kernel (CSM) [9] and the shortest path kernel (SP) [3]. All these kernels can deal with continuous labels. For sake of comparison, the results of the Weisfeiler-Lehman kernel (WL) [7] and ODDST [4] kernels, that can deal only with discrete attributes, are reported too.\nIn the ENZYMES dataset, the GH kernel is the one with lower accuracy. Among the competing kernels, the best is SP. The proposed ODDCLST kernel achieves the highest accuracy, while its approximated version is below SP in terms of accuracy. Note however that the computational requirements of SP are prohibitive on this dataset, as it will be detailed later in this section. In this dataset, the WL and the ODDST kernels\n6\nperform poorly, indicating that the information encoded by continuous attributes is relevant to the task. In PROTEINS, WL performs better than the other competitors, suggesting that in this case the continuous labels may be not much informative. However, ODDCLST and ODDCLApprox kernels achieve a slightly higher accuracy, being the kernels with the highest performances, confirming that our proposed kernels are able to extrapolate useful information from continuous labels in a more effective way than the other considered kernels. Before analyzing the results for the SYNTHETIC dataset, we need to draw some considerations. The first reported results on this dataset were affected by a bug. The correct results have been published later, and depicted a scenario where the kernels that did not consider continuous labels performed better than the others. However, it is interesting to evaluate the performance of the different kernels on this dataset because it shows how much a kernel is resistant to noise. In the SYNTHETIC dataset, even if the proposed kernel performs better than other competitors that consider continuous attributes, its performance are not able to achieve the WL and ODDST ones. This means that, for this dataset, the information provided by vectorial labels is basically noise. This fact is more evident when looking at the performance of the other kernels that consider vectorial labels. However, it is clear that the proposed ODDCLST kernel and its approximated version are more tolerant to this kind of noise than competing kernels. Moreover, for the proposed kernel it is possible to consider an additional parameter, to be cross-validated with the others, that indicates if the vectorial labels have to be considered or not. We recall that, without considering vectorial labels, ODDCLST kernel reduces to the ODDST kernel."}, {"heading": "C. Computational times", "text": "The computational times reported in this section refer to the Gram matrix computation of each kernel. Since the kernels are implemented in different languages, we considered for sake of comparison the computational times reported in [10]. We want to point out that the times reported in this section has to be considered just as orders of magnitude. Table II reports such computational times for the different kernels. The proposed ODDCLST kernel is faster than the SP kernel, but it is considerably slower than GraphHopper. On the contrary, its approximated version ODDCLApprox is the fastest kernel, with a significant difference with respect to GraphHopper while being more accurate in all the considered datasets. Interestingly, because of the different substructures considered by the two kernels, GraphHopper kernel have the lowest run-times in ENZYMES and SYNTHETIC, while for ODDCLST PROTEINS is the dataset that requires the lowest computational resources. We can argue that this happens because of the higher number of edges of proteins, that directly influence the sparsity of ODDCLST kernel. On the other hand, ODDCLApprox shows the maximum speedup on the ENZYMES dataset. The speedup obtained by the approximated version is proportional to the number of different discrete features generated by the ODD base kernel, and thus is influenced by the number of different discrete labels in the dataset, in addition to the graph topology."}, {"heading": "VIII. CONCLUSIONS", "text": "In this paper, we have presented an extension to continuous attributes of the ODD kernel framework for graphs. Moreover, we have studied the performances of a continuous attributes graph kernel derived by the ST kernel for trees. Experimental results on reference datasets show that the resulting kernel is both fast to compute and quite effective on all studied datasets, which is not the case for continuous attributes graph kernels presented in literature."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the University of Padova under the strategic project BIOINFOGEN."}], "references": [{"title": "Protein function prediction via graph kernels.", "author": ["K.M. Borgwardt", "C.S. Ong", "S. Sch\u00f6nauer", "S.V.N. Vishwanathan", "A.J. Smola", "H.-P. Kriegel"], "venue": "Bioinformatics (Oxford, England),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Adaptive Structured Pooling for Action Recognition", "author": ["S. Karaman", "L. Seidenari", "S. Ma"], "venue": "BMVA, pp. 1\u201312, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Shortest-Path Kernels on Graphs", "author": ["K. Borgwardt", "H.-P. Kriegel"], "venue": "ICDM, vol. 0. Los Alamitos, CA, USA: IEEE, 2005, pp. 74\u201381.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "A Tree-Based Kernel for Graphs", "author": ["G. Da San Martino", "N. Navarin", "A. Sperduti"], "venue": "Proceedings of the Twelfth SIAM International Conference on Data Mining, 2012, pp. 975\u2013986.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "On Graph Kernels: Hardness Results and Efficient Alternatives", "author": ["T. Gartner", "P. Flach", "S. Wrobel", "T. G\u00e4rtner"], "venue": "16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, ser. LNCS, vol. 2777. Springer Berlin Heidelberg, 2003, pp. 129\u2013143.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast neighborhood subgraph pairwise distance kernel", "author": ["F. Costa", "K. De Grave"], "venue": "ICML. Omnipress, 2010, pp. 255\u2013262.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast subtree kernels on graphs", "author": ["N. Shervashidze", "K. Borgwardt"], "venue": "NIPS, 2009, pp. 1660\u20131668.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient Graph Kernels by Randomization", "author": ["M. Neumann", "N. Patricia", "R. Garnett", "K. Kersting"], "venue": "ECML PKDD, ser. LNCS, vol. 7523. Springer Berlin Heidelberg, 2012, pp. 378\u2013393.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Subgraph matching kernels for attributed graphs", "author": ["N. Kriege", "P. Mutzel"], "venue": "ICML, 2012, pp. 1015\u20131022.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable kernels for graphs with continuous attributes", "author": ["A. Feragen", "N. Kasenburg", "J. Petersen", "M. de Bruijne", "K.M. Borgwardt"], "venue": "NIPS, 2013, pp. 216\u2013224.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Propagation kernels: efficient graph kernels from propagated information", "author": ["M. Neumann", "R. Garnett", "C. Bauckhage", "K. Kersting"], "venue": "Machine Learning, Jul. 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph invariant kernels", "author": ["F. Orsini", "P. Frasconi", "L.D. Raedt"], "venue": "IJCAI, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast Kernels for String and Tree Matching", "author": ["S.V.N. Vishwanathan", "A.J. Smola"], "venue": "NIPS, 2002, pp. 569\u2013576.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, 2009, pp. 1313\u20131320.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "BRENDA, the enzyme database: updates and major new developments.", "author": ["I. Schomburg", "A. Chang", "C. Ebeling", "M. Gremse", "C. Heldt", "G. Huhn", "D. Schomburg"], "venue": "Nucleic Acids Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Distinguishing Enzyme Structures from Non-enzymes Without Alignments", "author": ["P.D. Dobson", "A.J. Doig"], "venue": "Journal of Molecular Biology, vol. 330, no. 4, pp. 771\u2013783, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "However, in many application domains, such as bioinformatics and action recognition, non-discrete node attributes are available [1], [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "However, in many application domains, such as bioinformatics and action recognition, non-discrete node attributes are available [1], [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Two nodes are connected whenever they are neighbors either in the amino acid sequence or in space [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "however the resulting computational times become unfeasible, as in the case of [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "In this paper, we present a new kernel inspired by the graph kernel framework proposed in [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The features induced by the kernel are tree structures extracted from breadth-first visits of a graph (contrary to [1] an edge is only traversed once per visit).", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Early works presented kernels that have to be computed in closed form, such as the random walk kernel [5] or the shortest path kernel [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Early works presented kernels that have to be computed in closed form, such as the random walk kernel [5] or the shortest path kernel [3].", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "State-of-the-art kernels use explicit feature mapping techniques [4], [6], [7], with computational complexities almost linear in the size of the graphs.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "State-of-the-art kernels use explicit feature mapping techniques [4], [6], [7], with computational complexities almost linear in the size of the graphs.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "State-of-the-art kernels use explicit feature mapping techniques [4], [6], [7], with computational complexities almost linear in the size of the graphs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "adopt slower kernels or discretizing/ignoring the continuous attributes of the graphs, the latter approach was usually the preferred one [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 8, "context": "In [9] a kernel for graphs with continuous-valued labels has been presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [10] another more efficient kernel has been presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Very recently, two kernel frameworks able to deal with continuous and vectorial labels have been proposed: in [11] authors propose to use Locality Sensitive Hashing to discretize continuous and vectorial labels, while in [12] a very general framework of graph kernels is proposed.", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "Very recently, two kernel frameworks able to deal with continuous and vectorial labels have been proposed: in [11] authors propose to use Locality Sensitive Hashing to discretize continuous and vectorial labels, while in [12] a very general framework of graph kernels is proposed.", "startOffset": 221, "endOffset": 225}, {"referenceID": 3, "context": "In fact, in [4], [7] it is shown that tree-features can express a more suitable similarity measure for many tasks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "In fact, in [4], [7] it is shown that tree-features can express a more suitable similarity measure for many tasks.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "The framework presented in [4] is especially interesting since it allows to easily define a kernel for graphs from a vast class of tree kernels, and it constitutes the starting point of our proposal.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "This section briefly recalls the procedure, described in [4], for extracting, given a graph, the tree structures which the kernel we propose is based on.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "The decomposition we have defined ensures that isomorphic graphs are represented exactly by the same multiset of DAGs [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "\u03c0(chv[1])#\u03c0(chv[2]) .", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "\u03c0(chv[1])#\u03c0(chv[2]) .", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "The ordering function is defined in such a way to ensure that the swapping of nodes with the same \u03c0() value does not change the feature space representation of the examples [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 12, "context": "the paper is the Subtree Kernel (ST) [13], which counts the number of shared proper subtrees between the two input trees and has O(t log t) complexity (here t is the number of nodes of a tree).", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "We recall that the tree visits can be limited to a depth h and that, consequently, the size of the tree visits is constant if we assume \u03c1 and h constant: in this case the ODD kernel with ST as tree kernel has complexity O(n log n) [4].", "startOffset": 231, "endOffset": 234}, {"referenceID": 12, "context": "Let us first recall the definition of the C() function of ST [13]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "The value of CST () for two subtrees encoded by the same hash value is CST (v1, v2) = \u03bb (v1)| [13], thus we only need to know the size of the subtree.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "Since the nodes are sorted in inverse topological order, the encoding of line 8 can be computed with a time complexity of O(\u03c1) [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 13, "context": "Recently, [14] proposed a method to generate an (approximated) explicit feature space representation for the RBF kernel by Monte Carlo approximation of its Fourier transform.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Following [10], the kernel matrices have been normalized.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "We tested our method on the (publicly available) datasets from [10]: ENZYMES, PROTEINS and SYNTHETIC.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "ENZYMES is a set of proteins from the BRENDA database [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Two nodes are connected whenever they are neighbors either in the amino acid sequence or in the 3D space of the protein tertiary structure [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 15, "context": "PROTEINS is the dataset from [16].", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "SYNTHETIC is a dataset presented in [10].", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "In Table I we report the experimental results of the proposed ODDCLST kernel, its approximated version ODDCLApprox presented in Section VI, the base kernel ODDST , and the results from the paper [10], corrected according to the erratum.", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "The reported kernels are: the GraphHopper kernel (GH) [10], the connected subgraph matching kernel (CSM) [9] and the shortest path kernel (SP) [3].", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "The reported kernels are: the GraphHopper kernel (GH) [10], the connected subgraph matching kernel (CSM) [9] and the shortest path kernel (SP) [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "The reported kernels are: the GraphHopper kernel (GH) [10], the connected subgraph matching kernel (CSM) [9] and the shortest path kernel (SP) [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "For sake of comparison, the results of the Weisfeiler-Lehman kernel (WL) [7] and ODDST [4] kernels, that can deal only with discrete attributes, are reported too.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "For sake of comparison, the results of the Weisfeiler-Lehman kernel (WL) [7] and ODDST [4] kernels, that can deal only with discrete attributes, are reported too.", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "Since the kernels are implemented in different languages, we considered for sake of comparison the computational times reported in [10].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "\u2217: THE TIMES REFERRING TO GH AND SP ARE REPORTED FROM [10]", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "The availability of graph data with node attributes that can be either discrete or real-valued is constantly increasing. While existing kernel methods are effective techniques for dealing with graphs having discrete node labels, their adaptation to nondiscrete or continuous node attributes has been limited, mainly for computational issues. Recently, a few kernels especially tailored for this domain, have been proposed. In order to allieviate the computational problems, the size of the feature space of such kernels tend to be smaller than the ones of the kernels for discrete node attributes. However, such choice might have a negative impact on the predictive performance. In this paper, we propose a graph kernel for complex and continuous nodes\u2019 attributes, whose features are tree structures extracted from specific graph visits. Experimental results obtained on real-world datasets show that the (approximated version of the) proposed kernel is comparable with current state-of-the-art kernels in terms of classification accuracy while requiring shorter running times.", "creator": "LaTeX with hyperref package"}}}