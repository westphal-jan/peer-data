{"id": "1509.05982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2015", "title": "Denoising without access to clean data using a partitioned autoencoder", "abstract": "To solve this problem, we introduce a method in which an autoencoder is trained exclusively with loud data, with examples with and without the signal class of interest. The autoencoder learns a partitioned representation of signal and noise, learning to reconstruct each one individually. We illustrate the method by encoding birdsong audio (abundant in uncontrolled loud data sets) using a revolutionary autoencoder.", "histories": [["v1", "Sun, 20 Sep 2015 09:03:48 GMT  (1112kb,D)", "http://arxiv.org/abs/1509.05982v1", null], ["v2", "Tue, 22 Sep 2015 20:51:05 GMT  (1112kb,D)", "http://arxiv.org/abs/1509.05982v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["dan stowell", "richard e turner"], "accepted": false, "id": "1509.05982"}, "pdf": {"name": "1509.05982.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Richard E. Turner"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "An autoencoder (AE) is a neural network trained in unsupervised fashion, to encode its input to some latent representation and to decode that representation to a faithful reconstruction of its input. The autoencoder can then be used as a codec, or to convert data to its latent representation for downstream processing such as classification. The denoising autoencoder (DAE) is a variant of this in which the inputs are combined with some corruption (such as additive noise or masking), and the system is trained to recover the clean, de-noised data [1]. The DAE training scheme can be used in denoising applications, and is also a popular way to encourage the autoencoder to learn a more meaningful latent representation of the data. Autoencoders including the DAE have yielded leading results in recent years in deep learning for signal processing [1, 2].\nHowever, there is a significant problem with the DAE approach which hampers its use in practical applications: it may often be impossible to supply truly clean data. This is common in our application example\u2014natural sound recordings\u2014but also for video, image and audio applications across many domains. In fact, objects/events are often sparsely represented in data while background and other noise are densely represented, meaning that it is often easy to provide \u201cnoise-only\u201d examples while difficult to provide \u201cnoise-free\u201d examples.\nar X\niv :1\n50 9.\n05 98\n2v 1\n[ cs\n.N E\n] 2\nIn this paper we propose an alternative approach to train an AE so that it can perform denoising, given training data which can only be weakly labelled as \u201cnoise-only\u201d or \u201cnoise and possible signal\u201d. The system learns a partitioned latent representation, with identifiable noise and signal coefficients, and can then perform denoising and/or recover a signal-only latent representation for further analysis. The method is general-purpose; we illustrate it here with an application to denoising birdsong audio spectrograms."}, {"heading": "2 Partitioned autoencoders", "text": "A standard AE learns a function of the form X\u0302 = g(f(X)) where X is an input datum (a matrix, in this paper), and the autoencoder is composed of encoder f(\u00b7) and decoder g(\u00b7). The DAE learns a function X\u0302 = g(f(u(X))) where u(\u00b7) is a stochastic noise corruption process. The training objective is such that X\u0302 is encouraged to be as close to X as possible, often \u2211 i \u2016Xi \u2212 X\u0302i\u20162 where || \u00b7 || is the Frobenius norm and the sum is taken over a minibatch of training data. (A minibatch is a small subset of training data used for one iteration of stochastic gradient descent [SGD].) Given this objective, in practice a DAE will learn to denoise its input to the extent that X itself is clean, having no incentive to \u201covershoot\u201d and remove any noise that may be intrinsic to the original X. The latent representation output from f(\u00b7) parametrises the manifold on which the reconstructed signal data lie. Information about the noise present in each datum is not captured, except implicitly as X\u2212 X\u0302 if X\u0302 is a good estimate.\nMany equivalent parametrisations of the manifold may be possible, some allowing a more semantic interpretation of each latent coefficient than others, and the standard AE or DAE does not distinguish among these parametrisations. There has been recent interest in adapting the training schemes of autoencoders such that the latent representation is explicitly semantic, capturing attributes of the input in specific subsets of the latent variables [3, 4]. We refer to these as partitioned autoencoders since the latent variables are partitioned into subsets which are treated differently from each other during training. Crucially, in this prior work the training scheme relies heavily on the existence of large structured datasets: in [4] a balanced dataset of labelled digit images; in [3] a dataset of faces constructed through systematic variation of attributes such as pose and lighting. Without such known structure in the training data, their proposed training schemes will be either impossible to apply, or biased by the presence of unbalanced or correlated factors in the training data.\nOur present motivation is to learn a denoising representation, trained using data which does not contain truly clean examples. If we can develop a scheme that learns to represent both signal and noise, but partitioning them into separate latent coefficients, then we will be able to perform denoising or further analysis by using only the \u201cforeground\u201d (signal) coefficients and setting the remainder to zero. The scheme of [3] would be appropriate if the SNR of each training example were known and systematically varied, but in uncontrolled datasets this information is rarely available. Instead we propose a scheme based\non the observation that many scenarios consist of sparsely-present foreground and densely-present background, and so noise-only training data is much easier to come by than signal-only.\nOur training scheme is based on a standard autoencoder reconstruction objective, augmented with a structured regularisation of the latent variables. In order to encourage the model to use the background latents to represent noise and never to represent signal, we add a soft regulariser that penalises foreground latent activation for the noise-only examples. For each training example X associated with a weak label y taking the value 1 if the example is a \u201cnoise-only\u201d example and 0 otherwise, we train an autoencoder by minimising the following loss function:\nl(X, y) = \u2016X\u2212 X\u0302\u20162 + \u03bby C\u0304 \u2016C f(X)\u20162\nwhere X\u0302 = g(f(X)), \u03bb is a regularisation coefficient, represents elementwise multiplication, C is a masking matrix containing 1 for latents which should represent foreground and 0 otherwise, and C\u0304 the mean value of C. We will construct our training minibatches with a fixed proportion of noise-only items at each iteration. The value used for \u03bb will be relatively large, to impose a soft constraint pushing a subset of the latent values to zero in the case of noise-only items (Figure 1, Figure 2). This encourages the learned representation to use the non-regularised latents for the foreground signal. The parameters of f(\u00b7) and g(\u00b7) will be optimised through SGD. Once trained, denoising is achieved by reconstructing using only the foreground latents, i.e. setting the others to zero (Figure 3).\nFigure 2 emphasises that the proportion of latents dedicated to foreground vs. background, and the balance of \u201csignal-plus-noise\u201d and \u201cnoise-only\u201d examples in a minibatch, are independent configuration choices. We tend to reserve 25% of latents for background, and 25% of a minibatch as noise-only; in our evaluation we will evaluate the impact of varying the balance of latents.\nNote that the regularisation scheme is asymmetric: it encourages some latents to zero in certain cases, but it does not prevent them from being zero in the other cases. In principle the system could simply never use those latents; however the reconstruction cost encourages the system to use them to improve reconstruction in cases where it has that freedom. The asymmetry also means that the important aspect of data labelling is to identify \u201cnoise-only\u201d items with high precision. If some \u201cnoise-only\u201d items are not labelled as such, the system is free to represent them without making use of those latents. The scheme can thus be used on datasets which are too large to label completely, but in which a subset of noise-only examples can be identified."}, {"heading": "3 Convolutional partitioned autoencoder for au-", "text": "dio spectrograms\nThe training scheme can be applied to various autoencoder architectures. Convolutional neural networks have recently proven to be powerful for many tasks,\nwhile relatively easy to train because they have far fewer free parameters than an equivalent fully-connected network [2, 5]. In our application we aim to extract information from audio spectrograms, indexed by time and frequency. We will thus use an autoencoder which is convolutional in time and fully-connected in frequency, as is standard for recent neural network audio analysis [5]. Given an input matrix X indexed by discrete time n and frequency h, we define our encoding function to be\nf(X) = mp(r(Wc ? (X\u2212 \u00b5\u0302)/\u03c3\u0302))\nwhere Wc is a tensor of coding weights indexed by time m frequency h and latent index k, r(\u00b7) is a rectified linear unit nonlinearity, mp(\u00b7) represents the max-pooling operation applied along the time axis, \u00b5\u0302 and \u03c3\u0302 are the frequencywise means and standard deviations estimated from the training set and used for normalisation, and ? indicates one-dimensional convolution as follows:\n(A ? B)n,k = M\u2211 m=1 H\u2211 h=1 am,h,kbn\u2212m,h n \u2208 [1, N ], k \u2208 [1,K]\nresulting in a matrix indexed by time n and latent index k. Our decoding function is\ng(f(X)) = Wd ? mp\u22121(f(X))\nwhere Wd is a tensor of decoding weights, and mp\u22121(\u00b7) is the inverse of the max-pooling operation (the approximate inverse, as the non-maximal values are reconstituted by zeros).\nIn this temporally convolutional architecture, the mask matrix C is implemented as identical frame-wise mask matrices, with k indexing the set of latent time series.\nOur data will be non-negative. Data normalisation at the input is important for effective training, hence the use of \u00b5\u0302 and \u03c3\u0302 [6]. However we do not undo the normalisation at the decoder outputs: the non-negative target and the rectifier then give the property that the decoder learns a non-negative parts-based reconstruction. We do not use bias units, as we found the resulting system easier to train (cf. [7, 8]).\nFor the evaluation that follows, fixed configuration details are: input spectrograms have 512 time frames and 32 frequency bins, and we use 32 latent variables. Convolution filters have a length of 9 time frames. Our max-pooling downsamples the time axis by a factor of 16. We train the network using AdaDelta to control the SGD learning rates [9]. We do not use dropout. We initialise the tensor of filters as a set of K random orthogonal unit vectors of length MH, reshaping this to the tensor of shape M \u00d7H \u00d7K (cf. [10]).\nIn this study we explore only a single-layer autoencoder. Our approach applies straightforwardly to a deep autoencoder, and a deeper system would be expected to have a broader ability to generalise."}, {"heading": "4 Evaluation", "text": "We test our approach using a task to denoise birdsong audio spectrograms (Figure 4). As foreground we use recordings of chiff chaff (similarly to [11, 12]). For evaluation purposes we wish to add a background that offers a substantial test: it should be diverse, nonstationary and contain significant energy in the same frequency band as the birdsong. After considering many available options we settled on recorded restaurant noise, which contains multi-speaker human speech as well as diverse percussive sound events. We add this background\nnoise, both to create a known amount of \u201cintrinsic\u201d noise in the examples, and to create \u201cnoise-only\u201d examples.\nOur system is implemented in the Theano framework [13] making use of GPU processing. To create a diverse training/validation dataset within the constraints of limited GPU memory, we take advantage of the approximate additive nature of audio spectrograms as follows. In each experiment we load 30 seconds of signal, of intrinsic noise and extrinsic noise, and store their spectrograms to the GPU. Then to generate each \u201csignal-plus-noise\u201d datum we randomly sample 1.5 second segments from each of the three sources and mix them. To generate a \u201cnoise only\u201d datum we sample only from the extrinsic noise source. This creates a large generative dataset within limited memory. We also experimented with replacing the 30 second source material regularly throughout training but this made little difference, so we do not employ that in the present results.\nOur audio sources use sample rate 22.05 kHz, analysed using 128 bin FFTs with 50% hop, and the frequency axis then reduced to 32 bins of interest for the birdsong (1.7\u20137.2 kHz). Within this frequency band, we added intrinsic noise to give an SNR -10 dB, then extrinsic noise to give an SNR of -30 dB. The extrinsic noise corresponds to the \u201cnoise-only\u201d items that would be provided in practice. The intrinsic noise is added for evaluation only, to judge whether the system can remove it (Figure 5). We study two cases where the intrinsic and extrinsic noise sources are matched or unmatched.\nWe test our partitioned AE at various settings for the proportion of latents regularised, and also a standard DAE using the same training data, configured to use the extrinsic noise as the additive corruption u(\u00b7) in the DAE training process. In all cases we fix \u03bb = 0.75 and train with minibatches of size 16, and 106 iterations of SGD.\nResults evaluated on separately-sampled validation data (Figure 6) show a number of interesting properties. Firstly, all methods perform best against their\nexplicit objective in every case: i.e. their best SNR is the one relating to their objective function (simple reconstruction for the proposed system, reconstruction of the partly-clean input for the DAE). However, the statistic of interest is reconstruction of the truly-clean spectrogram. In matched conditions (upper plot), the proposed system strongly outperforms the DAE on this measure. Its strong performance is stable across a broad range of settings for the proportion of latents regularised (all except the extremes). The poor results at the setting with regularisation of all latents (100%) confirm that the regularisation used is strong: if applied to all latents, it causes underfitting. It is not merely the presence of regularisation that improves the results, but the partitioning scheme.\nIn unmatched conditions (lower plot) the strong performance is not sustained. The DAE continues to perform well at its partial-denoising task, but our proposed DAE fails to generalise across diverse background recordings. It seems likely that this is due to the small size and depth of the current setup: a single-layer autoencoder with only 32 convolutional filters has limited ability to approximate arbitrary functions. The strong performance in matched conditions suggests further study of the method with deeper networks. However, note that matched conditions are common in applications such as ours, where the noise-only examples can be taken from the main field recording sessions."}, {"heading": "5 Conclusions", "text": "We have introduced a partitioned autoencoder that can learn to denoise data with better fidelity than a standard DAE, in the common practical case where no noise-free examples are available for training. In matched conditions this partitioned scheme makes better use of the available data than a DAE. Unlike a DAE, our partitioned autoencoder learns to represent the signal and the noise content, and can reconstruct either or both. This may be part of its advantage over a standard DAE, which does not learn to represent the noise content. Further work will explore deeper/wider architectures to improve the generality, and use the representations to support classification and other tasks."}, {"heading": "6 Acknowledgments", "text": "We gratefully thank Pavel Linhart for recording the chiff chaff bird sounds used in this study."}], "references": [{"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1503.03167, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering hidden factors of variation in deep networks", "author": ["B. Cheung", "J.A. Livezey", "A.K. Bansal", "B.A. Olshausen"], "venue": "arXiv preprint arXiv:1412.6583, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Proc ICASSP, 2014, pp. 6964\u20136968.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["T.L. Paine", "P. Khorrami", "W. Han", "T.S. Huang"], "venue": "arXiv preprint arXiv:1412.6597, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["R. Memisevic", "K. Konda", "D. Krueger"], "venue": "arXiv preprint arXiv:1402.3337, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "arXiv preprint arXiv:1312.6120, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Segregating event streams and noise with a Markov renewal process model", "author": ["D. Stowell", "M.D. Plumbley"], "venue": "Journal of Machine Learning Research, vol. 14, pp. 1891\u20131916, 2013, preprint arXiv:1211.2972.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1891}, {"title": "Improved multiple birdsong tracking with distribution derivative method and Markov renewal process clustering", "author": ["D. Stowell", "S. Mu\u0161evi\u010d", "J. Bonada", "M.D. Plumbley"], "venue": "Proceedings of the International Conference on Audio and Acoustic Signal Processing (ICASSP), 2013, preprint arXiv:1302.3642.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Proceedings of NIPS 2012, 2012. 10", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "The denoising autoencoder (DAE) is a variant of this in which the inputs are combined with some corruption (such as additive noise or masking), and the system is trained to recover the clean, de-noised data [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 0, "context": "Autoencoders including the DAE have yielded leading results in recent years in deep learning for signal processing [1, 2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "Autoencoders including the DAE have yielded leading results in recent years in deep learning for signal processing [1, 2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "There has been recent interest in adapting the training schemes of autoencoders such that the latent representation is explicitly semantic, capturing attributes of the input in specific subsets of the latent variables [3, 4].", "startOffset": 218, "endOffset": 224}, {"referenceID": 3, "context": "There has been recent interest in adapting the training schemes of autoencoders such that the latent representation is explicitly semantic, capturing attributes of the input in specific subsets of the latent variables [3, 4].", "startOffset": 218, "endOffset": 224}, {"referenceID": 3, "context": "Crucially, in this prior work the training scheme relies heavily on the existence of large structured datasets: in [4] a balanced dataset of labelled digit images; in [3] a dataset of faces constructed through systematic variation of attributes such as pose and lighting.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Crucially, in this prior work the training scheme relies heavily on the existence of large structured datasets: in [4] a balanced dataset of labelled digit images; in [3] a dataset of faces constructed through systematic variation of attributes such as pose and lighting.", "startOffset": 167, "endOffset": 170}, {"referenceID": 2, "context": "The scheme of [3] would be appropriate if the SNR of each training example were known and systematically varied, but in uncontrolled datasets this information is rarely available.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "while relatively easy to train because they have far fewer free parameters than an equivalent fully-connected network [2, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 4, "context": "while relatively easy to train because they have far fewer free parameters than an equivalent fully-connected network [2, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 4, "context": "We will thus use an autoencoder which is convolutional in time and fully-connected in frequency, as is standard for recent neural network audio analysis [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "[7, 8]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[7, 8]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "We train the network using AdaDelta to control the SGD learning rates [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As foreground we use recordings of chiff chaff (similarly to [11, 12]).", "startOffset": 61, "endOffset": 69}, {"referenceID": 10, "context": "As foreground we use recordings of chiff chaff (similarly to [11, 12]).", "startOffset": 61, "endOffset": 69}, {"referenceID": 11, "context": "Our system is implemented in the Theano framework [13] making use of GPU processing.", "startOffset": 50, "endOffset": 54}], "year": 2017, "abstractText": "Training a denoising autoencoder neural network requires access to truly clean data, a requirement which is often impractical. To remedy this, we introduce a method to train an autoencoder using only noisy data, having examples with and without the signal class of interest. The autoencoder learns a partitioned representation of signal and noise, learning to reconstruct each separately. We illustrate the method by denoising birdsong audio (available abundantly in uncontrolled noisy datasets) using a convolutional autoencoder.", "creator": "TeX"}}}