{"id": "1509.06461", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "Deep Reinforcement Learning with Double Q-Learning", "abstract": "It is well known that the popular Q-Learning algorithm overestimates action values under certain conditions. So far, it was not known whether such overestimates are common in practice, whether this impairs performance and whether they can generally be prevented. In this article, we answer all these questions positively. In particular, we show that the latest DQN algorithm, which connects Q-Learning to a deep neural network, suffers from significant overestimates in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-Learning algorithm, which was introduced in a tabular environment, can be generalized to work with large-scale functional approximations. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimates, as assumed, but also leads to significantly better performance in several games.", "histories": [["v1", "Tue, 22 Sep 2015 04:40:22 GMT  (945kb,D)", "http://arxiv.org/abs/1509.06461v1", null], ["v2", "Fri, 20 Nov 2015 15:20:50 GMT  (1043kb,D)", "http://arxiv.org/abs/1509.06461v2", "AAAI 2016"], ["v3", "Tue, 8 Dec 2015 21:19:16 GMT  (1043kb,D)", "http://arxiv.org/abs/1509.06461v3", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hado van hasselt", "arthur guez", "david silver"], "accepted": true, "id": "1509.06461"}, "pdf": {"name": "1509.06461.pdf", "metadata": {"source": "CRF", "title": "DEEP REINFORCEMENT LEARNING WITH DOUBLE Q-LEARNING", "authors": ["HADO VAN HASSELT"], "emails": [], "sections": [{"heading": null, "text": "The goal of reinforcement learning (Sutton and Barto, 1998) is to learn good policies for sequential decision problems, by optimizing a cumulative future reward signal. Q-learning (Watkins, 1989) is one of the most popular reinforcement learning algorithms, but it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values.\nIn previous work, overestimations have been attributed to insufficiently flexible function approximation (Thrun and Schwartz, 1993) and noise (van Hasselt, 2010, 2011). In this paper, we unify these views and show overestimations can occur when the action values are inaccurate, irrespective of the source of approximation error. Of course, imprecise value estimates are the norm during learning, which indicates that overestimations may be much more common than previously appreciated.\nIt is an open question whether, if the overestimations do occur, this negatively affects performance in practice. Overoptimistic value estimates are not necessarily a problem in and of themselves. If all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse. Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known exploration technique (Kaelbling et al., 1996). If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. Thrun and Schwartz (1993) give specific examples in which this leads to suboptimal policies, even asymptotically.\nTo test whether overestimations occur in practice and at scale, we investigate the performance of the recent DQN algorithm (Mnih et al., 2015). DQN combines Q-learning with a flexible\ndeep neural network and was tested on a varied and large set of deterministic Atari 2600 games, reaching human-level performance on many games. In some ways, this setting is a bestcase scenario for Q-learning, because the deep neural network provides flexible function approximation with the potential for a low asymptotic approximation error, and the determinism of the environments prevents the harmful effects of noise. Perhaps surprisingly, we show that even in this comparatively favorable setting DQN sometimes substantially overestimates the values of the actions.\nWe show that the idea behind the Double Q-learning algorithm (van Hasselt, 2010), which was first proposed in a tabular setting, can be generalized to work with arbitrary function approximation, including deep neural networks. We use this to construct a new algorithm we call Double DQN. We then show that this algorithm not only yields more accurate value estimates, but leads to much higher scores on several games. This demonstrates that the overestimations of DQN were indeed leading to poorer policies and that it is beneficial to reduce them. In addition, by improving upon DQN we obtain state-of-the-art results on the Atari domain."}, {"heading": "BACKGROUND", "text": "To solve sequential decision problems we can learn estimates for the optimal value of each action, defined as the expected sum of future rewards when taking that action and following the optimal policy thereafter. Under a given policy \u03c0, the true value of an action a in a state s is\nQ\u03c0(s, a) \u2261 E [R1 + \u03b3R2 + . . . | S0 = s,A0 = a, \u03c0] , where \u03b3 \u2208 [0, 1] is a discount factor that trades off the importance of immediate and later rewards. The optimal value is then Q\u2217(s, a) = max\u03c0 Q\u03c0(s, a). An optimal policy is easily derived from the optimal values by selecting the highest-valued action in each state.\nEstimates for the optimal action values can be learned using Q-learning (Watkins, 1989), a form of temporal difference learning (Sutton, 1988). Most interesting problems are too large to learn all action values in all states separately. Instead, we can learn a parameterized value function Q(s, a;\u03b8t). The standard Q-learning update for the parameters after taking action At in state St and observing the immediate reward Rt+1 and resulting state St+1 is then\n(1) \u03b8t+1 = \u03b8t + \u03b1(Y Q t \u2212Q(St, At;\u03b8t))\u2207\u03b8tQ(St, At;\u03b8t) .\nwhere \u03b1 is a scalar step size and the target Y Qt is defined as\n(2) Y Qt \u2261 Rt+1 + \u03b3max a Q(St+1, a;\u03b8t) .\nThis update resembles stochastic gradient descent, updating the current value Q(St, At;\u03b8t) towards a target value Y Q t .\nDeep Q Networks. A deep Q network (DQN) is a multi-layered neural network that for a given state s outputs a vector of action valuesQ(s, \u00b7 ;\u03b8), where \u03b8 are the parameters of the network. For an n-dimensional state space and an action space containing m\n1\nar X\niv :1\n50 9.\n06 46\n1v 1\n[ cs\n.L G\n] 2\n2 Se\np 20\n15\nactions, the neural network is a function from Rn to Rm. Two important ingredients of the DQN algorithm as proposed by Mnih et al. (2015) are the use of a target network, and the use of experience replay. The target network, with parameters \u03b8\u2212, is the same as the online network except that its parameters are copied every \u03c4 steps from the online network, so that then \u03b8\u2212t = \u03b8t, and kept fixed on all other steps. The target used by DQN is then\n(3) Y DQNt \u2261 Rt+1 + \u03b3max a Q(St+1, a;\u03b8 \u2212 t ) .\nFor the experience replay (Lin, 1992), observed transitions are stored for some time and sampled uniformly from this memory bank to update the network. Both the target network and the experience replay dramatically improve the performance of the algorithm (Mnih et al., 2015).\nDouble Q-learning. The max operator in standard Q-learning and DQN, in (2) and (3), uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values, resulting in overoptimistic value estimates. To prevent this, we can decouple the selection from the evaluation. This is the idea behind Double Q-learning (van Hasselt, 2010).\nIn the original Double Q-learning algorithm, two value functions are learned by assigning each experience randomly to update one of the two value functions, such that there are two sets of weights, \u03b8 and \u03b8\u2032. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. For a clear comparison, we can first untangle the selection and evaluation in Q-learning and rewrite its target (2) as\nY Qt = Rt+1 + \u03b3Q(St+1, argmax a Q(St+1, a;\u03b8t);\u03b8t) .\nThe Double Q-learning error can then be written as\n(4) Y DoubleQt \u2261 Rt+1 + \u03b3Q(St+1, argmax a Q(St+1, a;\u03b8t);\u03b8 \u2032 t) .\nNotice that the selection of the action, in the argmax, is still due to the online weights \u03b8t. This means that, as in Q-learning, we are still estimating the value of the greedy policy according to the current values, as defined by \u03b8t. However, we use the second set of weights \u03b8\u2032t to fairly evaluate the value of this policy. This second set of weights can be updated symmetrically by switching the roles of \u03b8 and \u03b8\u2032."}, {"heading": "OVEROPTIMISM DUE TO ESTIMATION ERRORS", "text": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121m+1 , where m is the number of actions. In addition, Thrun and Schwartz give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies, and show the overestimations manifest themselves in a small toy problem when using function approximation. Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning as a solution.\nIn this section we demonstrate more generally that estimation errors of any kind can induce an upward bias, regardless of whether these errors are due to environmental noise, function\napproximation, non-stationarity, or any other source. This is important, because in practice any method will incur some inaccuracies during learning, simply due to the fact that the true values are initially unknown.\nThe result by Thrun and Schwartz (1993) cited above gives an upper bound to the overestimation for a specific setup, but it is also possible, and potentially more interesting, to derive a lower bound.\nTheorem 1. Consider a state s in which all the true optimal action values are equal at Q\u2217(s, a) = V\u2217(s) for some V\u2217(s). Let Qt be arbitrary value estimates that are on the whole unbiased in the sense that \u2211 a(Qt(s, a) \u2212 V\u2217(s)) = 0, but that are not\nall correct, such that 1m \u2211 a(Qt(s, a) \u2212 V\u2217(s))2 = C for some C > 0, where m \u2265 2 is the number of actions in s. Under these conditions, maxaQt(s, a) \u2265 V\u2217(s)+ \u221a C\nm\u22121 . This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learning estimate is zero. (Proof in appendix.)\nNote that we did not need to assume that estimation errors for different actions are independent. This theorem shows that even if the value estimates are on average correct, estimation errors of any source can drive the estimates up and away from the true optimal values.\nThe lower bound in Theorem 1 decreases with the number of actions. This is an artifact of considering the lower bound, which requires very specific values to be attained. More typically, the overoptimism increases with the number of actions as shown here:\n2 4 8 16 32 64 128 256 512 1024\nnumber of actions\n0.0\n0.5\n1.0\n1.5\ner ro\nr\nmaxaQ(s, a)\u2212 V\u2217(s) Q\u2032(s, argmaxaQ(s, a))\u2212 V\u2217(s)\nThe orange bars show the bias in a single Q-learning update when the action values are Q(s, a) = V\u2217(s) + a and the errors { a}ma=1 are independent standard normal random variables. The second set of action values Q\u2032, used for the blue bars, was generated identically and independently. All bars are the average of 100 repetitions. Q-learning\u2019s overestimations here indeed increase with the number of actions. Double Q-learning is unbiased. As another example, if for all actions Q\u2217(s, a) = V\u2217(s) and the estimation errorsQt(s, a)\u2212V\u2217(s) are uniformly random in [\u22121, 1], then the overoptimism is m\u22121m+1 . (Proof in appendix.)\nWe now turn to function approximation and consider a realvalued continuous state space with 10 discrete actions in each state. For simplicity, the true optimal action values in this example depend only on state so that in each state all actions have the same true value. These true values are shown in the left column of plots in Figure 1 (purple lines) and are defined as either Q\u2217(s, a) = sin(s) (top row) or Q\u2217(s, a) = 2 exp(\u2212s2) (middle and bottom rows). The left plots also show an approximation for a single action (green lines) as a function of state as well as the samples the estimate is based on (green dots). The estimate is a ddegree polynomial that is fit to the true values at sampled states, where d = 6 (top and middle rows) or d = 9 (bottom row). The\nsamples match the true function exactly: there is no noise and we assume we have ground truth for the action value on these sampled states. The approximation is inexact even on the sampled states for the top two rows because the function approximation is insufficiently flexible. In the bottom row, the function is flexible enough to fit the green dots, but this reduces the accuracy in unsampled states. Notice that the sampled states are spaced further apart near the left side of the left plots, resulting in larger estimation errors. In many ways this is a typical learning setting, where at each point in time we only have limited data.\nThe middle column of plots in Figure 1 shows estimated action value functions for all 10 actions (green lines), as functions of state, along with the maximum action value in each state (black dashed line). Although the true value function is the same for all actions, the approximations differ because we have supplied different sets of sampled states.1 The maximum is often higher than the ground truth shown in purple on the left. This is confirmed in the right plots, which shows the difference between the black and purple curves in orange. The orange line is almost always positive, indicating an upward bias. The right plots also show the estimates from Double Q-learning in blue2, which are on average much closer to zero. This demonstrates that Double Q-learning indeed can successfully reduce the overoptimism of Q-learning.\nThe different rows in Figure 1 show variations of the same experiment. The difference between the top and middle rows is the true value function, demonstrating that overestimations\n1Each action-value function is fit with a different subset of integer states. States \u22126 and 6 are always included to avoid extrapolations, and for each action two adjacent integers are missing: for action a1 states \u22125 and \u22124 are not sampled, for a2 states \u22124 and \u22123 are not sampled, and so on. This causes the estimated values to differ.\n2We arbitrarily used the samples of action ai+5 (for i \u2264 5) or ai\u22125 (for i > 5) as the second set of samples for the double estimator of action ai.\nare not an artifact of a specific true value function. The difference between the middle and bottom rows is the flexibility of the function approximation. In the left-middle plot, the estimates are even incorrect for some of the sampled states because the function is insufficiently flexible. The function in the bottom-left plot is more flexible but this causes higher estimation errors for unseen states, resulting in higher overestimations. This is important because flexible parametric function approximators are often employed in reinforcement learning (see, e.g., Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015).\nIn contrast to van Hasselt (2010) we did not use a statistical argument to find overestimations, the process to obtain Figure 1 is fully deterministic. In contrast to Thrun and Schwartz (1993), we did not rely on inflexible function approximation with irreducible asymptotic errors; the bottom row shows that a function that is flexible enough to cover all samples leads to high overestimations.\nIn the examples above, overestimations occur even when assuming we have samples of the true action value at certain states. The value estimates can further deteriorate if we bootstrap off of action values that are already overoptimistic, since this causes overestimations to propagate throughout our estimates. Although uniformly overestimating values might not hurt the resulting policy, in practice overestimation errors will differ for different states and actions. Overestimation combined with bootstrapping then has the pernicious effect of propagating the wrong relative information about which states are more valuable than others, directly affecting the quality of the learned policies.\nThe overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and Lo\u030brincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values. Conversely, the overestimations discussed here occur only\nafter updating, resulting in overoptimism in the face of apparent certainty. This was already observed by Thrun and Schwartz (1993), who noted that, in contrast to optimism in the face of uncertainty, these overestimations actually can impede learning an optimal policy. We will see this negative effect on policy quality confirmed later in the experiments as well: when we reduce the overestimations using Double Q-learning, the policies improve."}, {"heading": "DOUBLE DQN", "text": "A main goal of this paper is to investigate whether the overestimations of Q-learning occur in practice and, when they do occur, if they hurt performance. To test these hypotheses, in the next section we analyze the performance of the DQN algorithm and compare it to the Double DQN algorithm that we will now construct. The idea of Double Q-learning is to decompose the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks. We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value. In reference to both Double Q-learning and DQN, we refer to the resulting learning algorithm as Double DQN. The update used by Double DQN is the same as for DQN, but replacing the target Y DQNt with\nY DoubleDQNt \u2261 Rt+1 + \u03b3Q(St+1, argmax a Q(St+1, a;\u03b8t),\u03b8 \u2212 t ) .\nIn comparison to Double Q-learning (4), the weights of the second network \u03b8\u2032t are replaced with the weights of the target network \u03b8\u2212t for the evaluation of the current greedy policy. The update to the target network stays unchanged from DQN, and remains a periodic copy of the online network.\nThis version of Double DQN is perhaps the minimal possible change to DQN towards Double Q-learning. The goal is to get most of the benefit of Double Q-learning, while keeping the rest of the DQN algorithm intact for a fair comparison, and with minimal computational overhead."}, {"heading": "EMPIRICAL RESULTS", "text": "In this section, we analyze the overestimations of DQN and show that Double DQN improves over DQN both in terms of value accuracy and in terms of policy quality. To further test the robustness of the approach we additionally evaluate the algorithms with random starts generated from expert human trajectories, as proposed by Nair et al. (2015).\nOur testbed contains 49 Atari 2600 games, using the Arcade Learning Environment (Bellemare et al., 2013). The goal is for a single algorithm, with a fixed set of hyperparameters, to learn to play each of the games separately from interaction given only the screen pixels as input. This is a demanding testbed: not only are the inputs high-dimensional, the game visuals and game mechanics vary substantially between games. Good solutions must therefore rely heavily on the learning algorithm \u2014 it is not practically feasible to overfit the domain by relying only on tuning.\nWe closely follow the experimental setting and network architecture outlined by Mnih et al. (2015). Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al., 1998) with 3 convolution layers and a fully-connected\nhidden layer (approximately 1.5M parameters in total). The network takes the last four frames as input and outputs the action value of each action. On each game, the network is trained on a single GPU for 200M frames, or approximately 1 week.\nResults on overoptimism. Figure 2 shows examples of DQN\u2019s overestimations in six Atari games. DQN and Double DQN were both trained under the exact conditions described by Mnih et al. (2015). DQN is consistently and sometimes vastly overoptimistic about the value of the current greedy policy, as can be seen by comparing the orange learning curves in the top row of plots to the straight orange lines, which represent the actual discounted value of the best learned policy. More precisely, the (averaged) value estimates are computed regularly during training with full evaluation phases of length T = 125, 000 steps as\n1\nT T\u2211 t=1 argmax a Q(St, a;\u03b8) .\nThe ground truth averaged values are obtained by running the best learned policies for several episodes and computing the actual cumulative rewards. Without overestimations we would expect these quantities to match up (i.e., the curve to match the straight line at the right of each plot). Instead, the learning curves of DQN consistently end up much higher than the true values. The learning curves for Double DQN, shown in blue, are much closer to the blue straight line representing the true value of the final policy. Note that the blue straight line is often higher than the orange straight line. This indicates that Double DQN does not just produce more accurate value estimates but also better policies.\nMore extreme overestimations are shown in the middle two plots, where DQN is highly unstable on the games Asterix and Wizard of Wor. Notice the log scale for the values on the y-axis. The bottom two plots shows the corresponding scores for these two games. Notice that the increases in value estimates for DQN in the middle plots coincide with decreasing scores in bottom plots. Again, this indicates that the overestimations are harming the quality of the resulting policies. If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015). However, we see that learning is much more stable with Double DQN, suggesting that the cause for these instabilities is in fact Q-learning\u2019s overoptimism. Figure 2 only shows a few examples, but overestimations were observed for DQN in all 49 tested Atari games, albeit in varying amounts.\nQuality of the learned policies. Overoptimism does not always adversely affect the quality of the learned policy. For example, DQN achieves optimal behavior in Pong despite slightly overestimating of the policy value. Nevertheless, reducing overestimations can significantly benefit the stability of the learning; we see clear examples of this in Figure 2. We now assess more generally how much Double DQN helps in terms of policy quality by evaluating on all 49 games that DQN was tested on.\nAs described by Mnih et al. (2015) we start each evaluation episode by executing a special action that does not affect the environment, a so-called no-op action, up to 30 times to provide\n0 50 100 150 200\n10\n15\n20\nV a lu\ne e st\nim a te\ns\nAlien\n0 50 100 150 200\n4\n6\n8\nSpace Invaders\n0 50 100 150 200\n1.0\n1.5\n2.0\n2.5\nTime Pilot\n0 50 100 150 200\nTraining steps (in millions)\n0\n2\n4\n6\n8 DQN estimate\nDouble DQN estimate\nDQN true value Double DQN true value\nZaxxon\ndifferent starting points for the agent. Some exploration during evaluation provides additional randomization. For Double DQN we used the exact same hyper-parameters as for DQN, to allow for a controlled experiment focused just on reducing overestimations. The learned policies are evaluated for 5 mins of emulator time (18,000 frames) with an -greedy policy where = 0.05. The scores are averaged over 100 episodes. The only difference between Double DQN and DQN is the target, using Y DoubleDQNt rather than Y DQN. This evaluation is somewhat adversarial, as the used hyper-parameters were tuned for DQN but not for Double DQN.\nTo obtain summary statistics across games, we normalize the score for each game as follows:\n(5) scorenormalized = scoreagent \u2212 scorerandom scorehuman \u2212 scorerandom .\nThe \u2018random\u2019 and \u2018human\u2019 scores are the same as used by Mnih et al. (2015), and are given in the appendix.\nTable 1, under no ops, shows that on the whole Double DQN clearly improves over DQN. A detailed comparison (in appendix) shows that there are several games in which Double DQN greatly improves upon DQN. Noteworthy examples include Road Runner (from 233% to 617%), Asterix (from 70% to 180%), Zaxxon (from 54% to 111%), and Double Dunk (from 17% to 397%).\nIn the table we have not included the Gorila algorithm (Nair et al., 2015), which is a massively distributed version of DQN, because the architecture and infrastructure is sufficiently different to make a direct comparison unclear. For reference, we note that Gorila obtained median and mean normalized scores of 96% and 495%, respectively.\nRobustness to Human starts. One concern with the previous evaluation is that in deterministic games with a unique starting point the learner could potentially learn to remember sequences of actions without much need to generalize. While successful, the solution would not be particularly robust. By testing the agents from various starting points, we can test whether the found solutions generalize well, and as such provide a challenging testbed for the learned polices (Nair et al., 2015).\nWe obtained 100 starting points sampled for each game from a human expert\u2019s trajectory, as proposed by Nair et al. (2015). We start an evaluation episode from each of these starting points and run the emulator for up to 108,000 frames (30 mins at 60Hz\nincluding the trajectory before the starting point). Each agent is only evaluated on the rewards accumulated after the starting point.\nTable 1, under random starts, reports summary statistics for this evaluation. Double DQN obtains clearly higher median and mean scores. Again we propose not to compare directly to Gorila DQN (Nair et al., 2015), but for completeness note it obtained a median of 78% and a mean of 259%. Detailed results are available in Figure 3 and in the appendix. On several games the improvements are striking, in some cases bringing scores much closer to human, or even surpassing these. This includes Asterix (from \u22121% to 69%), Bank Heist (from 26% to 72%), Double Dunk (from \u221271% to 290%), Q-Bert (from 37% to 88%), Up and Down (from 30% to 80%), and Zaxxon (from 9% to 93%). Only on two games, Assault and Robotank, we see a notable decrease in performance, although in both cases the performance stays well above human level.\nDouble DQN appears more robust to this more challenging evaluation, suggesting that appropriate generalizations occur and that the found solutions do not exploit the determinism of the environments. This is appealing, as it indicates progress towards finding general solutions rather than a deterministic sequence of steps that would be less robust."}, {"heading": "DISCUSSION", "text": "This paper has five contributions. First, we have shown why Q-learning can be overoptimistic in large-scale problems, even if these are deterministic, due to the inherent estimation errors of learning. Second, by analyzing the value estimates on Atari games we have shown that these overestimations are more common and severe in practice than previously acknowledged. Third, we have shown that Double Q-learning can be used at scale to successfully reduce this overoptimism, resulting in more stable and reliable learning. Fourth, we have proposed a specific implementation called Double DQN, that uses the existing architecture and deep neural network of the DQN algorithm without requiring additional networks or parameters. Finally, we have shown that Double DQN finds better policies, obtaining new state-of-the-art results on the Atari 2600 domain."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Tom Schaul, Volodymyr Mnih, Marc Bellemare, Thomas Degris, and Richard Sutton for helpful comments on earlier versions of this document and the ideas presented therein, and everyone at Google DeepMind for technical and non-technical support and for helping create a constructive research environment."}, {"heading": "APPENDIX", "text": "Theorem 1. Consider a state s in which all the true optimal action values are equal at Q\u2217(s, a) = V\u2217(s) for some V\u2217(s). Let Qt be arbitrary value estimates that are on the whole unbiased in the sense that \u2211 a(Qt(s, a) \u2212 V\u2217(s)) = 0, but that are not all zero, such that 1 m \u2211 a(Qt(s, a) \u2212 V\u2217(s))\n2 = C for some C > 0, where m \u2265 2 is the number of actions in s. Under these conditions, maxaQt(s, a) \u2265 V\u2217(s) + \u221a C\nm\u22121 . This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learning estimate is zero.\nProof of Theorem 1. Define the errors for each action a as a = Qt(s, a)\u2212 V\u2217(s). Suppose that there exists a setting of { a} such that maxa a <\u221a\nC m\u22121 . Let { + i } be the set of positive of size n, and { \u2212 j } the set of strictly negative of size m \u2212 n (such that { } = { +i } \u222a { \u2212 j }).\nIf n = m, then \u2211\na a = 0 =\u21d2 a = 0 \u2200a, which contradicts\u2211 a 2 a = mC. Hence, it must be that n \u2264 m \u2212 1. Then, \u2211n i=1 + i \u2264 nmaxi + i < n \u221a C m\u22121 , and therefore (using the constraint \u2211 a a = 0)\nwe also have that \u2211m\u2212n\nj=1 | \u2212 j | < n\n\u221a C\nm\u22121 . This implies maxj | \u2212 j | < n \u221a\nC m\u22121 . By Ho\u0308lder\u2019s inequality, then\nm\u2212n\u2211 j=1 ( \u2212j ) 2 \u2264 m\u2212n\u2211 j=1 | \u2212j | \u00b7max j | \u2212j |\n< n\n\u221a C m\u2212 1n \u221a C m\u2212 1 .\nWe can now combine these relations to compute an upper-bound on the sum of squares for all a:\nm\u2211 a=1 ( a) 2 = n\u2211 i=1 ( +i ) 2 + m\u2212n\u2211 j=1 ( \u2212j ) 2\n< n C m\u2212 1 + n \u221a C m\u2212 1n \u221a C m\u2212 1\n= C n(n+ 1) m\u2212 1 \u2264 mC.\nThis contradicts the assumption that \u2211m\na=1 2 a < mC, and therefore maxa a \u2265 \u221a\nC m\u22121 for all settings of that satisfy the constraints. We can check that the lower-bound is tight by setting a = \u221a\nC m\u22121 for a = 1, . . . ,m \u2212 1 and m = \u2212 \u221a (m\u2212 1)C. This verifies \u2211 a 2 a = mC\nand \u2211\na a = 0. The only tight lower bound on the absolute error for Double Qlearning |Q\u2032t(s, argmaxaQt(s, a)) \u2212 V\u2217(s)| is zero. This can be seen by because we can have\nQt(s, a1) = V\u2217(s) + \u221a C m\u2212 1 m ,\nand\nQt(s, ai) = V\u2217(s)\u2212 \u221a C\n1\nm(m\u2212 1) , for i > 1.\nThen the conditions of the theorem hold. If then, furthermore, we have Q\u2032t(s, a1) = V\u2217(s) then the error is zero. The remaining action values Q\u2032t(s, ai), for i > 1, are arbitrary.\nTheorem 2. Consider a state s in which all the true optimal action values are equal at Q\u2217(s, a) = V\u2217(s). Suppose that the estimation errors\nQt(s, a)\u2212Q\u2217(s, a) are independently distributed uniformly randomly in [\u22121, 1]. Then,\nE [ max\na Qt(s, a)\u2212 V\u2217(s) ] = m\u2212 1 m+ 1\nProof. Define a = Qt(s, a) \u2212 Q\u2217(s, a); this is a uniform random variable in [\u22121, 1]. The probability that maxaQt(s, a) \u2264 x for some x is equal to the probability that a \u2264 x for all a simultaneously. Because the estimation errors are independent, we can derive\nP (max a\na \u2264 x) = P (X1 \u2264 x \u2227X2 \u2264 x \u2227 . . . \u2227Xm \u2264 x)\n= m\u220f a=1 P ( a \u2264 x) .\nThe function P ( a \u2264 x) is the cumulative distribution function (CDF) of a, which here is simply defined as\nP ( a \u2264 x) =  0 if x \u2264 \u22121 1+x 2\nif x \u2208 (\u22121, 1) 1 if x \u2265 1\nThis implies that\nP (max a\na \u2264 x) = m\u220f\na=1\nP ( a \u2264 x)\n=  0 if x \u2264 \u22121( 1+x 2 )m if x \u2208 (\u22121, 1) 1 if x \u2265 1\nThis gives us the CDF of the random variable maxa a. Its expectation can be written as an integral\nE [ max a a ] = \u222b 1 \u22121 xfmax(x) dx ,\nwhere fmax is the probability density function of this variable, defined as the derivative of the CDF: fmax(x) = ddxP (maxa a \u2264 x), so that for x \u2208 [\u22121, 1] we have fmax(x) = m2 ( 1+x 2 )m\u22121. Evaluating the integral yields\nE [ max a a ] = \u222b 1 \u22121 xfmax(x) dx\n=\n[( x+ 1\n2 )m mx\u2212 1 m+ 1 ]1 \u22121\n= m\u2212 1 m+ 1 ."}, {"heading": "EXPERIMENTAL DETAILS FOR THE ATARI 2600 DOMAIN", "text": "We selected the 49 games to match the list used by Mnih et al. (2015), see Tables below for the full list. Each agent step is composed of four frames (the last selected action is repeated during these frames) and reward values (obtained from the Arcade Learning Environment (Bellemare et al., 2013)) are clipped between -1 and 1.\nNetwork Architecture. The convolution network used in the experiment is exactly the one proposed by proposed by Mnih et al. (2015), we only provide details here for completeness. The input to the network is a 84x84x4 tensor containing a rescaled, and gray-scale, version of the last four frames. The first convolution layer convolves the input with 32 filters of size 8 (stride 4), the second layer has 64 layers of size 4 (stride 2), the final convolution layer has 64 filters of size 3 (stride 1). This is followed by a fully-connected hidden layer of 512 units. All these layers are separated by Rectifier Linear Units (ReLu). Finally, a fullyconnected linear layer projects to the output of the network, i.e., the Q-values. The optimization employed to train the network is RMSProp (with momentum parameter 0.95).\nHyper-parameters. In all experiments, the discount was set to \u03b3 = 0.99, and the learning rate to \u03b1 = 0.00025. The number of steps between target network updates was \u03c4 = 10, 000. Training is done over 50M steps (i.e., 200M frames). The agent is evaluated every 1M steps, and the best policy across these evaluations is kept as the output of the learning process. The size of the experience replay memory is 1M tuples. The memory gets sampled to update the network every 4 steps with minibatches of size 32. The simple exploration policy used is an -greedy policy with the decreasing linearly from 1 to 0.1 over 1M steps."}, {"heading": "SUPPLEMENTARY RESULTS IN THE ATARI 2600 DOMAIN", "text": "Figure 4 shows the normalized scores for DQN and Double DQN for the evaluation used by Mnih et al. (2015), and as given in Table 3 below. The Tables below provide further detailed results for our experiments in the Atari domain."}], "references": [{"title": "Sample mean based index policies with O(log n) regret for the multi-armed bandit problem", "author": ["R. Agrawal"], "venue": "Advances in Applied Probability,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Machine Learning: Proceedings of the Twelfth International Conference,", "citeRegEx": "Baird.,? \\Q1995\\E", "shortCiteRegEx": "Baird.", "year": 1995}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2003}, {"title": "Neocognitron: A hierarchical neural network capable of visual pattern recognition", "author": ["K. Fukushima"], "venue": "Neural networks,", "citeRegEx": "Fukushima.,? \\Q1988\\E", "shortCiteRegEx": "Fukushima.", "year": 1988}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L. Lin"], "venue": "Machine learning,", "citeRegEx": "Lin.,? \\Q1992\\E", "shortCiteRegEx": "Lin.", "year": 1992}, {"title": "Gradient temporal-difference learning algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "Maei.,? \\Q2011\\E", "shortCiteRegEx": "Maei.", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Proceedings of the 16th European Conference on Machine Learning", "citeRegEx": "Riedmiller.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller.", "year": 2005}, {"title": "Reinforcement learning with factored states and actions", "author": ["B. Sallans", "G.E. Hinton"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sallans and Hinton.,? \\Q2004\\E", "shortCiteRegEx": "Sallans and Hinton.", "year": 2004}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "In Proceedings of the seventh international conference on machine learning,", "citeRegEx": "Sutton.,? \\Q1990\\E", "shortCiteRegEx": "Sutton.", "year": 1990}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesv\u00e1ri", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "arXiv preprint arXiv:1503.04269,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "The many faces of optimism: a unifying approach", "author": ["I. Szita", "A. L\u0151rincz"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Szita and L\u0151rincz.,? \\Q2008\\E", "shortCiteRegEx": "Szita and L\u0151rincz.", "year": 2008}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "Proceedings of the 1993 Connectionist Models Summer School,", "citeRegEx": "Thrun and Schwartz.,? \\Q1993\\E", "shortCiteRegEx": "Thrun and Schwartz.", "year": 1993}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Double Q-learning", "author": ["H. van Hasselt"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hasselt.,? \\Q2010\\E", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "Insights in Reinforcement Learning", "author": ["H. van Hasselt"], "venue": "PhD thesis, Utrecht University,", "citeRegEx": "Hasselt.,? \\Q2011\\E", "shortCiteRegEx": "Hasselt.", "year": 2011}, {"title": "Learning from delayed rewards", "author": ["C.J.C.H. Watkins"], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}], "referenceMentions": [{"referenceID": 16, "context": "The goal of reinforcement learning (Sutton and Barto, 1998) is to learn good policies for sequential decision problems, by optimizing a cumulative future reward signal.", "startOffset": 35, "endOffset": 59}, {"referenceID": 25, "context": "Q-learning (Watkins, 1989) is one of the most popular reinforcement learning algorithms, but it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values.", "startOffset": 11, "endOffset": 26}, {"referenceID": 21, "context": "In previous work, overestimations have been attributed to insufficiently flexible function approximation (Thrun and Schwartz, 1993) and noise (van Hasselt, 2010, 2011).", "startOffset": 105, "endOffset": 131}, {"referenceID": 6, "context": "Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known exploration technique (Kaelbling et al., 1996).", "startOffset": 143, "endOffset": 167}, {"referenceID": 10, "context": "To test whether overestimations occur in practice and at scale, we investigate the performance of the recent DQN algorithm (Mnih et al., 2015).", "startOffset": 123, "endOffset": 142}, {"referenceID": 6, "context": "Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known exploration technique (Kaelbling et al., 1996). If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. Thrun and Schwartz (1993) give specific examples in which this leads to suboptimal policies, even asymptotically.", "startOffset": 144, "endOffset": 381}, {"referenceID": 25, "context": "Estimates for the optimal action values can be learned using Q-learning (Watkins, 1989), a form of temporal difference learning (Sutton, 1988).", "startOffset": 72, "endOffset": 87}, {"referenceID": 14, "context": "Estimates for the optimal action values can be learned using Q-learning (Watkins, 1989), a form of temporal difference learning (Sutton, 1988).", "startOffset": 128, "endOffset": 142}, {"referenceID": 10, "context": "Two important ingredients of the DQN algorithm as proposed by Mnih et al. (2015) are the use of a target network, and the use of experience replay.", "startOffset": 62, "endOffset": 81}, {"referenceID": 8, "context": "For the experience replay (Lin, 1992), observed transitions are stored for some time and sampled uniformly from this memory bank to update the network.", "startOffset": 26, "endOffset": 37}, {"referenceID": 10, "context": "Both the target network and the experience replay dramatically improve the performance of the algorithm (Mnih et al., 2015).", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121 m+1 , where m is the number of actions.", "startOffset": 56, "endOffset": 82}, {"referenceID": 21, "context": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121 m+1 , where m is the number of actions. In addition, Thrun and Schwartz give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies, and show the overestimations manifest themselves in a small toy problem when using function approximation. Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning as a solution.", "startOffset": 56, "endOffset": 541}, {"referenceID": 21, "context": "Q-learning\u2019s overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [\u2212 , ] then each target is overestimated up to \u03b3 m\u22121 m+1 , where m is the number of actions. In addition, Thrun and Schwartz give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies, and show the overestimations manifest themselves in a small toy problem when using function approximation. Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning as a solution. In this section we demonstrate more generally that estimation errors of any kind can induce an upward bias, regardless of whether these errors are due to environmental noise, function approximation, non-stationarity, or any other source. This is important, because in practice any method will incur some inaccuracies during learning, simply due to the fact that the true values are initially unknown. The result by Thrun and Schwartz (1993) cited above gives an upper bound to the overestimation for a specific setup, but it is also possible, and potentially more interesting, to derive a lower bound.", "startOffset": 56, "endOffset": 1133}, {"referenceID": 10, "context": "This is important because flexible parametric function approximators are often employed in reinforcement learning (see, e.g., Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015).", "startOffset": 114, "endOffset": 199}, {"referenceID": 15, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 0, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 6, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 1, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 4, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 19, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 13, "context": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and Tennenholtz, 2003; Szita and L\u0151rincz, 2008; Strehl et al., 2009), where an exploration bonus is given to states or actions with uncertain values.", "startOffset": 84, "endOffset": 233}, {"referenceID": 6, "context": ", Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015). In contrast to van Hasselt (2010) we did not use a statistical argument to find overestimations, the process to obtain Figure 1 is fully deterministic.", "startOffset": 58, "endOffset": 111}, {"referenceID": 6, "context": ", Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih et al. 2015). In contrast to van Hasselt (2010) we did not use a statistical argument to find overestimations, the process to obtain Figure 1 is fully deterministic. In contrast to Thrun and Schwartz (1993), we did not rely on inflexible function approximation with irreducible asymptotic errors; the bottom row shows that a function that is flexible enough to cover all samples leads to high overestimations.", "startOffset": 58, "endOffset": 270}, {"referenceID": 21, "context": "This was already observed by Thrun and Schwartz (1993), who noted that, in contrast to optimism in the face of uncertainty, these overestimations actually can impede learning an optimal policy.", "startOffset": 29, "endOffset": 55}, {"referenceID": 3, "context": "Our testbed contains 49 Atari 2600 games, using the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 80, "endOffset": 104}, {"referenceID": 5, "context": "Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al., 1998) with 3 convolution layers and a fully-connected hidden layer (approximately 1.", "startOffset": 68, "endOffset": 105}, {"referenceID": 7, "context": "Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al., 1998) with 3 convolution layers and a fully-connected hidden layer (approximately 1.", "startOffset": 68, "endOffset": 105}, {"referenceID": 3, "context": "Our testbed contains 49 Atari 2600 games, using the Arcade Learning Environment (Bellemare et al., 2013). The goal is for a single algorithm, with a fixed set of hyperparameters, to learn to play each of the games separately from interaction given only the screen pixels as input. This is a demanding testbed: not only are the inputs high-dimensional, the game visuals and game mechanics vary substantially between games. Good solutions must therefore rely heavily on the learning algorithm \u2014 it is not practically feasible to overfit the domain by relying only on tuning. We closely follow the experimental setting and network architecture outlined by Mnih et al. (2015). Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun et al.", "startOffset": 81, "endOffset": 672}, {"referenceID": 10, "context": "DQN and Double DQN were both trained under the exact conditions described by Mnih et al. (2015). DQN is consistently and sometimes vastly overoptimistic about the value of the current greedy policy, as can be seen by comparing the orange learning curves in the top row of plots to the straight orange lines, which represent the actual discounted value of the best learned policy.", "startOffset": 77, "endOffset": 96}, {"referenceID": 2, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 17, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 9, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 18, "context": "If seen in isolation, one might perhaps have be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2008; Maei, 2011; Sutton et al., 2015).", "startOffset": 185, "endOffset": 282}, {"referenceID": 10, "context": "As described by Mnih et al. (2015) we start each evaluation episode by executing a special action that does not affect the environment, a so-called no-op action, up to 30 times to provide", "startOffset": 16, "endOffset": 35}, {"referenceID": 10, "context": "The results are obtained by running DQN and Double DQN with 6 different random seeds with the hyper-parameters employed by Mnih et al. (2015). The darker line shows the median over seeds and we average the two extreme values to obtain the shaded area (i.", "startOffset": 123, "endOffset": 142}, {"referenceID": 10, "context": "The \u2018random\u2019 and \u2018human\u2019 scores are the same as used by Mnih et al. (2015), and are given in the appendix.", "startOffset": 56, "endOffset": 75}], "year": 2015, "abstractText": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance", "creator": "TeX"}}}