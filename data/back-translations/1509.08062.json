{"id": "1509.08062", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "End-to-End Text-Dependent Speaker Verification", "abstract": "In this paper, we present a data-driven, integrated approach to speaker verification that maps a test statement and a few reference statements directly to a single score for verification, and collectively optimizes the system components by applying the same evaluation protocol and metric as at test time. Such an approach results in simple and efficient systems that require little domain-specific knowledge and make few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including estimating a speaker model from a few statements, and evaluating it from our internal \"Ok Google\" benchmark for text-dependent speaker verification. The proposed approach seems to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint.", "histories": [["v1", "Sun, 27 Sep 2015 07:43:36 GMT  (473kb)", "http://arxiv.org/abs/1509.08062v1", "submitted to ICASSP 2016"]], "COMMENTS": "submitted to ICASSP 2016", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["georg heigold", "ignacio moreno", "samy bengio", "noam shazeer"], "accepted": false, "id": "1509.08062"}, "pdf": {"name": "1509.08062.pdf", "metadata": {"source": "CRF", "title": "End-to-End Text-Dependent Speaker Verification", "authors": ["Georg Heigold", "Ignacio Moreno", "Samy Bengio", "Noam Shazeer"], "emails": ["georg.heigold@dfki.de", "elnota@google.com", "bengio@google.com", "noam@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n08 06\n2v 1\n[ cs\n.L G\n] 2\n7 Se\np 20\n15"}, {"heading": "1. Introduction", "text": "Speaker verification is the process of verifying, based on a speaker\u2019s known utterances, whether an utterance belongs to the speaker. When the lexicon of the spoken utterances is constrained to a single word or phrase across all users, the process is referred to as global password text-dependent speaker verification. By constraining the lexicon, text-dependent speaker verification aims to compensate for phonetic variability, which poses a significant challenge in speaker verification [1]. At Google, we are interested in text-dependent speaker verification with the global password \u201dOk Google.\u201d The choice of this particularly short, approximately 0.6 seconds long global password relates to the Google Keyword Spotting system [2] and Google VoiceSearch [3] and facilitates the combination of the systems.\nIn this paper, we propose to directly map a test utterance together with a few utterances to build the speaker model, to a single score for verification. All the components are jointly optimized using a verification-based loss following the standard speaker verification protocol. Compared to existing approaches, such an end-to-end approach may have several advantages, including the direct modeling from utterances, which allows for capturing long-range context and reduces the complexity (one vs. number of frames evaluations per utterance), and the direct and joint estimation, which can lead to better and more compact models. Moreover, this approach often results in considerably simplified systems requiring fewer concepts and heuristics.\nMore specifically, the contributions of this paper include: \u2022 formulation of end-to-end speaker verification architec-\nture, including the estimation of a speaker model on a few utterances (Section 4);\n\u2217 Work done while the author was at Google.\n\u2022 empirical evaluation of end-to-end speaker verification, including comparison of frame (i-vectors, d-vectors) and utterance-level representations (Section 5.2) and analysis of the end-to-end loss (Section 5.3);\n\u2022 empirical comparison of feedforward and recurrent neural networks (Section 5.4).\nThis paper focuses on text-dependent speaker verification for small footprint systems, as discussed in [4]. But the approach is more general and could be used similarly for text-independent speaker verification.\nIn previous studies, the verification problem is broken down into more tractable, but loosely connected subproblems. For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11]. Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14]. For small footprint systems, however, a more direct deep learning modeling may be an attractive alternative [15, 4]. To the best of our knowledge, recurrent neural networks have been applied to related problems such as speaker identification [16] and language identification [17], but not to the speaker verification task. The proposed neural network architecture can be thought of as joint optimization of a generative-discriminative hybrid and is in the same spirit as deep unfolding [18] for adaptation.\nThe remainder of the paper is organized as follows. Section 2 provides a brief overview of speaker verification in general. Section 3 describes the d-vector approach. Section 4 introduces the proposed end-to-end approach to speaker verification. An experimental evaluation and analysis can be found in Section 5. The paper is concluded in Section 6."}, {"heading": "2. Speaker Verification Protocol", "text": "The standard verification protocol can be divided into the three steps: training, enrollment, and evaluation, which we describe in more detail next.\nTraining In the training stage, we find a suitable internal speaker representation from the utterance, allowing for a simple scoring function. In general, this representation depends on the type of the model (e.g., Gaussian subspace model or deep neural network), the representation level (e.g., frame or utterance), and the model training loss (e.g., maximum likelihood or softmax). State-of-the art representations are a summary of frame-level information, such as i-vectors [7, 8] and d-vectors (Section 3).\nEnrollment In the enrollment stage, a speaker provides a few utterances (see Table 1), which are used to estimate a speaker model. A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.\nEvaluation During the evaluation stage, the verification task is performed and the system is evaluated. For verification, the value of a scoring function of the utterance X and the test speaker spk, S(X, spk), is compared against a pre-defined threshold. We accept if the score exceeds the threshold, i.e., the utterance X comes from speaker spk, and reject otherwise. In this setup, two types of error can occur: false reject and false accept. Clearly, the false reject rate and the false accept rate depend on the threshold. When the two rates are equal, the common value is called equal error rate (EER).\nA simple scoring function is the cosine similarity between the speaker representation f(X) of an evaluation utterance X (see paragraph \u201dTraining\u201d) and the speaker model mspk (see paragraph \u201dEnrollment\u201d):\nS(X, spk) = [f(X)\u22a4mspk]/[\u2016f(X)\u2016 \u2016mspk\u2016].\nPLDA has been proposed as a more refined, data-driven scoring approach."}, {"heading": "3. D-Vector Baseline Approach", "text": "D-vectors are derived from a deep neural network (DNN), as speaker representation of an utterance. A DNN consists of the successive application of several non-linear functions in order to transform the speaker utterance into a vector where a decision can be easily made. Fig. 1 depicts the topology of our baseline DNN. It includes a locally-connected layer [4] and several fully connected layers. All layers use ReLU activation except the last, which is linear. During the training stage, the parameters of the DNN are optimized using the softmax loss, which, for convenience, we define to comprise a linear transformation with weight vectors wspk and biases bspk , followed by the softmax function and the cross-entropy loss:\nlsoftmax = \u2212 log exp(w\u22a4spky + bspk)\u2211 \u02dcspk exp(w\u22a4 \u02dcspk y + b \u02dcspk)\nwhere the activation vector of the last hidden layer is denoted by y and spk denotes the correct speaker. The normalization is over all competing training speakers \u02dcspk.\nAfter the training stage is completed, the parameters of the DNN are fixed. Utterance d-vectors are obtained by averaging the activation vectors of the last hidden layer for all frames of an utterance. Each utterance generates one d-vector. For enrollment, the speaker model is given by the average over the dvectors of the enrollment utterances. Finally, during the evaluation stage, the scoring function is the cosine similarity between the speaker model d-vector and the d-vector of a test utterance.\nCriticism about this baseline approach includes the limited context of the d-vectors derived from (a window of) frames and the type of the loss. The softmax loss attempts to discriminate between the true speaker and all competing speakers but does not follow the standard verification protocol in Section 2. As a result, heuristics and scoring normalization techniques becomes necessary to compensate for inconsistencies. Moreover, the softmax loss does not scale well with more data as the computational complexity is linear in the number of training speakers and requires a minimum amount of data per speaker to estimate the speaker-specific weights and biases. The complexity issue (but not the estimation issue) can be alleviated by candidate sampling [20].\nSimilar concerns can be expressed over the alternative speaker verification approaches, where some of the component blocks are either loosely connected or not directly optimized following the speaker verification protocol. For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7]."}, {"heading": "4. End-To-End Speaker Verification", "text": "In this section, we formulate the speaker verification problem as a single network architecture. Following Section 2, this architecture consists of a training component to compute the speaker representations, an enrollment component to estimate the speaker model, and an evaluation component with a suitable loss function for optimization, see Fig. 2.\nWe use neural networks to obtain the speaker representation of an utterance. The two types of networks we use in this work are depicted in Figs. 1 and 3: a deep neural network (DNN) with locally-connected and fully connected layers as our baseline DNN in Section 3 and a long short-term memory recurrent neural network (LSTM) [21, 22] with a single output. DNNs assume a fixed-length input. To comply with this constraint, we stack the frames of a sufficiently large window of fixed length over the utterance and use them as the input. This trick is not needed for LSTMs but we use the same window of frames for\nbetter comparability. Unlike vanilla LSTMs which have multiple outputs, we only connect the last output to the loss to obtain a single, utterance-level speaker representation.\nThe speaker model is the average over a small number of \u201denrollment\u201d representations (Section 2). We use the same network to compute the internal representations of the \u201dtest\u201d utterance and of the utterances for the speaker model. The actual number of utterances per speaker available in training typically is much larger (a few hundred or more) than in enrollment (fewer than ten), see Table 1. To avoid a mismatch, we sample for each training utterance only a few utterances from the same speaker to build the speaker model at training time. In general, we cannot assume to have N utterances per speaker. To allow for a variable number of utterances, we pass a weight along with the utterance to indicate whether to use the utterance.\nFinally, we compute the cosine similarity between the speaker representation and the speaker model, S(X, spk), and feed it to a logistic regression including a linear layer with a bias. The architecture is optimized using the end-to-end loss\nle2e = \u2212 log p(target) (1)\nwith the binary variable target \u2208 {accept, reject}, p(accept) = (1+exp(\u2212wS(X, spk)\u2212b))\u22121, and p(reject) = 1\u2212p(accept). The value \u2212b/w corresponds with the verification threshold.\nThe input of the end-to-end architecture are 1 + N utterances, i.e., an utterance to be tested and up to N different utterances of the same speaker to estimate the speaker model. To achieve a good tradeoff between data shuffling and memory, the input layer maintains a pool of utterances to sample 1+N utterances from for each training step and gets refreshed frequently for better data shuffling. As a certain number of utterances of the same speaker is needed for the speaker model, the data is presented in small groups of utterances of the same speaker.\nThe end-to-end architecture allows for direct optimization of the evaluation metric using the standard evaluation protocol with consistent speaker models. Conceivably, this will result in better accuracy without the need for heuristics and postprocessing steps, for example, score normalization. Moreover, the approach scales well as it neither depends on the number of training speakers and nor requires a minimum number of utterances per speaker."}, {"heading": "5. Experimental Evaluation", "text": "We evaluate the proposed end-to-end approach on our internal \u201dOk Google\u201d benchmark."}, {"heading": "5.1. Data Sets & Basic Setup", "text": "We tested the proposed end-to-end approach on a set ?Ok Google? utterances collected from anonymized voice search logs. For improved noise robustness, we perform multistyle\ntraining. The data were augmented by artificially adding in car and cafeteria noise at various SNRs, and simulating different distances between the speaker and the microphone, see [2] for further details. Enrollment and evaluation data include only real data. Table 1 shows some data set statistics.\nThe utterances are forced aligned to obtain the \u201dOk Google\u201d snippets. The average length of these snippets is around 80 frames, for a frame rate of 100 Hz. Based on this observation, we extracted the last 80 frames from each snippet, possibly padding or truncating frames at the beginning of the snippet. The frames consist of 40 log-filterbanks (with some basic spectral subtraction) each.\nFor DNNs, we concatenate the 80 input frames, resulting in a 80x40-dimensional feature vector. Unless specified otherwise, the DNN consists of 4 hidden layers. All hidden layers in the DNN have 504 nodes and use ReLU activation except the last, which is linear. The patch size for the locally-connected layer of the DNN is 10\u00d710. For LSTMs, we feed the 40-dimensional feature vectors frame by frame. We use a single LSTM layer with 504 nodes without a projection layer. The batch size is 32 for all experiments.\nResults are reported in terms of equal error rate (EER), without and with t-norm score normalization [23]."}, {"heading": "5.2. Frame-Level vs. Utterance-Level Representation", "text": "First, we compare frame-level and utterance-level speaker representations, see Table 2. Here, we use a DNN as described in Fig. 1 with a softmax layer and trained on train 2M (Table 1) with 50% dropout [24] in the linear layer. The utterance-level approach outperforms the frame-level approach by 30%. Score normalization gives a substantial performance boost (up to 20% relative) in either case. For comparison, two i-vector baselines\nare shown. The first baseline is based on [6], and uses 13 PLPs with first-order and second-order derivatives, 1024 Gaussians, and 300-dimensional i-vectors. The second baseline is based on [25] with 150 eigenvoices. The i-vector+PLDA baseline should be taken with a grain of salt as the PLDA model was only trained on a subset of the 2M train data set (4k speakers and 50 utterances per speaker) due to limitations of our current implementation.1 Also, this baseline does not include other refining techniques such as \u201duncertainty training\u201d [10] that have\n1However, training with only 30 utterances per speaker gives almost the same results.\nbeen reported to give substantial additional gains under certain conditions. Note that compared to [15], we have improved our d-vectors significantly [4]."}, {"heading": "5.3. Softmax vs. End-to-End Loss", "text": "Next, we compare the softmax loss (Section 2) and end-to-end loss (Section 4) for training utterance-level speaker representations. Table 3 shows the equal error rates for the DNN in Fig. 1. If trained on the small training set (train 2M), the error rates on the raw scores are comparable for the different loss functions. While dropout gives a 1% absolute gain for softmax, we did not observe a gain from dropout for the end-to-end loss. Similarly, t-normalization helps by 20% for softmax, but not at all for the end-to-end loss. This result is in agreement with the degree of consistency between the training loss and the evaluation metric. In particular, the end-to-end approach assuming a global threshold in training (see Eq. (1)), can implicitly learn normalized scores that are invariant under different noise conditions etc. and makes score normalization redundant. When using the softmax DNN for initialization of the end-to-end training, the error rate is reduced from 2.86% to 2.25%, suggesting an estimation problem.\nIf trained on the larger training set (train 22M), the end-toend loss clearly outperforms softmax, see Table 3. To reasonably scale the softmax layer to 80k speaker labels, we employed candidate sampling, similar to [20]. Again, t-normalization helps by 20% for softmax and softmax can catch up with the other losses, which do not benefit from t-normalization. The initialization for end-to-end training (random vs. \u201dpre-trained\u201d softmax DNN) does not make a difference in this case.\nAlthough the step time for the end-to-end approach is larger than for softmax with candidate sampling because the speaker model is computed on the fly, the overall convergence times are comparable.\nThe optimal choice of the number of utterances used to estimate the speaker model in training, referred to as the speaker model size, depends on the (average) number of enrollment utterances. In practice, however, smaller speaker model sizes may be more attractive to reduce the training time and make the training harder. Fig. 4 shows the dependency of the test equal error rate on the speaker model size, i.e., the number of utterances used to estimate the speaker model. There is a relatively broad optimum around a model size of 5 with 2.04% equal error rate, compared to 2.25% for a model size of 1. This model size is close to the true average model size, which is 6 for our enrollment set. Similar trends are observed for the other configurations in this paper (not shown). This indicates the consistency of the proposed training algorithm with the verification protocol and suggests that task-specific training tends to be better."}, {"heading": "5.4. Feedforward vs. Recurrent Neural Networks", "text": "So far we focused on the \u201dsmall footprint\u201d DNN in Fig. 1 with one locally-connected and three fully-connected hidden layers. Next, we explore larger and different network architectures, re-\ngardless of their size and computational complexity. The results are summarized in Table 4. Compared to the small footprint DNN, the \u201dbest\u201d DNN uses an additional hidden layer and gives a 10% relative gain. The LSTM in Fig. 3 adds another 30% gain over this best DNN. The number of parameters is comparable to that of the DNN but the LSTM involves about ten times more multiplications and additions. More hyperparameter tuning will hopefully bring the computational complexity further down to make it feasible. Slightly worse error rates are achieved with the softmax loss (using t-normalization, candidate sampling, dropout, and possibly early stopping, which were all not needed for the end-to-end approach). On train 2M, we observed similar relative gains in error rate over the respective DNN baselines."}, {"heading": "6. Summary & Conclusion", "text": "We proposed a novel end-to-end approach to speaker verification, which directly maps the utterance to a score and jointly optimizes the internal speaker representation and the speaker model using the same loss for training and evaluation. Assuming sufficient training data, the proposed approach improved our best small footprint DNN baseline from over 3% to 2% equal error rate on our internal \u201dOk Google\u201d benchmark. Most of the gain came from the utterance-level vs. frame-level modeling. Compared to other losses, the end-to-end loss achieved the same or slightly better results but with fewer additional concepts. In case of softmax, for example, we obtained comparable error rates only when using score normalization at runtime, candidate sampling to make training feasible, and dropout in training. Furthermore, we showed that the equal error rate can further be reduced to 1.4% using a recurrent neural network instead of a simple deep neural network, although at higher computational runtime cost. By comparison, a reasonable but not fully state-of-the-art i-vector/PLDA system gave 4.7%. Clearly, more comparative studies are needed. Nevertheless, we believe that our approach demonstrates a promising new direction for big data verification applications."}, {"heading": "7. References", "text": "[1] H. Aronowitz, R. Hoory, J. W. Pelecanos, and D. Na-\nhamoo, \u201cNew developments in voice biometrics for user authentication,\u201d in Interspeech, Florence, Italy, Aug. 2011, pp. 17 \u2013 20.\n[2] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. Sainath, \u201cAutomatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015, pp. 4704\u20134708.\n[3] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba, M. Cohen, M. Kamvar, and B. Strope, \u201c\u201cYour word is my command\u201d: Google search by voice: A case study,\u201d in Advances in Speech Recognition: Mobile Environments, Call Centers and Clinics. Springer, 2010, ch. 4, pp. 61\u201390.\n[4] Y. Chen, I. Lopez-Moreno, and T. Sainath, \u201cLocallyconnected and convolutional neural networks for small footprint speaker recognition,\u201d in Interspeech, Dresden, Germany, Sep. 2015.\n[5] P. Kenny, \u201cBayesian speaker verification with heavy-tailed priors,\u201d in Proc. Odyssey Speaker and Language Recognition Workshop, Brno, Czech Republic, Jul. 2010.\n[6] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[7] D. Reynolds, T. Quoter, and R. Dunn, \u201cSpeaker verification using adapted Gaussian mixture models,\u201d Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.\n[8] P. Kenny, G. Boulianne, P. Ouellet, and P. Dumouchel, \u201cJoint factor analysis versus eigenchannels in speaker recognition,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.\n[9] H. Aronowitz, \u201cText-dependent speaker verification using a small development set,\u201d in Proc. Odyssey Speaker and Language Recognition Workshop, Singapore, Jun. 2012.\n[10] T. Stafylakis, P. Kenny, P. Ouellet, P. Perez, J. Kockmann, and P. Dumouchel, \u201cText-dependent speaker recognition using PLDA with uncertainty propagation,\u201d in Interspeech, Lyon, France, Aug. 2013.\n[11] A. Larcher, K.-A. Lee, B. Ma, and H. Li, \u201cPhoneticallyconstrained PLDA modeling for text-dependent speaker verification with multiple short utterances,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 2013.\n[12] D. Garcia-Romero, X. Zhang, A. McCree, and D. Povey, \u201cImproving speaker recognition performance in the domain adaptation challenge using deep neural networks,\u201d in IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoie, NV, USA, Dec. 2014, pp. 378\u2013383.\n[13] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, \u201cA novel scheme for speaker recognition using a phoneticallyaware deep neural network,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence, Italy, May 2014, pp. 1695\u20131699.\n[14] F. Richardson, D. Reynolds, and N. Dehak, \u201cDeep neural network approaches to speaker and language recognition,\u201d IEEE Signal Processing Letters, 2005.\n[15] E. Variani, X. Lei, E. McDermott, I. Lopez-Moreno, and J. Gonzalez-Dominguez, \u201cDeep neural networks for small footprint text-dependent speaker verification,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence, Italy, May 2014.\n[16] S. Parveen, A. Qadeer, and P. Green, \u201cSpeaker recognition with recurrent neural networks,\u201d in Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000, Beijing, China, Oct 2000, pp. 16\u2013 20.\n[17] J. Gonzalez-Dominguez, I. Lopez-Moreno, H. Sak, J. Gonzalez-Rodriguez, and P. Moreno, \u201cAutomatic language identification using long short-term memory recurrent neural networks,\u201d in Interspeech, Singapore, Sep. 2014, pp. 2155\u20132159.\n[18] J. Hershey, J. L. Roux, and F. Weninger, \u201cDeep unfolding: Model-based inspiration of novel deep architectures,\u201d CoRR, vol. abs/1409.2574, 2014. [Online]. Available: http://arxiv.org/abs/1409.2574\n[19] C. Greenberg, D. Banse\u0301, G. Doddington, D. GarciaRomero, J. Godfrey, T. Kinnunen, A. Martin, A. McCree, M. Przybocki, and D. Reynolds, \u201cThe NIST 2014 speaker recognition i-vector machine learning challenge,\u201d in Odyssey 2014: The Speaker and Language Recognition Workshop, Joensuu, Finland, Jun. 2014.\n[20] S. Jean, K. Cho, R. Memisevic, and Y. Bengio, \u201cOn using very large target vocabulary for neural machine translation,\u201d CoRR, vol. abs/1412.2007, 2014. [Online]. Available: http://arxiv.org/abs/1412.2007\n[21] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.\n[22] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Interspeech, Singapore, Sep. 2014.\n[23] R. Auckenthaler, M. Carey, and H. Lloyd-Thomas, \u201cScore normalization for text-independent speaker verification systems,\u201d Digital Signal Processing, vol. 10, no. 1-3, pp. 42\u201354, 2000.\n[24] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d CoRR, vol. abs/1207.0580, 2012. [Online]. Available: http://arxiv.org/abs/1207.0580\n[25] D. Garcia-Romero and C. Espy-Wilson, \u201cAnalysis of ivector length normalization in speaker recognition systems,\u201d in Interspeech, Florence, Italy, Aug. 2011."}], "references": [{"title": "New developments in voice biometrics for user authentication", "author": ["H. Aronowitz", "R. Hoory", "J.W. Pelecanos", "D. Nahamoo"], "venue": "Interspeech, Florence, Italy, Aug. 2011, pp. 17 \u2013 20.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks", "author": ["R. Prabhavalkar", "R. Alvarez", "C. Parada", "P. Nakkiran", "T. Sainath"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015, pp. 4704\u20134708.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Your word is my command\u201d: Google search by voice: A case study", "author": ["J. Schalkwyk", "D. Beeferman", "F. Beaufays", "B. Byrne", "C. Chelba", "M. Cohen", "M. Kamvar", "B. Strope"], "venue": "Advances in Speech Recognition: Mobile Environments, Call Centers and Clinics. Springer, 2010, ch. 4, pp. 61\u201390.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Locallyconnected and convolutional neural networks for small footprint speaker recognition", "author": ["Y. Chen", "I. Lopez-Moreno", "T. Sainath"], "venue": "Interspeech, Dresden, Germany, Sep. 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian speaker verification with heavy-tailed priors", "author": ["P. Kenny"], "venue": "Proc. Odyssey Speaker and Language Recognition Workshop, Brno, Czech Republic, Jul. 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker verification using adapted Gaussian mixture models", "author": ["D. Reynolds", "T. Quoter", "R. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Text-dependent speaker verification using a small development set", "author": ["H. Aronowitz"], "venue": "Proc. Odyssey Speaker and Language Recognition Workshop, Singapore, Jun. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Text-dependent speaker recognition using PLDA with uncertainty propagation", "author": ["T. Stafylakis", "P. Kenny", "P. Ouellet", "P. Perez", "J. Kockmann", "P. Dumouchel"], "venue": "Interspeech, Lyon, France, Aug. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Phoneticallyconstrained PLDA modeling for text-dependent speaker verification with multiple short utterances", "author": ["A. Larcher", "K.-A. Lee", "B. Ma", "H. Li"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving speaker recognition performance in the domain adaptation challenge using deep neural networks", "author": ["D. Garcia-Romero", "X. Zhang", "A. McCree", "D. Povey"], "venue": "IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoie, NV, USA, Dec. 2014, pp. 378\u2013383.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel scheme for speaker recognition using a phoneticallyaware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence, Italy, May 2014, pp. 1695\u20131699.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural network approaches to speaker and language recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Processing Letters, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "X. Lei", "E. McDermott", "I. Lopez-Moreno", "J. Gonzalez-Dominguez"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence, Italy, May 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker recognition with recurrent neural networks", "author": ["S. Parveen", "A. Qadeer", "P. Green"], "venue": "Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000, Beijing, China, Oct 2000, pp. 16\u2013 20.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic language identification using long short-term memory recurrent neural networks", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "Interspeech, Singapore, Sep. 2014, pp. 2155\u20132159.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J. Hershey", "J.L. Roux", "F. Weninger"], "venue": "CoRR, vol. abs/1409.2574, 2014. [Online]. Available: http://arxiv.org/abs/1409.2574", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The NIST 2014 speaker recognition i-vector machine learning challenge", "author": ["C. Greenberg", "D. Bans\u00e9", "G. Doddington", "D. Garcia- Romero", "J. Godfrey", "T. Kinnunen", "A. Martin", "A. Mc- Cree", "M. Przybocki", "D. Reynolds"], "venue": "Odyssey 2014: The Speaker and Language Recognition Workshop, Joensuu, Finland, Jun. 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "CoRR, vol. abs/1412.2007, 2014. [Online]. Available: http://arxiv.org/abs/1412.2007", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Interspeech, Singapore, Sep. 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Score normalization for text-independent speaker verification systems", "author": ["R. Auckenthaler", "M. Carey", "H. Lloyd-Thomas"], "venue": "Digital Signal Processing, vol. 10, no. 1-3, pp. 42\u201354, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, vol. abs/1207.0580, 2012. [Online]. Available: http://arxiv.org/abs/1207.0580", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of ivector length normalization in speaker recognition systems", "author": ["D. Garcia-Romero", "C. Espy-Wilson"], "venue": "Interspeech, Florence, Italy, Aug. 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "By constraining the lexicon, text-dependent speaker verification aims to compensate for phonetic variability, which poses a significant challenge in speaker verification [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "6 seconds long global password relates to the Google Keyword Spotting system [2] and Google VoiceSearch [3] and facilitates the combination of the systems.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "6 seconds long global password relates to the Google Keyword Spotting system [2] and Google VoiceSearch [3] and facilitates the combination of the systems.", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "This paper focuses on text-dependent speaker verification for small footprint systems, as discussed in [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 95, "endOffset": 101}, {"referenceID": 5, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 95, "endOffset": 101}, {"referenceID": 6, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 7, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 4, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 5, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 8, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 236, "endOffset": 247}, {"referenceID": 9, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 236, "endOffset": 247}, {"referenceID": 10, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 236, "endOffset": 247}, {"referenceID": 11, "context": "Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14].", "startOffset": 137, "endOffset": 149}, {"referenceID": 12, "context": "Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14].", "startOffset": 137, "endOffset": 149}, {"referenceID": 13, "context": "Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14].", "startOffset": 137, "endOffset": 149}, {"referenceID": 14, "context": "For small footprint systems, however, a more direct deep learning modeling may be an attractive alternative [15, 4].", "startOffset": 108, "endOffset": 115}, {"referenceID": 3, "context": "For small footprint systems, however, a more direct deep learning modeling may be an attractive alternative [15, 4].", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "To the best of our knowledge, recurrent neural networks have been applied to related problems such as speaker identification [16] and language identification [17], but not to the speaker verification task.", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "To the best of our knowledge, recurrent neural networks have been applied to related problems such as speaker identification [16] and language identification [17], but not to the speaker verification task.", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "The proposed neural network architecture can be thought of as joint optimization of a generative-discriminative hybrid and is in the same spirit as deep unfolding [18] for adaptation.", "startOffset": 163, "endOffset": 167}, {"referenceID": 6, "context": "State-of-the art representations are a summary of frame-level information, such as i-vectors [7, 8] and d-vectors (Section 3).", "startOffset": 93, "endOffset": 99}, {"referenceID": 7, "context": "State-of-the art representations are a summary of frame-level information, such as i-vectors [7, 8] and d-vectors (Section 3).", "startOffset": 93, "endOffset": 99}, {"referenceID": 18, "context": "A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.", "startOffset": 62, "endOffset": 69}, {"referenceID": 3, "context": "A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.", "startOffset": 62, "endOffset": 69}, {"referenceID": 3, "context": "It includes a locally-connected layer [4] and several fully connected layers.", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "The complexity issue (but not the estimation issue) can be alleviated by candidate sampling [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7].", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7].", "startOffset": 244, "endOffset": 247}, {"referenceID": 20, "context": "1 and 3: a deep neural network (DNN) with locally-connected and fully connected layers as our baseline DNN in Section 3 and a long short-term memory recurrent neural network (LSTM) [21, 22] with a single output.", "startOffset": 181, "endOffset": 189}, {"referenceID": 21, "context": "1 and 3: a deep neural network (DNN) with locally-connected and fully connected layers as our baseline DNN in Section 3 and a long short-term memory recurrent neural network (LSTM) [21, 22] with a single output.", "startOffset": 181, "endOffset": 189}, {"referenceID": 1, "context": "The data were augmented by artificially adding in car and cafeteria noise at various SNRs, and simulating different distances between the speaker and the microphone, see [2] for further details.", "startOffset": 170, "endOffset": 173}, {"referenceID": 22, "context": "Results are reported in terms of equal error rate (EER), without and with t-norm score normalization [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "1 with a softmax layer and trained on train 2M (Table 1) with 50% dropout [24] in the linear layer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "frame i-vector [6] 5.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "11 i-vector+PLDA [25] 4.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "89 DNN, softmax [4] 3.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "The first baseline is based on [6], and uses 13 PLPs with first-order and second-order derivatives, 1024 Gaussians, and 300-dimensional i-vectors.", "startOffset": 31, "endOffset": 34}, {"referenceID": 24, "context": "The second baseline is based on [25] with 150 eigenvoices.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "Also, this baseline does not include other refining techniques such as \u201duncertainty training\u201d [10] that have", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Note that compared to [15], we have improved our d-vectors significantly [4].", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "Note that compared to [15], we have improved our d-vectors significantly [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 19, "context": "To reasonably scale the softmax layer to 80k speaker labels, we employed candidate sampling, similar to [20].", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system\u2019s components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domainspecific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal \u201dOk Google\u201d benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint.", "creator": "gnuplot 4.6 patchlevel 6"}}}