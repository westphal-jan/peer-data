{"id": "1509.08067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "abstract": "This paper presents a method, called\\ textit {AOGTracker}, for simultaneous tracking, learning and parsing (TLP) of objects in video sequences with a hierarchical and compositional and / or representation (AOG).Within our TLP framework, the AOG researches latent partial configurations to represent a target object. TLP is formulated in the Bayesque framework and a spatio-temporal dynamic programming algorithm is derived to derive object bounding boxes on the fly. During online learning, the AOG is discriminatively trained in the latent structural SVM framework to take into account the appearance (e.g. lighting and partial shutter) and structural (e.g. different poses and viewing angles) variations of the object, as well as the distortors (e.g. similar objects) in the background scene.", "histories": [["v1", "Sun, 27 Sep 2015 08:14:57 GMT  (4221kb,D)", "https://arxiv.org/abs/1509.08067v1", "14 pages"], ["v2", "Fri, 6 May 2016 01:10:42 GMT  (5270kb,D)", "http://arxiv.org/abs/1509.08067v2", "18 pages"], ["v3", "Wed, 11 May 2016 19:35:37 GMT  (6034kb,D)", "http://arxiv.org/abs/1509.08067v3", "18 pages"], ["v4", "Sun, 15 May 2016 06:02:48 GMT  (6397kb,D)", "http://arxiv.org/abs/1509.08067v4", "17 pages"], ["v5", "Wed, 18 May 2016 05:44:50 GMT  (5495kb,D)", "http://arxiv.org/abs/1509.08067v5", "17 pages"], ["v6", "Sat, 3 Sep 2016 00:26:58 GMT  (7977kb,D)", "http://arxiv.org/abs/1509.08067v6", "17 pages, Reproducibility: The source code is released with this paper for reproducing all results, which is available atthis https URL"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["tianfu wu", "yang lu", "song-chun zhu"], "accepted": false, "id": "1509.08067"}, "pdf": {"name": "1509.08067.pdf", "metadata": {"source": "CRF", "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "authors": ["Tianfu Wu", "Yang Lu", "Song-Chun Zhu"], "emails": ["wu@ncsu.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Visual Tracking, And-Or Graphs, Latent SVM, Dynamic Programming, Intrackability\nF"}, {"heading": "1 INTRODUCTION", "text": ""}, {"heading": "1.1 Motivation and Objective", "text": "O NLINE object tracking is an innate capability in human andanimal vision for learning visual concepts [7], and is an important task in computer vision. Given the state of an unknown object (e.g., its bounding box) in the first frame of a video, the task is to infer hidden states of the object in subsequent frames. Online object tracking, especially long-term tracking, is a difficult problem. It needs to handle variations of a tracked object, including appearance and structural variations, scale changes, occlusions (partial or complete), etc. It also needs to tackle complexity of the scene, including camera motion, background clutter, distractors, illumination changes, frame cropping, etc. Fig. 1 illustrates some typical issues in online object tracking. In recent literature, object tracking has received much attention due to practical applications in video surveillance, activity and event prediction, humancomputer interactions and traffic monitoring.\nThis paper presents an integrated framework for online tracking, learning and parsing (TLP) of unknown objects with a unified representation. We focus on settings in which object state is represented by bounding box, without using pre-trained models. We address five issues associated with online object tracking in\n\u2022 T.F. Wu is with the Department of Electrical and Computer Engineering and the Visual Narrative Cluster, North Carolina State University. This work was mainly done when T.F. Wu was research assistant professor at UCLA. E-mail: tianfu wu@ncsu.edu \u2022 Y. Lu is with the Department of Statistics, University of California, Los Angeles. E-mail: yanglv@ucla.edu \u2022 S.-C. Zhu is with the Department of Statistics and Computer Science, University of California, Los Angeles. E-mail: sczhu@stat.ucla.edu\nManuscript received MM DD, YYYY; revised MM DD, YYYY.\nthe following. Issue I: Expressive representation accounting for structural and appearance variations of unknown objects in tracking. We are interested in hierarchical and compositional object models. Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12]. A popular modeling scheme represents object categories by mixtures of deformable part-based models (DPMs) [1]. The number of mixture components is usually predefined and the part configuration of each component is fixed after initialization or directly based on strong supervision. In online tracking, since a tracker can only access the ground-truth object state in the first frame, it is not suitable for it to \u201cmake decisions\u201d on the number of mixture components and part configurations, and it does not have enough data to learn. It\u2019s desirable to have an object representation which has expressive power to represent a large number of part configurations, and\nar X\niv :1\n50 9.\n08 06\n7v 6\n[ cs\n.C V\n] 3\nS ep\n2 01\n6\ncan facilitate computationally effective inference and learning. We quantize the space of part configurations recursively in a principled way with a hierarchical and compositional And-Or graph (AOG) representation [8], [11]. We learn and update the most discriminative part configurations online by pruning the quantized space based on part discriminability.\nIssue II: Computing joint optimal solutions. Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14]. The likelihood or observation density is temporally inhomogeneous due to online updating of object models. Typically, the objective is to infer the most likely hidden state of a tracked object in a frame by maximizing a Bayesian marginal posterior probability given all the data observed so far. The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18]. In most prior approaches (e.g., the 29 trackers evaluated in the TB-100 benchmark [2]), no feedback inspection is applied to the history of inferred trajectory. We utilize tracking-by-parsing with hierarchical models in inference. By computing joint optimal solutions, we can not only improve prediction accuracy in a new frame by integrating past estimated trajectory, but also potentially correct errors in past estimated trajectory. Furthermore, we simultaneously address another key issue in online learning (Issue III).\nIssue III: Maintaining the purity of a training dataset. The dataset consists of a set of positive examples computed based on the current trajectory, and a set of negative examples mined from outside the current trajectory. In the dataset, we can only guarantee that the positives and the negatives in the first frame are true positives and true negatives respectively. A tracker needs to carefully choose frames from which it can learn to avoid model drifting (i.e., self-paced learning). Most prior approaches do not address this issue since they focus on marginally optimal solutions with which object models are updated, except for the P-N learning in TLD [17] and the self-paced learning for tracking [18]. Since we compute joint optimal solutions in online tracking, we can maintain the purity of an online collected training dataset in a better way.\nIssue IV: Failure-aware online learning of object models. In online learning, we mostly update model parameters incrementally after inference in a frame. Theoretically speaking, after an initial\nobject model is learned in the first frame, model drifting is inevitable in general setting. Thus, in addition to maintaining the purity of a training dataset, it is also important that we can identify critical moments (caused by different structural and appearance variations) automatically. At those moments, a tracker needs to re-learn both the structure and the parameters of object model using the current whole training dataset. We address this issue by computing uncertainty of an object model in a frame based on its response maps.\nIssue V: Computational efficiency by dynamic search strategy. Most tracking-by-detection methods run detection in the whole frame since they usually use relatively simple models such as a single object template. With hierarchical models in tracking and sophisticated online inference and updating strategies, the computational complexity is high. To speed up tracking, we need to utilize a dynamic search strategy. This strategy must take into account the trade-off between generating a conservative proposal state space for efficiency and allowing an exhaustive search for accuracy (e.g., to handle the situation where the object is completely occluded for a while or moves out the camera view and then reappears). We address this issue by adopting a simple search cascade with which we run detection in the whole frame only when local search has failed.\nOur TLP method obtains state-of-the-art performance on one popular tracking benchmark [2]. We give a brief overview of our method in the next subsection."}, {"heading": "1.2 Method Overview", "text": "As illustrated in Fig.2 (a), the TLP method consists of four components. We introduce them briefly as follows.\n(1) An AOG quantizing the space of part configurations. Given the bounding box of an object in the first frame, we assume object parts are also of rectangular shapes. We first divide it evenly into a small cell-based grid (e.g., 3 \u00d7 3) and a cell defines the smallest part. We then enumerate all possible parts with different aspect ratios and different sizes which can be placed inside the grid. All the enumerated parts are organized into a hierarchical and compositional AOG. Each part is represented by a terminalnode. Two types of nonterminal nodes as compositional rules: an And-node represents the decomposition of a large part into two smaller ones, and an Or-node represents alternative ways\nof decompositions through different horizontal or vertical binary splits. We call it the full structure AOG1. It is capable of exploring a large number of latent part configurations (see some examples in Fig. 2 (b)), meanwhile it makes the problem of online model learning feasible.\n(2) Learning object AOGs. An object AOG is a subgraph learned from the full structure AOG (see Fig. 2 (c) 2). Learning an object AOG consists of two steps: (i) The initial object AOG are learned by pruning branches of Or-nodes in the full structure AOG based on discriminative power, following breadth-first search (BFS) order. The discriminative power of a node is measured based on its training error rate. We keep multiple branches for each encountered Or-node to preserve ambiguities, whose training error rates are not bigger than the minimum one by a small positive value. (ii) We retrain the initial object AOG using latent SVM (LSVM) as it was done in learning the DPMs [1]. LSVM utilizes positive re-labeling (i.e., inferring the best configuration for each positive example) and hard negative mining. To further control the model complexity, we prune the initial object AOG through majority voting of latent assignments in positive re-labeling.\n(3) A spatial dynamic programming (DP) algorithm for computing all the proposals in a frame with the current object AOG. Thanks to the DAG structure of the object AOG, a DP parsing algorithm is utilized to compute the matching scores and the optimal parse trees of all sliding windows inside the search region in a frame. A parse tree is an instantiation of the object AOG which selects the best child for each encountered Or-node according to matching score. A configuration is obtained by collapsing a parse tree onto the image domain, capturing layout of latent parts of a tracked object in a frame.\n(4) A temporal DP algorithm for inferring the most likely trajectory. We maintain a DP table memorizing the candidate object states computed by the spatial DP in the past frames. Then, based on the first-order HMM assumption, a temporal DP algorithm is used to find the optimal solution for the past frames jointly with pair-wise motion constraints (i.e., the Viterbi path [14]). The joint solution can help correct potential tracking errors (i.e., false negatives and false positives collected online) by leveraging more spatial and temporal information. This is similar in spirit to methods of keeping N-best maximal decoder for part models [19] and maintaining diverse M-best solutions in MRF [20]."}, {"heading": "2 RELATED WORK", "text": "In the literature of object tracking, either single object tracking or multiple-object tracking, there are often two settings.\nOffline visual tracking [21], [22], [23], [24]. These methods assume the whole video sequence has been recorded, and consist of two steps. i) It first computes object proposals in all frames using some pre-trained detectors (e.g., the DPMs [1]) and then form \u201ctracklets\u201d in consecutive frames. ii) It seeks the optimal object trajectory (or trajectories for multiple objects) by solving an optimization problem (e.g., the K-shortest path or min-cost flow formulation) for the data association. Most work assumed firstorder HMMs in the formulation. Recently, Hong and Han [25]\n1. By full structure, it means all the possible compositions on top of the grid with binary composition being used for And-nodes\n2. We note that there are some Or-nodes in the object AOGs which have only one child node since they are subgraphs of the full structure AOG and we keep their original structures.\nproposed an offline single object tracking method by sampling tree-structured graphical models which exploit the underlying intrinsic structure of input video in an orderless tracking [26].\nOnline visual tracking for streaming videos. It starts tracking after the state of an object is specified in certain frame. In the literature, particle filtering [15] has been widely adopted, which approximately represents the posterior probability in a nonparametric form by maintaining a set of particles (i.e., weighted candidates). In practice, particle filtering does not perform well in high-dimensional state spaces. More recently, tracking-bydetection methods [16], [17] have become popular which learn and update object models online and encode the posterior probability using dense sampling through sliding-window based detection onthe-fly. Thus, object tracking is treated as instance-based object detection. To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]). Most popular methods also assume first-order HMMs except for the recently proposed online graph-based tracker [28]. There are four streams in the literature of online visual tracking:\ni) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.\nii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37]. The major limitation of appearance modeling of a tracked object is the lack of background models, especially in preventing model drift from distracotrs (e.g., players in sport games). Addressing this issue leads to discriminant tracking.\niii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].\nOur method belongs to the fourth stream of online visual tracking. Unlike predefined or fixed part configurations with starmodel structure used in previous work, our method learns both structure and appearance of object AOGs online, which is, to our knowledge, the first method to address the problem of online explicit structure learning in tracking. The advantage of introducing AOG representation are three-fold.\ni) More representational power: Unlike TLD [17] and many other methods (e.g., [18]) which model an object as a single template or a mixture of templates and thus do not perform well in tracking objects with large structural and appearance variations, an AOG represents an object in a hierarchical and compositional graph expressing a large number of latent part configurations.\nii) More robust tracking and online learning strategies: While the whole object has large variations or might be partially occluded from time to time during tracking, some other parts\nremain stable and are less likely to be occluded. Some of the parts can be learned to robustly track the object, which can also improve accuracy of appearance adaptation of terminalnodes. This idea is similar in spirit to finding good features to track objects [45], and we find good part configurations online for both tracking and learning. iii) Fine-grained tracking results: In addition to predicting bounding boxes of a tracked object, outputs of our AOGTracker (i.e., the parse trees) have more information which are potentially useful for other modules beyond tracking such as activity or event prediction.\nOur preliminary work has been published in [46] and the method for constructing full structure AOG was published in [8]. This paper extends them by: (i) adding more experimental results with state-of-the-art performance obtained and full source code released; (ii) elaborating details substantially in deriving the formulation of inference and learning algorithms; and (iii) adding more analyses on different aspects of our method. This paper makes three contributions to the online object tracking problem:\ni) It presents a tracking-learning-parsing (TLP) framework which can learn and track objects AOGs.\nii) It presents a spatial and a temporal DP algorithms for tracking-by-parsing with AOGs and outputs fine-grained tracking results using parse trees.\niii) It outperforms the state-of-the-art tracking methods in a recent public benchmark, TB-100 [2], and obtains comparable performance on a series of VOT benchmarks [4].\nPaper Organization. The remainder of this paper is organized as follows. Section 3 presents the formulation of our TLP framework under the Bayesian framework. Section 4 gives the details of spatial-temporal DP algorithm. Section 5 presents the online learning algorithm using the latent SVM method. Section 6 shows the experimental results and analyses. Section 7 concludes this paper and discusses issues and future work."}, {"heading": "3 PROBLEM FORMULATION", "text": ""}, {"heading": "3.1 Formulation of Online Object Tracking", "text": "In this section, we first derive a generic formulation from generative perspective in the Bayesian framework, and then derive the discriminative counterpart."}, {"heading": "3.1.1 Tracking with HMM", "text": "Let \u039b denote the image lattice on which video frames are defined. Denote a sequence of video frames within time range [1, T ] by,\nI1:T = {I1, \u00b7 \u00b7 \u00b7 , IT }. (1)\nDenote by Bt the bounding box of a target object in It. In online object tracking, B1 is given and Bt\u2019s are inferred by a tracker (t \u2208 [2, T ]). With first-order HMM, we have,\nThe prior model: B1 \u223c p(B1) , (2) The motion model: Bt|Bt\u22121 \u223c p(Bt|Bt\u22121) , (3)\nThe likelihood: It|Bt \u223c p(It|Bt). (4)\nThen, the prediction model is defined by, p(Bt|I1:t\u22121) = \u222b\n\u2126Bt\u22121\np(Bt|Bt\u22121)p(Bt\u22121|I1:t\u22121)dBt\u22121,\n(5)\nwhere \u2126Bt\u22121 is the candidate space of Bt\u22121, and the updating model is defined by,\np(Bt|I1:t) = p(It|Bt)p(Bt|I1:t\u22121)/p(It|I1:t\u22121), (6)\nwhich is a marginal posterior probability. The tracking result, the best bounding box B\u2217t , is computed by,\nB\u2217t = arg max Bt\u2208\u2126Bt p(Bt|I1:t), (7)\nwhich is usually solved using particle filtering [15] in practice. To allow feedback inspection of the history of a trajectory, we seek to maximize a joint posterior probability,\np(B1:t|I1:t) = p(B1:t\u22121|I1:t\u22121) p(Bt|Bt\u22121)p(It|Bt)\np(It|I1:t\u22121)\n= p(B1|I1) t\u220f i=2 p(Bi|Bi\u22121)p(Ii|Bi) p(Ii|I1:i\u22121) . (8)\nBy taking the logarithm of both sides of Eqn.(8), we have,\nB\u22171:t = arg max B1:t log p(B1:t|I1:t)\n= arg max B1:t {log p(B1) + log p(I1|B1)+\nt\u2211 i=2 [log p(Bi|Bi\u22121) + log p(Ii|Bi)]}. (9)\nwhere the image data term p(I1) and \u2211t i=2 p(Ii|I1:i\u22121) are not included in the maximization as they are treated as constant terms. Since we have ground-truth for B1, p(I1|B1) can also be treated as known after the object model is learned based on B1. Then, Eqn.(9) can be reproduced as,\nB\u22172:t = arg max B2:t log p(B2:t|I1:t, B1) (10)\n= arg max B2:t { t\u2211 i=2 [log p(Bi|Bi\u22121) + log p(Ii|Bi)]}."}, {"heading": "3.1.2 Tracking as Energy Minimization over Trajectories", "text": "To derive the discriminative formulation of Eqn.(10), we show that only the log-likelihood ratio matters in computing log p(Ii|Bi) in Eqn.(10) with very mild assumptions.\nLet \u039bBi be the image domain occupied by a tracked object, and \u039bBi the remaining domain (i.e., \u039bBi \u222a \u039bBi = \u039b and \u039bBi \u2229\u039bBi = \u2205) in a frame Ii. With the independence assumption between I\u039bBi and I\u039bBi given Bi, we have,\np(Ii|Bi) = p(I\u039bBi , I\u039bBi |Bi) = p(I\u039bBi |Bi)p(I\u039bBi |Bi)\n= p(I\u039bBi |Bi)q(I\u039bBi ) = q(I\u039b) p(I\u039bBi |Bi) q(I\u039bBi ) , (11)\nwhere q(I\u039b) is the probability model of background scene and we have q(I\u039bBi ) = p(I\u039bBi |Bi) w.r.t. context-free assumption. So, q(I\u039b) does not need to be specified explicitly and can be omitted in the maximization. This derivation gives an alternative explanation for discriminant tracking v.s. tracking by generative appearance modeling of an object [47].\nBased on Eqn.(10), we define an energy function by,\nE(B2:t|I1:t, B0) \u221d \u2212 log p(B2:t|I1:t, B1). (12)\nAnd, we do not compute log p(Ii|Bi) in the probabilistic way, instead we compute matching score defined by,\nScore(Ii|Bi) = log p(I\u039bBi |Bi) q(I\u039bBi )\n(13)\n= log p(Ii|Bi)\u2212 log q(I\u039b).\nwhich we can apply discriminative learning methods. Also, denote the motion cost by,\nCost(Bi|Bi\u22121) = \u2212 log p(Bi|Bi\u22121). (14)\nWe use a thresholded motion model in experiments: the cost is 0 if the transition is accepted based on the median flow [17] (which is a forward-backward extension of the Lucas-Kanade optimal flow [48]) and +\u221e otherwise. A similar method was explored in [18].\nSo, we can re-write Eqn.(10) in the minimization form,\nB\u22172:t = arg min B2:t E(B2:t|I1:t, B1) (15)\n= arg min B2:t { t\u2211 i=2 [Cost(Bi|Bi\u22121)\u2212 Score(Ii|Bi)]}.\nIn our TLP framework, we compute Score(Ii|Bi) in Eqn.( 15) with an object AOG. So, we interpret a sliding window by the optimal parse tree inferred from object AOG. We treat parts as latent variables which are modeled to leverage more information for inferring object bounding box. We note that we do not track parts explicitly in this paper."}, {"heading": "3.2 Quantizing the Space of Part Configurations", "text": "In this section, we first present the construction of a full structure AOG which quantizes the space of part configurations. We then introduce notations in defining an AOG.\nPart configurations. For an input bounding box, a part configuration is defined by a partition with different number of parts of different shapes (see Fig. 3 (a)). Two natural questions arise: (i) How many part configurations (i.e., the space) can be defined in a bounding box? (ii) How to organize them into a compact representation? Without posing some structural constraints, it is a combinatorial problem.\nWe assume rectangular shapes are used for parts. Then, a configuration can be treated as a tiling of input bounding box using either horizontal or vertical cuts. We utilize binary splitting\nrule only in decomposition (see Fig. 3 (b) and (c)). With these two constraints, we represent all possible part configurations by a hierarchical and compositional AOG constructed in the following.\nGiven a bounding box, we first divide it evenly into a cellbased grid (e.g., 9 \u00d7 10 grid in the right of Fig. 4). Then, in the grid, we define a dictionary of part types and enumerate all instances for all part types.\nA dictionary of part types. A part type is defined by its width and height. Starting from some minimal size (such as 2\u00d7 2 cells), we enumerate all possible part types with different aspect ratios and sizes which fit the grid (see A,B,C,D in Fig.4 (a)).\nPart instances. An instance of a part type is obtained by placing the part type at a position. Thus, a part instance is defined by a \u201csliding window\u201d in the grid. Fig.4 (b) shows an example of placing part type D (2\u00d75 cells) in a 9\u00d710 grid with 48 instances in total.\nTo represent part configurations compactly, we exploit the compositional relationships between enumerated part instances.\nThe full structure AOG. For any sub-grid indexed by the left-top position, width and height (e.g., (2, 3, 5, 2) in the rightmiddle of Fig.4 (c)), we can either terminate it directly to the corresponding part instance (Fig.4 (c.1)), or decompose it into two smaller sub-grids using either horizontal or vertical binary splits. Depending on the side length, we may have multiple valid splits along both directions (Fig.4 (c.2)). When splitting either side we allow overlaps between the two sub-grids up to some ratio (Fig.4 (c.3)). Then, we represent the sub-grid as an Or-node, which has a set of child nodes including a terminal-node (i.e. the part instance directly terminated from it), and a number of And-nodes (each of which represents a valid decomposition). This procedure is applied recursively for all child sub-grids. Starting\nfrom the whole grid and using BFS order, we construct a full structure AOG, all summarized in Algorithm 1 (see Fig. 5 for an example). Table. 1 lists the number of part configurations for three cases from which we can see that full structure AOGs cover a large number of part configurations using a relatively small set of part instances.\nInput: Image grid \u039b with W \u00d7H cells; Minimal size of a part type (w0, h0); Maximal overlap ratio r between two sub-grids. Output: The And-Or graph G =< V,E > (see Fig.5) Initialization: Create an Or-node O\u039b for the grid \u039b, V = {O\u039b}, E = \u2205, BFSqueue= {O\u039b}; while BFSqueue is not empty do\nPop a node v from the BFSqueue; if v is an Or-node then\ni) Add a terminal-node t (i.e. the part instance) V = V \u222a {t}, E = E \u222a {< v, t >}; ii) Create And-nodes Ai for all valid cuts; E = E \u222a {< v,Ai >}; if Ai /\u2208 V then\nV = V \u222a {Ai}; Push Ai to the back of BFSqueue;\nend else if v is an And-node then\nCreate two Or-nodes Oi for the two sub-grids; E = E \u222a {< v,Oi >}; if Oi /\u2208 V then\nV = V \u222a {Oi}; Push Oi to the back of BFSqueue;\nend end\nend Algorithm 1: Constructing the grid AOG using BFS\nWe denote an AOG by,\nG = (VAnd, VOr, VT , E,\u0398) (16)\nwhere VAnd, VOr and VT represent a set of And-nodes, Or-nodes and terminal-nodes respectively, E a set of edges and \u0398 a set of\nparameters (to be defined in Section 4.1). We have,\ni) The object/root Or-node (plotted by green circles), which represents alternative object configurations;\nii) A set of And-nodes (solid blue circles), each of which represents the rule of decomposing a complex structure (e.g., a walking person or a running basketball player) into simpler ones;\niii) A set of part Or-nodes, which handle local variations and configurations in a recursive way; iv) A set of terminal-nodes (red rectangles), which link an object and its parts to image data (i.e., grounding symbols) to account for appearance variations and occlusions (e.g., head-shoulder of a walking person before and after opening a sun umbrella).\nAn object AOG is a subgraph of a full structure AOG with the same root Or-node. For notational simplicity, we also denote by G an object AOG. So, we will write Score(Ii|Bi;G) in Eqn.( 15) with G added.\nA parse tree is an instantiation of an object AOG with the best child node (w.r.t. matching scores) selected for each encountered Or-node. All the terminal-nodes in a parse tree represents a part configuration when collapsed to image domain.\nWe note that an object AOG contains multiple parse trees to preserve ambiguities in interpreting a tracked object (see examples in Fig. 2 (c) and Fig. 7)."}, {"heading": "4 TRACKING-BY-PARSING WITH OBJECT AOGS", "text": "In this section, we present details of inference with object AOGs. We first define scoring functions of nodes in an AOG. Then, we present a spatial DP algorithm for computing Score(Ii|Bi;G), and a temporal DP algorithm for inferring the trajectory B\u22172:t in Eqn.(15)."}, {"heading": "4.1 Scoring Functions of Nodes in an AOG", "text": "Let F be the feature pyramid computed for either the local ROI or the whole image It, and \u039b the position space of pyramid F. Let p = (l, x, y) \u2208 \u039b specify a position (x, y) in the l-th level of pyramid F.\nGiven an AOG G = (VT , VAnd, VOr, E,\u0398) (e.g., the left in Fig.6), we define four types of edges, i.e., E = ET \u222a EDef \u222a EDec \u222a ESwitch as shown in Fig.6. We elaborate the definitions of parameters \u0398 = (\u0398app,\u0398def ,\u0398bias): i) Each terminal-node t \u2208 VT has appearance parameters \u03b8appt \u2282\n\u0398app, which is used to ground a terminal-node to image data. i) The parent And-node A of a part terminal-node with defor-\nmation edge has deformation parameters \u03b8defA \u2282 \u0398def . They are used for penalizing local displacements when placing a terminal-node around its anchor position. We note that the object template is not allowed to perturb locally in inference since we infer the optimal part configuration for each given object location in the pyramid with sliding window technique used, as done in the DPM [1], so the parent And-node of the object terminal-node does not have deformation parameters. iii) A child And-node of the root Or-node has a bias term \u0398bias = {b}. We do not define bias terms for child nodes of other Ornodes.\nAppearance Features. We use three types of features: histogram of oriented gradient (HOG) [49], local binary pattern features (LBP) [50], and RGB color histograms (for color videos).\nDeformation Features. Denote by \u03b4 = [dx, dy] the displacement of placing a terminal-node around its anchor location. The deformation feature is defined by \u03a6def (\u03b4) = [dx2, dx, dy2, dy]\u2032 as done in DPMs [1]. We use linear functions to evaluate both appearance scores and deformation scores. The score functions of nodes in an AOG are defined as follows:\ni) For a terminal-node t, its score at a position p is computed by,\nScore(t, p|F) =< \u03b8appt ,F(t, p) > (17)\nwhere < \u00b7, \u00b7 > represents inner product and F(t, p) extracts features in feature pyramid. ii) For an Or-node O, its score at position p takes the maximum score over its child nodes,\nScore(O, p|F) = max c\u2208ch(O) Score(c, p|F) (18)\nwhere ch(v) denotes the set of child nodes of a node v. iii) For an And-node A, we have three different functions w.r.t.\nthe type of its out-edge (i.e., Terminal-, Deformation-, or Decomposition-edge),\nScore(A, p|F) = (19) Score(t, p|F), eA,t \u2208 ET max\u03b4[Score(t, p\u2295 \u03b4|F)\u2212 < \u03b8defA ,\u03a6def (\u03b4) >], eA,t \u2208 EDef\u2211 c\u2208ch(A) Score(c, p|F), eA,c \u2208 EDec\nwhere the first case is for sharing score maps between the object terminal-node and its parent And-node since we do not allow local deformation for the whole object, the second case for computing transformed score maps of parent Andnode of a part terminal-node which is allowed to find the best placement through distance transformation [1], \u2295 represents the displacement operator in the position space in \u039b, and\nthe third case for computing the score maps of an And-node which has two child nodes through composition."}, {"heading": "4.2 Tracking-by-Parsing", "text": "With scoring functions defined above, we present a spatial DP and a temporal DP algorithms in solving Eqn.(15).\nSpatial DP: The DP algorithm (see Algorithm 2) consists of two stages: (i) The bottom-up pass computes score map pyramids (as illustrated in Fig. 6) for all nodes following the depth-firstsearch (DFS) order of nodes. It computes matching scores of all possible parse trees at all possible positions in feature pyramid. (ii) In the top-down pass, we first find all candidate positions for the root Or-node O based on its score maps and current threshold \u03c4G of the object AOG, denoted by\n\u2126cand = {p; Score(O, p|F) \u2265 \u03c4G and p \u2208 \u039b}. (20)\nThen, following BFS order of nodes, we retrieve the optimal parse tree at each p \u2208 P: starting from the root Or-node, we select the optimal branch (with the largest score) of each encountered Ornode, keep the two child nodes of each encountered And-node, and retrieve the optimal position of each encountered part terminalnode (by taking arg max for the second case in Eqn.(19)).\nAfter spatial parsing, we apply non-maximum suppression (NMS) in computing the optimal parse trees with a predefined intersection-over-union (IoU) overlap threshold, denoted by \u03c4NMS. We keep top Nbest parse trees to infer the best B\u2217t together with a temporal DP algorithm, similar to the strategies used in [19], [20].\nInput: An image Ii, a bounding box Bi, and an AOG G Output: Score(Ii|Bi;G) in Eqn.(8) and the optimal\nconfiguration C\u2217i from the parse tree for the object at frame i.\nInitialization: Build the depth-first search (DFS) ordering queue (QDFS) of all nodes in the AOG; Step 0: Compute scores for all nodes in QDFS ; while QDFS is not empty do\nPop a node v from the QDFS ; if v is an Or-node then\nScore(v) = maxu\u2208ch(v) Score(u); // ch(v) is the set of child nodes of v\nelse if v is an And-node then Score(v) = \u2211 u\u2208ch(v) LocalMax(Score(u)) else if v is a Terminal-node then Compute the filter response map for IN (\u039bv). // N (\u039bv) represents the image domain of the LocalMax operation of Terminal-node v.\nend end Score(Ii|Bi;G) = Score(RootOrNode).; Step 1: Compute C\u2217i using the breadth-first search; QBFS = {RootOrNode}, C\u2217i = (Bi), k = 1; while QBFS is not empty do\nPop a node v from the QBFS ; if v is an Or-node then\nPush the child node u with maximum score into QBFS(i.e., Score(u)=Score(v)).\nelse if v is an And-node then Push all the child nodes v\u2019s into QBFS . else if v is a Terminal-node then Add B(k)i = Deformed(\u039bv) to C\u2217i = (C\u2217i , B (k) i ).\nIncrease k = k + 1. end\nend Algorithm 2: The spatial DP algorithm for parsing with the AOG, Parse(Ii|Bi;G)\nTemporal DP: Assuming that all the N-best candidates for B2, \u00b7 \u00b7 \u00b7 , Bt are memoized after running spatial DP algorithm in I2 to It, Eqn.(15) corresponds to the classic DP formulation of forward and backward inference for decoding HMMs with \u2212Score(Ii|Bi;G) being the singleton \u201cdata\u201d term and Cost(Bi|Bi\u22121) the pairwise cost term.\nLet Bi[Bi] be energy of the best object states in the first i frames with the constraint that the i-th one is Bi. We have,\nB1[B1] = \u2212Score(I1|B1;G), Bi[Bi] = \u2212Score(Ii|Bi;G)\n+ min Bi\u22121\n(Bi\u22121[Bi\u22121] + Cost(Bi|Bi\u22121)). (21)\nWhen B1 is the input bounding box. Then, the temporal DP algorithm consists of two steps:\ni) The forward step for computing all Bi[Bi]\u2019s, and caching the optimal solution for Bi\u22121 as a function of Bi for later back-tracing starting at i = 2,\nTi[Bi] = arg min Bi\u22121 {Bi\u22121[Bi\u22121] + Cost(Bi|Bi\u22121)}.\nii) The backward step for finding the optimal trajectory (B1, B \u2217 2 , \u00b7 \u00b7 \u00b7 , B\u2217t ), where we first take,\nB\u2217t = arg min Bt Bt[Bt], (22)\nand then in the order of i = t\u2212 1, \u00b7 \u00b7 \u00b7 , 2 trace back,\nB\u2217i = Ti+1[B \u2217 i+1]. (23)\nIn practice, we often do not need to run temporal DP in the whole time range [1, t], especially for long-term tracking, since the target object might have changed significantly or we might have camera motion, instead we only focus on some short time range, [t\u2212\u2206t, t] (see settings in experiments).\nRemarks: In our TLP method, we apply the spatial and the temporal DP algorithms in a stage-wise manner and without tracking parts explicitly. Thus, we do not introduce loops in inference. If we instead attempt to learn a joint spatial-temporal AOG, it will be a much more difficult problem due to loops in joint spatial-temporal inference, and approximate inference is used.\nSearch Strategy: During tracking, at time t, Bt is initialized by Bt\u22121, and then a rectangular region of interest (ROI) centered at the center of Bt is used to compute feature pyramid and run parsing with AOG. The ROI is first computed as a square area with the side length being sROI times longer than the maximum of width and height of Bt and then is clipped with the image domain. If no candidates are found (i.e., \u2126cand is empty), we will run the parsing in whole image domain. So, our AOGTracker is capable of re-detecting a tracked object. If there are still no candidates (e.g., the target object was completely occluded or went out of camera view), the tracking result of this frame is set to be invalid and we do not need to run the temporal DP."}, {"heading": "4.3 The Trackability of an Object AOG", "text": "To detect critical moments online, we need to measure the quality of an object AOG, G at time t. We compute its trackability based on the score maps in which the optimal parse tree is placed. For each node v in the parse tree, we have its position in score map pyramid (i.e., the level of pyramid and the location in that level), (lv, xv, yv). We define the trackability of node v by,\nTrackability(v|It,G) = S(lv, xv, yv)\u2212 \u00b5S (24)\nwhere S(lv, xv, yv) is the score of node v, \u00b5S the mean score computed from the whole score map. Intuitively, we expect the score map of a discriminative node v has peak and steep landscape, as investigated in [51]. The trackabilities of part nodes are used to infer partial occlusion and local structure variations, and trackability of the inferred parse tree indicate the \u201cgoodness\u201d of current object AOG. We note that we treat trackability and intrackability (i.e., the inverse of th trackability) exchangeably. More sophisticated definitions of intrackability in tracking are referred to [52].\nWe model trackability by a Gaussian model whose mean and standard derivation are computed incrementally in [2, t]. At time t, a tracked object is said to be \u201cintrackable\u201d if its trackability is less than meantrackability(t) \u2212 3 \u00b7 stdtrackability(t). We note that the tracking result could be still valid even if it is \u201cintrackable\u201d (e.g., in the first few frames in which the target object is occluded partially, especially by similar distractors)."}, {"heading": "5 ONLINE LEARNING OF OBJECT AOGS", "text": "In this section, we present online learning of object AOGs, which consists of three components: (i) Maintaining a training dataset based on tracking results; (ii) Estimating parameters of a given object AOG; and (iii) Learning structure of the object AOG by pruning full structure AOG, which requires (ii) in the process."}, {"heading": "5.1 Maintaining the Training Dataset Online", "text": "Denote by Dt = D + t \u222a D\u2212t the training dataset at time t, consisting of D+t , a positive dataset, and D \u2212 t , a negative dataset.\nIn the first frame, we have D+1 = {(I1, B1)} and let B1 = (x1, y1, w1, h1). We augment it with eight locally shifted positives, i.e., {I1, B1,i; i = 1, \u00b7 \u00b7 \u00b7 , 8} where x1,i \u2208 {x1 \u00b1 d} and y1,i \u2208 {y\u00b1d} with width and height not changed. d is set to the cell size in computing HOG features. The initial D\u22121 uses the whole remaining image I\u039bB1 for mining hard negatives in training.\nAt time t, if Bt is valid according to tracking-by-parsing, we have D+t = D + t\u22121 \u222a {(It, Bt)}, and add to D \u2212 t all other candidates in \u2126cand (Eqn. 20) which are not suppressed by Bt according to NMS (i.e., hard negatives). Otherwise, we have Dt = Dt\u22121."}, {"heading": "5.2 Estimating Parameters of a Given Object AOG", "text": "We use latent SVM method (LSVM) [1]. Based on the scoring functions defined in Section 4.1, we can re-write the scoring function of applying a given object AOG, G on a training example (denoted by IB for simplicity),\nScore(IB ;G) = max pt\u2208\u2126G < \u0398,\u03a6(F, pt) > (25)\nwhere pt represents a parse tree, \u2126G the space of parse trees, \u0398 the concatenated vector of all parameters, \u03a6(F, pg) the concatenated vector of appearance and deformation features in feature pyramid F w.r.t. parse tree pt, and the bias term.\nThe objective function in estimating parameters is defined by the l2-regularized empirical hinge loss function,\nLDt(\u0398) = 1\n2 ||\u0398||22+\nC |Dt| [ \u2211\nIB\u2208D+t\nmax(0, 1\u2212 Score(IB ;G))\n\u2211 IB\u2208D\u2212t max(0, 1 + Score(IB ;G))] (26)\nwhere C is the trade-off parameter in learning. Eqn.( 26) is a semiconvexity function of the parameters \u0398 due to the empirical loss term on positives.\nIn optimization, we utilize an iterative procedure in a \u201ccoordinate descent\u201d way. We first convert the objective function to a convex function by assigning latent values for all positives using the spatial DP algorithm. Then, we estimate parameters. While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54]. The detection threshold, \u03c4G is estimated as the minimum score of positives.\n3. We reimplemented the matlab code available at http://www.cs.ubc.ca/\u223cschmidtm/Software/minConf.html in c++."}, {"heading": "5.3 Learning Object AOGs", "text": "With the training dataset Dt and the full structure AOG constructed based on B1, an object AOG is learned in three steps:\ni) Evaluating the figure of merits of nodes in the full structure AOG. We first train the root classifier (i.e., object appearance parameters and bias term) by linear SVM using D+t and datamining hard negatives in D\u2212t . Then, the appearance parameters for each part terminal-node t is initialized by cropping out the corresponding portion in the object template 4. Following DFS order, we evaluate the figure of merit of each node in the full structure AOG by its training error rate. The error rate is calculated on Dt where the score of a node is computed w.r.t. scoring functions defined in Section 4.1. The smaller the error rate is, the more discriminative a node is.\nii) Retrieving an initial object AOG and re-estimating parameters. We retrieve the most discriminative subgraph in the full structure AOG as initial object AOG. Following BFS order, we start from the root Or-node, select for each encountered Or-node the best child node (with the smallest training error rate among all children) and the child nodes whose training error rates are not\n4. We also tried to train the linear SVM classifiers for all the terminal-nodes individually using cropped examples, which increases the runtime, but does not improve the tracking performance in experiments. So, we use the simplified method above.\nbigger than that of the best child by some predefined small positive value (i.e., preserving ambiguities), keep the two child nodes for each encountered And-node, and stop at each encountered terminal-node. We show two examples in the left of Fig. 7. We train the parameters of initial object AOG using LSVM [1] with two rounds of positive re-labeling and hard negative mining respectively.\niii) Controlling model complexity. To do that, a refined object AOG for tracking is obtained by further selecting the most discriminative part configuration(s) in the initial object AOG learned in the step ii). The selection process is based on latent assignment in relabeling positives in LSVM training. A part configuration in the initial object AOG is pruned if it relabeled less than 10% positives (see the right of Fig. 7). We further train the refined object AOG with one round latent positive re-labeling and hard negative mining. By reducing model complexity, we can speed up the tracking-by-parsing procedure.\nVerification of a refined object AOG. We run parsing with a refined object AOG in the first frame. The refined object AOG is accepted if the score of the optimal parse tree is greater than the threshold estimated in training and the IoU overlap between the predicted bounding box and the input bounding box is greater than or equals the IoU NMS threshold, \u03c4NMS in detection.\nIdentifying critical moments in tracking. A critical moment means a tracker has become \u201cuncertain\u201d and at the same time accumulated \u201cenough\u201d new samples, which is triggered in tracking when two conditions were satisfied. The first is that the number of frames in which a tracked object is \u201cintrackable\u201d was larger than some value, NIntrackable. The second is that the number of new valid tracking results are greater than some value, NNewSample. Both are accumulated from the last time an object AOG was re-learned.\nThe spatial resolution of placing parts. In learning object AOGs, we first place parts at the same spatial resolution as the object. If the learned object AOG was not accepted in verification, we then place parts at twice the spatial resolution w.r.t. the object and re-learn the object AOG. In our experiments, the two specifications handled all testing sequences successfully.\nOverall flow of online learning. In the first frame or when a critical moment is identified in tracking, we learn both structure and parameters of an object AOG, otherwise we update parameters\nonly if the tracking result is valid in a frame based on tracking-byparsing."}, {"heading": "6 EXPERIMENTS", "text": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4]. We also analyze different aspects of our method. The source code 5 is released with this paper for reproducing all results. We denote the proposed method by AOG in tables and plots.\nParameter Setting. We use the same parameters for all experiments since we emphasize online learning in this paper. In learning object AOGs, the side length of the grid used for constructing the full structure AOG is either 3 or 4 depending the slide length of input bounding box (to reduce the time complexity of online learning). The number of intervals in computing feature pyramid is set to 6 with cell size being 4. The factor s in computing search ROI is set to sROI = 3. The NMS IoU threshold is set to \u03c4NMS = 0.7. The number of top parse trees kept after spatial DP parsing is set NBest = 10. The time range in temporal DP algorithm is set to \u2206t = 5. In identifying critical moments, we set NIntrackable = 5 and NNewSample = 10. The LSVM trade-off parameter in Eqn.(26) is set to C = 0.001. When re-learning structure and parameters, we could use all the frames with valid tracking results. To reduce the time complexity, the number of frames used in relearning is at most 100 in our experiments. At time t, we first take the first 10 frames with valid tracking results in [1, t] with the underlying intuition that they have high probabilities of being tracked correctly (note that we alway use the first frame since the ground-truth bounding box is given), and then take the remaining frames in reversed time order.\nSpeed. In our current c++ implementation, we adopt FFT in computing score pyramids as done in [54] which also utilizes multi-threads with OpenMP. We also provide a distributed version\n5. Available at https://github.com/tfwu/RGM-AOGTracker\nMetric Success Rate / Precision Rate Evaluation OPE SRE TRE\nSubset 100 50 CVPR2013 100 50 CVPR2013 100 50 CVPR2013\nAOG Gain 13.93 / 18.06 16.84 / 22.23 2.74 / 19.37 11.47 / 16.79 12.52 / 17.82 11.89 / 17.55 9.25 / 11.06 11.37 / 14.61 11.59 / 14.38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]\nSubsets in TB-50 DEF(23) FM(25) MB(19) IPR(29) BC(20) OPR(32) OCC(29) IV(22) LR(8) SV(38) OV(11)\nAOG Gain (success rate) 15.89 15.56 17.29 12.29 17.81 14.04 14.7 15.73 6.65 18.38 15.99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]\nbased on MPI 6 in evaluation. The FPS is about 2 to 3. We are experimenting GPU implementations to speed up our TLP.\n6. https://www.mpich.org/"}, {"heading": "6.1 Results on TB-50/100/CVPR2013", "text": "The TB-100 benchmark has 100 target objects (58, 897 frames in total) with 29 publicly available trackers evaluated. It is extended from a previous benchmark with 51 target objects released at CVPR2013 (denoted by TB-CVPR2013). Further, since some target objects are similar or less challenging, a subset of 50 difficult\nand representative ones (denoted by TB-50) is selected for an indepth analysis. Two types of performance metric are used, the precision plot (i.e., the percentage of frames in which estimated locations are within a given threshold distance of ground-truth positions) and the success plot (i.e., based on IoU overlap scores which are commonly used in object detection benchmarks, e.g., PASCAL VOC [79]). The higher a success rate or a precision rate is, the better a tracker is. Usually, success plots are preferred to rank trackers [2], [4] (thus we focus on success plots in comparison). Three types of evaluation methods are used as illustrated in Fig.8. To account for different factors of a test sequence affecting performance, the testing sequences are further categorized w.r.t. 11 attributes for more ind-depth comparisons: (1) Illumination Variation (IV, 38/22/21 sequences in TB-100/50/CVPR2013), (2) Scale Variation (SV, 64/38/28 sequences), (3) Occlusion (OCC, 49/29/29 sequences), (4) Deformation (DEF, 44/23/19 sequences), (5) Motion Blur (MB, 29/19/12 sequences), (6) Fast Motion\n(FM, 39/25/17 sequences), (7) In-Plane Rotation (IPR, 51/29/31 sequences), (8) Out-of-Plane Rotation (OPR, 63/32/39 sequences), (9) Out-of-View (OV, 14/11/6 sequences), (10) Background Clutters (BC, 31/20/21 sequences), and (11) Low Resolution (LR, 9/8/4 sequences). More details on the attributes and their distributions in the benchmark are referred to [2], [3].\nTable. 2 lists the 29 evaluated tracking algorithms which are categorized based on representation and search scheme. See more details about categorizing these trackers in [2]. In TB-CVPR2013, two recent trackers trained by deep convolutional network (CNT [5], SO-DLT [6]) were evaluated using OPE.\nWe summarize the performance gain of our AOGTracker in Table.3. Our AOGTracker obtains significant improvement (more than 12%) in the 10 subsets in TB-50. Our AOGTracker handles out-of-view situations much better than other trackers since it is capable of re-detecting target objects in the whole image, and it performs very well in the scale variation subset (see examples in the second and fourth rows in Fig. 11) since it searches over feature pyramid explicitly (with the expense of more computation). Our AOGTracker obtains the least improvement in the lowresolution subset since it uses HOG features and the discrepancy\nbetween HOG cell-based coordinate and pixel-based one can cause some loss in overlap measurement, especially in the low resolution subset. We will add automatic selection of feature types (e.g., HOG v.s. pixel-based features such as intensity and gradient) according to the resolution, as well as other factors in future work.\nFig.9 shows success plots of OPE, SRE and TRE in TB100/50/CVPR2013. Our AOGTracker consistently outperforms all other trackers. We note that for OPE in TB-CVPR2013, although the improvement of our AOGTracker over the SO-DLT [6] is not very big, the SO-DLT utilized two deep convolutional networks with different model update strategies in tracking, both of which are pretrained on the ImageNet [34]. Fig. 11 shows some qualitative results."}, {"heading": "6.2 Analyses of AOG models and the TLP Algorithm", "text": "To analyze contributions of different components in our AOGTracker, we compare performance of six different variants\u2013 three different object representation schema: AOG with and without structure re-learning (denoted by AOG and AOGFixed respectively), and whole object template only (i.e., without part configurations, denoted by ObjectOnly), and two different inference\nstrategies for each representation scheme: inference with and without temporal DP (denoted by -st and -s respectively). As stated above, we use a very simple setting for temporal DP which takes into account \u2206t = 5 frames, [t\u2212 5, t] in our experiments.\nFig. 12 shows performance comparison of the six variants. AOG-st obtains the best overall performance consistently. Trackers with AOG perform better than those with whole object template only. AOG structure re-learning has consistent overall performance improvement. But, we observed that AOGFixed-st works slightly better than AOG-st on two subsets out of 11, Motion-Blur and Out-of-View, on which the simple intrackability measurement is not good enough. For trackers with AOG, temporal DP helps improve performance, while for trackers with whole object templates only, the one without temporal DP (ObjectOnly-s) slightly outperform the one with temporal DP (ObjectOnly-st), which shows that we might need strong enough object models in integrating spatial and temporal information for better performance."}, {"heading": "6.3 Comparison with State-of-the-Art Methods", "text": "We explain why our AOGTracker outperforms other trackers on the TB-100 benchmark in terms of representation, online learning and inference.\nRepresentation Scheme. Our AOGTracker utilizes three types of complementary features (HOG+LBP+Color) jointly to capture appearance variations, while most of other trackers use simpler ones (e.g., TLD [17] uses intensity based Haar like features). More importantly, we address the issue of learning the optimal deformable part-based configurations in the quantized space of latent object structures, while most of other trackers focus on either whole objects [58] or implicit configurations (e.g., the random fern forest used in TLD). These two components are integrated in a latent structured-output discriminative learning framework, which improves the overall tracking performance (e.g., see comparisons in Fig. 12).\nOnline Learning. Our AOGTracker includes two components which are not addressed in all other trackers evaluated on TB-100: online structure re-learning based on intrackability, and a simple temporal DP for computing optimal joint solution. Both of them improve the performance based on our ablation experiments. The former enables our AOGTracker to capture both large structural and sudden appearance variations automatically, which is especially important for long-term tracking. In addition to improve the prediction performance, the latter improves the capability of maintaining the purity of online collected training dataset.\nInference. Unlike many other trackers which do not handle scale changes explicitly (e.g., CSK [58] and STRUCK [40]), our AOGTracker runs tracking-by-parsing in feature pyramid to detect scale changes (e.g., the car example in the second row in Fig. 11). Our AOGTracker also utilizes a dynamic search strategy which re-detects an object in whole frame if local ROI search failed. For example, our AOGTracker handles out-of-view situations much better than other trackers due to the re-detection component (see examples in the fourth row in Fig. 11).\nLimitations. All the performance improvement stated above are obtained at the expense of more computation in learning and tracking. Our AOGTracker obtains the least improvement in the low-resolution subset since it uses HOG features and the discrepancy between HOG cell-based coordinate and pixel-based one can cause some loss in overlap measurement, especially in the low resolution subset. We will add automatic selection of feature\ntypes (e.g., HOG v.s. pixel-based features such as intensity and gradient) according to the resolution, as well as other factors in future work."}, {"heading": "6.4 Results on VOT", "text": "In VOT, the evaluation focuses on short-term tracking (i.e., a tracker is not expected to perform re-detection after losing a target object), so the evaluation toolkit will re-initialize a tracker after it loses the target (w.r.t. the condition the overlap between the predicted bounding box and the ground-truth one drops to zero) with the number of failures counted. In VOT protocol, a tracker is tested on each sequence multiple times. The performance is measured in terms of accuracy and robustness. Accuracy is computed as the average of per-frame accuracies which themselves are computed by taking the average over the repetitions. Robustness is computed as the average number of failure times over repetitions.\nWe integrate our AOGTracker in the latest VOT toolkit7 to run experiments with the baseline protocol and to generate plots 8.\n7. Available at https://github.com/votchallenge/vot-toolkit, version 3.2 8. The plots for VOT2013 and 2014 might be different compared to those in\nthe original VOT reports [80], [81] due to the new version of vot-toolkit.\nThe VOT2013 dataset [80] has 16 sequences which was selected from a large pool such that various visual phenomena like occlusion and illumination changes, were still represented well within the selection. 7 sequences are also used in TB-100. There are 27 trackers evaluated. The readers are referred to the VOT technical report [80] for details.\nFig.13 shows the ranking plot and AR plot in VOT2013. Our AOGTracker obtains the best accuracy while its robustness is slightly worse than three other trackers (i.e., PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge). Our AOGTracker obtains the best overall rank.\nThe VOT2014 dataset [81] has 25 sequences extended from VOT2013. The annotation is based on rotated bounding box instead of up-right rectangle. There are 33 trackers evaluated. Details on the trackers are referred to [81]. Fig.14 shows the ranking plot and AR plot. Our AOGTracker is comparable to other trackers. One main limitation of AOGTracker is that it does not handle rotated bounding boxes well.\nThe VOT2015 dataset [84] consists of 60 short sequences (with rotated bounding box annotations) and VOT-TIR2015 comprises 20 sequences (with bounding box annotations). There are 62 and 28 trackers evaluated in VOT2015 and VOT-TIR2015 respectively. Our AOGTracker obtains 51% and 65% (tied for third place) in accuracy in VOT2015 and VOT-TIR2015 respectively. The details are referred to the reports [84] due to space limit here."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "We have presented a tracking, learning and parsing (TLP) framework and derived a spatial dynamic programming (DP) and a temporal DP algorithm for online object tracking with AOGs. We also have presented a method of online learning object AOGs including its structure and parameters. In experiments, we test our method on two main public benchmark datasets and experimental results show better or comparable performance.\nIn our on-going work, we are studying more flexible computing schemes in tracking with AOGs. The compositional property embedded in an AOG naturally leads to different bottom-up/topdown computing schemes such as the three computing processes studied by Wu and Zhu [85]. We can track an object by matching the object template directly (i.e. \u03b1-process), or computing some discriminative parts first and then combine them into object (\u03b2process), or doing both (\u03b1 + \u03b2-process, as done in this paper). In tracking, as time evolves, the object AOG might grow through online learning, especially for objects with large variations in longterm tracking. Thus, faster inference is entailed for the sake of real time applications. We are trying to learn near optimal decision policies for tracking using the framework proposed by Wu and Zhu [86].\nIn our future work, we will extend the TLP framework by incorporating generic category-level AOGs [8] to scale up the TLP framework. The generic AOGs are pre-trained offline (e.g., using the PASCAL VOC [79] or the imagenet [34]), and will help the online learning of specific AOGs for a target object (e.g., help to maintain the purity of the positive and negative datasets collected online). The generic AOGs will also be updated online together with the specific AOGs. By integrating generic and specific AOGs, we aim at the life-long learning of objects in videos without annotations. Furthermore, we are also interested in integrating scene grammar [87] and event grammar [88] to leverage more top-down information."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the DARPA SIMPLEX Award N6600115-C-4035, the ONR MURI grant N00014-16-1-2007, and NSF IIS-1423305. T. Wu was also supported by the ECE startup fund 201473-02119 at NCSU. We thank Steven Holtzen for proofreading this paper. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of one GPU."}], "references": [{"title": "Object detection with discriminatively trained part-based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI, vol. 32, no. 9, pp. 1627\u20131645, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Object tracking benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "PAMI, vol. 37, no. 9, pp. 1834\u20131848, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1834}, {"title": "Online object tracking: A benchmark", "author": ["\u2014\u2014"], "venue": "CVPR, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel performance evaluation methodology for single-target trackers", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "T. Vojir", "R.P. Pflugfelder", "G. Fern\u00e1ndez", "G. Nebehay", "F. Porikli", "L. Cehovin"], "venue": "CoRR, vol. abs/1503.01313, 2015. [Online]. Available: http://arxiv.org/abs/ 1503.01313", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust visual tracking via convolutional networks", "author": ["K. Zhang", "Q. Liu", "Y. Wu", "M.-H. Yang"], "venue": "arXiv preprint arXiv:1501.04505v2, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv preprint arXiv:1501.04587v2, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "The Origin of Concepts", "author": ["S. Carey"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Discriminatively trained and-or tree models for object detection", "author": ["X. Song", "T. Wu", "Y. Jia", "S.-C. Zhu"], "venue": "CVPR, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Object detection with grammar models", "author": ["R. Girshick", "P. Felzenszwalb", "D. McAllester"], "venue": "NIPS, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Object detection grammars", "author": ["P. Felzenszwalb", "D. McAllester"], "venue": "University of Chicago, Computer Science TR-2010-02, Tech. Rep., 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A stochastic grammar of images", "author": ["S.C. Zhu", "D. Mumford"], "venue": "Foundations and Trends in Computer Graphics and Vision, vol. 2, no. 4, pp. 259\u2013362, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "POP: patchwork of parts models for object recognition", "author": ["Y. Amit", "A. Trouv\u00e9"], "venue": "IJCV, vol. 75, no. 2, pp. 267\u2013282, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Object tracking: A survey", "author": ["A. Yilmaz", "O. Javed", "M. Shah"], "venue": "ACM Comput. Surv., vol. 38, no. 4, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Readings in speech recognition", "author": ["L.R. Rabiner"], "venue": "A. Waibel and K.-F. Lee, Eds., 1990, ch. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, pp. 267\u2013296.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "Condensation - conditional density propagation for visual tracking", "author": ["M. Isard", "A. Blake"], "venue": "IJCV, vol. 29, no. 1, pp. 5\u201328, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "People-tracking-by-detection and people-detection-by-tracking", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "CVPR, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Tracking-learning-detection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "PAMI, vol. 34, no. 7, pp. 1409\u20131422, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-paced learning for long-term tracking", "author": ["J.S. Supancic III", "D. Ramanan"], "venue": "CVPR, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "N-best maximal decoder for part models", "author": ["D. Park", "D. Ramanan"], "venue": "ICCV, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Diverse m-best solutions in markov random fields", "author": ["D. Batra", "P. Yadollahpour", "A. Guzm\u00e1n-Rivera", "G. Shakhnarovich"], "venue": "ECCV, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Global data association for multi-object tracking using network flows", "author": ["L. Zhang", "Y. Li", "R. Nevatia"], "venue": "CVPR, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Globally-optimal greedy algorithms for tracking a variable number of objects", "author": ["H. Pirsiavash", "D. Ramanan", "C.C. Fowlkes"], "venue": "CVPR, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple object tracking using k-shortest paths optimization", "author": ["J. Berclaz", "F. Fleuret", "E. T\u00fcretken", "P. Fua"], "venue": "PAMI, vol. 33, no. 9, pp. 1806\u2013 1819, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1806}, {"title": "An efficient implementation of a scaling minimum-cost flow algorithm", "author": ["A.V. Goldberg"], "venue": "J. Algorithms, vol. 22, no. 1, pp. 1\u201329, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Visual tracking by sampling tree-structured graphical models", "author": ["S. Hong", "B. Han"], "venue": "ECCV, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Orderless tracking through modelaveraged posterior estimation", "author": ["S. Hong", "S. Kwak", "B. Han"], "venue": "ICCV, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Part-based visual tracking with online latent structural learning", "author": ["R. Yao", "Q. Shi", "C. Shen", "Y. Zhang", "A. van den Hengel"], "venue": "CVPR, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Online graph-based tracking", "author": ["H. Nam", "S. Hong", "B. Han"], "venue": "ECCV, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "IJCV, vol. 77, no. 1-3, pp. 125\u2013141, 2008.  ARXIV VERSION  17", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernel-based object tracking", "author": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "venue": "PAMI, vol. 25, no. 5, pp. 564\u2013575, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust visual tracking and vehicle classification via sparse representation", "author": ["X. Mei", "H. Ling"], "venue": "PAMI, vol. 33, no. 11, pp. 2259\u20132272, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of 3d-dct compact representations for robust visual tracking", "author": ["X. Li", "A.R. Dick", "C. Shen", "A. van den Hengel", "H. Wang"], "venue": "PAMI, vol. 35, no. 4, pp. 863\u2013881, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "CVPR, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Highly nonrigid object tracking via patch-based dynamic appearance modeling", "author": ["J. Kwon", "K.M. Lee"], "venue": "PAMI, vol. 35, no. 10, pp. 2427\u20132441, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust visual tracking using an adaptive coupled-layer visual model", "author": ["L. Cehovin", "M. Kristan", "A. Leonardis"], "venue": "PAMI, vol. 35, no. 4, pp. 941\u2013 953, 2013.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "CVPR, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector tracking", "author": ["S. Avidan"], "venue": "PAMI, vol. 26, no. 8, pp. 1064\u2013 1072, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "PAMI, vol. 33, no. 8, pp. 1619\u20131632, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H.S. Torr"], "venue": "ICCV, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting the circulant structure of tracking-by-detection with kernels", "author": ["J. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "ECCV, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Biologically inspired object tracking using center-surround saliency mechanisms", "author": ["V. Mahadevan", "N. Vasconcelos"], "venue": "PAMI, vol. 35, no. 3, pp. 541\u2013554, 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Part-based visual tracking with online latent structural learning", "author": ["R. Yao", "Q. Shi", "C. Shen", "Y. Zhang", "A. van den Hengel"], "venue": "CVPR, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Structure preserving object tracking", "author": ["L. Zhang", "L. van der Maaten"], "venue": "CVPR, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Good feature to track", "author": ["J. Shi", "C. Tomasi"], "venue": "CVPR, 1994.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1994}, {"title": "Online object tracking, learning and parsing with and-or graphs", "author": ["Y. Lu", "T. Wu", "S.-C. Zhu"], "venue": "CVPR, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of appearance models in visual object tracking", "author": ["X. Li", "W. Hu", "C. Shen", "Z. Zhang", "A.R. Dick", "A. van den Hengel"], "venue": "CoRR, vol. abs/1303.4803, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Lucas-kanade 20 years on: A unifying framework", "author": ["S. Baker", "I. Matthews"], "venue": "IJCV, vol. 56, no. 3, pp. 221\u2013255, 2004.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR, 2005.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "Performance evaluation of texture measures with classification based on kullback discrimination of distributions", "author": ["T. Ojala", "M. Pietikainen", "D. Harwood"], "venue": "ICPR, 1994.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1994}, {"title": "Highly nonrigid object tracking via patch-based dynamic appearance modeling", "author": ["J. Kwon", "K.M. Lee"], "venue": "TPAMI, vol. 35, no. 10, pp. 2427\u20132441, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Intrackability: Characterizing video statistics and pursuing video representations", "author": ["H. Gong", "S.C. Zhu"], "venue": "IJCV, vol. 97, no. 3, pp. 255\u2013275, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM J. Sci. Comput., vol. 16, no. 5, pp. 1190\u20131208, 1995.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1995}, {"title": "Exact acceleration of linear object detectors", "author": ["C. Dubout", "F. Fleuret"], "venue": "ECCV, 2012.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "CVPR, 2012.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond semi-supervised tracking: Tracking should be as simple as detection, but not simpler than recognition", "author": ["S. Stalder", "H. Grabner", "L. van Gool"], "venue": "ICCV Workshop, 2009.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Color-based probabilistic tracking", "author": ["P. P\u00e9rez", "C. Hue", "J. Vermaak", "M. Gangnet"], "venue": "ECCV, 2002.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2002}, {"title": "Exploiting the circulant structure of tracking-by-detection with kernels", "author": ["J.F. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "ECCV, 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast compressive tracking", "author": ["K. Zhang", "L. Zhang", "M. Yang"], "venue": "PAMI, vol. 36, no. 10, pp. 2002\u20132015, 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "Context tracker: Exploring supporters and distracters in unconstrained environments", "author": ["T.B. Dinh", "N. Vo", "G.G. Medioni"], "venue": "CVPR, 2011.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "Distribution fields for tracking", "author": ["L. Sevilla-Lara", "E. Learned-Miller"], "venue": "CVPR, 2012.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Robustifying the flock of trackers", "author": ["T. Vojir", "J. Matas"], "venue": "Computer Vision Winter Workshop, 2011.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust fragments-based tracking using the integral histogram", "author": ["A. Adam", "E. Rivlin", "I. Shimshoni"], "venue": "CVPR, 2006.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2006}, {"title": "Real time robust L1 tracker using accelerated proximal gradient approach", "author": ["C. Bao", "Y. Wu", "H. Ling", "H. Ji"], "venue": "CVPR, 2012.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally orderless tracking", "author": ["S. Oron", "A. Bar-Hillel", "D. Levi", "S. Avidan"], "venue": "CVPR, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via locality sensitive histograms", "author": ["S. He", "Q. Yang", "R.W. Lau", "J. Wang", "M.-H. Yang"], "venue": "CVPR, 2013.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust tracking using local sparse appearance model and k-selection", "author": ["B. Liu", "J. Huang", "L. Yang", "C.A. Kulikowski"], "venue": "CVPR, 2011.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Least soft-thresold squares tracking", "author": ["D. Wang", "H. Lu", "M.-H. Yang"], "venue": "CVPR, 2013.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T.Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "CVPR, 2012.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time tracking via on-line boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "BMVC, 2006.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2006}, {"title": "Online robust image alignment via iterative convex optimization", "author": ["Y. Wu", "B. Shen", "H. Ling"], "venue": "CVPR, 2012.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via probability continuous outlier model", "author": ["D. Wang", "H. Lu"], "venue": "CVPR, 2014.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust object tracking via sparsitybased collaborative model", "author": ["W. Zhong", "H. Lu", "M. Yang"], "venue": "CVPR, 2012.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2012}, {"title": "Mean-shift blob tracking through scale space", "author": ["R.T. Collins"], "venue": "CVPR, 2003.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised on-line boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "ECCV, 2008.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2008}, {"title": "Online selection of discriminative tracking features", "author": ["R.T. Collins", "Y. Liu", "M. Leordeanu"], "venue": "PAMI, vol. 27, no. 10, pp. 1631\u20131643, 2005.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2005}, {"title": "Visual tracking decomposition", "author": ["J. Kwon", "K.M. Lee"], "venue": "CVPR, 2010.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2010}, {"title": "Tracking by sampling trackers", "author": ["\u2014\u2014"], "venue": "ICCV, 2011.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2011}, {"title": "The visual object tracking vot2013 challenge results", "author": ["M. Kristan"], "venue": "2013. [Online]. Available: http://www.votchallenge.net/vot2013/ program.html", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2013}, {"title": "The visual object tracking vot2014 challenge results", "author": ["\u2014\u2014"], "venue": "2014. [Online]. Available: http://www.votchallenge.net/vot2014/program.html", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust visual tracking using an adaptive coupled-layer visual model", "author": ["L. Cehovin", "M. Kristan", "A. Leonardis"], "venue": "PAMI, vol. 35, no. 4, pp. 941\u2013 953, 2013.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2013}, {"title": "An enhanced adaptive coupledlayer lgtracker++", "author": ["J. Xiao", "R. Stolkin", "A. Leonardis"], "venue": "Vis. Obj. Track. Challenge VOT2013, In conjunction with ICCV2013, 2013.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "The visual object tracking vot2015 and tir2015 challenge results", "author": ["M. Kristan"], "venue": "2015. [Online]. Available: http: //www.votchallenge.net/vot2015/program.html", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2015}, {"title": "A numerical study of the bottom-up and top-down inference processes in and-or graphs", "author": ["T. Wu", "S.C. Zhu"], "venue": "IJCV, vol. 93, no. 2, pp. 226\u2013252, 2011.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning near-optimal cost-sensitive decision policy for object detection", "author": ["T. Wu", "S. Zhu"], "venue": "TPAMI, vol. 37, no. 5, pp. 1013\u20131027, 2015.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2015}, {"title": "Image parsing with stochastic scene grammar", "author": ["Y. Zhao", "S.C. Zhu"], "venue": "NIPS, 2011.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking).", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking).", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking).", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5], [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5], [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "O NLINE object tracking is an innate capability in human and animal vision for learning visual concepts [7], and is an important task in computer vision.", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "1: Illustration of some typical issues in online object tracking using the \u201cskating1\u201d video in the benchmark [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "A popular modeling scheme represents object categories by mixtures of deformable part-based models (DPMs) [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "We quantize the space of part configurations recursively in a principled way with a hierarchical and compositional And-Or graph (AOG) representation [8], [11].", "startOffset": 149, "endOffset": 152}, {"referenceID": 10, "context": "We quantize the space of part configurations recursively in a principled way with a hierarchical and compositional And-Or graph (AOG) representation [8], [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 1, "context": "Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14].", "startOffset": 125, "endOffset": 128}, {"referenceID": 12, "context": "Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": ", the 29 trackers evaluated in the TB-100 benchmark [2]), no feedback inspection is applied to the history of inferred trajectory.", "startOffset": 52, "endOffset": 55}, {"referenceID": 16, "context": "Most prior approaches do not address this issue since they focus on marginally optimal solutions with which object models are updated, except for the P-N learning in TLD [17] and the self-paced learning for tracking [18].", "startOffset": 170, "endOffset": 174}, {"referenceID": 17, "context": "Most prior approaches do not address this issue since they focus on marginally optimal solutions with which object models are updated, except for the P-N learning in TLD [17] and the self-paced learning for tracking [18].", "startOffset": 216, "endOffset": 220}, {"referenceID": 1, "context": "Our TLP method obtains state-of-the-art performance on one popular tracking benchmark [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "(ii) We retrain the initial object AOG using latent SVM (LSVM) as it was done in learning the DPMs [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 13, "context": ", the Viterbi path [14]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "This is similar in spirit to methods of keeping N-best maximal decoder for part models [19] and maintaining diverse M-best solutions in MRF [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "This is similar in spirit to methods of keeping N-best maximal decoder for part models [19] and maintaining diverse M-best solutions in MRF [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 20, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": ", the DPMs [1]) and then form \u201ctracklets\u201d in consecutive frames.", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "Recently, Hong and Han [25]", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "proposed an offline single object tracking method by sampling tree-structured graphical models which exploit the underlying intrinsic structure of input video in an orderless tracking [26].", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "In the literature, particle filtering [15] has been widely adopted, which approximately represents the posterior probability in a nonparametric form by maintaining a set of particles (i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "More recently, tracking-bydetection methods [16], [17] have become popular which learn and update object models online and encode the posterior probability using dense sampling through sliding-window based detection onthe-fly.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "More recently, tracking-bydetection methods [16], [17] have become popular which learn and update object models online and encode the posterior probability using dense sampling through sliding-window based detection onthe-fly.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 26, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 165, "endOffset": 169}, {"referenceID": 8, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 204, "endOffset": 207}, {"referenceID": 9, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 215, "endOffset": 219}, {"referenceID": 27, "context": "Most popular methods also assume first-order HMMs except for the recently proposed online graph-based tracker [28].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 137, "endOffset": 141}, {"referenceID": 31, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 266, "endOffset": 269}, {"referenceID": 5, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 271, "endOffset": 274}, {"referenceID": 32, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 276, "endOffset": 280}, {"referenceID": 33, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 368, "endOffset": 372}, {"referenceID": 34, "context": "ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37].", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37].", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37].", "startOffset": 131, "endOffset": 135}, {"referenceID": 37, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 124, "endOffset": 128}, {"referenceID": 39, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 137, "endOffset": 141}, {"referenceID": 40, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 183, "endOffset": 187}, {"referenceID": 41, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 230, "endOffset": 234}, {"referenceID": 42, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 320, "endOffset": 324}, {"referenceID": 26, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 367, "endOffset": 371}, {"referenceID": 43, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 373, "endOffset": 377}, {"referenceID": 16, "context": "i) More representational power: Unlike TLD [17] and many other methods (e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": ", [18]) which model an object as a single template or a mixture of templates and thus do not perform well in tracking objects with large structural and appearance variations, an AOG represents an object in a hierarchical and compositional graph expressing a large number of latent part configurations.", "startOffset": 2, "endOffset": 6}, {"referenceID": 44, "context": "This idea is similar in spirit to finding good features to track objects [45], and we find good part configurations online for both tracking and learning.", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "Our preliminary work has been published in [46] and the method for constructing full structure AOG was published in [8].", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "Our preliminary work has been published in [46] and the method for constructing full structure AOG was published in [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "iii) It outperforms the state-of-the-art tracking methods in a recent public benchmark, TB-100 [2], and obtains comparable performance on a series of VOT benchmarks [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "iii) It outperforms the state-of-the-art tracking methods in a recent public benchmark, TB-100 [2], and obtains comparable performance on a series of VOT benchmarks [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 14, "context": "which is usually solved using particle filtering [15] in practice.", "startOffset": 49, "endOffset": 53}, {"referenceID": 46, "context": "tracking by generative appearance modeling of an object [47].", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "We use a thresholded motion model in experiments: the cost is 0 if the transition is accepted based on the median flow [17] (which is a forward-backward extension of the Lucas-Kanade optimal flow [48]) and +\u221e otherwise.", "startOffset": 119, "endOffset": 123}, {"referenceID": 47, "context": "We use a thresholded motion model in experiments: the cost is 0 if the transition is accepted based on the median flow [17] (which is a forward-backward extension of the Lucas-Kanade optimal flow [48]) and +\u221e otherwise.", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "A similar method was explored in [18].", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "We note that the object template is not allowed to perturb locally in inference since we infer the optimal part configuration for each given object location in the pyramid with sliding window technique used, as done in the DPM [1], so the parent And-node of the object terminal-node does not have deformation parameters.", "startOffset": 227, "endOffset": 230}, {"referenceID": 48, "context": "We use three types of features: histogram of oriented gradient (HOG) [49], local binary pattern features (LBP) [50], and RGB color histograms (for color videos).", "startOffset": 69, "endOffset": 73}, {"referenceID": 49, "context": "We use three types of features: histogram of oriented gradient (HOG) [49], local binary pattern features (LBP) [50], and RGB color histograms (for color videos).", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "The deformation feature is defined by \u03a6 (\u03b4) = [dx, dx, dy, dy]\u2032 as done in DPMs [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "where the first case is for sharing score maps between the object terminal-node and its parent And-node since we do not allow local deformation for the whole object, the second case for computing transformed score maps of parent Andnode of a part terminal-node which is allowed to find the best placement through distance transformation [1], \u2295 represents the displacement operator in the position space in \u039b, and And-node", "startOffset": 337, "endOffset": 340}, {"referenceID": 18, "context": "We keep top Nbest parse trees to infer the best B\u2217 t together with a temporal DP algorithm, similar to the strategies used in [19], [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "We keep top Nbest parse trees to infer the best B\u2217 t together with a temporal DP algorithm, similar to the strategies used in [19], [20].", "startOffset": 132, "endOffset": 136}, {"referenceID": 50, "context": "Intuitively, we expect the score map of a discriminative node v has peak and steep landscape, as investigated in [51].", "startOffset": 113, "endOffset": 117}, {"referenceID": 51, "context": "More sophisticated definitions of intrackability in tracking are referred to [52].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "We use latent SVM method (LSVM) [1].", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 61, "endOffset": 64}, {"referenceID": 52, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 193, "endOffset": 196}, {"referenceID": 53, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "We train the parameters of initial object AOG using LSVM [1] with two rounds of positive re-labeling and hard negative mining respectively.", "startOffset": 57, "endOffset": 60}, {"referenceID": 54, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 29, "endOffset": 33}, {"referenceID": 56, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 65, "endOffset": 69}, {"referenceID": 58, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 81, "endOffset": 85}, {"referenceID": 59, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 98, "endOffset": 102}, {"referenceID": 60, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 115, "endOffset": 119}, {"referenceID": 61, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 134, "endOffset": 138}, {"referenceID": 62, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 190, "endOffset": 194}, {"referenceID": 63, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 211, "endOffset": 215}, {"referenceID": 64, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 232, "endOffset": 236}, {"referenceID": 65, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 252, "endOffset": 256}, {"referenceID": 66, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 275, "endOffset": 279}, {"referenceID": 67, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 296, "endOffset": 300}, {"referenceID": 38, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 319, "endOffset": 323}, {"referenceID": 68, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 336, "endOffset": 340}, {"referenceID": 69, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 357, "endOffset": 361}, {"referenceID": 70, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 375, "endOffset": 379}, {"referenceID": 71, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 397, "endOffset": 401}, {"referenceID": 72, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 418, "endOffset": 422}, {"referenceID": 73, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 445, "endOffset": 449}, {"referenceID": 74, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 462, "endOffset": 466}, {"referenceID": 39, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 482, "endOffset": 486}, {"referenceID": 16, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 499, "endOffset": 503}, {"referenceID": 75, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 517, "endOffset": 521}, {"referenceID": 76, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 534, "endOffset": 538}, {"referenceID": 77, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 555, "endOffset": 559}, {"referenceID": 1, "context": "TABLE 2: Tracking algorithms evaluated in the TB-100 benchmark (reproduced from [2]).", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 53, "context": "In our current c++ implementation, we adopt FFT in computing score pyramids as done in [54] which also utilizes multi-threads with OpenMP.", "startOffset": 87, "endOffset": 91}, {"referenceID": 39, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 32, "endOffset": 35}, {"referenceID": 39, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 45, "endOffset": 49}, {"referenceID": 39, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 29, "endOffset": 33}, {"referenceID": 72, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 38, "endOffset": 42}, {"referenceID": 38, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "TABLE 3: Performance gain (in %) of our AOGTracker in term of success rate and precision rate in the benchmark [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "Two deep learning based trackers, CNT [5] and SO-DLT [6], are evaluated in TB-CVPR2013 using OPE (with their performance plots manually added in the left-bottom figure).", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "Two deep learning based trackers, CNT [5] and SO-DLT [6], are evaluated in TB-CVPR2013 using OPE (with their performance plots manually added in the left-bottom figure).", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Usually, success plots are preferred to rank trackers [2], [4] (thus we focus on success plots in comparison).", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Usually, success plots are preferred to rank trackers [2], [4] (thus we focus on success plots in comparison).", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "More details on the attributes and their distributions in the benchmark are referred to [2], [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "More details on the attributes and their distributions in the benchmark are referred to [2], [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "See more details about categorizing these trackers in [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "In TB-CVPR2013, two recent trackers trained by deep convolutional network (CNT [5], SO-DLT [6]) were evaluated using OPE.", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "In TB-CVPR2013, two recent trackers trained by deep convolutional network (CNT [5], SO-DLT [6]) were evaluated using OPE.", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "We note that for OPE in TB-CVPR2013, although the improvement of our AOGTracker over the SO-DLT [6] is not very big, the SO-DLT utilized two deep convolutional networks with different model update strategies in tracking, both of which are pretrained on the ImageNet [34].", "startOffset": 96, "endOffset": 99}, {"referenceID": 33, "context": "We note that for OPE in TB-CVPR2013, although the improvement of our AOGTracker over the SO-DLT [6] is not very big, the SO-DLT utilized two deep convolutional networks with different model update strategies in tracking, both of which are pretrained on the ImageNet [34].", "startOffset": 266, "endOffset": 270}, {"referenceID": 16, "context": ", TLD [17] uses intensity based Haar like features).", "startOffset": 6, "endOffset": 10}, {"referenceID": 57, "context": "More importantly, we address the issue of learning the optimal deformable part-based configurations in the quantized space of latent object structures, while most of other trackers focus on either whole objects [58] or implicit configurations (e.", "startOffset": 211, "endOffset": 215}, {"referenceID": 57, "context": ", CSK [58] and STRUCK [40]), our AOGTracker runs tracking-by-parsing in feature pyramid to detect scale changes (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 39, "context": ", CSK [58] and STRUCK [40]), our AOGTracker runs tracking-by-parsing in feature pyramid to detect scale changes (e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 78, "context": "The plots for VOT2013 and 2014 might be different compared to those in the original VOT reports [80], [81] due to the new version of vot-toolkit.", "startOffset": 96, "endOffset": 100}, {"referenceID": 79, "context": "The plots for VOT2013 and 2014 might be different compared to those in the original VOT reports [80], [81] due to the new version of vot-toolkit.", "startOffset": 102, "endOffset": 106}, {"referenceID": 78, "context": "The VOT2013 dataset [80] has 16 sequences which was selected from a large pool such that various visual phenomena like occlusion and illumination changes, were still represented well within the selection.", "startOffset": 20, "endOffset": 24}, {"referenceID": 78, "context": "The readers are referred to the VOT technical report [80] for details.", "startOffset": 53, "endOffset": 57}, {"referenceID": 78, "context": ", PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge).", "startOffset": 6, "endOffset": 10}, {"referenceID": 80, "context": ", PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge).", "startOffset": 16, "endOffset": 20}, {"referenceID": 81, "context": ", PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge).", "startOffset": 31, "endOffset": 35}, {"referenceID": 79, "context": "The VOT2014 dataset [81] has 25 sequences extended from VOT2013.", "startOffset": 20, "endOffset": 24}, {"referenceID": 79, "context": "Details on the trackers are referred to [81].", "startOffset": 40, "endOffset": 44}, {"referenceID": 82, "context": "The VOT2015 dataset [84] consists of 60 short sequences (with rotated bounding box annotations) and VOT-TIR2015 comprises 20 sequences (with bounding box annotations).", "startOffset": 20, "endOffset": 24}, {"referenceID": 82, "context": "The details are referred to the reports [84] due to space limit here.", "startOffset": 40, "endOffset": 44}, {"referenceID": 83, "context": "The compositional property embedded in an AOG naturally leads to different bottom-up/topdown computing schemes such as the three computing processes studied by Wu and Zhu [85].", "startOffset": 171, "endOffset": 175}, {"referenceID": 84, "context": "We are trying to learn near optimal decision policies for tracking using the framework proposed by Wu and Zhu [86].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "In our future work, we will extend the TLP framework by incorporating generic category-level AOGs [8] to scale up the TLP framework.", "startOffset": 98, "endOffset": 101}, {"referenceID": 33, "context": ", using the PASCAL VOC [79] or the imagenet [34]), and will help the online learning of specific AOGs for a target object (e.", "startOffset": 44, "endOffset": 48}, {"referenceID": 85, "context": "Furthermore, we are also interested in integrating scene grammar [87] and event grammar [88] to leverage more top-down information.", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5], [6]. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.", "creator": "LaTeX with hyperref package"}}}