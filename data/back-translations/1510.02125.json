{"id": "1510.02125", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model", "abstract": "The recently introduced \"Words as Classifiers\" model of grounded semantics (Kennington & amp; Snakes 2015) considers words as classifiers in (perceptual) contexts and composes the meaning of a phrase through \"soft\" intersections of the names of its components. The model is trained using examples of referential language use and was initially evaluated in a game scenario with a small number of different object types with references. We apply this model to a large number of real-world photos (SAIAPR TC-12, (Escalante et al. 2010)), which contain objects with a much greater variety of types, and show that it performs well in a reference resolution task on this dataset. We also extend the model to quantification and negation, and evaluate these extensions with good results. To investigate what classifiers learn, we introduce \"more intensive\" and \"denotational\" word vectors, and show that they differentiate from words in a way.", "histories": [["v1", "Wed, 7 Oct 2015 20:52:22 GMT  (1146kb,D)", "http://arxiv.org/abs/1510.02125v1", "13 pages"], ["v2", "Mon, 21 Mar 2016 00:33:40 GMT  (1747kb,D)", "http://arxiv.org/abs/1510.02125v2", "10 pages; substantial rewrite; added dataset; different feature extraction method; revised results"], ["v3", "Fri, 3 Jun 2016 11:53:31 GMT  (1749kb,D)", "http://arxiv.org/abs/1510.02125v3", "11 pages; as in Proceedings of ACL 2016, Berlin, 2016"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david schlangen", "sina zarrie\u00df", "casey kennington"], "accepted": true, "id": "1510.02125"}, "pdf": {"name": "1510.02125.pdf", "metadata": {"source": "CRF", "title": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model", "authors": ["David Schlangen", "Sina Zarrie\u00df", "Casey Kennington"], "emails": ["first.last@uni-bielefeld.de"], "sections": [{"heading": "1 Introduction", "text": "Wittgenstein\u2019s (1953) famous dictum \u201cmeaning is use\u201d (PU 43) is these days often taken as providing some form of implicit endorsement for corpusbased methods in semantics and is often referenced in the same paragraph as Firth\u2019s (1957) equally famous dictum \u201cyou shall know a word by the company it keeps\u201d (e.g., in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)). Making the\nimplied connection between these two claims, however, requires a bit of a conceptual leap, as words are typically not used to keep each other company, but rather because the person using them wants to achieve something. To this, the linguistic context of a word only indirectly gives witness.1\nA common use of language is to refer to something. This use is comparatively easy to record together with the language, for example by linking mentions of entities in the text to unique objects in a database (e.g., (Gabrilovich et al., 2013)), or by including in the corpus some other representation of the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014). Corpora of this type then form a good basis for inducing models that learn to know a word by the uses it is put to, and this is what we do in this paper.\nWe use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d). Phrase meanings can then be composed as averages of word responses, and, in the case of references to individual objects, the referred object can be identified as the best fitting candidate from the context of utterance. The approach\n1To be fair, a certain uncomfortableness with making this connection also often transpires from these same papers; see e.g. T&P who note that corpora that record multimodal data would be a better fit..\nar X\niv :1\n51 0.\n02 12\n5v 1\n[ cs\n.C L\n] 7\nO ct\n2 01\nis \u201cdialogue-ready\u201d, in the sense that it supports incremental processing (giving results while the incoming utterance is going on), incremental learning (being able to improve performance from interactive feedback) and modularity (as a \u201cdrop-in\u201d replacement for a semantic component compatible with existing syntactic processing)."}, {"heading": "2 Related Work", "text": "There are three somewhat independent strands of relevant related work.\nThe first is often labelled \u201cgrounded semantics\u201d and concerns itself with learning to execute natural language commands, typically in the context of robotics. Perhaps due to this background, the work here often follows a two-step approach, where first a world representation is created (or given), to which then the grounding is made. For example, Tellex et al. (2011) learn to map commands given to a robot to plans to be executed by it. The commands are grounded in the sense that their components are mapped to identifiers of objects and trajectories, but the world is represented symbolically through object labels and properties and not, as in the experiments described here, inferred from computer vision output.\nOf this area, (Krishnamurthy and Kollar, 2013) is perhaps closest to our concerns. This approach also combines formal semantics framework with classifiers that work on images. The connection between language and world, however, is somewhat less direct than in our model: in this approach, the given image (with regions) is labelled first, making hard decisions, to populate a knowledge base which is then matched to a referring expression. The approach is evaluated on scenes of a toy world (with limited numbers of objects and relations between them). As we will see, our approach maintains uncertainty by not separating the labelling and the matching.\nThe second area to mention here is the recently very active area of image-to-text generation, which has been spurred on by the availability of large datasets and competitions structured around them.2\n2E.g., most recently, the MS COCO Captioning Challenge, http://mscoco.org/dataset/ #captions-challenge2015.\nThe task here typically is to generate a description (a caption) for a given image. A frequently taken approach here is to use a convolutional neural network to map the image into a dense vector, and then condition a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015). (Fang et al., 2015) modify this approach somewhat, by using word detectors first to specifically propose words for image regions, out of which the caption is then generated. This has some similarity to our word models as described below, but again is tailored more towards generation. In general, while the approaches achieve impressive success on generation, it is harder to see how they could be made useful for the kinds of interactive use that we envision, with the properties mentioned in the introduction (incrementality, etc.).\nFinally, we take from distributional semantics (Turney and Pantel, 2010) the idea that rich word meanings can be learned from records of word uses. We also concern ourselves with compositionality, which has been a recent focus of attention in this field (Mitchell and Lapata, 2010; Baroni et al., 2013). Other recent work in this area (e.g., (Bruni et al., 2014)) has begun to extend the meanings learned from contexts with other words with image contexts. A crucial difference of our approach, however, is that we model words individually and not through their distributional history over a known vocabulary. Among other things, this makes incremental learning possible in our approach. An interesting, somewhat different approach is taken by Young et al. (2014), who specify a denotational semantics for certain words as sets of images, the caption of which the word appears in. This grounding, however, is only indirect, as the image themselves are not processed and serve just as triggers of descriptions and then, via unique identifiers of the files, as symbolic labels."}, {"heading": "3 The \u201cWords-As-Classifiers\u201d Model", "text": "The model of referential meaning that we apply here has the following sub-components:3\n3The model was introduced in ((Kennington and Schlangen, 2015)); we reformulate and extend it here. The general idea of modelling words as classifiers on perceptual input goes back at least to (Harnard, 1990) and was most recently developed from a more formal perspective by (Larsson, 2015).\n(a) A model of word meanings, where these are (for certain types of words) classifiers on (visual representations of) candidate objects, providing \u201cappropriateness judgements\u201d (scores) for each candidate object, and, through application to all candidate objects, leading to denotations which are distributions over these candidate objects in a given context; (b) A model of composition, where different phrase types can contribute different modes of composition; (c) A model of quantification, which consists in different ways of reducing the extensions of the quantified phrases to sets of the desired size.\nThe model takes from distributional semantics the idea that word meanings are best learned from records of word uses, but differs in that it draws on the referential word uses and not the distributional history in a text corpus. It takes from formal semantics the idea that denotations of (certain types of) words and expressions can be modelled as sets and composition as function application, but it uses the rich, grounded word meanings to impose an appropriateness ordering on the sets and lets the applied functions take this ordering into account.\nWe illustrate the model with an example here; a more formal presentation is given in the Appendix.\nThe matrix on the left in Figure 1 shows the responses for the words \u201cbrown shirt guy on right\u201d in a context with 9 candidate objects (o1, . . . , o9; see Figure 2 below). Each column in this matrix gives the responses for a single word classifier (component (a) from above) applied individually to all objects (hence, 9 rows), where each object is represented by a vector of computer vision features. That is, the first column lists the responses to \u201cbrown\u201d over all candidate objects, the second to \u201cshirt\u201d, and\nso on. The vector on the right represents the denotation for the whole phrase, which is the row-wise average of the matrix (component (b)). The determiner \u201cthe\u201d contributes the method of selection from that vector: the referred object is the one corresponding to the argmax of the vector, element 4 (o4). \u201c(The) two\u201d would have selected the top two elements from that vector, \u201c(the) three\u201d the top three, etc..\nWe also experiment with negation. In this model, the result for the (constructed) example \u201c(the) [not brown] shirt guy on the right\u201d in the same context (the same image) would look like this matrix, except with the first column \u2018inverted\u2019 (normalised; 1 -, for each element; renormalised). This way, o3 would be considered least appropriate for \u201cnot brown\u201d, and o5 most appropriate. (In this case, this does lead to selection of the person the furthest right on the image, who is indeed not wearing a brown shirt; see Figure 2.)\nThis model (with \u201cthe\u201d as the only quantifier, and without negation) has been evaluated in (Kennington and Schlangen, 2015) on referring expressions in a game-playing scenario with a small number of different types of objects, which were simple geometric shapes. In the following, we apply it to a scenario with a much larger set of real-world object types, occuring in the context of real-world photographs."}, {"heading": "4 Data: Images & Referring Expressions", "text": "We profited from the availability of a rich corpus that was successively built through various efforts. The basis is the IAPR TC-12 image retrieval benchmark collection of \u201c20,000 still natural images taken from locations around the world and comprising an assorted cross-section of still natural images\u201d (Grubinger et al., 2006). A typical example of an image from the collection is shown in Figure 2 on the left.\nThis dataset was later augmented by Escalante et al. (2010) with (among other things) segmentation masks identifying objects in the images (an average of 5 objects per image). Figure 2 (middle) gives an example of such a segmentation. These segmentations were done manually and provide close maskings of the objects (and not just rectangular bounding boxes). This dataset, also known as \u201cSAIAPR TC-12\u201d (for \u201csegmented and annotated IAPR TC12\u201d), also provides a vector of visual features for\neach region, as described in Table 1. The final component is provided by Kazemzadeh et al. (2014), who collected a large number of expressions referring to objects (for which segmentations exist) from these images. (An example is given in Figure 2 (right).) This was done using a crowdsourcing approach where two players were paired and a director needed to refer to a predetermined object to a matcher, who then selected it; this way, quality is automatically controlled. This corpus contributes 120k referring expressions, covering nearly all of the 99.5k regions from SAIAPR TC-12.\nThe data we used is summarised in Table 1. All data sets are publicly available.4\nWith this, we have all that we need to train the word models in WAC: we have objects in visual discourse universes (the regions from SAIAPR); repre-\n4 IAPR TC-12 and SAIAPR TC-12 from http: //imageclef.org; REFERITGAME from http: //tamaraberg.com/referitgame.\nsentations of these in terms of low-level vision features (also from SAIAPR); and instances of word uses in references to such objects (from REFERITGAME), from which we can train the appropriateness classifiers [[w]]obj for each occuring word w."}, {"heading": "5 Training the Word Classifiers", "text": "As explained above, the basis of our approach are the word classifiers. These need to be trained from data. While the previous section gave general information about the corpus we use, we look into the distribution of words in it in the following section, before we describe the training regime in more detail.\nThe training corpus Each training split contains about 108k referring expressions (this can vary slightly in the cross-validation, as the split is performed on the set of images), and about 370k word tokens (for an average of 3.4 per expression). These tokens realise 9,500 word types (of which 5,500 are hapax legomena, many of which are typos of more frequent words). The frequency distribution of word types shows a typical Zipfian pattern. After \u201cthe\u201d, which we exclude together with \u201ca\u201d and \u201can\u201d (as determiners are modelled differently; see above Section 3), the most frequent word is \u201cleft\u201d, which occurs 20,600 times. The 400th most frequent word (\u201cpersons\u201d) already only occurs 65 times.\nSelecting candidate words How do we then select the words for which we train perceptual classifiers? There is a technical consideration to be made here and a semantic one. The technical consideration is that we need sufficient training data for the classifiers, and so can only practically train classifiers for words that occur often enough in the training corpus. We set a threshold here of a minimum\nof 50 instances (postponing the investigation of the effect of training set size to Section 8 below).\nThe semantic consideration is that intuitively, the approach does not seem appropriate for all types of words. Nevertheless, for now, we make the assumption\u2014clearly not generally true\u2014that all words in a referring expression contribute information to the visual identification of its referent. We discuss the consequences of this decision below.\nThis assumption is violated in a different way in phrases that refer via a landmark, such as in \u201cthe thing next to the woman with the blue shirt\u201d. (Such constructions were modelled in (Kennington and Schlangen, 2015), in the puzzle game domain.) Here we cannot assume that the region that was referred to provides a good instance of \u201cblue\u201d for example (since it\u2019s not the target object that is described as blue but rather another object near to it), and so we exclude such phrases from the training set (by looking for a small set of expressions such as \u201cleft of\u201d, \u201cbehind\u201d, etc.). This reduces the size of the training set to 93k referring expressions for a total of 270k tokens.\nNow that we have decided on the set of words for which to train classifiers, how do we assemble the training data? To train the classifiers, we need positive (appropriate) and negative (inappropriate) instances of uses.\nPositive Instances Getting positive instances from the corpus is straightforward: We pair each word in a referring expression with the representation of the region it refers to. That is, if the word \u201cleft\u201d occurs 20,000 times in expressions in the training corpus, we have 20,000 positive instances for training its classifier.\nNegative Instances Acquiring negative instances, which are also needed for training the classifiers, is less straightforward. The corpus does not record inappropriate uses of a word, or \u2018negative referring expressions\u2019 (as in \u201cthis is not a red chair\u201d). To create negative instances, we make a second assumption which again is not generally correct, namely that when a word was used to refer in a certain context, it only applies to the object that was referred to and not to any of the others in the same scene. Hence we pair the word that is to be trained with a certain number of representations of regions from each\nscene where it was used, but other than the regions that the expression refers to.\nThis clearly can go wrong. E.g., in a case where the referring expression was \u201cred cross\u201d, there may very well be other objects in the scene that we could end up picking as negative instances for \u201cred\u201d, but that could legitimately be described as red as well. However, it is unlikely that all or even most of the other objects in the image will in fact be red, since then mentioning this property in the referring expression would have been redundant. Hence, as long as we sample enough of the other regions as negative examples\u2014for the results reported below, we sampled three negatives for each positive\u2014the likelihood that we add genuine negative examples should be high.5\nThe classifiers Following this regime, we train binary logistic regression classifiers (with `1 regularisation) on the visual object features representations (from SAIAPR), for the 400 most frequent words from the training set. We have experimented with support vector machines as well, which reach similar results with much higher training and application times; we will report results only for logistic regression here.6\nA final note: the outcome of the set contruction described above is that all classifiers are presented with data sets with the same balance of positive and negative examples, a fixed ratio of 1 positive to 3 negative. The classifiers themselves hence do not reflect any word frequency effects; any potential effects of this type are in any case better modelled separately."}, {"heading": "6 Exp. 1: References to Single Objects", "text": "The task is the following: Given an image (from SAIAPR, ie. with objects segmented and represented\n5We note in passing here that a nice consequence of this way of producing training data is that in principle, this can work incrementally and online, with each instance of a successful reference (e.g., a teacher showing a learner an object; that is, pointing the referent out also by means other than verbal) provides both positive and (indirectly) negative evidence which can be used to update the classifiers for the words used, and no memory of all previous episodes is required. We leave exploring possible connections to the literature on language acquisition to future work, however.\n6For both logistic regression and SVM we used the scikit learn package (Pedregosa et al., 2011).\nthrough feature vectures) and an expression (from REFERITGAME) that refers to one of these objects, predict which object the expression refers to. With an average of (slightly over) 5 objects per image (and the assumption that each one is equally likely to be referred by the utterance), the random baseline for this task is at (slightly under) 20%.\nIn all experiments described below, we split the data into a training set and a test set (90%/10%). The results reported below are from a 10-fold crossvalidation. Note that the split is made on the set of images and only via that on the set of referring expressions, to ensure that no image regions are shared between training and test sets."}, {"heading": "6.1 Applying the Model", "text": "To test the model on a given referring expression from the test set, we proceed as described above (Section 3): We test each (known) word from the referring expression on all objects from the image it belongs to, resulting in a vector of appropriateness judgements (technically, the probability returned by the classifier for this object being of the class of the word). We add up these vectors and normalise for length, which results in a vector that gives the scores for all objects. The intended referent is then identified as the argmax of this distribution. (See the Appendix for a formal description of this process.) Unknown words (for which there are no classifiers) are simply ignored and do not contribute to the distribution; if no words are known, a referent is randomly drawn from the set of candidates."}, {"heading": "6.2 Results", "text": "Figure 2 shows the results of this experiment (over 10 folds of the cross-validation; the deviations are so small between folds that we only show the average here), in terms of accuracy and mean reciprocal rank (MRR), and for increasingly more restrictive selections of the testcorpus. The first row shows the\nresults over the whole testset, which with 59% is already well above the random baseline (20%). The MRR of 0.71 indicates that even when the correct referent is not identified, it typically is in the top third, which is promising for the use of this approach in interactive systems which could ask clarification questions proposing alternatives. The second row shows results when expressions are removed for which the model just guessed, as no words were known, this increases accuracy one percentage point. More interesting are the further restrictions. If expressions containing relational expressions are removed (as they were from the training corpus), as the model as described here cannot correctly resolve them,7 accuracy and MRR increases further. If additionally only utterances are considered for which the majority of occurring words is known, accuracy rises to the best value of 67% (0.76 MRR) (on a much reduced testset of 69% of the original size).\nFigure 3 finally plots average accuracy as a function of utterance length. As this graph shows, composition clearly adds noise (one word expressions show the best accuracy), but accuracy does hold at a good level up until the rather large length of nine words.\nIn sum, we take these results as good evidence that the model does indeed succesfully capture elements of meaning that are useful for this task of reference resolution, and that the composition of the word meanings into expression meanings does work. We further corroborate these main results with an additional experiment on these data."}, {"heading": "7 Experiment 2: Negation and Quantifiers", "text": "With the second experiment we evaluate the extensions to the model covering negation and quantifi-\n7Some relations were modelled in (Kennington and Schlangen, 2015) in the puzzle domain.\ncation. For this we do not have naturally occuring expressions: the instructions to the participants in the REFERITGAME were to refer to single objects, and negations occur only a very few times.\nNegation We create test data automatically from the existing expressions, by substituting expressions or phrases with negated other phrases (ideally, antonyms). E.g., we can create out of the expression \u201cbrown shirt guy on the right\u201d the (synthetic) expression \u201c[not blue] shirt guy on the right\u201d, with the scope of the negation indicated by brackets.8 Table 3 shows results for this process for a number of pairs of expressions. We first show the accuracy of the normal model on all utterances from the test set containing the word in question (acc); then the negated word with which we replace the occurances in the expressions (not-X) and the accuracy on the resulting variant of the testset (n-acc). To get a baseline for the effect of this manipulation, we also show the accuracy with the original word removed (d-acc).\nFor colours, replacement with a negated other colour hurts accuracy almost as much as deleting the colour information alltogether (e.g., in the first line, replacing occurrences of red with not yellow makes accuracy go from 47% to 38%; deleting red yields 36% accuracy). This is perhaps as should be expected: the phrase not yellow contains quite a bit less information than the phrase red. The situation is different with clear opposites like left and right. Here the substitution seems to preserve information, whereas deleting the original expression\n8Just adding negation, e.g. going from \u201cbrown shirt guy on the right\u201d from Figure 2 to \u201c[not brown] shirt guy on the right\u201d creates a different situation: we would expect the changed expression to refer to a different object, but which that is we cannot get from the available annotation. (In this example, as manual inspection shows, the modified expression selects, appropriately, the rightmost person in Figure 2.)\nis quite hurtful. The final row shows an interesting case. Here an expression (man) is replaced with a non-related, but entailed negated phrase (not sky). Interestingly, the negation seems to provide more information than the original phrase, which even seems to be hurting performance (deleting it increases accuracy). This indicates that this particular word learns a low quality classifier; something we will investigate in the next section.\nQuantifiers To evaluate quantifiers other than \u201cthe\u201d, we make use of another resource in our corpora. SAIAPR provides for each region a semantic label (from a set of 275). Exactly 100 of those labels happen to be among the words for which we have trained word classifiers. We can then look for images where there is more than one region labelled with the same label, and can test the plural constructions on those. Table 4 shows results for \u201ctwo\u201d (as in \u201ctwo people\u201d), \u201cthree\u201d and also, for comparison, for \u201cone\u201d (which is equivalent to \u201cthe\u201d). For all these quantifiers, the classifiers beat the random baselines (22%, 8%, 4%, respectively). Again, performance for label words in the 1-best case (1, equivalent to \u201cthe\u201d) is better than the general performance for referring expressions (see above Table 2 and Figure 3). This indicates that the categories that the labels denote are learned well, and that the composition in multi-word expressions does add some noise.\nWe can use the same approach to evaluate \u201call\u201d. Here, we need a way to decide how from the distribution over candidate objects a decision is made which are to be included in the denotiation of the quantifier phrase. We tested the simplest approach here, which is to select all whose probability is above the mean probability. This gives the right set in 37% of all cases where this is a singleton set; that is, this is the only region with this label. This is quite a drop from the 70% reported above for the quantifier / cardinal \u201cone\u201d, but note that the situation is quite different: the latter explicitly instructs to find a single one, whereas \u201call\u201d could be any number.\nFurther analysis shows that this way of modelling\n\u201call\u201d is too inclusive: the correct set is included in the one selected this way in 69% of all cases, and on average, \u201call\u201d selects 33% of all regions, whereas it should be selecting only 24%."}, {"heading": "8 Analysis: What do the Models Learn?", "text": "After having shown that the classifiers and the composition of their judgments reaches satifactory performance on the reference resolution task, we return to looking at them directly, as models of word meanings. We first look in more detail at the learning behaviour, before approaching a different semantic task, namely predicting semantic similarity.\nEvaluating the word classifiers individually Evaluating the word classifiers individually poses an interesting problem. Even though we call them classifiers, for the purpose of resolving intended references as evaluated above, we are not actually interested in their classification decision. Remember that above in Section 3, there is an additional step of applying the classifier to all objects in a given scene and normalising over the responses to get a distribution, which is the thing that we are ultimately interested in. But getting this distribution right is not part of the training objective of the classifiers! For good reasons: if it were, we would need to train the classifiers to provide a selection judgment among a pre-specified number of candidate objects. The way we set it up here, the number of candidates to decide among at testing time is completely independent of the number of objects present in the training data\u2014 in fact, the number of objects in a scene does not figure at all in the training of the classifiers.\nWhat the individual classifiers do contribute to the overall task is the score they provide. We want this score to be high for appropriate objects, but we also want it to be low for inappropriate ones. To get an idea of how well the individual classifiers learn this, as a function of the size of their training set, we plot in blue in Figure 4 the average score that a classifier achieves when presented with an object from the test set that was referred to with an expression\nthat contains this word (that is, a positive example), and in red the average score when presented with an object that was not referred to with an expression containing this word. Good classifiers that discriminate well between appropriate and non-appropriate objects should show a large distance between these average scores. Figure 4 shows this for some highfrequency words (for which there is a large number of training instances) as well as for low-frequency words. The error bars show the standard deviation of the set of scores. As this figure shows, the effect of increased training set size mostly seems to be to \u201cstabilise\u201d the classifiers. The classifier for \u201cmiddle\u201d is an example of one that performs badly, despite there being a large number of training examples available. The classifiers for the selected lowfrequency words do not reach a good discriminatory power.\nTo get a picture of the distribution of good and bad classifiers and its dependence on the size of the training set, Figure 5 shows the distance between average scores for positive instances and for negative instances (at maximal size of training set) as a function of the frequency rank. Good classifiers should show a large positive bar (large distance between the responses to appropriate and inapproprate objects), weak classifiers a shorter positive bar, and actively hurtful ones a negative bar. What this figure shows is that there are good classifiers that can be trained even for words for which there are comparatively few training instances, and not so good classifiers even for words for which there are many instances. This is perhaps not surprising, and suggests that there simply are some types of words for which there is no perceptual component in their meaning, or at least none that can be picked up this way. Interestingly though, the actively hurtful classifiers seem to come in only for the low frequency words. We leave to future work an analysis of whether this is directly an effect of size of training set, or of word\ntype, and whether any automatic filtering could be performed before training to remove hurtful classifiers.\nSemantic similarity In what sense do these classifiers capture \u201cmeaning\u201d, though? We\u2019ve shown this indirectly in Section 6, through their performance (as part of a larger model) on a task\u2014reference resolution\u2014that can be seen as requiring access to the meaning of an expression. We now briefly look at another such task, which is more commonly used in distributional semantics, namely semantic similarity.\nWe follow here the recent trend of modelling word meanings as dense vectors and then interpreting distance as semantic (dis)similarity. We create these vectors in two different ways. The first just takes the weights of the logistic regression as the vector representation for the word; this gives us a 27- dimensional vector for each word. The intuition here is that for example for colour words the classifiers should have picked up on the fact that here the features representing colour are relevant and those representing the shape of the mask and the position of the region in the image less so, and so colour words should end up more similar to each other than to other words. We call these vectors intensional word vectors (see Appendix for the motivation).\nCorrespondingly, we also define denotational word vectors, as follows: We set aside a subset of the test set (2500 randomly selected regions), and record for each word classifier the response to each of these regions. This gives us a vector of 2500 responses, which we then take as the representation for this word. The intuition here is that semantically similar words should react similarly to similar objects.\nFor comparison, we also train word2vec representations (Mikolov et al., 2013).9\nTable 6 shows the five most similar words (cosine distance) for a selection of words and the three representation types. As this table shows, there is some agreement between the syntactic / ungrounded method (word2vec) and the grounded one; but the grounded vectors do capture perceptual similarity (woman & boy to statue) that the ungrounded one does not. Also, similarities on the intensional and denotational levels can cut across syntactic paradigms (green / plants; red / jacket). word2vec on the other hand can capture a dimension of similarity that the other methods cannot (left / right; red / black).\nFor our purposes here\u2014motivating the claim that the \u201cwords-as-classifiers\u201d model does indeed capture something that can usefully be called word\n9As implemented in gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010).\nmeaning\u2014this brief discussion shall suffice, and we leave further evaluation of the semantic similarity task to future work."}, {"heading": "9 Conclusions", "text": "We have presented a model of grounded semantics that has as basis the idea that (for certain types of words) the perceptual component of word meaning can be modelled as classification (or scoring) of contexts, and the composition of these meanings into meanings for phrases by intersection on the resulting denotations. This model has several properties that recommend it for use in interactive systems: it by design works incrementally, on successively growing input; and the word meanings can be learned and improved from episodes of use together with a feedback signal, and independently of other word meanings. Moreover, the model makes a precicely encapsulated contribution as a model of (compositional) semantics which in principle could be compatible with many sources of structural information (e.g., incremental parsing). We have evaluated the model here on a medium-scale real-world task that reasonably requires grounded meanings, and have reached\ndecent results. Much remains to be done. As Figure 5 shows, we learn some low-quality classifiers. Are there methods for automatically detecting that a classifier is of low quality, and excluding its judgements? Another, more substantial question is how much one wants to claim this captures of word meaning? This approach, quite purposefully, models words through their relations to percepts and not to other words. But clearly, a fully developed lexicon should capture relations between words (or perhaps, concepts) as well: left is the opposite as right; red, blue, and green are colours, and so on. It seems that a multi-faceted approach, where words can have perceptual as well as conceptual / encyclopedic meaning components should be able to provide a more fully rounded view of meaning. Related to this is the question of how far simple intersective composition can be taken. On the perceptual level, it does not seem to run into some of the obvious problems (e.g., a \u201cfake gun\u201d should probably look \u201cfake\u201d and like a \u201cgun\u201d, even if functionally, the modification serves to remove the referent from the set of all (real) guns); but this may be different for e.g. \u201cold\u201d in \u201cold dog\u201d vs. \u201cold man\u201d; or \u201cwhite\u201d in \u201cwhite dog\u201d, \u201cwhite paper\u201d, \u201cwhite skin\u201d, where the visual contribution of the attribute does seem to depend on what is being modified).\nThere are other possible connections to explore. The analysis in Section 8 showed that it is not necessarily the number of available training instances that explain performance. Recent work on human first language acquisition has recently shown the distinctiveness of contexts of encounter of words as a good predictor of productive use (Roy et al., 2015).\nFinally, on a more technical side, an obvious next step is to improve the feature extraction. We relied on \u201cfound\u201d features here (from SAIAPR) and did not systematically test the individual contributions of these features. We are currently working on learning the best features as well, by setting up the feature extraction step as a convoluational neural network that is shared across words, with only the dense top layer being specific to individual words (but still being a binary task rather than a massively multi-class one).\nFor now, we leave exploring all these avenues to future work.\nAppendix To relate this proposal to more common terminology, we provide a sketch of a formalisation. We only look at nominal phrases here, which we analyse in a shallow way as consisting only of DET (Determiners) and NOM (nominal non-terminal phrase), which are not further analysed syntactically.\nWords: Intensions and Extensions Let w be a word whose meaning is to be modelled as a classifier of the kind described above, and let x be a representation of an object in terms of its visual features. The word classifier then takes this representation and returns a score pw(x), indicating the \u201cappropriateness\u201d of the word for denoting the object.10\nNoting a (loose) correspondence to Montague\u2019s (1974) intensional semantics, where the intension of a word is a function from possible worlds to extensions (Gamut, 1991), the intensional meaning of the word w is then defined in (Kennington and Schlangen, 2015) as the classifier itself, a function from a representation of an object to an \u201cappropriateness score\u201d:\n[[w]]obj = \u03bbx.pw(x) (1)\n(Where [[.]] is a function returning the meaning of its argument, and x is of the type of feature given by fobj , the function computing a feature representation for a given object.)\nThe extension of a word in a given (here, visual) discourse universe W can then be modelled as a probability distribution ranging over all candidate objects in the given domain, resulting from the application of the word intension to each object (xi is the feature vector for object i, normalize() vectorized normalisation, and I a random variable ranging over the k candidates):11\n[[w]]Wobj =\nnormalize(([[w]]obj(x1), . . . , [[w]]obj(xk))) =\nnormalize((pw(x1), . . . , pw(xk))) = P (I|w) (2)\nComposition Unlike with distributional representations of word meanings, composition into meanings for phrases is straightforward in this approach. The meaning for a NOM-phrase such as \u201cred cross\u201d can be computed by interpolating the component distributions; and this is, in this model, the semantic contribution of the syntactic construction NOM:12\n10In (Kennington and Schlangen, 2015) and below, the classifier is a binary logistic regression and the score can be interpreted as a probability, but this is an implementational detail and incidental to the model.\n11The domain of I can be, as in the experiments reported here, reduced to all objects that are currently visible.\n12We simplify and ignore for now any further internal structure of the phrase, such as the specific semantic contribution of prepositional phrases.\n[[[nomw1, . . . , wk]]] W = [[NOM]]W [[w1, . . . , wk]] W =\navg([[w1]] W , . . . , [[wk]] W ) (3)\nwhere avg() is defined as\navg([[w1]] W , [[w2]] W ) = Pavg(I|w1, w2) with Pavg(I = i|w1, w2) =\n1 2 (P (I = i|w1) + P (I = i|w2)) for i \u2208 I (4)\n(We note in passing here that the averaging function is inherently incremental, in the sense that avg(a, b, c) = avg(avg(a, b), c) and hence it can be extended \u201con the right\u201d. As argued in **whatever old paper of ours**, readiness for incremental application is a necessary feature for use in naturalistic interactive systems.)\nQuantifiers To arrive at the desired extension of a full referring expression\u2014an individual object, or a set of objects\u2014, one additional element is needed, and this is contributed by the determiner. For uniquely referring expressions (\u201cthe red cross\u201d), as mentioned above, what is required is to pick the most likely candidate from the distribution:\n[[the]] = \u03bbx. argmax Dom(x) x (5)\n[[[the] [nomw1, . . . , wk]]] W =\nargmax i\u2208W\n[ [[[nomw1, . . . , wk]]] W ] (6)\nGoing beyond (Kennington and Schlangen, 2015), we extend this idea and define the contribution of other determiners similarly as selection functions operating on distributions. E.g., \u201ctwo red crosses\u201d can be resolved to a set of two objects by picking the top two candidate objects from the distribution [[red cross]]. More generally we define (where the operator \u2018arg topn\u2019 picks out the top n elements from a function):\n[[[CARDn] [nomw1, . . . , wk]]] W =\nn arg top\ni\u2208W [ [[[nomw1, . . . , wk]]]\nW ] (7)\nThe quantifier \u201call\u201d must work slightly differently. Intuitively, it should pick out all those objects from the set of candidates that have the indicated properties more than others. In the experiments reported below, we use a simple approach where all those objects are chosen whose probability is above the average probability for the whole distribution.\nNegation Lastly, negation seems to fit into this framework quite easily as well, as we can define the contribution of negation simply as the operation of inverting the distribution of the phrase that the negtion scopes over:\n[[neg([nomw1, . . . , wk])]] W =\ninverse([[[nomw1, . . . , wk]]] W ) (8)\nThis amounts to the claim that the object for which for example \u201cnot red\u201d is most appropriate is the one for which \u201cred\u201d is least appropriate. This is clearly not a logical entailment\u2014a pink object for example is also \u201cnot red\u201d\u2014, but intuitively, it does seem to have some pragmatic justification: In a situation where there is a red, a pink and a green object, the pink one would presumably not be referred to as \u201cnot red\u201d. We leave further investigation to future work and only report experimental results for this approach below."}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Linguistic Issues in Language Technologies (LiLT), 9(6):5\u2013110.", "citeRegEx": "Baroni et al\\.,? 2013", "shortCiteRegEx": "Baroni et al\\.", "year": 2013}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A Context-theoretic Framework for Compositionality in Distributional Semantics", "author": ["Daoud Clarke."], "venue": "Computational Linguistics, 38(1):28\u201348.", "citeRegEx": "Clarke.,? 2011", "shortCiteRegEx": "Clarke.", "year": 2011}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Com-", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "The segmented and annotated IAPR TC-12", "author": ["Hugo Jair Escalante", "Carlos a. Hern\u00e1ndez", "Jesus a. Gonzalez", "a. L\u00f3pez-L\u00f3pez", "Manuel Montes", "Eduardo F. Morales", "L. Enrique Sucar", "Luis Villase\u00f1or", "Michael Grubinger"], "venue": null, "citeRegEx": "Escalante et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Escalante et al\\.", "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollar", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt", "Lawrence Zitnick", "Geoffrey Zweig."], "venue": "Proceedings of CVPR,", "citeRegEx": "Fang et al\\.,? 2015", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Papers in Linguistics, 1934\u20131951", "author": ["John R. Firth."], "venue": "Oxford University Press, Oxford, UK.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Facc1: Freebase annotation", "author": ["Evgeniy Gabrilovich", "Michael Ringgaard", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "Logic, Language and Meaning: Intensional Logic and Logical Grammar, volume 2", "author": ["L.T.F. Gamut."], "venue": "Chicago University Press, Chicago.", "citeRegEx": "Gamut.,? 1991", "shortCiteRegEx": "Gamut.", "year": 1991}, {"title": "The IAPR TC-12 benchmark: a new evaluation resource for visual information systems", "author": ["Michael Grubinger", "Paul Clough", "Henning M\u00fcller", "Thomas Deselaers."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Grubinger et al\\.,? 2006", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "The symbol grounding problem", "author": ["Stevan Harnard."], "venue": "Physica D, 42:335\u2013346.", "citeRegEx": "Harnard.,? 1990", "shortCiteRegEx": "Harnard.", "year": 1990}, {"title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L Berg."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 787\u2013", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution", "author": ["Casey Kennington", "David Schlangen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Kennington and Schlangen.,? 2015", "shortCiteRegEx": "Kennington and Schlangen.", "year": 2015}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar."], "venue": "Transactions of the Association for Computational Linguistics, 1:193\u2013206.", "citeRegEx": "Krishnamurthy and Kollar.,? 2013", "shortCiteRegEx": "Krishnamurthy and Kollar.", "year": 2013}, {"title": "Formal semantics for perceptual classification", "author": ["Staffan Larsson."], "venue": "Journal of logic and computation, 25(2):335\u2013369.", "citeRegEx": "Larsson.,? 2015", "shortCiteRegEx": "Larsson.", "year": 2015}, {"title": "A Joint Model of Language and Perception for Grounded Attribute Learning", "author": ["Cynthia Matuszek", "Nicholas Fitzgerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."], "venue": "Proceedings of the International Conference on Machine Learning (ICML 2012).", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS 2013, pages 3111\u20133119, Lake Tahoe, Nevada, USA, December.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive science, 34(8):1388\u2014-1429, November.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Predicting the birth of a spoken word", "author": ["Brandon C Roy", "Michael C Frank", "Philip Decamp", "Matthew Miller", "Deb Roy."], "venue": "PNAS: Psychological and Cognitive Sciences, pages 1\u20136.", "citeRegEx": "Roy et al\\.,? 2015", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation", "author": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R. Walter", "Ashis Gopal Banerjee", "Seth Teller", "Nicholas Roy."], "venue": "AAAI Conference on Artificial Intel-", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Formal Philosophy: Selected Papers of Richard Montague", "author": ["Richmond H. Thomason", "editor"], "venue": null, "citeRegEx": "Thomason and editor.,? \\Q1974\\E", "shortCiteRegEx": "Thomason and editor.", "year": 1974}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research, 37:141\u2013 188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Tractatus Logicus Philosophicus und Philosophische Untersuchungen, volume 1 of Werkausgabe", "author": ["Ludwig Wittgenstein."], "venue": "Suhrkamp, Frankfurt am Main. this edition 1984.", "citeRegEx": "Wittgenstein.,? 1953", "shortCiteRegEx": "Wittgenstein.", "year": 1953}, {"title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "The recently introduced \u201cwords as classifiers\u201d model of grounded semantics (Kennington and Schlangen, 2015) views words as classifiers on (perceptual) contexts, and composes the meaning of a phrase through \u2018soft\u2019 intersection of the denotations of its component words.", "startOffset": 75, "endOffset": 107}, {"referenceID": 4, "context": "Here, we apply this model to a large set of real-world photographs (SAIAPR TC-12, (Escalante et al., 2010)) that contain objects with a much larger variety of types, and we show that it achieves good performance in a reference resolution task on this data set.", "startOffset": 82, "endOffset": 106}, {"referenceID": 23, "context": ", in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)).", "startOffset": 5, "endOffset": 65}, {"referenceID": 2, "context": ", in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)).", "startOffset": 5, "endOffset": 65}, {"referenceID": 0, "context": ", in (Turney and Pantel, 2010; Clarke, 2011; Baroni et al., 2013)).", "startOffset": 5, "endOffset": 65}, {"referenceID": 4, "context": "Wittgenstein\u2019s (1953) famous dictum \u201cmeaning is use\u201d (PU 43) is these days often taken as providing some form of implicit endorsement for corpusbased methods in semantics and is often referenced in the same paragraph as Firth\u2019s (1957) equally famous dictum \u201cyou shall know a word by the company it keeps\u201d (e.", "startOffset": 220, "endOffset": 235}, {"referenceID": 7, "context": ", (Gabrilovich et al., 2013)), or by including in the corpus some other representation of", "startOffset": 2, "endOffset": 28}, {"referenceID": 15, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 21, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 13, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 26, "context": "the context in which the reference took place and the objects that were referred to ( (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013; Young et al., 2014).", "startOffset": 86, "endOffset": 182}, {"referenceID": 9, "context": "We use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d).", "startOffset": 111, "endOffset": 184}, {"referenceID": 4, "context": "We use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d).", "startOffset": 111, "endOffset": 184}, {"referenceID": 11, "context": "We use a medium-sized corpus of images (20k) with object segmentations (100k) and referring expressions (120k; (Grubinger et al., 2006; Escalante et al., 2010; Kazemzadeh et al., 2014), respectively; see also Section 4 below) to induce word meanings in the form of object classifiers (\u201chow well does this word fit to this object?\u201d).", "startOffset": 111, "endOffset": 184}, {"referenceID": 21, "context": "For example, Tellex et al. (2011) learn to map commands given to a robot to plans to be executed by it.", "startOffset": 13, "endOffset": 34}, {"referenceID": 13, "context": "Of this area, (Krishnamurthy and Kollar, 2013) is", "startOffset": 14, "endOffset": 46}, {"referenceID": 24, "context": "tion a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015).", "startOffset": 86, "endOffset": 129}, {"referenceID": 3, "context": "tion a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015).", "startOffset": 86, "endOffset": 129}, {"referenceID": 5, "context": "(Fang et al., 2015) modify this approach somewhat, by using word detectors first to specifically propose words for image regions, out of which the caption is then generated.", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "Finally, we take from distributional semantics (Turney and Pantel, 2010) the idea that rich word meanings can be learned from records of word uses.", "startOffset": 47, "endOffset": 72}, {"referenceID": 17, "context": "which has been a recent focus of attention in this field (Mitchell and Lapata, 2010; Baroni et al., 2013).", "startOffset": 57, "endOffset": 105}, {"referenceID": 0, "context": "which has been a recent focus of attention in this field (Mitchell and Lapata, 2010; Baroni et al., 2013).", "startOffset": 57, "endOffset": 105}, {"referenceID": 1, "context": ", (Bruni et al., 2014)) has begun to extend the meanings learned from contexts with other words with image contexts.", "startOffset": 2, "endOffset": 22}, {"referenceID": 26, "context": "somewhat different approach is taken by Young et al. (2014), who specify a denotational semantics for certain words as sets of images, the caption of which the word appears in.", "startOffset": 40, "endOffset": 60}, {"referenceID": 12, "context": "The model was introduced in ((Kennington and Schlangen, 2015)); we reformulate and extend it here.", "startOffset": 29, "endOffset": 61}, {"referenceID": 10, "context": "The general idea of modelling words as classifiers on perceptual input goes back at least to (Harnard, 1990) and was most recently developed from a more formal perspective by (Larsson, 2015).", "startOffset": 93, "endOffset": 108}, {"referenceID": 14, "context": "The general idea of modelling words as classifiers on perceptual input goes back at least to (Harnard, 1990) and was most recently developed from a more formal perspective by (Larsson, 2015).", "startOffset": 175, "endOffset": 190}, {"referenceID": 12, "context": "This model (with \u201cthe\u201d as the only quantifier, and without negation) has been evaluated in (Kennington and Schlangen, 2015) on referring expressions in a game-playing scenario with a small number of dif-", "startOffset": 91, "endOffset": 123}, {"referenceID": 9, "context": "locations around the world and comprising an assorted cross-section of still natural images\u201d (Grubinger et al., 2006).", "startOffset": 93, "endOffset": 117}, {"referenceID": 4, "context": "This dataset was later augmented by Escalante et al. (2010) with (among other things) segmentation", "startOffset": 36, "endOffset": 60}, {"referenceID": 9, "context": "IAPR TC12 20k photographs of natural scenes (Grubinger et al., 2006)", "startOffset": 44, "endOffset": 68}, {"referenceID": 4, "context": "5k regions for IAPR TC12 images feature vectors (27 dim) for each region: area, boundary/area, width and height of the region, average and standard deviation in x and y, convexity, average, standard deviation and skewness in both color spaces RGB and CIE-Lab (Escalante et al., 2010)", "startOffset": 259, "endOffset": 283}, {"referenceID": 11, "context": "5k regions (Kazemzadeh et al., 2014)", "startOffset": 11, "endOffset": 36}, {"referenceID": 11, "context": "The final component is provided by Kazemzadeh et al. (2014), who collected a large number of expressions referring to objects (for which segmentations exist) from these images.", "startOffset": 35, "endOffset": 60}, {"referenceID": 12, "context": "(Such constructions were modelled in (Kennington and Schlangen, 2015), in the puzzle game domain.", "startOffset": 37, "endOffset": 69}, {"referenceID": 18, "context": "For both logistic regression and SVM we used the scikit learn package (Pedregosa et al., 2011).", "startOffset": 70, "endOffset": 94}, {"referenceID": 12, "context": "Some relations were modelled in (Kennington and Schlangen, 2015) in the puzzle domain.", "startOffset": 32, "endOffset": 64}, {"referenceID": 16, "context": "For comparison, we also train word2vec representations (Mikolov et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 19, "context": "As implemented in gensim (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 25, "endOffset": 50}, {"referenceID": 20, "context": "tiveness of contexts of encounter of words as a good predictor of productive use (Roy et al., 2015).", "startOffset": 81, "endOffset": 99}, {"referenceID": 8, "context": "10 Noting a (loose) correspondence to Montague\u2019s (1974) intensional semantics, where the intension of a word is a function from possible worlds to extensions (Gamut, 1991), the intensional meaning of the word w is then defined in (Kennington and Schlangen, 2015) as the classifier itself, a function from a representation of an object to an \u201cappropriateness score\u201d:", "startOffset": 158, "endOffset": 171}, {"referenceID": 12, "context": "10 Noting a (loose) correspondence to Montague\u2019s (1974) intensional semantics, where the intension of a word is a function from possible worlds to extensions (Gamut, 1991), the intensional meaning of the word w is then defined in (Kennington and Schlangen, 2015) as the classifier itself, a function from a representation of an object to an \u201cappropriateness score\u201d:", "startOffset": 230, "endOffset": 262}, {"referenceID": 12, "context": "In (Kennington and Schlangen, 2015) and below, the classifier is a binary logistic regression and the score can be interpreted as a probability, but this is an implementational detail and incidental to the model.", "startOffset": 3, "endOffset": 35}, {"referenceID": 12, "context": "Going beyond (Kennington and Schlangen, 2015), we extend this idea and define the contribution of other determiners similarly as selection functions operating on distributions.", "startOffset": 13, "endOffset": 45}], "year": 2017, "abstractText": "The recently introduced \u201cwords as classifiers\u201d model of grounded semantics (Kennington and Schlangen, 2015) views words as classifiers on (perceptual) contexts, and composes the meaning of a phrase through \u2018soft\u2019 intersection of the denotations of its component words. The model is trained from instances of referential language use, and was first evaluated with references in a game-playing scenario with a small number of different types of objects. Here, we apply this model to a large set of real-world photographs (SAIAPR TC-12, (Escalante et al., 2010)) that contain objects with a much larger variety of types, and we show that it achieves good performance in a reference resolution task on this data set. We also extend the model to deal with quantification and negation, and evaluate these extensions, with good results. To investigate what the classifiers learn, we introduce \u201cintensional\u201d and \u201cdenotational\u201d word vectors, and show that they capture meaning similarity in a way that is different from and complementary to word2vec word embeddings.", "creator": "TeX"}}}