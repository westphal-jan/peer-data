{"id": "1510.02675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Controlled Experiments for Word Embeddings", "abstract": "An experimental approach is proposed to investigate the properties of word embedding. Controlled experiments achieved by modifications of the training corpus allow the demonstration of direct relationships between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that vary independently word frequency and word side-by-side noise. Experiments show that word vector length depends more or less linearly on both word frequency and noise level in the coefficient distribution of the word. Coefficients of linearity depend on the word. The particular point in the attribute space, defined by the (artificial) word with pure noise occurring in its juxtaposition, proves to be small but not zero.", "histories": [["v1", "Fri, 9 Oct 2015 14:03:33 GMT  (166kb)", "https://arxiv.org/abs/1510.02675v1", "15 pages"], ["v2", "Mon, 14 Dec 2015 05:10:29 GMT  (159kb)", "http://arxiv.org/abs/1510.02675v2", "Chagelog: Rerun experiment with subsampling turned off; re-interpreted results in light of Schnabel et al. (2015). 15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["benjamin j wilson", "adriaan m j schakel"], "accepted": false, "id": "1510.02675"}, "pdf": {"name": "1510.02675.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["benjamin@lateral.io", "adriaan.schakel@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n02 67\n5v 2\n[ cs\n.C L\n] 1\n4 D\nAn experimental approach to studying the properties of word embeddings is proposed. Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise. The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word. The coefficients of linearity depend upon the word. The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero."}, {"heading": "1 Introduction", "text": "Word embeddings, or distributed representations of words, have been the subject of much recent research in the natural language processing and machine learning communities, demonstrating state-ofthe-art performance on word similarity and word analogy tasks, amongst others. Word embeddings represent words from the vocabulary as dense, real-valued vectors. Instead of one-hot vectors that merely indicate the location of a word in the vocabulary, dense vectors of dimension much smaller than the vocabulary size are constructed such that they carry syntactic and semantic information. Irrespective of the technique chosen, word embeddings are typically derived from word co-occurrences. More specifically, in a machine-learning setting, word embeddings are typically trained by scanning a short window over all the text in a corpus. This process can be seen as sampling word co-occurrence distributions, where it is recalled that the co-occurrence distribution of a target word w denotes the conditional probability P(w\u2032|w) that a word w\u2032 occurs in its context, i.e., given that w occurred. Most applications of word embeddings explore not the word vectors themselves, but relations between them to solve, for example, similarity and word relation tasks [2]. For these tasks, it was found that using normalised word vectors improves performance. Word vector length is therefore typically ignored.\nIn a previous paper [9], we proposed the use of word vector length as measure of word significance. Using a domain-specific corpus of scientific abstracts, we observed that words that appear only in similar contexts tend to have longer vectors than words of the same frequency that appear in a wide variety of contexts. For a given frequency band, we found meaningless function words clearly separated from proper nouns, each of which typically carries the meaning of a distinctive context in this corpus. In other words, the longer its vector, the more significant a word is. We also observed that word significance is not the only factor determining the length of a word vector, also the frequency with which a word occurs plays an important role.\nIn this paper, we wish to study in detail to what extent these two factors determine word vectors. For a given corpus, both term frequency and co-occurrence are, of course, fixed and it is not obvious how to unravel these dependencies in an unambiguous, objective manner. In particular, it is difficult to establish the distinctiveness of the contexts in which a word is used. To overcome these problems, we propose to modify the training corpus in a controlled fashion. To this end, we insert new tokens into the corpus with varying frequencies and varying levels of noise in their co-occurrence distributions. By modeling the frequency and co-occurrence distributions of these tokens, or pseudowords1, on existing words in the corpus, we are able to study their effect on word vectors independently of one another. We can thus study a family of pseudowords that all appear in the same context, but with different frequencies, or study a family of pseudowords that all have the same frequency, but appear in a different number of contexts. Starting from the limited number of contexts in which a word appears in the original corpus, we can increase this number by interspersing the word in arbitrary contexts at random. The word thus looses its significance in a controlled way. Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].\nWe show that the length of the word vectors generated by the CBOW model depends more or less linearly on both word frequency and level of noise in the co-occurrence distribution of the word. In both cases, the coefficient of linearity depends upon the word. If the co-occurrence distribution is fixed, then word vector length increases with word frequency. If, on the other hand, word frequency is held constant, then word vector length decreases as the level of noise in the co-occurrence distribution of the word is increased. In addition, we show that the direction of a word vector varies smoothly with word frequency and the level of co-occurrence noise. When noise is added to the co-occurrence distribution of a word, the corresponding vector smoothly interpolates between the original word vector and a small vector perpendicular to it that represents a word with pure noise in its co-occurrence distribution. Surprisingly, the special point in feature space, obtained by interspersing a pseudoword uniformly at random throughout the corpus with a frequency sufficiently large to sample all contexts, is non-zero.\nThis paper is structured as follows. Section 2 draws connections to related work, while Section 3 describes the corpus and the CBOW model used in our experiments. Section 4 describes a controlled experiment for varying word frequency while holding the co-occurrence distribution fixed. Section 5, in a complementary fashion, describes a controlled experiment for varying the level of noise in the cooccurrence distribution of a word while holding the word frequency fixed. The final section, Section 6, considers further questions and possible future directions."}, {"heading": "2 Related work", "text": "Our experimental finding that word vector length decreases with co-occurrence noise is related to earlier work by Vecchi, Baroni, and Zamparelli [11], where a relation between vector length and the \u201csemantic deviance\u201d of an adjective-noun composite was studied empirically. In that paper, which is also based on word co-occurrence statistics, the authors study adjective-noun composites. They built a vocabulary from the 8k most frequent nouns and 4k most frequent adjectives in a large general language corpus and added 22k adjective-noun composites. For each item in the vocabulary, they recorded the co-occurrences with the top 10k most frequent content words (nouns, adjectives or verbs), and constructed word embeddings via singular value decomposition of the co-occurrence matrix [5]. The authors considered several models for constructing vectors of unattested adjective-noun composites, the two simplest being adding and component-wise multiplying the adjective and noun vectors. They hypothesized that the length of the vectors thus constructed can be used to distinguish acceptable and semantically deviant adjectivenoun composites. Using a few hundred adjective-noun composites selected by humans for evaluation, they found that deviant composites have a shorter vector than acceptable ones, in accordance with their expectation. In contrast to their work, our approach does not require human annotation.\n1We refer to these tokens as pseudowords, since their properties are modeled upon words in the lexicon and because our corpus modification approach is reminiscent of the pseudoword approach for generating labeled data for word sense disambiguation tasks in [4].\nRecent theoretical work [1] has approached the problem of explaining the so-called \u201ccompositionality\u201d property exhibited by some word embeddings. In that work, unnormalised vectors are used in their model of the word relation task. It is hoped that experimental approaches such as those described here might enable theoretical investigations to describe the role of the word vector length in the word relation tasks."}, {"heading": "3 Corpus and model", "text": "Our training data is built from the Wikipedia data dump from October 2013. To remove the bulk of robot-generated pages from the training data, only pages with at least 20 monthly page views are retained.2 Stubs and disambiguation pages are also removed, leaving 463 thousand pages with a total of 482 million words. Punctuation marks and numbers were removed from the pages and all words were lower-cased. Word frequencies are summarised in Table 1. This base corpus is then modified as described in Sections 4 and 5. For recognisability, the pseudowords inserted into the corpus are upper-cased."}, {"heading": "3.1 Word2vec", "text": "Word2vec, a feed-forward neural network with a single hidden layer, learns word vectors from word co-occurrences in an unsupervised manner. Word2vec comes in two versions. In the continuous bagof-words (CBOW) model, the words appearing around a target word serve as input. That input is projected linearly onto the hidden layer and the network then attempts to predict the target word on output. Training is achieved through back-propagation. The word vectors are encoded in the weights of the first synaptic layer, \u201csyn0\u201d. The weights of the second synaptic layer (\u201csyn1neg\u201d, in the case of negative sampling) are typically discarded. In the other model, called skip-gram, target and context words swap places, so that the target word now serves as input, while the network attempts to predict the context words on output.\nFor simplicity only the word2vec CBOW word embedding with a single set of hyperparameters is considered. Specifically, a CBOW model with a hidden layer of size 100 is trained using negative sampling with 5 negative samples, a window size of 10, a minimum frequency of 128, and 10 passes through the corpus. Sub-sampling was not used so that the influence of word frequency could be more clearly discerned. Similar experimental results were obtained using hierarchical softmax, but these are omitted for succinctness. The relatively high low-frequency cut-off is chosen to ensure that word vectors, in all but degenerate cases, receive a sufficient number of gradient updates to be meaningful. This frequency cut-off results in a vocabulary of 81117 words (only unigrams were considered).\nThe most recent revision of word2vec was used.3 The source code for performing the experiments is made available on GitHub.4"}, {"heading": "3.2 Replacement procedure", "text": "In the experiments detailed below, we modify the corpus in a controlled manner by introducing pseudowords into the corpus via a replacement procedure. For the frequency experiment, the procedure is as follows. Consider a word, say cat. For each occurrence of this word, a sample i, 1 6 i 6 n is drawn from a truncated geometric distribution, and that occurrence of the word cat is replaced with the pseudoword CAT i. In this way, the word cat is replaced throughout the corpus by a family of pseudowords with varying frequencies but approximately the same co-occurrence distribution as cat. That is, all these pseudowords are used in roughly the same contexts as the original word.\n2For further justification and to obtain the dataset, see https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/\n3SVN revision 42, see http://word2vec.googlecode.com/svn/trunk/ 4https://github.com/benjaminwilson/word2vec-norm-experiments\nThe geometric distribution is truncated to limit the number of pseudowords inserted into the corpus. For any choice 0 < p < 1 and maximum value n > 0, the truncated geometric distribution is given by the probability density function\nPp,n(i) = pi\u22121(1\u2212 p)\n1\u2212 pn , 1 6 i 6 n. (1)\nThe factor in the denominator, which tends to unity in the limit n \u2192 \u221e, assures proper normalisation. We have chosen this distribution because the probabilities decay exponentially base p as a function of i. Of course, other distributions might equally well have been chosen for the experiments.\nFor the noise experiment, we take instead of a geometric distribution, the distribution\nPn(i) = 2(n\u2212 i)\nn(n\u2212 1) , 1 6 i 6 n. (2)\nWe have chosen this distribution for the noise experiment, because it leads to evenly spaced proportions of co-occurrence noise that cover the entire interval [0, 1]."}, {"heading": "4 Varying word frequency", "text": "In this first experiment, we investigate the effect of word frequency on the word embedding. Using the replacement procedure, we introduce a small number of families of pseudowords into the corpus. The pseudowords in each family vary in frequency but, replacing a single word, all share a common co-occurrence distribution. This allows us to study the role of word frequency in isolation, everything else being kept equal. We consider two types of pseudowords."}, {"heading": "4.1 Pseudowords derived from existing words", "text": "We choose uniformly at random a small number of words from the unmodified vocabulary for our experiment. In order that the inserted pseudowords do not have too low a frequency, only words which occur at least 10 thousand times are chosen. We also include the high-frequency stopword the for comparison. Table 2 lists the words chosen for this experiment along with their frequencies.\nThe replacement procedure of Section 3.2 is then performed for each of these words, using a geometric decay rate of p = 1\n2 , and maximum value n = 20, so that the 1st pseudoword is inserted with a\nprobability of about 0.5, the 2nd with a probability of about 0.25, and so on. This value of p is one of a range of values that ensure that, for each word, multiple pseudowords will be inserted with a frequency sufficient to survive the low-frequency cut-off of 128. A maximum value n = 20 suffices for this choice of p, since 220+log2 128 exceeds the maximum frequency of any word in the corpus. Figure 1 illustrates the effect of these modifications on a sample text, with a family of pseudowords CAT i, derived from the word cat. Notice that all occurrences of the word cat have been replaced with the pseudowords CAT i."}, {"heading": "4.2 Pseudowords derived from an artificial, meaningless word", "text": "Whereas the pseudowords introduced above all replace an existing word that carries a meaning, we now include for comparison a high-frequency, meaningless word. We choose to introduce an artificial, entirely meaningless word VOID into the corpus, rather than choose an existing (stop)word whose meaninglessness is only supposed. To achieve this, we intersperse the word uniformly at random throughout the corpus so that its relative frequency is 0.005. The co-occurrence distribution of VOID thus coincides with the unconditional word distribution. The replacement procedure is then performed for this word, using the same values for p and n as above. Figure 2 shows the effect of these modifications on a sample text, where a higher relative frequency of 0.05 is used instead for illustrative purposes."}, {"heading": "4.3 Experimental results", "text": "We next present the results of the word frequency experiment. We consider the effect of word frequency on the direction and on the length of word vectors separately."}, {"heading": "4.3.1 Word frequency and vector direction", "text": "Figure 3 shows the cosine similarity of pairs of vectors representing some of the pseudowords used in this experiment. Recall that the cosine similarity measures the extent to which two vectors have the same direction, taking a maximum value of 1 and a minimum value of \u22121. The number of different pseudowords associated with an experiment word is the number of times that its frequency can be halved and remain above the low-frequency cut-off of 128.\nConsider first the vectors for the pseudowords associated to the word the. Notice that the cosine similarity of the vectors for THE 1 and THE i decreases monotonically with i, while the cosine similarity of the vectors for THE i and THE 18 increases monotonically with i. Indeed the direction of the vector THE i changes systematically, interpolating between the directions of the vectors of the highest-frequency pseudoword THE 1 and the lowest-frequency pseudoword THE 18. The same trend is apparent (though over shorter frequency ranges) for all the families of pseudowords other than that for VOID.\nConsider now the vectors for pseudowords derived from the meaningless word VOID. The vectors for VOID 7, . . . , VOID 13 are approximately orthogonal to one another, just as would be expected from randomly drawn vectors in a high dimensional space. As the pseudoword VOID occurs by construction in every context, a much higher number of samples is required to capture its co-occurrence distribution, and thereby to learn its vector (the same is true, but to a lesser extent, for the stopword the). We conclude that the vectors corresponding to the lower frequency pseudowords VOID 7, . . . , VOID 13 have not been trained on a sufficient number of samples to establish their proper direction. These vectors are excluded from further analysis. The vectors for VOID 1, . . . , VOID 6, on the other hand, exhibit the smooth change in vector direction with word frequency described in the previous paragraph.\nIn recent work on the evaluation of word embeddings, Schnabel et al. [10] trained logistic regression models to predict whether a word was rare or frequent given only the direction of its word vector. For various word embedding methods, the prediction accuracy was measured as a function of the threshold for word rarity. It was found in the case of word2vec CBOW that word vector direction could be used to distinguish very rare words from all other words. Figure 3 is consistent with this finding, as it is apparent that word vector direction does change gradually with frequency. Schnabel et al. claim further that word vector direction must encode word frequency directly, and not indirectly via semantic information. Figure 3, considered for any particular experiment word in isolation (e.g. SQUAD), demonstrates that the variance of word vector direction with word frequency is indeed independent of co-occurrence (semantic) information, and thereby provides further evidence for this claim."}, {"heading": "4.3.2 Word frequency and vector length", "text": "We next consider the effect of frequency on word vector length. Throughout, we measure vector length using the Euclidean norm. Figure 4 shows this relation for individual words, both for the word vectors, represented by the weights of the first synaptic layer, syn0, in the word2vec neural network, and for the vectors represented by the weights of the second synaptic layer, syn1neg. We include the latter, which are typically ignored, for completeness. Each line corresponds to a single word, and the points on each line indicate the frequency and vector length of the pseudowords derived from that word. For example, the six points on the line corresponding to the word protestant are labeled, from right to left, by the pseudowords PROTESTANT 1, PROTESTANT 2, . . . , PROTESTANT 6. Again, the number of points on the line is determined by the frequency of the original word. For example, the frequency of the word protestant can be halved at most 6 times so that the frequency of the last pseudoword is still above the low-frequency cut-off. Because all the points on a line share the same co-occurrence distribution, the left panel in Figure 4 demonstrates conclusively that length does indeed depend on frequency directly.\nMoreover, this relation is seen to be approximately linear for each word considered. Notice also that the relative positions of the lengths of the word vectors associated with the experiment words are roughly independent of the frequency band, i.e., the plotted lines rarely cross.\nObserve that the lengths of the vectors representing the meaningless pseudowords VOID i are approximately constant (about 2.5). Since we already found the direction to be also constant, it is sensible to speak of the word vector of VOID irrespective of its frequency. In particular, the vector of the pseudoword VOID 1 may be taken as an approximation."}, {"heading": "5 Varying co-occurrence noise", "text": "This second experiment is complementary to the first. Whereas in the first experiment we studied the effect of word frequency on word vectors for fixed co-occurrence, we here study the effect of cooccurrence noise when the frequency is fixed. As before, we do so in a controlled manner."}, {"heading": "5.1 Generating noise", "text": "We take the noise distribution to be the (observed) unconditional word distribution. Noise can then be added to the co-occurrence distribution of a word by simply interspersing occurrences of that word\n9\nuniformly at random throughout the corpus. A word that is consistently used in a distinctive context in the unmodified corpus thus appears in the modified corpus also in completely unrelated contexts. As in Section 4, we choose a small number of words from the unmodified corpus for this experiment. Table 3 lists the words chosen, along with their frequencies in the corpus.\nFor each of these words, the replacement procedure of Section 3.2 is performed using the distribution (2) with n = 7. For every replacement pseudoword (e.g. CAT i), additional occurrences of this pseudoword are interspersed uniformly at random throughout the corpus, such that the final frequency of the replacement pseudoword is 2/n times that of the original word cat. For example, if the original word cat occurred 1000 times, then after the replacement procedure, CAT 2 occurs approximately 238 times, so a further (approximately) 2/7 \u00d7 1000 \u2212 238 \u2248 48 random occurrences of CAT 2 are interspersed throughout the corpus. In this way, the word cat is removed from the corpus and replaced with a family of pseudowords CAT i, 1 6 i 6 7. These pseudowords all have the same frequency, but their co-occurrence distributions, while based on that of cat, have an increasing amount of noise. Specifically, the proportion of noise for the ith pseudoword is\n1\u2212 n\n2 Pn(i) =\ni\u2212 1 n\u2212 1 , or 0, 1 n\u2212 1 , 2 n\u2212 1 , . . . , 1 for i = 1, 2, . . . , n,\nwhich is evenly distributed. The first pseudoword contains no noise at all, while the last pseudoword stands for pure noise. The particular choice of n assures a reasonable coverage of the interval [0, 1]. Other parameter values (or indeed other distributions) could, of course, have been used equally well.\nFigure 5 illustrates the effect of this modification in the case where the only word chosen is cat. The original text in this case concerned both cats and dogs. Notice that the word cat has been replaced entirely in the cats section by CAT i and, moreover, that these same pseudowords appear also in the dogs section. These occurrences (and additionally, with probability, some occurrences from the cats section) constitute noise."}, {"heading": "5.2 Experimental results", "text": "Figure 6 shows the cosine similarity of pairs of vectors representing some of the pseudowords used in this experiment. Remember that the first pseudoword (i = 1) in a family is without noise in its co-occurrence distribution, while the last one (i = n, with n = 7) stands for pure noise and has therefore no relation anymore with the word it derives from. The figure demonstrates that the vectors within a family only moderately deviate from the original direction defined by the first pseudoword (i = 1) when noise is added to the co-occurrence distribution. For 1 < i < 7, the deviation typically increases with the proportion of noise. The vector of the last pseudoword (i = n), associated with pure noise, is seen within each of the families to point in a completely different direction, more or less perpendicular to the original one. To understand this interpolating behavior, recall from Section 4.3 that the vector for the entirely meaningless word VOID is small but non-zero. Since the noise distribution coincides with the co-occurrence distribution of VOID, the vectors for the experiment words must tend to the word vector for VOID as the proportion of noise in their co-occurrence distributions approaches\n1. This convergence to a common point is only indistinctly apparent in Figure 6, as the frequency of the experiment pseudowords is insufficient to sample the full variety of the contexts of VOID, i.e., all contexts (see Section 4.3.1).\nThe left panel in Figure 7 reveals that vector length varies more or less linearly with the proportion of noise in the co-occurrence distribution of the word. This figure motivates an interpretation of vector length, within a sufficiently narrow frequency band, as a measure of the absence of co-occurrence noise, or put differently, of the extent to which a word carries the meaning of a distinctive context."}, {"heading": "6 Discussion", "text": "Our principle contribution has been to demonstrate that controlled experiments can be used to gain insight into a word embedding. These experiments can be carried out for any word embedding (or indeed language model), for they are achieved via modification of the training corpus only. They do not require knowledge of the model implementation. It would naturally be of interest to perform these experiments for other word embeddings other than word2vec CBOW, such as skipgrams and GloVe, as well as for different hyperparameters settings.\nMore elaborate experiments could be carried out. For instance, by introducing pseudowords into the corpus that mix, with varying proportions, the co-occurrence distributions of two words, the path between the word vectors in the feature space could be studied. The co-occurrence noise experiment described here would be a special case of such an experiment where one of the two words was VOID.\nQuestions pertaining to word2vec in particular arise naturally from the results of the experiments. Figures 4 and 7, for example, demonstrate that the word vectors obtained from the first synaptic layer, syn0, have very different properties from those that could be obtained from the second layer, syn1neg. These differences warrant further investigation.\n13\nThe co-occurrence distribution of VOID is the unconditional frequency distribution, and in this sense pure background noise. Thus the word vector of VOID is a special point in the feature space. Figure 4 shows that this point is not at the origin of the feature space, i.e., is not the zero vector. The origin, however, is implicitly the point of reference in word2vec word similarity tasks. This raises the question of whether improved performance on similarity tasks could be achieved by transforming the feature space or modifying the model such that the representation of pure noise, i.e., the vector for VOID, is at the origin of the transformed feature space."}, {"heading": "7 Acknowledgments", "text": "The authors thank Tobias Schnabel for helpful discussions."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "CoRR, abs/1502.03520,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Work on statistical methods for word sense disambiguation", "author": ["William A Gale", "Kenneth W Church", "David Yarowsky"], "venue": "In Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dutnais"], "venue": "PSYCHOLOGICAL REVIEW,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1310.4546,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Measuring word significance using distributed representations of words", "author": ["Adriaan M.J. Schakel", "Benjamin J. Wilson"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Linear) Maps of the Impossible: Capturing Semantic Anomalies in Distributional Space", "author": ["Eva M. Vecchi", "Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the Workshop on Distributional Semantics and Compositionality,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Most applications of word embeddings explore not the word vectors themselves, but relations between them to solve, for example, similarity and word relation tasks [2].", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "In a previous paper [9], we proposed the use of word vector length as measure of word significance.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 198, "endOffset": 204}, {"referenceID": 5, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 198, "endOffset": 204}, {"referenceID": 7, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 212, "endOffset": 215}, {"referenceID": 2, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 227, "endOffset": 230}, {"referenceID": 10, "context": "Our experimental finding that word vector length decreases with co-occurrence noise is related to earlier work by Vecchi, Baroni, and Zamparelli [11], where a relation between vector length and the \u201csemantic deviance\u201d of an adjective-noun composite was studied empirically.", "startOffset": 145, "endOffset": 149}, {"referenceID": 4, "context": "For each item in the vocabulary, they recorded the co-occurrences with the top 10k most frequent content words (nouns, adjectives or verbs), and constructed word embeddings via singular value decomposition of the co-occurrence matrix [5].", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "We refer to these tokens as pseudowords, since their properties are modeled upon words in the lexicon and because our corpus modification approach is reminiscent of the pseudoword approach for generating labeled data for word sense disambiguation tasks in [4].", "startOffset": 256, "endOffset": 259}, {"referenceID": 0, "context": "Recent theoretical work [1] has approached the problem of explaining the so-called \u201ccompositionality\u201d property exhibited by some word embeddings.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "We have chosen this distribution for the noise experiment, because it leads to evenly spaced proportions of co-occurrence noise that cover the entire interval [0, 1].", "startOffset": 159, "endOffset": 165}, {"referenceID": 9, "context": "[10] trained logistic regression models to predict whether a word was rare or frequent given only the direction of its word vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The particular choice of n assures a reasonable coverage of the interval [0, 1].", "startOffset": 73, "endOffset": 79}], "year": 2015, "abstractText": "An experimental approach to studying the properties of word embeddings is proposed. Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise. The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word. The coefficients of linearity depend upon the word. The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero.", "creator": "LaTeX with hyperref package"}}}