{"id": "1510.03931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2015", "title": "Structured Memory for Neural Turing Machines", "abstract": "Neural Turing Machines (NTM) contain memory components that simulate the \"working memory\" in the brain to store and retrieve information so that simple algorithms can learn more easily. So far, only a linearly organized memory has been suggested, and in experiments we have observed that the model does not always converge and easily match in the handling of certain tasks. We believe that memory components are the key to some of NTM's erroneous behavior, and better organization of the memory component could help combat these problems. In this paper, we propose several different memory structures for NTM, and we have demonstrated in experiments that two of our proposed structured memory components could lead to better convergence in terms of speed and prediction accuracy for copying tasks and associative memory tasks (Graves et al. 2014).", "histories": [["v1", "Wed, 14 Oct 2015 00:08:17 GMT  (912kb,D)", "https://arxiv.org/abs/1510.03931v1", "4 pages, NIPS workshop requirement"], ["v2", "Tue, 20 Oct 2015 21:52:04 GMT  (912kb,D)", "http://arxiv.org/abs/1510.03931v2", "4 pages, NIPS workshop requirement"], ["v3", "Sun, 25 Oct 2015 03:12:49 GMT  (912kb,D)", "http://arxiv.org/abs/1510.03931v3", "4 pages, accepted to Reasoning, Attention, Memory (RAM) NIPS 2015 Workshop"]], "COMMENTS": "4 pages, NIPS workshop requirement", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["wei zhang", "yang yu", "bowen zhou"], "accepted": false, "id": "1510.03931"}, "pdf": {"name": "1510.03931.pdf", "metadata": {"source": "CRF", "title": "Structured Memory for Neural Turing Machines", "authors": ["Wei Zhang", "Yang Yu", "Bowen Zhou"], "emails": ["zhangwei@us.ibm.com", "yu@us.ibm.com", "zhou@us.ibm.com", "wynnzh@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Memory components for Neural Networks has been recently introduced in Neural Turing Machines (NTMs) [2], Memory Networks [1], and Dynamic Memory Networks [3]. The purposes of those memory components are similar, which is to simulate \u201cworking memory\u201d in the brain to store temporary information through time to be used by attention module for reading and writing. In this paper, our focus is on NTMs. They are attractive in that memory could be randomly accessed by controller through blurry \u201cerase\u201d and \u201cwrite\u201d operations, which well aligns with Turing Machine operations. When memory size is small, nice convergence of the NTM model could be frequently observed, although not always. But when memory size is large, the model struggles to convergence, and sometimes the test loss jumps drastically in a large range, which is a sign of overfitting, as was observed in our experiments. We think that the dynamics of memory contents takes a crucial role in controlling model convergence speed and quality. Thus, we proposed and experimented different memory structures to explore if specific memory structure could lead to more stable memory contents, or say, perform \u201cmemory smoothing\u201d so that the memory content generated after read and write operations does not deviate too far from the \u201cexpected\u201d memory content, so that overfitting of parameters could be in turn alleviated. We proposed three different architectures, NTM1, NTM2 and NTM3, which are explained in detail in section 2, and experiments are shown in section 3."}, {"heading": "2 Architectures", "text": "We show the NTM original architecture introduced by Graves et al. [2] and three variants NTM1, NTM2 and NTM3 in Figure 1. Details of each model is explained in a moment. We introduce a notion of Memory visibility: We call memory \u201cControlled\u201d if it is modified by controller outputs through write heads directly, or \u201cHidden\u201d if not.\nNTM1 Compared to NTM, NTM1 has an additional hidden memory Mh, which is not controlled by controller module, but is connected to the controlled memory Mc. The hidden memory accumulates the content in the controlled memory, so that a type of memory smoothing is performed to prevent\n\u2217Alternative email for Wei Zhang: wynnzh@gmail.com\nar X\niv :1\n51 0.\n03 93\n1v 3\n[ cs\n.A I]\n2 5\nO ct\n2 01\nmemory from deviating from the \u201cexpected content\u201d. Specifically, the memory content for hidden memory Mh(t) of time t is generated as:\nwrite : Mc(t) = h ( Mc(t\u2212 1),w(t\u2212 1), c(t) ) update : Mh(t) = aMh(t\u2212 1) + bMc(t) read : r(t) = wr(t)Mh(t)\nMc is updated by t\u2212 1 time write head to generate t time controlled memory output, which in turn is used to update hidden memory Mh, and then the new hidden memory is used to generate read head at time t. w(t) is write weights, c(t) is the controller output of time t, and h() is the function that updates controlled memory and write weights that implement \u201cerase and add\u201d operations. wr is read weights. a and b are scalar mixture weights, which could be further extended to tensors. We use scalars in this work. The read head is reading from Mh(t) instead of M(t) as is did in [2].\nNTM2 The second architecture NTM2 is similar to NTM1 in that two memory blocks are used, and they are connected hierarchically. However, the difference from NTM1 is that hidden memory in NTM2 is no longer hidden from the controller, but connected to controller outputs through another write head. So two memory blocks are all controlled memories connecting to two different write heads. The upper level memory is denoted as M1, and the lower level one as M2. M1 is modified solely by L1 write head, but M2 is modified by both M1 contents and L2 write head. The logic is:\nwrite : M1(t),w1(t) = h ( M1(t\u2212 1),w1(t\u2212 1), c(t) ) M\u03032(t),w2(t) = h ( M2(t\u2212 1),w2(t\u2212 1), c(t)\n) update : M2(t) = aM\u03032(t) + bM1(t)\nread : r(t) = wr(t)M2(t)\nAdditionally, M1 and M2 are all generated by the same head function with same controller input, but with different write weights. Single read head r is used, which means the output is only read from the lowest layer memory (M2 in this case). Note that the architecture could be easily expanded into multiple layers by increasing number of heads and memory blocks.\nNTM3 The third architecture is significantly different from the previous two. First, the controller in the model have multiple layers. Second, each layer output is connecting to a memory by write heads. Different layers of memory contains different level of transformation of the input. In the case where multi-layer LSTM is used as NTM controller, output from each layer goes through the non-linear transformation of write heads, and writes to each individual memory. Then the deeper\nlevel memory is updated by upper level memory and write head jointly. write : M1(t),wL1(t) = h ( M1(t\u2212 1),wL1(t\u2212 1), cL1(t) ) M\u03032(t),wL2(t) = h ( M2(t\u2212 1),wL2(t\u2212 1), cL2(t)\n) update : M2(t) = aM\u03032(t) + bM1(t)\nread : r(t) = wr(t)M2(t)\nIn so doing, the memory blocks receives write operations not only from the final layer LSTM output, but also from the intermediate layer outputs as well. The purpose is to smooth the final layer memoroy with intermediate LSTM outputs. We can see in a moment how those three performs on copy and associative recall task."}, {"heading": "3 Experiments", "text": "The experiments is to show the convergence speed and quality of those three variants, compared to the NTM setting. In theory, the convergence should happen for every run of every model, which is not the case in practice. Randomness of parameter initialization might be the reason1. But if model convergences, the number of iterations used were quite stable across runs. Thus, to make the evaluation reliable, we repeat experiments at least 5 times for each model and choose the most frequent circumstance for showing.\nThe tasks we choose for model testing are copy task and associative recall task as in [2]. Associative recall task requires the model to target an item in sequence, and later on shift to the immediate next item to give as an answer. Copy task requires the model to consecutively output items in original input order, which requires long-term location based addressing capability. In each training iteration, we randomly generated each item as a binary vector of length 8. The number of items is random as well. In recall task, the items, and number of items to be generated are all chosen at random. All those randomness is to guarantee that every training iteration uses different training example, so that overfit to specific data set is no longer an issue, to guarantee that algorithms could be learned. This is critical to our further analysis.For all the models, the training criteria is binary cross entropy, and we use RMSProp [4] for optimization, with learning rate 1E \u2212 4, momentum 0.9, and decay 0.95. In Figure 2 we show our results on copy and recall respectively. Copy tasks are four graphs to the left. X-axis shows the number of iteration used (sampled every 25 iterations), and Y-axis is the\n1We applied the technique used by https://github.com/kaishengtai/torch-ntm to initialize each memory slot to define \u201cwrite/read order\u201d prior\nbinary cross entropy loss of each iteration. The shown result in each graph is representative among 5 runs, among which the selected curve is observed most frequently. We can see that NTM1 and NTM2 shows faster convergence and less outliers above the convergence range (closely around 0 near X-axis) than NTM or NTM3. The outliers (extremely high loss) means that the prediction on the item in that iteration is significantly incorrect, which is a sign of overfitting or underfitting. In NTM1 and NTM2 runs we see very few outliers, but NTM shows more outliers when using two read and write heads (the curve with sharp values), or does not converge as fast when using 1 read and 1 write head (flat curve). This pattern is consistently observed over 5 runs as well. We can also read out from Figure \u201cNTM-copy\u201d that using 1 read and 1 write head converges significantly slower than 2w/r heads. The series of experiments on copy task confirms our assumption that the introduction of additional memory in NTM1 and NTM2 do help stabilize the memory component, which in turn leads to better tuning of the parameters. However, NTM3 does not show nice curves, and produce outlier almost every 1000 iterations or so. This means that the memory that is attached to the intermediate layers of LSTM introduces more noise, which is unexpected. Although less stable, NTM does converge about 500 iterations faster than NTM1 and NTM2, but we observed the opposite in associative recall task.\nFor associative recall, we can see that outliers are produced much more frequently when loss significantly reduces, and we rarely observe convergence of original NTM or NTM3. In \u201cNTM3-recall\u201d graph, we show a non-converged run since it happens frequently for NTM3. But NTM1 and NTM2 converges more frequently across runs. And NTM2 shows much faster convergence, roughly with 37,000 iterations, compared to 50,000 for NTM or NTM1. Moreover, NTM1 and NTM2 generate less outliers than NTM. This means that, the prediction accuracy for NTM1 and NTM2 will be higher than NTM, since more training examples are correctly predicted than NTM. And, NTM1 and NTM2 will have higher probability to be a non-overfitting model than NTM. Unfortunately, NTM3 model does not converge as frequent as NTM1 and NTM2, which aligns with the observation in copy task about the difference of those models."}, {"heading": "4 Conclusion", "text": "This paper discussed three new structured memory architectures for Neural Turing Machines, and showed that organizing memory blocks in a proper hierarchical manner could alleviate overfitting and sometimes increase predictive accuracy compared to NTM."}, {"heading": "5 Future work", "text": "In the future we would also try NTM1 NTM2 on data sets that require more complex reasoning. We tested NTM on a synthetic QA data set proposed in [1], and observed the instability in convergence. We would try NTM1 and NTM2 on the same data set."}], "references": [{"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Memory components for Neural Networks has been recently introduced in Neural Turing Machines (NTMs) [2], Memory Networks [1], and Dynamic Memory Networks [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "Figure 1: NTM and NTM variants that use Long short-term memory [5] as controllers.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "For all the models, the training criteria is binary cross entropy, and we use RMSProp [4] for optimization, with learning rate 1E \u2212 4, momentum 0.", "startOffset": 86, "endOffset": 89}], "year": 2015, "abstractText": "Neural Turing Machines (NTM) [2] contain memory component that simulates \u201cworking memroy\u201d in the brain to store and retrieve information to ease simple algorithms learning. So far, only linearly organized memory is proposed, and during experiments, we observed that the model does not always converge, and overfits easily when handling certain tasks. We think memory component is key to some faulty behaviors of NTM, and better organization of memory component could help fight those problems. In this paper, we propose several different structures of memory for NTM, and we proved in experiments that two of our proposed structured-memory NTMs could lead to better convergence, in term of speed and prediction accuracy on copy task and associative recall task as in [2].", "creator": "LaTeX with hyperref package"}}}