{"id": "1510.04396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Filtrated Spectral Algebraic Subspace Clustering", "abstract": "Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial adaptation and differentiation to cluster noiseless data from an arbitrary union of subspaces. In practice, however, ASC is limited to equal-dimensional subspaces, since estimating the subspace dimension by algebraic methods is sensitive to noise. In this paper, a new ASC algorithm is proposed that can process noisy data from subspaces of any dimension. Key ideas are (1) to construct at each point a decreasing sequence of substances containing the subspace passing through that point; (2) to use the distances from each other point to each subspace in order to construct a subspace cluster group that is superior to alternative affinities both in theory and practice. Experiments with the Hopkins 155 datasets demonstrate the superiority of the proposed method in terms of sparse and low-granular methods of cluster formation.", "histories": [["v1", "Thu, 15 Oct 2015 04:12:37 GMT  (245kb,D)", "http://arxiv.org/abs/1510.04396v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["manolis c tsakiris", "rene vidal"], "accepted": false, "id": "1510.04396"}, "pdf": {"name": "1510.04396.pdf", "metadata": {"source": "CRF", "title": "Filtrated Spectral Algebraic Subspace Clustering", "authors": ["Manolis C. Tsakiris", "Ren\u00e9 Vidal"], "emails": ["m.tsakiris@jhu.edu", "rvidal@jhu.edu"], "sections": [{"heading": "1. Introduction", "text": "Subspace clustering is the problem of clustering a collection of points drawn approximately from a union of linear subspaces. This is an important problem in pattern recognition with diverse applications from computer vision [22] to genomics [15].\nRelated Work. Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization. The need for good initialization motivated the development of an algebraic technique called Generalized Principal Component Analysis (GPCA) [26], which solves the problem in closed form. The key idea behind GPCA is that a union of n subspaces can be represented by a collection of polynomials of degree n, with the property that their gradients at a data point give the normals to the subspace passing through that point. This\n\u2217This work was supported by grant NSF 1447822.\nis exploited in [24] and [8] for clustering a known number of subspaces. The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces. However, while in theory GPCA and AASC are applicable to subspaces of any dimensions, in practice the estimation of the subspaces is sensitive to data corruptions.\nThe need for methods that can handle high-dimensional data corrupted by noise and outliers motivated the quest for better subspace clustering affinities. State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces. Sparse and low rank representation techniques are then used to compute the coefficients, which are then used to build a subspace clustering affinity. These methods perform very well when the subspace dimensions are much smaller than the dimension of the ambient space, the subspaces are sufficiently separated and the data are well distributed inside the subspaces [6, 16]. However, these methods fail when the dimensions of the subspaces are large, e.g. a union of hyperplanes, which is the case where GPCA, to be henceforth called Algebraic Subspace Clustering (ASC), performs best. In addition, sparse methods produce low Inter-Class Connectivity, but the Intra-Class Connectivity is also low due to sparsity, leading to over-segmentation issues. Conversely, Low-Rank and `2 methods produce high Intra-Class Connectivity (since they are less sparse) but this also leads to high Inter-Class Conectivity. Consequently, there is a strong need for developing methods that produce high Intra-Class and low Inter-Class connectivity.\nPaper Contributions. The main contribution of this paper is to propose a new subspace clustering algorithm that can handle noisy data drawn from a union of subspaces of different dimensions. The key idea is to construct for each data point (the reference point) a sequence of projections onto hyperplanes that contain the reference subspace (the subspace associated to the reference point). The norms of the\nar X\niv :1\n51 0.\n04 39\n6v 1\n[ cs\n.C V\n] 1\n5 O\nct 2\n01 5\nprojected data points are used to define their affinity with the reference point. This process leads to an affinity matrix of high intra-class and low cross-class connectivity, upon which spectral clustering is applied. We provide a theorem of correctness of the proposed algorithm in the absence of noise as well as a variation suitable for noisy data. As a secondary contribution, we propose to replace the angle-based affinity proposed in [26] by a superior distance-based affinity. This modification is motivated by the fact that the anglebased affinity is theoretically correct only in the case of hyperplanes, and is not a good affinity for subspaces of varying dimensions. Our experiments demonstrate that the proposed method outperforms other subspace clustering algorithms on the Hopkins 155 motion segmentation database as well as on synthetic experiments for arbitrary-dimensional subspaces of a low-dimensional ambient space."}, {"heading": "2. Algebraic Subspace Clustering: A Review", "text": "We begin with a brief overview of the ASC theory and algorithms. We refer the reader to [25, 26, 3, 14] for details. Subspace Clustering Problem. LetX = {x1, . . . ,xN} be a set of points that lie in an unknown union of n > 1 subspacesA = S1\u222a\u00b7 \u00b7 \u00b7\u222aSn, where Si a linear subspace of RD of dimension di < D. The goal of subspace clustering is to find the number of subspaces, a basis for each subspace, and cluster the data points based on their subspace membership, i.e., find the correct decomposition or clustering of X as X = X1 \u222a \u00b7 \u00b7 \u00b7 \u222a Xn, where Xi = X \u2229 Si. To make the subspace clustering problem well-defined, we need to make certain assumptions on the geometry of both the subspaces Si and the data X . In this work we assume that the underlying union of subspaces A is transversal [14], which in particular implies that there are no inclusions between subspaces. Moreover, we assume that Xi \u2229 Si\u2032 = \u2205 for i 6= i\u2032, i.e., each of the given points is associated to a unique subspace. This guarantees that the above decomposition of X is in fact a partition, and it is unique. A final assumption that we need is that the data X are rich enough and in general position (see Definition 1). Unions of Subspaces as Algebraic Varieties. A key idea behind ASC is that a union of n subspacesA = S1\u222a\u00b7 \u00b7 \u00b7\u222aSn of RD is the zero set of a finite set of homogeneous polynomials of degree n with real coefficients in D indeterminates (x1, . . . , xD). Such a set is called an algebraic variety [1]. For example, a union of n hyperplanesA = H1\u222a \u00b7 \u00b7 \u00b7\u222aHn, where the ith hyperplane Hi = {x : b>i x = 0} is defined by its normal vector bi \u2208 RD, is the zero set of\np(x) = (b>1 x)(b > 2 x) \u00b7 \u00b7 \u00b7 (b > n x). (1)\nLikewise, the union of a plane with normal b and a line with normals b1, b2 \u2208 R3 is the zero set of the two polynomials p1(x) = (b>x)(b>1 x) and p2(x) = (b >x)(b>2 x).\nObserve that these vanishing polynomials are homogeneous of degree n, where n is the number of subspaces. Moreover, they are factorizable into linear forms, with each subspace contributing a linear form to the product. Each such linear form is in turn defined by a normal vector to the subspace.\nFinding Vanishing Polynomials. Note that the coefficients of the polynomials associated with a union of subspaces A can be obtained from sufficiently many samples X \u2282 A in general position by solving a linear system of equations.\nDefinition 1 We say that the data X \u2282 A is in general position if a degree n polynomial vanishes on X if and only if it vanishes on the underlying union of subspaces A.\nFor example, if A is a union of two planes in R3 with normals bi = (bi1, bi2, bi3), i = 1, 2, then we can write p as\np(x) = (b11x1 + b12x2 + b13x3)(b21x1 + b22x2 + b23x3)\n= c1x 2 1 + c2x1x2 + \u00b7 \u00b7 \u00b7+ c6x23 = c>\u03bd2(x), (2)\nwhere c = (c1, . . . , c6) and \u03bd2(x) = (x21, x1x2, . . . , x 2 3). Thus, we can find the vector of coefficients c by solving the set of linear equations c>\u03bd2(xj) = 0 for j = 1, . . . , N . More generally, each polynomial of degree n can be written as p(x) = c>\u03bdn(x), where \u03bdn : RD \u2192 RMn(D) is the Veronese embedding of degree n that maps a point x \u2208 RD to all Mn(D) := ( n+D\u22121\nn\n) distinct monomials of degree\nn in the entries of x. Consequently, a basis for the set of polynomials of degree n that vanishes in X can be found by computing a basis for the right nullspace of the embedded data matrix, i.e., by solving the linear system:\nVn(X )c = [\u03bdn(x1) \u03bdn(x2) \u00b7 \u00b7 \u00b7 \u03bdn(xN )]>c = 0. (3)\nHowever, the polynomials obtained by the above procedure may not factorize into a product of linear forms because the space of factorizable polynomials is not a linear space, e.g (x1 + x2)x1 \u2212 (x1 \u2212 x2)x2 = x21 + x22 is not factorizable. Polynomial Differentiation Algorithm. Even though an elegant solution based on polynomial factorization exists for the case of hyperplanes [25], it has not been generalized for subspaces of different dimensions. However, an alternative solution has been achieved by observing that given any degree n vanishing polynomial p on A, and a point x in A, the gradient of p evaluated at x will be orthogonal to the subspace associated with point x (see [26] and [14] for a geometric and algebraic argument respectively). Consequently, for the purpose of computing normal vectors to the subspaces, it is enough to compute general vanishing polynomials of degree n. The set of all such polynomials, denoted IX ,n, is a finite-dimensional vector space and a basis can be computed as a basis of the right nullspace of the Veronese matrix Vn(X ) := [\u03bdn(x1) \u03bdn(x2) \u00b7 \u00b7 \u00b7 \u03bdn(xN )]>, where \u03bdn : RD \u2192 RMn(D) is the Veronese embedding\nof degree n that maps a point x \u2208 RD to all Mn(D) :=( n+D\u22121\nn\n) distinct monomials of degree n in the entries of\nx. Having a basis p1, . . . , ps for IX ,n, it can be shown that the subspace associated to a point x \u2208 X can be identified as the orthogonal complement of the subspace spanned by the vectors\u2207p1|x,\u2207p2|x, . . . ,\u2207ps|x [26, 14]. Then we can remove the points that lie in the same subspace as x and iterate the procedure with the remaining points until all subspaces have been identified. It is remarkable that this procedure is provably correct for a known number of subspaces of arbitrary dimensions. Even though this result is general and insightful, algorithms that are directly based on it are extremely sensitive to noise. The main reason is that any procedure for estimating the dimension of the nullspace will unavoidably involve thresholding the singular values of Vn(X ), which will in turn yield very unstable estimates of the subspaces and subsequently poor clustering of the points. Spectral Algebraic Subspace Clustering Algorithm. In the interest of enhancing the robustness of ASC in the presence of noise and obtaining a working algebraic algorithm, the standard practice has been to apply a variation of the polynomial differentiation algorithm based on spectral clustering. More specifically, given noisy data X lying close to a union of n subspaces A, one computes an approximate vanishing polynomial p whose coefficients are given by the right singular vector of Vn(X ) corresponding to its smallest singular value. Given p, one computes the gradient of p at each point in X (which gives a normal vector associated with each point in X ), and builds an affinity matrix between points xj and xj\u2032 as the cosine of the angle between their corresponding normal vectors, i.e.,\nCjj\u2032 = \u2223\u2223\u2223\u2329 \u2207p|xj||\u2207p|xj || , \u2207p|xj\u2032||\u2207p|xj\u2032 || \u232a\u2223\u2223\u2223. (4) This affinity is then used as input to any spectral clustering algorithm to obtain the clusteringX = \u222ani=1Xi. We call this Spectral ASC method with angle-based affinity as SASC-A. To gain some intuition on C, suppose A is a union of n hyperplanes and that there is no noise. Then p must be of the form in (1). In that case Cjj\u2032 is simply the cosine of the angle between the normals to the hyperplanes that are associated with points xj and xj\u2032 . If both points lie in the same hyperplane, their normals must be equal, and hence Cjj\u2032 = 1. Otherwise, Cjj\u2032 < 1 is the cosine of the angles between the hyperplanes. Thus, assuming that these angles are not small, and that the points are well distributed on the union of the hyperplanes, spectral clustering on the affinity matrix C will in general yield the correct clustering. Even though SASC-A is much more robust in the presence of noise than purely algebraic methods for the case of a union of hyperplanes, it is fundamentally limited by the fact that it applies only to unions of hyperplanes. Indeed, if\nthe orthogonal complement of a subspace S has dimension greater than 1, there may be points x,x\u2032 inside S such that the angle between \u2207p|x and \u2207p|x\u2032 is as large as 90\u25e6. In such instances, points associated to the same subspace may be weakly connected and thus there is no guarantee for the success of spectral clustering. Abstract Filtration Scheme. Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20]. The procedure is abstract in the sense that it receives as input a union A \u2282 RD of an unknown number of subspaces of arbitrary dimensions, and it decomposes it to the list of its constituent subspaces. This is done recursively by identifying a single subspace each time: A is intersected with the hyperplane V1, whose normal vector is the gradient of a vanishing polynomial at a point x \u2208 A. Then V1 contains the subspace S associated to x and so does the new smaller union of subspaces A1 = A \u2229 V1. Next A1 is intersected with a hyperplane V2 of V1, whose normal is the gradient of a vanishing polynomial of A1 evaluated at x. As before, A2 = A1 \u2229 V1 contains S and the process repeats until no non-zero vanishing polynomial exists, in which case S is precisely Vc, c = D \u2212 dimS. By picking a point x \u2208 A \u2212 S a new subspace S \u2032 is identified and so on. This method has very strong theoretical guarantees (for noiseless data) but is fairly abstract in nature. It is the very purpose of the remaining of this paper to adapt the work of [19, 20] to a numerical algorithm and to experimentally demonstrate its merit."}, {"heading": "3. Filtrated Spectral ASC", "text": "In this section, we propose a new subspace clustering procedure which addresses the robustness of ASC with respect to noise and unknown subspace dimensions, especially in the case of subspaces of varying dimensions."}, {"heading": "3.1. A Distance-Based Affinity", "text": "Our first contribution is to replace the angle-based affinity in (4) by a distance-based-affinity and to show that the new affinity possesses superior theoretical guarantees.\nGiven unit norm data points X = {xj}Nj=1 lying close to an unknown union of n subspaces, let p be an approximate vanishing polynomial whose coefficients are given by the right singular vector of Vn(X ) associated with its smallest singular value. We define the distance-based-affinity as\nDjj\u2032 = 1\u2212 1\n2 \u2223\u2223\u3008\u2207p|xj ,xj\u2032\u3009\u2223\u2223\u2212 12 \u2223\u2223\u3008\u2207p|xj\u2032 ,xj\u3009\u2223\u2223 (5) where the gradient vectors are assumed to be normalized to unit Euclidean norm. We will refer to this Spectral ASC\nmethod with the distance-based affinity in (5) SASC-D. The denomination distance-based comes from the fact that the Euclidean distance from point xj\u2032 to the hyperplane H(j) defined by the unit normal vector \u2207p|xj is precisely\u2223\u2223\u3008\u2207p|xj ,xj\u2032\u3009\u2223\u2223. Moreover,H(j) contains the subspace passing through xj . Thus, if xj and xj\u2032 are in the same subspace, then the distance from xj\u2032 to H(j) is zero and so is the distance from xj to H(j\n\u2032). This implies that Djj\u2032 = 1. Of course, it may be the case that Djj\u2032 = 1 for points xj and xj\u2032 coming from distinct subspaces. For instance, consider a union of two lines in R3 and choose a plane containing one of the lines. If the plane happens to contain the two lines, then Djj\u2032 = 1 for all pairs of points in the two lines.\nTheorem 1 Let {xj}Nj=1 be points of RD lying in a union of n subspaces A. Let p be a homogeneous polynomial of any degree vanishing onA. Then the distance-based affinity in (5) is such that if points xj ,xj\u2032 lie in the same subspace, then Djj\u2032 = 1. The converse is not true in general."}, {"heading": "3.2. Filtrated ASC", "text": "Theorem 1 shows the superiority of the distance-based affinity in (5) over the angle-based affinity in (4) because it ensures that points from the same subspace will be given an affinity of maximal value 1. What still limits the theoretical guarantees of (5) is the fact that points from distinct subspaces may also have a maximal affinity 1.\nIn this section, we show that it is possible to further refine (5) by a filtration process illustrated in Figure 1. Let X = {x1, . . . ,xN} be a set of points in RD in general position in a union of n transversal subspacesA = \u222ani=1Si. Assume that each point lies in only one of the n subspaces and is normalized to have unit norm. The key idea behind the filtration process is that, given an arbitrary reference point x \u2208 X in one of the subspaces, say S, we can identify all other data points in the same subspace as x by 1) projecting all data points in X onto S and 2) finding the points in X whose norm after projection remains equal to one.\nThe fundamental challenge, however, is that we do not know S. The filtration process in Figure 1 is designed, precisely, to perform a sequence of projections, which ultimately give the projection onto S without knowing S.\nAt step 1 of the filtration, choose a vanishing polynomial p0 of A of degree n from the nullspace of Vn(X ) such that \u2207p0|x 6= 0. One can show that such a p0 always exists. Let\nb1 := \u2207p0|x/||\u2207p0|x|| and let H1 be the hyperplane of RD defined by b1. If |\u3008b1,xj\u3009| > 0, then by Theorem 1 we know that point xj is not in S. Consequently, we can filter the set X to obtain a subset X1 := {xj \u2208 X : |\u3008b1,xj\u3009| = 0}. Geometrically, X1 is precisely the subset of X that lies inside the hyperplaneH1, i.e., X1 = X \u2229H1.\nThe key observation now is that X1 is a set of points of RD drawn from the union of subspaces A1 := A \u2229 H1 =\u22c3n i=1(Si \u2229 H1). But A1 is up to isomorphism a union of subspaces of RD\u22121, since it is embedded in the hyperplane H1. In particular, consider the composite linear transformation \u03c01 : RD \u2192 H1\n\u223c\u2212\u2192 RD\u22121, where the first arrow is the orthogonal projection of RD onto H1 and the second arrow maps a basis of H1 to the standard basis of RD\u22121. We can replace the redundant representation X1 by X\u03041 := \u03c01(X1) \u2282 RD\u22121. It is important to note that the norm of every point in X1 remains unchanged and equal to 1 under the transformation \u03c01. Note also that now X\u03041 may be actually a subset of a union of n1 \u2264 n subspaces of RD\u22121, as it is quite possible that all points in X lying in some subspace Si 6= S were filtered out.\nNow, since X is in general position inside A, X1 will be in general position inside S1, and from this one can deduce that every vanishing polynomial of X\u03041 has gradient orthogonal to \u03c01(S1) at \u03c0(x), and that there is a vanishing polynomial p1 of degree n1 such that \u2207p1|\u03c01(x) 6= 0. Let H2 be a hyperplane of RD\u22121 defined by the normal vector b2 := \u2207p1|\u03c01(x)/||\u2207p1|\u03c01(x)||. Note that H2 contains all the points of X\u03041 that correspond to S. As before, we can filter the set X\u03041 to obtain a new setX2 = X\u03041\u2229H2. Once again, X2 lies in a union of at most n1 subspaces of RD\u22121, which is however embedded in the hyperplane H2, and thus we can replace X2 by its image X\u03042 under the composite linear transformation \u03c02 : RD\u22121 \u2192 H2\n\u223c\u2212\u2192 RD\u22122, in which the first arrow is the orthogonal projection of RD\u22121 onto H2, and the second arrow maps a basis of H2 to the standard basis of RD\u22122. Proceeding inductively, this process will terminate precisely after c steps, where c = D \u2212 dim(S) is the codimension of S. More specifically, there will be no non-zero vanishing polynomials on X\u0304c and Hc will be isomorphic to S. Thus X\u0304c will consist of the images of the points of X \u2229 S under the sequence of transformations \u03c0c \u25e6 \u03c0c\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c01. We note that the norm of these points remains unchanged and equal to 1 under \u03c0c \u25e6\u03c0c\u22121 \u25e6\u00b7 \u00b7 \u00b7\u25e6\u03c01.\nOnce the points of X that lie in S have been identified, we can remove them and repeat the process starting with the set X \u2212 X \u2229 S1, which lies in general position inside S2 \u222a \u00b7 \u00b7 \u00b7 \u222a Sn. This leads to Algorithm 1, which we term Filtrated-Algebraic-Subspace-Clustering (FASC), and is guaranteed to return the correct clustering:\nTheorem 2 Let X = {xj}Nj=1 be points of R D lying in a transversal union of subspaces A = \u222ani=1Si. Let Xi = X \u2229 Si,\u2200i \u2208 [n]. Assuming that the points of X are in\nAlgorithm 1 Filtrated Algebraic Subspace Clustering 1: procedure FASC(X = {x1, . . . ,xN} \u2282 RD, n) 2: Y \u2190 \u2205,Z \u2190 \u2205; 3: for i = 1 : n\u2212 1 do 4: X \u2032 \u2190 X \u2212Z; d\u2190 D; 5: take any x \u2208 X \u2032, k \u2190 0; 6: while ||x|| = 1 do 7: k \u2190 k + 1; 8: find p \u2208 N (V\u2264n(X \u2032)) s.t. \u2207p|x 6= 0; 9: \u03c0k \u2190 [ Rd \u2192 \u3008\u2207p|x\u3009\u22a5 \u223c\u2212\u2192 Rd\u22121 ] ; 10: X \u2032 \u2190 {\u03c0k(y) : y \u2208 X \u2032, \u3008\u2207p|x,y\u3009 = 0}; 11: x\u2190 \u03c0k(x); d\u2190 d\u2212 1; 12: end while 13: X \u2032 \u2190 {x \u2208 X : ||\u03c0k\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c01(x)|| = 1}; 14: Z \u2190 Z \u222a X \u2032; Y \u2190 Y \u222a {(X \u2032, d+ 1)}; 15: end for 16: Y \u2190 Y \u222a {(X \u2212 Z, rank(X \u2212 Z))}; 17: return Y; 18: end procedure\ngeneral position inside A, Algorithm 1 returns a set Y = {(Yi, di)}ni=1 such that Yi = X\u03c4(i), di = dimS\u03c4(i), i = 1, . . . , n, where \u03c4 is a permutation on n symbols."}, {"heading": "3.3. Filtrated Spectral ASC", "text": "Let us now consider the case where the data X are corrupted by noise. In this case, Algorithm 1 (FASC) is not applicable because the noisy embedded data matrix Vn(X ) is in general full rank. Nonetheless, we will show next that we can still exploit the insights revealed by the theoretical guarantees of FASC to construct a Robust-ASC algorithm.\nTo begin with, note that Algorithm 1 requires a single vanishing polynomial at each step of each filtration. We can use any approximate vanishing polynomial at step k. For example, letting X\u0304k\u22121 be the points that have passed through the filtration at step k \u2212 1, we can let pk\u22121 be the polynomial whose coefficients are given by the right singular vector of Vn(X\u0304k\u22121) corresponding to its smallest singular value. Notice that no thresholding is required to choose such a pk\u22121. This is in sharp contrast to the polynomial differentiation algorithm described in Section 2, which requires a thresholding on the singular values of Vn(X ) in order to estimate a basis of IX ,n. Now, for any point x \u2208 X , pk\u22121 gives a hyperplane \u3008\u2207pk\u22121|x\u3009\u22a5 that approximately contains the subspace associated to point x. However, we cannot go to the next step due to the following problems.\nProblem 1 In general, two points lying approximately in the same subspace S will produce different hyperplanes that approximately contain S with different levels of accuracy. In the noiseless case any point would be equally good. In\nthe presence of noise though, the choice of the reference point x becomes significant. How should x be chosen?\nProblem 2 Given a hyperplane produced by a point x, we need to determine which other points inX lie approximately in the hyperplane and filter out the remaining points. A simple approach is to filter out a point if its distance to the hyperplane is above a threshold \u03b4, or if the relative change in its norm is more than \u03b4. Clearly the choice of \u03b4 will affect the performance of the algorithm. How should \u03b4 be chosen?\nProblem 3 Finally, we also need to determine the number of steps needed to stop the filtration. This is equivalent to determining the codimension of the subspace associated to the reference point of that filtration. In the noiseless case, one stops when the norm of the reference point becomes less than 1. In the noisy case, because the hyperplanes used to construct the filtration are only approximate, the norm of the reference point could drop at every step of the filtration. Hence a suitable stopping criterion needs to be devised.\nInspired be the SASC-D algorithm, which handles noise by computing a normal vector for each data point and uses the normal vectors to define a distance-based affinity, we propose to address Problem 1 by constructing a filtration for each data point xj \u2208 X with reference point xj and using the norms of the data points to construct the affinity.\nLet \u03c0(j)k be the projection at step k of the filtration for point xj . Recall that at step k, only a subset of the original points will remain, while others will be filtered out. We can define an affinity matrix as\nCkjj\u2032 =\n{ \u2016\u03c0(j)k \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0 (j) 1 (xj\u2032)\u2016 if xj\u2032 remains\n0 otherwise. (6)\nThis affinity captures the fact that if points xj and xj\u2032 are in the same subspace, then the norm of xj\u2032 should not change from step 0 to step k of the filtration computed with reference point xj . Otherwise, if xj and xj\u2032 are in different subspaces, the norm of xj\u2032 is expected to be reduced by the time the filtration reaches step c = D \u2212 dim(S), where S is the reference subspace associated to xj . In the case of noiseless data, only the points in the correct subspace survive step c and their norms are precisely equal to one. Therefore, C(c)jj\u2032 = 1 if points xj and xj\u2032 are in the same subspace and C(c)jj\u2032 = 0 otherwise. In the case of noisy data, the above affinity will not be perfect due to Problems 2 and 3, which we address next.\nTo address Problem 2, let p be the approximate vanishing polynomial whose coefficients are the right singular vector of Vn(X ) corresponding to the smallest singular value. Let\n\u03b2(X ) = 1 N N\u2211 j=1 \u2223\u2223\u2223\u2329xj , \u2207p|xj||\u2207p|xj ||\u232a \u2223\u2223\u2223. (7)\nNotice that \u03b2 = 0 in the noiseless case. In the presence of noise, \u03b2(X ) is the average over all points of the distance of a point from the hyperplane that it produces. Evidently, small levels of noise will correspond to small values of \u03b2(X ). Thus, we propose to define \u03b4 = \u03b3 \u00b7 \u03b2(X ), where \u03b3 > 0 is a user defined parameter. To determine \u03b3, we propose to construct multiple filtrations for different values \u03b31, . . . , \u03b3M of \u03b3. Each filtration will result in a different affinity matrix. Suppose we have defined a stopping criterion to terminate each filtration so that we can use the affinity matrix at the last step of the filtration (see below for stopping criteria). Given these affinity matrices, we choose the one whose normalized Laplacian has the largest eigengap \u03bbn+1 \u2212 \u03bbn, where the eigenvalues are ordered increasingly.\nTo address Problem 3, we stop the filtration at step k if 1) the number of points is less than the ambient dimension of the Veronese-embedded points; 2) the reference point xj is filtered out at the (k + 1)th step; or 3) the number of points that passed through the filtration at step k + 1 is less than some integer \u00b5. This integer is the smallest number of points that our algorithm is allowed to consider as a cluster.\nFinally, the resulting affinity is symmetrized, and used for spectral clustering, as described in Algorithm 2.1"}, {"heading": "4. Experiments", "text": "Synthetic Data. We randomly generate n = 3 subspaces of dimensions di \u2208 {1, 2, 3, 4} in R5. For each choice of {di}, we randomly generate Ni = 100 unit norm points per subspace and add zero-mean Gaussian noise with standard deviation \u03c3 \u2208 {0, 0.01, 0.03, 0.05} in the direction orthogonal to the subspace. For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13]. We also use the heuristic post processing of the affinity for LRR (LRR-H) and LSR (LSR-H). For FSASC we use \u00b5 = 10 and \u03b3 \u2208 {0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10}, for SSC \u03b1 = 20 and \u03c1 = 0.7, for LRR \u03bb = 4, for LRSC \u03c4 = 420, \u03b1 = 4000 and for LSR \u03bb = 0.0048. We report average clustering errors, intra-cluster connectivities of the affinity matrices C produced by the methods (defined to be the minimum algebraic connectivity2 among the subgraphs corresponding to each of the three subspaces) and intercluster connectivities ( \u2211 xj\u2208Si,xj\u2032\u2208Si\u2032 ,i6=i\u2032\n|Cj,j\u2032 |/||C||1). Due to lack of space, we report errors on all methods only\n1SPECTRUM ( NL(C+C>) ) denotes the spectrum of the normalized\nLaplacian matrix of C + C>, SPECCLUST ( C\u2217 + (C\u2217)>, n ) denotes spectral clustering being applied to C\u2217 + C\u2217> to obtain n clusters, and VANISHING ( Vn(X ) ) is the polynomial whose coefficients are the right singular vector of Vn(X ) corresponding to the smallest singular value. 2The algebraic connectivity of a graph is the second smallest eigenvalue of the Laplacian of the graph. Here we use the normalized Laplacian.\nAlgorithm 2 Filtrated Spectral ASC 1: procedure FSASC(X , D, n, \u00b5, {\u03b3m}Mm=1) 2: if N <Mn(D) then 3: return (\u2019Not enough points\u2019); 4: else 5: eigengap\u2190 0; C\u2217 \u2190 0N\u00d7N ; 6: xj \u2190 xj/||xj ||, \u2200j \u2208 [N ]; 7: p\u2190 VANISHING(Vn(X )); 8: \u03b2 \u2190 1N \u2211N j=1\n\u2223\u2223\u3008xj , \u2207p|xj||\u2207p|xj || \u3009\u2223\u2223; 9: for k = 1 : M do\n10: \u03b4 \u2190 \u03b2 \u00b7 \u03b3k, C \u2190 0N\u00d7N ; 11: for j = 1 : N do 12: Cj,: \u2190 FILTRATION(X ,xj , p, \u00b5, \u03b4, n); 13: end for 14: {\u03bbs}Ns=1 \u2190 SPECTRUM(NL(C + C\n>)) ; 15: if (eigengap < \u03bbn+1 \u2212 \u03bbn) then 16: eigengap\u2190 \u03bbn+1 \u2212 \u03bbn; C\u2217 \u2190 C; 17: end if 18: end for 19: {Yi}ni=1 \u2190 SPECCLUST(C\n\u2217 + C\u2217>, n); 20: return {Yi}ni=1; 21: end if 22: end procedure\n23: function FILTRATION(X ,x, p, \u00b5, \u03b4, n) 24: d\u2190 D, J \u2190 [N ], q \u2190 p, c\u2190 01\u00d7N ; 25: flag\u2190 1; 26: while (d > 1) and (flag = 1) do 27: H \u2190 \u3008\u2207q|x\u3009\u22a5, \u03c0 \u2190 [ Rd \u2192 H \u223c\u2212\u2192 Rd\u22121 ] ;\n28: if (||x|| \u2212 ||\u03c0(x)||)/||x|| > \u03b4 then 29: if d = D then 30: c(j\u2032)\u2190 ||\u03c0(x\u2032j)||, \u2200j\u2032 \u2208 [N ]; 31: end if 32: flag\u2190 0; 33: else 34: J \u2190 { j\u2032 \u2208 [N ] : ||xj\u2032 ||\u2212||\u03c0(xj\u2032 )||||xj\u2032 || \u2264 \u03b4\n} 35: if |J | < \u00b5 then 36: flag\u2190 0; 37: else 38: c(j\u2032)\u2190 ||\u03c0(x\u2032j)||, \u2200j\u2032 \u2208 J ; 39: c(j\u2032)\u2190 0, \u2200j\u2032 \u2208 [N ]\u2212 J ; 40: if |J | <Mn(d) then 41: flag\u2190 0; 42: else 43: d\u2190 d\u2212 1,x\u2190 \u03c0(x); 44: xj\u2032 \u2190 \u03c0(xj\u2032)\u2200j\u2032 \u2208 J ; 45: X \u2190 {xj\u2032 : j\u2032 \u2208 J }; 46: q \u2190 VANISHING(Vn(X )) ; 47: end if 48: end if 49: end if 50: end while 51: return (c); 52: end function\nfor 0% and 5% noise. Table 1 reports the mean clustering errors. Observe that FSASC is the only method that gives 0 error for noiseless\ndata for all dimension configurations, thus verifying experimentally its strong theoretical guarantees: no restrictions on the dimensions of subspaces are required for correctness. As expected, SSC, LRR, LRSC and LSR yield perfect clustering when d/D is small, but their performance degrades significantly for large d/D. Observe also that, although SASC-D is much simpler than FSASC and has similar complexity to SASC-A, its performance is very close to that of FSASC, and much better than SASC-A. We attribute this phenomenon to the correctness Theorem 1 of SASC-D. As the noise level increases, FSASC remains stable across all dimension configurations with superior behavior among all compared methods. SASC-D is less robust in the presence of noise, except for the case of hyperplanes, in which it is the best method. This phenomenon is expected, since SASC-D is essentially equivalent to FSASC if the latter is configured to take only one step in each filtration in Figure 1, and this is precisely the optimal stopping point in every filtration when the subspaces are hyperplanes. In this case, if data are noisy, the criterion for stopping FSASC filtrations is determined by the parameter \u03b3 and by the level of noise via the quantity \u03b2, leading to suboptimal values (i.e., more than one step may be taken in the filtration).\nTables 2 and 3 indicate that FSASC yields higher quality affinity graphs for the purpose of clustering. To see why this is the case, observe that except for FSASC, we can distinguish two kinds of behavior in the remaining methods: the first kind gives high intra-cluster connectivity at the cost of high inter-cluster connectivity. Such methods are SASC-D, SASC-A, LRR, LRSC and LSR. The second kind gives low inter-cluster connectivity at the expense of low intra-cluster connectivity leading to unstable clustering results by the spectral clustering method. Such methods are SSC, LRR-H and LSR-H. This is expected because these methods use sparse affinities. On the other hand, FSASC circumvents this trade-off by giving high intra-cluster connectivity and low inter-cluster connectivity, thus enhancing the success of the spectral clustering step.\nMotion Segmentation. We evaluate different methods on the Hopkins155 motion segmentation data set [18], which contains 155 videos of n = 2,3 moving objects, each one with N = 100-500 feature point trajectories of dimension D = 56-80. While SSC, LRR, LRSC and LSR can operate directly on the raw data, algebraic methods require Mn(D) \u2264 N . Hence, for algebraic methods, we project the raw data onto the subspace spanned by theirD principal components, where D is the largest integer \u2264 8 such that Mn(D) \u2264 N , and then normalize each point to have unit norm. We apply SSC to i) the raw data (SSC-raw) and ii) the raw points projected onto their first 8 principal components and normalized to unit norm (SSC-proj). For FSASC, LRR, LRSC and LSR we use the same parameters as before, while for SSC the parameters are \u03b1 = 800 and \u03c1 = 0.7.\nThe clustering errors and the intra/inter-cluster connectivities are reported in Table 4 and Fig. 2. Notice the clustering errors of about 5% and 37% for SASC-A, which is the classical GPCA algorithm. Notice how changing the angle-based by the distance-based affinity (SASC-D) already gives errors of around 5.5% and 14%. But most dramatically, notice how FSASC further reduces those errors to 0.8% and 2.48%. This clearly demonstrates the advantage of FSASC over classical ASC. Moreover, even though the dimensions of the subspaces (di \u2208 {1, 2, 3, 4} for motion segmentation) are low relative to the ambient space dimension (D = 56-80) - a case that is specifically suited for SSC, LRR, LRSC, LSR - projecting the data to D \u2264 8, which makes the subspace dimensions comparable to the ambient dimension, is sufficient for FSASC to get superior performance relative to the best performing algorithms on Hopkins 155. We believe that this is because, overall, FSASC produces a much higher inter-cluster connectivity, without increasing the intra-cluster connectivity too much.\nHandwritten Digit Clustering. In this section we consider the problem of clustering two digits, one of which is the digit 1 (see Benford\u2019s law, e.g. [9]). For each pair (1, i), i = 0, 2, ..., 9, we randomly select 200 images from the MNIST database [10] corresponding to each digit and compute the clustering errors averaged over 100 independent experiments. SSC, LRR and LSR operate on raw data.\nLRR and LSR parameters are the same as before. For FSASC we set \u00b5 = 10 and \u03b3 = 1 and for SSC we set \u03b1 = 10 and \u03c1 = 0.7, as before. For the three algebraic methods we first project the raw data onto their first 13 principal components and then normalize each point to have unit norm. For comparison, we also run SSC on the projected data. Mean errors are reported in Table 5 with SASCA, LRR, LRSC omitted since they perform poorly (with LRR performing worse with the post-processing). We also do not show the numbers for SSC-proj since they are very\nclose to those of SSC-raw. As in the case of motion segmentation, we observe that FSASC outperforms SASC-D (this time by a large margin), which in turn significantly outperforms SASC-A. This confirms the superiority of FSASC over previous algebraic methods. As before, FSASC is also superior to SSC. The only method that performs better is LSR-H. We note that projecting the 784-dimensional data onto dimension 13, reduces the angles between the subspaces, thus making the clustering problem harder. As a result, for more than 2 digits the performance of FSASC degrades singinficantly, even for projection dimensions up to D = 17, since it becomes harder for the method to distinguish the subspaces. To circumvent this issue, a higher projection dimension would be required, which currently can not be handled by FSASC, due to the high complexity."}, {"heading": "5. Conclusions", "text": "We presented a novel algebraic subspace clustering method based on the geometric idea of filtrations and we experimentally demonstrated its robustness to noise using synthetic and real data and its superiority to the state-of-the-art algorithms on several occasions. Overall, the method works very well for subspaces of arbitrary dimensions in a lowdimensional ambient space, and it can handle higher dimensions via a projection. The main weakness of the method is its high computational complexity, which comes from the large number of filtrations required, as well as the exponential cost of fitting polynomials to n subspaces. Future research will be concerned with reducing the complexity, as well as dealing with outliers and missing entries."}], "references": [{"title": "Introduction to Commutative Algebra", "author": ["M. Atiyah", "I. MacDonald"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "Journal of Global Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Hilbert series of subspace arrangements", "author": ["H. Derksen"], "venue": "Journal of Pure and Applied Algebra,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Clustering disjoint subspaces via sparse representation", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A closed form solution to robust subspace estimation and clustering", "author": ["P. Favaro", "R. Vidal", "A. Ravichandran"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Minimum effective dimension for mixtures of subspaces: A robust GPCA algorithm and its applications", "author": ["K. Huang", "Y. Ma", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Benford\u2019s Law : Theory, the General Law of Relative Quantities, and Forensic Fraud Detection Applications", "author": ["A. Kossovsky"], "venue": "World Scientific Publishing Company,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Robust and efficient subspace segmentation via least squares regression", "author": ["C.-Y. Lu", "H. Min", "Z.-Q. Zhao", "L. Zhu", "D.-S. Huang", "S. Yan"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Estimation of subspace arrangements with applications in modeling and segmenting mixed data", "author": ["Y. Ma", "A. Yang", "H. Derksen", "R. Fossum"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Subspace clustering of high-dimensional data: a predictive approach", "author": ["B. McWilliams", "G. Montana"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A geometric analysis of subspace clustering with outliers", "author": ["M. Soltanolkotabi", "E.J. Cand\u00e8s"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M. Tipping", "C. Bishop"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "A benchmark for the comparison of 3-D motion segmentation algorithms", "author": ["R. Tron", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Abstract algebraic-geometric subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "In Proceedings of Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Abstract algebraic subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "CoRR, abs/1506.06289,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Nearest q-flat tom points", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Low rank subspace clustering (LRSC)", "author": ["R. Vidal", "P. Favaro"], "venue": "Pattern Recognition Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A new GPCA algorithm for clustering subspaces by fitting, differentiating and dividing polynomials", "author": ["R. Vidal", "Y. Ma", "J. Piazzi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "This is an important problem in pattern recognition with diverse applications from computer vision [22] to genomics [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "This is an important problem in pattern recognition with diverse applications from computer vision [22] to genomics [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization.", "startOffset": 126, "endOffset": 137}, {"referenceID": 20, "context": "Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization.", "startOffset": 126, "endOffset": 137}, {"referenceID": 16, "context": "Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization.", "startOffset": 126, "endOffset": 137}, {"referenceID": 25, "context": "The need for good initialization motivated the development of an algebraic technique called Generalized Principal Component Analysis (GPCA) [26], which solves the problem in closed form.", "startOffset": 140, "endOffset": 144}, {"referenceID": 23, "context": "is exploited in [24] and [8] for clustering a known number of subspaces.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "is exploited in [24] and [8] for clustering a known number of subspaces.", "startOffset": 25, "endOffset": 28}, {"referenceID": 18, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 67, "endOffset": 75}, {"referenceID": 7, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 98, "endOffset": 105}, {"referenceID": 25, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 61, "endOffset": 70}, {"referenceID": 4, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 61, "endOffset": 70}, {"referenceID": 5, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 61, "endOffset": 70}, {"referenceID": 11, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 6, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 22, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 10, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 5, "context": "These methods perform very well when the subspace dimensions are much smaller than the dimension of the ambient space, the subspaces are sufficiently separated and the data are well distributed inside the subspaces [6, 16].", "startOffset": 215, "endOffset": 222}, {"referenceID": 15, "context": "These methods perform very well when the subspace dimensions are much smaller than the dimension of the ambient space, the subspaces are sufficiently separated and the data are well distributed inside the subspaces [6, 16].", "startOffset": 215, "endOffset": 222}, {"referenceID": 25, "context": "As a secondary contribution, we propose to replace the angle-based affinity proposed in [26] by a superior distance-based affinity.", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 25, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 2, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 13, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 13, "context": "In this work we assume that the underlying union of subspaces A is transversal [14], which in particular implies that there are no inclusions between subspaces.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Such a set is called an algebraic variety [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 24, "context": "Even though an elegant solution based on polynomial factorization exists for the case of hyperplanes [25], it has not been generalized for subspaces of different dimensions.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "However, an alternative solution has been achieved by observing that given any degree n vanishing polynomial p on A, and a point x in A, the gradient of p evaluated at x will be orthogonal to the subspace associated with point x (see [26] and [14] for a geometric and algebraic argument respectively).", "startOffset": 234, "endOffset": 238}, {"referenceID": 13, "context": "However, an alternative solution has been achieved by observing that given any degree n vanishing polynomial p on A, and a point x in A, the gradient of p evaluated at x will be orthogonal to the subspace associated with point x (see [26] and [14] for a geometric and algebraic argument respectively).", "startOffset": 243, "endOffset": 247}, {"referenceID": 25, "context": ",\u2207ps|x [26, 14].", "startOffset": 7, "endOffset": 15}, {"referenceID": 13, "context": ",\u2207ps|x [26, 14].", "startOffset": 7, "endOffset": 15}, {"referenceID": 7, "context": "Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20].", "startOffset": 182, "endOffset": 185}, {"referenceID": 18, "context": "Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20].", "startOffset": 304, "endOffset": 312}, {"referenceID": 19, "context": "Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20].", "startOffset": 304, "endOffset": 312}, {"referenceID": 18, "context": "It is the very purpose of the remaining of this paper to adapt the work of [19, 20] to a numerical algorithm and to experimentally demonstrate its merit.", "startOffset": 75, "endOffset": 83}, {"referenceID": 19, "context": "It is the very purpose of the remaining of this paper to adapt the work of [19, 20] to a numerical algorithm and to experimentally demonstrate its merit.", "startOffset": 75, "endOffset": 83}, {"referenceID": 5, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 196, "endOffset": 199}, {"referenceID": 10, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 205, "endOffset": 213}, {"referenceID": 11, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 205, "endOffset": 213}, {"referenceID": 22, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 220, "endOffset": 224}, {"referenceID": 12, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 256, "endOffset": 260}, {"referenceID": 17, "context": "We evaluate different methods on the Hopkins155 motion segmentation data set [18], which contains 155 videos of n = 2,3 moving objects, each one with N = 100-500 feature point trajectories of dimension D = 56-80.", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": ", 9, we randomly select 200 images from the MNIST database [10] corresponding to each digit and compute the clustering errors averaged over 100 independent experiments.", "startOffset": 59, "endOffset": 63}], "year": 2015, "abstractText": "Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods.", "creator": "LaTeX with hyperref package"}}}