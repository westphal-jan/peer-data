{"id": "1510.04454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Online Markov decision processes with policy iteration", "abstract": "The Online Markov Decision Process (MDP) is a generalization of the classic Markov decision process, which includes changing reward functions. In this article, we propose practical online MDP algorithms with political iteration, and theoretically establish a sublinear boundary of regret. A notable advantage of the proposed algorithm is that it can easily be combined with a functional approximation, and thus large and possibly continuous states can be efficiently handled. In experiments, we demonstrate the usefulness of the proposed algorithm.", "histories": [["v1", "Thu, 15 Oct 2015 09:19:49 GMT  (166kb)", "http://arxiv.org/abs/1510.04454v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yao ma", "hao zhang", "masashi sugiyama"], "accepted": false, "id": "1510.04454"}, "pdf": {"name": "1510.04454.pdf", "metadata": {"source": "CRF", "title": "Online Markov decision processes with policy iteration", "authors": ["Yao Ma", "Hao Zhang", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\nThe online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm."}, {"heading": "1 Introduction", "text": "A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes. A standard way to solve the stochastic shortest path problem is to formulate it as a Markov decision process (MDP) and find a policy that maximizes the cumulative reward over the path. In the MDP problem, the agent chooses the best action according to the current state and moves to the next state following the Markovian dynamics. A fixed reward function assigns a reward value to each state-action pair.\nA generalization of MDP, called the online MDP, considers the situation where the reward function changes over time. At each time step, the learning agent decides the strategy of choosing actions by using the knowledge of past reward functions. Then, the current reward function which is chosen by the environment is revealed to the agent after observing its behavior. The goal of online MDP is to minimize the regret against the best offline policy, which is the optimal fixed policy in hindsight. We expect that the regret vanishes as the time step T tends to infinity, implying that the agent can behave as well as the best offline policy asymptotically.\nMany online problems can be solved as online MDP problems. By setting the optimization variables as the state, the online MDP algorithm chooses the change of variables (action) which performs reasonably well in a non-stationary environment. Even-Dar et al. (2009) presented several typical online problems which can be formulated as online MDP perfectly, e.g. paging, k-server, metrical task system and stochastic inventory control.\nThe online MDP problem was first introduced by Even-Dar et al. (2003, 2009) and an expert-based MDP algorithm (MDP-E) was proposed, which was shown to achieve regret O( \u221a T |A|) (|A| is the cardinality of action space) by placing an expert algorithm on every state. Furthermore, the MDP-E algorithm was proved to achieve regret O(L2 \u221a T log |A|) for online MDP problems with L-layered state space (Neu et al., 2010a). However, the MDP-E algorithm is not computationally feasible for problems with large state space, since it needs to put the expert algorithm on every state.\nAnother online MDP algorithm called the lazy follow-the-perturbed-leader (lazyFPL) (Yu et al., 2009) follows the main idea of the FPL algorithm which solves the Bellman equation using the average reward function. The \u201dlazy\u201d behavior of the lazyFPL algorithm divides the time horizon into short periods and the policy is only updated at the end of each period. This lazy-FPL algorithm was proved to achieve sublinear regret O(T 3/4+\u01eb log T (|S|+ |A|)|A|2) for \u01eb \u2208 (0, 1/3).\nSimilarly to lazy-FPL, the online relative entropy policy search (O-REPS) algorithm (Zimin and Neu, 2013) also requires to solve an optimization problem at the end of each time step. It was shown that the O-REPS algorithm achieves regret O(L \u221a T log(|S||A|/L)) for online MDP problems with L-layer state space. Thus, the regret bound of O-REPS is much sharper than those for the MDP-E algorithm when L is large. However, OREPS requires the length of time horizon T to be finite, because the step size for parameter update needs to be set as a function of T . Therefore, it cannot be directly extended to problems with infinite time horizon. By introducing the stationary occupation measure, Dick et al. (2014) proposed the mirror descent with approximation projections algorithm, which formulate the online MDP problem as online linear opti-\nmization. Their theoretical results show that the regret is bounded by O( \u221a T ) where the finite state space assumption is essential.Yu et al. (2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics.\nRecently, Ma et al. (2014) proposed the online policy gradient (OPG) algorithm for online MDP problems with continuous state and action spaces, and it was proved to achieve regret O( \u221a T ) under the concavity assumption about the expected average reward function. Although the OPG algorithm is natural and efficient for continuous problems, the concavity assumption may not be realistic in practice.\nThe aim of this paper is to develop a novel algorithm for solving online MDPs that is computationally efficient and performs well in problems with large state spaces. More specifically, we propose a policy iteration algorithm for online MDPs (OMDPPI), which has a close form update rule at each time step. We prove that our proposed algorithm achieves a sublinear regret with respect to a policy set. We further extend the proposed OMDP-PI algorithm with linear function approximation, which is essential for large (continuous) state space.\nThe remainder of this paper is organized as follows. In Section 2, we give the formal definition of online MDPs. In Section 3 we give the details of the proposed algorithm and analyze its regret. A generalization of the proposed algorithm with linear function approximation is also analyzed here. In Section 4, we present a discussion on solving online MDPs with stochastic iteration. In Section 5, we demonstrate the performance of the proposed algorithm in simulation experiments. In Section 6, we compare the related works with the proposed algorithm. Finally, in Section 7, we conclude the paper."}, {"heading": "2 Problem definition and preliminaries", "text": "In this section, we present the formal definition and involved preliminaries of the online MDP problem."}, {"heading": "2.1 Online Markov decision process", "text": "First, we formulate the problem of online MDP learning (Even-Dar et al., 2003, 2009) specified by {S,A, P, [rt]t=1,...,T}, where\n\u2022 S is the state space, and |S| is the cardinality of state space.\n\u2022 A is the action space, and |A| is the cardinality of action space.\n\u2022 P : S \u00d7 S \u00d7 A \u2192 [0, 1] is the transition probability, where p(s\u2032|s,a) gives the\nconditional probability of next state s\u2032 by taking action a at state s. We assume that the transition probability is available for the agent.\n\u2022 r1, . . . , rT is the reward function sequence, and only r1, . . . , rt are observed at\ntime step 1 \u2264 t \u2264 T .\nAt the end of each time step t = 1, . . . , T , trajectory ht is observed:\nht = {s1,a1, r1(s,a), . . . , st,at, rt(s,a)}.\nThe objective of an online MDP algorithm is to produce a strategy of choosing an action at time step t after observing ht. More specifically, let \u03c0t(a|s), \u2200s \u2208 S,a \u2208 A be a stochastic time-dependent policy, which is the conditional probability of action a to be taken at state s at time step t.\nAn online MDP algorithm A learns a time dependent policy that maximizes the\nexpected cumulative rewards:\nRA(T ) =\nT \u2211 t=1 E\u03c0t [rt(st,at)|A] ,\nwhere \u03c01, . . . , \u03c0T is the policy sequence generated by algorithm A and E\u03c0t [\u00b7|A] denotes the expectation over the joint state-action distribution pt(s,a|A) = p(st = s|A)\u03c0t(a|s) at time step t.\nHowever, given that no information is available about future reward functions, directly analyzing the expected cumulative rewards is not meaningful. Here, in the same way as standard online learning literature (Cesa-Bianchi and Lugosi, 2006), we consider the regret against the best offline time independent policy \u03c0\u2217 in the policy set \u03a0:\nLA(T ) = R\u03c0\u2217(T )\u2212RA(T ).\nMore precisely, R\u03c0\u2217(T ) is the return of \u03c0\u2217 the best offline time independent policy:\nR\u03c0\u2217(T ) = E\u03c0\u2217\n[\nT \u2211 t=1 rt(st,at)\n]\n= sup \u03c0\u2208\u03a0 E\u03c0\n[\nT \u2211 t=1 rt(st,at)\n]\n,\nwhere E\u03c0[\u00b7] denotes the expectation over the state-action joint distribution given policy \u03c0. Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set \u03a0. Namely instead of the best deterministic greedy policy, we consider a set of \u201cefficient\u201d policies, e.g., Gibbs policies with all possible parameters.\nWe expect that the regret LA(T ) is sublinear with respect to T , which means that the regret tends to zero as T tends to infinity and thus algorithm A performs as well as\nthe best offline policy \u03c0\u2217 asymptotically."}, {"heading": "2.2 Preliminaries", "text": "Next, we introduce some necessary notions for discussing online MDP problems. First, we show some criterion for evaluating the performance of any stochastic policy. For any policy \u03c0 \u2208 \u03a0, the expected average reward \u03c1(\u03c0) is defined as\n\u03c1r(\u03c0) = Es\u223cd\u03c0(s),a\u223c\u03c0 [r(s,a)]\n= \u2211\ns\u2208S\n\u2211 a\u2208A d\u03c0(s)\u03c0(a|s)r(s,a),\nwhere d\u03c0(s) is the stationary state distribution that satisfies\nd\u03c0(s \u2032) =\n\u2211 s\u2208S d\u03c0(s) \u2211 a\u2208A \u03c0(a|s)p(s\u2032|s,a).\nIt has been shown that every ergodic MDP has a unique stationary state distribution. In this paper, we assume that for all \u03c0 \u2208 \u03a0 the target MDP is ergodic.\nAnother way to evaluate the policy is to define the value function as\nV\u03c0r (s) = E\u03c0 [ \u221e \u2211\ni=1\n(r(si,ai)\u2212 \u03c1r(\u03c0))|s1 = s ] ,\nFor any arbitrary reward function r(s,a) and transition probability p(s\u2032|s,a), there exist at least one optimal policy \u03c0+ \u2208 \u03a0 such that\nV\u03c0+r (s) \u2265 V\u03c0r (s), \u2200\u03c0 \u2208 \u03a0, s \u2208 S,\n\u03c1r(\u03c0 +) \u2265 \u03c1r(\u03c0), \u2200\u03c0 \u2208 \u03a0.\nSimilarly, the state-action function is defined as\nQ\u03c0r (s,a) = E\u03c0\n[\n\u221e \u2211 i=1 (r(si,ai)\u2212 \u03c1r(\u03c0))|s1 = s,a1 = a ] .\nSince the optimal value function leads to the optimal policy, MDP is often solved by deriving the optimal value function (Sutton and Barto, 1998). So far, various efficient methods for approximating the optimal value function have been proposed. However, these algorithms were not proved to converge to the value function corresponding to the optimal deterministic policy. For this reason, in this paper we only consider the stochastic policy, since the convergence guarantee is provided (Tsitsiklis and Roy, 1999)."}, {"heading": "3 Online MDPs with policy iteration", "text": "In this section, we introduce the proposed method for online MDPs. The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the \u2018leader\u2019 policy. As Yu et al. (2009) pointed out, solving linear programming may not be appropriate for problems with large (continuous) state space. For this reason, we employ a policy iteration type method together with a stochastic policy in our proposed method."}, {"heading": "3.1 Algorithm", "text": "Firstly, we define the policy improvement operator \u0393 : \u03c0(a|s) = \u0393(r(s,a), V (s)), where r(s,a) is an arbitrary reward function, V (s) is an arbitrary value function. Below we use \u0393(r, V ) instead of \u0393(r(s,a), V (s)) for notational simplicity. Now we introduce two assumptions on the defined operator \u0393.\nAssumption 1. For an arbitrary reward function r and two arbitrary value functions\nV1(s) and V2(s), the policies \u03c01 = \u0393(r, V1) and \u03c02 = \u0393(r, V2) satisfy\n\u2016\u03c01(s, \u00b7)\u2212 \u03c02(s, \u00b7)\u20161 \u2264 \u03be\u2016V1(\u00b7)\u2212 V2(\u00b7)\u2016\u221e,\nwhere \u03be > 0 is the Lipschitz constant depending on the specific policy model. \u2016 \u00b7 \u20161 denotes the L1 norm, \u2016 \u00b7 \u2016\u221e denotes the infinity norm in this paper.\nAssumption 2. For an arbitrary value function V (s) and two arbitrary reward functions r(s,a) and r\u2032(s,a), the policies \u03c0 = \u0393(r, V ) and \u03c0\u2032 = \u0393(r\u2032, V ) satisfy\n\u2016\u03c0(s, \u00b7)\u2212 \u03c0\u2032(s, \u00b7)\u20161 \u2264\u03be\u2016r(s, \u00b7)\u2212 r\u2032(s, \u00b7)\u2016\u221e,\nThe Gibbs policy is a popular model which was demonstrated to work well:\n\u03c0(a|s) = exp 1 \u03ba\n(\nr(s,a) + \u2211 s\u2032\u2208S p(s \u2032|s,a)V (s\u2032)\n)\n) \u2211\na\u2032\u2208A exp 1 \u03ba\n(\nr(s,a\u2032) + \u2211 s\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032)\n) ,\nwhere \u03ba is the exploration parameter. We can show that the Gibbs policy satisfies Assumption 1 and Assumption 2 (the proofs are provided in Appendix A).\nThroughout this paper, we only consider stochastic policies that satisfy the above two assumptions. Let \u03a0 be the set of policies generated by the operator \u0393. Then our proposed online MDP with policy iteration (OMDP-PI) algorithm is given as follows:\n\u2022 Initialize the value function V0(s) = 0, \u2200s \u2208 S.\n\u2022 for t = 1, . . . ,\u221e\n1. Observe the current state st = s.\n2. Improve the policy as \u03c0t = \u0393(r\u0302t\u22121, Vt\u22121), where\nr\u0302t\u22121(s,a) = 1\nt\u2212 1\nt\u22121 \u2211 k=1 rk(s,a).\n3. Take action at = a by following \u03c0t.\n4. The reward function rt(s,a) is revealed.\n5. Update the value function according to\nVt(s) = (1\u2212 \u03b3t)Vt\u22121(s) + \u03b3tV\u03c0trt (s), (1)\nwhere the step size is \u03b3t = 1/t.\nIt is well known (Sutton and Barto, 1998) that the value function satisfies\nV\u03c0r (s) = E\u03c0 [ r(s,a)\u2212 \u03c1r(\u03c0) + \u2211\ns\u2032\u2208S\np(s\u2032|s,a)V\u03c0r (s\u2032) ] .\nThe above equation can be rewritten in matrix form as\nV\u03c0r = R(\u03c0)\u2212 e|S|\u03c1r(\u03c0) + P \u03c0V\u03c0r , (2)\nwhere V\u03c0r is the |S|-dimensional column vector whose sth element is V\u03c0r (s). R(\u03c0) is the |S|-dimensional column vector whose sth element is \u2211\na\u2208A \u03c0(a|s)r(s,a). P \u03c0\nis the transition matrix induced by the policy \u03c0, whose ss\u2032th element is p\u03c0(s|s\u2032) = \u2211\na\u2208A \u03c0(a|s)p(s\u2032|s,a). e|S| is the |S|-dimensional column vector with all ones. It is well known (Sutton and Barto, 1998) that the above equation has no unique solution. Here we introduce the following constraint on the value function:\nEs\u223cd\u03c0(s)[V\u03c0r (s)] = Es\u223cd\u03c0(s),a\u223c\u03c0 [ \u221e \u2211\ni=1\n(r(s,a)\u2212 \u03c1r(\u03c0)) ] = 0.\nBy this constraint, the solution of Equ.(2) becomes unique and satisfies\nV\u03c0r = R(\u03c0)\u2212 e|S|\u03c1r(\u03c0) + P \u03c0V\u03c0r \u2212 e|S|d\u22a4\u03c0 V\u03c0r , (3)\nwhere d\u03c0 is the |S|-dimensional column vector whose sth element is d\u03c0(s).\nThen the update rule (1) can be expressed in closed form as\nVt = (1\u2212 \u03b3t)Vt\u22121 + \u03b3t(I|S| \u2212 P \u03c0t + e|S|d\u22a4\u03c0t)\u22121(Rt(\u03c0t)\u2212 e|S|\u03c1rt(\u03c0t)).\nSince the stationary distribution can be obtain by the eigenvector corresponding to the unit eigenvalue, we can calculate \u03c1rt(\u03c0t) directly. Then, Vt(s) can be obtained directly without solving an optimization problem when the state space is not large (continuous). In the following sections, we will introduce an approximation method to handle large (continuous) state space problems."}, {"heading": "3.2 Regret analysis", "text": "In this section, we provide a regret analysis for the proposed OMDP-PI algorithm. Firstly, we introduce several essential assumptions involved in the proof. Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.\nAssumption 3. For all \u03c0 \u2208 \u03a0, there exist a positive constant \u03c4 such that two arbitrary state distributions d(s) and d\u2032(s) satisfy\n\u2211 s\u2208S \u2211 s\u2032\u2208S |d(s)\u2212 d\u2032(s)|p\u03c0(s\u2032|s) \u2264 e\u22121/\u03c4 \u2211 s\u2208S |d(s)\u2212 d\u2032(s)|.\nAssumption 4. The reward functions satisfy\nrt(s,a) \u2208 [0, 1], \u2200s \u2208 S, \u2200a \u2208 A, \u2200t = 1, . . . , T.\nUnder these assumptions, the regret of the OMDP-PI algorithm for a policy set \u03a0 is\nbounded as follows:\nTheorem 1. After T time steps, the regret against the best offline time independent policy of the OMDP-PI algorithm is bounded as\nLOMDP\u2212PI(T ) \u2264 2\u2212 e\u22121/\u03c4 1\u2212 e\u22121/\u03c4 C\u03beT Cv +\n(\n6\u03c4\u03be(2\u2212 e\u22121/\u03c4 ) 1\u2212 e\u22121/\u03c4 + 2\u03c4 3\n)\nlnT\n+\n(\n6\u03c4\u03be(2\u2212 e\u22121/\u03c4 ) 1\u2212 e\u22121/\u03c4 + 2\u03c4 3 + 2\u03c4 3e\u03c4+2 + 4\u03c4\n)\n,\nwhere C = 6\u03c4(2\u2212Cv+ 1Cv + 1\u2212Cv 1+Cv ), Cv = \u03beC\u03c0, and C\u03c0 is a positive constant such that for all \u03c01, \u03c02 \u2208 \u03a0,\n\u2016V\u03c01r \u2212 V\u03c02r \u2016\u221e \u2264 C\u03c0\u2016\u03c01 \u2212 \u03c02\u20161.\nThe existence of C\u03c0 is proved in Appendix E.\nRemark. The regret bound in Theorem 1 is sublinear with respect to T when Cv < 1. However, the quality of the policy is limited when Cv is small. Since the smaller the constant Cv is, the poorer the performance of the best offline policy is. In an extreme case, where all the policies in the set \u03a0 perform equally, when Cv = 0.\nTo prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014):\nLA(T ) =\n(\nE\u03c0\u2217\n[\nT \u2211 t=1 rt(st,at)\n]\n\u2212 T \u2211\nt=1\n\u03c1rt(\u03c0 \u2217)\n)\n+\n(\nT \u2211 t=1 \u03c1rt(\u03c0 \u2217)\u2212 T \u2211 t=1 \u03c1rt(\u03c0t)\n)\n+\n(\nT \u2211 t=1 \u03c1rt(\u03c0t)\u2212 E\u03c0t\n[\nT \u2211 t=1 rt(st,at)\n])\n.\nThe first term has been analyzed in previous works (Even-Dar et al., 2003, 2009; Ma et al., 2014), which is bounded as\nE\u03c0\u2217\n[\nT \u2211 t=1 rt(st,at)\n]\n\u2212 T \u2211\nt=1\n\u03c1rt(\u03c0 \u2217) \u2264 2\u03c4.\nBelow, we bound the second and the third terms in Lemma 2 and Lemma 3 which are proved in Appendix B and Appendix C.\nLemma 2. After T time steps, the policy sequence \u03c01, . . . , \u03c0T given by OMDP-PI and the best offline policy \u03c0\u2217 \u2208 \u03a0 satisfy\nT \u2211 t=1 \u03c1rt(\u03c0 \u2217)\u2212 T \u2211 t=1 \u03c1rt(\u03c0t) \u2264 2\u2212 e\u22121/\u03c4 1\u2212 e\u22121/\u03c4 ( C\u03beTCv + 6\u03c4\u03be lnT + 6\u03c4\u03be ) ,\nwhere C = 6\u03c4(2\u2212 Cv + 1Cv + 1\u2212Cv 1+Cv ).\nLemma 3. After T time steps, the policy sequence \u03c01, . . . , \u03c0T given by OMDP-PI satisfies\nT \u2211 t=1 \u03c1rt(\u03c0t)\u2212 E\u03c0t\n[\nT \u2211 t=1 rt(st,at)\n]\n\u2264 2\u03c4 3 lnT + 2\u03c4 3 + 2\u03c4 3e(\u03c4+2) + 2\u03c4.\nSummarizing these bounds, we can obtain Theorem 1."}, {"heading": "3.3 OMDP-PI algorithm with approximation", "text": "When considering large (continuous) state space in online MDP problems, it is essential to apply a function approximation technique. Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies. A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999). Below we present their theoretical results for discrete (possibly continuous) state space.\nBy following the same idea as Tsitsiklis and Roy (1999), we use the linear approxi-\nmation of the value function:\nV\u0302(s) = \u03b8\u22a4\u03c6(s),\nwhere \u03b8 \u2208 \u0398 is the approximation parameter, and \u0398 \u2282 RK is the parameter space, \u03c6(s) is the basis function. At each time step t, the value function V\u03c0trt (s) is approximated as follows:\n\u2022 for i = 1, 2, . . . until convergence\n1. Observe the state si.\n2. Take action ai following \u03c0t.\n3. Observe the next state si+1 and the reward rt(si,ai)\n4. Update the approximation parameter as\n\u03b8i+1 = \u03b8i + \u03b1t(rt(si,ai)\u2212 \u03c1\u0302\u03c0trt (i) + \u03b8\u22a4i \u03c6(si+1)\u2212 \u03b8\u22a4i \u03c6(si))\nand\n\u03c1\u0302\u03c0trt (i+ 1) = (1\u2212 \u03b1t)\u03c1\u0302\u03c0trt (i) + \u03b1trt(si,ai),\nwhere the step size \u03b1t satisfies\n\u221e \u2211 t=1 \u03b1t = \u221e and \u221e \u2211 t=1 \u03b12t < \u221e.\nThe approximation parameter was proved to converge to the unique solution of the following equation (Tsitsiklis and Roy, 1999):\nP(Rt(\u03c0t)\u2212 e|S|\u03c1rt(\u03c0t) + P \u03c0t\u03b8\u22a4\u03c6) = \u03b8\u22a4\u03c6, (4)\nwhere Rt(\u03c0t) is the |S|-dimensional column vector whose sth element is rt(s, \u03c0t) = \u2211\na\u2208A \u03c0t(a|s)rt(s,a). P is the projection operator such that for all V \u2208 R|S|,\nP(V ) = argmin V\u0304 \u2208{\u03b8\u22a4\u03c6|\u03b8\u2208RK} \u2016V \u2212 V\u0304 \u2016D\u03c0t ,\nwhere D\u03c0t is the diagonal matrix with the stationary distribution on the diagonal. It is clear that P is the projection from the |S|-dimensional real space to the space spanned by the basis function. The approximation sequence \u03c1\u0302\u03c0trt (1), \u03c1\u0302 \u03c0t rt (2), . . . satisfies\nlim i\u2192\u221e\n\u03c1\u0302\u03c0trt (i) \u2192 \u03c1rt(\u03c0t), with probability 1.\nFurthermore, by using Theorem 3 in Tsitsiklis and Roy (1999), the approximation error is bounded as\n\u2016(I|S| \u2212 e|S|d\u22a4\u03c0t)\u03b8\u2217\u22a4t \u03c6\u2212V\u03c0trt \u2016D\u03c0t \u2264 1\u221a\n1\u2212 e\u22122/\u03c4 inf \u03b8\u2208RK \u2016(I|S| \u2212 e|S|d\u22a4\u03c0t)\u03b8\u22a4\u03c6\u2212V\u03c0trt \u2016D\u03c0t ,\nwhere \u03b8\u2217t is the unique solution to Eq.(4) at time step t. We observe that the approximation error is zero when the linear approximation model is capable of exactly recovering the true value function."}, {"heading": "4 Online MDPs with stochastic iteration", "text": "In this section, we introduce a more general framework of our proposed method for online MDPs. More specifically, we extend our algorithm to use stochastic iteration (Bertsekas and Tsitsiklis, 1996) for policy evaluation together with policy improvement to solve online MDPs.\nA general form of the stochastic iteration algorithm (Szita et al., 2002; Csa\u0301ji and Monostori,\n2008) can be expressed as\nVt(s) = (1\u2212 \u03b3t(s))Vt\u22121(s) + \u03b3t(s) ((HtVt\u22121)(s) + wt(s)) , (5)\nwhere Vt \u2208 R|S|, Ht : R|S| \u2192 R|S|, \u2200t = 1, . . . , T is an operator on value functions, \u03b3t is the step size, and wt(s) is a noise term. Similarly to the Eq.(5), we define the update\nrule as\nVt(s) = (1\u2212 \u03b3t(s))Vt\u22121(s) + \u03b3t(s)((H\u03c0tt Vt\u22121)(s) + wt(s)), (6)\nwhere \u03c0t = \u0393(r\u0302t\u22121, Vt\u22121) satisfies Assumption 1 and Assumption 2. Note that the update rule (6) is different from standard stochastic iteration (5), where the operator Ht is replaced by the controlled operator H\u03c0t which the OMDP-PI algorithm uses: H\u03c0tt Vt\u22121(s) = V\u03c0trt (s). Additionally, we require the following assumptions.\nAssumption 5. The controlled operator H\u03c0t is a contraction mapping with respect to the value function. This means that, for two arbitrary value functions V and V \u2032 and two policies \u03c0 = \u0393(r, V ), \u03c0\u2032 = \u0393(r, V \u2032), there exist a no negative constant \u03b2t < 1 such that\n\u2016H\u03c0t V \u2212H\u03c0t V \u2032\u2016 \u2264 \u03b2t\u2016V \u2212 V \u2032\u2016,\nand there exist a fixed function V \u2217t satisfies\nHtV \u2217 t = V \u2217 t .\nAssumption 6. For all t = 1, . . . , T , the noisy terms wt(s) satisfy\nE[wt(s)] = 0 and E[w 2 t (s)] < Cw < \u221e,\nwhere Cw is a positive constant.\nAssumption 7. The step size \u03b3t satisfies\n\u221e \u2211 t=1 \u03b3t = \u221e and \u221e \u2211 t=1 \u03b32t < \u221e.\nAssumption 8. The value functions sequence V1(s), . . . , VT (s) generated by Eq.(6) satisfies\nlim T\u2192\u221e \u2016V \u2217T \u2212max \u03c0\u2208\u03a0 V\u03c0r\u0302T \u2016\u221e = 0.\nThen we have the following theorem:\nTheorem 4. If Assumptions 5-8 hold, the value function sequence V1(s), . . . , VT (s) generated by Eq.(6) satisfies\nlim T\u2192\u221e LA(T ) = 0.\nProof. By using Theorem 20 in Csa\u0301ji and Monostori (2008), we have\nlim T\u2192\u221e\n\u2016VT \u2212 V\u03c0 \u2217 r\u0302T \u2016\u221e = 0,\nwhere \u03c0\u2217 = argmax\u03c0\u2208\u03a0 \u03c1r\u0302T (\u03c0) is the best offline policy. Since \u03c0T+1 = \u0393(r\u0302T , VT ) and \u03c0T , we obtain\nlim T\u2192\u221e \u2016\u03c0T \u2212 \u03c0\u2217\u20161 \u2264 lim T\u2192\u221e (\u2016\u03c0T+1 \u2212 \u03c0\u2217\u20161 + \u2016\u03c0T+1 \u2212 \u03c0T\u20161)\n\u2264 lim T\u2192\u221e (\u03be\u2016VT \u2212 V\u03c0 \u2217 r\u0302T \u2016\u221e + \u2016\u03c0T+1 \u2212 \u03c0T\u20161) \u2264 lim T\u2192\u221e (\u03be\u2016VT \u2212 Vpi \u2217 r\u0302T \u2016\u221e + \u03be\u2016r\u0302T+1 \u2212 r\u0302T\u2016\u221e + \u03be\u2016VT \u2212 VT\u22121\u2016\u221e) = 0.\nIn the above derivation we used limT\u2192\u221e \u2016VT \u2212 VT\u22121\u2016\u221e = 0, which can be obtained by the update rule (6). The above result shows that the policies generated by the value sequence converges to the best offline policy as T goes to infinity. Hence, the claimed result hold by following the same line as the proof of Theorem 1.\nMany popular reinforcement learning algorithms based on value functions such as\nthe temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration. Theorem 4 shows that any stochastic iteration method that satisfies Assumptions 5-8 could be used to derive an online MDPs algorithm with sublinear regret."}, {"heading": "5 Experiments", "text": "In this section, we experimentally illustrate the behavior of the proposed online algorithm.\nThe goal of the grid world problem is to let an agent walk in the grid environment from the start block to the destination block. We conduct experiments on the grid world based on the Inverse Reinforcement Learning (IRL) toolkit1(Levine et al., 2011).\nIn our experiments, the grid world has 16 \u00d7 16 states with 2 actions in each state, which correspond to moving east and north. Each action has a 30% chance of moving in the other direction. The 256 states are further joined into 16 super-grids, each of which consists of 4\u00d7 4 states with the same reward.\nIn each episode, the agent tries to find a trajectory from the south-west corner to the north-east corner, with the highest cumulative rewards. In the north or east border states, the agent can only move east or north. We set T = 100000 and randomly change the rewards at episodes t = 1, 5001, 10001, ..., 95001. The proposed algorithm and the best offline algorithm (obtained using the standard MDP solver included in the IRL toolkit) are run on the grid world.\nFigure 1 shows the trajectories found by the offline policy and proposed OMDP-PI algorithm at episodes t = 25000, 50000, 75000, 100000. The darker the state is, the lower average reward it has. The direction of triangles shows the obtained policies. The states with red triangles indicate trajectories of the agent. Figure 2 shows the average regret and cumulative reward as functions of the number of episodes.\nThe results in Figure 2 show that the regret of the OMDP-PI algorithms vanishes,\n1http://graphics.stanford.edu/projects/gpirl\nsubstantiating that our theoretical analysis is valid."}, {"heading": "6 Comparison with previous works", "text": "\u2022 Expert algorithm based methods (Even-Dar et al., 2003, 2009; Neu et al., 2010a,\n2014): The basic idea of expert algorithm based methods is to put an expert algorithm in every state. By taking a close look at these algorithms, the idea does not take advantage of the state structure of the MDP problem. The OMDPPI algorithm can be easily combined with function approximation. Since it is popular to simplify the large state space problem by using the linear span of the state features, the OMDP-PI algorithm is natural to handle the large state space online MDPs.\n\u2022 Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014):\nBy introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems. The O( \u221a T ) regret bounds are proved for fixed time horizon online MDPs. More specifically, the step size parameter is optimized by using the length of the time horizon T . Moreover, the stationary occupancy measures are defined over finite state and action spaces, and thus it is not clear that whether the state-action probability density function could be learned by using their propose methods without parametrization. The OMDP-PI algorithm with function approximation parameterized the state-action density through the linear model of the value function.\n\u2022 Linear programming based method (Yu et al., 2009): Our OMDP-PI is motivated\nby the Lazy-FPL algorithm, which solves a linear programming problem at the end of each phase. Instead of obtaining the best policy by the linear programming, the OMDP-PI algorithm obtains the value function of the current policy which is much more efficient than the linear programming. As we showed in the update rule, the policy evaluation could be performed in O(|S|2.3728639 + |S|2|A|) where the matrix inversion could be solved in O(|S|2.3728639) (Le Gall, 2014)."}, {"heading": "7 Conclusion and future work", "text": "As a generalization of MDP, online MDP is a promising model which can handle many online problem with guaranteed performance. In this paper, we proposed a policy iteration algorithm with a closed form update rule for online MDP problems. We showed that the proposed algorithm achieves sublinear regret for a policy set. A notable fact is that the proposed algorithm is still practical for online MDP problems with large (continuous) state space. We showed that the propose algorithm can be easily combined with function approximation with theoretical guarantee. We illustrated the performance of the proposed algorithm through grid-world experiments.\nOur future work will extend the proposed algorithm to the bandit feedback scenario, where the full information of the reward function is not revealed to the agent. Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Csa\u0301ji and Monostori, 2008; Szita et al., 2002) is also an important future direction."}, {"heading": "A Proof of Gibbs policy", "text": "Proof. The KL divergence of two Gibbs policies \u03c0, \u03c0\u2032 generated by two different value function V and V \u2032 is\nD(\u03c0(\u00b7|s) \u2016 \u03c0\u2032(\u00b7|s))\n= E\u03c0\n[\n\u2211 s\u2032\u2208S\n1 \u03ba p(s\u2032|s,a) (V (s\u2032)\u2212 V \u2032(s\u2032))\n]\n+ log\n\u2211\na\u2032\u2208A exp 1 \u03ba\n((\nr(s,a\u2032) + \u2211 s\u2032\u2208S p(s \u2032|s,a\u2032)V \u2032(s\u2032)\n))\n\u2211\na\u2032\u2208A exp 1 \u03ba\n((\nr(s,a\u2032) + \u2211 s\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032)\n))\n= E\u03c0\n[\n1 \u03ba \u2211\ns\u2032\u2208S\np(s\u2032|s,a) (V (s\u2032)\u2212 V \u2032(s\u2032)) ]\n+ log\n\u2211\na\u2032\u2208A exp 1 \u03ba (r(s,a\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)(V \u2032(s\u2032)\u2212 V (s\u2032)))\n\u2211\na\u2032\u2208A exp 1 \u03ba (r(s,a\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032))\n= E\u03c0\n[\n1 \u03ba \u2211\ns\u2032\u2208S\np(s\u2032|s,a) (V (s\u2032)\u2212 V \u2032(s\u2032)) ]\n+ log\n\u2211\na\u2032\u2208A exp 1 \u03ba\n((\nr(s,a\u2032) + \u2211 s\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032)\n))\nexp 1 \u03ba\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)(V \u2032(s\u2032)\u2212 V (s\u2032))\n\u2211\na\u2032\u2208A exp 1 \u03ba\n((\nr(s,a\u2032) + \u2211 s\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032)\n))\n= E\u03c0\n[\n1 \u03ba \u2211\ns\u2032\u2208S\np(s\u2032|s,a) (V (s\u2032)\u2212 V \u2032(s\u2032)) ]\n+ logE\u03c0\n[\n1 \u03ba \u2211\ns\u2032\u2208S\np(s\u2032|s,a) (V \u2032(s\u2032)\u2212 V (s\u2032)) ]\n\u2264 \u2016 1 \u03ba\n\u2211\ns\u2032\u2208S p(s \u2032|s,a) (V (s\u2032)\u2212 V \u2032(s\u2032)) \u20162\u221e\n4 .\nFrom the Pinsker\u2019s inequality, there is\n\u2016\u03c0(\u00b7|s)\u2212 \u03c0\u2032(\u00b7|s)\u20161 \u2264 \u2016V (s)\u2212 V \u2032(s)\u2016\u221e\u221a\n2\u03ba ,\nSimilarly, the KL divergence of two Gibbs policies \u03c0, \u03c0\u2032 generated by two different reward function r(s,a) and r\u2032(s,a) is\nD(\u03c0(\u00b7|s)||\u03c0\u2032(\u00b7|s))\n= E\u03c0\n[\n1 k (r(s, \u00b7)\u2212 r\u2032(s, \u00b7))\n]\n+ log\n\u2211\na\u2032\u2208A exp 1 \u03ba (r\u2032(s,a\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032))\n\u2211\na\u2032\u2208A exp 1 \u03ba (r(s,a\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032))\n= E\u03c0\n[\n1 k (r(s, \u00b7)\u2212 r\u2032(s, \u00b7))\n]\n+ log\n\u2211\na\u2032\u2208A exp 1 \u03ba (r(s,a\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032) + r\u2032(s,a\u2032)\u2212 r(s,a\u2032))\n\u2211\na\u2032\u2208A exp 1 \u03ba (r(s,a\u2032) +\n\u2211\ns\u2032\u2208S p(s \u2032|s,a\u2032)V (s\u2032))\n= E\u03c0\n[\n1 \u03ba (r(s, \u00b7)\u2212 r\u2032(s, \u00b7))\n]\n+ logE\u03c0\n[\n1 \u03ba (r\u2032(s, \u00b7)\u2212 r(s, \u00b7))\n]\n\u2264 \u2016 1 \u03ba (r(s, \u00b7)\u2212 r\u2032(s, \u00b7))\u20162\u221e\n4 .\nFrom the Pinsker\u2019s inequality, we can conclude the proof as\n\u2016\u03c0(\u00b7|s)\u2212 \u03c0\u2032(\u00b7|s)\u20161 \u2264 \u2016r(s, \u00b7)\u2212 r\u2032(s, \u00b7)\u2016\u221e\u221a\n2\u03ba .\nwhich concludes the proof."}, {"heading": "B Proof of Lemma 2", "text": "Proposition 5. The value functions sequence V1(s), . . . , VT (s) generated by the Equ(1) satisfies\n\u2016V\u03c0 \u2217 t\nr\u0302t (\u00b7)\u2212 Vt(\u00b7)\u2016\u221e \u2264 CCv(t+ 1)Cv\u22121,\nwhere C = 6\u03c4(2\u2212 Cv + 1Cv + 1\u2212Cv 1+Cv ), and \u03c0\u2217t = argmax\u03c0\u2208\u03a0 \u03c1r\u0302t(\u03c0).\nBy Proposition 3 in Ma et al. (2014) and Proposition 4.1 in Yu et al. (2009), we\nobtain the following result\nT \u2211 t=1 (\u03c1rt(\u03c0 \u2217)\u2212 \u03c1rt(\u03c0t)) \u2264 T \u2211 t=1 (\u03c1rt(\u03c0 \u2217 t )\u2212 \u03c1rt(\u03c0t)) \u2264 T \u2211 t=1 2\u2212 e\u22121/\u03c4 1\u2212 e\u22121/\u03c4 \u2016\u03c0 \u2217 t \u2212 \u03c0t\u20161\nThe result in Proposition 5 leads the following inequalities\nT \u2211 t=1 (\u03c1rt(\u03c0 \u2217)\u2212 \u03c1rt(\u03c0t)) \u2264 T \u2211\nt=1\n2\u2212 e\u22121/\u03c4 1\u2212 e\u22121/\u03c4 (\u2016\u03c0 \u2217 t\u22121 \u2212 \u03c0t\u20161 + \u2016\u03c0\u2217t \u2212 \u03c0\u2217t\u22121\u20161)\n\u2264 T \u2211\nt=1\n2\u2212 e\u22121/\u03c4 1\u2212 e\u22121/\u03c4 \u03be(\u2016V \u03c0\u2217t\u22121 r\u0302t\u22121 \u2212 Vt\u22121\u2016\u221e + 4\u03c4 + 2 t )\n\u2264 2\u2212 e \u22121/\u03c4 1\u2212 e\u22121/\u03c4 ( C\u03be Cv TCv + 6\u03c4\u03be lnT + 6\u03c4\u03be ) ."}, {"heading": "C Proof of Lemma 3", "text": "The proof is following the same line as previous works(Even-Dar et al., 2003, 2009; Ma et al., 2014), we rewrite the proof with our notations. By the definition of the expected average reward function, we have\nT \u2211 t=1 \u03c1rt(\u03c0t)\u2212 E\u03c0t\n[\nT \u2211 t\u22121 rt(st,at)\n]\n= T \u2211\nt=1\n\u2211 s\u2208S \u2211 a\u2208A (d\u03c0t(s)\u03c0t(a|s)\u2212 dA,t(s)\u03c0t(a|s)) rt(s,a)\n\u2264 T \u2211\nt=1\n\u2211 s\u2208S |d\u03c0t(s)\u2212 dA,t(s)|,\nwhere dA,t(s) is the state distribution at time step t by following the policy sequence \u03c01, . . . , \u03c0t generated by the OMDP-PI algorithm. The last line can be obtain by rt(s,a) \u2208 [0, 1], \u2200t = 1, . . . , T .\nFor all k = 2, . . . , t, we have following results\n\u2016dA,k \u2212 d\u03c0t\u20161\n= \u2016dA,k\u22121P \u03c0k \u2212 d\u03c0t\u22121P \u03c0t\u20161 \u2264 \u2016dA,k\u22121P \u03c0k \u2212 dA,k\u22121P \u03c0t\u20161 + \u2016dA,k\u22121P \u03c0t \u2212 d\u03c0t\u22121P \u03c0t\u20161 \u2264 (ln (t\u2212 1)\u2212 ln (k \u2212 1)) + e\u22121/\u03c4\u2016dA,k\u22121 \u2212 d\u03c0t\u22121\u20161,\nRecurring the above result, we have\n\u2016dA,t \u2212 d\u03c0t\u20161 \u2264 t \u2211\nk=2\n(ln (t\u2212 1)\u2212 ln (k \u2212 1))e\u2212(t\u2212k)/\u03c4 + e\u2212t/\u03c4\u2016d1 \u2212 d\u03c0t\u20161\n\u2264 (1 + \u03c4) ( \u03c4 2\nt\u2212 1 + \u03c4e \u2212(t\u2212\u03c4\u22122)/\u03c4\n)\n+ 2e\u2212t/\u03c4 ,\nwhere the last inequality follows by\nt \u2211 k=2 (ln (t\u2212 1)\u2212 ln (k \u2212 1))e\u2212(t\u2212k)/\u03c4 = \u222b t\n2\n(ln (t\u2212 1)\u2212 ln (k \u2212 1))e\u2212(t\u2212k)/\u03c4dk + ln (t\u2212 1)e\u2212(t\u22122)/\u03c4\n= \u03c4\n\u222b t\n2\n(ln (t\u2212 1)\u2212 ln (k \u2212 1))de \u2212(t\u2212k)/\u03c4\ndk dk + ln (t\u2212 1)e\u2212(t\u22122)/\u03c4\n\u2264 \u03c4 \u222b t\n2\ne\u2212(t\u2212k)/\u03c4\nk \u2212 1 dk = \u03c4 2\n\u222b t\n2\n1 k \u2212 1 de\u2212(t\u2212k)/\u03c4 dk dk\n\u2264 \u03c4 2\nt\u2212 1 + \u03c4 2\n\u222b t\n2\ne\u2212(t\u2212k)/\u03c4 (k \u2212 1)2 dk\n= \u03c4 2\nt\u2212 1 + \u03c4 2\n\u222b t\n\u03c4+2\ne\u2212(t\u2212k)/\u03c4 (k \u2212 1)2 dk + \u222b \u03c4+2\n2\ne\u2212(t\u2212k)/\u03c4 (k \u2212 1)2 dk\n\u2264 \u03c4 2 t\u2212 1 + \u03c4 2 \u03c4 + 1 \u222b t\n2\ne\u2212(t\u2212k)/\u03c4 k \u2212 1 dk + \u222b \u03c4+2\n2\ne\u2212(t\u2212k)/\u03c4 (k \u2212 1)2 dk.\nHence, we have\n\u222b t\n2\ne\u2212(t\u2212k)/\u03c4 k \u2212 1 dk \u2264 ( 1 + 1 \u03c4 )( \u03c4 2 t\u2212 1 + \u03c4e \u2212(t\u2212\u03c4\u22122)/\u03c4 ) .\nThe claimed result in Lemma 3 can be obtained as\nT \u2211 t=1 \u2016dA,t \u2212 d\u03c0t\u20161 \u2264 2\u03c4 3 lnT + 2\u03c4 3 + 2\u03c4 3e(\u03c4+2) + 2\u03c4."}, {"heading": "D Proof of Proposition 5", "text": "Proposition 6. For arbitrary reward function r(s,a), the corresponding value functions induced by two arbitrary policy \u03c01 and \u03c02 satisfy\n\u2016V\u03c01r \u2212 V\u03c02r \u2016\u221e \u2264 C\u03c0\u2016\u03c01 \u2212 \u03c02\u20161.\nwhere C\u03c0 is a positive constant.\nLet us define an auxiliary sequence of functions V\u03c0 \u2217 t\nr\u0302t (s), t = 1, . . . , T which is\ndefined as\nV\u03c0 \u2217 t\nr\u0302t (s) = E\u03c0\u2217t\n[\n\u221e \u2211 i=1 (r\u0302t(s,a)\u2212 \u03c1r\u0302t(\u03c0\u2217t )) ] .\nIn above definition, \u03c0\u2217t is the optimal policy which satisfies\n\u03c0\u2217t = argmax \u03c0\u2208\u03a0 \u03c1r\u0302t(\u03c0),\nand for all s \u2208 S, there is\nV\u03c0 \u2217 t\nr\u0302t (s) \u2265 V\u03c0r\u0302t(s), \u2200\u03c0 \u2208 \u03a0.\nIt is simple to verify that the value function is linear with respect to the reward function,\ni.e., V\u03c0r\u0302t(s) = 1t \u2211t k=1 V\u03c0rt(s). Hence, we can rewrite the sequence as\nV\u03c0 \u2217 t+1 r\u0302t+1 (s) =V\u03c0 \u2217 t r\u0302t (s) +\n1\nt + 1\n(\nt+1 \u2211 k=1 V\u03c0 \u2217 t+1 rk (s)\u2212 t+ 1 t t \u2211 k=1 V\u03c0\u2217trk (s) )\n=V\u03c0 \u2217 t\nr\u0302t (s) +\n1\nt + 1 V\u03c0\n\u2217\nt+1 rt+1 (s)\u2212\n1\nt(t+ 1)\nt \u2211 k=1 V\u03c0\u2217trk (s) + 1 t + 1\n(\nt \u2211 k=1 V\u03c0 \u2217 t+1 rk (s)\u2212 t \u2211 k=1 V\u03c0\u2217trk (s) )\n=(1\u2212 1 t+ 1 )V\u03c0 \u2217 t r\u0302t (s) + 1 t+ 1 V\u03c0 \u2217 t+1 rt+1 (s) + t t+ 1 ( V\u03c0 \u2217 t+1 r\u0302t (s)\u2212 V\u03c0 \u2217 t r\u0302t (s) ) \u2264(1\u2212 1 t+ 1 )V\u03c0 \u2217 t r\u0302t (s) + 1 t+ 1 V\u03c0 \u2217 t+1 rt+1 (s),\nwhere the last inequality can be obtained by the fact that \u03c0\u2217t is the optimal policy satisfies\nV\u03c0 \u2217 t r\u0302t (s) \u2265 V\u03c0\n\u2217 t+1\nr\u0302t (s), \u2200s \u2208 S.\nOn the other hand, we can derive the lower bound as\nV\u03c0 \u2217 t+1 r\u0302t+1 (s) =V\u03c0 \u2217 t r\u0302t (s) +\n1\nt + 1\n(\nt+1 \u2211 k=1 V\u03c0 \u2217 t+1 rk (s)\u2212 t+ 1 t t \u2211 k=1 V\u03c0\u2217trk (s) )\n=V\u03c0 \u2217 t\nr\u0302t (s) +\n1\nt + 1\n(\nt+1 \u2211 k=1 V\u03c0 \u2217 t+1 rk (s)\u2212 t+ 1 t t+1 \u2211 k=1 V\u03c0\u2217trk (s) ) + 1 t V\u03c0\u2217trt+1(s)\n=V\u03c0 \u2217 t\nr\u0302t (s) +\n1\nt + 1\n(\nt+1 \u2211 k=1 V\u03c0 \u2217 t+1 rk (s)\u2212 t+1 \u2211 k=1 V\u03c0\u2217trk (s) ) \u2212 1 t(t + 1) t+1 \u2211 k=1 V\u03c0\u2217trk (s) + 1 t V\u03c0\u2217trt+1(s)\n=V\u03c0 \u2217 t r\u0302t (s) + (V\u03c0\n\u2217 t+1\nr\u0302t+1 (s)\u2212 V\u03c0 \u2217 t r\u0302t+1 (s))\u2212 1 t + 1 V\u03c0 \u2217 t r\u0302t (s) + 1 t + 1 V\u03c0\u2217trt+1(s)\n\u2265(1\u2212 1 t+ 1 )V\u03c0 \u2217 t r\u0302t (s) + 1 t+ 1 V\u03c0\u2217trt+1(s),\nwhere the last inequality comes from the fact \u03c0\u2217t+1 is the optimal policy satisfies\nV\u03c0 \u2217 t+1 r\u0302t+1 (s) \u2265 V\u03c0 \u2217 t r\u0302t+1 (s), \u2200s \u2208 S.\nThen, we can obtain the following result\n|V\u03c0 \u2217 t+1\nr\u0302t+1 (s)\u2212 Vt+1(s)| \u2264 (1\u2212\n1\nt+ 1 )|V\u03c0 \u2217 t r\u0302t (s)\u2212 Vt(s)|+\n1\nt+ 1 \u2206t+1. (7)\nIn above inequality, \u2206t+1 = max{|V \u03c0\u2217t+1 rt+1 (s)\u2212V \u03c0t+1rt+1 (s)|, |V \u03c0\u2217t rt+1(s)\u2212V \u03c0t+1rt+1 (s)|}, which satisfies\n\u2206t+1 \u2264C\u03c0 max{\u2016\u03c0\u2217t+1 \u2212 \u03c0t+1\u20161, \u2016\u03c0\u2217t \u2212 \u03c0t+1\u20161}\n\u2264C\u03c0(\u2016\u03c0\u2217t \u2212 \u03c0t+1\u20161 + \u2016\u03c0\u2217t \u2212 \u03c0\u2217t+1\u20161) \u2264Cv\u2016V\u03c0 \u2217 t r\u0302t \u2212 Vt\u2016\u221e + (4\u03c4 + 2)Cv t+ 1 .\nThe first term of the last inequality can be obtain by setting Cv = \u03beC\u03c0. The second part follows by the upper bound and the lower bound of V\u03c0 \u2217 t+1\nr\u0302t . Next we show the bound of\n\u2016V\u03c0 \u2217 t\nr\u0302t \u2212 Vt\u2016\u221e by recurring Equ.(7)\n\u2016V\u03c0 \u2217 t r\u0302t \u2212 Vt\u2016\u221e \u2264 (4\u03c4 + 2)Cv t2\n+ t\u22121 \u2211\nk=1\n(4\u03c4 + 2)Cv k2\nt \u220f\nm=k+1\n(\n1\u2212 1\u2212 Cv m\n)\n.\nLet us take the logarithm of \u220ft\nm=k+1\n(\n1\u2212 1\u2212Cv m\n)\n, there is\nln t \u220f\nm=k+1\n(\n1\u2212 1\u2212 Cv m\n)\n= t \u2211\nm=k+1\n(ln (m\u2212 1 + Cv)\u2212 lnm)\n\u2264 t \u2211\nm=k+1\n\u22121 + Cv m\n\u2264 \u2212(1\u2212 Cv) \u222b t+1\nk+1\n1\nm dm = \u2212(1\u2212 Cv) ln\nt + 1 k + 1 ,\nwhere the first inequality holds since the logarithm function is concave. Thus we derive\nthe bound as\n\u2016V\u03c0 \u2217 t\nr\u0302t \u2212 Vt\u2016\u221e \u2264(4\u03c4 + 2)Cv\nt \u2211 k=1 1 k2 (k + 1)1\u2212Cv (t+ 1)1\u2212Cv\n\u2264 (4\u03c4 + 2)Cv (t+ 1)1\u2212Cv\nt \u2211 k=1 1 k2 ( k1\u2212Cv + (1\u2212 Cv)k\u2212Cv )\n\u2264 (4\u03c4 + 2)Cv (t+ 1)1\u2212Cv\n[\n2\u2212 Cv + \u222b t\n1\nk\u2212Cv\u22121dk + (1\u2212 Cv) \u222b t\n1\nk\u2212Cv\u22122dk\n]\n= (4\u03c4 + 2)Cv (t+ 1)1\u2212Cv\n[\n2\u2212 Cv + 1 Cv \u2212 t\n\u2212Cv Cv + 1\u2212 Cv 1 + Cv \u2212 1\u2212 Cv 1 + Cv t\u2212Cv\u22121 ]\n\u2264CCv(t+ 1)Cv\u22121,\nwhere C = 6\u03c4(2\u2212Cv + 1Cv + 1\u2212Cv 1+Cv ). In above results, the second inequality follows by Taylor\u2019s theorem."}, {"heading": "E Proof of Proposition 6", "text": "Let us define the operator T \u03c0V\u03c0r (s) = E\u03c0 [ r(s,a)\u2212 \u03c1r(\u03c0) + \u2211 s\u2032\u2208S p(s \u2032|s,a)V\u03c0r (s\u2032) ] . we can obtain\nV\u03c01r (s)\u2212 V\u03c02r (s)\n= T \u03c01V\u03c01r (s)\u2212 T \u03c02V\u03c02r (s) = (T \u03c01V\u03c01r (s)\u2212 T \u03c02V\u03c01r (s)) + (T \u03c02V\u03c01r (s)\u2212 T \u03c02V\u03c02r (s)) .\nBy the definition of the operator, we rewrite the first term as\nT \u03c01V\u03c01r (s)\u2212 T \u03c02V\u03c01r (s)\n= E\u03c01\n[\nr(s,a)\u2212 \u03c1r(\u03c01) + \u2211\ns\u2032\u2208S\np(s\u2032|s,a)V\u03c01r (s\u2032) ]\n\u2212 E\u03c02\n[\nr(s,a)\u2212 \u03c1r(\u03c02) + \u2211\ns\u2032\u2208S\np(s\u2032|s,a)V\u03c01r (s\u2032) ]\n= (E\u03c01 [Q \u03c01 r (s,a)]\u2212 E\u03c02 [Q\u03c01r (s,a)]) + (\u03c1r(\u03c02)\u2212 \u03c1r(\u03c01)) = (Q\u03c01r (s, \u03c01)\u2212Q\u03c01r (s, \u03c02)) + Es\u223cd\u03c02 (s)[Q \u03c01 r (s, \u03c02)\u2212Q\u03c01r (s, \u03c01)].\nThe second term can be expressed as\nT \u03c02V\u03c01r (s)\u2212 T \u03c02V\u03c02r (s)\nE\u03c02\n[\nr(s,a)\u2212 \u03c1r(\u03c02) + \u2211\ns\u2032\u2208S\np(s\u2032|s,a)V\u03c01r (s\u2032)\u2212 r(s,a) + \u03c1r(\u03c02)\u2212 \u2211\ns\u2032\u2208S\np(s\u2032|s,a)V\u03c02r (s) ]\n= Es\u2032\u223cp\u03c02 (s\u2032|s)[V\u03c01r (s\u2032)\u2212 V\u03c02r (s\u2032)].\nBy summing up the above results, we obtain\nV\u03c01r (s)\u2212 V\u03c02r (s)\n= (Q\u03c01r (s, \u03c01)\u2212Q\u03c01r (s, \u03c02)) + Es\u223cd\u03c02 (s)[Q \u03c01 r (s, \u03c02)\u2212Q\u03c01r (s, \u03c01)]\n+ Es\u2032\u223cp\u03c02(s\u2032|s)[V\u03c01r (s\u2032)\u2212 V\u03c02r (s\u2032)].\nIn matrix notation, there is\nV\u03c01r \u2212 V\u03c02r = (Q\u03c01,\u03c01r \u2212Q\u03c01,\u03c02r )\u2212 e|S|d\u22a4\u03c02(Q \u03c01,\u03c01 r \u2212Q\u03c01,\u03c02r ) + P \u03c02(V\u03c01r \u2212 V\u03c02r ),\nwhere V\u03c0r and Q\u03c0,\u03c0 \u2032 r are the length |S| vectors whose sth element isV\u03c0r (s) and Q\u03c0r (s, \u03c0\u2032), respectively. e|S| denotes the length |S| vector with all elements equal to 1. d\u03c0 is the\n|S|-dimensional vector whose sth element is d\u03c0(s). P \u03c0 is defined as the transition matrix induced by the policy \u03c0 and the transition p(s\u2032|s,a). Thus, we obtain\n(I|S| \u2212 P \u03c02)(V\u03c01r \u2212 V\u03c02r ) = (I|S| \u2212 e|S|d\u22a4\u03c02)(Q \u03c01,\u03c01 r \u2212Q\u03c01,\u03c02r ).\nIt is known that the Bellman equation with average reward function has no unique solution. However, the unique value function satisfies d\u22a4\u03c0 V\u03c0r = 0. Hence, we add this condition to the above equation as\n(I|S| \u2212 P \u03c02)(V\u03c01r \u2212 V\u03c02r )\n= (I|S| \u2212 e|S|d\u22a4\u03c02)(Q \u03c01,\u03c01 r \u2212Q\u03c01,\u03c02r )\u2212 e|S|d\u22a4\u03c01V \u03c01 r + e|S|d \u22a4 \u03c02 V\u03c02r = (I|S| \u2212 e|S|d\u22a4\u03c02)(Q \u03c01,\u03c01 r \u2212Q\u03c01,\u03c02r )\u2212 e|S|d\u22a4\u03c02(V \u03c01 r \u2212 V\u03c02r )\u2212 (e|S|d\u22a4\u03c01 \u2212 e|S|d \u22a4 \u03c02)V \u03c01 r\nThen, by rearranging the above result:\n(I|S| \u2212 P \u03c02 + e|S|d\u22a4\u03c02)(V \u03c01 r \u2212 V\u03c02r ) = (Q\u03c01,\u03c01r \u2212Q\u03c01,\u03c02r )\u2212 (P \u03c01sa \u2212 P \u03c02sa )Q\u03c01r ,\nwhere P \u03c0sa is the |S| \u00d7 |A| matrix whose (s,a)th element is d\u03c0(s)\u03c0(a|s). Using Proposition 12 in Ma et al. (2014), we obtain\n\u2016V\u03c01r \u2212 V\u03c02r \u2016\u221e \u2264 2\u2212 2e\u22121/\u03c4 1\u2212 e\u22121/\u03c4 \u2016(I|S| \u2212 P \u03c02 + e|S|d \u22a4 \u03c02 )\u22121Q\u03c01r \u2016\u221e\u2016\u03c01 \u2212 \u03c02\u20161,\nwhich concludes the proof by setting max\u03c0\u2208\u03a0 2\u22122e \u22121/\u03c4 1\u2212e\u22121/\u03c4 \u2016(I|S|\u2212P \u03c0 +e|S|d\u22a4\u03c0 )\u22121Q\u03c0r \u2016\u221e \u2264 C\u03c0."}], "references": [{"title": "Online learning in Markov decision processes with adversarially chosen transition probability distributions", "author": ["Y. Abbasi-Yadkori", "P. Bartlett", "V. Kanade", "Y. Seldin", "C. Szepesvari"], "venue": "Advances in Neural Information Processing Systems 26, pages 2508\u20132516.", "citeRegEx": "Abbasi.Yadkori et al\\.,? 2013", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2013}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific, 1st edition.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press, New York, NY, USA.", "citeRegEx": "Cesa.Bianchi and Lugosi,? 2006", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Value function based reinforcement learning in changing Markovian environments", "author": ["B.C. Cs\u00e1ji", "L. Monostori"], "venue": "Journal of Machine Learning Research, 9:1679\u2013 1709.", "citeRegEx": "Cs\u00e1ji and Monostori,? 2008", "shortCiteRegEx": "Cs\u00e1ji and Monostori", "year": 2008}, {"title": "Online learning in Markov decision processes with changing cost sequences", "author": ["T. Dick", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 512\u2013520.", "citeRegEx": "Dick et al\\.,? 2014", "shortCiteRegEx": "Dick et al\\.", "year": 2014}, {"title": "Experts in a Markov decision process", "author": ["E. Even-Dar", "S.M. Kakade", "Y. Mansour"], "venue": "Advances in Neural Information Processing System, pages 401\u2013408.", "citeRegEx": "Even.Dar et al\\.,? 2003", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2003}, {"title": "Online Markov decision processes", "author": ["E. Even-Dar", "S.M. Kakade", "Y. Mansour"], "venue": "Mathematics of Operations Research, 34(3):726\u2013736. 21", "citeRegEx": "Even.Dar et al\\.,? 2009", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2009}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["F. Le Gall"], "venue": "Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation, ISSAC \u201914, pages 296\u2013303, New York, NY, USA. ACM.", "citeRegEx": "Gall,? 2014", "shortCiteRegEx": "Gall", "year": 2014}, {"title": "Nonlinear inverse reinforcement learning with Gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "Advances in Neural Information Processing Systems 24, pages 19\u201327.", "citeRegEx": "Levine et al\\.,? 2011", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "An online policy gradient algorithm for Markov decision processes with continuous states and actions", "author": ["Y. Ma", "T. Zhao", "K. Hatano", "M. Sugiyama"], "venue": "Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II, pages 354\u2013369.", "citeRegEx": "Ma et al\\.,? 2014", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "G. Andr\u00e1s", "C. Szepesv\u00e1ri", "A. Antos"], "venue": "IEEE Transactions on Automatic Control, 59(3):676\u2013 691.", "citeRegEx": "Neu et al\\.,? 2014", "shortCiteRegEx": "Neu et al\\.", "year": 2014}, {"title": "The online loop-free stochastic shortest-path problem", "author": ["G. Neu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 231\u2013243.", "citeRegEx": "Neu et al\\.,? 2010a", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "The adversarial stochastic shortest path problem with unknown transition probabilities", "author": ["G. Neu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12), volume 22, pages 805\u2013813. 22", "citeRegEx": "Neu et al\\.,? 2012", "shortCiteRegEx": "Neu et al\\.", "year": 2012}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "A. Gyrgy", "C. Szepesvri", "A. Antos"], "venue": "Advances in Neural Information Processing System,NIPS, pages 1804\u20131812.", "citeRegEx": "Neu et al\\.,? 2010b", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning, pages 9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "MDPs: Learning in varying environments", "author": ["I. Szita", "B. Tak\u00e1cs", "A. L\u00f6rincz"], "venue": "Journal of Machine Learning Research, 3:145\u2013174.", "citeRegEx": "Szita et al\\.,? 2002", "shortCiteRegEx": "Szita et al\\.", "year": 2002}, {"title": "Average cost temporal-difference learning", "author": ["J.N. Tsitsiklis", "B.V. Roy"], "venue": "Automatica, 35:1799\u20131808.", "citeRegEx": "Tsitsiklis and Roy,? 1999", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1999}, {"title": "Markov decision processes with arbitrary reward processes", "author": ["J.Y. Yu", "S. Mannor", "N. Shimkin"], "venue": "Mathematics of Operations Research, 34(3):737\u2013757.", "citeRegEx": "Yu et al\\.,? 2009", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Online learning in episodic Markovian decision processes by relative entropy policy search", "author": ["A. Zimin", "G. Neu"], "venue": "Advances in Neural Information Processing Systems 26, pages 1583\u20131591. 23", "citeRegEx": "Zimin and Neu,? 2013", "shortCiteRegEx": "Zimin and Neu", "year": 2013}, {"title": "Proof of Lemma 3 The proof is following the same line as previous works(Even-Dar", "author": [], "venue": "Ma et al.,", "citeRegEx": "C,? \\Q2003\\E", "shortCiteRegEx": "C", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes.", "startOffset": 132, "endOffset": 164}, {"referenceID": 1, "context": "1 Introduction A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes. A standard way to solve the stochastic shortest path problem is to formulate it as a Markov decision process (MDP) and find a policy that maximizes the cumulative reward over the path. In the MDP problem, the agent chooses the best action according to the current state and moves to the next state following the Markovian dynamics. A fixed reward function assigns a reward value to each state-action pair. A generalization of MDP, called the online MDP, considers the situation where the reward function changes over time. At each time step, the learning agent decides the strategy of choosing actions by using the knowledge of past reward functions. Then, the current reward function which is chosen by the environment is revealed to the agent after observing its behavior. The goal of online MDP is to minimize the regret against the best offline policy, which is the optimal fixed policy in hindsight. We expect that the regret vanishes as the time step T tends to infinity, implying that the agent can behave as well as the best offline policy asymptotically. Many online problems can be solved as online MDP problems. By setting the optimization variables as the state, the online MDP algorithm chooses the change of variables (action) which performs reasonably well in a non-stationary environment. Even-Dar et al. (2009) presented several typical online problems which can be formulated as online MDP perfectly, e.", "startOffset": 133, "endOffset": 1561}, {"referenceID": 11, "context": "Furthermore, the MDP-E algorithm was proved to achieve regret O(L \u221a T log |A|) for online MDP problems with L-layered state space (Neu et al., 2010a).", "startOffset": 130, "endOffset": 149}, {"referenceID": 18, "context": "Another online MDP algorithm called the lazy follow-the-perturbed-leader (lazyFPL) (Yu et al., 2009) follows the main idea of the FPL algorithm which solves the Bellman equation using the average reward function.", "startOffset": 83, "endOffset": 100}, {"referenceID": 19, "context": "Similarly to lazy-FPL, the online relative entropy policy search (O-REPS) algorithm (Zimin and Neu, 2013) also requires to solve an optimization problem at the end of each time step.", "startOffset": 84, "endOffset": 105}, {"referenceID": 4, "context": "By introducing the stationary occupation measure, Dick et al. (2014) proposed the mirror descent with approximation projections algorithm, which formulate the online MDP problem as online linear opti3", "startOffset": 50, "endOffset": 69}, {"referenceID": 12, "context": "Yu et al. (2009), Abbasi-Yadkori et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "(2009), Abbasi-Yadkori et al. (2013), and Neu et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 0, "context": "(2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics.", "startOffset": 8, "endOffset": 60}, {"referenceID": 0, "context": "(2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics. Recently, Ma et al. (2014) proposed the online policy gradient (OPG) algorithm for online MDP problems with continuous state and action spaces, and it was proved to achieve regret O( \u221a T ) under the concavity assumption about the expected average reward function.", "startOffset": 8, "endOffset": 187}, {"referenceID": 2, "context": "Here, in the same way as standard online learning literature (Cesa-Bianchi and Lugosi, 2006), we consider the regret against the best offline time independent policy \u03c0 in the policy set \u03a0: LA(T ) = R\u03c0\u2217(T )\u2212RA(T ).", "startOffset": 61, "endOffset": 92}, {"referenceID": 19, "context": "Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set \u03a0.", "startOffset": 76, "endOffset": 145}, {"referenceID": 4, "context": "Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set \u03a0.", "startOffset": 76, "endOffset": 145}, {"referenceID": 15, "context": "Since the optimal value function leads to the optimal policy, MDP is often solved by deriving the optimal value function (Sutton and Barto, 1998).", "startOffset": 121, "endOffset": 145}, {"referenceID": 17, "context": "For this reason, in this paper we only consider the stochastic policy, since the convergence guarantee is provided (Tsitsiklis and Roy, 1999).", "startOffset": 115, "endOffset": 141}, {"referenceID": 18, "context": "The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the \u2018leader\u2019 policy.", "startOffset": 81, "endOffset": 98}, {"referenceID": 18, "context": "The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the \u2018leader\u2019 policy. As Yu et al. (2009) pointed out, solving linear programming may not be appropriate for problems with large (continuous) state space.", "startOffset": 81, "endOffset": 184}, {"referenceID": 15, "context": "It is well known (Sutton and Barto, 1998) that the value function satisfies V r (s) = E\u03c0 [", "startOffset": 17, "endOffset": 41}, {"referenceID": 15, "context": "It is well known (Sutton and Barto, 1998) that the above equation has no unique solution.", "startOffset": 17, "endOffset": 41}, {"referenceID": 18, "context": "Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.", "startOffset": 32, "endOffset": 120}, {"referenceID": 9, "context": "Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.", "startOffset": 32, "endOffset": 120}, {"referenceID": 0, "context": "To prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014): LA(T ) = (", "startOffset": 117, "endOffset": 192}, {"referenceID": 9, "context": "To prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014): LA(T ) = (", "startOffset": 117, "endOffset": 192}, {"referenceID": 9, "context": "The first term has been analyzed in previous works (Even-Dar et al., 2003, 2009; Ma et al., 2014), which is bounded as E\u03c0\u2217 [ T \u2211", "startOffset": 51, "endOffset": 97}, {"referenceID": 17, "context": "A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999).", "startOffset": 96, "endOffset": 122}, {"referenceID": 17, "context": "Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies.", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies. A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999). Below we present their theoretical results for discrete (possibly continuous) state space. By following the same idea as Tsitsiklis and Roy (1999), we use the linear approximation of the value function: V\u0302(s) = \u03b8\u03c6(s), 13", "startOffset": 0, "endOffset": 389}, {"referenceID": 17, "context": "The approximation parameter was proved to converge to the unique solution of the following equation (Tsitsiklis and Roy, 1999): P(Rt(\u03c0t)\u2212 e|S|\u03c1rt(\u03c0t) + P t\u03b8\u03c6) = \u03b8\u03c6, (4) where Rt(\u03c0t) is the |S|-dimensional column vector whose sth element is rt(s, \u03c0t) =", "startOffset": 100, "endOffset": 126}, {"referenceID": 17, "context": "Furthermore, by using Theorem 3 in Tsitsiklis and Roy (1999), the approximation error is bounded as \u2016(I|S| \u2212 e|S|d\u22a4\u03c0t)\u03b8\u2217\u22a4 t \u03c6\u2212Vt rt \u2016D\u03c0t \u2264 1 \u221a 1\u2212 e\u22122/\u03c4 inf \u03b8\u2208RK \u2016(I|S| \u2212 e|S|d\u22a4\u03c0t)\u03b8\u22a4\u03c6\u2212V\u03c0 rt \u2016D\u03c0t , where \u03b8 t is the unique solution to Eq.", "startOffset": 35, "endOffset": 61}, {"referenceID": 1, "context": "More specifically, we extend our algorithm to use stochastic iteration (Bertsekas and Tsitsiklis, 1996) for policy evaluation together with policy improvement to solve online MDPs.", "startOffset": 71, "endOffset": 103}, {"referenceID": 16, "context": "A general form of the stochastic iteration algorithm (Szita et al., 2002; Cs\u00e1ji and Monostori, 2008) can be expressed as Vt(s) = (1\u2212 \u03b3t(s))Vt\u22121(s) + \u03b3t(s) ((HtVt\u22121)(s) + wt(s)) , (5) where Vt \u2208 R, Ht : R \u2192 R, \u2200t = 1, .", "startOffset": 53, "endOffset": 100}, {"referenceID": 3, "context": "A general form of the stochastic iteration algorithm (Szita et al., 2002; Cs\u00e1ji and Monostori, 2008) can be expressed as Vt(s) = (1\u2212 \u03b3t(s))Vt\u22121(s) + \u03b3t(s) ((HtVt\u22121)(s) + wt(s)) , (5) where Vt \u2208 R, Ht : R \u2192 R, \u2200t = 1, .", "startOffset": 53, "endOffset": 100}, {"referenceID": 1, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 128, "endOffset": 198}, {"referenceID": 15, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 128, "endOffset": 198}, {"referenceID": 14, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 128, "endOffset": 198}, {"referenceID": 1, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 223, "endOffset": 279}, {"referenceID": 15, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 223, "endOffset": 279}, {"referenceID": 2, "context": "By using Theorem 20 in Cs\u00e1ji and Monostori (2008), we have lim T\u2192\u221e \u2016VT \u2212 V \u2217 r\u0302T \u2016\u221e = 0, where \u03c0 = argmax\u03c0\u2208\u03a0 \u03c1r\u0302T (\u03c0) is the best offline policy.", "startOffset": 23, "endOffset": 50}, {"referenceID": 8, "context": "We conduct experiments on the grid world based on the Inverse Reinforcement Learning (IRL) toolkit1(Levine et al., 2011).", "startOffset": 99, "endOffset": 120}, {"referenceID": 19, "context": "\u2022 Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014): By introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems.", "startOffset": 43, "endOffset": 83}, {"referenceID": 4, "context": "\u2022 Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014): By introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems.", "startOffset": 43, "endOffset": 83}, {"referenceID": 18, "context": "\u2022 Linear programming based method (Yu et al., 2009): Our OMDP-PI is motivated 19", "startOffset": 34, "endOffset": 51}, {"referenceID": 1, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al.", "startOffset": 73, "endOffset": 129}, {"referenceID": 15, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al.", "startOffset": 73, "endOffset": 129}, {"referenceID": 3, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al., 2002) is also an important future direction.", "startOffset": 164, "endOffset": 211}, {"referenceID": 16, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al., 2002) is also an important future direction.", "startOffset": 164, "endOffset": 211}], "year": 2015, "abstractText": "The online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}