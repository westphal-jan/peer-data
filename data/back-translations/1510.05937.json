{"id": "1510.05937", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Binary Speaker Embedding", "abstract": "The popular i-vector model represents loudspeakers as low-dimensional continuous vectors (i-vectors) and is therefore a way to continuously embed loudspeakers. In this paper, we examine binary loudspeaker embedding, which transforms ivectors into binary vectors (codes) through a hash function. We assume local hashing (LSH), a simple binary approach in which binary codes are derived from a series of random hash functions. A potential problem with LSH is that the randomly sampled hash functions may be suboptimal, so we propose an improved hamming distance learning approach, in which the hash function is learned through variable block training, in which each dimension of the original i-vectors is projected onto differently sized binary codes. Our experiments show that binary loudspeaker embedding can provide competitive or even better results both in terms of identification and during speaker identification, which can be significantly reduced.", "histories": [["v1", "Tue, 20 Oct 2015 15:49:59 GMT  (94kb)", "https://arxiv.org/abs/1510.05937v1", null], ["v2", "Thu, 31 Mar 2016 05:33:49 GMT  (28kb)", "http://arxiv.org/abs/1510.05937v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["lantian li", "dong wang", "chao xing", "kaimin yu", "thomas fang zheng"], "accepted": false, "id": "1510.05937"}, "pdf": {"name": "1510.05937.pdf", "metadata": {"source": "CRF", "title": "Binary Speaker Embedding", "authors": ["Lantian Li", "Chao Xing", "Dong Wang", "Kaimin Yu", "Thomas Fang Zheng"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "xingchao@cslt.riit.tsinghua.edu.cn;", "yukm@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn", "fzheng@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n05 93\n7v 2\n[ cs\n.S D\n] 3\n1 M\nar 2\n01 6\ndimensional continuous vectors (i-vectors), and hence it is a way of continuous speaker embedding. In this paper, we investigate binary speaker embedding, which transforms i-vectors to binary vectors (codes) by a hash function. We start from locality sensitive hashing (LSH), a simple binarization approach where binary codes are derived from a set of random hash functions. A potential problem of LSH is that the randomly sampled hash functions might be suboptimal. We therefore propose an improved Hamming distance learning approach, where the hash function is learned by a variable-sized block training that projects each dimension of the original i-vectors to variablesized binary codes independently.\nOur experiments show that binary speaker embedding can deliver competitive or even better results on both speaker verification and identification tasks, while the memory usage and the computation cost are significantly reduced. Index Terms: i-vector, LSH, Hamming distance learning, binary embedding, speaker recognition"}, {"heading": "1. Introduction", "text": "The popular i-vector model for speaker recognition assumes that a speech segment can be represented as a low-dimensional continuous vector (i-vector) in a subspace that involves both speaker and channel variances [1, 2]. Normally the cosine distance is used as the distance measure in this i-vector space. Various discrimination or normalization approaches have been proposed to improve the i-vector model, e.g., linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5]. We prefer LDA because it is simple and effective, achieving similar performance as the complex PLDA while preserving the simple scoring based on cosine distance, which is highly important for large-scale applications. In this paper, whenever we mention the i-vector model or i-vectors, we mean i-vectors with LDA employed.\nThe i-vector model can be regarded as a continuous speaker embedding, which projects a complex and high-dimensional structural data (speech signal) to a simple speaker space that is low-dimensional and continuous. Despite the broad success of this approach, there are some potential problems associated\nThis work was supported by the National Natural Science Foundation of China under Grant No. 61371136 and No. 61271389, it was also supported by the National Basic Research Program (973 Program) of China under Grant No. 2013CB329302. D.W. and T.F.Z. are with Division of Technical Innovation and Development of Tsinghua National Laboratory for Information Science and Technology and Research Institute of Information Technology (RIIT) of Tsinghua University. This paper is also supported by Pachira.\nwith the continuous embedding. Firstly, although i-vectors are quite compact representations of speakers (compared to the conventional Gaussian mixture models (GMMs)), memory usage and computation cost are still demanding for large-scale tasks. For example, if the dimensionality of an i-vector is 150 and each dimension is a float (8 bytes), representing one billion people (the population of China) requires 1.2 TB memory. To search for a people given a reference i-vector, the computation cost involves one billion cosine distance calculations, which is very demanding. Note that the computation will become prohibitive if the model is based on GMMs or if the scoring is based on PLDA, that is why we focus on the LDA-projected i-vector model in this paper.\nAnother potential problem of the continuous speaker embedding, as we conjecture, is the over sensitivity to non-speaker variances. We argue that since the vectors are continuous and can be changed by any small variances in the speech signal, ivectors tend to be \u2018over representative\u2019 for subtle information that are irrelevant to speakers. LDA can partly solve this problem, but it is the nature of the continuous representation that makes it fragile with corruptions. This resembles to the fact that analog signals tend to be impacted by transmission errors.\nIn this paper, we propose to use binary speaker embedding to solve the above problems. More specifically, we transfer ivectors to binary vectors (codes) on the principle that the cosine distance in the original i-vector space is largely preserved in the new binary space measured by the Hamming distance. The binary embedding leads to significant reduction in storage and computing cost; additionally, since binary vectors are less sensitive to subtle change, we expect more robustness in conditions with noise or channel mismatch.\nWe start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9]. Particularly, we propose a variable-sized block training algorithm that can improve the learning speed and allocate more bits for important dimensions.\nOne may argue that the binary embedding is a retraction back to the historical one-hot encoding, and binary codes are less representative than continuous vectors unless a very large dimensionality is used. However, our experiments showed that this is not the truth: very compact binary vectors can represent tens of thousands of speakers pretty well, and binary vectors work even better in some circumstances. These observations indicate that binary embedding is not an odd retraction to the one-hot encoding; it is essentially a simple speaker information distillation via hashing.\nThe rest of this paper is organized as follows. Section 2 describes the related work; Section 3 presents the LSH-based binary embedding, and Section 4 presents the variable-sized block training. The experiments are presented in Section 5, and Sec-\ntion 6 concludes the paper."}, {"heading": "2. Related work", "text": "Binary embedding has not been fully recognized in the speaker recognition community. The limited research focuses on employing the advantages of binary codes in robustness and fast computing. For example, [10] proposed a time-spectral binary masking approach to improve robustness of speaker recognition in conditions with high interference. Besides, [11] presented a solution for large-scale speaker search and indexing under the ivector model, where the search and indexing algorithm is based on LSH. The work proposed in [12] is more relevant to our proposal. By their approach, a universal background model (UBM) is employed to divide the acoustic space into subregions, and each subregion is populated with a set of Gaussian components. Each acoustic frame is then converted to a binary vector by evaluating the Gaussian components that the frame belongs to, and the frame-level vectors are finally accumulated to produce the segment-level speaker vector. Better robustness compared with the conventional GMM-UBM approach was reported by the authors."}, {"heading": "3. Binary speaker embedding with LSH", "text": "We present the binary embedding approach for speaker recognition. Basically the continuous i-vectors are projected to binary codes in such a way that the distance between i-vectors is largely preserved by the binary codes. We consider the cosine distance for i-vectors (which is the most simple and effective for speaker recognition) and the Hamming distance for binary codes (which is the most popular distance measure for binary codes).\nLet x denote a length-normalized i-vector, and the similarity between i-vectors is measured by the cosine distance. Our goal is to project a continuous vector x to a binary code h(x) of b bits. The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.\nWe employ a simple LSH approach proposed in [7]. It selects b hash functions hr(\u00b7), each of which simply rounds the output of the product of x with a random hyperplane defined by a random vector r:\nhr(x) =\n{\n1 if rTx \u2265 0 0 otherwise\n(1)\nwhere r is sampled from a zero-mean multivariate Gaussian N(0; I). It was shown by [13] that the following LSH requirement is satisfied:\nP [h(xi) = h(xj)] = 1\u2212 1\n\u03c0 \u03b8(xi, xj) (2)\nwhere \u03b8(xi, xj) is the angle between xi and xj and is closely related to their cosine distance. Intuitively, this means that similar i-vectors have more chance to be encoded by the same binary vector than dissimilar ones, which just coincides our goal of preserving similarities of i-vectors with the binary codes."}, {"heading": "4. Binary embedding with variable-sized block training", "text": "A potential problem of the LSH embedding is that x is not necessarily uniformly distributed on the hyper sphere, and so the uniformly sampled hash functions {hr} might be suboptimal. A\nbetter approach is to derive the hash function by learning from data. An interesting method of this category is the Hamming distance learning proposed by [9]. This section presents this approach first, and then proposes a variable-sized block training method that can improve training speed and quality."}, {"heading": "4.1. Hamming distance learning", "text": "The Hamming distance learning approach [9] learns a projection function f(x;w) where x is the input (an i-vector in our case) and w is the model parameter. Once the projection function is learned, the binary code for x is obtained simply by b(x;w) = sign(f(x;w)). Choosing different f leads to different learning methods. The simple linear model f(x;w) = wTx is chosen in this study. Note that if w is randomly sampled from N(0; I) and no training is performed, this approach is equivalent to LSH.\nThe Hamming distance learning defines a loss function on triplets (x, x+, x\u2212), where x is an i-vector of a particular speaker, x+ is another i-vector of the same speaker derived from a different speech segment, and x\u2212 is the i-vector of an imposter. The goal of Hamming distance learning is to optimize w such that b(x;w) is closer to b(x+;w) than b(x\u2212;w) in terms of Hamming distance. Denoting (h, h+, h\u2212) as the binary codes obtained by applying b(x,w) to the triplet (x, x+, x\u2212), the loss function of the learning is:\nl(h, h+, h\u2212) = [||h\u2212 h+||H \u2212 ||h\u2212 h \u2212||H + 1]+ (3)\nwhere || \u00b7 ||H is the Hamming distance, defined as the number of 1\u2032s in the vector. Adding the loss function and a regularization term, the training objective function with respect to w is defined as follows:\nL(w) = \u2211\n(x,x+,x\u2212)\u2208D\nl(b(x;w), b(x+;w), b(x\u2212;w))+ \u03bb\n2 ||w||2\n(4) where D = {(xi, x+i , x \u2212 i )} n i=1 denotes the training samples, and \u03bb is a factor to scale the contribution of the regularization term. Note that this approach has been employed to image retrieval in [9], though in this paper we use it for speaker recognition."}, {"heading": "4.2. Variable-sized block training", "text": "A particular problem of the Hamming distance learning is the high computation demand if the dimensions of the continuous and/or binary vector are large. Additionally, the learning algorithm treats each dimension of the input continuous vector equally, which is not optimal for the LDA-projected i-vectors for which the low dimensions involve more discriminative information. We propose a variable-sized blocking training approach to solve this problem.\nConsidering that the expected number of bits of the binary codes is b, we hope these bits are distributed to the dimensions of the original i-vectors unequally, subjected to the constraints \u2211D\ni=1 Ti = b where D is the dimensionality of the original ivectors, and Ti is the number of bits allocated to dimension i. Ti is designed to be linearly descended as follows:\nTi = D + 1\u2212 i\nD T1 (5)\nThis leads to Ti = 2b(D+1\u2212i) D(D+1) , and the ceil value Ti = \u2308 2b(D+1\u2212i) D(D+1)\n\u2309 is selected as the number of encoding bits for the i-th dimension.\nSpecifically, the variable-sized block training first defines the number of bits Ti, and then the Hamming distance learning is employed to learn the projection matrix wi for the i-th dimension. The learned wi is used to embed the i-th dimension of the i-vectors to binary codes. Since the learning and embedding for every dimension i is independent, this in fact leads to a block diagonal parameter matrix w (so the block training is named):\nw =\n\n    w1 0 0 \u00b7 \u00b7 \u00b7 0 0 w2 0 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 0 \u00b7 \u00b7 \u00b7 wD\n\n    .\nNote that this block training learns each dimension independently so it is faster than the conventional Hamming distance learning where the projection matrix w is learned as a whole. Additionally, because more bits are allocated for low dimensions (which involve more information due to LDA), the resultant binary codes are more representative and discriminative."}, {"heading": "5. Experiments", "text": "The proposed binary embedding approach was tested on both speaker verification and identification tasks. We first present the data and configurations used in the experiments, and then report the results on the verification and identification tasks respectively.\n5.1. Data\n\u2022 Development data:\n\u2013 Fisher: 7, 196 female speakers with 13, 287 utterances were used to train the i-vector, LDA models. The same data were also used to conduct the variable-sized block training.\n\u2022 Evaluation data:\n\u2013 NIST SRE08: The data of the NIST SRE08 core test in short2 and short3 conditions [14] were used for the speaker verification evaluation. It consists of 1, 997 female enrollment utterances and 3, 858 test utterances. We constructed 59, 343 trials based on the database, including 12, 159 target trials and 47, 184 imposter trials.\n\u2013 WSJ: The WSJ database was used for the speaker identification evaluation. It consists of 282 female speakers and 37, 317 utterances. For each speaker, 5 utterances were randomly selected to train the speaker models, and the remaining utterances were used for evaluation, including 35, 907 test trials."}, {"heading": "5.2. Experimental setup", "text": "The acoustic feature involved 19-dimensional Mel frequency cepstral coefficients (MFCCs) together with the log energy. The first and second order derivatives were augmented to the static features, resulting in 60-dimensional feature vectors. The UBM involved 2, 048 Gaussian components and was trained with about 8, 000 female utterances selected from the Fisher database randomly. The dimensionality of the i-vectors was 400. The LDA model was trained with utterances of 7, 196 female speakers, again randomly selected from the Fisher\ndatabase. The dimensionality of the LDA projection space was set to 150. For the variable-sized block training, utterances in the Fisher database were sampled randomly to build the contrastive triples and were used to train the projection function."}, {"heading": "5.3. Speaker verification task", "text": "The first experiment investigates the performance of binary speaker embedding on the speaker verification task. All the ivectors have been transformed by LDA, and the dimensionality is 150. The performance is evaluated in terms of equal error rate (EER) under the NIST SRE08 evaluation set, and the results are shown in Table 1 for the LSH approach, and Table 2 for the variable-sized block training. In each table, the performance with binary codes (denoted by \u2018b-vector\u2019) of various sizes are reported. Note that we didn\u2019t report the time cost in this experiment since the computation is not a serious problem in speaker verification, although binary vectors are certainly faster.\nFrom the results in Table 1 and Table 2, it can be observed that binary vectors can achieve performance comparable to the conventional i-vectors, in spite of the much smaller number of bits. For example, with the largest binary codes, the number of bits is only one tenth of that of the original i-vectors. Compared the two binary embedding methods, it is clear that the variablesized block training performs better consistently. In condition 1 and 3, the binary codes derived by the variable-sized block training work even better than the i-vectors. Note that the conditions where the binary codes perform better than i-vectors are all with microphones, which are different from the condition of the training data (Fisher database that was recorded by telephones). This seems to support our conjecture that binary codes are more robust to speaker-irrelevant variations."}, {"heading": "5.4. Speaker identification task", "text": "The advantage of the binary embedding is more evident on the speaker identification task, where significant computation is required when computing the k-nearest candidates of a given speaker vector. We use the WSJ database for evaluation, which contains 282 female speakers, and 35, 907 target trials. For each trial (x, y) \u2208 V , where V is the speaker correspondence set, x is the enrollment speaker vector and y is the test speaker vector. In speaker identification, given a test utterance y whose speaker vector is x, the task is to search for the k-nearest speaker vectors around x. If a vector y is in the k-nearest candidates and (x, y) is in the speaker correspondence set V , then a top-k hit is obtained. We evaluate the performance of speaker identification by the top-k accuracy, which is defined as the proportion of the top-k hits in all the trials. Note that we use only a naive k-nearest search which calculates the distance of the test vector to all the speaker vectors and select the k-nearest candidates. In fact, various methods can be employed to improve efficiency of the search in particular for binary codes, e.g., the PLEB algorithm [15, 7]. We focus on computation cost of the basic algorithm in this paper.\nThe top-k accuracy with the two binary embedding approaches are reported in Table 3 and Table 4, respectively. For comparison, the bits of the vectors and the computation cost (relative to the i-vector system) are also reported. From these results, we observe that binary vectors can approach performance of the conventional i-vectors with much fewer bits and much faster computation. Compared to LSH, the variable-sized block training leads to slightly worse performance. We attribute this result to the fact that the objective function of the variablesized block training is pair-wised discrimination (true or feigned speakers), which is not directly related to the metric in speaker identification. The results we obtained in Table 3 and Table 4 clearly demonstrate that the binary embedding performs much faster than the conventional continuous embedding, and thus is highly suitable for large-scale identification tasks, e.g., nationalwide criminal search."}, {"heading": "6. Conclusions", "text": "This paper investigated the binary embedding approach for speaker recognition. We studied two binarization approaches, one is based on LSH and the other is based on Hamming distance learning. Our experiments on both speaker verification and identification tasks show that binary speaker vectors can deliver competitive results with smaller vectors and less computation compared to the conventional i-vectors. This is particularly true with the proposed variable-sized block training algorithm, an extension of the conventional Hamming distance learning method.\nAlthough it has not completely beat the continuous ivectors, the binary speaker embedding proposed in this paper is still very promising. Future work will study more powerful methods to learn the hash function, and investigate the methods to learn binary vectors from speech signals directly."}, {"heading": "7. References", "text": "[1] P. Kenny, G. Boulianne, P. Ouellet, and P. Dumouchel, \u201cJoint\nfactor analysis versus eigenchannels in speaker recognition,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.\n[2] \u2014\u2014, \u201cSpeaker and session variability in gmm-based speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448\u20131460, 2007.\n[3] N. Dehak, P. Kenny, R. Dehak, P. Ouellet, and P. Dumouchel, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[4] A. O. Hatch, S. Kajarekar, and A. Stolcke, \u201cWithin-class covariance normalization for svm-based speaker recognition,\u201d Interspeech, 2006.\n[5] S. Ioffe, \u201cProbabilistic linear discriminant analysis,\u201d Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.\n[6] A. Gionis, P. Indyk, R. Motwani et al., \u201cSimilarity search in high dimensions via hashing,\u201d in VLDB, vol. 99, 1999, pp. 518\u2013529.\n[7] M. S. Charikar, \u201cSimilarity estimation techniques from rounding algorithms,\u201d in Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM, 2002, pp. 380\u2013388.\n[8] A. Andoni and P. Indyk, \u201cNear-optimal hashing algorithms for approximate nearest neighbor in high dimensions,\u201d in Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on. IEEE, 2006, pp. 459\u2013468.\n[9] M. Norouziy, D. J. Fleety, and R. Salakhutdinovy, \u201cHamming distance metric learning,\u201d NIPS, pp. 1061\u20131069, 2012.\n[10] Y. Shao and D. Wang, \u201cRobust speaker recognition using binary time-frequency masks,\u201d in Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 1. IEEE, 2006, pp. I\u2013I.\n[11] R. Leary and W. Andrews, \u201cRandom projections for large-scale speaker search,\u201d Interspeech, 2014.\n[12] J.-F. Bonastre, P.-M. Bousquet, D. Matrouf, and X. Anguera, \u201cDiscriminant binary data representation for speaker recognition,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5284\u20135287.\n[13] M. X. Goemans and D. P. Williamson, \u201cImproved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming,\u201d Journal of the ACM (JACM), vol. 42, no. 6, pp. 1115\u20131145, 1995.\n[14] NIST, \u201cthe nist year 2008 speaker recognition evaluation plan\u201d,\u201d Online: http://www.itl.nist.gov/iad/mig/tests/sre/2008/ sre08 evalplan release4.pdf, 2008.\n[15] P. Indyk and R. Motwani, \u201cApproximate nearest neighbors: towards removing the curse of dimensionality,\u201d in Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM, 1998, pp. 604\u2013613."}], "references": [{"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker and session variability in gmm-based speaker verification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448\u20131460, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Within-class covariance normalization for svm-based speaker recognition", "author": ["A.O. Hatch", "S. Kajarekar", "A. Stolcke"], "venue": "Interspeech, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "VLDB, vol. 99, 1999, pp. 518\u2013529.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM, 2002, pp. 380\u2013388.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on. IEEE, 2006, pp. 459\u2013468.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Hamming distance metric learning", "author": ["M. Norouziy", "D.J. Fleety", "R. Salakhutdinovy"], "venue": "NIPS, pp. 1061\u20131069, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust speaker recognition using binary time-frequency masks", "author": ["Y. Shao", "D. Wang"], "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 1. IEEE, 2006, pp. I\u2013I.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Random projections for large-scale speaker search", "author": ["R. Leary", "W. Andrews"], "venue": "Interspeech, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminant binary data representation for speaker recognition", "author": ["J.-F. Bonastre", "P.-M. Bousquet", "D. Matrouf", "X. Anguera"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5284\u20135287.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM (JACM), vol. 42, no. 6, pp. 1115\u20131145, 1995.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "the nist year 2008 speaker recognition evaluation plan", "author": ["NIST"], "venue": "Online: http://www.itl.nist.gov/iad/mig/tests/sre/2008/ sre08 evalplan release4.pdf, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM, 1998, pp. 604\u2013613.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "The popular i-vector model for speaker recognition assumes that a speech segment can be represented as a low-dimensional continuous vector (i-vector) in a subspace that involves both speaker and channel variances [1, 2].", "startOffset": 213, "endOffset": 219}, {"referenceID": 1, "context": "The popular i-vector model for speaker recognition assumes that a speech segment can be represented as a low-dimensional continuous vector (i-vector) in a subspace that involves both speaker and channel variances [1, 2].", "startOffset": 213, "endOffset": 219}, {"referenceID": 2, "context": ", linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": ", linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": ", linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 91, "endOffset": 100}, {"referenceID": 6, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 91, "endOffset": 100}, {"referenceID": 7, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 91, "endOffset": 100}, {"referenceID": 8, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "For example, [10] proposed a time-spectral binary masking approach to improve robustness of speaker recognition in conditions with high interference.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Besides, [11] presented a solution for large-scale speaker search and indexing under the ivector model, where the search and indexing algorithm is based on LSH.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "The work proposed in [12] is more relevant to our proposal.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.", "startOffset": 17, "endOffset": 26}, {"referenceID": 6, "context": "The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.", "startOffset": 17, "endOffset": 26}, {"referenceID": 7, "context": "The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.", "startOffset": 17, "endOffset": 26}, {"referenceID": 6, "context": "We employ a simple LSH approach proposed in [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "It was shown by [13] that the following LSH requirement is satisfied:", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "An interesting method of this category is the Hamming distance learning proposed by [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "The Hamming distance learning approach [9] learns a projection function f(x;w) where x is the input (an i-vector in our case) and w is the model parameter.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "Note that this approach has been employed to image retrieval in [9], though in this paper we use it for speaker recognition.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "\u2013 NIST SRE08: The data of the NIST SRE08 core test in short2 and short3 conditions [14] were used for the speaker verification evaluation.", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": ", the PLEB algorithm [15, 7].", "startOffset": 21, "endOffset": 28}, {"referenceID": 6, "context": ", the PLEB algorithm [15, 7].", "startOffset": 21, "endOffset": 28}], "year": 2016, "abstractText": "The popular i-vector model represents speakers as lowdimensional continuous vectors (i-vectors), and hence it is a way of continuous speaker embedding. In this paper, we investigate binary speaker embedding, which transforms i-vectors to binary vectors (codes) by a hash function. We start from locality sensitive hashing (LSH), a simple binarization approach where binary codes are derived from a set of random hash functions. A potential problem of LSH is that the randomly sampled hash functions might be suboptimal. We therefore propose an improved Hamming distance learning approach, where the hash function is learned by a variable-sized block training that projects each dimension of the original i-vectors to variablesized binary codes independently. Our experiments show that binary speaker embedding can deliver competitive or even better results on both speaker verification and identification tasks, while the memory usage and the computation cost are significantly reduced.", "creator": "LaTeX with hyperref package"}}}