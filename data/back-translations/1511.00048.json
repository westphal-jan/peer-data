{"id": "1511.00048", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "The Pareto Regret Frontier for Bandits", "abstract": "I show that the price of such unbalanced worst-case remorse guarantees is quite high. In particular, if an algorithm enjoys a worst-case remorse of B in relation to any action, then there must be another action for which the worst-case remorse is at least {\\ Omega} (nK / B), where n is the horizon and K is the number of actions. I also specify ceilings in both stochastic and hostile environments, which show that this result cannot be improved. In the stochastic case, the pareto remorse limit is precisely characterized to constant factors.", "histories": [["v1", "Fri, 30 Oct 2015 23:30:30 GMT  (42kb)", "http://arxiv.org/abs/1511.00048v1", "14 pages. To appear at NIPS 2015"]], "COMMENTS": "14 pages. To appear at NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore"], "accepted": true, "id": "1511.00048"}, "pdf": {"name": "1511.00048.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tor.lattimore@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n00 04\n8v 1\n[ cs\n.L G\n] 3"}, {"heading": "1 Introduction", "text": "The multi-armed bandit is the simplest class of problems that exhibit the exploration/exploitation dilemma. In each time step the learner chooses one of K actions and receives a noisy reward signal for the chosen action. A learner\u2019s performance is measured in terms of the regret, which is the (expected) difference between the rewards it actually received and those it would have received (in expectation) by choosing the optimal action.\nPrior work on the regret criterion for finite-armed bandits has treated all actions uniformly and has aimed for bounds on the regret that do not depend on which action turned out to be optimal. I take a different approach and ask what can be achieved if some actions are given special treatment. Focussing on worst-case bounds, I ask whether or not it is possible to achieve improved worst-case regret for some actions, and what is the cost in terms of the regret for the remaining actions. Such results may be useful in a variety of cases. For example, a company that is exploring some new strategies might expect an especially small regret if its existing strategy turns out to be (nearly) optimal.\nThis problem has previously been considered in the experts setting where the learner is allowed to observe the reward for all actions in every round, not only for the action actually chosen. The earliest work seems to be by Hutter and Poland [2005] where it is shown that the learner can assign a prior weight to each action and pays a worst-case regret of O(\n\u221a\u2212n log \u03c1i) for expert i where \u03c1i is the prior belief in expert i and n is the horizon. The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant regret with respect to a single action while suffering minimally on the remainder. The problem was studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe the pareto regret frontier when K = 2.\nOther related work (also in the experts setting) is where the objective is to obtain an improved regret against a mixture of available experts/actions [Even-Dar et al., 2008, Kapralov and Panigrahy, 2011]. In a similar vain, Sani et al. [2014] showed that algorithms for prediction with expert advice\ncan be combined with minimal cost to obtain the best of both worlds. In the bandit setting I am only aware of the work by Liu and Li [2015] who study the effect of the prior on the regret of Thompson sampling in a special case. In contrast the lower bound given here applies to all algorithms in a relatively standard setting.\nThe main contribution of this work is a characterisation of the pareto regret frontier (the set of achievable worst-case regret bounds) for stochastic bandits.\nLet \u00b5i \u2208 R be the unknown mean of the ith arm and assume that supi,j \u00b5i \u2212 \u00b5j \u2264 1. In each time step the learner chooses an action It \u2208 {1, . . . ,K} and receives reward gIt,t = \u00b5i + \u03b7t where \u03b7t is the noise term that I assume to be sampled independently from a 1-subgaussian distribution that may depend on It. This model subsumes both Gaussian and Bernoulli (or bounded) rewards. Let \u03c0 be a bandit strategy, which is a function from histories of observations to an action It. Then the n-step expected pseudo regret with respect to the ith arm is\nR\u03c0\u00b5,i = n\u00b5i \u2212 E n \u2211\nt=1\n\u00b5It ,\nwhere the expectation is taken with respect to the randomness in the noise and the actions of the policy. Throughout this work n will be fixed, so is omitted from the notation. The worst-case expected pseudo-regret with respect to arm i is\nR\u03c0i = sup \u00b5 R\u03c0\u00b5,i . (1)\nThis means that R\u03c0 \u2208 RK is a vector of worst-case pseudo regrets with respect to each of the arms. Let B \u2282 RK be a set defined by\nB =\n\n\n\nB \u2208 [0, n]K : Bi \u2265 min\n\n\n\nn, \u2211\nj 6=i\nn\nBj\n\n\n\nfor all i\n\n\n\n. (2)\nThe boundary of B is denoted by \u03b4B. The following theorem shows that \u03b4B describes the pareto regret frontier up to constant factors.\nTheorem There exist universal constants c1 = 8 and c2 = 252 such that:\nLower bound: for \u03b7t \u223c N (0, 1) and all strategies \u03c0 we have c1(R\u03c0 +K) \u2208 B Upper bound: for all B \u2208 B there exists a strategy \u03c0 such that R\u03c0i \u2264 c2Bi for all i\nObserve that the lower bound relies on the assumption that the noise term be Gaussian while the upper bound holds for subgaussian noise. The lower bound may be generalised to other noise models such as Bernoulli, but does not hold for all subgaussian noise models. For example, it does not hold if there is no noise (\u03b7t = 0 almost surely).\nThe lower bound also applies to the adversarial framework where the rewards may be chosen arbitrarily. Although I was not able to derive a matching upper bound in this case, a simple modification of the Exp-\u03b3 algorithm [Bubeck and Cesa-Bianchi, 2012] leads to an algorithm with\nR\u03c01 \u2264 B1 and R\u03c0k . nK\nB1 log\n(\nnK B21\n)\nfor all k \u2265 2 ,\nwhere the regret is the adversarial version of the expected regret. The details may be found in the Appendix.\nThe new results seem elegant, but disappointing. In the experts setting we have seen that the learner can distribute a prior amongst the actions and obtain a bound on the regret depending in a natural way on the prior weight of the optimal action. In contrast, in the bandit setting the learner pays an enormously higher price to obtain a small regret with respect to even a single arm. In fact, the learner must essentially choose a single arm to favour, after which the regret for the remaining arms has very limited flexibility. Unlike in the experts setting, if even a single arm enjoys constant worst-case regret, then the worst-case regret with respect to all other arms is necessarily linear."}, {"heading": "2 Preliminaries", "text": "I use the same notation as Bubeck and Cesa-Bianchi [2012]. Define Ti(t) to be the number of times action i has been chosen after time step t and \u00b5\u0302i,s to be the empirical estimate of \u00b5i from the first s times action i was sampled. This means that \u00b5\u0302i,Ti(t\u22121) is the empirical estimate of \u00b5i at the start of the tth round. I use the convention that \u00b5\u0302i,0 = 0. Since the noise model is 1-subgaussian we have\n\u2200\u03b5 > 0 P {\u2203s \u2264 t : \u00b5\u0302i,s \u2212 \u00b5i \u2265 \u03b5/s} \u2264 exp ( \u2212\u03b5 2\n2t\n)\n. (3)\nThis result is presumably well known, but a proof is included in Appendix E for convenience. The optimal arm is i\u2217 = argmaxi \u00b5i with ties broken in some arbitrary way. The optimal reward is \u00b5\u2217 = maxi \u00b5i. The gap between the mean rewards of the jth arm and the optimal arm is \u2206j = \u00b5\u2217 \u2212 \u00b5j and \u2206ji = \u00b5i \u2212 \u00b5j . The vector of worst-case regrets is R\u03c0 \u2208 RK and has been defined already in Eq. (1). I write R\u03c0 \u2264 B \u2208 RK if R\u03c0i \u2264 Bi for all i \u2208 {1, . . . ,K}. For vector R\u03c0 and x \u2208 R we have (R\u03c0 + x)i = R\u03c0i + x."}, {"heading": "3 Understanding the Frontier", "text": "Before proving the main theorem I briefly describe the features of the regret frontier. First notice that if Bi = \u221a n(K \u2212 1) for all i, then\nBi = \u221a n(K \u2212 1) = \u2211\nj 6=i\n\u221a n/(K \u2212 1) = \u2211\nj 6=i\nn\nBj .\nThus B \u2208 B as expected. This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al., 2002], which suffers Rucbi \u2208 \u2126( \u221a nK logn).\nOf course the uniform choice of B is not the only option. Suppose the first arm is special, so B1 should be chosen especially small. Assume without loss of generality that B1 \u2264 B2 \u2264 . . . \u2264 BK \u2264 n. Then by the main theorem we have\nB1 \u2265 K \u2211\ni=2\nn\nBi \u2265\nk \u2211\ni=2\nn Bi \u2265 (k \u2212 1)n Bk .\nTherefore\nBk \u2265 (k \u2212 1)n\nB1 . (4)\nThis also proves the claim in the abstract, since it implies that BK \u2265 (K \u2212 1)n/B1. If B1 is fixed, then choosing Bk = (k \u2212 1)n/B1 does not lie on the frontier because\nK \u2211\nk=2\nn\nBk =\nK \u2211\nk=2\nB1 k \u2212 1 \u2208 \u2126(B1 logK)\nHowever, if H = \u2211K k=2 1/(k \u2212 1) \u2208 \u0398(logK), then choosing Bk = (k \u2212 1)nH/B1 does lie on the frontier and is a factor of logK away from the lower bound given in Eq. (4). Therefore up the a logK factor, points on the regret frontier are characterised entirely by a permutation determining the order of worst-case regrets and the smallest worst-case regret.\nPerhaps the most natural choice of B (assuming again that B1 \u2264 . . . \u2264 BK) is\nB1 = n p and Bk = (k \u2212 1)n1\u2212pH for k > 1 .\nFor p = 1/2 this leads to a bound that is at most \u221a K logK worse than that obtained by MOSS and\nOC-UCB while being a factor of \u221a K better for a select few.\nAssumptions\nThe assumption that \u2206i \u2208 [0, 1] is used to avoid annoying boundary problems caused by the fact that time is discrete. This means that if \u2206i is extremely large, then even a single sample from this arm can cause a big regret bound. This assumption is already quite common, for example a worst-case regret of \u2126( \u221a Kn) clearly does not hold if the gaps are permitted to be unbounded. Unfortunately there is no perfect resolution to this annoyance. Most elegant would be to allow time to be continuous with actions taken up to stopping times. Otherwise you have to deal with the discretisation/boundary problem with special cases, or make assumptions as I have done here."}, {"heading": "4 Lower Bounds", "text": "Theorem 1. Assume \u03b7t \u223c N (0, 1) is sampled from a standard Gaussian. Let \u03c0 be an arbitrary strategy, then 8(R\u03c0 +K) \u2208 B.\nProof. Assume without loss of generality that R\u03c01 = mini R \u03c0 i (if this is not the case, then simply re-order the actions). If R\u03c01 > n/8, then the result is trivial. From now on assume R \u03c0 1 \u2264 n/8. Let c = 4 and define\n\u03b5k = min\n{\n1 2 , cR\u03c0k n\n}\n\u2264 1 2 .\nDefine K vectors \u00b51, . . . , \u00b5K \u2208 RK by\n(\u00b5k)j = 1\n2 +\n\n \n \n0 if j = 1 \u03b5k if j = k 6= 1 \u2212\u03b5j otherwise .\nTherefore the optimal action for the bandit with means \u00b5k is k. Let A = {k : R\u03c0k \u2264 n/8} and A\u2032 = {k : k /\u2208 A} and assume k \u2208 A. Then\nR\u03c0k (a) \u2265 R\u03c0\u00b5k,k (b) \u2265 \u03b5kE\u03c0\u00b5k\n\n\n\u2211\nj 6=k\nTj(n)\n\n (c) = \u03b5k ( n\u2212 E\u03c0\u00b5kTk(n) ) (d) = cR\u03c0k (n\u2212 E\u03c0\u00b5kTk(n)) n ,\nwhere (a) follows since R\u03c0k is the worst-case regret with respect to arm k, (b) since the gap between the means of the kth arm and any other arm is at least \u03b5k (Note that this is also true for k = 1 since \u03b51 = mink \u03b5k. (c) follows from the fact that \u2211\ni Ti(n) = n and (d) from the definition of \u03b5k. Therefore\nn\n(\n1\u2212 1 c\n)\n\u2264 E\u03c0\u00b5kTk(n) . (5)\nTherefore for k 6= 1 with k \u2208 A we have\nn\n(\n1\u2212 1 c\n)\n\u2264 E\u03c0\u00b5kTk(n) (a) \u2264 E\u03c0\u00b51Tk(n) + n\u03b5k \u221a E\u03c0\u00b51Tk(n)\n(b) \u2264 n\u2212 E\u03c0\u00b51T1(n) + n\u03b5k \u221a E\u03c0\u00b51Tk(n) (c) \u2264 n c + n\u03b5k \u221a E\u03c0\u00b51Tk(n) ,\nwhere (a) follows from standard entropy inequalities and a similar argument as used by Auer et al. [1995] (details given in Appendix C), (b) since k 6= 1 and E\u03c0\u00b51T1(n) + E\u03c0\u00b51Tk(n) \u2264 n, and (c) by Eq. (5). Therefore\nE \u03c0 \u00b51Tk(n) \u2265 1\u2212 2c \u03b52k ,\nwhich implies that\nR\u03c01 \u2265 R\u03c0\u00b51,1 = K \u2211\nk=2\n\u03b5kE \u03c0 \u00b51Tk(n) \u2265\n\u2211\nk\u2208A\u2212{1}\n1\u2212 2c \u03b5k = 1 8 \u2211\nk\u2208A\u2212{1}\nn\nR\u03c0k .\nTherefore for all i \u2208 A we have\n8R\u03c0i \u2265 \u2211\nk\u2208A\u2212{1}\nn R\u03c0k \u00b7 R\n\u03c0 i\nR\u03c01 \u2265\n\u2211\nk\u2208A\u2212{i}\nn\nR\u03c0k .\nTherefore\n8R\u03c0i + 8K \u2265 \u2211\nk 6=i\nn\nR\u03c0k + 8K \u2212\n\u2211\nk\u2208A\u2032\u2212{i}\nn\nR\u03c0k \u2265\n\u2211\nk 6=i\nn\nR\u03c0k ,\nwhich implies that 8(R\u03c0 +K) \u2208 B as required."}, {"heading": "5 Upper Bounds", "text": "I now show that the lower bound derived in the previous section is tight up to constant factors. The algorithm is a generalisation MOSS [Audibert and Bubeck, 2009] with two modifications. First, the width of the confidence bounds are biased in a non-uniform way, and second, the upper confidence bounds are shifted. The new algorithm is functionally identical to MOSS in the special case that Bi is uniform. Define log+(x) = max {0, log(x)}.\n1: Input: n and B1, . . . , BK 2: ni = n\n2/B2i for all i 3: for t \u2208 1, . . . , n do\n4: It = argmax i \u00b5\u0302i,Ti(t\u22121) +\n\u221a\n4\nTi(t\u2212 1) log+\n(\nni Ti(t\u2212 1)\n) \u2212 \u221a 1\nni 5: end for\nAlgorithm 1: Unbalanced MOSS\nTheorem 2. Let B \u2208 B, then the strategy \u03c0 given in Algorithm 1 satisfies R\u03c0 \u2264 252B.\nCorollary 3. For all \u00b5 the following hold:\n1. R\u03c0\u00b5,i\u2217 \u2264 252Bi\u2217 .\n2. R\u03c0\u00b5,i\u2217 \u2264 mini(n\u2206i + 252Bi)\nThe second part of the corollary is useful when Bi\u2217 is large, but there exists an arm for which n\u2206i and Bi are both small. The proof of Theorem 2 requires a few lemmas. The first is a somewhat standard concentration inequality that follows from a combination of the peeling argument and Doob\u2019s maximal inequality.\nLemma 4. Let Zi = max 1\u2264s\u2264n\n\u00b5i \u2212 \u00b5\u0302i,s \u2212 \u221a 4\ns log+ (ni s ) . Then P {Zi \u2265 \u2206} \u2264 20ni\u22062 for all \u2206 > 0.\nProof. Using the peeling device.\nP {Zi \u2265 \u2206} (a) = P\n{\n\u2203s \u2264 n : \u00b5i \u2212 \u00b5\u0302i,s \u2265 \u2206+ \u221a 4\ns log+ (ni s )\n}\n(b) \u2264 \u221e \u2211\nk=0\nP\n{ \u2203s < 2k+1 : s(\u00b5i \u2212 \u00b5\u0302i,s) \u2265 2k\u2206+ \u221a 2k+2 log+\n( ni 2k+1 )\n}\n(c) \u2264 \u221e \u2211\nk=0\nexp ( \u22122k\u22122\u22062 ) min\n{\n1, 2k+1\nni\n}\n(d) \u2264 ( 8\nlog(2) + 8\n)\n\u00b7 1 ni\u22062 \u2264 20 ni\u22062 ,\nwhere (a) is just the definition of Zi, (b) follows from the union bound and re-arranging the equation inside the probability, (c) follows from Eq. (3) and the definition of log+ and (d) is obtained by upper bounding the sum with an integral.\nIn the analysis of traditional bandit algorithms the gap \u2206ji measures how quickly the algorithm can detect the difference between arms i and j. By design, however, Algorithm 1 is negatively biasing its estimate of the empirical mean of arm i by \u221a\n1/ni. This has the effect of shifting the gaps, which I denote by \u2206\u0304ji and define to be\n\u2206\u0304ji = \u2206ji + \u221a 1/nj \u2212 \u221a 1/ni = \u00b5i \u2212 \u00b5j + \u221a 1/nj \u2212 \u221a 1/ni .\nLemma 5. Define stopping time \u03c4ji by\n\u03c4ji = min\n{\ns : \u00b5\u0302j,s +\n\u221a\n4 s log+ (nj s )\n\u2264 \u00b5j + \u2206\u0304ji/2 } .\nIf Zi < \u2206\u0304ji/2, then Tj(n) \u2264 \u03c4ji.\nProof. Let t be the first time step such that Tj(t\u2212 1) = \u03c4ji. Then\n\u00b5\u0302j,Tj(t\u22121)+\n\u221a\n4\nTj(t\u2212 1) log+\n(\nnj Tj(t\u2212 1)\n)\n\u2212 \u221a 1/nj \u2264 \u00b5j + \u2206\u0304ji/2\u2212 \u221a 1/nj\n= \u00b5j + \u2206\u0304ji \u2212 \u2206\u0304ji/2\u2212 \u221a 1/nj = \u00b5i \u2212 \u221a 1/ni \u2212 \u2206\u0304ji/2\n< \u00b5\u0302i,Ti(t\u22121) +\n\u221a\n4\nTi(t\u2212 1) log+\n(\nni Ti(t\u2212 1)\n)\n\u2212 \u221a\n1/ni ,\nwhich implies that arm j will not be chosen at time step t and so also not for any subsequent time steps by the same argument and induction. Therefore Tj(n) \u2264 \u03c4ji.\nLemma 6. If \u2206\u0304ji > 0, then E\u03c4ji \u2264 40\n\u2206\u03042ji +\n64\n\u2206\u03042ji ProductLog\n(\nnj\u2206\u0304 2 ji\n64\n)\n.\nProof. Let s0 be defined by\ns0 =\n\u2308\n64\n\u2206\u03042ji ProductLog\n(\nnj\u2206\u0304 2 ji\n64\n)\u2309\n=\u21d2 \u221a 4\ns0 log+\n(\nnj s0\n)\n\u2264 \u2206\u0304ji 4 .\nTherefore\nE\u03c4ji =\nn \u2211\ns=1\nP {\u03c4ji \u2265 s} \u2264 1 + n\u22121 \u2211\ns=1\nP\n{\n\u00b5\u0302i,s \u2212 \u00b5i,s \u2265 \u2206\u0304ji 2\n\u2212 \u221a 4\ns log+ (nj s )\n}\n\u2264 1 + s0 + n\u22121 \u2211\ns=s0+1\nP\n{\n\u00b5\u0302i,s \u2212 \u00b5i,s \u2265 \u2206\u0304ji 4\n} \u2264 1 + s0 + \u221e \u2211\ns=s0+1\nexp\n(\n\u2212 s\u2206\u03042ji 32\n)\n\u2264 1 + s0 + 32 \u2206\u03042ji \u2264 40 \u2206\u03042ji + 64 \u2206\u03042ji ProductLog\n(\nnj\u2206\u0304 2 ji\n64\n)\n,\nwhere the last inequality follows since \u2206\u0304ji \u2264 2. Proof of Theorem 2. Let \u2206 = 2/ \u221a ni and A = {j : \u2206ji > \u2206}. Then for j \u2208 A we have \u2206ji \u2264 2\u2206\u0304ji and \u2206\u0304ji \u2265 \u221a 1/ni + \u221a 1/nj . Letting \u2206\u2032 = \u221a 1/ni we have\nR\u03c0\u00b5,i = E\n\n\nK \u2211\nj=1\n\u2206jiTj(n)\n\n\n\u2264 n\u2206+ E\n\n\n\u2211\nj\u2208A\n\u2206jiTj(n)\n\n\n(a) \u2264 2Bi + E\n\n\n\u2211\nj\u2208A\n\u2206ji\u03c4ji + nmax j\u2208A\n{ \u2206ji : Zi \u2265 \u2206\u0304ji/2 }\n\n\n(b) \u2264 2Bi + \u2211\nj\u2208A\n(\n80\n\u2206\u0304ji +\n128 \u2206\u0304ji ProductLog\n(\nnj\u2206\u0304 2 ji\n64\n))\n+ 4nE[Zi1{Zi \u2265 \u2206\u2032}]\n(c) \u2264 2Bi + \u2211\nj\u2208A\n90 \u221a nj + 4nE[Zi1{Zi \u2265 \u2206\u2032}] ,\nwhere (a) follows by using Lemma 5 to bound Tj(n) \u2264 \u03c4ji when Zi < \u2206\u0304ji. On the other hand, the total number of pulls for arms j for which Zi \u2265 \u2206\u0304ji/2 is at most n. (b) follows by bounding \u03c4ji in expectation using Lemma 6. (c) follows from basic calculus and because for j \u2208 A we have \u2206\u0304ji \u2265 \u221a 1/ni. All that remains is to bound the expectation.\n4nE[Zi1{Zi \u2265 \u2206\u2032}] \u2264 4n\u2206\u2032P {Zi \u2265 \u2206\u2032}+ 4n \u222b \u221e\n\u2206\u2032 P {Zi \u2265 z}dz \u2264\n160n \u2206\u2032ni = 160n\u221a ni = 160Bi ,\nwhere I have used Lemma 4 and simple identities. Putting it together we obtain\nR\u03c0\u00b5,i \u2264 2Bi + \u2211\nj\u2208A\n90 \u221a nj + 160B1 \u2264 252Bi ,\nwhere I applied the assumption B \u2208 B and so \u2211j 6=1 \u221a nj = \u2211\nj 6=1 n/Bj \u2264 Bi. The above proof may be simplified in the special case that B is uniform where we recover the minimax regret of MOSS, but with perhaps a simpler proof than was given originally by Audibert and Bubeck [2009]."}, {"heading": "On Logarithmic Regret", "text": "In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problemdependent regret in terms of the minimum gap [Lattimore, 2015]. Specifically, it can happen that\nRmoss\u00b5,i\u2217 \u2208 \u2126 ( K\n\u2206min logn\n)\n, (6)\nwhere \u2206min = mini:\u2206i>0 \u2206i. On the other hand, the order-optimal asymptotic regret can be significantly smaller. Specifically, UCB by Auer et al. [2002] satisfies\nRucb\u00b5,i\u2217 \u2208 O ( \u2211\ni:\u2206i>0\n1\n\u2206i logn\n)\n, (7)\nwhich for unequal gaps can be much smaller than Eq. (6) and is asymptotically order-optimal [Lai and Robbins, 1985]. The problem is that MOSS explores only enough to obtain minimax regret, but sometimes obtains minimax regret even when a more conservative algorithm would do better. It is worth remarking that this effect is harder to observe than one might think. The example given in the afforementioned technical report is carefully tuned to exploit this failing, but still requires n = 109 and K = 103 before significant problems arise. In all other experiments MOSS was performing admirably in comparison to UCB.\nAll these problems can be avoided by modifying UCB rather than MOSS. The cost is a factor of O( \u221a log n). The algorithm is similar to Algorithm 1, but chooses the action that maximises the following index.\nIt = argmax i \u00b5\u0302i,Ti(t\u22121) +\n\u221a\n(2 + \u03b5) log t Ti(t\u2212 1) \u2212 \u221a logn ni ,\nwhere \u03b5 > 0 is a fixed arbitrary constant.\nTheorem 7. If \u03c0 is the strategy of unbalanced UCB with ni = n2/B2i and B \u2208 B, then the regret of the unbalanced UCB satisfies:\n1. (problem-independent regret). R\u03c0\u00b5,i\u2217 \u2208 O ( Bi\u2217 \u221a logn ) .\n2. (problem-dependent regret). Let A = { i : \u2206i \u2265 2 \u221a 1/ni\u2217 logn } . Then\nR\u03c0\u00b5,i\u2217 \u2208 O ( Bi\u2217 \u221a logn1{A 6= \u2205}+ \u2211\ni\u2208A\n1\n\u2206i logn\n)\n.\nThe proof may be found in Appendix B. The indicator function in the problem-dependent bound vanishes for sufficiently large n provided ni\u2217 \u2208 \u03c9(log(n)), which is equivalent to Bi\u2217 \u2208 o(n/ \u221a logn). Thus for reasonable choices of B1, . . . , BK the algorithm is going to enjoy the same asymptotic performance as UCB. Theorem 7 may be proven for any index-based algorithm for which it can be shown that\nETi(n) \u2208 O ( 1\n\u22062i logn\n)\n,\nwhich includes (for example) KL-UCB [Cappe\u0301 et al., 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009]."}, {"heading": "A Note on Constants", "text": "The constants in the statement of Theorem 2 can be improved by carefully tuning all thresh-holds, but the proof would grow significantly and I would not expect a corresponding boost in practical performance. In fact, the reverse is true, since the \u201cweak\u201d bounds used in the proof would propagate to the algorithm. Also note that the 4 appearing in the square root of the unbalanced MOSS algorithm is due to the fact that I am not assuming rewards are bounded in [0, 1] for which the variance is at most 1/4. It is possible to replace the 4 with 2+ \u03b5 for any \u03b5 > 0 by changing the base in the peeling argument in the proof of Lemma 4 as was done by Bubeck [2010] and others.\nExperimental Results\nI compare MOSS and unbalanced MOSS in two simple simulated examples, both with horizon n = 5000. Each data point is an empirical average of \u223c104 i.i.d. samples, so error bars are too small to see. Code/data is available in the supplementary material. The first experiment has K = 2 arms and B1 = n 1 3 and B2 = n 2\n3 . I plotted the results for \u00b5 = (0,\u2212\u2206) for varying \u2206. As predicted, the new algorithm performs significantly better than MOSS for positive \u2206, and significantly worse otherwise (Fig. 1). The second experiment has K = 10 arms. This time B1 = \u221a n and Bk =\n(k \u2212 1)H\u221an with H = \u22119k=1 1/k. Results are shown for \u00b5k = \u22061{k = i\u2217} for \u2206 \u2208 [0, 1/2] and i\u2217 \u2208 {1, . . . , 10}. Again, the results agree with the theory. The unbalanced algorithm is superior to MOSS for i\u2217 \u2208 {1, 2} and inferior otherwise (Fig. 2).\n\u22120.4 \u22120.2 0 0.2 0.4 0\n200\n400\n600\n800\n\u2206\nR eg\nre t\nMOSS U. MOSS\nFigure 1 0 1 2 3 4 5 0\n1,000\n2,000\n\u03b8\nR eg\nre t\nFigure 2: \u03b8 = \u2206+ (i\u2217 \u2212 1)/2\nSadly the experiments serve only to highlight the plight of the biased learner, which suffers\nsignificantly worse results than its unbaised counterpart for most actions."}, {"heading": "6 Discussion", "text": "I have shown that the cost of favouritism for multi-armed bandit algorithms is rather serious. If an algorithm exhibits a small worst-case regret for a specific action, then the worst-case regret of the remaining actions is necessarily significantly larger than the well-known uniform worst-case bound of \u2126( \u221a Kn). This unfortunate result is in stark contrast to the experts setting for which there exist algorithms that suffer constant regret with respect to a single expert at almost no cost for the remainder. Surprisingly, the best achievable (non-uniform) worst-case bounds are determined up to a permutation almost entirely by the value of the smallest worst-case regret.\nThere are some interesting open questions. Most notably, in the adversarial setting I am not sure if the upper or lower bound is tight (or neither). It would also be nice to know if the constant factors can be determined exactly asymptotically, but so far this has not been done even in the uniform case. For the stochastic setting it is natural to ask if the OC-UCB algorithm can also be modified. Intuitively one would expect this to be possible, but it would require re-working the very long proof."}, {"heading": "Acknowledgements", "text": "I am indebted to the very careful reviewers who made many suggestions for improving this paper. Thank you!"}, {"heading": "B Proof of Theorem 7", "text": "Recall that the proof of UCB depends on showing that\nETi(n) \u2208 O ( 1\n\u22062i logn\n)\n.\nNow unbalanced UCB operates exactly like UCB, but with shifted rewards. Therefore for unbalanced UCB we have\nETi(n) \u2208 O ( 1\n\u2206\u03042i logn\n)\n,\nwhere\n\u2206\u0304i \u2265 \u2206i + \u221a logn ni \u2212 \u221a logn ni\u2217 .\nDefine :\nA =\n{\ni : \u2206i \u2265 2 \u221a logn\nni\u2217\n}\nIf i \u2208 A, then \u2206i \u2264 2\u2206\u0304i and \u2206\u0304i \u2265 \u221a\nlog n ni . Therefore\n\u2206iETi(n) \u2208 O ( \u2206i \u2206\u03042i log n ) \u2286 O ( 1 \u2206\u0304i logn ) \u2286 O ( \u221a ni log n ) \u2286 O ( n Bi \u221a logn ) .\nFor i /\u2208 A we have \u2206i < 2 \u221a\nlogn ni\u2217 thus\nE\n[\n\u2211\ni/\u2208A\n\u2206iTi(n)\n] \u2208 O ( n \u221a logn\nni\u2217\n)\n\u2286 O ( Bi\u2217 \u221a logn ) .\nTherefore\nR\u03c0\u00b5,i\u2217 = K \u2211\ni=1\n\u2206iETi(n) \u2208 O (( Bi\u2217 + \u2211\ni\u2208A\nn\nBi\n)\n\u221a\nlogn\n)\n= O ( Bi\u2217 \u221a log n )\nas required. For the problem-dependent bound we work similarly.\nR\u03c0\u00b5,i\u2217 =\nK \u2211\ni=1\n\u2206iETi(n)\n\u2208 O ( \u2211\ni\u2208A\n1\n\u2206\u0304i logn+ 1{A 6= \u2205}Bi\u2217\n\u221a\nlogn\n)\n\u2208 O ( \u2211\ni\u2208A\n1\n\u2206i logn+ 1{A 6= \u2205}Bi\u2217\n\u221a\nlogn\n)\n."}, {"heading": "C KL Techniques", "text": "Let \u00b51, \u00b5k \u2208 RK be two bandit environments as defined in the proof of Theorem 1. Here I prove the claim that\nE \u03c0 \u00b5kTk(n)\u2212 E\u03c0\u00b51Tk(n) \u2264 n\u03b5k\n\u221a\nE\u03c0\u00b51Tk(n) .\nThe result follows along the same lines as the proof of the lower bounds given by Auer et al. [1995]. Let {Ft}nt=1 be a filtration where Ft contains information about rewards and actions chosen up to time step t. So gIt,t and 1{It = i} are measurable with respect to Ft. Let P1 and Pk be the measures on F induced by bandit problems \u00b51 and \u00b5k respectively. Note that Tk(n) is a Fn-measurable random variable bounded in [0, n]. Therefore\nE \u03c0 \u00b5kTk(n)\u2212 E\u03c0\u00b51Tk(n)\n(a) \u2264 n sup A |P1(A) \u2212 P2(A)|\n(b) \u2264 n \u221a 1\n2 KL(P1, Pk) ,\nwhere the supremum in (a) is taken over all measurable sets (this is the total variation distance) and (b) follows from Pinsker\u2019s inequality. It remains to compute the KL divergence. Let P1,t and Pk,t be the conditional measures on the tth reward. By the chain rule for the KL divergence we have\nKL(P1, Pk) = n \u2211\nt=1\nEP1 KL(P1,t, Pk,t) (a) = 2\u03b52k\nn \u2211\nt=1\nEP11{It = k} = 2\u03b52kE\u03c0\u00b51Tk(n) ,\nwhere (a) follows by noting that if It 6= k, then the distribution of the rewards at time step t is the same for both bandit problems \u00b51 and \u00b5k. For It = k we have the difference in means is (\u00b5k)k\u2212(\u00b51)k = \u03b5k and since the distributions are Gaussian the KL divergence is 2\u03b52k. For Bernoulli random noise the KL divergence is also \u0398(\u03b52k) provided (\u00b5k)k \u2248 (\u00b51)k \u2248 1/2 and so a similar proof works for this case. See the work by Auer et al. [1995] for an example."}, {"heading": "D Adversarial Bandits", "text": "In the adversarial setting I obtain something similar. First I introduce some new notation. Let gi,t \u2208 [0, 1] be the gain/reward from choosing action i at time step t. This is chosen in an arbitrary way by the adversary with gi,t possibly even dependent on the actions of the learner up to time step\nt. The regret difference between the gains obtained by the learner and those of the best action in hindsight.\nR\u03c0g = max i\u2208{1,...,K} E\n[\nn \u2211\nt=1\ngi,t \u2212 gIt,t ] .\nI make the most obvious modification to the Exp3-\u03b3 algorithm, which is to bias the prior towards the special action and tune the learning rate accordingly. The algorithm accepts as input the prior \u03c1 \u2208 [0, 1]K , which must satisfy \u2211i \u03c1i = 1, and the learning rate \u03b7.\n1: Input: K , \u03c1 \u2208 [0, 1]K , \u03b7 2: wi,0 = \u03c1i for each i 3: for t \u2208 1, . . . , n do 4: Let pi,t =\nwi,t\u22121\u2211 K i=1 wi,t\u22121\n5: Choose action It = i with probability pi,t and observe gain gIt,t 6: \u2113\u0303t,i =\n(1\u2212gt,i)1{It=i} pi,t\n7: wi,t = wi,t\u22121 exp ( \u2212\u03b7\u2113\u0303t,i )\n8: end for\nAlgorithm 2: Exp3-\u03b3\nThe following result follows trivially from the standard proof.\nTheorem 8 (Bubeck and Cesa-Bianchi [2012]). Let \u03c0 be the strategy determined by Algorithm 2, then\nR\u03c0g \u2264 \u03b7Kn+ 1\n\u03b7 log\n1\n\u03c1i\u2217 .\nCorollary 9. If \u03c1 is given by\n\u03c1i =\n{ exp ( \u2212 B 2 1\n4Kn\n)\nif i = 1\n(1\u2212 \u03c11)/(K \u2212 1) otherwise\nand \u03b7 = B1/(2Kn), then\nR\u03c0g \u2264 { B1 if i\u2217 = 1 B1 2 + 2Kn B1 log ( 4Kn(K\u22121) B2\n1\n)\notherwise .\nProof. The proof follows immediately from Theorem 8 by noting that for i\u2217 6= 1 we have\nlog 1\n\u03c1i\u2217 = log\n\n K \u2212 1 1\u2212 exp (\n\u2212 B214Kn )\n\n\n\u2264 log ( 4Kn(K \u2212 1) B21 )\nas required."}, {"heading": "E Concentration", "text": "The following straight-forward concentration inequality is presumably well known and the proof of an almost identical result is available by Boucheron et al. [2013], but an exact reference seems hard to find.\nTheorem 10. Let X1, X2, . . . , Xn be independent and 1-subgaussian, then\nP\n\n\n \u2203t \u2264 n : 1 t \u2211\ns\u2264t\nXs \u2265 \u03b5\nt\n\n\n\n\u2264 exp ( \u2212 \u03b5 2\n2n\n)\n.\nProof. Since Xi is 1-subgaussian, by definition it satisfies\n(\u2200\u03bb \u2208 R) E [exp (\u03bbXi)] \u2264 exp ( \u03bb2/2 ) .\nNow X1, X2, . . . are independent and zero mean, so by convexity of the exponential function exp(\u03bb\n\u2211t s=1 Xs) is a sub-martingale. Therefore if \u03b5 > 0, then by Doob\u2019s maximal inequality\nP\n{\n\u2203t \u2264 n : t \u2211\ns=1\nXs \u2265 \u03b5 }\n= inf \u03bb\u22650 P\n{ \u2203t \u2264 n : exp ( \u03bb t \u2211\ns=1\nXs\n) \u2265 exp (\u03bb\u03b5) }\n\u2264 inf \u03bb\u22650 exp\n(\n\u03bb2n\n2 \u2212 \u03bb\u03b5\n)\n= exp\n(\n\u2212 \u03b5 2\n2n\n)\nas required."}], "references": [{"title": "Further optimal regret bounds for thompson sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["Stephane Boucheron", "Gabor Lugosi", "Pascal Massart"], "venue": "OUP Oxford,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Bandits games and clustering foundations", "author": ["S\u00e9bastien Bubeck"], "venue": "PhD thesis, Universite\u0301 des Sciences et Technologie de Lille-Lille I,", "citeRegEx": "Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Bubeck.", "year": 2010}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems. Foundations and Trends in Machine Learning", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Now Publishers Incorporated,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Kullback\u2013Leibler upper confidence bounds for optimal sequential allocation", "author": ["Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Odalric-Ambrym Maillard", "R\u00e9mi Munos", "Gilles Stoltz"], "venue": "The Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi"], "venue": null, "citeRegEx": "Cesa.Bianchi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi.", "year": 2006}, {"title": "Regret to the best vs. regret to the average", "author": ["Eyal Even-Dar", "Michael Kearns", "Yishay Mansour", "Jennifer Wortman"], "venue": "Machine Learning,", "citeRegEx": "Even.Dar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2008}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["Marcus Hutter", "Jan Poland"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "Prediction strategies without loss", "author": ["Michael Kapralov", "Rina Panigrahy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kapralov and Panigrahy.,? \\Q2011\\E", "shortCiteRegEx": "Kapralov and Panigrahy.", "year": 2011}, {"title": "The pareto regret frontier", "author": ["Wouter M Koolen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koolen.,? \\Q2013\\E", "shortCiteRegEx": "Koolen.", "year": 2013}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Optimally confident UCB : Improved regret for finite-armed bandits", "author": ["Tor Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "On the prior sensitivity of thompson sampling", "author": ["Che-Yu Liu", "Lihong Li"], "venue": "arXiv preprint arXiv:1506.03378,", "citeRegEx": "Liu and Li.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Li.", "year": 2015}, {"title": "Exploiting easy data in online optimization", "author": ["Amir Sani", "Gergely Neu", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Concentration The following straight-forward concentration inequality is presumably well known and the proof of an almost identical result is available by Boucheron et al", "author": ["E required"], "venue": null, "citeRegEx": "required.,? \\Q2013\\E", "shortCiteRegEx": "required.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006].", "startOffset": 151, "endOffset": 171}, {"referenceID": 9, "context": "The earliest work seems to be by Hutter and Poland [2005] where it is shown that the learner can assign a prior weight to each action and pays a worst-case regret of O( \u221a\u2212n log \u03c1i) for expert i where \u03c1i is the prior belief in expert i and n is the horizon.", "startOffset": 33, "endOffset": 58}, {"referenceID": 9, "context": "The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant regret with respect to a single action while suffering minimally on the remainder. The problem was studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe the pareto regret frontier when K = 2.", "startOffset": 152, "endOffset": 378}, {"referenceID": 9, "context": "The uniform regret is obtained by choosing \u03c1i = 1/K , which leads to the well-known O( \u221a n logK) bound achieved by the exponential weighting algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant regret with respect to a single action while suffering minimally on the remainder. The problem was studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe the pareto regret frontier when K = 2. Other related work (also in the experts setting) is where the objective is to obtain an improved regret against a mixture of available experts/actions [Even-Dar et al., 2008, Kapralov and Panigrahy, 2011]. In a similar vain, Sani et al. [2014] showed that algorithms for prediction with expert advice 1", "startOffset": 152, "endOffset": 720}, {"referenceID": 16, "context": "In the bandit setting I am only aware of the work by Liu and Li [2015] who study the effect of the prior on the regret of Thompson sampling in a special case.", "startOffset": 53, "endOffset": 71}, {"referenceID": 7, "context": "Although I was not able to derive a matching upper bound in this case, a simple modification of the Exp-\u03b3 algorithm [Bubeck and Cesa-Bianchi, 2012] leads to an algorithm with R 1 \u2264 B1 and R k .", "startOffset": 116, "endOffset": 147}, {"referenceID": 6, "context": "2 Preliminaries I use the same notation as Bubeck and Cesa-Bianchi [2012]. Define Ti(t) to be the number of times action i has been chosen after time step t and \u03bc\u0302i,s to be the empirical estimate of \u03bci from the first s times action i was sampled.", "startOffset": 43, "endOffset": 74}, {"referenceID": 2, "context": "This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al.", "startOffset": 62, "endOffset": 89}, {"referenceID": 15, "context": "This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al.", "startOffset": 101, "endOffset": 118}, {"referenceID": 4, "context": "This particular B is witnessed up to constant factors by MOSS [Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al., 2002], which suffers Rucb i \u2208 \u03a9( \u221a nK logn).", "startOffset": 132, "endOffset": 151}, {"referenceID": 3, "context": "E\u03bc1Tk(n) , where (a) follows from standard entropy inequalities and a similar argument as used by Auer et al. [1995] (details given in Appendix C), (b) since k 6= 1 and E\u03bc1T1(n) + E\u03bc1Tk(n) \u2264 n, and (c) by Eq.", "startOffset": 98, "endOffset": 117}, {"referenceID": 2, "context": "The algorithm is a generalisation MOSS [Audibert and Bubeck, 2009] with two modifications.", "startOffset": 39, "endOffset": 66}, {"referenceID": 15, "context": "On Logarithmic Regret In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problemdependent regret in terms of the minimum gap [Lattimore, 2015].", "startOffset": 160, "endOffset": 177}, {"referenceID": 2, "context": "The above proof may be simplified in the special case that B is uniform where we recover the minimax regret of MOSS, but with perhaps a simpler proof than was given originally by Audibert and Bubeck [2009]. On Logarithmic Regret In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problemdependent regret in terms of the minimum gap [Lattimore, 2015].", "startOffset": 179, "endOffset": 206}, {"referenceID": 3, "context": "Specifically, UCB by Auer et al. [2002] satisfies R \u03bc,i \u2208 O (", "startOffset": 21, "endOffset": 40}, {"referenceID": 14, "context": "(6) and is asymptotically order-optimal [Lai and Robbins, 1985].", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": ", which includes (for example) KL-UCB [Capp\u00e9 et al., 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 38, "endOffset": 58}, {"referenceID": 15, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 130, "endOffset": 147}, {"referenceID": 2, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 156, "endOffset": 183}, {"referenceID": 0, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].", "startOffset": 47, "endOffset": 113}, {"referenceID": 0, "context": ", 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009]. A Note on Constants The constants in the statement of Theorem 2 can be improved by carefully tuning all thresh-holds, but the proof would grow significantly and I would not expect a corresponding boost in practical performance. In fact, the reverse is true, since the \u201cweak\u201d bounds used in the proof would propagate to the algorithm. Also note that the 4 appearing in the square root of the unbalanced MOSS algorithm is due to the fact that I am not assuming rewards are bounded in [0, 1] for which the variance is at most 1/4. It is possible to replace the 4 with 2+ \u03b5 for any \u03b5 > 0 by changing the base in the peeling argument in the proof of Lemma 4 as was done by Bubeck [2010] and others.", "startOffset": 47, "endOffset": 867}], "year": 2015, "abstractText": "Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least \u03a9(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.", "creator": "LaTeX with hyperref package"}}}