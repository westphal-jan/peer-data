{"id": "1511.00971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2015", "title": "Data Stream Classification using Random Feature Functions and Novel Method Combinations", "abstract": "In this scenario, Hoeffding Trees is an established method of classification. There are several extensions, including powerful ensemble configurations such as online configurations and the use of dead ends. The nearest $k $neighbors are also a popular choice, with most extensions addressing the inherent performance limitations of a potentially infinite stream.", "histories": [["v1", "Tue, 3 Nov 2015 16:29:57 GMT  (1653kb,D)", "http://arxiv.org/abs/1511.00971v1", "20 pages, journal"]], "COMMENTS": "20 pages, journal", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["diego marr\\'on", "jesse read", "albert bifet", "nacho navarro"], "accepted": false, "id": "1511.00971"}, "pdf": {"name": "1511.00971.pdf", "metadata": {"source": "CRF", "title": "Data Stream Classification using Random Feature Functions and Novel Method Combinations", "authors": ["Diego Marr\u00f3n", "Jesse Read", "Albert Bifet", "Nacho Navarro"], "emails": ["dmarron@ac.upc.edu", "jesse.read@aalto.fi", "albert.bifet@telecom-paristech.fr", "nacho@ac.upc.edu"], "sections": [{"heading": null, "text": "Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream.\nAt the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyperparameter options and initial conditions to be considered an effective \u2018off-theshelf\u2019 data-streams solution.\nIn this work, we look at combinations of Hoeffding-trees, nearest neighbour, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power.\nWe further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability.\nOur empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification.\nKeywords: Data Stream Mining, Big Data, Classification, GPUs."}, {"heading": "1. Introduction", "text": "There is a trend towards working with big and dynamic data sources. This tendency is clear both in real world applications and the academic literature.\nEmail addresses: dmarron@ac.upc.edu (Diego Marro\u0301n), jesse.read@aalto.fi (Jesse Read), albert.bifet@telecom-paristech.fr (Albert Bifet), nacho@ac.upc.edu (Nacho Navarro)\nPreprint submitted to Elsevier November 4, 2015\nar X\niv :1\n51 1.\n00 97\n1v 1\n[ cs\n.L G\n] 3\nN ov\nMany modern data sources are not only dynamic but often generated at high speed and must be classified in real time. Such contexts can be found in sensor applications (e.g., tracking and activity monitoring), demand prediction (e.g., of electricity), manufacturing processes, robotics, email, news feeds, and social networks. Real-time analysis of data streams is becoming a key area of data mining research as the number of applications in this area grows.\nThe requirements for a classifier in a data stream are to\n\u2022 Be able to make a classification at any time\n\u2022 Deal with a potentially infinite number of examples\n\u2022 Access each example in the stream just once\nThese requirements can in fact be met by variety of learning schemes, including even batch learners (e.g., [1]), where batches are constantly gathered over time, and newer models replace older ones as memory fills up. Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4]. Support for these options is given by large-scale empirical comparisons [5], where it is also found that methods such as naive Bayes and stochastic gradient descent-based (SGD) are relatively poor performers.\nClassification in data streams is a major area of research, in which Hoeffding trees have long been a favoured method. The main contribution of this paper is to show that random feature function can be leveraged by other algorithms to obtain similar or even improved performance over tree-based methods.\nWith the recent popularity of Deep Learning (DL) methods we also want to test how a random feature in the form of random projection layer performs on Deep Neural Networks (DNNs).\nDL aims for a better data representation at multiple layers of abstraction, and for each layer the network needs to be fine-tuned. In classification, a common algorithm to fine-tune the network is the SGD which tries to minimize the error at the output layer using an objective function, such as Mean Squared Error (MSE). A Gradient vector is used to back-propagate the error to previous layers. This gradient nature of the algorithm makes it suitable to be trained incrementally in batches of size one, similar to how incremental training is done. Unfortunately, DNN are very sensitive to hyper-parameters such as learning rate (\u03b7), momentum (\u00b5), number of number neurons per level, or the number of levels. It is then not straight forward to provide an of-the-shelf method for data streams.\nPropagation between layers is usually done in the form of matrix-vector or matrix-matrix multiplications, which are computational intensive operation. Often hardware accelerators such as FPGAs or GPUs are used to accelerate the calculations. Despite some efforts, acceleration of HT and kNN algorithms for data streams on the GPUs are has some limitations. We talk briefly about this in Section 2.\nIn recent years, Extreme Learning Machines [6] (ELMs) have emerged as a popular framework in Machine Learning. ELMs are a type of feed-forward\nneural networks characterized by a random initialization of their hidden layer, combined with a fast training algorithm. Our random feature method is based on this approach.\nWe made use of the MOA (Massive Online Analysis) framework [7], a software environment for implementing algorithms and running experiments for online learning from data streams in Java. It implements a large number of modern methods for classification in streams, including HT, kNN, and SGDbased methods. We make use of MOA\u2019s extensive library of methods to form novel combinations with these methods and further employ an extremely rapid preprocessing technique of projecting the input into a new space via random feature functions (similar to ELMs). We then took the methods purely related to Neural Networks (those which proved most promising under random projections) and implemented them using NVIDIA GPUs and CUDA 7.0; comparing performance to the methods in MOA.\nThis paper is organized as follows: Section 2 introduces related work on tree based approaches, neural networks, and data streams on GPU. We discuss the use of random features in Sections 3 and 4 for HT/kNN methods and neural networks respectively. We first present the evaluation of tree-based methods in Section 5 and later in Section 6 we extend the SGD method in the form of DNNs, using different activation functions. We finally conclude the paper in Section 7."}, {"heading": "2. Related Work", "text": "Hoeffding trees [2] are state-of-the-art in classification for data streams and they predict by choosing the majority class at each leaf. However, these trees may be conservative at first and in many situations naive Bayes method outperforms the standard Hoeffding tree initially, although it is eventually overtaken [8]. A proposed hybrid adaptive method (by [8]) is a Hoeffding tree with naive Bayes at the leaves, i.e., returning a naive Bayes prediction at the leaves, if it has been so far more accurate overall than the majority class. Given it\u2019s widespread acceptance, this is the default in MOA, and we denote this method in the experimental Section simply as HT. In fact, the naive Bayes classification comes for free, since it can be made with the same statistics that are collected anyway by the tree.\nOther established examples include using principal component analysis (reviewed also in [9]) for this transformation, and also Restricted Boltzmann Machines (RBMs) [10]. RBMs can be seen as a probabilistic binary version of PCA, for finding higher-level feature representations. They have received widespread popularity in recent years due to their use in successful deep learning approaches. In this case, z = \u03c6(x) = f(W>x) for some non-linearity f : a sigmoid function is typical, but more recently rectified linear units (ReLUs, [11]) have fallen into favour. The weight matrix W is learned with gradient-based methods [12], and the projected output should provide a better feature representation for a neural network or any off-the-shelf method. This approach was applied to data streams\nalready in [13], but concluded that the sensitivity to hyper-parameters and initial conditions prevented good \u2018out-of-the-box\u2019 deployment in data streams.\nApproaches such as the so-called extreme learning machines (ELMs) [14] avoid tricky parametrizations by simply using random functions (indeed, ELMs are basically linear learners on top of non-linear data transformations). Despite the hidden layer weights being random , it has been proven that ELMs is still capable of universal approximation of any non-constant piecewise contiuous function [15].\nAlso an incremental version of ELMs is proposed in [16]. It starts with an small network, and new neurons are added at each step until an stopping criterion of size or residual error is reached. The difference with our incremental build is that we use one instance at time simulating they arrive in time, and we incrementally train the network. Also our number of neurons is fixed during the training, in other words, we don\u2019t add/remove any neuron during the process.\nNowadays, in 2015, it is difficult when talking about DL and DNNs not to mention GPUs. They are a massive parallel architectures providing an outstanding performance for High Performance Computing and a very good performance/watt ratio, as their architecture suits very fine to their needs of DNNs computations. Many tools includes a back-end to offload the computation to the GPU. NVIDIA has its own portal for deep learning on GPUs at https://developer.nvidia.com/deep-learning.\nGPUs has not only used to accelerate DL/DNN computations due to its performance, it has been also been used to successfully accelerate HT and ensembles. However, few works are provided in the context of data streams and GPUs.\nThe only work we are aware of regarding to HT in the context of online realtime data streams mining is[17], were the authors present a parallel implementation of HT and Random Forests for binary trees and data streams achieving goods speedups, but with limitations on the size and with high memory consumption. More generic HT implementation of Random Forests is presented in [18]. In [19] the authors introduced an open source library, available at github, to predict images labelling using random forests. The library is also tested their on a cell phone with VGA resolution in real-time with good results.\nAlso, kNN has already been successfully ported to GPUs [20]. That paper presented one of the first implementations of the \u201cbrute force\u201d kNN on GPUs, and compared with several CPU-based implementations with speedups up to teo orders of magnitude. kNN is also used in business intelligence [21] and has also its implementation on the GPU. The same way as with HT, a tool for machine learning (including kNN) is described in [22]."}, {"heading": "3. Tree Based Random Feature Functions", "text": "Transforming the feature space prior to learning and classification is an established idea in the statistical and machine learning literature [9], for example with basis (or feature-) functions. Suppose the input instance is x of length d.\nThis vector is transformed to a new space z = \u03c6(x) via function \u03c6, creating new vector z of length h. Any off-the-shelf model now treats z as if it were the input. The functions can be either chosen suitably by a domain expert, or simply chosen to achieve a more dimensioned representation of the input. Polynomials and splines are a typical choice.\nRegarding HTs with additional algorithms in the leaves (as described in Section 2), this filter can either be placed before the HT, or before the method in the leaves, or both.\nIn this paper we adapt this methodology to deal with other classifiers in a similar way, namely kNN and an SGD-based method (rather than naive Bayes) at the leaves. We denote these cases HT-kNN and HT-SGD, respectively. For example, in HT-SGD, a gradient descent learner is employed in the leaves of each tree. Similarly to HT, predictions by the kNN and an SGD-based method are only used if they are more accurate on average than the majority class."}, {"heading": "3.1. Ensembles in Data Streams", "text": "Bagging is an ensemble method used to improve the accuracy of classifier methods. Non-streaming bagging [23] builds a set of M base models, training each model with a bootstrap sample of size N created by drawing random samples with replacement from the original training set. Each base model\u2019s training set contains each of the original training example K times where P (K = k) follows a binomial distribution. This binomial distribution for large values of N tends to a Poisson(\u03bb = 1) distribution, where Poisson(\u03bb = 1)= exp(\u22121)/k!. Using this fact, Oza and Russell [24, 25] proposed Online Bagging, an online method that instead of sampling with replacement, gives each example a weight according to Poisson(1). ADWIN Bagging [26] is an adaptive version of Online Bagging that uses a change detector to decide when to discard under-performing ensemble models.\nLeveraging Bagging (LB, [3]) improves ADWIN Bagging, increasing the weights of this resampling using a larger value \u03bb to compute the value of the Poisson distribution. The Poisson distribution is used to model the number of events occurring within a given time interval. It proved very competitive.\nAgain, we can run a filter on the input instances before entering the ensemble of trees, or at the leaves. It is even possible to run the filter again on the output of an ensemble (i.e., the votes), before running an additional stacking procedure. This kind of methodology can give way to rather \u2018deep\u2019 classifiers. Figure 1 illustrates a possible setup. In this sense of multiple levels we could also call our approach deep learning. It is debatable whether decision trees can be called a deep method (their levels involve partitioning an existing feature set rather than because they simple partition a space rather than create higherlevel feature space). However, several of the methods we investigate have at least multiple levels of feature transformation, which is behind the power of most deep methods. In the following Section we investigate the empirical performance of several novel combinations based on the methodology described so far."}, {"heading": "4. Neural Networks with Random Projections for Data Streams", "text": "Data streams are potentially infinite, and so, they can evolve with time. This means the statistical distribution of the data we are interested on can change. The idea behind the random layer is to improve data localization across the space the trained layer sees. Imagine the instance is a tiny luminous point on the space, with enough random neurons acting as a mirror we hope the trained layer can capture better the data movement. The strategy used by the random projection layer es shown in Figure 2\nExcept for the fact it is never trained, the random layer is a normal layer and need its activation functions, in this work sigmoid, ReLU, ReLU incremental and a Radial Basis Function are used.\nThe sigmoid function used is the standard one, with \u03c3(x) \u2208 [\u22121, 1]:\n\u03c3(ak) = 1\n1 + e\u2212ak\nwhere ak = W > k x is the k-th activation function and W is the weight d\u00d7 h\nmatrix (d input attributes, h output features).\nReLU functions are defined as:\nzk = f(ak) = max(0, ak)\nAs stated in Section 2, ReLUs activation are very efficient as they require only a comparison. In our random projection we expect near 50% of the neurons to be active for a single instance ( the terrain of a ReLU is exemplified in Figure 3 ).\nOne variation we can do to the standard ReLU is to use the mean value of the attribute as a threshold. The mean value is calculated incrementally as instances arrive. We call this variant ReLU incremental, and is defined as:\nf(ak) = max(a\u0304k, ak)\nThe last activation function we are using is the Radial Basis Function (RBF):\n\u03c6(x) = e\u2212 (x\u2212ci)\n2\n2\u03c32\nwhere x is an input instance attribute value and ci is a random value set at initialization time. Each neuron in the random layer has its own set of ci, the length of both vector x and c are the same. So, we can see the operation (x\u2212ci)2 as the euclidean distance of the current instance to a randomly positioned center. The \u03c32 is a free parameter. A simplification we can do to this notation is:\n\u03b3 = 1\n2\u03c32\nIn our experiments we try different \u03b3 values passed at command line. We use the following notation in our experiments:\n\u03c6(x) = e\u2212\u03b3(x\u2212ci) 2\nAll matrices and vectors in our model are initialized using random numbers. Matrices are used as normal weight matrix, but the function of the vectors are activation function dependent. Usually initialization is done using random numbers with \u00b5 = 0 and \u03c3 = 1. Assuming our data range \u2208 [\u22121, 1] if we put\na Gaussian centered at one of the endpoints, half of its are of influence area if wasted and will never see a point making it harder to fill the whole space and so the discovering of points.\nIf a smaller range is used, \u03c3 \u2208 (0, 1) (note the open interval), we can improve each neuron\u2019s area of influence, as shown in figure 4b. In red the random numbers range is smaller than data range so if we put a Gaussian at the random endpoint can improve its influence are. In this example we used a Gaussian function as an example, but we the idea extends the same for activation functiones. In fact this is what we do in Section 6, specially when talking about the sigmoid neurons as they are always used at the trained layer."}, {"heading": "5. Random Feature Function Evaluation", "text": "Among the methods we investigate (e.g., HT, kNN, SGD1), different levels of filters and ensembles and possibly additional classification in the leaves (in the case of HT), there are a multitude of possible combinations. We first investigate the viability of random feature functions and their effect on the different classifiers (comparing these common methods with their \u2018filtered\u2019 versions that we denote HT-SGD, kNN-F, and SGD-F. This study led us to novel combinations, which we further compare to the benchmark methods and state-of-the-art Leveraging Bagging (LB-HT).\nThe random feature used in these evaluations are basically ELMs [14]. In this Section, we use only ReLU (explained in Section 4) as the activation function. In Section 4 we extend the functions used within the random feature to define a random projectin layer for DNNs.\nOur random feature is based on ELMs, which are defined using Radial Basis Funciontions, but instead in this section we use ReLU as the activation functions. Both functions are defined in detail in Section 4\nAll experiments in this section were carried out using the MOA framework [7] with prequential evaluation: each individual example is used to test the model\n1We refer, in this case, to the instantiation with default parameters in MOA, i.e., minimizing hinge loss\nbefore it is used for training, and from this the accuracy can be incrementally updated. We used an 8-core (3.20GHz each) desktop machine allowing up to 1 gigabyte of RAM per run (all methods were able to finish).\nTable 1 lists the data sources used. A thorough description of most of the datasets is given in [5]. Of the others, LOC1 and LOC2 are datasets dealing with classifying the location of an object in a grid of 5 \u00d7 5 and 50 \u00d7 50 pixels respectively, described in [27]. SUSY [28] has features that are kinematic properties measured by particle detectors in an accelerator. The binary class distinguishes between a signal process which produces supersymmetric particles and a background process which does not. It is one of the largest datasets in the UCI repository that we could find.\nFor the feature filter we used parameters h = 5d hidden units for kNN-F and h = 10d for SGD-F and HT-F (a decision based on the relative computational sensitivity of kNN to a larger attribute space \u2013 for LOC2 this means 25,000 attributes in the projected space for SGD-F, and half of that for kNN-F) \u2013 except where this is varied in Figure 5. For kNN we used a buffer size of 5000. For LB we specify 10 models. In other cases, the default parameters in MOA are used.\nFigure 5 displays the results of varying the relative size of the new feature space (wrt to the original feature space) on two real-world datasets. Note that the feature space is different, so even when this ratio is 1 : 1, performance may differ.\nWith regard to kNN, performance improves with more feature functions. In one of the two cases, this is sufficient to overtake kNN on the original feature space. Unfortunately, kNN is particularly sensitive to the number of attributes, so complexity becomes an issue long before other methods. The new feature space does not help the performance of HT, and in neither case does it reach the performance of HT on the original feature space. In fact, it begins to decrease again. This is because too many features makes it difficult for HT to become confident enough to split on, and may split poorly. Also, by partitioning the feature space, interaction between the features is lost. SGD reacts best to a new\nfeature space. As noticed earlier [5], SGD is a poor performer compared to HTs, however, working in a feature space of random ReLUs, SGD-F actually reaches HT performance (on SUSY, and looks promising under ELEC) with similar time complexity. Even at 1,000 times the original feature space, running time is acceptable (only a several seconds per 10,000 instances). On the other hand, the increased memory use is significant across all methods. SGD requires 1,000 times more memory in this setting.\nFrom this initial investigation we formulate several method combinations for a more extensive evaluation. Table 2 displays the final accuracy over the data stream. The first four columns represent the baselines and state-of-the-art (LB-HT), and remaining columns are a selection of new method combinations. Figure 6 gives a more detailed over-time view of the largest dataset (SUSY), with the average performance plotted over the entire stream over 100 intervals, and also the first 1/10th of the data (again, over 100 intervals). The second plot gives more of an idea about how models respond to fresh concepts. Learning new concepts is a fundamental part in data streams of adapting to concept drift.\nRegarding this experiment some of the most important observations and conclusions are as follows:\n\u2022 SGD-F (i.e., SGD with random feature functions), even in this first analysis, out-competes established methods like kNN on several datasets.\n\u2022 kNN benefits relatively less (than SGD) from the feature functions filter. This is expected, since kNN is already a non-linear learner. However, on a few datasets accuracy is 5-10 percentage points higher with the filter.\n\u2022 kNN can be used effectively in the leaves of HT instead of the default of naive Bayes. There is an additional computational cost involved, but results showed this to be highly competitive method \u2013 best equal overall in predictive performance tied with state-of-the-art LB-HT\n\u2022 HT is difficult to improve on using feature functions (at least with the ReLUs that we experimented with). Again, this can be attributed to HT being a non-linear learner. Peak accuracy is reached in relatively short space of time.\n\u2022 SGD takes longer than HT or LB-HT to reach competitive accuracy, but the gap narrows significantly with more examples (for example, under SUSY). On the largest datasets, the final average accuracy is within a percentage point \u2013 and this average includes initial poorer performance. Therefore, on particularly big data streams (which are increasingly common), HTs could find themselves increasingly challenged to stay ahead of these methods.\n\u2022 HT-SGD-F is comparable to the state of the art LB-HT on several datasets, but demonstrates more favourable running times.\n\u2022 Unlike many deep learning techniques, these random functions do not require sensitive calibration.\n\u2022 Unsurprisingly, kNN-based methods perform best on the dataset RBFD which has a drifting concept, since they automatically phase out older concepts. We did not look into detail about dealing with concept drift in this paper, but this can be dealt with by \u2018meta methods\u2019, e.g., [29].\n\u2022 Employing random feature functions as a \u2018filter\u2019 in the MOA framework is a convenient and flexible way to apply it in a range of different data-stream methods."}, {"heading": "6. GPU Extended Evaluation", "text": "In the previous Section evaluations, we noticed SGD methods have the strongest advantage from random feature functions. This added to the increasing popularity of DL methods, we elected this strategy for further investigation and experimentation in this Section. A natural choice for implementing DNNs is to use GPUs to accelerate the calculations. Our experiments were evaluated on an NVIDIA Tesla K40c with 12GB of RAM each, 15 SMX and up to 2880 simultaneous threads and CUDA 7.0.\nAnother motivation to use to GPUs the selection of the network hyper parameters by using cross-validation: for each dataset and activation functions different configurations are tested and best performing one is chosen. In turned out this was a high number of combinations and a way to accelerate the process is using GPUs.\nThe random projection layer is implemented using an standard two layers feed-forward fully connected network. The input is fed to the random layer, which is never trained, and the output from this layer is forwarded to the trained layer. In this work we use the SGD and MSE as the training algorithm and objective function respectively for the last layer.\nWe use three of the data sources from Table 1 Covertype (COVT), Electricity (ELEC), and SUSY. This way we can compare the accuracy obtained in this Section against well known state-of-the-art algorithms.\nThe initialization of each layer depends on the activation function used, we tried different random number initialization strategies and those for which we achieved the best results are summarized in Table 3. Most of the weight matrices are initialized using random numbers with mean=0 and \u03c3 = 1.0, except for the sigmoid activation function. The bias vector purpose and usage is activation function dependent.\nDifferent activation functions have been tested at the random layer: RBFgamma, Sigmoid, ReLU, incremental ReLU. Sigmoid and ReLU are used in the standard way. As we can see in Table 3 bias vector for RBF stores the gammas, in out evaluations we use \u03b3 = {0.001, 0.01, 0.1, 1.0, 10.0}. ReLU incremental used the bias vector to store the incremental mean for each output attribute. At the trained layer always, we always use the standard sigmoid as the activation function.\nThe same way as in Section 5, the network is built incrementally using prequential learning; we visit each instance only one time. This is in contrast to typical DNNs training, where instances are loaded in batches and the algorithm iterates over them given number of times and, every time the error is reduced the model is checkpointed and spread to be used.\nTable 4 summarizes the best results we obtained, and it compares them with the best results obtained in Section 5 evaluations. We choose the algorithms by accuracy, and compared the time to run against them. Configuration were chosen by cross-validation using the following parameters: \u00b5 \u2208 [0.1, 1.0] with an increment of 0.1 and a similar range for learning rate. Sizes tested: [10, 100] increment of 10, [100, 1000] increment of 100, and two more sizes: 1500, 2000.\nFor the electricity dataset random projection layer (RPL) obtained an accuracy of 85.33% using a random layer of 100 neurons and a sigmoid activation function. As we can see in Table 4 the best performing algorithm is the LB-HT which achieved a 89.8%. If compared with results at Table 2, we can see our\nmethod is the second best result, 4.47 percentage points less. In the covertype dataset evaluation RPL obtained the best result for this dataset with an accuracy of 94.59%, improving 2.39 percentage points the kNN algorithm using a ReLU activation functions.\nFinally, our RPL performed relatively poorly in the SUSY dataset using 600 random neurons. We obtained a 77.63% , 1.07 percentage points less than the LB-HT. This distance is lees than the distance obtained with the electricity algorithm, but if we rank out results with those in Table 2 we are at the sixth position.\nWith regard the time to complete, we can see the GPU is faster in all of the three datasets. For the electricity dataset RPL is 8 times faster, for the CoverType dataset 17 times faster and 3 times faster for the SUSY dataset.\nNow we detail for each dataset the activation curves, the momentum and learning rate for this figure are the same across all sizes and we used the ones for the best results to see how size affects the accuracy."}, {"heading": "6.1. Electricity Dataset", "text": "Table 5 summarizes the best results for each activation functions, and its configurations. As saw previously sigmoid activation function performed better than the others for this dataset. In the second position we find ReLU and ReLU inc activation functions which gave similar results, and slightly worse than the sigmoid. Regarding the RBF, all configurations we tried performed worse if compared to sigmoid, ReLU and ReLU inc, but very similar for the different gammas. In Figure-7 we can see how accuracy changes with different sizes we tested."}, {"heading": "6.2. CoverType Dataset", "text": "Table 6 give us the best results for the COVT dataset each activation function. We can see a similar pattern as with the ELEC evaluation, SIG, ReLU and ReLU inc performed much better than the RBFs, and all three can beat results shown in Table 2. This time the best result is obtained with the ReLU activation function at the random layer.\nIn figure 8 we can see the activation curves. Although we got the best result with ReLU, the sigmoid has a better learning curve and it is very close to he ReLU accuracy. ReLU inc has a very similar learning curve as the standard ReLU. The different RBFs for the same momentum and learning with different sizes gives very similar (if not equal) results, so we chose the lower sizes."}, {"heading": "6.3. SUSY Dataset", "text": "Table 7 shows the best results for the SUSY dataset and activation function, and figure 9 the learning curves. The most noticeable effect is sigmoid, ReLU and ReLU inc the stop learning very soon, with only 20 random neurons ReLU reached its maximum peak with 74.85%. The RBFs which performed poorly in previous evaluations, here are those with the best results.\nOne curious result we can see is that the RBFs are performing around 7x% in all 3 evaluations. Even if 2 of the 3 results are not very good, it seems they are not very sensitive to the different datasets, and somehow the results are stable across different data distributions."}, {"heading": "7. Conclusions", "text": "In this paper, we studied combinations of Hoeffding trees, nearest neighbour, and gradient descent methods adding a layer based on a random feature function filter. We found that this random layer can turn a simple gradient descent learner into a competitive method for real-time data analysis. With this first attempt we could even improve on current state-of-the-art algorithms, scoring the best and the second best results for two out of three datasets tested. Like Hoeffding Trees and nearest neighbour methods, but unlike many many other gradient descent-based methods, the random layer works well without intensive parameter tuning.\nWe successfully extended and implemented on GPUs, obtaining powerful predictive performance. This suggests that using GPUs for data stream mining is a promising research topic for obtaining new fast and adaptive machine learning methodologies.\nIn the future we intend to look for adding and pruning units incrementally in the stream over time to respond to make more efficient use of memory and adapt to drifting concepts. Also we would like to continue studying how to obtain new high scalable methods using GPUs."}, {"heading": "Acknowledgment", "text": "This work was supported in part by the Aalto University AEF research programme http://energyefficiency.aalto.fi/en/, by NVIDIA through the UPC/BSC GPU Center of Excellence, and the Spanish Ministry of Science and Technology through the TIN2012-34557.\n[1] W. Qu, Y. Zhang, J. Zhu, Q. Qiu, Mining multi-label concept-drifting data streams using dynamic classifier ensemble, in: Asian Conference on Machine Learning, Vol. 5828 of Lecture Notes in Computer Science, Springer, 2009, pp. 308\u2013321.\n[2] P. Domingos, G. Hulten, Mining high-speed data streams, in: Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp. 71\u201380.\n[3] A. Bifet, G. Holmes, B. Pfahringer, Leveraging bagging for evolving data streams, in: ECML PKDD\u201910, Springer-Verlag, Berlin, Heidelberg, 2010, pp. 135\u2013150.\n[4] A. Shaker, E. Hu\u0308llermeier, Instance-based classification and regression on data streams, in: Learning in Non-Stationary Environments, Springer New York, 2012, pp. 185\u2013201.\n[5] J. Read, A. Bifet, B. Pfahringer, G. Holmes, Batch-incremental versus instance-incremental learning in dynamic and evolving data, in: 11th Int. Symposium on Intelligent Data Analysis, 2012.\n[6] G. Huang, What are extreme learning machines? filling the gap between frank rosenblatt\u2019s dream and john von neumann\u2019s puzzle, Cognitive Computation 7 (3) (2015) 263\u2013278. doi:10.1007/s12559-015-9333-0. URL http://dx.doi.org/10.1007/s12559-015-9333-0\n[7] A. Bifet, G. Holmes, R. Kirkby, B. Pfahringer, MOA: Massive Online Analysis http://moa.cs.waikato.ac.nz/, Journal of Machine Learning Research (JMLR).\n[8] G. Holmes, R. Kirkby, B. Pfahringer, Stress-testing Hoeffding trees, in: 9th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD \u201905), 2005, pp. 495\u2013502.\n[9] T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning, Springer Series in Statistics, Springer New York Inc., New York, NY, USA, 2001.\n[10] G. Hinton, R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science 313 (5786) (2006) 504 \u2013 507.\n[11] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltzmann machines, in: Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, 2010, pp. 807\u2013814.\n[12] G. Hinton, Training products of experts by minimizing contrastive divergence, Neural Computation 14 (8) (2000) 1711\u20131800.\n[13] J. Read, F. Perez-Cruz, A. Bifet, Deep learning in multi-label data-streams, in: Symposium on Applied Computing, ACM, 2015.\n[14] G.-B. Huang, D. Wang, Y. Lan, Extreme learning machines: a survey, International Journal of Machine Learning and Cybernetics 2 (2) (2011) 107\u2013122. doi:10.1007/s13042-011-0019-y.\n[15] G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, Neural Networks, IEEE Transactions on 17 (4) (2006) 879\u2013892. doi: 10.1109/TNN.2006.875977.\n[16] G. bin Huang, M. bin Li, L. Chen, C. kheong Siew, Incremental extreme learning machine with fully complex hidden nodes (2007).\n[17] D. Marron, A. Bifet, G. D. F. Morales, Random forests of very fast decision trees on gpu for mining evolving big data streams, in: 21st European Conference on Artificial Intelligence 2014, 2014.\n[18] H. Grahn, N. Lavesson, M. H. Lapajne, D. Slat, Cudarf: A cuda-based implementation of random forests., in: H. J. Siegel, A. El-Kadi (Eds.), AICCSA, IEEE Computer Society, 2011, pp. 95\u2013101.\n[19] H. Schulz, B. Waldvogel, R. Sheikh, S. Behnke, CURFIL: random forests for image labeling on GPU, in: VISAPP 2015 - Proceedings of the 10th International Conference on Computer Vision Theory and Applications, Volume 2, Berlin, Germany, 11-14 March, 2015., 2015, pp. 156\u2013164.\n[20] V. Garcia, E. Debreuve, M. Barlaud, Fast k nearest neighbor search using gpu, in: 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2008, pp. 1\u20136.\n[21] L. Huang, Z. Liu, Z. Yan, P. Liu, Q. Cai, An implementation of high performance parallel knn algorithm based on gpu, in: Networking and Distributed Computing (ICNDC), 2012 Third International Conference on, 2012, pp. 30\u201330. doi:10.1109/ICNDC.2012.15.\n[22] D. Liu, T. Chen, S. Liu, J. Zhou, S. Zhou, O. Teman, X. Feng, X. Zhou, Y. Chen, Pudiannao: A polyvalent machine learning accelerator, in: Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS \u201915, 2015, pp. 369\u2013381.\n[23] L. Breiman, Bagging predictors, Mach. Learn. 24 (2) (1996) 123\u2013140. doi: http://dx.doi.org/10.1023/A:1018054314350.\n[24] N. C. Oza, S. J. Russell, Experimental comparisons of online and batch versions of bagging and boosting, in: KDD, 2001, pp. 359\u2013364.\n[25] N. Oza, S. Russell, Online bagging and boosting, in: Artificial Intelligence and Statistics 2001, Morgan Kaufmann, 2001, pp. 105\u2013112.\n[26] A. Bifet, G. Holmes, B. Pfahringer, R. Kirkby, R. Gavalda\u0300, New ensemble methods for evolving data streams, in: ACM SIGKDD international conference on Knowledge discovery and data mining (KDD \u201909), 2009, pp. 139\u2013148.\n[27] J. Read, J. Hollme\u0301n, A deep interpretation of classifier chains, in: Advances in Intelligent Data Analysis XIII - 13th International Symposium, IDA 2014, 2014, pp. 251\u2013262.\n[28] P. Baldi, P. Sadowski, D. Whiteson, Searching for exotic particles in highenergy physics with deep learning, Nature Communications 5 (4308).\n[29] A. Bifet, R. Gavalda\u0300, Learning from time-changing data with adaptive windowing, in: SIAM International Conference on Data Mining, 2007."}], "references": [{"title": "Mining multi-label concept-drifting data streams using dynamic classifier ensemble", "author": ["W. Qu", "Y. Zhang", "J. Zhu", "Q. Qiu"], "venue": "in: Asian Conference on Machine Learning, Vol. 5828 of Lecture Notes in Computer Science, Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "in: Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Leveraging bagging for evolving data streams", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer"], "venue": "in: ECML PKDD\u201910, Springer-Verlag, Berlin, Heidelberg", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Instance-based classification and regression on data streams", "author": ["A. Shaker", "E. H\u00fcllermeier"], "venue": "in: Learning in Non-Stationary Environments, Springer New York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch-incremental versus instance-incremental learning in dynamic and evolving data", "author": ["J. Read", "A. Bifet", "B. Pfahringer", "G. Holmes"], "venue": "in: 11th Int. Symposium on Intelligent Data Analysis", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "What are extreme learning machines? filling the gap between frank rosenblatt\u2019s dream and john von neumann\u2019s puzzle", "author": ["G. Huang"], "venue": "Cognitive Computation 7 (3) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Stress-testing Hoeffding trees", "author": ["G. Holmes", "R. Kirkby", "B. Pfahringer"], "venue": "in: 9th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD \u201905)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer Series in Statistics, Springer New York Inc., New York, NY, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science 313 (5786) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "in: Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation 14 (8) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep learning in multi-label data-streams", "author": ["J. Read", "F. Perez-Cruz", "A. Bifet"], "venue": "in: Symposium on Applied Computing, ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G.-B. Huang", "D. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics 2 (2) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.-B. Huang", "L. Chen", "C.-K. Siew"], "venue": "Neural Networks, IEEE Transactions on 17 (4) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Incremental extreme learning machine with fully complex hidden nodes", "author": ["G. bin Huang", "M. bin Li", "L. Chen", "C. kheong Siew"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Random forests of very fast decision trees on gpu for mining evolving big data streams", "author": ["D. Marron", "A. Bifet", "G.D.F. Morales"], "venue": "in: 21st European Conference on Artificial Intelligence 2014", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["H. Grahn", "N. Lavesson", "M.H. Lapajne"], "venue": "Slat, Cudarf: A cuda-based implementation of random forests., in: H. J. Siegel, A. El-Kadi (Eds.), AICCSA, IEEE Computer Society", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "CURFIL: random forests for image labeling on GPU", "author": ["H. Schulz", "B. Waldvogel", "R. Sheikh", "S. Behnke"], "venue": "in: VISAPP 2015 - Proceedings of the 10th International Conference on Computer Vision Theory and Applications, Volume 2, Berlin, Germany, 11-14 March, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast k nearest neighbor search using gpu", "author": ["V. Garcia", "E. Debreuve", "M. Barlaud"], "venue": "in: 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "An implementation of high performance parallel knn algorithm based on gpu", "author": ["L. Huang", "Z. Liu", "Z. Yan", "P. Liu", "Q. Cai"], "venue": "in: Networking and Distributed Computing (ICNDC), 2012 Third International Conference on", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Pudiannao: A polyvalent machine learning accelerator", "author": ["D. Liu", "T. Chen", "S. Liu", "J. Zhou", "S. Zhou", "O. Teman", "X. Feng", "X. Zhou", "Y. Chen"], "venue": "in: Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS \u201915", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn. 24 (2) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Experimental comparisons of online and batch versions of bagging and boosting", "author": ["N.C. Oza", "S.J. Russell"], "venue": "in: KDD", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Online bagging and boosting", "author": ["N. Oza", "S. Russell"], "venue": "in: Artificial Intelligence and Statistics 2001, Morgan Kaufmann", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "New ensemble methods for evolving data streams", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer", "R. Kirkby", "R. Gavald\u00e0"], "venue": "in: ACM SIGKDD international conference on Knowledge discovery and data mining (KDD \u201909)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A deep interpretation of classifier chains", "author": ["J. Read", "J. Hollm\u00e9n"], "venue": "in: Advances in Intelligent Data Analysis XIII - 13th International Symposium, IDA 2014", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from time-changing data with adaptive windowing", "author": ["A. Bifet", "R. Gavald\u00e0"], "venue": "in: SIAM International Conference on Data Mining", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", [1]), where batches are constantly gathered over time, and newer models replace older ones as memory fills up.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4].", "startOffset": 152, "endOffset": 158}, {"referenceID": 2, "context": "Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4].", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4].", "startOffset": 186, "endOffset": 189}, {"referenceID": 4, "context": "Support for these options is given by large-scale empirical comparisons [5], where it is also found that methods such as naive Bayes and stochastic gradient descent-based (SGD) are relatively poor performers.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "In recent years, Extreme Learning Machines [6] (ELMs) have emerged as a popular framework in Machine Learning.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Hoeffding trees [2] are state-of-the-art in classification for data streams and they predict by choosing the majority class at each leaf.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "However, these trees may be conservative at first and in many situations naive Bayes method outperforms the standard Hoeffding tree initially, although it is eventually overtaken [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "A proposed hybrid adaptive method (by [8]) is a Hoeffding tree with naive Bayes at the leaves, i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Other established examples include using principal component analysis (reviewed also in [9]) for this transformation, and also Restricted Boltzmann Machines (RBMs) [10].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Other established examples include using principal component analysis (reviewed also in [9]) for this transformation, and also Restricted Boltzmann Machines (RBMs) [10].", "startOffset": 164, "endOffset": 168}, {"referenceID": 9, "context": "In this case, z = \u03c6(x) = f(W>x) for some non-linearity f : a sigmoid function is typical, but more recently rectified linear units (ReLUs, [11]) have fallen into favour.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "The weight matrix W is learned with gradient-based methods [12], and the projected output should provide a better feature representation for a neural network or any off-the-shelf method.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "already in [13], but concluded that the sensitivity to hyper-parameters and initial conditions prevented good \u2018out-of-the-box\u2019 deployment in data streams.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Approaches such as the so-called extreme learning machines (ELMs) [14] avoid tricky parametrizations by simply using random functions (indeed, ELMs are basically linear learners on top of non-linear data transformations).", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Despite the hidden layer weights being random , it has been proven that ELMs is still capable of universal approximation of any non-constant piecewise contiuous function [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Also an incremental version of ELMs is proposed in [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "The only work we are aware of regarding to HT in the context of online realtime data streams mining is[17], were the authors present a parallel implementation of HT and Random Forests for binary trees and data streams achieving goods speedups, but with limitations on the size and with high memory consumption.", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "More generic HT implementation of Random Forests is presented in [18].", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "In [19] the authors introduced an open source library, available at github, to predict images labelling using random forests.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Also, kNN has already been successfully ported to GPUs [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "kNN is also used in business intelligence [21] and has also its implementation on the GPU.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "The same way as with HT, a tool for machine learning (including kNN) is described in [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 7, "context": "Transforming the feature space prior to learning and classification is an established idea in the statistical and machine learning literature [9], for example with basis (or feature-) functions.", "startOffset": 142, "endOffset": 145}, {"referenceID": 21, "context": "Non-streaming bagging [23] builds a set of M base models, training each model with a bootstrap sample of size N created by drawing random samples with replacement from the original training set.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "Using this fact, Oza and Russell [24, 25] proposed Online Bagging, an online method that instead of sampling with replacement, gives each example a weight according to Poisson(1).", "startOffset": 33, "endOffset": 41}, {"referenceID": 23, "context": "Using this fact, Oza and Russell [24, 25] proposed Online Bagging, an online method that instead of sampling with replacement, gives each example a weight according to Poisson(1).", "startOffset": 33, "endOffset": 41}, {"referenceID": 24, "context": "ADWIN Bagging [26] is an adaptive version of Online Bagging that uses a change detector to decide when to discard under-performing ensemble models.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "Leveraging Bagging (LB, [3]) improves ADWIN Bagging, increasing the weights of this resampling using a larger value \u03bb to compute the value of the Poisson distribution.", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "The random feature used in these evaluations are basically ELMs [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "A thorough description of most of the datasets is given in [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 25, "context": "Of the others, LOC1 and LOC2 are datasets dealing with classifying the location of an object in a grid of 5 \u00d7 5 and 50 \u00d7 50 pixels respectively, described in [27].", "startOffset": 158, "endOffset": 162}, {"referenceID": 4, "context": "As noticed earlier [5], SGD is a poor performer compared to HTs, however, working in a feature space of random ReLUs, SGD-F actually reaches HT performance (on SUSY, and looks promising under ELEC) with similar time complexity.", "startOffset": 19, "endOffset": 22}, {"referenceID": 26, "context": ", [29].", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "Sizes tested: [10, 100] increment of 10, [100, 1000] increment of 100, and two more sizes: 1500, 2000.", "startOffset": 14, "endOffset": 23}, {"referenceID": 0, "context": "[1] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] A.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyperparameter options and initial conditions to be considered an effective \u2018off-theshelf\u2019 data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbour, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification.", "creator": "LaTeX with hyperref package"}}}