{"id": "1511.01282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Factorizing LambdaMART for cold start recommendations", "abstract": "Recommendation systems are often based on point-by-point loss metrics such as the mean square error, but few items are presented to the user in real recommendation settings, and this observation has recently encouraged the use of rank-based metrics. LambdaMART is the most advanced algorithm for learning the ranking based on such a metric. Despite its success, it has no principled regulation mechanism based on empirical approaches to controlling model complexity, making it prone to overmatch.", "histories": [["v1", "Wed, 4 Nov 2015 10:49:15 GMT  (50kb)", "http://arxiv.org/abs/1511.01282v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["phong nguyen", "jun wang", "alexandros kalousis"], "accepted": false, "id": "1511.01282"}, "pdf": {"name": "1511.01282.pdf", "metadata": {"source": "CRF", "title": "Factorizing LambdaMART for cold start recommendations", "authors": ["Phong Nguyen", "Jun Wang", "Alexandros Kalousis"], "emails": ["Phong.Nguyen@unige.ch", "jwang1@expedia.com", "Alexandros.Kalousis@hesge.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n01 28\n2v 1\n[ cs\n.L G\n] 4\nN ov\n2 01\nMotivated by the fact that very often the users\u2019 and items\u2019 descriptions as well as the preference behavior can be well summarized by a small number of hidden factors, we propose a novel algorithm, LambdaMARTMatrix Factorization (LambdaMART-MF), that learns a low rank latent representation of users and items using gradient boosted trees. The algorithm factorizes lambdaMART by defining relevance scores as the inner product of the learned representations of the users and items. The low rank is essentially a model complexity controller; on top of it we propose additional regularizers to constraint the learned latent representations that reflect the user and item manifolds as these are defined by their original feature based descriptors and the preference behavior. Finally we also propose to use a weighted variant of NDCG to reduce the penalty for similar items with large rating discrepancy.\nWe experiment on two very different recommendation datasets, meta-mining and movies-users, and evaluate the performance of LambdaMART-MF, with and without regularization, in the cold start setting as well as in the simpler matrix completion setting. In both cases it outperforms in a significant manner current state of the art algorithms.\nCategories and Subject Descriptors H.4 [Information Systems Applications]: cold start, recommender systems, learning to rank"}, {"heading": "1. INTRODUCTION", "text": "Most recommendation algorithms minimize a point-wise loss function such as the mean squared error or the mean average error between the predicted and the true user preferences. For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores. Such cost functions are clearly not appropri-\nate for recommendation problems since what matters there is the rank order of the preference scores and not their absolute values, i.e. items that are very often top ranked should be highly recommended. It is only recently that recommendation methods have started using ranking-based loss functions in their optimization problems. Cofirank [13], a collaborative ranking algorithm, does maximum-margin matrix factorization by optimizing an upper bound of NDCG measure, a ranking-based loss function. However, like many recommendation algorithms, it cannot address the cold start problem, i.e. it cannot recommend new items to new users.\nIn preference learning [8] we learn the preference order of documents for a given query. Preference learning algorithm are used extensively in search engines and IR systems and optimize ranking-based loss functions such as NDCG. Probably the best known example of such algorithms is LambdaMART, [4], the state-of-the-art learning to rank algorithm. Its success is due to its ability to model preference orders using features describing side-information of the querydocument pairs in a very flexible manner. Nevertheless it is prone to overfitting because it does not come with a rigorous regularization formalization and relies instead on rather empirical approaches to protect against it, such as validation error, early stopping, restricting the size of the base level trees etc.\nIn this paper we develop a new recommendation algorithm with an emphasis on the cold start problem and the exploitation of available side-information on users and items. Our algorithm can be thought of as a variant of LambdaMART in which instead of directly learning the user-item preferences, as LambdaMART does, we first learn low rank latent factors that describe the users and the items and use them to compute the user-item preference scores; essentially our algorithm does a low-rank matrix factorization of the preferences matrix. Moreover we provide additional databased regularizers for the learned latent representations of the users and items that reflect the user and item manifolds as they are established from their side-information as well as from the preference matrix. We evaluate the performance of our algorithm on two very different recommendation applications and compare its performance to a number of baselines, amongst which LambdaMART, and demonstrate very significant performance improvements."}, {"heading": "2. PRELIMINARIES", "text": "We are given a (sparse) preference matrix, Y, of size n\u00d7m. The (i, j) non-missing entry of Y represents the preference score of the ith user for the jth item in recommendation\nproblems or the relevance score of the jth document for the ith query in learning to rank problems, the larger the value of the (i, j) entry the larger the relevance or preference is. In addition to the preference matrix, we also have the descriptions of the users and items. We denote by ci = (ci1, . . . , cid)\nT \u2208 Rd, the d-dimensional description of the ith user, and by C the n\u00d7d user description matrix, the ith row of which is given by the cTi . Similarly, we denote by dj = (dj1, . . . , djl)\nT \u2208 Rl, the l-dimensional description of the jth item and by D the m \u00d7 l item description matrix, the jth row of which is given by the dTj .\nAs already mentioned when recommending an item to a user we only care about the rank order of yij and not the actual preference score value yij . Thus in principle a preference learning algorithm does not need to predict the exact value of yij but only its rank order as this induced by the preference vector y. We will denote by rci the target rank vector of the ith user. We construct rci by ordering in a decreasing manner the ith user\u2019s non-missing preference scores over the items; its kth entry, rcik, is the rank of the kth nonmissing preference score, with the highest preference score having a rank of one. In the inverse problem, i.e. matching users to items, we will denote by rdj the target rank vector of the jth item, given by ordering the jth item\u2019s non-missing relevance scores for the users."}, {"heading": "2.1 Evaluation Metric", "text": "In real applications of preference learning, such as recommendation, information retrieval, etc, only a few top-ranked items are finally shown to the users. As a result appropriate evaluation measures for preference learning focus on the correctness of the top-ranked items. One such, very often used, metric is the Discounted Cumulative Gain (DCG)[10], which is defined as follows:\nDCG(r,y)@k = M \u2211\ni=1\n2yi \u2212 1\nlog2(ri + 1) I(ri \u2264 k) (1)\nwhere k is the truncation level at which DCG is computed and I is the indicator function which returns 1 if its argument holds otherwise 0. y is them dimensional ground truth relevance vector and r is a rank vector that we will learn. The DCG score measures the match between the given rank vector r and the rank vector of the relevance score vector y. It is easy to check that if the rank vector r correctly preserves the order induced by y then the DCG score will achieve its maximum. Due to the log in the denominator the DCG score will incur larger penalties when misplacing top items compared to than low end items, emphasizing like that the correctness of top items.\nSince the DCG score also depends on the length of the relevance vector y, it is often normalized with respect to its maximum score, resulting to what is known as the Normalized DCG (NDCG), defined as:\nNDCG(y, y\u0302)@k = DCG(r(y\u0302),y)@k\nDCG(r(y),y)@k (2)\nr(\u00b7) is a rank function that outputs the rank vector, in decreasing order, of its input argument vector. Thus the vector r(y) is the rank vector of the ground truth relevance vector y and r(y\u0302) is the rank vector of the predicted relevance vector y\u0302 provided by the learned model. With normalization, the value of NDCG ranges from 0 to 1, the larger the better.\nIn this paper, we will also use this metric as our evaluation metric."}, {"heading": "2.2 LambdaMART", "text": "The main difficulty in learning preferences is that rank functions are not continuous and have combinatorial complexity. Thus most often instead of the rank of the preference scores the pairwise order constraints over the items\u2019 preferences are used. LambdaMART is one of the most popular algorithms for preference learning which follows exactly this idea, [4]. Its optimization problem relies on a distance distribution measure, cross entropy, between a learned distribution1 that gives the probability that item j is more relevant than item k from the true distribution which has a probability mass of one if item i is really more relevant than item j and zero otherwise. The final loss function of LambdaMart defined over all users i and overall the respective pairwise preferences for items j, k, is given by:\nL(Y, Y\u0302) =\nn \u2211\ni=1\n\u2211\n{jk}\u2208Z\n|\u2206NDCGijk | log(1 + e \u2212\u03c3(y\u0302ij\u2212y\u0302ik)) (3)\nwhere Z is the set of all possible pairwise preference constraints such that in the ground truth relevance vector holds yij > yik, and \u2206NDCG i jk is given by:\n\u2206NDCGijk = NDCG(r(yi), r(y\u0302i))\u2212NDCG(r(y jk i ), r(y\u0302i))\nwhere yjki is the same as the ground truth relevance vector yi except that the values of yij and yik are swapped. This is also equal to the NDCG difference that we get if we swap the y\u0302ij , y\u0302ik, estimates. Thus the overall loss function of LambdaMART eq 3 is the sum of the logistic losses on all pairwise preference constraints weighted by the respective NDCG differences. Since the NDCG measure penalizes heavily the error on the top items, the loss function of LambdaMART has also the same property. LambdaMART minimizes its loss function with respect to all y\u0302ij , y\u0302ik, and its optimization problem is:\nmin Y\u0302 L(Y, Y\u0302) (4)\n[14] have shown empiricially that solving this problem also optimizes the NDCG metric of the learned model. The partial derivative of LambdaMART\u2019s loss function with respect to the estimated scores y\u0302ij is\n\u2202L(Y, Y\u0302)\n\u2202y\u0302ij = \u03bbij =\n\u2211\n{k|jk}\u2208Z\n\u03bb i jk \u2212\n\u2211\n{k|kj}\u2208Z\n\u03bb i kj (5)\nand \u03bbijk is given by:\n\u03bb i jk =\n\u2212\u03c3\n1 + e\u03c3(y\u0302ij\u2212y\u0302ik) | \u25b3NDCG (y\u0302ij , y\u0302ik)| (6)\nWith a slight abuse of notation below we will write \u2202L(yij, \u02c6yij)\n\u2202y\u0302ij\ninstead of \u2202L(Y,Y\u0302) \u2202y\u0302ij , to make explicit the dependence of the partial derivative only on yij , y\u0302ij due to the linearity of L(Y, Y\u0302). LambdaMART uses Multiple Additive Regression Trees (MART) [7] to solve its optimization problem. It does so\n1This learned distribution is generated by the sigmoid function P ijk = 1\n1+e \u03c3(y\u0302ij\u2212y\u0302ik)\nof the estimated preferences y\u0302ij , y\u0302ik.\nthrough a gradient descent in the functional space that generates preference scores from item and user descriptions, i.e. y\u0302ij = f(ci,dj), where the update of the preference scores at the t step of the gradient descent is given by:\ny\u0302 (t) ij = y\u0302 (t\u22121) ij \u2212 \u03b7\n\u2202L(yij , y\u0302 (t\u22121) ij )\n\u2202y\u0302 (t\u22121) ij\n(7)\nor equivalently:\nf (t)(ci,dj) = f (t\u22121)(ci,dj)\u2212 \u03b7 \u2202L(yij , f\n(t\u22121)(ci,dj))\n\u2202f (t\u22121)(ci,dj) (8)\nwhere \u03b7 is the learning rate. We terminate the gradient descent when we reach a given number of iterations T or when the validation loss NDCG starts to increase. We approxi-\nmate the derivative \u2202L(yij,y\u0302\n(t\u22121) ij )\n\u2202y\u0302 (t\u22121) ij\nby learning at each step t\na regression tree h(t)(c,d) that fits it by minimizing the sum of squared errors. Thus at each update step we have\nf (t)(ci,dj) = f (t\u22121)(ci,dj) + \u03b7h (t)(ci,dj) (9)\nwhich if we denote by \u03b3tk the prediction of the kth terminal node of the h(t) tree and by htk the respective partition of the input space, we can rewrite as:\nf (t)(ci,dj) = f (t\u22121)(ci,dj) + \u03b7\u03b3tkI((c,d) \u2208 htk) (10)\nwe can further optimize over the \u03b3tk values to minimize the loss function of eq 3 over the instances of each htk partition using Newton\u2019s approximation. The final preference estimation function is given by:\ny\u0302 = f(c,d) = T \u2211\nt=1\n\u03b7h (t)(c,d) (11)\nor\ny\u0302 = f(c,d) =\nT \u2211\nt=1\n\u03b7\u03b3tkI((c,d) \u2208 htk) (12)\nLambdaMart is a very effective algorithm for learning to rank problems, see e.g [5, 6]. It learns non-linear relevance scores, y\u0302ij , using gradient boosted regression trees. The number of the parameters it fits is given by the number of available preference scores (this is typically some fraction of n \u00d7 m); there is no regularization on them to prevent overfitting. The only protection against overfitting can come from rather empirical approaches such as constraining the size of the regression trees or by selecting learning rate \u03b7."}, {"heading": "3. FACTORIZED LAMBDA-MART", "text": "In order to address the rather ad-hoc approach of LambdaMART to prevent overfitting we propose here a factorized variant of it that does regularization in a principled manner. Motivated by the fact that very often the users\u2019 and the items\u2019 descriptions and their preference relations can be well summarized by a small number of hidden factors we learn a low rank hidden representation of users and items using gradient boosted trees, MART. We define the relevance score as the inner product of the new representation of the users and items which has the additional advantage of introducing low rank structural regularization in the learned preference matrix.\nConcretely, we define the relevance score of the ith user and jth item by y\u0302ij = u T i vj , where ui and vj are the rdimensional user and item latent descriptors. We denote by U : n\u00d7 r and V : m\u00d7 r the new representation matrices of users and items. The dimensionality of r is a small number, r << min(n,m). The loss function of eq 3 now becomes:\nLMF (Y, U\u0302, V\u0302) = n\u2211\ni=1\n\u2211\n{jk}\u2208Z\n| \u25b3NDCG | log(1 + e \u2212\u03c3(u\u0302i(v\u0302j\u2212v\u0302k))) (13)\nThe partial derivatives of this lost function with respect to u\u0302i, v\u0302j , are given by:\n\u2202LMF (Y, U\u0302, V\u0302)\n\u2202u\u0302i =\nm \u2211\nj=1\n\u2202LMF (Y, Y\u0302)\n\u2202y\u0302ij\n\u2202y\u0302ij \u2202u\u0302i =\nm \u2211\nj=1\n\u03bb i j\n\u2202y\u0302ij \u2202u\u0302i (14)\n\u2202LMF (Y, U\u0302, V\u0302)\n\u2202v\u0302j =\nn \u2211\ni=1\n\u2202LMF (Y, Y\u0302)\n\u2202y\u0302ij\n\u2202y\u0302ij \u2202v\u0302i =\nn \u2211\ni=1\n\u03bb i j\n\u2202y\u0302ij \u2202vj (15)\nNote that the formulation we give in equation 13 is very similar to those used in matrix factorization algorithms. Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.\nSimilar to LambdaMART we will seek a function f that will optimize the LMF loss function. To do so we will learn functions of the latent profiles of the users and items from their side information. We will factorize f(c,d) by\nf(c,d) = fu(c) T fv(d) = u T v = y\u0302\nwhere fu : C \u2192 U is a learned user function that gives us the latent factor representation of user from his/her side information descriptor c; fv : D \u2192 V is the respective function for the items. We will follow the LambdaMART approach described previously and learn an ensemble of trees for each one of the fu and fv functions. Concretely:\nu\u0302i = fu(ci) =\nT \u2211\nt=1\n\u03b7h (t) u (ci) (16)\nv\u0302j = fv(dj) = T \u2211\nt=1\n\u03b7h (t) v (dj) (17)\nUnlike standard LambdaMART the trees we will learn are multi-output regression trees, predicting the complete latent profile of users or items. Now at each step t of the gradient descent we learn the h (t) u (c) and h (t) v (d) trees that fit the negative of the partial derivatives given at equations 14, 15. We learn the trees by greedily optimizing the sum of squared errors over the over the dimensions of the partial gradients. The t gradient descent step for ui is given by:\nu (t) i = u (t\u22121) i \u2212\nm \u2211\nj=1\n\u2202LMF (Y, Y\u0302)\n\u2202y\u0302ij\n\u2223 \u2223 \u2223\ny\u0302ij=u\u0302 (t\u22121)T i v\u0302 (t\u22121) j\n\u2202u\u0302Ti v\u0302 (t\u22121) j\n\u2202u\u0302i\nand for vj by:\nv (t) j = v (t\u22121) j \u2212\nn \u2211\ni=1\n\u2202LMF (Y, Y\u0302)\n\u2202y\u0302ij\n\u2223 \u2223 \u2223\ny\u0302ij=u\u0302 (t\u22121)T i v\u0302 (t\u22121) j\n\u2202u\u0302 (t\u22121)T i v\u0302j\n\u2202v\u0302j\nThe functions of each step are now:\nf (t) u (c) = f (t\u22121) u (c) + \u03b7h (t) u (c) (18)\nf (t) v (d) = f (t\u22121) v (d) + \u03b7h (t) v (d) (19)\nwhich give rise to the final form functional estimates that we already gave in equations 16 and 17 respectively. Optimizing for both ui and vj at each step of the gradient descent results to a faster convergence, than first optimizing for one while keeping the other fixed and vice versa. We will call the resulting algorithm LambdaMARTMatrix Factorization and denote it by LM-MF."}, {"heading": "4. REGULARIZATION", "text": "In this section, we will describe a number of different regularization methods to constraint in a meaningful manner the learning of the user and item latent profiles. We will do so by incorporating different regularizers inside the gradient boosting tree algorithm to avoid overfitting during the learning of the fu and fv functions."}, {"heading": "4.1 Input-Output Space Regularization", "text": "LambdaMART-MF learns a new representation of users and items. We will regularize these representations by constraining them by the geometry of the user and item spaces as this is given by the c and d descriptors respectively. Based on these descriptors we will define user similarities and item similarities, which we will call input-space similarities in order to make explicit that they are compute on the sideinformation vectors describing the users and the items. In addition to the input-space similarities we will also define what we will call output space similarity which will reflect the similarities of users(items) according to the respective similarities of their preference vectors. We will regularize the learned latent representations by the input/output space similarities constraining the former to follow the latter.\nTo define the input space similarities we will use the descriptors of users and items. Concretely given two users ci and cj we measure their input space similarity sUin(ci, cj) using the heat kernel over their descriptors as follows:\nsUin(ci, cj ;\u03c3) = e \u2212\u03c3||ci\u2212cj ||\n2\n(20)\nwhere \u03c3 is the inverse kernel width of the heat kernel; we set its value to the squared inverse average Euclidean distance of all the users in the C space, i.e. \u03c3 = (( 1\nn\n\u2211\nij ||ci\u2212cj ||) 2)\u22121.\nBy applying the above equation over all user pairs, we get the SUin : n \u00d7 n user input similarity matrix. We will do exactly the same to compute the SVin : m \u00d7 m item input similarity matrix using the item descriptors D.\nTo define the output space similarities we will use the preference vectors of users and items. Given two users i and j and their preference vectors yi\u00b7 and yj\u00b7, we will measure their output space similarity sUout(yi\u00b7,yj\u00b7) using NDCG@k since this is the metric that we want to optimize. To define the similarities, we first compute the NDCG@k on the (yi\u00b7,yj\u00b7) pair as well as on the (yj\u00b7,yi\u00b7), because the NDCG@k is not symmetric, and compute the distance dNDCG@k(yi\u00b7,yj\u00b7) between the two preference vectors as the average of the two previous NDCG@k measures which we have subtracted from one:\ndNDCG@k(yi\u00b7,yj\u00b7) = 1\n2 ((1\u2212 NDCG@k(yi\u00b7,yj\u00b7)) +(1\u2212 NDCG@k(yj\u00b7,yi\u00b7)))\nWe define finally the output space similarity sUout(yi\u00b7,yj\u00b7) by the exponential of the negative distance:\nsUout(yi\u00b7,yj\u00b7) = e \u2212dNDCG@k(yi\u00b7,yj\u00b7) (21)\nThe resulting similarity measure gives high similarity to preference vectors that are very similar in their top-k elements, while preference vectors which are less similar in their top-k elements will get much lower similarities. We apply this measure over all user preference vector pairs to get the SUout : n \u00d7 n user output similarity matrix. We do the same for items using now the y\u00b7i and y\u00b7j preference vectors for each ijth item pair to get the SVout : m\u00d7m item output similarity matrix.\nTo regularize the user and item latent profiles, we will use graph laplacian regularization and force them to reflect the manifold structure of the users and items as these are given by the input and outpout space similarity matrices. Concretely, we define the user and item regularizers RU and RV as follows:\nRU = \u00b51||U\u0302 T LUinU\u0302|| 2 F + \u00b52||U\u0302 T LUoutU\u0302|| 2 F (22) RV = \u00b51||V\u0302 T LVinV\u0302|| 2 F + \u00b52||V\u0302 T LVoutV\u0302|| 2 F (23)\nwhere the four laplacian matrices LUin , LUout , LVin and LVout are defined as L = D\u2212 S where Dii = \u2211 j Sij and S are the corresponding similarity matrices. \u00b51 and \u00b52 are regularization parameters that control the relative importance of the input and output space similarities respectively."}, {"heading": "4.2 Weighted NDCG Cost", "text": "In addition to the graph laplacian regularization over the latent profiles, we also provide a soft variant of the NDCG loss used in LambdaMART. Recall that in NDCG the loss is determined by the pairwise difference incurred if we exchange the position of two items j and k for a given user i. This loss can be unreasonably large even if the two items are similar to each other with respect to the similarity measures defined above. A consequence of such large penalties will be a large deviance of the gradient boosted trees under which similar items will not anymore fall in the same leaf node. To alleviate that problem we introduce a weighted NDCG difference which takes into account the items\u2019 input and output similarities, which we define as follows:\nSV = \u00b51SVin + \u00b52SVout (24)\n\u25b3WNDCGijk = \u25b3NDCG i jk(1\u2212 sVjk ) (25)\nUnder the weighted variant if two items j and k are very similar the incurred loss will be by construction very low leading to a smaller loss and thus less deviance of the gradient boosted trees for the two items."}, {"heading": "4.3 Regularized LambdaMART-MF", "text": "By combing the input-output space regularization and the weighted NDCG with LM-MF given in equation 13 we obtain the regularised LambdaMART matrix factorization the objective function of which is\nLRMF (Y, U\u0302, V\u0302) = n \u2211\ni=1\n\u2211\n{jk}\u2208Z\n| \u25b3WNDCG | log(1 + e \u2212\u03c3(u\u0302i(v\u0302j\u2212v\u0302k))) +RU +RV\n(26)\nIts partial derivatives with respect to u\u0302i, v\u0302j , are now given\nAlgorithm 1 Regularized LambdaMART Matrix Factorization\nInput: C, D,Y, SUout ,SUin , SVout ,SVin ,\u00b51, \u00b52, \u03b7,r, and T Output: fu and fv initialize: f0u(c) and f 0 v(d) with random values initialize t = 1 repeat\na) compute rtui = \u2212 \u2202LRMF (Y,U\u0302,V\u0302)\n\u2202u\u0302i\n\u2223 \u2223 \u2223U\u0302=U\u0302t\u22121,V\u0302=V\u0302t\u22121\naccording to equation 27, for i = 1 to n b) compute rtvj = \u2212 \u2202LRMF (Y,U\u0302,V\u0302)\n\u2202v\u0302j\n\u2223 \u2223 \u2223U\u0302=U\u0302t\u22121,V\u0302=V\u0302t\u22121\naccording to equation 28, for j = 1 to m c)fit a multi-output regression tree htu for {(ci, r t ui )}ni=1\nd)fit a multi-output regression tree htv for {(di, r t vi )}mi=1\ne) f tu(ci) = f t\u22121 u (ci) + \u03b7h t u(ci) f) f tv(dj) = f t\u22121 v (dj) + \u03b7h t v(dj)\nuntil converges or t=T\nby:\n\u2202LRMF (Y, U\u0302, V\u0302)\n\u2202u\u0302i =\nm \u2211\nj=1\n\u03bb i j\n\u2202y\u0302ij\n\u2202u\u0302i\n+2\u00b51 \u2211\nj\u2208Ni Uin\nsUin,ij(u\u0302i \u2212 u\u0302j)\n+2\u00b52 \u2211\nj\u2208Ni Uout\nsUout,ij(u\u0302i \u2212 u\u0302j)(27)\n\u2202LRMF (Y, U\u0302, V\u0302)\n\u2202v\u0302j =\nn \u2211\ni=1\n\u03bb i j\n\u2202y\u0302ij\n\u2202vj\n+2\u00b51 \u2211\ni\u2208N j Vin\nsVin,ij(v\u0302j \u2212 v\u0302i)\n+2\u00b52 \u2211\ni\u2208N j\nVout\nsVout,ij(v\u0302j \u2212 v\u0302i)(28)\nwhere NiUin is the set of the k nearest neighbors of the ith user defined on the basis of the input similarity.\nTo optimize our final objective function, equation 26, we learn the latent profiles of users and items by gradient boosted trees. We will call the resulting algorithm Regularized LambdaMART Matrix Factorization and denote it by LM-MFReg. The algorithm is described in Algorithm 1. At t iteration, we first compute the partial derivatives of the objective function at point (U\u0302t\u22121, V\u0302t\u22121). Then we fit the trees htu and h t v for the user and item descriptions respectively. Finally, we update the predictions of U\u0302 and V\u0302 according to the output of regression trees. The learning process is continued until the maximum number of trees is reached or the early stop criterion is satisfied. In all our experiments, the maximum number of trees is set by 15000. The early stopping criterion is no validation set error improvement in 200 iterations.\n5. EXPERIMENTS\nWe will evaluate the two basic algorithms that we presented above, LM-MF and LM-MF-Reg, on two recommendation problems, meta-mining and MovieLens, and compare their performance to a number of baselines.\nMeta-mining [9] applies the idea of meta-learning or learning to learn to the whole DM process. Data mining workflows and datasets are extensively characterised by side information. The goal is to suggest which data mining workflow should be applied on which dataset in view of optimizing some performance measure, e.g. accuracy in classification problems, by mining past experiments. Recently [11] have proposed tackling the problem as a hybrid recommendation problem: dataset-workflow pairs can be seen as user-item pairs which are related by the relative performance achieved by the workflows applied on the datasets. We propose here to go one step beyond [11] and consider the meta-mining as a learning to rank problem. We should note that meta-mining is a quite difficult problem, because what we are trying to do is in essence to predict-recommend what are the workflows that will achieve the top performance on a given dataset.\nMovieLens2 is a well-known benchmark dataset for collaborative filtering. It provides one to five star ratings of users for movies, where each user has rated at least twenty movies. In addition the dataset provides limited side information on users and items. It has been extensively experimented with most often in a non cold start setting, due to the difficulty of exploitation of the available side information. The dataset has two versions: the first contains one hundred thousand ratings (100K) and the second one million (1M). We will experiment with both of them.\nIn Table 1 we give a basic description of the three different datasets, namely the numbers of ratings, users, items, the numbers of user and item descriptors, d and l respectively, and the percentage of available entries in the Y preference matrix. The meta-mining dataset has a quite extensive set of features giving the side information and a complete preference matrix, it is also considerably smaller than the two MovieLens variants. The MovieLens datasets are characterized by a large size, the limited availability of side information, and the very small percentage of available entries in the preference matrix. Overall we have two recommendation problems with very different characteristics."}, {"heading": "5.1 Recommendation Tasks", "text": "We will evaluate the performance of the algorithms we presented above in two different variants of the cold start problem. In the first one that we will call User Cold Start we will evaluate the quality of the recommendations they provide to unseen users when the set of items over which we provide recommendations is fixed. In the second variant, which we will call Full Cold Start, we will provide suggestions over both unseen users and items. We will also evaluate the performance of our algorithms in a Matrix Com-\n2http://grouplens.org/datasets/movielens/\npletion setting; here we will randomly remove items from the users preference lists and predict the preferences of the removed items with a model learned on the remaining observed ratings. In model-based collaborative filtering, matrix completion has been usually addressed by low-rank matrix factorization algorithms that do not exploit side information. We want to see whether exploiting the side information can bring substantial improvements in the matrix completion performance. We will do this only for the metamining problem because the MovieLens dataset have rather limited side-information.\nNote that provision of recommendations in the cold start setting is much more difficult than the matrix completion since in the former we do not have historical data for the new users and new items over which we need to provide the recommendations and thus we can only rely on their side information to infer the latter."}, {"heading": "5.2 Comparison Baselines", "text": "As first baseline we will use LambdaMART (LM), [4], in both cold start variants as well as in the matrix completion setting. To ensure a fair comparison of our methods against LamdaMART we will train all of them using the same values over the hyperparameters they share, namely learning rate, size of regression trees and number of iterations. In the matrix completion we will also add CofiRank (CR) as a baseline, a state-of-the-art matrix factorization algorithm in collaborative ranking [13]. CR minimizes an upper bound of NDCG and uses the l2 norm to regularize the latent factors. Its objective function is thus similar to those of our methods and LambdaMART\u2019s but it learns directly the latent factors by gradient descent without using the side information.\nFor the two cold start evaluation variants, we will also have as a second baseline a memory-based approach, one of the most common approaches used for cold start recommendations [3]. In the user cold start setting we will provide item recommendations for a new user using its nearest neighbors. This user memory-based (UB) approach will compute the preference score for the jth item as:\ny\u0302j = 1\n|N|\n\u2211\ni\u2208N\nyij (29)\nwhere N is the set of the 5-nearest neighbors for the new user for which we want to provide the recommendations. We compute the nearest neighbors using the Euclidean distance on the user side information. In the full cold start setting, we will provide recommendations for a new user-item pair by joining the nearest neighbors of the new user with the nearest neighbors of the new item. This full memory-based (FB) approach will compute the preference score for the ith user and jth item as:\ny\u0302ij = 1\n|Ni| \u00d7 |Nj |\n\u2211\nc\u2208Ni\n\u2211\nd\u2208Nj\nycd (30)\nwhere Ni is the set of the 5-nearest neighbors for the new user and Nj is the set of the 5-nearest neighbors for the new item. Both neighborhoods are computed using the Euclidean distance on the user and item side-information features respectively.\nFinally we will also experiment with the LambdaMART variant in which we use the weighted NDCG cost that we described in section 4.2 in order to evaluate the potential ben-\nefit it brings over the plain NDCG; we will call this method LMW."}, {"heading": "5.3 Meta-Mining", "text": "The meta-mining problem we will consider is the one provided by [11]. It consists of the application of 35 feature selection plus classification workflows on 65 real world datasets with genomic microarray or proteomic data related to cancer diagnosis or prognosis, mostly from National Center for Biotechnology Information3. In [11], the authors used four feature selection algorithms: Information Gain, IG, Chisquare, CHI, ReliefF, RF, and recursive feature elimination with SVM, SVMRFE; they fixed the number of selected features to ten. For classification, they used seven classification algorithms: one-nearest-neighbor, 1NN, the C4.5 and CART decision tree algorithms, a Naive Bayes algorithm with normal probability estimation, NBN, a logistic regression algorithm, LR, and SVM with the linear, SVMl and the rbf, SVMr, kernels. In total, they ended up with 35\u00d7 65 = 2275 base-level DM experiments; they used as performance measure the classification accuracy which they estimated using ten-fold cross-validation.\nThe preference score that corresponds to each datasetworkflow pair is given by a number of significance tests on the performances of the different workflows that are applied on a given dataset. More concretely, if a workflow is significantly better than another one on the given dataset it gets one point, if there is no significant difference between the two workflows then each gets half a point, and if it is significantly worse it gets zero points. Thus if a workflow outperforms in a statistically significant manner all other workflows on a given dataset it will get m\u2212 1 points, where m is the total number of workflows (here 35). In the matrix completion and the full cold start settings we compute the preference scores with respect to the training workflows since for these two scenarios the total number of workflows is less than 35. In addition, we rescale the performance measure from the 0-34 interval to the 0-5 interval to avoid large preference scores from overwhelming the NDCG due to the exponential nature of the latter with respect to the preference score.\nFinally, to describe the datasets and the data mining workflows we use the same characteristics that were used in [11]. Namely 113 dataset characteristics that give statistical, informationtheoretic, geometrical-topological, landmarking and modelbased descriptors of the datasets and 214 workflow characteristics derived from a propositionalization of a set of 214 tree-structured generalized workflow patterns extracted from the ground specifications of DM workflows.\n5.3.1 Evaluation Setting We fix the parameters that LambdaMART uses to con-\nstruct the regression trees to the following values: the maximum number of nodes for the regression trees is three, the maximum percent of instances in each leaf node is 10% and the learning rate, \u03b7, of the gradient boosted tree algorithm to 10\u22122. To build the ensemble trees of LM-MF and LMMF-Reg we use the same parameter settings as the ones we use in LambdaMART. We select their input and output regularization parameters \u00b51 and \u00b52 in the grid [0.1, 1, 5, 7, 10]\n2, by three-fold inner cross-validation. We fix the number of nearest neighbors for the Laplacian matrices to five. To\n3http://www.ncbi.nlm.nih.gov/\ncompute the output-space similarities, we used the NDCG similarity measure defined in Eq. 21 where the truncation level k is set to the truncation level at which each time we report the results.\nTo build the different recommendation scenarios, we have proceeded as follows. In matrix completion we randomly select N workflows for each dataset to build the training set. We choose N in the range [5, 10, 15]. For each dataset, we use ten workflows, different from the N ones selected in training, for validation and we use the rest for testing. This scenario emulates a 86%, 71% and 57% of missing values in the preference matrix. Since in the matrix completion setting the numbers of users and items are fixed, we fix the number of hidden factors r to min(n,m) = 35 for all three matrix factorization algorithms, LM-MF, LM-MF-Reg and CofiRank. For this baseline we used the default parameters as these are provided in [13]. We report the average NDCG@5 measure on the test workflows of each dataset.\nIn the user cold start scenario, we will evaluate the performance in providing accurate recommendations for new datasets. To do so we do a leave-one-dataset-out. We train the different methods on 64 datasets and evaluate their recommendations on the left-out dataset. Since there are no missing workflows as previously we also fix the number of hidden factors r to min(n,m) = 35. In the full cold start scenario on top of the leave-one-dataset-out we also randomly partition the set of workflows in two sets, where we use 70% of the workflows for training and the remaining 30% as the test workflows for the left-out dataset. That is the number of workflows in the train set is equal to \u230a0.7\u00d735\u230b = 24 which defines the number of hidden factors r. We use the 11 remaining workflows as the test set. For the both cold start scenarios, we report the average testing NDCGmeasure. We compute the average NDCG score at the truncation levels of k = 1, 3, 5\nFor each of the two algorithms we presented we compute the number of times we have a performance win or loss compared to the performance of the baselines. On these win/loss pairs we do a McNemar\u2019s test of statistical significance and report its results, we set the significance level at p = 0.05. For the matrix completion we denote the results of the performance comparisons against the CofiRank and LambdaMART baselines by \u03b4CR and \u03b4LM respectively. We give the complete results in table 2. For the user cold start we denote the results of the performance comparisons against the user-memory based and the LambdaMART baselines by \u03b4UB and \u03b4LM respectively, table 3a. For the full cold start we denote by \u03b4FB and \u03b4LM the performance comparisons against the full-memory-based and the LambdaMART baselines respectively, table 3b.\n5.3.2 Results\nMatrix Completion. CR achieves the lowest performance for N = 5 and N = 10, compared to the other methods; for N = 15 the performances of all methods are very similar, table 2. The strongest performance advantages over the CR baseline appear at N = 10; there LMW and LM-MF are significantly better and LM-MF-Reg is close to being significantly better, p-value=0.0824. At N = 5 only LM-MF-Reg is significantly better than CR. Overall it seems that using the side information in matrix completion problems brings a performance\nimprovement mainly when the preference matrix is rather sparse, as it is the case for N = 5 an N = 10, for lower sparsity levels the use of side information does not seem to bring any performance improvement.\nUser Cold Start. LM-MF and LM-MF-Reg are the two methods that achieve the larger performance improvements over both the UB and the LM baselines for all values of the k truncation level, table 3a. At k = 1 both of them beat in a statistically significant manner the UB baseline, and they are also close to beating in a statistical significant manner the LM baseline as well, p-value=0.0636. At k = 3 both beat in a statistically significant manner UB but not LM, at = 5 there are no significant differences. Finally, note the LambdaMART variant that makes use of the weighted NDCG, LMW, does not seem to bring an improvement over the plain vanilla NDCG, LM.\nFull Cold Start. Here the performance advantage of LM-MF and LM-MFReg is even more pronounced, table 3b. Both of them beat in a statistically significant manner the LM baseline for all values k with a performance improvement of 0.7 in average. They also beat the FB baseline in a statistically significant manner at k = 1 with LM-MF beating it also in a statistical significant manner at k = 3 as well.\n5.3.3 Analysis In order to study in more detail the effect of regularization\nwe give in figures 1 and 2 the frequency with which the two \u00b51 and \u00b52 input and output space regularization parameters are set to the different possible values of the selection grid [0.1, 1, 5, 7, 10], over the 65 repetitions of the leave-onedataset-out for the two cold start settings. The figures give the heatmaps of the pairwise selection frequencies; in the y-axis we have the \u00b51 parameter (input-space regularization hyperparameter) and in the x-axis the \u00b52 (output-space regularization hyperparameter). A yellow to white cell corresponds to an upper mid to high selection frequency whereas an orange to red cell a bottom mid to low selection frequency.\nIn the user cold start experiments, figure 1, we see that for k = 1 the input space regularization is more important than the output space regularization, see the mostly white column in the (0.1, \u00b7) cells. This makes sense since the latter regularizer uses only the top-one user or item preference to compute the pairwise similarities. As such, it does not carry much information to regularize appropriately the latent factors. For k = 3 the selection distribution spreads more evenlty over the different pairs of values which means that we have more combinations of the two regularizers. The most frequent pair is the (0.1, 0.1) which corresponds to low regularization, For k = 5 we have a selection peak at (0.1, 10) and one at (7,1), it seems that none of the two regularizers has a clear advantage over the other.\nLooking now at the full cold start experiments, figure 2, we have a rather different picture. For k = 1 there is as before an advantage for the input space regularization where the (0.1, 5) cell is the most frequent one, however this time we also observe a high selection frequency on a strong inputoutput space regularization, the (10,10) selection peak. This is also valid for k = 3 where we have now a high selection frequency on strong regularization both for the input as well as the output space regulatization, see the selection peaks\non, and around, (10,10). For k = 5 we have now three peaks at (5, 1), (7, 0.1) and (10, 5) which indicate that the output space regularizer is more important than the input space regularizer.\nOverall in the full cold start setting the regularization parameters seem to have more importance than in the user cold start setting. This is reasonable since in the former scenario we have to predict the preference of a new useritem pair, the latent profiles of which have to be regularized appropriately to avoid performance degradation."}, {"heading": "5.4 MovieLens", "text": "The MovieLens recommendation dataset has a quite different morphology than that of the meta-mining. The descriptions of the users and the items are much more limited; users are described only by four features: sex, age, occupation and location, and movies by their genre which takes 19 different values, such as comedy, sci-fi to thriller, etc. We model these descriptors using one-of-N representation.\nIn the MovieLens dataset the side information has been mostly used to regularize the latent profiles and not to predict them, see e.g [1, 2]. Solving the cold start problem in the presence of so limited side information is a rather difficult task and typical collaborative filtering approaches make only use of the preference matrix Y to learn the user and items latent factors.\n5.4.1 Evaluation setting We use the same baseline methods as in the meta-mining\nproblem. We train LM with the following hyper-parameter setting: we fix the maximum number of nodes for the re-\ngression trees to 100, the maximum percentage of instances in each leaf node to 1%, and the learning rate, \u03b7, of the gradient boosted tree algorithm to 10\u22122. As in the metamining problem for the construction of the ensemble of regression trees in LM-MF and LM-MF-Reg we use the same setting for the hyperparameters they share with LM. We optimize their \u00b51 and \u00b52 regularization parameters within the grid [0.1, 1, 5, 10, 15, 20, 25, 50, 80, 100]2 using five-fold inner cross validation. We fix the number of factors r to 50 and the number of nearest neighbors in the Laplacian matrices to five. As in the meta-mining problem we used the NDCG similarity measure to define output-space similarities with the truncation level k set to the same value as the one at which we evaluate the methods.\nWe will evaluate and compare the different methods under the two cold start scenarios we described previously. In the user cold start problem, we randomly select 50% of the users for training and we test the recommendations that the learned models generate on the remaining 50%. In the full cold start problem, we use the same separation to training and testing users and in addition we also randomly divide the item set to two equal size subsets, where we use one for training and the other for testing. Thus the learned models are evaluated on users and items that have never been seen in the model training phase. We give the results for the 100k and 1M variants of the MovieLens dataset for the user cold start scenario in table 4a and for the full cold start scenario in table 4b.\n5.4.2 Results In the user cold start scenario, table 4a, we can see first\nthat both LM-MF and LM-MF-Reg beat in a statistically significant manner the UB baseline in both MovieLens variants for all values of the truncation parameter, with the single exception of LM-MF at 100k and k = 5 for which the performance difference is not statistically significant. LM and its weighted NDCG variant, LMW, are never able to beat UB; LM is even statistically significantly worse compared to UB at 1M for k = 5. Moreover both LM-MF and its regularized variant beat in a statistically significant manner LM in both MovieLens variants and for all values of k. LM-MF-Reg has a small advantage over LM-MF for 100K, it achieves a higher average NDCG score, however this advantage disappears when we move to the 1M dataset, which is rather normal since due to the much larger dataset size there is more need for additional regularization. Moreover since we do a low-rank matrix factorization, we have r = 50 for the roughly 6000 users. which already is on its own a quite strong regularization rendering also unnecessary the need for the input/output space regularizers.\nA similar picture arises in the full cold start scenario, table 4b. With the exception of MovieLens 100K at k = 10 both LM-MF and LM-MF-Reg beat always in a statistically significant the FB baseline. Note also that now LM as well as its weigthed NDCG variant are significantly better than the FB baseline for the 1M dataset and k = 5, 10. In adition the weigthed NDCG variant is significantly better than LM for 100k and 1M at k = 5 and k = 10 respectively. Both LM-MF and LM-MF-Reg beat in a statistically significant manner LM, with the exception of 100k at k = 10 where there is no significant difference. Unlike the user cold start problem, now it is LM-MF that achieves the best overall performance."}, {"heading": "6. CONCLUSION", "text": "Since only top items are observable by users in real recommendation systems, we believe that ranking loss functions that focus on the correctness of the top item predictions are more appropriate for this kind of problem. We explore the use of a state of the art learning to rank algorithm, LambdaMART, in a recommendation setting with an emphasis on the cold-start problem one of the most challenging problems in recommender systems. However plain vanilla LambdaMART has a certain number of limitations which spring namely from the fact that it lacks a principled way to control overfitting relying in ad-hoc approaches. We proposed a number of ways to deal with these limitations. The most important is that we cast the learning to rank problem as learning a low-rank matrix factorization; our underlying assumption here being that the descriptions of the users and items as well as their preference behavior are governed by a few latent factors. The user item-preferences are now computed as inner products of the learned latent representations of users and items. In addition to the regularization effect of the low rank factorization we bring in additional regular-\nizers to control the complexity of the latent representations, regularizers which reflect the users and items manifolds as these are explicited by user and item feature descriptions as well as the preference behavior. We report results on two very different recommendation problems, meta-mining and MovieLens, and show that the performance of the algorithms we propose beats in a statistically significant manner a number of baselines often used in recommendation systems."}, {"heading": "7. REFERENCES", "text": "[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert.\nLow-rank matrix factorization with attributes. arXiv preprint cs/0611124, 2006.\n[2] D. Agarwal and B.-C. Chen. flda: matrix factorization through latent dirichlet allocation. In Proceedings of the third ACM international conference on Web search and data mining, WSDM \u201910, pages 91\u2013100, New York, NY, USA, 2010. ACM.\n[3] R. M. Bell and Y. Koren. Scalable collaborative filtering with jointly derived neighborhood interpolation weights. In Data Mining, 2007. ICDM\n2007. Seventh IEEE International Conference on, pages 43\u201352. IEEE, 2007.\n[4] C. J. Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11:23\u2013581, 2010.\n[5] C. J. Burges, K. M. Svore, P. N. Bennett, A. Pastusiak, and Q. Wu. Learning to rank using an ensemble of lambda-gradient models. Journal of Machine Learning Research-Proceedings Track, 14:25\u201335, 2011.\n[6] P. Donmez, K. M. Svore, and C. J. Burges. On the local optimality of lambdarank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 460\u2013467. ACM, 2009.\n[7] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 1189\u20131232, 2001.\n[8] J. Fu\u0308rnkranz and E. Hu\u0308llermeier. Preference learning. Springer, 2010.\n[9] M. Hilario, P. Nguyen, H. Do, A. Woznica, and A. Kalousis. Ontology-based meta-mining of knowledge discovery workflows. In N. Jankowski, W. Duch, and K. Grabczewski, editors, Meta-Learning in Computational Intelligence. Springer, 2011.\n[10] K. Ja\u0308rvelin and J. Keka\u0308la\u0308inen. Ir evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 41\u201348. ACM, 2000.\n[11] P. Nguyen, J. Wang, M. Hilario, and A. Kalousis. Learning heterogeneous similarity measures for hybrid-recommendations in meta-mining. In IEEE 12th International Conference on Data Mining (ICDM), pages 1026 \u20131031, dec. 2012.\n[12] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1329\u20131336. MIT Press, Cambridge, MA, 2005.\n[13] M. Weimer, A. Karatzoglou, Q. V. Le, and A. Smola. Maximum margin matrix factorization for collaborative ranking. Advances in neural information processing systems, 2007.\n[14] Y. Yue and C. Burges. On using simultaneous perturbation stochastic approximation for ir measures, and the empirical optimality of lambdarank. In NIPS Machine Learning for Web Search Workshop, 2007."}], "references": [{"title": "Low-rank matrix factorization with attributes", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert"], "venue": "arXiv preprint cs/0611124,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "flda: matrix factorization through latent dirichlet allocation", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "In Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["R.M. Bell", "Y. Koren"], "venue": "In Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["C.J. Burges"], "venue": "Learning, 11:23\u2013581,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning to rank using an ensemble of lambda-gradient models", "author": ["C.J. Burges", "K.M. Svore", "P.N. Bennett", "A. Pastusiak", "Q. Wu"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the local optimality of lambdarank", "author": ["P. Donmez", "K.M. Svore", "C.J. Burges"], "venue": "In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Ontology-based meta-mining of knowledge discovery workflows", "author": ["M. Hilario", "P. Nguyen", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "Meta-Learning in Computational Intelligence. Springer,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Ir evaluation methods for retrieving highly relevant documents", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Learning heterogeneous similarity measures for hybrid-recommendations in meta-mining", "author": ["P. Nguyen", "J. Wang", "M. Hilario", "A. Kalousis"], "venue": "In IEEE 12th International Conference on Data Mining (ICDM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q.V. Le", "A. Smola"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "On using simultaneous perturbation stochastic approximation for ir measures, and the empirical optimality of lambdarank", "author": ["Y. Yue", "C. Burges"], "venue": "In NIPS Machine Learning for Web Search Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores.", "startOffset": 134, "endOffset": 144}, {"referenceID": 0, "context": "For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores.", "startOffset": 134, "endOffset": 144}, {"referenceID": 1, "context": "For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores.", "startOffset": 134, "endOffset": 144}, {"referenceID": 11, "context": "Cofirank [13], a collaborative ranking algorithm, does maximum-margin matrix factorization by optimizing an upper bound of NDCG measure, a ranking-based loss function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "Probably the best known example of such algorithms is LambdaMART, [4], the state-of-the-art learning to rank algorithm.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "One such, very often used, metric is the Discounted Cumulative Gain (DCG)[10], which is defined as follows:", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "LambdaMART is one of the most popular algorithms for preference learning which follows exactly this idea, [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 12, "context": "[14] have shown empiricially that solving this problem also optimizes the NDCG metric of the learned model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "LambdaMART uses Multiple Additive Regression Trees (MART) [7] to solve its optimization problem.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "g [5, 6].", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": "g [5, 6].", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 11, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 0, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 1, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 7, "context": "Meta-mining [9] applies the idea of meta-learning or learning to learn to the whole DM process.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "Recently [11] have proposed tackling the problem as a hybrid recommendation problem: dataset-workflow pairs can be seen as user-item pairs which are related by the relative performance achieved by the workflows applied on the datasets.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "We propose here to go one step beyond [11] and consider the meta-mining as a learning to rank problem.", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "As first baseline we will use LambdaMART (LM), [4], in both cold start variants as well as in the matrix completion setting.", "startOffset": 47, "endOffset": 50}, {"referenceID": 11, "context": "In the matrix completion we will also add CofiRank (CR) as a baseline, a state-of-the-art matrix factorization algorithm in collaborative ranking [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "For the two cold start evaluation variants, we will also have as a second baseline a memory-based approach, one of the most common approaches used for cold start recommendations [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "The meta-mining problem we will consider is the one provided by [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "In [11], the authors used four feature selection algorithms: Information Gain, IG, Chisquare, CHI, ReliefF, RF, and recursive feature elimination with SVM, SVMRFE; they fixed the number of selected features to ten.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Finally, to describe the datasets and the data mining workflows we use the same characteristics that were used in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "We choose N in the range [5, 10, 15].", "startOffset": 25, "endOffset": 36}, {"referenceID": 8, "context": "We choose N in the range [5, 10, 15].", "startOffset": 25, "endOffset": 36}, {"referenceID": 11, "context": "For this baseline we used the default parameters as these are provided in [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "g [1, 2].", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "g [1, 2].", "startOffset": 2, "endOffset": 8}], "year": 2015, "abstractText": "Recommendation systems often rely on point-wise loss metrics such as the mean squared error. However, in real recommendation settings only few items are presented to a user. This observation has recently encouraged the use of rankbased metrics. LambdaMART is the state-of-the-art algorithm in learning to rank which relies on such a metric. Despite its success it does not have a principled regularization mechanism relying in empirical approaches to control model complexity leaving it thus prone to overfitting. Motivated by the fact that very often the users\u2019 and items\u2019 descriptions as well as the preference behavior can be well summarized by a small number of hidden factors, we propose a novel algorithm, LambdaMARTMatrix Factorization (LambdaMART-MF), that learns a low rank latent representation of users and items using gradient boosted trees. The algorithm factorizes lambdaMART by defining relevance scores as the inner product of the learned representations of the users and items. The low rank is essentially a model complexity controller; on top of it we propose additional regularizers to constraint the learned latent representations that reflect the user and item manifolds as these are defined by their original feature based descriptors and the preference behavior. Finally we also propose to use a weighted variant of NDCG to reduce the penalty for similar items with large rating discrepancy. We experiment on two very different recommendation datasets, meta-mining and movies-users, and evaluate the performance of LambdaMART-MF, with and without regularization, in the cold start setting as well as in the simpler matrix completion setting. In both cases it outperforms in a significant manner current state of the art algorithms.", "creator": "LaTeX with hyperref package"}}}