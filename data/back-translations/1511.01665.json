{"id": "1511.01665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding", "abstract": "This article discusses how word embedding can be used as a feature in the Chinese sentiment classification. First, a Chinese opinion corpus is built with one million comments from hotel review websites. Then, the word embedding representing each comment is used as an input into various machine learning methods for mood classification, including SVM, logistic regression, Convolutionary Neural Network (CNN), and ensemble methods. These methods perform better than N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed that delivers outstanding performance in terms of precision, memory, and F1 score.", "histories": [["v1", "Thu, 5 Nov 2015 09:25:21 GMT  (535kb,D)", "http://arxiv.org/abs/1511.01665v1", "The 29th Pacific Asia Conference on Language, Information and Computing"]], "COMMENTS": "The 29th Pacific Asia Conference on Language, Information and Computing", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yiou lin", "hang lei", "jia wu", "xiaoyu li"], "accepted": false, "id": "1511.01665"}, "pdf": {"name": "1511.01665.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding", "authors": ["Yiou Lin", "Hang Lei", "Jia Wu", "Xiaoyu Li"], "emails": ["lyoshiwo@gmail.com", "xiaoyuuestc}@uestc.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Sentiment analysis or opinion mining is the computational study of people\u2019s opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics and their attributes (Liu and Zhang, 2012). The task of sentiment analysis is technically challenging and practically very useful. For example, businesses always want to find public or consumer opinions about their products and services. Consumers also need a sounding board rather than thinking alone while making decisions. With the development of Internet, opinionated texts from social media (e.g., reviews, blogs\nand micro-blogs) are used frequently for decision making, which makes automated sentiment analysis techniques more and more important. Among those tasks of the sentiment analysis, the key one is to classify the polarity of given texts. Many works have been done in recent years to improve English sentiment polarity classification. There are two categories of such works. One is called \u201cmachine learning\u201d which is firstly proposed to determine whether a review is positive or negative by using three machine learning methods, including NB, ME and SVM (Pang et al., 2002). The other category called \u201csemantic orientation\u201d is applied to classify words into various classes by giving a score to each word to evaluate the strength of sentiment. And an overall score is calculated to assign the review to a specific class (Turney, 2002).\nRecently, researchers have tried to handle tasks of Natural Language Processing (NLP) with the help of deep learning approaches. Among those approaches, a useful one called word2vec has attracted increasing interest. Word2vec translates words to vector representations (called word embeddings) efficiently by using skip-gram algorithm (Mikolov et al., 2013a). It is also proposed that the induced vector representations capture meaningful syntactic and semantic regularities, for example, \u201cKing\u201d - \u201cMan\u201d + \u201cWoman\u201d results in a vector very close to \u201cQueen\u201d (Mikolov et al., 2013b).\nBesides, with the advancement of information technology, for the first time in Chinese history, a huge volume of Chinese opinionated data recorded in digital form is ready for analysis. Though Chinese language plays an important role in economic\nar X\niv :1\n51 1.\n01 66\n5v 1\n[ cs\n.C L\n] 5\nN ov\nglobalization, there are few works have been done for Chinese sentiment analysis with huge databases. It inspires us to make an empirical study on Chinese sentiment with bigger databases than usual.\nThe remain of the article is organized as follows: Section 2 briefly describes related work. Section 3 describes details of the methods used in training procedure. Section 4 reports and discusses the results. Finally, we summarize our works in Section 5."}, {"heading": "2 Related work", "text": "According to Liu and Zhang (2012), the sentiment analysis research mainly started from early 2000 by Turney (2002) and Pang et al. (2002). Turney (2002) firstly used a few semantically words (e.g., excellent and poor) to label other phrases with the hit counts by queries through search engines. Then, researchers had also proposed several custom techniques specifically for sentiment classification, e.g., the score function based on words in positive and negative reviews (Dave et al., 2003) and feature weighting schemes used to enhance classification accuracy (Paltoglou and Thelwall, 2010). Besides, the other situation of sentiment analysis is to represent texts by vectors which indicate these words appear in the text but do not preserve word order. And a machine learning approach will be used for classification in the end. In such way, Pang et al. (2002) considered classifying documents according to standard machine learning techniques. In addition, subsequent research used more features in learning, making the main task of sentiment classification engineer an effective set of features (Pang and Lee, 2008).\nHowever, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005). Li and Sun (2007) presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes. They found that SVM achieved the best performance. After that, Tan and Zhang (2008) found 6,000 or bigger for the size of features would be sufficient for Chinese sentiment analysis, and sentiment classifiers were severely dependent on domains or topics.\nNowadays, inspired by the availability of large text corpus and the success of deep learning approaches, some researchers (e.g., Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al. (2014) got some state-of-the-art performance. But the work of Collobert et al. (2011) was paid most attention for describing a unified architecture for NLP tasks which learned features by training a deep neural network even when being given very limited prior knowledge. These NLP tasks included part-of-speech tagging, chunking, named-entity recognition, language model learning and semantic role labeling."}, {"heading": "3 Methodology", "text": "This section presents the methodology used in our experiment."}, {"heading": "3.1 Feature selection methods", "text": ""}, {"heading": "3.1.1 Sentiment lexicon and CHI", "text": "A sentiment lexicon accommodating sentiment words plays an important role in sentiment analysis. A combination of two Chinese sentiment lexicons (Hownet (Dong and Dong, 2006) and DLLEX (Xu et al., 2008)) is constructed, including 30406 words in total. After removing those words which do not appear in the corpus, 10444 sentiment words are preserved. After several experiments, CHI (Galavotti et al., 2000) is chosen for information gain. Finally, 150 most valuable words are added into the new lexicon. At last, 10543 words are obtained as features."}, {"heading": "3.1.2 Word2vec", "text": "Word2vec (Mikolov et al., 2013a) has gained kinds of traction today. As the name shows, it translates words to vectors called word embeddings. That is to say, it gets the vector representations of words. Gensim1, a python tool is used to get word2vec module. The method of training word2vec model is unsupervised learning and 300 is set as the quantity\n1http://radimrehurek.com/gensim/\nof the dimension of vectors. Table 1 shows the word embeddings of a Chinese hotel review which means the room is very clean and neat. For convenient display, each value of dimension is multiplied by 10,000 and indicated by di (i = 1, ..., 300)."}, {"heading": "3.2 Traditional methods", "text": ""}, {"heading": "3.2.1 Naive Bayes Classification", "text": "Naive Bayes (NB) is widely used in sentiment classification which is used to classify a given review document d to the class c\u2217 = argmaxcP (c|d). According to Bayes\u2019s rule,\nP (cj |d) = P (cj)P (d|cj)\nP (d)\nwhere cj is a kind of class and P (d) plays no role in selecting c\u2217. Let\u2019s mark f1, f2, ...fm as the set of features that appear in all reviews, and set ni(d) as the number of times fi appears in d. Usually, ni(d) is set as 1, if fi appears more than one time. Then, a formulation can be gotten as\nP (cj |d) = P (cj)\n\u220fm i P (fi|cj) ni(d)\nP (d)\nwhere the estimation of P (fi|cj) is calculated as follows, using add-one smoothing\nP\u0302 (fi|cj) = 1 + nij m+ \u2211m\nk=1 nkj"}, {"heading": "3.2.2 Maximum Entropy Classification", "text": "Maximum Entropy Classification follows the principle of maximum entropy (Jaynes, 1957), which means, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one\nwith largest entropy. Thus, the estimate of P (cj |d) is showed as follows\nP (cj |d) = 1\n\u03c0 (d) exp (\u2211m i=1 \u03bbi,cjFi,cj (d, cj) )\nFi,cj (d, x) = { 1 if ni > 0 and x = cj 0 otherwise\nwhere \u03c0(d) is a normalization function and \u03bbi,cj is the weight of fj in maximum entropy cj .The other parameters are defined in the same way as Section 3.2.1. After fifteen iterations of the improved iterative scaling algorithm (Pietra et al., 1997) implemented in Natural Language Toolkit (Bird, 2006), the parameters of \u03bbi,cj are adjusted to maximize the entropy of distribution of training data."}, {"heading": "3.2.3 Support Vector Machines", "text": "Support Vector Machines (SVM) is a very effective machine learning method firstly introduced by (Cortes and Vapnik, 1995). SVM constructs a hyperplane or a set of hyperplanes in a high dimensional space represented by ~w. Since the larger the margin, the lower the error of the classifier, after training, the largest distance of support vector to nearest trainingdata point in any classes is achieved. Then the problem of maximizing the margin turns to\nargmin ~w, b\n1 2 ||w||2\nwhere yi(~w \u00b7 xi \u2212 b) \u2265 1\nand its unconstrained dual form is the following optimization problem: maximize L\u0303(\u03b1) where\nL\u0303(\u03b1) = n\u2211 i=1 \u03b1i \u2212 1 2 \u2211 i,j \u03b1i\u03b1jyiyjk(xi,xj)\n= n\u2211\ni=1\n\u03b1i \u2212 1\n2 \u2211 i,j \u03b1i\u03b1jyiyjx T i xj\nsubject to \u03b1i \u2265 0 (i = 1, . . . , n). Usually, the kernel here is linear, which means\nk(xi,xj) = xi \u00b7 xj\nFor SVM models, python tool scikit-learn (Pedregosa et al., 2011) is chosen for training and testing. Scikit-learn2 was started in 2007 as a Google\n2http://scikit-learn.org\nSummer of Code project, and has became the most efficient and useful tool for data mining and analysis in Python. With all default parameters, LinearSVC and SVC with linear kernel are used in our article."}, {"heading": "3.3 Ensemble methods", "text": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one. There are two families of ensemble methods, averaging methods and boosting methods. In averaging methods, several estimators will be built to average their predictions. It is a kind of vote, namely, on average. The combined estimator is usually better than any of the fundamental estimators since its variance is reduced (e.g., Bagging methods and Forests of randomized trees). By contrast, in boosting methods, fundamental estimators are built sequentially and each one tries to reduce the bias of the combined estimator. The idea behind it is to combine several weak models to generate a more powerful ensemble model (e.g., AdaBoost and Gradient Tree Boosting).\nThe ensemble method modules are chosen from scikit-learn, including AdaBoost, Gradient Tree Boosting and Random Forests. For each Chinese review, the average value of word embeddings is used as the input."}, {"heading": "3.4 CNN methods", "text": "CNN is short for Convolutional Neural Networks. Its key module is to calculates the convolution between input and output. Just as CNN used in computer vision, a matrix is needed, as the input of CNN. After several experiments, we set D = 60 as the dimension quantity of word embeddings for CNN. If there are L words in a sentence, combine their word embeddings together to construct a matrix of size L\u00d7D as shown in Figure 1. L = 60 is set since fixed L is needed, and which means, only 60 words are preserved from the beginning of a review. On the other hand, if the length is less than 60, the matrix will be filled with used vectors from the beginning of the review by repeating them. At last, every review is represented by a matrix of size 60\u00d7 60.\nFormally, in computer vision, given n images (Xl, l = 1, ..., n) of size r \u00d7 c, k kernels of size\na \u00d7 b are set. For each kernel patches a small image(Xs) in the large image (Xl), K(kerneli, Xs) is computed, where K() is the kernel function, giving us k \u00d7 (r \u2212 a + 1) \u00d7 (c \u2212 b + 1) array of convolved features (more detail,see the tutorial3). Max-pooling is the key module to help training deeper model. It works like this: Expect that there is a 60 \u00d7 60 matrix. Let\u2019s set pooling size 10 \u00d7 10, then the 60\u00d760 matrix will be divided into 36 small matrixes of size 10\u00d7 10. Just pick the biggest value in each small matrix and combine them together. At last a 6 \u00d7 6 matrix instead of 60 \u00d7 60 matrix is gotten. Extending the implementation 4 of the lenet5 (LeCun et al., 1998), the convolutional layer and max-pooling layer are merged as one layer. The structure of ConvNets used is shown in Table 2.\nWith the fourth layer, a fully-connect sigmoidal layer is constructed to classify the output values. After experiments, there are some rules can be concluded:\n\u2022 The quantity of the word embedding dimensions shall be more than 50.\n\u2022 Do not use pooling between the dimensions of word embedding (thus, in Table 2, the size of pooling is 2\u00d71).\n\u2022 Adding more fully-connect sigmoidal layer dose not help in improving F1 score.\n3http://ufldl.stanford.edu/wiki/index.php 4http://deeplearning.net/tutorial/lenet.html#lenet"}, {"heading": "4 Experiment results", "text": ""}, {"heading": "4.1 Corpuses", "text": "Unlike English corpuses, Chinese corpuses are relatively small and usually focus on POS tagging (Mingqin et al., 2003), parsing (Xue et al., 2005) and translating (Xiao, 2010). In Chinese sentiment classification, the most popular corpus is ChnSentiCorp (Tan and Zhang, 2008) with 7,000 positive reviews and 3,000 negative reviews5. Since the amount of data collected by previous Chinese NLP researchers is too small for our work, we build a new corpus, MioChnCorp, with a million Chinese hotel reviews. The corpus is public and can be downloaded directly6. The reviews are crawled and filtrated from the website7 which has coarse-grained rating (5-star scale) for each review. We give up the 3-star reviews which may be ambiguous, and mark the five-star and four-star reviews as positive and the rest as negative. Finally 908189 positive reviews and 101762 negative reviews are obtained. After word segmentation8 being done, the sentiment classification process is executed.\nSince ChnSentiCorp is small, the result may be unstable. Thus, Tan and Zhang (2008) gave the best performance and mean performance to evaluate a\n5http://www.datatang.com/data/11936 6http://pan.baidu.com/s/1dDo9s8h 7http://www.dianping.com/hotel 8https://github.com/fxsjy/jieba\nclassification method. Zhai et al. (2011) tried to get a believable result using the average value from 30 experiments. See Figure 2, Naive Bayes and Logistic Regression are used as classification methods to show the performance curves with different amount of reviews. The first sub-graph is tested on ChnSentiCorp, the second on is tested on MioChNCorp. Balanced corpuses are split into 3 equal-sized folds, two for training, the rest for testing. After repeating each experiment five times, the best performance and worst performance are marked. At last, two conclusions can be made: Firstly, when the amount of reviews is less than 60,000, the performance will be improbable (the best performance of model minus worst performance is bigger than 0.01). Secondly, more data usually help to get better performance, but the performance will be finally stable when data are big enough (e.g., 120,000 reviews)."}, {"heading": "4.2 The performance measure", "text": "F1 score (also called F-measure) is a measurement of a test\u2019s accuracy which combines recall and precision as follows:\nF1 = 2 \u00b7 Precision \u00b7Recall Precision+Recall\nRecall = correct positive predictions amount\npositive example amount\nPrecision = correct positive predictions amount\npositive predictions amount\nsince there are two categories (positive and negative) in MioChnCorp, Macro F1 is used to evaluate the performance of classification method over the corpus\nMacro F1 = Postive F1 +Negative F1\n2\nin the rest of this article, F1 score means Macro F1 score."}, {"heading": "4.3 Experimental design", "text": "Nine methods are designed to classify MioChnCorp using different features. NB and ME use 10543 words (the sentimental words and CHI words). LinearSVC use unigram and bigram. Five methods (SVC, LR, AdaBoost, Gradient Tree Boosting (GBT) and Random Forests (RF)) use the average vectors of word embeddings and CHI words (extending the dimension quantity to 450). CNN use the matrix constructed by word embeddings from words in a review as feature.\nThough all of these models are effective, the combination of different machine learning methods is supposed to acquire better F1 score. There are two ways to combine those methods. First is vote, the idea is simple, \u201cthe minority is subordinate to the majority\u201d (marked as Vote all ). The other way is to over-fit in the validate set. Add one more fold for validating into these tree folds. After training, nine models will be constructed. And each model gives one predication list for validating set. For each review, there are nine predications (e.g., [0 0 1 0 1 0 0 1 0 ], 0 means negative, 1 means positive). Using the predication vectors of validating reviews as input, and the labels of validating reviews as the Logistic Regression output, after training on validating set, the combination model (called LR all) is built to test on testing set. The Framework of LR all is shown in Algorithm 1."}, {"heading": "4.4 Comparison and analysis", "text": "Table 3, Table 4 and Table 5 show the performance of different machine learning methods. Subjected to hardware recourse (RAM:8G, CPU:Intel I5, GPU:GTX960M), the experiments are explored over corpuses with tree size: 40,000, 80,000 and 120,000. Each corpus is divided into four folds\nAlgorithm 1 Framework of combination model Input:\nThe experiment set of labelled samples: train set, validate set and test set; n machine learning classifiers (marked as Ci, i = 1, ..., n) with default parameters\nOutput: For each classifier Ci, train on train set;\n1: Test on validate set by Ci and storing predication as validate list i; 2: Use logistic regression to predicate the label list of validate set by combination data of validate list i (i = 1, ..., n) and store the model as LR all; 3: For each classifier Ci, test over test set, and storing the predication as test list i; 4: Use test list i as input of LR all and produce final predication of test set, storing as test list\n5: return Ci (i = 1, ..., n), LR all, test list;\nwhich are equal in size, two for training, one for validating, the rest for testing.\nThere are nine methods to construct the LR all model, but not all of them make contribution. Weka Explorer9 provides attribute selection module to choose most useful attributes to the target attribute (namely, the label list of validate set in our situation). Extracting the validate list i (0 <= i < n) used in Algorithm 1, and combining these nine prediction lists with the label list of validate set, totally, ten attributes will be gotten. With 10-fold cross-validation, CfsSubsetEval attribute evaluator and BestFisrt search Method, Weka selects five most valuable attributes (ME, SVC, LinearSVC, RF and CNN). It is reasonable because they are most outstanding machine learning models which represent their own feature selection methods. Considering the limit of hardware resource and running time, LR is used to instead of SVC and CNN is abandoned. The result is shown in Figure 3. To our surprise, even only four feature is chosen, the F1 score is not reduced.\nThe more reviews we use in model building, typically the better performance we get till the per-\n9http://www.cs.waikato.ac.nz/ml/weka/\nformance is stable. SVM (linearSVC and SVC with linear kernel) has best performance not only in traditional bag of words models, but also in word embedding models. Three ensemble methods work similarly and bigger data help to improve their\nperformance obviously. There may be three reasons why CNN works better than NB and ME, but does not reach the expectant performance. Firstly, the amount of reviews is not big enough to train a deep learning model. Secondly, the architecture of the model may not be enough suitable as a language model. Finally, the features (word embeddings with 60 dimensions) for CNN is not accurate enough to present syntax and semantics in sentence. Vote all does not work well in improving performance, but has the highest negative recall and positive precision. LR all has better performance than Vote all because the same weights chosen by Vote all make these sub-models are equally important."}, {"heading": "5 Conclusion and Future Work", "text": "In this article, an empirical study of sentiment categorization on Chinese hotel review is introduced. In order to conduct this experiment, a Chinese corpus, MioChnCorp10, with a million Chinese hotel reviews is collected. Using MioChnCorp, a word2vec model is trained to present distributed representations of words and phrases in Chinese hotel domain.\nThen the experimental results indicate that the more data we use, the better performance we get. And 60,000 or larger size (e.g. 120,000) of reviews are sufficient in sentiment analysis of Chinese hotel review.\nWhat\u2019s more, we employ word embeddings as input features without any sentiment lexicons, and find such features perform well by using ensemble\n10http://pan.baidu.com/s/1dDo9s8h\nmethods, LR, SVM and CNN. With respect to these learning methods, SVM works best. Though CNN works not as good as expect, it still has better performance than NB and ME. The roles we used to construct the CNN model is introduce in Section 3.\nFinally, a methodology, LR all is constructed to combine different machine learning methods and get an outstanding performance in precision, recall and F1 score of 0.920.\nIn the future, more work will be explored in building better CNN model for Chinese sentimental analysis and constructing combinational model in other tasks of NLP using word embedding."}, {"heading": "6 Acknowledgements", "text": "The financial support for this work is provided by The Fundamental Research Funds for the Central Universities, No. ZYGX2014J065."}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird.,? \\Q2006\\E", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews", "author": ["Dave et al.2003] Kushal Dave", "Steve Lawrence", "David M Pennock"], "venue": "In Proceedings of the 12th international conference on World Wide Web,", "citeRegEx": "Dave et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dave et al\\.", "year": 2003}, {"title": "Ensemble methods in machine learning", "author": ["Thomas G Dietterich"], "venue": "In Multiple classifier systems,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "HowNet and the Computation of Meaning", "author": ["Dong", "Dong2006] Zhendong Dong", "Qiang Dong"], "venue": "World Scientific", "citeRegEx": "Dong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2006}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["Jerome H Friedman"], "venue": "Annals of statistics,", "citeRegEx": "Friedman.,? \\Q2001\\E", "shortCiteRegEx": "Friedman.", "year": 2001}, {"title": "Feature selection and negative evidence in automated text categorization", "author": ["Fabrizio Sebastiani", "Maria Simi"], "venue": "In Proceedings of KDD", "citeRegEx": "Galavotti et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Galavotti et al\\.", "year": 2000}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["LeCun et al.1998] Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Experimental study on sentiment classification of chinese review using machine learning techniques", "author": ["Li", "Sun2007] Jun Li", "Maosong Sun"], "venue": "In Natural Language Processing and Knowledge Engineering,", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "A survey of opinion mining and sentiment analysis", "author": ["Liu", "Zhang2012] Bing Liu", "Lei Zhang"], "venue": "In Mining text data,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLTNAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Building a large chinese corpus annotated with semantic dependency", "author": ["Mingqin et al.2003] Li Mingqin", "Li Juanzi", "Dong Zhendong", "Wang Zuoying", "Lu Dajin"], "venue": "In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume", "citeRegEx": "Mingqin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mingqin et al\\.", "year": 2003}, {"title": "A study of information retrieval weighting schemes for sentiment analysis", "author": ["Paltoglou", "Thelwall2010] Georgios Paltoglou", "Mike Thelwall"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Paltoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paltoglou et al\\.", "year": 2010}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Scikit-learn: Machine learning in python", "author": ["Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Inducing features of random fields", "author": ["Vincent Della Pietra", "John Lafferty"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Generalized boosted models: A guide to the gbm", "author": ["Greg Ridgeway"], "venue": "package. Update,", "citeRegEx": "Ridgeway.,? \\Q2007\\E", "shortCiteRegEx": "Ridgeway.", "year": 2007}, {"title": "An empirical study of sentiment analysis for chinese documents", "author": ["Tan", "Zhang2008] Songbo Tan", "Jin Zhang"], "venue": "Expert Systems with Applications,", "citeRegEx": "Tan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2008}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["Peter D Turney"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Turney.,? \\Q2002\\E", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "How different is translated chinese from native chinese? a corpus-based study of translation universals", "author": ["Richard Xiao"], "venue": "International Journal of Corpus Linguistics,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["Xu", "Sarikaya2013] Puyang Xu", "Ruhi Sarikaya"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Construction and analysis of emotional corpus", "author": ["Xu et al.2008] LH Xu", "HF Lin", "Jing Zhao"], "venue": "Journal of Chinese information processing,", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005] Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer"], "venue": "Natural language engineering,", "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Sentiment classification for chinese reviews: A comparison between svm and semantic approaches", "author": ["Ye et al.2005] Qiang Ye", "Bin Lin", "Yi-Jun Li"], "venue": "In Machine Learning and Cybernetics,", "citeRegEx": "Ye et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2005}, {"title": "Exploiting effective features for chinese sentiment classification", "author": ["Zhai et al.2011] Zhongwu Zhai", "Hua Xu", "Bada Kang", "Peifa Jia"], "venue": "Expert Systems with Applications,", "citeRegEx": "Zhai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "learning\u201d which is firstly proposed to determine whether a review is positive or negative by using three machine learning methods, including NB, ME and SVM (Pang et al., 2002).", "startOffset": 156, "endOffset": 175}, {"referenceID": 22, "context": "And an overall score is calculated to assign the review to a specific class (Turney, 2002).", "startOffset": 76, "endOffset": 90}, {"referenceID": 20, "context": "According to Liu and Zhang (2012), the sentiment analysis research mainly started from early 2000 by Turney (2002) and Pang et al.", "startOffset": 101, "endOffset": 115}, {"referenceID": 16, "context": "According to Liu and Zhang (2012), the sentiment analysis research mainly started from early 2000 by Turney (2002) and Pang et al. (2002). Turney", "startOffset": 119, "endOffset": 138}, {"referenceID": 2, "context": "the score function based on words in positive and negative reviews (Dave et al., 2003) and feature weighting schemes used to enhance classification accuracy (Paltoglou and Thelwall, 2010).", "startOffset": 67, "endOffset": 86}, {"referenceID": 16, "context": "In such way, Pang et al. (2002) considered classifying documents according to standard machine learning techniques.", "startOffset": 13, "endOffset": 32}, {"referenceID": 27, "context": "However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005).", "startOffset": 146, "endOffset": 163}, {"referenceID": 27, "context": "However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005). Li and Sun (2007) presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes.", "startOffset": 147, "endOffset": 183}, {"referenceID": 27, "context": "However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005). Li and Sun (2007) presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes. They found that SVM achieved the best performance. After that, Tan and Zhang (2008) found 6,000 or bigger for the size of features would be sufficient for Chinese", "startOffset": 147, "endOffset": 415}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.", "startOffset": 2, "endOffset": 26}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.", "startOffset": 2, "endOffset": 52}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al.", "startOffset": 2, "endOffset": 272}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al. (2014) got some state-of-the-art performance.", "startOffset": 2, "endOffset": 303}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al. (2014) got some state-of-the-art performance. But the work of Collobert et al. (2011) was paid most attention for describing a unified architecture for NLP tasks which learned features by training a deep neural network even when being given very limited prior knowledge.", "startOffset": 2, "endOffset": 382}, {"referenceID": 25, "context": "A combination of two Chinese sentiment lexicons (Hownet (Dong and Dong, 2006) and DLLEX (Xu et al., 2008)) is constructed, including 30406 words in total.", "startOffset": 88, "endOffset": 105}, {"referenceID": 6, "context": "After several experiments, CHI (Galavotti et al., 2000) is chosen for information gain.", "startOffset": 31, "endOffset": 55}, {"referenceID": 19, "context": "After fifteen iterations of the improved iterative scaling algorithm (Pietra et al., 1997) implemented in Natural Language Toolkit (Bird, 2006), the parameters of \u03bbi,cj are adjusted to maximize the entropy of distribution of training data.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": ", 1997) implemented in Natural Language Toolkit (Bird, 2006), the parameters of \u03bbi,cj are adjusted to maximize the entropy of distribution of training data.", "startOffset": 48, "endOffset": 60}, {"referenceID": 18, "context": "For SVM models, python tool scikit-learn (Pedregosa et al., 2011) is chosen for training and testing.", "startOffset": 41, "endOffset": 65}, {"referenceID": 3, "context": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one.", "startOffset": 17, "endOffset": 67}, {"referenceID": 5, "context": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one.", "startOffset": 17, "endOffset": 67}, {"referenceID": 20, "context": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one.", "startOffset": 17, "endOffset": 67}, {"referenceID": 9, "context": "Extending the implementation 4 of the lenet5 (LeCun et al., 1998), the convolutional layer", "startOffset": 45, "endOffset": 65}, {"referenceID": 14, "context": "atively small and usually focus on POS tagging (Mingqin et al., 2003), parsing (Xue et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 26, "context": ", 2003), parsing (Xue et al., 2005) and translating (Xiao, 2010).", "startOffset": 17, "endOffset": 35}, {"referenceID": 23, "context": ", 2005) and translating (Xiao, 2010).", "startOffset": 24, "endOffset": 36}, {"referenceID": 28, "context": "Zhai et al. (2011) tried to get a believable result using the average value from 30", "startOffset": 0, "endOffset": 19}], "year": 2015, "abstractText": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.", "creator": "LaTeX with hyperref package"}}}