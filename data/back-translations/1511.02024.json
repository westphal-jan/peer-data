{"id": "1511.02024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Towards a Better Understanding of Predict and Count Models", "abstract": "In a recent paper, Levy and Goldberg pointed out an interesting link between predictive word embedding models and counter models based on meaningful mutual information. Under certain conditions, they showed that both models ultimately optimize equivalent objective functions. This paper examines this relationship more closely and explains the factors that lead to differences between these models. We find that the most relevant differences from an optimization perspective (i) predict models work in a low-dimensional space where vectors can interact strongly; (ii) because predictive models have fewer parameters, they are less prone to overadjustments.", "histories": [["v1", "Fri, 6 Nov 2015 10:29:26 GMT  (199kb,D)", "http://arxiv.org/abs/1511.02024v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["s sathiya keerthi", "tobias schnabel", "rajiv khanna"], "accepted": false, "id": "1511.02024"}, "pdf": {"name": "1511.02024.pdf", "metadata": {"source": "CRF", "title": "Towards a Better Understanding of Predict and Count Models", "authors": ["S. Sathiya Keerthi", "Tobias Schnabel", "Rajiv Khanna"], "emails": [], "sections": [{"heading": null, "text": "Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible."}, {"heading": "1 Introduction", "text": "Distributional semantic models [12], also called count models [1] have been popular in the computational linguistics literature for several decades. In the last few years, however, predict models such as the Skip-Gram model and Continuous Bag-Of-Words model have become the de facto standard for word modeling [5, 6]. These methods have origins in neural language modeling [7], where the goal was to improve classic language models. A recent evaluation study [1] suggested that the predict models are superior to the count models on a range of word similarity tasks. Newer work, however, attributes a large amount of the differences in performance between count and predict models to a lack of proper\n\u2217Cloud & Information Services Lab, Microsoft, Mountain View, CA 94043 \u2020Department of Computer Science, Cornell University, Ithaca, NY 14853 \u2021Department of Electrical and Computer Engineering, The University of Texas at Austin,\nAustin, TX 78712\nar X\niv :1\n51 1.\n02 02\n4v 1\n[ cs\n.L G\n] 6\nN ov\nhyperparameter optimization [4]. This has prompted the need for understanding the differences between the two types of models.\nTo this end, Levy and Goldberg [2] made an interesting connection between two key models: a traditional count model based on pointwise mutual information (PMI) and a predict model, namely the skip-gram model with negative sampling (SGNS). The key result is that the SGNS model is equivalent to a shifted version of the PMI method, where all values get shifted by a factor of log k. However, the proof assumes that the input and output dimensions are the same; hence the word vectors will still be of the size of the vocabulary. As we show in our analysis, this assumption has important implications for subsequent steps, such as dimensionality reduction methods like singular value decomposition (SVD).\nThe aim of this paper is to analyze connections between count and predict models in a more detailed fashion. In particular, we investigate the differences between SGNS and PMI methods more deeply. We make several observations that help this understanding. The first useful result is that the Shifted PMI model comes out as an explicit version of SGNS in which the context word vectors are fixed at their one-hot representations. We point out two differences between the Shifted PMI model and the SGNS model from an optimization perspective. First, SGNS usually works in lower dimensions, making it possible for all word vectors two interact. Second, as count models have usually more parameters, this might cause them to overfit on the data.\nWe use the insight from our analysis to propose several interesting extensions to classic word embedding models. For example, our analysis allows regularization to be added into PMI methods in a systematic way. We also propose a new convex word vector model based on CBOW which offers interpretability and a well-defined training objective.\nIn short, we draw connections between existing count and prediction models and augment them in several ways. However, in order to fully understand the factors that differentiate various methods, a comprehensive set of experiments is needed. We plan to carry out these experiments as part of future work."}, {"heading": "2 Notations", "text": "Consider a corpus D = {wt}|D|t=1 that is a sequence of words over a vocabulary V. We will use w when referring to the index of a target word and c when referring to the index of a context word. We define the exact notion of context in Section 2.1. Note that w and c are integers taking values from 1 to |V|.\nEach word and context word has a vectorial representation. We will use w \u2208 Rm and c \u2208 Rn to denote the word vector and context vector corresponding to word w and context word c. For vector x, xi will denote the i-th component of x. Let {w} and {c} denote the set of all word and context word vectors. One can also think of {w} and {c} as matrices W and C whose w-th and c-th rows are given by w and c respectively. We will use W and C to synonymously refer to {w} and {c}.\nLet us use # to denote a count and weighting function. In the simplest case, for example, #(w, c) could denote the number of occurrences of the (word, context word) pair (w, c) in corpus D. Let us define #(w, \u00b7) = \u2211 c\u2208V #(w, c)\nand #(\u00b7, c) = \u2211\nw\u2208V #(w, c), the total counts for word w and context vector c."}, {"heading": "2.1 Defining context", "text": "An important design choice in count models is the definition of context. The definition of context determines which co-occurrences get considered when creating a model, along with the importance of single co-occurrences. A common way of defining context is based on choosing a window around each occurrence of a word w in D. There are many options to customize this window; for example, it can either be symmetric or asymmetric around w. There could also exist weights (e.g., dependent on the relative positions of w and c) associated with each (w, c) occurrence in D. One may also include down-weighting of common words, possibly for both for the current word w as well as for the context words c1. All these choices determine the value of #(w, c). Note that since the weightings can be real, #(w, c), #(w, \u00b7) and #(\u00b7, c) can take non-negative real values in general.\nIf the window is symmetric around w and as well as the weighting functions are symmetric, then\n1. #(w, c) is symmetric, i.e., #(w, c) = #(c, w) and\n2. #(w, \u00b7) = #(\u00b7, c) if w and c denote the same word\nWe will refer to this as the symmetry condition. In Section 3.2.1 we will revisit this condition and see that it helps our understanding of non-uniqueness and interchangeability of word vectors and context word vectors.\nExample. Consider an asymmetric window K that includes l words to the left and r words to the right of a word w. In other words,\nK = (\u2212l, \u00b7 \u00b7 \u00b7 ,\u22121, 1, . . . , r).\nTake one (word, context word) pair (wt, wt+i), where i \u2208 K. Let P (wt) be the probability with which word wt is chosen for inclusion into the training set. For example, Mikolov et al [6] suggest the following down-sampling probability:\nP1(w t) = 1\u2212\n\u221a \u03c4\nf(wt) (1)\nwhere f(wt) is the frequency of word wt in the corpus D, and \u03c4 is a threshold. Let P2(w\nt+i) be the weight associated with wt+i as context word. Here again we may want to down-sample frequent words similar to (1). Let P3(i) be the weight associated with the position of the context word wt+i in the context\n1Mikolov et al [6] used down-weighting of common words. It is unclear if Levy and Goldberg [2] used it.\nwindow. Using all these individual weights we can define the overall weight for the (wt, wt+i) pair:\nP (wt, wt+i) = P1(w t)P2(w t+i)P3(i) (2)\nAccumulating all this info over the entire corpus gives us the total value for (word, context word) pairs:\n#(w, c) = \u2211\n{(t,i):wt=w,wt+i=c}\nP (wt, wt+i) (3)\nNow #(w, \u00b7) and #(\u00b7, c) can be formed using #(w, c) as described in the beginning of this section."}, {"heading": "3 Background", "text": "Remember that the goal of this paper is to investigate the differences between count and prediction models. To do this, we start by summarizing two special instances of count and prediction models. We first introduce PMI models as representatives of count models in Section 3.1, and then discuss the Skip-Gram model with Negative Sampling (SGNS) in Section 3.2."}, {"heading": "3.1 PMI models", "text": "PMI models have been used extensively in distributional semantic models [12] to compute similarities between words. As the name implies, the key quantity in these models is Pointwise Mutual Information (PMI). Its definition and MLE estimate from data are given by\nPMI(w, c) = log P (w, c) P (w)P (c) \u2248 log #(w, c) \u00b7 |D| #(w, \u00b7) \u00b7#(\u00b7, c) , (4)\nwhere #(w, c) is the simple count function. For a given word w, PMI represents w by forming a vector whose components are PMI(w, c) for all c \u2208 V .\nNote that PMI is not defined when #(w, c) = 0. To circumvent this problem, Positive PMI (PPMI) replaces all negative values, i.e.,\nPPMI(w, c) = max(PMI(w, c), 0).\nMore recently, Levy and Goldberg [2] have defined shifted variants of PMI and the PPMI metrics. Shifted PMI (SPMI) is just defined as PMI(w, c)\u2212 log k, and Shifted Positive PMI (SPPMI) is defined as max(SPMI(w, c), 0) where k is some positive integer. PMI and PPMI are, respectively, special cases of Shifted PMI and Shifted PPMI, with k set to 1. We will return to Shifted PMI and Shifted PPMI in the next section, where we talk about the equivalence of certain methods.\nAs all vectors produced by PMI have the vocabulary size as the dimensionality, dimensionality reduction techniques are often applied to the original matrix to decrease the memory and computational requirements."}, {"heading": "3.2 Skip-Gram with Negative Sampling (SGNS)", "text": "SGNS [6] is a popular predict model that aims to predict which word w occurs with a context word c. One can derive the SGNS [6] model from a binary classification setting where the target variable y specifies whether a word w occurs with context word c. SGNS tries to solve this task with the logistic loss applied to the input score x = w \u00b7 c.\nRemember that our corpus D is a sequence of words, D = {wt}|D|t=1. To make notation easier, here we assume that the context is defined by all the words within a window Ct around each word wt. Let the corresponding sequence of sets of context words be {Ct}Tt=1. To construct the training set for the SGNS model, one forms one training example for each t and each context word ct \u2208 Ct. For each ct \u2208 Ct, a set of k negative context words, N tct is chosen at random and, the logistic loss function f(wt, ct;N tct) is applied. This loss is the aggregation of losses for one positive example and k negative examples. More formally, the loss of one training example `(wt, Ct) is:\n`(wt, Ct) = \u2211\nct\u2208Ct f(wt, ct;N tct) def = L(wt \u00b7 ct, 1) + \u2211 c\u2208N t\nct\nL(wt \u00b7 c,\u22121), (5)\nwhere L(w \u00b7c, y) is the logistic loss corresponding to the word-context pair (w, c) and target y \u2208 {1,\u22121}. The loss over the entire corpus is then the sum of all individual losses:\n`(W,C) = |D|\u2211 t=1 `(wt, Ct). (6)\nLet us refer to this way of writing the training objective (sum over occurrences in the corpus) as the corpus format.\nLevy and Goldberg [2] showed that, by accumulating data over co-occurrences of words and context words, the objective function in (6) can be rewritten as2:\n`(W,C) = \u2211 w\u2208V \u2211 c\u2208V `w,c(w, c) (7)\n`w,c(w, c) = #(w, c)L(w \u00b7 c, 1) + k \u00b7 #(w, \u00b7) \u00b7#(\u00b7, c)\n|D| L(w \u00b7 c,\u22121) (8)\nLet us refer to this (equivalent) way of writing the training objective as the cooccurrence format. Note that in the co-occurrence format, we do not compute losses for each token individually, but instead compute a loss for each unique (w, c) pair.\nIn (8) we will from now on also consider other loss functions L(x, y) apart from the logistic loss like in the original formulation. We consider general loss functions, L(x, y) (here y \u2208 {1,\u22121} is the target variable) with the logistic loss\n2Levy and Goldberg [2] write the problem as the maximization of likelihood; hence the ` here and the one in [2] are negatives of each other.\nbeing a special case. This generalization allows us to define custom loss functions that can incorporate domain knowledge or other side information. Table 1 lists a set of popular loss functions.\nObservation 1 Unlike SGNS, not all models can be reduced to the co-occurrence format - an example is the CBOW model [5]. In fact, even with the skip-gram model, the reduction to the co-occurrence format is not possible if we use the traditional softmax or hierarchical softmax [7] approach to speed up training. In general, models reducible to the co-occurrence format have to only be based on counts of words, context words and their co-occurrences. That property makes them have close relations with count models [1, 2, 10]. Another model that can be expressed in co-occurrence form is Glove [9]."}, {"heading": "3.2.1 Remarks on symmetry", "text": "In case we assume symmetric windows as outlined in Section 2.1 for counting co-occurrences, we can swap word and context vectors at optimality.\nObservation 2 Suppose (W\u2217, C\u2217) is a minimizer of `. Then (C\u2217, W\u2217) is also a minimizer of `; in other words, at optimality, if the word vectors and context word vectors are completely swapped, optimality still holds. Thus, depending on how the numerical optimization of (7) initialized, W\u2217 and C\u2217 can end up being completely swapped.\nSimilar swap properties can be given for other models in the co-occurrence format, e.g., Glove [9].\nObservation 3 Note that Observation 2 does not mean that W = C. If ` had been a convex function, then, using the fact that any convex combination of optimal solutions is also optimal, one can show the existence of an optimal solution with W = C. However, the objective function ` in (7) is far from convex. Such non-convex objectives are usually associated with optima in which the symmetric components take very different values. In general, even when the symmetry condition does not hold, this discussion indicates that SGNS can end up at different optima depending on the initialization of the optimization process.\nObservation 4 Mikolov et al. [6] derive (7) starting from the problem of predicting a context word given a word. However, when using negative sampling, the symmetry condition implies that we get exactly the same formulation and solution as for a model where we would predict a word given a context word. It is useful to note that this comment does not hold if traditional softmax or hierarchical softmax [7] or noise-contrastive estimation [8] is used to model probabilities with the associated negative likelihood loss."}, {"heading": "4 Connecting count and predict models", "text": "In this section, we revisit the idea that Levy and Goldberg [2] used to connect PMI models with SGNS and extend it. The central step is to explicitly solve SGNS in closed form; we can then see that the explicit solution of SGNS corresponds to the vectors that are constructed in a Shifted PMI model."}, {"heading": "4.1 Solving SGNS in closed form", "text": "Here, we extend the analysis of Levy and Goldberg [2]. We provide closed-form solutions for SGNS and a broad class of loss functions. As Levy and Goldberg showed, it turns out that the closed-form solution of the SGNS objective is equivalent to a solution constructed by the Shifted PMI model. In contrast to Levy and Goldberg, we apply approximate quadratic analysis to the SGNS objective to give a more detailed insight for a broad class of loss functions.\nThe central idea of the analysis is to assume that, given a sufficient number of dimensions, each score x = w \u00b7 c can be minimized independently of all other scores. Let us define\n\u03c1w,c(x) = `w,c(w, c) = \u03c1w,c(w \u00b7 c). (9)\nTo get x\u2217w,c, the minimizer of \u03c1w,c, we solve\n\u03c1\u2032w,c(x) = #(w, c)L \u2032(x, 1) + k \u00b7 #(w, \u00b7) \u00b7#(\u00b7, c)\n|D| L\u2032(x,\u22121) = 0 (10)\nThe Taylor series around x\u2217w,c is given by\n\u03c1w,c(x) = const. + 1\n2 \u03b1w,c(x\u2212 x\u2217w,c)2 (11)\n\u03b1w,c = (#(w, c)L \u2032\u2032(x\u2217w,c, 1) + k \u00b7#(w, \u00b7) \u00b7#(\u00b7, c) |D| L\u2032\u2032(x\u2217w,c,\u22121)) (12)\nwhich, in terms of `w,c, is\n`w,c(w, c) = const. + 1\n2 \u03b1w,c(w \u00b7 c\u2212 x\u2217w,c)2 (13)\nLet\n\u03b4w,c = #(w, c) + k \u00b7#(w, \u00b7) \u00b7#(\u00b7, c)\n|D| (14)\nFor the loss functions listed in Table 1, \u03b1w,c takes the form\n\u03b1w,c = \u03b3w,c\u03b4w,c (15)\nTable 2 gives details for the various loss functions.\nObservation 5 When #(w, c) = 0, the first terms in (8), (10) and (12) involving L(x, 1) go away, causing x\u2217w,c to take the extreme value of \u2212\u221e for the logistic loss and \u22121 for other losses. Consider the expressions for x\u2217w,c in Table 2 to verify this. This issue does not arise for SGNS because it operates with reduced dimensional embeddings of words and context words in which information associated with various (word, context word) pairs interact heavily. We believe that this is a crucial difference between SGNS and PMI methods.\nObservation 6 For building semantic representations that are used for computing similarity, it is often not desirable to have negative components in the word vectors. PosCondition in the last column of Table 2 checks when this case occurs. This condition turns out to be the same for all losses, which is interesting. There is no difference between squared loss, squared hinge loss and Huber loss. This is because, in the interval x \u2208 [\u22121, 1], all these losses have identical \u03c1w,c(\u00b7) and the minimizer of \u03c1w,c always occurs in [\u22121, 1]. These three losses have an expression for x\u2217w,c that is quite different from that for the logistic loss. Hinge loss prefers to set x\u2217w,c at the extremes: 1 or \u22121."}, {"heading": "4.2 Connecting SGNS and Shifted PMI", "text": "One of the key results of Levy and Goldberg [2] is that the vectors created by Shifted PMI are a solution to the SGNS objective. We use the quadratic analysis of Section 4.1 to say this more cleanly.\nLet ei denote the unit vector in R|V| whose i-th component is 1 and all other components are zero. Suppose we use a one-hot representation for context vectors, i.e., we fix c = ec for all c. Thus, we are fixing C.\nObservation 7 Suppose we fix each context vector to the one-hot representation given above. Then, only W remains as the set of variables, and the following hold.\n(a) The minimizer W\u2217 of (8) is given by w = x\u2217w \u2200w, where x\u2217w is a vector with {x\u2217w,c}c as components.\n(b) x\u2217w,c is the Shifted PMI representation as defined in Section 3.1.\n(b) Also, (W\u2217,C) together form an optimal solution of SGNS.\nIn other words, we have a closed-form solution for SGNS (assuming m = n = |V|). Though Levy and Goldberg [2] do not mention the above construction, this is a simple observation that easily follows from their analysis.\nProof of Observation 7. Let\u2019s take one c. By the way we defined c, we have w \u00b7 c = wc. In ` given by (7), the variable wc occurs only in the term `w,c. Therefore, w \u2217 c = arg min `w,c = x \u2217 w,c, which proves part (a) of Observation 7. Part (c) follows from the fact that, the pair, (W\u2217,C) is such that `w,c is minimized for every (w, c) pair and it is not possible to do better than that."}, {"heading": "5 Count and predict models: differences", "text": "Empirically, there seems to be evidence that predict and count models perform differently (e.g., [1]). This is interesting since they all consider the same input data \u2013 namely the co-occurrences of words in text. What are the reasons for this? Although the previous section pointed out a strong connection between SGNS and PMI methods (with Observation 7 even indicating a near equivalence), we believe there are two main differences between the two methods pertaining (a) the dimension of the embeddings, and (b) C being fixed as the one-hot representation. Let us now expand on the two factors.\nObservation 8 Dimension of embeddings. As already mentioned in Observation 5, the small embedding dimension used by SGNS for words can be crucial for learning good word vectors. For example, co-occurring words can influence each other. The full dimension used by PMI methods does not allow this to happen; the full dimension also has the disadvantage of suffering from overfitting due to an excessive number of variables.\nObservation 9 One-hot representation for C. Similar to the previous observation, there is a difference in which variables are optimized. SGNS operates with both, W and C as variables. As shown in Subsection 4.2, PMI methods, on the other hand, correspond to using a fixed one-hot representation for C. An important consequence of such a representation is that it does not allow close context words to influence each other. Future work should empirically investigate the role of this factor."}, {"heading": "5.1 Further differences in Shifted PPMI", "text": "Recall that the solution for SGNS given by Shifted PMI is unusable in practice, since we have entries that are \u2212\u221e. Also, the Shifted PMI solution has a high number of dimensions, and this might not be useful in practice. Levy and Goldberg present two heuristics remedy these problems. First, they propose omitting negative terms in the objective function to make a solution feasible. Second, they suggest a subsequent SVD step to reduce the dimensionality of the Shifted PPMI matrix. We below discuss each of these heuristics and their consequences in more detail."}, {"heading": "5.1.1 Omitting terms", "text": "Since in practice, we cannot work with vectors that have \u2212\u221e entries, Goldberg and Levy propose Shifted PPMI instead of Shifted PMI to remedy this problem. Shifted PPMI corresponds to leaving out all (w, c) terms from (7) that have negative Shifted PMI values. This is equivalent to a modified SGNS method in which during negative sampling examples not satisfying the PosCondition, i.e., those with PMI\u2212 log k < 0 are left out.\nThere is two issues with the above approach. First, we no longer can guarantee optimality for this solution. Second, this also seems inconsistent with the main idea behind negative sampling, which is to sample from unobserved (w, c) pairs. Levy and Goldberg [2] make the following statement in the second paragraph of Section 5.1 of their paper: \u201cWe observe that Shifted PPMI is indeed a near-perfect approximation of the optimal solution, even though it discards a lot of information when considering only positive cells.\u201d However, this does not explain the role of the discarded terms. In particular, when training SGNS with a low number of embedding dimensions, discarding those terms could be of real importance."}, {"heading": "5.1.2 Applying SVD", "text": "Levy and Goldberg also propose to apply SVD to the SPPMI matrix in order to obtain low-dimensional embeddings. If we follow this step, we loose again some of the optimality we had with the Shifted PMI solution. More formally, consider a SPPMI matrix M whose (w, c)\u2019th term is max(0, x\u2217w,c). To form word vectors of dimension d lower than |V|, Goldberg and Levy suggest to apply SVD to the matrix M . Let M = U\u03a3V T denote the SVD. If one is interested in an embedding of dimension d < |V|, the best rank d approximation of M , Ud\u03a3dV T d is used. To form word vectors, one can use either W SVD\nd = Ud\u03a3d or the \u201csymmetric version\u201d, WSSVDd = Ud \u221a \u03a3d.\nObservation 10 Levy and Goldberg [2] propose that SVD is done on M or one of its variants.3 However, if remaining faithful to SGNS is the aim, (13)\n3Levy and Goldberg [2] recommend using the matrix corresponding to Shifted PPMI.\nindicates that the weighting term \u03b1w,c also be included and that we solve\nmin W,C\n1\n2 \u2211 w,c \u03b1w,c(w \u00b7 c\u2212 x\u2217w,c)2 (16)\nIt is non-trivial to solve this problem; an SVD based approach will not work.\nLet us now look at the limiting full case, i.e., d = |V|, to point out some relations and differences within the methods in the PMI class.\nObservation 11 We can use the full SVD of M and define word vectors WFull = U\u03a3 and WSymmFull = U \u221a \u03a3. Note that these word vectors are not the same as Shifted PPMI which uses M itself as word vectors. However, because MMT = U\u03a3V TV \u03a3UT = U\u03a3\u03a3UT = WFullW T Full, the dot products between any two word vectors is identical for WSPPMI (i.e., M) and WFull. On the other hand, in general, WSymmFull (W Symm Full )\nT 6= MMT . What this means is that SVD is consistent with Shifted PPMI, but Symmetric SVD is not consistent with Shifted PPMI.\nLevy and Goldberg [2] recommend Shifted PPMI and refer to the spectral word vectors for it as SVD and Symmetric SVD. Their experiments showed the symmetric version to yield better results than SGNS."}, {"heading": "5.2 Summary", "text": "The above subsections went into various ways of connecting SGNS and PMI methods and also brought out various differences. Figure 1 gives a rough and\nquick view of the various morphings of SGNS into different PMI methods. The use of a small embedding dimension by SGNS as opposed to the full dimensional one hot representation used by PMI methods is probably the most important difference. The discarding of certain negative examples, the approximation of the original non-linear objective by a quadratic, and the leaving out of the \u03b1w,c factors from the quadratic objective, are additional differences. Carefully designed experiments are needed to understand the individual effects of each difference on the quality of the resulting word vectors. The various differences also imply that, even though the methods at the two left ends of the figure involve low dimensional embeddings, they could be vastly different due to the various reasons given in this section."}, {"heading": "6 Extensions", "text": "In the following two subsections, we propose two extensions to the basic PMI model. First, we show how to add regularization to PMI models and give explicit solutions for L2 and L1 regularization. We hope that regularization will help improve issues with data sparsity. After that, we propose a new convex model for word embeddings that is not only easy to learn, but also yields intuitive word vectors since each dimension corresponds exactly to one context word."}, {"heading": "6.1 Adding regularization to PMI methods", "text": "Let us continue with the formulation of Section 4.1 and add regularization. If overfitting is one of the causes of the inferior performance of PMI methods compared to SGNS, then regularization should help improve performance. Consider the decoupled determination of x = wc. Our modified objective is now\nargmin x \u03c1w,c(x) +Rw,c(x)\nwhere Rw,c is the regularization term. As we argued in Section 4.1, each wc is decoupled from other variables. Hence, we can solve each one-dimensional optimization problem in isolation \u2013 even with regularization added. We now derive closed-form solutions for L2 and L1 regularization."}, {"heading": "6.1.1 L2 regularization", "text": "We can down-weigh frequently occurring words and context words and add a standard L2 regularizer as follows:\nRw,c(x) = \u03bb #(w, \u00b7) \u00b7#(\u00b7, c)\n|D| R(x) where R(x) =\n1 2 x2 (17)\nWe will discuss L1 regularizer R(x) = |x| later. Let us also focus only on the logistic loss. Dividing by\n#(w,\u00b7)\u00b7#(\u00b7,c) |D| we get\nw\u2217c = arg min x ePMI log(1 + e\u2212x) + k log(1 + ex) +\n\u03bb 2 x2 (18)\nSetting the derivative of the objective to zero and simplifying, we get w\u2217c to be the solution of\n\u03bbx = h(x) where h(x) = ePMI \u2212 kex\n1 + ex . (19)\nThe line from A to B in Figure 2 approximates h(x) nicely in the region where the optimal x lies. The equation of this line is given by\nh\u0303(x) = ePMI \u2212 k 2 \u2212 e PMI \u2212 k 2(PMI\u2212 log k) x (20)\nSolve for h\u0303(x) = \u03bbx to get the optimal x as\nx = Harmonic mean of (PMI\u2212 log k, e PMI \u2212 k\n2\u03bb ) (21)"}, {"heading": "6.1.2 L1 regularization", "text": "Similarly, it is easy to work out closed form expressions for w\u2217c for L1 regularization. In this case, many of the optimal values go to zero: the larger the value of \u03bb, more is the number of zeros. There are three cases to consider. For L1 regularization, the value, h(0) = e\nPMI\u2212k 2 plays a key role.\nCase 1. \u2212\u03bb \u2264 h(0) \u2264 \u03bb. For this case it is easy to check that, at x = 0, the left hand side derivative of the objective in (18) is non-positive, and the right hand side derivative is non-negative. Hence w\u2217c = 0 is the optimal solution.\nCase 2. h(0) > \u03bb. For this case, the right hand side derivative at x = 0 is negative and so the optimum value is positive. The optimum is found by solving h(x) = \u03bb, which yields\nw\u2217c = log ePMI \u2212 \u03bb k + \u03bb\n(22)\nCase 3. h(0) < \u2212\u03bb. For this case, the left hand side derivative at x = 0 is positive and so the optimum value is negative. The optimum is found by solving h(x) = \u2212\u03bb, which yields\nw\u2217c = log ePMI + \u03bb\nk \u2212 \u03bb (23)"}, {"heading": "6.1.3 Discussion", "text": "Adding regularization to count models has two benefits. First, while the nonregularized solution (see Table 2; same as SPMI) is unusable because x\u2217w,c goes to extreme values when #(w, c) = 0, regularized solutions are always well-defined. Second, we expect regularization to help if overfitting is degrading performance with PMI models. Future work is needed to empirically study and validate the usefulness of regularization in count models."}, {"heading": "6.2 A convex formulation for word vectors", "text": "Motivated by the analysis in Section 4.1, we develop a new and simple convex formulation for determining word vectors. Similar to the skip-gram model, we phrase our model as the task of predicting a word conditioned on context. However, instead of learning representations for both words and their context words, we fix the context vectors to a one-hot representation. This makes our objective function convex. Another advantage is that this results in intelligible models \u2013 each vector entry refers to a weight given to a context word. Also, to obtain compact representations, we instead use L1 regularization on the word vectors instead of learning embeddings in lower-dimensional spaces like traditional predict models.\nLet us describe the formulation in some more detail. Let m denote the dimension of the vector representation of context. Depending on the context modeling employed, m can be one to several times of |V|. Let z \u2208 Rm denote the context representation. z is a function of the context, C, written as z(C). The weight vectors {w} are also points in Rm. The score of a positive example is w \u00b7z(C). Remember that the input corpus D is a sequence of words, {wt}Tt=1. Let the corresponding sequence of contexts be {Ct}Tt=1. We solve the following minimization problem.\nmin W |V|\u2211 w=1 \u03bb\u2016w\u20161 + 1 T T\u2211 t=1 f(wt; z(Ct)) (24)\nIn the above, \u03bb > 0 is a regularization constant4 which can be tuned to balance sparsity of W and performance; f includes the effect of positive as well as negative examples. Other forms of regularization are also possible, for example a combination of L1 and L2 regularization (Generalized Elastic net).\nContext. We can define context in several ways. Here are a few possibilities:\n1. A simple way is to have a context z \u2208 R|V|, with each dimension corresponding to one word in the vocabulary V . Similar to the SGNS model, we only assume individual interactions between words and context pairs, i.e., each z will exactly contain one non-zero entry. This way, we can rewrite the objective in the co-occurrence form and obtain all vectors in closed form by accumulating the statistics for each (word, context word) pair.\n2. We can do better than option 1 above and pair a word with a window of context words simultaneously, i.e., z can now have multiple non-zero entries. A bag-of-words representation z \u2208 R|V| can be used to represent the context input using the context words. For example, we can set zc = \u03c1(c, w) if c occurs in the context window and zero otherwise, where \u03c1(c, w) is some weighting that is a function of how far c is from w.\n3. If we want to make use of word order, we can define L\u00d7|V| context inputs where L is the length of the context window and one block of |V| inputs is used for each context word, and blocks concatenated in the proper order of occurrence in the window.\n4. Depending on the purpose for which the word vectors are developed we can also use dependency-tree based context features [3]. Again, we can use one-hot representations of these features.\nLearning. Except option 1, aggregating at the individual (w, c) level (to convert the objective into the co-occurrence form (7)) as well as forming a closed form solution, is in general infeasible. This means we have to train our model\n4One can also think of schemes for making \u03bb dependent on w, for example, make \u03bbw dependent on the number of context words to which word w is attached.\nusing the objective in corpus format. To do the training, we can choose from various optimization strategies. We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].\nWe now present examples for two of the four optimization strategies mentioned above. With softmax, our objective is\nf(wt; z(Ct)) = \u2212wt \u00b7 z(Ct) + log (\u2211 w w \u00b7 z(Ct) ) . (25)\nLet us now consider optimization via Negative Sampling. Let {wjneg}kj=1 denote a randomly sampled set of negative examples. Then\nf(wt; z(Ct)) = \u2212 log \u03c3(wt \u00b7 z(Ct))\u2212 k\u2211\nj=1\nlog \u03c3(\u2212wjneg \u00b7 z(Ct)), (26)\nwhere \u03c3(x) = 1/(1 + e\u2212x).\nObservation 12 One of the advantages of the convex formulation described above is interpretability. If the rth component of some w is large and positive, it can be directly translated to the role played by the rth context term of z(C). This direct interpretability also means that domain knowledge may be easy to infuse into the formulation by specification of constraints and extra regularization on the weights.\nObservation 13 The model developed in this section is in between conventional low-dimensional word embeddings (no interpretability, allows for higherorder effects) and distributional word representations (full interpretability, no higher-order effects). Here, the term \u201chigher-order effects\u201d refers to the effects (influences) that occur during the joint optimization of context and word representations, e.g, two words can be made similar because they occur in contexts that are also similar."}, {"heading": "7 Conclusion", "text": "In this report, we showed how to explicitly solve the SGNS objective for a broad range of loss functions. This step allowed us to connect Shifted PMI models to SGNS models under general loss functions. Furthermore, we pointed out two important differences between the Shifted PMI model and SGNS model. First, in the SGNS model, far fewer embeddings are used in practice, making the SGNS model less prone to overfitting. Second, in the PMI model, the context vectors are fixed; hence, there is also no interaction between context vectors and word vectors.\nFinally, we proposed two extensions to existing models. First, we showed how we can incorporate regularization into PMI models to alleviate overfitting.\nSecond, we presented a new embedding model that not only has a convex objective, but also results in intelligible embeddings. Future work is needed to empirically validate our proposed methods and extensions."}], "references": [{"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural word embedding as implicit factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "TACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G. Hinton"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Linking Glove with word2vec.arXiv: 1411.5595v2", "author": ["T. Shi", "Z. Liu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Model-based Word Embeddings from Decompositions of Count Matrices", "author": ["K. Stratos", "M. Collins", "D. Hsu"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P. Turney", "P. Pantel"], "venue": "JAIR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Abstract In a recent paper, Levy and Goldberg [2] pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "Distributional semantic models [12], also called count models [1] have been popular in the computational linguistics literature for several decades.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Distributional semantic models [12], also called count models [1] have been popular in the computational linguistics literature for several decades.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "In the last few years, however, predict models such as the Skip-Gram model and Continuous Bag-Of-Words model have become the de facto standard for word modeling [5, 6].", "startOffset": 161, "endOffset": 167}, {"referenceID": 5, "context": "In the last few years, however, predict models such as the Skip-Gram model and Continuous Bag-Of-Words model have become the de facto standard for word modeling [5, 6].", "startOffset": 161, "endOffset": 167}, {"referenceID": 6, "context": "These methods have origins in neural language modeling [7], where the goal was to improve classic language models.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "A recent evaluation study [1] suggested that the predict models are superior to the count models on a range of word similarity tasks.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "hyperparameter optimization [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "To this end, Levy and Goldberg [2] made an interesting connection between two key models: a traditional count model based on pointwise mutual information (PMI) and a predict model, namely the skip-gram model with negative sampling (SGNS).", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "For example, Mikolov et al [6] suggest the following down-sampling probability:", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Let P3(i) be the weight associated with the position of the context word w in the context 1Mikolov et al [6] used down-weighting of common words.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "It is unclear if Levy and Goldberg [2] used it.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "1 PMI models PMI models have been used extensively in distributional semantic models [12] to compute similarities between words.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "More recently, Levy and Goldberg [2] have defined shifted variants of PMI and the PPMI metrics.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "2 Skip-Gram with Negative Sampling (SGNS) SGNS [6] is a popular predict model that aims to predict which word w occurs with a context word c.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "One can derive the SGNS [6] model from a binary classification setting where the target variable y specifies whether a word w occurs with context word c.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Levy and Goldberg [2] showed that, by accumulating data over co-occurrences of words and context words, the objective function in (6) can be rewritten as:", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "We consider general loss functions, L(x, y) (here y \u2208 {1,\u22121} is the target variable) with the logistic loss 2Levy and Goldberg [2] write the problem as the maximization of likelihood; hence the ` here and the one in [2] are negatives of each other.", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "We consider general loss functions, L(x, y) (here y \u2208 {1,\u22121} is the target variable) with the logistic loss 2Levy and Goldberg [2] write the problem as the maximization of likelihood; hence the ` here and the one in [2] are negatives of each other.", "startOffset": 216, "endOffset": 219}, {"referenceID": 4, "context": "Observation 1 Unlike SGNS, not all models can be reduced to the co-occurrence format - an example is the CBOW model [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "In fact, even with the skip-gram model, the reduction to the co-occurrence format is not possible if we use the traditional softmax or hierarchical softmax [7] approach to speed up training.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "That property makes them have close relations with count models [1, 2, 10].", "startOffset": 64, "endOffset": 74}, {"referenceID": 1, "context": "That property makes them have close relations with count models [1, 2, 10].", "startOffset": 64, "endOffset": 74}, {"referenceID": 9, "context": "That property makes them have close relations with count models [1, 2, 10].", "startOffset": 64, "endOffset": 74}, {"referenceID": 8, "context": "Another model that can be expressed in co-occurrence form is Glove [9].", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": ", Glove [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "[6] derive (7) starting from the problem of predicting a context word given a word.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "It is useful to note that this comment does not hold if traditional softmax or hierarchical softmax [7] or noise-contrastive estimation [8] is used to model probabilities with the associated negative likelihood loss.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "It is useful to note that this comment does not hold if traditional softmax or hierarchical softmax [7] or noise-contrastive estimation [8] is used to model probabilities with the associated negative likelihood loss.", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "In this section, we revisit the idea that Levy and Goldberg [2] used to connect PMI models with SGNS and extend it.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "1 Solving SGNS in closed form Here, we extend the analysis of Levy and Goldberg [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "2 Connecting SGNS and Shifted PMI One of the key results of Levy and Goldberg [2] is that the vectors created by Shifted PMI are a solution to the SGNS objective.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Though Levy and Goldberg [2] do not mention the above construction, this is a simple observation that easily follows from their analysis.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": ", [1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Levy and Goldberg [2] make the following statement in the second paragraph of Section 5.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Observation 10 Levy and Goldberg [2] propose that SVD is done on M or one of its variants.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "However, if remaining faithful to SGNS is the aim, (13) 3Levy and Goldberg [2] recommend using the matrix corresponding to Shifted PPMI.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Levy and Goldberg [2] recommend Shifted PPMI and refer to the spectral word vectors for it as SVD and Symmetric SVD.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "Depending on the purpose for which the word vectors are developed we can also use dependency-tree based context features [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].", "startOffset": 163, "endOffset": 166}], "year": 2015, "abstractText": "In a recent paper, Levy and Goldberg [2] pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible.", "creator": "LaTeX with hyperref package"}}}