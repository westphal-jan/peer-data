{"id": "1511.02163", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Submodular Hamming Metrics", "abstract": "We show that there is a largely unexplored class of functions (positive polymatroids) that can define appropriate discrete metrics via pairs of binary vectors and that are relatively easy to optimize. By leveraging submodularity, we are able to provide hardness results and approximation algorithms for optimizing such metrics. Furthermore, we demonstrate empirically the effectiveness of these metrics and associated algorithms both in a metric minimization task (a form of clustering) and in a metric maximization task (generation of various k leaderboards).", "histories": [["v1", "Fri, 6 Nov 2015 17:13:32 GMT  (1353kb,D)", "http://arxiv.org/abs/1511.02163v1", "15 pages, 1 figure, a short version of this will appear in the NIPS 2015 conference"]], "COMMENTS": "15 pages, 1 figure, a short version of this will appear in the NIPS 2015 conference", "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.DM", "authors": ["jennifer gillenwater", "rishabh k iyer", "bethany lusch", "rahul kidambi", "jeff a bilmes"], "accepted": true, "id": "1511.02163"}, "pdf": {"name": "1511.02163.pdf", "metadata": {"source": "CRF", "title": "Submodular Hamming Metrics", "authors": ["Jennifer Gillenwater", "Rishabh Iyer", "Bethany Lusch", "Rahul Kidambi", "Jeff Bilmes"], "emails": ["bilmes}@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "A good distance metric is often the key to an effective machine learning algorithm. For instance, when clustering, the distance metric largely defines which points end up in which clusters. Similarly, in large-margin learning, the distance between different labelings can contribute as much to the definition of the margin as the objective function itself. Likewise, when constructing diverse k-best lists, the measure of diversity is key to ensuring meaningful differences between list elements.\nWe consider distance metrics d : {0, 1}n \u00d7 {0, 1}n \u2192 R+ over binary vectors, x \u2208 {0, 1}n. If we define the set V = {1, . . . , n}, then each x = 1A can seen as the characteristic vector of a set A \u2286 V , where 1A(v) = 1 if v \u2208 A, and 1A(v) = 0 otherwise. For sets A,B \u2286 V , with 4 representing the symmetric difference, A4B , (A \\B) \u222a (B \\A), the Hamming distance is then:\ndH(A,B) = |A4B| = n\u2211 i=1 1A4B(i) = n\u2211 i=1 1(1A(i) 6= 1B(i)). (1)\nA Hamming distance between two vectors assumes that each entry difference contributes value one. Weighted Hamming distance generalizes this slightly, allowing each entry a unique weight. Mahalanobis distance generalizes further, allowing weighted pairwise interactions of the following form:\ndM (A,B) = 1 > A4BS1A4B = n\u2211 i=1 n\u2211 j=1 Sij1A4B(i)1A4B(j). (2)\nWhen S is a positive semi-definite matrix, this type of distance is a metric. For many practical applications, however, it is desirable to have entries interact with each other in more complex and higher-order ways than Hamming or Mahalanobis allow. Yet, arbitrary interactions would result in non-metric functions whose optimization would be intractable. In this work, therefore, we consider an alternative class of functions that goes beyond pairwise interactions, yet is computationally feasible, is natural for many applications, and preserves metricity.\nGiven a set function f : 2V \u2192 R, we can define a distortion between two binary vectors as follows: df (A,B) = f(A4B). By asking f to satisfy certain properties, we will arrive at a class of discrete metrics that is feasible to optimize and preserves metricity. We say that f is positive if f(A) > 0 whenever A 6= \u2205; f is normalized if f(\u2205) = 0; f is monotone if f(A) \u2264 f(B)\nar X\niv :1\n51 1.\n02 16\n3v 1\n[ cs\n.D S]\n6 N\nov 2\nfor all A \u2286 B \u2286 V ; f is subadditive if f(A) + f(B) \u2265 f(A \u222a B) for all A,B \u2286 V ; f is modular if f(A) + f(B) = f(A \u222a B) + f(B \u2229 A) for all A,B \u2286 V ; and f is submodular if f(A) + f(B) \u2265 f(A \u222a B) + f(B \u2229 A) for all A,B \u2286 V . If we assume that f is positive, normalized, monotone, and subadditive then df (A,B) is a metric (see Theorem 3.1), but without useful computational properties. If f is positive, normalized, monotone, and modular, then we recover the weighted Hamming distance. In this paper, we assume that f is positive, normalized, monotone, and submodular (and hence also subadditive). These conditions are sufficient to ensure the metricity of df , but allow for a significant generalization over the weighted Hamming distance. Also, thanks to the properties of submodularity, this class yields efficient optimization algorithms with guarantees for practical machine learning problems. In what follows, we will refer to normalized monotone submodular functions as polymatroid functions; all of our results will be concerned with positive polymatroids. We note here that despite the restrictions described above, the polymatroid class is in fact quite broad; it contains a number of natural choices of diversity and coverage functions, such as set cover, facility location, saturated coverage, and concave-over-modular functions.\nGiven a positive polymatroid function f , we refer to df (A,B) = f(A4B) as a submodular Hamming (SH) distance. We study two optimization problems involving these metrics (each fi is a positive polymatroid, each Bi \u2286 V , and C denotes a combinatorial constraint):\nSH-min: min A\u2208C m\u2211 i=1 fi(A4Bi), and SH-max: max A\u2208C m\u2211 i=1 fi(A4Bi). (3)\nWe will use F as shorthand for the sequence (f1, . . . , fm), B for the sequence (B1, . . . , Bm), and F (A) for the objective function \u2211m i=1 fi(A4Bi). We will also make a distinction between the homogeneous case where all fi are the same function, and the more general heterogeneous case where each fi may be distinct. In terms of constraints, in this paper\u2019s theory we consider only the unconstrained (C = 2V ) and the cardinality-constrained (e.g., |A| \u2265 k, |A| \u2264 k) settings. In general though, C could express more complex concepts such as knapsack constraints, or that solutions must be an independent set of a matroid, or a cut (or spanning tree, path, or matching) in a graph.\nIntuitively, the SH-min problem can be thought of as a centroid-finding problem; the minimizing A should be as similar to the Bi\u2019s as possible, since a penalty of fi(A4Bi) is paid for each difference. Analogously, the SH-max problem can be thought of as a diversification problem; the maximizing A should be as distinct from all Bi\u2019s as possible, as fi(A4B) is awarded for each difference. Given modular fi (the weighted Hamming distance case), these optimization problems can be solved exactly and efficiently for many constraint types. For the more general case of submodular fi, we establish several hardness results and offer new approximation algorithms, as summarized in Tables 1 and 2. Our main contribution is to provide (to our knowledge), the first systematic study of the properties of submodular Hamming (SH) metrics, by showing metricity, describing potential machine learning applications, and providing optimization algorithms for SH-min and SH-max.\nThe outline of this paper is as follows. In Section 2, we offer further motivation by describing several applications of SH-min and SH-max to machine learning. In Section 3, we prove that for a positive\npolymatroid function f , the distance df (A,B) = f(A4B) is a metric. Then, in Sections 4 and 5 we give hardness results and approximation algorithms, and in Section 6 we demonstrate the practical advantage that submodular metrics have over modular metrics for several real-world applications."}, {"heading": "2 Applications", "text": "We motivate SH-min and SH-max by showing how they occur naturally in several applications.\nClustering: Many clustering algorithms, including for example k-means [1], use distance functions in their optimization. If each item i to be clustered is represented by a binary feature vector bi \u2208 {0, 1}n, then counting the disagreements between bi and bj is one natural distance function. Defining sets Bi = {v : bi(v) = 1}, this count is equivalent to the Hamming distance |Bi4Bj |. Consider a document clustering application where V is the set of all features (e.g., n-grams) and Bi is the set of features for document i. Hamming distance has value 2 both when Bi4Bj = {\u201csubmodular\u201d, \u201csynapse\u201d} and when Bi4Bj = {\u201csubmodular\u201d, \u201cmodular\u201d}. Intuitively, however, a smaller distance seems warranted in the latter case since the difference is only in one rather than two distinct concepts. The submodular Hamming distances we propose in this work can easily capture this type of behavior. Given feature clustersW , one can define a submodular function as:\nf(Y ) = \u2211 W\u2208W \u221a |Y \u2229W |. (4)\nApplying this with Y = Bi4Bj , if the documents\u2019 differences are confined to one cluster, the distance is smaller than if the differences occur across several word clusters. In the case discussed above, the distances are 2 and \u221a 2. If this submodular Hamming distance is used for k-means clustering, then the mean-finding step becomes an instance of the SH-min problem. That is, if cluster j contains documents Cj , then its mean takes exactly the following SH-min form:\n\u00b5j \u2208 argmin A\u2286V \u2211 i\u2208Cj f(A4Bi). (5)\nStructured prediction: Structured support vector machines (SVMs) typically rely on Hamming distance to compare candidate structures to the true one. The margin required between the correct structure score and a candidate score is then proportional to their Hamming distance. Consider the problem of segmenting an image into foreground and background. Let Bi be image i\u2019s true set of foreground pixels. Then Hamming distance between Bi and a candidate segmentation with foreground pixels A counts the number of mis-labeled pixels. However, both [2] and [3] observe poor performance with Hamming distance and recent work by [4] shows improved performance with richer distances that are supermodular functions of A. One potential direction for further enriching image segmentation distance functions is thus to consider non-modular functions from within our submodular Hamming metrics class. These functions have the ability to correct for the over-penalization that the current distance functions may suffer from when the same kind of difference happens repeatedly. For instance, ifBi differs fromA only in the pixels local to a particular block of the image, then current distance functions could be seen as over-estimating the difference. Using a submodular Hamming function, the \u201closs-augmented inference\u201d step in SVM optimization becomes an SH-max problem. More concretely, if the segmentation model is defined by a submodular graph cut g(A), then we have: maxA\u2286V g(A) + f(A4Bi). (Note that g(A) = g(A4\u2205).) In fact, [5] observes superior results with this type of loss-augmented inference using a special case of a submodular Hamming metric for the task of multi-label image classification.\nDiverse k-best: For some machine learning tasks, rather than finding a model\u2019s single highest-scoring prediction, it is helpful to find a diverse set of high-quality predictions. For instance, [6] showed that for image segmentation and pose tracking a diverse set of k solutions tended to contain a better predictor than the top k highest-scoring solutions. Additionally, finding diverse solutions can be beneficial for accommodating user interaction. For example, consider the task of selecting 10 photos to summarize the 100 photos that a person took while on vacation. If the model\u2019s best prediction (a set of 10 images) is rejected by the user, then the system should probably present a substantially different prediction on its second try. Submodular functions are a natural model for several summarization problems [7, 8]. Thus, given a submodular summarization model g, and a set of existing diverse\nsummaries A1, A2, . . . , Ak\u22121, one could find a kth summary to present to the user by solving:\nAk = argmax A\u2286V,|A|=` g(A) + k\u22121\u2211 i=1 f(A4Ai). (6)\nIf f and g are both positive polymatroids, then this constitutes an instance of the SH-max problem."}, {"heading": "3 Properties of the submodular Hamming metric", "text": "We next show several interesting properties of the submodular Hamming distance. Proofs for all theorems and lemmas can be found in the supplementary material. We begin by showing that any positive polymatroid function of A4B is a metric. In fact, we show the more general result that any positive normalized monotone subadditive function of A4B is a metric. This result is known (see for instance Chapter 8 of [9]), but we provide a proof (in the supplementary material) for completeness. Theorem 3.1. Let f : 2V \u2192 R be a positive normalized monotone subadditive function. Then df (A,B) = f(A4B) is a metric on A,B \u2286 V .\nProof. Let A,B,C \u2286 V be arbitrary. We check each of the four properties of metrics:\n1. Proof of non-negativity: d(A,B) = f(A4B) \u2265 0 because f is normalized and positive.\n2. Proof of identity of indiscernibles: d(A,B) = 0\u21d4 f(A4B) = 0\u21d4 A4B = \u2205 \u21d4 A = B. The third implication follows because of normalization and positivity of f , and the fourth follows from the definition of4.\n3. Proof of symmetry: d(A,B) = f(A4B) = f(B4A) = d(B,A), by definition of4.\n4. Proof of the triangle inequality: First, note that A4B \u2286 (A4C) \u222a (C4B). This follows because each element v \u2208 A \\ B is either in C \\ B (true if v \u2208 C) or in A \\ C (true if v /\u2208 C). Similarly, each element v \u2208 B \\ A is either in C \\ A (true if v \u2208 C) or in B \\ C (true if v /\u2208 C). Then, because f is monotone and subadditive, we have:\nf(A4B) \u2264 f((A4C) \u222a (C4B)) \u2264 f(A4C) + f(C4B). (7)\nWhile these subadditive functions are metrics, their optimization is known to be very difficult. The simple subadditive function example in the introduction of [10] shows that subadditive minimization is inapproximable, and Theorem 17 of [11] states that no algorithm exists for subadditive maximization that has an approximation factor better than O\u0303( \u221a n). By contrast, submodular minimization is poly-time in the unconstrained setting [12], and a simple greedy algorithm from [13] gives a 1\u2212 1/eapproximation for maximization of positive polymatroids subject to a cardinality constraint. Many other approximation results are also known for submodular function optimization subject to various other types of constraints. Thus, in this work we restrict ourselves to positive polymatroids. Corollary 3.1.1. Let f : 2V \u2192 R+ be a positive polymatroid function. Then df (A,B) = f(A4B) is a metric on A,B \u2286 V .\nThis restriction does not entirely resolve the question of optimization hardness though. Recall that the optimization in SH-min and SH-max is with respect to A, but that the fi are applied to the sets A4Bi. Unfortunately, the function gB(A) = f(A4B), for a fixed set B, is neither necessarily submodular nor supermodular in A. The next example demonstrates this violation of submodularity. Example 3.1.1. To be submodular, the function gB(A) = f(A4B) must satisfy the following condition for all sets A1, A2 \u2286 V : gB(A1) + gB(A2) \u2265 gB(A1 \u222a A2) + gB(A1 \u2229 A2). Consider the positive polymatroid function f(Y ) = \u221a |Y | and let B consist of two elements: B = {b1, b2}. Then for A1 = {b1} and A2 = {c} (with c /\u2208 B): gB(A1) + gB(A2) = \u221a 1 + \u221a 3 < 2 \u221a\n2 = gB(A1 \u222aA2) + gB(A1 \u2229A2). (8) This violates the definition of submodularity, implying that gB(A) is not submodular.\nAlthough gB(A) = f(A4B) can be non-submodular, we are interestingly still able to make use of the fact that f is submodular in A4B to develop approximation algorithms for SH-min and SH-max."}, {"heading": "4 Minimization of the submodular Hamming metric", "text": "In this section, we focus on SH-min (the centroid-finding problem). We consider the four cases from Table 1: the constrained (A \u2208 C \u2282 2V ) and unconstrained (A \u2208 C = 2V ) settings, as well as the homogeneous case (where all fi are the same function) and the heterogeneous case. Before diving in, we note that in all cases we assume not only the natural oracle access to the objective function F (A) = \u2211m i=1 fi(A4Bi) (i.e., the ability to evaluate F (A) for any A \u2286 V ), but also knowledge of the Bi (the B sequence). Theorem 4.1 shows that without knowledge of B, SH-min is inapproximable. In practice, requiring knowledge of B is not a significant limitation; for all of the applications described in Section 2, B is naturally known. Theorem 4.1. Let f be a positive polymatroid function. Suppose that the subset B \u2286 V is fixed but unknown and gB(A) = f(A4B). If we only have an oracle for gB , then there is no poly-time approximation algorithm for minimizing gB , up to any polynomial approximation factor.\nProof. Define f(Y ) as follows:\nf(Y ) = { 0 if Y = \u2205 1 otherwise.\n(9)\nThen gB(A) = 1 unless A = B. Thus, it would take any algorithm an exponential number of queries on gB to find B."}, {"heading": "4.1 Unconstrained setting", "text": "Submodular minimization is poly-time in the unconstrained setting [12]. Since a sum of submodular functions is itself submodular, at first glance it might then seem that the sum of fi in SH-min can be minimized in poly-time. However, recall from Example 3.1.1 that the fi\u2019s are not necessarily submodular in the optimization variable, A. This means that the question of SH-min\u2019s hardness, even in the unconstrained setting, is an open question. Theorem 4.2 resolves this question for the heterogeneous case, showing that it is NP-hard and that no algorithm can do better than a 4/3-approximation guarantee. The question of hardness in the homogeneous case remains open. Theorem 4.2. The unconstrained and heterogeneous version of SH-min is NP-hard. Moreover, no poly-time algorithm can achieve an approximation factor better than 4/3.\nProof. We first show that for any graph G = (V,E) it is possible to construct fi and Bi such that the corresponding sum in the SH-min problem has minimum value if and only if A is a vertex cover for G. For constants k > 1 and > 0, let \u03b31 = 2 k\u22121 2k[(2k\u22121)1/k\u22121]k + and \u03b32 = \u03b31 2k\u22121 . For every edge e = (u, v) \u2208 E, define two positive polymatroid functions:\nf1e(Y ) = (\u03b31|Y \u2229 u|+ \u03b32|Y \u2229 v|)1/k, and f2e(Y ) = (\u03b32|Y \u2229 u|+ \u03b31|Y \u2229 v|)1/k. (10)\nLet B1e = {u} and B2e = {v} and define the sum F c(A): F c(A) = \u2211 e\u2208E he(A), for he(A) = f1e(A4B1e) + f2e(A4B2e). (11)\nThe value of each term in this sum is shown in Table 3. Note that the definition of \u03b32 ensures that (\u03b31 + \u03b32) 1/k = 2\u03b3 1/k 2 .\nUsing Table 3, we can show that the minimizers of F c(A) are exactly the set covers of G:\n\u2022 Case 1\u2014show that every vertex cover of G is a minimizer of F c: By the definition of \u03b32, we know \u03b31 > \u03b32, and so the minimum value of F c occurs when all he are 2\u03b3 1/k 2 , which is\nclearly achievable by setting A = V . Any set A that is a vertex cover contains at least one endpoint of each edge, and hence also has value 2\u03b31/k2 for each he.\n\u2022 Case 2\u2014show that every minimizer of F c is a vertex cover of G: Suppose that A\u2217 is a minimizer of F c but not a vertex cover of G. Then there exists some uncovered edge e = (u, v) with neither endpoint in A\u2217. Consider adding u to A\u2217 to form a set A\u2032. The corresponding difference in he value is: he(A\u2217) \u2212 he(A\u2032) = 2\u03b31/k1 \u2212 2\u03b3 1/k 2 > 0. The\ndifference in h-value for each other edge e\u2032 that touches u is similarly 2\u03b31/k1 \u2212 2\u03b3 1/k 2 if e \u2032 is uncovered in A\u2217, or 0 if e\u2032 is covered by A\u2217. All other h-values remain unchanged. Thus, F c(A\u2032) < F c(A\u2217), contradicting the assumption that A\u2217 is a minimizer of F c.\nBorrowing from [14]\u2019s Theorem 3.1, we now define a particular graph and two additional positive polymatroid functions. Consider the bipartite graph G = (V1 \u222a V2, E) where |V1| = |V2| = r and the edge set consists of r edges that form a perfect matching of A to B. Let R be a random minimum-cardinality vertex cover of G. Define the following two functions:\nfa0 (Y ) = min {|Y |, r} f b0(Y ) = min { |Y \u2229 R\u0304|+ min { |Y \u2229R|, (1 + \u03b4)r\n2\n} , r } (12)\nwhere \u03b4 is set so that 2/(1 + \u03b4) = 2 \u2212 . [14] shows that, knowing G but given only value-oracle access to the f0, no poly-time algorithm can distinguish between fa0 and f b 0 . Moreover, if restricted to vertex cover solutions, it is easy to see that the function fa0 is minimized on any of the 2 r possible vertex covers, for which it has value r, while the function f b0 is minimized on the set Y = R, for which it has value (1+\u03b4)r2 . The ratio of these minimizers is 2\u2212 , which allows [14] to show that no poly-time algorithm can achieve a (2\u2212 )-approximation for the minimum submodular vertex cover problem.\nNow, instead of explicitly restricting to vertex cover solutions, consider unconstrained minimization on F a(A) = fa0 (A)+F c(A) and F b(A) = f b0(A)+F c(A). Since fa0 and f b 0 cannot be distinguished in poly-time, neither can F a and F b. We can also show that: (1) any minimizer of F a(A) or F b(A) must be a vertex cover, and (2) the ratio of the corresponding vertex cover minimizers is 4/3.\n\u2022 Show F a\u2019s minimizers are vertex covers: Suppose that A\u2217 is a minimizer of F a but not a vertex cover of G. Then there exists some uncovered edge e = (u, v) with neither endpoint in A\u2217. Consider adding u to A\u2217 to form a set A\u2032. As shown above, the corresponding difference in F c value is 2\u03b31/k1 \u22122\u03b3 1/k 2 . The difference f a 0 (A\n\u2217)\u2212fa0 (A\u2032) is\u22121 if |A\u2217| < r and 0 otherwise. Thus, all we need is for 2\u03b31/k1 \u2212 2\u03b3 1/k 2 to be > 1. Plugging in the definition of \u03b31 and \u03b32, this inequality can be seen to hold for all k > 1. Thus, overall F a(A\u2032) < F a(A\u2217), contradicting the assumption that A\u2217 is a minimizer of F a.\n\u2022 Show F b\u2019s minimizers are vertex covers: The reasoning here is analogous to the F a case; the difference f b0(A\n\u2217) \u2212 f b0(A\u2032) is always > \u22121, since adding a single node can never change the f b0 value by more than 1.\n\u2022 F a\u2019s minimum value: Any vertex cover A includes at least r nodes and thus has value fa0 (A) = r. Since there are r edges total, F c(A) = 2r\u03b3 1/k 2 for a vertex cover. Combining\nthese we see that F a has minimum value r(1 + 2\u03b31/k2 ).\n\u2022 F b\u2019s minimum value: The vertex cover consisting of the set R minimizes f b0 : f b0(R) = (1+\u03b4)r\n2 . Thus, the minimum F b value is r\n( (1+\u03b4)\n2 + 2\u03b3 1/k 2\n) .\nLetting k \u2192\u221e, we have that \u03b31/k2 \u2192 1/2. Thus, in the limit the as k \u2192\u221e, the ratio of minimizers is: 2/( (1+\u03b4)2 + 1) = 4 3+\u03b4 . Plugging in the definition of \u03b4 from above, the ratio in terms of is: 4\u22122 3\u2212 > 4 3 \u2212 2 3\u2212 = 4 3 \u2212 o(1).\nSince unconstrained SH-min is NP-hard, it makes sense to consider approximation algorithms for this problem. We first provide a simple 2-approximation, UNION-SPLIT (see Algorithm 1). This algorithm splits f(A4B) = f((A \\B)\u222a (B \\A)) into f(A \\B) + f(B \\A), then applies standard submodular minimization (see e.g. [15]) to the split function. Theorem 4.3 shows that this algorithm is a 2-approximation for SH-min. It relies on Lemma 4.2.1, which we state first. Lemma 4.2.1. Let f be a positive monotone subadditive function. Then, for any A,B \u2286 V :\nf(A4B) \u2264 f(A \\B) + f(B \\A) \u2264 2f(A4B). (13)\nProof. The upper bound follows from the definition of4 and the fact that f is subadditive: f(A4B) = f((A \\B) \u222a (B \\A)) \u2264 f(A \\B) + f(B \\A). (14)\nThe lower bound on 2f(A4B) follows due to the monotonicity of f : f(A \\ B) \u2264 f(A4B) and f(B \\A) \u2264 f(A4B). Summing these two inequalities gives the bound.\nTheorem 4.3. UNION-SPLIT is a 2-approximation for unconstrained SH-min. Proof. An SH-min instance seeks the minimizer of F (A) = \u2211m i=1 fi(A4Bi). Define F\u0304 (A) =\u2211m\ni=1 [fi(A \\Bi) + fi(Bi \\A)]. From Lemma 4.2.1, we see that minA F\u0304 (A) is a 2-approximation for minA F (A) (any submodular function is also subadditive). Thus, if F\u0304 can be minimized exactly, the result is a 2-approximation for SH-min. Exact minimization of F\u0304 is possible because F\u0304 is submodular in A. The submodularity of F\u0304 follows from the fact that submodular functions are closed under restriction, complementation, and addition (see [16], page 9). These closure properties imply that, for each i, fi(A \\Bi) and fi(Bi \\A) are both submodular in A, as is their sum.\nNote that UNION-SPLIT\u2019s 2-approximation bound is tight; there exists a problem instance where exactly a factor of 2 is achieved. More concretely, consider V = {1, 2}, B1 = {1}, B2 = {2}, and f1(Y ) = f2(Y ) = |Y |(1/\u03b1) for \u03b1 > 1. Then according to the F \u2032 passed to SUBMODULAR-OPT, all solutions have value 2. Yet, under the true F the solutions {1} and {2} have the better (smaller) value 2(1/\u03b1). Letting \u03b1\u2192\u221e, the quantity 2(1/\u03b1) approaches 1, making the ratio between the correct solution and the one given by UNION-SPLIT possibly as large as 2.\nRestricting to the homogeneous setting, we can provide a different algorithm that has a better approximation guarantee than UNION-SPLIT. This algorithm simply checks the value of F (A) = \u2211m i=1 f(A4Bi) for each Bi and returns the minimizing Bi. We call this algorithm BEST-B (Algorithm 2). Theorem 4.4 gives the approximation guarantee for BEST-B. This result is known [17], as the proof of the guarantee only makes use of metricity and homogeneity (not submodularity), and these properties are common to much other work. We provide the proof in our notation for completeness though. Theorem 4.4. For m = 1, BEST-B exactly solves unconstrained SH-min. For m > 1, BEST-B is a( 2\u2212 2m ) -approximation for unconstrained homogeneous SH-min.\nProof. Define F (A) = \u2211m i=1 fi(A4Bi), for fi positive polymatroid. Since each fi is normalized and positive, each is minimized by \u2205: fi(\u2205) = 0. Thus, any given fi(A4Bi) is minimized by setting A = Bi. For m = 1, this implies that SH-min is exactly solved by setting A = B1.\nNow consider m > 1 and the homogeneous setting where there is a single f : fi = f \u2200i. By Theorem 3.1, f(A4Bi) is a metric, so it obeys the triangle inequality:\nf(A4Bi) + f(A4Bj) \u2265 f(Bi4Bj) \u2200i, j. (15) Fixing some i and summing this inequality over all j 6= i:\u2211\nj 6=i [f(A4Bi) + f(A4Bj)] \u2265 \u2211 j 6=i f(Bi4Bj) = m\u2211 i=1 f(Bi4Bj) (16)\nwhere the last equality is due to the fact that polymatroids are normalized: f(Bi4Bi) = f(\u2205) = 0. Regrouping terms, f(A4Bi) is independent of j, so it can be pulled out of the summation:\n(m\u2212 2)f(A4Bi) + m\u2211 j=1 f(A4Bj) \u2265 m\u2211 j=1 f(Bi4Bj). (17)\nAlgorithm 1 UNION-SPLIT Input: F , B Define f \u2032i(Y ) = fi(Y \\Bi) + fi(Bi \\ Y ) Define F \u2032(Y ) = \u2211m i=1 f \u2032 i(Y )\nOutput: SUBMODULAR-OPT (F \u2032)\nAlgorithm 2 BEST-B Input: F , B A\u2190 B1 for i = 2, . . . ,m do\nif F (Bi) < F (A): A\u2190 Bi Output: A\nAlgorithm 3 MAJOR-MIN Input: F , B, C A\u2190 \u2205 repeat c\u2190 F (A) Set wF\u0302 as in Equation 23 A\u2190 MODULAR-MIN (wF\u0302 , C)\nuntil F (A) = c Output: A\nNotice that \u2211m j=1 f(A4Bj) is exactly F (A) and \u2211m j=1 f(Bi4Bj) is F (Bi). Substituting in this notation and summing over all i: m\u2211 i=1 [(m\u2212 2)f(A4Bi) + F (A)] \u2265 m\u2211 i=1 F (Bi). (18) On the left-hand side we can again replace the sum with F (A), yielding: 2(m \u2212 1)F (A) \u2265\u2211m i=1 F (Bi). Since a sum over m items is larger than m times the minimum term in the sum, the remaining sum here can be replaced by a min:\n2(m\u2212 1)F (A) \u2265 m min i\u2208{1,...,m} F (Bi). (19)\nThe left-hand size is exactly what the BEST-B algorithm computes, and hence the minimizing Bi found by BEST-B is a (2\u2212 2/m)-approximation for unconstrained homogeneous SH-min.\nNote that as a corollary of this result, in the case when m = 2, the optimal solution for unconstrained homogeneous SH-min is to take the best of B1 and B2. Also note that since UNION-SPLIT\u2019s 2- approximation bound is tight, BEST-B is theoretically better in terms of worst-case performance in the unconstrained setting. However, UNION-SPLIT\u2019s performance on practical problems is often better than the BEST-B\u2019s, as many practical problems do not hit upon this worst case. For example, consider the case where V = {1, 2, 3}, f is simply cardinality, f(A) = |A|, and each Bi consists of two items: B1 = {1, 2}, B2 = {1, 3}, B3 = {2, 3}. Then the best Bi has F -value 4, while the set {1, 2, 3} found by UNION-SPLIT has a lower (better) F -value of 3."}, {"heading": "4.2 Constrained setting", "text": "In the constrained setting, the SH-min problem becomes more difficult. Essentially, all of the hardness results established in existing work on constrained submodular minimization applies to the constrained SH-min problem as well. Theorem 4.5 shows that, even for a simple cardinality constraint and identical fi (homogeneous setting), not only is SH-min NP-hard, but also it is hard to approximate with a factor better than \u2126( \u221a n).\nTheorem 4.5. Homogeneous SH-min is NP-hard under cardinality constraints. Moreover, no algorithm can achieve an approximation factor better than \u2126\n( \u221a n\n1+( \u221a n\u22121)(1\u2212\u03baf )\n) , where \u03baf =\n1\u2212minj\u2208V f(j|V \\j)f(j) denotes the curvature of f . This holds even when m = 1.\nProof. Let m = 1 and B1 = \u2205. Then under cardinality constraints, SH-min becomes minA:|A|\u2265k f(A). Corollary 5.1 of [18] establishes that this problem is NP-hard and has a hardness of \u2126( \u221a n\n1+( \u221a n\u22121)(1\u2212\u03baf ) ).\nWe can also show similar hardness results for several other combinatorial constraints including matroid constraints, shortest paths, spanning trees, cuts, etc. [18, 14]. Note that the hardness established\nin Theorem 4.5 depends on a quantity \u03baf , which is also called the curvature of a submodular function [19, 18]. Intuitively, this factor measures how close a submodular function is to a modular function. The result suggests that the closer the function is being modular, the easier it is to optimize. This makes sense, since with a modular function, SH-min can be exactly minimized under several combinatorial constraints. To see this for the cardinality-constrained case, first note that for modular fi, the corresponding F -function is also modular. Lemma 4.5.1 formalizes this.\nLemma 4.5.1. If the fi in SH-min are modular, then F (A) = \u2211m i=1 fi(A4Bi) is also modular.\nProof. Any normalized modular function fi can be represented as a vector wi \u2208 Rn, such that fi(Y ) = \u2211 j\u2208Y wi(j) = w > i 1Y . With Y = A4Bi, this can be written:\nfi(A4Bi) =w>i [ 1Bi + diag(1V \\Bi \u2212 1Bi)1A ] (20)\n= \u2211 j\u2208Bi wi(j) + \u2211 j\u2208A (\u22121)1(j\u2208Bi)wi(j). (21)\nSumming over i and letting C = \u2211m i=1 \u2211 j\u2208Bi wi(j) represent the part that is constant with respect to A, we have:\nF (A) = C + \u2211 j\u2208A (\u22121)1(j\u2208Bi)wi(j). (22)\nThus, F can be represented by offset C and vector wF \u2208 Rn such that F (A) = C + \u2211 j\u2208A wF (j),\nwith entries wF (j) = \u2211m i=1(\u22121)1(j\u2208Bi)wi(j). This is sufficient to prove modularity. (For optimization purposes, note that C can be dropped without affecting the solution to SH-min.)\nGiven Lemma 4.5.1, from the definition of modularity we know that there exists some constant C and vector wF \u2208 Rn, such that F (A) = C + \u2211 j\u2208A wF (j). From this representation it is clear that F can be minimized subject to the constraint |A| \u2265 k by choosing as the set A the items corresponding to the k smallest entries in wF . Thus, for modular fi, or fi with small curvature \u03bafi , such constrained minimization is relatively easy.\nHaving established the hardness of constrained SH-min, we now turn to considering approximation algorithms for this problem. Unfortunately, the UNION-SPLIT algorithm from the previous section requires an efficient algorithm for submodular function minimization, and no such algorithm exists in the constrained setting; submodular minimization is NP-hard even under simple cardinality constraints [20] (although see [21] that shows it is possible to get solutions for a subset of the cardinality constraints). Similarly, the BEST-B algorithm breaks down in the constrained setting; its guarantees carry over only if all the Bi are within the constraint set C. Thus, for the constrained SH-min problem we instead propose a majorization-minimization algorithm. Theorem 4.6 shows that this algorithm has an O(n) approximation guarantee, and Algorithm 3 formally defines the algorithm.\nEssentially, MAJOR-MIN proceeds by iterating the following two steps: constructing F\u0302 , a modular upper bound for F at the current solution A, then minimizing F\u0302 to get a new A. F\u0302 consists of superdifferentials [22, 23] of F \u2019s component submodular functions. We use the superdifferentials defined as \u201cgrow\u201d and \u201cshrink\u201d in [24]. Defining sets S, T as S = V \\ j, T = A4Bi for \u201cgrow\u201d, and S = (A4Bi) \\ j, T = \u2205 for \u201cshrink\u201d, the wF\u0302 vector that represents the modular F\u0302 can be written:\nwF\u0302 (j) = m\u2211 i=1 { fi(j | S) if j \u2208 A4Bi fi(j | T ) otherwise,\n(23)\nwhere f(Y | X) = f(Y \u222aX)\u2212 f(X) is the gain in f -value when adding Y to X . We now state the main theorem characterizing algorithm MAJOR-MIN\u2019s performance on SH-min.\nTheorem 4.6. MAJOR-MIN is guaranteed to improve the objective value, F (A) = \u2211m i=1 fi(A4Bi), at every iteration. Moreover, for any constraint over which a modular function can be exactly optimized, it has a ( maxi\n|A\u22174Bi| 1+(|A\u22174Bi|\u22121)(1\u2212\u03bafi (A\u22174Bi))\n) approximation guarantee, where A\u2217 is\nthe optimal solution of SH-min.\nProof. We first define the full \u201cgrow\u201d and \u201cshrink\u201d superdifferentials: mfA,1(Y ) , f(A)\u2212 \u2211\nj\u2208A\\Y\nf(j | V \\ j) + \u2211\nj\u2208Y \\A\nf(j | A), and (24)\nmfA,2(Y ) , f(A)\u2212 \u2211\nj\u2208A\\Y\nf(j | A \\ j) + \u2211\nj\u2208Y \\A\nf(j | \u2205). (25)\nWhen referring to either of these modular functions, we use mfA. Note that the m f A upper-bound f in the following sense: mfA(Y ) \u2265 f(Y ) \u2200Y \u2286 V , and m f A(A) = f(A).\nMAJOR-MIN proceeds as follows. Starting from A0 = \u2205 and applying either \u201cgrow\u201d or \u201cshrink\u201d to construct a modular approximation to F at \u2205 yields the following simple surrogate function for each fi: f\u0302i(Y ) = \u2211 j\u2208Y fi(j). The below bound then holds (from [18]):\nfi(Y ) \u2264 f\u0302i(Y ) \u2264 |Y |\n1 + (|Y | \u2212 1)(1\u2212 \u03bafi(Y )) fi(Y ), \u2200Y \u2286 V. (26)\nLet A\u0302 = argminA\u2208C \u2211m i=1 f\u0302i(A4Bi). Also, let A\u2217 = argminA\u2208C \u2211m i=1 fi(A4Bi). Then, it holds that: m\u2211 i=1 fi(A\u03024Bi) \u2264 m\u2211 i=1 f\u0302i(A\u03024Bi) (27)\n\u2264 m\u2211 i=1 f\u0302i(A \u22174Bi) (28)\n\u2264 |A \u2217|\n1 + (|A\u2217| \u2212 1)(1\u2212 \u03bafi(A\u2217)) m\u2211 i=1 fi(A \u22174Bi) (29)\nThe first inequality follows from the definition of the modular upper bound, the second inequality follows from the fact that A\u0302 is the minimizer of the modular optimization, and the third inequality follows from Equation 26. We now show that MAJOR-MIN improves the objective value at every iteration:\nm\u2211 i=1 fi(A t+14Bi) \u2264 m\u2211 i=1 mfiAt4Bi(A t+14Bi) \u2264 m\u2211 i=1 mfiAt4Bi(A t4Bi) = m\u2211 i=1 fi(A t4Bi). (30)\nWhile MAJOR-MIN does not have a constant-factor guarantee (which is possible only in the unconstrained setting), the bounds are not too far from the hardness of the constrained setting. For example, in the cardinality case, the guarantee of MAJOR-MIN is n1+(n\u22121)(1\u2212\u03baf ) , while the hardness shown in\nTheorem 4.5 is \u2126 ( \u221a\nn 1+(n\u22121)(1\u2212\u03baf )\n) ."}, {"heading": "5 Maximization of the submodular Hamming metric", "text": "We next characterize the hardness of SH-max (the diversification problem) and describe approximation algorithms for it. We first show that all versions of SH-max, even the unconstrained homogeneous one, are NP-hard. Note that this is a non-trivial result. Maximization of a monotone function such as a polymatroid is not NP-hard; the maximizer is always the full set V . But, for SH-max, despite the fact that the fi are monotone with respect to their argument A4Bi, they are not monotone with respect to A itself. This makes SH-max significantly harder. After establishing that SH-max is NP-hard, we show that no poly-time algorithm can obtain an approximation factor better 3/4 in the unconstrained setting, and a factor of (1 \u2212 1/e) in the constrained setting. Finally, we provide a simple approximation algorithm which achieves a factor of 1/4 for all settings. Theorem 5.1. All versions of SH-max (constrained or unconstrained, heterogeneous or homogeneous) are NP-hard. Moreover, no poly-time algorithm can obtain a factor better than 3/4 for the unconstrained versions, or better than 1\u2212 1/e for the cardinality-constrained versions.\nProof. We first show that homogeneous unconstrained SH-max is NP-hard. We proceed by constructing an F that can represent any symmetric positive normalized (non-monotone) submodular function. Maximization is NP-hard for this type of function, since it subsumes the MAX-CUT problem. Hence, the reduction to unconstrained SH-max suffices to show NP-hardness.\nConsider an instance of SH-max with m = 2 and B1 = \u2205, B2 = V : max A\u2286V F (A) = max A\u2286V f(A) + f(V \\A). (31)\nGiven a symmetric positive normalized submodular function h, define: f(A) = h(A)\u2212 \u2211 i\u2208A h(i | V \\ i), (32)\nwhere h(i | V \\ i) is short for h(V ) \u2212 h(v \\ i). To see that f is a positive polymatroid function, first recall that a symmetric set function is one for which h(A) = h(V \\ A) \u2200A \u2286 V . Thus, h(i | V \\i) = h(V )\u2212h(V \\i) = h(\u2205)\u2212h(i) = \u2212h(i). This implies that f(A) = h(A)+ \u2211 i\u2208A h(i), which is clearly a positive polymatroid. Now, notice that:\nF (A) = f(A) + f(V \\A) = h(A) + h(V \\A) + \u2211 i\u2208V h(i) = 2h(A) + \u2211 i\u2208V h(i). (33)\nHence, given an instance of symmetric submodular function maximization, we can transform it into an instance of SH-max, with F defined as above; since \u2211 i\u2208V h(i) is a constant, it does not affect which set is the maximizer. Thus, unconstrained SH-max is NP-hard.\nTo show the hardness of approximation, we borrow a proof technique from Theorem 4.5 of [25]. The idea is to construct two symmetric submodular functions, h1 and h2, which are indistinguishable. That is, any randomized algorithm would require an exponential number of calls to the value oracles to tell h1 and h2 apart. The construction of [25] suggests that for both these functions, h1(i) = h2(i) = n \u2212 1, and hence the constant \u2211 i\u2208V h(i) = n(n \u2212 1). Thus, we can write F1(A) = 2h1(A)+n(n\u22121) and F2(A) = 2h2(A)+n(n\u22121). Since h1 and h2 are indistinguishable, so are F1 and F2. Moreover, according to [25], the maximum value of h1(A) is n2/4, while that of h2(A) is n2/2. Hence, F1\u2019s maximum value is n2/2 + n2 \u2212 n = 3n2/2 \u2212 n, and F2\u2019s is n2 + n2 \u2212 n = 2n2 \u2212 n. The ratio of these is: (3n/2 \u2212 1)/(2n \u2212 1) = 3/4 + o(1). Thus, no poly-time algorithm can achieve a factor better than 3/4.\nFinally, we establish the hardness of approximation for a cardinality-constrained version of SH-max. In this case, let m = 1 and B1 = \u2205. This SH-max instance is exactly the problem of monotone submodular maximization subject to cardinality constraint, which is not only NP-hard but has a hardness of 1\u2212 1/e [26].\nWe turn now to approximation algorithms. For the unconstrained setting, Lemma 5.1.1 shows that simply choosing a random subset, A \u2286 V provides a 1/8-approximation in expectation. Lemma 5.1.1. A random subset is a 1/8-approximation for SH-max in the unconstrained (homogeneous or heterogeneous) setting.\nProof. This result follows from the fact that a random subset is a 1/4-approximation for the problem of unconstrained non-monotone submodular maximization [25, Theorem 2.1], and that the nonmonotone submodular function F\u0304 (A) = \u2211m i=1 [fi(A \\Bi) + fi(Bi \\A)] is within a factor 2 of the F (A) of SH-max (see Lemma 4.2.1). Thus, a random set is a 1/4-approximation for maxA F\u0304 (A) and a 1/8-approximation for maxA F (A).\nAn improved approximation guarantee of 1/4 can be shown for a variant of UNION-SPLIT (Algorithm 1), if the call to SUBMODULAR-OPT is a call to a SUBMODULAR-MAX algorithm. Theorem 5.2 makes this precise for both the unconstrained case and a cardinality-constrained case. It might also be of interest to consider more complex constraints, such as matroid independence and base constraints, but we leave the investigation of such settings to future work. Theorem 5.2. Maximizing F\u0304 (A) = \u2211m i=1 (fi(A \\Bi) + fi(Bi \\A)) with a bi-directional greedy\nalgorithm [27, Algorithm 2] is a linear-time 1/4-approximation for maximizing F (A) =\u2211m i=1 fi(A4Bi), in the unconstrained setting. Under the cardinality constraint |A| \u2264 k, using the randomized greedy algorithm [28, Algorithm 1] provides a 12e -approximation.\nProof. F\u0304 (A) is a (non-monotone) submodular function that is within a factor 2 of F (A) (see Lemma 4.2.1). The bi-directional greedy algorithm [27, Algorithm 2] provides a 1/2-approximation to non-monotone submodular maximization in the unconstrained setting. Thus, applying it to F\u0304 yields a 1/4-approximation for maxA F (A). Similarly, in the cardinality-constrained setting, one can use the randomized greedy algorithm [28, Algorithm 1], which has a 1/e approximation guarantee."}, {"heading": "6 Experiments", "text": "To demonstrate the effectiveness of the submodular Hamming metrics proposed here, we apply them to a metric minimization task (clustering) and a metric maximization task (diverse k-best)."}, {"heading": "6.1 SH-min application: clustering", "text": "We explore the document clustering problem described in Section 2, where the groundset V is all unigram features and Bi contains the unigrams of document i. We run k-means clustering and each iteration find the mean for cluster Cj by solving:\n\u00b5j \u2208 argmin A:|A|\u2265` \u2211 i\u2208Cj f(A4Bi). (34)\nThe constraint |A| \u2265 ` requires the mean to contain at least ` unigrams, which helps k-means to create richer and more meaningful cluster centers. We compare using the submodular function f(Y ) = \u2211 W\u2208W \u221a |Y \u2229W | (SM), to using Hamming distance (HM). The problem of finding \u00b5j above can be solved exactly for HM, since it is a modular function. In the SM case, we apply MAJORMIN (Algorithm 3). As an initial test, we generate synthetic data consisting of 100 \u201cdocuments\u201d assigned to 10 \u201ctrue\u201d clusters. We set the number of \u201cword\u201d features to n = 1000, and partition the features into 100 word classes (theW in the submodular function). Ten word classes are associated with each true document cluster, and each document contains one word from each of these word classes. That is, each word is contained in only one document, but documents in the same true cluster have words from the same word classes. We set the minimum cluster center size to ` = 100. We use k-means++ initialization [29] and average over 10 trials. Within the k-means optimization, we enforce that all clusters are of equal size by assigning a document to the closest center whose current size is < 10. With this setup, the average accuracy of HM is 28.4% (\u00b12.4), while SM is 69.4% (\u00b110.5). The HM accuracy is essentially the accuracy of a random assignment of documents to clusters; this makes sense, as no documents share words, rendering the Hamming distance useless. In real-world data there would likely be some word overlap though; to better model this, we let each document contain a random sampling of 10 words from the word clusters associated with its document cluster. In this case, the average accuracy of HM is 57.0% (\u00b16.8), while SM is 88.5% (\u00b18.4). The results for SM are even better if randomization is removed from the initialization (we simply choose the next center to be one with greatest distance from the current centers). In this case, the average accuracy of HM is 56.7% (\u00b17.1), while SM is 100% (\u00b10.0). This indicates that as long as the starting point for SM contains one document from each cluster, the SM optimization will recover the true clusters.\nMoving beyond synthetic data, we applied the same method to the problem of clustering NIPS papers. The initial set of documents that we consider consists of all NIPS papers1 from 1987 to 2014. We filter the words of a given paper by first removing stopwords and any words that don\u2019t appear at least 3 times in the paper. We further filter by removing words that have small tf-idf value (< 0.001) and words that occur in only one paper or in more than 10% of papers. We then filter the papers themselves, discarding any that have fewer than 25 remaining words and for each other paper retaining only its top (by tf-idf score) 25 words. Each of the 5,522 remaining papers defines aBi set. Among theBi there are 12,262 unique words. To get the word clustersW , we first run the WORD2VEC code of [30], which generates a 100-dimensional real-valued vector of features for each word, and then run k-means clustering with Euclidean distance on these vectors to define 100 word clusters. We set the center size cardinality\n1Papers were downloaded from http://papers.nips.cc/.\nconstraint to ` = 100 and set the number of document clusters to k = 10. To initialize, we again use k-means++ [29], with k = 10. Results are averaged over 10 trials. While we do not have groundtruth labels for NIPS paper clusters, we can use within-cluster distances as a proxy for cluster goodness (lower values, indicating tighter clusters, are better). Specifically, we compute: k-means-score =\u2211k\nj=1 \u2211 i\u2208Cj g(\u00b5j4Bi). With Hamming for g, the average ratio of HM\u2019s k-means-score to SM\u2019s is 0.916\u00b1 0.003. This indicates that, as expected, HM does a better job of optimizing the Hamming loss. However, with the submodular function for g, the average ratio of HM\u2019s k-means-score to SM\u2019s is 1.635\u00b1 0.038. Thus, SM does a significantly better job optimizing the submodular loss.\n6.2 SH-max application: diverse k-best\nIn this section, we explore a diverse k-best image collection summarization problem, as described in Section 2. For this problem, our goal is to obtain k summaries, each of size l, by selecting from a set consisting of n l images. The idea is that either: (a) the user could choose from among these k summaries the one that they find most appealing, or (b) a (more computationally expensive) model could be applied to re-rank these k summaries and choose the best. As is described in Section 2, we obtain the kth summary Ak, given the first k \u2212 1 summaries A1:k\u22121 via: Ak = argmaxA\u2286V,|A|=` g(A) + \u2211k\u22121 i=1 f(A4Ai).\nFor g we use the facility location function: g(A) = \u2211 i\u2208V maxj\u2208A Sij , where Sij is a similarity score for images i and j. We compute Sij by taking the dot product of the ith and jth feature vectors, which are the same as those used by [8]. For f we compare two different functions: (1) f(A4Ai) = |A4Ai|, the Hamming distance (HM), and (2) f(A4Ai) = g(A4Ai), the submodular facility location distance (SM). For HM we optimize via the standard greedy algorithm [13]; since the facility location function g is monotone submodular, this implies an approximation guarantee of (1 \u2212 1/e). For SM, we experiment with two algorithms: (1) standard greedy [13], and (2) UNION-SPLIT (Algorithm 1) with standard greedy as the SUBMODULAR-OPT function. We will refer to these two cases as \u201csingle part\u201d (SP) and \u201ctwo part\u201d (TP). Note that neither of these\noptimization techniques has a formal approximation guarantee, though the latter would if instead of standard greedy we used the bi-directional greedy algorithm of [27]. We opt to use standard greedy though, as it typically performs much better in practice. We employ the image summarization dataset from [8], which consists of 14 image collections, each of which contains n = 100 images. For each image collection, we seek k = 15 summaries of size ` = 10. For evaluation, we employ the V-ROUGE score developed by [8]; the mean V-ROUGE (mV-ROUGE) of the k summaries provides a quantitative measure of their goodness. V-ROUGE scores are normalized such that a score of 0 corresponds to randomly generated summaries, while a score of 1 is on par with human-generated summaries.\nTable 4 shows that SP and TP outperform HM in terms of mean mV-ROUGE, providing support for the idea of using submodular Hamming distances in place of (modular) Hamming for diverse k-best applications. TP also outperforms SP, suggesting that the objective-splitting used in UNION-SPLIT is of practical significance. Table 5 provides additional evidence of TP\u2019s superiority, indicating that for 10 out of the 14 image collections, TP has the best mV-ROUGE score of the three approaches.\nFigure 1 provides some qualitative evidence of TP\u2019s goodness. Notice that the images in the green rectangle tend to be more redundant with images from the previous summaries in the HM case than in the TP case; the HM solution contains many images with a \u201csky\u201d theme, while TP contains more images with other themes. This shows that the HM solution lacks diversity across summaries. The quality of the individual summaries also tends to become poorer for the later HM sets; considering the images in the red rectangles overlaid on the montage, the HM sets contain many images of tree branches here. By contrast, the TP summary quality remains good even for the last few summaries."}, {"heading": "7 Conclusion", "text": "In this work we defined a new class of distance functions: submodular Hamming metrics. We established hardness results for the associated SH-min and SH-max problems, and provided approximation algorithms. Further, we demonstrated the practicality of these metrics for several applications. There remain several open theoretical questions (e.g., the tightness of the hardness results and the NP-hardness of SH-min), as well as many opportunities for applying submodular Hamming metrics to other machine learning problems (e.g., the structured prediction application from Section 2)."}], "references": [{"title": "Least Squares Quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory, 28(2):129\u2013137,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1982}, {"title": "Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions", "author": ["T. Hazan", "S. Maji", "J. Keshet", "T. Jaakkola"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning CRFs Using Graph Cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "ECCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Perceptually Inspired Layout-Aware Losses for Image Segmentation", "author": ["A. Osokin", "P. Kohli"], "venue": "ECCV,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Submodular Losses with the Lovasz Hinge", "author": ["J. Yu", "M. Blaschko"], "venue": "ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Diverse M-Best Solutions in Markov Random Fields", "author": ["D. Batra", "P. Yadollahpour", "A. Guzman", "G. Shakhnarovich"], "venue": "ECCV,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Mixtures of Submodular Functions for Image Collection Summarization", "author": ["S. Tschiatschek", "R. Iyer", "H. Wei", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Measure Theory", "author": ["P. Halmos"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1974}, {"title": "Approximation Bounds for Inference using Cooperative Cuts", "author": ["S. Jegelka", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular Secretary Problem and Extensions", "author": ["M. Bateni", "M. Hajiaghayi", "M. Zadimoghaddam"], "venue": "Technical report, MIT,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "On Submodular Function Minimization", "author": ["W.H. Cunningham"], "venue": "Combinatorica, 3:185 \u2013 192,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1985}, {"title": "An Analysis of Approximations for Maximizing Submodular Set Functions I", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "14(1),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1978}, {"title": "Approximability of combinatorial problems with multi-agent submodular cost functions", "author": ["G. Goel", "C. Karande", "P. Tripathi", "L. Wang"], "venue": "FOCS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with Submodular Functions: A Convex Optimization Perspective", "author": ["Francis R. Bach"], "venue": "CoRR, abs/1111.6453,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology", "author": ["D. Gusfield"], "venue": "Cambridge University Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Curvature and Efficient Approximation Algorithms for Approximation and Minimization of Submodular Functions", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Submodularity and Curvature: The Optimal Algorithm", "author": ["J. Vondr\u00e1k"], "venue": "RIMS Kokyuroku Bessatsu, 23,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular Approximation: Sampling-Based Algorithms and Lower Bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "FOCS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Size-constrained Submodular Minimization through Minimum Norm Base", "author": ["K. Nagano", "Y. Kawahara", "K. Aihara"], "venue": "ICML,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodularity Beyond Submodular Energies: Coupling Edges in Graph Cuts", "author": ["S. Jegelka", "J. Bilmes"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "The Submodular Bregman and Lov\u00e1sz-Bregman Divergences with Applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast Semidifferential-Based Submodular Function Optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximizing non-monotone submodular functions", "author": ["Uriel Feige", "Vahab S Mirrokni", "Jan Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A Threshold of ln n for Approximating Set Cover", "author": ["Uriel Feige"], "venue": "Journal of the ACM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "A Tight Linear Time (1/2)-Approximation for Unconstrained Submodular Maximization", "author": ["N. Buchbinder", "M. Feldman", "J. Naor", "R. Schwartz"], "venue": "FOCS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Submodular maximization with cardinality constraints", "author": ["N. Buchbinder", "M. Feldman", "J. Naor", "R. Schwartz"], "venue": "SODA,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "k-means++: The Advantages of Careful Seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Clustering: Many clustering algorithms, including for example k-means [1], use distance functions in their optimization.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "However, both [2] and [3] observe poor performance with Hamming distance and recent work by [4] shows improved performance with richer distances that are supermodular functions of A.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "However, both [2] and [3] observe poor performance with Hamming distance and recent work by [4] shows improved performance with richer distances that are supermodular functions of A.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "However, both [2] and [3] observe poor performance with Hamming distance and recent work by [4] shows improved performance with richer distances that are supermodular functions of A.", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": ") In fact, [5] observes superior results with this type of loss-augmented inference using a special case of a submodular Hamming metric for the task of multi-label image classification.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "For instance, [6] showed that for image segmentation and pose tracking a diverse set of k solutions tended to contain a better predictor than the top k highest-scoring solutions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Submodular functions are a natural model for several summarization problems [7, 8].", "startOffset": 76, "endOffset": 82}, {"referenceID": 7, "context": "This result is known (see for instance Chapter 8 of [9]), but we provide a proof (in the supplementary material) for completeness.", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "The simple subadditive function example in the introduction of [10] shows that subadditive minimization is inapproximable, and Theorem 17 of [11] states that no algorithm exists for subadditive maximization that has an approximation factor better than \u00d5( \u221a n).", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "The simple subadditive function example in the introduction of [10] shows that subadditive minimization is inapproximable, and Theorem 17 of [11] states that no algorithm exists for subadditive maximization that has an approximation factor better than \u00d5( \u221a n).", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "By contrast, submodular minimization is poly-time in the unconstrained setting [12], and a simple greedy algorithm from [13] gives a 1\u2212 1/eapproximation for maximization of positive polymatroids subject to a cardinality constraint.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "By contrast, submodular minimization is poly-time in the unconstrained setting [12], and a simple greedy algorithm from [13] gives a 1\u2212 1/eapproximation for maximization of positive polymatroids subject to a cardinality constraint.", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Submodular minimization is poly-time in the unconstrained setting [12].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "Borrowing from [14]\u2019s Theorem 3.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "[14] shows that, knowing G but given only value-oracle access to the f0, no poly-time algorithm can distinguish between f 0 and f b 0 .", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The ratio of these minimizers is 2\u2212 , which allows [14] to show that no poly-time algorithm can achieve a (2\u2212 )-approximation for the minimum submodular vertex cover problem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "The submodularity of F\u0304 follows from the fact that submodular functions are closed under restriction, complementation, and addition (see [16], page 9).", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "This result is known [17], as the proof of the guarantee only makes use of metricity and homogeneity (not submodularity), and these properties are common to much other work.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "1 of [18] establishes that this problem is NP-hard and has a hardness of \u03a9( \u221a n 1+( \u221a n\u22121)(1\u2212\u03baf ) ).", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "[18, 14].", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[18, 14].", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "5 depends on a quantity \u03baf , which is also called the curvature of a submodular function [19, 18].", "startOffset": 89, "endOffset": 97}, {"referenceID": 15, "context": "5 depends on a quantity \u03baf , which is also called the curvature of a submodular function [19, 18].", "startOffset": 89, "endOffset": 97}, {"referenceID": 17, "context": "Unfortunately, the UNION-SPLIT algorithm from the previous section requires an efficient algorithm for submodular function minimization, and no such algorithm exists in the constrained setting; submodular minimization is NP-hard even under simple cardinality constraints [20] (although see [21] that shows it is possible to get solutions for a subset of the cardinality constraints).", "startOffset": 271, "endOffset": 275}, {"referenceID": 18, "context": "Unfortunately, the UNION-SPLIT algorithm from the previous section requires an efficient algorithm for submodular function minimization, and no such algorithm exists in the constrained setting; submodular minimization is NP-hard even under simple cardinality constraints [20] (although see [21] that shows it is possible to get solutions for a subset of the cardinality constraints).", "startOffset": 290, "endOffset": 294}, {"referenceID": 19, "context": "F\u0302 consists of superdifferentials [22, 23] of F \u2019s component submodular functions.", "startOffset": 34, "endOffset": 42}, {"referenceID": 20, "context": "F\u0302 consists of superdifferentials [22, 23] of F \u2019s component submodular functions.", "startOffset": 34, "endOffset": 42}, {"referenceID": 21, "context": "We use the superdifferentials defined as \u201cgrow\u201d and \u201cshrink\u201d in [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "The below bound then holds (from [18]): fi(Y ) \u2264 f\u0302i(Y ) \u2264 |Y | 1 + (|Y | \u2212 1)(1\u2212 \u03bafi(Y )) fi(Y ), \u2200Y \u2286 V.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "5 of [25].", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "The construction of [25] suggests that for both these functions, h1(i) = h2(i) = n \u2212 1, and hence the constant \u2211 i\u2208V h(i) = n(n \u2212 1).", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "Moreover, according to [25], the maximum value of h1(A) is n/4, while that of h2(A) is n/2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "This SH-max instance is exactly the problem of monotone submodular maximization subject to cardinality constraint, which is not only NP-hard but has a hardness of 1\u2212 1/e [26].", "startOffset": 170, "endOffset": 174}, {"referenceID": 26, "context": "We use k-means++ initialization [29] and average over 10 trials.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "To get the word clustersW , we first run the WORD2VEC code of [30], which generates a 100-dimensional real-valued vector of features for each word, and then run k-means clustering with Euclidean distance on these vectors to define 100 word clusters.", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "To initialize, we again use k-means++ [29], with k = 10.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "We compute Sij by taking the dot product of the ith and jth feature vectors, which are the same as those used by [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "For HM we optimize via the standard greedy algorithm [13]; since the facility location function g is monotone submodular, this implies an approximation guarantee of (1 \u2212 1/e).", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "For SM, we experiment with two algorithms: (1) standard greedy [13], and (2) UNION-SPLIT (Algorithm 1) with standard greedy as the SUBMODULAR-OPT function.", "startOffset": 63, "endOffset": 67}, {"referenceID": 24, "context": "Note that neither of these optimization techniques has a formal approximation guarantee, though the latter would if instead of standard greedy we used the bi-directional greedy algorithm of [27].", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "We employ the image summarization dataset from [8], which consists of 14 image collections, each of which contains n = 100 images.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "For evaluation, we employ the V-ROUGE score developed by [8]; the mean V-ROUGE (mV-ROUGE) of the k summaries provides a quantitative measure of their goodness.", "startOffset": 57, "endOffset": 60}], "year": 2015, "abstractText": "We show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over. By exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics. Additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).", "creator": "LaTeX with hyperref package"}}}