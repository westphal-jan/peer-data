{"id": "1511.02385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2015", "title": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction", "abstract": "Our polarity correction technique takes into account the consistency of the (positive and negative) polarities of sentences within each product test before performing the actual machine learning task. While sentences with conflicting polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of product testing, outperforming basic models without the correction method. Experimental results show an average of 82% F measure across four different product inspection areas.", "histories": [["v1", "Sat, 7 Nov 2015 18:38:22 GMT  (302kb,D)", "http://arxiv.org/abs/1511.02385v1", "15 pages. This paper is based on the same sentence-level technique proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy it: sentiment classification on Amazon reviews using sentence polarity shift. In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer Berlin Heidelberg"]], "COMMENTS": "15 pages. This paper is based on the same sentence-level technique proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy it: sentiment classification on Amazon reviews using sentence polarity shift. In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer Berlin Heidelberg", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["sylvester olubolu orimaye", "saadat m alhashmi", "eu-gene siew", "sang jung kang"], "accepted": false, "id": "1511.02385"}, "pdf": {"name": "1511.02385.pdf", "metadata": {"source": "CRF", "title": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction", "authors": ["Sylvester Olubolu Orimaye", "Saadat M. Alhashmi", "Sang Jung Kang"], "emails": ["sylvester.orimaye@monash.edu)", "salhashmi@sharjah.ac.ae)", "siew.eu-gene@monash.edu)", "sjkan2@student.monash.edu)"], "sections": [{"heading": null, "text": "Key Words: Sentiment Analysis, Review-Level Classification, Polarity Correction, Data Mining, Machine Learning"}, {"heading": "1 Introduction", "text": "Sentiment classification has attracted a number of research studies in the past decade. The most prominent in the literature is Pang et al,[1] which employed supervised machine learning techniques to classify positive and negative sentiments in movie reviews. The significance of that work influenced the research\nar X\niv :1\n51 1.\n02 38\ncommunity and created different research directions within the field of sentiment analysis and opinion mining.[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.[6, 7] While the number of reviews has continued to grow, and sentiments are expressed in a subtle manner, it is important to develop more effective sentiment classification techniques that can correctly classify sentiments despite natural language ambiguities, which include the use of irony.[8, 9, 10, 11]\nIn this work, we classify sentiments expressed on individual product types by learning a language model classifier. We focus on online product reviews which contain individual product domains and express explicit sentiment polarities. For example, it is quite common that the opinion expressed in reviews are targeted at the specific products on which the reviews are written.[2, 7] This enables the reviewer to express a substantial level of sentiments on the particular product alone without necessarily splitting the opinions between different products. Also, in a review, sentiments are likely to be expressed on specific aspects of the particular product.[12] For example, an iPad user may express positive sentiment about the \u2018camera quality\u2019 of the device but expresses negative sentiment about the \u2018audio quality\u2019 of the device. This provides useful and collaborative information on aspects of the product that need improvements.[13, 14, 15]\nThe application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.[16, 2, 3] This is because the different categories of sentiments (e.g. positive and negative) represent the actual stances of humans on a particular target (e.g. a product). A product manufacturer for example, can have an overview of how many people \u2018like\u2019 and \u2018dislike\u2019 the product by using the number of positive and negative reviews. Similarly, sentiment classification has been quite useful in finance industries, especially for stock market prediction.[17, 18, 19]\nSentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research. More importantly, sentiments expressed in each product review sometimes include ambiguous and unexpected sentences, [20] and are often alternated between the two different positive and negative polarities. This causes inconsistencies in the sentiments expressed and consequentially leading to the mis-classification of the review document.[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.[3, 4] We emphasize that most negative reviews contain positive sentences and often express negative sentiments by using just a few negative sentences.[21] We show an example as follows:\nI bought myself one of these and used it minimally and was happy (POSITIVE) I am using my old 15 year old Oster (NEGATIVE) Also to my surprise it is doing a better job (POSITIVE) Just not as pretty (NEGATIVE) I have KA stand mixer, hand blender, food processors large and small... (OBJECTIVE) Will buy other KA but not this again (NEGATIVE)\nThe above problem often degrades the accuracy of sentiment classifiers as many review documents get mis-classified to the opposite category. This is regarded as false positives and false negatives as the case may be.\nWhile the above problem is non-trivial, we propose a polarity correction technique that extracts sentences with consistent polarities in a review. Our correction technique includes three separate steps. First, we perform training set correction by training a \u2018na\u0308\u0131ve\u2019 sentence-level polarity classifier to identify false negatives in both positive and negative categories. We then combine the true positives sentences and the false negative sentences of the two opposite categories to form a new training set for each category. Second, we propose a sentence-level polarity correction algorithm to identify consistent polarities in each review, while discarding sentences with inconsistent polarities. Finally, we learn different Machine Learning algorithms to perform the sentiment classification task.\nThe above steps were performed on four different Amazon product review domains and improved the accuracy of sentiment classification of the reviews over a baseline technique and give comparable performance with standard biagram, bag-of-words, and unigram techniques. In terms of F-measure, the technique achieve an average of 82% on all the product review domains.\nThe rest of this paper is organized as follows. We discuss related research work in Section 2. In Section 3, we propose the training set correction technique for sentiment classification task. Section 4 describes the sentence-level polarity correction technique and the corresponding algorithm. Our machine learning experiments and results are presented in Section 5. Finally, Section 6 presents conclusions and future work."}, {"heading": "2 Related Work", "text": "Pang and Lee,[22] proposed a subjectivity summarization technique that is based on minimum cuts to classify sentiment polarities in IMDb movie reviews. The intuition is to identify and extract subjective portions of the review document using minimum cuts in graphs. The minimum cut approach takes into consideration, the pairwise proximity information via graph cuts that partitions sentences which are likely to be in the same class. For example, a strongly subjective sentence might have lexical dependencies on its preceding or next sentence. Thus Pang and Lee,[22] showed that minimum cuts in graph put such sentences in\nthe same class. In the end, the identified subjective portions as a result of the minimum graph cuts are then classified as either negative or positive polarity. This approach showed significant improvement from 82.8% to 86.4% with just 60% subjective portion of the documents.\nIn our work, we introduce additional steps by not only extracting subjective sentences. Instead, we extract subjective sentences with consistent sentiment polarities. We then discard other subjective sentences with inconsistent sentiment polarities that may contribute noise and reduce the performance of the sentiment classifier. Thus, contrary to Pang and Lee,[22] our work has the ability to effectively learn sentiments by identifying the likely subjective sentences with consistent sentiments. Again, we emphasize that some subjective sentences may not necessarily express sentiments towards the subject matter.[3, 4] Consider, for example, the following excerpt from a \u2018positive-labelled\u2019 movie review:\n\u20181real life, however, consists of long stretches of boredom with a few dramatic moments and characters who stand around, think thoughts and do nothing, or come and go before events are resolved. 2Spielberg gives us a visually spicy and historically accurate real life story. 3You will like it.\u2019\nIn the above excerpt, sentence 1 is a subjective sentence which does not contribute to the sentiment on the movie. Explicit sentiments are expressed in sentence 2 and 3. We propose that discarding sentences such as sentence 1 from reviews is likely to improve the accuracy of a sentiment classifier.\nSimilarly, Wilson et al,[23] used instances of polar words to detect contextual polarity of phrases from the MPQA corpus. Each phrase detected is verified to be either polar or non-polar phrase by using the presence of opinionated words from a polarity lexicon. Polar phrases are then processed further to detect their respective contextual polarities which can then be used to train machine learning techniques. Identifying the polarity of phrase-level expression is a challenge in sentiment analysis.[3] Earlier in Section 1, we have illustrated some example sentences to that effect. For clarity, consider the sentence \u2018I am not saying the picture quality of the camera is not good\u2019. In this sentence, the presence of the negation word \u2018not\u2019 does not represent \u2018negative\u2019 polarity of the sentence in context. In fact it emphasizes a \u2018desired state\u2019 that the \u2018picture quality\u2019 of the camera entity is \u2018good\u2019. However, without effective contextual polarity detection, such sentences could be easily classified as \u2018negative\u2019 by ordinary machine learning techniques. To this extent, Wilson et al,[23] performed manual annotation of contextual polarities in the MPQA corpus to train a classifier with a combination of ten features resulting to 65.7% accuracy giving room for more improvement.\nChoi and Cardie,[24] proposed a compositional semantics approach to learn the polarity of sentiments from the sub-sentential level of opinionated expres-\nsions. The compositional semantic approach breaks the lexical constituents of an expression into different semantic components. Thus, the work used content word negators (e.g. sceptic, disbelief) to identify the sentiment polarities from the different semantic components of the expression. Content word negators are negation words other than function words such as not, but, never and so on. Identified sentiment polarities are then combined using a set of heuristic rules to form an overall sentiment polarity feature which can then be used to train machine learning techniques. Interestingly, on the Multi-Perspective Question Answering (MPQA) corpus created by Wiebe et al,[25] this combination yielded a performance of 90.7% over the 89.1% performance of ordinary classifier (e.g. using bag-of-words).\nThe performance achieved by Choi and Cardie,[24] is understandable given that the MPQA corpus contains well \u2018structured\u2019 news articles which are mostly well written on certain topics. Moreover, sentences or expressions which are contained in news articles are most likely to express sequential sentiments for a reasonable classification performance.[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles. In contrast, sentiment classification on product reviews is more challenging as there is often inconsistent or mixed sentiment polarities in the reviews. We have illustrated an example to that effect in Section 1. It would be interesting to know the performance of the heuristics used by Choi and Cardie,[24] on standard product review datasets such as Amazon online product review datasets. A detailed review of other sentiment classification techniques on review documents is provided in Tang et al.[28]\nOur main contribution to the sentiment classification task is to do training set correction and further detect inter-sentence polarity consistency that could improve a sentiment classifier. That is, given a review of n\u2212sentences, we try to understand how the sentiment polarity varies from sentence 1 to sentence n. We hypothesize that detecting consistent sentiment patterns in reviews could improve a sentiment classifier without further sophisticated natural language techniques (e.g. using compositional semantics or linguistic dependencies).[17]\nMore importantly, we believe every sentence in the review may not necessarily contribute to the classification of the review to the appropriate class.[22] We say that certain sequential sentences with consistent sentiment polarities could be sufficient to represent and distinguish between the sentiment classes of a review. Representative features have been argued to be the key to effective classification technique.[29, 30] We emphasize that our approach is promising and can be easily integrated by any sentiment classification system regardless of the sentiment detection technique employed."}, {"heading": "3 Training Set Correction", "text": "Training set polarity correction has been largely ignored in sentiment classification tasks.[6] Earlier, we emphasized that a review document could contain both positive and negative sentences. Moreover, since reviewers often express sentiments on different aspects of products, it is probable that some aspects of the products will receive positive sentiments while others get negative sentiments.[3] In a negative-labeled product review for example, it is more likely that negative sentiments will be expressed within the first few portion of the review and then followed by positive sentiments in the later portion of the review on some of the aspects of the product that gave some satisfactions.[5, 3] This could be because reviewers tend to emphasize on the negative aspects of a product than the positive aspects, and in some cases, both polarities are expressed alternately, which we will discuss in Section 4. Thus, using such mixed sentiments in each category, for training a machine learning algorithm will only result to bias and reduce the accuracy of the classifier.[22, 31]\nAs such, we propose a promising approach to reduce the bias in the training set by first learning a \u2018na\u0308\u0131ve\u2019 sentence-level classifier on all sentences from both the positive and negative categories. A \u2018na\u0308\u0131ve\u2019 classifier could be any classifier trained with surface-level features (e.g. unigram or bag-of-words),[22, 32] without necessarily performing sophisticated features engineering since the final sentiment classifier will be constructed with more fine-grained features. [33] For example, one could learn the popular Na\u0308\u0131ve Bayes classifier with only unigram features.[22, 34, 35] It is also possible to use a more complexly constructed classifier at the expense of efficiency. Having said that, the \u2018na\u0308\u0131ve\u2019 classifier is then used to also test the same sentences from both the positive and negative categories. The idea is to identify positive-labelled sentences that will be classified as negative and negative-labelled sentences that will be classified as positive. Having identified this, it is therefore imperative to correct the training set by combining the wrongly classified sentences to their original respective categories. That is, positive-labelled sentences that are classified as negative should be combined with the original negative sentences (in the negative category) and negativelabelled sentences that are classified as positive should be combined with the original positive sentences (in the positive category).\nWhile this technique may result to a meta classification,[36] we propose to include the technique as part of the training process of the final sentiment classifier. In addition, in order to minimize wrongly classified sentences, we implement the \u2018na\u0308\u0131ve\u2019 classifier to maximize the Joint-Log-Probability score of a given sentence belonging to either of positive or negative categories. This is because most ordinary classifiers maximize the conditional probability over all categories, which is at the expense of better accuracy.[37] We compute the Joint-Log-Probability as follows:\nP (S,C) = log2 P (S|C) + log2 P (C) (1)\nPc = argmaxc\u2208CP (S,C) (2)\nwhere P (S,C) is the probability of a sentence given a class, Pc is the probability of the sentence belonging to either a \u2018positive\u2019 category c or a \u2018negative\u2019 category c and P (C) is a multivariate distribution on the positive and negative categories."}, {"heading": "4 Sentence-Level Polarity Correction", "text": "Following the training set correction in Section 3, we propose the sentence-level polarity correction to further reduce mis-classification in both \u2018training\u2019 and \u2018testing\u2019 sets. More importantly, because the bag-of-words approach has seldom improve the accuracy of a sentiment classifier,[3, 4] a sentence-level approach could give better improvement since most sentiments are expressed at sentencelevel anyway.[38] However, we have indicated in Section 3 that many review documents have the tendency to contain both positive and negative sentences, regardless of their individual categories (i.e. positive or negative). While the consistent sentence polarities of both categories might be helpful to the classification task, it would be better to remove sentences with outlier polarities that cause inconsistencies by using a polarity correction approach.[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.\nThe idea of the sentence-level polarity correction is to remove inconsistent sentence polarities from each review. We observed that sentences with inconsistent polarity deviate from the previous consistent polarity. More often than not, the polarities of sentences in a given review are expressed consistently except for some outliers polarities.[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately. Figure 1 shows an illustration depicting a possible review with consistent polarities and inconsistent polarities (or outlier polarities).\nGiven a 10-sentence review, a reviewer has expressed negative sentiments with the first three sentences. This is followed by a single positive sentence on line 4. Lines 5 to 7 consist of another three negative sentences. Lines 8 to 9 expressed positive sentences. Finally, line 10 concluded with a negative sentence. Thus, we regard line 4 (positive sentence) and line 10 (negative sentence) as outlier polarities because there is no subsequent exact polarity after each of them. Our polarity correction algorithm removes such outliers, leaving only the consistent polarities. It is to be noted that at this stage, the algorithm is independent of a particular sentiment category (i.e. positive or negative). We consider exact\nsubsequent polarities - either positive or negative - since a review is likely to contain both polarities as discussed earlier. Our intuition is that sentences with consistent polarity could better represent the overall sentiment expressed in a review document by providing a wider margin between the categories of the major consistent sentiment polarities.[17, 21] Note that this technique is different from intra-sentence polarity detection as studied in Li et al.[40] An additional thing we did was to performed negation tagging by tagging 1 to 3 words after a negation word in each sentence. In contrast to our baseline, the negation tagging showed some improvements in our correction technique.\nThus, given a review document with n-number of sentences S1, ..., Sn, we classify each sentence with the \u2018na\u0308\u0131ve\u2019 classifier and compare the polarity \u03a6s of the first sentence with the polarity \u03a6sn+1 of the next sentence until sn\u22121. Where \u03a6s is the starting polarity, the polarity of the subsequent sentence \u03a6sn+1 is compared with the polarity of the prior sentence \u03a6\u03bbsn+1 . When \u03a6\u03bbsn+1 equals \u03a6sn+1 , the sentence is stored into the consistent category, otherwise, the sentence is considered outlier. Note that we set a consistency threshold by specifying a parameter \u03b8, which indicates the minimum number of subsequent and the same sentence polarities that must be considered consistent. As such, consistent sentence polarities that are lower than the \u03b8 value are ignored.\nIn our experiment, we set \u03b8 = 2 to simulate the default case. Our empirical observation shows that \u03b8 = 2 sufficiently captures consistent polarities for a sparse review document containing as low as 7 sentences. Figure 2 shows how consistent polarities are extracted with different threshold \u03b8, where \u03b8 = 2 retrieves sentences n3 to n7 and \u03b8 = 3 retrieves only sentences n5 to n7."}, {"heading": "5 Experiment and Results", "text": "We performed several experiments with our correction technique and compare between the performance on popular state-of-the-art classifiers with and without our polarity correction techniques. The classifiers comprises of the Sequential Minimum Optimization (SMO) variant of Support Vector Machines (SVM),[41] and Na\u0308\u0131ve Bayes (NB) classifier.[42] We used SVM and NB on the WEKA machine learning platform,[43] with bag-of-words,unigram, and word bigram features. We did not include word trigram features as both word unigram and word bigram features have been studied to improved sentiment classification tasks.[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.\nFor selecting the best parameters for the baseline algorithms, we performed hyperparameters search using Auto-Weka,[44] with cross-validation and the Sequential Model-based Algorithm Configuration (SMAC) optimization algorithm, which is an Bayesian optimization method proposed as part of Auto-Weka.[44] We performed the search by using the unigram features on the training set of each domain. This is because unigram features have shown robust performance in sentiment analysis.[1, 22, 3]"}, {"heading": "5.1 Dataset and Baseline", "text": "Our dataset is the multi-domain sentiment dataset constructed by Blitzer et al.[5] The dataset was first used in year 2007 and consists of Amazon online\nproduct reviews from four different types of product domains1, which includes, beauty products,books, software, and kitchen. Each product domain has 1000 positive reviews and 1000 negatives reviews, which were identified based on the customers\u2019 star ratings according to Blitzer et al.[5] For each domain, we separated 800 documents per category as training set and used the remaining 200 documents as unseen testing set. We extracted the review text and performed sentence boundary identification by optimizing the output of the MedlineSentenceModel available as part of the LingPipe library.2\nAs our baseline, we implemented a sentence-level sentiment classifier using a technique similar to Pang and Lee,[22] on the same dataset but without our correction technique. The baseline technique has worked very well in most sentiment classification tasks. The baseline work removes objective sentences from the training and testing documents by using an automatic subjectivity detector component which uses subjective sentences only for sentence-level classification."}, {"heading": "5.2 Evaluation", "text": "We used three evaluation metrics comprising of precision, recall, and F-Measure or F-1. The precision is calculated as TP/(TP +FP ), recall as TP/(TP +FN), and F-Measure as (2 \u2217 precision \u2217 recall)/(precision + recall). Note that TP, TN, FP, and FN are defined as true positives, true negatives, false positives, and false negatives, respectively. All results are based on 95% Confidence Interval."}, {"heading": "5.3 Results and Discussion", "text": "We present the results in Tables 2 - 5, where Model is the type of classifier, Pr. is the precision, Rc. is the recall, and F-1 is the F-measure, respectively. We identify the models with our correction technique with \u2018cor\u2019 after the model names. For\n1 http://www.cs.jhu.edu/ mdredze/datasets/sentiment/ 2 http://alias-i.com/lingpipe/docs/api/com/aliasi/sentences/MedlineSentenceModel.html\nexample \u2018SVM-Unigram-Cor\u2019 depicts a model using SVM with unigram features and our correction techniques. Standard models are identified by the algorithm name and the feature used. Baseline models are identified with \u2018Baseline\u2019. In addition, we identify our best performing model above the baseline with (\u2217) and comparable performance with standard models is identified with (\u2020).\nWe see that the model with our correction techniques outperformed the baseline model without the correction techniques on all domains. Other than the baseline model, our technique show comparable performance with the standard bigram, bag-of-words, and unigram models. Not surprisingly, SVM performed better than NB in most cases with bag-of-words and unigram features. On the other hand, NB performed better than SVM with bigram features. The improvement on the baseline technique and the comparable performance on the standard models show the importance of our polarity correction techniques as\napplicable to sentiment classification. It also emphasizes the fact that using the unseen test sets without sentence-level polarity corrections is likely to lead to mis-classification as a result of inconsistent polarities within each review. Perhaps, it could be beneficial to consider the integration of our polarity correction techniques into an independent sentiment classifier for more accurate sentiment classification.\nThe limitation of our polarity correction techniques, however, could be in the construction and the performance of the initial \u2018na\u0308\u0131ve\u2019 classifier for performing both the training set and the sentence-level polarity corrections. Also, the classifier needed to be trained on each review domain. At the same time, we emphasize that a moderate classifier - taking a NB classifier as an example - trained with the standard bag-of-word features, gives an average of approximately 72% F-measure across all domains as observed in our results. Therefore, we believe that the process is likely to have a minimal or negligible effect on the resulting sentiment classifier. As such, in favor of a more efficient classification task, especially on very large datasets, we do not recommend sophisticated classifiers for the initial correction processes. We also like to emphasize that any reasonable sentence-level polarity identification technique,[3] used in place of the \u2018na\u0308\u0131ve\u2019 classifier in the correction processes, is likely to work just fine and give improved results for the overall sentiment classification task."}, {"heading": "6 Conclusions", "text": "In this work, we have proposed a training set and sentence-level polarity correction for the sentiment classification task on review documents. We performed experiments on different Amazon product review domains and show that a sentiment classifier with training set and sentence-level polarity corrections, showed improved performance and outperformed a state-of-the-art sentiment classification baseline on all the review domains. Our correction techniques first remove polarity bias from the training set and then inconsistent sentence-level polarities from both training and testing sets. Given the difficulty of the sentiment classification task [3], we believe that the improvement shown by the correction technique is promising and could lead to building a more accurate sentiment classifier.\nIn the future, we will integrate the training and sentence-level polarity correction techniques as part of an independent sentiment detection algorithm and perform larger scale experiment on large datasets such as the SNAP Web Data: Amazon reviews dataset3, which was prepared by McAuley and Leskovec.[45]\n3 http://snap.stanford.edu/data/web-Amazon.html"}], "references": [{"title": "Thumbs up?: sentiment classification us- ing machine learning techniques,\u201d in Proceedings of the ACL-02 conference on Em- pirical methods in natural language processing", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Facet-based opinion retrieval from blogs,", "author": ["O. Vechtomova"], "venue": "Information Processing & Management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Sentiment analysis and opinion mining,", "author": ["B. Liu"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "New avenues in opinion mining and sentiment analysis,", "author": ["E. Cambria", "B. Schuller", "Y. Xia", "C. Havasi"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification,", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "(Association of Compu- tational Linguistics (ACL)),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "G\u00f3mez-Rod\u0155\u0131guez, \u201cA syntactic approach for opinion mining on spanish reviews,", "author": ["D. Vilares", "M.A. Alonso"], "venue": "Natural Language Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "What computing with words means to me [discussion fo- rum],", "author": ["J. Mendel", "L. Zadeh", "E. Trillas", "R. Yager", "J. Lawry", "H. Hagras", "S. Guadarrama"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A hierarchical approach to mood classification in blogs,", "author": ["F. Keshtkar", "D. Inkpen"], "venue": "Natural Language Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "On the difficulty of automatically detecting irony: beyond a simple case of negation,", "author": ["A. Reyes", "P. Rosso"], "venue": "Knowledge and Information Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Selection of correction candidates for the normalization of spanish user-generated content,", "author": ["M. Melero", "M. Costa-Juss\u00e0", "P. Lambert", "M. Quixal"], "venue": "Natural Lan- guage Engineering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Building a concept-level sentiment dictionary based on commonsense knowledge,", "author": ["A. Tsai", "R. Tsai", "J. Hsu"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Enhanced sen- ticnet with affective labels for concept-based opinion mining,", "author": ["S. Poria", "A. Gelbukh", "A. Hussain", "D. Das", "S. Bandyopadhyay"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Sentiment and behaviour annotation in a corpus of dialogue summaries,", "author": ["N.T. Roman", "P. Piwek", "A.M.B.R. Carvalho", "A.R. Alvares"], "venue": "Journal of Universal Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Opinion mining and sentiment analysis,", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Sentiment polarity identification in financial news: A cohesion-based approach,", "author": ["A. Devitt", "K. Ahmad"], "venue": "Proceedings of the 45th Annual Meeting of the Asso- ciation of Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An introduction to evolutionary com- putation in finance,", "author": ["A. Brabazon", "M. O\u2019Neill", "I. Dempsey"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Evaluating and un- derstanding text-based stock price prediction models,", "author": ["E.J. Fortuny", "T.D. Smedt", "D. Martens", "W. Daelemans"], "venue": "Information Processing & Management,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Extraction of unexpected sen- tences: A sentiment classification assessed approach,", "author": ["D. Li", "A. Laurent", "P. Poncelet", "M. Roche"], "venue": "Intelligent Data Analysis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "The effect of negation on sentiment analysis and retrieval effectiveness,", "author": ["L. Jia", "C. Yu", "W. Meng"], "venue": "Proceeding of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "A sentimental education: sentiment analysis using subjec- tivity summarization based on minimum cuts,", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, (Barcelona,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Recognizing contextual polarity in phrase- level sentiment analysis,", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, (Vancouver, British Columbia,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Learning with compositional semantics as structural in- ference for subsentential sentiment analysis,", "author": ["Y. Choi", "C. Cardie"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, (Honolulu,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Annotating expressions of opinions and emo- tions in language,", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language Resources and Evaluation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Access: news and blog analysis for the social sciences,", "author": ["M. Bautin", "C.B. Ward", "A. Patil", "S.S. Skiena"], "venue": "Proceedings of the 19th international conference on World wide web,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Mining the blogosphere for top news stories identification,", "author": ["Y. Lee", "H.-y. Jung", "W. Song", "J.-H. Lee"], "venue": "Proceeding of the 33rd international ACM SIGIR confer- ence on Research and development in information retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "A survey on sentiment detection of reviews,", "author": ["H. Tang", "S. Tan", "X. Cheng"], "venue": "Expert Systems with Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Self-training from labeled features for sentiment analysis,", "author": ["Y. He", "D. Zhou"], "venue": "Information Processing & Management,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "From bias to opin- ion: a transfer-learning approach to real-time sentiment analysis,", "author": ["P.H. Calais Guerra", "A. Veloso", "W. Meira Jr.", "V. Almeida"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Comparative experiments on sentiment classifi- cation for online product reviews,", "author": ["H. Cui", "V. Mittal", "M. Datar"], "venue": "(American Association for Artificial Intelligence (AAAI)),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "Montecinos, \u201cRobust classification of imbalanced data us- ing one-class and two-class svm-based multiclassifiers,", "author": ["C.S. Maldonado"], "venue": "Intelligent Data Analysis,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Adapting naive bayes to domain adap- tation for sentiment analysis,", "author": ["S. Tan", "X. Cheng", "Y. Wang", "H. Xu"], "venue": "Advances in Information Retrieval,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Feature ensemble plus sample selec- tion: A comprehensive approach to domain adaptation for sentiment classification,", "author": ["R. Xia", "C. Zong", "X. Hu", "E. Cambria"], "venue": "IEEE Intelligent Systems, vol. 28,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Naive bayes models for probability estimation,", "author": ["D. Lowd", "P. Domingos"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Sentence-level sentiment polarity clas- sification using a linguistic approach,\u201d Digital Libraries: For Cultural Heritage, Knowledge Dissemination, and Future Creation", "author": ["L. Tan", "J. Na", "Y. Theng", "K. Chang"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Recognizing contextual polarity: An ex- ploration of features for phrase-level sentiment analysis,", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Computational Linguis- tics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Sentiment classifica- tion and polarity shifting,", "author": ["S. Li", "S.Y.M. Lee", "Y. Chen", "C.-R. Huang", "G. Zhou"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, (Beijing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines,", "author": ["J. Platt"], "venue": "Tech. Rep. MSR-TR-98-14, Microsoft Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1998}, {"title": "An empirical study of the naive bayes classifier,", "author": ["I. Rish"], "venue": "IJCAI 2001 workshop on empirical methods in artificial intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2001}, {"title": "The weka data mining software: an update,", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newslet- ter, vol. 11,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Auto-weka: Com- bined selection and hyperparameter optimization of classification algorithms,", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge dis- covery and data mining,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text,", "author": ["J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The most prominent in the literature is Pang et al,[1] which employed supervised machine learning techniques to classify positive and negative sentiments in movie reviews.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.", "startOffset": 0, "endOffset": 9}, {"referenceID": 2, "context": "[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 2, "context": "[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "[6, 7] While the number of reviews has continued to grow, and sentiments are expressed in a subtle manner, it is important to develop more effective sentiment classification techniques that can correctly classify sentiments despite natural language ambiguities, which include the use of irony.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 1, "context": "[2, 7] This enables the reviewer to express a substantial level of sentiments on the particular product alone without necessarily splitting the opinions between different products.", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[2, 7] This enables the reviewer to express a substantial level of sentiments on the particular product alone without necessarily splitting the opinions between different products.", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "[13, 14, 15] The application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "[13, 14, 15] The application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "[13, 14, 15] The application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[16, 2, 3] This is because the different categories of sentiments (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 1, "context": "[16, 2, 3] This is because the different categories of sentiments (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[16, 2, 3] This is because the different categories of sentiments (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 14, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 76, "endOffset": 86}, {"referenceID": 2, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 76, "endOffset": 86}, {"referenceID": 3, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 76, "endOffset": 86}, {"referenceID": 17, "context": "More importantly, sentiments expressed in each product review sometimes include ambiguous and unexpected sentences, [20] and are often alternated between the two different positive and negative polarities.", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[3, 4] We emphasize that most negative reviews contain positive sentences and often express negative sentiments by using just a few negative sentences.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4] We emphasize that most negative reviews contain positive sentences and often express negative sentiments by using just a few negative sentences.", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[21] We show an example as follows:", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Pang and Lee,[22] proposed a subjectivity summarization technique that is based on minimum cuts to classify sentiment polarities in IMDb movie reviews.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Thus Pang and Lee,[22] showed that minimum cuts in graph put such sentences in", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "Thus, contrary to Pang and Lee,[22] our work has the ability to effectively learn sentiments by identifying the likely subjective sentences with consistent sentiments.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "[3, 4] Consider, for example, the following excerpt from a \u2018positive-labelled\u2019 movie review:", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4] Consider, for example, the following excerpt from a \u2018positive-labelled\u2019 movie review:", "startOffset": 0, "endOffset": 6}, {"referenceID": 20, "context": "Similarly, Wilson et al,[23] used instances of polar words to detect contextual polarity of phrases from the MPQA corpus.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "[3] Earlier in Section 1, we have illustrated some example sentences to that effect.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "To this extent, Wilson et al,[23] performed manual annotation of contextual polarities in the MPQA corpus to train a classifier with a combination of ten features resulting to 65.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Choi and Cardie,[24] proposed a compositional semantics approach to learn the polarity of sentiments from the sub-sentential level of opinionated expres-", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "Interestingly, on the Multi-Perspective Question Answering (MPQA) corpus created by Wiebe et al,[25] this combination yielded a performance of 90.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "The performance achieved by Choi and Cardie,[24] is understandable given that the MPQA corpus contains well \u2018structured\u2019 news articles which are mostly well written on certain topics.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles.", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles.", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles.", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "It would be interesting to know the performance of the heuristics used by Choi and Cardie,[24] on standard product review datasets such as Amazon online product review datasets.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "[28] Our main contribution to the sentiment classification task is to do training set correction and further detect inter-sentence polarity consistency that could improve a sentiment classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] More importantly, we believe every sentence in the review may not necessarily contribute to the classification of the review to the appropriate class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] We say that certain sequential sentences with consistent sentiment polarities could be sufficient to represent and distinguish between the sentiment classes of a review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29, 30] We emphasize that our approach is promising and can be easily integrated by any sentiment classification system regardless of the sentiment detection technique employed.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3] In a negative-labeled product review for example, it is more likely that negative sentiments will be expressed within the first few portion of the review and then followed by positive sentiments in the later portion of the review on some of the aspects of the product that gave some satisfactions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5, 3] This could be because reviewers tend to emphasize on the negative aspects of a product than the positive aspects, and in some cases, both polarities are expressed alternately, which we will discuss in Section 4.", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[5, 3] This could be because reviewers tend to emphasize on the negative aspects of a product than the positive aspects, and in some cases, both polarities are expressed alternately, which we will discuss in Section 4.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[22, 31] As such, we propose a promising approach to reduce the bias in the training set by first learning a \u2018n\u00e4\u0131ve\u2019 sentence-level classifier on all sentences from both the positive and negative categories.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[22, 31] As such, we propose a promising approach to reduce the bias in the training set by first learning a \u2018n\u00e4\u0131ve\u2019 sentence-level classifier on all sentences from both the positive and negative categories.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "unigram or bag-of-words),[22, 32] without necessarily performing sophisticated features engineering since the final sentiment classifier will be constructed with more fine-grained features.", "startOffset": 25, "endOffset": 33}, {"referenceID": 28, "context": "unigram or bag-of-words),[22, 32] without necessarily performing sophisticated features engineering since the final sentiment classifier will be constructed with more fine-grained features.", "startOffset": 25, "endOffset": 33}, {"referenceID": 29, "context": "[33] For example, one could learn the popular N\u00e4\u0131ve Bayes classifier with only unigram features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22, 34, 35] It is also possible to use a more complexly constructed classifier at the expense of efficiency.", "startOffset": 0, "endOffset": 12}, {"referenceID": 30, "context": "[22, 34, 35] It is also possible to use a more complexly constructed classifier at the expense of efficiency.", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "While this technique may result to a meta classification,[36] we propose to include the technique as part of the training process of the final sentiment classifier.", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "[37] We compute the Joint-Log-Probability as follows:", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "More importantly, because the bag-of-words approach has seldom improve the accuracy of a sentiment classifier,[3, 4] a sentence-level approach could give better improvement since most sentiments are expressed at sentencelevel anyway.", "startOffset": 110, "endOffset": 116}, {"referenceID": 3, "context": "More importantly, because the bag-of-words approach has seldom improve the accuracy of a sentiment classifier,[3, 4] a sentence-level approach could give better improvement since most sentiments are expressed at sentencelevel anyway.", "startOffset": 110, "endOffset": 116}, {"referenceID": 33, "context": "[38] However, we have indicated in Section 3 that many review documents have the tendency to contain both positive and negative sentences, regardless of their individual categories (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.", "startOffset": 0, "endOffset": 11}, {"referenceID": 35, "context": "[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately.", "startOffset": 0, "endOffset": 10}, {"referenceID": 35, "context": "[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately.", "startOffset": 0, "endOffset": 10}, {"referenceID": 14, "context": "[17, 21] Note that this technique is different from intra-sentence polarity detection as studied in Li et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[17, 21] Note that this technique is different from intra-sentence polarity detection as studied in Li et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "[40] An additional thing we did was to performed negation tagging by tagging 1 to 3 words after a negation word in each sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The classifiers comprises of the Sequential Minimum Optimization (SMO) variant of Support Vector Machines (SVM),[41] and N\u00e4\u0131ve Bayes (NB) classifier.", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "[42] We used SVM and NB on the WEKA machine learning platform,[43] with bag-of-words,unigram, and word bigram features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[42] We used SVM and NB on the WEKA machine learning platform,[43] with bag-of-words,unigram, and word bigram features.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.", "startOffset": 0, "endOffset": 10}, {"referenceID": 19, "context": "[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.", "startOffset": 0, "endOffset": 10}, {"referenceID": 39, "context": "For selecting the best parameters for the baseline algorithms, we performed hyperparameters search using Auto-Weka,[44] with cross-validation and the Sequential Model-based Algorithm Configuration (SMAC) optimization algorithm, which is an Bayesian optimization method proposed as part of Auto-Weka.", "startOffset": 115, "endOffset": 119}, {"referenceID": 39, "context": "[44] We performed the search by using the unigram features on the training set of each domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 22, 3]", "startOffset": 0, "endOffset": 10}, {"referenceID": 19, "context": "[1, 22, 3]", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 22, 3]", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[5] The dataset was first used in year 2007 and consists of Amazon online", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] For each domain, we separated 800 documents per category as training set and used the remaining 200 documents as unseen testing set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "As our baseline, we implemented a sentence-level sentiment classifier using a technique similar to Pang and Lee,[22] on the same dataset but without our correction technique.", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "We also like to emphasize that any reasonable sentence-level polarity identification technique,[3] used in place of the \u2018n\u00e4\u0131ve\u2019 classifier in the correction processes, is likely to work just fine and give improved results for the overall sentiment classification task.", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "Given the difficulty of the sentiment classification task [3], we believe that the improvement shown by the correction technique is promising and could lead to building a more accurate sentiment classifier.", "startOffset": 58, "endOffset": 61}, {"referenceID": 40, "context": "[45]", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains.", "creator": "TeX"}}}