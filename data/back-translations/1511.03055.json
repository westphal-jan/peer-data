{"id": "1511.03055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing", "abstract": "A typical image retrieval pipeline starts by comparing global descriptors from a large database to find a short list of candidate matches. A good image descriptor is the key to the retrieval pipeline and should reconcile two conflicting requirements: provide recall rates as high as possible and as compact as possible for quick matching. Following the recent success of Deep Convolutional Neural Networks (DCNN) in large-scale image classification, descriptors extracted from DCNNs are increasingly being used instead of traditional handcrafted descriptors such as Fisher Vectors (FV) with better retrieval performance. However, the dimensionality of a typical DCNN descriptor - either extracted from the visual feature pyramid or the fully networked layers - is quite high with several thousand scalar values properties. In this paper, we suggest unsupervised triviplexed hashing (HUP), a fully compressed, non-binary method.", "histories": [["v1", "Tue, 10 Nov 2015 10:38:37 GMT  (1299kb,D)", "http://arxiv.org/abs/1511.03055v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CV cs.LG", "authors": ["jie lin", "olivier mor\\`ere", "julie petta", "vijay chandrasekhar", "antoine veillard"], "accepted": false, "id": "1511.03055"}, "pdf": {"name": "1511.03055.pdf", "metadata": {"source": "CRF", "title": "Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing", "authors": ["Jie Lin", "Olivier Mor\u00e8re", "Julie Petta", "Vijay Chandrasekhar", "Antoine Veillard"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully unsupervised method to compute extremely compact binary hashes \u2013in the 32-256 bits range\u2013 from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First, Stacked Restricted Boltzmann Machines (SRBM), a type of unsupervised deep neural nets, are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then, triplet networks, a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range."}, {"heading": "1 INTRODUCTION", "text": "For mobile visual search and augmented reality applications, the size of data sent over the wireless network needs to be as small as possible to reduce latency and improve user experience. One approach to the problem is to transmit JPEG compressed images or MPEG compressed videos over the wireless network, but this might be prohibitively expensive at low uplink speeds. An alternate approach to sending JPEG images or MPEG videos is to extract feature descriptors on the mobile device, compress the descriptors and transmit them over the wireless network. Such an approach has been demonstrated to reduce the amount of transmission data by orders of magnitude for both visual search\n\u2217 Equal contributions from Jie Lin, Olivier More\u0300re and Julie Petta.\nar X\niv :1\n51 1.\n03 05\n5v 1\n[ cs\n.I R\n] 1\n0 N\nov 2\n2 and augmented reality applications [1] [2]. To this end, MPEG has issued an international standard titled Compact Descriptors for Visual Search (CDVS) [3] for descriptor extraction and compression very recently, and is embarking on extending the CDVS standard to video sources, titled Compact Descriptors for Video Analysis (CDVA).\nState-of-the-art content-based image retrieval pipelines consist of two blocks: (1) retrieving a subset of images from the database that are similar, and (2) using Geometric Consistency Checks (GCC) (e.g., RANSAC) for finding relevant database images with high precision. The GCC step is computationally complex and can only be performed on a small number of images (hundreds). As a result, the first step of the pipeline is critical to achieving high recall. For the first step, state-of-the-art schemes are based on comparing global representations of images. The global descriptor of an image is represented by a single high dimensional vector with tens of thousands of dimensions. Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9]. Subsequently, local descriptors like SIFT [10] and CHoG [1] are used in the GCC step to check if a valid geometric transform exists between database and query images.\nThe problem of global descriptor compression is an important one. The more compact the global descriptor, the faster the descriptors can be compared for matching. Further, it is highly desirable that the global descriptors be binary to enable fast matching through Hamming distances. With internet-scale image databases, like the recently released Yahoo 100M image database [11], compact global descriptors will be key to fast web-scale image-retrieval. Ideally, a 32-bit or 64-bit binary global descriptor is highly desirable, as it can be directly addressed in RAM. However, finding such a representation is extremely challenging."}, {"heading": "2 RELATED WORK AND CONTRIBUTIONS", "text": "Motivated by the remarkable success of deep learning for large-scale image classification [8] [9], recent work show that the use of representations extracted from DCNN is quickly gaining ground over hand-crafted descriptors such as Fisher Vectors as favoured state-of-the-art global image descriptors for image instance retrieval [7] [12]. While neural networks have been around for several decades, their resurgence can be attributed to two key factors: availability of large training data, and large amounts of computing power, which makes training large and deep networks possible. E.g., the neural networks in [8] [9] have 7 and 19 layers respectively, and take weeks to train with millions of images on GPU.\nWhile there is plenty of work on learning binary codes [13] for compressing small descriptors like SIFT, there is relatively little work on compression of high-dimensional global descriptors. One promising approach is unsupervised hashing. The goal of unsupervised hashing is to compress original descriptors into small binary codes with binary embedding functions. These binary embedding functions are usually learned from data without any side information. For example, Gong et al. proposed the popular Iterative Quantization (ITQ) [14]. ITQ first performs PCA and then learn a rotation to minimize the quantization error of mapping the transformed data to the vertices of a zero-centered binary hypercube. Our previous work [15] employed deeply stacked Restriction Boltzmann Machines (SRBM) to learn low dimensional non-linear subspaces of uncompressed descriptors. Other examples include Locality Sensitive Hashing (LSH)\n3 [16], Spectral Hashing (SH) [17] and Bilinear Projection Binary Codes (BPBC) [18], etc. We note that most of the unsupervised hashing schemes formulate image retrieval as an optimization problem, following either the principle of Mean Squared Error (MSE) like ITQ or Maximum-Likelihood Estimation (MLE) like SRBM. These hashing schemes are sub-optimal solutions because they ignore ranking order information in image retrieval, i.e., relevant images should be ranked ahead of irrelevant ones for queries.\nBesides hashing schemes, quantization based method such as Product Quantization (PQ) [19] are an alternative way to compress descriptors. Jegou et al. proposed PCA followed by random rotations and PQ for obtaining compact representations [19]. While this results in highly compact descriptors, the resulting representation is not binary and cannot be compared with Hamming distances. The MPEG CDVS standard adopted the Scalable Compressed Fisher Vector [6], which was based on direct scalar quantization of high-dimensional Fisher Vectors. The size of the compressed descriptor in the MPEGCDVS standard ranges from 256 bytes to several thousand bytes per image, based on the operating points. In this paper, we focus on extremely low bitrate binary coding (32 to 256 bits) of high-dimensional global descriptors, thus, the quantization approaches are outside the scope of this work.\nWe propose Unsupervised Triplet Hashing (UTH), a scheme for learning binary embedding functions of high-dimensional representations. UTH consists of a two-stage deep learning pipeline. First, we use SRBMs to learn a first version of the binary embedding functions in a fashion similar to our previous work in [15] where we showed that the method is able to produce very compact hashes with good retrieval performances. However, SRBMs is purely based on data reconstruction and do not purposefully try to preserve the good metric properties of the original high-dimensional space. Therefore, we add a second step to fine-tune the network weights (embedding parameters). This second step uses triplets of weight-sharing networks and learns to preserve the ranking order of triplets of images. Unlike other approaches using triplet learning networks [20] [21] [22], our approach is fully-unsupervised and does not require additional label data for the triplets. For our experiments, we use for our starting representation an intermediate layer from the 19-layer OxfordNet which is state-of-the-art for image classification.\nOur contributions are three-fold: \u2022 We propose a method for learning binary embedding functions able to produce very\ncompact hashes (down to 32 bits) for image instance retrieval. UTH expands our previous work on hash compression [15] with a fine-tuning step based on triplet networks to preserve the ranking information from the high-dimensional global descriptors.\n\u2022 Unlike with other approaches, the learning-to-rank pipeline is fully unsupervised and does not require any additional label data. To our knowledge, this is the first work on learning to rank with unsupervised deep network applied to image instance retrieval.\n\u2022 Through a thorough empirical evaluation on small and large datasets, we show that UTH can reduce the data size of uncompressed descriptors by 512\u00d7 (256 bits hashes) without considerable retrieval performance loss. We also show that UTH is able to outperform other unsupervised schemes in the 32-256 bits range."}, {"heading": "3 EVALUATION FRAMEWORK", "text": "We use 3 popular data sets for small scale experiments: INRIA Holidays (500 queries, 991 database images) [23], University of Kentucky Benchmark (UKbench) (10200 queries,\n10200 database images) [24], and Oxford5k Buildings (55 queries, 5063 database images) [25]. For large-scale retrieval experiments, we present results on Holidays and UKbench data sets, combined with the 1 million MIR-FLICKR distractor data set [26]. Note that the UKbench dataset is used in the CDVS evaluation framework [3].\nMost schemes, including our proposed scheme, require a training step. We use the ImageNet data set for training, which consists of 1 million images from 1000 different classes [27] (class labels are never used in the scope of this work). We randomly sample a subset of 150K images from ImageNet to learn binary embedding functions. This training set is independent of the query and database data described above ensuring there is no over-fitting while testing.\nThe starting global image descriptor is extracted from the 19-layer OxfordNet proposed for ImageNet classification in the seminal contribution by the VGG group [9]. The features are extracted using the open-source software Caffe [28]. We find that layer fc6 (the first fully connected layer before softmax) performs the best for image retrieval, similar to the recently reported results in [7]. We refer to this 4096-dimensional fc6 features as the DCNN features from here-on."}, {"heading": "4 UNSUPERVISED TRIPLET HASHING", "text": "This work focuses on compressing high-dimensional descriptors into low bitrate binary codes for image instance retrieval. UTH is a two-step procedure. First, we use SRBMs to pre-train an embedding model able to achieve the desired bitrate. Next, we fine-tune the model in order to preserve the ranking information from the original uncompressed descriptors using triplets of images. The entire process is unsupervised and does not require any labeled data and is illustrated in Figure 1.\nDeep Embedding Network. Our previous work showed that high dimensional vectors can be converted to low-dimensional codes by training multi-layer neural networks based on stacked Restricted Boltzmann Machines (SRBM), which can perform significantly better than most unsupervised hashing approaches for image instance retrieval [15].\nAn RBM is an undirected bipartite graphical model consisting of a layer of visible (input) units v and a layer of hidden (output) units h. A set of symmetric weights W connects v and h. For an RBM with visible and hidden binary units, the activation\n5 probabilities of units in one layer can be sampled by fixing the states of the other layer as follows:\nP(hj = 1|v) = \u03c3(bj + \u2211 i wijvi) (1)\nP(vi = 1|h) = \u03c3(bi + \u2211 j wijhj) (2)\nwhere vi and hj are the binary states of visible and hidden units i and j respectively, wij are the weights connecting the units, bi and bj are their respective bias terms, and \u03c3(\u00b7) is the sigmoid function. RBM can be trained by minimizing the contrastive divergence objective [29], which approximates the maximum likelihood of the input distribution. Alternating Gibbs sampling based on Equations (1) and (2) is used to obtain the network states to update the parameters wij , bi, bj through gradient descent.\nWe propose stacking multiple RBMs (SRBM) to create a deep embedding network with several layers. Each layer captures higher order correlations between the units of the previous layer in the network. An SRBM model with two stacked RBMs is illustrated in Figure 1 (b). The first layer of the deep embedding network takes a high-dimensional image descriptor q as visible input. We use binary latent units with sigmoid activation function, because binary output bits are desired for our hash. Besides, binary RBMs are a lot faster and easier to train than continuous RBMs [29]. We perform greedy layer-bylayer training by fully training one RBM at a time using contrastive divergence. Each new RBM layer models the output layer of the previous layer. The number of layers and the number of hidden units in each layer are parameters that are typically chosen experimentally [30] [8] [9]. Here, we progressively decrease the dimensionality of hidden layers by a factor of 2, and train several RBMs with varying number of hidden layers and output units to optimize parameters. More details are available in our previous work [15].\nWhile great at achieving high compression rates, SRBMs do not take into account the metric properties of the high-dimensional input space which is not ideal for image instance retrieval. Accordingly, we perform model fine-tuning in the next step aimed at preserving the good retrieval properties of the original uncompressed descriptors throughout the dimensionality reduction pipeline.\nTriplet Networks. The models obtained from the previous step are fine-tuned using the ranking information provided through triplets. A triplet (q, q+, q\u2212) contains a query image descriptor q, a positive image descriptor q+ and a negative image descriptor q\u2212, query q is more similar (closer) to positive image q+ than to negative image q\u2212. We learn a binary embedding function p : Rn 7\u2192 2m, typically n >> m, such that d(p(q), p(q+)) < d(p(q), p(q\u2212)). Accordingly, we define a triplet ranking loss, l(q, q+, q\u2212) = max{0, g + d(p(q), p(q+)) \u2212 d(p(q), p(q\u2212))}, where g is a positive margin parameter. By normalising the two distances with softmax (denoted d\u2032) and taking g = 1, we can rewrite the triplet loss function as,\nl(q, q+, q\u2212) = max{0, 1 + d\u2032(p(q), p(q+))\u2212 d\u2032(p(q), p(q\u2212))} (3)\nThe idea of using weight sharing network for model fine-tuning is not new and previous work can be found for image classification and semantic retrieval [20] [21] [22]. Unlike with previously proposed approaches, triplet learning in UTH in fully unsupervised and does not require dedicated labeled data. Based on the observation that the original uncompressed descriptors already provide good retrieval performance, we\nsimply construct triplets according to the Euclidean distance in the original space, i.e. such that \u2016q \u2212 q+\u20162 < \u2016q \u2212 q\u2212\u20162. Accordingly, our global objective function is,\nmin l(q, q+, q\u2212) s.t. \u2016q \u2212 q+\u20162 < \u2016q \u2212 q\u2212\u20162 (4)\nThe objective function is solved by Stochastic Gradient Descent (SGD) [20]. Triplet Sampling. A simple sampling strategy is to take three random images, arbitrarily chose q and label two of them as q+ and q\u2212 according to their relative distance to q. However, we should note that in the context of image retrieval, we are only interested in correctly discriminating ranks for the high end of the retrieval list. The space of all possible random triplets is also very vast. Accordingly, we need a more targeted sampling strategy.\nWe propose a threshold sampling method to generate informative triplets. Given a training set containing M images, we build a loop-up table with M buckets offline. For the m-th bucket, we compute distances between the m-th image descriptor and the rest, rank the distances in descending order and store the distances and their associated image IDs in that bucket. To sample a triplet, we first randomly draw an image q from the training set, then we sample a positive image q+ from the bucket associated with q, with the constraint that \u2016q \u2212 q+\u20162 should approach to a pre-defined threshold Tp. Similarly, we sample a negative image q\u2212 from the same bucket such that \u2016q \u2212 q\u2212\u20162 \u2192 Tn . We note that Tp < Tn. To avoid over-fitting, a large number of informative triplets would be required in the fine-tuning step. In our case, we found the optimization converges to a sweet point with a number of 128K triplets for each epoch.\nGenerate binary codes for new image. We use the fine-tuned deep embedding network to compress the high-dimensional image descriptors into binary codes. As shown in Figure 1 (b), each output component in the final layer is obtained by a composite of several non-linear functions (feedforward projection), followed by component-wise binarization at 0.5 to produce the binary codes,\nb = { 1, if gi > 0.5 0, otherwise\n(5)\nwhere gi denotes the i-th component of the deep network output \u03c3(b2+ \u2211 w2\u03c3(b1+w1q)) in Figure 1 (b), wl and bl are fine-tuned weights and bias terms for l-th layer, respectively.\n7"}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "We conduct experiments for the following extremely low bit rates: 32, 64, 128 and 256 bits. For the proposed UTH scheme, we use the following SRBM layer sizes: 4K \u2212 2K \u2212 256, 4K \u2212 2K \u2212 128, 4K \u2212 1K \u2212 64 and 4K \u2212 2K \u2212 32 for {256, 128, 64, 32} bits respectively. These parameters are chosen from the greedy optimization discussed in our previous work [15]. During training, we set the learning rate in the range [0.0005, 0.01] for the weight and bias parameters, momentum to 0.9, and ran the training for a maximum 150 epochs. We illustrate the impact of fine-tuning epochs in terms of mAP on Holidays in Figure 2(a), the results show that the triplet loss optimization in the fine-tuning stage converges quickly after a few epochs. Next, we evaluate the key components of the proposed scheme 1) SRBM based network weights initialization in the pre-training stage and 2) triplet sampling in the fine-tuning stage.\nTo evaluate the effect of network weights initialization in the pre-training stage, we present mAP results on Holidays dataset at output size 64 bits (see Figure 2(b)), for comparison (1) SRBM based network weights proposed in our previous work [15] with (2) random unit-norm network weights (denoted as UniW). In addition, we report the results combining the proposed UTH scheme with either SRBM (denoted as UTH SRBM) or UniW (denoted as UTH UniW). Firstly, SRBM largely improves mAP from 17.4% to 51.6%, compared to UniW. It shows that pre-training the deep network properly is important. Secondly, our UTH can further boost the performance when combined with SRBM (+5.5% for UTH SRBM) or UniW (+4.6% for UTH UniW). This demonstrates the effectiveness of UTH in the fine-tuning stage.\nTo evaluate the impact of triplet sampling strategies in the fine-tuning stage, we report the results in terms of mAP on Holidays at 64 bits, for comparison (1) the proposed threshold triplet sampling (denoted as ThrTri) with (2) uniformly triplet sampling (denoted as UniTri). Note that the number of triplet samples used for fine-tuning is the same for both sampling methods. As shown in Figure 2(c), we observe that our ThrTri performs significantly better than UniTri (54.7% vs. 57.1%). This result shows the proposed triplet sampling method can choose more informative triplets for fine-tuning.\nNext, we compare retrieval experiments for several state-of-the-art hashing and compression schemes. Some of these schemes have been proposed for lower dimensional vectors like SIFT, but we evaluate their performance on high-dimensional DCNN features. (1) LSH [16]. LSH is based on random unit-norm projections of the DCNN, followed by signed binarization. (2) SKLSH [31]. Shift-invariant Kernel LSH (SKLSH) is a distributionfree encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel between the vectors. (3) SH [17]. Spectral Hashing (SH) is to minimize the sum of the Hamming distances between pairs of binary codes weighted by the Gaussian kernel between the corresponding vectors. (4) PCAHash [14]. PCA is applied to the data, and the top ranked dimensions are retained. A random rotation matrix is then applied to balance the variance across projected dimensions, followed by signed binarization. (5) ITQ [14]. For the Iterative Quantization (ITQ) scheme, the authors proposed signed binarization after applying two transforms: first the PCA matrix, followed by a rotation matrix, which minimizes the quantization error of mapping the PCA-transformed data to the vertices of a zero-centered binary hypercube. (6) BPBC [18]. For high dimensional data, the PCA projection matrix might require hundreds of megabytes stored in memory. Instead of the large projection matrices used in [14], the authors apply bilinear random projections, which require far less memory, to transform the data. This is followed by\n8\nsigned binarization to generate binary hashes. (7) SRBM [15]. SRBM is the unsupervised deep hashing scheme based on stacked RBMs, as proposed in our previous work. (8) UTH. The proposed unsupervised triplet hashing (UTH) scheme, fine-tune the deep network initialized by SRBM.\nAs a baseline, we also show the performance of the uncompressed descriptors (4096- dimensional floating point representation). L2 norm is used for uncompressed descriptors, while hamming distances are used for all binary hashing schemes. We present recall@10, recall@100 and mAP results in Figure 3 for the different schemes on Holidays, UKbench and Oxford5k data sets, at varying bitrates. We make the following observations. \u2022 With the fine-tuning stage, the proposed UTH performs much better than SRBM\non all datasets, especially for Holidays and Oxford5k at low bitrates. Besides, ITQ,\n9 PCAHash and SH obtain higher accuracy than LSH and BPBC. SKLSH performs the worst. The ordering of schemes is largely consistent across recall@10, recall@100 and mAP results.\n\u2022 UTH outperforms most schemes across various bitrates on different datasets. For both Holidays and UKbench, UTH performs significantly better than the baseline schemes at extremely low bitrates (e.g., 32), for example, +5% in terms of recall@100 than ITQ on UKbench at 32 bits. The baseline schemes catch up in performance as bitrate increases, i.e., the improvements of UTH over the rest schemes become smaller at 256 bits (expect SKLSH). We note that UTH performs slightly worse than ITQ, PCAHash and SH (about \u22122% in mAP) on Oxford5k. Considering there are only 55 query images from Oxford5k, \u22122% in mAP means UTH is only worse than ITQ, PCAHash and SH on one single query on average, which is a relatively small difference.\n\u2022 There is a significant gap between the uncompressed descriptor and all the compression schemes at extremely low bitrates on all datasets, while the performance of compression schemes approach to uncompressed descriptors as bitrate increases to 256 bits. For instance, there is a 26% drop in recall@10 at 32 bits on Holidays for UTH, compared to uncompressed descriptors, the drop is largely reduced to 2% at 256 bits.\nFinally, we present large-scale retrieval experiments in Figure 4 for Holidays (500 queries) and UKbench (10200 queries) data sets combined with 1 million distractor images, respectively. For instance retrieval, the GCC step is computationally complex and can only be performed on a small number of images. As a result, it is important for the relevant image to be present in the short list, so that the GCC step can find it. Hence, we present recall at typical operating points R = 1000 after the first step in the retrieval pipeline: matching of global descriptors. Best parameters for UTH are chosen as described before. We note that UTH outperforms all other schemes at all bitrates. This trend are consistent with the small scale experiments.\nConsidering the significant drop in performance at 32 bits, improving retrieval accuracy at extremely low rates is an exciting direction for future work. Besides, the deep ranking scheme may be further improved using supervised information: e.g., taking into side information such as image labels to find more compact and discriminative embedding functions. Another promising direction would be to learn compact global descriptors for instance retrieval, directly from image pixel data using CNNs [9]."}, {"heading": "6 CONCLUSION", "text": "We proposed UTH, a novel fully-unsupervised method for compressing global image descriptors to extremely small binary hashes (32-256 bits). Following a pre-training step using SRBMs, the model parameters are fine-tuned using image triplets in order to preserve the good retrieval performances of the uncompressed descriptors. With a thorough empirical evaluation, we showed that the fine-tuning step consistently improves the retrieval results and that UTH comes close to matching the performance of the uncompressed descriptor at 256 bits. On average, results suggest that UTH is currently best unsupervised hashing scheme outperforming other popular schemes such as LSH, PCAHash or ITQ.\n10"}], "references": [{"title": "Compressed Histogram of Gradients: A Low Bitrate Descriptor", "author": ["V. Chandrasekhar", "G. Takacs"], "venue": "International Journal of Computer Vision, pp. 384\u2013399, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Interframe coding of feature descriptors for mobile augmented reality", "author": ["M. Makar", "V. Chandrasekhar"], "venue": "IEEE Transactions on Image Processing, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale Image Retrieval with Compressed Fisher Vectors", "author": ["F. Perronnin", "Y. Liu", "J. Sanchez", "H. Poirier"], "venue": "Proceedings of IEEE Conference on CVPR, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Residual Enhanced Visual Vector as a Compact Signature for Mobile Visual Search", "author": ["D.M. Chen", "S.S. Tsai"], "venue": "Signal Processing, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Rate-adaptive compact fisher codes for mobile visual search", "author": ["J. Lin", "L.-Y. Duan"], "venue": "IEEE Signal Processing Letters, pp. 195\u2013198, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural Codes for Image Retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Object Recognition from Local Scale-Invariant Features", "author": ["D. Lowe"], "venue": "Proceedings of IEEE Conference on CVPR, 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "A Practical Guide to CNNs and Fisher Vectors for Image Instance Retrieval", "author": ["V. Chandrasekhar", "J. Lin", "O. Morere", "H. Goh", "A. Veillard"], "venue": "arXiv:1508.02496, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Binary Hash Codes for Large-Scale Image Search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision, 2013, pp. 49\u201387.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative Quantization: A Procrustean Approach to Learning Binary Codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "Proceedings of IEEE Conference on CVPR, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Compact Global Descriptors for Visual Search", "author": ["V. Chandrasekhar", "J. Lin", "O. Morere", "A. Veillard", "H. Goh"], "venue": "Proceedings of IEEE Data Compression Conference (DCC), 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Locality-Sensitive Hashing Scheme based on p-stable Distributions", "author": ["M. Datar", "N. Immorlica"], "venue": "Proceedings of the Twentieth Annual Symposium on Computational Geometry, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Spectral Hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Proceedings of Neural Information Processing Systems (NIPS), 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning Binary Codes for High-Dimensional Data Using Bilinear Projections.", "author": ["Y. Gong", "S. Kumar"], "venue": "Proceedings of IEEE Conference on CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1704\u20131716, 2012.  11", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep metric learning using Triplet network", "author": ["E. Hoffer", "N. Ailon"], "venue": "International Conference on Learning Representations, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Fine-grained Image Similarity with Deep Ranking", "author": ["J.Wang", "Y.Wang"], "venue": "Proceedings of IEEE Conference on CVPR, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Simultaneous Feature Learning and Hash Coding with Deep Neural Networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "Proceedings of IEEE Conference on CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Proceedings of ECCV, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable Recognition with a Vocabulary Tree", "author": ["D. Nist\u00e9r", "H. Stew\u00e9nius"], "venue": "Proceedings of IEEE Conference on CVPR, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Object Retrieval with Large Vocabularies and Fast Spatial Matching", "author": ["J. Philbin", "O. Chum"], "venue": "Proceedings of IEEE Conference on CVPR, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "New Trends and Ideas in Visual Concept Detection: The MIR Flickr Retrieval Evaluation Initiative", "author": ["M. Huiskes", "B. Thomee", "M. Lew"], "venue": "ACM International Conference on MIR, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong"], "venue": "Proceedings of IEEE Conference on CVPR, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer"], "venue": "arXiv:1408.5093, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G. Hinton"], "venue": "Neural Networks: Tricks of the Trade, 2012, pp. 599\u2013619.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, pp. 504\u2013507, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality-Sensitive Binary Codes from Shift-Invariant Kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Proceedings of Neural Information Processing Systems (NIPS), 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "and augmented reality applications [1] [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "and augmented reality applications [1] [2].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9].", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9].", "startOffset": 214, "endOffset": 217}, {"referenceID": 6, "context": "Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9].", "startOffset": 235, "endOffset": 238}, {"referenceID": 7, "context": "Examples include Fisher Vectors [4], Residual Enhanced Visual Vector(REVV) [5], Scalable Compressed Fisher Vector (SCFV) [6], and the recently proposed descriptor based on Deep Convolutional Neural Networks (DCNN) [7], such as AlexNet [8] and OxfordNet [9].", "startOffset": 253, "endOffset": 256}, {"referenceID": 8, "context": "Subsequently, local descriptors like SIFT [10] and CHoG [1] are used in the GCC step to check if a valid geometric transform exists between database and query images.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "Subsequently, local descriptors like SIFT [10] and CHoG [1] are used in the GCC step to check if a valid geometric transform exists between database and query images.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Motivated by the remarkable success of deep learning for large-scale image classification [8] [9], recent work show that the use of representations extracted from DCNN is quickly gaining ground over hand-crafted descriptors such as Fisher Vectors as favoured state-of-the-art global image descriptors for image instance retrieval [7] [12].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "Motivated by the remarkable success of deep learning for large-scale image classification [8] [9], recent work show that the use of representations extracted from DCNN is quickly gaining ground over hand-crafted descriptors such as Fisher Vectors as favoured state-of-the-art global image descriptors for image instance retrieval [7] [12].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "Motivated by the remarkable success of deep learning for large-scale image classification [8] [9], recent work show that the use of representations extracted from DCNN is quickly gaining ground over hand-crafted descriptors such as Fisher Vectors as favoured state-of-the-art global image descriptors for image instance retrieval [7] [12].", "startOffset": 330, "endOffset": 333}, {"referenceID": 9, "context": "Motivated by the remarkable success of deep learning for large-scale image classification [8] [9], recent work show that the use of representations extracted from DCNN is quickly gaining ground over hand-crafted descriptors such as Fisher Vectors as favoured state-of-the-art global image descriptors for image instance retrieval [7] [12].", "startOffset": 334, "endOffset": 338}, {"referenceID": 6, "context": ", the neural networks in [8] [9] have 7 and 19 layers respectively, and take weeks to train with millions of images on GPU.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": ", the neural networks in [8] [9] have 7 and 19 layers respectively, and take weeks to train with millions of images on GPU.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "While there is plenty of work on learning binary codes [13] for compressing small descriptors like SIFT, there is relatively little work on compression of high-dimensional global descriptors.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "proposed the popular Iterative Quantization (ITQ) [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "Our previous work [15] employed deeply stacked Restriction Boltzmann Machines (SRBM) to learn low dimensional non-linear subspaces of uncompressed descriptors.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "[16], Spectral Hashing (SH) [17] and Bilinear Projection Binary Codes (BPBC) [18], etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16], Spectral Hashing (SH) [17] and Bilinear Projection Binary Codes (BPBC) [18], etc.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "[16], Spectral Hashing (SH) [17] and Bilinear Projection Binary Codes (BPBC) [18], etc.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "Besides hashing schemes, quantization based method such as Product Quantization (PQ) [19] are an alternative way to compress descriptors.", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "proposed PCA followed by random rotations and PQ for obtaining compact representations [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "The MPEG CDVS standard adopted the Scalable Compressed Fisher Vector [6], which was based on direct scalar quantization of high-dimensional Fisher Vectors.", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "First, we use SRBMs to learn a first version of the binary embedding functions in a fashion similar to our previous work in [15] where we showed that the method is able to produce very compact hashes with good retrieval performances.", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "Unlike other approaches using triplet learning networks [20] [21] [22], our approach is fully-unsupervised and does not require additional label data for the triplets.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Unlike other approaches using triplet learning networks [20] [21] [22], our approach is fully-unsupervised and does not require additional label data for the triplets.", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "Unlike other approaches using triplet learning networks [20] [21] [22], our approach is fully-unsupervised and does not require additional label data for the triplets.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "UTH expands our previous work on hash compression [15] with a fine-tuning step based on triplet networks to preserve the ranking information from the high-dimensional global descriptors.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "We use 3 popular data sets for small scale experiments: INRIA Holidays (500 queries, 991 database images) [23], University of Kentucky Benchmark (UKbench) (10200 queries,", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "10200 database images) [24], and Oxford5k Buildings (55 queries, 5063 database images) [25].", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "10200 database images) [24], and Oxford5k Buildings (55 queries, 5063 database images) [25].", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "For large-scale retrieval experiments, we present results on Holidays and UKbench data sets, combined with the 1 million MIR-FLICKR distractor data set [26].", "startOffset": 152, "endOffset": 156}, {"referenceID": 24, "context": "We use the ImageNet data set for training, which consists of 1 million images from 1000 different classes [27] (class labels are never used in the scope of this work).", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "The starting global image descriptor is extracted from the 19-layer OxfordNet proposed for ImageNet classification in the seminal contribution by the VGG group [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 25, "context": "The features are extracted using the open-source software Caffe [28].", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "We find that layer fc6 (the first fully connected layer before softmax) performs the best for image retrieval, similar to the recently reported results in [7].", "startOffset": 155, "endOffset": 158}, {"referenceID": 12, "context": "Our previous work showed that high dimensional vectors can be converted to low-dimensional codes by training multi-layer neural networks based on stacked Restricted Boltzmann Machines (SRBM), which can perform significantly better than most unsupervised hashing approaches for image instance retrieval [15].", "startOffset": 302, "endOffset": 306}, {"referenceID": 26, "context": "RBM can be trained by minimizing the contrastive divergence objective [29], which approximates the maximum likelihood of the input distribution.", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "Besides, binary RBMs are a lot faster and easier to train than continuous RBMs [29].", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "The number of layers and the number of hidden units in each layer are parameters that are typically chosen experimentally [30] [8] [9].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "The number of layers and the number of hidden units in each layer are parameters that are typically chosen experimentally [30] [8] [9].", "startOffset": 127, "endOffset": 130}, {"referenceID": 7, "context": "The number of layers and the number of hidden units in each layer are parameters that are typically chosen experimentally [30] [8] [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 12, "context": "More details are available in our previous work [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "The idea of using weight sharing network for model fine-tuning is not new and previous work can be found for image classification and semantic retrieval [20] [21] [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "The idea of using weight sharing network for model fine-tuning is not new and previous work can be found for image classification and semantic retrieval [20] [21] [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 19, "context": "The idea of using weight sharing network for model fine-tuning is not new and previous work can be found for image classification and semantic retrieval [20] [21] [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 17, "context": "The objective function is solved by Stochastic Gradient Descent (SGD) [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "These parameters are chosen from the greedy optimization discussed in our previous work [15].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "To evaluate the effect of network weights initialization in the pre-training stage, we present mAP results on Holidays dataset at output size 64 bits (see Figure 2(b)), for comparison (1) SRBM based network weights proposed in our previous work [15] with (2) random unit-norm network weights (denoted as UniW).", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "(1) LSH [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "(2) SKLSH [31].", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "(3) SH [17].", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "(4) PCAHash [14].", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "(5) ITQ [14].", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "(6) BPBC [18].", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Instead of the large projection matrices used in [14], the authors apply bilinear random projections, which require far less memory, to transform the data.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "(7) SRBM [15].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "Another promising direction would be to learn compact global descriptors for instance retrieval, directly from image pixel data using CNNs [9].", "startOffset": 139, "endOffset": 142}], "year": 2015, "abstractText": "A typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches. A good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements: providing recall rates as high as possible and being as compact as possible for fast matching. Following the recent successes of Deep Convolutional Neural Networks (DCNN) for large scale image classification, descriptors extracted from DCNNs are increasingly used in place of the traditional hand crafted descriptors such as Fisher Vectors (FV) with better retrieval performances. Nevertheless, the dimensionality of a typical DCNN descriptor \u2013extracted either from the visual feature pyramid or the fully-connected layers\u2013 remains quite high at several thousands of scalar values. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully unsupervised method to compute extremely compact binary hashes \u2013in the 32-256 bits range\u2013 from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First, Stacked Restricted Boltzmann Machines (SRBM), a type of unsupervised deep neural nets, are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then, triplet networks, a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range.", "creator": "LaTeX with hyperref package"}}}