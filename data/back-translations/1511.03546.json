{"id": "1511.03546", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Hierarchical Latent Semantic Mapping for Automated Topic Generation", "abstract": "Traditional generative models (PLSA, LDA) are the most advanced approaches to topic modeling, and recent research on topic generation focuses on improving or extending these models. However, the results of traditional generative models are sensitive to the number of topics K must be specified manually. The problem of generating topics from corpus is similar to recognizing communities in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of communities. Inspired by these algorithms, we propose in this paper a novel method called Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each word pair in the latent topic space, then constructs a unified latent network of several words from this network and compares these approaches with the performance of SM systems.", "histories": [["v1", "Wed, 11 Nov 2015 15:58:30 GMT  (513kb,D)", "https://arxiv.org/abs/1511.03546v1", "9 pages, 3 figures, Under Review as a conference at ICLR 2016. arXiv admin note: text overlap witharXiv:1010.0431,arXiv:0812.1242by other authors"], ["v2", "Mon, 16 Nov 2015 13:47:53 GMT  (513kb,D)", "http://arxiv.org/abs/1511.03546v2", "9 pages, 3 figures, Under Review as a conference at ICLR 2016"], ["v3", "Tue, 17 Nov 2015 05:23:58 GMT  (388kb,D)", "http://arxiv.org/abs/1511.03546v3", "9 pages, 3 figures, Under Review as a conference at ICLR 2016"], ["v4", "Thu, 26 Nov 2015 01:35:58 GMT  (388kb,D)", "http://arxiv.org/abs/1511.03546v4", "9 pages, 3 figures, Under Review as a conference at ICLR 2016"]], "COMMENTS": "9 pages, 3 figures, Under Review as a conference at ICLR 2016. arXiv admin note: text overlap witharXiv:1010.0431,arXiv:0812.1242by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["guorui zhou", "guang chen"], "accepted": false, "id": "1511.03546"}, "pdf": {"name": "1511.03546.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Guorui Zhou", "Guang Chen"], "emails": ["chenguang}@bupt.edu.cn"], "sections": [{"heading": "1 INTRODUCTION", "text": "Managing large allocation of documents has become a popular challenge in many fields. Topic modeling, which assigns topics to documents, offers a promising solution for this challenge.\nTopic models generate topics from a set of documents and assign topics to these documents. Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications. There has been an exceptional amount of research on topic-model algorithms. PLSA and LDA are highly modular and can therefore be easily extended. Since LDA\u2019s introduction, there is much research based on it. The Correlated Topic Model Advances Blei & Lafferty (2006) follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA Blei et al. (2010b), where topics are joined together in a hierarchy by using the nested Chinese restaurant process.\nThe core assumption of standard topic-model algorithms is that a corpus consisted of N documents. And each document is generated by the processing selecting one topic from K topics with probability p(topic|doc) then selecting one word from Nw distinct words with probability p(word|topic). Then, our problem is translated to estimate NK probabilities p(topic|doc) and KNw probabilities p(word\u2014topic). LDA and PLSI both aim to estimate the values of these probabilities with the highest likelihood of generating the corpus (Hofmann, 1999; Blei & Jordan, 2003; Griffiths & Steyvers, 2004; Nallapati et al., 2007). Thus, the inference problem is transformed to an optimization problem (Blei et al., 2010a). But there exist many competing models with nearly identical likelihoods. Due to the high degeneracy of the likelihood landscape, standard optimization algorithms will more likely infer different models after different optimization runs than infer the model with the highest\nar X\niv :1\n51 1.\n03 54\n6v 4\n[ cs\n.L G\n] 2\n6 N\nov 2\n01 5\nlikelihood,as has been previously reported Blei et al. (2010a); Wallach et al. (2009). A research on the validity of LDA optimization algorithms for inferring topic models also proposed that current implementations of LDA had low validity (Lancichinetti et al., 2015).\nMeanwhile, selecting the number of topics K is one of the most problematic modeling choices in finite topic modeling. There is no effective method for choosing K or evaluating the probability of held-out data for various values of K so far. And degree to which LDA is robust to a poor setting of K is not well-understood (Wallach et al., 2009). Ideally, if LDA has sufficient topics to model the data set well, an increase in K would not have a impact on the assignments of tokens to topics \u2013i.e., the additional topics should be used with low frequency. For example, if twenty topics is adequacy to exactly model the data, then inferred topic assignments would not be significantly affected by increasing the number of topics to fifty. If this is the case, using large K would not have a improvement on the inference. In another words, we still need a robust K. Actually, K could be seem as the rank of the solution space for topic generation. Setting K is same as manually selecting the rank of the solution space, which is obviously not reasonable.\nThe standard topic-model algorithms focus on the modeling the process of generating documents with topics. In this paper we propose an approach to get an initial guess of topics from the distribution of words and documents. If we think about an easy problem, in which one word can only belongs to one topic. Generating topics from corpus closely approximates to the processing of community detection in networks. A substantial amount of work in the area of community detection in networks has proposed effective algorithms to reveal the struct of the network only using the original information of the network without other prior knowledge. So we create a network of words in the corpus and detecting the communities of the network as the initial guess for topics, then refine these coarse topics. And the words with top p(word|topic) in the topics extracted by HLSM are interesting, it seems that HLSM distinguishes the topics in a more concrete level. For example, image, jpeg, gif\u201d and \u201d3d, graphic, ray\u201d will be assigned to different topics.\nThe contribution of this paper can be summarized as follows:\n\u2022 Propose a novel approach to constructing network of words closely related to the latent topic space.\n\u2022 Adapt approaches from community detection in networks to initial hierarchical topic generation, and also propose a method to further refine the topics.\n\u2022 To evaluate the effectiveness of the proposed approach, we conducted experiments on several real-world text data sets. The experimental results demonstrate that our approach provides greatly improvements in terms of documents classification."}, {"heading": "2 HIERARCHICAL LATENT SEMANTIC MAPPING", "text": "Hierarchical Latent Semantic Mapping (HLSM) is a network approach to topic modeling. Similar to the well-known topic models, each document is represented as a mixture over latent topics. The key feature that distinguishes the HLSM model from the existing topic models is that HLSM directly clusters words and defines each cluster as a topic, then refines these initial topics, thus HLSM estimates the probability distributions p(word|topic) in a novel process. The HLSM model infers topics as the following steps:\nstep 1. Construct the unipartite network.we calculate the association between each pair of words that co-occur in at least one document. Then we construct the unipartite network in which words are connected with the association above the threshold.\nstep 2. Clustering of words hierarchically.The words in the unipartite network are connected by the association in the latent topic space. Naturally we suppose that topics in the corpus will give rise to communities of words in the network. Thus we use the Hierarchical Map Equation Rosvall & Bergstrom (2011) to detect the communities. And in most of corpus, topics come in the form of multiple levels of abstraction. Abstract topic consists of several concrete topics. Thus we detect some massive communities corresponding to the abstract topics, then we detect minor communities, which correspond to the concrete topics, from the massive communities. We take the communities as a prior guess for the number of topics and word\ncomposition of each of the topics used to generate the documents.It is worth noting that we do not set the number of levels and the number of communities for each level. Hierarchical Map Equation can reveal the multilevel organization in the network of words automatically.\nstep 3. Refine the prior guess. After the last level of clustering of words, one may get some single communities of words, and in the step 2, one may get some single words not in the network. Thus the prior topics detected in step 2 are rough, we refine the topics using a PLSA-like likelihood optimization."}, {"heading": "2.1 CONSTRUCT THE UNIPARTITE NETWORK", "text": "The association between words must be closely related to the topics to ensure the validity of clustering words based on this network. But the topics are latent, and all observations are the words collected into documents. If we assigns topics to documents artificially with prior human knowledge, one can observe that documents share the same topics also are more likely to share some words. Naturally we can believe that the words co-occur in many documents share the same topic, in another word these words are more similar in the latent topic space. To calculate the association between words in the latent topic space. Like the core idea of Latent Semantic Analysis (LSI), we map words to a vector space of reduced dimensionality based on a Singular Value Decomposition (SVD) of the co-occurrence matrix M , which each row i corresponds to a word, each column j to a document in which the word appeared, and each matrix entry Mij corresponds to the number of occurrences of word i in document j.\nStarting with the standard SVD given by M = U\u03a3V t, (1)\nthe diagonal matrix \u03a3 contains the singular values of M. The approximation of M is computed by setting all but the largest K singular values in \u03a3 to zero (= \u03a3\u0303), which is rank K optimal in the sense of the L2-matrix norm.\nOne obtains the approximation\nM\u0303 = U \u03a3\u0303V t \u2248 U\u03a3V t = M, (2)\nThe corresponding low-dimensional latent vectors will typically not be sparse, while the original high-dimensional Matrix M is sparse. This implies that one can calculate meaningful association values between pairs of words in the latent topic space. In HLSM, we calculate the cosine similarity between the rows of U \u03a3\u0303 as the association of each pair of words in the latent topic space, and connects word i and j with this association S(i, j) :\nW = U \u03a3\u0303, S(i, j) = \u3008Wi \u00b7Wj\u3009 \u2016Wi\u2016 \u00b7 \u2016Wj\u2016 .\nAfter calculating all the values of connections. Suppose that the association values between some pair of words are so low that we presume these connections are noise. One can set a threshold of q to purne the connections lower than q."}, {"heading": "2.2 CLUSTERING WORDS HIERARCHICALLY", "text": "In most of corpus, the structure of topics is not simple and always can be multiple levels. Some concrete topics sit under a same abstract topic. For example, words in a corpus focusing on \u201csoccer\u201d might be drawn from the topics \u201cstars\u201d, \u201cmatches\u201d, \u201chistory of soccer\u201d, etc.\nWe construct the network of words based on the association between words in the latent topic space. If the original structure of topics is multiple levels, the network should also have a multilevel structure. To reveal communities at multiple levels, we choose the Hierarchical Map Equation Rosvall & Bergstrom (2011). It is worth noting that we do not set the number of levels and the number of communities for each level. Instead Hierarchical Map Equation can reveal the multilevel organization in the network of words automatically.\nThe Map Equation proposed the duality between finding community structure in networks and minimizing the specification length of a random walker\u2019s movements on a network. For a given network partition, the map equation definiens the limit L(M) of how laconic one can describe the trajectory of this random walk in theory.\nThe core idea of map equation is that if the random walker tends to stay in some blocks of the network for a long time, the code used for specification can be compressed. Therefore, when the proxy for real flow random walk in the network, estimating the minimum map equation over all possible network partitions could reveals the structure of the network with respect to the dynamics on the network.\nIn our problem, for a hierarchical network M of n nodes, each node corresponds to one word, segmentated into m modules. There is a a submap M i with mi submodules in one modules. Correspondingly, there is a submap M ij with mij submodules in each each submodule ij, and so on.\nThe corresponding hierarchical map equation is\nL(M) = qswitchH(Q) + m\u2211 i=1 L(M i) (3)\nwith the specification length of submap M i at intermediary levels given by\nL(M i) = qiswitchH(Q i) + mi\u2211 j=1 L(M ij) (4)\nand at the final modular level by\nL(M ij...k) = pij...kinH(P ij...k) (5)\nWeight of codebook depends on the rate of use of it, and L(M) is the sum of average length of codewords for each codebook. H(Q) is the average length of codewords in the index codebook according to the rate of use of it, while the entropy terms depends on the rate at which the codebooks are used. On any given step the random walker switches the first level modules at probability of qswitch, while qswitch is the rate of index codebook is used.\nAt each submodule level, H(Qi) is the average length of the codewords according to the using rate in the subindex codebook and qiswitch is the rate of codeword use for entering the mi submodules or exiting to a higher level. At the last level, H(P ij...k) is the average length of the codewords according to the using rate in the submodule codebook and pij...kin is the rate of codeword use for visiting nodes in submodules ij...k or exiting to other submodules. The problem of seeking the hierarchical structure that best represents the structure is translated to finding the hierarchical partition of the network with the minimum map equation. Fig.2 illustrates an example for map equation.\nIn this example we can assume that all weights for connections in the network are equal, thus all rates can be calculated by counting links and normalizing. The specification length for an unpartitioned network is\u2212log2(1/18) = 4.17bits. After the network is partitioned, the codewords of the first level modules are used at a total rate qswitch = 250 ( There are 25 lines in the network and 50 possible moves when considering direction, while only 2 moves can switch between the first level module.), while relative rates Q = 12 , 1 2 . And Q 1 = 28 , 2 8 , 3 8 , 1 8 , noticing that there is a rate at 1 8 random walker\nexisting to Module 2, while q1switch is 8 50 . Thus L(M) is:\nL(M) = qswitchH(Q) +  q1switchH(Q 1)+  p11inH(P 11) p12inH(P 12) p13inH(P 13) q2switchH(Q 2)+  p21inH(P 21) p22inH(P 22)\np23inH(P 23)\nL(M) = 0.04 bits + 0.61bits + 2.54 bits = 3.19 bits ."}, {"heading": "2.3 REFINE THE PRIOR GUESS", "text": "Once the network is built, we detect clusters (same as the modules detected by Hierarchical Map Equation) of highly associated words using the Hierarchical Map Equation. After the last level of clustering, we get a hard partition of words, meaning that words can only belong to a single cluster. Actually a word may have multiple senses and multiple types of usage in different context. Consequently if we simply define every cluster as a topic, these rough topics can not provide a reasonable probabilistic interpretation of the corpus in terms of the latent topic space. Therefore we propose a method to further refine these rough topics.\nWe now discuss how we can compute the distributions p(topic|doc) and p(word|topic), given a partition of words. In the prior partition of words, we define every cluster as a topic. In fact, each word in the network can sit in only one module after the Hierarchical Map Equation processing. Therefore, p(t|w) = \u03b4t,w . \u03b4t,w = 1 only if the word w sits in the module, which corresponds to the topic t . For other topics t\u0304 , \u03b4t\u0304,w = 0 . Noticing that in this step word w can only belongs to one topic t, so p(w, t) = p(w) , thus:\np(w|t) = p(w)\u2211 w p(w)\u00d7 \u03b4t,w and p(t|d) = 1 Ld \u2211 w wdw\u03b4t,w. (6)\nLd is the number of words in document d, wdw is the number of times wordw occurs in the document d. It is also useful to introduce n(w, t) = LC \u00d7 p(w, t), which is the number of times topic t was chosen and word w was drawn.LC is the number of the words in the corpus. So far, the PLSA-like likelihood of our model is:\nL = log( \u220f w,d p(w, d)) = log( \u220f w,d \u2211 t p(w|t)p(t|d))\n= \u2211 d \u2211 w wdw \u00d7 log( \u2211 t p(w|t)p(t|d)) . (7)\nWe can improve this likelihood by simply making documents more specific to fewer topics. For that our optimization algorithm simply finds, for each document, words assigned with some infrequent topics and reassigns the most significant topic in that document to these words.\n1. For each document d, we find the most significant topic ts with the smallest p-value, considering a null model where each word is independently sampled from topic t with probability p(t) =\n\u2211 w p(w)p(t|w). Calling x the number of words which actually come from topic t, (x = Ld \u00d7 p(t|d) , see Eq . (6) ) , the p-value of topic t is then computed using a binomial distribution, B(x;Ld, p(t)). Obviously p-value represents the significance of the word better than x, which only depends on the p(t|d).\n2. For each document d, recall that after the step 2 we may get some single words not in the network. We simply assign these words to the most significant topic ts and we can calculate a baseline of the PLSA-like likelihood L(see Eq .(7)).\n3. For each document d, we define the infrequent topics tin simply as those which occur with probability smaller than a parameter: p(tin|d) < \u03b7. We assign the most significant topic ts to the words which belong to the all infrequent topics tin. The p(ts|d) will be incremented by the sum of all p(tin|d), while all p(tin|d) are set to zero. Similarly, n(w, tin)(see above) will be decreased by wdw for each word w which belongs to an infrequent topic, and n(w, ts) is increased accordingly.\n4. After previous step for all document, we compute:\np(w|t) = n(w, t)\u2211 w n(w, t)\n(8)\nand the likelihood of model, L\u03b7 , where we made explicit its dependency on \u03b7. We pick the model with maximum L\u03b7 by looping over all possible values of \u03b7 (from 0% to 50% with steps of 1%).\nHLSM estimates the probabilities p(w|t) and p(t) = \u2211 w p(w)p(t|w) from training data set, and calculates p(t|w) = p(t)p(w|t)p(w) , for a new document from held out data set, p(w|t) won\u2019t be changed, p(t|d) can be calculated by :\np(t|d) = \u2211 w p(t|w) Ld\n(9)\nHLSM fixed the probabilities p(w|t) and p(t) after the training process, and hence is plagued by overfitting. It will be a shortcoming of the HLSM model, when the scale of the training data set is small."}, {"heading": "3 EXPERIMENTAL EVALUATIONS", "text": "HLSM is a topic model towards collections of text corpora. It can be applied to lots of applications such as classifying, clustering, filtering, information retrieval and related areas. Follow Blei\u2019s idea Blei & Jordan (2003), in this section, we investigate two important applications: document modeling and document classification."}, {"heading": "3.1 DOCUMENT MODELING", "text": "The goal of document modeling is to generalize the trained model from the training dataset to a new dataset. The documents in the corpora are unlabeled, our goal is density estimation, thus we wish to obtain high likelihood on a held-out test set. In particular, we computed the perplexity of a held-out test set to evaluate the models. Models which yield a lower perplexity are considered to achieve a better generalization performance because the model is less surprised by a portion of the datasets which the model have never seen before. Formally, for a test set of M documents, the perplexity is defined as:\nperplexity(Dtest) = exp { \u2212 \u2211M i=1 logp(di)\u2211M i=1 Li } (10)\nWe conduct this experiment on a subset of the 20Newsgroups data set, which has been widely used for evaluating the performance of cross-domain text classification algorithms. It contains nearly 20,000 newsgroup documents which have been evenly partitioned into 20 different newsgroups. We chose 3878 documents (we filtered some little documents) from domain comp.graphics, com.sys.mac.hardware, sci.crypt, and sci.med as our dataset used in the evaluation. We held out 20% of the corpus for test purpose and trained the models on the remaining 80%. In data preprocessing, we removed 163 stop words in standard list and the words occurrences less than 3 times\nfrom each corpus. We compare HLSM against PLSA, asymmetric LDA and TopicMapping. The initial \u03b1 for asymmetric LDA was set to 0.01 for all topics.\nFig. 3 shows the perplexity results where the number of the topics varies from 5 to 100. As can be seen, the HLSM model achieves slight improvement in terms of perplexity, while TopicMapping is close to asymmetric LDA. Experiment shows that the prior guess of HLSM makes great difference on the topic generation. Table 1 presents the examples of top 12 extracted topics on data set Comp and Sci, some topics with lower probability were not exhibited. We sorted the words with the learned topic-word probability. By examining the topical words, we can observe that the words in the same topic are always semantically relevant. For example, Topic 1 is about Mac hardware, and one domain in the data set Comp and Sci is comp.sys.mac.hardware, respectively. It is noteworthy that, some topics look similar in abstract level, but there are still some distinctions between them. For instance, words in Topic 2 and Topic 4 are semantically relevant but Topic 2 is more related to medical treatment, while Topic 4 probably describes some reports about disease. The result shows that our method can effectively identify the correlations between domain-specific features from different domains. Furthermore, our method can extracted narrow topics under the level of domain. And we conduct the next experiment on the whole 20Newsgroups data set."}, {"heading": "3.2 DOCUMENT CLASSIFICATION", "text": "In the text classification problem, topic models are wished to classify a document into two or more mutually exclusive classes. The choice of features is a challenging aspect of the document classification problem. By representing the documents in terms of latent topic space, the topic models can\ngenerate the probabilities p(t|d). If one use the vector of p(t|d) as the feature of documents to fix the text classification problem, the probabilities vector generated by the most effective model can perform better than the probabilities vector generated by other models.\nTo test the effectiveness of HLSM, we compared it with the following representative topic models and chose AC as the evaluation there. PLSA, symmetric LDA, asymmetric LDA, TopicMapping.\nWe generated six cross-domain text data sets from 20Newsgroups by utilizing its labeled structure. There are 4 fields in each data set, Table 3 summarizes the data sets generated from 20Newsgroups. To make the classification problem more effective and convincing, the task was defined as a multilabel classification.\nIn these experiments, we estimated the probabilities p(t|d) using the above topic models on all the documents of each data sets, and used the vector of probabilities p(t|d) as the only features to train a support vector machine (SVM) for multi-label classification. For each data set, 20% of the documents were held out as the test data and we trained a SVM for multi-label classification with the remaining 80% labeled documents. We used these classifiers to predict the class labels of unlabeled documents in the test data. Notice that there were 4 field in each data set, the classification process was considered as correct only if the document was classified into the original field.\nWe did the same data preprocessing as above, and the number of topics in each data set for LDA, PLSA, and asymmetric LDA was set to 4. Table 2 summarizes the classification performance on each data set, the first three row shows the best accuracy while the number of topics for LDA, PLSA, and asymmetric LDA varies. The last row of the table shows the average accuracy over all data sets. From the table we can observe that HLSM outperformed all other topic models on six data sets."}, {"heading": "4 CONCLUSION", "text": "A topic model HLSM is presented in this paper to apply an approach from the area of community detection to topic generation. We apply the HLSM model to several document collections for document modeling and document clustering, and the experimental comparisons against state-of- the-art approaches demonstrate the promising performance. In particular, in the area of community detec-\ntion, a substantial amount of work has been done on stochastic block models, which tries to fit a model to reveal community structure in networks. We believe this work, which is similar to topic model in spirit, would offer new insights into topic modeling."}], "references": [{"title": "Cross-domain text classification using semantic based approach", "author": ["B.U.A. Barathi"], "venue": "In Sustainable Energy and Intelligent Systems (SEISCON", "citeRegEx": "Barathi,? \\Q2011\\E", "shortCiteRegEx": "Barathi", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Probabilistic topic models", "author": ["D. Blei", "L. Carin", "D. Dunson"], "venue": "Signal Processing Magazine,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Dynamic topic models", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["Blei", "David M", "Griffiths", "Thomas L", "Jordan", "Michael I"], "venue": "J. ACM,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Using topic keyword clusters for automatic document clustering", "author": ["Chang", "Hsi-Cheng", "Hsu", "Chiun-Chieh"], "venue": "In Information Technology and Applications,", "citeRegEx": "Chang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2005}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, pp", "citeRegEx": "Hofmann,? \\Q1999\\E", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "High-reproducibility and high-accuracy method for automated topic classification", "author": ["Lancichinetti", "Andrea", "Sirer", "M. Irmak", "Wang", "Jane X", "Acuna", "Daniel", "K\u00f6rding", "Konrad", "Amaral", "Lu\u0131\u0301s A. Nunes"], "venue": "Phys. Rev. X,", "citeRegEx": "Lancichinetti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lancichinetti et al\\.", "year": 2015}, {"title": "Parallelized variational em for latent dirichlet allocation: An experimental evaluation of speed and scalability", "author": ["Nallapati", "Ramesh", "Cohen", "William", "J. Lafferty"], "venue": "In Data Mining Workshops,", "citeRegEx": "Nallapati et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2007}, {"title": "Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems", "author": ["Rosvall", "Martin", "Bergstrom", "Carl T"], "venue": "PLoS ONE, 6(4):e18209,", "citeRegEx": "Rosvall et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rosvall et al\\.", "year": 2011}, {"title": "Rethinking lda: Why priors matter", "author": ["Wallach", "Hanna M", "Mimno", "David M", "McCallum", "Andrew"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wallach et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 1981}], "referenceMentions": [{"referenceID": 7, "context": "LDA and PLSI both aim to estimate the values of these probabilities with the highest likelihood of generating the corpus (Hofmann, 1999; Blei & Jordan, 2003; Griffiths & Steyvers, 2004; Nallapati et al., 2007).", "startOffset": 121, "endOffset": 209}, {"referenceID": 9, "context": "LDA and PLSI both aim to estimate the values of these probabilities with the highest likelihood of generating the corpus (Hofmann, 1999; Blei & Jordan, 2003; Griffiths & Steyvers, 2004; Nallapati et al., 2007).", "startOffset": 121, "endOffset": 209}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications.", "startOffset": 80, "endOffset": 95}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications.", "startOffset": 80, "endOffset": 145}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications. There has been an exceptional amount of research on topic-model algorithms. PLSA and LDA are highly modular and can therefore be easily extended. Since LDA\u2019s introduction, there is much research based on it. The Correlated Topic Model Advances Blei & Lafferty (2006) follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet.", "startOffset": 80, "endOffset": 476}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications. There has been an exceptional amount of research on topic-model algorithms. PLSA and LDA are highly modular and can therefore be easily extended. Since LDA\u2019s introduction, there is much research based on it. The Correlated Topic Model Advances Blei & Lafferty (2006) follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA Blei et al. (2010b), where topics are joined together in a hierarchy by using the nested Chinese restaurant process.", "startOffset": 80, "endOffset": 677}, {"referenceID": 8, "context": "A research on the validity of LDA optimization algorithms for inferring topic models also proposed that current implementations of LDA had low validity (Lancichinetti et al., 2015).", "startOffset": 152, "endOffset": 180}, {"referenceID": 1, "context": "likelihood,as has been previously reported Blei et al. (2010a); Wallach et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 1, "context": "likelihood,as has been previously reported Blei et al. (2010a); Wallach et al. (2009). A research on the validity of LDA optimization algorithms for inferring topic models also proposed that current implementations of LDA had low validity (Lancichinetti et al.", "startOffset": 43, "endOffset": 86}], "year": 2015, "abstractText": "Much of information sits in an unprecedented amount of text data. Managing allocation of these large scale text data is an important problem for many areas. Topic modeling performs well in this problem. The traditional generative models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and most recent research on topic generation has been focusing on improving or extending these models. However, results of traditional generative models are sensitive to the number of topics K, which must be specified manually and determines the rank of solution space for topic generation. The problem of generating topics from corpus resembles community detection in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of the communities. Inspired by these algorithms, in this paper, we propose a novel method named Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each pair of words in the latent topic space, then constructs a unipartite network of words with this association and hierarchically generates topics from this network. We apply HLSM to several document collections and the experimental comparisons against several state-of-the-art approaches demonstrate the promising performance.", "creator": "LaTeX with hyperref package"}}}