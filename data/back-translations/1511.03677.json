{"id": "1511.03677", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Learning to Diagnose with LSTM Recurrent Neural Networks", "abstract": "Clinical medical data, especially in intensive care (ICU), consists of multivariate time series of observations; for each patient visit (or episode), sensor data and laboratory test results are recorded in the patient's electronic medical record (EHR). Although the data potentially contains a wealth of insights, they are difficult to effectively access due to varying length, irregular samples, and lack of data. Recursive neural networks (RNNNs), especially those that use Hidden Long-Term Memory (LSTM), are powerful and increasingly popular models for learning from sequence data. They skillfully model different length sequences and capture dependencies over long distances. We present the first study to empirically evaluate the ability of LSTMs to detect patterns in multivariate time series of clinical measurements. Specifically, we are looking at multi-label based diagnostic series and training models to classify 128 diagnoses that are empirically classified, but are frequently regulated.", "histories": [["v1", "Wed, 11 Nov 2015 21:01:28 GMT  (216kb,D)", "http://arxiv.org/abs/1511.03677v1", null], ["v2", "Fri, 13 Nov 2015 01:31:00 GMT  (216kb,D)", "http://arxiv.org/abs/1511.03677v2", null], ["v3", "Tue, 17 Nov 2015 11:22:34 GMT  (218kb,D)", "http://arxiv.org/abs/1511.03677v3", null], ["v4", "Fri, 20 Nov 2015 19:20:41 GMT  (839kb,D)", "http://arxiv.org/abs/1511.03677v4", null], ["v5", "Thu, 7 Jan 2016 09:29:14 GMT  (775kb,D)", "http://arxiv.org/abs/1511.03677v5", null], ["v6", "Tue, 1 Mar 2016 01:55:57 GMT  (775kb,D)", "http://arxiv.org/abs/1511.03677v6", null], ["v7", "Tue, 21 Mar 2017 21:29:50 GMT  (775kb,D)", "http://arxiv.org/abs/1511.03677v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zachary c lipton", "david c kale", "charles elkan", "randall wetzel"], "accepted": true, "id": "1511.03677"}, "pdf": {"name": "1511.03677.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zachary C. Lipton", "David C. Kale"], "emails": ["zlipton@cs.ucsd.edu", "dkale@usc.edu", "elkan@cs.ucsd.edu", "rwetzel@chla.usc.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Time series data comprised of clinical measurements, as recorded by caregivers in the pediatric intensive care unit (PICU), constitute an abundant and largely untapped source of medical insights. Potential uses of such data include classifying diagnoses accurately, predicting length of stay, predicting future illness, and predicting mortality. However, besides the difficulty of acquiring data, several obstacles stymie machine learning research with clinical time series. Episodes vary wildly in length, with stays ranging from just a few hours to multiple months. Observations, which include sensor data, vital signs, lab test results, and subjective assessments, are sampled irregularly, and are plagued by missing values (Marlin et al., 2012). Additionally, long-term time dependencies complicate learning with many algorithms. Lab results that taken together imply a particular diagnosis may be separated by days or weeks. Other examples of long-term dependencies include delays separating \u2217Author website: http://zacklipton.com \u2020Author website: http://www-scf.usc.edu/\u223cdkale/\nar X\niv :1\n51 1.\n03 67\n7v 1\n[ cs\n.L G\n] 1\n1 N\nonsets of diseases from the appearance of symptoms. For example, symptoms of acute respiratory distress syndrome may not appear until 24-48 hours after lung injury (Mason et al., 2010).\nRecurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsky\u0301, 2001; Xu et al., 2007). LSTMs can capture long range dependencies and nonlinear dynamics. Some sequence models, such as Markov models, conditional random fields, and Kalman filters, deal with sequential data but are ill-equipped to learn long-range dependencies. Other models require domain knowledge or feature engineering, offering less chance for serendipitous discovery. In contrast, neural networks learn representations and can discover unforeseen structure.\nThis paper presents the first empirical study of LSTMs applied to classifying diagnoses given multivariate PICU time series. Precisely, we formulate the problem as multilabel classification, since diagnoses are not mutually exclusive. Our examples are clinical episodes, each consisting of 13 frequently but irregularly sampled time series of clinical measurements, including body temperature, heart rate, diastolic and systolic blood pressure, and blood glucose among others. Associated with each patient are a subset of 429 diagnosis codes. As some are exceedingly rare, we focus on the 128 most common codes, classifying each episode with one or more diagnoses.\nBecause LSTMs have never been used in this setting, we methodically verify their utility. After first establishing set of strong baselines, we verify that even a simple LSTM architecture, trained to output predictions only at the final time step, beats a linear classifier when trained only on raw time series and nearly matches a MultiLayer Perceptron (MLP) trained on carefully hand-engineered features. We test a straightforward but novel target replication strategy for recurrent neural networks, inspired by the deep supervision technique of Lee et al. (2014) for training convolutional neural networks. We compose our optimization objective as a convex combination of the loss at the final sequence step and the mean of the losses over all sequence steps. Experiments demonstrate that target replication with a well-tuned hyper-parameter acts as a powerful regularizer, improving performance on all evaluation metrics. Finally, we evaluate the efficacy of using additional information in the patient\u2019s chart as auxiliary outputs, a technique previously used with feed-forward nets (Caruana et al., 1996). We show that this too regularizes the model and that in combination with target replication, it yields even higher performance on some metrics."}, {"heading": "2 RELATED WORK", "text": "Our research sits at the intersection of LSTMs, medical informatics, and multilabel classification, three well-developed fields, each with a long history and rich body of research. While we cannot possibly do justice to all three, we highlight the most relevant works below."}, {"heading": "2.1 LSTM RNNS", "text": "LSTMs were originally introduced in Hochreiter & Schmidhuber (1997), following a long line of research into RNNs for sequence learning. Notable earlier work includes Elman (1990), which first used back-propagation through time to train recurrent neural networks to perform supervised machine learning. The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al. (2013) explores other mechanisms by which a recurrent neural network could be made deep. Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al. (2015), which surveys natural language applications, and Lipton et al. (2015), which provides a broad overview of RNNs for sequence learning, focusing on modern applications."}, {"heading": "2.2 NEURAL NETWORKS FOR MEDICAL DATA", "text": "Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we know of no work on applying LSTMs to multivariate clinical time series of the type we analyze here. Several papers have applied RNNs to physiologic signals, including electrocardiograms (Silipo & Marchesi, 1998; Amari & Cichocki, 1998; U\u0308beyli, 2009) and glucose measurements (Tresp & Briegel, 1998). RNNs have also been used for prediction problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsky\u0301, 2001). Multiple recent papers apply modern deep learning techniques (but not RNNs) to modeling psychological conditions (Dabek & Caban, 2015), head injuries (Rughani et al., 2010), and Parkinson\u2019s disease (Hammerla et al., 2015). Recently, feed-forward networks have been applied to medical time series in sliding window fashion to discover meaningful patterns of physiology (Lasko et al., 2013; Che et al., 2015)."}, {"heading": "2.3 NEURAL NETWORKS FOR MULTILABEL CLASSIFICATION", "text": "Only a few published papers apply LSTMs to multilabel classification tasks, all of which, to our knowledge, are outside of the medical context. Liu et al. (2014) formulates music composition as a multilabel classification task, using sigmoidal output units. Most recently, Yeung et al. (2015) uses LSTM networks with multilabel outputs to recognize actions in videos. While we could not locate any published papers using LSTMs for multilabel classification in the medical domain, several papers use feed-forward nets for this task. One of the earliest papers to investigate multi-task neural networks modeled risk in pneumonia patients (Caruana et al., 1996). More recently, Che et al. (2015) formulated diagnosis as multilabel classification using a sliding window multilayer perceptron."}, {"heading": "2.4 MACHINE LEARNING FOR CLINICAL TIME SERIES", "text": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies. Quinn et al. (2009) used linear dynamical systems with latent switching variables to model physiologic events like bradycardias. Citing inspiration from deep learning, Stanculescu et al. (2015) proposed models with a second \u201clayer\u201d of latent factors to capture correlations between latent states."}, {"heading": "2.5 KEY DIFFERENCES", "text": "Our experiments show that LSTMs can successfully classify multivariate time series of clinical measurements, a topic not addressed in any prior work. Additionally, while some papers use LSTMs for multilabel classification, ours is the first to address this problem in the medical context. Moreover, for classifying varying length sequence with fixed length output vectors, this paper is the first, to our knowledge, to demonstrate the efficacy of a target replication strategy, achieving both faster training and better generalization."}, {"heading": "3 DATA DESCRIPTION", "text": "Our experiments use a collection of fully anonymized clinical time series extracted from the EHR system at Children\u2019s Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study. The data consists of 10, 401 PICU episodes, each a multivariate time series of 13 variables: diastolic and systolic blood pressure, peripheral capillary refill rate, end-tidal CO2, fraction of inspired O2, Glascow coma scale, blood glucose, heart rate, pH, respiratory rate, blood oxygen saturation, body temperature, and urine output. Episodes vary in length from 12 hours to 30 days.\nEach episode is associated with zero or more diagnostic labels from an in-house taxonomy used for research and billing, similar to codes from the Ninth Revision of the International Classification of Diseases (ICD-9) (World Health Organization, 2004). The dataset contains 429 distinct labels indicating a variety of conditions, such as acute respiratory distress, congestive heart failure, seizures, renal failure, and sepsis. Because many of the diagnoses are exceedingly rare, we focus on the most common 128, each of which occurs more than 50 times in the dataset.\nThe time series are irregularly sampled multivariate time series with both missing values and variables. We resample all time series to an hourly rate, taking the mean measurement within each one hour window. We use forward- and back-filling to fill gaps created by the window-based resampling. When a single variable\u2019s time series is missing entirely, we impute a clinically normal value, as defined by clinical experts. These procedures make reasonable assumptions about clinical practice: many variables are recorded at rates proportional to how quickly they change, and when a variable is absent, it is often because clinicians believed it to be normal and chose not to measure it. Nonetheless, they are not appropriate in all settings. Back-filling, for example, passes information from the future backwards. This is acceptable for classifying entire episodes but not for forecasting.\nWe rescale all variables to [0, 1] using ranges defined by clinical experts. We use published tables of normal values from large population studies to correct for differences in vital signs (e.g., blood pressure) that are due to age and gender (Fleming et al., 2011)(NHBPEP Working Group 2004)."}, {"heading": "4 METHODS", "text": "In this work, we are interested in recognizing diagnoses and, more broadly, the observable physiologic characteristics of patients, a task generally termed phenotyping (Oellrich et al., 2015). We cast the problem of phenotyping clinical time series as multilabel classification. Given a series of observations x(1), ...,x(T ), we learn a classifier to generate hypotheses y\u0302 of the true labels y. Here, t indexes sequence steps, and for any example, T stands for the length of the sequence. Our proposed LSTM RNN uses memory cells with forget gates (Gers et al., 2000b) but without peephole connections (Gers et al., 2003). As output, we use a fully connected layer atop the highest LSTM layer followed by an element-wise sigmoid activation function, because our problem is multilabel. We use log loss as the loss function at each output.\nThe following equations give the update for a layer of memory cells htl where h (t) l\u22121 stands for the previous layer at the same sequence step (could be a previous LSTM layer or the input layer x(t)) and h(t\u22121)l stands for the same layer at the previous sequence step:\ng(t) = \u03c6(Wgxh (t) l\u22121 +Wghh (t\u22121) l + bg)\ni(t) = \u03c3(Wixh (t) l\u22121 +Wihh (t\u22121) l + bi)\nf (t) = \u03c3(Wfxh (t) l\u22121 +Wfhh (t\u22121) l + bf )\no(t) = \u03c3(Woxh (t) l\u22121 +Wohh (t\u22121) l + bo)\ns(t) = g(t) i(i) + s(t\u22121) f (t))\nh (t) l = \u03c6(s (t)) o(t). In these equations, \u03c3 stands for an element-wise application of the sigmoid function, \u03c6 stands for an element-wise application of the tanh function and is the Hadamard (element-wise) product. The input, output, and forget gates are denoted by i, o, and f respectively, while g is the input node and has a tanh activation."}, {"heading": "4.1 LSTM ARCHITECTURES FOR MULTILABEL CLASSIFICATION", "text": "We explore several recurrent neural network architectures for multilabel classification of time series. The first and simplest (Figure 1) passes over all inputs in chronological order, generating outputs only at the final sequence step. In this approach, we only have output y\u0302 at the final sequence step, at\nwhich our loss function is the average of the losses at each output node. Thus the loss calculated at a single sequence step is the average of log loss calculated separately on each label.\nloss(y\u0302,y) = 1\n|L| l=|L|\u2211 l=1 \u2212(yl \u00b7 log(y\u0302l) + (1\u2212 yl) \u00b7 log(1\u2212 y\u0302l))."}, {"heading": "4.2 SEQUENTIAL TARGET REPLICATION", "text": "One problem with the simple approach is that the network must learn to pass information across many sequence steps in order to affect the output. We attack this problem by replicating our static targets at each sequence step (Figure 2), providing intermediate targets offering a local error signal. This approach is partly inspired by the deep supervision technique that Lee et al. (2014) apply to convolutional nets. This technique is especially sensible in our case because we expect the model to predict accurately even if the sequence were truncated by a small amount. The approach differs from Lee et al. (2014) because we use the same output weights to calculate y\u0302(t) for all t. Further, we use this target replication to generate output at each sequence step, but not at each hidden layer.\nFor the model with target replication, we generate an output y\u0302(t) at every sequence step. Our loss is then a convex combination of the final loss with an average of the losses over all steps:\n\u03b1 \u00b7 1 T T\u2211 t=1 loss(y\u0302(t),y(t)) + (1\u2212 \u03b1) \u00b7 loss(y\u0302(T ),y(T ))\nwhere T is the total number of sequence steps and \u03b1 \u2208 [0, 1] is a hyper-parameter which determines the relative importance of hitting these intermediary targets. At prediction time, we take only the output at the final step. In our experiments, networks using target replication outperformed those with a loss applied only on the final node in the sequence."}, {"heading": "5 JUNKOUT REGULARIZATION WITH AUXILIARY TARGETS", "text": "Recall that our initial data contained 429 diagnostic labels but that our task is to predict only 128. Given the well-documented successes of multitask learning with shared representations and feedforward networks, it seems plausible that we can train a stronger model by using the remaining 301 labels or other information, such as diagnostic categories, as auxiliary targets (Caruana et al., 1996). These additional targets serve as regularizers: the model aims to minimize the loss on the labels of interest but must also learn representations that minimize loss on the auxiliary targets (Figure 3). Given the lower quality of some of the extra labels, we name this model \u201cJunkOut.\u201d"}, {"heading": "6 EXPERIMENTS", "text": "We train the network using stochastic gradient descent with momentum. Absent momentum, the variance of the gradient is large, and single examples can destroy the model. Interestingly, the presence of exploding gradients had no apparent connection to the loss on the particular example that caused it. To combat exploding gradients, we experimented with `22 weight decay, gradient clipping, and truncated back-propagation. Additionally, we found that deep supervision with target replication not only helped to learn faster but also served as a regularizer. In our final network, we used `22 regularization strength of 10\n\u22126, deep supervision with \u03b1 = 0.6, hyperparameters which were chosen using validation data. Our final network used 2 hidden layers and 64 nodes per layer, an architecture also determined on the validation set. All models are trained on 80% of the data and tested on 10%. The remaining 10% is used as a validation set."}, {"heading": "6.1 MULTILABEL EVALUATION METHODOLOGY", "text": "We report micro- and macro-averaged versions area under the ROC curve (AUC). By micro-AUC we mean a single AUC computed on flattened Y\u0302 and Y matrices, whereas macro-AUC is an average of the AUC computed separately for each label. The blind classifier achieves 0.5 macro-AUC but can exceed .5 on micro AUC by predicting labels in descending order by base rate. Additionally, we report micro- and macro- averaged F1 score. Similarly, micro is reported on flattened matrices and macro- is an average over the labels. F1 metrics require a thresholding strategy, and here we select thresholds based upon validation set performance. We refer to Lipton et al. (2014) for an analysis of the strengths and weaknesses of each multilabel F-score and a characterization of optimal thresholds.\nFinally, we report precision at 10, which captures the fraction of true diagnoses among the model\u2019s top 10 predictions, with a best possible score of 0.2281 on the test split of this data set because there are only 2.281 diagnoses per patient on average. While F1 and AUC are both useful for determining the relative quality of a classifier\u2019s predictions, neither is tailored to a real-world application. Thus, we consider a medically plausible use case to motivate this more interpretable metric: generating a short list of the 10 most probable diagnoses. If we could create a high recall, moderate precision list\nof 10 likely diagnoses, it could be a valuable hint-generation tool. Further, testing for only the 10 most probable conditions is much more realistic than testing for all conditions."}, {"heading": "6.2 BASELINE CLASSIFIERS", "text": "We provide results for a base rate model that predicts diagnoses in descending order by incidence to provide a minimum performance baseline. We also report the performance of logistic regression, which is widely used in clinical research, training a separate classifier for each diagnosis and choosing L2 regularization penalty on a validation set. For a much stronger baseline, we train an MLP with rectified linear activations, dropout of 0.5, and 3 hidden layers and 300 hidden nodes, hyperparameters chosen based on validation set performance. Each baseline is tested with two sets of inputs: raw time series and hand-engineered features. For raw time series, we used the first and last six hours. This provided classifiers with temporal information about changes in patient state from admission to discharge within a fixed-size input, as required by all baselines. We found this worked better than providing the first or last 12 hours. Our hand-engineered features are inspired by those used in state-of-the-art severity of illness scores (Pollack et al., 1996) and capture extremes (e.g., maximum), central tendencies (e.g., mean), variability, and trends. Such features have previously been shown to be very effective for these data (Marlin et al., 2012; Che et al., 2015)."}, {"heading": "6.3 RESULTS", "text": "LSTM models with two layers of 64 hidden units and trained with deep supervision (DS) performed best among models using only raw time series as inputs. Table 1 shows a summary results for all models. All three DS models achieve Micro and Macro AUCs of over 0.84 and 0.78, respectively. We believe that there is further room for improvement, especially given access to a richer data set including a larger number of variables and treatment information. Table 2 LSTM\u2019s predictive performance for five critical care diagnoses with the highest F1 scores. Full per-diagnosis results may be found in Appendix A.\nDeep supervision yielded substantial improvements over simple LSTMs on all metrics. It accelerated learning and allowed the model to continue improving for a larger number of epochs. Computing classification error at each step provides local targets to guide training, which may aid the LSTM-DS in modeling long-range interactions indicating changes in patient state. The performance of the MLP using the first and last six hours suggests the importance such interactions.\nUsing additional targets (\u201cJunkOut\u201d) improved performance for most metrics, and reduced overfitting, but the effect was less dramatic than for deep supervision. Additionally, these gains came at the cost of slower training: the JunkOut models required more epochs, especially when using the 300 auxiliary labels. This may be due in part to class imbalance. For many of these labels it may take an entire epoch just to learn that they are occasionally nonzero.\nThe best performing model in our experiments was an MLP using hand-engineered features as input. This is not surprising as this model is able to directly exploit domain knowledge and features like\nvariability and trends that the LSTMs must learn. The LSTMs with deep supervision surpass the MLP on all metrics when both are trained on raw features."}, {"heading": "7 DISCUSSION", "text": "Our results indicate that LSTM RNNs, especially when provided with deep supervision in the form of target replication, can successfully classify diagnoses of critical care patients given clinical time series data. Our experiments demonstrate a clear advantage over all linear baselines and over traditional feedforward architectures applied to raw measurements. However, this is only a first step in this line of research. Recognizing current diagnoses given only sensor data demonstrates that LSTMs can capture meaningful signal, but ultimately we would like to accurately predict unknown conditions and events, including outcomes such as mortality and treatment responses. In this work we used diagnostic labels without timestamps, but we are working to obtain time-stamped diagnoses, which would enable us to train models to perform early diagnosis by predicting future conditions. On the strength of these results, we are currently extending this work to a larger PICU data set with a richer set of measurements, including treatments and medications.\nOn the methodological side, we would like to both improve and better exploit the capabilities of LSTMs, Results from speech recognition have shown that LSTMs shine in comparison to other models using raw features. In the clinical setting, LSTMs may allow us to exploit previously difficult to mine sources of data while minimizing the preprocessing and feature engineering required. In contrast, our current data preparation pipeline removes valuable structure and information from clinical time series that could be exploited by a LSTM. For example, our forward- and back-filling imputation strategies discard useful information about which observations were recorded when. Imputing normal values for missing time series ignores the meaningful distinction between truly normal and missing measurements. In future work, we plan to introduce indicator variables to allow the LSTM to distinguish actual from missing or imputed measurements. Also, our window-based resampling procedure reduces the variability of more frequently measured vital signs (e.g., heart rate). The flexibility of the LSTM architecture should also enable us to eliminate age-based corrections and to incorporate non-sequential inputs, such as age, weight, and height (or even hand-engineered features), into predictions.\nNext steps in this direction include developing LSTM architectures to directly handle missing values and irregular sampling. We also are encouraged by the success of target replication and plan to explore other variants of this technique and to apply it to other domains and tasks. Additionally, we acknowledge that there remains a debate about the interpretability of neural networks when applied to complex medical problems. We are developing methods to interpret the representations learned by LSTMs in order to better expose patterns of health and illness to clinical users. We also hope to make practical use of the distributed representations of patients for patient similarity search."}, {"heading": "8 ACKNOWLEDGEMENTS", "text": "Zachary C. Lipton was supported by the Division of Biomedical Informatics at the University of California, San Diego, via training grant (T15LM011271) from the NIH/NLM. David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship. The VPICU was supported by grants from the Laura P. and Leland K. Whittier Foundation. We acknowledge NVIDIA Corporation for Tesla K40 GPU hardware donation and Professors Julian McAuley and Greg Ver Steeg for their support and advice."}, {"heading": "A PER DIAGNOSIS RESULTS", "text": "(continued) Classifier Performance on Each Diagnostic Code Sorted by AUC Label AUC AUPRC F1 Precision Recall Pneumonia 0.8139 0.0876 0.1463 0.1 0.2727 Injury to unspecifed intra-abdominal organs 0.8101 0.1986 0.0408 0.0212 0.5 Septicemia NOS 0.8097 0.1589 0.1686 0.1076 0.3888 Respriatory complications resulting from a procedure 0.8068 0.2027 0.2388 0.25 0.2285 Cystic Fibrosis with pulmonary mainfestations 0.8059 0.1197 0.0320 0.0164 0.6 Acute Respiratory Failure 0.8057 0.3235 0.4428 0.3604 0.5740 Encephalopathy NOS 0.7991 0.0410 0.1428 0.25 0.1 Arteriousvenous malformation of the brain 0.7984 0.1900 0.2448 0.24 0.25 Apnea 0.7978 0.1033 0.1111 0.1333 0.0952 Malignant neoplasm 0.7902 0.0274 0.0361 0.0192 0.3 Disorder of Muscle NEC 0.7894 0.1847 0.0347 0.0178 0.6363 Other Shock 0.7889 0.0257 0.0545 0.03 0.3 Diabetes mellitus 0.7861 0.0775 0.1333 0.125 0.1428 Motor vehicle accident involving collision w/ another motor vehicle 0.7853 0.0864 0.1506 0.0956 0.3548 Less than 37 weeks gestation 0.7847 0.1304 0.1981 0.1594 0.2619 Conditions due to anomaly of unspecified circumstance 0.7823 0.2163 0.25 0.4285 0.1764 Other respiratory symtpom 0.7814 0.2565 0.3333 0.2807 0.4102 Supraventricular premature beats 0.7813 0.0455 0.0952 0.0714 0.1428 Anomalies of the skull and face bone 0.7797 0.0617 0.1428 0.125 0.1666 Cardiac dysrhythmia 0.7795 0.1699 0.125 0.1428 0.1111 Other diseases of the respiratory system 0.7792 0.1467 0.2020 0.2127 0.1923 Obesity 0.7777 0.0418 0.0661 0.0366 0.3333 Hematemesis 0.7761 0.0129 0.0191 0.0097 0.6 Pneumonitis due to inhalation of food or vomitus 0.7756 0.0222 0.0476 0.0256 0.3333 Congenital hydrocephalus 0.7752 0.1847 0.2432 0.3 0.2045 Fracture 0.7739 0.1425 0.0740 0.0625 0.0909 Other primary cardiomyopathies 0.7652 0.2835 0.2666 0.2727 0.2608 Hyposmolality and/or hyponatremia 0.7638 0.0104 0.0176 0.0090 0.4 Other and unspecified coagulation defects 0.7635 0.1154 0.1086 0.0657 0.3125 null 0.7615 0.0389 0.0701 0.0384 0.4 Epilepsy 0.7584 0.3188 0.3873 0.3282 0.4725 Mechanical complication of V-P shunt 0.7580 0.0203 0.1 0.0833 0.125 Delayed milestones 0.7564 0.3421 0.3944 0.3467 0.4574 Esophageal reflux 0.7545 0.0585 0.1234 0.0833 0.2380 Croup Syndrome 0.7538 0.0403 0.0165 0.0083 0.7142 Heart Disease 0.7526 0.0426 0.0769 0.0666 0.0909 Subdural hemorrage following injury w/out open wound 0.7477 0.0553 0.0810 0.0535 0.1666 Malignant neoplasm 0.7431 0.0101 0.0152 0.0078 0.3333 History of surgery to major organs 0.7429 0.0474 0.0454 0.0270 0.1428 Congenital Central Alveolar Hypoventilation Syndrome 0.7400 0.0127 0.0095 0.0049 0.1428 Acute and subacute necrosis of the liver 0.7383 0.1321 0.1904 0.1666 0.2222 Anemia 0.7321 0.0388 0.0327 0.0173 0.2857 Dehydration 0.7319 0.0360 0.0888 0.0588 0.1818 Unspecified disease of spinal cord 0.7291 0.0614 0.0720 0.0421 0.25 Primary Malignant neoplasm 0.7274 0.0170 0.0270 0.0141 0.3 Urinary Tract Infection 0.7259 0.0491 0.0941 0.0579 0.25 Malignancy of bone - no site specified 0.7184 0.0180 0.0833 0.0487 0.2857 Drowning and non-fatal submersion 0.7174 0.0146 0.0272 0.0142 0.2857 Observation of newborn and infant for supsected condition not found 0.7169 0.0347 0.0683 0.04 0.2352 Insomnia with sleep apnea 0.7108 0.2015 0.0961 0.0574 0.2941 Bone Marrow Transplant Status 0.7077 0.0770 0.0983 0.0588 0.3 Thrombocytopenia 0.7060 0.0209 0.0512 0.0280 0.3 Unspecified Intestinal obstruction 0.7045 0.0495 0.0638 0.0379 0.2 Unspecified injury 0.7020 0.0333 0.0 0.0 0.0 Unspecified disorder of intestine 0.6961 0.0550 0.075 0.0468 0.1875 Blood in stool 0.6908 0.0089 0.0245 0.0127 0.3333 Respiratory Arrest 0.6883 0.1622 0.1052 0.0833 0.1428 Malignant neoplasm 0.6790 0.0590 0.0486 0.0254 0.5333 Panhypopituitarism 0.6784 0.0146 0.0242 0.0127 0.25 Other complications of nervous sytem device 0.6760 0.0132 0.0289 0.0153 0.25 Ostium secundum type atrial septal defect 0.6750 0.0218 0.0437 0.0234 0.3333 Hereditary hemolytic anemia 0.6733 0.0115 0.0262 0.0135 0.4285\n(continued) Classifier Performance on Each Diagnostic Code Sorted by AUC Label AUC AUPRC F1 Precision Recall Sickle-cell anemia 0.6712 0.0093 0.0263 0.0134 0.6 Ventricular septal defect 0.6580 0.0170 0.0444 0.025 0.2 Other specified cardiac dysrhytmia 0.6554 0.0483 0.0168 0.0086 0.3333 Conditions due to anomaly of unspecified circumstance 0.6537 0.0991 0.1637 0.1031 0.3965 Injury crushing NOS 0.6445 0.0085 0.0191 0.0098 0.4 Respiratory complications resulting from a procedure 0.6385 0.0275 0.0291 0.0157 0.2 Hemophilus meningitis 0.6240 0.0080 0.0135 0.0068 0.3333 HEMATOLOGIC - Other 0.6078 0.0212 0.0394 0.0215 0.2352 Unspecified disorder of thyroid 0.6068 0.0182 0.0151 0.0077 0.375 Disorders Of The Nervous System 0.5847 0.0315 0.0553 0.0290 0.5652 Unspecified disorder of immune mechanism 0.5658 0.0205 0.0377 0.0208 0.2 Unspecified disorder of metabolism 0.5552 0.0313 0.0555 0.0454 0.0714 GI SURGERY - Other 0.5412 0.0152 0.0241 0.0129 0.1875 Asthma NOS 0.5398 0.0168 0.0335 0.0175 0.4117 Tetralogy of Fallot 0.4325 0.0058 0.0082 0.0042 0.2857"}], "references": [{"title": "Probabilistic detection of short events, with application to critical care monitoring", "author": ["Aleks", "Norm", "Russell", "Stuart J", "Madden", "Michael G", "Morabito", "Diane", "Staudenmayer", "Kristan", "Cohen", "Mitchell", "Manley", "Geoffrey T"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Aleks et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aleks et al\\.", "year": 2009}, {"title": "Adaptive blind signal processing-neural network approaches", "author": ["Amari", "Shun-ichi", "Cichocki", "Andrzej"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Amari et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Amari et al\\.", "year": 1998}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli", "Michael", "Galley", "Michel", "Quirk", "Chris", "Zweig", "Geoffrey"], "venue": "In EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Application of artificial neural networks to clinical medicine", "author": ["W.G. Baxt"], "venue": "The Lancet,", "citeRegEx": "Baxt,? \\Q1995\\E", "shortCiteRegEx": "Baxt", "year": 1995}, {"title": "Using the future to \u201csort out\u201d the present: Rankprop and multitask learning for medical risk evaluation", "author": ["Caruana", "Rich", "Baluja", "Shumeet", "Mitchell", "Tom"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Caruana et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 1996}, {"title": "Deep computational phenotyping", "author": ["Che", "Zhengping", "Kale", "David C", "Li", "Wenzhe", "Bahadori", "Mohammad Taha", "Liu", "Yan"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Che et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Che et al\\.", "year": 2015}, {"title": "A neural network based model for predicting psychological conditions", "author": ["Dabek", "Filip", "Caban", "Jesus J"], "venue": "In Brain Informatics and Health,", "citeRegEx": "Dabek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dabek et al\\.", "year": 2015}, {"title": "A survey on the application of recurrent neural networks to statistical language modeling", "author": ["De Mulder", "Wim", "Bethard", "Steven", "Moens", "Marie-Francine"], "venue": "Computer Speech & Language,", "citeRegEx": "Mulder et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mulder et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Normal ranges of heart rate and respiratory rate in children from birth to 18 years: A systematic review of observational studies", "author": ["Fleming", "Susannah", "Thompson", "Matthew", "Stevens", "Richard", "Heneghan", "Carl", "Plddemann", "Annette", "Maconochie", "Ian", "Tarassenko", "Lionel", "Mant", "David"], "venue": null, "citeRegEx": "Fleming et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fleming et al\\.", "year": 2011}, {"title": "Recurrent nets that time and count", "author": ["Gers", "Felix", "Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "A multivariate timeseries modeling approach to severity of illness assessment and forecasting in ICU with sparse, heterogeneous clinical data", "author": ["Ghassemi", "Marzyeh", "Pimentel", "Marco AF", "Naumann", "Tristan", "Brennan", "Thomas", "Clifton", "David A", "Szolovits", "Peter", "Feng", "Mengling"], "venue": "In Proc. Twenty-Ninth AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Ghassemi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Fern\u00e1ndez", "Santiago", "Bertolami", "Roman", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Pd disease state assessment in naturalistic environments using deep learning", "author": ["Hammerla", "Nils Y", "Fisher", "James M", "Andras", "Peter", "Rochester", "Lynn", "Walker", "Richard", "Pl\u00f6tz", "Thomas"], "venue": "In Proc. Twenty-Ninth AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Hammerla et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hammerla et al\\.", "year": 2015}, {"title": "A targeted real-time early warning score (trewscore) for septic shock", "author": ["Henry", "Katharine E", "Hager", "David N", "Pronovost", "Peter J", "Saria", "Suchi"], "venue": "Science Translational Medicine, 7(299):299ra122\u2013 299ra122,", "citeRegEx": "Henry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henry et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data", "author": ["Lasko", "Thomas A", "Denny", "Joshua C", "Levy", "Mia A"], "venue": "PLoS ONE,", "citeRegEx": "Lasko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasko et al\\.", "year": 2013}, {"title": "Optimal thresholding of classifiers to maximize f1 measure", "author": ["Lipton", "Zachary C", "Elkan", "Charles", "Naryanaswamy", "Balakrishnan"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Lipton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2014}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Lipton", "Zachary C", "Berkowitz", "John", "Elkan", "Charles"], "venue": "arXiv preprint arXiv:1506.00019,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Bach in 2014: Music composition with recurrent neural network", "author": ["I Liu", "Ramakrishnan", "Bhiksha"], "venue": "arXiv preprint arXiv:1412.3191,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "A novel approach to online handwriting recognition based on bidirectional long short-term memory networks", "author": ["Liwicki", "Marcus", "Graves", "Alex", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proc. 9th Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "Liwicki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liwicki et al\\.", "year": 2007}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering models", "author": ["Marlin", "Ben M", "Kale", "David C", "Khemani", "Robinder G", "Wetzel", "Randall C"], "venue": "IHI,", "citeRegEx": "Marlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2012}, {"title": "The digital revolution in phenotyping", "author": ["Oellrich", "Anika", "Collier", "Nigel", "Groza", "Tudor", "Rebholz-Schuhmann", "Dietrich", "Shah", "Nigam", "Bodenreider", "Olivier", "Boland", "Mary Regina", "Georgiev", "Ivo", "Liu", "Hongfang", "Livingston", "Kevin", "Luna", "Augustin", "Mallon", "Ann-Marie", "Manda", "Prashanti", "Robinson", "Peter N", "Rustici", "Gabriella", "Simon", "Michelle", "Wang", "Liqin", "Winnenburg", "Rainer", "Dumontier", "Michel"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "Oellrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oellrich et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "PRISM III: an updated Pediatric Risk of Mortality score", "author": ["M.M. Pollack", "K.M. Patel", "U.E. Ruttimann"], "venue": "Critical Care Medicine,", "citeRegEx": "Pollack et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Pollack et al\\.", "year": 1996}, {"title": "Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles", "author": ["Pollastri", "Gianluca", "Przybylski", "Darisz", "Rost", "Burkhard", "Baldi", "Pierre"], "venue": "Proteins: Structure, Function, and Bioinformatics,", "citeRegEx": "Pollastri et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pollastri et al\\.", "year": 2002}, {"title": "Factorial switching linear dynamical systems applied to physiological condition monitoring", "author": ["Quinn", "John", "Williams", "Christopher KI", "McIntosh", "Neil"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Quinn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quinn et al\\.", "year": 2009}, {"title": "Use of an artificial neural network to predict head injury", "author": ["Rughani", "Anand I", "Dumont", "Travis M", "Lu", "Zhenyu", "Bongard", "Josh", "Horgan", "Michael A", "Penar", "Paul L", "Tranmer", "Bruce I"], "venue": "outcome: clinical article. Journal of neurosurgery,", "citeRegEx": "Rughani et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rughani et al\\.", "year": 2010}, {"title": "Learning individual and population level traits from clinical temporal data", "author": ["Saria", "Suchi", "Koller", "Daphne", "Penn", "Anna"], "venue": "In Proc. Neural Information Processing Systems (NIPS), Predictive Models in Personalized Medicine workshop. Citeseer,", "citeRegEx": "Saria et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saria et al\\.", "year": 2010}, {"title": "Clustering longitudinal clinical marker trajectories from electronic health data: Applications to phenotyping and endotype discovery", "author": ["Schulam", "Peter", "Wigley", "Fredrick", "Saria", "Suchi"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Schulam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulam et al\\.", "year": 2015}, {"title": "Artificial neural networks for automatic ecg analysis", "author": ["Silipo", "Rosaria", "Marchesi", "Carlo"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Silipo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Silipo et al\\.", "year": 1998}, {"title": "Autoregressive hidden markov models for the early detection of neonatal sepsis", "author": ["Stanculescu", "Ioan", "Williams", "Christopher K", "Freer", "Yvonne"], "venue": "Biomedical and Health Informatics, IEEE Journal of,", "citeRegEx": "Stanculescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stanculescu et al\\.", "year": 2014}, {"title": "A hierarchical switching linear dynamical system applied to the detection of sepsis in neonatal condition monitoring", "author": ["Stanculescu", "Ioan", "Williams", "Christopher KI", "Freer", "Yvonne"], "venue": null, "citeRegEx": "Stanculescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stanculescu et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A solution for missing data in recurrent neural networks with an application to blood glucose prediction", "author": ["Tresp", "Volker", "Briegel", "Thomas"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Tresp et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Tresp et al\\.", "year": 1998}, {"title": "Combining recurrent neural networks with eigenvector methods for classification of ecg beats", "author": ["\u00dcbeyli", "Elif Derya"], "venue": "Digital Signal Processing,", "citeRegEx": "\u00dcbeyli and Derya.,? \\Q2009\\E", "shortCiteRegEx": "\u00dcbeyli and Derya.", "year": 2009}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Neural network model of gene expression", "author": ["Vohradsk\u00fd", "Ji\u0159\u0131"], "venue": "The FASEB Journal,", "citeRegEx": "Vohradsk\u00fd and Ji\u0159\u0131\u0301.,? \\Q2001\\E", "shortCiteRegEx": "Vohradsk\u00fd and Ji\u0159\u0131\u0301.", "year": 2001}, {"title": "Inference of genetic regulatory networks with recurrent neural network models using particle swarm optimization", "author": ["Xu", "Rui", "Wunsch II", "Donald", "Frank", "Ronald"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Every moment counts: Dense detailed labeling of actions in complex videos", "author": ["Yeung", "Serena", "Russakovsky", "Olga", "Jin", "Ning", "Andriluka", "Mykhaylo", "Mori", "Greg", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1507.05738,", "citeRegEx": "Yeung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yeung et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Observations, which include sensor data, vital signs, lab test results, and subjective assessments, are sampled irregularly, and are plagued by missing values (Marlin et al., 2012).", "startOffset": 159, "endOffset": 180}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 38, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 41, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 25, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 16, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 30, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 43, "context": "Recurrent Neural Networks (RNNs), in particular those based on Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), powerfully model varying-length sequential data, achieving state-of-the-art results for problems spanning natural language processing, image captioning, handwriting recognition, and genomic analysis (Auli et al., 2013; Sutskever et al., 2014; Vinyals et al., 2014; Karpathy & Fei-Fei, 2014; Liwicki et al., 2007; Graves et al., 2009; Pollastri et al., 2002; Vohradsk\u00fd, 2001; Xu et al., 2007).", "startOffset": 326, "endOffset": 518}, {"referenceID": 4, "context": "Finally, we evaluate the efficacy of using additional information in the patient\u2019s chart as auxiliary outputs, a technique previously used with feed-forward nets (Caruana et al., 1996).", "startOffset": 162, "endOffset": 184}, {"referenceID": 9, "context": "The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al.", "startOffset": 125, "endOffset": 311}, {"referenceID": 9, "context": "The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al. (2013) explores other mechanisms by which a recurrent neural network could be made deep.", "startOffset": 125, "endOffset": 334}, {"referenceID": 9, "context": "The design of modern LSTM memory cells has remained close to the original, with the commonly used additions of forget gates (Gers et al., 2000b) and peep-hole connections (Gers et al., 2000a). The connectivity pattern among multiple LSTM layers in our models follows the architecture described by Graves (2013). Pascanu et al. (2013) explores other mechanisms by which a recurrent neural network could be made deep. Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al.", "startOffset": 125, "endOffset": 464}, {"referenceID": 7, "context": "Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al. (2015), which surveys natural language applications, and Lipton et al.", "startOffset": 108, "endOffset": 129}, {"referenceID": 7, "context": "Surveys of the literature include Graves (2012), a thorough dissertation on sequence labeling with RNNs, De Mulder et al. (2015), which surveys natural language applications, and Lipton et al. (2015), which provides a broad overview of RNNs for sequence learning, focusing on modern applications.", "startOffset": 108, "endOffset": 200}, {"referenceID": 4, "context": "Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we know of no work on applying LSTMs to multivariate clinical time series of the type we analyze here.", "startOffset": 85, "endOffset": 119}, {"referenceID": 3, "context": "Neural networks have been applied to medical problems and data for at least 20 years (Caruana et al., 1996; Baxt, 1995), although we know of no work on applying LSTMs to multivariate clinical time series of the type we analyze here.", "startOffset": 85, "endOffset": 119}, {"referenceID": 30, "context": "RNNs have also been used for prediction problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsk\u00fd, 2001).", "startOffset": 61, "endOffset": 119}, {"referenceID": 43, "context": "RNNs have also been used for prediction problems in genomics (Pollastri et al., 2002; Xu et al., 2007; Vohradsk\u00fd, 2001).", "startOffset": 61, "endOffset": 119}, {"referenceID": 32, "context": "Multiple recent papers apply modern deep learning techniques (but not RNNs) to modeling psychological conditions (Dabek & Caban, 2015), head injuries (Rughani et al., 2010), and Parkinson\u2019s disease (Hammerla et al.", "startOffset": 150, "endOffset": 172}, {"referenceID": 17, "context": ", 2010), and Parkinson\u2019s disease (Hammerla et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 21, "context": "Recently, feed-forward networks have been applied to medical time series in sliding window fashion to discover meaningful patterns of physiology (Lasko et al., 2013; Che et al., 2015).", "startOffset": 145, "endOffset": 183}, {"referenceID": 5, "context": "Recently, feed-forward networks have been applied to medical time series in sliding window fashion to discover meaningful patterns of physiology (Lasko et al., 2013; Che et al., 2015).", "startOffset": 145, "endOffset": 183}, {"referenceID": 4, "context": "One of the earliest papers to investigate multi-task neural networks modeled risk in pneumonia patients (Caruana et al., 1996).", "startOffset": 104, "endOffset": 126}, {"referenceID": 22, "context": "Liu et al. (2014) formulates music composition as a multilabel classification task, using sigmoidal output units.", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "Liu et al. (2014) formulates music composition as a multilabel classification task, using sigmoidal output units. Most recently, Yeung et al. (2015) uses LSTM networks with multilabel outputs to recognize actions in videos.", "startOffset": 0, "endOffset": 149}, {"referenceID": 4, "context": "One of the earliest papers to investigate multi-task neural networks modeled risk in pneumonia patients (Caruana et al., 1996). More recently, Che et al. (2015) formulated diagnosis as multilabel classification using a sliding window multilayer perceptron.", "startOffset": 105, "endOffset": 161}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al.", "startOffset": 149, "endOffset": 189}, {"referenceID": 31, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al.", "startOffset": 149, "endOffset": 189}, {"referenceID": 36, "context": ", 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al.", "startOffset": 40, "endOffset": 86}, {"referenceID": 18, "context": ", 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al.", "startOffset": 40, "endOffset": 86}, {"referenceID": 26, "context": ", 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015).", "startOffset": 38, "endOffset": 81}, {"referenceID": 34, "context": ", 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015).", "startOffset": 38, "endOffset": 81}, {"referenceID": 26, "context": "Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015).", "startOffset": 216, "endOffset": 260}, {"referenceID": 13, "context": "Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015).", "startOffset": 216, "endOffset": 260}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies.", "startOffset": 150, "endOffset": 796}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies. Quinn et al. (2009) used linear dynamical systems with latent switching variables to model physiologic events like bradycardias.", "startOffset": 150, "endOffset": 965}, {"referenceID": 0, "context": "Neural network methodology aside, a growing body of research applies machine learning to temporal clinical data for tasks including artifact removal (Aleks et al., 2009; Quinn et al., 2009), early detection and prediction (Stanculescu et al., 2014; Henry et al., 2015), and clustering and subtyping (Marlin et al., 2012; Schulam et al., 2015). Many recent papers use models with latent factors to capture nonlinear dynamics in clinical time series and to discover meaningful representations of health and illness. Gaussian processes and related techniques have proved popular because they can directly handle irregular sampling and encode prior knowledge via choice of covariance functions between time steps and across variables (Marlin et al., 2012; Ghassemi et al., 2015). Saria et al. (2010) combined a hierarchical dirichlet process with autoregressive models to infer latent disease \u201ctopics\u201d in the heart rate signals of premature babies. Quinn et al. (2009) used linear dynamical systems with latent switching variables to model physiologic events like bradycardias. Citing inspiration from deep learning, Stanculescu et al. (2015) proposed models with a second \u201clayer\u201d of latent factors to capture correlations between latent states.", "startOffset": 150, "endOffset": 1139}, {"referenceID": 26, "context": "Our experiments use a collection of fully anonymized clinical time series extracted from the EHR system at Children\u2019s Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study.", "startOffset": 130, "endOffset": 169}, {"referenceID": 5, "context": "Our experiments use a collection of fully anonymized clinical time series extracted from the EHR system at Children\u2019s Hospital LA (Marlin et al., 2012; Che et al., 2015) as part of an IRB-approved study.", "startOffset": 130, "endOffset": 169}, {"referenceID": 9, "context": ", blood pressure) that are due to age and gender (Fleming et al., 2011)(NHBPEP Working Group 2004).", "startOffset": 49, "endOffset": 71}, {"referenceID": 27, "context": "In this work, we are interested in recognizing diagnoses and, more broadly, the observable physiologic characteristics of patients, a task generally termed phenotyping (Oellrich et al., 2015).", "startOffset": 168, "endOffset": 191}, {"referenceID": 12, "context": ", 2000b) but without peephole connections (Gers et al., 2003).", "startOffset": 42, "endOffset": 61}, {"referenceID": 4, "context": "Given the well-documented successes of multitask learning with shared representations and feedforward networks, it seems plausible that we can train a stronger model by using the remaining 301 labels or other information, such as diagnostic categories, as auxiliary targets (Caruana et al., 1996).", "startOffset": 274, "endOffset": 296}, {"referenceID": 22, "context": "We refer to Lipton et al. (2014) for an analysis of the strengths and weaknesses of each multilabel F-score and a characterization of optimal thresholds.", "startOffset": 12, "endOffset": 33}, {"referenceID": 29, "context": "Our hand-engineered features are inspired by those used in state-of-the-art severity of illness scores (Pollack et al., 1996) and capture extremes (e.", "startOffset": 103, "endOffset": 125}, {"referenceID": 26, "context": "Such features have previously been shown to be very effective for these data (Marlin et al., 2012; Che et al., 2015).", "startOffset": 77, "endOffset": 116}, {"referenceID": 5, "context": "Such features have previously been shown to be very effective for these data (Marlin et al., 2012; Che et al., 2015).", "startOffset": 77, "endOffset": 116}], "year": 2017, "abstractText": "Clinical medical data, especially in the intensive care unit (ICU), consists of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient\u2019s Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They adeptly model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective deep supervision strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperforms several strong baselines on a wide variety of metrics, and nearly matches the performance of a multilayer perceptron trained on carefully hand-engineered features, establishing the usefulness of LSTMs for modeling medical data. The best LSTM model accurately classifies many diagnoses, including diabetic ketoacidosis (F1 score of .714), scoliosis (.677), and status asthmaticus (.632).", "creator": "LaTeX with hyperref package"}}}