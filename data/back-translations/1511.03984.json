{"id": "1511.03984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester Using Artificial Neural Networks and Support Vector Machine", "abstract": "3\\ b {eta} -O phthalic acid esters of betulinic acid are of great importance in cancer studies. However, the optimization of their reaction conditions requires a large number of experimental work. To simplify the number of optimization times in experimental work, we use artificial neural network (ANN) and support vector (SVM) models to predict yields of 3\\ b {eta} -O phthalic acid esters synthesized by betulinic acid and phthalic acid anhydride, using lipase as a biocatalyst. General neural regression network (GRNN), multilayer feed-forward neural network (MLFN) and the SVM models were trained on the basis of experimental data. Four indicators were determined as independent variables, including time (h), temperature (C), amount of enzyme (mg) and molar ratio, while the SVM-SVM results (SV3) and SVM-GRM results were the best in both SVM and SVM results (SV3).", "histories": [["v1", "Thu, 12 Nov 2015 17:40:42 GMT  (911kb)", "http://arxiv.org/abs/1511.03984v1", "32 pages, 11 figures, 1 table"]], "COMMENTS": "32 pages, 11 figures, 1 table", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["run wang", "qiaoli mo", "qian zhang", "fudi chen", "dazuo yang"], "accepted": false, "id": "1511.03984"}, "pdf": {"name": "1511.03984.pdf", "metadata": {"source": "CRF", "title": "Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester Using Artificial Neural Networks and Support Vector Machine", "authors": ["Run Wang", "Qiaoli Mo", "Qian Zhang", "Fudi Chen", "Dazuo Yang"], "emails": ["dzyang1979@hotmail.com"], "sections": [{"heading": null, "text": "3\u03b2-O-phthalic ester of betulinic acid is of great importance in anticancer studies. However, the optimization of its reaction conditions requires a large number of experimental works. To simplify the number of times of optimization in experimental works, here, we use artificial neural network (ANN) and support vector machine (SVM) models for the prediction of yields of 3\u03b2-O-phthalic ester of betulinic acid synthesized\nby betulinic acid and phthalic anhydride using lipase as biocatalyst. General regression neural network (GRNN), multilayer feed-forward neural network (MLFN) and the SVM models were trained based on experimental data. Four indicators were set as independent variables, including time (h), temperature (\u00baC), amount of enzyme (mg) and molar ratio, while the yield of the 3\u03b2-O-phthalic ester of betulinic acid was set as the dependent variable. Results show that the GRNN and SVM models have the best prediction results during the testing process, with comparatively low RMS errors (4.01 and 4.23respectively) and short training times (both 1s). The prediction accuracy of the GRNN and SVM are both 100% in testing process, under the tolerance of 30%.\nKeywords: Artificial neural network, betulinic acid ester, biocatalyst, support vector machine."}, {"heading": "Introduction", "text": "3\u03b2-O-phthalic ester of betulinic acid has clinical potential as an anticancer medicine, which can be synthesized from reaction of betulinic acid and phthalic anhydride using lipase as biocatalyst (Fig. 1). It has a variety of properties including inhibition of antibacterial, anti-inflammatory, anti-malarial, anthelmintic, antioxidant and human immunodeficiency virus (HIV) (Yogeeswari, 2005).According to previous studies, the introduction of polar groups at the C-3 and C-28 positions also highly increases the anticancer activity and hydro-solubility (Thibeault et al., 2007; Gauthier and Legault, 2008).However, the practical applications of betulinic acid in the pharmaceutical and\nmedical industry is deeply constrained because it is insoluble in water (approximately 0.02 mg/mL) under ordinary circumstances, leading to great difficulties in preparation of injectable formulations for biological experiments and decreases the bioavailability.\nThe detailed approaches for the synthesis of 3\u03b2-O-phthalic ester of betulinic acid based on chemical catalytic esterification have been reported by previous research reports(Mukherjee et al., 2004; Kvasnica et al., 2005; Mukherjee et al., 2006; Rajendran et al., 2008), which have several disadvantages (e.g. high energy consumption and by-products) (Yasin et al., 2008). Compared with traditional chemical approaches, the application of enzymes in organic synthesis offers a series of advantages, including high catalytic efficiency, high selectivity, mild reaction condition and high product purity and quality (Loughlin, 2000; Zarev\u00facka and Wimmer, 2008). However, the best detailed conditions for the synthesis are difficult to obtain due to the large-scale and complex laboratory experiments. Moghaddam and colleagues(2010) used artificial neural network (ANN) models to develop models for predicting the yield of enzymatic synthesis of betulinic acid ester. They successfully found that the quick propagation algorithm was the best model during their computational experiments. Nevertheless, previous ANN models are based on comparatively complex operations and the selection method of the best ANN model was based on a limited number of results, which are not robust and user-friendly enough, compared with latest machine learning models. Here, we aim at using novel and user-friendly approaches of ANN models and support vector machine (SVM) to train the data of the yield of enzymatic\nsynthesis of betulinic acid ester, and obtain a series of best machine learning models for the prediction of the yield. Comparisons are made in order to determine the most suitable machine learning model for the prediction.\n2 Materials and Methods 2.1 Data Set\nAccording to previous research, the synthetic conditions of enzymatic synthesis of betulinic acid ester includes time (h), temperature (\u00baC), amount of enzyme (mg) and molar ratio(mmol betulinic acid/mmol phthalic anhydride) (Moghaddam et al., 2010). Here, we aim at using novel ANN and SVM models to fit the four conditions and to predict the isolated yield (%) of the enzymatic synthesis.\nA complete machine learning model consists of two parts, the independent variable(s) and the dependent variable(s). Here, we set the time (h), temperature (\u00baC), amount of enzyme (mg) and molar ratio as independent variables, while the isolated yield (%)was set as the dependent variable. 65% data group was set as training set, which 35% data group was set as testing set.\n2.2 ANN models A series of statistical learning algorithms with the name of Artificial Neural Networks (ANN) could give judgments when inputted into a large amount of information\n(Hopfield, 1988; Judith and DeLeo, 2001; Yegnanarayana, 2009). Like the brain, a biological neural network, ANNs are usually comprised of neurons that can make instant calculations in different conditions with the connection with each other. Different from ordinary networks with one or two layers, there are three layers in Artificial Neural Networks which can learn inputs efficiently and recognize patterns in a direct way. Furthermore, ANNs can also use complicated algorithms to make prediction and find the optimum solution. Therefore, when dealing with problems that are too complex to solve, ANNs can take the place of human brains, and the application of ANNs is more and more popular in the scientific research. In this passage, we introduce the use of two kinds of ANNs, multilayer feed-forward neural networks (MLFN) and general regression neural networks (GRNN) to build the models to forecast the yield of enzymatic synthesis of betulinic acid ester.\n1.2.1 Multilayer feed-forward neural networks (MLFN)\nWith the training of a back-propagation learning arithmetic, multilayer feed-forward neural networks, one of the most popular neural networks, can be used to predict a large range of chemical reactions. (Johansson et al., 1991; Smits et al., 1994; Svozil and Kvasnickab, 1997)\nNeurons in the MLFN models are put into different layers (Figure2). Input layer is the first layer and output layer is the last one. Between them, hidden layers play a role of\ncalculating and modeling. To be specific, we could use the mapping function\u0413 that allocates a subset \u0413(\ud835\udc56) \u2286 \ud835\udc49 to each neuron i to describe the neurons in a formal way, and The subset \u0413(\ud835\udc56) is made up of all ancestors of the neuron. Meanwhile, there is a subset \u0413(\ud835\udc56)\u22121 \u2286 \ud835\udc49 containing all ancestors of the given neuron\ud835\udc56. All neurons in a given layer is connected with any one of the neurons in the past layer. A weight coefficient \ud835\udf14\ud835\udc56\ud835\udc57 can be used to present the connection of the \ud835\udc56th and jth neuron, and we apply the threshold coefficient \u03d1i (Fig. 3) to present the \ud835\udc56th neuron. The level of significance of a particular connection in the neural network can be indicated by the weight coefficient. Additionally, Eqs.(1) and (2) can determine the output value (activity) of the \ud835\udc56th neuron \ud835\udc65\ud835\udc56. It holds that:\n\ud835\udc65\ud835\udc56 = \ud835\udc53(\ud835\udf09\ud835\udc56) (1)\n\ud835\udf09\ud835\udc56 = \ud835\udf17\ud835\udc56 + \u2211 \ud835\udf14\ud835\udc56\ud835\udc57\ud835\udc57\u2208\ud835\udc5f\ud835\udc56\u22121 \ud835\udc65\ud835\udc57 (2)\nWhere the potential of the i th neuron is represented by \ud835\udf01\ud835\udc56 and function \ud835\udc53(\ud835\udf01\ud835\udc56)indicates the transfer function (the summation in Eq. (2) runs over all neurons \ud835\udc57 devolving the signal to the\ud835\udc56th neuron). The threshold coefficient could be comprehend as a weight coefficient of the connection with regularly added neuron\ud835\udc57, where\ud835\udc65\ud835\udc57 = 1(so-called bias). For the transfer function it holds that:\n\ud835\udc53(\ud835\udf01) = 1\n1+\ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf01) (3)\nTo minimize the total of the squared differences between the required and calculated output values, The weight coefficients \u03c9ij and threshold coefficients \u03d1i are devolved by the supervised adaptation process. Minimization of the objective functionE can complete this by:\n\ud835\udc38 = \u2211 1\n20 (\ud835\udc650 \u2212 x\u0302 0)\n2 (4)\nWhere\ud835\udc650 and x\u0302 0 are vectors comprised of the required and calculated processes of the output neurons and the summation carried out over all output neurons\ud835\udc5c.\n1.2.2 General regression neural network (GRNN)\nThe Nadaraya-Watson kernel based general regression neural network (GRNN) which is put forward by Specht (1991) is widely applied in medical diagnosis, pattern identification, forecasting, three-dimensional modeling, chemical engineering and function approximation (Hoskins and Himmelblau, 1988; Khan et al., 2001; Goulermas et al., 2007; Kandirmaz et al., 2014; Li and Leng et al., 2014;Li and Wang et al., 2014). Compared with other statistical neural networks like feed-forward networks, GRNN represents more accurate in regard to function approximation (Kandirmaz et al., 2014). Even if it is the first time to be used with the aim of function approximation, in some\nstudies, GRNN is also applied to classification problems with small modifications (Kandirmaz et al., 2014). As can be seen from Figure 4, there are 4 layers in GRNN, respectively, input layer, pattern layer, summation layer, and output layer, with the characteristics of rapid learning, coherence, and finding optimum with a great many specimens(Yang and Li, 2014).\nIn the input layer, corresponding inputs are conserved and every input vector \ud835\udc65 can be devolved to pattern layer which contains different neurons for training data. As illustrated in Eq. (5), calculations are made about weighted squared Euclidean distance in the pattern layer. Before aggregated, inputs which are used for activation function, whether squares or absolute values, should be deducted from neuron values in pattern layer when applying to network. Then, neurons in summation layer, to which the outcomes are transferred, add dot product of pattern layer weights and outputs. As can be seen from Figure 4, \ud835\udc53(\ud835\udc65)\ud835\udc3e indicates weighted outputs of pattern layer where \ud835\udc3e is a Parzen window related constant. \ud835\udc4c\ud835\udc53(\ud835\udc65)\ud835\udc3e refers to multiplication of training data output\ud835\udc4cvalues and pattern layer outputs. In output layer, \ud835\udc53(\ud835\udc65)\ud835\udc3e separates \ud835\udc4c\ud835\udc53(\ud835\udc65)\ud835\udc3e to estimate desired values, held in Eqs. (6) and(7) (Goulermas et al., 2007; Yang and Li, 2014):\n\ud835\udc37\ud835\udc57 = (\ud835\udc65 \u2212 \ud835\udc65\ud835\udc57) \ud835\udc47 (\ud835\udc65 \u2212 \ud835\udc65\ud835\udc57),(5)\n\ud835\udc4c(\ud835\udc65) = \u222b \ud835\udc4c\ud835\udc53(\ud835\udc65,\ud835\udc4c)\ud835\udc51\ud835\udc4c \u221e \u2212\u221e\n\u222b \ud835\udc4c\ud835\udc53(\ud835\udc65,\ud835\udc4c)\ud835\udc51\ud835\udc4c \u221e\n\u2212\u221e\n,(6)\n\ud835\udc4c(\ud835\udc65) = \u2211 \ud835\udc66\ud835\udc57\ud835\udc52[\u2212\ud835\udc37\ud835\udc57 2\ud835\udf0e\n2\u2044 ] \ud835\udc5d \ud835\udc57=1\n\u2211 \ud835\udc52[\u2212\ud835\udc37\ud835\udc57 2\ud835\udf0e 2\u2044 ] \ud835\udc5d \ud835\udc57\n(7)\n2.3 SVM model\nThere is a formidable machine learning technique with the name of support vector machine (SVM) established from the statistical learning theory (Deng et al., 2012). In terms of increasing generalization, this theory can give an integral optimization in an efficient way, with restricted information of specimens between the learning capacity and the complicacy of models. Separating all specimens with the maximum margin, the main theory of SVM is a plane which has the ability of discovering the optimal hyperplane and linear separable dualistic classification (Zhong et al., 2013; Chen et al., 2015). Additionally, the plane can also increase the forecasting ability of the model and can decrease the mistake occurring accidentally when classifying. As can be seen from Figure 5, specimens of type 1 are represented by \u201c+\u201d and specimens of type \u22121 are represented by \u201c\u2212\u201d to shows the optimal hyperplane.\nTo explain the main structure of a representative support vector machine, Figure 6 shows a small subset derived from the training data by related algorithm that contains the SVM. Kernels are characterized by the letter \u201cK\u201d (Kim et al., 2005). To have a forecasting accuracy, appropriate kernels and suitable parameters should be selected in terms of classification. However, we could not find an available international standard\nto select these parameters. In most cases, to solve this task in a relatively reasonable way, we could take the advantage of the experiences from massive calculations, the contrast of experiment results, and the application of cross validation which is realizable in program package (Fan et al., 2008; Guoand Liu, 2010; Chen et al., 2015).\n2.4 Model Development\nThe ANN prediction models were constructed by the NeuralTools \u00ae software (trial version, Palisade Corporation, NY, USA) (Pollar and Jaroensutasinee, 2007; Friesen et al., 2011; Vouk et al., 2011). The GRNN and MLFN was chosen as the learning machines of ANNs.\nWe used RMS error and training time as the indicators to measure the performances of ANN and SVM models (Table 1). The nodes of MLFN models were set from 2 to 25, from which we could find out the change regulation of the MLFN models when dealing with development processes.\nTable 1 indicates that the GRNN, SVM and MLFN with 4 nodes have comparatively low mean RMS errors (4.01, 4.23and 5.56 respectively). It is clear that the GRNN and SVM have the lowest RMS errors and training times, while the MLFN models have comparatively higher RMS errors and longer training times. In addition, the prediction accuracy (under the tolerance of 30%) of the GRNN and SVM are both 100%. Here, we\ndiscuss the availability of the GRNN, SVM and MLFN respectively in order to determine the most suitable model for the prediction of the yield."}, {"heading": "3 Results and Discussion", "text": "3.1 Comparison between the GRNN and MLFN\nAs for the GRNN, it has the lowest RMS error and training time during our research, compared with other 24 MLFN models. And according to the robustness of the principles of the GRNN, it has a high reproducibility, which has an overwhelming advantage compared with other ANN models during our research. In order to test the robustness of the GRNN, the computational experiments for the GRNN was repeated, which are shown in Figure 7:\nFigure 7 shows the RMS errors of the GRNN models in repeated experiments. It is significant that there is a stable fluctuation during repeated experiments, which shows that the GRNN model for the optimization process is robust. And what is more importantly, the mean RMS error is relatively low, which ensures the availability of the GRNN model.\nTo illustrate the change of the MLFN models with different numbers of nodes, Figure 8 is used for showing the results of MLFN models with different nodes.\nIt can be seen that with the increase of nodes, the RMS errors and training time of MLFN models become unsteadily fluctuant, which highly corresponds to the fluctuation character of typical MLFN models. It is worth mentioning that results in different MLFN models presented by Table 1 is not a fixed result, because of the effects of different random initial values chosen by the computer when training. However, it is still clear that the MLFN model may have a good result (relatively low RMS error and short training time) in a relatively low number of nodes. For practical applications, operators can use related software to find out the most suitable model for the optimization of reaction conditions in the range of low number of nodes. However, compared to the GRNN, MLFN models commonly cost longer training time and the fluctuations are not as stable as those of GRNN model. Therefore, we still consider the GRNN model is a more suitable model for the prediction of the yield of enzymatic synthesis of betulinic acid ester\n3.2 Training and Testing Results of the GRNN and the SVM\nHere, we use one of the typical examples of the training and testing results to present the availability of the GRNN, and also illustrate the testing results of the SVM. Figures 9 and 10 are used for the illustration of the training and testing results of the GRNN, while Figure 11 is used for the illustration of the testing results of the SVM. The training and testing sets of the GRNN and SVM are the same.\nFor showing the capacity for recall of the GRNN model for the optimization of the design, Figure 9 is used for illustrating the training results of the GRNN.\nFigure 9 shows that the GRNN model has a strong capacity for recall. The predicted values is highly close to the actual values (Figure 9. (a)), which indicates that the non-linear fitting effects of the model is highly decent. The comparisons between the residual values and actual/predicted values (Figure 9. (b) and (c)) also show that the residual values are relatively low, which suggests the robustness of the development of the GRNN model.\nFor showing the availability of the GRNN model after a training process, we use the data set which has not been used for the training process. Results are shown in Figure 10.\nFigure 10 shows the precise predicted results during the testing process. Predicted values are close to the actual values [Figure 10. (a)]. Residual values presented by Figure 10. (b) and (c) show that the residual values are relatively low. Results present the robustness and availability of the GRNN model when testing.\nIn terms of the testing results of the SVM, Figure 11 is illustrated for showing the correctness and robustness of the SVM in the prediction section.\nBeing similar to the results of the GRNN in the aspects of the RMS error and the training time, the testing results of the SVM are also highly similar to those of the GRNN. We can see that the SVM can generate a fairly analogical and precise result, compared with the testing results of the GRNN.\nTo make a comparison between the GRNN and SVM, we should firstly note that the initial values for the training process of the GRNN are random, leading to different results in repeated experiments. Compared with the GRNN, the SVM has very good repeatable results due to its principle. Therefore, it seems that the GRNN is not as robust as the SVM. However, results of repeated experiments (Figure 7) show that regardless of those fluctuations of RMS errors, the GRNN is also highly robust because the fluctuations are in a controllable range, which ensures the robustness of the GRNN. In terms of the training time, the GRNN and SVM are too short to find out the differences. However, we should note that the GRNN model can be mainly developed by packed software, while the SVM needs to use the Matlab and finish a series of processes, which requires a higher requirements of computer configuration and comparatively longer time. For a more convenient operation, the GRNN seems more practical than the SVM. Nevertheless, due to the high robustness and repeatability, the SVM should not be neglected in practical applications.\nHere, enzymatic synthesis of betulinic acid ester is a typical example for the application of machine learning techniques like ANNs and SVM to the prediction of yields in\nlaboratory experiments. It shows that machine learning techniques have a huge potential applications for the prediction of yields in chemical reactions. And what is more importantly, we can optimize the reaction conditions via the \"well-trained\" models and predict their yields without trying repeatedly in laboratory experiments."}, {"heading": "Conclusion", "text": "3\u03b2-O-phthalic ester of betulinic acid is of great importance in clinical research. Here, we successfully find that the application of the ANNs and SVM are useful for the prediction of yields of 3\u03b2-O-phthalic ester of betulinic acid synthesized by betulinic acid and phthalic anhydride using lipase as biocatalyst. Results show that the GRNN and the SVM model have the best prediction results during the testing process, with comparatively low RMS errors (4.01and 4.23 respectively) and short training times (both 1s). The prediction accuracy of the GRNN and SVM are both 100% in testing process, under the tolerance of 30%. Both the GRNN and SVM have very good repeatability and robustness. Our research successfully show that machine learning techniques like the ANNs and SVM can be used for the prediction of yields and optimization of conditions of traditional organic synthesis. What is more, it is also proved that support vector machine is a novel and strong machine learning tool for related research and applications."}, {"heading": "Conflict of Interests", "text": "The authors declare that they have no conflict of interests."}, {"heading": "Financial support", "text": "This work was funded by the National Marine Public Welfare Research Project (nos. 201305002 and 201305043), National Natural Science Foundation of China (no. 30901107), and the Project of Marine Ecological Restoration Technology Research to the Penglai 19-3 Oil Spill Accident (no. 19-3YJ09)."}], "references": [{"title": "User-friendly optimization approach of fed-batch fermentation conditions for the production of iturin A using artificial neural networks and support vector machine", "author": ["F. Chen", "H. Li", "Z. Xu", "S. Hou", "D. Yang"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Liblinear: a library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "J. Mach. Learn", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Synthesis of two natural betulinic acid saponins containing \u03b1- l -rhamnopyranosyl-(1\u21922)-\u03b1- l -arabinopyranose and their analogues", "author": ["Gauthier", "C. andJ. Legault"], "venue": "modeling. Ecography,", "citeRegEx": "Gauthier and Legault,? \\Q2008\\E", "shortCiteRegEx": "Gauthier and Legault", "year": 2008}, {"title": "Artificial neural network models of knowledge representation in chemical engineering", "author": ["J.C. Hoskins", "D.M. Himmelblau"], "venue": "Comput. Chem", "citeRegEx": "Hoskins and Himmelblau,? \\Q1988\\E", "shortCiteRegEx": "Hoskins and Himmelblau", "year": 1988}, {"title": "Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks", "author": ["M. Schwab", "C.R. Antonescu", "C. Peterson", "P.S. Meltzer"], "venue": "Nat. Med.,7(6):", "citeRegEx": "Schwab et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schwab et al\\.", "year": 2001}, {"title": "Synthesis of phthalates of betulinic acid and betulin with cytotoxic activity", "author": ["M. Kvasnica", "J. Sarek", "E. Klinotova", "P. Dzubak", "M. Hajduch"], "venue": "Bioorgan. Med", "citeRegEx": "Kvasnica et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kvasnica et al\\.", "year": 2005}, {"title": "Biotransformations in organic synthesis. Bioresource", "author": ["W.A. Loughlin"], "venue": null, "citeRegEx": "Loughlin,? \\Q2000\\E", "shortCiteRegEx": "Loughlin", "year": 2000}, {"title": "Prediction of Henry's law constants for organic compounds using multilayer feedforward neural networks based on linear solvation energy relationship", "author": ["H. Li", "X. Wang", "T. Yi", "Z. Xu", "X. Liu"], "venue": "J. Chem. Pharm. Research.,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "2006.Betulinic Acid Derivatives as Anticancer Agents: Structure Activity Relationship", "author": ["R. Mukherjee", "V. Kumar", "S.K.S.K. Srivastava", "Agarwal", "A.C. Burman"], "venue": "anti-angiogenic agents. Bioorg. Med. Cgem. Lett.,", "citeRegEx": "Mukherjee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2010}, {"title": "A general regression neural network", "author": ["D.F. Specht"], "venue": "anticancer activity. Invest. New", "citeRegEx": "Specht,? \\Q1991\\E", "shortCiteRegEx": "Specht", "year": 1991}, {"title": "lupane-type 3\u03b2- o \u2013monodesmosidic saponins starting from betulin\u2606", "author": ["P.S.D. Yogeeswari"], "venue": "Bioorgan. Med", "citeRegEx": "Yogeeswari and 2005.,? \\Q2011\\E", "shortCiteRegEx": "Yogeeswari and 2005.", "year": 2011}, {"title": "Response surface methodology as a tool to study the lipase-catalyzed synthesis of betulinic acid ester", "author": ["Y. Yasin", "M. Basri", "F. Ahmad", "A.B. Salleh"], "venue": "J. Chem. Technol", "citeRegEx": "Yasin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yasin et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "introduction of polar groups at the C-3 and C-28 positions also highly increases the anticancer activity and hydro-solubility (Thibeault et al., 2007; Gauthier and Legault, 2008).", "startOffset": 126, "endOffset": 178}, {"referenceID": 5, "context": "The detailed approaches for the synthesis of 3\u03b2-O-phthalic ester of betulinic acid based on chemical catalytic esterification have been reported by previous research reports(Mukherjee et al., 2004; Kvasnica et al., 2005; Mukherjee et al., 2006; Rajendran et al., 2008), which have several disadvantages (e.", "startOffset": 173, "endOffset": 268}, {"referenceID": 11, "context": "high energy consumption and by-products) (Yasin et al., 2008).", "startOffset": 41, "endOffset": 61}, {"referenceID": 6, "context": "Compared with traditional chemical approaches, the application of enzymes in organic synthesis offers a series of advantages, including high catalytic efficiency, high selectivity, mild reaction condition and high product purity and quality (Loughlin, 2000; Zarev\u00facka and Wimmer, 2008).", "startOffset": 241, "endOffset": 285}, {"referenceID": 5, "context": ", 2004; Kvasnica et al., 2005; Mukherjee et al., 2006; Rajendran et al., 2008), which have several disadvantages (e.g. high energy consumption and by-products) (Yasin et al., 2008). Compared with traditional chemical approaches, the application of enzymes in organic synthesis offers a series of advantages, including high catalytic efficiency, high selectivity, mild reaction condition and high product purity and quality (Loughlin, 2000; Zarev\u00facka and Wimmer, 2008). However, the best detailed conditions for the synthesis are difficult to obtain due to the large-scale and complex laboratory experiments. Moghaddam and colleagues(2010) used artificial neural network (ANN) models to develop models for predicting the yield of enzymatic synthesis of betulinic acid ester.", "startOffset": 8, "endOffset": 639}, {"referenceID": 3, "context": "The Nadaraya-Watson kernel based general regression neural network (GRNN) which is put forward by Specht (1991) is widely applied in medical diagnosis, pattern identification, forecasting, three-dimensional modeling, chemical engineering and function approximation (Hoskins and Himmelblau, 1988; Khan et al., 2001; Goulermas et al., 2007; Kandirmaz et al., 2014; Li and Leng et al., 2014;Li and Wang et al., 2014).", "startOffset": 265, "endOffset": 413}, {"referenceID": 8, "context": "The Nadaraya-Watson kernel based general regression neural network (GRNN) which is put forward by Specht (1991) is widely applied in medical diagnosis, pattern identification, forecasting, three-dimensional modeling, chemical engineering and function approximation (Hoskins and Himmelblau, 1988; Khan et al.", "startOffset": 98, "endOffset": 112}, {"referenceID": 0, "context": "Separating all specimens with the maximum margin, the main theory of SVM is a plane which has the ability of discovering the optimal hyperplane and linear separable dualistic classification (Zhong et al., 2013; Chen et al., 2015).", "startOffset": 190, "endOffset": 229}, {"referenceID": 1, "context": "In most cases, to solve this task in a relatively reasonable way, we could take the advantage of the experiences from massive calculations, the contrast of experiment results, and the application of cross validation which is realizable in program package (Fan et al., 2008; Guoand Liu, 2010; Chen et al., 2015).", "startOffset": 255, "endOffset": 310}, {"referenceID": 0, "context": "In most cases, to solve this task in a relatively reasonable way, we could take the advantage of the experiences from massive calculations, the contrast of experiment results, and the application of cross validation which is realizable in program package (Fan et al., 2008; Guoand Liu, 2010; Chen et al., 2015).", "startOffset": 255, "endOffset": 310}, {"referenceID": 6, "context": "Loughlin, W. A., 2000. Biotransformations in organic synthesis. Bioresource. Technol.,74(1): 49-62(14). Li, H., W. J. Leng, Y. B. Zhou, F. Chen, Z. Xiu and D. Yang, 2014. Evaluation models for soil nutrient based on support vector machine and artificial neural networks. Sci. World. J., 2014 (2014). Li, H.", "startOffset": 0, "endOffset": 299}], "year": 2015, "abstractText": "3\u03b2-O-phthalic ester of betulinic acid is of great importance in anticancer studies. However, the optimization of its reaction conditions requires a large number of experimental works. To simplify the number of times of optimization in experimental works, here, we use artificial neural network (ANN) and support vector machine (SVM) models for the prediction of yields of 3\u03b2-O-phthalic ester of betulinic acid synthesized", "creator": "Microsoft\u00ae Word 2010 \u8bd5\u7528\u7248"}}}