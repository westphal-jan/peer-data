{"id": "1511.05175", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Convolutional Models for Joint Object Categorization and Pose Estimation", "abstract": "Object recognition is a dichotomy between categorizing objects and estimating object positions, with the former requiring an eye-invariant representation, while the latter requires representation that is capable of capturing information about different object categories. As deep architectures have risen, the emphasis has been on object category recognition, and deep learning methods have been a great success in this task. By contrast, the regression of object positions has received relatively less attention with these approaches. In this paper, we show how deep architectures, especially Convolutional Neural Networks (CNN), can be adapted to the task of simultaneously categorizing and evaluating objects. We study and analyze the layers of different CNN models and compare them comprehensively to find out how the layers of distributed representations of CNN's information are at odds with object category representations.", "histories": [["v1", "Mon, 16 Nov 2015 21:08:22 GMT  (1728kb,D)", "https://arxiv.org/abs/1511.05175v1", null], ["v2", "Thu, 19 Nov 2015 23:17:11 GMT  (1754kb,D)", "http://arxiv.org/abs/1511.05175v2", null], ["v3", "Thu, 7 Jan 2016 23:40:23 GMT  (3323kb,D)", "http://arxiv.org/abs/1511.05175v3", "Jan 7 update"], ["v4", "Wed, 20 Jan 2016 22:41:19 GMT  (3518kb,D)", "http://arxiv.org/abs/1511.05175v4", "Jan 19 update"], ["v5", "Mon, 22 Feb 2016 23:54:23 GMT  (3518kb,D)", "http://arxiv.org/abs/1511.05175v5", "only for workshop presentation at ICLR"], ["v6", "Tue, 19 Apr 2016 17:56:34 GMT  (3520kb,D)", "http://arxiv.org/abs/1511.05175v6", "only for workshop presentation at ICLR"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["mohamed elhoseiny", "tarek el-gaaly", "amr bakry", "ahmed elgammal"], "accepted": false, "id": "1511.05175"}, "pdf": {"name": "1511.05175.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mohamed Elhoseiny", "Tarek El-Gaaly", "Amr Bakry", "Ahmed Elgammal"], "emails": ["m.elhoseiny@cs.rutgers.edu", "tgaaly@cs.rutgers.edu", "amrbakry@cs.rutgers.edu", "elgammal@cs.rutgers.edu"], "sections": [{"heading": null, "text": "In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets."}, {"heading": "1 INTRODUCTION", "text": "Impressive progress has been made over the last decade towards solving the problems of object categorization, localization and detection. It is desirable for a vision system to address two tasks under general object recognition: object categorization and object pose estimation (estimating the relative pose of an object with respect to a camera). Pose estimation is crucial in many applications. These two broad tasks are contradicting in nature. An optimal object categorization system should be able to recognize the category of an object, independent of its viewpoint. This means that the system should be able to learn viewpoint-invariant representations of object categories. In contrast, a pose estimation system requires a representation that preserves the geometric and visual features of the objects in order to distinguish its pose.\nThis gives rise to a fundamental question: should categorization and pose estimation be solved simultaneously, and if so, can one aid the other? Contrasting paradigms approach this question differently. Traditional instance-based 3D pose estimation approaches solve the instance-recognition and pose estimation problems simultaneously, given model bases of instances in 2D or 3D (Grimson & Lozano-Perez, 1985; Lamdan & Wolfson, 1988; Lowe, 1987; Shimshoni & Ponce, 1997). Most recent object pose estimation approaches solve the problem within the detection process, where category-specific object detectors that encode part geometry are trained (Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012). Since part-geometry is a function of the pose, these approaches are able to provide coarse estimate of the object pose with the detection. However the underlying assumption here is that the categorization is done a-priori, and the representation is view-variant. Other recent approaches try to solve the pose estimation simultaneously with categorization through learning dual representations: view-invariant category representation and view-variant category-invariant representation (Zhang et al., 2013a; Bakry & Elgammal, 2014).\n\u2217Equal contribution\nar X\niv :1\n51 1.\n05 17\n5v 6\n[ cs\n.C V\n] 1\n9 A\npr 2\n01 6\nWith the rise of deep architectures, the main focus has been on category recognition. A wide success has been achieved on this task. In contrast, pose estimation has not received much attention. The impressive results of Convolutional Neural Networks (CNNs) in tasks of categorizations (Krizhevsky et al., 2012) and detection (Sermanet et al., 2013; Girshick et al., 2013) motivated many researchers to explore their applicability in different vision tasks. Several approaches recently showed successful results where they used networks that are pre-trained for a specific task (e.g.categorization) and then use the representation of higher layers as features for another task (Frome et al., 2013; Donahue et al., 2013; Zeiler & Fergus, 2013). This process is known as transfer learning. As pointed out by (Yosinski et al., 2014), this process is useful when the target task has significantly smaller training data than what is needed to train the model. Typically the first n-layers are copied from the pre-trained network to initialize the corresponding layers for the target task. Within the CNN literature, typically the layers up until FC7 (which is the last layer before the output layer) are used for that purpose (Frome et al., 2013).\nPose estimation is an example of a task that inherently suffers from lack of data. In fact the largest available dataset for multiview recognition and pose estimation has 51 object categories with a total of about 300 instances (Lai et al., 2011a). It is hard to imagine the availability of a dataset of thousands of objects where different views are sampled around each object in order to be able to train a learning machine such as a CNN with millions of parameters. Therefore, transfer learning is critical for this task. However the challenge lies in the contradicting objective that has been described in the first paragraph. Current CNN models are optimized for categorization, and therefore they are expected to achieve view invariant representation. Therefore it is not expected that feature representation at deeper layers are useful for pose estimation. However, feature representation in shallower layers tend to be more general and less category-specific and thus may hold enough information to discriminate between different poses. This is a key hypothesis that is explored in this paper and this work is the first exploration of the capability of CNNs on the task of object pose estimation.\nThe contributions of this paper are: (1) we show how CNNs can be adapted to the task of simultaneous categorization and pose estimation of objects, (2) we investigate how each of these tasks affect the other, i.e.how category-specific information can help estimate the pose of an object and how a balance between these contrasting tasks can be achieved, (3) we analyze different CNN models and extensively compare between them to find an efficient balance between accurate categorization and robust pose estimation, (4) we validate our work by extensive experiments on two recent large and challenging multi-view datasets. We achieve better than state-of-the-art performance on both datasets."}, {"heading": "2 RELATED WORK", "text": "Due to the surge of work in deep architectures over the last few years, there has amassed a large number of research studies. Despite this, using CNNs for regression and capturing pose information is still a relatively unexplored area. This motivates the goals of this paper.\nWe focus on the most relevant work, in particular, studies that focus on understanding the functions of CNN layers and CNNs that solve for pose information. We also briefly touch upon previous approaches in object categorization and pose estimation.\nAlthough fundamentally different to object pose estimation, some research has explored using CNNs to recognize human pose (Toshev & Szegedy, 2013; LI et al., 2014; Pfister et al., 2014). Recently (Gkioxari et al., 2014) proposed joint optimization on human pose and activity. In human pose estimation, there is no problem getting millions of image of people at different postures. Human activities are correlated with human poses, while in the object-pose domain the category is independent of pose. This makes joint learning of category and pose more challenging than joint learning of human activity and pose. Some very recent work has explored joint detection and pose estimation using CNNs (Tulsiani & Malik, 2014).\nRecent in-depth studies explore the intricacies of CNNs; including the effects of transfer-learning and fine-tuning, properties and dimensions of CNN layers and the study of invariances captured in CNN layers (e.g.(Yosinski et al., 2014; Zeiler & Fergus, 2013; Chatfield et al., 2014)). A data-centric analysis of existing CNN models for object detection has appeared in (Pepik et al., 2015a).\nA comprehensive review of recent work in object recognition and pose estimation is detailed in (Savarese & Fei-fei, 2010). We highlight the most relevant research. Successful work have been done in estimating the object pose of a single object (Cyr & Kimia, 2004; Mei et al., 2011; Schels\net al., 2012). This model, referred to as single-instance 3D model, has the limitation of being category-specific and does not scalable to a large number of categories and deal with high intraclass variation. Recently, detection and pose have been solved simultaneously (e.g.(Tulsiani & Malik, 2014; Pepik et al., 2015b; Xiang et al., 2014; Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012; Lai et al., 2011b)). Most of these methods belongs to the category of limited-pose (discrete-pose) approaches since it uses classification for pose estimation. Very few works formulate the pose estimation problem as regression over a continuous space. In (Lai et al., 2011b), an object pose tree is built for doing multilevel inference. This involves a classification strategy for pose which results in coarse estimates and does not utilize information present in the continuous distribution of descriptor spaces. Work presented in (Zhang et al., 2013b) and (Torki & Elgammal, 2011) explicitly model the continuous pose variations of objects but the scalability of these models is limited. A more recent work (Bakry & Elgammal, 2014) proposes a feedforward approach to solve the two problems jointly by balancing between continuous and discrete modeling of pose in order to increase performance and scalability. In these models, the nonlinearity in the category representations is not modeled, which is mandatory for many applications."}, {"heading": "3 MOTIVATION", "text": "The first question we pose in this paper is how good are pre-trained representations of different CNN layers, without fine-tuning, for the task of pose estimation? To answer this we analyzed a state-ofthe-art CNN trained on ImageNet (Krizhevsky et al., 2012) by testing it on dense multi-view images from the RGBD dataset (Lai et al., 2011a) to see how well it represented object view-manifolds and hence able to estimate object poses. This CNN is composed of 8 layers: Conv1, Pool1, Conv2, Pool2, Conv3, Conv4, Conv5, Pool5, FC6, FC7, FC8. Pool indicates Max-Pooling layers, Conv indicates convolutional layers and FC indicates fully connected layers.\nIn order to quantitatively evaluate the representations of pose within the CNN, we trained both a pose regressor (using Kernel Ridge-Regression) and an SVM classifier for categorization (linear one-vs-all) on the features extracted from each of the layers. Fig. 1-Left is the result of the regressor and classifier. It clearly shows the conflict in representation of the pre-trained network. For pose estimation, the performance increases until around Pool5 and then decreases. This confirms that shallow layers that have sufficient abstractive representation offer better feature encoding for pose estimation. It appears that Pool5 provides a representation that captures the best compromise in performance, between categorization and pose discrimination.\nIn Fig 1-Left we report cross-evaluation of categorization using pose features and vice versa. FC8 (output) which is task specific, does not perform good pose estimation, while FC6/FC7 perform much better. It is interesting to observe the opposite is not true; when optimizing on pose, much of the category-specific information is still represented by the features of the CNN, as seen by the increase in performance of category recognition using the pose-optimized features.\nWe further explored using other regressors on multiple views of a single object instance (GPR (Rasmussen & Williams, 2005), WKNN (H et al., 1996), SVR (H et al., 1996), KTA (H et al., 1996)). We use a coffee mug instance that has enough visual and shape features to discriminate its poses. Fig 1-Right shows the MAE of the pose regression. The results confirm that the pose representation improves as we approach Pool5. This indicates that Pool5 has the best representation of the object\u2019s view-manifold. We also found that the performance of features based on Pool5 are the closest to correlate with the performance when using HOG features on the objects\u2019 multi-view images (Fig 1- Right). This further proves that Pool5 has the abstraction capability to represent pose efficiently.\nIt is important to point out here that, in addition to our analysis, in-depth manifold analysis was previously conducted to analyze the object-view manifolds and their representations within CNN layers. This can be found in (Bakry et al., 2015). This in-depth study corroborates the conclusions we make here."}, {"heading": "4 ANALYZED MODELS", "text": "We used a state-of-the-art CNN built by Krizhevsky et al. (2012) as our baseline network in our experiments (winner of the LSVRC-2012 Imagenet Challenge Russakovsky et al. (2014)). We refer to this model as Model0: base network. This model is pre-trained on Imagenet. The last fully connected layer (FC8) is fed to a 1000-way softmax which produces a distribution over the category labels. Dropout was employed during training and Rectified Linear Units (ReLU) were used for\nfaster training. Stochastic gradient descent is used for back propagation. Model0 is not fine-tuned, and thus an analysis of it shows how the layers of a CNN trained on categorization of ImageNet lacks the ability to represent pose efficiently. Throughout this study we vary the architecture of the base network and the loss functions. All other models described are pre-trained on ImageNet and fine-tuned on each of the two large dataset we experimented on.\nWe propose and investigate four different CNN models: Parallel Model (PM), Cross-Product Model (CPM), Late Branching Model (LBM) and Early Branching Model (EBM).\nPM is a parallel version of the base network; two parallel and independent versions of the base network, one for categorization and one for pose. CPM has an output layer with units for each category and pose combination to jointly train (depicted in Figure 2-a). LBM and EBM models are also depicted in Figure 2. LBM branches into two last layers, one for categorization and one for pose. EBM performs early branching into two subnetworks, each specialized in categorization and pose estimation, respectively. The output layer FC8 is not merged but instead the LBM and EBM networks are optimized over two loss functions, one concerned with building view-invariance representations for categorization and the other with category-invariant representations for pose estimation. Because of the branching, this causes two units to be active, one in each branch, at the same time. The only work that has done something similar to this is the work byGkioxari et al. (2014).\nAll losses are optimized by the multinomial logistic regression objective, similar to Krizhevsky et al. (2012) (Softmax loss). We denote softmax loss of label l \u2208 {lc, lp} and image x as lossi(x, l), where i indicates if this loss is over category or pose modes, lp and lc are the labels for pose and category, respectively.\nIn the following subsections we describe each model in detail.\nParallel Model (PM): This model consists of two base networks running in parallel, each solving categorization and pose estimation independently. There is no sharing of information between the two networks. The goal of this model is to see how well the traditional CNN is capable of representing object-view manifolds and hence estimating object pose, independent of category-specific information. The category and pose losses minimized in this model are: lossc(x, lc) and lossp(x, lp), one for each of the tasks of categorization and pose estimation, respectively.\nCross-Product Model (CPM): CPM explores a way to combine categorization and pose estimation by building a last layer capable of capturing both (see Fig2-a). We build a layer with the number of units equivalent to the number of combinations of category and pose, i.e.the cross-product of category and pose labels. The number of categories varies according to the dataset as we will see. The pose angles (in this case yaw or azimuth angle of an object) is discretized into angle bins across\nthe viewing circle. This is the case with all our pose estimating models. The loss function for CPM is the softmax loss over the cross-product of category and pose labels: loss(x, lp \u00d7 lc). Late Branching Model (LBM): We introduce a change in the architecture by splitting/branching the network into two last layers, each designed to be specific to the two tasks: categorization and pose estimation. Thus, this network has a shared representation for both category and pose information up until layer FC7 (see Fig2-b).\nThe goal is to learn category and pose information simultaneously from the representations encoded in the previous layers of the CNN. The question behind this model is whether or not one last layer would be enough to recover the pose information from the previous layers, in other words untangle the object view-manifold and give accurate pose estimates. In other words, one can think of this as testing the ability of the deep distributed representations of a CNN in holding both categoryspecific pose-invariant information as well as pose-variant information. LBM is trained using a linear combination of losses over category and pose: \u03bb1 \u00b7 lossc(x, lc) + \u03bb2 \u00b7 lossp(x, lp) where \u03bb1, \u03bb2 are weights found empirically (see appendix G).\nEarly Branching Model (EBM): The question of moving the branching to an earlier layer in the network poses itself here: Can the branching be moved earlier in the network to where the pose knowledge is still well preserved and in fact maximal across the layers?\nFrom our experiments (described later on) we observe that the objects\u2019 view-manifolds are maximally represented at Pool5. Thus, this network has a shared representation for both category and pose information up until layer Pool5. At Pool5 it branches out into two subnetworks, that are jointly optimized using a combined loss function (same as for LBM): \u03bb1 \u00b7 lossc(x, lc) + \u03bb2 \u00b7 lossp(x, lp). Similar to LBM, it is important to note that this network optimizes over two losses. This model achieves the most efficient balance between categorization and pose estimation and achieve stateof-the-art results on two large challenging datasets, as we shall see in Section 7."}, {"heading": "5 DATASETS", "text": ""}, {"heading": "5.1 RGBD DATASET", "text": "One of the largest and challenging multi-view datasets available is the RGB-D dataset Lai et al. (2011a). It consists of 300 tabletop object instances over 51 different categories. Images are captured of objects rotating on a turn-table, resulting in dense views of each object. The camera is positioned at three different heights with elevation angles: 30\u25e6, 45\u25e6 and 60\u25e6.\nIn previous approaches the middle height (45\u25e6) is left out for testing Lai et al. (2011b); Zhang et al. (2013a); Bakry & Elgammal (2014); El-Gaaly et al. (2012). This means that object instances at test time have been seen before from other heights during training. For this dataset it was important to\nexperiment with an additional training-testing split of the data to give more meaningful results. We wanted to ensure that objects at test time had never seen before. We also wanted to make sure that the instances we are dealing with have non-degenerate view manifolds. We observed that many of the objects in the dataset are ill-posed, in the sense that the poses of the object are not distinct. This happens when the objects have no discriminating texture or shape to be able to identify its poses (e.g. a texture-less ball or orange). This causes object-view manifold degeneracy. For this reason, we select 34 out of the 51 categories as objects that possess variation across the viewpoints, and thus are not ill-posed with respect to pose estimation. We split the data into training, validation and testing. In this datasets, most categories have few instances; therefore we left out two random instances per category, one for validation and one for testing. In the case where a category has less than 5 instances, we form the validation set for that category by randomly sampling one object instance from the training set. We also left out all the middle height for testing. Thus, the testing set is composed of unseen instances and unseen heights and this allows us to more accurately evaluate the capability of CNNs in discriminating categories and poses of tabletop objects. We call this split, Split 1. In order to compare with state-of-the-art we also used the split used by previous approaches (we call this Split 2)."}, {"heading": "5.2 PASCAL3D+ DATASET", "text": "We experiment on the recently released challenging dataset of multi-view images, called Pascal3D+ Xiang et al. (2014). Pascal3D+ is very challenging because it consists of images in the wild, in other words, images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset Everingham et al. (2010). These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset Russakovsky et al. (2014). The bottle category is omitted in state-of-the-art results. To be consistent, we do the same. This leaves 11 categories to experiment with. There are about 11,500 and 7,000 training images in ImageNet and Pascal3D+ subsets, respectively. We take a small portion of these images for validation and use the rest for training. For testing, there are about 11,200 and 6,900 testing images for ImageNet and Pascal3D+, respectively. On average there are about 3,000 object instances per category in Pascal3D+ captured in the wild, making it a challenging dataset for estimating object pose."}, {"heading": "6 CNN LAYER ANALYSIS", "text": "Similar to the analysis performed in Section 3, we do the same on all our described models. This gives insight into the ability of these models to represent pose and the intrinsic differences between them. We perform kernel Ridge-regression and SVM classification on each layer of the CNN models. The results of this analysis on the two muti-view datasets are shown in Fig. 3 and 4.\nFrom Fig. 3 and 4, we can see that the base network monotonically decreases in pose accuracy after layer Pool5. Pool5 seems to again hold substantial pose information, before it is lost in the following layers. This is the premise behind the design of our EBM model. EBM is able to efficiently untangle the object-view manifold and achieve good pose estimation on the branch specific to pose estimation.\nFrom Fig. 3 and 4, it can be observed that the LBM is able to achieve a good boost in pose performance at its last layer. From 3-right, it can be observed that at the layers Conv4 and Pool5 EBM has slightly worse accuracy than LBM and PM on the RGBD dataset. This indicates that the optimization is putting emphasis on the category information just before branching to achieve better pose estimation at deeper layers.\nCPM does quite worse than the other models on both datasets, in both categorization and pose estimation. This can be seen in Fig. 3-Left and 3-Right and to some extent in Fig. 4. The reason for this lies in the fact that CPM shares information to jointly optimize over category and pose. The drop is more evident in the task of categorization, indicating again that category information aids in estimating the pose, but not the other way round. The drop is more on the RGBD dataset because there are a lot more categories than Pascal3D+ and thus a lot more inter-class confusion. This is analogous to using category labels to separate between objects of different categories which may help bring similar posed objects of the same category together in the latent space encoded in the layers. On the other hand there is no clear untangling of the object-view manifold, where the pose\ninformation is stored, and thus this lack of pose information negatively impacts the categorization of objects.\nk-NN Layer Analysis We conduct k \u2212 NearestNeighbor pose estimation over the Pascal3D dataset on all the layers of the 4 models with varying neighborhood sizes (shown in Fig. 5). Comparing the two models (LBM and EBM), we gain slight improvement in categorization and a large improvement in pose estimation performance when using EBM. From Figure 5 and 9 in the Appendix, we conclude that as we go deeper into the network - up to layer Conv5 - we gain more category separation and object-view manifold preservation. This shows how the early branching better resolves the contradiction between the pose estimation and categorization tasks while sharing the low level filter representations that are helpful for both tasks. After Conv5, there are two common layers in EBM. In these two layers, linear separability between categories increases (seen in Fig. 9), but the object-view manifolds collapse (as seen in Fig. 5). This hurts the pose estimation. At the same time, this supports the aforementioned claim that enforcing better categorization (fine-tuning) hurts pose estimation. In our best performing model (EBM), in Figure 5, remarkable improvement to the pose object-view manifold is attained. For pose, the drop in KNN-classifier as the K increases vanishes when going deeper in network; see FC6 and FC7 layers EBM in Fig. 5. KNN figure for categorization on Pascal3D dataset could be seen in appendix A. In a similar behavior EBM behaves better than CPM and PM. An interesting behavior that CPM works clearly on Pascal3D dataset compared to RGBD; see Appendix A for KNN analysis on RGBD dataset. This is due that RGBD dataset has both dense poses and also larger number of categories ( 5 times Pas-\ncal3D). This increases the information/uncertainty to model that are beyond the capacity of CPM for RGBD dataset and generally as the number of categories and poses increase.\nLocal Pose Measurement Analysis: In Appendix B, we further analyzed four local measurement pose analysis proposed in (Bakry et al., 2015) to analyze layers of the five models we study against the pose manifold. The purpose of this analysis is to show how the learning representations for each model is untangle to the circle manifold where the pose inhabits"}, {"heading": "7 EXPERIMENTS", "text": "Here we describe the experimental setup and present the quantitative results of our experiments as well as comparisons with state-of-the-art."}, {"heading": "7.1 TRAINING AND TESTING", "text": "All models are trained by back propagation with Stochastic gradient descent. Refer to appendix E for parameter settings, e.g. learning rate, decay, etc. At training time, we randomly sample 227x227 patches from the down-scaled 256x256 images. At test time the center 227x227 patches are taken.\nAll classification losses are optimized by the multinomial logistic regression objective. Similar to (Krizhevsky et al., 2012), we optimized it by maximizing the average of the log-probability of the correct label under the prediction distribution across training cases. The pose softmax output (FC8) layer produces the pose probability distribution given the image. For each of the category and pose losses, the gradient with respect to the CNN parameters is computed which is then fed into CNN training for back propagation.\nAll the results that are presented in the paper were based on the prediction of argmaxposep(pose|x), where pose is one of the 16 pose bins and x is the given image.\nIn addition, we conduct an experiment where we predict the pose by computing the expected pose in the distribution of p(pose|x); see Eq. 1.\nE(p(pose|x)) = \u2211 i p(posei|x)\u00d7 \u03c6(posei) (1)\nwhere \u03c6(posei) is the center angle of the corresponding bin posei (the pose of the ith bin).\nThe detailed definitions of the performance metrics used in our experiments are described in appendix D."}, {"heading": "7.2 RESULTS", "text": "Table 1 shows the category recognition and pose estimation performance for the different models on the two training-testing splits of the RGBD dataset. Table 2 shows our best performing model EBM compared to state-of-the-art approaches. Using the pose prediction rule of eq. 1, the pose accuracy of EBM(800) increased from 78.83% to 79.30% for the argmax prediction on split 2 of RGBD.\nLooking at the closest previous approach in Table 2, (Bakry & Elgammal, 2014) achieves 96.01% classification accuracy. This is achieved using both visual and depth channels. We only used RGB (without depth) in our approach. (Bakry & Elgammal, 2014) achieves a lower 94.84% with RGB only, which shows the advantage of our CNN models for classification.\nWe achieve 2.3% increase in category recognition and about 2% increase in pose estimation (79.30%) using EBM(800), when compared with state-of-the-art. These measurements are likely to increase further when using EBM(4096), as we see slight improvement of EBM(4096) over EBM(800) in Table 2. It is also possible that running k-NN on top of the layer features could improve performance further. We achieved 97.14% categorization using EBM. We also achieved 99.0% classification accuracy using Nearest Neighbor classification on the Pool5 layer of EBM, which indicates that we learned better convolutional filters.\nTable 3 shows the performance of our models on Pascal3D+. We compare the accuracy achieved by our models with state-of-the-art results by (Zhang et al., 2015; Xiang et al., 2014; Pepik et al., 2015b; Tulsiani & Malik, 2014). It must be noted here that we are solving slightly different problems to some of these approaches. In Xiang et al. (2014), the authors solve detection and pose estimation, assuming correct detection. On the other hand (Zhang et al., 2015) solve just pose estimation, assuming that the object categories are known. (Pepik et al., 2015b; Tulsiani & Malik, 2014) solve joint detection and pose estimation. In our case we are jointly solving both category recognition and pose estimation, which can be considered a harder problem than that of (Zhang et al., 2015) and (Xiang et al., 2014). Our pose estimation performance is better than all these previous approaches. For the sake of this comparison, we computed the pose performance using the metrics applied in (Zhang et al., 2015). These metrics are pose accuracy for images with pose errors < 22.5\u25e6 and < 45\u25e6.\nTable 3 shows both our categorization and pose estimation results on Pascal3D compared against previous approaches. The table indicates 13.69% improvement of our method over (Zhang et al., 2015) (the best performing previous approach) in pose < 22.5\u25e6 metric and 4% improvement in pose < 45\u25e6, which are significant results. It is important to note that comparing to (Xiang et al., 2014; Tulsiani & Malik, 2014; Pepik et al., 2015b) is slightly unfair because these works solve for detection and pose simultaneously, while we do not solve detection.\nWe also show our performance when including ImageNet images in the training set and also the test set - see Table 3 (rows 7-8). The results show the benefit of ImageNet training images which boosts pose performance to 76.9% (from 57.89%) and 88.26% (from 63.0%) for pose < 22.5\u25e6 and pose < 45\u25e6, respectively.\nIn Table 3, on the in the wild images of Pascal3D+, our EBM model achieves an impressive increase of\u223c8% and\u223c5% over the state-of-the-art models using the two pose accuracy metrics, respectively."}, {"heading": "7.3 COMPUTATIONAL ANALYSIS AND CONVERGENCE", "text": "We performed computational analysis on the convergence of the models and show that EBM converges substantially faster than all the other models. In Fig. 6 we show the convergence rates of the proposed models. EBM here is the larger EBM (4096) network. Despite having many more parameters than most of the other models (about \u223c112 million parameters compared to 60 million in the base network), EBM converges substantially faster than all the other models. This shows the ability of this particular network to specialize faster in the two tasks. The shared first five layers are able to build up the object-view manifolds, preserve them and enhance them in the pose subnetwork of the model, while the other subnetwork specializes in pose-invariant category recognition."}, {"heading": "8 DISCUSSION", "text": "Analysis of the layers of all the CNN models is shown in Fig. 3 to 9. We provide quantitative results over two challenging datasets and summarize them in Tables 1, 2 and 3.\nWe compare our models with multiple baselines in Table 1: Linear SVM and Kernel RidgeRegression on HOG descriptors Dalal & Triggs (2005) as well as on features extracted from the best performing layers of the base network on each task. These baseline results were expected to be quite lower than our models\u2019 performance due to the lack of fine-tuning in the base model and due to the sharing of the network layers between the two tasks of category recognition and pose estimation.\nWithout fine-tuning the base network does not represent the object-view manifold well enough to estimate the pose efficiently. After fine-tuning on each of the respective datasets, we were able to achieve good category performance using the PM model. The downside of this model is its inability to perform robust pose estimation on the more challenging natural sparser-views of Pascal3D+. This is evident in the results shown in Table 3, where PM achieves less pose accuracy.\nIn Tables 1 and 3, we see again that CPM does worse than the other models in both datasets. This is more evident in the task of categorization, e.g., a drop of \u223c7% and 2%-3% in category and pose accuracy on Pascal3D+, respectively, and similarly \u223c10% and \u223c6% on the RGBD dataset. This motivates the need for branching in the networks and branching at the particular layer that better represents both category and pose. Interestingly, we found that CPM performs relatively better on Pascal3D+. We argue that the reason is that object poses in natural images are dominated by a smaller range of viewpoints and hence most of the pose-bins have vanishing probability (easier to learn). In addition, Pascal3D+ has a smaller number of categories.\nLBM performs relatively well on RGBD, but not on Pascal3D+. This can be attributed to the fact that RGBD has many more categories and is composed of images of objects under controlled settings and not in-the-wild like in Pascal3D+. The images in the RGBD dataset are captured at dense views as the object rotates on a turn-table. This is why the pose information is more prevalent in the last layers. This is evident from the steep monotonically increasing curve of LBM in Fig. 3-Left. This is not the case in Pascal3D+ where the increase is more steady and in fact there is a decrease after layer FC6 (see Fig. 4-Right).\nThe reason why EBM performs better than PM even though its weights are randomly initialized is that PM\u2019s FC6 and FC7 layers in the pose-specific branch are initialized with category-specific weights from pre-training. This adversely affects pose estimation since it is a contradictory task that requires view-variant representations and not view-invariant representations like that required in categorization. Therefore initializing FC6/FC7 by another network trained for a different task is not likely to help. We show that learning the convolutional filters jointly with categories help make them discriminative for both tasks and thus achieves a good accuracy on both tasks (see Fig. 3 to 9 and Tables 1, 2 and 3).\nComparing between EBM and LBM, we see that early branching is able to achieve a good balance between categorization and pose estimation by sharing the representations up to where we found the layer representation still capture pose information. We see this in Tables 1, 3 and Figures 1 to 5, where better pose accuracy and slightly better categorization accuracy is achieved by EBM. We also see that in Figure 5 that the object view-manifold collapses in the last two layers (one layer before LBM) and thus achieves better pose discrimination than LBM.\nThe slight effect of decreasing the size of the layers in the pose subnetwork of EBM can be observed from the results in Table 1 and 3."}, {"heading": "9 CONCLUSION", "text": "This paper is the first exploration of using CNNs for object pose estimation. We present our analysis and comparison of CNN models with the goal of performing both efficient object categorization and pose estimation. Despite the dichotomy in categorization and pose estimation, we show how CNNs can be adapted, by novel means introduced in this paper, to simultaneously solve both tasks. We make key observations about the intrinsics of CNNs in their ability to represent pose. We quantitatively analyze the models on two large challenging datasets with extensive experiments and achieve better than state-of-the-art accuracy on both datasets."}, {"heading": "Appendices", "text": "A k-NN RESULTS"}, {"heading": "A.1 PASCAL3D", "text": "KNN figures for categorization on Pascal3D dataset."}, {"heading": "A.2 RGBD DATASET", "text": "It is clear how CPM is very unstable for dense poses that exist in RGBD dataset."}, {"heading": "B LOCAL POSE MEASUREMENT ANALYSIS ON RGBD DATASET", "text": "We applied three local measurement analysis proposed in (Bakry et al., 2015) to analyze features against dense poses. For more details about the description of these measurements, please refer to (Bakry et al., 2015). The main property that these measurements quantified is how these representations align with the the circle manifold that represent the pose of the categories.\nAll the figures shows that EBM achieves the best behavior in untangling both the categorization and the pose branches. It is clear that CPM behaves the worst for pose estimation as we argued in the paper for several reasons.\n1. Z-EffectiveSV 90 (The lower the better) 2. TPS-RCond-CF-poly (The higher the better)\u201d 3. Nuclear Norm (The higher the better) 4. KPLS-Kernel Regression Error (The lower the better)"}, {"heading": "C COMPUTATIONAL ANALYSIS AND CONVERGENCE(MORE DETAILS)", "text": "The following figures show the loss/validation curves for the trained CNNs. The loss is shown per the batch being processed at each iteration(one training batch/iteration). The interesting behavior we notice in all the networks is that the categorization part converges very quickly, while the pose part takes sometime to converge. This is since, in all networks, the layers were initialized with the ImageNet categorization CNN. For the pose part, initialization for a categorization network might not be helpful especially for the top layers (e.g. FC6,FC7, and FC8), since they were trained for a different purpose that might be conflicting. Furthermore, training on a joint loss as in EBM and LBM positively affects the convergence as can be seen in figures 17 and 18. It is not hard to see that the Early Branching reduced the validation pose error significantly faster compared to the remaining models despite having much more parameters than many of the other models. Refer to section 4 for the number of parameters."}, {"heading": "D ERROR METRICS", "text": "The two metrics < 22.5 and < 45 used to evaluate the performance of pose estimation are the percentages of test samples that satisfy AE < 22.5\u25e6 and AE < 45\u25e6, respectively, where the Absolute Error (AE) is AE = |EstimatedAngle\u2212GroundTruth|). The AAAI pose accuracy (used extensively in the previous work we compare with) is equal to 1\u2212 [min(|\u03b8i \u2212 \u03b8j |, 2\u03c0 \u2212 |\u03b8i \u2212 \u03b8j |)/\u03c0]."}, {"heading": "E TRAINING PARAMETERS", "text": "The base learning rate is assigned 0.5 \u00d7 10\u22123. For fine-tuning, the learning rate of the randomly initialized parameters (e.g. FC8 parameters in PM) are assigned to be ten times higher than the learning rate of the parameters initialized from the pretrained CNN (e.g. Conv1 to Pool5 in all the models). The decay of the learning rate \u03b3 is 0.1. While training our CNNs, we drop the learning rate by a factor of \u03b3 every 5000 iterations. The momentum and the weight decay were assigned to 0.9 and 0.0001 respectively. Training images are randomly shuffled before feeding the CNN for training. The training batch size was 100 images.\nF INITIALIZATION AND MODELS\u2019 PARAMETERS The ImageNet CNN used in our paper (AlexNet) [16] has \u223c60 million parameters. In this section, we present how all the models were initialized in our experiments. Then, we analyze the number of parameters in the model for each of RGBD and Pascal3D datasets.\nF.1 INITIALIZATION"}, {"heading": "F.1.1 EBM MODEL", "text": "In EBM, we initialize all the convolutional layers by the convolution layer parameters of AlexNet. We initialize FC6 and FC7 of the category branch by the parameters of AlexNet model. The remaining layers were initialized randomly (i.e.FC6, FC7, and FC8 of the pose branch subnetwork, and FC8 of the category Branch subnetwork)."}, {"heading": "F.1.2 LBM MODEL", "text": "For LBM, we initialize all the layers by the pretrained AlexNet model for the convolution layers, FC6, and FC7. FC8 weights are initialized randomly."}, {"heading": "F.1.3 CPM MODEL", "text": "We initialize all the layers by the pretrained AlexNet model for the convolution layers, FC6, and FC7. We initialized FC8 parameters randomly."}, {"heading": "F.1.4 PM MODEL", "text": "Since there are two separate models, one for category and one for Pose. We initialize all the layers by the pretrained AlexNet model for the convolution layers, FC6, and FC7. We initialized FC8 parameters randomly."}, {"heading": "F.2 NUMBER OF MODEL PARAMETERS FOR RGBD DATASET", "text": ""}, {"heading": "F.2.1 EBM MODEL", "text": "EBM Model has 111,654,944 parameters. These are as follows starting from the input layer (number of filters x filter width x filter height x number of channels): 96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + Pool5FC6 (pose) 9216x4096, FC6(pose)-FC7(pose) 4096x4096, FC7(pose)-FC8(pose) 4096x16 Pool5-FC6(category) 9216x4096, FC6(category)-FC7(category) 4096x4096, FC7(category)FC8(category) 4096x51."}, {"heading": "F.2.2 LBM MODEL", "text": "LBM has 57,133,088 params = convolution-layers\u2019 parameters (96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + 9216x4096 + 4096x4096 + (fc8-cat) 4096x51 + (fc8-pose) 4096x16. These are organized as (number of filters x filter width x filter height x number of channels) and starting from the input layer."}, {"heading": "F.2.3 CPM MODEL", "text": "CPM has 60,200,992 params = convolution-layers\u2019 parameters (96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + 9216x4096 + 4096x4096 + (fc8-cat and pose) 4096x51x16. These are organized as (number of filters x filter width x filter height x number of channels) and starting from the input layer."}, {"heading": "F.2.4 PM MODEL", "text": "PM has 113,991,744 parameters (56,924,192 for pose and 57,067,552 for category). These are shown below as (number of filters x filter width x filter height x number of channels) and starting from the input layer.\nPM Model Pose Parameters: The 56,924,192 pose parameters comes from convolution-layers\u2019 parameters (96x11x11x3+ 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + fully connected layers\u2019 parameters (9216x4096 + 4096x4096 + 4096x16).\nPM Model Category Parameters: The 57,067,552 category parameters comes from convolutionlayers\u2019 parameters (96x11x11x3+ 256x5x548 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + fully connected layers\u2019 parameters (9216x4096, + 4096x4096 + 4096x51)."}, {"heading": "F.3 NUMBER OF MODEL PARAMETERS FOR PASCAL3D DATASET", "text": ""}, {"heading": "F.3.1 EBM MODEL", "text": "EBM Model 111,495,200 params = convolution layers\u2019 parameters (96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + Pool5-FC6 (pose) 9216x4096, FC6(pose)FC7(pose) 4096x4096, FC7(pose)-FC8(pose) 4096x16 Pool5-FC6(category) 9216x4096, FC6(category)-FC7(category) 4096x4096, FC7(pose)-FC8(pos) 4096x16. These are organized as (number of filters x filter width x filter height x number of channels) and starting from the input layer."}, {"heading": "F.3.2 LBM MODEL", "text": "LBM has 56,969,248 params = convolution layers\u2019 parameters (96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192+ 256x3x3x192)+ 9216x4096 + 4096x4096 + (fc8-cat) 4096x11 + (fc8-pose) 4096x16. These are organized as (number of filters x filter width x filter height x number of channels) and starting from the input layer."}, {"heading": "F.3.3 CPM MODEL", "text": "CPM has 57,579,552 params = convolution layers\u2019 parameters (96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + 9216x4096 + 4096x4096 + (fc8-cat and pose) 4096x11x16. These are organized as (number of filters x filter width x filter height x number of channels) and starting from the input layer."}, {"heading": "F.3.4 PM MODEL", "text": "PM has 113,827,904 parameters (56,924,192 for pose and 56,903,712 for category. These are shown below as (number of filters x filter width x filter height x number of channels) and starting from the input layer.\nPM Model Pose Parameters: The 56,924,192 pose parameters comes from convolution layers\u2019 parameters (96x11x11x3+ 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + fully connected layers\u2019 parameters (9216x4096, + 4096x4096 + 4096x16).\nPM Model Category Parameters: The 56,903,712 category parameters comes from convolution layers\u2019 parameters (96x11x11x3 + 256x5x5x48 + 384x3x3x256 + 384x3x3x192 + 256x3x3x192) + fully connected layers\u2019 parameters (9216x4096, + 4096x4096 + 4096x11)."}, {"heading": "G EFFECT OF LOSS FUNCTION WEIGHTS ON EBM MODEL", "text": "We found that changing the weights for EBM slightly affected the performance; see table 4. Our intuition behind this behavior is that EBM splits into separate parameters starting from Pool5, which makes each of the pose and the category have some independent parameters (FC6,FC7,FC8) in addition to the shared parameters (Conv1 to Pool5).\nSince \u03bb1 = 1 , \u03bb2 = 1 is slightly better than others, we performed all of our Model 5 experiments in the paper with this setting."}], "references": [{"title": "Untangling object-view manifold for multiview recognition and pose estimation", "author": ["Amr Bakry", "Ahmed Elgammal"], "venue": null, "citeRegEx": "Bakry and Elgammal.,? \\Q2014\\E", "shortCiteRegEx": "Bakry and Elgammal.", "year": 2014}, {"title": "Digging deep into the layers of cnns: In search of how cnns achieve view invariance", "author": ["Amr Bakry", "Mohamed Elhoseiny", "Tarek El-Gaaly", "Ahmed Elgammal"], "venue": "arXiv preprint arXiv:1508.01983,", "citeRegEx": "Bakry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bakry et al\\.", "year": 2015}, {"title": "Joint object recognition and pose estimation using a nonlinear view-invariant latent generative model", "author": ["Amr Bakry", "Tarek El-Gaaly", "Mohamed Elhoseiny", "Ahmed Elgammal"], "venue": "To appear in IEEE Winter Conference on Applications of Computer Vision (WACV)", "citeRegEx": "Bakry et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bakry et al\\.", "year": 2016}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["Ken Chatfield", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "CoRR, abs/1405.3531,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "A similarity-based aspect-graph approach to 3D object recognition", "author": ["CM Cyr", "BB Kimia"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Cyr and Kimia.,? \\Q2004\\E", "shortCiteRegEx": "Cyr and Kimia.", "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs"], "venue": null, "citeRegEx": "Dalal and Triggs.,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Rgbd object pose recognition using local-global multi-kernel regression", "author": ["Tarek El-Gaaly", "Marwan Torki", "Ahmed Elgammal", "Maneesh Singh"], "venue": null, "citeRegEx": "El.Gaaly et al\\.,? \\Q2012\\E", "shortCiteRegEx": "El.Gaaly et al\\.", "year": 2012}, {"title": "The pascal visual object classes (VOC) challenge", "author": ["Mark Everingham", "Luc Van Gool", "C.K.I. Williams", "J. Winn", "Andrew Zisserman"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Gregory S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross B. Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "CoRR, abs/1311.2524,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "R-cnns for pose estimation and action detection", "author": ["Georgia Gkioxari", "Bharath Hariharan", "Ross B. Girshick", "Jitendra Malik"], "venue": "CoRR, abs/1406.5212,", "citeRegEx": "Gkioxari et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gkioxari et al\\.", "year": 2014}, {"title": "Recognition and localization of overlapping parts from sparse data in two and three dimensions", "author": ["WJEL Grimson", "T Lozano-Perez"], "venue": "In Robotics and Automation. Proceedings", "citeRegEx": "Grimson and Lozano.Perez.,? \\Q1985\\E", "shortCiteRegEx": "Grimson and Lozano.Perez.", "year": 1985}, {"title": "Support vector regression machines", "author": ["H Drucker", "CJC Burges", "L Kaufman", "A Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Drucker et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Drucker et al\\.", "year": 1996}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A large-scale hierarchical multi-view rgb-d object dataset", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Lai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2011}, {"title": "A scalable tree-based approach for joint object and pose recognition", "author": ["Kevin Lai", "Liefeng Bo", "Xiaofeng Ren", "Dieter Fox"], "venue": "In AAAI,", "citeRegEx": "Lai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2011}, {"title": "Geometric hashing: A general and efficient model-based recognition", "author": ["Y. Lamdan", "H. Wolfson"], "venue": null, "citeRegEx": "Lamdan and Wolfson.,? \\Q1988\\E", "shortCiteRegEx": "Lamdan and Wolfson.", "year": 1988}, {"title": "Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network", "author": ["Sijin LI", "Zhi-Qiang Liu", "Antoni B. Chan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,", "citeRegEx": "LI et al\\.,? \\Q2014\\E", "shortCiteRegEx": "LI et al\\.", "year": 2014}, {"title": "Three-dimensional object recognition from single two-dimensional images", "author": ["David G Lowe"], "venue": "Artificial intelligence,", "citeRegEx": "Lowe.,? \\Q1987\\E", "shortCiteRegEx": "Lowe.", "year": 1987}, {"title": "Robust object pose estimation via statistical manifold modeling", "author": ["Liang Mei", "Jingen Liu", "Alfred Hero", "Silvio Savarese"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Mei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2011}, {"title": "From contours to 3d object detection and pose estimation", "author": ["Nadia Payet", "Sinisa Todorovic"], "venue": "In ICCV,", "citeRegEx": "Payet and Todorovic.,? \\Q2011\\E", "shortCiteRegEx": "Payet and Todorovic.", "year": 2011}, {"title": "Teaching 3d geometry to deformable part models", "author": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Pepik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pepik et al\\.", "year": 2012}, {"title": "What is holding back convnets for detection? CoRR, abs/1508.02844, 2015a", "author": ["Bojan Pepik", "Rodrigo Benenson", "Tobias Ritschel", "Bernt Schiele"], "venue": "URL http://arxiv.org/abs/1508", "citeRegEx": "Pepik et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pepik et al\\.", "year": 2015}, {"title": "3d object class detection", "author": ["Bojan Pepik", "Michael Stark", "Peter V. Gehler", "Tobias Ritschel", "Bernt Schiele"], "venue": "in the wild. CoRR,", "citeRegEx": "Pepik et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pepik et al\\.", "year": 2015}, {"title": "Deep convolutional neural networks for efficient pose estimation in gesture videos", "author": ["Tomas Pfister", "Karen Simonyan", "James Charles", "Andrew Zisserman"], "venue": "In Asian Conference on Computer Vision (ACCV),", "citeRegEx": "Pfister et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pfister et al\\.", "year": 2014}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2005\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2005}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Li FeiFei"], "venue": "CoRR, abs/1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "3d generic object categorization, localization and pose estimation", "author": ["S. Savarese", "Li Fei-Fei"], "venue": null, "citeRegEx": "Savarese and Fei.Fei.,? \\Q2007\\E", "shortCiteRegEx": "Savarese and Fei.Fei.", "year": 2007}, {"title": "View synthesis for recognizing unseen poses of object", "author": ["S. Savarese", "F.F. Li"], "venue": "classes. pp. III: 602\u2013615,", "citeRegEx": "Savarese and Li.,? \\Q2008\\E", "shortCiteRegEx": "Savarese and Li.", "year": 2008}, {"title": "Multi-view Object Categorization and Pose Estimation", "author": ["Silvio Savarese", "Li Fei-fei"], "venue": "In Computer Vision, SCI 285. Springer-Verlag Berlin Heidelberg,", "citeRegEx": "Savarese and Fei.fei.,? \\Q2010\\E", "shortCiteRegEx": "Savarese and Fei.fei.", "year": 2010}, {"title": "Learning an object class representation on a continuous viewsphere", "author": ["Johannes Schels", "Joerg Liebelt", "Rainer Lienhart"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Schels et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schels et al\\.", "year": 2012}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Finite-resolution aspect graphs of polyhedral objects", "author": ["Ilan Shimshoni", "Jean Ponce"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shimshoni and Ponce.,? \\Q1997\\E", "shortCiteRegEx": "Shimshoni and Ponce.", "year": 1997}, {"title": "Regression from local features for viewpoint and pose estimation", "author": ["M. Torki", "A. Elgammal"], "venue": null, "citeRegEx": "Torki and Elgammal.,? \\Q2011\\E", "shortCiteRegEx": "Torki and Elgammal.", "year": 2011}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["Alexander Toshev", "Christian Szegedy"], "venue": "CoRR, abs/1312.4659,", "citeRegEx": "Toshev and Szegedy.,? \\Q2013\\E", "shortCiteRegEx": "Toshev and Szegedy.", "year": 2013}, {"title": "Viewpoints and keypoints", "author": ["Shubham Tulsiani", "Jitendra Malik"], "venue": "CoRR, abs/1411.6067,", "citeRegEx": "Tulsiani and Malik.,? \\Q2014\\E", "shortCiteRegEx": "Tulsiani and Malik.", "year": 2014}, {"title": "Beyond pascal: A benchmark for 3d object detection in the wild", "author": ["Yu Xiang", "Roozbeh Mottaghi", "Silvio Savarese"], "venue": "In IEEE Winter Conference on Applications of Computer Vision (WACV),", "citeRegEx": "Xiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "ArXiv e-prints,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "Zeiler and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2013}, {"title": "Joint object and pose recognition using homeomorphic manifold analysis", "author": ["Haopeng Zhang", "Tarek El-Gaaly", "Ahmed Elgammal"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Joint object and pose recognition using homeomorphic manifold analysis", "author": ["Haopeng Zhang", "Tarek El-Gaaly", "Ahmed Elgammal", "Zhiguo Jiang"], "venue": "In AAAI,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Factorization of view-object manifolds for joint object recognition and pose estimation", "author": ["Haopeng Zhang", "Tarek El-Gaaly", "Ahmed Elgammal", "Zhiguo Jiang"], "venue": "arXiv preprint arXiv:1503.06813,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Traditional instance-based 3D pose estimation approaches solve the instance-recognition and pose estimation problems simultaneously, given model bases of instances in 2D or 3D (Grimson & Lozano-Perez, 1985; Lamdan & Wolfson, 1988; Lowe, 1987; Shimshoni & Ponce, 1997).", "startOffset": 176, "endOffset": 267}, {"referenceID": 20, "context": "Most recent object pose estimation approaches solve the problem within the detection process, where category-specific object detectors that encode part geometry are trained (Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012).", "startOffset": 173, "endOffset": 304}, {"referenceID": 31, "context": "Most recent object pose estimation approaches solve the problem within the detection process, where category-specific object detectors that encode part geometry are trained (Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012).", "startOffset": 173, "endOffset": 304}, {"referenceID": 22, "context": "Most recent object pose estimation approaches solve the problem within the detection process, where category-specific object detectors that encode part geometry are trained (Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012).", "startOffset": 173, "endOffset": 304}, {"referenceID": 14, "context": "The impressive results of Convolutional Neural Networks (CNNs) in tasks of categorizations (Krizhevsky et al., 2012) and detection (Sermanet et al.", "startOffset": 91, "endOffset": 116}, {"referenceID": 32, "context": ", 2012) and detection (Sermanet et al., 2013; Girshick et al., 2013) motivated many researchers to explore their applicability in different vision tasks.", "startOffset": 22, "endOffset": 68}, {"referenceID": 10, "context": ", 2012) and detection (Sermanet et al., 2013; Girshick et al., 2013) motivated many researchers to explore their applicability in different vision tasks.", "startOffset": 22, "endOffset": 68}, {"referenceID": 9, "context": "categorization) and then use the representation of higher layers as features for another task (Frome et al., 2013; Donahue et al., 2013; Zeiler & Fergus, 2013).", "startOffset": 94, "endOffset": 159}, {"referenceID": 6, "context": "categorization) and then use the representation of higher layers as features for another task (Frome et al., 2013; Donahue et al., 2013; Zeiler & Fergus, 2013).", "startOffset": 94, "endOffset": 159}, {"referenceID": 38, "context": "As pointed out by (Yosinski et al., 2014), this process is useful when the target task has significantly smaller training data than what is needed to train the model.", "startOffset": 18, "endOffset": 41}, {"referenceID": 9, "context": "Within the CNN literature, typically the layers up until FC7 (which is the last layer before the output layer) are used for that purpose (Frome et al., 2013).", "startOffset": 137, "endOffset": 157}, {"referenceID": 18, "context": "Although fundamentally different to object pose estimation, some research has explored using CNNs to recognize human pose (Toshev & Szegedy, 2013; LI et al., 2014; Pfister et al., 2014).", "startOffset": 122, "endOffset": 185}, {"referenceID": 25, "context": "Although fundamentally different to object pose estimation, some research has explored using CNNs to recognize human pose (Toshev & Szegedy, 2013; LI et al., 2014; Pfister et al., 2014).", "startOffset": 122, "endOffset": 185}, {"referenceID": 11, "context": "Recently (Gkioxari et al., 2014) proposed joint optimization on human pose and activity.", "startOffset": 9, "endOffset": 32}, {"referenceID": 38, "context": "(Yosinski et al., 2014; Zeiler & Fergus, 2013; Chatfield et al., 2014)).", "startOffset": 0, "endOffset": 70}, {"referenceID": 3, "context": "(Yosinski et al., 2014; Zeiler & Fergus, 2013; Chatfield et al., 2014)).", "startOffset": 0, "endOffset": 70}, {"referenceID": 37, "context": "(Tulsiani & Malik, 2014; Pepik et al., 2015b; Xiang et al., 2014; Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012; Lai et al., 2011b)).", "startOffset": 0, "endOffset": 215}, {"referenceID": 20, "context": "(Tulsiani & Malik, 2014; Pepik et al., 2015b; Xiang et al., 2014; Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012; Lai et al., 2011b)).", "startOffset": 0, "endOffset": 215}, {"referenceID": 31, "context": "(Tulsiani & Malik, 2014; Pepik et al., 2015b; Xiang et al., 2014; Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012; Lai et al., 2011b)).", "startOffset": 0, "endOffset": 215}, {"referenceID": 22, "context": "(Tulsiani & Malik, 2014; Pepik et al., 2015b; Xiang et al., 2014; Savarese & Fei-Fei, 2007; Savarese & Li, 2008; Mei et al., 2011; Payet & Todorovic, 2011; Schels et al., 2012; Pepik et al., 2012; Lai et al., 2011b)).", "startOffset": 0, "endOffset": 215}, {"referenceID": 14, "context": "3 MOTIVATION The first question we pose in this paper is how good are pre-trained representations of different CNN layers, without fine-tuning, for the task of pose estimation? To answer this we analyzed a state-ofthe-art CNN trained on ImageNet (Krizhevsky et al., 2012) by testing it on dense multi-view images from the RGBD dataset (Lai et al.", "startOffset": 246, "endOffset": 271}, {"referenceID": 1, "context": "This can be found in (Bakry et al., 2015).", "startOffset": 21, "endOffset": 41}, {"referenceID": 14, "context": "4 ANALYZED MODELS We used a state-of-the-art CNN built by Krizhevsky et al. (2012) as our baseline network in our experiments (winner of the LSVRC-2012 Imagenet Challenge Russakovsky et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 14, "context": "4 ANALYZED MODELS We used a state-of-the-art CNN built by Krizhevsky et al. (2012) as our baseline network in our experiments (winner of the LSVRC-2012 Imagenet Challenge Russakovsky et al. (2014)).", "startOffset": 58, "endOffset": 197}, {"referenceID": 11, "context": "The only work that has done something similar to this is the work byGkioxari et al. (2014). All losses are optimized by the multinomial logistic regression objective, similar to Krizhevsky et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 11, "context": "The only work that has done something similar to this is the work byGkioxari et al. (2014). All losses are optimized by the multinomial logistic regression objective, similar to Krizhevsky et al. (2012) (Softmax loss).", "startOffset": 68, "endOffset": 203}, {"referenceID": 14, "context": "One of the largest and challenging multi-view datasets available is the RGB-D dataset Lai et al. (2011a). It consists of 300 tabletop object instances over 51 different categories.", "startOffset": 86, "endOffset": 105}, {"referenceID": 14, "context": "One of the largest and challenging multi-view datasets available is the RGB-D dataset Lai et al. (2011a). It consists of 300 tabletop object instances over 51 different categories. Images are captured of objects rotating on a turn-table, resulting in dense views of each object. The camera is positioned at three different heights with elevation angles: 30\u25e6, 45\u25e6 and 60\u25e6. In previous approaches the middle height (45\u25e6) is left out for testing Lai et al. (2011b); Zhang et al.", "startOffset": 86, "endOffset": 462}, {"referenceID": 14, "context": "One of the largest and challenging multi-view datasets available is the RGB-D dataset Lai et al. (2011a). It consists of 300 tabletop object instances over 51 different categories. Images are captured of objects rotating on a turn-table, resulting in dense views of each object. The camera is positioned at three different heights with elevation angles: 30\u25e6, 45\u25e6 and 60\u25e6. In previous approaches the middle height (45\u25e6) is left out for testing Lai et al. (2011b); Zhang et al. (2013a); Bakry & Elgammal (2014); El-Gaaly et al.", "startOffset": 86, "endOffset": 484}, {"referenceID": 14, "context": "One of the largest and challenging multi-view datasets available is the RGB-D dataset Lai et al. (2011a). It consists of 300 tabletop object instances over 51 different categories. Images are captured of objects rotating on a turn-table, resulting in dense views of each object. The camera is positioned at three different heights with elevation angles: 30\u25e6, 45\u25e6 and 60\u25e6. In previous approaches the middle height (45\u25e6) is left out for testing Lai et al. (2011b); Zhang et al. (2013a); Bakry & Elgammal (2014); El-Gaaly et al.", "startOffset": 86, "endOffset": 509}, {"referenceID": 7, "context": "(2013a); Bakry & Elgammal (2014); El-Gaaly et al. (2012). This means that object instances at test time have been seen before from other heights during training.", "startOffset": 34, "endOffset": 57}, {"referenceID": 35, "context": "We experiment on the recently released challenging dataset of multi-view images, called Pascal3D+ Xiang et al. (2014). Pascal3D+ is very challenging because it consists of images in the wild, in other words, images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses.", "startOffset": 98, "endOffset": 118}, {"referenceID": 8, "context": "Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset Everingham et al. (2010). These objects are annotated with pose information (azimuth, elevation and distance to camera).", "startOffset": 92, "endOffset": 117}, {"referenceID": 8, "context": "Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset Everingham et al. (2010). These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset Russakovsky et al. (2014). The bottle category is omitted in state-of-the-art results.", "startOffset": 92, "endOffset": 330}, {"referenceID": 1, "context": "Local Pose Measurement Analysis: In Appendix B, we further analyzed four local measurement pose analysis proposed in (Bakry et al., 2015) to analyze layers of the five models we study against the pose manifold.", "startOffset": 117, "endOffset": 137}, {"referenceID": 14, "context": "Similar to (Krizhevsky et al., 2012), we optimized it by maximizing the average of the log-probability of the correct label under the prediction distribution across training cases.", "startOffset": 11, "endOffset": 36}, {"referenceID": 42, "context": "We compare the accuracy achieved by our models with state-of-the-art results by (Zhang et al., 2015; Xiang et al., 2014; Pepik et al., 2015b; Tulsiani & Malik, 2014).", "startOffset": 80, "endOffset": 165}, {"referenceID": 37, "context": "We compare the accuracy achieved by our models with state-of-the-art results by (Zhang et al., 2015; Xiang et al., 2014; Pepik et al., 2015b; Tulsiani & Malik, 2014).", "startOffset": 80, "endOffset": 165}, {"referenceID": 42, "context": "On the other hand (Zhang et al., 2015) solve just pose estimation, assuming that the object categories are known.", "startOffset": 18, "endOffset": 38}, {"referenceID": 42, "context": "In our case we are jointly solving both category recognition and pose estimation, which can be considered a harder problem than that of (Zhang et al., 2015) and (Xiang et al.", "startOffset": 136, "endOffset": 156}, {"referenceID": 37, "context": ", 2015) and (Xiang et al., 2014).", "startOffset": 12, "endOffset": 32}, {"referenceID": 42, "context": "For the sake of this comparison, we computed the pose performance using the metrics applied in (Zhang et al., 2015).", "startOffset": 95, "endOffset": 115}, {"referenceID": 42, "context": "69% improvement of our method over (Zhang et al., 2015) (the best performing previous approach) in pose < 22.", "startOffset": 35, "endOffset": 55}, {"referenceID": 37, "context": "It is important to note that comparing to (Xiang et al., 2014; Tulsiani & Malik, 2014; Pepik et al., 2015b) is slightly unfair because these works solve for detection and pose simultaneously, while we do not solve detection.", "startOffset": 42, "endOffset": 107}, {"referenceID": 22, "context": ", 2014; Pepik et al., 2015b; Tulsiani & Malik, 2014). It must be noted here that we are solving slightly different problems to some of these approaches. In Xiang et al. (2014), the authors solve detection and pose estimation, assuming correct detection.", "startOffset": 8, "endOffset": 176}, {"referenceID": 2, "context": "57 (Bakry et al., 2016) 85.", "startOffset": 3, "endOffset": 23}, {"referenceID": 37, "context": "Table 3: Pascal3D dataset (Xiang et al., 2014): Comparison with state-of-the-art approaches on category recognition and pose estimation.", "startOffset": 26, "endOffset": 46}, {"referenceID": 42, "context": "The AAAI pose metric is the performance metric used in(Zhang et al., 2015; Lai et al., 2011b; Zhang et al., 2013a).", "startOffset": 54, "endOffset": 114}, {"referenceID": 37, "context": "5) Pose % (error < 45) Pose AAAI metric (Xiang et al., 2014) 15.", "startOffset": 40, "endOffset": 60}, {"referenceID": 22, "context": "70 (Pepik et al., 2012) 17.", "startOffset": 3, "endOffset": 23}, {"referenceID": 42, "context": "(Zhang et al., 2015) 44.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "In the task of Object Recognition, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose regression using these approaches has received relatively much less attention. In this paper we show how deep architectures, specifically Convolutional Neural Networks (CNN), can be adapted to the task of simultaneous categorization and pose estimation of objects. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations of CNNs represent object pose information and how this contradicts object category representations. We extensively experiment on two recent large and challenging multi-view datasets. Our models achieve better than state-of-the-art performance on both datasets.", "creator": "LaTeX with hyperref package"}}}