{"id": "1511.06276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Faster method for Deep Belief Network based Object classification using DWT", "abstract": "A Deep Belief Network (DBN) requires large, multiple hidden layers with a high number of hidden units to learn good features from the raw pixels of large images, which implies more training time and computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. Low-resolution images obtained after using DWT are used to train multiple DBNs. Results from these DBNs are combined using a weighted matching algorithm, and the performance of this method proves competent and faster than traditional DBNs.", "histories": [["v1", "Thu, 19 Nov 2015 17:41:08 GMT  (407kb)", "http://arxiv.org/abs/1511.06276v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["saurabh sihag", "pranab kumar dutta"], "accepted": false, "id": "1511.06276"}, "pdf": {"name": "1511.06276.pdf", "metadata": {"source": "CRF", "title": "Faster method for Deep Belief Network based Object Classification using DWT", "authors": ["Saurabh Sihag", "Pranab Kr. Dutta"], "emails": [], "sections": [{"heading": null, "text": "Faster method for Deep Belief Network based Object Classification using DWT Saurabh Sihag and Pranab Kr. Dutta\nA Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs.\nIntroduction: Deep learning has found applications in various areas of information processing such as audio processing, natural language modeling and processing, object recognition and computer vision because of the high accuracy of its models. Deep Belief Networks (DBNs) learn complex function mapping from input to output directly from raw pixels of data. DBN training, which includes the pre-training and fine-tuning processes, in conventional central processing units (CPU) platforms is computationally expensive because multiple hidden layers with high number of hidden units are required to train the mapping function for high dimensional raw pixel data. Large amounts of training data are needed to learn the parameters of such a network for preventing overfitting. Execution of Deep Learning algorithms can be made faster by reducing the dimensionality of the data, as illustrated in recent years. One such approach is presented in [1] which illustrates the use of Discrete Cosine Transform (DCT) for image classification. Wavelet transform, rough set theory, and artificial neural networks are combined together to form a hybrid image classification method in [2]. Multiresolution image features have been utilized for object detection in [3]. In this letter, a new approach is proposed for image classification that integrates discrete wavelet transform (DWT) and DBNs. The idea is to break up an input image into several low resolution sub-band images using 2 level DWT and train a separate DBN for each sub-band image. These DBNs use less number of hidden units and require less training time and memory individually as compared to that of a traditional DBN that is trained using raw pixels from the input data. The classification results obtained from each DBN are processed using a weighted voting algorithm to achieve the final result.\nTheoretical Description: Restricted Boltzmann machines (RBMs) [4] are probabilistic models that are used as nonlinear unsupervised feature learners, consisting of a set of binary hidden units h, a set of (binary or real-valued) visible units v, and a weight matrix W associated with the connections between the two layers. Joint probability function for visible and hidden units is defined as P(v,h) = exp(\u2212 ( , \u210e)) where Z is the partition function and E(v,h) is the energy function. For an RBM with real valued visible units, the energy function is defined as:\n( , \u210e) = \u2212 \u2212\n\u210e \u2212\n\u210e\nwhere vs and ht are the sth and tth units of v and h, bsv and bth are the biases associated with unit vs and ht, and wst is the weight associated with the connection between vs and ht. A deep belief network (DBN) consists of layers of RBMs, each of which contains a layer of visible units and a layer of hidden units. Two adjacent layers have a full set of connections between them, but no two units in the same layer are connected. An efficient algorithm for training deep belief networks has been proposed in [5]. Discrete Wavelet Transform (DWT) is used to decompose an original image into different sub-band images, with each sub-band image occupying low and high frequency bands.\nMethodology: A. Generating low resolution images from the input image Two level DWT is used to get sixteen sub-band images from the input image. Unlike the conventional DWT process, all the four sub-band\nimages obtained after first level of DWT are decomposed further to generate four sub-band images each or a total of sixteen sub-band images. Size of each image is one-sixteenth of the size of original image.\nB. Formation of Deep Belief Networks The sixteen low resolution images obtained from the previous step are used as the training features for DBNs. Sixteen DBNs are formed corresponding to each low resolution image obtained from the previous step. Each DBN has same number of hidden layers as well as hidden units.\nC. Training the DBNs Initially each DBN is pre-trained using Contrastive Divergence, as specified in [5]. This is the unsupervised training step. In the second step, back Propagation is used for fine-tuning of the weights. The pre-training step helps to achieve faster convergence at the finetuning stage. D. Testing and calculation of results After training each DBN, the accuracy of each DBNi is determined. Each DBN is associated with a weight wi where wi = 1 \u2212 %&'()* +, '- ./0 -,-)1 -'02) - *0- - 2 ) 3+ 0/ &'()* +, -'02) - ) *0- - 2 ) Thus, higher weights are assigned to the DBN with higher accuracy. The final classification result for a test image is obtained by voting using the weights of each DBN. The algorithm for calculating the weighted votes for each image is given below.\nAlgorithm1: Weighted voting_______________________________ Input: Number of classes, n_________________________________ Output: Predicted value, p __________________________________ 1. Initialize t = zero matrix (size: n x 1) 2. for i = 1,\u2026.n 3. for j = 1,2\u2026\u2026.16 4. if i is equal to predicted class 5. t(i) = t(i) + wi 6. p = max(t)\nThe index i with maximum value in t is taken as the predicted value. The above steps are visualized in Fig. 1 and Fig.2. Experimental Results and Discussion: In order to evaluate the proposed approach, experiments are performed on two standard datasets. The\nStage-1\nFigure 1 Preprocessing of Training dataset and training of DBNs\nTraining Images\n2 level DWT\nLR image1\nLR\nimage2\nLR image15\nLR image16\nTrain DBN1\nTrain DBN2\nTrain DBN15\nTrain DBN16\nCalculation of accuracy & weight\nw1\nCalculation of accuracy &weight\nw2\nCalculation of accuracy & weight\nw15 Calculation of\naccuracy & weight w16\nStage-2\nFigure 2 Prediction of Final Result using test Images\nTest Images 2 level DWT LR image1 LR image2 LR image15 LR image16 DBN1 DBN2 DBN15 DBN16 Final Result Prediction using Weighted voting\nnumber of hidden units and learning rate of DBNs are decided empirically.\n1. COIL-20 Dataset Columbia Object Image Library (COIL-20) [6] consists of gray-scale images of 20 objects, as shown in Figure 3.\nSize of each image is 128 x 128. Each image is down-sampled to a size of 64 x 64 and 2 level DWT is taken subsequently. This results in sixteen images of size 16 x 16. Each of these images is reshaped to 1 x 256 and passed on as inputs to train their corresponding DBNs. The results for this dataset are compared with that of the wavelet transform based method adopted in [2], in which 5 objects are chosen at random. These 5 categories are evaluated at 75/25 hold out ratio (training images/test images) to compare the results obtained using the approach proposed in this letter and those specified in [2] as shown in Table 1. Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units\nResults on the whole dataset are compared with that of methods adopted in [7] and [8] for 70/30 hold out ratio (training images/test images) in Table-2. For whole dataset, Architecture of DBN: [40,20,20] Time taken to train each DBN: 3.55 sec on Intel Core i5-4200U CPU @1.60 GHz\n2. USPS Dataset The USPS data set [9] consists of images of handwritten digits, collected from mail envelopes. Each image is of size 16 x 16. The dataset is divided into a training set (consisting of 7291 images) and a testing set (consisting of 2007 images). Some random sample images from this dataset are shown in Fig. 4.\nResults for other DBN based methods [10] are given in Table 4\nTherefore, from the results in Table 3 and Table 4, it can be concluded that the approach in this letter can give faster results compared to other DBN based methods listed in [10], albeit with a little trade off in accuracy. Human error rate for USPS dataset is 2.5%.\nConclusion: Results on two datasets: COIL-20 and USPS show that this approach can achieve competent results with much less computation time because of reduction in size of input data to DBNs and less number of weight parameters. Since the architecture of DBNs trained using this approach is very small as compared to that of DBNs trained on raw pixels, this system can also be extended to image recognition problems with high resolution. If all the DBNs can be executed in parallel, this approach can be used to train DBNs and test them on a moderately powerful processor (such as the one used in this paper) in a matter of few minutes even for large datasets, with competent accuracy.\nSaurabh Sihag and Pranab Kr Dutta (Department of Electrical Engineering, IIT Kharagpur, Kharagpur-721302, India )\nE-mail: pkd@ee.iitkgp.ernet.in"}], "references": [{"title": "High speed deep networks based on Discrete Cosine Transformation,", "author": ["Xiaoyi Zou", "Xiangmin Xu", "Chunmei Qing", "Xiaofen Xing"], "venue": "IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Rough-neural image classification using wavelet transform,", "author": ["Jun-Hai Zhai", "Xi-Zhao Wang", "Su-Fang Zhang"], "venue": "IEEE International Conference on Machine Learning and Cybernetics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "An introduction to restricted Boltzmann machines,", "author": ["Asja Fischer", "Christian Igel"], "venue": "Progress in Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Columbia object image library (COIL-20),", "author": ["SA Nene", "SK Nayar", "H Murase"], "venue": "Technical Report CUCS-005-96,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "A supervised non-linear dimensionality reduction approach for manifold learning,", "author": ["B Raducanu", "F Dornaika"], "venue": "Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Optimal Feature Extraction and Classification of Tensors via Matrix Product State Decomposition,", "author": ["J.A. Bengua", "H. N Phien", "H.D. Tuan"], "venue": "arXiv preprint arXiv:1503.00516,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Sparse penalty in deep belief networks: using the mixed norm constraint,", "author": ["X Halkias", "S Paris", "H Glotin"], "venue": "arXiv preprint arXiv:1301.3533,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One such approach is presented in [1] which illustrates the use of Discrete Cosine Transform (DCT) for image classification.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Wavelet transform, rough set theory, and artificial neural networks are combined together to form a hybrid image classification method in [2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Theoretical Description: Restricted Boltzmann machines (RBMs) [4] are probabilistic models that are used as nonlinear unsupervised feature learners, consisting of a set of binary hidden units h, a set of (binary or real-valued) visible units v, and a weight matrix W associated with the connections between the two layers.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "COIL-20 Dataset Columbia Object Image Library (COIL-20) [6] consists of gray-scale images of 20 objects, as shown in Figure 3.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "The results for this dataset are compared with that of the wavelet transform based method adopted in [2], in which 5 objects are chosen at random.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "These 5 categories are evaluated at 75/25 hold out ratio (training images/test images) to compare the results obtained using the approach proposed in this letter and those specified in [2] as shown in Table 1.", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units Table 1 (Comparison of the results mentioned in [2] and results obtained using the approach in this letter) Object ID Approach from [2] Our Approach", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units Table 1 (Comparison of the results mentioned in [2] and results obtained using the approach in this letter) Object ID Approach from [2] Our Approach", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units Table 1 (Comparison of the results mentioned in [2] and results obtained using the approach in this letter) Object ID Approach from [2] Our Approach", "startOffset": 261, "endOffset": 264}, {"referenceID": 4, "context": "Results on the whole dataset are compared with that of methods adopted in [7] and [8] for 70/30 hold out ratio (training images/test images) in Table-2.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Results on the whole dataset are compared with that of methods adopted in [7] and [8] for 70/30 hold out ratio (training images/test images) in Table-2.", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Results for other DBN based methods [10] are given in Table 4", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "Table 4 (Results on USPS dataset mentioned in [10]; CPU: AMD Opteron processor 8435, 2.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "Therefore, from the results in Table 3 and Table 4, it can be concluded that the approach in this letter can give faster results compared to other DBN based methods listed in [10], albeit with a little trade off in accuracy.", "startOffset": 175, "endOffset": 179}], "year": 2015, "abstractText": "A Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs.", "creator": "SPDF"}}}