{"id": "1511.06361", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Order-Embeddings of Images and Language", "abstract": "Hypernymia, textual entanglements, and captions can be considered special cases of a single visual-semantic hierarchy of words, sentences, and images. In this essay, we advocate explicit modeling of the partial order structure of this hierarchy. To this end, we introduce a general method for learning orderly representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches to hypernym prediction and caption retrieval.", "histories": [["v1", "Thu, 19 Nov 2015 20:56:14 GMT  (4288kb,D)", "http://arxiv.org/abs/1511.06361v1", null], ["v2", "Tue, 8 Dec 2015 21:19:30 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v2", null], ["v3", "Thu, 10 Dec 2015 04:32:53 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v3", null], ["v4", "Thu, 7 Jan 2016 04:58:08 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v4", null], ["v5", "Sun, 17 Jan 2016 03:08:20 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v5", null], ["v6", "Tue, 1 Mar 2016 08:23:50 GMT  (4290kb,D)", "http://arxiv.org/abs/1511.06361v6", "ICLR camera-ready version"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["ivan vendrov", "ryan kiros", "sanja fidler", "raquel urtasun"], "accepted": true, "id": "1511.06361"}, "pdf": {"name": "1511.06361.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "emails": ["vendrov@cs.toronto.edu", "rkiros@cs.toronto.edu", "fidler@cs.toronto.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Computer vision and natural language processing are becoming increasingly intertwined. Recent work in vision has moved beyond discriminating between a fixed set of object classes, to automatically generating open-ended lingual descriptions of images (Vinyals et al., 2015). Recent methods for natural language processing such as Young et al. (2014) learn the semantics of language by grounding it in the visual world. Looking to the future, autonomous artificial agents will need to jointly model vision and language in order to parse the visual world and communicate with people.\nBut what, precisely, is the relationship between images and the words or captions we use to describe them? We argue that it is akin to the hypernym relation between words, and textual entailment among phrases. In fact, all three relations can be seen as special cases of a partial order over images and language, which we refer to as the visual-semantic hierarchy. We refer the reader to Fig. 1 for an illustration. As a partial order, this relation is transitive: \u201cwoman walking her dog\u201d, \u201cwoman walking\u201d, \u201cperson walking\u201d, \u201cperson\u201d, and \u201centity\u201d are all valid descriptions of the rightmost image. Our goal in this work is to learn representations that respect this partial order structure.\nFigure 1: A slice of the visual-semantic hierarchy\nMost recent approaches to modeling the hypernym, entailment, and image-caption relations involve learning distributed representations or embeddings. This is a very powerful and general approach which maps the objects of interest\u2014words, phrases, images\u2014 to points in a high-dimensional vector space. One line of work, exemplified by Chopra et al. (2005) and first applied to the caption-image relationship by Socher et al. (2014), requires the mapping to be distance-preserving: semantically\nar X\niv :1\n51 1.\n06 36\n1v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nsimilar objects are mapped to points that are nearby in the embedding space. A symmetric distance measure such as Euclidean or cosine distance is typically used. Since the semantic hierarchy is an antisymmetric relation, we expect this approach to introduce systematic model error.\nOther approaches do not have such explicit constraints, learning a more-or-less general binary relation between the objects of interest, e.g. Bordes et al. (2011); Socher et al. (2013); Ma et al. (2015). Notably, no existing approach directly imposes the transitivity and antisymmetry of the partial order, leaving the model to induce these properties from data.\nIn contrast, we propose to exploit the partial order structure of the semantic hierarchy by learning a mapping which is not distance-preserving but order-preserving between the semantic hierarchy and a partial order over the embedding space. We call embeddings learned in this way order-embeddings. This idea can be integrated into existing relational learning methods simply by replacing the comparison operation with ours. By modifying existing methods in this way, we find that order-embeddings provide a marked improvement over the state-of-art for hypernymy prediction and caption-image retrieval, and near state-of-the-art for natural language inference.\nThis paper is structured as follows. We begin, in Section 2, by giving a unified mathematical treatment of our tasks, and describing the general approach of learning order-embeddings. In the next three sections we describe in detail the tasks we tackle, how we apply the order-embeddings idea to each of them, and the results we obtain. The tasks are hypernym prediction (Section 3), captionimage retrieval (Section 4), and textual entailment (Section 5).\nIn the supplementary material, we visualize novel vector regularities that emerge in our learned embeddings of images and language."}, {"heading": "2 LEARNING ORDER-EMBEDDINGS", "text": "To unify our treatment of various tasks, we introduce the problem of partial order completion. In partial order completion, we are given a set of positive examples P = {(u, v)} of ordered pairs drawn from a partially ordered set (X, X), and a set of negative examples N which we know to be unordered. Our goal is to predict whether an unseen pair (u\u2032, v\u2032) is ordered. Note that hypernym prediction, caption-image retrieval, and textual entailment are all special cases of this task, since they all involve classifying pairs of concepts in the (partially ordered) visual-semantic hierarchy.\nWe tackle this problem by learning a mapping from X into a partially ordered embedding space (Y, Y ). The idea is to predict the ordering of an unseen pair in X based on its ordering in the embedding space. This is possible only if the mapping satisfies the following crucial property: Definition 1. A function f : (X, X)\u2192 (Y, Y ) is an order-embedding if for all u, v \u2208 X ,\nu X v if and only if f(u) Y f(v)\nThis definition implies that each combination of embedding space Y , order Y , and orderembedding f determines a unique completion of our data as a partial order X . In the following, we first consider the choice of Y and Y , and then discuss how to find an appropriate f .\n2.1 THE REVERSED PRODUCT ORDER ON RN+\nThe choice of Y and Y is somewhat application-dependent. For the purpose of modeling the semantic hierarchy, our choices are narrowed by the following considerations.\nMuch of the expressive power of human language comes from abstraction and composition. For any two concepts, say \u201cdog\u201d and \u201ccat\u201d, we can name a concept that is an abstraction of the two, such as \u201cmammal\u201d, as well as a concept that composes the two, such as \u201cdog chasing cat\u201d. So, in order to represent the semantic hierarchy, we need to choose an order Y that is at least as rich\u2014every pair in Y should have a common ancestor and a common descendant.\nWe restrict ourselves to orders Y with a top element, which is above every other element in the order. In the semantic hierarchy, this element represents the most general possible concept; practically, it provides an anchor for the embedding.\nFinally, we choose the embedding space Y to be continuous in order to allow optimization with gradient-based methods.\nA natural choice that satisfies all three properties is the reversed product order on RN+ , defined by the conjunction of total orders on each coordinate:\nx y if and only if N\u2227 i=1 xi \u2265 yi (1)\nfor all vectors x, y with nonnegative coordinates. Note the reversal of direction: smaller coordinates imply higher position in the partial order. The origin is then the top element of the order, representing the most general concept.\nInstead of viewing our embeddings as single points x \u2208 RN+ , we can also view them as sets {y : x y}. The meaning of a word is then the union of all concepts of which it is a hypernym, and the meaning of a sentence is the union of all sentences that entail it. The visual-semantic hierarchy can then be seen as a special case of the subset relation, a connection also used by Young et al. (2014)."}, {"heading": "2.2 PENALIZING ORDER VIOLATIONS", "text": "Having fixed the embedding space and order, we now consider the problem of finding an orderembedding to this space. In practice, the order embedding condition (Definition 1) is too restrictive to impose as a hard constraint. Instead, we aim to find an approximate order-embedding: a mapping which violates the order-embedding condition, imposed as a soft constraint, as little as possible.\nMore precisely, we define a penalty that measures the degree to which a pair of points violates the product order. In particular, we define the penalty for an ordered pair (x, y) of points in RN+ as\nE(x, y) = ||max(0, y \u2212 x)||2 (2)\nCrucially, E(x, y) = 0 \u21d0\u21d2 x y according to the reversed product order; if the order is not satisfied, E(x, y) is positive. This effectively imposes a strong prior on the space of relations, encouraging our learned relation to satisfy the partial order properties of transitivity and antisymmetry. This penalty is key to our method. Throughout the remainder of the paper, we will use it where previous work has used symmetric distances or learned comparison operators.\nRecall that P and N are our positive and negative examples, respectively. Then, to learn an approximate order-embedding f , we could use a max-margin loss which encourages positive examples to have zero penalty, and negative examples to have penalty greater than a margin:\u2211\n(u,v)\u2208P\nE(f(u), f(v)) + \u2211\n(u\u2032,v\u2032)\u2208N\nmax{0, \u03b1\u2212 E(f(u\u2032), f(v\u2032))} (3)\nIn practice we are often not given negative examples, in which case this loss admits the trivial solution of mapping all objects to the same point. The best way of dealing with this problem depends on the application, so we will describe task-specific variations on this loss in the next several sections."}, {"heading": "3 HYPERNYM PREDICTION", "text": "To test the ability of our model to learn partial orders from incomplete data, our first task is to predict withheld hypernym pairs in WordNet (Miller, 1995). A hypernym pair is a pair of concepts where the first word is a specialization or an instance of the second, e.g., (woman, person) or (New York, city). Our setup differs significantly from previous work in that we use only the WordNet hierarchy as training data. The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors. Bordes et al. (2011) and Socher et al. (2013) also evaluate on the WordNet hierarchy, but they use other relations in WordNet as training data (and external linguistic data, in Socher\u2019s case). Additionally, the latter two consider only direct hypernyms, rather than the full, transitive hypernymy relation.\nPredicting the transitive hypernym relation is a better-defined problem because individual hypernym edges in WordNet vary dramatically in the degree of abstraction they require. For instance, (person, organism) is a direct hypernym pair, but it takes eight hypernym edges to get from cat to organism."}, {"heading": "3.1 LOSS FUNCTION", "text": "To apply order-embeddings to hypernymy, we follow the setup of Socher et al. (2013) in learning an N-dimensional vector for each concept in WordNet, but we replace their neural tensor network with our order-violation penalty defined in Eq. (2). Just like them, to generate negative examples we corrupt each hypernym pair by replacing one of the two concepts with a randomly chosen concept, and use these corrupted pairs as negative examples for both training and evaluation. We use their max-margin loss, which encourages the order-violation penalty to be zero for positive examples, and greater than a margin \u03b1 for negative examples:\u2211\n(u,v)\u2208WordNet\nE(f(u), f(v)) + max{0, \u03b1\u2212 E(f(u\u2032), f(v\u2032))} (4)\nwhere E is our order-violation penalty, and (u\u2032, v\u2032) is a corrupted version of (u, v). Since we learn an independent embedding for each concept, the mapping f is simply a lookup table."}, {"heading": "3.2 DATASET", "text": "The transitive closure of the WordNet hierarchy gives us 838073 edges between 82192 concepts in WordNet. Like Bordes et al. (2011), we randomly select 4000 edges for the test split, and another 4000 for the development set. Note that the majority of test set edges can be inferred simply by applying transitivity, giving us a strong baseline."}, {"heading": "3.3 DETAILS OF TRAINING", "text": "We learn a 50-dimensional nonnegative vector for each concept in WordNet using the max-margin objective (4) with margin \u03b1 = 1, sampling 500 true and 500 false hypernym pairs in each batch. We train for 30-50 epochs using the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.01 and early stopping on the validation set. During evaluation, we find the optimal classification threshold on the validation set, then apply it to the test set.\n3.4 RESULTS\nSince our setup is novel, there are no published numbers to compare to. We therefore compare three variants of our model to two baselines, with results shown in Table 1.\nThe transitive closure baseline involves no learning; it simply classifies hypernyms pairs as positive if they are in the transitive closure of the union of edges in the training and validation sets.\nThe word2gauss baseline evaluates the approach of Vilnis & McCallum (2015) to represent words as Gaussian densities rather than points in the embedding space. This allows a natural representation of hierarchies using the KL divergence. We used 50-dimensional diagonal Gaussian embeddings, trained for 200 epochs on a max-margin objective with margin 7, chosen by grid search1.\norder-embeddings (symmetric) is our full model, but using symmetric cosine distance instead of our asymmetric penalty.\norder-embeddings (bilinear) replaces our penalty with the bilinear model used by Socher et al. (2013).\norder-embeddings is our full model.\nOnly our full model can do better than the transitive baseline, showing the value of exploiting partial order structure in contrast to using symmetric similarity or learning a general binary relation as most previous work and our bilinear baseline do.\n1We used the code of http://github.com/seomoz/word2gauss"}, {"heading": "4 CAPTION-IMAGE RETRIEVAL", "text": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a). The task involves ranking a large dataset of images by relevance for a query caption (Image Retrieval), and ranking captions by relevance for a query image (Caption Retrieval). Given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score S(c, i) to be used at test time.\nMany modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015). While Karpathy & Li (2015) and Plummer et al. (2015) model a finer-grained alignment between regions in the image and segments of the caption, the similarity they use is still symmetric. An alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image (Vinyals et al., 2015; Mao et al., 2015) or using a multimodal CNN (Ma et al., 2015).\nIn contrast to these lines of work, we propose to treat the caption-image pairs as a two-level partial order, and let S(c, i) = \u2212E(fc(c), fi(i)) with E our order-violation penalty defined in Eq (2), and fc, fi are embedding functions from captions and images into RN+ ."}, {"heading": "4.1 LOSS FUNCTION", "text": "To facilitate comparison, we use the same pairwise ranking loss that Socher et al. (2014), Kiros et al. (2014) and Karpathy & Li (2015) have used on this task\u2014simply replacing their symmetric similarity measure with our asymmetric order-violation penalty. This loss functions encourages S(c, i) for ground truth caption-image pairs to be greater than that for all other pairs, by a margin:\u2211\n(c,i) (\u2211 c\u2032 max{0, \u03b1\u2212 S(c, i) + S(c\u2032, i)}+ \u2211 i\u2032 max{0, \u03b1\u2212 S(c, i) + S(c, i\u2032)} ) (5)\nwhere (c, i) is a ground truth caption-image pair, c\u2032 is a caption that does not describe i, and i\u2032 is an image that is not described by c."}, {"heading": "4.2 IMAGE AND CAPTION EMBEDDINGS", "text": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into RN+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value. Thus, to embed images, we use\nfi(i) = |Wi \u00b7 CNN(i)| (6) where Wi is a learned N \u00d7 4096 matrix, N being the dimensionality of the embedding space. CNN(i) is the same image feature used by Klein et al. (2015): we rescale images to have smallest side 256 pixels, we take 224 \u00d7 224 crops from the corners, center, and their horizontal reflections, run the 10 crops through the 19-layer VGG network of Simonyan & Zisserman (2015), and average their fc7 features.\nTo embed the captions, we use a recurrent neural net encoder with GRU activations (Cho et al., 2014), so fc(c) = |GRU(c)|, the absolute value of hidden state after processing the last word."}, {"heading": "4.3 DATASET", "text": "We evaluate on the Microsoft COCO dataset (Lin et al., 2014b), which has over 120,000 images, each with at least five human-annotated captions per image. This is by far the largest dataset commonly used for caption-image retrieval. We use the data splits of Karpathy & Li (2015) for training (113,287 images), validation (5000 images), and test (5000 images)."}, {"heading": "4.4 DETAILS OF TRAINING", "text": "To train the model, we use the standard pairwise ranking objective from Eq. (5). We sample minibatches of 128 random image-caption pairs, and draw all contrastive terms from the minibatch, giving us 127 contrastive images for each caption and captions for each image. We train for 15-30 epochs using the Adam optimizer with learning rate 0.001, and early stopping on the validation set.\nWe set the dimension of the embedding space and the GRU hidden state N to 1024, the dimension of the learned word embeddings to 300, and the margin \u03b1 to 0.05. All these hyperparameters, as well as the learning rate and batchsize, were selected using the validation set. To prevent overfitting, we constrain the caption and image embeddings to have unit L2 norm."}, {"heading": "4.5 RESULTS", "text": "Given a query caption or image, we sort all the images or captions of the test set in order of increasing penalty. We use standard ranking metrics for evaluation. We measure Recall@K, the percent of queries for which the GT term is one of the first K retrieved; and median and mean rank, which are statistics over the position of the GT term in the retrieval order.\nTable 2 shows a comparison between all state-of-the-art and some older methods2 along with our own; see Ma et al. (2015) for a more complete listing.\nThe best results overall are in bold, and the best results using 1-crop VGG image features are underlined. Note that the comparison is additionally complicated by the following:\n\u2022 m-CNNENS is an ensemble of four different models, whereas the other entries are all single models.\n\u2022 STV and FV use external text corpora to learn their language features, whereas the other methods learn them from scratch.\nTo facilitate the comparison and to evaluate the contributions of various components of our model, we evaluate four variations of order-embeddings:\norder-embeddings is our full model as described above.\norder-emb (reversed) reverses the order of captions and image embeddings in our order-violation penalty\u2014placing images above captions in the partial order learned by our model. This seemingly slight variation performs atrociously, confirming our prior that captions are much more abstract than images, and should be placed higher in the semantic hierarchy.\norder-emb (1-crop) computes the image feature using just the center crop, instead of averaging over 10 crops.\norder-emb (symm.) replaces our asymmetric penalty with the symmetric cosine distance, and allows embedding coordinates to be negative\u2014essentially replicating MNLM, but with better image features. Here we find that a different margin (\u03b1 = 0.2) works best.\nBetween these four models, the only previous work whose results are incommensurable with ours is DVSA, since it uses the less discriminative CNN of Krizhevsky et al. (2012) but 20 region features instead of a single whole-image feature.\nAside from this limitation, and if only single models are considered, order-embeddings significantly outperform the state-of-art approaches for image retrieval even when we control for image features."}, {"heading": "4.6 EXPLORATION", "text": "Why would order-embeddings do well on such a shallow partial order? Why are they much more helpful for image retrieval than for caption retrieval?\nIntuitively, symmetric similarity should fail when an image has captions with very different levels of detail, because the captions are so dissimilar that it\u2019s impossible to map both their embeddings close to the same image embedding. Order-embeddings don\u2019t have this problem: the less detailed caption can be embedded very far away from the image while remaining above it in the partial order.\nTo evaluate this intuition, we use caption length as a proxy for level of detail and select, among pairs of co-referring captions in our validation set, the 100 pairs with the biggest length difference. For image retrieval with 1000 target images, the mean rank over captions in this set is 6.4 for orderembeddings and 9.7 for cosine similarity, a much bigger difference than over the entire dataset. Some particularly dramatic examples of this are shown in Figure 3. Moreover, if we use the shorter caption as a query, and retrieve captions in order of increasing error, the mean rank of the longer caption is 34.0 for order-embeddings and 47.6 for cosine similarity, showing that order-embeddings are able to capture the relatedness of co-referring captions with very different lengths.\nThis also explains why order-embeddings provide a much smaller improvement for caption retrieval than for image retrieval: all the caption retrieval metrics are based on the position of the first ground truth caption in the retrieval order, so the embeddings need only learn to retrieve one of each image\u2019s five captions well, which symmetric similarity is well suited for.\n2Note that the numbers for MNLM come not from the published paper but from the recently released code at http://github.com/ryankiros/visual-semantic-embedding."}, {"heading": "5 TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE", "text": "Natural language inference can be seen as a generalization of hypernymy from words to longer sentences. For example, from \u201cwoman walking her dog in a park\u201d we can infer both \u201cwoman walking her dog\u201d and \u201cdog in a park\u201d, but not \u201dold woman\u201d or \u201dblack dog\u201d. Given a pair of sentences, our task is to predict whether we can infer the second sentence (the hypothesis) from the first (the premise)."}, {"heading": "5.1 LOSS FUNCTION", "text": "To apply order-embeddings to this task, we again view it as partial order completion\u2014we can infer a hypothesis from a premise exactly when the hypothesis is above the premise in the visual-semantic hierarchy.\nUnlike our other tasks, for which we had to generate contrastive negatives, datasets for natural language inference include negative examples. So, we can simply use a max-margin loss:\u2211\n(p,h)\nE(f(p), f(h)) + \u2211\n(p\u2032,h\u2032)\nmax{0, \u03b1\u2212 E(f(p\u2032), f(h\u2032))} (7)\nwhere (p, h) are positive and (p\u2032, h\u2032) negative pairs of premise and hypothesis. To embed sentences, we use the same GRU encoder as in the caption-image retrieval task."}, {"heading": "5.2 DATASET", "text": "To evaluate order-embeddings on the natural language inference task, we use the recently proposed SNLI corpus (Bowman et al., 2015), which contains 570,000 pairs of sentences, each labeled with \u201centailment\u201d if the inference is valid, \u201ccontradiction\u201d if the two sentences contradict , or \u201cneutral\u201d if the inference is invalid but there is no contradiction. Our method only allows us to discriminate between entailment and non-entailment, so we merge the \u201ccontradiction\u201d and \u201cneutral\u201d classes together to serve as our negative examples."}, {"heading": "5.3 IMPLEMENTATION DETAILS", "text": "Just as for caption-image ranking, we set the dimensions of the embedding space and GRU hidden state to be 1024, the dimension of the word embeddings to be 300, and constrain the embeddings to have unit L2 norm. We train for 10 epochs with batches of 128 sentence pairs. We use the Adam optimizer with learning rate 0.001 and early stopping on the validation set. During evaluation, we find the optimal classification threshold on validation, then use the threshold to classify the test set."}, {"heading": "5.4 RESULTS", "text": "The state-of-the-art method for 3-class classification on SNLI is that of Rockta\u0308schel et al. (2015). Unfortunately, they do not compute 2-class accuracy, so we cannot compare to them directly.\nAs a bridge to facilitate comparison, we use a challenging baseline which can be evaluated on both the 2-class and 3-class problems. The baseline, referred to as skip-thoughts, involves a feedforward neural network on top of skip-thought vectors (Kiros et al., 2015), a state-of-the-art semantic representation of sentences. Given pairs of sentence vectors u and v, the input to the network is the concatenation of u, v and the absolute difference |u \u2212 v|. We tuned the number of layers, layer dimensionality and dropout rates to optimize performance on the development set, using the Adam optimizer. Batch normalization (Ioffe & Szegedy, 2015) and PReLU units (He et al., 2015) were used. Our best network used 2 hidden layers of 1000 units each, with dropout rate of 0.5 across both the input and hidden layers. We did not backpropagate through the skip-thought encoder.\nWe also evaluate against EOP classifier, a 2-class baseline introduced by (Bowman et al., 2015), and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric).\nThe results for all models are shown in Table 3. We see that order-embeddings outperform the skipthought baseline despite not using external text corpora. While our method is almost certainly worse than the state-of-the-art method of Rockta\u0308schel et al. (2015), which uses a word-by-word attention mechanism, it is also much simpler."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We introduced a simple method to encode order into learned distributed representations, which allows us to explicitly model the partial order structure of the visual-semantic hierarchy. Our method can be easily integrated into existing relational learning methods, as we demonstrated on three challenging tasks involving computer vision and natural language processing. On two of these tasks, hypernym prediction and caption-image retrieval, our methods outperform all previous work.\nA promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, including Frome et al. (2013) and Norouzi et al. (2014) have embedded words and images into a shared semantic space with symmetric similarity\u2014which our experiments suggest to be a poor fit with the partial order structure of the visual-semantic hierarchy. We expect significant progress on ImageNet classification, and the related problems of one-shot and zero-shot learning, using order-embeddings.\nGoing further, order-embeddings may enable learning the entire semantic hierarchy in a single model which jointly reasons about hypernymy, entailment, and the relationship between perception and language, unifying what have been until now almost independent lines of work."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Kaustav Kundu for many fruitful discussions throughout the development of this paper. The work was supported in part by an NSERC Graduate Scholarship."}, {"heading": "7 SUPPLEMENTARY MATERIAL", "text": "Mikolov et al. (2013) showed that learned vector-space word representations exhibit semantic regularities, such as king\u2212man+woman \u223c queen. Kiros et al. (2014) showed that similar regularities hold for joint image-language models. We find that order-embeddings exhibit a novel form of regularity, shown in Figure 4. The elementwise max and min operations in the embedding space roughly correspond to composition and abstraction, respectively.\nTo get some intuition for what our learned embedding space looks like, we use t-SNE (Van der Maaten & Hinton, 2008) to embed the images and phrases from our initial illustration of the visualsemantic hierarchy. The results, for both order-embeddings and the symmetric cosine distance baseline, are shown in Figure 5."}], "references": [{"title": "Entailment above the word level in distributional semantics", "author": ["Baroni", "Marco", "Bernardi", "Raffaella", "Do", "Ngoc-Quynh", "Shan", "Chung-chieh"], "venue": "In EACL,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes", "Antoine", "Weston", "Jason", "Collobert", "Ronan", "Bengio", "Yoshua"], "venue": "In AAAI,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman", "Samuel R", "Angeli", "Gabor", "Potts", "Christopher", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia"], "venue": "metrics. JAIR,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Li", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Associating neural word embeddings with deep image representations using fisher vectors", "author": ["Klein", "Benjamin", "Lev", "Guy", "Sadeh", "Gil", "Wolf", "Lior"], "venue": "In CVPR,", "citeRegEx": "Klein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Visual semantic search: Retrieving videos via complex textual queries", "author": ["Lin", "Dahua", "Fidler", "Sanja", "Kong", "Chen", "Urtasun", "Raquel"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Multimodal convolutional neural networks for matching image and sentence", "author": ["Ma", "Lin", "Lu", "Zhengdong", "Shang", "Lifeng", "Li", "Hang"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan"], "venue": "In ICLR,", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Norouzi", "Mohammad", "Mikolov", "Tomas", "Bengio", "Samy", "Singer", "Yoram", "Shlens", "Jonathon", "Frome", "Andrea", "Corrado", "Greg S", "Dean", "Jeffrey"], "venue": "In ICLR,", "citeRegEx": "Norouzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2014}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models", "author": ["Plummer", "Bryan", "Wang", "Liwei", "Cervantes", "Chris", "Caicedo", "Juan", "Hockenmaier", "Julia", "Lazebnik", "Svetlana"], "venue": "arXiv preprint arXiv:1505.04870,", "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Rockt\u00e4schel", "Tim", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Ko\u010disk\u1ef3", "Tom\u00e1\u0161", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher", "Richard", "Karpathy", "Andrej", "Le", "Quoc V", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "TACL,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Word representations via gaussian embedding", "author": ["Vilnis", "Luke", "McCallum", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Vilnis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilnis et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young", "Peter", "Lai", "Alice", "Hodosh", "Micah", "Hockenmaier", "Julia"], "venue": "TACL,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Recent work in vision has moved beyond discriminating between a fixed set of object classes, to automatically generating open-ended lingual descriptions of images (Vinyals et al., 2015).", "startOffset": 163, "endOffset": 185}, {"referenceID": 29, "context": "Recent work in vision has moved beyond discriminating between a fixed set of object classes, to automatically generating open-ended lingual descriptions of images (Vinyals et al., 2015). Recent methods for natural language processing such as Young et al. (2014) learn the semantics of language by grounding it in the visual world.", "startOffset": 164, "endOffset": 262}, {"referenceID": 4, "context": "One line of work, exemplified by Chopra et al. (2005) and first applied to the caption-image relationship by Socher et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "One line of work, exemplified by Chopra et al. (2005) and first applied to the caption-image relationship by Socher et al. (2014), requires the mapping to be distance-preserving: semantically", "startOffset": 33, "endOffset": 130}, {"referenceID": 1, "context": "Bordes et al. (2011); Socher et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bordes et al. (2011); Socher et al. (2013); Ma et al.", "startOffset": 0, "endOffset": 43}, {"referenceID": 1, "context": "Bordes et al. (2011); Socher et al. (2013); Ma et al. (2015). Notably, no existing approach directly imposes the transitivity and antisymmetry of the partial order, leaving the model to induce these properties from data.", "startOffset": 0, "endOffset": 61}, {"referenceID": 30, "context": "The visual-semantic hierarchy can then be seen as a special case of the subset relation, a connection also used by Young et al. (2014).", "startOffset": 115, "endOffset": 135}, {"referenceID": 0, "context": "The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors.", "startOffset": 45, "endOffset": 66}, {"referenceID": 0, "context": "The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors. Bordes et al. (2011) and Socher et al.", "startOffset": 45, "endOffset": 169}, {"referenceID": 0, "context": "The most similar evaluation has been that of Baroni et al. (2012), who use external linguistic data in the form of distributional semantic vectors. Bordes et al. (2011) and Socher et al. (2013) also evaluate on the WordNet hierarchy, but they use other relations in WordNet as training data (and external linguistic data, in Socher\u2019s case).", "startOffset": 45, "endOffset": 194}, {"referenceID": 25, "context": "1 LOSS FUNCTION To apply order-embeddings to hypernymy, we follow the setup of Socher et al. (2013) in learning an N-dimensional vector for each concept in WordNet, but we replace their neural tensor network with our order-violation penalty defined in Eq.", "startOffset": 79, "endOffset": 100}, {"referenceID": 1, "context": "Like Bordes et al. (2011), we randomly select 4000 edges for the test split, and another 4000 for the development set.", "startOffset": 5, "endOffset": 26}, {"referenceID": 25, "context": "order-embeddings (bilinear) replaces our penalty with the bilinear model used by Socher et al. (2013). order-embeddings is our full model.", "startOffset": 81, "endOffset": 102}, {"referenceID": 8, "context": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a).", "startOffset": 105, "endOffset": 145}, {"referenceID": 26, "context": "Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al.", "startOffset": 163, "endOffset": 204}, {"referenceID": 12, "context": "Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al.", "startOffset": 163, "endOffset": 204}, {"referenceID": 13, "context": ", 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015).", "startOffset": 112, "endOffset": 132}, {"referenceID": 29, "context": "An alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image (Vinyals et al., 2015; Mao et al., 2015) or using a multimodal CNN (Ma et al.", "startOffset": 122, "endOffset": 162}, {"referenceID": 18, "context": "An alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image (Vinyals et al., 2015; Mao et al., 2015) or using a multimodal CNN (Ma et al.", "startOffset": 122, "endOffset": 162}, {"referenceID": 17, "context": ", 2015) or using a multimodal CNN (Ma et al., 2015).", "startOffset": 34, "endOffset": 51}, {"referenceID": 8, "context": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a). The task involves ranking a large dataset of images by relevance for a query caption (Image Retrieval), and ranking captions by relevance for a query image (Caption Retrieval). Given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score S(c, i) to be used at test time. Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015). While Karpathy & Li (2015) and Plummer et al.", "startOffset": 106, "endOffset": 838}, {"referenceID": 8, "context": "The caption-image retrieval task has become a standard evaluation of joint models of vision and language (Hodosh et al., 2013; Lin et al., 2014a). The task involves ranking a large dataset of images by relevance for a query caption (Image Retrieval), and ranking captions by relevance for a query image (Caption Retrieval). Given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score S(c, i) to be used at test time. Many modern approaches model the caption-image relationship symmetrically, either by embedding into a common \u201cvisual-semantic\u201d space with inner-product similarity (Socher et al., 2014; Kiros et al., 2014), or by using Canonical Correlations Analysis between distributed representations of images and captions (Klein et al., 2015). While Karpathy & Li (2015) and Plummer et al. (2015) model a finer-grained alignment between regions in the image and segments of the caption, the similarity they use is still symmetric.", "startOffset": 106, "endOffset": 864}, {"referenceID": 24, "context": "To facilitate comparison, we use the same pairwise ranking loss that Socher et al. (2014), Kiros et al.", "startOffset": 69, "endOffset": 90}, {"referenceID": 12, "context": "(2014), Kiros et al. (2014) and Karpathy & Li (2015) have used on this task\u2014simply replacing their symmetric similarity measure with our asymmetric order-violation penalty.", "startOffset": 8, "endOffset": 28}, {"referenceID": 12, "context": "(2014), Kiros et al. (2014) and Karpathy & Li (2015) have used on this task\u2014simply replacing their symmetric similarity measure with our asymmetric order-violation penalty.", "startOffset": 8, "endOffset": 53}, {"referenceID": 12, "context": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into R+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value.", "startOffset": 47, "endOffset": 67}, {"referenceID": 12, "context": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into R+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value. Thus, to embed images, we use fi(i) = |Wi \u00b7 CNN(i)| (6) where Wi is a learned N \u00d7 4096 matrix, N being the dimensionality of the embedding space. CNN(i) is the same image feature used by Klein et al. (2015): we rescale images to have smallest side 256 pixels, we take 224 \u00d7 224 crops from the corners, center, and their horizontal reflections, run the 10 crops through the 19-layer VGG network of Simonyan & Zisserman (2015), and average their fc7 features.", "startOffset": 47, "endOffset": 415}, {"referenceID": 12, "context": "For learning fc and fi, we use the approach of Kiros et al. (2014) except that (since we are embedding into R+ ) we constrain the embedding vectors to have nonnegative entries by taking their absolute value. Thus, to embed images, we use fi(i) = |Wi \u00b7 CNN(i)| (6) where Wi is a learned N \u00d7 4096 matrix, N being the dimensionality of the embedding space. CNN(i) is the same image feature used by Klein et al. (2015): we rescale images to have smallest side 256 pixels, we take 224 \u00d7 224 crops from the corners, center, and their horizontal reflections, run the 10 crops through the 19-layer VGG network of Simonyan & Zisserman (2015), and average their fc7 features.", "startOffset": 47, "endOffset": 633}, {"referenceID": 12, "context": "Caption Retrieval Image Retrieval Model R@1 R@10 Med r Mean r R@1 R@10 Med r Mean r 1k Test Images MNLM (Kiros et al., 2014) 43.", "startOffset": 104, "endOffset": 124}, {"referenceID": 18, "context": "9 3 * m-RNN (Mao et al., 2015) 41.", "startOffset": 12, "endOffset": 30}, {"referenceID": 13, "context": "6 4 * FV (Klein et al., 2015) 39.", "startOffset": 9, "endOffset": 29}, {"referenceID": 17, "context": "1 m-CNN (Ma et al., 2015) 38.", "startOffset": 8, "endOffset": 25}, {"referenceID": 13, "context": "Metrics for our models on 1k test images are averages over five 1000-image splits of the 5000-image test set, as in (Klein et al., 2015).", "startOffset": 116, "endOffset": 136}, {"referenceID": 3, "context": "To embed the captions, we use a recurrent neural net encoder with GRU activations (Cho et al., 2014), so fc(c) = |GRU(c)|, the absolute value of hidden state after processing the last word.", "startOffset": 82, "endOffset": 100}, {"referenceID": 15, "context": "We evaluate on the Microsoft COCO dataset (Lin et al., 2014b), which has over 120,000 images, each with at least five human-annotated captions per image. This is by far the largest dataset commonly used for caption-image retrieval. We use the data splits of Karpathy & Li (2015) for training (113,287 images), validation (5000 images), and test (5000 images).", "startOffset": 43, "endOffset": 279}, {"referenceID": 17, "context": "Table 2 shows a comparison between all state-of-the-art and some older methods2 along with our own; see Ma et al. (2015) for a more complete listing.", "startOffset": 104, "endOffset": 121}, {"referenceID": 14, "context": "Between these four models, the only previous work whose results are incommensurable with ours is DVSA, since it uses the less discriminative CNN of Krizhevsky et al. (2012) but 20 region features instead of a single whole-image feature.", "startOffset": 148, "endOffset": 173}, {"referenceID": 2, "context": "To evaluate order-embeddings on the natural language inference task, we use the recently proposed SNLI corpus (Bowman et al., 2015), which contains 570,000 pairs of sentences, each labeled with \u201centailment\u201d if the inference is valid, \u201ccontradiction\u201d if the two sentences contradict , or \u201cneutral\u201d if the inference is invalid but there is no contradiction.", "startOffset": 110, "endOffset": 131}, {"referenceID": 23, "context": "Neural Attention (Rockt\u00e4schel et al., 2015) * 83.", "startOffset": 17, "endOffset": 43}, {"referenceID": 2, "context": "5 EOP classifier (Bowman et al., 2015) 75.", "startOffset": 17, "endOffset": 38}, {"referenceID": 7, "context": "Batch normalization (Ioffe & Szegedy, 2015) and PReLU units (He et al., 2015) were used.", "startOffset": 60, "endOffset": 77}, {"referenceID": 2, "context": "We also evaluate against EOP classifier, a 2-class baseline introduced by (Bowman et al., 2015), and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric).", "startOffset": 74, "endOffset": 95}, {"referenceID": 20, "context": "The state-of-the-art method for 3-class classification on SNLI is that of Rockt\u00e4schel et al. (2015). Unfortunately, they do not compute 2-class accuracy, so we cannot compare to them directly.", "startOffset": 74, "endOffset": 100}, {"referenceID": 2, "context": "We also evaluate against EOP classifier, a 2-class baseline introduced by (Bowman et al., 2015), and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric). The results for all models are shown in Table 3. We see that order-embeddings outperform the skipthought baseline despite not using external text corpora. While our method is almost certainly worse than the state-of-the-art method of Rockt\u00e4schel et al. (2015), which uses a word-by-word attention mechanism, it is also much simpler.", "startOffset": 75, "endOffset": 504}, {"referenceID": 5, "context": "A promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy.", "startOffset": 80, "endOffset": 99}, {"referenceID": 5, "context": "A promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, including Frome et al. (2013) and Norouzi et al.", "startOffset": 81, "endOffset": 220}, {"referenceID": 5, "context": "A promising direction of future work is to learn better classifiers on ImageNet (Deng et al., 2009), which has over 21k image classes arranged by the WordNet hierarchy. Previous approaches, including Frome et al. (2013) and Norouzi et al. (2014) have embedded words and images into a shared semantic space with symmetric similarity\u2014which our experiments suggest to be a poor fit with the partial order structure of the visual-semantic hierarchy.", "startOffset": 81, "endOffset": 246}], "year": 2017, "abstractText": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.", "creator": "LaTeX with hyperref package"}}}