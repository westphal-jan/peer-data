{"id": "1511.06438", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Joint Word Representation Learning Using a Corpus and a Semantic Lexicon", "abstract": "Methods for learning word representation using large corpora text have lately attracted a lot of attention due to their impressive performance in numerous natural language processing (NLP) tasks, such as semantic similarity measurement and recognition of word analogies. Despite their success, these data-driven word-building learning methods do not take into account the rich semantic relationship structure between words in a simultaneous context. On the other hand, a lot of manual effort has already been invested in constructing sectional lexicographs such as WordNet, which represent the meanings of words by defining the various relationships that exist between words in a language. We are considering whether we can improve the word representations learned using a corpora by integrating knowledge from semantic lexicographs?. To this end, we propose a common method of word representation that simultaneously accommodates the simultaneous occurrences of two words in a sentence that is subject to the relational constraints given by the semantic lexicon.", "histories": [["v1", "Thu, 19 Nov 2015 22:58:10 GMT  (321kb)", "http://arxiv.org/abs/1511.06438v1", "Accepted to AAAI-2016"]], "COMMENTS": "Accepted to AAAI-2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["danushka bollegala", "mohammed alsuhaibani", "takanori maehara", "ken-ichi kawarabayashi"], "accepted": true, "id": "1511.06438"}, "pdf": {"name": "1511.06438.pdf", "metadata": {"source": "CRF", "title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon", "authors": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 43\n8v 1\n[ cs\n.C L\n] 1\n9 N\nov 2"}, {"heading": "1 Introduction", "text": "Learning representations for words is a fundamental step in various NLP tasks. If we can accurately represent the meanings of words using some linear algebraic structure such as vectors, we can use those word-level semantic representations to compute representations for larger lexical units such as phrases, sentences, or texts (Socher et al. 2012; Le and Mikolov 2014). Moreover, by using word representations as features in downstream NLP applications, significant improvements in performance have been obtained (Turian et al. 2010; Bollegala et al. 2015; Collobert et al. 2011). Numerous approaches for learning word representations from large text corpora have been proposed, such as counting-based methods (Turney and Pantel 2010) that follow the distributional hypothesis and use contexts of a word to represent that word,\n\u2217Japan Science and Technology Agency, ERATO, Kawarabayashi Large Graph Project Copyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nand prediction-based methods (Mikolov et al. 2013b) that learn word representations by predicting the occurrences of a word in a given context (Baroni et al. 2014).\nComplementary to the corpus-based data-driven approaches for learning word representations, significant manual effort has already been invested in creating semantic lexicons such as the WordNet (Miller 1995). Semantic lexicons explicitly define the meaning of words by specifying the relations that exist between words, such as synonymy, hypernymy, or meronymy. Although it is attractive to learn word representation purely from a corpus in an unsupervised fashion because it obviates the need for manual data annotation, there exist several limitations to this corpus-only approach where a semantic lexicon could help to overcome. First, corpus-based approaches operate on surface-level word cooccurrences, and ignore the rich semantic relations that exist between the two words that co-occur in the corpus. Second, unlike in a semantic lexicon where a word is grouped with the other words with similar senses (e.g., WordNet synsets), occurrences of a word in a corpus can be ambiguous. Third, the corpus might not be sufficiently large to obtain reliable word co-occurrence counts, which is problematic when learning representations for rare words.\nOn the other hand, purely using a semantic lexicon to learn word representations (Bollegala et al. 2014) can also be problematic. First, unlike in a corpus where we can observe numerous co-occurrences between two words in different contexts, in a semantic lexicon we often have only a limited number of entries for a particular word. Therefore, it is difficult to accurately estimate the strength of the relationship between two words using only a semantic lexicon. Second, a corpus is likely to include neologisms or novel, creative uses of existing words. Because most semantic lexicons are maintained manually on a periodical basis, such trends will not be readily reflected in the semantic lexicon. Considering the weaknesses of methods that use either only a corpus or only a semantic lexicon, it is a natural motivation for us to explore hybrid approaches.\nTo illustrate how a semantic lexicon can potentially assist the corpus-based word representation learning process, let us consider the sentence \u201cI like both cats and dogs\u201d. If this was the only sentence in the corpus where the three words cat, dog, and like occur, we would learn word representations for those three words that predict cat to be\nequally similar to like as to dog because, there is exactly one co-occurrence between all three pairs generated from those three words. However, a semantic lexicon would list cat and dog as hyponyms of pet, but not of like. Therefore, by incorporating such constraints from a semantic lexicon into the word representation learning process, we can potentially overcome this problem.\nWe propose a method to learn word representations using both a corpus and a semantic lexicon in a joint manner. Initially, all words representations are randomly initialized with fixed, low-dimensional, real-valued vectors, which are subsequently updated to predict the co-occurrences between two words in a corpus. We use a regularized version of the global co-occurrence prediction approach proposed by Pennington et al. (2014) as our objective function. We use the semantic lexicon to construct a regularizer that enforces two words that are in a particular semantic relationship in the lexicon to have similar word representations. Unlike retrofitting (Faruqui et al. 2015), which fine-tunes pretrained word representations in a post-processing step, our method jointly learns both from the corpus as well as from the semantic lexicon, thereby benefitting from the knowledge in the semantic lexicon during the word representation learning stage.\nIn our experiments, we use seven relation types found in the WordNet, and compare the word representations learnt by the proposed method. Specifically, we evaluate the learnt word representations on two standard tasks: semantic similarity prediction (Bollegala. et al. 2007), and word analogy prediction (Duc et al. 2011). On both those tasks, our proposed method statistically significantly outperforms all previously proposed methods for learning word representations using a semantic lexicon and a corpus. The performance of the proposed method is stable across a wide-range of vector dimensionalities. Furthermore, experiments conducted using different sized corpora show that the benefit of incorporating a semantic lexicon is more prominent for smaller corpora."}, {"heading": "2 Related Work", "text": "Learning word representations using large text corpora has received a renewed interest recently due to the impressive performance gains obtained in downstream NLP applications using the word representations as features (Collobert et al. 2011; Turian et al. 2010). Continuous bag-of-words (CBOW) and skip-gram (SG) methods proposed by Mikolov et al. (Mikolov et al. 2013a) use the local co-occurrences of a target word and other words in its context for learning word representations. Specifically, CBOW predicts a target word given its context, whereas SG predicts the context given the target word. Global vector prediction (GloVe) (Pennington et al. 2014) on the other hand first builds a word co-occurrence matrix and predicts the total cooccurrences between a target word and a context word. Unlike SG and CBOW, GloVe does not require negative training instances, and is less likely to affect from random local co-occurrences because it operates on global counts. However, all of the above mentioned methods are limited to using\nonly a corpus, and the research on using semantic lexicons in the word representation learning process has been limited.\nYu and Dredze (2014) proposed the relation constrained model (RCM) where they used word similarity information to improve the word representations learnt using CBOW. Specifically, RCM assigns high probabilities to words that are listed as similar in the lexicon. Although we share the same motivation as Yu and Dredze (2014) for jointly learning word representations from a corpus and a semantic lexicon, our method differs from theirs in several aspects. First, unlike in RCM where only synonymy is considered, we use different types of semantic relations in our model. As we show later in Section 4, besides synonymy, numerous other semantic relation types are useful for different tasks. Second, unlike the CBOW objective used in RCM that considers only local co-occurrences, we use global co-occurrences over the entire corpus. This approach has several benefits over the CBOW method, such as we are not required to normalize over the entire vocabulary to compute conditional probabilities, which is computationally costly for large vocabularies. Moreover, we do not require pseudo negative training instances. Instead, the number of co-occurrences between two words is predicted using the inner-product between the corresponding word representations. Indeed, Pennington et al. (2014) show that we can learn superior word representations by predicting global co-occurrences instead of local co-occurrences.\nXu et al. (2014) proposed RC-NET that uses both relational (R-NET) and categorical (C-NET) information in a knowledge base (KB) jointly with the skip-gram objective for learning word representations. They represent both words and relations in the same embedded space. Specifically, given a relational tuple (h, r, t) where a semantic relation r exists in the KB between two words h and t, they enforce the constraint that the vector sum (h + r) of the representations for h and r must be similar to t for words t that have the relation r with h. Similar to RCM, RC-NET is limited to using local co-occurrence counts.\nIn contrast to the joint learning methods discussed above, Faruqui et al. (2015) proposed retrofitting, a post-processing method that fits pre-trained word representations for a given semantic lexicon. The modular approach of retrofitting is attractive because it can be used to fit arbitrary pretrained word representations to an arbitrary semantic lexicon, without having to retrain the word representations. Johansson and Nieto Pin\u0303a (2015) proposed a method to embed a semantic network consisting of linked word senses into a continuous-vector word space. Similar to retrofitting, their method takes pre-trained word vectors and computes sense vectors over a given semantic network. However, a disadvantage of such an approach is that we cannot use the rich information in the semantic lexicon when we learn the word representations from the corpus. Moreover, incompatibilities between the corpus and the lexicon, such as the differences in word senses, and missing terms must be carefully considered. We experimentally show that our joint learning approach outperforms the post-processing approach used in retrofitting.\nIacobacci et al. (2015) used BabelNet and consider words\nthat are connected to a source word in BabelNet to overcome the difficulties when measuring the similarity between rare words. However, they do not consider the semantic relations between words and only consider words that are listed as related in the BabelNet, which encompasses multiple semantic relations. Bollegala et al. (2014) proposed a method for learning word representations from a relational graph, where they represent words and relations respectively by vectors and matrices. Their method can be applied on either a manually created relational graph, or an automatically extracted one from data. However, during training they use only the relational graph and do not use the corpus."}, {"heading": "3 Learning Word Representations", "text": "Given a corpus C, and a semantic lexicon S, we describe a method for learning word representations wi \u2208 Rd for words wi in the corpus. We use the boldface wi to denote the word (vector) representation of the i-th word wi, and the vocabulary (i.e., the set of all words in the corpus) is denoted by V . The dimensionality d of the vector representation is a hyperparameter of the proposed method that must be specified by the user in advance. Any semantic lexicon that specifies the semantic relations that exist between words could be used as S, such as the WordNet (Miller 1995), FrameNet (Baker et al. 1998), or the Paraphrase Database (Ganitkevitch et al. 2013). In particular, we do not assume any structural properties unique to a particular semantic lexicon. In the experiments described in this paper we use the WordNet as the semantic lexicon.\nFollowing Pennington et al. (2014), first we create a cooccurrence matrix X in which words that we would like to learn representations for (target words) are arranged in rows of X, whereas words that co-occur with the target words in some contexts (context words) are arranged in columns of X. The (i, j)-th element Xij of X is set to the total cooccurrences of i and j in the corpus. Following the recommendations in prior work on word representation learning (Levy et al. 2015), we set the context window to the 10 tokens preceding and succeeding a word in a sentence. We then extract unigrams from the co-occurrence windows as the corresponding context words. We down-weight distant (and potentially noisy) co-occurrences using the reciprocal 1/l of the distance in tokens l between the two words that co-occur.\nA word wi is assigned two vectors wi and w\u0303i denoting whether wi is respectively the target of the prediction (corresponding to the rows of X), or in the context of another word (corresponding to the columns of X). The GloVe objective can then be written as:\nJC = 1\n2\n\u2211\ni\u2208V\n\u2211\nj\u2208V\nf(Xij) ( wi \u22a4 w\u0303j + bi + b\u0303j \u2212 log(Xij) ) 2 (1)\nHere, bi and b\u0303j are real-valued scalar bias terms that adjust for the difference between the inner-product and the logarithm of the co-occurrence counts. The function f discounts the co-occurrences between frequent words and is given by:\nf(t) =\n{\n(t/tmax) \u03b1 if t < tmax 1 otherwise (2)\nFollowing (Pennington et al. 2014), we set \u03b1 = 0.75 and tmax = 100 in our experiments. The objective function defined by (1) encourages the learning of word representations that demonstrate the desirable property that vector difference between the word embeddings for two words represents the semantic relations that exist between those two words. For example, Mikolov et al. (2013c) observed that the difference between the word embeddings for the words king and man when added to the word embedding for the word woman yields a vector similar to that of queen.\nUnfortunately, the objective function given by (1) does not capture the semantic relations that exist between wi and wj as specified in the lexicon S. Consequently, it considers all co-occurrences equally and is likely to encounter problems when the co-occurrences are rare. To overcome this problem we propose a regularizer, JS , by considering the three-way co-occurrence among words wi, wj , and a semantic relation R that exists between the target word wi and one of its context words wj in the lexicon as follows:\nJS = 1\n2\n\u2211\ni\u2208V\n\u2211\nj\u2208V\nR(i, j) (wi \u2212 w\u0303j) 2 (3)\nHere, R(i, j) is a binary function that returns 1 if the semantic relation R exists between the words wi and wj in the lexicon, and 0 otherwise. In general, semantic relations are asymmetric. Thus, we have R(i, j) 6= R(j, i). Experimentally, we consider both symmetric relation types, such as synonymy and antonymy, as well as asymmetric relation types, such as hypernymy and meronymy. The regularizer given by (3) enforces the constraint that the words that are connected by a semantic relation R in the lexicon must have similar word representations.\nWe would like to learn target and context word representations wi, w\u0303j that simultaneously minimize both (1) and (3). Therefore, we formulate the joint objective as a minimization problem as follows:\nJ = JC + \u03bbJS (4)\nHere, \u03bb \u2208 R+ is a non-negative real-valued regularization coefficient that determines the influence imparted by the semantic lexicon on the word representations learnt from the corpus. We use development data to estimate the optimal value of \u03bb as described later in Section 4.\nThe overall objective function given by (4) is non-convex w.r.t. to the four variables wi, w\u0303j , bi, and b\u0303j . However, if we fix three of those variables, then (4) becomes convex in the remaining one variable. We use an alternative optimization approach where we first randomly initialize all the parameters, and then cycle through the set of variables in a pre-determined order updating one variable at a time while keeping the other variables fixed.\nThe derivatives of the objective function w.r.t. the variables are given as follows:\n\u2202J\n\u2202wi =\n\u2211\nj\nf(Xij)w\u0303j ( wi \u22a4 w\u0303j + bi + b\u0303j \u2212 log(Xij) )\n+\u03bb \u2211\nj\nR(i, j)(wi \u2212 w\u0303j) (5)\n\u2202J \u2202bi =\n\u2211\nj\nf(Xij) ( wi \u22a4 w\u0303j + bi + b\u0303j \u2212 log(Xij) ) (6)\n\u2202J\n\u2202w\u0303j =\n\u2211\ni\nf(Xij)wi ( wi \u22a4 w\u0303j + bi + b\u0303j \u2212 log(Xij) )\n\u2212\u03bb \u2211\nj\nR(i, j)(wi \u2212 w\u0303j) (7)\n\u2202J \u2202b\u0303j =\n\u2211\ni\nf(Xij) ( wi \u22a4 w\u0303j + bi + b\u0303j \u2212 log(Xij) ) (8)\nWe use stochastic gradient descent (SGD) with learning rate scheduled by AdaGrad (Duchi et al. 2011) as the optimization method. The overall algorithm for learning word embeddings is listed in Algorithm 1.\nWe used the ukWaC1 as the corpus. It has ca. 2 billion tokens and have been used for learning word embeddings in prior work. We initialize word embeddings by randomly sampling each dimension from the uniform distribution in the range [\u22121,+1]. We set the initial learning rate in AdaGrad to 0.01 in our experiments. We observed that T = 20 iterations is sufficient for the proposed method to converge to a solution.\nBuilding the co-occurrence matrix X is an essential preprocessing step for the proposed method. Because the cooccurrences between rare words will also be rare, we can first count the frequency of each word and drop words that have total frequency less than a pre-defined threshold to manage the memory requirements of the co-occurrence matrix. In our experiments, we dropped words that are less than 20 times in the entire corpus when building the cooccurrence matrix. For storing larger co-occurrence matrices we can use distributed hash tables and sparse representations.\nThe for-loop in Line 3 of Algorithm 1 iterates over the non-zero elements in X. If the number of non-zero elements in X is n, the overall time complexity of Algorithm 1 can be estimated as \u00d8(|V|dTn), where |V| denotes the number of words in the vocabulary. Typically, the global co-occurrence matrix is highly sparse, containing less than 0.03% of nonzero entries. It takes under 50 mins. to learn 300 dimensional word representations for |V| = 434, 826 words (n = 58, 494, 880) from the ukWaC corpus on a Xeon 2.9GHz 32 core 512GB RAM machine. The source code and data for the proposed method is publicly available2."}, {"heading": "4 Experiments and Results", "text": "We evaluate the proposed method on two standard tasks: predicting the semantic similarity between two words, and predicting proportional analogies consisting of two pairs of words. For the similarity prediction task, we use the following benchmark datasets: Rubenstein-Goodenough (RG, 65 wordpairs) (Rubenstein and Goodenough 1965), Miller-Charles (MC, 30 word-pairs) (Miller and Charles 1998), rare words dataset (RW, 2034 word-pairs) (Luong et al. 2013),\n1http://wacky.sslmit.unibo.it 2https://github.com/Bollegala/jointreps\nAlgorithm 1 Jointly learning word representations using a corpus and a semantic lexicon. Input: Word co-occurrence matrix X specifying the co-\noccurrences between words in the corpus C, relation function R specifying the semantic relations between words in the lexicon S , dimensionality d of the word embeddings, and the maximum number of iterations T . Output: Embeddings wi, w\u0303j \u2208 Rd, of all words i, j in the vocabulary V .\n1: Initialize word vectors wi, w\u0303j \u2208 Rd randomly. 2: for t = 1 to T do 3: for (i, j) \u2208 X do 4: Use (5) to update wi 5: Use (6) to update bi 6: Use (7) to update w\u0303j 7: Use (8) to update bj 8: end for 9: end for\n10: return wi, w\u0303j for all words i, j \u2208 V .\nStanford\u2019s contextual word similarities (SCWS, 2023 wordpairs) (Huang et al. 2012), and the MEN test collection (3000 word-pairs) (Bruni et al. 2012). Each word-pair in those benchmark datasets has a manually assigned similarity score, which we consider as the gold standard rating for semantic similarity.\nFor each word wi, the proposed method learns a target representation wi, and a context representation w\u0303i. (Levy et al. 2015) show that the addition of the two vectors, wi + w\u0303i, gives a better representation for the word wi. In particular, when we measure the cosine similarity between two words using their word representations, this additive approach considers both first and second-order similarities between the two words. (Pennington et al. 2014) originally motivated this additive operation as an ensemble method. Following these prior recommendations, we add the target and context representations to create the final representation for a word. The remainder of the experiments in the paper use those word representations.\nNext, we compute the cosine similarity between the two corresponding embeddings of the words. Following the standard approach for evaluating using the above-mentioned benchmarks, we measure Spearman correlation coefficient between gold standard ratings and the predicted similarity scores. We use the Fisher transformation to test for the statistical significance of the correlations. Table 1 shows the Spearman correlation coefficients on the five similarity benchmarks, where high values indicate a better agreement with the human notion of semantic similarity.\nFor the word analogy prediction task we used two benchmarks: Google word analogy dataset (Mikolov et al. 2013b), and SemEval 2012 Task 2 dataset (Jurgens et al. 2012) (SemEval). Google dataset consists of 10, 675 syntactic (syn) analogies, and 8869 semantic analogies (sem). The SemEval dataset contains manually ranked word-pairs for 79 word-pairs describing various semantic relation types, such as defective, and agent-goal. In total there are 3218 word-pairs in the Se-\nmEval dataset. Given a proportional analogy a : b :: c : d, we compute the cosine similarity between b \u2212 a + c and c, where the boldface symbols represent the embeddings of the corresponding words. For the Google dataset, we measure the accuracy for predicting the fourth word d in each proportional analogy from the entire vocabulary. We use the binomial exact test with Clopper-Pearson confidence interval to test for the statistical significance of the reported accuracy values. For SemEval we use the official evaluation tool3 to compute MaxDiff scores.\nIn Table 1, we compare the word embeddings learnt by the proposed method for different semantic relation types in the WordNet. All word embeddings compared in Table 1 are 300 dimensional. We use the WordSim-353 (WS) dataset (Finkelstein et al. 2002) as validation data to find the optimal value of \u03bb for each relation type. Specifically, we minimize (4) for different \u03bb values, and use the learnt word representations to measure the cosine similarity for the word-pairs in the WS dataset. We then select the value of \u03bb that gives the highest Spearman correlation with the human ratings on the WS dataset. This procedure is repeated separately with each semantic relation type R. We found that \u03bb values greater than 10000 to perform consistently well on all relation types. The level of performance if we had used only the corpus for learning word representations (without using a semantic lexicon) is shown in Table 1 as the corpus only baseline. This baseline corresponds to setting \u03bb = 0 in (4).\nFrom Table 1, we see that by incorporating most of the semantic relations found in the WordNet we can improve over the corpus only baseline. In particular, the improvements reported by synonymy over the corpus only baseline is statistically significant on RG, MC, SCWS, MEN, syn, and SemEval. Among the individual semantic relations, synonymy consistently performs well on all benchmarks. Among the other relations, part-holonyms and member-holonyms perform best respectively for predicting semantic similarity between rare words (RW), and for predicting semantic analogies (sem) in the Google dataset. Meronyms and holonyms are particularly effective for predicting semantic similarity between rare words. This result is important because it shows that a semantic lexicon can assist the representation learning of rare words, among which the co-occurrences are small even in large corpora (Luong et al. 2013), The fact that the proposed method could significantly improve performance on this task empirically justifies our proposal for\n3https://sites.google.com/site/semeval2012task2/\nusing a semantic lexicon in the word representation learning process. Table 1 shows that not all relation types are equally useful for learning word representations for a particular task. For example, hypernyms and hyponyms report lower scores compared to the corpus only baseline on predicting semantic similarity for rare (RW) and ambiguous (SCWS) wordpairs.\nIn Table 2, we compare the proposed method against previously proposed word representation learning methods that use a semantic lexicon: RCM is the relational constrained model proposed by Yu and Dredze (2014), R-NET, C-NET, and RC-NET are proposed by Xu et al. (2014), and respectively use relational information, categorical information, and their union from the WordNet for learning word representations, and Retro is the retrofitting method proposed by Faruqui et al. (2015). Details of those methods are described in Section 2. For Retro, we use the publicly available implementation4 by the original authors, and use pretrained word representations on the same ukWaC corpus as used by the proposed method. Specifically, we retrofit word vectors produced by CBOW (Retro (CBOW)), and skipgram (Retro (SG)). Moreover, we retrofit the word vectors learnt by the corpus only baseline (Retro (corpus only)) to compare the proposed joint learning approach to the post-processing approach in retrofitting. Unfortunately, for RCM, R-NET, C-NET, and RC-NET their implementations, nor trained word vectors were publicly available. Consequently, we report the published results for those methods. In cases where the result on a particular benchmark dataset is not reported in the original publication, we have indicated this by a dash in Table 2.\nAmong the different semantic relation types compared in Table 1, we use the synonym relation which reports the best\n4https://github.com/mfaruqui/retrofitting\nperformances for the proposed method in the comparison in Table 2. All word embeddings compared in Table 2 are 300 dimensional and use the WordNet as the sentiment lexicon. From Table 2, we see that the proposed method reports the best scores on all benchmarks. Except for the smaller (only 65 word-pairs) RG dataset where the performance of retrofitting is similar to that of the proposed method, in all other benchmarks the proposed method statistically significantly outperforms prior work that use a semantic lexicon for word representation learning.\nWe evaluate the effect of the dimensionality d on the word representations learnt by the proposed method. For the limited availability of space, in Figure 1 we report results when we use the synonymy relation in the proposed method and on the semantic similarity benchmarks. Similar trends were observed for the other relation types and benchmarks. From Figure 1 we see that the performance of the proposed method is relatively stable across a wide range of dimensionalities. In particular, with as less as 100 dimensions we can obtain a level of performance that outperforms the corpus only baseline. On RG, MC, and MEN datasets we initially see a gradual increase in performance with the dimensionality of the word representations. However, this improvement saturates after 300 dimensions, which indicates that it is sufficient to consider 300 dimensional word representations in most cases. More importantly, adding new dimensions does not result in any decrease in performance.\nTo evaluate the effect of the corpus size on the performance of the proposed method, we select a random subset containing 10% of the sentences in the ukWaC corpus, which we call the small corpus, as opposed to the original large corpus. In Figure 2, we compare three settings: corpus (corresponds to the baseline method for learning using only the corpus, without the semantic lexicon), synonyms (proposed method with synonym relation), and part-\nholonyms (proposed method with part-holonym relation). Figure 2 shows the Spearman correlation coefficient on the MEN dataset for the semantic similarity prediction task. We see that in both small and large corpora settings we can improve upon the corpus only baseline by incorporating semantic relations from the WordNet. In particular, the improvement over the corpus only baseline is more prominent for the smaller corpus than the larger one. Similar trends were observed for the other relation types as well. This shows that when the size of the corpus is small, word representation learning methods can indeed benefit from a semantic lexicon."}, {"heading": "5 Conclusion", "text": "We proposed a method for using the information available in a semantic lexicon to improve the word representations learnt from a corpus. For this purpose, we proposed a global word co-occurrence prediction method using the semantic relations in the lexicon as a regularizer. Experiments using ukWaC as the corpus and WordNet as the semantic lexicon show that we can significantly improve word representations learnt using only the corpus by incorporating the information from the semantic lexicon. Moreover, the proposed method significantly outperforms previously proposed methods for learning word representations using both a corpus and a semantic lexicon in both a semantic similarity prediction task, and a word analogy detection task. The effectiveness of the semantic lexicon is prominent when the corpus size is small. Moreover, the performance of the proposed method is stable over a wide-range of dimensionalities of word representations. In future, we plan to apply the word representations learnt by the proposed method in downstream NLP applications to conduct extrinsic evaluations."}, {"heading": "633, 1965.", "text": "[2012] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compositionality through recursive matrix-vector spaces. In Proc. of EMNLP, pages"}, {"heading": "1201\u20131211, 2012.", "text": "[2010] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: A simple and general method for semi-supervised learning. In ACL, pages 384 \u2013 394, 2010.\n[2010] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. Journal of Aritificial Intelligence Research, 37:141 \u2013 188, 2010.\n[2014] Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. Rc-net: A general framework for incorporating knowledge into word representations. In Proc. of CIKM, pages 1219\u20131228, 2014.\n[2014] Mo Yu and Mark Dredze. Improving lexical embeddings with semantic knowledge. In Proc. of ACL, pages 545 \u2013 550, 2014."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection. Despite their success, these datadriven word representation learning methods do not consider the rich semantic relational structure between words in a cooccurring context. On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language. We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?. For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon. We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus. Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy.", "creator": "LaTeX with hyperref package"}}}