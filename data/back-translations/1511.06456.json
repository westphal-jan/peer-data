{"id": "1511.06456", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Task Loss Estimation for Sequence Prediction", "abstract": "Often the performance of a supervised machine learning task is evaluated with a loss function that cannot be directly optimized. A common solution to this problem is the simple optimization of a replacement loss function in the hope that the loss of interest in the task will decrease as well. The loss function defines the cost of all possible answers and is probably the most important element of the problem. In this paper, we argue that it is advantageous to use a replacement loss that is explicitly aware of the loss of interest in the task. To this end, we propose a general method to define such loss-conscious optimization criteria for a broad class of structured output problems. We are particularly interested in the end-to-end-to-end training scenario in which the system may produce only a discrete response, such as speech recognition or machine translation. To this end, we suggest training an estimator of the loss function ourselves and making predictions by looking at the output with the lowest estimated loss, showing that the theoretical method offered by a number of prerequisites can be applied, such as the one of the suggested method, and the one of the guaranteed date.", "histories": [["v1", "Thu, 19 Nov 2015 23:51:31 GMT  (54kb,D)", "https://arxiv.org/abs/1511.06456v1", "Submitted to ICLR 2016"], ["v2", "Fri, 27 Nov 2015 22:53:47 GMT  (80kb,D)", "http://arxiv.org/abs/1511.06456v2", "Submitted to ICLR 2016"], ["v3", "Fri, 8 Jan 2016 15:28:19 GMT  (89kb,D)", "http://arxiv.org/abs/1511.06456v3", "Submitted to ICLR 2016"], ["v4", "Tue, 19 Jan 2016 20:48:19 GMT  (90kb,D)", "http://arxiv.org/abs/1511.06456v4", "Submitted to ICLR 2016"]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dzmitry bahdanau", "dmitriy serdyuk", "phil\\'emon brakel", "nan rosemary ke", "jan chorowski", "aaron courville", "yoshua bengio"], "accepted": false, "id": "1511.06456"}, "pdf": {"name": "1511.06456.pdf", "metadata": {"source": "CRF", "title": "TASK LOSS ESTIMATION FOR SEQUENCE PREDICTION", "authors": ["Dzmitry Bahdanau", "Dmitriy Serdyuk", "Aaron Courville", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "There has been an increase of interest in learning systems that can solve tasks in an \u201cend-to-end\u201d fashion. An early example of such a system is a highly successful convolutional network handwriting recognition pipeline (LeCun et al., 1998). More recent examples are deep convolutional networks designed for image recognition (Krizhevsky et al., 2012), neural translation systems (Sutskever et al., 2014; Bahdanau et al., 2015a), and speech recognizers (Graves & Jaitly, 2014; Hannun et al., 2014a; Chorowski et al., 2015; Bahdanau et al., 2015b). Parts of end-to-end systems, such as image features extracted by convolutional networks, often successfully replace hand-designed ones (Yosinski et al., 2014). This demonstrates how useful it can be that all parts of a system are learned to solve the relevant task.\nIn practice however, it often happens that the relevant task loss function, such as error rate in classification, word error rate in speech recognition, or BLEU score in machine translation, is only used for model evaluation, while a different surrogate loss is used to train the model. There are several reasons for the evaluation loss \u2013 training loss discrepancy: the evaluation criterion may be non-differentiable, it can be non-convex or otherwise inconvenient to optimize, or one may want to emphasize certain problem-agnostic model properties, such as a class separation margin (Vapnik, 1998). For instance, classification models are often evaluated based on their error rates, which corresponds to a 0-1 task loss. However, people often minimize surrogate losses like the cross-entropy (Bishop, 2006) or the hinge loss (Vapnik, 1998) instead. For classification, these surrogate losses are well-motivated and their minimization tends to lead to a low error rate. It is not clear, however,\n\u2217Yoshua Bengio is a CIFAR Senior Fellow\nar X\niv :1\n51 1.\n06 45\n6v 4\n[ cs\n.L G\n] 1\n9 Ja\nn 20\n16\nthat the same methods should be preferred for structured output problems, in which typically there is a gradation in the quality of answers.\nIn this work, we revisit the problem of choosing an appropriate surrogate loss for training. We focus on the broad class of models that define a score for every input-output pair and make predictions by looking for the output with the lowest score. Our main idea is that if the scores defined by the model are approximately equal to the task loss, then the task loss of the model\u2019s prediction should be low. We hence propose to define the surrogate loss as the estimation error of a score function that is trained to mimic the task loss, a method we will refer to as task loss estimation. We prove that minimization of such a surrogate loss leads to the minimization of the targeted task loss as well, a property that we call consistency with the task loss. The main distinct feature of our new approach is that it prescribes a target value for the score of every input-output pair. This target value does not depend on the score of other outputs, which is the key property of the proposed method and the key difference from other approaches to define consistent surrogate losses, such as the generalized hinge loss used in Structured Support Vector Machines (Tsochantaridis et al., 2005).\nFurthermore, we apply the task loss estimation principle to derive new surrogate losses for sequence prediction models of the Encoder-Decoder family. The Decoder, typically a recurrent network, produces the score for an input-output pair by summing terms associated with every element of the sequence. The fact that the target for the score is fixed in our approach allows us to define targets for each of the terms separately. By doing so we strive to achieve two goals: to facilitate faster training and to ensure that the greedy search and the beam search used to obtain predictions from an Encoder-Decoder work reasonably well. To validate our ideas we carry out an experiment on a speech recognition task. We show that when no external language model is used using a new surrogate loss indeed results in a relative 13% improvement of the CER compared to cross-entropy training for an Encoder-Decoder speech recognizer."}, {"heading": "2 TASK LOSS ESTIMATION FOR SUPERVISED LEARNING", "text": "Basic Definitions Consider the broad class of supervised learning problems in which the trained learner is only allowed to deterministically produce a single answer y\u0302 \u2208 Y at run-time, when given an input x \u2208 X . After training, the learner\u2019s performance is evaluated in terms of the task loss L(x, y\u0302) that it suffered from outputting y\u0302 for x. We assume that the task loss is non-negative and that there exists a unique ground truth answer y = g(x) such that L(x, g(x)) = 0.1 During the training, the learner is provided with training pairs (xi, yi), where yi = g(xi). We assume that given the ground truth yi, the loss L(x, y\u0302) can be efficiently for any answer y\u0302.\nThe training problem is then defined as follows. Given a family of parametrized mappings {h\u03b1} , \u03b1 \u2208 A from X to Y , try to choose one that minimizes (as much as possible) the risk functional:\nR(\u03b1) = \u222b x L(x, h\u03b1(x))P (x)dx, (1)\nwhere P is an unknown data distribution. The choice must be made using only a sample S = {xi}Ni=1 from the distribution P with ground truth answers {yi} N i=1 available for xi \u2208 S.\nHere are two examples of task losses that are pretty much standard in some key supervised learning problems:\n\u2022 the 0-1 loss used in classification problems is L(x, y) = { 1, g(x) = y\n0, g(x) 6= y ;\n\u2022 the Levenshtein distance used in speech recognition is L(x, y) = \u03c1levenstein(g(x), y) is the minimum number of changes required to transform a transcript y into the correct transcript g(x).\n1Both assumptions are made to keep the exposition simple, they are not cricial for applicability of the task loss estimation approach.\nEmpirical Risk and Surrogate Losses Under the assumptions that S is big enough and the family A is limited or some form of regularization is introduced, the empirical risk R\u0302(\u03b1) can be minimized\nR\u0302(\u03b1) = 1\nN N\u2211 i=1 L(xi, h\u03b1(xi)), (2)\ninstead of R (Vapnik, 1998).\nA common practical issue with minimizing the empirical risk functional R\u0302(\u03b1) is that L(x, y) is often not differentiable with respect to y, which in turn renders R\u0302(\u03b1) non-differentiable with respect to \u03b1 and therefore difficult to optimize. The prevalent workaround is to define h\u03b1(x) as the minimum of a scoring function F\u03b1(x, y) (often also called energy):\nhmin\u03b1 (x) = argminy F\u03b1(x, y).\nParameters \u03b1 of the scoring function are chosen to minimize a (technically empirical) surrogate risk R(\u03b1) defined as the average surrogate loss L(xi, \u00b7):\nR(\u03b1) = 1 N N\u2211 i=1 L(xi, F\u03b1(xi)), (3)\nwhere F\u03b1(xi) \u2208 R|Y| is the vector of scores computed on all elements of Y2.\nWe argue that, for the transition from the empirical risk R\u0302 to the surrogate risk R to be helpful, a number of conditions should hold:\n1. It must be easy to compute predictions hmin\u03b1 (x). Thus F\u03b1(x, y) must be easy to minimize over y, at least in an approximate sense. For instance, in most classification problems this is not an issue at all because the output space Y is small. On the other hand, for structured output prediction this might be a significant issue.\n2. R should be simpler to optimize than R\u0302.\n3. Optimization ofR should result in optimization of R\u0302.\nLet us consider two examples of surrogate losses\n2Without loss of generality, we assume here that the output space is discrete.\n\u2022 The cross-entropy surrogate loss LCE is applicable when the scores F\u03b1(x, y) are interpreted as unnormalized negative log-probabilities:\nLCE(x, F\u03b1(x)) = F\u03b1(x, g(x))\u2212 log( \u2211 y\u2032\u2208Y exp(F\u03b1(x, y \u2032))), (4)\nRCE(\u03b1) = 1\nN N\u2211 i=1 LCE(xi, F\u03b1(xi)). (5)\nWith LCE choosing \u03b1 that minimizes RCE(\u03b1) corresponds to Maximum Likelihood Estimation (MLE).\n\u2022 A generalized hinge loss used in Structured Support Vector Machines (Tsochantaridis et al., 2005):\nLhinge(x, F\u03b1(x)) = max y (F\u03b1(x, g(x))\u2212 F\u03b1(x, y) + L(g(x), y), 0) .\nThe respective surrogate riskRhinge is defined similarly toRCE.\nBoth of these surrogate loss functions have properties that make them relatively simple to optimize. The cross-entropy is both differentiable and convex. The hinge loss is piecewise differentiable and convex as well. We refer the reader to LeCun et al. (2006) for a survey of surrogate loss functions (note that their definition of a loss function differs slightly from the one we use in this text).\nPopular surrogate losses are often agnostic to the task loss L, the cross-entropy surrogate loss LCE being a good example. Even if we find parameters \u03b1CE which make the cross-entropy RCE(\u03b1CE) arbitrary small, there is no guarantee that the empirical risk R\u0302(\u03b1CE) will also be small. However, some surrogate losses, such as the generalized hinge loss Lhinge, provide certain guarantees for the empirical risk. Specifically, one can see that L(x, hmin\u03b1 (x)) \u2264 Lhinge(x, F (x)), which implies R\u0302(\u03b1) \u2264 Rhinge(\u03b1), or simply put, minimizingRhinge necessarily pushes R\u0302 down.\nTask Loss Estimation In this paper we introduce a novel paradigm for building surrogate losses with guarantees similar to those of Lhinge. Namely, we propose to interpret the scoring function F as an estimate of the task loss L itself. In other words we want F\u03b1(x, y) \u2248 L(x, y). We can motivate this approach by showing that for the empirical risk to be low, it is sufficient for the task loss and the score to be similar at only two points: the ground truth g(x) and the prediction h\u03b1(x). We combine the estimation errors for these two outputs to obtain a new surrogate loss Lmin,min which we call the min-min loss. Theorem 1. Let Lmin,min be defined as follows:\nLmin, min(L(x), F\u03b1(x)) = |F\u03b1(x, y)|+ |L(x, y\u0302)\u2212 F\u03b1(x, y\u0302)|, (6)\nhere y = g(x), y\u0302 = h\u03b1(x). Then the respective surrogate risk Rmin,min provides the following bound on R\u0302\nR\u0302(\u03b1) \u2264 Rmin, min(\u03b1) +M(\u03b1), (7)\nwhere\nM(\u03b1) = 1\nN N\u2211 i=1 max (F (xi, y\u0302i)\u2212 F (xi, yi), 0) .\nFigure 1 illustrates the statement of Theorem 1. Simply put, the theorem says that if h\u03b1 = hmin\u03b1 , or if h\u03b1 is a good enough approximation of hmin\u03b1 such that the term M(\u03b1) is small, the surrogate loss Rmin,min is a sensible substitute for R\u0302. Please see Appendix for a formal proof of the theorem. The key difference of our new approach from the generalized hinge loss is that it assigns a fixed target L(x, y) for the score F (x, y) of every pair (x, y) \u2208 X \u00d7 Y . This target is independent of the values of F (x, y\u2032) for all other y\u2032 \u2208 Y . The knowledge that L is the target can be used at the stage of\ndesigning the model F\u03b1(x, y). For example, when y has a structure, a L(x, y) might be decomposed into separate targets for every element of y, thereby making optimization ofR more tractable. In consideration of optimization difficulties, our new surrogate loss Lmin, min is piece-wise smooth like Lhinge, but it is not convex and even not continuous. In practice, we tackle the optimization by fixing the outputs h\u03b1(x) for a subset of the sample S, improving Lmin, min with the fixed outputs by e.g. a gradient descent step, and doing the same iteratively."}, {"heading": "3 TASK LOSS ESTIMATION FOR SEQUENCE PREDICTION", "text": "In sequence prediction problems the outputs are sequences over an alphabet C. We assume that the alphabet is not too big, more specifically, that a loop over its elements is feasible. In addition we extend the alphabet C with a special end-of-sequence token $, creating the extended alphabet C = C \u222a {$}. For convenience, we assume that all valid output sequences must end with this token. Now we can formally define the output space as the set of all sequences which end with the end-of-sequence token Y = {y$ : y \u2208 C\u2217}, where C\u2217 denotes a set of all finite sequences over the alphabet C.\nWe will now describe how task loss estimation can be applied to sequence prediction for the following specific scenario:\n\u2022 The score function is an Encoder-Decoder model. \u2022 The prediction hmin\u03b1 is approximated with a beam search or a greedy search."}, {"heading": "3.1 ENCODER-DECODER MODEL", "text": "A popular model for sequence prediction is the Encoder-Decoder model. In this approach, the Decoder is trained to model the probability P (yj |z(x), y1...j\u22121) of the next token yj given a representation of the input z(x) produced by the Encoder, and the previous tokens y1...j\u22121, where y = g(x) is the ground truth output. Decoders are typically implemented using recurrent neural networks. Using the terminology of this paper, one can say that a standard Encoder-Decoder implements a parametrized function \u03b4\u03b1(c, x, y1...j\u22121) that defines the scoring function as follows:\nFED1\u03b1 (x, y) = |y|\u2211 j=1 \u2212 log q\u03b1(yj , x, y1...j), (8)\nq\u03b1(y j , x, y1...j) =\nexp ( \u03b4\u03b1(y j , x, y1...j) )\u2211\nc\u2208C exp (\u03b4\u03b1(c, x, y1...j))\n. (9)\nThe cross-entropy surrogate loss can be used for training Encoder-Decoders. Since the score function (8) defined by an Encoder-Decoder is a proper distribution, the exact formula for the surrogate loss is simpler than in Equation 4\nLCE(x, FED1\u03b1 (x)) = FED1\u03b1 (x, y) = |y|\u2211 j=1 \u2212 log q\u03b1(yj , x, y1...j\u22121),\nwhere y = g(x).\nExactly computing hmin\u03b1 is not possible for Encoder-Decoder models. A beam search procedure is used to compute an approximation hB\u03b1 , where B is the beam size. In beam search at every step k the beam, that is a set of B \u201cgood prefixes\u201d of length k, is replaced by a set of good prefixes of length k + 1. The transition is done by considering all continuations of the beam\u2019s sequences and leaving only those B candidates for which the partial sum of log q\u03b1 is minimal."}, {"heading": "3.2 APPLYING TASK LOSS ESTIMATION TO ENCODER-DECODERS", "text": "Adapting the Min-Min Loss. We want to keep the structure of the scoring function defined in Equation (8). However, the normalization carried out in (9) is not necessary any more, so our new\nscoring function is simply the sum of \u03b4\u03b1:\nFED2\u03b1 (x, y) = |y|\u2211 j=1 \u03b4\u03b1(y j , x, y1...j\u22121).\nNow, in theory, the min-min loss Lmin, min could be used for training FED2\u03b1 . However, there are two concerns which render this straight-forward approach less attractive:\n\u2022 Intuitively, constraining only the sum of \u03b4\u03b1 might provide not enough supervision for training. Namely, the gradient of Lmin, min would be the same with respect to all \u03b4\u03b1(y j , x, y1...j\u22121), which might hamper gradient-based optimization methods.\n\u2022 There is no guarantee that the beam search will be able to work with \u03b4\u03b1 values learnt this way.\nTo circumvent both of these potential issues, we propose to break the target loss L(x, y) into subtargets \u03b4jL(x, y) assigned token-wise. We define the optimistic task loss Lo(x, y) for an output prefix y as the loss of the best possible continuation of the prefix y. For completed output sequences, that is those ending with the end-of-sequence token, we say that the optimistic task loss is equal to the task loss. This results in the following formal definition:\nLo(x, y) =\n{ min z\u2208B\u2217 L(x, yz$), y \u2208 C\u2217;\nL(x, y), y \u2208 Y, (10)\nWe argue that the change of the optimistic task loss \u03b4o(yj , x, y1...j\u22121) = Lo(x, yc) \u2212 Lo(x, y) is a good target for \u03b4\u03b1(yj , x, y1...j\u22121). Indeed, the pruning during beam search is done by looking at\nthe sum s(x, y1...k) = k\u2211 j=1 \u03b4\u03b1(y j , x, yj\u22121) for the prefixes y from the beam. Informally, the pruning procedure should remove prefixes whose continuations are unlikely to be beneficial. The optimistic loss Lo(x, y) tells us what is the lowest loss one can obtain by continuing y in an arbitrary way, and hence, it can be used for selecting the prefixes to be continued. Assuming that the network learns to output \u03b4\u03b1(c, x, y1...j) \u2248 \u03b4o(c, x, y1...j), we can hope that pruning by sk(x, y1...j) \u2248 Lopt(x, y1...k) will keep the good prefixes in.\nOur new surrogate loss consisting of the sum of token-wise errors looks as follows:\nLEDmin, min(x, \u03b4\u03b1(x)) = |y|\u2211 j=1 |\u03b4\u03b1(yj , x, y1...j\u22121)\u2212 \u03b4o(yj , x, y1...j\u22121)| (11)\n+ |y\u0302|\u2211 j=1 |\u03b4\u03b1(y\u0302j , x, y\u03021...j\u22121)\u2212 \u03b4o(y\u0302j , x, y\u03021...j\u22121)|, (12)\nwhere y = g(x), y\u0302 = hmin\u03b1 (x). Note, that LEDmin,min extends our previous surrogate loss definition from (3) by working not on F\u03b1(x) but on its additive components \u03b4\u03b1(yj , x, y1...j\u22121). One can also see that LEDmin, min(x, \u03b4\u03b1(x)) \u2265 Lmin, min(x, \u03b4\u03b1(x)) because of the triangle inequality, which implies that the respective surrogate risk is a bound for the empirical riskREDmin, min \u2265 R\u0302(\u03b1).\nA careful reader might have noticed, that in practice we do not have access to LEDmin, min, because we can not compute hmin\u03b1 (x). The best we can have is LEDmin,B(x, y) defined in a similar way but using the beam search to compute y\u0302 = hB\u03b1 (x) instead of the intractable exact minimization. However, according to Theorem 1 minimizing LEDmin,B guarantees low empirical risk for beam search predictions hB\u03b1 (x), as long as the beam search finds an output with a score that is lower than the score of the ground truth. In our experience, this is usually the case for Encoder-Decoder models.\nA Loss for the Greedy Search One disadvantage of the approach with LEDmin,B is that computing the surrogate loss, and therefore also its gradients, becomes quite expensive. We address this issue by proposing another surrogate loss which only involves beam search with the beam size B = 1, also often called greedy search. The new surrogate loss LEDgreedy is defined as follows:\nLEDgreedy(x, \u03b4\u03b1(x)) = |y\u0302|\u2211 j=1 |\u03b4\u03b1(y\u0302j , x, y\u03021...j\u22121)\u2212 \u03b4o(y\u0302j , x, y\u03021...j\u22121)|+ |\u03b4\u03b1(cjmin, x, y\u0302 1...j\u22121)|, (13)\nwhere y\u0302 = h1\u03b1(x), c j min = argminc\u2208C \u03b4o(c, x, y 1...j\u22121). We can show, that optimizing the respective surrogate riskREDgreedy necessarily improves the performance of greedy search:\nTheorem 2. The empirical risk R\u0302greedy associated with using h1\u03b1 for giving predictions is bounded byREDgreedy, that is R\u0302greedy(\u03b1) \u2264 REDgreedy(\u03b1).\nThe proof can be found in the Appendix. Now, with the greedy search, the gradient of R\u0302greedy(\u03b1) can be computed just as fast as the gradient of the average cross-entropy, since the computation of the gradient can be combined with the search.\nTricks of the Trade Driven by our intuition about the training process we make two further changes to the loss Lgreedy. First, we change Equation 13 by adding all characters into consideration:\nLEDgreedy1(x, \u03b4\u03b1(x)) = |y\u0302|\u2211 j=1 \u2211 c\u2208C |\u03b4\u03b1(c, x, y\u03021...j\u22121)\u2212 \u03b4o(c, x, y\u03021...j\u22121)|. (14)\nOur reasoning is that by providing a more informative training signal at each step we help optimization. We note, that the bound on the empirical risk provided by the respective surrogate risk REDgreedy1(\u03b1) is looser then the one by REDgreedy(\u03b1) since REDgreedy \u2264 REDgreedy1. On the other hand, REDgreedy1 enforces a margin between the best next token and all the worse ones, which can possibly help generalization.\nFinally, we found LEDgreedy1 hard to optimize because the gradient of |a \u2212 b| is always either +1 or -1, that is it does not get smaller when a and b approach each other. To tackle this we replaced the absolute value by the square:\nLEDgreedy2(x, \u03b4\u03b1(x)) = |y\u0302|\u2211 j=1 \u2211 c\u2208C (\u03b4\u03b1(c, x, y\u0302 1...j\u22121)\u2212 \u03b4o(c, x, y\u03021...j\u22121))2.\nExample: Edit Distance We explain how the decomposition of the task loss L(x, y) into a sum |y|\u2211 j=1 \u03b4o(y j , x, y1...j\u22121) works on the example of the edit distance. The edit distance \u03c1levenstein(s1, s2) between two strings s1, s2 \u2208 C\u2217 is the minimal number of actions required to transform s1 into s2, where the actions allowed are token deletion, insertion and substitution. If the loss L(x, y) is defined as the edit distance \u03c1levenstein(g(x), y), there is a compact expression for the optimistic loss Lo(x, y):\nLo(x, y) =\n{ min\nk=|g(x)| k=0 \u03c1levenstein(y, g(x) 1...k), y \u2208 C\u2217, \u03c1levenstein(y, g(x)), y \u2208 Y.\n(15)\nEquation (15) formalizes the consideration that the optimal way to continue a prefix y is to append a suffix of the ground truth g(x). From the obtained expression forLo(x, y) one can see that \u03b4o(c, x, y) can only be 0 or -1 when c 6= $. Indeed, by definition \u03b4o \u2265 0, and also adding a character c to a prefix y can only change the edit distance \u03c1(y, g(x)1...k) by 1 at most. For the case of c = $ the value \u03b4o($, x, y) can be an arbitrarily large negative number, in particular for prefixes y which are shorter then g(x). It would be a waste of the model capacity to try to exactly approximate such larger numbers, and in practice we clip the values \u03b4o($, x, y) to be at most -5.\nAn attentive reader might have noticed, that for complex loss functions such as e.g. BLEU and METEOR computing the loss decomposition like we did it above might be significantly harder. However, we believe that by considering all ground truth suffixes one can often find a close to optimal continuation."}, {"heading": "4 RELATED WORK", "text": "In an early attempt to minimize the empirical risk for speech recognition models, word error rate scores were used to rescale a loss similar to the objective that is often referred to as Maximum Mutual Information (Povey & Woodland, 2002). For each sequence in the data, this objective requires a summation over all possible sequences to compute the expected word error rate from the groundtruth, something that is possible for only a restricted class of models. A recent survey (He et al., 2008) explains and documents improvements in speech recognition brought by other methods of discriminative training of speech recognition systems.\nIn the context of Encoder-Decoders for sequence generation, a curriculum learning (Bengio et al., 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al., 2015). It was shown that the performance on several sequence prediction tasks can be improved by gradually transitioning from a fully guided training scheme to one where the model is increasingly conditioned on symbols it generated itself to make training more similar to the decoding stage in which the model will be conditioned on its own predictions as well. While this approach has an intuitive appeal and clearly works well in some situations, it doesn\u2019t take the task loss into account and to our knowledge no clear theoretical motivation for this method has been provided yet. Another issue is that one needs to decide how fast to transition between the two different types of training schemes.\nRecently, methods for direct empirical risk minimization for structured prediction have been proposed that treat the model and the approximate inference procedure as a single black-box method for generating predictions (Stoyanov et al., 2011; Domke, 2012). The gradient of the loss is backpropagated through the approximate inference procedure itself. While this approach is certainly more direct than the optimization of some auxiliary loss, it requires the loss to be differentiable.\nHazan et al. (2010) propose a method for direct loss minimization that approximates the gradient of the task loss using a loss adjusted inference procedure. This method has been extended to Hidden Markov Models and applied to phoneme recognition (Keshet et al., 2011).\nFor a model that provides a distribution over structured output configurations, the gradient with respect to any expectation over that distribution can be estimated using a sampling approach. This technique has been used for speech recognition (Graves & Jaitly, 2014) to estimate the gradient of the transcription loss (i.e., the word error rate) and is equivalent to the REINFORCE method (Williams, 1992) from reinforcement learning. A downside of this method is that in many cases the gradient estimates have high variance. The method also assumes that it is possible and computationally\nfeasible to sample from the model. A related approach is to use an inference method to generate a list of the n best candidate output predictions according to the model (note that for this the model doesn\u2019t need to be probabilistic) and approximate the expected loss using an average over these candidate predictions Gao & He (2013). Similarly, one can anneal from a smooth expectation approximated with a large number of candidates towards the loss of a single prediction Smith & Eisner (2006)."}, {"heading": "5 EXPERIMENTAL SETUP AND RESULTS", "text": "For experimental confirmation3 of the theory discussed in Sections 2 and 3, we use a characterlevel speech recognition task similar to Bahdanau et al. (2015b). Like in our previous work, we used the Wall Street Journal (WSJ) speech corpus for our experiments. The model is trained on the full 81 hour \u2019train-si284\u2019 training set, we use the \u2019dev93\u2019 development set for validation and model selection, and we report the performance on the \u2019eval92\u2019 test set. The inputs to our models were sequences of feature vectors. Each feature vector contained the energy and 40 mel-filter bank features with their deltas and delta-deltas, which means that the dimensionality of the feature vector is 123. We use the standard trigram language model shipped with the WSJ dataset; in addition we experiment with its extended version created by Kaldi WSJ s5 recipe (Povey et al., 2011).\nOur main baseline is an Encoder-Decoder from our previous work on end-to-end speech recognition (Bahdanau et al., 2015b), trained with the cross-entropy surrogate loss. We trained a model with the same architecture but using the task loss estimation LEDgreedy2 criterion, which involves greedy prediction of the candidate sequence y\u0302 during training. Algorithm 1 formally describes our training procedure.\nOur main result is the 13% relative improvement of Character Error Rate that task loss estimation training brings compared to the baseline model when no external language model is used (see Table 1). This setup, being not typical for speech recognition research, is still an interesting benchmark for sequence prediction algorithms. We note, that the Word Error Rate of 18% we report here is the best in the literature. Another class of models for which results without the language model are sometimes reported are Connectionist Temporal Classification (CTC) models (Graves & Jaitly, 2014; Miao et al., 2015; Hannun et al., 2014b), and the best result we are aware of is 26.9% reported by Miao et al. (2015).\nIn our experiments with the language models we linearly interpolated the scores produced by the neural networks with the weights of the Finite State Transducer (FST), similarly to (Miao et al., 2015) and (Bahdanau et al., 2015b). Addition of language models resulted in a typical large performance improvement, but the advantage over the cross-entropy trained model was largely lost. Both the baseline and the experimental model perform worse than a combination of a CTC-trained network and a language model. As discussed in our previous work (Bahdanau et al., 2015b), we attribute it to the overfitting from which Encoder-Decoder models suffers due to their implicit language modelling capabilities.\nwhile LEDgreedy2 improves on the validation set do fetch a batch of input sequences B; generate y\u0302i for each xi from B using the greedy search; compute the score components \u03b4\u03b1(c, xi, y\u0302 1...j\u22121 i ) ;\ncompute the component-wise targets \u03b4o(c, xi, y\u0302 1...j\u22121 i ) as changes of the optimistic task loss;\nLEDgreedy2 = 1 |B| |B|\u2211 i=1 |y\u0302|\u2211 j=1 \u2211 c\u2208C ( \u03b4\u03b1(c, xi, y\u0302 1...j\u22121 i )\u2212max ( \u03b4o(c, xi, y\u0302 1...j\u22121 i ),\u22125 ))2 ;\ncompute the gradient of LEDgreedy2 and update the parameters \u03b1; end Algorithm 1: The training procedure used in our experiments. Note, that generation of y\u0302i and gradient computation can be combined in an efficient implementation, making it exactly as fast as cross-entropy training.\n3Our code is available at https://github.com/rizar/attention-lvcsr\nIt is notable, that the performance of the experimental model changes very little when we change the beam size from 10 to 1. An unexpected result of our experiments is that the sentence error rate for the loss estimation model is consistently lower. Cross-entropy is de-facto the standard surrogate loss for classifiers, and the sentence error rate is essentially the classification error, for which reasons we did not expect an improvement of this performance measure. This result suggests that for classification problems with very big number of classes the cross-entropy might be a non-optimal surrogate loss."}, {"heading": "6 CONCLUSION AND DISCUSSION", "text": "The main contributions of this work are twofold. First, we have developed a method for constructing surrogate loss functions that provide guarantees about the task loss. Second, we have demonstrated that such a surrogate loss for sequence prediction performs better than the cross-entropy surrogate loss at minimizing the character error rate for a speech recognition task.\nOur loss function is somewhat similar to the one used in the Structured SVM (Tsochantaridis et al., 2005). The main difference is that while the structured SVM uses the task loss to define the difference between the energies assigned to the correct and incorrect predictions, we use the task loss to directly define the desired score for all outputs. Therefore, the target value for the score of an output does not change during training.\nWe can also analyze our proposed loss from the perspective of score-landscape shaping (LeCun et al., 2006). Maximum likelihood loss applied to sequence prediction pulls down the score of correct sequences, while directly pulling up on the score of sequences differing in only one element. This is also known as teacher-forcing \u2013 the model is only trained to predict the next element of a correct prefixes of training sequences. In contrast, our proposed loss function defines the desired score level for all possible output sequences. Thus it is not only possible to train the model by lowering the score of the correct outputs and raising the score of neighboring incorrect ones, but by precisely raising the score of any incorrect one. Therefore, the model can be trained on its own mistakes.\nFuture work should investigate the applicability of our framework to other task loss functions like the BLEU score. Our results with the language models stress the importance of developing methods of joint training of the whole system, including the language model. Finally, theoretical work needs to\nbe done to extend our framework to different approximate inference algorithms as well and to be able to make stronger claims about the suitability of the surrogate losses for gradient-based optimization.\nAcknowledgments: We thank the developers of Theano (Bastien et al., 2012) and Blocks (van Merrie\u0308nboer et al., 2015) for their great work. We thank NSERC, Compute Canada, Canada Research Chairs, CIFAR, Samsung, Yandex, and National Science Center (Poland) for their support. We also thank Faruk Ahmed and David Krueger for valuable feedback."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of the ICLR 2015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "CoRR, abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "arXiv preprint arXiv:1506.03099,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher"], "venue": null, "citeRegEx": "Bishop and Christopher.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and Christopher.", "year": 2006}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1506.07503,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Generic methods for optimization-based modeling", "author": ["Domke", "Justin"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Domke and Justin.,? \\Q2012\\E", "shortCiteRegEx": "Domke and Justin.", "year": 2012}, {"title": "Training mrf-based phrase translation models using gradient ascent", "author": ["Gao", "Jianfeng", "He", "Xiaodong"], "venue": "In HLT-NAACL,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni Y", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan C", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "author": ["Hannun", "Awni Y", "Maas", "Andrew L", "Jurafsky", "Daniel", "Ng", "Andrew Y"], "venue": "arXiv preprint arXiv:1408.2873,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Direct loss minimization for structured prediction", "author": ["Hazan", "Tamir", "Keshet", "Joseph", "McAllester", "David A"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2010}, {"title": "Discriminative learning in sequential pattern recognition", "author": ["He", "Xiaodong", "Deng", "Li", "Chou", "Wu"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "He et al\\.,? \\Q2008\\E", "shortCiteRegEx": "He et al\\.", "year": 2008}, {"title": "Direct error rate minimization of hidden markov models", "author": ["Keshet", "Joseph", "Cheng", "Chih-Chieh", "Stoehr", "Mark", "McAllester", "David A", "Saul", "LK"], "venue": "In INTERSPEECH,", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Miao", "Yajie", "Gowayyed", "Mohammad", "Metze", "Florian"], "venue": "arXiv preprint arXiv:1507.08240,", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Minimum phone error and i-smoothing for improved discriminative training", "author": ["Povey", "Daniel", "Woodland", "Philip C"], "venue": "In Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Povey et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2002}, {"title": "The kaldi speech recognition toolkit", "author": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz", "Silovsky", "Jan", "Stemmer", "Georg", "Vesely", "Karel"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society,", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Minimum risk annealing for training log-linear models", "author": ["Smith", "David A", "Eisner", "Jason"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions,", "citeRegEx": "Smith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2006}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["Stoyanov", "Veselin", "Ropson", "Alexander", "Eisner", "Jason"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Stoyanov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Tsochantaridis", "Ioannis", "Joachims", "Thorsten", "Hofmann", "Thomas", "Altun", "Yasemin"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Statistical learning theory, volume 1", "author": ["Vapnik", "Vladimir Naumovich"], "venue": "Wiley New York,", "citeRegEx": "Vapnik and Naumovich.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Naumovich.", "year": 1998}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "An early example of such a system is a highly successful convolutional network handwriting recognition pipeline (LeCun et al., 1998).", "startOffset": 112, "endOffset": 132}, {"referenceID": 15, "context": "More recent examples are deep convolutional networks designed for image recognition (Krizhevsky et al., 2012), neural translation systems (Sutskever et al.", "startOffset": 84, "endOffset": 109}, {"referenceID": 22, "context": ", 2012), neural translation systems (Sutskever et al., 2014; Bahdanau et al., 2015a), and speech recognizers (Graves & Jaitly, 2014; Hannun et al.", "startOffset": 36, "endOffset": 84}, {"referenceID": 6, "context": ", 2015a), and speech recognizers (Graves & Jaitly, 2014; Hannun et al., 2014a; Chorowski et al., 2015; Bahdanau et al., 2015b).", "startOffset": 33, "endOffset": 126}, {"referenceID": 27, "context": "Parts of end-to-end systems, such as image features extracted by convolutional networks, often successfully replace hand-designed ones (Yosinski et al., 2014).", "startOffset": 135, "endOffset": 158}, {"referenceID": 23, "context": "This target value does not depend on the score of other outputs, which is the key property of the proposed method and the key difference from other approaches to define consistent surrogate losses, such as the generalized hinge loss used in Structured Support Vector Machines (Tsochantaridis et al., 2005).", "startOffset": 276, "endOffset": 305}, {"referenceID": 23, "context": "\u2022 A generalized hinge loss used in Structured Support Vector Machines (Tsochantaridis et al., 2005): Lhinge(x, F\u03b1(x)) = max y (F\u03b1(x, g(x))\u2212 F\u03b1(x, y) + L(g(x), y), 0) .", "startOffset": 70, "endOffset": 99}, {"referenceID": 16, "context": "We refer the reader to LeCun et al. (2006) for a survey of surrogate loss functions (note that their definition of a loss function differs slightly from the one we use in this text).", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": "A recent survey (He et al., 2008) explains and documents improvements in speech recognition brought by other methods of discriminative training of speech recognition systems.", "startOffset": 16, "endOffset": 33}, {"referenceID": 4, "context": "In the context of Encoder-Decoders for sequence generation, a curriculum learning (Bengio et al., 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 3, "context": ", 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al., 2015).", "startOffset": 148, "endOffset": 169}, {"referenceID": 21, "context": "Recently, methods for direct empirical risk minimization for structured prediction have been proposed that treat the model and the approximate inference procedure as a single black-box method for generating predictions (Stoyanov et al., 2011; Domke, 2012).", "startOffset": 219, "endOffset": 255}, {"referenceID": 14, "context": "This method has been extended to Hidden Markov Models and applied to phoneme recognition (Keshet et al., 2011).", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "In the context of Encoder-Decoders for sequence generation, a curriculum learning (Bengio et al., 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al., 2015). It was shown that the performance on several sequence prediction tasks can be improved by gradually transitioning from a fully guided training scheme to one where the model is increasingly conditioned on symbols it generated itself to make training more similar to the decoding stage in which the model will be conditioned on its own predictions as well. While this approach has an intuitive appeal and clearly works well in some situations, it doesn\u2019t take the task loss into account and to our knowledge no clear theoretical motivation for this method has been provided yet. Another issue is that one needs to decide how fast to transition between the two different types of training schemes. Recently, methods for direct empirical risk minimization for structured prediction have been proposed that treat the model and the approximate inference procedure as a single black-box method for generating predictions (Stoyanov et al., 2011; Domke, 2012). The gradient of the loss is backpropagated through the approximate inference procedure itself. While this approach is certainly more direct than the optimization of some auxiliary loss, it requires the loss to be differentiable. Hazan et al. (2010) propose a method for direct loss minimization that approximates the gradient of the task loss using a loss adjusted inference procedure.", "startOffset": 83, "endOffset": 1468}, {"referenceID": 19, "context": "We use the standard trigram language model shipped with the WSJ dataset; in addition we experiment with its extended version created by Kaldi WSJ s5 recipe (Povey et al., 2011).", "startOffset": 156, "endOffset": 176}, {"referenceID": 17, "context": "Another class of models for which results without the language model are sometimes reported are Connectionist Temporal Classification (CTC) models (Graves & Jaitly, 2014; Miao et al., 2015; Hannun et al., 2014b), and the best result we are aware of is 26.", "startOffset": 147, "endOffset": 211}, {"referenceID": 17, "context": "In our experiments with the language models we linearly interpolated the scores produced by the neural networks with the weights of the Finite State Transducer (FST), similarly to (Miao et al., 2015) and (Bahdanau et al.", "startOffset": 180, "endOffset": 199}, {"referenceID": 0, "context": "For experimental confirmation3 of the theory discussed in Sections 2 and 3, we use a characterlevel speech recognition task similar to Bahdanau et al. (2015b). Like in our previous work, we used the Wall Street Journal (WSJ) speech corpus for our experiments.", "startOffset": 135, "endOffset": 159}, {"referenceID": 0, "context": "For experimental confirmation3 of the theory discussed in Sections 2 and 3, we use a characterlevel speech recognition task similar to Bahdanau et al. (2015b). Like in our previous work, we used the Wall Street Journal (WSJ) speech corpus for our experiments. The model is trained on the full 81 hour \u2019train-si284\u2019 training set, we use the \u2019dev93\u2019 development set for validation and model selection, and we report the performance on the \u2019eval92\u2019 test set. The inputs to our models were sequences of feature vectors. Each feature vector contained the energy and 40 mel-filter bank features with their deltas and delta-deltas, which means that the dimensionality of the feature vector is 123. We use the standard trigram language model shipped with the WSJ dataset; in addition we experiment with its extended version created by Kaldi WSJ s5 recipe (Povey et al., 2011). Our main baseline is an Encoder-Decoder from our previous work on end-to-end speech recognition (Bahdanau et al., 2015b), trained with the cross-entropy surrogate loss. We trained a model with the same architecture but using the task loss estimation L greedy2 criterion, which involves greedy prediction of the candidate sequence \u0177 during training. Algorithm 1 formally describes our training procedure. Our main result is the 13% relative improvement of Character Error Rate that task loss estimation training brings compared to the baseline model when no external language model is used (see Table 1). This setup, being not typical for speech recognition research, is still an interesting benchmark for sequence prediction algorithms. We note, that the Word Error Rate of 18% we report here is the best in the literature. Another class of models for which results without the language model are sometimes reported are Connectionist Temporal Classification (CTC) models (Graves & Jaitly, 2014; Miao et al., 2015; Hannun et al., 2014b), and the best result we are aware of is 26.9% reported by Miao et al. (2015). In our experiments with the language models we linearly interpolated the scores produced by the neural networks with the weights of the Finite State Transducer (FST), similarly to (Miao et al.", "startOffset": 135, "endOffset": 1982}, {"referenceID": 17, "context": "The last section contains results from Graves & Jaitly (2014) and Miao et al. (2015). We found that increasing the beam size over 100 for the CE model does not give any improvement.", "startOffset": 66, "endOffset": 85}, {"referenceID": 23, "context": "Our loss function is somewhat similar to the one used in the Structured SVM (Tsochantaridis et al., 2005).", "startOffset": 76, "endOffset": 105}], "year": 2016, "abstractText": "Often, the performance on a supervised machine learning task is evaluated with a task loss function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a surrogate loss function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call consistency with the task loss. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error may be used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this property to design specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant 13% relative improvement in terms of Character Error Rate (CER) in the case when no extra corpora are used for language modeling.", "creator": "LaTeX with hyperref package"}}}