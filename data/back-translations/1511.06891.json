{"id": "1511.06891", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "abstract": "Unlike existing work, our active learning problem involves not only selecting the most informative sample locations to observe, but also the type of measurements at each selected location to minimize the predictive uncertainty (i.e. posterior common entropy) of a target phenomenon of interest given a sample budget. Unfortunately, such an entropy criterion scales poorly in the number of sample locations and the selected observations when optimized. To solve this problem, we first use a common structure to save MOGP models to derive a new active learning criterion. Then, we use a relaxed form of the submodularity characteristic of our new criterion to develop a polynomic time approximation algorithm that ensures a constant factor approximation of what is achieved through the optimal compilation of selected observations. Empirical evaluation based on real GP models shows that our approach applies the individual MOGP methods in the real world.", "histories": [["v1", "Sat, 21 Nov 2015 15:08:53 GMT  (176kb,D)", "https://arxiv.org/abs/1511.06891v1", "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 13 pages"], ["v2", "Tue, 24 Nov 2015 08:45:36 GMT  (176kb,D)", "http://arxiv.org/abs/1511.06891v2", "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 13 pages"]], "COMMENTS": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended version with proofs, 13 pages", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["yehong zhang", "trong nghia hoang", "kian hsiang low", "mohan s kankanhalli"], "accepted": true, "id": "1511.06891"}, "pdf": {"name": "1511.06891.pdf", "metadata": {"source": "CRF", "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "authors": ["Yehong Zhang", "Trong Nghia Hoang", "Kian Hsiang Low", "Mohan Kankanhalli"], "emails": ["mohan}@comp.nus.edu.sg,", "idmhtn@nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "For many budget-constrained environmental sensing and monitoring applications in the real world, active learning/sensing is an attractive, frugal alternative to passive high-resolution (hence, prohibitively costly) sampling of the spatially varying target phenomenon of interest. Different from the latter, active learning aims to select and gather the most informative observations for modeling and predicting the spatially varying phenomenon given some sampling budget constraints (e.g., quantity of deployed sensors, energy consumption, mission time).\nIn practice, the target phenomenon often coexists and correlates well with some auxiliary type(s) of phenomena whose measurements may be more spatially correlated, less noisy (e.g., due to higher-quality sensors), and/or less tedious to sample (e.g., due to greater availability/quantity, higher sampling rate, and/or lower sampling cost of deployed sensors of these type(s)) and can consequently be\n1This paper is an extended version (with proofs) of (Zhang et al. 2016).\nexploited for improving its prediction. For example, to monitor soil pollution by some heavy metal (e.g., Cadmium), its complex and time-consuming extraction from soil samples can be alleviated by supplementing its prediction with correlated auxiliary types of soil measurements (e.g., pH) that are easier to sample (Goovaerts 1997). Similarly, to monitor algal bloom in the coastal ocean, plankton abundance correlates well with auxiliary types of ocean measurements (e.g., chlorophyll a, temperature, and salinity) that can be sampled more readily. Other examples of real-world applications include remote sensing, traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015), monitoring of groundwater and indoor environmental quality (Xu et al. 2014), and precision agriculture (Webster and Oliver 2007), among others. All of the above practical examples motivate the need to design and develop an active learning algorithm that selects not just the most informative sampling locations to be observed but also the types of measurements (i.e., target and/or auxiliary) at each selected location for minimizing the predictive uncertainty of unobserved areas of a target phenomenon given a sampling budget, which is the focus of our work here2.\nTo achieve this, we model all types of coexisting phenomena (i.e., target and auxiliary) jointly as a multi-output Gaussian process (MOGP) (A\u0301lvarez and Lawrence 2011; Bonilla, Chai, and Williams 2008; Teh and Seeger 2005), which allows the spatial correlation structure of each type of phenomenon and the cross-correlation structure between different types of phenomena to be formally characterized. More importantly, unlike the non-probabilistic multivariate regression methods, the probabilistic MOGP regression model allows the predictive uncertainty of the target phenomenon (as well as the auxiliary phenomena) to be formally quantified (e.g., based on entropy or mutual information criterion) and consequently exploited for deriving the active learning criterion.\nTo the best of our knowledge, this paper is the first to present an efficient algorithm for active learning of a MOGP\n2Our work here differs from multivariate spatial sampling algorithms (Bueso et al. 1999; Le, Sun, and Zidek 2003) that aim to improve the prediction of all types of coexisting phenomena, for which existing active learning algorithms for sampling measurements only from the target phenomenon can be extended and applied straightforwardly, as detailed in Section 3.\nar X\niv :1\n51 1.\n06 89\n1v 2\n[ st\nat .M\nL ]\n2 4\nN ov\n2 01\nmodel. We consider utilizing the entropy criterion to measure the predictive uncertainty of a target phenomenon, which is widely used for active learning of a single-output GP model. Unfortunately, for the MOGP model, such a criterion scales poorly in the number of candidate sampling locations of the target phenomenon (Section 3) and even more so in the number of selected observations (i.e., sampling budget) when optimized (Section 4). To resolve this scalability issue, we first exploit a structure common to a unifying framework of sparse MOGP models (Section 2) for deriving a novel active learning criterion (Section 3). Then, we define a relaxed notion of submodularity3 called -submodularity and exploit the -submodularity property of our new criterion for devising a polynomial-time approximation algorithm that guarantees a constant-factor approximation of that achieved by the optimal set of selected observations (Section 4). We empirically evaluate the performance of our proposed algorithm using three real-world datasets (Section 5)."}, {"heading": "2 Modeling Coexisting Phenomena with Multi-Output Gaussian Process (MOGP)", "text": "Convolved MOGP (CMOGP) Regression. A number of MOGP models such as co-kriging (Webster and Oliver 2007), parameter sharing (Skolidis 2012), and linear model of coregionalization (LMC) (Teh and Seeger 2005; Bonilla, Chai, and Williams 2008) have been proposed to handle multiple types of correlated outputs. A generalization of LMC called the convolved MOGP (CMOGP) model has been empirically demonstrated in (A\u0301lvarez and Lawrence 2011) to outperform the others and will be the MOGP model of our choice due to its approximation whose structure can be exploited for deriving our active learning criterion and in turn an efficient approximation algorithm, as detailed later.\nLet M types of coexisting phenomena be defined to vary as a realization of a CMOGP over a domain corresponding to a set D of sampling locations such that each location x \u2208 D is associated with noisy realized (random) output measurement y\u3008x,i\u3009 (Y\u3008x,i\u3009) if x is observed (unobserved) for type i for i = 1, . . . ,M . Let D+i , {\u3008x, i\u3009}x\u2208D and D+ , \u22c3M i=1D + i . Then, measurement Y\u3008x,i\u3009 of type i is defined as a convolution between a smoothing kernel Ki(x) and a latent measurement function L(x)4 corrupted by an additive noise \u03b5i \u223c N (0, \u03c32ni) with noise variance \u03c3 2 ni :\nY\u3008x,i\u3009 , \u222b x\u2032\u2208D Ki(x\u2212 x\u2032) L(x\u2032) dx\u2032 + \u03b5i .\nAs shown in (A\u0301lvarez and Lawrence 2011), if {L(x)}x\u2208D is a GP, then {Y\u3008x,i\u3009}\u3008x,i\u3009\u2208D+ is also a GP, that is, every finite subset of {Y\u3008x,i\u3009}\u3008x,i\u3009\u2208D+ follows a multivariate\n3The original notion of submodularity has been used in (Krause and Golovin 2014; Krause, Singh, and Guestrin 2008) to theoretically guarantee the performance of their algorithms for active learning of a single-output GP model.\n4To ease exposition, we consider a single latent function. Note, however, that multiple latent functions can be used to improve the fidelity of modeling, as shown in (A\u0301lvarez and Lawrence 2011). More importantly, our proposed algorithm and theoretical results remain valid with multiple latent functions.\nGaussian distribution. Such a GP is fully specified by its prior mean \u00b5\u3008x,i\u3009 , E[Y\u3008x,i\u3009] and covariance \u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009 , cov[Y\u3008x,i\u3009, Y\u3008x\u2032,j\u3009] for all \u3008x, i\u3009, \u3008x\u2032, j\u3009 \u2208 D+, the latter of which characterizes the spatial correlation structure for each type of phenomenon (i.e., i = j) and the cross-correlation structure between different types of phenomena (i.e., i 6= j). Specifically, let {L(x)}x\u2208D be a GP with prior covariance \u03c3xx\u2032 , N (x \u2212 x\u2032|0, P\u221210 ) and Ki(x) , \u03c3siN (x|0, P \u22121 i ) where \u03c32si is the signal variance controlling the intensity of measurements of type i, P0 and Pi are diagonal precision matrices controlling, respectively, the degrees of correlation between latent measurements and cross-correlation between latent and type imeasurements, and 0 denotes a column vector comprising components of value 0. Then,\n\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009=\u03c3si\u03c3sjN (x\u2212x\u2032|0, P\u221210 +P \u22121 i +P \u22121 j )+\u03b4 ij xx\u2032\u03c3 2 ni (1) where \u03b4ijxx\u2032 is a Kronecker delta of value 1 if i = j and x = x\u2032, and 0 otherwise.\nSupposing a column vector yX of realized measurements is available for some set X , \u22c3M i=1Xi of tuples of observed locations and their corresponding measurement types where Xi \u2282 D+i , a CMOGP regression model can exploit these observations to provide a Gaussian predictive distribution N (\u00b5Z|X ,\u03a3ZZ|X) of the measurements for any set Z \u2286 D+ \\ X of tuples of unobserved locations and their corresponding measurement types with the following posterior mean vector and covariance matrix:\n\u00b5Z|X , \u00b5Z + \u03a3ZX\u03a3 \u22121 XX(yX \u2212 \u00b5X)\n\u03a3ZZ|X , \u03a3ZZ \u2212 \u03a3ZX\u03a3\u22121XX\u03a3XZ (2)\nwhere \u03a3AA\u2032 , (\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009)\u3008x,i\u3009\u2208A,\u3008x\u2032,j\u3009\u2208A\u2032 and \u00b5A , (\u00b5\u3008x,i\u3009) > \u3008x,i\u3009\u2208A for any A,A \u2032 \u2286 D+.\nSparse CMOGP Regression. A limitation of the CMOGP model is its poor scalability in the number |X| of observations: Computing its Gaussian predictive distribution (2) requires inverting \u03a3XX , which incurs O(|X|3) time. To improve its scalability, a unifying framework of sparse CMOGP regression models such as the deterministic training conditional, fully independent training conditional, and partially independent training conditional (PITC) approximations (A\u0301lvarez and Lawrence 2011) exploit a vector LU , (L(x))>x\u2208U of inducing measurements for some small set U \u2282 D of inducing locations (i.e., |U | |D|) to approximate each measurement Y\u3008x,i\u3009:\nY\u3008x,i\u3009 \u2248 \u222b x\u2032\u2208D Ki(x\u2212 x\u2032) E[L(x\u2032)|LU ] dx\u2032 + \u03b5i .\nThey also share two structural properties that can be exploited for deriving our active learning criterion and in turn an efficient approximation algorithm: (P1) Measurements of different types (i.e., YD+i and YD+j for i 6= j) are conditionally independent given LU , and (P2) YX and YZ are conditionally independent given LU . PITC will be used as the sparse CMOGP regression model in our work here since the others in the unifying framework impose further assumptions. With the above structural properties, PITC can utilize\nthe observations to provide a Gaussian predictive distribution N (\u00b5PITCZ|X ,\u03a3 PITC ZZ|X) where\n\u00b5PITCZ|X , \u00b5Z + \u0393ZX(\u0393XX + \u039bX) \u22121(yX \u2212 \u00b5X)\n\u03a3PITCZZ|X , \u0393ZZ + \u039bZ \u2212 \u0393ZX(\u0393XX + \u039bX) \u22121\u0393XZ\n(3)\nsuch that \u0393AA\u2032 , \u03a3AU\u03a3\u22121UU\u03a3UA\u2032 for any A,A \u2032 \u2286 D+, \u03a3AU (\u03a3UU ) is a covariance matrix with covariance components \u03c3\u3008x,i\u3009x\u2032 = \u03c3siN (x\u2212x\u2032|0, P\u221210 +P \u22121 i ) for all \u3008x, i\u3009 \u2208 A and x\u2032 \u2208 U (\u03c3xx\u2032 for all x, x\u2032 \u2208 U ), \u03a3UA\u2032 is the transpose of \u03a3A\u2032U , and \u039bA is a block-diagonal matrix constructed from the M diagonal blocks of \u03a3AA|U , \u03a3AA \u2212 \u0393AA for any A \u2286 D+, each of which is a matrix \u03a3AiAi|U for i = 1, . . . ,M where Ai \u2286 D+i and A , \u22c3M i=1Ai. Note that computing (3) does not require the inducing locations U to be observed. Also, the covariance matrix \u03a3XX in (2) is approximated by a reduced-rank matrix \u0393XX summed with the resulting sparsified residual matrix \u039bX . So, by using the matrix inversion lemma to invert the approximated covariance matrix \u0393XX + \u039bX and applying some algebraic manipulations, computing (3) incursO(|X|(|U |2 + (|X|/M)2)) time (A\u0301lvarez and Lawrence 2011) in the case of |U | \u2264 |X| and evenly distributed observations among all M types."}, {"heading": "3 Active Learning of CMOGP", "text": "Recall from Section 1 that the entropy criterion can be used to measure the predictive uncertainty of the unobserved areas of a target phenomenon. Using the CMOGP model (Section 2), the Gaussian posterior joint entropy (i.e., predictive uncertainty) of the measurements YZ for any set Z \u2286 D+ \\ X of tuples of unobserved locations and their corresponding measurement types can be expressed in terms of its posterior covariance matrix \u03a3ZZ|X (2) which is independent of the realized measurements yX :\nH(YZ |YX) , 1\n2 log(2\u03c0e)|Z||\u03a3ZZ|X | .\nLet index t denote the type of measurements of the target phenomenon5. Then, active learning of a CMOGP model involves selecting an optimal set X\u2217 , \u22c3M i=1X \u2217 i of N tuples (i.e., sampling budget) of sampling locations and their corresponding measurement types to be observed that minimize the posterior joint entropy of type t measurements at the remaining unobserved locations of the target phenomenon:\nX\u2217 , arg min X:|X|=N H(YVt\\Xt |YX) (4)\nwhere Vt \u2282 D+t is a finite set of tuples of candidate sampling locations of the target phenomenon and their corresponding measurement type t available to be selected for observation. However, evaluating the H(YVt\\Xt |YX) term in (4) incurs O(|Vt|3 + N3) time, which is prohibitively expensive when the target phenomenon is spanned by a large number |Vt| of candidate sampling locations. If auxiliary types of phenomena are missing or ignored (i.e.,\n5Our proposed algorithm can be extended to handle multiple types of target phenomena, as demonstrated in Section 5.\nM = 1), then such a computational difficulty can be eased instead by solving the well-known maximum entropy sampling (MES) problem (Shewry and Wynn 1987): X\u2217t = arg maxXt:|Xt|=N H(YXt) which can be proven to be equivalent to (4) by using the chain rule for entropy H(YVt) = H(YXt) + H(YVt\\Xt |YXt) and noting that H(YVt) is a constant. Evaluating the H(YXt) term in MES incurs O(|Xt|3) time, which is independent of |Vt|. Such an equivalence result can in fact be extended and applied to minimizing the predictive uncertainty of all M types of coexisting phenomena, as exploited by multivariate spatial sampling algorithms (Bueso et al. 1999; Le, Sun, and Zidek 2003):\narg max X:|X|=N H(YX) = arg min X:|X|=N\nH(YV \\X |YX), (5)\nwhere V , \u22c3M\ni=1 Vi and Vi is defined in a similar manner to Vt but for measurement type i 6= t. This equivalence result (5) also follows from the chain rule for entropy H(YV ) = H(YX) +H(YV \\X |YX) and the fact that H(YV ) is a constant. Unfortunately, it is not straightforward to derive such an equivalence result for our active learning problem (4) in which a target phenomenon of interest coexists with auxiliary types of phenomena (i.e., M > 1): If we consider maximizing H(YX) or H(YXt), then it is no longer equivalent to minimizing H(YVt\\Xt |YX) (4) as the sum of the two entropy terms is not necessarily a constant.\nExploiting Sparse CMOGP Model Structure. We derive a new equivalence result by considering instead a constant entropyH(YVt |LU ) that is conditioned on the inducing measurements LU used in sparse CMOGP regression models (Section 2). Then, by using the chain rule for entropy and structural property P2 shared by sparse CMOGP regression models in the unifying framework (A\u0301lvarez and Lawrence 2011) described in Section 2, (4) can be proven (see Appendix A) to be equivalent to\nX\u2217 , arg max X:|X|=N H(YXt |LU )\u2212 I(LU ;YVt\\Xt |YX) (6)\nwhere\nI(LU ;YVt\\Xt |YX) , H(LU |YX)\u2212H(LU |YX\u222aVt\\Xt)(7)\nis the conditional mutual information between LU and YVt\\Xt given YX . Our novel active learning criterion in (6) exhibits an interesting exploration-exploitation trade-off: The inducing measurements LU can be viewed as latent structure of the sparse CMOGP model to induce conditional independence properties P1 and P2. So, on one hand, maximizing the H(YXt |LU ) term aims to select tuples Xt of locations with the most uncertain measurements YXt of the target phenomenon and their corresponding type t to be observed given the latent model structure LU (i.e., exploitation). On the other hand, minimizing the I(LU ;YVt\\Xt |YX) term (7) aims to select tuplesX to be observed (i.e., possibly of measurement types i 6= t) so as to rely less on measurements YVt\\Xt of type t at the remaining unobserved locations of the target phenomenon to infer latent model structure LU (i.e., exploration) since YVt\\Xt won\u2019t be sampled.\nSupposing |U | \u2264 |Vt|, evaluating our new active learning criterion in (6) incurs O(|U |3 + N3) time for every X \u2282 V and a one-off cost of O(|Vt|3) time (Appendix B). In contrast, computing the original criterion in (4) requires O(|Vt|3 +N3) time for every X \u2282 V , which is more costly, especially when the number N of selected observations is much less than the number |Vt| of candidate sampling locations of the target phenomenon due to, for example, a tight sampling budget or a large sampling domain that usually occurs in practice. The trick to achieving such a computational advantage can be inherited by our approximation algorithm to be described next."}, {"heading": "4 Approximation Algorithm", "text": "Our novel active learning criterion in (6), when optimized, still suffers from poor scalability in the number N of selected observations (i.e., sampling budget) like the old criterion in (4) because it involves evaluating a prohibitively large number of candidate selections of sampling locations and their corresponding measurement types (i.e., exponential in N ). However, unlike the old criterion, it is possible to devise an efficient approximation algorithm with a theoretical performance guarantee to optimize our new criterion, which is the main contribution of our work in this paper.\nThe key idea of our proposed approximation algorithm is to greedily select the next tuple of sampling location and its corresponding measurement type to be observed that maximally increases our criterion in (6), and iterate this till N tuples are selected for observation. Specifically, let\nF (X) , H(YXt |LU )\u2212I(LU ;YVt\\Xt |YX)+I(LU ;YVt)(8) denote our active learning criterion in (6) augmented by a positive constant I(LU ;YVt) to make F (X) non-negative. Such an additive constant I(LU ;YVt) is simply a technical necessity for proving the performance guarantee and does not affect the outcome of the optimal selection (i.e., X\u2217 = arg maxX:|X|=N F (X)). Then, our approximation algorithm greedily selects the next tuple \u3008x, i\u3009 of sampling location x and its corresponding measurement type i that maximizes F (X \u222a {\u3008x, i\u3009})\u2212 F (X): \u3008x, i\u3009+, arg max\n\u3008x,i\u3009\u2208V \\X F (X \u222a {\u3008x, i\u3009})\u2212 F (X)\n= arg max \u3008x,i\u3009\u2208V \\X H(Y\u3008x,i\u3009|YX)\u2212 \u03b4iH(Y\u3008x,i\u3009|YX\u222aVt\\Xt)\n(9) where \u03b4i is a Kronecker delta of value 0 if i = t, and 1 otherwise. The derivation of (9) is in Appendix C. Our algorithm updates X \u2190 X \u222a {\u3008x, i\u3009+} and iterates the greedy selection (9) and the update till |X| = N (i.e., sampling budget is depleted). The intuition to understanding (9) is that our algorithm has to choose between observing a sampling location with the most uncertain measurement (i.e., H(Y\u3008x,t\u3009|YX)) of the target phenomenon (i.e., type t) vs. that for an auxiliary type i 6= t inducing the largest reduction in predictive uncertainty of the measurements at the remaining unobserved locations of the target phenomenon since H(Y\u3008x,i\u3009|YX) \u2212 H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) = I(Y\u3008x,i\u3009;YVt\\Xt |YX) = H(YVt\\Xt |YX)\u2212H(YVt\\Xt |YX\u222a{\u3008x,i\u3009}).\nIt is also interesting to figure out whether our approximation algorithm may avoid selecting tuples of a certain auxiliary type i 6= t and formally analyze the conditions under which it will do so, as elucidated in the following result:\nProposition 1 Let V-t , \u22c3 i 6=t Vi, X-t , \u22c3\ni 6=tXi, \u03c1i , \u03c32si/\u03c3 2 ni , and R(\u3008x, i\u3009, Vt \\ Xt) , \u2211 \u3008x\u2032,t\u3009\u2208Vt\\Xt N (x \u2212 x\u2032|0, P\u221210 + P \u22121 i + P \u22121 t )\n2. Assuming absence of suppressor variables, H(Y\u3008x,i\u3009|YX) \u2212 H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) \u2264 0.5 log(1+4\u03c1t\u03c1iR(\u3008x, i\u3009, Vt\\Xt)) for any \u3008x, i\u3009 \u2208 V-t\\X-t. Its proof (Appendix D) relies on the following assumption of the absence of suppressor variables which holds in many practical cases (Das and Kempe 2008): Conditioning does not make Y\u3008x,i\u3009 and Y\u3008x\u2032,t\u3009 more correlated for any \u3008x, i\u3009 \u2208 V-t \\ X-t and \u3008x\u2032, t\u3009 \u2208 Vt \\ Xt. Proposition 1 reveals that when the signal-to-noise ratio \u03c1i of auxiliary type i is low (e.g., poor-quality measurements due to high noise) and/or the cross correlation (1) between measurements of the target phenomenon and auxiliary type i is small due to low \u03c32st\u03c3 2 siR(\u3008x, i\u3009, Vt \\Xt), our greedy criterion in (9) returns a small value, hence causing our algorithm to avoid selecting tuples of auxiliary type i.\nTheorem 1 (Time Complexity) Our approximation algorithm incurs O(N(|V ||U |2 +N3) + |Vt|3) time. Its proof is in Appendix E. So, our approximation algorithm only incurs quartic time in the number N of selected observations and cubic time in the number |Vt| of candidate sampling locations of the target phenomenon.\nPerformance Guarantee. To theoretically guarantee the performance of our approximation algorithm, we will first motivate the need to define a relaxed notion of submodularity. A submodular set function exhibits a natural diminishing returns property: When adding an element to its input set, the increment in its function value decreases with a larger input set. To maximize a nondecreasing and submodular set function, the work of Nemhauser, Wolsey, and Fisher (1978) has proposed a greedy algorithm guaranteeing a (1\u22121/e)-factor approximation of that achieved by the optimal input set.\nThe main difficulty in proving the submodularity of F (X) (8) lies in its mutual information term being conditioned on X . Some works (Krause and Guestrin 2005; Renner and Maurer 2002) have shown the submodularity of such conditional mutual information by imposing conditional independence assumptions (e.g., Markov chain). In practice, these strong assumptions (e.g., YA \u22a5 Y\u3008x,i\u3009|YVt\\Xt for any A \u2286 X and \u3008x, i\u3009 \u2208 V-t \\ X-t) severely violate the correlation structure of multiple types of coexisting phenomena and are an overkill: The correlation structure can in fact be preserved to a fair extent by relaxing these assumptions, which consequently entails a relaxed form of submodularity of F (X) (8); a performance guarantee similar to that of Nemhauser, Wolsey, and Fisher (1978) can then be derived for our approximation algorithm.\nDefinition 1 A function G : 2B \u2192 R is -submodular if G(A\u2032 \u222a {a}) \u2212 G(A\u2032) \u2264 G(A \u222a {a}) \u2212 G(A) + for any A \u2286 A\u2032 \u2286 B and a \u2208 B \\A\u2032.\nLemma 1 Let \u03c32n\u2217 , mini\u2208{1,...,M} \u03c32ni . Given 1 \u2265 0, if\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u0303\u222aVt\\Xt \u2212 \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aVt\\Xt \u2264 1 (10)\nfor any X\u0303 \u2286 X and \u3008x, i\u3009 \u2208 V-t \\ X-t, then F (X) is - submodular where = 0.5 log(1 + 1/\u03c32n\u2217).\nIts proof is in Appendix F. Note that (10) relaxes the above example of conditional independence assumption (i.e., assuming 1 = 0) to one which allows 1 > 0. In practice, 1 is expected to be small: Since further conditioning monotonically decreases a posterior variance (Xu et al. 2014), an expected large set Vt \\Xt of tuples of remaining unobserved locations of the target phenomenon tends to be informative enough to make \u03a3PITC\n\u3008x,i\u3009\u3008x,i\u3009|X\u0303\u222aVt\\Xt small and hence the\nnon-negative variance reduction term and 1 in (10) small. Furthermore, (10) with a given small 1 can be realized by controlling the discretization of the domain of candidate sampling locations. For example, by refining the discretization of Vt (i.e., increasing |Vt|), the variance reduction term in (10) decreases because it has been shown in (Das and Kempe 2008) to be submodular in many practical cases. We give another example in Lemma 2 to realize (10) by controlling the discretization such that every pair of selected observations are sufficiently far apart.\nIt is easy to derive that F (\u2205) = 0. The \u201cinformation never hurts\u201d bound for entropy (Cover and Thomas 1991) entails a nondecreasing F (X): F (X \u222a {\u3008x, i\u3009}) \u2212 F (X) = H(Y\u3008x,i\u3009|YX) \u2212 \u03b4iH(Y\u3008x,i\u3009|YX\u222aVt\\Xt) \u2265 H(Y\u3008x,i\u3009|YX) \u2212 H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) \u2265 0. The first inequality requires \u03c32n\u2217 \u2265 (2\u03c0e)\u22121 so that H(Y\u3008x,i\u3009|YA) = 0.5 log 2\u03c0e\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|A \u2265 0.5 log 2\u03c0e\u03c3 2 n\u2217 \u2265 0,6 which is reasonable in practice due to ubiquitous noise. Combining this result with Lemma 1 yields the performance guarantee:\nTheorem 2 Given 1 \u2265 0, if (10) holds, then our approximation algorithm is guaranteed to select X s.t. F (X) \u2265 (1\u2212 1/e)(F (X\u2217)\u2212N ) where = 0.5 log(1 + 1/\u03c32n\u2217). Its proof (Appendix G) is similar to that of the well-known result of Nemhauser, Wolsey, and Fisher (1978) except for exploiting -submodularity of F (X) in Lemma 1 instead of submodularity.\nFinally, we present a discretization scheme that satisfies (10): Let \u03c9 be the smallest discretization width of Vi for i = 1, . . . ,M . Construct a new set V \u2212 \u2282 V of tuples of candidate sampling locations and their corresponding measurement types such that every pair of tuples are at least a distance of p\u03c9 apart for some p > 0; each candidate location thus has only one corresponding type. Such a construction V \u2212 constrains our algorithm to select observations sparsely across the spatial domain so that any \u3008x, i\u3009 \u2208 V-t \\ X-t has sufficiently many neighboring tuples of remaining unobserved locations of the target phenomenon from Vt \\ Xt to keep \u03a3PITC\n\u3008x,i\u3009\u3008x,i\u3009|X\u0303\u222aVt\\Xt small and hence the variance re-\nduction term and 1 in (10) small. Our previous theoretical results still hold if V \u2212 is used instead of V . The result below gives the minimum value of p to satisfy (10):\n6\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|A \u2265 \u03c32n\u2217 is proven in Lemma 3 in Appendix D.\nLemma 2 Let \u03c32s\u2217 , maxi\u2208{1,...,M} \u03c32si , ` be the largest first diagonal component of P\u221210 + P \u22121 i + P \u22121 j for all i, j = 1, . . . ,M , and \u03be , exp(\u2212\u03c92/(2`)). Given 1 > 0 and assuming absence of suppressor variables, if\np2> log\n{ 1\n2\u03c32s\u2217 min\n( \u03c32n\u2217\nN ,\n1\n2\n(\u221a 21 + 4 1\u03c32n\u2217\nN \u2212 1\n))}/ log \u03be,\nthen (10) holds. See Appendix H for its proof."}, {"heading": "5 Experiments and Discussion", "text": "This section evaluates the predictive performance of our approximation algorithm (m-Greedy) empirically on three real-world datasets: (a) Jura dataset (Goovaerts 1997) contains concentrations of 7 heavy metals collected at 359 locations in a Swiss Jura region; (b) Gilgai dataset (Webster 1977) contains electrical conductivity and chloride content generated from a line transect survey of 365 locations of Gilgai territory in New South Wales, Australia; and (c) indoor environmental quality (IEQ) dataset (Bodik et al. 2004) contains temperature (\u25e6F) and light (Lux) readings taken by 43 temperature sensors and 41 light sensors deployed in the Intel Berkeley Research lab. The sampling locations for the Jura and IEQ datasets are shown in Appendix I.\nThe performance of m-Greedy is compared to that of the (a) maximum variance/entropy (m-Var) algorithm which greedily selects the next location and its corresponding measurement type with maximum posterior variance/entropy in each iteration; and (b) greedy maximum entropy (s-Var) (Shewry and Wynn 1987) and mutual information (s-MI) (Krause, Singh, and Guestrin 2008) sampling algorithms for gathering observations only from the target phenomenon.\nFor all experiments, k-means is used to select inducing locations U by clustering all possible locations available to be selected for observation into |U | clusters such that each cluster center corresponds to an element of U . The hyper-parameters (i.e., \u03c32si , \u03c3 2 ni , P0 and Pi for i = 1, . . . ,M ) of MOGP and single-output GP models are learned using the data via maximum likelihood estimation (A\u0301lvarez and Lawrence 2011). For each dataset, observations (i.e., 100 for Jura and Gilgai datasets and 10 for IEQ dataset) of type t are randomly selected to form the test set T ; the tuples of candidate sampling locations and corresponding type t therefore become less than that of auxiliary types. The root mean squared error (RMSE) metric\u221a |T |\u22121 \u2211 x\u2208T (y\u3008x,t\u3009 \u2212 \u00b5\u3008x,t\u3009|X)2 is used to evaluate the performance of the tested algorithms. All experimental results are averaged over 50 random test sets. For a fair comparison, the measurements of all types are normalized before using them for training, prediction, and active learning.\nJura Dataset. Three types of correlated lg-Cd, Ni, and lg-Zn measurements are used in this experiment; we take the log of Cd and Zn measurements to remove their strong skewness, as proposed as a standard statistical practice in (Webster and Oliver 2007). The measurement types with the smallest and largest signal-to-noise ratios (respectively, lgCd and Ni; see Appendix J) are each set as type t.\nFigs. 1a-c and 1d-f show, respectively, results of the tested algorithms with lg-Cd and Ni as type t. It can be observed that the RMSE of m-Greedy decreases more rapidly than that of m-Var, especially when observations of auxiliary types are selected after about N = 200. This is because our algorithm selects observations of auxiliary types that induce the largest reduction in predictive uncertainty of the measurements at the remaining unobserved locations of the target phenomenon (Section 4). In contrast, m-Var may select observations that reduce the predictive uncertainty of auxiliary types of phenomena, which does not directly achieve the aim of our active learning problem. With increasing |U |, both m-Greedy and m-Var reach smaller RMSEs, but mGreedy can achieve this faster with much less observations. As shown in Figs. 1a-f, m-Greedy performs much better than s-Var and s-MI, which means observations of correlated auxiliary types can indeed be used to improve the prediction of the target phenomenon. Finally, by comparing the results between Figs. 1a-c and 1d-f, the RMSE of m-Greedy with Ni as type t decreases faster than that with lg-Cd as type t, especially in the beginning (i.e., N \u2264 200) due to higher-quality Ni measurements (i.e., larger signal-to-noise ratio).\nGilgai Dataset. In this experiment, the lg-Cl contents at depth 0-10cm and 30-40cm are used jointly as two types of target phenomena while the log of electrical conductivity, which is easier to measure at these depths, is used as the auxiliary type. Fig. 2a shows results of the average RMSE over the two lg-Cl types with |U | = 100. Similar to the results of the Jura dataset, with two types of target phenomena, the RMSE of m-Greedy still decreases more rapidly with increasing N than that of m-Var and achieves a much smaller RMSE than that of s-Var and s-MI; the results of s-Var and s-MI are also averaged over two independent single-output GP predictions of lg-Cl content at the two depths.\nIEQ Dataset. Fig. 2b shows results with light as type t and |U | = 40. The observations are similar to that of the Jura and Gilgai datasets: RMSE of m-Greedy decreases faster than that of the other algorithms. More importantly, with the same number of observations, m-Greedy achieves much smaller RMSE than s-Var and s-MI that can sample only from the target phenomenon. This is because m-Greedy selects observations of the auxiliary type (i.e., temperature) that are less\nnoisy (\u03c32ni = 0.13) than that of light (\u03c3 2 nt = 0.23), which demonstrates its advantage over s-Var and s-MI when type t measurements are noisy (e.g., due to poor-quality sensors)."}, {"heading": "6 Related Work", "text": "Existing works on active learning with multiple output measurement types are not driven by the MOGP model and have not formally characterized the cross-correlation structure between different types of phenomena: Some spatial sampling algorithms (Bueso, Angulo, and Alonso 1998; Angulo and Bueso 2001) have simply modeled the auxiliary phenomenon as a noisy perturbation of the target phenomenon that is assumed to be latent, which differs from our work here. Multi-task active learning (MTAL) and active transfer learning (ATL) algorithms have considered the prediction of each type of phenomenon as one task and used the auxiliary tasks to help learn the target task. But, the MTAL algorithm of Zhang (2010) requires relations between different classification tasks to be manually specified, which is highly non-trivial to achieve in practice and not applicable to MOGP regression. Some ATL and active learning algorithms (Roth and Small 2006; Zhao et al. 2013) have used active learning criteria (e.g., margin-based criterion) specific to their classification or recommendation tasks that cannot be readily tailored to MOGP regression."}, {"heading": "7 Conclusion", "text": "This paper describes a novel efficient algorithm for active learning of a MOGP model. To resolve the issue of poor scalability in optimizing the conventional entropy criterion, we exploit a structure common to a unifying framework of sparse MOGP models for deriving a novel active learning criterion (6). Then, we exploit the -submodularity property of our new criterion (Lemma 1) for devising a polynomialtime approximation algorithm (9) that guarantees a constantfactor approximation of that achieved by the optimal set of selected observations (Theorem 2). Empirical evaluation on three real-world datasets shows that our approximation algorithm m-Greedy outperforms existing algorithms for active learning of MOGP and single-output GP models, especially when measurements of the target phenomenon are more noisy than that of the auxiliary types. For our future work, we plan to extend our approach by generalizing non-myopic active learning (Cao, Low, and Dolan 2013; Hoang et al. 2014; Ling, Low, and Jaillet 2016; Low, Dolan, and Khosla 2009; Low, Dolan, and Khosla 2008; Low, Dolan, and Khosla 2011) of single-output GPs to that of MOGPs and improving its scalability to big data through parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al. 2014), and stochastic variational inference (Hoang, Hoang, and Low 2015). Acknowledgments. This research was carried out at the\nSeSaMe Centre. It is supported by Singapore NRF under its IRC@SG Funding Initiative and administered by IDMPO."}, {"heading": "A Derivation of Novel Active Learning", "text": "Criterion (6)\narg min X:|X|=N H(YVt\\Xt |YX) = arg max X:|X|=N H(YVt |LU )\u2212H(YVt\\Xt |YX) = arg max X:|X|=N H(YVt |LU )\u2212H(YVt\\Xt |LU ) +H(YVt\\Xt |LU ) \u2212\nH(YVt\\Xt |YX , LU ) +H(YVt\\Xt |YX , LU )\u2212H(YVt\\Xt |YX) = arg max\nX:|X|=N H(YXt |LU , YVt\\Xt) + I(YVt\\Xt ;YX |LU ) \u2212\nI(LU ;YVt\\Xt |YX) = arg max\nX:|X|=N H(YXt |LU )\u2212 I(LU ;YVt\\Xt |YX) .\nThe first equality follows from the fact that H(YVt |LU ) is a constant. The third equality is due to the chain rule for entropy H(YVt |LU ) = H(YVt\\Xt |LU ) +H(YXt |LU , YVt\\Xt) as well as the definition of conditional mutual information I(YVt\\Xt ;YX |LU ) , H(YVt\\Xt |LU )\u2212H(YVt\\Xt |YX , LU ) and I(LU ;YVt\\Xt |YX) , H(YVt\\Xt |YX) \u2212 H(YVt\\Xt |YX , LU ). The last equality follows from structural property P2 shared by sparse CMOGP regression models in the unifying framework (A\u0301lvarez and Lawrence 2011) described in Section 2, which results in H(YXt |LU , YVt\\Xt) = H(YXt |LU ) and I(YVt\\Xt ;YX |LU ) = 0."}, {"heading": "B Time Complexity of Evaluating Active", "text": "Learning Criterion in (6)\nH(YXt |LU ) = 1\n2 log(2\u03c0e)|Xt||\u03a3XtXt|U |\nwhere \u03a3XtXt|U = \u03a3XtXt \u2212 \u03a3XtU\u03a3 \u22121 UU\u03a3UXt by definition (see last paragraph of Section 2). So, evaluatingH(YXt |LU ) incurs O(|U |3 + N3) time for every X \u2282 V ; this worstcase time complexity occurs when all the tuples in X are of measurement type t (i.e., X = Xt).\nI(LU ;YVt\\Xt |YX) = H(LU |YX)\u2212H(LU |YX\u222aVt\\Xt)\n= 1\n2 log |\u03a3UU |X | |\u03a3UU |X\u222aVt\\Xt |\n= 1\n2 log |\u03a3UU |X | |\u03a3UU |\u22c3i6=t Xi\u222aVt |\nwhere\n\u03a3UU |A = \u03a3UU (\u03a3UU + \u03a3UA\u039b \u22121 A \u03a3AU ) \u22121\u03a3UU\nfor any A \u2282 D+, as derived in (A\u0301lvarez and Lawrence 2011). Therefore, evaluating |\u03a3UU |X | incursO(|U |3 +N3) time for every X \u2282 V ; this worst-case time complexity occurs when all the tuples in X are of one measurement type.\nLet A , \u22c3\ni 6=tXi\u222aVt. Then, by the definition of \u039bA (see last paragraph of Section 2),\n\u03a3UA\u039b \u22121 A \u03a3AU = \u2211 i 6=t \u03a3UXi\u03a3 \u22121 XiXi|U\u03a3XiU+\u03a3UVt\u03a3 \u22121 VtVt|U\u03a3VtU .\nEvaluating the \u2211\ni6=t \u03a3UXi\u03a3 \u22121 XiXi|U\u03a3XiU term incurs\nO(|U |3 + N3) time for every X \u2282 V ; this worst-case time\ncomplexity occurs when all the tuples in X are of one measurement type. Note that the \u03a3UVt\u03a3 \u22121 VtVt|U\u03a3VtU term remains the same for every X \u2282 V (i.e., since it is independent of X) and hence only needs to be computed once in O(|Vt|3) time. Therefore, evaluating |\u03a3UU |\u22c3i6=t Xi\u222aVt | = |\u03a3UU |A| incurs O(|U |3 + N3) time for every X \u2282 V and a one-off cost of O(|Vt|3) time. Consequently, evaluating I(LU ;YVt\\Xt |YX) incurs O(|U |3 +N3) time for every X \u2282 V and a one-off cost of O(|Vt|3) time.\nSo, evaluating our active learning criterion in (6) incurs O(|U |3 + N3) time for every X \u2282 V and a one-off cost of O(|Vt|3) time.\nC Derivation of Greedy Criterion in (9) If i = t, then\nF (X \u222a {\u3008x, t\u3009})\u2212 F (X) = H(YXt\u222a{\u3008x,t\u3009}|LU ) \u2212\n(H(LU |YX\u222a{\u3008x,t\u3009})\u2212H(LU |YX\u222a{\u3008x,t\u3009}\u222aVt\\(Xt\u222a{\u3008x,t\u3009}))) \u2212 (H(YXt |LU )\u2212 (H(LU |YX)\u2212H(LU |YX\u222aVt\\Xt)))\n= H(YXt\u222a{\u3008x,t\u3009}|LU )\u2212H(YXt |LU ) + (H(LU |YX)\u2212H(LU |YX\u222a{\u3008x,t\u3009})) = H(Y\u3008x,t\u3009|YXt , LU ) +H(Y\u3008x,t\u3009|YX)\u2212H(Y\u3008x,t\u3009|YX , LU ) = H(Y\u3008x,t\u3009|LU ) +H(Y\u3008x,t\u3009|YX)\u2212H(Y\u3008x,t\u3009|LU ) = H(Y\u3008x,t\u3009|YX) . (11) The first equality follows from (6) and (8). The second equality is due to H(LU |YX\u222a{\u3008x,t\u3009}\u222aVt\\(Xt\u222a{\u3008x,t\u3009})) = H(LU |YX\u222aVt\\Xt). The third equality is due to the chain rule for entropy H(YXt\u222a{\u3008x,t\u3009}|LU ) = H(YXt |LU ) + H(Y\u3008x,t\u3009|YXt , LU ) as well as the definition of conditional mutual information I(LU ;Y\u3008x,t\u3009|YX) , H(LU |YX) \u2212 H(LU |YX\u222a{\u3008x,t\u3009}) = H(Y\u3008x,t\u3009|YX) \u2212 H(Y\u3008x,t\u3009|YX , LU ). The second last equality follows from structural property P2 shared by sparse CMOGP regression models in the unifying framework (A\u0301lvarez and Lawrence 2011) described in Section 2.\nOtherwise (i.e., i 6= t),\nF (X \u222a {\u3008x, i\u3009})\u2212 F (X) = H(YXt |LU ) \u2212\n(H(LU |YX\u222a{\u3008x,i\u3009})\u2212H(LU |YX\u222a{\u3008x,i\u3009}\u222aVt\\Xt)) \u2212 (H(YXt |LU )\u2212 (H(LU |YX)\u2212H(LU |YX\u222aVt\\Xt)))\n= H(YXt |LU )\u2212H(YXt |LU ) + (H(LU |YX)\u2212H(LU |YX\u222a{\u3008x,i\u3009})) + H(LU |YX\u222aVt\\Xt\u222a{\u3008x,i\u3009})\u2212H(LU |YX\u222aVt\\Xt) = H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|LU , YX) + H(Y\u3008x,i\u3009|YX\u222aVt\\Xt , LU )\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) = H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|LU ) +H(Y\u3008x,i\u3009|LU )\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) = H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) . (12) The first equality follows from (6) and (8). The third equality is due to the definition of conditional mutual information I(LU ;Y\u3008x,i\u3009|YX) , H(LU |YX) \u2212 H(LU |YX\u222a{\u3008x,i\u3009}) = H(Y\u3008x,i\u3009|YX) \u2212 H(Y\u3008x,i\u3009|LU , YX) and I(LU ;Y\u3008x,i\u3009|YX\u222aVt\\Xt) ,\nH(LU |YX\u222aVt\\Xt) \u2212 H(LU |YX\u222aVt\\Xt\u222a{\u3008x,i\u3009}) = H(Y\u3008x,i\u3009|YX\u222aVt\\Xt \u2212 H(Y\u3008x,i\u3009|YX\u222aVt\\Xt , LU ). The second last equality follows from structural properties P1 and P2 shared by sparse CMOGP regression models in the unifying framework (A\u0301lvarez and Lawrence 2011) described in Section 2. Therefore, (9) results."}, {"heading": "D Proof of Proposition 1", "text": "Before proving Proposition 1, the following lemmas are needed:\nLemma 3 For all X \u2282 V and \u3008x, i\u3009 \u2208 V \\ X , \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X \u2265 \u03c3 2 ni .\nIts proof follows closely to that of Lemma 6 in (Cao, Low, and Dolan 2013).\nLemma 4 Assuming absence of suppressor variables, for all X \u2282 V and \u3008x, i\u3009, \u3008x\u2032, j\u3009 \u2208 V \\ X , |\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|X | \u2264 2|\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009|.\nProof. If i = j, then\n|\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009| = |\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009| \u2264 2|\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009| . (13)\nIf i 6= j, then\n|\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|= |\u0393\u3008x,i\u3009\u3008x\u2032,j\u3009| = |\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009 \u2212 \u03a3\u3008x,i\u3009\u3008x\u2032,j\u3009|U | \u2264 |\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009|+ |\u03a3\u3008x,i\u3009\u3008x\u2032,j\u3009|U | \u2264 2|\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009| .\n(14)\nThe first equality is due to (3) while the second equality follows from the definition of \u0393\u3008x,i\u3009\u3008x\u2032,j\u3009 (see last paragraph of Section 2). The last inequality follows from the practical assumption of absence of suppressor variables (Das and Kempe 2008): |\u03a3\u3008x,i\u3009\u3008x\u2032,j\u3009|U | \u2264 |\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009|. Then,\n|\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|X | \u2264 |\u03a3 PITC \u3008x,i\u3009\u3008x\u2032,j\u3009| \u2264 2|\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009| .\nThe first inequality follows from the practical assumption of absence of suppressor variables (Das and Kempe 2008). The second inequality is due to (13) and (14).\nMain Proof. Let B , Vt \\ Xt. Using the spectral theorem, (\u03a3PITCBB|X)\n\u22121 = WQW> where the columns of W are the eigenvectors of (\u03a3PITCBB|X)\n\u22121 and Q is a diagonal matrix comprising the eigenvalues of (\u03a3PITCBB|X)\n\u22121. Let \u03bbmax(A) and \u03bbmin(A) denote, respectively, the maximum and minimum\neigenvalues of matrix A, and \u03b1 ,W>\u03a3PITCB\u3008x,i\u3009|X .\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X \u2212 \u03a3 PITC \u3008x,i\u3009\u3008x,i\u3009|X\u222aVt\\Xt = \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X \u2212( \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X \u2212 \u03a3 PITC \u3008x,i\u3009B|X(\u03a3 PITC BB|X) \u22121\u03a3PITCB\u3008x,i\u3009|X\n) = \u03a3PITC\u3008x,i\u3009B|X(\u03a3 PITC BB|X)\n\u22121\u03a3PITCB\u3008x,i\u3009|X = \u03a3PITC\u3008x,i\u3009B|XWQW\n>\u03a3PITCB\u3008x,i\u3009|X = \u03b1>Q\u03b1 \u2264 \u03bbmax((\u03a3PITCBB|X) \u22121)\u03b1>\u03b1 = \u03a3PITC\u3008x,i\u3009B|XWW >\u03a3PITCB\u3008x,i\u3009|X\n\u03bbmin(\u03a3 PITC BB|X)\n= \u2016\u03a3PITC\u3008x,i\u3009B|X\u2016 2 2\n\u03bbmin(\u03a3 PITC BB|X)\n=\n\u2211 \u3008x\u2032,t\u3009\u2208B |\u03a3PITC\u3008x,i\u3009\u3008x\u2032,t\u3009|X | 2\n\u03bbmin(\u03a3 PITC BB|X) \u2264 \u2211 \u3008x\u2032,t\u3009\u2208B 4|\u03c3\u3008x,i\u3009\u3008x\u2032,t\u3009|2\n\u03bbmin(\u03a3 PITC BB|X)\n\u2264 4\u03c32si\u03c3 2 st\n\u2211 \u3008x\u2032,t\u3009\u2208B N (x\u2212 x\u2032|0, P \u22121 0 + P \u22121 i + P \u22121 t ) 2\n\u03c32nt = 4\u03c1t\u03c3 2 siR(\u3008x, i\u3009, B) . (15) The first equality is due to the incremental update formula of GP posterior variance (see Appendix C in (Xu et al. 2014)). The first inequality is due to the fact that Q is a diagonal matrix comprising the eigenvalues of (\u03a3PITCBB|X)\n\u22121. The fifth equality is due to a property of eigenvalues that \u03bbmax(A\n\u22121) = 1/\u03bbmin(A). The sixth equality follows from the fact that WW> = I . The second inequality follows from Lemma 4. The third inequality is due to (1) and the fact that \u03bbmin(\u03a3PITCBB|X) = \u03bbmin(\u03a3 PITC BB|X \u2212 \u03c3 2 ntI + \u03c3 2 ntI) = \u03bbmin(\u03a3 PITC BB|X \u2212 \u03c3 2 ntI) + \u03c3 2 nt \u2265 \u03c3 2 nt since \u03bbmin(\u03a3 PITC BB|X \u2212 \u03c32ntI) \u2265 0 (i.e., \u03a3 PITC BB|X \u2212 \u03c3 2 ntI is a positive semi-definite matrix). Then,\nH(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\Xt)\n= 1\n2 log\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aB\n\u2264 1 2\nlog \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aB + 4\u03c1t\u03c3 2 siR(\u3008x, i\u3009, B)\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aB\n\u2264 1 2 log\n( 1 +\n4\u03c1t\u03c3 2 siR(\u3008x, i\u3009, B) \u03c32ni ) = 1\n2 log(1 + 4\u03c1t\u03c1iR(\u3008x, i\u3009, B)) .\nThe first inequality is due to (15) while the second inequality follows from Lemma 3."}, {"heading": "E Proof of Theorem 1", "text": "If i = t, then\nH(Y\u3008x,t\u3009|YX) = 1\n2 log(2\u03c0e)\u03a3PITC\u3008x,t\u3009\u3008x,t\u3009|X\nwhere \u03a3PITC\u3008x,t\u3009\u3008x,t\u3009|X is previously defined in (3). So, evaluating H(Y\u3008x,t\u3009|YX) incurs O(|U |2) time for every \u3008x, t\u3009 \u2208 Vt \\Xt andO(|U |3 +N3) time in each iteration; this worstcase time complexity occurs when all the tuples in X are of one measurement type.\nOtherwise (i.e., i 6= t),\nH(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\Xt)\n= 1\n2 log\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aVt\\Xt\n= 1\n2 log\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009| \u22c3\ni6=t Xi\u222aVt\nwhere \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X and \u03a3 PITC \u3008x,i\u3009\u3008x,i\u3009| \u22c3 i6=t Xi\u222aVt are previously defined in (3). Therefore, evaluating \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X incurs O(|U |2) time for every \u3008x, i\u3009 \u2208 V-t \\X-t andO(|U |3 +N3) time in each iteration; this worst-case time complexity occurs when all the tuples in X are of one measurement type.\nLet A , \u22c3\ni 6=tXi\u222aVt. Then, by the definition of \u039bA (see last paragraph of Section 2),\n\u03a3UA\u039b \u22121 A \u03a3AU = \u2211 i 6=t \u03a3UXi\u03a3 \u22121 XiXi|U\u03a3XiU+\u03a3UVt\u03a3 \u22121 VtVt|U\u03a3VtU .\nEvaluating the \u2211\ni6=t \u03a3UXi\u03a3 \u22121 XiXi|U\u03a3XiU term incurs\nO(|U |3 + N3) time in each iteration; this worst-case time complexity occurs when all the tuples in X are of one measurement type. Note that the \u03a3UVt\u03a3 \u22121 VtVt|U\u03a3VtU term remains the same in each iteration (i.e., since it is independent of X) and hence only needs to be computed once in O(|Vt|3) time in our approximation algorithm. As a result, evaluating \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|\u22c3i6=t Xi\u222aVt = \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|A (specifically, its efficient formulation exploiting \u03a3UA\u039b\u22121A \u03a3AU , as shown in (A\u0301lvarez and Lawrence 2011)) incurs O(|U |2) time for every \u3008x, i\u3009 \u2208 V-t \\ X-t and O(|U |3 + N3) time in each iteration, and a one-off cost of O(|Vt|3) time. Consequently, evaluating H(Y\u3008x,i\u3009|YX) \u2212 H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) incurs O(|U |2) time for every \u3008x, i\u3009 \u2208 V-t \\ X-t and O(|U |3 + N3) time in each iteration, and a one-off cost of O(|Vt|3) time.\nSince |U | \u2264 |Vt| < |V |, our approximation algorithm thus incurs O(N(|V ||U |2 +N3) + |Vt|3) time."}, {"heading": "F Proof of Lemma 1", "text": "To prove that F (X) is -submodular, we have to show that\nF (X \u2032 \u222a {\u3008x, i\u3009})\u2212F (X \u2032) \u2264 F (X \u222a {\u3008x, i\u3009})\u2212F (X) +\nfor any X \u2286 X \u2032 \u2286 V and \u3008x, i\u3009 \u2208 V \\ X \u2032. Before doing this, the following lemma is needed:\nLemma 5 Suppose that 1 \u2265 0 is given. For any \u3008x, i\u3009 \u2208 V-t \\ X \u2032-t, if \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aVt\\X\u2032t \u2212 \u03a3 PITC \u3008x,i\u3009\u3008x,i\u3009|X\u2032\u222aVt\\X\u2032t\n\u2264 1, then I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX\u2032) \u2264 I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX) + where = 0.5 log(1 + 1/\u03c32n\u2217).\nProof. Let X\u0304 , X \u2032 \\X . Then, I(Y\u3008x,i\u3009;YX\u0304 |YX\u222aVt\\X\u2032t) = H(Y\u3008x,i\u3009|YX\u222aVt\\X\u2032t)\u2212H(Y\u3008x,i\u3009|YX\u0304\u222aX\u222aVt\\X\u2032t)\n= 1\n2 log \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u222aVt\\X\u2032t \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u0304\u222aX\u222aVt\\X\u2032t\n\u2264 1 2\nlog \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u0304\u222aX\u222aVt\\X\u2032t + 1\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u0304\u222aX\u222aVt\\X\u2032t\n= 1\n2 log\n( 1 +\n1 \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|X\u0304\u222aX\u222aVt\\X\u2032t\n)\n\u2264 1 2 log\n( 1 +\n1 \u03c32ni ) \u2264 1\n2 log\n( 1 +\n1 \u03c32n\u2217\n) .\n(16)\nThe first inequality is due to the sufficient condition. The second inequality follows from Lemma 3. Then, by the definition of conditional mutual information,\nI(Y\u3008x,i\u3009;YVt\\X\u2032t |YX\u0304\u222aX) + I(Y\u3008x,i\u3009;YX\u0304 |YX) = H(Y\u3008x,i\u3009|YX\u0304\u222aX)\u2212H(Y\u3008x,i\u3009|YX\u0304\u222aX\u222aVt\\X\u2032t) + H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u0304\u222aX) = H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u0304\u222aX\u222aVt\\X\u2032t) = H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\X\u2032t) + H(Y\u3008x,i\u3009|YX\u222aVt\\X\u2032t)\u2212H(Y\u3008x,i\u3009|YX\u0304\u222aX\u222aVt\\X\u2032t)\n= I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX) + I(Y\u3008x,i\u3009;YX\u0304 |YX\u222aVt\\X\u2032t) . Therefore, I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX\u2032) = I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX\u0304\u222aX) = I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX) + I(Y\u3008x,i\u3009;YX\u0304 |YX\u222aVt\\X\u2032t) \u2212 I(Y\u3008x,i\u3009;YX\u0304 |YX) \u2264 I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX) + I(Y\u3008x,i\u3009;YX\u0304 |YX\u222aVt\\X\u2032t)\n\u2264 I(Y\u3008x,i\u3009;YVt\\X\u2032t |YX) + 0.5 log (\n1 + 1 \u03c32n\u2217\n) .\nThe first inequality is due to the fact that conditional mutual information is non-negative. The last inequality follows from (16).\nMain Proof. To prove that F (X) is -submodular, we have to show that H(Y\u3008x,i\u3009|YX\u2032) \u2212 \u03b4iH(Y\u3008x,i\u3009|YX\u2032\u222aVt\\X\u2032t) \u2264 H(Y\u3008x,i\u3009|YX) \u2212 \u03b4iH(Y\u3008x,i\u3009|YX\u222aVt\\Xt) + for any X \u2286 X \u2032 \u2286 V and \u3008x, i\u3009 \u2208 V \\X \u2032.\nIf i = t, then H(Y\u3008x,i\u3009|YX\u2032) \u2264 H(Y\u3008x,i\u3009|YX) \u2264 H(Y\u3008x,i\u3009|YX) + for any \u2265 0 due to the \u201cinformation never hurts\u201d bound for entropy (Cover and Thomas 1991).\nOtherwise (i.e., i 6= t), H(Y\u3008x,i\u3009|YX\u2032)\u2212H(Y\u3008x,i\u3009|YX\u2032\u222aVt\\X\u2032t) \u2264 H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\X\u2032t) + \u2264 H(Y\u3008x,i\u3009|YX)\u2212H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) +\nwhere = 0.5 log ( 1 + 1/\u03c3 2 n\u2217 ) . The first inequality is due to Lemma 5. The second inequality follows from the \u201cinformation never hurts\u201d bound for entropy (Cover and Thomas\n1991): H(Y\u3008x,i\u3009|YX\u222aVt\\X\u2032t) \u2265 H(Y\u3008x,i\u3009|YX\u222aVt\\Xt) since (Vt \\X \u2032t) \u2286 (Vt \\Xt)."}, {"heading": "G Proof of Theorem 2", "text": "Our proof here is similar to that of Theorem 1.5 in (Krause and Golovin 2014) which is a generalization of the wellknown result of Nemhauser, Wolsey, and Fisher (1978). The key difference is that we exploit -submodularity of F (X) (i.e., Lemma 1) instead of submodularity, as shown below for completeness.\nLet X\u2217 , {\u3008x1, s1\u3009\u2217, . . . , \u3008xN , sN \u3009\u2217} be the optimal set of selected observations, Xk be the set of tuples selected by our approximation algorithm in iteration k = 1, . . . , N , X0 , \u2205, and \u2206(\u3008x, i\u3009|X) , F (X \u222a {\u3008x, i\u3009}) \u2212 F (X) . Then,\nF (X\u2217) \u2264 F (X\u2217 \u222aXk)\n= F (Xk) + N\u2211 j=1 \u2206\n( \u3008xj , sj\u3009\u2217 \u2223\u2223\u2223\u2223\u2223 j\u22121\u22c3 r=1 {\u3008xr, sr\u3009\u2217} \u222aXk )\n\u2264 F (Xk) + N\u2211 j=1 ( \u2206(\u3008xj , sj\u3009\u2217|Xk) + ) \u2264 F (Xk) +\nN\u2211 j=1 ( F (Xk+1)\u2212 F (Xk) + ) \u2264 F (Xk) +N ( F (Xk+1)\u2212 F (Xk) + ) .\nThe first inequality follows from the nondecreasing property of F (X). The first equality is a straightforward telescoping sum. The second inequality follows from the - submodularity of F (X), as proven in Lemma 1. The third inequality follows from (9). Then,\nF (X\u2217)\u2212 F (Xk) \u2264 N ( F (Xk+1)\u2212 F (Xk) + ) . (17)\nLet \u03b6k , F (X\u2217) \u2212 F (Xk). Then, (17) can be rewritten as \u03b6k \u2264 N(\u03b6k \u2212 \u03b6k+1 + ) which can be rearranged to yield\n\u03b6k+1 \u2264 (\n1\u2212 1 N\n) \u03b6k + . (18)\nThen, by recursion of (18), it is straightforward to get\n\u03b6k \u2264 (\n1\u2212 1 N\n)k \u03b60 +N ( 1\u2212 ( 1\u2212 1\nN\n)k) . (19)\nThen, by substituting \u03b6k = F (X\u2217) \u2212 F (Xk) and \u03b60 = F (X\u2217)\u2212 F (X0) = F (X\u2217), (19) can be rearranged to\nF (Xk)\u2265 ( 1\u2212 ( 1\u2212 1\nN\n)k) (F (X\u2217)\u2212N )\n\u2265 (1\u2212 e\u2212k/N )(F (X\u2217)\u2212N ) .\nThe second inequality follows from the well-known inequality e\u2212x \u2265 1\u2212x. Finally, Theorem 2 is obtained when k = N and = 0.5 log(1 + 1/\u03c32n\u2217), as defined in Lemma 1."}, {"heading": "H Proof of Lemma 2", "text": "LetB , X\u0303 \u222aVt \\Xt andA , X \\X\u0303 . From the incremental update formula of GP posterior variance (see Appendix C in (Xu et al. 2014)),\n\u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|B \u2212 \u03a3 PITC \u3008x,i\u3009\u3008x,i\u3009|B\u222aA = \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|B \u2212( \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|B \u2212 \u03a3 PITC \u3008x,i\u3009A|B(\u03a3 PITC AA|B) \u22121\u03a3PITCA\u3008x,i\u3009|B\n) = \u03a3PITC\u3008x,i\u3009A|B(\u03a3 PITC AA|B)\n\u22121\u03a3PITCA\u3008x,i\u3009|B . (20)\nLet \u03a3PITCAA|B , C + E where C is defined as a matrix with the same diagonal components as \u03a3PITCAA|B and off-diagonal components 0 while E is defined as a matrix with diagonal components 0 and the same off-diagonal components as \u03a3PITCAA|B . Then,\n\u2016C\u22121\u20162 = \u03bbmax(C\u22121) = 1\n\u03bbmin(C)\n= 1\nmin\u3008x,i\u3009\u2208A \u03a3 PITC \u3008x,i\u3009\u3008x,i\u3009|B\n\u2264 1 \u03c32ni \u2264 1 \u03c32n\u2217 .\n(21)\nThe first equality is due to a property of matrix norm in Section 10.4.5 in (Petersen and Pedersen 2012). The second equality is due to a property of eigenvalues that \u03bbmax(C\n\u22121) = 1/\u03bbmin(C). The third equality is due to the diagonal property ofC. The first inequality is due to Lemma 3.\nMatrix E comprises off-diagonal components \u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|B for all \u3008x, i\u3009, \u3008x\n\u2032, j\u3009 \u2208 A such that \u3008x, i\u3009 6= \u3008x\u2032, j\u3009, each of which has an absolute value not more than 2\u03c32s\u2217\u03be p2 :\n|\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|B | \u2264 2|\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009| = 2|\u03c3si\u03c3sj |N (x\u2212 x\u2032|0, P\u221210 + P \u22121 i + P \u22121 j )\n= 2|\u03c3si\u03c3sj | exp\n{ \u22121\n2 d\u2211 v=1 (xv \u2212 x\u2032v)2 `ijv\n}\n\u2264 2|\u03c3si\u03c3sj | exp\n{ \u2212 (x1 \u2212 x \u2032 1) 2\n2`ij1 } \u2264 2|\u03c3si\u03c3sj | exp { \u2212p 2\u03c92\n2` } = 2|\u03c3si\u03c3sj |\u03bep 2 \u2264 2\u03c32s\u2217\u03bep 2\nwhere xv is the v-th component of a d-dimensional location vector x and `ijv denotes the v-th diagonal component of P\u221210 + P \u22121 i + P \u22121 j . The first inequality follows from Lemma 4. The second equality is due to the precision matrices being diagonal. The third inequality follows from ` , maxi,j\u2208{1,...,M} ` ij 1 and the fact that the distance between x1 and x\u20321 of any \u3008x, i\u3009, \u3008x\u2032, j\u3009 \u2208 A must be at least p\u03c9 due to the construction of V \u2212. Therefore,\n\u2016E\u20162 \u2264 2N\u03c32s\u2217\u03bep 2\n(22)\ndue to a property that the 2-norm of a matrix is at most its largest absolute component multiplied by its dimension (Golub and Van Loan 1996).\nSimilarly, \u03a3PITC\u3008x,i\u3009A|B comprises components \u03a3 PITC \u3008x,i\u3009\u3008x\u2032,j\u3009|B for all \u3008x\u2032, j\u3009 \u2208 A, each of which has an absolute value not more than 2\u03c32s\u2217\u03be p2 :\n|\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|B | \u2264 2|\u03c3\u3008x,i\u3009\u3008x\u2032,j\u3009| \u2264 2\u03c3 2 s\u2217\u03be p2 . (23)\nNow, \u03a3PITC\u3008x,i\u3009A|B(C + E) \u22121\u03a3PITCA\u3008x,i\u3009|B \u2212 \u03a3 PITC \u3008x,i\u3009A|BC \u22121\u03a3PITCA\u3008x,i\u3009|B\n= \u03a3PITC\u3008x,i\u3009A|B { (C + E)\u22121 \u2212 C\u22121 } \u03a3PITCA\u3008x,i\u3009|B \u2264 \u2016\u03a3PITC\u3008x,i\u3009A|B\u2016 2 2\u2016(C + E)\u22121 \u2212 C\u22121\u20162\n\u2264 \u2211\n\u3008x\u2032,j\u3009\u2208A\n|\u03a3PITC\u3008x,i\u3009\u3008x\u2032,j\u3009|B | 2 \u2016C\u22121\u20162\u2016E\u20162\n1 \u2016C\u22121\u20162 \u2212 \u2016E\u20162\n\u2264 4N\u03c34s\u2217\u03be2p 2 \u2016C\u22121\u20162\u2016E\u20162\n1 \u2016C\u22121\u20162 \u2212 \u2016E\u20162\n.\n(24) The first inequality is due to Cauchy-Schwarz inequality and submultiplicativity of the matrix norm (Stewart and Sun 1990). The second inequality follows from an important result in the perturbation theory of matrix inverses (in particular, Theorem III.2.5 in (Stewart and Sun 1990)). It requires the assumption \u2016C\u22121E\u20162 < 1. Using (21), (22), and the matrix norm property in Section 10.4.2 in (Petersen and Pedersen 2012), this assumption can be satisfied by\n\u2016C\u22121E\u20162 \u2264 \u2016C\u22121\u20162\u2016E\u20162 \u2264 2N\u03c32s\u2217\u03be\np2\n\u03c32n\u2217 < 1.\nThen,\np2 > log\n( \u03c32n\u2217\n2N\u03c32s\u2217\n)/ log \u03be . (25)\nThe last inequality in (24) is due to (23). Then, from both (20) and (24), \u03a3PITC\u3008x,i\u3009\u3008x,i\u3009|B \u2212 \u03a3 PITC \u3008x,i\u3009\u3008x,i\u3009|B\u222aA\n= \u03a3PITC\u3008x,i\u3009A|B(C + E) \u22121\u03a3PITCA\u3008x,i\u3009|B\n\u2264 \u03a3PITC\u3008x,i\u3009A|BC \u22121\u03a3PITCA\u3008x,i\u3009|B + 4N\u03c3 4 s\u2217\u03be 2p2 \u2016C\u22121\u20162\u2016E\u20162 1\n\u2016C\u22121\u20162 \u2212 \u2016E\u20162\n\u2264 \u2016\u03a3PITC\u3008x,i\u3009A|B\u2016 2 2\u2016C\u22121\u20162 + 4N\u03c34s\u2217\u03be2p 2 \u2016C\u22121\u20162\u2016E\u20162 1\n\u2016C\u22121\u20162 \u2212 \u2016E\u20162\n\u2264 4N\u03c34s\u2217\u03be2p 2 \u2016C\u22121\u20162 + 4N\u03c34s\u2217\u03be2p 2 \u2016C\u22121\u20162\u2016E\u20162\n1 \u2016C\u22121\u20162 \u2212 \u2016E\u20162\n= 4N\u03c34s\u2217\u03be 2p2\u2016C\u22121\u20162\n( 1 +\n\u2016E\u20162 1\n\u2016C\u22121\u20162 \u2212 \u2016E\u20162 ) = 4N\u03c34s\u2217\u03be 2p2\n1 \u2016C\u22121\u20162 \u2212 \u2016E\u20162\n\u2264 4N\u03c3 4 s\u2217\u03be\n2p2\n\u03c32n\u2217 \u2212 2N\u03c32s\u2217\u03bep 2 .\nThe first inequality is due to (24). The second inequality is due to Cauchy-Schwarz inequality. The third inequality is due to (23). The last inequality follows from (21) and (22).\nTo satisfy (10) in Lemma 1, let\n4N\u03c34s\u2217\u03be 2p2\n\u03c32n\u2217 \u2212 2N\u03c32s\u2217\u03bep 2 \u2264 1 .\nThen,\np2 \u2265 log\n{ 1\n4\u03c32s\u2217\n(\u221a 21 + 4 1\u03c32n\u2217\nN \u2212 1\n)}/ log \u03be .\n(26) Finally, from both (25) and (26), Lemma 2 results."}, {"heading": "I Jura and IEQ Datasets", "text": "J Signal-to-Noise Ratios for Jura Dataset"}], "references": [{"title": "Computationally efficient convolved multiple output Gaussian processes", "author": ["\u00c1lvarez", "M.A. Lawrence 2011] \u00c1lvarez", "N.D. Lawrence"], "venue": null, "citeRegEx": "\u00c1lvarez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "\u00c1lvarez et al\\.", "year": 2011}, {"title": "Random perturbation methods applied to multivariate spatial sampling design", "author": ["Angulo", "J.M. Bueso 2001] Angulo", "M.C. Bueso"], "venue": "Environmetrics", "citeRegEx": "Angulo et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Angulo et al\\.", "year": 2001}, {"title": "Multi-task Gaussian process prediction", "author": ["Chai Bonilla", "E.V. Williams 2008] Bonilla", "K.M.A. Chai", "C.K. Williams"], "venue": "In Proc. NIPS", "citeRegEx": "Bonilla et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2008}, {"title": "A state-space model approach to optimum spatial sampling design based on entropy. Environmental and Ecological Statistics 5(1):29\u201344", "author": ["Angulo Bueso", "M.C. Alonso 1998] Bueso", "J.M. Angulo", "F.J. Alonso"], "venue": null, "citeRegEx": "Bueso et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bueso et al\\.", "year": 1998}, {"title": "CruzSanjuli\u00e1n, J.; and Garc\u0131\u0301a-Ar\u00f3stegui", "author": ["Bueso"], "venue": "J. L", "citeRegEx": "Bueso,? \\Q1999\\E", "shortCiteRegEx": "Bueso", "year": 1999}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["Low Cao", "N. Dolan 2013] Cao", "K.H. Low", "J.M. Dolan"], "venue": "In Proc. AAMAS", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2012\\E", "shortCiteRegEx": "Chen", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["Chen"], "venue": "In Proc. UAI,", "citeRegEx": "Chen,? \\Q2013\\E", "shortCiteRegEx": "Chen", "year": 2013}, {"title": "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobility-on-demand systems", "author": ["Chen"], "venue": "IEEE Trans. Autom", "citeRegEx": "Chen,? \\Q2015\\E", "shortCiteRegEx": "Chen", "year": 2015}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["Low Chen", "J. Tan 2013] Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "In Proc. RSS", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Elements of Information Theory", "author": ["Cover", "T. Thomas 1991] Cover", "J. Thomas"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1991}, {"title": "Algorithms for subset selection in linear regression", "author": ["Das", "A. Kempe 2008] Das", "D. Kempe"], "venue": "In Proc. STOC,", "citeRegEx": "Das et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Das et al\\.", "year": 2008}, {"title": "Matrix Computations", "author": ["Golub", "G.H. Van Loan 1996] Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1996}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["Hoang"], "venue": "In Proc. ICML,", "citeRegEx": "Hoang,? \\Q2014\\E", "shortCiteRegEx": "Hoang", "year": 2014}, {"title": "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data", "author": ["Hoang Hoang", "T.N. Low 2015] Hoang", "Q.M. Hoang", "K.H. Low"], "venue": null, "citeRegEx": "Hoang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoang et al\\.", "year": 2015}, {"title": "Submodular function maximization", "author": ["Krause", "A. Golovin 2014] Krause", "D. Golovin"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2014}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["Krause", "A. Guestrin 2005] Krause", "C. Guestrin"], "venue": "In Proc. UAI", "citeRegEx": "Krause et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2005}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["Singh Krause", "A. Guestrin 2008] Krause", "A. Singh", "C. Guestrin"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Designing networks for monitoring multivariate environmental fields using data with monotone pattern", "author": ["Sun Le", "N.D. Zidek 2003] Le", "L. Sun", "J.V. Zidek"], "venue": "Technical Report #2003-5,", "citeRegEx": "Le et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Le et al\\.", "year": 2003}, {"title": "Gaussian process planning with Lipschitz continuous reward functions: Towards unifying Bayesian optimization, active learning, and beyond", "author": ["Low Ling", "C.K. Jaillet 2016] Ling", "K.H. Low", "P. Jaillet"], "venue": "In Proc. AAAI", "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation", "author": ["Low"], "venue": "In Proc. AAAI", "citeRegEx": "Low,? \\Q2015\\E", "shortCiteRegEx": "Low", "year": 2015}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["Dolan Low", "K.H. Khosla 2008] Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. AAMAS,", "citeRegEx": "Low et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Information-theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["Dolan Low", "K.H. Khosla 2009] Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. ICAPS", "citeRegEx": "Low et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["Dolan Low", "K.H. Khosla 2011] Low", "J.M. Dolan", "P. Khosla"], "venue": null, "citeRegEx": "Low et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "An analysis of approximations for maximizing submodular set functions - I", "author": ["Wolsey Nemhauser", "G.L. Fisher 1978] Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "The Matrix Cookbook", "author": ["Petersen", "K.B. Pedersen 2012] Petersen", "M.S. Pedersen"], "venue": null, "citeRegEx": "Petersen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petersen et al\\.", "year": 2012}, {"title": "About the mutual (conditional) information", "author": ["Renner", "R. Maurer 2002] Renner", "U. Maurer"], "venue": "In Proc. IEEE ISIT", "citeRegEx": "Renner et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Renner et al\\.", "year": 2002}, {"title": "Margin-based active learning for structured output spaces", "author": ["Roth", "D. Small 2006] Roth", "K. Small"], "venue": "In Proc. ECML,", "citeRegEx": "Roth et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2006}, {"title": "Maximum entropy sampling", "author": ["Shewry", "M.C. Wynn 1987] Shewry", "H.P. Wynn"], "venue": "J. Applied Stat", "citeRegEx": "Shewry et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Shewry et al\\.", "year": 1987}, {"title": "Matrix Perturbation Theory", "author": ["Stewart", "G.W. Sun 1990] Stewart", "Sun", "J.-G"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "Semiparametric latent factor models", "author": ["Teh", "Y.W. Seeger 2005] Teh", "M. Seeger"], "venue": "In Proc. AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2005}, {"title": "Geostatistics for Environmental Scientists", "author": ["Webster", "R. Oliver 2007] Webster", "M. Oliver"], "venue": null, "citeRegEx": "Webster et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Webster et al\\.", "year": 2007}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["Xu"], "venue": "In Proc. AAAI,", "citeRegEx": "Xu,? \\Q2014\\E", "shortCiteRegEx": "Xu", "year": 2014}, {"title": "Near-optimal active learning of multi-output Gaussian processes", "author": ["Zhang"], "venue": "In Proc. AAAI", "citeRegEx": "Zhang,? \\Q2016\\E", "shortCiteRegEx": "Zhang", "year": 2016}, {"title": "Active transfer learning for cross-system recommendation", "author": ["Zhao"], "venue": "In Proc. AAAI,", "citeRegEx": "Zhao,? \\Q2013\\E", "shortCiteRegEx": "Zhao", "year": 2013}, {"title": "\u00c1lvarez and Lawrence 2011)) incurs O(|U |) time for every \u3008x, i\u3009 \u2208 V-t \\ X-t and O(|U | + N) time in each iteration, and a one-off cost", "author": ["\u03a3AU A"], "venue": null, "citeRegEx": "A,? \\Q2011\\E", "shortCiteRegEx": "A", "year": 2011}], "referenceMentions": [{"referenceID": 35, "context": "A submodular set function exhibits a natural diminishing returns property: When adding an element to its input set, the increment in its function value decreases with a larger input set. To maximize a nondecreasing and submodular set function, the work of Nemhauser, Wolsey, and Fisher (1978) has proposed a greedy algorithm guaranteeing a (1\u22121/e)-factor approximation of that achieved by the optimal input set.", "startOffset": 0, "endOffset": 293}, {"referenceID": 35, "context": "A submodular set function exhibits a natural diminishing returns property: When adding an element to its input set, the increment in its function value decreases with a larger input set. To maximize a nondecreasing and submodular set function, the work of Nemhauser, Wolsey, and Fisher (1978) has proposed a greedy algorithm guaranteeing a (1\u22121/e)-factor approximation of that achieved by the optimal input set. The main difficulty in proving the submodularity of F (X) (8) lies in its mutual information term being conditioned on X . Some works (Krause and Guestrin 2005; Renner and Maurer 2002) have shown the submodularity of such conditional mutual information by imposing conditional independence assumptions (e.g., Markov chain). In practice, these strong assumptions (e.g., YA \u22a5 Y\u3008x,i\u3009|YVt\\Xt for any A \u2286 X and \u3008x, i\u3009 \u2208 V-t \\ X-t) severely violate the correlation structure of multiple types of coexisting phenomena and are an overkill: The correlation structure can in fact be preserved to a fair extent by relaxing these assumptions, which consequently entails a relaxed form of submodularity of F (X) (8); a performance guarantee similar to that of Nemhauser, Wolsey, and Fisher (1978) can then be derived for our approximation algorithm.", "startOffset": 0, "endOffset": 1196}, {"referenceID": 35, "context": "Its proof (Appendix G) is similar to that of the well-known result of Nemhauser, Wolsey, and Fisher (1978) except for exploiting -submodularity of F (X) in Lemma 1 instead of submodularity.", "startOffset": 11, "endOffset": 107}, {"referenceID": 4, "context": "Existing works on active learning with multiple output measurement types are not driven by the MOGP model and have not formally characterized the cross-correlation structure between different types of phenomena: Some spatial sampling algorithms (Bueso, Angulo, and Alonso 1998; Angulo and Bueso 2001) have simply modeled the auxiliary phenomenon as a noisy perturbation of the target phenomenon that is assumed to be latent, which differs from our work here. Multi-task active learning (MTAL) and active transfer learning (ATL) algorithms have considered the prediction of each type of phenomenon as one task and used the auxiliary tasks to help learn the target task. But, the MTAL algorithm of Zhang (2010) requires relations between different classification tasks to be manually specified, which is highly non-trivial to achieve in practice and not applicable to MOGP regression.", "startOffset": 246, "endOffset": 709}, {"referenceID": 9, "context": "2014; Ling, Low, and Jaillet 2016; Low, Dolan, and Khosla 2009; Low, Dolan, and Khosla 2008; Low, Dolan, and Khosla 2011) of single-output GPs to that of MOGPs and improving its scalability to big data through parallelization (Chen et al. 2013; Low et al. 2015), online learning (Xu et al.", "startOffset": 226, "endOffset": 261}, {"referenceID": 9, "context": "[Chen et al. 2013] Chen, J.", "startOffset": 0, "endOffset": 18}], "year": 2015, "abstractText": "This paperaddresses the problem of active learning of a multi-output Gaussian process (MOGP) model representing multiple types of coexisting correlated environmental phenomena. In contrast to existing works, our active learning problem involves selecting not just the most informative sampling locations to be observed but also the types of measurements at each selected location for minimizing the predictive uncertainty (i.e., posterior joint entropy) of a target phenomenon of interest given a sampling budget. Unfortunately, such an entropy criterion scales poorly in the numbers of candidate sampling locations and selected observations when optimized. To resolve this issue, we first exploit a structure common to sparse MOGP models for deriving a novel active learning criterion. Then, we exploit a relaxed form of submodularity property of our new criterion for devising a polynomial-time approximation algorithm that guarantees a constant-factor approximation of that achieved by the optimal set of selected observations. Empirical evaluation on real-world datasets shows that our proposed approach outperforms existing algorithms for active learning of MOGP and single-output GP models.", "creator": "LaTeX with hyperref package"}}}