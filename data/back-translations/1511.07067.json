{"id": "1511.07067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes", "abstract": "We propose a model for learning visually grounded word embeddings (vis-w2v) to capture visual notions of semantic kinship. While word embeddings that are trained with text have been extremely successful, they cannot uncover the notions of semantic kinship that are implicit in our visual world. For example, visual grounding can help us recognize that concepts like eating and staring are related, because when people eat something, they also tend to stare at food. Exploring a rich variety of relationships like eating and staring at vision is a challenging task, despite recent progress in vision. We realize that the visual grounding of words depends on the semantics of our visual world, not on the literal pixels. Therefore, we use abstract scenes arising from clip parts to provide visual grounding. We realize that the embeddings we learn are grains of grained visual kinship from semantic notions.", "histories": [["v1", "Sun, 22 Nov 2015 20:46:42 GMT  (3974kb,D)", "http://arxiv.org/abs/1511.07067v1", "15 pages, 11 figures"], ["v2", "Wed, 29 Jun 2016 18:15:25 GMT  (4864kb,D)", "http://arxiv.org/abs/1511.07067v2", "15 pages, 11 figures"]], "COMMENTS": "15 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["satwik kottur", "ramakrishna vedantam", "jos\\'e m f moura", "devi parikh"], "accepted": false, "id": "1511.07067"}, "pdf": {"name": "1511.07067.pdf", "metadata": {"source": "CRF", "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes", "authors": ["Satwik Kottur", "Ramakrishna Vedantam", "Jos\u00e9 M. F. Moura", "Devi Parikh"], "emails": ["moura}@andrew.cmu.edu", "parikh}@vt.edu"], "sections": [{"heading": "1. Introduction", "text": "Language and vision are the two important interaction modalities for humans. Understanding and modeling the rich interplay between vision and language is thus one of the fundamental problems in artificial intelligence (AI). That is, AI is inherently multi-modal.\nLanguage modeling is an important problem in natural language processing (NLP). A language model estimates the likelihood of a word conditioned on other words in a sentence. There is a rich history of works which do n-gram based language modeling [17, 4]. The idea is that simple, count-based models trained on millions of sentences can give good results. However, in recent years, neural language models [3, 30] have been growing in popularity. Neural language models typically project raw one-hot encoded words into a vector space, and construct models on top to maxi-\nmize the log-likelihood of word occurrences. One popular choice for this vector space is the word2vec [31, 29] embedding. This embedding captures rich notions of semantic relatedness and compositionality between words [31]. For instance, it is possible to realize that Paris and France are related concepts by measuring similarity using these embeddings. Surprisingly, these embeddings also generalize to analogies. Thus, France - Paris + Berlin is very close to Germany. These embeddings seem to capture not only semantic relatedness but also how words are related. Interestingly, word2vec never receives supervision that Paris and France are related concepts or how they are related. The semantic relatedness is captured as an emergent property of the network training procedure.\nSince AI is a multi-modal problem, it could be beneficial\n1\nar X\niv :1\n51 1.\n07 06\n7v 1\n[ cs\n.C V\n] 2\n2 N\nov 2\n01 5\nto model semantics as dictated by both text and vision. It is especially challenging to model fine-grained interactions between objects using only text. Consider the relations eating and stares at in Fig. 1. When reasoning using only text, it is difficult to realize that these relations could be semantically similar. However, by grounding the concepts into vision, we realize that these relations are more similar than it appears in text. Thus, visual grounding provides a complimentary notion of semantic relatedness to text. In this work, we aim to learn word embeddings to capture this grounding. Our experiments show that modeling such semantic relatedness is useful for a number of tasks in text that could benefit from visual reasoning.\nGrounding fine grained notions of semantic relatedness between words like eating and stare at into vision is a challenging problem. While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc., modeling fine grained semantics of interactions between objects is still a challenging task. However, we observe that it is the semantics of the visual scene that matter for inferring the visually grounded semantic relatedness, and not the literal pixels (Fig. 1). We thus ground word embeddings using abstract scenes made from clipart. Abstract scenes give us access to a rich semantic feature space where the positions, pose, expressions, gaze, age etc. of objects are readily accessible [40, 41].\nOur model considers visual cues from abstract scenes as context for words. Given a set of words and associated images, we train to predict the visual grounding or context from pre-initialized word embeddings. We cluster the scenes to identify semantically similar visual concepts, which are then used as the visual context. The idea is to bring words with similar visual instantiations closer together, and push words with different visual instantiations farther. The clusters act as surrogate classes. Note that each surrogate class may have images belonging to concepts which are different in text, but are visually similar in the semantic clipart feature space. Since we predict a single context output given a set of input words, our model can be viewed as a multi-modal extension of the CBOW (Continuous Bag Of Words) [31] word2vec model.\nContributions: We propose a novel method Visual word2vec (vis-w2v) to learn visually grounded word embeddings. We use abstract scenes made from clipart to provide the grounding. We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc. Visual paraphrasing [23] is the problem of determining whether two sentences describe the same underlying scene or not. Common sense assertion classification [33] is the task of modeling the plausibil-\nity of common sense assertions of the form (boy, eats, cake). Text-based image retrieval is the task of retrieving images by matching accompanying text with textual queries. We show consistent improvements over baseline word2vec models (w2v) in all experiments. In fact, on some tasks such as common sense assertion classification, our models surpass the state-of-the-art.\nThe rest of the paper is organized as follows. Sec. 2 discusses some related work on learning word embeddings, learning from visual abstraction, etc. Sec. 3 presents our approach. Sec. 4 describes the datasets we work with. We provide experimental details in Sec. 5 and results in Sec. 6."}, {"heading": "2. Related Work", "text": "We first discuss related work in learning word embeddings. We then explore other works which learn with surrogate supervision. We then look at previous work on learning from visual abstractions and on relevant work at the intersection of language and vision.\nWord Embeddings: Neural network based word embeddings [31, 6] have gained a lot of popularity in recent times. These embeddings are learnt offline and then typically fed into a multi-layer Neural Network Language Model [3, 30]. Similar to those approaches, we learn word embeddings from text offline, and refine them to predict visual context for vis-w2v. Xu et al. [39] use visual cues to improve the w2v representation by predicting global visual context using fc7 features from real images. We want to model fine grained semantics using abstract scenes while their focus is on capturing appearance cues. Since dogs and cats look different, their work attempts to separate them in the word embedding space. Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc. These multimodal embeddings capture regularities like compositional structure between images and words (attributes). Thus, in this multi-modal embedding space, \u201cimage of blue car\u201d - \u201cblue\u201d + \u201cred\u201d gives a vector close to \u201cimage of red car\u201d. In contrast, we want to learn complex semantic relatedness for words using abstract scenes, i.e., our embeddings themselves are unimodal. For example, we want to learn that that eating and staring at are related, or that placed in and decorated on are (visually) similar.\nSurrogate Classification: There has been a lot of recent work on learning with surrogate labels due to interest in unsupervised representation learning. Previous works have used surrogate labels to learn image features [9, 7]. In contrast, we are interested in augmenting word embeddings with visual semantics. Previous works have created surrogate labels using data transformations [9] or sampling [7]. We create semantic surrogate labels by clustering images in\nthe abstract scenes feature space.\nLearning from Visual Abstraction: There is a lot of recent literature on learning from visual abstractions for a variety of high-level scene understanding tasks. Zitnick et al. [41, 40] learn the importance of various visual features (occurrence, co-occurrence, expression, gaze, etc.) in determining the meaning or semantics of a scene. [42] and [10] learn the visual interpretation of sentences and the dynamics of objects in temporal abstract scenes respectively. Antol et al. [2] learn models of fine-grained interactions between pairs of people using visual abstractions. Lin and Parikh [23] \u201cimagine\u201d abstract scenes corresponding to text, and use the common sense depicted in these imagined scenes to solve textual tasks such as fill-in-the-blanks and paraphrasing. Vedantam et al. [33] classify common sense assertions as plausible or not by using textual and visual cues. In this work, we experiment with the tasks of [23] and [33], which are two tasks in text that could benefit from visual grounding. Interestingly, by learning vis-w2v, we eliminate the need for visual signal at test time, i.e. the visual grounding captured in our word embeddings suffices.\nLanguage, Vision and Common Sense: Recent work has explored many problems at the intersection of language and vision. We study one such application of text-based image retrieval with our vis-w2v model (Sec. 4). There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc. In contrast to these works, we are interested in tasks which are ostensibly in text, but could benefit from visual cues. [36, 33] study one such problem of assessing the plausibility of common sense assertions. We evaluate our vis-w2v model on the common sense assertion classification task of [33]. We show that our vis-w2v model yields improvements over their joint text + vision model. In contrast to their model, our vis-w2v model implicitly captures visual cues, removing the need for abstract scenes at test time."}, {"heading": "3. Approach", "text": "Recall that our vis-w2v model grounds word embeddings into vision by treating vision as context. We first explain our experimental setup briefly. We then describe our procedure for clustering to get surrogate semantic labels, which are used as context by our model. We then describe our vis-w2v model architecture and make connections to w2v models.\nSetup: We need a multi-modal training set in order to train vis-w2v. That is, we need a set of pairs of visual scenes and associated text D = {(v, w)}d. We extract features from the image and denote the feature vector as v while w\nrefers to the set of words associated with the image. Our model is flexible and can accept text in a variety of forms such as full sentences or tuples of the form (Primary Object, Relation, Secondary Object). At each step in training, we select a window Sw \u2286 w to fine-tune the model. The choice of these windows is a design parameter depending upon the task. It could include all of w (e.g., in the case of concise tuples) or a subset of the words (e.g., in the case of windowbased representations on entire sentences). We next explain our clustering step that gives us the visual context for words.\nClustering: We group images into clusters in order to discover similarities in the visual feature space to use as surrogate labels for training vis-w2v. The surrogate labels serve the role of visual context for words. Since we want to learn fine-grained semantic relatedness between concepts from vision (Fig. 1), the feature space should capture finegrained details such as pose, expressions, attributes, interactions between objects, etc. We thus use the features from abstract scenes [33, 23] that are trivially fully annotated with a range of semantics. We cluster abstract images using Kmeans to obtain the surrogate labels1. K is a model parameter, kept constant during training. Clustering gives us a grouping function G(v) that assigns surrogate labels to each v, i.e., G maps all the visual features to one of K class labels {1,2, \u00b7 \u00b7 \u00b7 ,K}.\nModel: Our vis-w2v model (Fig. 2) is a neural network that accepts as input a set of words Sw and a visual feature instance v. Each of the words wi \u2208 Sw is represented via a one-hot encoding. A one-hot encoding enumerates over the set of words in a vocabulary (NV ) and places a 1 at the index corresponding to the given word. This one-hot encoded\n1Alternatively, one could regress directly to the feature values v. However, our experiments showed clustering performs better.\ninput is transformed using inner weight matrix WI of size NV \u00d7NH that connects the input layer to the hidden layer, where the hidden layer has a dimension of NH . Intuitively, NH decides the representative capacity of the embedding. Consider an input one-hot encoded word wi whose jth index is set to 1. Since wi is one-hot encoded, the hidden activation for this word (Hwi ) is a row in the weight matrix W jI , i.e., Hwi = W j I . The resultant hidden activation H would then be the average of individual hidden activations Hwi as WI is shared among all the words Sw, i.e.,:\nH = 1 |Sw| \u2211\nwi\u2208Sw\u2286w\nHwi (1)\nNote that the set of words Sw admits a wide variety of choices ranging from full sentence descriptions to words in primary, relation, or secondary object phrases. This allows us to train different versions of vis-w2v depending upon the task at hand.\nGiven the hidden activations H , we multiply it with an output weight matrix WO of size NH \u00d7NK , where NK is the number of surrogate classes as explained earlier. We normalize the output activations O = H \u00d7WO to form a distribution using the softmax function. Given the softmax outputs, we minimize the negative log-likelihood of the correct (surrogate) class conditioned on the input words:\nmin WI ,WO\n\u2212 logP (G(v)|Sw,WI ,WO) (2)\nWe use standard backpropagation techniques to optimize this objective function via stochastic gradient descent. We perform training until the performance on a held-out validation set saturates. While the training is task agnostic, and only needs access to the visual context, the validation performance may be calculated on a task of interest. We consider tasks such as common sense assertion classification, visual paraphrasing, and text-based image retrieval in this work. The learning rates for the layers are set to 0.01.\nConnections to w2v: Our model has some connections to the Continuous Bag of Words (CBOW) w2v models. The CBOW w2v objective maximizes the likelihood P (w\u2032|Sw,WI ,WO) for a word w\u2032 and its context Sw. On the other hand, we maximize the likelihood of the visual context given a set of words Sw (Eq. 2).\nFinetuning: We initialize the network input-to-hidden layer parameters WI with those from training w2v on large text corpora. The hidden-to-output layer parameters are initialized randomly. Using w2v is advantageous for us in two ways: i) w2v embeddings have been shown to capture rich semantics and generalize to a large number of tasks in text. Thus, they provide an excellent starting point to finetune the embeddings to account for visual similarity as well. ii) Training on a large corpus gives us good coverage in terms\nof the vocabulary. Further, since the gradients during backward propagation only affect parameters / embeddings for words seen during training, one can view vis-w2v as augmenting w2v with visual information when available. For other words, we retain the rich amount of non-visual information already present in it2. Indeed, we find that the random initialization does not perform as well as initialization with w2v when training vis-w2v."}, {"heading": "4. Applications", "text": "We study the benefit of using vis-w2v over w2v on common sense assertion classification, visual paraphrasing, and text-based image retrieval. All of these tasks are ostensibly in text, but could benefit from visual reasoning. We now detail the tasks and the datasets used to perform them."}, {"heading": "4.1. Common Sense Assertion Classification", "text": "We study the common sense (CS) assertion classification task introduced by Vedantam et al. [33]. Given common sense tuples of the form (primary object or P, relation or R, secondary object or S) e.g. (boy, eats, cake), the task is to classify it as plausible or not. The CS dataset contains 14,332 TEST assertions (spanning 203 relations) out of which 37% are plausible, as indicated by human annotations. These TEST assertions are extracted from the MSCOCO dataset [22], which contains real images and captions. Thus the TEST set contains common sense assertions about the real world. [33] approach the task by constructing a multi-modal similarity function between TEST assertions whose plausibility is to be evaluated, and TRAIN assertions that are known to be plausible. The TRAIN dataset contains 4260 abstract scenes made from clipart depicting 213 relations between various objects (20 scenes per relation). Each scene is also annotated with one tuple that names the primary object, relation, and secondary object depicted in the scene. Features describing the interaction between objects such as relative location, pose, absolute location, etc., are obtained from authors [33]. We use this dataset and the provided features to learn vis-w2v. More details of their features can be found in the appendix. We use their VAL set (14,548 assertions) to pick the number of clusters for training. Since the dataset contains tuples of the form (P, R, S), we explore learning visual word2vec with separate models for each, and a shared model irrespective of the word being P, R, or S.\n2We verified empirically that this does not cause \u2018calibration\u2019 issues. Specifically, given a pair of words where one word was refined using visual information but the other was not (unseen during training), using vis-w2v for the former and w2v for the latter when computing similarities between the two outperforms using w2v for both."}, {"heading": "4.2. Visual Paraphrasing", "text": "The visual paraphrasing (VP) task was introduced in [23]. Visual Paraphrasing is the problem of determining if a pair of descriptions describes the same scene or two different scenes. Each description contains three sentences. The dataset contains 10,020 abstract scenes and descriptions from Zitnick et al. [42]. The dataset contains 30,600 pairs of descriptions, of which a third are positive (describe the same scene) and the rest are negatives. The TRAIN dataset contains 24,000 VP pairs whereas the TEST dataset contains 6,060 VP pairs. We use the features provided by [41] to perform clustering of images. We withhold a set of 1000 pairs (333 positive and 667 negative) from TRAIN to form a VAL set to pick the number of clusters for training vis-w2v. Thus, our VP TRAIN set has 23,000 pairs."}, {"heading": "4.3. Text-based Image Retrieval", "text": "In order to check if our model has truly learnt the visual grounding of concepts, we study the task of text-based image retrieval. Given a query tuple, the task is to retrieve the image of interest by matching the query and ground truth tuples using word embeddings. We augment the common sense (CS) dataset [33] (Sec. 4.1) to collect three query tuples for each of the original 4260 CS TRAIN scenes. Each scene in the CS TRAIN dataset has annotations for which objects in the scene are the primary and secondary objects in the ground truth tuples. We highlight the primary and secondary objects in the scene and ask workers on AMT to name the primary, secondary objects, and the relation depicted by the interaction between the primary and secondary objects. Some example data can be seen in Fig. 3. Interestingly, some scenes elicit diverse tuples whereas others tend to be more constrained. This is related to the notion of Image Specificity [15]. Note that the workers do not see the original tuple written for the scene from the CS TRAIN\ndataset. More details of the interface can be found in the appendix. We use the collected tuples as queries for performing the retrieval task. Note that the queries used at test time are never used for training vis-w2v."}, {"heading": "5. Experimental Setup", "text": "We now explain some experimental details. We first explain how we integrate our vis-w2v and w2v models into the text-based methods explored for the common sense (CS), visual paraphrasing (VP), and text-based image retrieval tasks. We also provide evaluation details. We then explain the methods and baselines we compare to for each task and discuss some design choices. For text tokenization, we use the NLTK toolkit. We implement vis-w2v in the same framework as the Google C implementation of word2vec3."}, {"heading": "5.1. Common Sense Assertion Classification", "text": "The task in common sense assertion classification [33] is to compute the plausibility of a test assertion based on its similarity to a set of training tuples (\u2126) known to be plausible. Given a tuple (Primary Object tP, Relation tR, Secondary Object tS) and a training instance \u2126i, the plausibility scores are computed as follows:\nh(t\u2032,\u2126i) = WP (t \u2032 P ) T \u00b7WP (tiP ) +WR(t \u2032 R) T \u00b7WR(tiR) +WS(t\u2032S)T \u00b7WS(tiS) (3)\nwhere WP ,WR,WS represent the corresponding w2v embedding spaces. The final text score is given as follows:\nf(t\u2032) = 1 |I| \u2211 i\u2208I max(h(t\u2032,\u2126i)\u2212 \u03b4, 0) (4)\nwhere i sums over the entire set of training tuples. We use the values of \u03b4 used by [33] for our experiments.\n[33] share embedding parameters across P, R, S in their text based model. That is, WP = WR = WS . We call this the shared model. When WP ,WR,WS are learnt independently for (P, R, S), we call it the separate model.\nThe approach of [33] also has a visual similarity function that combines text and abstract scenes that is used along with this text-based similarity. We use the text-based approach for evaluating both vis-w2v and baseline w2v. However, we also report results including the visual similarity function along with text similarity from vis-w2v. In line with [33], we also evaluate our results using average precision (AP) as a performance metric.\n3https://code.google.com/p/word2vec/"}, {"heading": "5.2. Visual Paraphrasing", "text": "For the visual paraphrasing task, we are given a pair of descriptions at test time. We need to assign a score to each pair indicating how likely they are to be paraphrases, i.e., describing the same scene. Following [23] we average the word2vec (w2v) embeddings for the sentences and plug them into their text-based evalaution scoring function. This scoring function combines term frequency, word cooccurrence statistics and averaged w2v to assess the final paraphrasing score. The results are evaluated using average precision (AP) as the metric. While training w2v for the task, we append the sentences from the train set of [23] to the w2v training corpora in order to counter outof-vocabulary issues."}, {"heading": "5.3. Text-based Image Retrieval", "text": "We compare w2v and vis-w2v on the task of textbased image retrieval (Sec. 4.2). The task involves retrieving the target image from an image database, for a query tuple. Each image in the database has an associated ground truth tuple describing it. We use these to rank images by computing similarity with the query tuple4. Given tuples of the form (Primary Object, Relation, Secondary Object) or (P, R, S), we average the vector embeddings for all words in P, R, S. We then explore separate and shared models just as we did for common sense assertion classification. In the separate model, we first compute the cosine similarity between the query and the ground truth for P, R, S separately and average the three similarities. In the shared model, we average the w2v for P, R, S for query and ground truth and then compute the cosine similarity between the averaged embeddings. The similarity scores are then used to rank the images in the database for the query. We use standard retrieval metrics to evaluate: Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10) and median rank (med R) of target image in the returned results."}, {"heading": "5.4. Methods and Baselines", "text": "We next describe our methods and baselines. In general, consider two kinds of w2v models: those learnt from generic text, e.g., Wikipedia (w2v-wiki) and those learnt from visual text, e.g., MSCOCO (w2v-coco). Visual text is text describing images, such as the captions from the MSCOCO dataset. Embeddings learnt from visual text typically contain more visual information [33]. For all applications (Sec. 4), we train vis-w2v on the corresponding dataset. vis-w2v-wiki are vis-w2v embeddings learnt using w2v-wiki as a starting point, while vis-w2v-coco are the vis-w2v embeddings learnt using w2v-coco as the starting point. In all settings, we\n4One could think of our embeddings as a semantic hash of the images to help retrieve them better.\nare interested in studying the performance gains on using vis-w2v over w2v. Although our training procedure itself is task agnostic, we train separately on corresponding datasets for each task. Additionally, we also pick hyperparameters, such as the number of clusters (K) and size of hidden units (NH ), via validation performance on these tasks of interest for best results."}, {"heading": "6. Results", "text": "We present results demonstrating the advantage of vis-w2v over w2v on three applications: common sense assertion classification, visual paraphrasing, and text-based image retrieval. We compare our approach to various baselines as explained in Sec. 5 for each applications. Finally, we train our model using real images instead of abstract scenes, and analyze differences."}, {"heading": "6.1. Common Sense Assertion Classification", "text": "As explained in Sec. 4.1, the common sense task [33] requires us to classify common sense tuples as plausible or not. Here, we fix the size of the hidden units (NH ) to 200 when comparing to [33]. Table. 1 summarizes our results. Our best performing model on this task is a separate model trained with 25 clusters. We handle tuple elements, etc. P, R or S, with more than one word by placing each word in a separate window (i.e. |Sw| = 1). For instance, the element lay next to is trained by predicting the associated visual context thrice with lay, next and to as inputs. We find an increase of 2.6% with our best vis-w2v-coco model over the w2v-coco model used in [33]. We achieve larger gains (6.1%) with vis-w2v-wiki over w2v-wiki.\nPerformance Comparison: Our model outperforms the joint text + vision model from [33] that reasons about visual features for a given test tuple, which we do not. Interestingly, both models use the same training and validation data, which suggests that our vis-w2v model captures the grounding better than their multi-modal text + visual similarity model. Our best performing vis-w2v model gets an AP of 75.4% when we instantiate the model withNH = 50.\nSeparate vs. Shared: We next compare the performance\nwhen using the separate and shared vis-w2v models. We find that separate models trained on MSCOCO, perform better than shared models (74.8% vs 74.5%), presumably because the vis-w2v embeddings can specialize to the semantic roles words play when participating in P, R or S. In terms of shared models alone, we achieve a gain in performance of 2.3% over the w2v-coco model of [33], whose textual models are all shared.\nWhat Does Clustering Capture? We next visualize the semantics captured by the initial clustering step which gives us the surrogate labels to train on. We remind the reader that we use the semantically rich abstract scenes feature space. As explained in Sec. 4, our visual features in this experiment focus on interactions between primary and secondary objects in the scene [33], and capture semantics of location, pose, relative location, expression, gaze etc. To visualize the semantic relatedness captured by clustering, we pick some relations and display other relations that co-occur the most with it in the same cluster (Fig. 4). Interestingly, words like prepare to cut, hold, give occur often with stare at. Thus, we realize that when we prepare to cut something, we also tend to stare at it. Reasoning about such notions of semantic relatedness using purely textual cues would be prohibitively difficult. We provide more examples in the appendix."}, {"heading": "6.2. Visual Paraphrasing", "text": "We next describe our results on the Visual Paraphrasing (VP) task (Sec. 4.2). The task is to determine if a pair of descriptions are describing the same scene. Each description has three sentences. Table. 2 summarizes our results and compares performance to w2v. We vary the size of the context window Sw and check performance on the VAL set. We obtain best results with the entire description as the context window Sw. We found that applying Principal Component Analysis (PCA) on the image features before clus-\ntering gives marginal improvements. Our vis-w2v models give an improvement of 0.7 % on both w2v-wiki and w2v-coco respectively. In comparison to the w2v-wiki approach reported in [23], we get an overall gain of 1.2% with our vis-w2v-coco embeddings. [23] imagine the visual scene corresponding to text to solve the task. Their combined text + imagination model performs 0.2% better (95.5%) than our model. Note that our approach does not have the additional expensive step of generating an imagined visual scene for each instance at test time. Qualitative examples of success and failure cases are shown in Fig. 5.\nContext Window: Since the VP task is on multi-sentence descriptions, it gives us an opportunity to study how size of the context window (Sw) used in training affects performance. We show the gains obtained by using context sizes of entire description, single sentence, 5 words, and single word respectively. We find that description level context windows and sentence level context windows give equal gains. However, performance tapers off as we reduce the context to 5 words (0.6% gain) and a single word (0.1% gain). This is intuitive, since VP requires us to reason about entire scenes to determine if the two descriptions are paraphrases. Further, since the visual features in this dataset are scene level (and not about isolated interactions between objects), the signal in the hidden layer is stronger when an entire sentence is used."}, {"heading": "6.3. Text-based Image Retrieval", "text": "We next present results on the text-based image retrieval task (Sec. 4.3). This task requires visual grounding as the query and the ground truth tuple can often be really different by textual similarity, but can be describing the same scene (Fig. 3). We study generalization of the embeddings learnt during the common sense experiments for this task. Table. 3 presents our results. Note that vis-w2v here refers to the embeddings learnt using the CS dataset. We find that the best performing models are vis-w2v-wiki shared and vis-w2v-coco separate. Both give Recall@10 scores of \u2248 0.495 whereas the baseline w2v-wiki embeddings give scores of 0.454 and 0.476, respectively."}, {"heading": "6.4. Real Image Experiment", "text": "Finally, we test our vis-w2v approach with real images on the CS task. That is, instead of semantic features from abstract scenes, we obtain surrogate labels by clustering real images from the MSCOCO dataset using fc7 features from the VGG-16 [37] CNN. We cross validate to find the best number of clusters and hidden units. We perform real image experiments in two settings: 1) We use all of the MSCOCO dataset after removing the images whose tuples are in the CS TEST set of [33]. This gives us a collection of \u2248 78K images to learn vis-w2v. MSCOCO dataset has a collection of 5 captions for each image. We use all these five captions with sentence level context5 windows to learn vis-w2v80K. 2) We create a real image dataset by collecting 20 real images from MSCOCO and their corresponding tuples, for each of 213 relations randomly selected from the VAL set (Sec. 5.1). Analogous to the CS TRAIN set containing abstract scenes, this gives us a dataset of 4260 real images along with an associate tuple, depicting the 213 CS VAL relations. We refer to this model as vis-w2v4K.\nWe report the gains in performance over w2v baselines\n5We experimented with other choices but found this works best.\nin both scenario 1) and 2) for the common sense task. We find that using real images gives a best-case performance of 73.7% starting from w2v-coco for vis-w2v80K (as compared to 74.8% using CS TRAIN abstract scenes). For vis-w2v4K-coco, the performance on the validation actually goes down during training. If we train vis-w2v4K starting with generic text based w2v-wiki, we get a performance of 70.8% (as compared to 74.2% using CS TRAIN abstract scenes). This shows that abstract scenes are better at visual grounding as compared to real images, due to their rich semantic features."}, {"heading": "7. Discussion", "text": "Antol et al. [2] have studied generalization of classification models learnt on abstract scenes to real images. The idea is to transfer fine-grained concepts that are easier to learn in the fully-annotated abstract domain to tasks in the real domain. Our work attempts to solve the same problem in spirit. Our vis-w2v embeddings can be viewed as a way to transfer knowledge learnt in the abstract domain to the real domain, such as common sense assertion classification. While the abstract and real domains differ in pixels, they have common underlying semantics. vis-w2v embeddings can thus form the bridge between the abstract and real domains.\nWe next discuss some considerations in the design of the model. A possible design choice when learning embeddings could have been to construct a triplet loss function, where the similarity between a tuple and a pair of visual instances can be specified. That is, given a textual instance A, and two images B and C, one could construct a loss that enforces sim(A,B) >= sim(A,C), and learn joint embeddings for words and images. However, since we want to learn hidden semantic relatedness (e.g., \u201ceating\u201d, \u201cstare at\u201d), there is no explicit supervision available at train time on which images and words should be related. Although the visual scenes and associated text inherently provide information about related words, they do not capture the unrelatedness between words, i.e., we do not have negatives to help us learn the semantics. Thus, we discover these concepts in the semantic visual feature space and finetune generic word embeddings to respect the discovered visual relationships.\nAnother view of our vis-w2v approach is in terms of data augmentation. It is easy to realize that with infinite text data describing scenes, distributional semantics of w2v would capture all possible visual patterns as well. For instance, in Fig. 1, if the statements \u201cgirl eating ice cream\u201d and \u201cgirl stares at ice cream\u201d were both present in a textual corpus, it would be trivial to realize their semantic relatedness using a w2v-like approach. In this sense, there is nothing special about the visual grounding, except as a way to learn complimentary concepts while making efficient use of data. Thus, the visual grounding can be seen as augment-\ning textual data. In other words, a picture could literally be worth a 1000 words in our case."}, {"heading": "8. Conclusion", "text": "We learn visually grounded word embeddings named vis-w2v from abstract scenes and associated text. Abstract scenes, being trivially fully annotated, give us access to a rich semantic feature space. We leverage this feature space to learn visually grounded notions of semantic relatedness between words that would be difficult to capture using text alone and using real images. We demonstrate the visual grounding captured by our model on three applications that are in text, but benefit from visual cues: 1) common sense assertion classification, 2) visual paraphrasing, and 3) text-based image retrieval. Our method outperforms w2v baselines on all the tasks. Our method can be viewed as a modality to transfer knowledge from the abstract scenes domain to the real domain via text. All our datasets, code, and vis-w2v embeddings will be available for public use.\nAcknowledgments: This work was supported in part by the The Paul G. Allen Family Foundation via an award to D.P., ICTAS at Virginia Tech via an award to D.P., a Google Faculty Research Award to D.P. the Army Research Office YIP Award to D.P, and ONR grant N000141210903.\nAppendix We present detailed performance results of Visual Word2Vec (vis-w2v) on all three tasks :\n\u2022 Common sense assertion classification (Sec. A)\n\u2022 Visual paraphrasing (Sec. B)\n\u2022 Text-based image retrieval (Sec. C)\nSpecifically, we study the affect of various hyperparameters like number of surrogate labels (K), number of hidden layer nodes (NH ), etc., on the performance of both vis-w2v-coco and vis-w2v-wiki. We remind the reader that vis-w2v-coco models are initialized with w2v learnt on visual text, i.e., MSCOCO captions in our case while vis-w2v-wiki models are initialized with w2v learnt on generic Wikipedia text. We also show few visualizations and examples to qualitatively illustrate why vis-w2v performs better in these tasks that are ostentatiously in text, but benefit from visual cues. We conclude by presenting the results of training on real images (Sec. D)."}, {"heading": "A. Common Sense Assertion Classification", "text": "Recall that the common sense assertion classification task [33] is to determine if a tuple of the form (primary object or P, relation or R, secondary object or S) is plausible or not. In this section, we first describe the abstract visual features used by [33]. We follow it with results for vis-w2v-coco, both shared and separate models, by varying the number of surrogate classesK. We next discuss the effect of number of hidden units NH which can be seen as the complexity of the model. We then vary the amount of training data and study performance of vis-w2v-coco. Learning separate word embeddings for each of these specific roles, i.e., P, R or S results in separate models while learning single embeddings for all of them together gives us shared models. Additionally, we also perform and report similar studies for vis-w2v-wiki. Finally, we visualize the clusters learnt for the common sense task through word clouds, similar to Fig. 4 in the main paper.\nA.1. Abstract Visual Features\nWe describe the features extracted from abstract scenes for the task of common sense assertion classification. Our visual features are essentially the same as those used by [33]: a) Features corresponding to primary and secondary object, i.e., P and S respectively. These include type (category ID and instance ID), absolute location modeled via Gaussian Mixture Model (GMM), orientation, attributes and poses for both P and S present in the scene. We use Gaussian Mixture at hands and foot locations to model pose, measuring relative positions and joint locations. Human attributes are age (5 discrete values), skin color (3 discrete\nvalues) and gender (2 discrete values). Animals have 5 discrete poses. Human pose features are constructed using keypoint locations. b) Features corresponding to relative location of P and S, once again modeled using Gaussian Mixture Models. These features are normalized by the flip and depth of the primary object, which results in the features being asymmetric. We compute these with respect to both P and S to make the features symmetric. c) Features related to the presence of other objects in the scene, i.e., category ID and instance ID for all the other objects. Overall the feature vector is of dimension 1222.\nA.2. Varying number of clusters K\nIntuition: We cluster the images in the semantic clipart feature space to get surrogate labels. We use these labels as visual context, and predict them using words to enforce visual grounding. Hence, we study the influence of the number of surrogate classes relative to the number of images. This is indicative of how coarse/detailed the visual grounding for a task needs to be.\nSetup: We train vis-w2v models by clustering visual features with and without dimensionality reduction through Principal Component Analysis (PCA), giving us Orig and PCA settings, respectively. Notice that each of the elements of tuples, i.e., P, R or S could have multiple words, e.g., lay next to. We handle these in two ways: a) Place each of the words in separate windows and predict the visual context repeatedly. Here, we train by predicting the same visual context for lay, next, to thrice. This gives us the Words setting. b) Place all the words in a single window and predict the visual context for the entire element only once. This gives the Phrases setting. We explore the cross product space of settings a) and b). PCA/Phrases (red in Fig. 6) refers to the model trained by clustering the dimensionality reduced visual features and handling multi-word elements by including them in a single window. We vary the number of surrogate classes from 15 to 35 in steps of 5, re-train vis-w2v for each K, and report the accuracy on the common sense task. The number of hidden units NH is kept fixed to 200 to be comparable to the text-only baseline reported in [33]. Fig. 6 shows the performance on the common sense task as K varies for both shared and separate models in four possible configurations each, as described above.\nObservations:\n\u2022 As K varies, the performance for both shared and separate models increases initially and then either saturates or decreases. For a given dataset, low values of K result in the visual context being too coarse to learn the visual grounding. On the other hand, K being too high results in clusters which do not capture visual semantic related-\nness. We found the best model to have around 25 clusters in both the cases.\n\u2022 Words models perform better than Phrases models in both cases. Common sense task involves reasoning about the specific role (P, R or S) each word plays. For example, (man, eats, sandwich) is plausible while (sandwich, eats, sandwich) or (man, sandwich, eats) is not. Potentially, vis-w2v could learn these roles in addition to the learning semantic relatedness between the words. This explains why separate models perform better than shared models, and Words outperform Phrases setting.\n\u2022 For lower K, PCA models dominate over Orig models while the latter outperforms as K increases. As low values of K correspond to coarse visual information, surrogate classes in PCA models could be of better quality and thus help in learning the visual semantics.\nA.3. Varying number of hidden units NH\nIntuition: One of the model parameters for our vis-w2v is the number of hidden units NH . This can be seen as the capacity of the model. We vary NH while keeping the other factors constant during training to study its affect on performance of the vis-w2v model.\nSetup: To understand the role of NH , we consider two vis-w2v models trained separately with K set to 10 and 25 respectively. Additionally, both of these are separate models with Orig/Words configuration (see Sec. A.2). We particularly choose these two settings as the former is trained with a very coarse visual semantic information while the latter is the best performing model. Note that as [33] fix the number of hidden units to 200 in their evaluation, we cannot directly compare the performance to their baseline. We, therefore, recompute the baselines for each value of NH \u2208 {20, 30, 40, 50, 100, 200, 400} and use it to compare our two models, as shown in Fig. 8.\nObservations: Models of low complexity, i.e., low values of NH , perform the worst. This could be due to the inherent limitation of low NH to capture the semantics, even for w2v. On the other hand, high complexity models also perform poorly, although better than the low complexity models. The number of parameters to be learnt, i.e. WI and WO, increase linearly with NH . Therefore, for a finite amount of training data, models of high complexity tend to overfit resulting in drop in performance on an unseen test set. The baseline w2v models also follow a similar trend. It is interesting to note that the improvement of vis-w2v over w2v for less complex models (smaller NH ) is at 5.32% (for NH = 20) as compared to 2.6% (for NH = 200). In other words, lower complexity models benefit more from the vis-w2v enforced visual grounding. In\nfact, vis-w2v of low complexity (NH ,K) = (20, 25), outperforms the best w2v baseline across all possible settings of model parameters. This provides a strong evidence for the usefulness of visually grounding word embeddings in capturing visually-grounded semantics better.\nA.4. Varying size of training data\nIntuition: We next study how varying the size of the training data affects performance of the model. The idea is to\nanalyze whether more data about relations would help the task, or more data per relation would help the task.\nSetup: We remind the reader that vis-w2v for common sense task is trained on CS TRAIN dataset that contains 4260 abstract scenes made from clipart depicting 213 relations between various objects (20 scenes per relation). We identify two parameters: the number of relations nR and the number of abstract scenes per relation nT . Therefore, CS TRAIN dataset originally has (nT , nR) = (20, 213). We\nvary the training data size in two ways: a) Fix nR = 213 and vary nT \u2208 {1, 2, 5, 10, 12, 14, 16, 18, 20}. b) Fix nT = 20 and vary nR in steps of 20 from 20 to 213. These cases denote two specific situations\u2013the former limits the model in terms of how much it knows about each relation, i.e. its depth, keeping the number of relations, i.e. its breadth, constant; while the latter limits the model in terms of how many relations it knows, i.e., it limits the breadth keeping the depth constant. Throughout this study, we select the best performing vis-w2v model with (K,NH) = (25, 200) in the Orig/Words configuration. Fig. 7a shows the performance on the common sense task when nR is fixed while Fig. 7b is the performance when nT is fixed.\nObservations: The performance increases with the increasing size of training data in both the situations when nT and nR is fixed. However, the performance saturates in the former case while it increases with almost a linear rate in the latter. This shows that breadth helps more than the depth in learning visual semantics. In other words, training with more relations and fewer scenes per relation\nis more beneficial than training with fewer relations and more scenes per relation. To illustrate this, consider performance with approximately around half the size of the original CS TRAIN dataset. In the former case, it corresponds to 73.5% at (nT , nR) = (10, 213) while 70.6% at (nT , nR) = (20, 100) in the latter. Therefore, we conclude that the model learns semantics better with more concepts (relations) over more instances (abstract scenes) per concept.\nA.5. Cluster Visualizations\nWe show the cluster visualizations for a randomly sampled set of relations from the CS VAL set (Fig. 9). As in the main paper (Fig. 4), we analyze how frequently two relations co-occur in the same clusters. Interestingly, relations like drink from co-occur with relations like blow out and bite into which all involve action with a person\u2019s mouth.\nB. Visual Paraphrasing\nThe Visual Paraphrasing (VP) task [23] is to classify whether a pair of textual descriptions are paraphrases of each other. These descriptions have three sentence each. Table 4 presents results on VP for various settings of the model that are described below.\nModel settings: We vary the number of hidden units NH \u2208 {50, 100, 200} for both vis-w2v-coco and vis-w2v-wiki models. We also vary our context window size to include entire description (Descs), individual sentences (Sents), window of size 5 (Winds) and individual words (Words). As described in Sec. A.2, we also have Orig and PCA settings.\nObservations: From Table 4, we see improvements over the text baseline [23]. In general, PCA configuration outperforms Orig for low complexity models (NH = 50). Using entire description or sentences as the context window gives almost the same gains, while performs drops when smaller context windows are used (Winds and Words). As VP is a sentence level task where one needs to reason about the entire sentence to determine whether the given descriptions are paraphrases, these results are intuitive."}, {"heading": "C. Text-based Image Retrieval", "text": "Recall that in Text-based Image Retrieval (Sec. 4.3 in main paper), we highlight the primary object (P) and secondary object (S) and ask workers on Amazon Mechanical Turk (AMT) to describe the relation illustrated by the scene with tuples. An illustration of our tuple collection interface can be found in Fig. 10. Each of the tuples entered in the text-boxes is treated as the query for text-based image retrieval.\nSome qualitative examples of success and failure cases of vis-w2v-wiki with respect to w2v-wiki are shown in Fig. 11. We see that vis-w2v-wiki captures notions such as the relationship between holding and opening better than w2v-wiki."}, {"heading": "D. Real Image Experiments", "text": "We now present the results when training vis-w2vwith real images from MSCOCO dataset by clustering using fc7 features from the VGG-16 [37] CNN.\nIntuition: We train vis-w2v embeddings with real images and compare them to those trained with abstract scenes, through the common sense task.\nSetup: We experiment with two settings: a) Considering\nall the 78k images from MSCOCO dataset, along with associated captions. Each image has around 5 captions giving us a total of around 390k captions to train. We call vis-w2v trained on this dataset as vis-w2v80k. b) We randomly select 213 relations from VAL set and collect 20 real images from MSCOCO and their corresponding tuples. This would give us 4260 real images with tuples, depicting the 213 CS VAL relations. We refer to this model as vis-w2v4k.\nWe first train vis-w2v80k with NH = 200 and\nuse the fc7 features as is, i.e. without PCA, in the Sents configuration (see Sec. B). Further, to investigate the complementarity between visual semantics learnt from real and visual scenes, we initialize vis-w2v-coco with vis-w2v-coco80k, i.e., we learn the visual semantics from the real scenes and train again to learn from abstract scenes. Table 5 shows the results for vis-w2v-coco80k, varying the number of surrogate classes K.\nWe then learn vis-w2v4k with NH = 200 in the Orig/Words setting (see Sec. A). We observe that the performance on the validation set reduces for vis-w2v-coco4k. Table 6 summarizes the results for vis-w2v-wiki4k.\nObservations: From Table 5 and Table 6, we see that there\nare indeed improvements over the text baseline of w2v. The complementarity results (Table 5) show that abstract scenes help us ground word embeddings through semantics complementary to those learnt from real images. Comparing the improvements from real images (best AP of 73.7%) to those from abstract scenes (best AP of 74.8%), we see that that abstract visual features capture visual semantics better than real images for this task. It if often difficult to capture localized semantics in the case of real images. For instance, extracting semantic features of just the primary and secondary objects given a real image, is indeed a challenging detection problem in vision. On the other hand, abstract scene offer these fine-grained semantics features therefore making them an ideal for visually grounding word embeddings."}], "references": [{"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Zero-shot learning via visual abstraction", "author": ["S. Antol", "C.L. Zitnick", "D. Parikh"], "venue": "In Computer Vision - ECCV 2014 - 13th European Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CoRR, abs/1411.5654,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Discriminative unsupervised feature learning with  convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Predicting object dynamics in scenes", "author": ["D.F. Fouhey", "C.L. Zitnick"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "A. Yuille"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "metrics. J. Artif. Intell. Res. (JAIR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Image Specificity", "author": ["M. Jas", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S.M. Katz"], "venue": "In IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1987}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "page 13,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "In Proceedings of the 24th CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Microsoft COCO: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Don\u2019t just listen, use your imagination: Leveraging visual common sense for non-visual tasks", "author": ["X. Lin", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR (to appear),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Action recognition from a distributed representation of pose and appearance", "author": ["S. Maji", "L. Bourdev", "J. Malik"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "CoRR, abs/1410.0210,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "CoRR, abs/1505.01121,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "CoRR, abs/1410.1090,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Neural network based language models for highly inflective languages", "author": ["T. Mikolov", "J. Kopecky", "L. Burget", "O. Glembek", "J. Cernocky"], "venue": "In 2009 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Midge: Generating descriptions of images", "author": ["M. Mitchell", "X. Han", "J. Hayes"], "venue": "In Proceedings of the Seventh International Natural Language Generation Conference,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Learning common sense through visual abstraction", "author": ["Xiao Lin"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R.S. Zemel"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In CVPR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Improving word representations via global visual context", "author": ["R. Xu", "J. Lu", "C. Xiong", "Z. Yang", "J.J. Corso"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C. Zitnick", "R. Vedantam", "D. Parikh"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "There is a rich history of works which do n-gram based language modeling [17, 4].", "startOffset": 73, "endOffset": 80}, {"referenceID": 3, "context": "There is a rich history of works which do n-gram based language modeling [17, 4].", "startOffset": 73, "endOffset": 80}, {"referenceID": 2, "context": "However, in recent years, neural language models [3, 30] have been growing in popularity.", "startOffset": 49, "endOffset": 56}, {"referenceID": 29, "context": "However, in recent years, neural language models [3, 30] have been growing in popularity.", "startOffset": 49, "endOffset": 56}, {"referenceID": 30, "context": "One popular choice for this vector space is the word2vec [31, 29] embedding.", "startOffset": 57, "endOffset": 65}, {"referenceID": 28, "context": "One popular choice for this vector space is the word2vec [31, 29] embedding.", "startOffset": 57, "endOffset": 65}, {"referenceID": 30, "context": "This embedding captures rich notions of semantic relatedness and compositionality between words [31].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 153, "endOffset": 157}, {"referenceID": 39, "context": "of objects are readily accessible [40, 41].", "startOffset": 34, "endOffset": 42}, {"referenceID": 40, "context": "of objects are readily accessible [40, 41].", "startOffset": 34, "endOffset": 42}, {"referenceID": 30, "context": "Since we predict a single context output given a set of input words, our model can be viewed as a multi-modal extension of the CBOW (Continuous Bag Of Words) [31] word2vec model.", "startOffset": 158, "endOffset": 162}, {"referenceID": 22, "context": "We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 32, "context": "We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc.", "startOffset": 194, "endOffset": 198}, {"referenceID": 14, "context": "We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc.", "startOffset": 226, "endOffset": 230}, {"referenceID": 22, "context": "Visual paraphrasing [23] is the problem of determining whether two sentences describe the same underlying scene or not.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "Common sense assertion classification [33] is the task of modeling the plausibility of common sense assertions of the form (boy, eats, cake).", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Word Embeddings: Neural network based word embeddings [31, 6] have gained a lot of popularity in recent times.", "startOffset": 54, "endOffset": 61}, {"referenceID": 5, "context": "Word Embeddings: Neural network based word embeddings [31, 6] have gained a lot of popularity in recent times.", "startOffset": 54, "endOffset": 61}, {"referenceID": 2, "context": "These embeddings are learnt offline and then typically fed into a multi-layer Neural Network Language Model [3, 30].", "startOffset": 108, "endOffset": 115}, {"referenceID": 29, "context": "These embeddings are learnt offline and then typically fed into a multi-layer Neural Network Language Model [3, 30].", "startOffset": 108, "endOffset": 115}, {"referenceID": 38, "context": "[39] use visual cues to improve the w2v representation by predicting global visual context using fc7 features from real images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc.", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 17, "context": "Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 8, "context": "Previous works have used surrogate labels to learn image features [9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "Previous works have used surrogate labels to learn image features [9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 8, "context": "Previous works have created surrogate labels using data transformations [9] or sampling [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Previous works have created surrogate labels using data transformations [9] or sampling [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 40, "context": "[41, 40] learn the importance of various visual features (occurrence, co-occurrence, expression, gaze, etc.", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[41, 40] learn the importance of various visual features (occurrence, co-occurrence, expression, gaze, etc.", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "[42] and [10] learn the visual interpretation of sentences and the dynamics of objects in temporal abstract scenes respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[42] and [10] learn the visual interpretation of sentences and the dynamics of objects in temporal abstract scenes respectively.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "[2] learn models of fine-grained interactions between pairs of people using visual abstractions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Lin and Parikh [23] \u201cimagine\u201d abstract scenes corresponding to text, and use the common sense depicted in these imagined scenes to solve textual tasks such as fill-in-the-blanks and paraphrasing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "[33] classify common sense assertions as plausible or not by using textual and visual cues.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In this work, we experiment with the tasks of [23] and [33], which are two tasks in text that could benefit from visual grounding.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "In this work, we experiment with the tasks of [23] and [33], which are two tasks in text that could benefit from visual grounding.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 4, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 7, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 37, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 27, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 18, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 13, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 31, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 20, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 7, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 122, "endOffset": 129}, {"referenceID": 34, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 10, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 26, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 33, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 25, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 11, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 15, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 207, "endOffset": 215}, {"referenceID": 17, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 207, "endOffset": 215}, {"referenceID": 35, "context": "[36, 33] study one such problem of assessing the plausibility of common sense assertions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[36, 33] study one such problem of assessing the plausibility of common sense assertions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "We evaluate our vis-w2v model on the common sense assertion classification task of [33].", "startOffset": 83, "endOffset": 87}, {"referenceID": 32, "context": "We thus use the features from abstract scenes [33, 23] that are trivially fully annotated with a range of semantics.", "startOffset": 46, "endOffset": 54}, {"referenceID": 22, "context": "We thus use the features from abstract scenes [33, 23] that are trivially fully annotated with a range of semantics.", "startOffset": 46, "endOffset": 54}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "These TEST assertions are extracted from the MSCOCO dataset [22], which contains real images and captions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 32, "context": "[33] approach the task by constructing a multi-modal similarity function between TEST assertions whose plausibility is to be evaluated, and TRAIN assertions that are known to be plausible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": ", are obtained from authors [33].", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "The visual paraphrasing (VP) task was introduced in [23].", "startOffset": 52, "endOffset": 56}, {"referenceID": 41, "context": "[42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "We use the features provided by [41] to perform clustering of images.", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "We augment the common sense (CS) dataset [33] (Sec.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "This is related to the notion of Image Specificity [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "The task in common sense assertion classification [33] is to compute the plausibility of a test assertion based on its similarity to a set of training tuples (\u03a9) known to be plausible.", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "We use the values of \u03b4 used by [33] for our experiments.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "[33] share embedding parameters across P, R, S in their text based model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "The approach of [33] also has a visual similarity function that combines text and abstract scenes that is used along with this text-based similarity.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "In line with [33], we also evaluate our results using average precision (AP) as a performance metric.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Following [23] we average the word2vec (w2v) embeddings for the sentences and plug them into their text-based evalaution scoring function.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "While training w2v for the task, we append the sentences from the train set of [23] to the w2v training corpora in order to counter outof-vocabulary issues.", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "Embeddings learnt from visual text typically contain more visual information [33].", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "w2v-coco (from [33]) 72.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "2 w2v-wiki (from [33]) 68.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "1 w2v-coco + vision (from [33]) 73.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "Table 1: Performance on the common sense task of [33]", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "1, the common sense task [33] requires us to classify common sense tuples as plausible or not.", "startOffset": 25, "endOffset": 29}, {"referenceID": 32, "context": "Here, we fix the size of the hidden units (NH ) to 200 when comparing to [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "6% with our best vis-w2v-coco model over the w2v-coco model used in [33].", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "Performance Comparison: Our model outperforms the joint text + vision model from [33] that reasons about visual features for a given test tuple, which we do not.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "3% over the w2v-coco model of [33], whose textual models are all shared.", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "4, our visual features in this experiment focus on interactions between primary and secondary objects in the scene [33], and capture semantics of location, pose, relative location, expression, gaze etc.", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "In comparison to the w2v-wiki approach reported in [23], we get an overall gain of 1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "[23] imagine the visual scene corresponding to text to solve the task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "w2v-wiki (from [23]) 94.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Table 2: Performance on visual paraphrasing task of [23].", "startOffset": 52, "endOffset": 56}, {"referenceID": 36, "context": "That is, instead of semantic features from abstract scenes, we obtain surrogate labels by clustering real images from the MSCOCO dataset using fc7 features from the VGG-16 [37] CNN.", "startOffset": 172, "endOffset": 176}, {"referenceID": 32, "context": "We perform real image experiments in two settings: 1) We use all of the MSCOCO dataset after removing the images whose tuples are in the CS TEST set of [33].", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "[2] have studied generalization of classification models learnt on abstract scenes to real images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "Recall that the common sense assertion classification task [33] is to determine if a tuple of the form (primary object or P, relation or R, secondary object or S) is plausible or not.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "In this section, we first describe the abstract visual features used by [33].", "startOffset": 72, "endOffset": 76}, {"referenceID": 32, "context": "Our visual features are essentially the same as those used by [33]: a) Features corresponding to primary and secondary object, i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": "The number of hidden units NH is kept fixed to 200 to be comparable to the text-only baseline reported in [33].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "Note that as [33] fix the number of hidden units to 200 in their evaluation, we cannot directly compare the performance to their baseline.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "The Visual Paraphrasing (VP) task [23] is to classify whether a pair of textual descriptions are paraphrases of each other.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "Observations: From Table 4, we see improvements over the text baseline [23].", "startOffset": 71, "endOffset": 75}, {"referenceID": 36, "context": "We now present the results when training vis-w2vwith real images from MSCOCO dataset by clustering using fc7 features from the VGG-16 [37] CNN.", "startOffset": 134, "endOffset": 138}, {"referenceID": 32, "context": "Table 5: Performance on the common sense task of [33] using 78k real images with text baseline at 72.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "Table 6: Performance on the common sense task of [33] using 4k real images with with text baseline at 68.", "startOffset": 49, "endOffset": 53}], "year": 2017, "abstractText": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food. Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision. We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness. We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets will be available online.", "creator": "LaTeX with hyperref package"}}}