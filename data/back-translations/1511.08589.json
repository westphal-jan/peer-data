{"id": "1511.08589", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Shaping Proto-Value Functions via Rewards", "abstract": "In this paper, we combine task-dependent reward shaping and task-independent proto-value functions to obtain reward-dependent proto-value functions (RPVFs). In constructing the RPVFs, we use the immediate rewards available during the sampling phase but not used in the PVF construction. We demonstrate from experiments that learning with an RPVF-based representation is better than learning with pure reward formation or PVFs. Especially when the state space is symmetrical and the rewards are asymmetrical, the RPVF captures the asymmetry better than the PVFs.", "histories": [["v1", "Fri, 27 Nov 2015 09:13:04 GMT  (1911kb,D)", "http://arxiv.org/abs/1511.08589v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["chandrashekar lakshmi narayanan", "raj kumar maity", "shalabh bhatnagar"], "accepted": false, "id": "1511.08589"}, "pdf": {"name": "1511.08589.pdf", "metadata": {"source": "CRF", "title": "Shaping Proto-Value Functions via Rewards", "authors": ["Chandrashekar Lakshmi Narayanan", "Raj Kumar Maity"], "emails": [], "sections": [{"heading": null, "text": "In contrast to supervised learning, agent performing an RL task needs to learn from the rewards. However, in goal-based RL tasks, the rewards are delayed, i.e., the agents receive feedback only after reaching the goal state and such delay can cause poor learning rates. Reward shaping is the mechanism of providing additional rewards for correct behavior in non-goal states, thereby aiding the learning process.\nIn this paper, we combine task-dependent reward shaping and task-independent protovalue functions to obtain reward dependent proto-value functions (RPVFs). In constructing the RPVFs we are making use of the immediate rewards which are avaialble during the sampling phase but are not used in the PVF construction. We show via experiments that learning with an RPVF based representation is better than learning with just reward shaping or PVFs. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs."}, {"heading": "1 Introduction", "text": "Reinforcement Learning (RL) tasks problems are cast in the framework of Markov decision processes (MDPs). In the MDP setting, dynamics of the underlying environment evolves within a set of states called the state-space, and the agent performs actions to control the state of the system. The agent receives a reward which is dependent on the state and the action it performs. The agent\nar X\niv :1\n51 1.\n08 58\n9v 1\n[ cs\n.A I]\n2 7\nN ov\n2 01\naims to maximize the discounted infinite sum of the rewards obtained as a result of its actions. Formally, any action selection mechanism is known as a policy and the agent aims to learn the optimal policy.\nIn order to learn the optimal behavior/policy, the agent first needs to evaluate the current behavior. The value function Ju corresponding to a given policy u, is a map from the state space to real numbers, and captures the total discounted reward that agent collects by following the policy u. From the knowledge of the value function, the agent can improve its behavior and hence learning the value function in an efficient manner assumes importance.\nAgent\u2019s choice for representing the value function affects the learning process. A desirable property is that the representation has to be compact (i.e., it should be easy to compute and store the value function). The linear function representation is the most widely used, wherein, the value function is represented as a linear combination of the basis functions. In general, the choice of the basis is guided by the task-specific knowledge and when there is no such task-specific information, primitive functions such as radial basis functions, polynomial bases and tile coded bases are chosen. Neverthless, it is desirable to be able to construct basis functions with little or no information about the task.\nThe proto-value functions (PVFs) [9] are bases that can be constructed in task-independent manner, and have been applied to a wide variety of domains. The PVFs are obtained by diagonalizing symmetric diffusion operators on an empirically learned graph representing the underlying state space. A diffusion model or the random walk on an undirected graph, where the probability of transitioning from a vertex (state) to its neighbor is proportional to its degree, is intended to capture information flow on a graph. The PVFs being equivalent to Fourier bases were shown to capture the intricate connectivity in the underlying state space which primitive bases such as Fourier or polynomials do not capture. [10] presented the representational policy iteration (RPI) algorithm by combining the PVF basis construction with least squares policy iteration (LSPI).\nWhilst topology dictated by the underlying graph of the MDP constitutes one form of domain knowledge (feature selection), researchers have also looked at other means to enable faster learning. Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process. The reward function has to be chosen in such a way\nthat it preserves the optimal policy. Reward shaping is task-specific since the shaping function is not dependent on the state space and varies depending on the goal or the reward structure. Our Contribution In this paper, we combine the ideas of task-independent PVF construction and the task-specific reward shaping to construct Reward based Proto-Value Functions (RPVFs). The idea behind such a construction is the observation that the actual neighborhood we are interested in is the one that is generated by the value function itself. While, topologically near states might have similar values, it is also true that the value is affected by the immediate rewards. In a general reward MDP, though the immediate rewards are obtained during the sampling phase, they are not used in the PVF construction. We modify the diffusion operator using the immediate rewards in order to construct the RPVFs. We show success of RPVFs in experiments on benchmark RL tasks. Highlights Our experiments demonstrate that similarity matrices other than the diffusion matrix can be used to generate features, and that reward shaping does benifit when the features are ill chosen. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs. Organization We first present the overview of the reinforcement learning paradigm emphasizing the need to learn the value function. We then discuss proto-value functions in brief. Next we present the RPI algorithm following which we discuss the objective of reward shaping. Finally, we present RPVF and experimental results."}, {"heading": "2 Reinforcement Learning Paradigm", "text": "Environment The dynamics of the underlying environment can be captured in the framework of Markov decision process (MDP). An MDP is a 4-tuple < S,A, P,R >, where S is the state space, A is the action space, P is the probability transition kernel and R is the reward function. The probability transition kernel P specifies the probability pa(s, s\u2032) of transitioning from state s to state s\u2032 under the action a. The reward function R is a map R : S \u00d7 A \u2192 R that specifies the reward obtained for performing action a \u2208 A in state s \u2208 S and is denoted by ra(s). Agent Behavior The behavior of the agent is captured by the way the actions it makes in each and every state. In the MDP parlance, this action selection mechanism is called the policy. Formally, by a policy, we mean a sequence \u00b5 = {\u00b50, . . . , \u00b5n, . . .} of functions \u00b5i, i \u2265 0 that describe the manner in which an action is picked in a given state at time i. Two important types of policies that\nare also useful are: (i) Stationary Randomized Policy (SRP), given by \u00b5 = {\u00b50, . . . , \u00b5i, . . .}, where \u00b5i \u2261 \u03c0, \u2200i \u2265 0 with \u03c0(s, \u00b7) being a probability distribution over the set of actions for any s \u2208 S. (ii) Stationary Deterministic Policy (SDP), given by \u00b5 = {\u00b50, . . . , \u00b5i, . . .}, where \u00b5i \u2261 u, \u2200i \u2265 0 with u : S \u2192 A being a map from the state space to the action space.\nNote that an SDP is trivially a SRP as well. By abuse of notation, we refer an SRP by \u03c0 and an SDP by u. Further, under a stationary policy u (or \u03c0), the MDP is a Markov chain and we denote its probability transition kernel by Pu = (pu(i)(i, j), i, j = 1, . . . , n) (or P\u03c0 = (p\u03c0(i)(i, j), i, j = 1, . . . , n), where p\u03c0(i)(i, j) = \u2211 a\u2208A \u03c0(i, a)pa(i, j) and \u03c0(i) = (\u03c0(i, a), a \u2208 A)).\nValue Function We define the infinite horizon discounted reward value function under an SRP \u03c0 as J\u03c0(s) = E [\u2211\u221e t=0 \u03b1 trt|s0 = s, \u03c0 ] , where \u03b1 \u2208 (0, 1) is the discount factor and rt = rat(st)\nwith at \u223c \u03c0(st, \u00b7),\u2200t \u2265 1. Similarly, we also define the infinite horizon discounted reward stateaction value function under an SRP \u03c0 as Q\u03c0(s, a) = E [\u2211\u221e\nt=0 \u03b1 trt|s0 = s, a0 = a, \u03c0\n] .\nThe optimal policy1 and the optimal value function obey the Bellman equation (BE) given below: \u2200s \u2208 S,\nQ\u2217(s, a) = ( ra(s) + \u03b1 \u2211 s\u2032 pa(s, s \u2032) max a\u2032\u2208A Q\u2217(s\u2032, a\u2032) ) ,\nu\u2217(s) = arg max a\u2208A\nQ\u2217(s, a). (1)\nRL Agent Any RL agent has three important building blocks or sub-functions namely sample collection, the representation and the learning algorithm. Learner represents state of the environment st at time t as a point in the feature space. Learning algorithm makes use of samples (st, rt), t \u2264 n obtained from the environment and its own past behavior at, t \u2264 nto learn. The behavior of the agent is dictated by policy \u03c0 it makes use to choose the actions. From (??) it is clear that in order to compute the optimal behavior u\u2217, the agent needs to learn Q\u2217. Even in the case when agent wants to improve a given policy \u03c0, it has to evaluate Q\u03c0 and then substituting Q\u03c0 in (??) will lead to an improved policy [1]. Thus, learning the value function is central in learning the correct behavior. Such learning is dependent on the agent\u2019s way of representing the value functions. Value Function Representation The most widely used representation is the linear function representation, wherein, the value Q\u03c0(s, a) of state-action pair (s, a) is expressed as a weighted combination of the feature corresponding to that state, i.e., Q\u03c0(s, a) = \u2211k i=1 \u03c6i(s, a)\n>w\u03c0(i), where (\u03c6i(s, a), i = 1, . . . , k) \u2208 Rk is the feature of the state s and w\u03c0 \u2208 Rk is a learned weight vector. Any linear representation can be compactly represented by its feature matrix \u03a6 = [\u03c61| . . . , \u03c6k], where \u03c6i \u2208 RC, i = 1 . . . , k(C = |S||A|) are the k basis functions.\nClassical numerical schemes such as value iteration, policy iteration and linear programming choose a look up table representation. Under the look up table representation the standard basis is chosen, i.e., \u03c61 = (1, 0, . . . , 0)>. Thus there are as many basis functions as number of state-action pairs and as a result they might not always be efficient.\n1In the infinite horizon discounted reward setting that we consider, one can find an SDP that is optimal [1, 12]\nLearning Algorithm The least squares policy iteration (LSPI) algorithm is a widely used algorithm that makes use of a linear representation to learn the value function. LSPI [8] makes use of\nleast squares temporal difference learning (LSTD) [2] which computes Q\u0302\u03c0 = k\u2211 i=1 \u03c6iw \u03c0 i by solving an approximate fixed-point equation given by \u03a6w\u03c0 \u2248 R+ \u03b1H\u03c0Q\u0302\u03c0, where H\u03c0 is the C \u00d7 C matrix specifying the probability of transitioning between state-action pairs. On re-arranging we have \u03a6w\u03c0 \u2212 \u03b1H\u03c0\u03a6w\u03c0 \u2248 R.\nFrom least-squares regression we know that w\u03c0 = (\u03a6>D\u03c0(\u03a6w\u03c0 \u2212 \u03b1H\u03c0))\u22121\u03a6>D\u03c0R, where D\u03c0 is a diagonal matrix whose entries are the stationary distributions of the various state-action pairs under the SRP \u03c0. Further, by letting A\u03c0 = (\u03a6w\u03c0 \u2212 \u03b1H\u03c0) and b\u03c0 = \u03a6>D\u03c0R, it follows that w\u03c0 = (A\u03c0)\u22121b\u03c0.\nIn the section to follow, we will describe the proto-value functions, and the representational policy iteration algorithm (RPI). The RPI uses PVFs in the LSPI algorithm to learn the value function."}, {"heading": "3 Proto-Value Functions", "text": "It is in general a good idea to select basis functions by using domain knowledge. When the domain knowledge is absent, representations based on well known primitive functions such as radial, polynomial or Fourier can be used. However, such representations based on the primitive functions might not yield good results.Hence, it is desirable to be able to choose the basis functions in a task-independent manner. The proto-value functions are task-independent basis functions and are based on the topology of the state space. Being eigen functions of the random walk operator, the proto-value functions capture the connectivity/neighborhood information. We observe that such neighborhood information is also affected by the reward structure.\nLet G = (E, V ) denote a graph with edge set E and the vertex set V . Let A = (Aij, i, j = 1, . . . , |V |, ) denote the adjacency matrix, with Aij = 1 when vertices i and j are connected, and Aij = 0 when i and j are not connected. We now define the following matrices\nA Adjacency Matrix D Diagonal matrix with entries\nas row sums of A L = D \u2212 A Combinatorial Laplacian\nL = D\u22121/2LD\u22121/2 Normalized Laplacian W = D\u22121A Random walk diffusion matrix\nIn a graphical representation where vertices are the states of a system, the adjacency matrix A can be treated as the measure of similarity and the eigen vectors can be chosen for the representation of the basis function. For instance, the spectral clustering technique uses eigen-value (spectrum) of the similarity matrix to perform dimensionality reduction. One of the well used methods is choosing the eigen vector corresponding to the second smallest eigen- value of the (symmetric)\nnormalized Laplacian. A spectrally similar and alternative way is to choose eigen vector corresponding to the highest eigen-values of the random walk diffusion matrix W = D\u22121A which represent the transition probability from a vertex to its neighbor vertex that is proportional to its degree. To see this note that I \u2212 L = D\u2212 12AD\u2212 12 = D\u22121A.\nA look at the following expression of the value function J\u03c0 throws light into why the diffusion matrix is helpful.\nJ\u03c0 = (I + \u03b1P\u03c0 + \u03b1 2P 2\u03c0 + . . .)R. Assuming that the transition matrix P\u03c0 is diagonalizable, i.e., P\u03c0 = \u03a6\u03c0(\u039b)\u03a6\u03c0> = \u2211n i=1 \u03bbi(\u03c6 \u03c0 i )\u03c6 \u03c0 i >, the above expansion then becomes\nJ\u03c0 = (I + \u03b1\u03a6\u03c0(\u039b)\u03a6\u03c0> + \u03b12\u03a6\u03c0(\u039b2)\u03a6\u03c0>)R\n= n\u2211 i=1\n1\n1\u2212 \u03b1\u03bb\u03c0i \u03c6\u03c0i \u03c6 \u03c0 i >R \u2248 k\u2211 i=1\n1\n1\u2212 \u03b1\u03bb\u03c0i \u03c6\u03c0i \u03c6 \u03c0 i >R\nwhere i = 1, . . . , k are such that \u03bb\u03c0i , i = 1, . . . , k are the largest eigen values. Thus the value function can be approximated as a linear combination of the eigen vectors corresponding to the largest eigen values of the transition matrix (since \u03b1 is fixed, 1\n1\u2212\u03b1\u03bb is higher for higher values of \u03bb). However, in the absence of knowledge of the transition matrix P\u03c0, one can make use of the diffusion matrix W obtained from the graph adjacency matrix.\nConsider the three-room problem [10], wherein, the agent has to move from the starting position S in the top-left side of the first room to the goal state G in the bottom-right of the third room. The task is particularly difficult because of the presence of the walls, which create a discontinuity, i.e., any representation based on primitive functions such as polynomial or radial would not account for the discontinuity. In [10], the authors demonstrated power of the proto-value functions in approximating such a complicated value function."}, {"heading": "4 Representational Policy Iteration (RPI)", "text": "The RPI algorithm [10] will be the template algorithm that we will be using for our experiments. Since the model information is not available, the LSTD algorithm learns it from the samples trajectories. We now present the LSPI algorithm (see Algorithm 1) which makes use of LSTDQ (see Algorithm 2), a variant of the LSTD algorithm).\nAlgorithm 1 Representational Policy Iteration (D, \u03b1,\u0398, k, \u03c00) 1: Choose feature matrix \u03a6 to be the top k eigen-vectors of \u0398. 2: for i = 0, 1, 2, . . . , t\u2212 1 do 3: Policy Evaluation Step: w\u03c0i = LSTDQ(D, \u03c0i) 4: Policy Improvement Step: Set \u03c0i+1(s) = arg max\na\u2208A (Q\u0302\u03c0i(s, a)), \u2200s \u2208 S.\n5: end for 6: Return \u03c0\u0398 \u2206 = \u03c0t.\nAlgorithm 2 LSTDQ(D, \u03c0i) 1: Initialize a policy A0 = 0, b0 = 0. 2: for i = 0, 1, 2, . . . , T do 3: A\u03c0i+1 = A \u03c0 i + \u03c6(si, ai)(\u03c6(si, ai)\u2212 \u03b1\u03c6(s\u2032i, \u03c0(s\u2032i))>.\n4: b\u03c0i+1 = b \u03c0 i + \u03c6(si, ai)rai(si+1). 5: end for 6: return w\u03c0 = (A\u03c0)\u22121b\u03c0.\nHere D is the sampled data, \u0398 is the matrix whose k eigen vectors are used as features, \u03c00 is the initial policy, t and T are integers which are chosen large enough to ensure convergence."}, {"heading": "5 Reward Shaping", "text": "The most fundamental difference between reinforcement learning (RL) tasks and supervised learning is that, in RL, the agent needs to learn using the feedback obtained in the form of the rewards it receives for its actions. Such learning via feedback makes RL tasks more challenging than supervised learning problems wherein the correct/right actions are provided to the learner in the training stage of the problem. This difficulty of learning from feedback is pronounced especially in the case of goal based tasks, wherein, the agent has to reach the goal-state from any part of the state space, however, the agent receives no reward at all in states other than the goal-state. Thus the behavior in the states other than the goal-state is not clear, which results in slower convergence of the RL algorithms. In such a scenario, rewarding the correct behavior of the agents in the intermediate states can be helpful. This mechanism of providing external rewards for right behavior in addition to the rewards obtained from the environment is called reward shaping. Reward shaping serves as indicator of the agent\u2019s progress, and can be seen as an improvement to the algorithmic part of the learning agent.\nReward-shaping was first introduced in [11], wherein, the authors furnished the conditions under which reward shaping preserves the optimal policies. In particular, it is known that reward shaping functions R\u2032 that are potential functions as well preserve the structure (see ??).\nTheorem 1 (Theorem 1 of [11]). Given an MDPM = {S,A, P,R} and a reward shaping function R\u2032 : S \u00d7 A \u00d7 S \u2192 R, the MDPM\u2032 = {S,A, P,R + R\u2032} has the same optimal policies asM iff\nR\u2032 is potential based, i.e.,\nR\u2032(s, a, s\u2032) = \u03b1\u03c8(s\u2032)\u2212 \u03c8(s), (2)\nfor some \u03c8 : S \u2192 R."}, {"heading": "6 Proto Value function shaping using rewards", "text": "The PVFs as well as reward shaping, though conceptually different, ultimately help in efficient value function learning. PVFs capture the underlying neighborhood information by making use of the connectivity in the graph associated with the MDP. However, in reality what we care about is not the nearness associated with the topological neighborhood in state space but the nearness of the value functions. Such nearness, we observe, is also affected by the underlying reward structure. Also, PVFs are constructed by sampling the state space, a phase during which we also get to observe the immediate rewards. While in the case of goal oriented tasks, the immediate rewards are 0, it might not hold true for MDPs with a general reward structure.\nImmediate rewards are indicators of agent\u2019s preference and actions locally. Consider for instance a goal-based MDP, however, with negative rewards for certain states. Given that the agent needs to move a step closer to the goal at each stage and that the states with negative rewards are equivalent to making additional steps, the agent\u2019s immediate action will be to prefer states that have the least negative reward amongst its immediate neighbors. Given the adjacency matrix A, it is then at state s, an intuitive model for the agent\u2019s actions can be\nwr(s, s \u2032) =\nexp\u03b2R(s \u2032)\u2211\ns\u223cs\u2032\u2032 exp \u03b2R(s\u2032\u2032)\n, (3)\nwhere \u03b2 > 0 is a positive constant that models affinity. In this paper, we construct the Reward based Proto-Value Functions (RPVFs) by looking at the n \u00d7 n diffusion matrix WR = (wr(s, s\u2032), s \u2208 S, s\u2032 \u2208 S) which combines the task-dependent rewards and the task-independent connectivity information."}, {"heading": "7 Experiments", "text": "We demonstrate the following via the experiments in this section. 1) Similarity matrices other than the diffusion matrix can be used to generate features: To this end, show that the Gaussian kernel matrix generated using the optimal value function as data points also yields good features. Further, we show that the proto-value functions of the three-room problem [10] can be recoverd even when the walls are absent if one assigns appropriate negative rewards for those cells corresponding to the \u2018wall\u2019 states. In short, we show that using W or WR (with negative rewards) is equivalent in this case. 2) Reward shaping does not work with all the features: We show that irrespective of whether additional reward shaping is used or not, the profile of the learnt value function is limited to the choice of the basis. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs. The Gaussian Kernel\nGiven a set {x1, . . . , xn} \u2282 Rd of n data points in d-dimensions, the n \u00d7 n Gaussian kernel matrix K = (K(i, j)) is given by\nK(xi, xj) = exp \u2212 ||xi\u2212xj || 2\u03c32 , (4)\nwhere \u03c3 > 0 is a positive scaling constant. Note that K is a similarity matrix which assigns the nearby states a higher value, a fact evident from (??). The spectral clustering technique involves computing the top k eigen-vectors of K to obtain a k-dimensional embedding {yi, i = 1 . . . , n} \u2282 Rk, where yi = (yi(1), . . . , yi(k)) \u2208 Rk with yi(j) being the ith component of the jth eigen-vector. In an MDP, we are interested in the set of data points {J\u2217(1), . . . , J\u2217(n)} \u2282 R and the kernel matrix with entries\nK(xi, xj) = exp \u2212 ||J \u2217(i)\u2212J\u2217(j)|| 2\u03c32 . (5)\nWe observe that the second eigen-vector of the kernel matrix in (??) is a close approximation to the optimal value function. The eigen-vectors of the graph Laplacian of the 3-room MDP and the graph Laplacian of the reward based diffusion matrix WR of a simple 21 \u00d7 60 grid which has negative rewards in the place of the wall are shown in ??.\nDoes Reward shaping work with any features? We now look at an instance of goal-based MDP (see ??). Here, the goal-state is in the right hand corner. The agent receives a reward of 10\non reaching the goal state and actions in the intermediate states do not receive any reward. The allowable actions are to move up, down, right or left. The state space for an N \u00d7 N grid (such as the one in ??) is given by S = {s = (x, y), x = 1, . . . , N, y = 1, . . . , N}, where (1, 1) denotes the bottom-left cell and (N,N) the top-right cell. It is evident that the learning process can be sped if the agent is rewarded for those actions that take it either up or right.\n\u03c8(x, y) > \u03c8(x\u2032, y,\u2032 ),\u2200x > x\u2032, y > y\u2032. (6)\nright table in ). Even in this case the profile of the learnt value function did not change, and the resulting policy performed only moderately. We ran the RPI algorithm with \u0398 = WR (with \u03b2 = 1) and the results are shown (second from bottom) in . In this case, the profile of the learnt value function resembles the optimal value function. Further, we also observed that the policy \u03c0WR returned by RPI in this case performed better than \u03c0W (with/without reward shaping), i.e.,\u2211\ns\u2208S J\u03c0WR (s) = 1660. The reason why the RPVFs perform better than the PVFs can be explained by looking at the\ncorresponding eigen functions.\nWe made use of the PVF based representation for the grid world problem in ??. The optimal value function is shown in the top most plot of ?? We chose k = 4, i.e., 4 eigen functions corresponding to 4 largest eigen values of the diffusion matrix P constructed from the adjacency matrix. We ran the RPI algorithm with \u0398 = W and the result is shown (second from top) in ??. Notice that the value function learnt by the RPI algorithm does not quite resemble the profile of the optimal value function and consequently resulted only in a moderately good policy. We evaluated the policy \u03c0W returned by RPI in this case (i.e., \u0398 = W ) and it turned out that \u2211 s\u2208S J\u03c0W (s) = 1132\nas opposed to \u2211\ns\u2208S J \u2217(s) = 1887. Further, we also ran the RPI, by retaining \u0398 = W , however\nprovided additional reward shaping feedback using the potential function \u03c8 (see The eigen functions corresponding to the first two largest eigen values were the same in both the cases. However, the third and fourth eigen functions differed (see ??). We can see from (bottom most plot) ??, that this difference in eigen function shows up in the difference in the profiles of the corresponding learnt value functions. We also compared the performance of PVFs and RPVFs in a\nvariant of the grid world problem, where, in addition to the goal-state, there are certain mine states with negative rewards. We chose these mine states at random and then compared the performances across 10 such different random grid world problems and for each problem we averaged the result across 10 initial policies for the RPI. We observed that in 9 out of the 10 systems, RPI with RPVF features (generated for \u03b2 = 0.1) significantly outperforms the policy learnt using the PVF."}, {"heading": "8 Conclusion", "text": "We combined the task-independent proto-value function (PVF) construction and the task-specific reward shaping to obtain Reward based Proto-Value Functions (RPVFs). The RPVF construction made use of the immediate rewards which were avaialble during the sampling phase but were not used in the PVF construction. We also observed that the RPVFs perform better than the PVFs in goal-based RL tasks. The salient feature of the RPVFs was that captured the asymmetry in the value function induced by the reward structure better than the PVFs. As an interesting future direction, we can look at extending RPVF to continous domains."}], "references": [{"title": "Dynamic Programming and Optimal Control, volume II", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, Belmont,MA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Least-squares temporal difference learning", "author": ["Justin A Boyan"], "venue": "In ICML, pages 49\u201356. Citeseer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Policy transfer using reward shaping", "author": ["Tim Brys", "Anna Harutyunyan", "Matthew E Taylor", "Ann Now\u00e9"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 181\u2013188. International Foundation for Autonomous Agents and Multiagent Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Multi-objectivization of reinforcement learning problems by reward shaping", "author": ["Tim Brys", "Anna Harutyunyan", "Peter Vrancx", "Matthew E Taylor", "Daniel Kudenko", "Ann Now\u00e9"], "venue": "In Neural Networks (IJCNN),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Plan-based reward shaping for reinforcement learning", "author": ["Marek Grzes", "Daniel Kudenko"], "venue": "In Intelligent Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Learning shaping rewards in model-based reinforcement learning", "author": ["Marek Grzes", "Daniel Kudenko"], "venue": "In Proc. AAMAS 2009 Workshop on Adaptive Learning Agents,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Autonomous shaping: Knowledge transfer in reinforcement learning", "author": ["George Konidaris", "Andrew Barto"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision Processes", "author": ["S.S. Mahadevan", "M. Maggioni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Proto-value functions: Developmental reinforcement learning", "author": ["Sridhar Mahadevan"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Markov Decision Processes: Discrete Stochastic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}], "referenceMentions": [{"referenceID": 8, "context": "The proto-value functions (PVFs) [9] are bases that can be constructed in task-independent manner, and have been applied to a wide variety of domains.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "[10] presented the representational policy iteration (RPI) algorithm by combining the PVF basis construction with least squares policy iteration (LSPI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 4, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 5, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 10, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 2, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 0, "context": "Even in the case when agent wants to improve a given policy \u03c0, it has to evaluate Q and then substituting Q in (??) will lead to an improved policy [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "1In the infinite horizon discounted reward setting that we consider, one can find an SDP that is optimal [1, 12]", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "1In the infinite horizon discounted reward setting that we consider, one can find an SDP that is optimal [1, 12]", "startOffset": 105, "endOffset": 112}, {"referenceID": 7, "context": "LSPI [8] makes use of least squares temporal difference learning (LSTD) [2] which computes Q\u0302 = k \u2211", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "LSPI [8] makes use of least squares temporal difference learning (LSTD) [2] which computes Q\u0302 = k \u2211", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "Consider the three-room problem [10], wherein, the agent has to move from the starting position S in the top-left side of the first room to the goal state G in the bottom-right of the third room.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "In [10], the authors demonstrated power of the proto-value functions in approximating such a complicated value function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "The RPI algorithm [10] will be the template algorithm that we will be using for our experiments.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Reward-shaping was first introduced in [11], wherein, the authors furnished the conditions under which reward shaping preserves the optimal policies.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Theorem 1 (Theorem 1 of [11]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "Further, we show that the proto-value functions of the three-room problem [10] can be recoverd even when the walls are absent if one assigns appropriate negative rewards for those cells corresponding to the \u2018wall\u2019 states.", "startOffset": 74, "endOffset": 78}], "year": 2015, "abstractText": "Learning value function is an important sub-problem in solving a given reinforcement learning task. The choice of representation for the value function directly affects learning. The most widely used representation for the value function is the linear architecture, wherein, the value function is written as a linear combination of a \u2018pre-selected\u2019 set of basis functions. In such a scenario, choosing the right basis function is crucial in achieving success. Often, the basis functions are either selected in an ad-hoc manner or their choice is based on the domain knowledge that is specific to the given RL task. However, it is desirable to be able to choose the basis functions in a task-independent manner. The proto-value functions (PVFs) are taskindependent basis functions and are based on the topology of the state space. Being eigen functions of the random walk operator, the proto-value functions capture the connectivity and neighborhood information. In contrast to supervised learning, agent performing an RL task needs to learn from the rewards. However, in goal-based RL tasks, the rewards are delayed, i.e., the agents receive feedback only after reaching the goal state and such delay can cause poor learning rates. Reward shaping is the mechanism of providing additional rewards for correct behavior in non-goal states, thereby aiding the learning process. In this paper, we combine task-dependent reward shaping and task-independent protovalue functions to obtain reward dependent proto-value functions (RPVFs). In constructing the RPVFs we are making use of the immediate rewards which are avaialble during the sampling phase but are not used in the PVF construction. We show via experiments that learning with an RPVF based representation is better than learning with just reward shaping or PVFs. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs.", "creator": "LaTeX with hyperref package"}}}