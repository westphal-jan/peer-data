{"id": "1511.08629", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Category Enhanced Word Embedding", "abstract": "Distributed word representations prove effective in capturing semantic and syntactic regularities. Unsupervised representations learn similar representations from large, unmarked corpora for words that exhibit similar co- event statistics. In addition to local incident statistics, global current information is also an important knowledge that can help distinguish one word from another. In this essay, we evaluate the learned word vectors using mood analysis and text classification tasks, demonstrating the superiority of our learned word vectors. Our models outperform several state-of-the-art models in word analogy and word similarity tasks. In addition, we evaluate the learned word vectors, demonstrating the superiority of our learned word vectors. We also learn high-quality category embedding that reflects current meanings.", "histories": [["v1", "Fri, 27 Nov 2015 11:38:57 GMT  (122kb,D)", "https://arxiv.org/abs/1511.08629v1", null], ["v2", "Mon, 30 Nov 2015 07:33:09 GMT  (122kb,D)", "http://arxiv.org/abs/1511.08629v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunting zhou", "chonglin sun", "zhiyuan liu", "francis c m lau"], "accepted": false, "id": "1511.08629"}, "pdf": {"name": "1511.08629.pdf", "metadata": {"source": "CRF", "title": "Category Enhanced Word Embedding", "authors": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Besides addressing the issue of dimensionality, word embedding also has the good property of generalization. Training word vectors from a large amount of data helps learn the intrinsic statistics of languages. A popular approach to training a statistical language\nmodel is to build a simple neural network architecture with an objective to maximize the probability of predicting a word given its context words. After the training has converged, words with similar meanings are projected into similar vector representations and linear regularities are preserved.\nDistributed word representation learning based on local context windows could only capture semantic and syntactic similarities through word neighborhoods. Recently, instead of purely unsupervised learning from large corpora, linguistic knowledge such as semantic and syntactic knowledge have been added to the training process. Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014). For example, Yu and Dredze (2014) incorporate relational knowledge in their neural network model to improve lexical semantic embeddings.\nTopical information is another kind of knowledge that appears to be also attractive for training more effective word embeddings. Liu et al (2015) leverage implicit topics generated by LDA to train topical word embeddings for multi-prototype vectors of each word. Co-occurrence of words within local context windows provides partial and basic statistical information between words; however, words in different documents with dissimilar topics may show different categorical properties. For example, \u201ccat\u201d and \u201ctiger\u201d are likely to occur under the same category of \u201cFelidae\u201d (from Wikipedia) but less likely to occur within the same context window. It is important for a word to know the categories of ar X\niv :1\n51 1.\n08 62\n9v 2\n[ cs\n.C L\n] 3\n0 N\nov 2\nits belonging documents when the neural network is trained on large corpora.\nIn this work, we propose to incorporate explicit document category knowledge as additional input information and also as auxiliary supervision. WikiData is a document-based corpus where each document is labeled with several categories. We leverage this corpus to train both word embeddings and category embeddings in a document-wise manner. Generally, we represent each category as a dense real-valued vector which has the same dimension as word embeddings in the model. We propose two models for integrating category knowledge, namely category enhanced word embedding (CeWE) and globally supervised category enhanced word embedding (GCeWE). In the wellknown CBOW (Mikolov et al., 2013a) architecture, each middle word is predicted by a context window, which is convenient for plugging category information into the context window when making predictions. In the CeWE model, we find that with local additional category knowledge, word embeddings outperform CBOW and GloVe (Pennington et al., 2014) significantly in word similarity tasks. In the GCeWE model, based on the above local reinforcement, we investigate predicting corresponding categories using words in a document after the document has been trained through a local-window model. Such auxiliary supervision can be viewed as a global constraint at the document level. We also demonstrate that by combining additional local information and global supervision, the learned word embeddings outperform CBOW and GloVe in the word analogy task (Mikolov et al., 2013a).\nOur main contribution is that we integrate explicit category information into the learning of word representation to train high-quality word embeddings. The resulting category embeddings also capture the semantic meanings of topics."}, {"heading": "2 Related Work", "text": "Word representation is a key component of many NLP and IR related tasks. The conventional representation for words known as \u201cbag-of-words\u201d (BOW) ignores the word order and suffers from high dimensionality, and reflects little relatedness and distance between words. Continuous word em-\nbedding was first proposed in (Rumelhart et al., 1988) and has become a successful representation method in many NLP applications including machine translation (Zou et al., 2013), parsing (Socher et al., 2011), named entity recognition (Passos et al., 2014), sentiment analysis (Glorot et al., 2011), partof-speech tagging (Collobert et al., 2011) and text classification (Le and Mikolov, 2014).\nMany prior works have explored how to learn effective word embeddings that can capture the words\u2019 intrinsic similarities and discriminations. Bengio et al. (2003) proposed to train an n-gram model using a neural network architecture with one hidden layer, and obtained good generalization. In (Mnih and Hinton, 2007), Minh and Hinton proposed three new probabilistic models in which they used binary hidden variables to control the connection between preceding words and the next word.\nThe methods mentioned above require high computational cost. To reduce the computational complexity, softmax models with hierarchical decomposition of probabilities (Mnih and Hinton, 2009; Morin and Bengio, 2005) have been proposed to speed up the training and recognition. More recently, Mikolov et al. (2013a; 2013b) proposed two models\u2014CBOW and Skip-Gram\u2014with highly efficient training methods to learn high-quality word representations; they adopted a negative sampling approach as an alternative to the hierarchical softmax. Another example that explored the cooccurrence statistics between words is GloVe (Pennington et al., 2014), which combines global matrix\nfactorization and local context window methods. The above models exploit word correlations within context windows; however, several recently proposed models explored how to integrate other sources of knowledge into word representation learning. For example, Qiu et al. (2014) incorporated morphological knowledge to help learn embeddings for rare and unknown words.\nIn this work, we design models to incorporate document category information into the learning of word embeddings where the objective is to correctly predict a word with not only context words but also its category knowledge. We show that word embeddings learned with document category knowledge have better performance in word similarity tasks and word analogical reasoning tasks. Besides, we also evaluate the learned word embeddings on text classification tasks and show the superiority of our models."}, {"heading": "3 Methods", "text": "In this section, we show two methods of integrating document category knowledge into the learning of word embeddings. First, we introduce the CeWE model where the context vector for predicting the middle word is enriched with document categories. Next, based on CeWE, we introduce the GCeWE model where word embeddings and category embeddings are jointly trained under a document-wise global supervision on words within a document."}, {"heading": "3.1 Category Enhanced Word Embedding", "text": "In this section, we present our method for training word embeddings and category embeddings jointly within local windows. We extend the CBOW (Mikolov et al., 2013a) architecture by incorporating category information of each document, to learn more comprehensive and enhanced word representations. The architecture of the CBOW model is shown in Figure 1 and its objective is to maximize the log probability of the current word t, given its context window s:\nJ(\u03b8) = V\u2211 t=1 \u2211 s\u2208context(t) log(p(t|s)) (1)\nwhere V is the size of the word vocabulary and context(t) is the set of observed context windows for the word t. CBOW basically defines the probability p(t|s) using the softmax function:\np(t|s) = exp(w \u2032T t vs)\u2211\nj\u2208V,j 6=t exp(w \u2032T j vs)\n(2)\nwhere w \u2032 t is the output word vector of word t. Meanwhile, each word t is maintained with an input word vector wt. And a context window vector vs is usually formulated as the average of context word vectors 12k \u2211 t\u2212k\u2264j\u2264t+k,j 6=twj , where k is the size of the window to the left and to the right. Mikolov (2013a; 2013b) also proposed some efficient techniques including hierarchical softmax and negative sampling to replace full softmax during the optimization process.\nContext window based models are prone to suffer from the lack of global information. Except for those frequently used words such as function words \u201che\u201d, \u201cwhat\u201d, etc., most words are used commonly under some certain language environment. For example, \u201crightwing\u201d and \u201canticommunist\u201d occur most likely under politically related topics; the football club \u201cMillwall\u201d occurs most likely under football related topics. To make semantically similar words behave more closely within the vector space, we propose to take advantage of the topic background in which the words lie during training. Different from the CBOW model, we plug in the category information to align word vectors under the\nsame topic more closely and linearly when predicting the middle word, which is as shown in Figure 2. To train this model, we create a continuous realvalued vector for each category. The dimension of the category vector is set to be the same as the word vector. Since the number of categories for each document is not fixed, we denote the last category vector in Figure 2 as cn. We train the CeWE model in a document-wise manner instead of taking the entire corpus as a sequence of words. In this way, we utilize the Wikipedia dumps which have associated each document with multiple categories. The creation of our dataset is described in details in Section 4.1.\nWe combine the average of the context window vector vs together with the weighted average of the category vectors to act as the new context vectors. Let ci denote the vector for the ith category, and category(m) the set of categories for the mth document. The new objective function is then:\nJ(\u03b8) = V\u2211 t=1 \u2211 s\u2208context(t) log(p(t|s, u)) (3)\nwhere the current context window s belongs to document m. The probability p(t|s, u) of observing the current word t given its context window s and document categories u is defined as follows:\np(t|s, u) = exp(w \u2032T t (vs + \u03bbzu))\u2211\nj\u2208V,j 6=t exp(w \u2032T j (vs + \u03bbzu))\n(4)\nwhere zu is the document category representation formulated as the average of category vectors 1 |category(m)| \u2211 i\u2208category(m) ci, and \u03bb is a hyperparameter to control the weight of the category vectors which play a role in predicting the middle word. We make use of negative sampling to optimize the objective function (3)."}, {"heading": "3.2 Globally Supervised CeWE", "text": "In the above model, we only integrate category information into local windows, enforcing inferred words to capture topical information and pulling word vectors under the same topic closer. However, an underlying assumption that can be easily seen is that the distribution of document representations should be in accordance with the distribution\nof categories. Thus, based on CeWE, we use the document representation to predict the corresponding categories as a global supervision on words, resulting in our GCeWE model."}, {"heading": "3.2.1 Model Description", "text": "The objective of GCeWE has two parts: the first one is the same as that of the CeWE model, and the other one is to maximize the log probability of observing document category i given a document m, as follows:\nJ(\u03b8) = V\u2211 t=1 \u2211 s\u2208context(t) log(p(t|s, u)) +\nM\u2211 m=1 \u2211 i\u2208category(m) log(p(i|m)) (5)\nSimilarly, p(i|m) is defined as:\np(i|m) = exp(c T i dm)\u2211\nj\u2208C,j 6=i exp(c T j dm)\n(6)\nwhere C is the size of all categories, dm denotes the document representation of the mth document and i \u2208 category(m).\nAnother problem to be solved is how to effectively represent a document to make the document representation discriminative. From experiments we find that with either average or TF-IDF weighted document representation that involves all words in a document, word embeddings trained by the GCeWE model shows little superiority in the word analogy task. We conjecture that the average operation makes the document representation less discriminative so that the negative sampling method could not sample informative negative categories, as we discuss below.\nIt has been shown that the TF-IDF value is a good measure of whether a word is closely related to the document topics. Therefore, before imposing the global supervision on the document representation, we first calculate the average TF-IDF value of all words in a document denoted as AVGT, and we select words that have a TF-IDF value larger than AVGT to participate in the global supervision. Instead of an average operation on these selected words, we use each of these words to predict the\ndocument categories separately. Thus, our new objective function becomes:\nJ(\u03b8) = V\u2211 t=1 \u2211 s\u2208context(t) log(p(t|s, u)) +\nM\u2211 m=1 \u2211 l\u2208Lm \u2211 i\u2208category(m) log(p(i|l)) (7)\nwhere Lm is the set of words selected from the mth document according to AVGT. The probability of observing a category i given a selected word l is defined similarly to Equation (6), as below:\np(i|l) = exp(c T i wl)\u2211\nj\u2208C,j 6=i exp(c T j wl)\n(8)"}, {"heading": "3.2.2 Optimization with Adaptive Negative Sampler", "text": "We also adopt the efficient negative sampling as in (Mikolov et al., 2013b) to maximize the second part of the objective function. For positive samples, we rely on the document representation to predict all categories of its belonging document. To select the most \u201crelevant\u201d negative category samples that could help accelerate the convergence, we employ the adaptive and context-dependent negative sampling proposed in (Rendle and Freudenthaler, 2014) for pairwise learning. Steffen and Freudenthaler\u2019s sampling method aims to sample the most informative negative samples for a given user and it works well in learning recommender systems where the target is to recommend the most relevant items for a user. It is analogous to selecting the most informative negative categories for a document. Note that the category popularity has a tailed distribution: only a small subset of categories have a high occurring frequency while the majority of categories do not occur very often at all. SGD algorithms with samples that have a tailed distribution may suffer from noninformative negative samples when using a uniform sampler. Noninformative samples have no contribution to the SGD algorithm, as shown in (Rendle and Freudenthaler, 2014), which slow down the convergence.\nWe employ the adaptive nonuniform sampler of (Rendle and Freudenthaler, 2014) by regarding each word as a context and each category as an item under\nthe matrix factorization (MF) framework. Elements of word vectors and category vectors can be viewed as a sequence of factors. According to a sampled factor of the document representation, we sample negative categories that should not approximate the document representation in the vector space.\nWe will show that with GCeWE the semantic word analogy accuracy is improved remarkably as compared with the CBOW model."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "WikiData is a document-oriented database, which is suitable for our training methodology. We extract document contents and categories from a 2014 Wikipedia dump. Each document is associated with several categories. As both the number of documents and that of categories are very large, we only reserve documents with category tags corresponding to the top 105 most frequently occurring categories. We note that there are many redundant meaningless category entries like \u201c1880 births\u201d, \u201c1789 deaths\u201d, etc., which usually consist of thousands of documents from different fields under one category. Although we cannot exclude all noisy categories, we eliminate a fraction of these categories by some rules, resulting in 86,664 categories and 2,271,411 documents. These categories occur in the entire dataset 152 times on average. We also remove all stop words in a predefined set from the corpus. Besides, in our experiment, we remove all the words that occur less than 20 times. Our final training data set has 0.87B tokens and a vocabulary of 533,112 words."}, {"heading": "4.2 Experiment Settings and Training Details", "text": "We employ stochastic gradient descent (SGD) for the optimization using four threads on a 3.6GHz Intel i7-4790 machine. We randomly select 100,000 documents as held-out data for tuning hyperparameters and use all documents for training. The dimension of word vectors is chosen to be 300 for all models in the experiment, and so the dimension of category vectors is also 300. 20 negative words are sampled in the negative sampling of CeWE and 20 negative categories are sampled in the adaptive negative sampling of GCeWE. Different learning rates\nare used when the category acts as additional input and the supervised target and are denoted \u03b1 and \u03b2 respectively. We set \u03b1 to be 0.02 and \u03b2 0.015. We also use subsampling of frequent words as proposed in (Mikolov et al., 2013b) with the parameter of 1e4. For the hyperparameter \u03bb, we set it to be 1/cw where cw is the number of words within a context window. To make a fair comparison, we train all models except GloVe for two epochs. In each epoch, the dataset is gone through once in its entirety.\nThe adaptive nonuniform negative sampling in the GCeWe model involves two sampling steps: one is to sample an importance factor f from all factors of a given word embedding and the other one is to sample a rank r from 300 factor dimensions. We draw a factor given a word embedding from p(f |w) \u221d |wf |\u03c3f wherewf is the f th factor of word vector w and \u03c3f is the standard deviation of factor f over all categories. A factor with a smaller rank over all factors has greater weights than other factors. To sample a smaller rank r, we draw r from a geometric distribution p(r) \u221d exp(\u2212r/\u03bb) which has a tailed distribution. And in our experiment, \u03bb = 5."}, {"heading": "4.3 Evaluation Methods", "text": "Word Similarity Tasks. The word similarity task is a basic method for evaluating word vectors. We evaluate the CeWE model on five datasets including WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al., 2013), and SCWS (Huang et al., 2012), which contain 353, 30, 65, 2003, 1762 word pairs respectively. We use SCWS to evaluate our word vectors without context information. In these datasets, each word pair\nis given a human labeled correlation score according to the similarity and relatedness of the word pair. We compute the spearman rank correlation between the similarity scores calculated based on word embeddings and human labeled scores. Word Analogy Task. The word analogy task was first introduced by Mikolov (2013a). It consists of analogical questions in the form of \u201ca is to b as b is to ?\u201d. The dataset contains two categories of questions: 8869 semantic questions and 10675 syntactic questions. There are five types of relationships in the semantic questions including capital and city, currency, city-in-state, man and woman. For example, \u201cbrother is to sister as grandson is to ?\u201d is a question for \u201cman and woman\u201d. And there are nine types of relationships in the syntactic questions including adjective to adverb, opposite, comparative, etc. For example, \u201ceasy is to easiest is lucky is to\n?\u201d is one question of \u201csuperlative\u201d. We answer such questions by finding the word whose word embedding wd has the maximum cosine distance to the vector \u201cwb \u2212 wa + wc\u201d. Sentiment Classification and Text Classification We evaluate the learned embeddings on two dataset: the IMDB (Maas et al., 2011) and 20NewsGroup 1. IMDB is a benchmark dataset for binary sentiment classification which contains 25K highly polar movie reviews for training and 25K movie reviews for testing. 20NewsGroup is a dataset of around 20000 documents organized into 20 different newsgroups. We use the \u201cbydate\u201d version of 20NewsGroup, which splits the dataset into 11314 and 7532 documents for training and testing respectively. We\n1http://qwone.com/\u02dcjason/20Newsgroups/.\nchoose LDA, TWE-1 (Liu et al., 2015), Skip-Gram, CBOW, and GloVe as baseline models. LDA represents each document as the inferred topic distribution. For Skip-Gram, CBOW, GloVe and our models, we simply represent each document by aggregating embeddings of words that have a TF-IDF value larger the AVGT and use them as document features to train a linear classifier with Liblinear (Fan et al., 2008). For TWE-1, the document embedding is represented by aggregating all topical word embeddings as described in (Liu et al., 2015), and the length of topical word embedding is double that of word embedding or topic embedding. We set the dimension of both word embedding and topic embedding in TWE-1 to be 300."}, {"heading": "4.4 Results and Analysis", "text": "For word similarity and word analogical reasoning tasks, we compare our models with CBOW, SkipGram and the state-of-the-art GloVe model. GloVe takes advantage of the global co-occurrence statistics with weighted least square. All models presented are trained using our dataset. For GloVe, we set the model hyperparameters as reported in the original paper, which have achieved the best performance. CBOW and Skip-Gram are trained using the word2vec tool 2. We first present our results on word similarity tasks in Table 1 where the CeWE model consistently achieves the best performance on all five datasets. This indicates that additional category information helps to learn high-quality word embeddings that capture more precisely the semantic meanings. We also find that as the window size increases, the CeWE model performs better for some similarity tasks. The reason probably is that when the window size becomes larger, more information of the context is added to the input vector, and the additional category information enhances the contextual meaning. However, the performance decreases as the window size exceeds 14.\nTable 2 presents the results of the word analogy task. The CeWE model performs better than the CBOW model with additional category information. By applying global supervision, the GCeWe model outperforms CeWE and GloVe in this task. We also observe that CeWE performs better in the\n2http://code.google.com/p/word2vec/\nword analogy task when using larger window size, but GCeWE model has a better performance when the window size is 10. So we only report the result of GCeWE with window size of 10. Also, we note that GCeWE performs worse compared to CeWE in word similarity tasks but better than CBOW and the Skip-Gram model, and so we only report the result of the CeWE model for the word similarity tasks.\nTable 3 presents the results of the tasks of sentiment classification and text classification, and it is evident that document representations computed by our learned word embeddings consistently outperform other baseline models. Although the documents are represented by discarding word orders, they still show good performance in the document classification tasks. This indicates that our models can learn high-quality word embeddings with category knowledge. Moreover, we can see that GCeWE performs better than CeWE on these two tasks."}, {"heading": "4.5 Qualitative Evaluation of Category", "text": "Embeddings\nTo show that our learned category embeddings capture the topical information, we randomly select 5 categories: supercomputers, IOS games, political terminology, animal anatomy, astronomy in the United Kingdom, and compute the top 10 nearest words for each of them. For a given category, we select words by comparing the cosine distance between the category embedding and all other words in the vocabulary. Table 1 in the supplementary material lists words that have a distance to the category embedding within the top 10 maximum distances. For example, given the category \u201cAnimal Anatomy\u201d, it returns the anatomical terminologies\nthat are highly related to animal anatomy. We also project the embeddings of categories and words described above to the 2-dimensional space using the t-SNE algorithm (Van der Maaten and Hinton, 2008), which is presented in Figure 1 in the supplementary material. It is shown that categories and corresponding neighbor words are projected into similar positions, forming five clusters. Besides, we compute the 5 nearest categories for the categories listed above respectively and we visualize it in Figure 3. As it can be seen, categories with similar topical meanings are projected into nearby positions."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented two models that integrate document category knowledge into the learning of word embeddings and demonstrate the ability of generalization of the learned word embeddings in several NLP tasks. For our future research work, we have plans to integrate refined category knowledge and remove redundant categories that may hinder the learning of word representations. We will also consider how to leverage the learned category embeddings in other NLP related tasks such as multi-label text classification."}], "references": [{"title": "Knowledge-powered deep learning for word embedding", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Workshop at International Conference on Learning Representation", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Charles."], "venue": "Language and cognitive processes, 6(1):1\u201328.", "citeRegEx": "Charles.,? 1991", "shortCiteRegEx": "Charles.", "year": 1991}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1404.5367", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations. COLING", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": null, "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Improving pairwise learning for item recommendation from implicit feedback", "author": ["Rendle", "Christoph Freudenthaler"], "venue": "In Proceedings of the 7th ACM international conference on Web search and data mining,", "citeRegEx": "Rendle et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Yoshua et al.2003] Bengio Yoshua", "Ducharme R\u00e9jean", "Vincent Pascal", "Jauvin Christian"], "venue": "Journal of Machine Learning Research(JMLR),", "citeRegEx": "Yoshua et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yoshua et al\\.", "year": 2003}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 284}, {"referenceID": 23, "context": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 284}, {"referenceID": 18, "context": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 284}, {"referenceID": 0, "context": "Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014).", "startOffset": 174, "endOffset": 193}, {"referenceID": 0, "context": "Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014). For example, Yu and Dredze (2014) incorporate relational knowledge in their neural network model to improve lexical semantic embeddings.", "startOffset": 175, "endOffset": 229}, {"referenceID": 0, "context": "Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014). For example, Yu and Dredze (2014) incorporate relational knowledge in their neural network model to improve lexical semantic embeddings. Topical information is another kind of knowledge that appears to be also attractive for training more effective word embeddings. Liu et al (2015) leverage implicit topics generated by LDA to train topical word embeddings for multi-prototype vectors of each word.", "startOffset": 175, "endOffset": 478}, {"referenceID": 18, "context": "In the CeWE model, we find that with local additional category knowledge, word embeddings outperform CBOW and GloVe (Pennington et al., 2014) significantly in word similarity tasks.", "startOffset": 116, "endOffset": 141}, {"referenceID": 22, "context": "bedding was first proposed in (Rumelhart et al., 1988) and has become a successful representation method in many NLP applications including machine translation (Zou et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 27, "context": ", 1988) and has become a successful representation method in many NLP applications including machine translation (Zou et al., 2013), parsing (Socher et al.", "startOffset": 113, "endOffset": 131}, {"referenceID": 23, "context": ", 2013), parsing (Socher et al., 2011), named entity recognition (Passos et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 17, "context": ", 2011), named entity recognition (Passos et al., 2014), sentiment analysis (Glorot et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": ", 2014), sentiment analysis (Glorot et al., 2011), partof-speech tagging (Collobert et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": ", 2011), partof-speech tagging (Collobert et al., 2011) and text classification (Le and Mikolov, 2014).", "startOffset": 31, "endOffset": 55}, {"referenceID": 18, "context": "Another example that explored the cooccurrence statistics between words is GloVe (Pennington et al., 2014), which combines global matrix", "startOffset": 81, "endOffset": 106}, {"referenceID": 1, "context": ", 2011), partof-speech tagging (Collobert et al., 2011) and text classification (Le and Mikolov, 2014). Many prior works have explored how to learn effective word embeddings that can capture the words\u2019 intrinsic similarities and discriminations. Bengio et al. (2003) proposed to train an n-gram model using a neural network architecture with one hidden layer, and obtained good generalization.", "startOffset": 32, "endOffset": 267}, {"referenceID": 19, "context": "For example, Qiu et al. (2014) incorporated morphological knowledge to help learn embeddings for rare and unknown words.", "startOffset": 13, "endOffset": 31}, {"referenceID": 4, "context": "We evaluate the CeWE model on five datasets including WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al.", "startOffset": 66, "endOffset": 92}, {"referenceID": 9, "context": ", 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al., 2013), and SCWS (Huang et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 6, "context": ", 2013), and SCWS (Huang et al., 2012), which contain 353, 30, 65, 2003, 1762 word pairs respectively.", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "We evaluate the CeWE model on five datasets including WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al., 2013), and SCWS (Huang et al., 2012), which contain 353, 30, 65, 2003, 1762 word pairs respectively. We use SCWS to evaluate our word vectors without context information. In these datasets, each word pair is given a human labeled correlation score according to the similarity and relatedness of the word pair. We compute the spearman rank correlation between the similarity scores calculated based on word embeddings and human labeled scores. Word Analogy Task. The word analogy task was first introduced by Mikolov (2013a). It consists of analogical questions in the form of \u201ca is to b as b is to ?\u201d.", "startOffset": 67, "endOffset": 705}, {"referenceID": 10, "context": "Sentiment Classification and Text Classification We evaluate the learned embeddings on two dataset: the IMDB (Maas et al., 2011) and 20NewsGroup 1.", "startOffset": 109, "endOffset": 128}, {"referenceID": 8, "context": "choose LDA, TWE-1 (Liu et al., 2015), Skip-Gram, CBOW, and GloVe as baseline models.", "startOffset": 18, "endOffset": 36}, {"referenceID": 3, "context": "For Skip-Gram, CBOW, GloVe and our models, we simply represent each document by aggregating embeddings of words that have a TF-IDF value larger the AVGT and use them as document features to train a linear classifier with Liblinear (Fan et al., 2008).", "startOffset": 231, "endOffset": 249}, {"referenceID": 8, "context": "For TWE-1, the document embedding is represented by aggregating all topical word embeddings as described in (Liu et al., 2015), and the length of topical word embedding is double that of word embedding or topic embedding.", "startOffset": 108, "endOffset": 126}, {"referenceID": 10, "context": "(Maas et al., 2011) and (Liu et al.", "startOffset": 0, "endOffset": 19}], "year": 2015, "abstractText": "Distributed word representations have been demonstrated to be effective in capturing semantic and syntactic regularities. Unsupervised representation learning from large unlabeled corpora can learn similar representations for those words that present similar cooccurrence statistics. Besides local occurrence statistics, global topical information is also important knowledge that may help discriminate a word from another. In this paper, we incorporate category information of documents in the learning of word representations and to learn the proposed models in a documentwise manner. Our models outperform several state-of-the-art models in word analogy and word similarity tasks. Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors. We also learn high-quality category embeddings that reflect topical meanings.", "creator": "LaTeX with hyperref package"}}}