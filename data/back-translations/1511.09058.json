{"id": "1511.09058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2015", "title": "Multiple-Instance Learning: Radon-Nikodym Approach to Distribution Regression Problem", "abstract": "For the distribution regression problem, where a bag of $x $--observations is mapped to a single $x $value, a one-step solution is proposed; the problem of random distribution is transformed into a random vector into a random value by taking the distribution moments of $x $observations in a bag as a random vector; then the radon nicodymium or least square theory can be applied, resulting in a $y (x) $estimator; the probability distribution of $y $is also obtained, which requires the solution of the general eigenvalue problem; the matrix spectrum (not dependent on $x $) gives possible $y $results; and depending on the $x $probabilities of the results, one can obtain the distribution by projecting a fixed $x $value (delta function) onto a corresponding eigenvector; a library is available that makes numerically stable polynomic basis for these proposed calculations, which provides the practical basis for the proposed calculations.", "histories": [["v1", "Sun, 29 Nov 2015 18:41:02 GMT  (166kb)", "https://arxiv.org/abs/1511.09058v1", null], ["v2", "Wed, 2 Dec 2015 11:02:01 GMT  (167kb)", "http://arxiv.org/abs/1511.09058v2", "Gramar fixes. Off by one error in eigenvalues problem fixed"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladislav gennadievich malyshkin"], "accepted": false, "id": "1511.09058"}, "pdf": {"name": "1511.09058.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ira Kudryashova"], "emails": ["malyshki@ton.ioffe.ru"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n09 05\n8v 2\n[ cs\n.L G\n] 2\nD ec\n2 01\n5"}, {"heading": "Multiple\u2013Instance Learning: Radon\u2013Nikodym Approach to", "text": ""}, {"heading": "Distribution Regression Problem.", "text": "Vladislav Gennadievich Malyshkin\u2217\nIoffe Institute, Politekhnicheskaya 26, St Petersburg, 194021, Russia\n(Dated: November, 27, 2015)\n$Id: DistReg1Step.tex,v 1.41 2015/12/02 11:00:50 mal Exp $\nFor distribution regression problem, where a bag of x\u2013observations is mapped to a\nsingle y value, a one\u2013step solution is proposed. The problem of random distribution\nto random value is transformed to random vector to random value by taking distri-\nbution moments of x observations in a bag as random vector. Then Radon\u2013Nikodym\nor least squares theory can be applied, what give y(x) estimator. The probability\ndistribution of y is also obtained, what requires solving generalized eigenvalues prob-\nlem, matrix spectrum (not depending on x) give possible y outcomes and depending\non x probabilities of outcomes can be obtained by projecting the distribution with\nfixed x value (delta\u2013function) to corresponding eigenvector. A library providing nu-\nmerically stable polynomial basis for these calculations is available, what make the\nproposed approach practical.\n\u2217 malyshki@ton.ioffe.ru\n2 Dedicated to Ira Kudryashova\nI. INTRODUCTION\nMultiple instance learning[1] is an important Machine Learning (ML) concept having numerous applications[2]. In multiple instance learning class label is associated not with a single observation, but with a \u201cbag\u201d of observations. A very close problem is distribution regression problem, where a sample distribution of x is mapped to a single y value. There are numerous heuristics methods developed from both: ML and distribution regression sides, see [3, 4] for review.\nAs in any ML problem the most important part is not so much the learning algorithm, but the way how the learned knowledge is represented. Learned knowledge is often represented as a set of propositional rules, regression function, Neural Network weights, etc. In this paper we consider the case where knowledge is represented as a function of distribution moments. Recent progress in numerical stability of high order moments calculation[5] allow the moments of very high order to be calculated, e.g. in Ref. [6] up to hundreds, thus make this approach practical.\nMost of distribution regression algorithms deploy a two\u2013step type of algorithm[4] to solve the problem. In our previous work [7] a two\u2013step solution with knowledge representation in a form of Christoffel function was developed. However, there is exist a one\u2013step solution to distribution regression problem, a random distribution to random value, that converts each bag\u2019s observations to moments of it, then solving the problem random vector (the moments of random distribution) to random value. Once this transition is made an answer of least squares or Radon\u2013Nikodym type from Ref. [5] can be applied and close form result obtained. The distribution of outcomes, if required, can be obtained by solving generalized eigenvalues problem, then matrix spectrum give possible y outcomes, and the square of projection of localized at given x bag distribution to eigenvector give each outcome probability. This matrix spectrum ideology is similar to the one we used in [7], but is more generic and not reducible to Gauss quadrature.\nThe paper is organized as following: In Section II a general theory of distribution regression is discussed and close form result or least squares and Radon\u2013Nikodym type are presented. Then in Section III an algorithm is described and numerical example of calcula-\n3 tions is presented. In Section IV possible further development is discussed."}, {"heading": "II. ONE\u2013STEP SOLUTION", "text": "Consider distribution regression problem where a bag of N observations of x is mapped\nto a single outcome observation y for l = [1..M ].\n(x1, x2, . . . , xj , . . . , xN) (l) \u2192 y(l) (1)\nA distribution regression problem can have a goal to estimate y, average of y, distribution of y, etc. given specific value of x\nFor further development we need x basis Qk(x) and some x and y measure. For simplicity, not reducing the generality of the approach, we are going to assume that x measure is a sum over j index \u2211N\nj=1, y measure is a \u2211M l=1, the basis functions Qk(x) are polynomials\nk = 0..dx \u2212 1, where dx is the number of elements in x basis, typical value for dx is below 10\u201315.\nLet us convert the problem \u201crandom distribution\u201d to \u201crandom variable\u201d to the problem \u201cvector of random variables\u201d to \u201crandom variable\u201d. The simplest way to obtain \u201cvector of random variables\u201d from x (l) j distributions is to take the moments of it. Now the < Qk > (l) would be this random vector:\n< Qk > (l)=\nN \u2211\nj=1\nQk(x (l) j ) (2)\n(\n< Q0 > (l), . . . , < Qdx\u22121 >\n(l) )\n\u2192 y(l) (3)\nThen the (3) becomes vector to value problem. Introduce\nYq = M \u2211\nl=1\ny(l) < Qq > (l) (4)\n(G)qr = M \u2211\nl=1\n< Qq > (l)< Qr > (l) (5)\n(yG)qr = M \u2211\nl=1\ny(l) < Qq > (l)< Qr > (l) (6)\nThe problem now is to estimate y (or distribution of y) given x distribution, now mapped to a vector of moments < Qk > calculated on this x distribution. Let us denote these input\n4 moments as Mk to avoid confusion with measures on x and y. For the case we study the x value is given, and for a state with exact x the Mk values are:\nMk(x) = NQk(x) (7)\nwhat means that all N observations in a bag give exactly the same x value. The problem now becomes a standard: random vector to random variable. We have solutions of two types for this problem, see [5] Appendix D, Least Squares ALS and Radon\u2013Nikodym ARN . The answers would be:\nALS(x) = dx\u22121 \u2211\nq,r=0\nMq(x) (G) \u22121 qr Yr (8)\nARN (x) =\ndx\u22121 \u2211\nq,r,s,t=0 Mq(x) (G)\n\u22121 qr (yG)rs (G) \u22121 st Mt(x)\ndx\u22121 \u2211 q,r=0 Mq(x) (G) \u22121 qr Mr(x)\n(9)\nThe (8) is least squares answer to y estimation given x. The (9) is Radon\u2013Nikodym answer to y estimation given x. These are the two y estimators at given x for distribution regression problem 1. These answers can be considered as an extension of least squares and Radon\u2013 Nikodym type of interpolation from value to value problem to random distribution to random variable problem. In case N = 1 the ALS and ARN are reduced exactly to value to value problem considered in Ref. [5]. Note, that the ALS(x) answer not necessary preserve y sign, but ARN (x) always preserve y sign, same as in value to value problem.\nIf y distribution at given x need to be estimated this problem can also be solved. With one\u2013step approach of this paper we do not need Qm(y) basis used in two\u2013step approach of Ref. [7] and outcomes of y are estimated from x moments only. Generalized eigenvalues problem[5] give the answer:\ndx\u22121 \u2211\nr=0\n(yG)qr \u03c8 (i) r = y\n(i) dx\u22121 \u2211\nr=0\n(G)qr \u03c8 (i) r (10)\nThe result of (10) is eigenvalues y(i) (possible outcomes) and eigenvectors \u03c8(i) (can be used to compute the probabilities of outcomes). The problem now becomes: given x value estimate possible y\u2013outcomes and their probabilities. The moments of states with given x value are NQq(x) from (7), so the distribution with (7) moments should be projected to distributions corresponding to \u03c8(i)q states, the square of this projection give the weight and normalized weight give the probability. This is actually very similar to ideology we used in [7], but the\n5 eigenvalues from (10) no longer have a meaning of Gauss quadrature nodes. The eigenvectors \u03c8(i)r correspond to distribution with moments < Qq >= \u2211dx\u22121 r=0 (G)qr \u03c8 (i) r , and the distribution with such moments correspond to y(i) value. These distributions can be considered as \u201cnatural distribution basis\u201d. This is an important generalizatioh of Refs. [5, 6] approach to random distribution, where natural basis for random value, not random distribution, was considered.\nThe projection of two x distributions with moments M (1) k and M (2) k on each other is\n< M (1)|M (2) >\u03c0 = dx\u22121 \u2211\nq,r=0\nM (1)q (G) \u22121 qr M (2) r (11)\nthen the required probabilities, calculated by projecting the (7) distribution to natural basis states, are:\nw(i)(x) =\n\n\ndx\u22121 \u2211\nr=0\nMr(x)\u03c8 (i) r\n\n\n2\n(12)\nP (i)(x) = w(i)(x)/ dx\u22121 \u2211\nr=0\nw(r)(x) (13)\nThe (10) and (13) is one\u2013step answer to distribution regression problem: find the outcomes y(i) and their probabilities P (i)(x). Note, that in this setup possible outcomes y(i) do not depend on x, and only probabilities P (i)(x) of outcomes depend on x. This is different from a two\u2013step solution of [7] where outcomes and their probabilities both depend on x. Also note that \u2211dx\u22121\nr=0 w (r)(x) = \u2211dx\u22121 q,r=0Mq(x) (G) \u22121 qr Mr(x).\nOne of the major difference between the probabilities (13) and probabilities from Christoffel function approach [7] is that the (13) has a meaning of \u201ctrue\u201d probability while in two\u2013step solution [7] Christoffel function value is used as a proxy to probability on first step. It is important to note how the knowledge is represented in these models. The model (8) has learned knowledge represented in dx by dx matrix (5) and dx size vector (4). The model (9) as well as distribution answer (13) has learned knowledge represented in two dx by dx matrices (5) and (6)."}, {"heading": "III. NUMERICAL ESTIMATION OF ONE\u2013STEP SOLUTION", "text": "Numerical instability similar to the one of two\u2013stage Christoffel function approach [7] also arise for approach in study, but now the situation is much less problematic, because we\n6 do not have y\u2013basis Qm(y), and all the dependence on y enter the answer through matrix (6). In this case the only stable x basis Qk(x) is required.\nThe algorithm for y estimators of (8) or (9) is this: Calculate < Qk > (l) moments from (2, then calculate matrices (5) and (6), if least squares approximation is required also calculate moments (4). In contrast with Christoffel function approach where < QqQr > ; q, r = [0..dx \u2212 1] matrix can be obtained from Qk; k = [0..2dx \u2212 1] moments by application of polynomials multiplication operator, here the (5) and (6) can be hardly obtained this way for N > 1 and should be calculated directly from sample. This is not a big issue, because dx is typically not large. Then inverse matrix (G)qr from (5), this matrix is some kind similar to Gramm matrix, but uses distribution moments, not basis functions. Finally put all these to (8) for least squares y(x) estimation or to (9) for Radon\u2013Nikodym y(x) estimation.\nIf y\u2013 distribution is required then solve generalized eigenvalues problem (10), obtain y(i) as possible y\u2013outcomes (they do not depend on x), and calculate x\u2013dependent probabilities (13), these are squared projection coefficient of a state with specific x value, point\u2013 distribution (7), or some other x distribution of general form, to \u03c8(i) eigenvector.\nTo show an application of this approach let us take several simple distribution to apply the theory. Let \u01eb be a uniformly distributed [\u22121; 1] random variable and take N = 1000 and M = 10000. Then consider sample distributions build as following 1) For l = [1..M ] take random x out of [\u22121; 1] interval. 2) Calculate y = f(x), take this y as y(l). 3) Build a bag of x observations as xj = x + R\u01eb; j = [1..N ], where R is a parameter. The following three f(x) functions for building sample distribution are used:\nf(x) = x (14) f(x) = 1\n1 + 25x2 (15)\nf(x) =\n  \n \n0 x \u2264 0 1 x > 0 (16)\nIn Figs. 1, 2, 3, the (8) and (9) answers are presented for f(x) from (14), (15) and (16) respectively for R = {0.1, 0.3} and dx = {10, 20}. The x range is specially taken slightly wider that [\u22121; 1] interval to see possible divergence outside of measure support. In most cases Radon\u2013Nikodym answer is superior, and in addition to that it preserves the sign of y. Least squares approximation is good for special case f(x) = x and typically diverges at x outside of measure support.\nThe numerical estimation of probability function (the y(i) and P (i)(x)) were also calculated and eigenvalue index i, corresponding to maximal P typically correspond to y(i), for which f(x) is most close. For simplistic case (14) see Fig. 4. See the Ref. [8], file com/polytechnik/ algorithms/ ExampleDistribution1Stage.scala for algorithm implementation."}, {"heading": "IV. DISCUSSION", "text": "In this work a one\u2013stage approach is applied to distribution regression problem. The bag\u2019s observations are initially converted to moments, then least squares or Radon\u2013Nikodym theory can be applied and closed form answer to be received. These (8) and (9) estimate y value given x. This answer can be generalized to \u201cwhat is y estimate given distribution of x\u201d. For this problem obtain moments < Qk >, corresponding to given distribution of x,\n9 first, then use them in (8) or (9) instead of Mk(x), corresponding to localized at x state. Similary, if probabilities of y outcomes are required for given distribution of x, the < Qk > should be used in weights expression (12) instead of Mk(x) (this is a special case of two distribution projection on each other (11)). Computer code implementing the algorithms is available[8].\nAnd in conclusion we want to discuss possible directions of future development.\n\u2022 In this work a closed form solution for random distribution to random value problem\n(1) is found. The question arise about problem order increase, replace \u201crandom distribution\u201d by \u201crandom distribution of random distribution\u201d (or even further \u201crandom distribution of random distribution of random distribution\u201d, etc.). In this case each xj in (1) should be treated as a sample distribution itself, and index j then can be treated as 2D index xj1,j2. Working with 2D indexes is actually very similar to working with images, see Ref. [6] where the 2D index was used for image reconstruction by applying Radon\u2013Nikodym or least squares approximation. Similarly, the results of this paper, can be generalized to higher order problems, by considering all indexes as 2D.\n\u2022 Obtaining possible y outcomes as matrix spectrum (10) and then calculating their\nprobabilities by projection (11) of given distribution (point distribution (7) is a simplest example of such) to eigenvectors (13) is a powerful approach to estimation of y distribution under given condition. We can expect this approach to show good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. The reason is because the (10) is expressed in terms of probability states, what make the role of outliers much less important, compared to methods based on L2 norm, particulary least squares. For example, this approach can be applied to distribtions where only first moment of y is finite, while the L2 norm approaches require second moment of y to be finite, what make them inapplicable to distributions with infinite standard deviation. We expect the (10) approach can be a good foundation for construction of Robust Statistics[9].\n[1] Thomas G Dietterich, Richard H Lathrop, and Toma\u0301s Lozano-Pe\u0301rez, \u201cSolving the multiple\ninstance problem with axis-parallel rectangles,\u201d Artificial intelligence 89, 31\u201371 (1997).\n10\n[2] Jun Yang, Review of multi-instance learning and its applications , Tech. Rep. (Tech. Rep,\n2005).\n[3] Zhi-Hua Zhou, \u201cMulti-Instance Learning: A Survey,\u201d Department of Computer Science & Technology, Nanjing\n[4] Zolta\u0301n Szabo\u0301, Arthur Gretton, Barnaba\u0301s Po\u0301czos, and Bharath Sriperumbudur, \u201cLearning\ntheory for distribution regression,\u201d arXiv preprint arXiv:1411.2066 (2014).\n[5] Vladislav Gennadievich Malyshkin and Ray Bakhramov, \u201cMathematical Foundations of Real-\ntime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines.\nhttp://arxiv.org/abs/1510.05510,\u201d ArXiv e-prints (2015), arXiv:1510.05510 [q-fin.CP].\n[6] Vladislav Gennadievich Malyshkin, \u201cRadon\u2013Nikodym approximation in applica-\ntion to image analysis. http://arxiv.org/abs/1511.01887,\u201d ArXiv e-prints (2015),\narXiv:1511.01887 [cs.CV].\n[7] Vladislav Gennadievich Malyshkin, \u201cMultiple\u2013Instance Learning: Christoffel Function Ap-\nproach to Distribution Regression Problem,\u201d ArXiv e-prints (2015), arXiv:1511.07085 [cs.LG].\n[8] Vladislav Gennadievich Malyshkin, (2014), the code for polynomials calculation,\nhttp://www.ioffe.ru/LNEPS/malyshkin/code.html.\n[9] Peter J Huber, Robust statistics (Springer, 2011)."}], "references": [{"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Thomas G Dietterich", "Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez"], "venue": "Artificial intelligence 89, 31\u201371 (1997).  10", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Review of multi-instance learning and its applications", "author": ["Jun Yang"], "venue": "Tech. Rep. (Tech. Rep,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Multi-Instance Learning: A Survey", "author": ["Zhi-Hua Zhou"], "venue": "Department of Computer Science & Technology, Nanjing", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "Learning theory for distribution regression", "author": ["Zolt\u00e1n Szab\u00f3", "Arthur Gretton", "Barnab\u00e1s P\u00f3czos", "Bharath Sriperumbudur"], "venue": "arXiv preprint arXiv:1411.2066 (2014).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Mathematical Foundations of Realtime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines. http://arxiv.org/abs/1510.05510", "author": ["Vladislav Gennadievich Malyshkin", "Ray Bakhramov"], "venue": "ArXiv e-prints (2015), arXiv:1510.05510 [q-fin.CP].", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Radon\u2013Nikodym approximation in application to image analysis. http://arxiv.org/abs/1511.01887", "author": ["Vladislav Gennadievich Malyshkin"], "venue": "ArXiv e-prints (2015), arXiv:1511.01887 [cs.CV].", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple\u2013Instance Learning: Christoffel Function Approach to Distribution Regression Problem", "author": ["Vladislav Gennadievich Malyshkin"], "venue": "ArXiv e-prints (2015), arXiv:1511.07085 [cs.LG].", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Multiple instance learning[1] is an important Machine Learning (ML) concept having numerous applications[2].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Multiple instance learning[1] is an important Machine Learning (ML) concept having numerous applications[2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "There are numerous heuristics methods developed from both: ML and distribution regression sides, see [3, 4] for review.", "startOffset": 101, "endOffset": 107}, {"referenceID": 3, "context": "There are numerous heuristics methods developed from both: ML and distribution regression sides, see [3, 4] for review.", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "Recent progress in numerical stability of high order moments calculation[5] allow the moments of very high order to be calculated, e.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "[6] up to hundreds, thus make this approach practical.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Most of distribution regression algorithms deploy a two\u2013step type of algorithm[4] to solve the problem.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "In our previous work [7] a two\u2013step solution with knowledge representation in a form of Christoffel function was developed.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "[5] can be applied and close form result obtained.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This matrix spectrum ideology is similar to the one we used in [7], but is more generic and not reducible to Gauss quadrature.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "We have solutions of two types for this problem, see [5] Appendix D, Least Squares ALS and Radon\u2013Nikodym ARN .", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] and outcomes of y are estimated from x moments only.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Generalized eigenvalues problem[5] give the answer: dx\u22121 \u2211", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "This is actually very similar to ideology we used in [7], but the", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "[5, 6] approach to random distribution, where natural basis for random value, not random distribution, was considered.", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[5, 6] approach to random distribution, where natural basis for random value, not random distribution, was considered.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "This is different from a two\u2013step solution of [7] where outcomes and their probabilities both depend on x.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "One of the major difference between the probabilities (13) and probabilities from Christoffel function approach [7] is that the (13) has a meaning of \u201ctrue\u201d probability while in two\u2013step solution [7] Christoffel function value is used as a proxy to probability on first step.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "One of the major difference between the probabilities (13) and probabilities from Christoffel function approach [7] is that the (13) has a meaning of \u201ctrue\u201d probability while in two\u2013step solution [7] Christoffel function value is used as a proxy to probability on first step.", "startOffset": 196, "endOffset": 199}, {"referenceID": 6, "context": "Numerical instability similar to the one of two\u2013stage Christoffel function approach [7] also arise for approach in study, but now the situation is much less problematic, because we", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "[6] where the 2D index was used for image reconstruction by applying Radon\u2013Nikodym or least squares approximation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] Thomas G Dietterich, Richard H Lathrop, and Tom\u00e1s Lozano-P\u00e9rez, \u201cSolving the multiple instance problem with axis-parallel rectangles,\u201d Artificial intelligence 89, 31\u201371 (1997).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "10 [2] Jun Yang, Review of multi-instance learning and its applications , Tech.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "[3] Zhi-Hua Zhou, \u201cMulti-Instance Learning: A Survey,\u201d Department of Computer Science & Technology, Nanjing [4] Zolt\u00e1n Szab\u00f3, Arthur Gretton, Barnab\u00e1s P\u00f3czos, and Bharath Sriperumbudur, \u201cLearning theory for distribution regression,\u201d arXiv preprint arXiv:1411.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[3] Zhi-Hua Zhou, \u201cMulti-Instance Learning: A Survey,\u201d Department of Computer Science & Technology, Nanjing [4] Zolt\u00e1n Szab\u00f3, Arthur Gretton, Barnab\u00e1s P\u00f3czos, and Bharath Sriperumbudur, \u201cLearning theory for distribution regression,\u201d arXiv preprint arXiv:1411.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "[5] Vladislav Gennadievich Malyshkin and Ray Bakhramov, \u201cMathematical Foundations of Realtime Equity Trading.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Vladislav Gennadievich Malyshkin, \u201cRadon\u2013Nikodym approximation in application to image analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Vladislav Gennadievich Malyshkin, \u201cMultiple\u2013Instance Learning: Christoffel Function Approach to Distribution Regression Problem,\u201d ArXiv e-prints (2015), arXiv:1511.", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "For distribution regression problem, where a bag of x\u2013observations is mapped to a single y value, a one\u2013step solution is proposed. The problem of random distribution to random value is transformed to random vector to random value by taking distribution moments of x observations in a bag as random vector. Then Radon\u2013Nikodym or least squares theory can be applied, what give y(x) estimator. The probability distribution of y is also obtained, what requires solving generalized eigenvalues problem, matrix spectrum (not depending on x) give possible y outcomes and depending on x probabilities of outcomes can be obtained by projecting the distribution with fixed x value (delta\u2013function) to corresponding eigenvector. A library providing numerically stable polynomial basis for these calculations is available, what make the proposed approach practical.", "creator": "gnuplot 4.4 patchlevel 4"}}}