{"id": "1511.09128", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "Aspect-based Opinion Summarization with Convolutional Neural Networks", "abstract": "To enable real applications, an AOS system must address two central sub-tasks: aspect extraction and sentiment classification. Most existing approaches to aspect extraction that use linguistic analysis or topic modeling are generic for different products, but not precise enough or suitable for specific products. Instead, we take a less generic but more precise scheme, mapping each review set directly into predefined aspects. To address aspect extraction and sentiment classification, we propose two methods based on Convolutional Neural Network (CNN), cascaded CNN and multitask CNN. Cascaded CNN contains two levels of Convolutionary Networks. Several Level 1 CNNs deal with aspect mapping, and a single CNN on Level 2 deals with sentiment classification. Multitask CNN also contains several aspects of CNNs and one CNN sentiment, but different networks share the same word embeddings. Experimental results indicate that both multifunctional CNN and SVM functioned better in general.", "histories": [["v1", "Mon, 30 Nov 2015 01:46:15 GMT  (287kb)", "http://arxiv.org/abs/1511.09128v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["haibing wu", "yiwei gu", "shangdi sun", "xiaodong gu"], "accepted": false, "id": "1511.09128"}, "pdf": {"name": "1511.09128.pdf", "metadata": {"source": "CRF", "title": "Aspect-based Opinion Summarization with Convolutional Neural Networks", "authors": ["Haibing Wu", "Yiwei Gu", "Shangdi Sun", "Xiaodong Gu"], "emails": ["xdgu}@fudan.edu.cn"], "sections": [{"heading": null, "text": "This paper considers Aspect-based Opinion Summarization (AOS) of reviews on particular products. To enable real applications, an AOS system needs to address two core subtasks, aspect extraction and sentiment classification. Most existing approaches to aspect extraction, which use linguistic analysis or topic modelling, are general across different products but not precise enough or suitable for particular products. Instead we take a less general but more precise scheme, directly mapping each review sentence into pre-defined aspects. To tackle aspect mapping and sentiment classification, we propose two Convolutional Neural Network (CNN) based methods, cascaded CNN and multitask CNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs at level 1 deal with aspect mapping task, and a single CNN at level 2 deals with sentiment classification. Multitask CNN also contains multiple aspect CNNs and a sentiment CNN, but different networks share the same word embeddings. Experimental results indicate that both cascaded and multitask CNNs outperform SVM-based methods by large margins. Multitask CNN generally performs better than cascaded CNN."}, {"heading": "1 Introduction", "text": "User generated reviews on products are expanding rapidly with the emergence and advancement of e-commerce. These reviews are valuable to business organizations for improving their products and to individual consumers for making informed decisions. Unfortunately, reading though all the product reviews is hard, especially for popular products with large volume of review texts. It is t-\nherefore essential to provide coherent and concise summaries of user generated reviews. This has bred a new line of research on Aspect-based Opinion Summarization (AOS) (Hu and Liu, 2004). Given a set of product reviews, an AOS system extracts aspects discussed in the reviews and predicts reviewers\u2019 sentiments toward these aspects. Figure 1 presents an example summary of smartphone reviews. The smartphone aspects, such as battery life and screen, with the hyperlinks and numbers of positive and negative opinions, are illustrated in a structured way.\nStandard AOS typically involves two component subtasks, aspect extraction and sentiment classification. Aspect extraction finds related aspects and extracts all textual mentions associated with each aspect. Sentiment classification task classifies sentiment over each aspect using the associated textual mentions.\nExisting researches on aspect extraction move along two quite different lines. The first extracts aspect expressions using linguistic patterns or supervised sequence labeling (see Section 2). This scheme is very limited for only identifying explicit aspects and failing to handle implicit aspects. Besides, it needs additional efforts to group synonymous aspect expressions into the same category. The second is based on topic modeling (see Section 2). Topic modeling is fully unsupervised, saving the labeling of training data. It handles implicit aspects well, and simultaneously extracts\nand groups aspects. It is, however, not suitable for summarizing reviews on particular products in many respects. The unsupervised nature makes it more general across different products, but less precise for particular products compared to supervised learning methods. The learned topics of topic modeling are implicit and often do not correlate well with human judgments, making it not applicable if users care about some particular product aspects. Topic modeling categorizes aspects, but its unsupervised nature makes the grouping not controllable or adaptable. Categorizing aspects is subjective because for different applications the user may need different categorizations. For example, in smartphone reviews, front camera and back camera can be regarded as two separate aspects, but can also be one general aspect, camera.\nFor some vertical e-commerce websites that focus on particular products, users already know what aspects a product has. Ontologies negates the need for identifying aspects automatically. Herein the most pressing challenge is to extract all relevant text mentions for each aspect. Therefore, this paper takes a line different from prior work on aspect extraction: directly mapping each review sentence into pre-defined aspect categories. That is, we formulate aspect extraction as sentence-level aspect mapping (or classification) problem. This scheme extracts relevant text mentions for pre-defined aspects and enjoys a lot of advantages. It handles both explicit and implicit aspects, and simultaneously extracts and categorizes different aspect expressions into the same aspect category. It also enables users to design different aspect categories for different application purposes.\nBesides aspect extraction, sentiment classification is also necessary to enable real applications. This paper presents an aspect-based summary system which addresses both tasks. Most previous work on AOS deals with a single task, either aspect extraction or sentiment classification, using traditional machine learning. Motivated by the recent success of deep Convolutional Neural Network (CNN) and multi-task representation learning, we propose two CNN-based approaches to jointly tackle aspect mapping and sentiment classification problems. The first one is a two-level Cascaded CNN (C-CNN). At level 1, multiple convolutional networks map the input sentences into pre-defined aspects. At level 2, a single convolutional network predicts the sentiment polarities of the input sentences. The second is a Multi-\ntask CNN (M-CNN). Different from C-CNN, aspect mapping and sentiment classification tasks share the word embedding representation in MTCNN, making the learned word embedding universal across tasks. This reduces over-fitting to a specific task and thus profits generalization to held-out test data. Empirical results show that both C-CNN and M-CNN with pre-trained word embedding representation outperform linear classifiers with bag-of-words representation by large margins. M-CNN performs better than C-CNN, despite not showing significant superiority."}, {"heading": "2 Related Work", "text": "AOS has attracted a lot of attentions with the advent of online user generated reviews. Deep learning and representation learning, initially enjoying great success in computer vision, have also achieved some success in Natural Language Processing (NLP).\nAn AOS system needs to address two core tasks,\naspect extraction or sentiment classification. One line of work on aspect extraction detects aspect expressions using linguistic patterns (e.g. part-ofspeech and dependency relations) (Hu and Liu, 2004; Joshi and Rose, 2009; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al., 2007; Yang and Cardie, 2012). This scheme is very limited in many respects. It only extracts explicit aspect expressions, and cannot deal with implicit aspects well. For example, in the sentence \u201cthis phone runs smoothly and fast, but its battery life is very poor\u201d, battery life is explicitly mentioned, while running speed is implicitly mentioned and thus cannot directly discovered using linguistic patterns or sequence labeling. This scheme is also limited for not grouping aspect expressions into aspect categories. For example, screen, display and retina refer to the same aspect for iPhone. After extracting all aspect expressions, additional efforts are required to categorize domain synonyms into the same aspect.\nAnother line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013). Topic modeling deals with implicit aspects to some degree, and simulta-\nneously extracts and groups aspects. However, it often learns incoherent topics since its objective functions do not always correlate well with human judgments. Compared with supervised methods, unsupervised topic modeling is more general across different products, but less precise for particular products. In addition, mapping from topics to aspects is not explicit, making it not a good choice if users care about opinions on some particular aspects. Topic modeling categories aspects based on co-occurrence counts. However, categorizing aspects is subjective because for different applications the user may need different categorizations. For example, in smartphone reviews, front camera and back camera can be treated as two different aspects, but can also be only one aspect. The unsupervised nature of topic modeling makes the grouping not controllable or adaptable.\nAn AOS system also involves sentiment classification. This task aims to classify an opinionated review as expressing positive or negative sentiment over an aspect. Compared to aspect extraction, sentiment classification was studied earlier and more extensively. Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014). Very recently, some researchers applied deep convolutional neural networks to sentence sentiment classification and reported considerably better results than traditional approaches (Kalchbrenner et al., 2014; Santos and Gatti, 2014; Kim, 2014).\nConvolutional neural network (CNN) is currently underpinning the cutting edge in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2014). It has also achieved state-of-the-art results in many traditional NLP tasks (Collobert et al.,\n2011) and other NLP areas such as information retrieval (Shen et al., 2014) and relation classification (Zeng et al., 2014; Santos et al., 2015). Words are encoded as low-dimensional word vectors in CNN, instead of high dimensional one-hot representations. Word vector representations capture semantic information, so semantically close words are likewise close in low dimensional vector space. CNN models for specific NLP tasks often use unsupervised pre-trained word vectors (Mikolov et al., 2013) as initialization, which are then improved by optimizing supervised objectives.\nMulti-task learning learns shared representation for multiple tasks. In NLP, the marriage between multi-task learning and neural networks is quite natural as different NLP tasks could share word embeddings. For example, (Collobert et al., 2011) tackled part-of-speech tagging, chunking, and named entity recognition tasks using a multitask sequence labeller. (Liu et al., 2015) trained a multi-task deep neural network for query classification and web search ranking."}, {"heading": "3 Methodology", "text": "An architectural overview of our aspect-based summarization system is given in figure 2. The input to the system is a set of crawled reviews for a particular product. The sentence segmenter divides review texts into a set of sentences. The aspect mapper maps these sentences into pre-defined aspect categories. In this step only sentences belonging to the pre-defined aspects are extracted and retained. The sentiment classifier then predicts the polarity of each of these extracted sentences as positive or negative. After labelling each sentence with aspect and sentiment, the final opinion aggregator counts the number of positive and negative opinionated sentences corresponding to\nCrawled\nSentences Sentences with aspect labelled Sentences with sentiment labelled\neach aspect, and gives the hyperlinks to these sentences.\nThe sentence segmenter in our system is an offthe-shelf segmentation tool, NLTK PunktSentenceTokenizer.1 The aspect mapper and sentiment classifier we use is a C-CNN or M-CNN. For each input sentence, the network first maps it into corresponding aspects. If the sentence belongs to one or more pre-defined aspect categories, the network then predicts its sentiment polarity."}, {"heading": "3.1 Cascaded CNN", "text": "The architecture of our C-CNN is shown is figure 3. The network contains C CNN aspect mappers and a CNN sentiment classifier. Aspect-mapping CNN and sentiment-classification CNN are organized in a cascaded way. Each mapper determines whether the input sentence belongs to its corresponding aspect. If that is the case, the sentiment classifier predicts sentiment polarity as positive or negative.\nBefore diving into details about CNN layers, we address two considerations about the cascaded network. (1) The network only contains one sentiment classifier. One may think it is problematic as a single sentence (e.g. \u201cThis phone runs fast, but does not loses its charge too quickly!\u201d) could contain different aspects, and sentiments towards these aspects could be opposite. We do not train a separate sentiment classifier for each aspect category since in practice only a few sentences imply opposite sentiments for different aspects. (2) The sentiment classifier only deals with sentences be-\n1http://www.nltk.org/api/nltk.tokenize.html\nlonging to at least one pre-defined aspects as practical applications only care the sentiments of aspect related sentences. In addition, sentences not belonging to any pre-defined aspect could be objective. It is not suitable to classifying the sentiments of objective sentences as positive or negative.\nEach CNN contains a word embedding layer, a convolutional and pooling layer, and a fully-connected layer.\nWord embedding. This layer encodes each word in the input sentence as a word vector. Let Rl  be the sentence length, RD  be the vocabulary size and Dk RW  )1( be the embedding matrix of k-dimensional word vectors. The i-th word in a sentence is transformed into a k-dimensional vector wi by matrix-vector product:\nii W xw )1( . (1)\nHere xi is the one-hot representation for the i-th word.\nConvolution. After encoding the input sentence with word vectors, the convolution operations are applied on top of these vectors to produce new features. A convolution operation involves a filter hkRu applied to a window of 12  rh words. For example, a feature if is produced from a window of words riri  :w by\n)( : uw   ririi gf . (2)\nHere g denotes a non-linear activation function. This filter is applied to every possible windows of the input sentence to generate a feature map2\n2Each sentence is augmented with r \u201cpadding\u201d words respectively at the beginning and the end, so each word in a sentence corresponds to a convolution window.\n]...,,,[ 21 lffff . (3)\nThe above describes the process that one feature map is extracted from one filter. The network uses im (i = 1, 2, \u2026, C) filters to generate im feature maps for the i-th aspect mapper and\n1Cm filters for the sentiment classifier. The filter weights for i-th aspect mapper are stored in a imhk  -dimensional matrix imhk i RW  )2( . For sentiment classifier, 2)2( 1 mhk C RW    .\nPooling. This layer applies max-over-time pooling (Collobert et al. 2011) to each of the feature maps produced by convolutional layers:\n)...,,,max(\u02c6 21 lffff  . (4)\nMax-over-time pooling takes the maximum element in each feature map and naturally deals with variable sentence lengths. It produces a fixedsized feature vector im i Rv for the i-th task.\nFully-connection. The fixed-sized feature vectors produced by pooling layers are fed into fullyconnected layers. Concretely, iv is passed to a binary logistic regression classifier.\n1...,,2,1,)1(1 )3(\n1   Ciea ii W v . (5)\nHere imn i RW  )3( is the weight matrix for i-th task, and ia is the aspect output vector. For aspect mapper, ia (i = 1, 2, \u2026, C) is the probability of the input sentence belonging to the i-th aspect category; for sentiment classifier )1(  Ciai is the positive-sentiment probability."}, {"heading": "3.2 Multitask CNN", "text": "The architecture of our designed M-CNN is shown is figure 4. M-CNN also contains C aspect mappers and a sentiment classifier. But different\nto C-CNN, aspect mappers and sentiment classifier share word embedding layer in M-CNN. So the word embedding parameter )1(W is shared across different tasks, whereas other parameters, i.e. )2( iW and )3( iW (i = 1, 2, \u2026, C + 1), are task specific.\nConventional multitask learning optimizes model parameters ),,( )3()2()1( WWW by minimizing the loss functions of all tasks. This gives slightly worse results in our experiments. Instead we sequentially set task i as the main task, and set the other tasks as auxiliary tasks. The aim is to optimize the main task, with the assistance of auxiliary tasks. To this end, we formulate the optimization objective as\n   ij jjiJ   )( , (6)\nwhere i is loss function of the main task, and j is the loss function of each auxiliary task. The parameter j ( 10  j ) denotes the importance of j-th task. Logistic loss is used as loss functions for both aspect mapping and sentiment classification tasks.\nIn order to optimize )(J we use mini-batch stochastic gradient descent as shown in Algorithm 1. The algorithm sequentially set task i as the main task, set and other tasks as auxiliary tasks. It then updates )1(W , )2( iW and )3( iW for the main task and all auxiliary tasks using gradient descent. After training parameters for T epochs, the learned model is test on the held-out test sentences for task i.\nNote that if a task is sentiment classification, gradients are only computed over sentences belo-\nAlgorithm 1: Training and testing our M-CNN for i = 1 to C + 1\n1. set task i as the main task, and set other tasks as auxiliary tasks 2. repeat step 3-10 for T epochs 3. permute training sentences randomly and partition them into mini-batches\n4. for each mini-batch 5. compute gradients of )(J w.r.t. )1(W , )2(iW and )3( iW for the main task 6. update )1(W , )2( iW and )3( iW using gradient descent 7. compute gradients of )(J w.r.t. )1(W , )2( jW and )3( jW ( ij  ) for all auxiliary tasks 9. update )1(W , )2( jW and )3( jW ( ij  ) using gradient descent 10. end\n11. test the learned model on the test sentences for task i\nend\nging to at least one aspect category because sentences not belonging to any pre-defined aspect are filtered out."}, {"heading": "4 Experiments", "text": "Dataset. To train our C-CNN and M-CNN, we need a collection of sentences labeled with aspects and sentiments. As there is no such benchmark corpus, we create an Amazon Smartphone Review (ASR) dataset and will make it publicly available for research purpose. ASR contains 300,000 smartphone reviews crawled from amazon.com. We manually annotate 12,700 sentences of 1679 reviews with respect to five pre-defined aspects, {battery, screen, camera, speaker, running speed}. Sentences belonging to at least one aspect are also labeled as expressing positive or negative sentiment. The number of sentences belonging to each aspect is shown in table 1.\nEvaluation metrics. We use precision, recall and F1-score for performance evaluation of aspect mapping, and classification accuracy for sentiment classification. All comparisons are done using 5-fold cross validation. That is, the overall results are averaged over five folds.\nBaselines. The baselines exploit Support Vector Machines (SVMs) as classifiers. Specially, we adopt the L2-regularized L2-loss linear SVM and the implementation software is scikit-learn.3 Multiple SVMs are cascaded in the way like C-CNN. One-hot representation of each word (or term) is employed as feature for training SVMs. The weight of each word in the one-hot representation is simply assigned term presence (tp), i.e. 1 for presence and 0 for absence. The most commonly used weighting scheme, term frequency (tf), is not used as it produce very close results to tp. The reason may be that in our experiments most words in a sentence only occur one time, so weights assigned by tp and tf are almost the same with each other.\nNetwork settings. We use rectified linear units (Nair and Hinton, 2010) as activation functions for convolutional layer, and sigmoid function for output layer. Network models are trained using stochastic mini-batch gradient descent with batch size of 1000, momentum of 0.9, learning rate of 0.5. The weights in all layers are initialized from a zero-mean Gaussian distribution with 0.1 as sta ndard deviation and the constant 0 as the neuron biases. For M-CNN, the parameter j is manually chosen according to the performance on development set. The settings of hyper-parameters on the architectures of C-CNN and M-CNN are shown in table 2.\nword2vec (w2v). Besides random initialization,\nwe also pre-train word embeddings using word2vec tool,4 which implements continuous bag-of-words and skip-gram architectures for learning word vector representations (Mikolov et\n3scikit-learn.org/ 4code.google.com/p/word2vec/\nal., 2013). We train skip-gram model with context window size of 9 on corpus of December 2013 English Wikipedia."}, {"heading": "4.1 Results for Aspect Mapping", "text": "Table 3 presents the results of our CNN-based methods against SVM-based methods for aspect mapping. Clearly, our C-CNN and M-CNN with randomly initialized word embeddings perform better than SVM with tp for all five aspects. For example, comparing C-CNN to SVM, the increases of F1-score are respectively 2.7% (71.7% vs. 74.4%), 1.5% (68.1% vs. 69.6%), 1.7% (72.1 % vs. 73.8%), 2.4 % (73.2% 75.6%) and 0.5% (79.4% vs. 79.9%). In terms of precision, CNN methods also beat SVMs with large margins, while their recall performances are close.\nM-CNN generally outperforms C-CNN, but does not show significant superiority. For the aspects battery, running speed, speaker and camera, M-CNN produces higher F1-score than C-CNN, but for screen M-CNN slightly underperforms CCNN (75.3% vs. 75.6%). Overall, the performance between M-CNN and C-CNN is close to each other, with the former performing slightly better than the latter.\nPre-training word embeddings using word2vec provides significant gains of precision, recall and F1-score for C-CNN and M-CNN on all aspect mapping tasks. The improvement of F1-score ranges from +0.7% (75.6% vs. 76.3%) to +1.7% (74.4% vs. 76.1%). We also observe the evidence of the benefits of multitask learning after using word2vec as word embedding initializer. In terms of F1-score, M-CNN+w2v outperforms CCNN+w2v on 4 of 5 aspects."}, {"heading": "4.2 Results for Sentiment Classification", "text": "Table 4 presents the classification accuracy of our CNN-based methods against SVM-based methods for sentiment classification task. Different\nmethods\u2019 performances for this task are generally in accordance to their performances for aspect mapping tasks. SVM with tp produces an accuracy of 80.3%. Again, both C-CNN and M-CNN outperform SVM+tp by large margins. The accuracy differences are respectively +2.1% (80.3 % vs. 82.4%) and 2.6% (80.3% vs. 82.9%). CCNN+w2v and M-CNN+w2v achieve accuracy of 83.5% (vs. 82.4% by C-CNN) and 84.1% (vs. 82.9% by M-CNN) respectively, implying the power of unsupervised pre-training of word embeddings. M-CNN+w2v achieves the highest accuracy among all evaluated methods."}, {"heading": "5 Conclusions", "text": "In this paper we have presented an aspect-based opinion summarization system for particular products. Our system directly maps each review sentence into pre-defined aspects. This is particularly suitable for some vertical e-commerce websites that only sell particular products, or if users\nonly care about opinions on particular product aspects. To attack aspect mapping and sentiment classification tasks, we have proposed two convolutional network based approaches, C-CNN and M-CNN. Both C-CNN and M-CNN contains multiple aspect mappers and a single sentiment classifier. The difference is that in M-CNN, word embeddings are shared across different tasks. Empirical results imply the superiority of our CNNbased methods over SVM-based methods; MCNN tends to perform better than C-CNN, despite not showing significant superiority."}, {"heading": "Acknowledgements", "text": "This work was supported in part by National Natural Science Foundation of China under grant 61371148 and Shanghai National Natural Science Foundation under grant 12ZR1402500."}], "references": [{"title": "Sentiment classification based on supervised latent n-gram analysis", "author": ["Dmitriy Bespalov", "Bing Bai", "Yanjun Qi", "Ali Shokoufandeh."], "venue": "Proceedings of CIKM 2011.", "citeRegEx": "Bespalov et al\\.,? 2011", "shortCiteRegEx": "Bespalov et al\\.", "year": 2011}, {"title": "Identifying expressions of opinion in context", "author": ["Eric Breck", "Yejin Choi", "Claire Cardie."], "venue": "Proceedings of IJCAI 2007.", "citeRegEx": "Breck et al\\.,? 2007", "shortCiteRegEx": "Breck et al\\.", "year": 2007}, {"title": "An unsupervised aspect-sentiment model for online reviews", "author": ["Samuel Brody", "Noemie Elhadad."], "venue": "Proceedings of NAACL 2010.", "citeRegEx": "Brody and Elhadad.,? 2010", "shortCiteRegEx": "Brody and Elhadad.", "year": 2010}, {"title": "Aspect extraction with automated prior knowledge learning", "author": ["Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu."], "venue": "Proceedings of ACL 2014.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Content models with attitude", "author": ["Sauper Christina", "Aria Haghighi", "Regina Barzilay"], "venue": "In Proceedings of ACL", "citeRegEx": "Christina et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Christina et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 1 (2000): 1\u201348.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of COLING 2010.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Mining opinion features in customer reviews", "author": ["Minqing Hu", "Bing Liu"], "venue": "In Proceedings AAAI", "citeRegEx": "Hu and Liu.,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proceedings of EMNLP 2014.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Extracting opinion targets in a single- and cross-domain setting with conditional random fields", "author": ["Niklas Jakob", "Iryna Gurevych."], "venue": "Proceedings of", "citeRegEx": "Jakob and Gurevych.,? 2010", "shortCiteRegEx": "Jakob and Gurevych.", "year": 2010}, {"title": "A novel lexicalized HMM-based learning framework for web opinion mining", "author": ["Wei Jin", "Hay H. Hung."], "venue": "Proceedings of ICML 2009.", "citeRegEx": "Jin and Hung.,? 2009", "shortCiteRegEx": "Jin and Hung.", "year": 2009}, {"title": "Aspect and sentiment unification model for online review analysis", "author": ["Yohan Jo", "Alice H. Oh."], "venue": "Proceedings of WSDM 2011.", "citeRegEx": "Jo and Oh.,? 2011", "shortCiteRegEx": "Jo and Oh.", "year": 2011}, {"title": "Generalizing dependency features for opinion mining", "author": ["Mahesh Joshi", "Carolyn Penstein-Rose."], "venue": "Proceedings of ACL-IJCNLP 2009.", "citeRegEx": "Joshi and Penstein.Rose.,? 2009", "shortCiteRegEx": "Joshi and Penstein.Rose.", "year": 2009}, {"title": "A hierarchical aspect-sentiment model for online reviews", "author": ["Suin Kim", "Jianwen Zhang", "Zheng Chen", "Alice Oh", "Shixia Liu."], "venue": "Proceedings of AAAI 2013.", "citeRegEx": "Kim et al\\.,? 2013", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of ACL 2014.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of EMNLP 2014.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Extracting aspect-evaluation and aspect-of relations in opinion mining", "author": ["Nozomi Kobayashi", "Kentaro Inui", "Yuji Matsumoto."], "venue": "Proceedings of EMNLP-CoNLL 2007.", "citeRegEx": "Kobayashi et al\\.,? 2007", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2007}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Proceedings of NIPS 2012.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang."], "venue": "Proceedings of NAACL 2015.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of NIPS 2013.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the design of LDA models for aspect-based opinion mining", "author": ["Samaneh Moghaddam", "Martin Ester."], "venue": "Proceedings of CIKM 2012.", "citeRegEx": "Moghaddam and Ester.,? 2012", "shortCiteRegEx": "Moghaddam and Ester.", "year": 2012}, {"title": "Aspect extraction through semi-supervised modeling", "author": ["Arjun Mukherjee", "Bing Liu."], "venue": "Proceedings of ACL 2012.", "citeRegEx": "Mukherjee and Liu.,? 2012", "shortCiteRegEx": "Mukherjee and Liu.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton."], "venue": "Proceedings of ICML 2010.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Dependency tree-based sentiment classification using CRFs with hidden variables", "author": ["Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi."], "venue": "Proceedings of NAACL 2010.", "citeRegEx": "Nakagawa et al\\.,? 2010", "shortCiteRegEx": "Nakagawa et al\\.", "year": 2010}, {"title": "Examining the role of linguistic knowledge", "author": ["Vincent Ng", "Sajib Dasgupta", "S.M. Niaz Arifin"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2006}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["Cicero Nogueira dos Santos", "Maira Gatti."], "venue": "Proceedings of COLING 2014.", "citeRegEx": "Santos and Gatti.,? 2014", "shortCiteRegEx": "Santos and Gatti.", "year": 2014}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Cicero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of ACL 2015.", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "A study of information retrieval weighting schemes for sentiment analysis", "author": ["Georgios Paltoglou", "Mike Thelwall."], "venue": "Proceedings of ACL 2010.", "citeRegEx": "Paltoglou and Thelwall.,? 2010", "shortCiteRegEx": "Paltoglou and Thelwall.", "year": 2010}, {"title": "Thumbs up? Sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."], "venue": "Proceedings of EMNLP 2002.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen."], "venue": "Computational Linguistics, 37 (1):9\u201327.", "citeRegEx": "Qiu et al\\.,? 2011", "shortCiteRegEx": "Qiu et al\\.", "year": 2011}, {"title": "Feature subsumption for opinion analysis", "author": ["Ellen Riloff", "Siddharth Patwardhan", "Janyce Wiebe."], "venue": "Proceedings of EMNLP 2006.", "citeRegEx": "Riloff et al\\.,? 2006", "shortCiteRegEx": "Riloff et al\\.", "year": 2006}, {"title": "Building a sentiment summarizer for local service reviews", "author": ["Blair-Goldensohn Sasha", "Kerry Hannan", "Ryan McDonald", "Tyler Neylon", "George A. Reis."], "venue": "Workshop at WWW 2008.", "citeRegEx": "Sasha et al\\.,? 2008", "shortCiteRegEx": "Sasha et al\\.", "year": 2008}, {"title": "Automatic aggregation by joint modeling of aspects and values", "author": ["Christina Sauper", "Regina Barzilay."], "venue": "Journal Artificial Intelligence Research, 46:89\u2013127.", "citeRegEx": "Sauper and Barzilay.,? 2013", "shortCiteRegEx": "Sauper and Barzilay.", "year": 2013}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil."], "venue": "Proceedings of WWW 2014.", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil."], "venue": "Proceedings of CIKM 2014.", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of ECCV 2014.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["Ivan Titov", "Ryan McDonald."], "venue": "Proceedings of WWW 2008.", "citeRegEx": "Titov and McDonald.,? 2008", "shortCiteRegEx": "Titov and McDonald.", "year": 2008}, {"title": "Reducing overweighting in supervised term weighting for sentiment analysis", "author": ["Haibing Wu", "Xiaodong Gu."], "venue": "Proceedings of COLING 2014.", "citeRegEx": "Wu and Gu.,? 2014", "shortCiteRegEx": "Wu and Gu.", "year": 2014}, {"title": "Extracting opinion expressions with semi-markov conditional random fields", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of EMNLP-CoNLL 2012.", "citeRegEx": "Yang and Cardie.,? 2012", "shortCiteRegEx": "Yang and Cardie.", "year": 2012}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao."], "venue": "Proceedings of COLING 2014.", "citeRegEx": "Zeng et al\\.,? 2014", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Movie review mining and summarization", "author": ["Li Zhuang", "Feng Jing", "Xiao-Yan Zhu."], "venue": "Proceedings of CIKM 2006.", "citeRegEx": "Zhuang et al\\.,? 2006", "shortCiteRegEx": "Zhuang et al\\.", "year": 2006}, {"title": "Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid", "author": ["Wayne X. Zhao", "Jing Jiang", "Hongfei Yan", "Xiaoming Li."], "venue": "Proceedings of EMNLP 2010.", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "This has bred a new line of research on Aspect-based Opinion Summarization (AOS) (Hu and Liu, 2004).", "startOffset": 81, "endOffset": 99}, {"referenceID": 7, "context": "part-ofspeech and dependency relations) (Hu and Liu, 2004; Joshi and Rose, 2009; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al.", "startOffset": 40, "endOffset": 136}, {"referenceID": 40, "context": "part-ofspeech and dependency relations) (Hu and Liu, 2004; Joshi and Rose, 2009; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al.", "startOffset": 40, "endOffset": 136}, {"referenceID": 29, "context": "part-ofspeech and dependency relations) (Hu and Liu, 2004; Joshi and Rose, 2009; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al.", "startOffset": 40, "endOffset": 136}, {"referenceID": 10, "context": ", 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al., 2007; Yang and Cardie, 2012).", "startOffset": 53, "endOffset": 166}, {"referenceID": 9, "context": ", 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al., 2007; Yang and Cardie, 2012).", "startOffset": 53, "endOffset": 166}, {"referenceID": 8, "context": ", 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al., 2007; Yang and Cardie, 2012).", "startOffset": 53, "endOffset": 166}, {"referenceID": 1, "context": ", 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al., 2007; Yang and Cardie, 2012).", "startOffset": 53, "endOffset": 166}, {"referenceID": 38, "context": ", 2011) or supervised sequence labeling such as CRFs (Jin and Hung, 2009; Jakob and Gurevych, 2010; Irsoy and Cardie, 2014; Breck et al., 2007; Yang and Cardie, 2012).", "startOffset": 53, "endOffset": 166}, {"referenceID": 36, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 4, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 2, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 41, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 11, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 20, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 3, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 21, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 13, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 32, "context": "Another line of related work applies variants of standard topic modeling such as LDA (Titov and McDonald, 2008; Christina et al., 2011; Brody and Elhadad, 2010; Zhao et al., 2010; Jo and Oh, 2011; Moghaddam and Ester, 2012; Chen et al., 2014; Mukherjee and Liu, 2012; Kim et al., 2013; Sauper and Barzilay, 2013).", "startOffset": 85, "endOffset": 312}, {"referenceID": 28, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 24, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 30, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 6, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 27, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 23, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 0, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 37, "context": "Most prior work used traditional machine learning with complicated feature engineering (Pang et al., 2002; Ng et al., 2006; Riloff et al., 2006; Davidov et al., 2010; Paltoglou and Thelwall, 2010; Nakagawa et al., 2010; Bespalov et al., 2011; Wu and Gu, 2014).", "startOffset": 87, "endOffset": 259}, {"referenceID": 14, "context": "Very recently, some researchers applied deep convolutional neural networks to sentence sentiment classification and reported considerably better results than traditional approaches (Kalchbrenner et al., 2014; Santos and Gatti, 2014; Kim, 2014).", "startOffset": 181, "endOffset": 243}, {"referenceID": 25, "context": "Very recently, some researchers applied deep convolutional neural networks to sentence sentiment classification and reported considerably better results than traditional approaches (Kalchbrenner et al., 2014; Santos and Gatti, 2014; Kim, 2014).", "startOffset": 181, "endOffset": 243}, {"referenceID": 15, "context": "Very recently, some researchers applied deep convolutional neural networks to sentence sentiment classification and reported considerably better results than traditional approaches (Kalchbrenner et al., 2014; Santos and Gatti, 2014; Kim, 2014).", "startOffset": 181, "endOffset": 243}, {"referenceID": 17, "context": "Convolutional neural network (CNN) is currently underpinning the cutting edge in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2014).", "startOffset": 97, "endOffset": 144}, {"referenceID": 35, "context": "Convolutional neural network (CNN) is currently underpinning the cutting edge in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2014).", "startOffset": 97, "endOffset": 144}, {"referenceID": 5, "context": "It has also achieved state-of-the-art results in many traditional NLP tasks (Collobert et al., 2011) and other NLP areas such as information retrieval (Shen et al.", "startOffset": 76, "endOffset": 100}, {"referenceID": 33, "context": ", 2011) and other NLP areas such as information retrieval (Shen et al., 2014) and relation classification (Zeng et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 39, "context": ", 2014) and relation classification (Zeng et al., 2014; Santos et al., 2015).", "startOffset": 36, "endOffset": 76}, {"referenceID": 26, "context": ", 2014) and relation classification (Zeng et al., 2014; Santos et al., 2015).", "startOffset": 36, "endOffset": 76}, {"referenceID": 19, "context": "CNN models for specific NLP tasks often use unsupervised pre-trained word vectors (Mikolov et al., 2013) as initialization, which are then improved by optimizing supervised objectives.", "startOffset": 82, "endOffset": 104}, {"referenceID": 5, "context": "For example, (Collobert et al., 2011) tackled part-of-speech tagging, chunking, and named entity recognition tasks using a multitask sequence labeller.", "startOffset": 13, "endOffset": 37}, {"referenceID": 18, "context": "(Liu et al., 2015) trained a multi-task deep neural network for query classification and web search ranking.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "This layer applies max-over-time pooling (Collobert et al. 2011) to each of the feature maps produced by convolutional layers:", "startOffset": 41, "endOffset": 64}, {"referenceID": 22, "context": "We use rectified linear units (Nair and Hinton, 2010) as activation functions for convolutional layer, and sigmoid function for output layer.", "startOffset": 30, "endOffset": 53}], "year": 2015, "abstractText": "This paper considers Aspect-based Opinion Summarization (AOS) of reviews on particular products. To enable real applications, an AOS system needs to address two core subtasks, aspect extraction and sentiment classification. Most existing approaches to aspect extraction, which use linguistic analysis or topic modelling, are general across different products but not precise enough or suitable for particular products. Instead we take a less general but more precise scheme, directly mapping each review sentence into pre-defined aspects. To tackle aspect mapping and sentiment classification, we propose two Convolutional Neural Network (CNN) based methods, cascaded CNN and multitask CNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs at level 1 deal with aspect mapping task, and a single CNN at level 2 deals with sentiment classification. Multitask CNN also contains multiple aspect CNNs and a sentiment CNN, but different networks share the same word embeddings. Experimental results indicate that both cascaded and multitask CNNs outperform SVM-based methods by large margins. Multitask CNN generally performs better than cascaded CNN.", "creator": "Microsoft\u00ae Word 2013"}}}