{"id": "1511.09249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models", "abstract": "In 2013, our large recurring neural networks (RNNs) learned to control simulated cars from the ground up using high-dimensional video input. However, real brains are more powerful in many ways. Specifically, they learn a predictive model of their initially unknown environment and somehow use it for abstract (e.g. hierarchical) planning and reasoning. Led by algorithmic information theory, we describe RNN-based AIs (RNNAIs) that are designed to do the same. Unlike our previous model, which built RNN-based RL machines that date back to 1990, the RNNAIs themselves learn in a strange, playful way to improve their RNN-based world model. Unlike our previous RNN-based RL machines, the RNNAIs will actively question their model and make decisions to essentially explain the results of this system. \"", "histories": [["v1", "Mon, 30 Nov 2015 11:35:26 GMT  (54kb,D)", "http://arxiv.org/abs/1511.09249v1", "36 pages, 1 figure. arXiv admin note: substantial text overlap witharXiv:1404.7828"]], "COMMENTS": "36 pages, 1 figure. arXiv admin note: substantial text overlap witharXiv:1404.7828", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["juergen schmidhuber"], "accepted": false, "id": "1511.09249"}, "pdf": {"name": "1511.09249.pdf", "metadata": {"source": "CRF", "title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models", "authors": ["J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n09 24\n9v 1\n[ cs\nContents"}, {"heading": "1 Introduction to Reinforcement Learning (RL) with Recurrent Neural Networks (RNNs) in Partially Observable Environments 2", "text": "1.1 RL through Direct and Indirect Search in RNN Program Space . . . . . . . . . . . . 3 1.2 Deep Learning in NNs: Supervised & Unsupervised Learning (SL & UL) . . . . . . 3 1.3 Gradient Descent-Based NNs for RL . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3.1 Early RNN Controllers with Predictive RNN World Models . . . . . . . . . 5 1.3.2 Early Predictive RNN World Models Combined with Traditional RL . . . . . 5\n1.4 Hierarchical & Multitask RL and Algorithmic Transfer Learning . . . . . . . . . . . 6"}, {"heading": "2 Algorithmic Information Theory (AIT) for RNN-based AIs 6", "text": "2.1 Basic AIT Argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 One RNN-Like System Actively Learns to Exploit Algorithmic Information of Another 7 2.3 Consequences of the AIT Argument for Model-Building Controllers . . . . . . . . . 8"}, {"heading": "3 The RNNAI and its Holy Data 8", "text": "3.1 Standard Activation Spreading in Typical RNNs . . . . . . . . . . . . . . . . . . . . 9 3.2 Alternating Training Phases for Controller C and World Model M . . . . . . . . . . 9"}, {"heading": "4 The Gradient-Based World Model M 10", "text": "4.1 M \u2019s Compression Performance on the History so far . . . . . . . . . . . . . . . . . 10 4.2 M \u2019s Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.3 M may have a Built-In FNN Preprocessor . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "5 The Controller C Learning to Exploit RNN World Model M 11", "text": "5.1 C as a Standard RL Machine whose States are M \u2019s Activations . . . . . . . . . . . 12 5.2 C as an Evolutionary RL (R)NN whose Inputs are M \u2019s Activations . . . . . . . . . 12 5.3 C Learns to Think with M : High-Level Plans and Abstractions . . . . . . . . . . . 12 5.4 Incremental / Hierarchical / Multitask Learning of C with M . . . . . . . . . . . . 14"}, {"heading": "6 Exploration: Rewarding C for Experiments that Improve M 14", "text": ""}, {"heading": "7 Conclusion 15", "text": ""}, {"heading": "1 Introduction to Reinforcement Learning (RL) with Recurrent", "text": "Neural Networks (RNNs) in Partially Observable Environments1\nGeneral Reinforcement Learning (RL) agents must discover, without the aid of a teacher, how to interact with a dynamic, initially unknown, partially observable environment in order to maximize their expected cumulative reward signals, e.g., [123, 272, 310]. There may be arbitrary, a priori unknown delays between actions and perceivable consequences. The RL problem is as hard as any problem of computer science, since any task with a computable description can be formulated in the RL framework, e.g., [109].\nTo become a general problem solver that is able to run arbitrary problem-solving programs, the controller of a robot or an artificial agent must be a general-purpose computer [67, 35, 282, 194].\n1Parts of this introduction are similar to parts of a much more extensive recent Deep Learning overview [245] which has many additional references.\nArtificial recurrent neural networks (RNNs) fit this bill. A typical RNN consists of many simple, connected processors called neurons, each producing a sequence of real-valued activations. Input neurons get activated through sensors perceiving the environment, other neurons get activated through weighted connections or wires from previously active neurons, and some neurons may affect the environment by triggering actions. Learning or credit assignment is about finding real-valued weights that make the NN exhibit desired behavior, such as driving a car. Depending on the problem and how the neurons are connected, such behavior may require long causal chains of computational stages, where each stage transforms the aggregate activation of the network, often in a non-linear manner.\nUnlike feedforward NNs (FNNs; [95, 23]) and Support Vector Machines (SVMs; [287, 253]), RNNs can in principle interact with a dynamic partially observable environment in arbitrary, computable ways, creating and processing memories of sequences of input patterns [258]. The weight matrix of an RNN is its program. Without a teacher, reward-maximizing programs of an RNN must be learned through repeated trial and error."}, {"heading": "1.1 RL through Direct and Indirect Search in RNN Program Space", "text": "It is possible to train small RNNs with a few 100 or 1000 weights using evolutionary algorithms [200, 255, 105, 56, 68] to search the space of NN weights [165, 307, 44, 321, 180, 259, 320, 164, 173, 69, 71, 187, 121, 313, 66, 270, 269, 305], or through policy gradients (PGs) [314, 315, 316, 274, 18, 1, 63, 128, 313, 210, 192, 191, 256, 85, 312, 190, 82, 93][245, Sec. 6.6]. For example, our evolutionary algorithms outperformed traditional, Dynamic Programming [20]-based RL methods [272][245, Sec. 6.2] in partially observable environments, e.g., [72]. However, these techniques by themselves are insufficient for solving complex control problems involving high-dimensional sensory inputs such as video, from scratch. The program search space for networks of the size required for these tasks is simply too large.\nHowever, the search space can often be reduced dramatically by evolving compact encodings of neural networks (NNs), e.g., through Lindenmeyer Systems [115], graph rewriting [127], Cellular Encoding [83], HyperNEAT [268], and other techniques [245, Sec. 6.7]. In very general early work, we used universal assembler-like languages to encode NNs [235], later coefficients of a Discrete Cosine Transform (DCT) [132]. The latter method, Compressed RNN Search [132], was used to successfully evolve RNN controllers with over a million weights (the largest ever evolved) to drive a simulated car in a video game, based solely on a high-dimensional video stream [132]\u2014learning both control and visual processing from scratch, without unsupervised pre-training of a vision system. This was the first published Deep Learner to learn control policies directly from high-dimensional sensory input using RL.\nOne can further facilitate the learning task of controllers through certain types of supervised learning (SL) and unsupervised learning (UL) based on gradient descent techniques. In particular, UL/SL can be used to compress the search space, and to build predictive world models to accelerate RL, as will be discussed later. But first let us review the relevant NN algorithms for SL and UL."}, {"heading": "1.2 Deep Learning in NNs: Supervised & Unsupervised Learning (SL & UL)", "text": "The term Deep Learning was first introduced to Machine Learning in 1986 [49] and to NNs in 2000 [3, 244]. The first deep learning NNs, however, date back to the 1960s [113, 245] (certain more recent developments are covered in a survey [139]).\nTo maximize differentiable objective functions of SL and UL, NN researchers almost invariably use backpropagation (BP) [125, 30, 52] in discrete graphs of nodes with differentiable activation functions [151, 265][245, Sec. 5.5]. Typical applications include BP in FNNs [297], or BP through time (BPTT) and similar methods in RNNs, e.g., [299, 317, 208][245]. BP and BPTT suffer from the\nFundamental Deep Learning Problem first discovered and analyzed in my lab in 1991: with standard activation functions, cumulative backpropagated error signals decay exponentially in the number of layers, or they explode [98, 99]. Hence most early FNNs [297, 211] had few layers. Similarly, early RNNs [245, Sec. 5.6.1] could not generalize well under both short and long time lags between relevant events. Over the years, several ways of overcoming the Fundamental Deep Learning Problem have been explored. For example, deep stacks of unsupervised RNNs [228] or FNNs [13, 96, 139] help to accelerate subsequent supervised learning through BPTT [228, 230] or BP [96]. One can also \u201cdistill\u201d or compress the knowledge of a teacher RNN into a student RNN by forcing the student to predict the hidden units of the teacher [228, 230].\nLong Short-Term Memory (LSTM; [101, 61, 77]) alleviates the Fundamental Deep Learning Problem, and was the first RNN architecture to win international contests (in connected handwriting), e.g., [79, 247][245]. Connectionist Temporal Classification (CTC) [76] is a widely used gradient-based method for finding RNN weights that maximize the probability of teacher-provided label sequences, given (typically much longer and more high-dimensional) streams of real-valued input vectors. For example, CTC was used by Baidu to break an important speech recognition record [88]. Many recent state-of-the-art results in sequence processing are based on LSTM, which learned to control robots [159], and was used to set benchmark records in prosody contour prediction [55] (IBM), textto-speech synthesis [54] (Microsoft), large vocabulary speech recognition [213] (Google), and machine translation [271] (Google). CTC-trained LSTM greatly improved Google Voice [214] and is now available to over a billion smartphone users. Nevertheless, at least in some applications, other RNNs may sometimes yield better results than gradient-based LSTM [158, 217, 323, 116, 250, 186, 133]. Alternative NNs with differentiable memory have been proposed [229, 47, 175, 232, 231, 103, 80, 303].\nToday\u2019s faster computers, such as GPUs, mitigate the Fundamental Deep Learning Problem for FNNs [181, 34, 198, 38, 40]. In particular, many recent computer vision contests were won by fully supervised Max-Pooling Convolutional NNs (MPCNNs), which consist of alternating convolutional [58, 19] and max-pooling [296] layers topped off by standard fully connected output layers. All weights are trained by backpropagation [140, 199, 220, 245]. Ensembles [218, 28] of GPUbased MPCNNs [40, 41] achieved dramatic improvements of long-standing benchmark records, e.g., MNIST (2011), won numerous competitions [247, 38, 41, 39, 161, 42, 36, 134, 322, 37, 245], and achieved the first human-competitive or even superhuman results on well-known benchmarks, e.g., [247, 42, 245]. There are many recent variations and improvements [64, 74, 124, 75, 277, 266, 245]. Supervised Transfer Learning from one dataset to another [32, 43] can speed up learning. A combination of Convolutional NNs (CNNs) and LSTM led to best results in automatic image caption generation [288]."}, {"heading": "1.3 Gradient Descent-Based NNs for RL", "text": "Perhaps the most well-known RL application is Tesauro\u2019s backgammon player [280] from 1994 which learned to achieve the level of human world champions, by playing against itself. It uses a reactive (memory-free) policy based on the simplifying assumption of Markov Decision Processes: the current input of the RL agent conveys all information necessary to compute an optimal next output event or decision. The policy is implemented as a gradient-based FNN trained by the method of temporal differences [272][245, Sec. 6.2]. During play, the FNN learns to map board states to predictions of expected cumulative reward, and selects actions leading to states with maximal predicted reward. A very similar approach (also based on over 20-year-old methods) employed a CNN (see Sec. 1.2) to play several Atari video games directly from 84\u00d784 pixel 60 Hz video input [167], using Neural Fitted Q-Learning (NFQ) [201] based on experience replay (1991) [149]. Even better results were\nachieved by using (slow) Monte Carlo tree planning to train comparatively fast deep NNs [86]. Such FNN approaches cannot work in realistic partially observable environments where memories of previous inputs have to be stored for a priori unknown time intervals. This triggered work on partially observable Markov decision problems (POMDPs) [223, 222, 227, 204, 205, 206, 316, 148, 278, 122, 152, 25, 114, 160, 126, 308, 309, 183]. Traditional RL techniques [272][245, Sec. 6.2] based on Dynamic Programming [20] can be combined with gradient descent methods to train an RNN as a value-function approximator that maps entire event histories to predictions of expected cumulative reward [227, 148]. LSTM [101, 61, 189, 78, 77] (see Sec. 1.2) was used in this way for RL robots [12].\nGradient-based UL may be used to reduce an RL controller\u2019s search space by feeding it only compact codes of high-dimensional inputs [118, 142, 46][245, Sec. 6.4]. For example, NFQ [201] was applied to real-world control tasks [138, 202] where purely visual inputs were compactly encoded in hidden layers of deep autoencoders [245, Sec. 5.7 and and 5.15]. RL combined with unsupervised learning based on Slow Feature Analysis [318, 131] enabled a humanoid robot to learn skills from raw video streams [154]. A RAAM RNN [193] was employed as a deep unsupervised sequence encoder for RL [65]."}, {"heading": "1.3.1 Early RNN Controllers with Predictive RNN World Models", "text": "One important application of gradient-based UL is to obtain a predictive world model, M , that a controller, C, may use to achieve its goals more efficiently, e.g., through cheap, \u201cmental\u201d M -based trials, as opposed to expensive trials in the real world [301, 273]. The first combination of an RL RNN C and an UL RNN M was ours and dates back to 1990 [223, 222, 226, 227], generalizing earlier similar controller/model systems (CM systems) based on FNNs [298, 179]; compare related work [177, 119, 301, 300, 209, 120, 178, 302, 73, 45, 144, 166, 153, 196, 60][245, Sec. 6.1]. M tries to learn to predict C\u2019s inputs (including reward signals) from previous inputs and actions. M is also temporarily used as a surrogate for the environment: M and C form a coupled RNN where M \u2019s outputs become inputs of C, whose outputs (actions) in turn become inputs of M . Now a gradient descent technique [299, 317, 208](see Sec. 1.2) can be used to learn and plan ahead by training C in a series of M -simulated trials to produce output action sequences achieving desired input events, such as high real-valued reward signals (while the weights of M remain fixed). An RL active vision system, from 1991 [249], used this basic principle to learn sequential shifts (saccades) of a fovea to detect targets in a visual scene, thus learning a rudimentary version of selective attention.\nThose early CM systems, however, did not yet use powerful RNNs such as LSTM. A more fundamental problem is that if the environment is too noisy, M will usually only learn to approximate the conditional expectations of predicted values, given parts of the history. In certain noisy environments, Monte Carlo Tree Sampling (MCTS; [29]) and similar techniques may be applied toM to plan successful future action sequences for C. All such methods, however, are about simulating possible futures time step by time step, without profiting from human-like hierarchical planning or abstract reasoning, which often ignores irrelevant details."}, {"heading": "1.3.2 Early Predictive RNN World Models Combined with Traditional RL", "text": "In the early 1990s, an RNNM as in Sec. 1.3.1 was also combined [227, 150] with traditional temporal difference methods [122, 272][245, Sec. 6.2] based on the Markov assumption (Sec. 1.3). While M is processing the history of actions and observations to predict future inputs and rewards, the internal states ofM are used as inputs to a temporal difference-based predictor of cumulative predicted reward, to be maximized through appropriate action sequences. One of our systems described in 1991 [227] actually collapsed the cumulative reward predictor into the predictive world model, M ."}, {"heading": "1.4 Hierarchical & Multitask RL and Algorithmic Transfer Learning", "text": "Work on NN-based Hierarchical RL (HRL) without predictive world models has been published since the early 1990s. In particular, gradient-based subgoal discovery with RNNs decomposes RL tasks into subtasks for submodules [225]. Numerous alternative HRL techniques have been proposed [204, 206, 117, 279, 295, 171, 195, 50, 162, 51, 15, 215, 11, 260]. While HRL frameworks such as Feudal RL [48] and options [275, 16, 261] do not directly address the problem of automatic subgoal discovery, HQ-Learning [309] automatically decomposes problems in partially observable environments into sequences of simpler subtasks that can be solved by memoryless policies learnable by reactive subagents. Related methods include incremental NN evolution [70], hierarchical evolution of NNs [306, 285], and hierarchical Policy Gradient algorithms [63]. Recent HRL organizes potentially deep NNbased RL sub-modules into self-organizing, 2-dimensional motor control maps [203] inspired by neurophysiological findings [81]. The methods above, however, assign credit in hierarchical fashion by limited fixed schemes that are not themselves improved or adapted in problem-specific ways. The next sections will describe novel CM systems that overcome such drawbacks of above-mentioned methods.\nGeneral methods for incremental multitask RL and algorithmic transfer learning that are not NN-specific include the evolutionary ADATE system [182], the Success-Story Algorithm for SelfModifying Policies running on general-purpose computers [233, 252, 251], and the Optimal Ordered Problem Solver [238], which learns algorithmic solutions to new problems by inspecting and exploiting (in arbitrary computable fashion) solutions to old problems, in a way that is asymptotically time-optimal. And POWERPLAY [243, 267] incrementally learns to become a more and more general algorithmic problem solver, by continually searching the space of possible pairs of new tasks and modifications of the current solver, until it finds a more powerful solver that, unlike the unmodified solver, solves all previously learned tasks plus the new one, or at least simplifies/compresses/speeds up previous solutions, without forgetting any."}, {"heading": "2 Algorithmic Information Theory (AIT) for RNN-based AIs", "text": "Our early RNN-based CM systems (1990) mentioned in Sec. 1.3.1 learn a predictive model of their initially unknown environment. Real brains seem to do so too, but are still far superior to present artificial systems in many ways. They seem to exploit the model in smarter ways, e.g., to plan action sequences in hierarchical fashion, or through other types of abstract reasoning, continually building on earlier acquired skills, becoming increasingly general problem solvers able to deal with a large number of diverse and complex tasks. Here we describe RNN-based Artificial Intelligences (RNNAIs) designed to do the same by \u201clearning to think.\u201d2\nWhile FNNs are traditionally linked [23] to concepts of statistical mechanics and information theory [24, 257, 136], the programs of general computers such as RNNs call for the framework of Algorithmic Information Theory (AIT) [263, 130, 33, 145, 264, 147] (own AIT work: [234, 235, 236, 237, 238]). Given some universal programming language [67, 35, 282, 194] for a universal computer, the algorithmic information content or Kolmogorov complexity of some computable object is the length of the shortest program that computes it. Since any program for one computer can be translated into a functionally equivalent program for a different computer by a compiler program of constant size, the Kolmogorov complexity of most objects hardly depends on the particular computer used. Most computable objects of a given size, however, are hardly compressible, since there are only relatively few programs that are much shorter. Similar observations hold for practical variants\n2The terminology is partially inspired by our RNNAISSANCE workshop at NIPS 2003 [246].\nof Kolmogorov complexity that explicitly take into account program runtime [146, 6, 291, 147, 235, 237]. Our RNNAIs are inspired by the following argument."}, {"heading": "2.1 Basic AIT Argument", "text": "According to AIT, given some universal computer, U , whose programs are encoded as bit strings, the mutual information between two programs p and q is expressed as K(q | p), the length of the shortest program w\u0304 that computes q, given p, ignoring an additive constant of O(1) depending on U (in practical applications the computation will be time-bounded [147]). That is, if p is a solution to problem P , and q is a fast (say, linear time) solution to problem Q, and if K(q | p) is small, and w\u0304 is both fast and much shorter than q, then asymptotically optimal universal search [146, 238] for a solution to Q, given p, will generally find w\u0304 first (to compute q and solve Q), and thus solve Q much faster than search for q from scratch [238]."}, {"heading": "2.2 One RNN-Like System Actively Learns to Exploit Algorithmic Information of Another", "text": "The AIT argument 2.1 above has broad applicability. Let both C and M be RNNs or similar general parallel-sequential computers [229, 47, 175, 232, 231, 103, 80, 303]. M \u2019s vector of learnable realvalued parameters wM is trained by any SL or UL or RL algorithm to perform a certain well-defined task in some environment. Then wM is frozen. Now the goal is to train C\u2019s parameters wC by some learning algorithm to perform another well-defined task whose solution may share mutual algorithmic information with the solution to M \u2019s task. To facilitate this, we simply allow C to learn to actively inspect and reuse (in essentially arbitrary computable fashion) the algorithmic information conveyed by M and wM .\nLet us consider a trial during which C makes an attempt to solve its given task within a series of discrete time steps t = ta, ta + 1, . . . , tb. C\u2019s learning algorithm may use the experience gathered during the trial to modify wC in order to improve C\u2019s performance in later trials. During the trial, we give C an opportunity to explore and exploit or ignore M by interacting with it. In what follows, C(t), M(t), sense(t), act(t), query(t), answer(t), wM , wC denote vectors of real values; fC , fM denote computable [67, 35, 282, 194] functions.\nAt any time t, C(t) andM(t) denoteC\u2019s andM \u2019s current states, respectively. They may represent current neural activations or fast weights [229, 232, 231] or other dynamic variables that may change during information processing. sense(t) is the current input from the environment (including reward signals if any); a part of C(t) encodes the current output act(t) to the environment, another a memory of previous events (if any). Parts of C(t) and M(t) intersect in the sense that both C(t) and M(t) also encode C\u2019s current query(t) to M , and M \u2019s current answer(t) to C (in response to previous queries), thus representing an interface between C and M .\nM(ta) and C(ta) are initialized by default values. For ta \u2264 t < tb,\nC(t+ 1) = fC(wC , C(t),M(t), sense(t), wM )\nwith learnable parameters wC ; act(t) is a computable function of C(t) and may influence in(t+ 1), and M(t + 1) = fM (C(t),M(t), wM ) with fixed parameters wM . So both M(t + 1) and C(t + 1) are computable functions of previous events including queries and answers transmitted through the learnable fC .\nAccording to the AIT argument, provided that M conveys substantial algorithmic information about C\u2019s task, and the trainable interface fC between C and M allows C to address and extract and exploit this information quickly, and wC is small compared to the fixed wM , the search space of C\u2019s\nlearning algorithm (trying to find a good wC through a series of trials) should be much smaller than the one of a similar competing system C \u2032 that has no opportunity to query M but has to learn the task from scratch.\nFor example, suppose that M has learned to represent (e.g., through predictive coding [228, 248]) videos of people placing toys in boxes, or to summarize such videos through textual outputs. Now suppose C\u2019s task is to learn to control a robot that places toys in boxes. Although the robot\u2019s actuators may be quite different from human arms and hands, and although videos and video-describing texts are quite different from desirable trajectories of robot movements, M is expected to convey algorithmic information about C\u2019s task, perhaps in form of connected high-level spatio-temporal feature detectors representing typical movements of hands and elbows independent of arm size. Learning a wC that addresses and extracts this information from M and partially reuses it to solve the robot\u2019s task may be much faster than learning to solve the task from scratch without access to M .\nThe setups of Sec. 5.3 are special cases of the general scheme in the present Sec. 2.2."}, {"heading": "2.3 Consequences of the AIT Argument for Model-Building Controllers", "text": "The simple AIT insight above suggests that in many partially observable environments it should be possible to greatly speed up the program search of an RL RNN, C, by letting it learn to access, query, and exploit in arbitrary computable ways the program of a typically much bigger gradient-based UL RNN, M , used to model and compress the RL agent\u2019s entire growing interaction history of all failed and successful trials.\nNote that the w\u0304 of Sec. 2.1 may implement all kinds of well-known, computable types of reasoning, e.g., by hierarchical reuse of subprograms of p [238], by analogy, etc. That is, we may perhaps even expect C to learn to exploit M for human-like abstract thought.\nSuch novel CM systems will be a central topic of Sec. 5. Sec. 6 will also discuss exploration based on efficiently improving M through C-generated experiments."}, {"heading": "3 The RNNAI and its Holy Data", "text": "In what follows, letm,n, o denote positive integer constants, and i, k, h, t, \u03c4 positive integer variables assuming ranges implicit in the given contexts. The i-th component of any real-valued vector, v, is denoted by vi. Let the RNNAI\u2019s life span a discrete sequence of time steps, t = 1, 2, . . . , tdeath.\nAt the beginning of a given time step, t, there is a \u201cnormal\u201d sensory input vector, in(t) \u2208 Rm, and a reward input vector, r(t) \u2208 Rn. For example, parts of in(t) may represent the pixel intensities of an incoming video frame, while components of r(t) may reflect external positive rewards, or negative values produced by pain sensors whenever they measure excessive temperature or pressure. Let sense(t) \u2208 Rm+n denote the concatenation of the vectors in(t) and r(t). The total reward at time t is R(t) = \u2211n i=1 ri(t). The total cumulative reward up to time t is CR(t) = \u2211t \u03c4=1R(\u03c4). During time step t, the RNNAI produces an output action vector, out(t) \u2208 Ro, which may influence the environment and thus future sense(\u03c4) for \u03c4 > t. At any given time, the RNNAI\u2019s goal is to maximize CR(tdeath).\nLet all(t) \u2208 Rm+n+o denote the concatenation of sense(t) and out(t). Let H(t) denote the sequence (all(1), all(2), . . . , all(t)) up to time t.\nTo be able to retrain its components on all observations ever made, the RNNAI stores its entire, growing, lifelong sensory-motor interaction history H(\u00b7) including all inputs and actions and reward signals observed during all successful and failed trials [239, 240], including what initially looks like noise but later may turn out to be regular. This is normally not done, but is feasible today.\nThat is, all data is \u201choly\u201d, and never discarded, in line with what mathematically optimal general problem solvers should do [109, 237]. Remarkably, even human brains may have enough storage capacity to store 100 years of sensory input at a reasonable resolution [240]."}, {"heading": "3.1 Standard Activation Spreading in Typical RNNs", "text": "Many RNN-like models can be used to build general computers, e.g., neural pushdown automata [47, 175], NNs with quickly modifiable, differentiable external memory based on fast weights [229], or closely related RNN-based meta-learners [232, 231, 103, 219]. Using sloppy but convenient terminology, we refer to all of them as RNNs. A typical implementation of M uses an LSTM network (see Sec. 1.2). If there are large 2-dimensional inputs such as video images, then they can be first filtered through a CNN (compare Sec. 1.2 and 4.3) before fed into the LSTM. Such a CNN-LSTM combination is still an RNN.\nHere we briefly summarize information processing in standard RNNs. Using notation similar to the one of a previous survey [245, Sec. 2], let i, k, s denote positive integer variables assuming ranges implicit in the given contexts. Let nu, nw, T also denote positive integers.\nAt any given moment, an RNN (such as the M of Sec. 4) can be described as a connected graph with nu units (or nodes or neurons) in a set N = {u1, u2, . . . , unu} and a set H \u2286 N \u00d7N of directed edges or connections between nodes. The input layer is the set of input units, a subset of N . In fully connected RNNs, all units have connections to all non-input units.\nThe RNN\u2019s behavior or program is determined by nw real-valued, possibly modifiable, parameters or weights, wi (i = 1, . . . , nw). During an episode of information processing (e.g., during a trial of Sec. 3.2), there is a partially causal sequence xs(s = 1, . . . , T ) of real values called events. Here the index s is used in a way that is much more fine-grained than the one of the index t in Sec. 3, 4, 5: a single time step may involve numerous events. Each xs is either an input set by the environment, or the activation of a unit that may directly depend on other xk(k < s) through a current NN topology-dependent set, ins, of indices k representing incoming causal connections or links. Let the function v encode topology information, and map such event index pairs, (k, s), to weight indices. For example, in the non-input case we may have xs = fs(nets) with real-valued nets = \u2211 k\u2208ins xkwv(k,s) (additive case) or nets = \u220f k\u2208ins xkwv(k,s) (multiplicative case), where fs is a typically nonlinear real-valued activation function such as tanh. Other net functions combine additions and multiplications [113, 112]; many other activation functions are possible. The sequence, xs, may directly affect certain xk(k > s) through outgoing connections or links represented through a current set, outs, of indices k with s \u2208 ink. Some of the non-input events are called output events.\nMany of the xs may refer to different, time-varying activations of the same unit, e.g., in RNNs. During the episode, the same weight may get reused over and over again in topology-dependent ways. Such weight sharing across space and/or time may greatly reduce the NN\u2019s descriptive complexity, which is the number of bits of information required to describe the NN (Sec. 4). Training algorithms for the RNNs of our RNNAIs will be discussed later."}, {"heading": "3.2 Alternating Training Phases for Controller C and World Model M", "text": "Several novel implementations of C are described in Sec. 5. All of them make use of a variable size RNN called the world model, M , which learns to compactly encode the growing history, for example, through predictive coding, trying to predict (the expected value of) each input component, given the history of actions and observations. M \u2019s goal is to discover algorithmic regularities in the data so far by learning a program that compresses the data better in a lossless manner. Example details will be specified in Sec. 4.\nAlgorithm 1 Train C and M in Alternating Fashion 1. Initialize C and M and their weights. 2. Freeze M \u2019s weights such that they cannot change while C learns. 3. Execute a new trial by generating a finite action sequence that prolongs the history of actions and observations. Actions may be due to C which may exploit M in various ways (see Sec. 5). Train C\u2019s weights on the prolonged (and recorded) history to generate action sequences with higher expected reward, using methods of Sec. 5. 4. Unfreeze M \u2019s weights, and re-train M in a \u201csleep phase\u201d to better predict/compress the prolonged history; see Sec. 4. 5. If no stopping criterion is met, goto 2.\nBoth C and M have real-valued parameters or weights that can be modified to improve performance. To avoid instabilities, C and M are trained in alternating fashion, as in Algorithm 1."}, {"heading": "4 The Gradient-Based World Model M", "text": "A central objective of unsupervised learning is to compress the observed data [14, 228]. M \u2019s goal is to compress the RL agent\u2019s entire growing interaction history of all failed and successful trials [239, 241], e.g., through predictive coding [228, 248]. M has m+n+o input units to receive all(t) at time t < tdeath, and m + n output units to produce a prediction pred(t + 1) \u2208 Rm+n of sense(t + 1) [223, 226, 222, 227]."}, {"heading": "4.1 M \u2019s Compression Performance on the History so far", "text": "Let us address details of trainingM in a \u201csleep phase\u201d of step 4 in algorithm 1. (The training ofC will be discussed in Sec. 5.) Consider some M with given (typically suboptimal) weights and a default initialization of all unit activations. One example way of making M compress the history (but not the only one) is the following. Given H(t), we can train M by replaying [149] H(t) in semi-offline training, sequentially feeding all(1), all(2), . . . all(t) into M \u2019s input units in standard RNN fashion (Sec. 1.2, 3.1). Given H(\u03c4) (\u03c4 < t), M calculates pred(\u03c4 + 1), a prediction of sense(\u03c4 + 1). A standard error function to be minimized by gradient descent in M \u2019s weights (Sec. 1.2) would be E(t) = \u2211t\u22121 \u03c4=1 \u2016pred(\u03c4 + 1)\u2212 sense(\u03c4 + 1)\u20162, the sum of the deviations of the predictions from the observations so far. However, M \u2019s goal is not only to minimize the total prediction error, E. Instead, to avoid the erroneous \u201cdiscovery\u201d of \u201cregular patterns\u201d in irregular noise, we use AIT\u2019s sound way of dealing with overfitting [263, 130, 289, 207, 147, 84], and measure M \u2019s compression performance by the number of bits required to specify M , plus the bits needed to encode the observed deviations from M \u2019s predictions [239, 241]. For example, whenever M incorrectly predicts certain input pixels of a perceived video frame, those pixel values will have to be encoded separately, which will cost storage space. (In typical applications, M can only execute a fixed number of elementary computations per time step to compress and decompress data, which usually has to be done online. That is, in general M will not reflect the data\u2019s true Kolmogorov complexity [263, 130], but at best a time-bounded variant thereof [147].)\nLet integer variables, bitsM and bitsH , denote estimates of the number of bits required to encode (by a fixed algorithmic scheme) the current M , and the deviations of M \u2019s predictions from the observations on the current history, respectively. For example, to obtain bitsH , we may naively assume some simple, bell-shaped, zero-centered probability distribution Pe on the finite number of possible\nreal-valued prediction errors ei,\u03c4 = (predi(\u03c4)\u2212 sensei(\u03c4))2 (in practical applications the errors will be given with limited precision), and encode each ei,\u03c4 by \u2212logPe(ei,\u03c4 ) bits [108, 257]. That is, large errors are considered unlikely and cost more bits than small ones. To obtain bitsM , we may naively multiply the current number ofM \u2019s non-zero modifiable weights by a small integer constant reflecting the weight precision. Alternatively, we may assume some simple, bell-shaped, zero-centered probability distribution, Pw, on the finite number of possible weight values (given with limited precision), and encode each wi by \u2212logPw(wi) bits. That is, large absolute weight values are considered unlikely and cost more bits than small ones [91, 294, 135, 97]. Both alternatives ignore the possibility that M \u2019s entire weight matrix might be computable by a short computer program [235, 132], but have the advantage of being easy to calculate. Moreover, since M is a general computer itself, at least in principle it has a chance of learning equivalents of such short programs."}, {"heading": "4.2 M \u2019s Training", "text": "To decrease bitsM + bitsH , we add a regularizing term to E, to punish excessive complexity [4, 5, 91, 294, 155, 135, 97, 170, 169, 104, 290, 7, 290, 87, 286, 319, 100, 102].\nStep 1 of algorithm 1 starts with a small M . As the history grows, to find an M with small bitsM + bitsH , step 4 uses sequential network construction: it regularly changes M \u2019s size by adding or pruning units and connections [111, 112, 8, 168, 59, 107, 304, 176, 141, 92, 143, 204, 53, 296, 106, 31, 57, 185, 283]. Whenever this helps (after additional training with BPTT of M\u2014see Sec. 1.2) to improve bitsM + bitsH on the history so far, the changes are kept, otherwise they are discarded. (Note that even animal brains grow and prune neurons.)\nGiven history H(t), instead of re-training M in a sleep phase (step 4 of algorithm 1) on all of H(t), we may re-train it on parts thereof, by selecting trials randomly or otherwise from H(t), and replay them to retrainM in standard fashion (Sec. 1.2). To do this, however, all ofM \u2019s unit activations need to be stored at the beginning of each trial. (M \u2019s hidden unit activations, however, do not have to be stored if they are reset to zero at the beginning of each trial.)"}, {"heading": "4.3 M may have a Built-In FNN Preprocessor", "text": "To facilitate M \u2019s task in certain environments, each frame of the sensory input stream (video, etc.) can first be separately compressed through autoencoders [211] or autoencoder hierarchies [13, 21] based on CNNs or other FNNs (see Sec. 1.2) [42] used as sensory preprocessors to create less redundant sensory codes [118, 138, 142, 46]. The compressed codes are then fed into an RNN trained to predict not the raw inputs, but their compressed codes. Those predictions have to be decompressed again by the FNN, to evaluate the total compression performance, bitsM + bitsH , of the FNN-RNN combination representing M ."}, {"heading": "5 The Controller C Learning to Exploit RNN World Model M", "text": "Here we describe ways of using the world model, M , of Sec. 4 to facilitate the task of the RL controller, C. Especially the systems of Sec. 5.3 overcome drawbacks of early CM systems mentioned in Sec. 1.3.1, 1.3.2. Some of the setups of the present Sec. 5 can be viewed as special cases of the general scheme in Sec. 2.2."}, {"heading": "5.1 C as a Standard RL Machine whose States are M \u2019s Activations", "text": "We start with details of an approach whose principles date back to the early 1990s [227, 150] (Sec. 1.3.2). Given an RNN or RNN-like M as in Sec. 4, we implement C as a traditional RL machine [272][245, Sec. 6.2] based on the Markov assumption (Sec. 1.3). While M is processing the history of actions and observations to predict future inputs, the internal states of M are used as inputs to a predictor of cumulative expected future reward.\nMore specifically, in step 3 of algorithm 1, consider a trial lasting from time ta \u2265 1 to tb \u2264 tdeath. M is used as a preprocessor forC as follows. At the beginning of a given time step, t, of the trial (ta \u2264 t < tb), let hidden(t) \u2208 Rh denote the vector of M \u2019s current hidden unit activations (those units that are neither input nor output units). Let state(t) \u2208 R2m+2n+h denote the concatenation of sense(t), hidden(t) and pred(t). (In cases where M \u2019s activations are reset after each trial, hidden(ta) and pred(ta) are initialized by default values, e.g., zero vectors.)\nC is an RL machine with 2m + 2n + h-dimensional inputs and o-dimensional outputs. At time t, state(t) is fed into C, which then computes action out(t). Then M computes from sense(t), hidden(t) and out(t) the values hidden(t + 1) and pred(t + 1). Then out(t) is executed in the environment, to obtain the next input sense(t+ 1).\nThe parameters or weights of C are trained to maximize reward by a standard RL method such as Q-learning or similar methods [17, 292, 293, 172, 254, 212, 262, 10, 122, 188, 157, 281, 26, 216, 197, 272, 311, 9, 163, 174, 22, 27, 2, 137, 276, 156, 284]. Note that most of these methods evaluate not only input events but pairs of input and output (action) events.\nIn one of the simplest cases, C is just a linear perceptron FNN (instead of an RNN like in the early system [227]). The fact that C has no built-in memory in this case is not a fundamental restriction since M is recurrent, and has been trained to predict not only normal sensory inputs, but also reward signals. That is, the state of M must contain all the historic information relevant to maximize future expected reward, provided the data history so far already contains the relevant experience, and M has learned to compactly extract and represent its regular aspects.\nThis approach is different from other, previous combinations of traditional RL [272][245, Sec. 6.2] and RNNs [227, 148, 12] which use RNNs only as value function approximators that directly predict cumulative expected reward, instead of trying to predict all sensations time step by time step. TheCM system in the present section separates the hard task of prediction in partially observable environments from the comparatively simple task of RL under the Markovian assumption that the current input to C (which is M \u2019s state) contains all information relevant for achieving the goal."}, {"heading": "5.2 C as an Evolutionary RL (R)NN whose Inputs are M \u2019s Activations", "text": "This approach is essentially the same as the one of Sec. 5.1, except that C is now an FNN or RNN trained by evolutionary algorithms [200, 255, 105, 56, 68] applied to NNs [165, 321, 180, 259, 72, 90, 89, 110, 94], or by policy gradient methods [314, 315, 316, 274, 18, 1, 63, 128, 313, 210, 192, 191, 256, 85, 312, 190, 82, 93][245, Sec. 6.6], or by Compressed NN Search; see Sec. 1. C has 2m+ 2n+h input units and o output units. At time t, state(t) is fed into C, which computes out(t); then M computes hidden(t+ 1) and pred(t+ 1); then out(t) is executed to obtain sense(t+ 1)."}, {"heading": "5.3 C Learns to Think with M : High-Level Plans and Abstractions", "text": "Our RNN-based CM systems of the early 1990s [223, 226](Sec. 1.3.1) could in principle plan ahead by performing numerous fast mental experiments on a predictive RNN world model, M , instead of time-consuming real experiments, extending earlier work on reactive systems without memory [301, 273]. However, this can work well only in (near-)deterministic environments, and, even there, M\nwould have to simulate many entire alternative futures, time step by time step, to find an action sequence for C that maximizes reward. This method seems very different from the much smarter hierarchical planning methods of humans, who apparently can learn to identify and exploit a few relevant problem-specific abstractions of possible future events; reasoning abstractly, and efficiently ignoring irrelevant spatio-temporal details.\nWe now describe a CM system that can in principle learn to plan and reason like this as well, according to the AIT argument (Sec. 2.1). This should be viewed as a main contribution of the present paper. See Figure 1.\nConsider an RNN C (with typically rather small feasible search space) as in Sec. 5.2. We add standard and/or multiplicative learnable connections (Sec. 3.1) from some of the units of C to some of the units of the typically huge unsupervised M , and from some of the units of M to some of the units of C. The new connections are said to belong to C. C and M now collectively form a new RNN called CM , with standard activation spreading as in Sec. 3.1. The activations of M are initialized to default values at the beginning of each trial. Now CM is trained on RL tasks in line with step 3 of algorithm 1, using search methods such as those of Sec. 5.2 (compare Sec. 1). The (typically many) connections of M , however, do not change\u2014only the (typically relatively few) connections of C do.\nWhat does that mean? It means that now C\u2019s relatively small candidate programs are given time to \u201cthink\u201d by feeding sequences of activations into M , and reading activations out of M , before and while interacting with the environment. Since C and M are general computers, C\u2019s programs may query, edit or invoke subprograms of M in arbitrary, computable ways through the new connections. Given some RL problem, according to the AIT argument (Sec. 2.1), this can greatly accelerate C\u2019s search for a problem-solving weight vector w\u0302, provided the (time-bounded [147]) mutual algorithmic information between w\u0302 and M \u2019s program is high, as is to be expected in many cases since M \u2019s environment-modeling program should reflect many regularities useful not only for prediction and coding, but also for decision making.3\nThis simple but novel approach is much more general than previous computable, but restricted, ways of letting a feedforward C use a model M (Sec. 1.3.1)[301, 273][245, Sec. 6.1], by simulating entire possible futures step by step, then propagating error signals or temporal difference errors backwards (see Section 1.3.1). Instead, we giveC\u2019s program search an opportunity to discover sophisticated computable ways of exploiting M \u2019s code, such as abstract hierarchical planning and analogybased reasoning. For example, to represent previous observations, an M implemented as an LSTM network (Sec. 1.2) will develop high-level, abstract, spatio-temporal feature detectors that may be active for thousands of time steps, as long as those memories are useful to predict (and thus compress) future observations [62, 61, 189, 79]. However, C may learn to directly invoke the corresponding \u201cabstract\u201d units in M by inserting appropriate pattern sequences into M . C might then short-cut from there to typical subsequent abstract representations, ignoring the long input sequences normally required to invoke them in M , thus quickly anticipating a few possible positive outcomes to be pursued (plus computable ways of achieving them), or negative outcomes to be avoided.\nNote that M (and by extension M ) does not at all have to be a perfect predictor. For example, it won\u2019t be able to predict noise. InsteadM will have learned to approximate conditional expectations of future inputs, given the history so far. A naive way of exploiting M \u2019s probabilistic knowledge would be to plan ahead through naive step-by-step Monte-Carlo simulations of possibleM -predicted futures, to find and execute action sequences that maximize expected reward predicted by those simulations. However, we won\u2019t limit the system to this naive approach. Instead it will be the task of C to learn to address useful problem-specific parts of the current M , and reuse them for problem solving. Sure,\n3 An alternative way of letting C learn to access the program of M is to add C-owned connections from the weights of M to units of C, treating the current weights of M as additional real-valued inputs to C. This, however, will typically result in a much larger search space for C. There are many other variants of the general scheme described in Sec. 2.2.\nC will have to intelligently exploit M , which will cost bits of information (and thus search time for appropriate weight changes of C), but this is often still much cheaper in the AIT sense than learning a good C program from scratch, as in our previous non-RNN AIT-based work on algorithmic transfer learning [238], where self-invented recursive code for previous solutions sped up the search for code for more complex tasks by a factor of 1000.\nNumerous topologies are possible for the adaptive connections from C to M , and back. Although in some applications C may find it hard to exploit M , and might prefer to ignore M (by setting connections to and from M to zero), in some environments under certain CM topologies, C can greatly profit from M .\nWhile M \u2019s weights are frozen in step 3 of algorithm 1, the weights of C can learn when to make C attend to history information represented by M \u2019s state, and when to ignore such information, and instead use M \u2019s innards in other computable ways. This can be further facilitated by introducing a special unit, u\u0302, to C, where u\u0302(t)all(t) instead of all(t) is fed into M at time t, such that C can easily (by setting u\u0302(t) = 0) force M to completely ignore environmental inputs, to use M for \u201cthinking\u201d in other ways.\nShouldM later grow (or shrink) in step 4 of algorithm 1, in line with Sec. 4.2, C may in turn grow additional connections to and from M (or lose some) in the next incarnation of step 3."}, {"heading": "5.4 Incremental / Hierarchical / Multitask Learning of C with M", "text": "A variant of the approach in Sec. 5.3 incrementally trains C on a never-ending series of tasks, continually building on solutions to previous problems, instead of learning each new problem from scratch. In principle, this can be done through incremental NN evolution [70], hierarchical NN evolution [306, 285], hierarchical Policy Gradient algorithms [63], or asymptotically optimal ways of algorithmic transfer learning [238].\nGiven a new task and aC trained on several previous tasks, such hierarchical/incremental methods may freeze the current weights of C, then enlarge C by adding new units and connections which are trained on the new task. This process reduces the size of the search space for the new task, giving the new weights the opportunity to learn to use the frozen parts of C as subprograms.\nIncremental variants of Compressed RNN Search [132] (Sec. 1) do not directly search in C\u2019s potentially large weight space, but in the frequency domain by representing the weight matrix as a small set of Fourier-type coefficients. By searching for new coefficients to be added to already learned set responsible for solving previous problems, C\u2019s weight matrix is fine tuned incrementally and indirectly (through superpositions). Given a current problem, in AIT-based OOPS style [238], we may impose growing run time limits on programs tested on C, until a solution is found."}, {"heading": "6 Exploration: Rewarding C for Experiments that Improve M", "text": "Humans, even as infants, invent their own tasks in a curious and creative fashion, continually increasing their problem solving repertoire even without an external reward or teacher. They seem to get intrinsic reward for creating experiments leading to observations that obey a previously unknown law that allows for better compression of the observations\u2014corresponding to the discovery of a temporarily interesting, subjectively novel regularity [224, 239, 241] (compare also [261, 184]).\nFor example, a video of 100 falling apples can be greatly compressed via predictive coding once the law of gravity is discovered. Likewise, the video-like image sequence perceived while moving through an office can be greatly compressed by constructing an internal 3D model of the office space [243]. The 3D model allows for re-computing the entire high-resolution video from a compact\nsequence of very low-dimensional eye coordinates and eye directions. The model itself can be specified by far fewer bits of information than needed to store the raw pixel data of a long video. Even if the 3D model is not precise, only relatively few extra bits will be required to encode the observed deviations from the predictions of the model.\nEven mirror neurons [129] are easily explained as by-products of history compression as in Sec. 3 and 4. They fire both when an animal acts and when the animal observes the same action performed by another. Due to mutual algorithmic information shared by perceptions of similar actions performed by various animals, efficient RNN-based predictive coding (Sec. 3, 4) profits from using the same feature detectors (neurons) to encode the shared information, thus saving storage space.\nGiven the C-M combinations of Sec. 5, we motivate C to become an efficient explorer and an artificial scientist, by adding to its standard external reward (or fitness) for solving user-given tasks another intrinsic reward for generating novel action sequences (= experiments) that allow M to improve its compression performance on the resulting data [239, 241].\nAt first glance, repeatedly evaluating M \u2019s compression performance on the entire history seems impractical. A heuristic to overcome this is to focus on M \u2019s improvements on the most recent trial, while regularly re-training M on randomly selected previous trials, to avoid catastrophic forgetting.\nA related problem is that C\u2019s incremental program search may find it difficult to identify (and assign credit to) those parts of C responsible for improvements of a huge, black box-like, monolithic M . But we can implement M as a self-modularizing, computation cost-minimizing, winner-take-all RNN [221, 242, 267]. Then it is possible to keep track of which parts of M are used to encode which parts of the history. That is, to evaluate weight changes of M , only the affected parts of the stored history have to be re-tested [243]. Then C\u2019s search can be facilitated by tracking which parts of C affected those parts of M . By penalizing C\u2019s programs for the time consumed by such tests, the search for C is biased to prefer programs that conduct experiments causing data yielding quickly verifiable compression progress of M . That is, the program search will prefer to change weights of M that are not used to compress large parts of the history that are expensive to verify [242, 243]. The first implementations of this simple principle were described in our work on the POWERPLAY framework [243, 267], which incrementally searches the space of possible pairs of new tasks and modifications of the current program, until it finds a more powerful program that, unlike the unmodified program, solves all previously learned tasks plus the new one, or simplifies/compresses/speeds up previous solutions, without forgetting any. Under certain conditions this can accelerate the acquisition of external reward specified by user-defined tasks."}, {"heading": "7 Conclusion", "text": "We introduced novel combinations of a reinforcement learning (RL) controller, C, and an RNN-based predictive world model,M . The most generalCM systems implement principles of algorithmic [263, 130, 147] as opposed to traditional [24, 257] information theory. Here both M and C are RNNs or RNN-like systems. M is actively exploited in arbitrary computable ways byC, whose program search space is typically much smaller, and which may learn to selectively probe and reuse M \u2019s internal programs to plan and reason. The basic principles are not limited to RL, but apply to all kinds of active algorithmic transfer learning from one RNN to another. By combining gradient-based RNNs and RL RNNs, we create a qualitatively new type of self-improving, general purpose, connectionist control architecture. This RNNAI may continually build upon previously acquired problem solving procedures, some of them self-invented in a way that resembles a scientist\u2019s search for novel data with unknown regularities, preferring still-unsolved but quickly learnable tasks over others."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>This paper addresses the general problem of reinforcement learning (RL) in partially observable<lb>environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to<lb>drive simulated cars from high-dimensional video input. However, real brains are more powerful<lb>in many ways. In particular, they learn a predictive model of their initially unknown environment,<lb>and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic<lb>information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an<lb>RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user,<lb>others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world<lb>model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the<lb>RNNAI learns to actively query its model for abstract reasoning and planning and decision making,<lb>essentially \u201clearning to think.\u201d The basic ideas of this report can be applied to many other cases<lb>where one RNN-like system exploits the algorithmic information content of another. They are<lb>taken from a grant proposal submitted in Fall 2014, and also explain concepts such as \u201cmirror<lb>neurons.\u201d Experimental results will be described in separate papers. 1<lb>ar<lb>X<lb>iv<lb>:1<lb>51<lb>1.<lb>09<lb>24<lb>9v<lb>1<lb>[<lb>cs<lb>.A<lb>I]<lb>3<lb>0<lb>N<lb>ov<lb>2<lb>01<lb>5", "creator": "LaTeX with hyperref package"}}}