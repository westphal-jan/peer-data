{"id": "1512.00659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2015", "title": "Centroid Based Binary Tree Structured SVM for Multi Classification", "abstract": "Support Vector Machines (SVMs) are designed primarily for 2-class classification, but have also been extended to N-class classification, based on the requirement of multi-glasses in practical use. Although N-class classification via SVM enjoys considerable research attention, achieving a minimum number of classifiers at the time of training and testing is still an ongoing research. We propose a new algorithm CBTS-SVM (Centroid based Binary Tree Structured SVM) that addresses this problem. We are building a binary tree of SVM models based on the similarity of class labels by determining their distance to the corresponding centrifuges at the root level. Experimental results show the comparable accuracy for CBTS with OVO at reasonable gamma and cost values. On the other hand, when CBTS is compared with OVA, there is better accuracy at reduced test time and is also available in calibration time.", "histories": [["v1", "Wed, 2 Dec 2015 11:48:38 GMT  (123kb,D)", "http://arxiv.org/abs/1512.00659v1", "Presented in ICACCI, Kochi, India, 2015"]], "COMMENTS": "Presented in ICACCI, Kochi, India, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aruna govada", "bhavul gauri", "s k sahay"], "accepted": false, "id": "1512.00659"}, "pdf": {"name": "1512.00659.pdf", "metadata": {"source": "CRF", "title": "Centroid Based Binary Tree Structured SVM for Multi Classification", "authors": ["Aruna Govada"], "emails": ["garuna@goa.bits-pilani.ac.in,", "bhavul93@gmail.com,", "ssahay@goa.bits-pilani.ac.in"], "sections": [{"heading": null, "text": "Keywords-K-Means Clustering / Centroid based clustering; SVM; Multi-Classification; Binary Tree;\nI. INTRODUCTION\nA Support Vector Machine (SVM) is a distinguishing classifier conventionally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which classifies the unseen examples[1-2]. SVM is not limited statistics or machine learning but can be applied in wide number of applications. SVM proved to be the best classifier in different applications from handwritten digit recognition to text categorization. SVM doesn\u2019t have any effect on classification due to the curse of dimensionality. It works well with high dimensional data.\nLiterature survey witnesses that the Support vector machines are the best classifiers for 2-class classification problems. The real time applications are not limited to binary classification but multiclass classification[3]. For example, to classify an astronomical object as a star, a galaxy or a quasar requires a multi-class classifier but not binary classifier.\nThere are two major approaches in solving the N-class problem one is a single large optimization problem [4- 5], and the alternative is to decompose N-class problem into multiple 2-class problems. But solving a single large optimization problem will be expensive in terms of computational time and not suitable for practical applications. There are several algorithms based on the second approach like OVO(one-versus-one),OVA(one-versus-\nall), DAG(Directed Acyclic Graph)[6-7]. In this paper we propose a new algorithm which decomposes the single Nclass problem into multiple 2-class problems. And it requires less number of binary classifiers when compared with the above mentioned algorithms and gives a better accuracy too.\nThe rest of the paper is organized as follows. The related work is discussed in section 2.The proposed approach is presented in section 3. Section 4 contains the experimental results and comparison of the proposed approach with the existing algorithms. Finally, section 5 contains Conclusion and future work."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Binary SVM", "text": "The learning task in binary SVM can be represented as the following\nminw = \u2016 w \u20162\n2\nsubject to yi(w.xi + b) \u2265 1, i = 1, 2, ....k where w and b are the parameters of the model for total k number of instances.\nUsing Lagrange multiplier method the following equation is to be solved,\nLp = \u2016 w \u20162\n2 \u2212 \u2211 i=1...k \u03bbi(yi(w.xi + b)\u2212 1)\nThe dual version of the above problem is\nLD = \u2211\ni=1...k\n\u03bbi \u2212 1\n2 \u2211 i,j \u03bbi\u03bbjyiyjxi.xj\nsubject to \u03bbi \u2265 0\n\u03bbi(yi(w.xi + b)\u2212 1) = 0\nwhere \u03bbi are known as the Lagrange multipliers. By solving this dual problem, SVM will be found. Once the SVM model is built,the class label of a testing object z can be predicted as follows.\nf(z) = sign \u2211\ni=1...k\n(\u03bbiyixi.z + b)\nif f(z) \u2265 0 z will be predicted as + class else -ve class.\nar X\niv :1\n51 2.\n00 65\n9v 1\n[ cs\n.L G\n] 2\nD ec\n2 01\n5"}, {"heading": "B. Multi-Class SVM", "text": "1) One versus All: The simple approach is to decompose the problem of classifying N classes into N binary problems, where each problem differentiates a given class from the other N-1 classes [8]. In this approach, we require K=N binary classifiers, where the Nth classifier is trained with positive examples belonging to class N and negative examples belonging to the other N-1 classes. When an unknown example is to be predicted, the classifier achieving the maximum output is considered as the best choice, and the corresponding class label is assigned to that test object. Though this approach is simple [8], it provides performance that is comparable to other more complicated approaches when the binary classifier is tuned well.\n2) One versus One: In this approach, each class is compared to every other class [9-10]. A binary classifier is built to differentiate between each pair of classes, while discarding the rest of the classes. This requires building N(N-1)/2 binary classifiers. When testing a new object, a voting is performed among the classifiers and the class with the maximum number of votes will be considered as the best choice. Results [6,11]show that this approach is in general better than the one-versus-all approach."}, {"heading": "III. THE PROPOSED APPROACH", "text": "The N-class problem is decomposed into multiple 2- class problems in a binary tree structured manner. K-Means clustering is used as a preprocessing step to get the rough estimation of similarity between the class labels. This will let us divide the class labels into two disjoint sets and build the SVM for the root node. Thereafter every node is divided at the mid point for creating disjoint sets. The order of the class labels is computed based on the SSE. The least SSE will be first in the list and the highest SSE will appear last in the list. This way (n-1) binary SVMs will be built, and hence needs only (n-1)/2 SVMs evaluation to classify the unclassified record. This is better than the worst case (n1) of OVA and n(n-1)/2 of OVO. At the same time, our experimental results show that the accuracy is comparable with OVO. But when CBTS is compared with OVA it shows a better accuracy with reduced training time and testing time. The algorithm for training and testing is illustrated below."}, {"heading": "A. The Training model of CBTS SVM", "text": "Input the training objects. Add all the training objects to the root node. Let the class labels are from 1....N Preprocessing Step: Divide the training objects i.e.the root node into two clusters/nodes (IL) and (IR) using K-Means clustering (Centroid based).\n1) The objects will be adjusted to (IL) as positive class or (IR) as negative class based on the majority of their class labels from the two clusters.\n2) For (IL) and (IR) calculate the SSE of all objects based on the same class labels and sort them in the ascending order. The SSE is given by SSE = \u2211 i=1...k(xi, C)\n3) For both (IL) and (IR) Repeat\na) If the number of class labels of the node are two, construct the binary classifier and return. If the number of class labels are more than two\nb) Divide the each node exactly at the mid point, construct the binary SVM and repeat this till we reach only two class labels."}, {"heading": "B. The Testing model of CBTS SVM", "text": "1) The test object should be evaluated on the root node the binary tree of SVMs.\n2) Repeat: \u2022 If the value is positive traverse to the left node\n(IL) else to the right node (IR).\n\u2022 Until we reach the leaf node.\n3) Classify the test object into the class label of leaf node.\nLet\u2019s run through an example. Consider figure 1, suppose if we have 8 classes, we first run k-means clustering with k=2 to divide the data objects according to their distribution. Then, through the cluster distribution and based on the majority we get to know which class labels fall on one side (positive) and which ones on the other side (negative). In the example shown, set IL {1,3,7} belong to the positive class and set of rest of class labels, IR {4,6,8,2,5} belong to the negative class. The order of class labels within the node will be in the ascending order of SSE. We then build a SVM model by constructing a binary SVM between data objects belonging to IL as belonging to positive class and those belonging to IR as negative class.\nNow, for the left child of the root node, sample space is all the data objects that belong to class labels in IL i.e. {1,3,7}. We divide exactly at the mid point (Remember, clustering is done at the first step only!) and hence make new IL and IR sets for this node. So, new IL is {1,3} and IR is {7}. We now consider all data objects having class labels in IL as having positive class and all in IR as negative class and build a SVM model. This SVM model acts as left child of the root node.\nThis way we build the whole Binary Tree of SVM functions recursively for both left and right nodes till we reach a leaf node. When an unseen object has to be classified, the search starts from the root node and then it moves on\nto the left or right based on the evaluation function value recursively till the leaf node and assigns the corresponding class label to the test object.\nIn Figure 2 ,the hyperplanes are shown for different class labels based on the above approach. The order of the construction of the hyperplanes will be based on the binary tree structure."}, {"heading": "IV. EXPERIMENTAL ANALYSIS", "text": "We implemented the algorithm on twelve classification data sets and compared the training time, testing time and accuracy with OVO and OVA. Glass, Iris, Letter, Mfeat-fac, Mfeat-fou, Mfeat-kar and Mfeat-mor, Pendigits, Satimage, Segment, Shuttle, Vowel are UCI data sets and taken from https://archive.ics.uci.edu/ml/datasets.html. The detailed properties of the data sets are given in Table I. For Letter, Shuttle and Satimage dataset, a separate training and testing file was used. But, for the rest of all the data sets 2/3 part of data is considered for training and 1/3 is considered\nfor testing. All the datasets were scaled to [0,1] and have been randomized (similar class labels data will not appear together). In our implementation, we make use of Radial Basis Function(RBF) Kernel and the range of value of \u03b3 \u2208 2\u221210 to 24 , C \u2208 2\u22122 to 212. RBF kernel works best for any kind of problem [16]. In all the three approaches OVO, OVA, CBTS the best combination of \u03b3 and C is chosen from the above mentioned range so as to provide the higher accuracy and lower testing and training time [17]. All the computations are done on a computer with 1.60GHz Intel i5-4200U Dual core processor and RAM of 6GB and using the software LIBSVM [17].\nIn Table II, the training time and testing time of CBTS is compared with OVO and OVA. CBTS performs very well when compared with OVA. Though the training time and testing time of CBTS is little more than OVO , \u03b3 and Cost values are justified to have a good model.\nIn Table III, the results show that the accuracy of all twelve datasets by CBTS are comparable with OVO with less cost C. A higher value of C means to choose more samples as support vectors. But limiting the support vectors i.e.limiting the value of C will use the minimum memory possible and the prediction will be faster. Even when the \u03b3 is compared, CBTS is choosing the intermediate values as for OVO it is too small. For a good model \u03b3 can\u2019t be too small or too large. If \u03b3 is too small the model is restricted and cannot capture the complexity or shape of the data. If \u03b3 is too large there is a possibility that model will become over fit the data [18]. Coming to the comparison with OVA, CBTS gives a better accuracy.\nIn Table IV, the number of binary classifiers required for one classification is shown for OVO, OVA and CBTS. If we observe CBTS requires the minimum number of binary classifiers when compared with OVO and OVA.\nIn addition to above data sets we analyzed the Sloan Digital Sky Survey (SDSS) data set also. SDSS is a major multi-filter imaging and spectroscopic redshift survey using a dedicated 2.5-m wide-angle optical telescope at Apache Point Observatory in New Mexico, United States, astronomical telescope survey. It has 6 class labels and only 5 features (u,g,r,i,z) are considered (it has many). The data can be downloaded from SQL interface on http://skyserver.sdss.org/dr7/en/tools/search/sql.asp\nThe results are shown in table V for SDSS data set. CBTS outperforms both OVA and OVO in the aspects of accuracy and training time with the best Gamma and Cost values. The size of the instances are varied from 30,000 to 75,000. OVA couldn\u2019t build the binary classifiers when the data size is 75,000 because of scalability issue whereas OVO and CBTS could build the required models with out any problem. Hence CBTS is scalable. And it gives a better accuracy with reduced training time."}, {"heading": "V. CONCLUSION", "text": "We propose a new algorithm CBTS (Centroid based Binary Tree Structured SVM) , a binary tree structure in which the root node contains all the class labels and is splitted based on the K-means Clustering at the first step. And then the left and right nodes are recursively splitted into half till we have only two class labels. In this N1 SVMs are required to construct but the required SVMs in OVO,OVA are N(N-1)/2 , N respectively. The testing\ntime for all OVO, OVA, DAG is linear on N i.e. O(N) but for our algorithm it is O(log N) as it needs only (N1)/2 classifiers to predict the class label for the test object. Experimental results show that the accuracy of CBTS is comparable with OVO approach and it out performs with OVA both in accuracy and training,testing time. CBTS also capable of handling large data sets, hence scalable. By using a more optimized approach of implementing CBTS, testing time may further be reduced. Instead of K-Means clustering alternative partition techniques can also be explored at the root level. Furthermore, a parallel/distributed version of our algorithm can be taken up as a future work. In our algorithm there are independent SVM models that can be constructed\nsimultaneously which results in reducing both training and testing time of CBTS even further."}, {"heading": "ACKNOWLEDGMENT", "text": "We are thankful for the resources provided by the Department of Computer Science and Information Systems, BITS, Pilani, K.K. Birla Goa Campus to carry out the experimental analysis."}], "references": [{"title": "Support Vector network. Machine Learning", "author": ["C.Cortes", "V. Vapnik"], "venue": "Volume 20, Issue", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "NY: Springer-Verlag", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "A comparison of Methods for Multi Class Support Vector Machines", "author": ["Chih-Wei Hsu", "Chih-jen Lin"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Analysis of Multi Support Vector Machines", "author": ["Abe", "Shigeo"], "venue": "Proc. International Conference on Computational Intelligence for Modelling Control and Automation (CIMCA2003),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["Erin Allwein", "Robert Shapire", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Comparison of classifier methods : a case study in handwriting digit recognition", "author": ["L.Bottou", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "L. Jackel", "Y. LeCun", "U. Muller", "E. Sackinger", "P. Simard", "V. Vapnik"], "venue": "In International Conference on Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Single-layer learning revisited: a stepwise procedure for building and training a neural network. Neurocomputing: Algorithms", "author": ["S. Knerr", "L. Personnaz", "G. Dreyfus"], "venue": "Architectures and Applications. Springer-Verlag,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Pairwise classification and support vector machines", "author": ["U. Krebel"], "venue": "Advances in Kernel Methods Support Vector learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "LIBSVM : a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "API design for machine learning software: experiences from the scikit-learn project", "author": ["Buitinck"], "venue": "Available at http : //scikit \u2212 learn.org/stable/autoexamples/svm/plotrbfparameters", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which classifies the unseen examples[1-2].", "startOffset": 147, "endOffset": 152}, {"referenceID": 1, "context": "In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which classifies the unseen examples[1-2].", "startOffset": 147, "endOffset": 152}, {"referenceID": 2, "context": "There are several algorithms based on the second approach like OVO(one-versus-one),OVA(one-versusall), DAG(Directed Acyclic Graph)[6-7].", "startOffset": 130, "endOffset": 135}, {"referenceID": 3, "context": "There are several algorithms based on the second approach like OVO(one-versus-one),OVA(one-versusall), DAG(Directed Acyclic Graph)[6-7].", "startOffset": 130, "endOffset": 135}, {"referenceID": 4, "context": "2) One versus One: In this approach, each class is compared to every other class [9-10].", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "Results [6,11]show that this approach is in general better than the one-versus-all approach.", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "Results [6,11]show that this approach is in general better than the one-versus-all approach.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "All the datasets were scaled to [0,1] and have been randomized (similar class labels data will not appear together).", "startOffset": 32, "endOffset": 37}, {"referenceID": 9, "context": "In all the three approaches OVO, OVA, CBTS the best combination of \u03b3 and C is chosen from the above mentioned range so as to provide the higher accuracy and lower testing and training time [17].", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "60GHz Intel i5-4200U Dual core processor and RAM of 6GB and using the software LIBSVM [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "If \u03b3 is too large there is a possibility that model will become over fit the data [18].", "startOffset": 82, "endOffset": 86}], "year": 2015, "abstractText": "Support Vector Machines (SVMs) were primarily designed for 2-class classification. But they have been extended for N-class classification also based on the requirement of multiclasses in the practical applications. Although N-class classification using SVM has considerable research attention, getting minimum number of classifiers at the time of training and testing is still a continuing research. We propose a new algorithm CBTS-SVM (Centroid based Binary Tree Structured SVM) which addresses this issue. In this we build a binary tree of SVM models based on the similarity of the class labels by finding their distance from the corresponding centroids at the root level. The experimental results demonstrates the comparable accuracy for CBTS with OVO with reasonable gamma and cost values. On the other hand when CBTS is compared with OVA, it gives the better accuracy with reduced training time and testing time. Furthermore CBTS is also scalable as it is able to handle the large data sets. Keywords-K-Means Clustering / Centroid based clustering; SVM; Multi-Classification; Binary Tree;", "creator": "LaTeX with hyperref package"}}}