{"id": "1512.02167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Simple Baseline for Visual Question Answering", "abstract": "We describe a very simple wordbag baseline for the visual answer to questions. This baseline concatenates the word characteristics from the question and CNN characteristics from the image to predict the answer. If evaluated on the challenging VQA dataset [2], it shows a performance comparable to many newer approaches that use recurrent neural networks. To explore the strength and weakness of the trained model, we also offer an interactive web demo and open source code.", "histories": [["v1", "Mon, 7 Dec 2015 19:00:54 GMT  (376kb,D)", "http://arxiv.org/abs/1512.02167v1", null], ["v2", "Tue, 15 Dec 2015 05:17:49 GMT  (523kb,D)", "http://arxiv.org/abs/1512.02167v2", "One comparison method's scores are put into the correct column, and a new experiment of generating attention map is added"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["bolei zhou", "yuandong tian", "sainbayar sukhbaatar", "arthur szlam", "rob fergus"], "accepted": false, "id": "1512.02167"}, "pdf": {"name": "1512.02167.pdf", "metadata": {"source": "CRF", "title": "Simple Baseline for Visual Question Answering", "authors": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Combining Natural Language Processing with Computer Vision for high-level scene interpretation is a recent trend, e.g., image captioning [10, 15, 7, 4]. These works have benefited from the rapid development of deep learning for visual recognition (object recognition [8] and scene recognition [19]), and have been made possible by the emergence of large image datasets and text corpus (e.g., [9]). Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].\nCompared with the image captioning task, in which an algorithm is required to generate free-form text description for a given image, visual QA can involve a wider range of knowledge and reasoning skills. A captioning algorithm has the liberty to pick the easiest relevant descriptions of the image, whereas as responding to a question needs to find the correct answer for *that* question. Furthermore, the algorithms for visual QA are required to answer all kinds of questions people might ask about the image, some of which might be relevant to the image contents, such as \u201cwhat books are under the television\u201d and \u201cwhat is the color of the boat\u201d, while others might require knowledge or reasoning beyond the image content, such as \u201cwhy is the baby crying?\u201d and \u201cwhich chair is the most expensive?\u201d. Building robust algorithms for visual QA that perform at near human levels would be an important step towards solving AI.\nRecently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1]. Some of them are derived from the image captioning framework, in which the output of a recurrent neural network, (e.g., LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer. Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.\nInterestingly, we notice that in one of the earliest VQA papers [12], the simple baseline Bag-ofwords + image feature (referred to as BOWIMG baseline) outperforms the LSTM-based models on a synthesized visual QA dataset built up on top of the image captions of COCO dataset [9]. For the recent much larger COCO VQA dataset [2], the BOWIMG baseline performs worse than the LSTM-based models [2].\n1http://visualqa.csail.mit.edu 2https://github.com/metalbubble/VQAbaseline\nar X\niv :1\n51 2.\n02 16\n7v 1\n[ cs\n.C V\n] 7\nD ec\n2 01\nIn this work, we carefully implement the BOWIMG baseline model. We call it iBOWIMG to avoid confusion with the implementation in [2]. With proper setup and training, this simple baseline model shows comparable performance to many recent recurrent network-based approaches for visual QA. Further analysis shows that the baseline learns to correlate the informative words in the question sentence and visual concepts in the image with the answer. The source code and the visual QA demo based on the trained model are publicly available. In the demo, iBOWIMG baseline gives answers to any question relevant to the given images. Playing with the visual QA models interactively could reveal the strengths and weakness of the trained model."}, {"heading": "2 iBOWIMG for Visual Question Answering", "text": "In most of the recent proposed models, visual QA is simplified to a classification task: the number of the different answers in the training set is the number of the final classes the models need to learn to predict. The general pipeline of those models is that the word feature extracted from the question sentence is concatenated with the visual feature extracted from the image, then they are fed into a softmax layer to predict the answer class. The visual feature is usually taken from the top of the VGG network or GoogLeNet, while the word features of the question sentence are usually the popular LSTM-based features [12, 2].\nIn our iBOWIMG model, we simply use naive bag-of-words as the text feature, and use the deep features from GoogLeNet [14] as the visual features. Figure 1 shows the framework of the iBOWIMG model, which can be implemented in Torch with no more than 10 lines of code. The input question is first converted to a one-hot vector, which is transformed to word feature via a word embedding layer and then is concatenated with the image feature from CNN. The combined feature is sent to the softmax layer to predict the answer class, which essentially is a multi-class logistic regression model."}, {"heading": "3 Experiments", "text": "Here we train and evaluate the iBOWIMG model on the Full release of COCO VQA dataset [2], the largest VQA dataset so far. In the COCO VQA dataset, there are 3 questions annotated by Amazon Mechanical Turk (AMT) workers for each image in the COCO dataset. For each question, there are 10 answers annotated by another batch of AMT workers. To pre-process the annotation for training, we perform majority voting on the 10 ground-truth answers to get the most certain answer for each question. Here the answer could be in single word or multiple words. Then we have the 3 questionanswer pairs from each image for training. There are in total 248,349 pairs in train2014 and 121,512\npairs in val2014, for 123,287 images overall in the training set. Here train2014 and val2014 are the standard splits of the image set in the COCO dataset.\nTo generate the training set and validation set for our model, we first randomly split the images of COCO val2014 into 70% subset A and 30% subset B. To avoid potential overfitting, questions sharing the same image will be placed into the same split. The question-answer pairs from the images of COCO train2014 + val2014 subset A are combined and used for training, while the val2014 subset B is used as validation set for parameter tuning. After we find the best model parameters, we combine the whole train2014 and val2014 to train the final model. We submit the prediction result given by the final model on the testing set (COCO test2015) to the evaluation server, to get the final accuracy on the test-dev and test-standard set."}, {"heading": "3.1 Benchmark Performance", "text": "According to the evaluation standard of the VQA dataset, the result of the any proposed VQA models should report accuracy on the test-standard set for fair comparison. We report our baseline on the test-dev set in Table 1 and the test-standard set in Table 2. The test-dev set is used for debugging and validation experiments and allows for unlimited submission to the evaluation server, while teststandard is used for model comparison with limited submission times.\nSince this VQA dataset is rather new, the publicly available models evaluated on the dataset are all from non-peer reviewed arXiv papers. We include the performance of the models available at the time of writing (Dec.5, 2015) [2, 6, 1, 13, 16, 11]. Note that some models are evaluated on either test-dev or test-standard for either Open-Ended or Multiple-Choice track.\nThe full set of the VQA dataset was released on Oct.6 2015; previously the v0.1 version and v0.9 version had been released. We notice that some models are evaluated using non-standard setups, rendering performance comparisons difficult. [17] (arXiv dated at Nov.17 2015) used v0.9 version of VQA with their own split of training and testing; [18] (arXiv dated at Nov.7 2015) used their own split of training and testing for the val2014; [3] (arXiv dated at Nov.18 2015) used v0.9 version of VQA dataset. So these are not included in the comparison.\nExcept for these IMG, BOW, BOWIMG baselines provided in the [2], all the compared methods use either deep or recursive neural networks. However, our iBOWIMG baseline performs comparably to these much more complex models, except for DPPnet [11] that is about 1.5% better."}, {"heading": "3.2 Training Details", "text": "Learning rate and weight clip. We find that setting up a different learning rate and weight clipping for the word embedding layer and softmax layer leads to better performance. The learning rate for the word embedding layer should be much higher than the learning rate of softmax layer to learn a good word embedding. From the performance of BOW in Table 1, we can see that a good word model is crucial to the accuracy, as BOW model alone could achieve closely to 48%, even without looking at the image content.\nModel parameters to tune. Though our model could be considered as the simplest baseline so far for visual QA, there are several model parameters to tune: 1) the number of epochs to train. 2) the learning rate and weight clip. 3) the threshold for removing less frequent question word and answer classes. We iterate to search the best value of each model parameter separately on the val2014 subset B. In our best model, there are 5,746 words in the dictionary of question sentence, 5,216 classes of answers. The specific model parameters could be seen in the source code."}, {"heading": "3.3 Understanding the Visual QA model", "text": "From the comparisons above, we can see that our baseline model performs as well as the recurrent neural network models on the VQA dataset. Furthermore, thanks to the simplicity of the model, we are able to interpret its behavior easily, in an attempt to see what it learned for visual QA.\nEssentially, the BOWIMG baseline model learns to memorize the correlation between the answer class and the informative words in the question sentence along with the visual feature. We split the learned weights of softmax into two parts, one part for the word feature and the other part for the visual feature, thus\nr = Mwxw + Mvxv. (1)\nHere the softmax matrix M is decomposed into the weights Mw for word feature xw and the weights Mv for the visual feature xv whereas M = [Mw,Mv]. r is the response of the answer class before softmax normalization. Then we denote the response rw = Mwxw as the contribution from question words and rv = Mvxv as the contribution from the image contents. Thus for each predicted answer, we could know exactly the proportions of contribution from word and image content respectively. We also could rank rw and rv to know what the predicted answer could be if the model only relies on one side of information.\nFigure 2 shows some examples of the predictions, revealing that the question words usually have dominant influence on predicting the answer. For example, the correctly predicted answers for the two questions given for the first image \u2018what is the color of sofa\u2019 and \u2018which brand is the laptop\u2019 come mostly from the question words, without the need for image. Another example is the question \u2018what are they doing\u2019 for the second image: if it purely replies on the words, the predictions are \u2018playing wii (10.62), eating (9.97), playing frisbee (9.24)\u2019, to combine the information of image and question words, the baseline reaches the correct prediction \u2018playing baseball (10.67 = 2.01 [image] + 8.66 [word])\u2019. It results from the bias in the frequency of object and actions appearing in the images of COCO dataset."}, {"heading": "4 Interactive Visual QA Demo", "text": "Question answering is essentially an interactive activity, thus it would be good to make the trained models able to interact with people in real time. Aided by the simplicity of the baseline model, we built a web demo that people could type question about a given image and our AI system powered by iBOWIMG will reply the most possible answers. Here the deep feature of the images are extracted beforehand. Figure 3 shows a snapshot of the demo. People could play with the demo to see the strength and weakness of VQA model."}, {"heading": "5 Concluding Remarks", "text": "For visual question answering on COCO dataset, our implementation of a simple baseline achieves comparable performance to several recently proposed recurrent neural network-based approaches. To reach the correct prediction, the baseline captures the correlation between the informative words in the question and the answer, and that between image contents and the answer. How to move beyond this, from memorizing the correlations to actual reasoning and understanding of the question and image, is a goal for future research."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1511.02799,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "arXiv preprint arXiv:1505.00468,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.-C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "arXiv preprint arXiv:1505.05612,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Compositional memory for visual question answering", "author": ["A. Jiang", "F. Wang", "F. Porikli", "Y. Li"], "venue": "arXiv preprint arXiv:1511.05676,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "arXiv preprint arXiv:1412.6632,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "arXiv preprint arXiv:1511.05756,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "arXiv preprint arXiv:1511.07394,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Ask me anything: Freeform visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. v. d. Hengel", "A. Dick"], "venue": "arXiv preprint arXiv:1511.06973,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "arXiv preprint arXiv:1511.02274,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks.", "startOffset": 46, "endOffset": 49}, {"referenceID": 9, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 14, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 6, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 3, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 7, "context": "These works have benefited from the rapid development of deep learning for visual recognition (object recognition [8] and scene recognition [19]), and have been made possible by the emergence of large image datasets and text corpus (e.", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "These works have benefited from the rapid development of deep learning for visual recognition (object recognition [8] and scene recognition [19]), and have been made possible by the emergence of large image datasets and text corpus (e.", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": "Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].", "startOffset": 79, "endOffset": 89}, {"referenceID": 1, "context": "Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].", "startOffset": 79, "endOffset": 89}, {"referenceID": 4, "context": "Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].", "startOffset": 79, "endOffset": 89}, {"referenceID": 12, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 16, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 4, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 17, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 15, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 2, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 10, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 0, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 15, "context": ", LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer.", "startOffset": 7, "endOffset": 18}, {"referenceID": 10, "context": ", LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer.", "startOffset": 7, "endOffset": 18}, {"referenceID": 0, "context": ", LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer.", "startOffset": 7, "endOffset": 18}, {"referenceID": 16, "context": "Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.", "startOffset": 51, "endOffset": 62}, {"referenceID": 12, "context": "Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.", "startOffset": 51, "endOffset": 62}, {"referenceID": 2, "context": "Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.", "startOffset": 51, "endOffset": 62}, {"referenceID": 11, "context": "Interestingly, we notice that in one of the earliest VQA papers [12], the simple baseline Bag-ofwords + image feature (referred to as BOWIMG baseline) outperforms the LSTM-based models on a synthesized visual QA dataset built up on top of the image captions of COCO dataset [9].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Interestingly, we notice that in one of the earliest VQA papers [12], the simple baseline Bag-ofwords + image feature (referred to as BOWIMG baseline) outperforms the LSTM-based models on a synthesized visual QA dataset built up on top of the image captions of COCO dataset [9].", "startOffset": 274, "endOffset": 277}, {"referenceID": 1, "context": "For the recent much larger COCO VQA dataset [2], the BOWIMG baseline performs worse than the LSTM-based models [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "For the recent much larger COCO VQA dataset [2], the BOWIMG baseline performs worse than the LSTM-based models [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "We call it iBOWIMG to avoid confusion with the implementation in [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "The visual feature is usually taken from the top of the VGG network or GoogLeNet, while the word features of the question sentence are usually the popular LSTM-based features [12, 2].", "startOffset": 175, "endOffset": 182}, {"referenceID": 1, "context": "The visual feature is usually taken from the top of the VGG network or GoogLeNet, while the word features of the question sentence are usually the popular LSTM-based features [12, 2].", "startOffset": 175, "endOffset": 182}, {"referenceID": 13, "context": "In our iBOWIMG model, we simply use naive bag-of-words as the text feature, and use the deep features from GoogLeNet [14] as the visual features.", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "Here we train and evaluate the iBOWIMG model on the Full release of COCO VQA dataset [2], the largest VQA dataset so far.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Open-Ended Overall yes/no number others IMG [2] 28.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "77 BOW [2] 48.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "14 BOWIMG [2] 52.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "37 LSTMIMG [2] 53.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "42 CompMem [6] 52.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "46 NMN+LSTM [1] 54.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "[13] ACK [16] 55.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[13] ACK [16] 55.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "13 DPPnet [11] 57.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Open-Ended Overall yes/no number others LSTMIMG [2] 54.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "06 NMN+LSTM [1] 55.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "10 ACK [16] 55.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "10 DPPnet [11] 57.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 5, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 12, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 15, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 10, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 16, "context": "[17] (arXiv dated at Nov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "9 version of VQA with their own split of training and testing; [18] (arXiv dated at Nov.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "7 2015) used their own split of training and testing for the val2014; [3] (arXiv dated at Nov.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Except for these IMG, BOW, BOWIMG baselines provided in the [2], all the compared methods use either deep or recursive neural networks.", "startOffset": 60, "endOffset": 63}, {"referenceID": 10, "context": "However, our iBOWIMG baseline performs comparably to these much more complex models, except for DPPnet [11] that is about 1.", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo1, and open-source code2.", "creator": "LaTeX with hyperref package"}}}