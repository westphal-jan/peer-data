{"id": "1512.04134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect", "abstract": "The Microsoft Kinect camera and its skeletal tracking features have been used by many researchers and commercial developers in various applications of human real-time motion analysis. In this paper, we evaluate the accuracy of human kinematic motion data in the first and second generation of the Kinect system and compare the results to an optical motion detection system. We collected motion data in 12 exercises for 10 different subjects and from three different angles. We report on the accuracy of joint localization and bone length estimation of Kinect skeletons compared to motion detection. We also analyze the distribution of joint localization equations by adjusting a mixture of Gaussian and uniform distribution models to determine the outliers in Kinect motion data. Our analysis shows that Kinect 2 overall has a more robust and accurate tracking of the human pose than Kinect 1.", "histories": [["v1", "Sun, 13 Dec 2015 22:58:55 GMT  (9121kb,D)", "http://arxiv.org/abs/1512.04134v1", "10 pages, IEEE International Conference on Healthcare Informatics 2015 (ICHI 2015)"]], "COMMENTS": "10 pages, IEEE International Conference on Healthcare Informatics 2015 (ICHI 2015)", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["qifei wang", "gregorij kurillo", "ferda ofli", "ruzena bajcsy"], "accepted": false, "id": "1512.04134"}, "pdf": {"name": "1512.04134.pdf", "metadata": {"source": "CRF", "title": "Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect", "authors": ["Qifei Wang", "Gregorij Kurillo", "Ferda Ofli", "Ruzena Bajcsy"], "emails": ["gregorij}@eecs.berkeley.edu", "fofli@qf.org.qa", "bajcsy@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAffordable markerless motion capture technology is becoming increasingly pervasive in applications of human-computer and human-machine interaction, entertainment, healthcare, communication, surveillance and others. Although the methods for capturing and extracting human pose from image data have been around for several years, the advances in sensor technologies (infrared sensors) and computing power (e.g., GPUs) have facilitated new systems that provide robust and relatively accurate markerless acquisition of human movement. An important milestone for wide adoption of these technologies was the release of Microsoft Kinect camera [1] for the gaming console Xbox 360 in 2010, followed by the release of Kinect for Windows with the accompanying Software Development Kit (SDK) in 2011. The Kinect SDK for Windows featured real-time full-body tracking of human limbs based on the algorithm by Shotton et al. [2]. Several other technology makers followed the suit by releasing their own 3D cameras that focused on capture of human motion for interactive applications (Xtion by Asus, RealSense by Intel). Many researchers and commercial developers embraced the Kinect in wide range of applications that took advantage of its real-time 3D acquisition capabilities and provided skeletal tracking, such as in physical therapy and rehabilitation [3], fall detection [4] and exercise in elderly [5], [6], ergonomics [7], [8] and anthropometry [9], computer vision [10], and many\nThis research was supported by the National Science Foundation (NSF) under Grant No. 1111965.\nothers. In 2013 the second generation of the Kinect camera was released as part of the Xbox One gaming console. In 2014 a standalone version of Kinect for Windows (k4w) was officially released featuring wider camera angle, higher resolution of depth and color images, improved skeletal tracking, and detection of facial expressions.\nIn this paper we focus on the evaluation of accuracy and performance of skeletal tracking in the two Kinect systems (referred to as Kinect 1 and Kinect 2 in the remainder of this paper) compared to a marker-based motion capture system. Several publications have previously addressed the accuracy of the skeletal tracking of Kinect 1 for various applications; however, the accuracy of Kinect 2 has been reported only to a limited extent in the research literature. Furthermore, concurrent comparison of the two systems has not yet been published to the best of our knowledge. Although both Kinect systems employ similar methodology for human body segmentation and tracking based on the depth data, the underlying technology for acquisition of the depth differs between the two. We report the accuracy rates of the skeletal tracking and the corresponding error distributions in a set of exercise motions that include standing and sitting body configurations. Such an extensive performance assessment of the technology is intended to assist researchers who rely on Kinect as a measurement device in the studies of human motion."}, {"heading": "II. RELATED WORK", "text": "In this section we review several publications related to evaluation of the Kinect systems. Kinect 1 has been extensively investigated in terms of 3D depth map acquisition as well as body tracking accuracy for various applications. Khoshelman and Elbernik [11] examined the accuracy of depth acquisition in Kinect 1, and found that the depth error ranges from a few millimeters up to about 4 cm at the maximum range. They recommended that the data for mapping applications should be acquired within 1-3 m distance. Smisek et al. [12] proposed a geometrical model and calibration method to improve the accuracy of Kinect 1 for 3D measurements. Kinect 1 and Kinect 2 were jointly evaluated by Gonzalez-Jorge et al. [13] who reported that the precision of both systems is similar (about 8 mm) in the range of under 1 m, while Kinect 2 outperforms Kinect 1 at the range of 2 m with the error values\nar X\niv :1\n51 2.\n04 13\n4v 1\n[ cs\n.C V\n] 1\n3 D\nec 2\n01 5\nof up to 25 mm. They also reported that precision of Kinect 1 decreases rapidly following a second order polynomial, while Kinect 2 exhibits a more stable behavior inside its work range. 3D accuracy of Kinect 2 was recently evaluated by Yang et al. [14] who reported on the spatial distribution of the depth accuracy in regard to the vertical and horizontal displacement.\nSkeletal tracking of Kinect was examined primarily in the context of biomechanical and exercise performance analyses. In this review, we limit ourselves only to the evaluations of skeletal tracking based on the official Microsoft Kinect SDK. Obdrz\u030ca\u0301lek et al. [15] performed accuracy and robustness analysis of the Kinect skeletal tracking in six exercises for elderly population. Their paper reports on the error bounds for particular joints obtained from the comparison with an optical motion capture system. The authors conclude that employing a more anthropometric kinematic model with fixed limb lengths could improve the performance. Clark et al. [16] examined the clinical feasibility of using Kinect for postural control assessment. The evaluation with Kinect and motion capture performed in 20 healthy subjects included three postural tests: forward reach, lateral reach, and single-leg eyes-closed standing balance assessment. The authors found high intertrail reliability and excellent concurrent validity for majority of the measurements. The study, however, revealed presence of proportional biases for some of the outcome measures, in particular for sternum and pelvis evaluations. The authors proposed the use of calibration equations that could potentially correct for such biases. Several other works have examined the body tracking accuracy for specific applications in physical therapy, such as for example upper extremity function evaluation [17], assessment of balance disorders [18], fullbody functional assessment [19], and movement analysis in Parkinson\u2019s disease [20]. Plantard et al. [21] performed an extensive evaluation of Kinect 1 skeletal tracking accuracy for ergonomic assessment. By using a virtual mannequin, they generated a synthetic depth map that was input into the Kinect SDK algorithm to predict potential accuracy of joint locations in a large number of skeletal configurations and camera positions. The simulation results were validated by a small number of real experiments. The authors concluded that the kinematic information obtained by the Kinect is generally accurate enough for ergonomic assessment.\nTo the best of our knowledge, publication by Xu and McGorry [22] is to date the only work that reported on the evaluation of Kinect 2 skeletal tracking alongside an optical motion capture system. In their study the authors examined 8 standing and 8 sitting static poses of daily activities. Similar poses were also captured with Kinect 1, however the two data sets were not obtained concurrently. The authors reported that the average static error across all the participants and all Kinect-identified joint centers was 76 mm for Kinect 1 and 87 mm for Kinect 2. They further concluded that there was no significant difference between the two Kinects. This conclusion, however, is of limited validity as the comparison was done indirectly with two different sets of subjects.\nSince the Kinect 1 system is being replaced by the Kinect 2\nsystem in many applications, it is important to evaluate the performance of the new camera and software for tracking of dynamic human activities. This is especially relevant since the depth estimation in the two systems is based on two different physical principles. Side-by-side comparison can thus provide a better understanding of the performance improvements as well as potential drawbacks. In this paper, we report on the experimental evaluation of the joint tracking accuracy of the two Kinects in comparison to an optical motion capture system. We analyze the results for 12 different activities that include standing and sitting poses as well as slow and fast movements. Furthermore we examine the performance of pose estimation with respect to three different horizontal orientation angles. We provide the error bounds for joint positions and extracted limb lengths for both systems. We also analyze the distribution of joint localization errors by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the motion data."}, {"heading": "III. ACQUISITION SYSTEMS", "text": "In this section we provide more details on the experimental setup and a brief overview of the technology behind each Kinect system. For the experimental evaluation, the movements were simultaneously captured by Kinect 1, Kinect 2, and a marker-based optical motion capture system which served as a baseline. The two Kinects were secured together and mounted on a tripod at the height of about 1.5 m. All three systems were geometrically calibrated and synchronized prior to the data collection using the procedure described below."}, {"heading": "A. Motion Capture System (MoCap)", "text": "The motion capture data were acquired using PhaseSpace (San Leandro, CA, USA) system Impulse X2 with 8 infrared stereo cameras. The cameras were positioned around the capture space of about 4 m by 4 m. The system provides 3D position of LED markers with sub-millimeter accuracy and frequency of up to 960 Hz. Capture rate of 480 Hz was selected for this study. For each subject 43 markers were attached at standard body landmarks to a motion capture suit using velcro. To obtain the skeleton from the marker data, a rigid kinematic structure was dynamically fitted into the 3D point cloud. We used PhaseSpace Recap2 software to obtain the skeleton for each subject based on collected calibration data which consisted of a sequence of individual joint rotations. The built-in algorithm determines the length of the body segments based on the set of markers associated with different parts of the body and generates a skeleton with 29 joint positions. Once individual\u2019s kinematic model is calibrated, the skeletal sequence can be extracted for any motion of that person."}, {"heading": "B. Kinect 1", "text": "Kinect 1 sensor features acquisition rates of up to 30 Hz for the color and depth data with the resolution of 640 \u00d7 480 pixels and 320 \u00d7 240 pixels, respectively. The depth data are obtained using structured light approach, where a pseudorandom infrared dot pattern is projected onto the scene while\nbeing captured by an infrared camera. Stereo triangulation is used to obtain 3D position of the points from their projections. This approach provides a robust 3D reconstruction even in low-light conditions. The accuracy of the depth decreases with the square of the distance with typical accuracy ranging from about 1-4 cm in the range of 1-4 m [12]. To obtain a dense depth map, surface interpolation is applied based on the acquired depth values at the data points. Fixed density of the points limits the accuracy when moving away from the camera as the points become sparser. The boundaries of surfaces in the distance are thus often jagged.\nReal-time skeletal tracking provided by the Microsoft Kinect SDK is based on the depth data using body part estimation algorithm based on random decision forest proposed by Shotton et al. [2]. The algorithm estimates candidate body parts based on a large training set of synthetically-generated depth images of humans of many different poses and shapes in various poses from a motion capture database [1]. The Kinect 1 SDK can track up to two users, providing the 3D location of 20 joints for each tracked skeleton."}, {"heading": "C. Kinect 2", "text": "Kinect 2 sensor features high definition color (1920 \u00d7 1080 pixels) and higher resolution depth data (512 \u00d7 424 pixels) as compared to Kinect 1. The depth acquisition is based on the time-of-flight (ToF) principle where the distance to points on the surface is measured by computing the phase-shift distance of modulated infrared light. The intensity of the captured image is thus proportional to the distance of the points in 3D space. The ToF technology as opposed to the structured light inherently provides a dense depth map, however the results can suffer from various artifacts caused by the reflections of light signal from the scene geometry and the reflectance properties of observed materials. The depth accuracy of Kinect 2 is relatively constant within a specific capture volume, however it depends on the vertical and horizontal displacement as the light pulses are scattered away from the center of the camera [14]. The reported average depth accuracy is under 2 mm in the central viewing cone and increases to 2-4 mm in the range of up to 3.5 m. The maximal range captured by Kinect 2 is 4.0 m where the average error typically increases beyond 4 mm.\nThe skeletal tracking method implemented in Kinect SDK v2.0 has not been fully disclosed by Microsoft; however, it appears to follow similar methodology as for Kinect 1 while taking advantage of GPU computation to reduce the latency and to improve the performance. The Kinect SDK v2.0 features skeletal tracking of up to 6 users with 3D locations of 25 joints for each skeleton. In comparison to Kinect 1, the skeleton includes additional joints at the hand tip, thumb tip and neck. The arrangement of the joints, i.e. the kinematic structure of the model, is comparable to standard motion capture skeleton. Kinect 2 includes some additional features, such as detection of hand opening/closing and tracking of facial features."}, {"heading": "D. Calibration and Data Acquisition", "text": "For the capture of the database, we connected the two Kinect cameras to a single PC running Windows 8.1, with Kinect 1 connected via USB 2.0 and Kinect 2 connected via USB 3.0 on a separate PCI bus. Such arrangement allowed for both sensors to capture at the full frame rate of 30 Hz. The skeletal data for both cameras were extracted in real time via Microsoft Kinect SDK v1.8 and Kinect for Windows SDK v2.0 for Kinect 1 and Kinect 2, respectively.\nThe temporal synchronization of the captured data was performed using Network Time Protocol (NTP). The motion capture server provided the time stamps for the Kinect PC over the local area network. Meinberg NTP Client Software (Meinberg Radio Clocks GmbH, Bad Pyrmont, Germany) was installed on the Windows computer to obtain more precise clock synchronization.\nPrior to the data acquisition, we first calibrated the motion capture system using provided calibration software. The coordinate frames of the Kinect cameras were then aligned with the motion capture coordinates using the following procedure. A planar checkerboard with three motion capture markers attached to corners of the board was placed in three different positions in front of the Kinects. In each configuration, marker position, color and depth data were recorded. Next, the 3D positions of the corners were extracted from the depth data using the intrinsic parameters of the Kinect and corresponding depth pixel values. Finally, a rigid transformation matrix that maps 3D data captured by each Kinect into the motion capture coordinate system was determined by minimizing the squared distance between the Kinect acquired points and the corresponding marker locations."}, {"heading": "E. Data Processing", "text": "Collected marker data were first processed in Recap2 to obtain the skeletal sequence for each subject, and then exported to BVH file format. The rest of the analysis was performed in MATLAB (MathWorks, Natick, MA). First, the skeletal sequences from the Kinect cameras were mapped to the\nmotion capture coordinate space using the rigid transformation obtained from the calibration. Next, we aligned the sequences using the time stamps, and re-sampled all the data points to the time stamps of Kinect 2 in order to compare the joint localization at the same time instances. Fig. 1 demonstrates the three skeletal configurations projected into the motion capture coordinate space after the calibration.\nAfter the spatial transformation and temporal alignment, we obtained three sequences of 3D joint positions for Kinect 1, Kinect 2, and motion capture. Since the three skeletal configurations have different arrangements and number of joints, we selected 20 joints that are common to all the three systems. Other remaining joints were ignored in this analysis. Next, we evaluated the position accuracy by calculating the distance between the corresponding joints in each time frame. When the Kinect skeletal tracking loses track of the body parts for certain joints (e.g. due to occlusions), such frames can be flagged as outliers. Since the data of the outliers can be assigned arbitrary values, we use a uniform distribution to model the distribution of the outliers. The distribution of the valid (ontrack) data samples is on the other hand modeled by a Gaussian distribution with the mean representing the average offset of that joint. The overall distribution of the joint offset data, p(\u03b8), can thus be modeled by a mixture model of Gaussian and uniform distributions as follows:\np(\u03b8) = \u03c1\u00d7N(\u00b5, \u03c3) + (1\u2212 \u03c1)\u00d7 U(x1, x2). (1)\nIn equation (1), \u00b5 and \u03c3 denote the parameters of the Gaussian distribution, N , respectively. x1 and x2 denote the parameters of the uniform distribution, U , respectively. \u03c1 denotes the weight of the Gaussian distribution. In this paper, we use the maximum-likelihood method to estimate these parameters with the input data samples. After estimating the mixture model, the data are classified into either on-track or off-track state. The off-track data is then excluded from the accuracy evaluation.\nAnother important parameter for the accuracy assessment of human pose tracking is the variability of the limb lengths. The human skeleton is typically modeled as a kinematic chain with rigid body segments. The Kinect skeletal tracking, however, does not explicitly constrain the length of body segments. In the paper, we thus report on the variability of the bone lengths by calculating the distance between two end-joints of each bone for the Kinect systems. For motion capture, the bone length is extracted from the segment length parameters in the BVH file."}, {"heading": "IV. EXPERIMENTS", "text": "In this section we describe the experimental protocol for the data accuracy evaluation. As described in Section III, the motion data were captured by the setup consisting of Kinect 1, Kinect 2, and the motion capture system. We selected 12 different exercises (Table I, Fig. 2), consisting of six sitting (and sit-to-stand) exercises and six standing exercises. In the first set of exercises, subjects were interacting with the chair, while no props were used in the second set. We analyze the two sets of exercises separately.\nWe captured the motion data in 10 subjects (mean age: 27). Before starting the recording, each subject was instructed on how to perform the exercise via a video. We first recorded the motion capture calibration sequence for the subsequent skeleton fitting. Each exercise recording consisted of five repetitions, except for the Jogging which required subjects to perform ten jogging steps. The recording of the 12 exercises was repeated for three different orientation angles of the subjects with respect to the Kinect cameras, i.e. at 0\u25e6 with subject facing the cameras and at 30\u25e6 and 60\u25e6 with subject rotated to the left of the cameras. Figs. 2 and 3 show the video snapshots and the corresponding motion capture skeletons of the key poses for the 12 exercises in one of the subjects.\nAfter the data acquisition, the joint coordinates of Kinect 1 and Kinect 2 were transformed into the global coordinate system of the motion capture. Additionally, the temporal data were synchronized according to the time stamp of the sequence captured by Kinect 2, as described in Section III-D.\nFor the analysis of joint position accuracy, we selected 20 joints that are common between the three systems. These joints and their abbreviated names are shown in Fig. 4. In addition to the joint position accuracy, we also evaluated the accuracy of the bone lengths for the upper and lower extremities. Those bones and their abbreviated names are also shown in Fig. 4."}, {"heading": "V. RESULTS AND DISCUSSION", "text": "In this section, we present detailed analysis of the pose tracking accuracy in Kinect 1 and Kinect 2 in comparison to the motion capture system which we use as a baseline. All the reported results are the average values across all the subjects. The values in the sitting or standing pose represents the mean values of all the exercises in the sitting or standing set."}, {"heading": "A. Joint Position Accuracy", "text": "Tables II and III summarize the mean offsets for all the joints in the sitting and standing sets of exercises in three different viewpoint directions. The mean offset represents the average distance between the corresponding joint position of\nKinect 1 or Kinect 2 as compared to the location identified from the motion capture data.\nIn the sitting set of exercises (Table II), the majority of the mean joint offsets range between 50 mm and 100 mm for both Kinect systems. The largest offset in Kinect 1 is consistently observed in the pelvic area which includes the following three joints: ROOT, HIP L, and HIP R. Kinect 2 on the other hand has smaller offsets for these particular joints. In Kinect 2, the largest offsets are observed in the following four joints of the lower extremities: ANK L, ANK R, FOO L, and FOO R. These joints typically have a large vertical offset from the ground plane, while the same is not observed in Kinect 1. Similar observations can be made in the standing set\nof exercises (Table III) where the largest offsets in Kinect 1 are again in the pelvic area and the largest offsets in Kinect 2 are found in the lower extremities. These observations are also clearly visible in Fig. 1.\nTables II and III also summarize the standard deviation (SD) of the joint position offsets which reflects the variability of a particular joint tracking. For most of the joints, the SD ranges between 10 mm and 50 mm. The joints that exhibit considerable motion during an exercise have much higher variability, and thus SD, typically greater than 50 mm. In most cases, the SDs of the joint positions in Kinect 2 are considerably smaller than those in Kinect 1. This is most likely due to an increased resolution and reduced noise level of Kinect 2 depth maps.\nFurthermore, we can observe that the mean offset and SD of the joints that are most active in a particular exercise are both increasing with the viewpoint angle. This is especially noticeable on the side of the skeleton that is turned further away from the camera as the occlusion of joints increases the uncertainty of the pose detection. In our experiments, the left side of the skeleton was turning away from the camera with the increasing viewpoint angle.\nIn order to examine whether Kinect 2 is performing better than Kinect 1, we performed statistical analysis of the accuracy for each of the 20 joints in comparison to the motion capture. We used pair-wise t-test for the joint position analysis. Our hypothesis was that the position of a particular joint is not significantly different between a Kinect and the motion capture. The results of the analysis are shown in Tables II and III where the joints with significant difference are denoted with * symbol when p < 0.05 and ** symbol when p < 0.01, respectively.\nThe results of the t-test analyses show that the joint position offsets of the joints ROOT, SPINE, HIP L, HIP R, ANK L, ANK R, FOO L, are FOO R for Kinect 2 have significantly different mean offsets as compared to Kinect 1. The mean joint position offsets of other joints share the same distribution.\nSimilar conclusions can be drawn for the standing set of exercises (Table III). For example, the SDs of the joint position offset in Kinect 2 are usually smaller than those of Kinect 1. The variances in the more active joints are typically increasing with the viewpoint angle. Statistically significant differences in the accuracy of Kinect 1 vs. Kinect 2 can be found in the following joints: ROOT, SPINE, HIP L, HIP R, ANK L, ANK R, FOO L, and FOO R. Overall, the means and SDs of the joint position offsets in the standing poses are usually larger than those in the sitting poses. In the sitting poses there are higher number of static joints, which in general have smaller variability.\nFigs. 5, 6, 7, and 8 demonstrate the means and SDs of the joint position offsets for the exercises Cops & Robbers, Lateral Stepping, Jogging, and Punching, respectively. In the figures, the skeleton in magenta represents one of the key poses in the exercise sequence as captured by the motion capture system. The blue or black lines on the other hand represent the corresponding skeletons generated from the mean\njoint position offsets that were observed in either Kinect 1 or Kinect 2, respectively. The ellipsoids at each joint denote the SDs in the 3D space, analyzed for each axis of the local coordinate system attached to the corresponding segment. The larger size of the ellipsoid indicates larger SD of the joint position in Kinect compared with the joint position captured by the motion capture.\nSuch visualization of results provides a quick and intuitive way for comparison of accuracy of different joints. The results show that the overall SDs are larger in Kinect 1 as compared to Kinect 2. The variability of offsets is also increasing with the viewpoint angle. In more dynamic movements, such as Jogging (Fig. 7) and Punching (Fig. 8), the end-points, such as feet and hands, have considerably larger SD with increasing viewpoint angle. Finally, we can observe that certain joints have consistently large offsets from the baseline skeleton, such as ROOT, HIP L, and HIP R in Kinect 1 and ANK L, ANK R, FOO L, and FOO R, in Kinect 2.\nThe joint position offsets in general depend on various sources of error, such as systematic errors (e.g. offset of hips in Kinect 1), noise from depth computation, occlusions, loss of tracking, etc. In our further analysis, we analyze the error distribution to discriminate between the random errors and the errors due to tracking loss. We expect that the random errors follow Gaussian distribution while the errors due to tracking loss can be treated as outliers belonging to a uniform distribution. As an example, we show the histogram of the joint position offsets for the right elbow and right knee as captured in the exercises Cops & Robbers and Jogging from different viewpoint angles (Figs. 9 and 10, respectively). These two joints are among the more active joints in these two exercises. The histograms demonstrate our assumption about the error distribution where the joint position offsets are mainly concentrated close to zero with a long tail to the right side. In order to determine the outliers in the tracking data, we use a mixture model of a Gaussian distribution and a uniform distribution to approximate the distribution of the joint position offsets, as defined in equation (1). Fig. 11 demonstrates the distribution fitting results for the right elbow in the exercise Cops & Robbers. The results show the mixture model of the Gaussian and uniform distributions overlaid on the data histograms.\nAfter applying the mixture model fitting for each joint independently, we can classify the data into either on-track state or off-track state. Table IV shows the percentage of the average on-track ratio for each joint defined as the ratio between the number of on-track samples and the total number of frames. The results show that in most joints the on-track ratio is above 90%. For the frontal view, the on-track ratio of all the joint is relatively similar. In the viewpoints of 30\u25e6 and 60\u25e6, the active joints which are further away from the camera typically have lower ratios than the joints closer to the camera. If all the joints in a frame are on-track, that frame is marked as a valid frame for the data accuracy evaluation. The last row in Table IV summarized the percentage of valid frames. The percentage of valid frames is typically higher for Kinect 2 than\nKinect 1. Furthermore, the percentages of valid frames in the viewpoints of 30\u25e6 and 60\u25e6 drop by 10% and 15% compared to those in the frontal view, respectively. Finally, in Tables V and VI we show the mean and SD of the joint position offsets after the removal of outliers.\nCompared with the results in Tables II and III, both the mean and SD of most joints in Tables V and VI are reduced since the outliers are excluded from the analysis. Table VII summarized the average reduction of the mean and SD of the joint position offsets after excluding the outliers. The results demonstrate that the data accuracy can be significantly improved by fitting\nthe data into the mixture model."}, {"heading": "B. Bone Length Accuracy", "text": "Another important parameter for evaluation of Kinect pose tracking performance is the bone length. As mentioned previously, the Kinect tracking algorithm does not specifically pre-define or calibrate for the anthropometric values of the body/bone segments. On the other hand, the human skeleton can be approximated as a kinematic structure with rigid segments, so we expect that the bone lengths should stay relatively constant. The size of the variance (and SD) of the bone length over time can thus be interpreted as a measure of\nrobustness of the extracted kinematic model. For the Kinect skeleton, we define the bone length as the l2 distance between the positions of two subsequent joints. The bone length for the motion capture is on the other hand determined during the calibration phase and remains constant during the motion sequence. Figs. 12 and 13 show the means and SDs of the bone length difference of Kinect 1 and Kinect 2, respectively, as compared to the bone length calibrated from the motion capture data across all the subjects. The mean bone length difference does not change too much between different exercises. The SDs are typically increasing with larger viewpoint angle. We can observe that the bone lengths in Kinect 1 usually have larger offsets and SDs as compared to Kinect 2, especially for the upper legs due to the large vertical offset of the hip joints. Tables VIII and IX summarize the mean and SD of the bone length differences in Kinect 1 and Kinect 2 in the three viewpoints for sitting and standing exercises, receptively. We can observe that the mean differences in the bone lengths and SDs are smaller in Kinect 2, suggesting that the kinematic\nstructure of its skeleton is more robust."}, {"heading": "C. Summary of Findings", "text": "Based on the experimental results reported in this paper, we can make the following observations: \u2022 As reported by other researchers, the hip joints in Kinect 1\nare located much higher than normal with the offset of about 200 mm. The offsets should be considered when calculating knee and hip angles, in particular in sitting position. On\nthe other hand, the skeleton in Kinect 2 is in general more anthropometric with smaller offsets. \u2022 The foot and ankle joints of Kinect 2 are offset from the ground plane for about 100 mm or more. The orientation of the feet is thus unreliable. Once the foot is lifted off the ground, the tracking of the joints is more accurate. The unreliable foot position may be originating from ToF artifacts that generate large amounts of noise close to large planar surfaces.\n0\u25e6 30\u25e6 60\u25e6\nFig. 9. Distribution of joint position offsets for the right elbow in the exercise Cops & Robbers.\n0\u25e6 30\u25e6 60\u25e6\nFig. 10. Distribution of joint position offsets for the right knee in the exercise Jogging.\nKinect 1 Kinect 2\nFig. 11. Mixture model fitting into the distribution of joint position offsets for the right elbow in the exercise Cops and Robbers (viewpoint: 60\u25e6).\n\u2022 Overall accuracy of joint positions in Kinect 2 is better than in Kinect 1, except the location of feet. The average offsets are typically between 50 mm and 100 mm. \u2022 The analysis of the distribution mixture shows that Kinect 2 has smaller uniform distribution component (i.e. less outliers) suggesting that the tracking of joints is more robust. Kinect 2 also tracks human movement more reliably even\nTABLE IV ON-TRACK RATIOS FOR KINECT 1 AND KINECT 2.\nSitting (%) Standing (%) Kinect 1 Kinect 2 Kinect 1 Kinect 2\n0\u25e6 30\u25e6 60\u25e6 0\u25e6 30\u25e6 60\u25e6 0\u25e6 30\u25e6 60\u25e6 0\u25e6 30\u25e6 60\u25e6\nROOT 95 97 96 99 98 100 93 97 99 96 99 99 SPINE 96 98 98 99 99 100 95 100 99 97 99 99 NECK 96 98 99 99 99 99 98 98 98 97 99 99 HEAD 96 99 99 100 99 99 97 98 99 97 99 99 SHO L 96 98 99 99 97 95 97 96 95 97 97 97 ELB L 98 98 96 99 97 97 96 94 92 97 96 92 WRI L 97 93 90 99 91 96 97 95 91 97 95 96 HAN L 97 93 92 97 89 93 96 93 89 97 96 95 SHO R 97 97 99 99 98 99 98 98 97 98 99 97 ELB R 98 94 97 99 98 99 97 95 98 98 99 99 WRI R 98 93 91 99 99 99 99 99 99 98 99 99 HAN R 97 92 88 99 98 99 98 99 98 98 99 100 HIP L 96 97 99 99 98 99 95 99 98 95 98 99 KNE L 96 99 96 98 98 97 97 95 87 97 96 91 ANK L 99 98 97 98 98 94 94 93 90 96 91 96 FOO L 99 98 98 97 94 96 95 89 91 96 94 96 HIP R 94 93 98 99 96 100 94 98 99 98 99 100 KNE R 97 96 97 98 97 94 97 98 96 97 98 98 ANK R 99 96 99 97 99 98 94 95 95 97 96 98 FOO R 99 99 98 96 96 95 96 97 95 97 97 99 Overall 92 81 75 94 84 81 90 79 73 92 82 80\nwith partial body occlusions. \u2022 The difference and variance of the actual limb lengths are\nsmaller in Kinect 2 than in Kinect 1. \u2022 The skeleton tracking in Kinect 2 has much smaller latency\nas compared to Kinect 1 which is noticeable especially during fast actions (e.g. exercises Jogging and Punching)."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we compared the human pose estimation for the first and second generations of Microsoft Kinect with standard motion capture technology. The results of our analysis showed that overall Kinect 2 has better accuracy in joint estimation while providing skeletal tracking that is more robust to occlusions and body rotation. Only the lower legs were tracked with large offsets, possibly due to ToF artifacts. This phenomena was not observed in Kinect 1 which employs structured light for depth acquisition. For Kinect 1, the largest offsets were observed in the pelvic area as also noted by others. Our analyses show that Kinect 1 can be exchanged with Kinect 2 for majority of motions. Furthermore, by applying a mixture of Gaussian and uniform distribution models we were able to evaluate the robustness of the pose tracking. We showed that the SDs of the joint positions can be reduced by 30% to 40% by employing the classification with a mixture distribution model. This finding suggests that by excluding the outliers from the data and compensating for the offsets, more accurate human motion analysis can be achieved."}], "references": [{"title": "Microsoft Kinect sensor and its effect", "author": ["Z. Zhang"], "venue": "MultiMedia, IEEE, vol. 19, no. 2, pp. 4\u201310, Feb 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Washington, DC, USA: IEEE Computer Society, 2011, pp. 1297\u20131304.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A review on technical and clinical impact of microsoft Kinect on physical therapy and rehabilitation", "author": ["H.M. Hondori", "M. Khademi"], "venue": "Journal of Medical Engineering, vol. 2014, p. 16, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of an inexpensive depth camera for passive in-home fall risk assessment", "author": ["E.E. Stone", "M. Skubic"], "venue": "Proceedings of 5th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth) and Workshops, 2011, pp. 71\u201377.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Systematic review of Kinect applications in elderly care and stroke rehabilitation", "author": ["D. Webster", "O. Celik"], "venue": "Journal of Neuroengineering and Rehabilitation, vol. 11, p. 24, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Design and evaluation of an interactive exercise coaching system for older adults: Lessons learned", "author": ["F. Ofli", "G. Kurillo", "S. Obdrzalek", "R. Bajcsy", "H. Jimison", "M. Pavel"], "venue": "IEEE Journal of Biomedical and Health Informatics, p. 15, Epub ahead of print. 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Using Kinect sensor in observational methods for assessing postures at work", "author": ["J.A. Diego-Mas", "J. Alcaide-Marzal"], "venue": "Applied Ergonomics, vol. 45, no. 4, pp. 976\u2013985, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Pose estimation with a Kinect for ergonomic studies: Evaluation of the accuracy using a virtual mannequin", "author": ["P. Plantard", "E. Auvinet", "A.-S. Pierres", "F. Multon"], "venue": "Sensors, vol. 15, no. 1, pp. 1785\u20131803, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1803}, {"title": "Estimating anthropometry with microsoft Kinect", "author": ["M. Robinson", "M.B. Parkinson"], "venue": "Proceedings of the 2nd International Digital Human Modeling Symposium, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Enhanced computer vision with microsoft Kinect sensor: A review", "author": ["J. Han", "L. Shao", "D. Xu", "J. Shotton"], "venue": "IEEE Transactions on Cybernetics, vol. 43, no. 5, pp. 1318\u20131334, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Accuracy and resolution of Kinect depth data for indoor mapping applications", "author": ["K. Khoshelham", "S.O. Elberink"], "venue": "Sensors, vol. 12, no. 2, pp. 1437\u20131454, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "3D with Kinect", "author": ["J. Smisek", "M. Jancosek", "T. Pajdla"], "venue": "Proceedings of IEEE International Conference on Computer Vision Workshops (ICCV Workshops), Nov 2011, pp. 1154\u20131160.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Metrological comparison between Kinect I and Kinect II sensors", "author": ["H. Gonzalez-Jorge", "P. Rodr\u0131\u0301guez-Gonz\u00e0lvez", "J. Mart\u0131\u0301nez-S\u00e0nchez", "D. Gonz\u00e0lez-Aguilera", "P. Arias", "M. Gesto", "L. D\u0131\u0301az-Vilari\u00f1o"], "venue": "Measurement, vol. 70, pp. 21\u201326, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating and improving the depth accuracy of Kinect for Windows v2", "author": ["L. Yang", "L. Zhang", "H. Dong", "A. Alelaiwi", "A. El Saddik"], "venue": "Sensors Journal, IEEE, p. 11, Epub ahead of print. 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Accuracy and robustness of kinect pose estimation in the context of coaching of elderly population", "author": ["S. Obdrzalek", "G. Kurillo", "F. Ofli", "R. Bajcsy", "E. Seto", "H. Jimison", "M. Pavel"], "venue": "Proceedings of Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Aug 2012, pp. 1188\u20131193.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Validity of the Microsoft Kinect for assessment of postural control", "author": ["R.A. Clark", "Y.-H. Pua", "K. Fortin", "C. Ritchie", "K.E. Webster", "L. Denehy", "A.L. Bryant"], "venue": "Gait & Posture, vol. 36, no. 3, pp. 372 \u2013 377, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of upper extremity reachable workspace using kinect camera", "author": ["G. Kurillo", "A. Chen", "R. Bajcsy", "J.J. Han"], "venue": "Technology and Health Care, vol. 21, no. 6, pp. 641\u2013656, Nov. 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Accuracy assessment of Kinect body tracker in instant posturography for balance disorders", "author": ["H. Funaya", "T. Shibata", "Y. Wada", "T. Yamanaka"], "venue": "Proceedings of 7th International Symposium on Medical Information and Communication Technology (ISMICT), March 2013, pp. 213\u2013217.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Validity and reliability of the Kinect within functional assessment activities: Comparison with standard stereophotogrammetry", "author": ["B. Bonnech\u00e8re", "B. Jansen", "P. Salvia", "H. Bouzahouene", "L. Omelina", "F. Moiseev", "V. Sholukha", "J. Cornelis", "M. Rooze", "S. Van Sint Jan"], "venue": "Gait & Posture, vol. 39, no. 1, pp. 593 \u2013 598, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Accuracy of the Microsoft Kinect sensor for measuring movement in people with Parkinson\u015b disease", "author": ["B. Galna", "G. Barry", "D. Jackson", "D. Mhiripiri", "P. Olivier", "L. Rochester"], "venue": "Gait & Posture, vol. 39, no. 4, pp. 1062 \u2013 1068, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Pose estimation with a Kinect for ergonomic studies: Evaluation of the accuracy using a virtual mannequin", "author": ["P. Plantard", "E. Auvinet", "A.-S.L. Pierres", "F. Multon"], "venue": "Sensors, vol. 15, no. 1, pp. 1785\u20131803, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1803}, {"title": "The validity of the first and second generation Microsoft Kinect for identifying joint center locations during static postures", "author": ["X. Xu", "R.W. McGorry"], "venue": "Applied Ergonomics, vol. 49, pp. 47 \u2013 54, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "An important milestone for wide adoption of these technologies was the release of Microsoft Kinect camera [1] for the gaming console Xbox 360 in 2010, followed by", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Many researchers and commercial developers embraced the Kinect in wide range of applications that took advantage of its real-time 3D acquisition capabilities and provided skeletal tracking, such as in physical therapy and rehabilitation [3], fall detection [4] and exercise in elderly [5], [6], ergonomics [7],", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": "Many researchers and commercial developers embraced the Kinect in wide range of applications that took advantage of its real-time 3D acquisition capabilities and provided skeletal tracking, such as in physical therapy and rehabilitation [3], fall detection [4] and exercise in elderly [5], [6], ergonomics [7],", "startOffset": 257, "endOffset": 260}, {"referenceID": 4, "context": "Many researchers and commercial developers embraced the Kinect in wide range of applications that took advantage of its real-time 3D acquisition capabilities and provided skeletal tracking, such as in physical therapy and rehabilitation [3], fall detection [4] and exercise in elderly [5], [6], ergonomics [7],", "startOffset": 285, "endOffset": 288}, {"referenceID": 5, "context": "Many researchers and commercial developers embraced the Kinect in wide range of applications that took advantage of its real-time 3D acquisition capabilities and provided skeletal tracking, such as in physical therapy and rehabilitation [3], fall detection [4] and exercise in elderly [5], [6], ergonomics [7],", "startOffset": 290, "endOffset": 293}, {"referenceID": 6, "context": "Many researchers and commercial developers embraced the Kinect in wide range of applications that took advantage of its real-time 3D acquisition capabilities and provided skeletal tracking, such as in physical therapy and rehabilitation [3], fall detection [4] and exercise in elderly [5], [6], ergonomics [7],", "startOffset": 306, "endOffset": 309}, {"referenceID": 7, "context": "[8] and anthropometry [9], computer vision [10], and many", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8] and anthropometry [9], computer vision [10], and many", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "[8] and anthropometry [9], computer vision [10], and many", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Khoshelman and Elbernik [11] examined the accuracy of depth acquisition in Kinect 1, and found that the depth error ranges from a few millimeters up to about 4 cm at the maximum range.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "[12] proposed a geometrical model and calibration method to improve the accuracy of Kinect 1 for 3D measurements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] who reported that the precision of both systems is similar", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] who reported on the spatial distribution of the depth accuracy in regard to the vertical and horizontal displacement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] performed accuracy and robustness analysis of the Kinect skeletal tracking in six exercises for elderly population.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] examined the clinical feasibility of using Kinect for postural control assessment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Several other works have examined the body tracking accuracy for specific applications in physical therapy, such as for example upper extremity function evaluation [17], assessment of balance disorders [18], full-", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": "Several other works have examined the body tracking accuracy for specific applications in physical therapy, such as for example upper extremity function evaluation [17], assessment of balance disorders [18], full-", "startOffset": 202, "endOffset": 206}, {"referenceID": 18, "context": "body functional assessment [19], and movement analysis in Parkinson\u2019s disease [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "body functional assessment [19], and movement analysis in Parkinson\u2019s disease [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "[21] performed an extensive evaluation of Kinect 1 skeletal tracking accuracy for ergonomic assessment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "McGorry [22] is to date the only work that reported on the evaluation of Kinect 2 skeletal tracking alongside an optical motion capture system.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "The accuracy of the depth decreases with the square of the distance with typical accuracy ranging from about 1-4 cm in the range of 1-4 m [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "various poses from a motion capture database [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "The depth accuracy of Kinect 2 is relatively constant within a specific capture volume, however it depends on the vertical and horizontal displacement as the light pulses are scattered away from the center of the camera [14].", "startOffset": 220, "endOffset": 224}], "year": 2015, "abstractText": "Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1.", "creator": "LaTeX with hyperref package"}}}