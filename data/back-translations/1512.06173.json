{"id": "1512.06173", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2015", "title": "Discriminative Subnetworks with Regularized Spectral Learning for Global-state Network Data", "abstract": "Data mining practitioners face challenges from data with a network structure. In this paper, we address a specific class of global state networks that consists of a set of network instances that share a similar structure but have different values at local nodes. Each instance is associated with a global state that indicates the occurrence of an event. The goal is to uncover a small group of discriminatory subnetworks that can optimally classify global network values. Unlike most existing studies that examine an exponential subnetwork space, we address this difficult problem by taking an approach to space transformation. Specifically, we present an algorithm that optimizes a limited two-dimensional function to learn a low-dimensional subspace that is able to accommodate networks that are marked by different global states, while aligning itself with the common network topology across instances. Our algorithm takes an appealing approach of spectral resolution and optimal graph learning that can be achieved globally through the mattraperture-X.", "histories": [["v1", "Sat, 19 Dec 2015 01:20:02 GMT  (250kb)", "http://arxiv.org/abs/1512.06173v1", "manuscript for the ECML 2014 paper"]], "COMMENTS": "manuscript for the ECML 2014 paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xuan hong dang", "ambuj k singh", "petko bogdanov", "hongyuan you", "bayyuan hsu"], "accepted": false, "id": "1512.06173"}, "pdf": {"name": "1512.06173.pdf", "metadata": {"source": "CRF", "title": "Discriminative Subnetworks with Regularized Spectral Learning for Global-state Network Data", "authors": ["Xuan Hong Dang", "Ambuj K. Singh", "Petko Bogdanov"], "emails": ["xdang@cs.ucsb.edu", "ambuj@cs.ucsb.edu", "petko@cs.ucsb.edu", "hyou@cs.ucsb.edu", "soulhsu@cs.ucsb.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n06 17\n3v 1\n[ cs\n.L G\n] 1\n9 D\nec 2"}, {"heading": "1 Introduction", "text": "With the increasing advances in hardware and software technologies for data collection and management, practitioners in data mining are now confronted with more challenges from the collected datasets: the data are no longer as simple as objects with flattened representation but now embedded with relationships among variables describing the objects. This sort of data is often referred to as network or graph data. In the literature, there are a large number of techniques developed to mine useful patterns from network databases, ranging from frequent (sub)networks mining [15], network classification/clustering [1,18] to anomaly detection [2]. Often, even for the same data mining task, we may need different algorithms to be developed depending on whether the networks are directed or indirected, or whether the data resides at nodes, edges or both of them [15].\nIn this work, the focus is on a specific class of interesting networks in which we have a series of network instances that share a common structure but may have different dynamic values at local nodes and/or edges. In addition, each network instance is associated with a global state indicating the occurrence of an event. Such a class of global-state network data can be used to model a number of real-world applications ranging from opinion evolution in social networks [20], regulatory networks in biology [21] to brain networks in neuroscience [10]. For\n2 example, we possess the same set of genes (nodes) embedded in regulatory networks. Yet, research in systems biology shows that the gene expression levels (node values) may vary across individuals and for some specific genes, their over-expressions may impact those in the neighbors through the regulatory network. These local effects may jointly encode a logical function that determines the occurrence of a disease [21,25]. In analyzing these types of network data, a natural question to be asked is how one can learn a function that can determine the global-state values of the networks based on the dynamic values captured at their local nodes along with the network topology? More specifically, is it possible to identify a small succinct set of influential discriminative subnetworks whose local-node values have the maximum impact on the global states and thus uncover the complex relationships between local entities and the global-state network properties? In searching for an answer, obviously, a naive approach would be to enumerate all possible subnetworks and seek those who have the most discriminative potential. Nonetheless, as the number of subnetworks is exponentially proportional to the numbers of nodes and edges, this approach generally is analytically intractable and might not be feasible for large scale networks. A more practical approach is to perform heuristic sampling from the space of subnetworks. Though greatly reducing the number of subnetworks to be visited, the sampling approaches might still suffer from suboptimal solutions and might further lose explanation capability due to the large number of generating subnetworks.\nIn this paper, we propose a novel algorithm for mining a set of concise subnetworks whose local-state node values discriminate networks of different global-state values. Unlike the existing techniques that directly search through the exponential space of subnetworks, our proposed method is fundamentally different by investigating the discriminative subnetworks in a low dimensional transformed subspace. Toward this goal, we construct on top of the network database three meta-graphs to learn the network neighboring relationships. The first meta-graph is built to capture the network topology sharing across network instances which serves as the network constraint in our subspace learning function, whereas the two subsequent ones are build to essentially capture the relationships between neighboring networks, especially those located close to the potential discriminative boundary. By this setting, our algorithm aims to discover a unique low dimensional subspace to which: i) networks sharing similar global state values are mapped close to each other while those having different global values are mapped far apart; ii) the common network topology is smoothly preserved through constraints on the learning process. In this way, our algorithm helps to attack two challenging issues at the same time. It first avoids searching through the original space of exponential number of subnetworks by learning a single subspace via the optimization of a single dual-objective function. Second, our network topology constraint not only matches properly with our subspace learning function, its quadratic form naturally imposes the L2-norm shrinkage over the connecting nodes, resulting in an effective selection of relevant and dominated nodes for the subnetworks embedded in the induced subspace. Additionally, the principal technical contributions of our work is the formulation\n3 of our learning objective function that is mathematically founded on spectral learning and its advantages therefore not only ensure the stability but also the global optimum of the uncovered solutions.\nIn summary, we claim the following contributions: (i) Novelty: We formulate the problem of mining discriminative subnetworks by transformed subspace learning\u2014an approach that is fundamentally different from most existing techniques that address the problem in the original high-dimensional network space. (ii) Flexibility: We propose a novel dual-objective function along with constraints to ensure learning of a single subspace in which different global state networks are well discriminated while smoothly retaining their common topology. (iii) Optimality: We develop a mathematically sound solution to solve the constrained optimization problem and show that the optimal solution can be achieved via matrix eigen-decomposition. (iv) Practical relevance: We evaluate the performance of the proposed technique on both synthetic and real world datasets and demonstrate its appealing performance against related techniques in the literature."}, {"heading": "2 Preliminaries and Problem Setting", "text": "In this section, we first introduce some preliminaries related to network data with global state values and then give the definition of our problem on mining discriminative subgraphs to distinguish global state networks.\nDefinition 1. (Network data instance) Given Vi = {v1, v2, . . . , vni} as a set of nodes and Ei \u2286 Vi \u00d7 Vi as a set of edges, each connecting two nodes (vp, vq) if they are known to relate or influence each other, we define a network instance (or snapshot) Ni as a quadruple Ni = (Vi, Ei, Li, Si) in which Li is a function operating on the local states of nodes Li : Vi \u2192 R and Si encodes the global network state of Ni.\nWe consider Ni as an indirected network and values at its local nodes are numerical (both continuous and binary) while its global state is a discrete value. Since each Ni is associated with Si as its state property, Ni is often referred to as a global-state network. For example, in the gene expression data, each Ni corresponds to a subject and a local state indicates the gene expression level at node vp \u2208 Vi whereas the global state encodes the presence or absence of the disease, i.e., Si \u2208 {presence, absence}. Likewise in a dynamic social network, a value at each node vp may encode the political standpoint of an individual whereas the global state indicates the overall political viewpoint of the entire community at some specific time (snapshot). Both local and global states may change across different network snapshots. Note that, for network instances/snapshots with different structures, we may use the null value to denote the state of a missing node and consequently, an edge in a network instance is valid only if it connects two non-null nodes.\nNow, let us consider a database consistingm network instances N = {N1, N2, . . . , Nm}, we further define the following network over these network instances:\nDefinition 2. (Generalized network - first meta-graph)\n4 We define the generalized network N as a triple N = (V,E,K) where V = V1 \u222a V2 . . . \u222a Vm and if \u2203(vp, vq) \u2208 Ei, such an edge also exists in E. For a valid edge E(p, q) \u2208 E, we associate a weight K(p, q) as the fraction of network instances having edge E(p, q) in their topology structure,i.e., K(p, q) = m\u22121 \u00d7\u2211\niEi(p, q) with Ei(p, q) = 1 if there exists an edge between vp, vq in network Ni. As such, K(p, q) is naturally normalized between (0, 1]. The value of 1 means the corresponding edge exists in all Ni\u2019s while a value close to 0 shows that the edge only exists in a small fraction of network data.\nIt should be noted here that while we have no edge values at individual networks Ni\u2019s, we have non-zero value associated with each existing edge E(p, q) in the generalized network N . Indeed, the corresponding K(p, q) reflects how frequently there is an edge between vp and vq or equivalently, how strongly is the mutual influence between two entities vp and vq across all networks. As N is defined based on all network instances, we also view N as our first meta-graph with V being its vertices and K capturing its graph topology generalized from the network topology of all network instances. We are now ready to define our problem as follows.\nDefinition 3. (Mining Discriminative Subnetworks Problem) Given a database of network data instances/snapshots N = {N1, N2, . . . , Nm}, we aim to learn an optimal and succinct set of subnetworks with respect to the topology structure generalized in the first meta-graph that well discriminate network instances with different global state values."}, {"heading": "3 Our approach", "text": ""}, {"heading": "3.1 Meta-Graphs over Network Instances", "text": "As mentioned in the above sections, searching for optimal subnetworks in the fully high dimensional original network space is always challenging and potentially intractable. We adopt an indirect yet more viable approach by transforming the original space into a low dimensional space of which networks with different global-states are well distinguished while concurrently retaining the generalized network topology captured by our first meta-graph. Toward this goal, we develop two neighboring meta-graphs based on both the local state values and global state values.\nWe denote these two meta-graphs respectively by G+ and G\u2212. Their vertices correspond to the network instances while a link connecting two vertices represents the neighboring relationship between two corresponding network instances. For the meta-graph G+, we denote A+ as its affinity matrix that captures the similarity of neighboring networks having the same global state values. Likewise, we denote A\u2212 as the affinity matrix for meta-graph G\u2212 that captures the similarity of neighboring networks yet having different global network states. As such, A+ and A\u2212 respectively encode the weights on the vertex-links of two corresponding graphs G+ and G\u2212. In computing values for these affinity matrices, with each given network instance Ni, we find its k nearest neighboring networks based on the local state values and divide them into two sets, those sharing similar global state values and those having different global states. More specifically,\n5 let kNN(Ni) be the neighboring set of Ni, then elements of A + and A\u2212 are computed as: A+ij = vi\u00b7vj \u2016vi\u2016\u2016vj\u2016 if Si = Sj and Nj \u2208 kNN(Ni) or Ni \u2208 kNN(Nj), otherwise we set A+ij = 0. And A \u2212 ij = vi\u00b7vj \u2016vi\u2016\u2016vj\u2016 if Si 6= Sj and Nj \u2208 kNN(Ni) or Ni \u2208 kNN(Nj), otherwise A \u2212 ij = 0. In these equations, we have denoted the boldface letters vi and vj as the vectors encoding the dynamic local states of Ni\u2019s and Nj\u2019s nodes, and have used the cosine distance to define the similarity between two network instances. It is worth mentioning that, though existing other measures for network data [27], our using of cosine distance is motivated by the observation that we can view each node as a single feature and thus the network data can be essentially considered as a special case of very high dimensional data. As such, the symmetric cosine measure can be effectively used though obviously the other ones [27] can also be directly applied here.\nIt is also important to give the intuition behind our above computation. First, notice that both A+ and A\u2212 are the affinity matrices having the same size of m\u00d7m since we calculate for every network instance. Second, while A+ captures the similarity of network instances sharing the same global states and neighboring to each other, A\u2212 encodes the similarity of different global state networks yet also neighboring to each other. Such networks are likely to locate close to the discriminative boundary function and thus they play essential roles in our subsequent learning function. Third, both A+ and A\u2212 are sparse and symmetric matrices since only k neighbors are involved in computing for each network and if Nj is neighboring to Ni, we also consider the inverse relation, i.e., Ni is neighboring to Nj. Moreover, A\n\u2212 is generally sparser compared to A+ as the immediate observation from the second remark."}, {"heading": "3.2 Constrained Dual-Objective Function", "text": "Let us recall that vi is the vector encoding the node states of the corresponding network Ni and let us denote the transformation function that maps vi into our novel target subspace by f(vi). We first formulate the two objective functions as follows:\nargmin f\nm\u2211\ni=1\nm\u2211\nj=1\n(f(vi)\u2212 f(vj)) 2A+ij (1)\nargmax f\nm\u2211\ni=1\nm\u2211\nj=1\n(f(vi)\u2212 f(vj)) 2A\u2212ij (2)\nTo gain more insights into these setting objectives, let us take a closer look at the first Eq.(1). If two network instances Ni and Nj have similar local states in the original space (i.e., A+ij is large), this first objective function will be penalized if the respective points f(vi) and f(vj) are mapped far part in the transformed space. As such, minimizing this cost function is equivalent to maximizing the similarity amongst instances having the same global network states in the reduced dimensional subspace. On the other hand, looking at Eq.(2) can tell us that the function will incur a high penalty (proportional to A\u2212ij) if two networks having different global states are mapped close in the induced subspace. Thus, maximizing this function is equivalent to minimizing the similarity among neighboring networks having different global states in the novel reduced subspace.\n6 As mentioned earlier, such networks tend to locate close to the discriminative boundary function and hence, maximizing the second objective function leads to the maximal margin among clusters of different global-state networks.\nHaving the mapping function f(.) to be optimized above, it is crucial to ask which is an appropriate form for it. Either a linear or non-linear function can be selected as long as it effectively optimizes two objectives concurrently. Nonetheless, keeping in mind that our ultimate goal is to derive a set of succinct discriminative subnetworks along with their explicit nodes. Optimizing a nonlinear function is generally not only more complex but importantly may lose the capability in explaining how the new features have been derived (since they will be the non-linear combinations of the original nodes). We therefore would prefer f(.) as in the form of a linear combination function and following this, f(.) can be represented explicitly as a transformation matrix Un\u00d7d that linearly combines n nodes into d novel features (d \u226a n) of the induced subspace. For the sake of discussion, we elaborate here for the projection onto 1-dimensional subspace (i.e., d = 1). The solution for the general case d > 1 will be straightforward once we obtain the solution for this base case. Given this simplification and with little algebra, we recast our first objective function as follows:\nargmin u\nm\u2211\ni=1\nm\u2211\nj=1\n\u2016uTvi \u2212 u Tvj\u2016 2A+ij =\nm\u2211\ni=1\nm\u2211\nj=1\ntr ( uT (vi \u2212 vj)(vi \u2212 vj) Tu ) A+ij\n= tr\n  m\u2211\ni=1\nm\u2211\nj=1\n( uT (vi \u2212 vj)A + ij(vi \u2212 vj) T ) u\n\n\n= 2tr ( uTVD+VTu ) \u2212 2tr ( uTVA+VTu ) = 2tr ( uTVL+VTu ) (3)\nin which we have used tr(.) to denote the trace of a matrix and V as the matrix whose column ith accommodates the dynamic local states of network instance Ni (i.e., vi), forming its size of n \u00d7 m. Also, D is the diagonal matrix whose D+ii = \u2211 j A + ij and we have defined L\n+ = D+ \u2212A+, which can be shown to be the Laplacian matrix [12]. For the second objective function in Eq.(2), we can repeat the same computation which yields to the following form:\nargmax u\nm\u2211\ni=1\nm\u2211\nj=1\n\u2016uTvi \u2212 u Tvj\u2016 2A\u2212ij\n= 2tr ( uTVD\u2212VTu ) \u2212 2tr ( uTVA\u2212VTu ) = 2tr ( uTVL\u2212VTu ) (4)\nwhere again D\u2212 is the diagonal matrix with D\u2212ii = \u2211 j A \u2212 ij and we have defined L\u2212 = D\u2212 \u2212A\u2212. Notice that while the above formulations aim at discriminating different global state networks in the low dimensional subspace, it has not yet taken into consideration the generalized network structure captured by our first metagraph. As described previously, the mutual interactions among nodes are also important in determining the global network states. Also according to Definition 2, the larger the value placing on the link between nodes vp and vq, the\n7 more likely they are being involved in the same process. Therefore, we would expect our mapping vector u not only separating well different global state networks but also ensuring its smoothness property w.r.t. the generalized network topology characterized by the first meta-graph N .\nToward the above objective, we formulate the network topology as a constraint in our learning objective function, and in order to be consistent with the approach based on spectral graph analysis, we encode the topology captured in N by an n\u00d7 n constraint matrix C whose elements are defined by:\nCpq = Cqp =    \u2211 q K(p, q) if vp \u2261 vq \u2212K(p, q) if vp and vq are connected\n0 otherwise\n(5)\nIt is easy to show that, by this definition, C is also the Laplacian matrix and its quadratic form, taking u as the vector, is always non-negative:\nuTCu = n\u2211\np=1\nu2p\nn\u2211\nq=1\nK(p, q)\u2212 n\u2211\np=1\nn\u2211\nq=1\nupuqK(p, q)\n= 1\n2\nn\u2211\np=1\nn\u2211\nq=1\nK(p, q)(up \u2212 uq) 2 \u2265 0 (6)\nin which up, uq are components of vector u. It is possible to observe that if K(p, q) is large, indicating nodes vp and vq are strongly interacted in large portion of the network instances, the coefficients of up and uq should be similar (i.e., smooth) in order to minimize this equation. From the network-structure perspective, we would say that if vp is known as a node affecting the global network state, its selection in the transformed space will increase the possibility of being selected of its nearby connected node vq if K(p, q) is large, leading to the formation of discriminative subnetworks in the induced subspace. Therefore, in combination with the dual-objective function formulated above, we finally claim our constrained optimization problem as follows (the constants can be omitted due to optimization):\nu\u2217 =argmax u\n{ tr ( uTV(L\u2212 \u2212 L+)V T u )}\nsubject to uTCu \u2264 t\nand uTVD+VTu = 1 (7)\nThe first network topology constraint aims to retain the smoothness property of u whereas the second constraint aims to remove its freedom, meaning that we need u\u2019s direction rather than its magnitude. The network topology constraint is beneficial in two ways. First as presented above, it offers a convenient and natural way to incorporate the network topology into our space transformation learning process. Second, as being formulated in the vector quadratic form, it essentially imposes the features/nodes selection through the coefficients of u by shrinking those of irrelevant nodes toward zero while crediting large values to those of relevant nodes. Indeed, this quadratic L2-norm is a kind of regularization which is often referred to as the ridge shrinkage in statistics for regression [13,7]. The parameter t is used to control the amount of shrinkage. The smaller the value of t, the larger the amount of shrinkage.\n8"}, {"heading": "3.3 Solving the Function", "text": "In order to solve our dual objective function associated with constraints, we resort the Lagrangemultipliers method and following this, Eq. (7) can be rephrased as follows:\nL(u, \u03bb) =uT ( VL\u0303V T \u2212 \u03b1C ) u\u2212 \u03bb ( uTVDVTu\u2212 1 ) (8)\nof which, to simplify notations, we have denoted L\u0303 = L\u2212 \u2212 L+, D = D+ and \u03b1 is used in replacement for t as there is a one-to-one correspondence between them [13]. Taking the derivative of L(u, \u03bb) with respect to vector u yields:\n\u2202L(u, \u03bb)\n\u2202u = 2\n( VL\u0303V T \u2212 \u03b1C ) u\u2212 2\u03bbVDVTu (9)\nAnd equating it to zero leads to the generalized eigenvalue problem: ( VL\u0303V T \u2212 \u03b1C ) u = \u03bbVDVTu (10)\nIt is noticed that V is a singular matrix and its rank is at most min(n,m), making the combined matrix on the right hand side not directly invertible. We therefore decompose VD1/2 into P\u03a3QT , where columns in P and Q are respectively called the left and right (orthonormal) singular vector of VD1/2 while \u03a3 stores its singular values. Note that this decomposition is always possible since D is a non-negative diagonal matrix of node degrees. Additionally, both P and Q can be represented in the square matrices while \u03a3 a rectangular one of n\u00d7m size according to the most general decomposition form in [6]. Following this, the combined matrix on the right hand size can be rewritten as:\nVDVT = P\u03a32PT (11)\nAnd in order to get a stable solution, we keep the top ranked singular values in \u03a3 such as their summation explains for no less than 95% of the total singular values1. Let us denote B\u2217 = P\u03a3\u22122PT as the inversion of the right hand side and before showing our optimal solution, we need the following proposition:\nProposition 1. Let P be the matrix of left singular vectors of VD1/2 defined above, then its row vectors are also orthogonal, i.e., PPT = I\nProof. Let a be an arbitrary vector, we need to show PPTa = a. Due to the orthogonal property of left singular vectors, it is true that PTP = I. The inversion of P therefore is equal to PT and given arbitrary vector a, there is a uniquely determined vector b such that Pb = a. Consequently,\nPPTa = PPTPb = Pb = a\nIt follows that PPT = I since a is an arbitrary vector.\nTheorem 1. Given B = P\u03a32PT , we have BB\u2217 = I\n1 Note that since (VD1/2)(VD1/2)T is Hermitian and positive semidefinite, the diagonal entries in \u03a3 are always real and nonnegative.\n9 Proof. The proof of this theorem is straightforward given Proposition 1.\nNow, for simplicity, let us denote A for the combined matrix (VL\u0303V T \u2212\u03b1C), then it is straightforward to see that u turns out to be the eigenvector of the equation:\nB\u2217A = \u03bbu (12)\nwith the maximum value is given by the following theorem.\nTheorem 2. Given matrix A = VL\u0303V T \u2212 \u03b1C and B = VDVT defined above, the maximum value of uTAu subjected to uTBu = 1 is the largest eigenvalue of B\u2217A.\nProof. Due to Theorem 1, it is straightforward to see that:\nuTAu = uTBB\u2217Au\nOn the other hand, uTBB\u2217Au = uTB\u03bbu by equation Eq. (12) and further taking into account our second constraint, it follows that:\nmax u:uTBu=1\n{uTAu} = max{\u03bb}\nFrom this theorem, it is safe to say that u\u2217 = u1 as the first eigenvector of B\u2217A corresponding to its largest eigenvalue \u03bb1 is our optimal solution. Since eigenvectors and eigenvalues go in pair, the second optimal solution is the second eigenvector u2 corresponding to the second largest eigenvalue \u03bb2 and so on. Consequently, in the general case, if d is the number of unique global network states, our optimal transformed space is the one spanned by the top d eigenvectors. In the next section, we present a method to select optimal features/nodes along with the subnetworks formed by these nodes."}, {"heading": "3.4 Subnetwork Selection", "text": "In essence, our top d eigenvectors play the role of space transformation which projects network data from the original high dimensional space into the induced subspace of d dimensions. Their coefficients essentially reflect how the original nodes (features) have been combined or more specifically, the degree of node\u2019s importance in contributing to the subspace that optimally discriminates network instances. Following the approach adopted in [8] with c as the user parameter, we select top c entries in each {ui} d i=1 corresponding to the selective nodes. Nonetheless, it is possible that there will be more than c nodes selected by combining from d eigenvectors. Therefore, in practice, we may use a simple approach by first selecting the largest absolute entries across d eigenvectors:\nv = {v1, . . . , vn} where vp = max i |ui,p| (13)\nwhere ui,p is the p-th entry of eigenvector ui, and then selecting nodes according to the top c ranking entries in v. The subnetworks forming from these nodes can be straightforwardly obtained by matching to the nodes in our generalized network N defined in Definition 2, along with their connecting edges stored in E. These subnetworks can be visualized which offers the user an intuitive way to examine the results.\n10"}, {"heading": "3.5 Computational Complexity", "text": "We name our algorithm SNL, an acronym stands for SubNetwork spectral Learning. Its computation complexity is analyzed as follows. We first need to compute edges\u2019 weights according to Definition 2 to build our first meta-graph which takes O(n2m) since there are at most n(n \u2212 1)/2 edges in the generalized networkN . Second, in building the two subsequent meta-graphs, the cosine distance between any two network instances is computed which amounts to O(n2m) or O(mn log n) in case the multidimensional binary search tree is used [3]. Also, since the size of matrix VD1/2 is m\u00d7 n, its singular value decomposition takes O(mn log n) with the Lanczos technique [12]. Likewise, the eigen-decomposition of the matrix B\u2217A takes O(n2 logn) since its size is n\u00d7 n. Therefore, in combination, the overall complexity is at most O(n2m + n2 logn) assuming that the number of nodes is larger than the number of network instances."}, {"heading": "4 Empirical Studies", "text": ""}, {"heading": "4.1 Datasets and Experimental Setup", "text": "We compare the performance of SNL against MINDS [25] which is among the first approaches formally addressing the global-state network classification problem by a subnetwork sampling. Another algorithm for comparison is the Network Guided Forests (NGF) [11] designed specifically for protein protein interaction (PPI) networks. We use both synthetic and real world datasets for experimentation. Since global states are available in all datasets, we compare average accuracy in 10-fold cross validation for synthetic data, and 5-fold cross validation for real data (due to smaller numbers of network instances). For SNL, the cross validation is further used to select its optimal \u03b1 parameter (shortly discussed below). Unless otherwise indicated, we set k = 10 and use the linear-SVM to perform training and testing in the transformed space (keeping top 50 nodes) in SNL. We set MINDS\u2019 parameters as follows: 10000 sampling iterations, 0.8 discriminative potential threshold and K = 200 as recommended in the original paper [25]. The Gini index is used for the tree building in NGF and we set its improvement threshold \u01eb = 0.02 [11]."}, {"heading": "4.2 Results on Synthetic Datasets", "text": "We use synthetic data to evaluate the performance of our technique in training robust classifiers and selecting relevant subnetworks.We generate scale-free backbone networks by preferential attachment of a predefined size adding 20 edges for each new node. The probabilities of backbone edges are sampled from a truncated Gaussian distributions: N(0.9, 0.1) for edges among ground truth nodes (pre-selected nodes of high-correlation with the network state) and N(0.7, 0.1) for the rest of the edges. The weighted backbone serves as our generalized template to generate network instances by independently sampling the existence of every edge based on its probability. The global states are binary Si \u2208 {0, 1} with balanced distribution. We further add noise to both global and local states of ground truth nodes, respectively with levels of 10% and 30%.\n0.5\n0.6\n0.7\n0.8\n0.9\nA cc\nur ac\ny\nVarying |Vgt|: In the first set of experiments, we aim to test whether the performance of all algorithms is affected by the number of ground truth nodes. To this end, we generate 5 datasets by fixing m = 1000 instances, n = 3000 nodes and vary the ground truth nodes |Vgt| from 10 to 50. In Figure 1, we report the average accuracy (and standard deviation) of all algorithms in 10-fold cross validation. As one may observe, SNL performs stably regardless of the change in the ground truth sizes. Compared to the other techniques, its classification is always consistently higher across all cases. The MINDS technique also performs well on this experimental setting yet the NGF seems to be sensitive to the small ground truth sizes. For small |Vgt|, the sampling strategy based on density areas employed in NGF has little chance to select the ground truth nodes, making its accuracy close to a random technique. When more ground truth nodes are introduced, NGF has higher possibility to sample high-utility nodes and in the last two datasets, its performance is on par with that of MINDS. Nonetheless, its accuracy only peaks at 73% in the best case which is lower than 77% in SNL (last column). Varying network size: In the second set of experiments, we evaluate the performance of all algorithms by varying the network sizes. Specifically, we fix m = 3000 network instances, |Vgt| = 50 ground truth nodes and generate 5 datasets having the network size varied from 2000 to 5000 nodes. The classification performance along with the standard deviation is reported in Figure 2. It is possible to see that the performance traits are similar to those in our first set of experiments. SNL\u2019s classification accuracy remains high while that of NGF decreases with the increase of network size. This again can be explained by the extension of the searching subnetwork space, leading to the lower likelihood of both NGF and MINDS in identifying relevant subnetworks with potentially discriminative nodes. The slightly better performance of MINDS (compared to NGF) is due to its accuracy thresholding in selecting candidate substructures. The set of MINDS\u2019 selected trees are thus qualitatively better. Nonetheless, as compared to SNL, our subspace learning approach show more competitive results. Moreover, since the low-dimensional subspace learnt in SNL is unique and linearly combined from the most discriminative nodes, its performance also shows more stable, indicated by the small standard deviation across all cases.\nEffect of network topology: In order to provide more insights into the performance of SNL, we further test the network effect. As presented in Section 3,\n12\n\u03b1 is the parameter controlling the influence of the network information on the subspace learning process. The higher the \u03b1, the more preference putting on the heavily connected nodes. We report in Figures 3(a),4(a) the accuracy of SNL by varying \u03b1 from 0.1 to 6.5 and in Figures 3(b),4(b) its ability in discovering the ground truth nodes. For the latter case, we validate the performance through the usage of area under the ROC curve (AUC) [13].\nAs expected, incorporating the network structure in the subspace learning process improves both classification rate and the AUC in uncovering the ground truth nodes. The plots in Figures 3(a),4(a) show that the accuracy initially improves for increasing influence of the network (\u03b1 \u2264 5) and then decreases as the network component becomes prevalently dominant (\u03b1 > 5). This is because for large \u03b1, SNL tends to incorporate irrelevant nodes solely based on their strong connections to the neighbors (yet their local values might not help classifying global state values). Another notable observation is that, in larger instances or ground truth feature sets, the optimal \u03b1 tends to increase as well. Moreover, the values of \u03b1 that maximize classification accuracy also result in optimal AUC in identifying the ground truth nodes (Fig. 3(b),4(b)). These experiments clearly show the helpful information provided by the network topology in uncovering the groundtruth features. Also, we exclude NGF and MINDS from these experiments (to save space) and leave the discussion over their AUC performance with the real-world datasets."}, {"heading": "4.3 Real-world Datasets", "text": "We use 4 real-world datasets to evaluate the performance of SNL and its competing methods. The features in all datasets correspond to micro-array expression measurements of genes; the topology structures relating features correspond to\n13\ngene interaction networks; and the global network states correspond to phenotypic traits of the subjects/instances. The statistics of our datasets are listed in Table 1. Two of our real-world datasets, breast cancer and embryonic development, were also used for experimentation in the original NGF method [11]. Our other datasets come from a study on maize properties [14] and a human liver metastasis study [19] combined with a functional network [9]. The network samples are used as provided in the original studies, except for maize where we down-sample one of the classes to balance the global state distribution.\nClassification performance: The comparison of classification accuracy for all techniques and datasets is presented in Figure 5(a). We report the average accuracy and standard deviation from the 5-fold stratified cross validation. All techniques perform competitively on the breast cancer data, achieving more than 70% of classification accuracy on average. The accuracy of SNL dominates significantly that of the sampling techniques on the embryonic and maize datasets (at least 15% and 10% improvement respectively) and less so in the liver dataset. The separation is highest in the datasets of small number of instances and big number of feature nodes \u2013 the settings in which SNL is particularly effective. Beyond average performance improvement, SNL\u2019s accuracy is also more stable across all folds as it considers the global network structure when learning a subspace for classification, while the alternatives perform sampling in the exponential space of substructures. Subnetwork discovery: Unlike the synthetic datasets where we can control the ground truth network features, it is generally much harder to obtain ground truth subnetworks for real world datasets. However, as an attempt to look deeper into the results, we choose the Liver metastasis and further investigate the meaningful subnetworks generated by the SNL. For this dataset, out of top 50 nodes of highest coefficient values (ref. Section 3.4), about one third of the nodes are connected into four subnetworks. We depict in Figure 5(b) the two largest ones which respectively contain 7 and 4 connected gene nodes. Among these selected\n14\nsubnetworks, the genes REG1A and REG3A are particularly interesting since they are in agreement with the ones found in [19] which was shown to be involved in the liver metastasis cancer. As a comparison against MINDS and NGF, we notice that both methods generate multiple binary-trees where each node has only a single parent. Moreover, while SNL can provide a natural rank of important genes based on their coefficients (from the learnt subspace), it is less trivial to define important genes from NGF and MINDS as they both generate thousands of trees. For the purpose of measuring biological relevance of obtained genes, we define a ranking for these competing techniques based on the frequency of genes appeared in the generated trees. For comparison, we select 46 metastasis-specific genes identified in [19] to serve as a ground truth set (39 intersect with our network and expression data) and plot the ROC performance of all algorithms in Figure 5(c). Note that, this is only a partial ground truth set, since identifying all genes associated with this disease is a subject of ongoing research [19]. It is observed that the ranking produced by SNL includes more ground truth genes than those of NGF and MINDS at increasing false-positive rates. The higher true positive rates of SNL makes it a better method for identifying new genes associated with the phenotype of interest. In practice, this is an important feature of the algorithm since validating even a single gene related to cancer is both timewise and financially costly. As shown in Figure 5(c), while the ROC performance of NGF and MINDS are only at 0.59 and 0.57 AUC, that value of SNL is 0.69 which clearly demonstrates large gap of better performance."}, {"heading": "5 Related Work", "text": "Mining discriminative subspaces from global-state networks is a novel and challenging problem. Two lines of work close to this problem are network classification and mining evolving subgraphs from dynamic network data. In the network classification case, most representative algorithms are LEAP [28], graphSig [26], GAIA [17] and COM [16] which generally assume a database consisting of positive and negative networks that need to be classified. These approaches, though diverse in terms of their underlying algorithms, all aim at extracting a set significant subnetworks that are more frequent in one class of positive networks and less frequent in the negative class. Different from the above problems, we aim to mine subnetworks which are represented in all network instances; yet the node values along with the network structures can discriminate the global states of the networks. Another line of related research focuses on mining dynamic evolving subnetworks [23,4,5]. The problem in this case is to obtain subnetworks over time that evolve significantly (outliers) from other network locations. This setting therefore do not model the problem developed in this paper since the dynamic network snapshots neither contain global-state values nor can remove their temporal property.\nSeveral studies in systems biology have indicated the critical role of the network structure in identifying protein modules related to clinical outcomes, for both regression [22,24,21] and classification [11,25]. In the classification setting which is related to our study, the NGF [11] is an ensemble approach that builds a forest of trees jointly voting for the class of a network instance. Resided at\n15\nthe NGF\u2019s core is the CART (classification and Regression tree) technique and in order to build a decision tree within the PPI network, NGF starts with a root node and progressively includes connected nodes as long as the improvement in class separation (measured by Gini index) is no smaller than a given threshold. The study in [25] is the first one to formally introduce the problem of subnetwork mining in global-state networks and further propose the MINDS algorithm to solve it. Similar to NGF, MINDS adopts network-constraint decision trees and is also an ensemble classifier. Nonetheless, it increases the quality of decision trees by developing a novel concept of editing map over the space of potential subnetworks and exploits Monte Carlo Markov Chain sampling over this novel data structure to seek decision trees with maximum classification potential. Unlike the frequency-based and sampling classification discussed above, our approach is fundamentally different as it searches for the most discriminative subnetworks in a single low dimensional subspace through the spectral learning technique, which generally leads to more stable and high-accuracy performance."}, {"heading": "6 Conclusion", "text": "We proposed a novel algorithm named SNL to address the challenging problem of uncovering the relationship between local state values residing on nodes and the global network events. While most existing studies address this problem by sampling the exponential subnetworks space, we adopt an efficient and effective subspace transformation approach. Specifically, we define three meta-graphs to capture the essential neighboring relationships among network instances and devise a spectral graph theory algorithm to learn an optimal subspace in which networks with different global-states are well separated while the common structure across samples is smoothly respected to enable subnetwork discovery. Through experimental analysis on synthetic data and real-world datasets, we demonstrated its appealing performance in both classification accuracy and the real-world relevance of the discovered discriminative subnetwork features. Acknowledgements: The research work was supported in part by the NSF (IIS-1219254) and the NIH (R21-GM094649)."}], "references": [{"title": "A survey of clustering algorithms for graph data", "author": ["C.C. Aggarwal", "H. Wang"], "venue": "In Managing and Mining Graph Data, pages 275\u2013301.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "oddball: Spotting anomalies in weighted graphs", "author": ["L. Akoglu", "M. McGlohon", "C. Faloutsos"], "venue": "In PAKDD (2), pages 410\u2013421,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "18(9):509\u2013517,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1975}, {"title": "M", "author": ["P. Bogdanov", "C. Faloutsos"], "venue": "Mongiov\u0300\u0131, E. E. Papalexakis, R. Ranca, and A. K. Singh. Netspot: Spotting significant anomalous regions on dynamic networks. In SDM, pages 28\u201336,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["P. Bogdanov"], "venue": "Mongiov\u0300\u0131, and A. K. Singh. Mining heavy subgraphs in timeevolving networks. In ICDM, pages 81\u201390,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Computation of the Singular Value Decomposition", "author": ["A.K. Cline", "I.S. Dhillon"], "venue": "Handbook of Linear Algebra, CRC Press,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Discriminative features for identifying and interpreting outliers", "author": ["X.H. Dang", "I. Assent", "R.T. Ng", "A. Zimek", "E. Schubert"], "venue": "In ICDE, pages 88\u201399,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Local outlier detection with interpretation", "author": ["X.H. Dang", "B. Micenkov\u00e1", "I. Assent", "R.T. Ng"], "venue": "In ECML/PKDD (3), pages 304\u2013320,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "and A", "author": ["R. Dannenfelser", "N.R. Clark"], "venue": "Ma\u2019ayan. Genes2fans: connecting genes through functional association networks. BMC bioinformatics, 13(1):156,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Network discovery via constrained tensor analysis of fmri data", "author": ["I.N. Davidson", "S. Gilpin", "O.T. Carmichael", "P.B. Walker"], "venue": "In KDD, pages 194\u2013202,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Protein networks as logic functions in development and cancer", "author": ["J. Dutkowski", "T. Ideker"], "venue": "PLoS Computational Biology, 7(9),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F.V. Loan"], "venue": "The Johns Hopkins University Press, 3rd edition,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Data Mining, Inference, and Prediction.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "et al", "author": ["L. Hui", "P. Zhiyu"], "venue": "Genome-wide association study dissects the genetic architecture of oil biosynthesis in maize kernels. Nature Genetics, 45:43\u201350,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of frequent subgraph mining algorithms", "author": ["C. Jiang", "F. Coenen", "M. Zito"], "venue": "Knowledge Eng. Review, 28(1):75\u2013105,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph classification based on pattern cooccurrence", "author": ["N. Jin", "C. Young", "W. Wang"], "venue": "In CIKM, pages 573\u2013582,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Gaia: graph classification using evolutionary computation", "author": ["N. Jin", "C. Young", "W. Wang"], "venue": "In SIGMOD Conference, pages 879\u2013890,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Empirical comparison of graph classification algorithms", "author": ["N.S. Ketkar", "L.B. Holder", "D.J. Cook"], "venue": "In ICDM, pages 259\u2013266,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Whole genome analysis for liver metastasis gene signatures in colorectal cancer", "author": ["D.H. Ki", "H.-C. Jeung", "C.H. Park", "S.H. Kang", "G.Y. Lee", "W.S. Lee", "N.K. Kim", "H.C. Chung", "S.Y. Rha"], "venue": "Int J Cancer, 121(9):2005\u20132012,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Network-constrained regularization and variable selection for analysis of genomic data", "author": ["C. Li", "H. Li"], "venue": "Bioinformatics, 24(9):1175\u20131182,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Variable selection and regression analysis for graph-structured covariates with an application to genomics", "author": ["C. Li", "H. Li"], "venue": "The Annals of Applied Statistics, 4(3):1498\u20131516,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Mining evolving network processes", "author": ["M. Mongiov\u0300\u0131", "P. Bogdanov", "A.K. Singh"], "venue": "In ICDM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Identifying differentially expressed subnetworks with mmg", "author": ["J. Noirel", "G. Sanguinetti", "P.C. Wright"], "venue": "Bioinformatics, 24(23):2792\u20132793,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining discriminative subgraphs from globalstate networks", "author": ["S. Ranu", "M. Hoang", "A.K. Singh"], "venue": "In KDD, pages 509\u2013517,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Graphsig: A scalable approach to mining significant subgraphs in large graph databases", "author": ["S. Ranu", "A.K. Singh"], "venue": "In ICDE, pages 844\u2013855,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "and B", "author": ["S. Soundarajan", "T. Eliassi-Rad"], "venue": "Gallagher. Which network similarity method should you choose? In Workshop on Information Networks at NYU,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining significant graph patterns by leap search", "author": ["X. Yan", "H. Cheng", "J. Han", "P.S. Yu"], "venue": "In SIGMOD Conference, pages 433\u2013444,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "In the literature, there are a large number of techniques developed to mine useful patterns from network databases, ranging from frequent (sub)networks mining [15], network classification/clustering [1,18] to anomaly detection [2].", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "In the literature, there are a large number of techniques developed to mine useful patterns from network databases, ranging from frequent (sub)networks mining [15], network classification/clustering [1,18] to anomaly detection [2].", "startOffset": 199, "endOffset": 205}, {"referenceID": 17, "context": "In the literature, there are a large number of techniques developed to mine useful patterns from network databases, ranging from frequent (sub)networks mining [15], network classification/clustering [1,18] to anomaly detection [2].", "startOffset": 199, "endOffset": 205}, {"referenceID": 1, "context": "In the literature, there are a large number of techniques developed to mine useful patterns from network databases, ranging from frequent (sub)networks mining [15], network classification/clustering [1,18] to anomaly detection [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 14, "context": "Often, even for the same data mining task, we may need different algorithms to be developed depending on whether the networks are directed or indirected, or whether the data resides at nodes, edges or both of them [15].", "startOffset": 214, "endOffset": 218}, {"referenceID": 19, "context": "Such a class of global-state network data can be used to model a number of real-world applications ranging from opinion evolution in social networks [20], regulatory networks in biology [21] to brain networks in neuroscience [10].", "startOffset": 186, "endOffset": 190}, {"referenceID": 9, "context": "Such a class of global-state network data can be used to model a number of real-world applications ranging from opinion evolution in social networks [20], regulatory networks in biology [21] to brain networks in neuroscience [10].", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "These local effects may jointly encode a logical function that determines the occurrence of a disease [21,25].", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "These local effects may jointly encode a logical function that determines the occurrence of a disease [21,25].", "startOffset": 102, "endOffset": 109}, {"referenceID": 25, "context": "It is worth mentioning that, though existing other measures for network data [27], our using of cosine distance is motivated by the observation that we can view each node as a single feature and thus the network data can be essentially considered as a special case of very high dimensional data.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "As such, the symmetric cosine measure can be effectively used though obviously the other ones [27] can also be directly applied here.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Also, D is the diagonal matrix whose D+ii = \u2211 j A + ij and we have defined L + = D \u2212A, which can be shown to be the Laplacian matrix [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Indeed, this quadratic L2-norm is a kind of regularization which is often referred to as the ridge shrinkage in statistics for regression [13,7].", "startOffset": 138, "endOffset": 144}, {"referenceID": 6, "context": "Indeed, this quadratic L2-norm is a kind of regularization which is often referred to as the ridge shrinkage in statistics for regression [13,7].", "startOffset": 138, "endOffset": 144}, {"referenceID": 12, "context": "of which, to simplify notations, we have denoted L\u0303 = L \u2212 L, D = D and \u03b1 is used in replacement for t as there is a one-to-one correspondence between them [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "Additionally, both P and Q can be represented in the square matrices while \u03a3 a rectangular one of n\u00d7m size according to the most general decomposition form in [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 7, "context": "Following the approach adopted in [8] with c as the user parameter, we select top c entries in each {ui} d i=1 corresponding to the selective nodes.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Second, in building the two subsequent meta-graphs, the cosine distance between any two network instances is computed which amounts to O(nm) or O(mn log n) in case the multidimensional binary search tree is used [3].", "startOffset": 212, "endOffset": 215}, {"referenceID": 11, "context": "Also, since the size of matrix VD is m\u00d7 n, its singular value decomposition takes O(mn log n) with the Lanczos technique [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 23, "context": "We compare the performance of SNL against MINDS [25] which is among the first approaches formally addressing the global-state network classification problem by a subnetwork sampling.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Another algorithm for comparison is the Network Guided Forests (NGF) [11] designed specifically for protein protein interaction (PPI) networks.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "8 discriminative potential threshold and K = 200 as recommended in the original paper [25].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "02 [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "For the latter case, we validate the performance through the usage of area under the ROC curve (AUC) [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Two of our real-world datasets, breast cancer and embryonic development, were also used for experimentation in the original NGF method [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Our other datasets come from a study on maize properties [14] and a human liver metastasis study [19] combined with a functional network [9].", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "Our other datasets come from a study on maize properties [14] and a human liver metastasis study [19] combined with a functional network [9].", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "Our other datasets come from a study on maize properties [14] and a human liver metastasis study [19] combined with a functional network [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 18, "context": "subnetworks, the genes REG1A and REG3A are particularly interesting since they are in agreement with the ones found in [19] which was shown to be involved in the liver metastasis cancer.", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "For comparison, we select 46 metastasis-specific genes identified in [19] to serve as a ground truth set (39 intersect with our network and expression data) and plot the ROC performance of all algorithms in Figure 5(c).", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "Note that, this is only a partial ground truth set, since identifying all genes associated with this disease is a subject of ongoing research [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "In the network classification case, most representative algorithms are LEAP [28], graphSig [26], GAIA [17] and COM [16] which generally assume a database consisting of positive and negative networks that need to be classified.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "In the network classification case, most representative algorithms are LEAP [28], graphSig [26], GAIA [17] and COM [16] which generally assume a database consisting of positive and negative networks that need to be classified.", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "In the network classification case, most representative algorithms are LEAP [28], graphSig [26], GAIA [17] and COM [16] which generally assume a database consisting of positive and negative networks that need to be classified.", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "In the network classification case, most representative algorithms are LEAP [28], graphSig [26], GAIA [17] and COM [16] which generally assume a database consisting of positive and negative networks that need to be classified.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "Another line of related research focuses on mining dynamic evolving subnetworks [23,4,5].", "startOffset": 80, "endOffset": 88}, {"referenceID": 3, "context": "Another line of related research focuses on mining dynamic evolving subnetworks [23,4,5].", "startOffset": 80, "endOffset": 88}, {"referenceID": 4, "context": "Another line of related research focuses on mining dynamic evolving subnetworks [23,4,5].", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "Several studies in systems biology have indicated the critical role of the network structure in identifying protein modules related to clinical outcomes, for both regression [22,24,21] and classification [11,25].", "startOffset": 174, "endOffset": 184}, {"referenceID": 22, "context": "Several studies in systems biology have indicated the critical role of the network structure in identifying protein modules related to clinical outcomes, for both regression [22,24,21] and classification [11,25].", "startOffset": 174, "endOffset": 184}, {"referenceID": 19, "context": "Several studies in systems biology have indicated the critical role of the network structure in identifying protein modules related to clinical outcomes, for both regression [22,24,21] and classification [11,25].", "startOffset": 174, "endOffset": 184}, {"referenceID": 10, "context": "Several studies in systems biology have indicated the critical role of the network structure in identifying protein modules related to clinical outcomes, for both regression [22,24,21] and classification [11,25].", "startOffset": 204, "endOffset": 211}, {"referenceID": 23, "context": "Several studies in systems biology have indicated the critical role of the network structure in identifying protein modules related to clinical outcomes, for both regression [22,24,21] and classification [11,25].", "startOffset": 204, "endOffset": 211}, {"referenceID": 10, "context": "In the classification setting which is related to our study, the NGF [11] is an ensemble approach that builds a forest of trees jointly voting for the class of a network instance.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "The study in [25] is the first one to formally introduce the problem of subnetwork mining in global-state networks and further propose the MINDS algorithm to solve it.", "startOffset": 13, "endOffset": 17}], "year": 2014, "abstractText": "Data mining practitioners are facing challenges from data with network structure. In this paper, we address a specific class of global-state networks which comprises of a set of network instances sharing a similar structure yet having different values at local nodes. Each instance is associated with a global state which indicates the occurrence of an event. The objective is to uncover a small set of discriminative subnetworks that can optimally classify global network values. Unlike most existing studies which explore an exponential subnetwork space, we address this difficult problem by adopting a space transformation approach. Specifically, we present an algorithm that optimizes a constrained dualobjective function to learn a low-dimensional subspace that is capable of discriminating networks labelled by different global states, while reconciling with common network topology sharing across instances. Our algorithm takes an appealing approach from spectral graph learning and we show that the globally optimum solution can be achieved via matrix eigen-decomposition.", "creator": "gnuplot 4.6 patchlevel 3"}}}