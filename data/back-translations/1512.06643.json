{"id": "1512.06643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "The 2015 Sheffield System for Transcription of Multi-Genre Broadcast Media", "abstract": "We describe the University of Sheffield's system for participating in the Multi-Genre Broadcast (MGB) Challenge task 2015, the transcription of cross-genre broadcasts. Transcription was one of four tasks proposed in the MGB Challenge, with the aim of advancing the state of the art in automatic speech recognition, speaker diarization and automatic subtitling for broadcast media. In this thesis, four topics are examined: data selection techniques for training with unreliable data, automatic speech segmentation of broadcast broadcasts, acoustic modelling and adaptation in highly variable environments, and voice modelling of multi-genre broadcasts. The final system works in multiple passes, using an unadapted decryption level to fine-tune segmentation, followed by three adapted passes: a hybrid DNN pass with input functions normalized by speaker-based cephalic normalization, and a further hybrid input stage with loudspeaker functions normalized by MR.", "histories": [["v1", "Mon, 21 Dec 2015 14:31:31 GMT  (44kb,D)", "http://arxiv.org/abs/1512.06643v1", "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA"]], "COMMENTS": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oscar saz", "mortaza doulaty", "salil deena", "rosanna milner", "raymond w m ng", "madina hasan", "yulan liu", "thomas hain"], "accepted": false, "id": "1512.06643"}, "pdf": {"name": "1512.06643.pdf", "metadata": {"source": "CRF", "title": "THE 2015 SHEFFIELD SYSTEM FOR TRANSCRIPTION OF MULTI\u2013GENRE BROADCAST MEDIA", "authors": ["Oscar Saz", "Mortaza Doulaty", "Salil Deena", "Rosanna Milner", "Raymond W.M. Ng", "Madina Hasan", "Yulan Liu", "Thomas Hain"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Multi\u2013genre broadcasts, automatic speech recognition, data selection, speech segmentation, acoustic adaptation, language adaptation."}, {"heading": "1. INTRODUCTION", "text": "Audio-visual media is an area of high interest for research in a variety of topics related to computer vision, speech processing and natural language processing. The ability to search into vast media archives, browse through thousands of hours of recordings or structure the complete resources of a media company would significantly increase the efficiency of these organisations and the services provided to the end users.\nFrom the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3]. However, other types of broadcast media shows have not been so widely explored. The transcription of multigenre data is a complex task due to the large amounts of variability arising from multiple, diverse speakers, the variety of acoustic and recording conditions and the lexical and linguistic diversity of the topics covered [4].\nEvaluations of technology covering different aspects of research in audio-visual media have been a major driver behind some of the\nThis work was supported by the EPSRC Programme Grant EP/I031022/1 (Natural Speech Technology).\nmost recently achieved results in audio-visual media processing. The MediaEval evaluation campaign [5] has brought together researchers from many areas to work in automatic classification and retrieval of broadcast data. Evaluation series such as the NIST-organised Hub4 tasks [6] helped start the earlier efforts in broadcast news transcriptions in English, while the Topic Detection and Tracking (TDT) campaign [7] expanded this work to other tasks related to broadcast news. More recently, the Ester campaigns [8] have created increased interest in the transcription of French broadcast news and the Albayzin campaigns [9] have pushed the efforts in audio processing of Spanish broadcast news.\nFollowing these efforts, the Multi-Genre Broadcast (MGB) challenge [10] aimed to take on several tasks of an increasing complexity in broadcast media. This work tries to address that with advances in several areas of ASR and its application in a fully functional system for Task 1 of the MGB challenge: Speech-to-text transcription of broadcast television.\nThe rest of the paper is organised as follows: Section 2 describes the experimental setup. Section 3 explains data selection techniques used for acoustic model training. Section 4 introduces new procedures for improved automatic segmentation for ASR. Sections 5 and 6 describe different approaches for acoustic model adaptation and language modelling adaptation for multi-genre shows. Section 7 outlines the final system. Overall results are presented in Section 8. Finally, Section 9 discusses outcomes and concludes the paper."}, {"heading": "2. MGB CHALLENGE - TASK 1", "text": "The MGB challenge 2015 consisted of four different tasks, covering the topics of multi-genre broadcast show transcription, lightly supervised alignment, longitudinal broadcast transcription and longitudinal speaker diarisation. The focus of this work was on Task 1: Speech-to-text transcription of broadcast television, although aspects of the system presented here were used in submissions to other challenge tasks. A full description of this and the other tasks in the challenge can be found in [10], but a brief description of the task is given here.\nParticipation in this task required the automatic transcription of a set of shows broadcast by the British Broadcasting Corporation (BBC). These shows were chosen to cover the multiple genres in broadcast TV, categorised in terms of 8 genres: advice, children\u2019s, comedy, competition, documentary, drama, events and news. Acoustic Model (AM) training data was fixed and limited to more than 2,000 shows, broadcast by the BBC during 6 weeks in April and May of 2008. The development data for the task consisted of 47 shows that were broadcast by the BBC during a week in mid-May 2008. The numbers of shows and the associated broadcast time for training and development data are shown in Table 1.\nar X\niv :1\n51 2.\n06 64\n3v 1\n[ cs\n.C L\n] 2\n1 D\nec 2\n01 5\nAdditional data was available for Language Model (LM) training in the form of subtitles from shows broadcast since 1979 to March 2008, with a total of 650 million words, and referred to as LM1. The subtitles from the 2,000+ shows for acoustic modelling could also be used for LM training, referred to as LM2. Statistics for these 2 sets can be seen in Table 2."}, {"heading": "2.1. Common system description", "text": "Throughout this work, two different types of systems were used. This Section describes the fundamental features for both of them, while specific descriptions will be given in the paper, if further experiments are addressing specific issues.\nThe first types of systems used were Hybrid DNN-HMM systems, built using the Kaldi toolkit [11]. These were based on a Deep Neural Network (DNN) where the input were 5 contiguous spliced frames of Perceptual Linear Prediction (PLP) features of 40 dimensions. Features were obtained by using a linear discriminant analysis transformation of 117 spliced PLP features (from 13 dimensions with a context of 4 to the left and right and middle frame), followed by a global CMLLR transform. Features were transformed using a boosted Maximum Mutual Information (bMMI) discriminative transformation [12], unless otherwise stated. DNNs consisted of 6 hidden layers of 2,048 neurons, and an output layer of 6,478 triphone state targets. State-level Minimum Bayes Risk (sMBR) [13, 14] as target functions, unless otherwise mentioned, and Stochastic Gradient Descent (SGD) was used as the optimisation method. Decoding with Hybrid systems was performed in two stages; in the first stage, lattices were generated using a highly pruned 3-gram, and afterwards the lattices were rescored using a complete 4-gram and the 1-best obtained and scored using the official MGB scoring package.\nThe second system types used are so-called Bottleneck DNNGMM-HMM systems built using the TNet toolkit [15] for DNN training and the HTK toolkit [16] for Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) training and decoding. Bottleneck systems used a DNN as a front-end for extracting a set of 26 bottleneck features. Such DNNs took as input 15 contiguous log-filterbank frames and consisted of 4 hidden layers of 1,745 neurons plus the 26-neuron bottleneck layer, and an output layer of 8,000 triphone state targets. sMBR was used for training, unless otherwise stated. Feature vectors for training the GMM-HMM systems were 65-dimensional, including the 26 dimensional bottleneck features, as well as 13 dimensional PLP features together with their\nfirst and second derivatives. GMM-HMM models were trained using 16 Gaussian components per state, and around 8k distinct triphone states. Decoding with Bottleneck systems was also performed in two stages; in a first stage, lattices were generated using a 2-gram, and afterwards these lattices were rescored using a 4-gram and the 1-best obtained and scored with the official MGB scoring package.\nAll decoding experiments were performed using a 50,000-word vocabulary, constructed from the most frequent words in the subtitles as provided for language model training. Pronunciations were obtained using the Combilex pronunciation dictionary[17], which was provided to the challenge participants. When a certain word was not contained in the lexicon, automatically generated pronunciations were obtained using the Phonetisaurus toolkit [18]. These pronunciations were expanded to incorporate pronunciation probabilities, learnt from the alignment of the AM training data [19]. Unless otherwise stated, language models used were obtained by interpolation of several language models trained with the LM1 and LM2 language model data from Table 2. LM training was performed with the SRILM toolkit [20]."}, {"heading": "3. DATA SELECTION AND TRAINING", "text": "One of the main difficulties for transcription in the MGB challenge was the efficient use of the acoustic training data provided, as the use of prior models or other data was not allowed. The transcription of the training data was not created for ASR training purposes. Only the subtitle text broadcast with each show could be used, which is of varying quality for a variety of reasons. An aligned version of the subtitles was provided where the time stamps of the subtitles had been corrected in a lightly supervised manner [10, 21]. After this process, 1,196.73 hours of speech were left available for training.\nThe provided transcripts for the training shows were unreliable in two ways: First, the subtitle text might not always match the actual spoken words; and second, the time boundaries given might have errors arising from the lightly supervised alignment process. This work did not aim to improve on the second aspect, but instead it studied how to perform data selection in order to train with those segments with the most accurate transcripts. An initial selection strategy was based on selecting segments for training based on their Word Matching Error Rate (WMER), a by-product of the semi-supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [10, 21].\nA more complex selection strategy was designed using confidence scores for each segment. The scores were obtained from the posterior probabilities given by a 4-layer DNN trained on the initial selection of data whose targets were 144 monophone states [22]. The inputs to this DNN were 15 contiguous log-Mel-filter-bank frames, and each hidden layer had 1,745 neurons. For each segment in the training set, the monophone state sequence was obtained using forced alignment, and the segment-based confidence measure was calculated as the average of the logarithmic posteriors of each frame for its corresponding monophone state, excluding silence areas.\nTwo different training data setups arose from these two strategies: TRN1, which contained 512.6 hours of speech segments with WMER of 40% or less; and TRN2, which contained 698.9 hours of speech segments with confidence score above \u22123.0. The amount of data per genre in each data training definition can be found in Table 3.\nBoth training strategies were evaluated on the Hybrid and Bottleneck systems, as defined in Section 2.1, in this case using Cross-Entropy (CE) training [23]. Recognition experiments were\nperformed on the manual segmentation available for the development data, with the Word Error Rate (WER) results shown in Table 4. The results indicate that there is a 1% absolute improvement from using TRN2 instead of TRN1, although the gain might have been due mainly to the extra 180 hours of data included in TRN2. The gain was independent of the system setup, and was achieved in both Hybrid and Bottleneck systems."}, {"heading": "4. AUTOMATIC SEGMENTATION", "text": "Automatic speech segmentation is a very important aspect in automatic processing of broadcast media, where the presence of music, applause, laughter and other background sounds can significantly degrade the ability to detect sections containing speech. Errors in segmentation can then propagate as ASR errors in regions of undetected speech or those where speech was incorrectly detected. In this work, a multi-stage automatic segmentation procedure is introduced: an initial segmentation based on DNN posteriors is subsequently improved using the output of an ASR system.\nNNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26]. The neural networks are trained to classify each frame in one of two classes, one corresponding to speech being present and the other one representing speech not being present. One of the challenges in this work\u2019s setup was, as seen in the previous section, the unreliability of the data and the requirement to have efficient data selection strategies. Two strategies were tested to cope with the issue. In the first one, SNS1, all acoustic training data available were used for training the DNN, the originally defined segments were force-aligned to determine which areas were speech and which areas were non-speech. All audio that was not assigned to a speech segment in the original segments was labelled as non-speech. The second strategy, SNS2, took the 512.5 hours from the TRN1 data selection strategy, as defined in Section 3, and used force alignment to label areas as speech and non-speech, without adding any extra non-speech areas. The amount of training data can be seen in Table 5.\nThe segmentation DNN provided, for any given audio output, the estimated values of the posterior probabilities of speech or non-\nspeech for each frame. A two-state HMM was used to smooth this sequence of posteriors to a sequence of valid speech segments, with extra 0.25 seconds added at the beginning and the end of each speech segment. This, with either of the strategies SNS1 or SNS2, gave the initial segmentation used for recognition in the first pass.\nWith the output of decoding based on the original segmentations, a refinement stage was performed as follows. Confidence measures based on the posteriors of a 144-monophone-target DNN were obtained for each word in the hypothesis, as seen for acoustic data selection in section 3. Then, the raw confidence scores were mapped using a decision tree trained on the development data, using decision targets that were either 1 if the word was in an area of speech as defined in the reference segmentation, or 0 if the word was in an area of non-speech. The features to the decision tree were the raw confidence score of each word, the confidence score of the segment, the length of the word (in seconds), the length of the word (in phonemes) and the length of the segment (in seconds). Once the confidences were calculated, words with confidence score below a threshold were removed from the transcript. New segments were redefined then around the remaining words.\nThe results of the this systems are presented in Table 6, in terms of segmentation error: i.e. missed speech and false alarms, and WER for sMBR Hybrid and Bottleneck systems trained on the TRN2 data. Both DNN segmenters produced a significant degradation compared to the use of manually defined segments. However, SNS2 was found to achieve a much larger false alarm rate than SNS1, possibly due to the unbalanced amount of data used for training SNS2. This made SNS2 more suitable for the refinement stage, where areas of false speech detection could be pruned by the use of confidence measures in the ASR output. Table 6 shows how this refinement stage using ASR gave more than 1% absolute improvement over SNS1 and SNS2, despite its segmentation error rate of 9.4%, similar to SNS1 at 9.2%."}, {"heading": "5. ACOUSTIC BACKGROUND MODELLING", "text": "Tackling acoustic variability is one of the main issues arising for multi-genre broadcast transcription. The presence of a large variety of possible recording conditions and acoustic background environments presents a real challenge for ASR systems. In this work, two approaches to compensating for such variability were studied. The first aimed to normalise the background variability in the input to DNNs for hybrid systems, while the second one aimed to use asynchronous Constrained Maximum Likelihood Linear Regression (aCMLLR) transformations [27] for the compensation of dynamic background noises in bottleneck systems."}, {"heading": "5.1. Domain adaptation of hybrid systems", "text": "Adaptation of DNN-based ASR systems is currently one of the most extensively researched areas of speech recognition technology. While several approaches have been evaluated in the past, the normalisation of the input features is most commonly employed. For example, for speaker adaptation, this has been done by directly\ntransforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].\nLatent Dirichlet Allocation (LDA) models have been recently used to model hidden acoustic categories in audio data. In [31], it was shown that LDA is a suitable model for structuring acoustic data from unknown origin, into unsupervised categories, that could be used to provide domain adaptation in ASR. In this work, 64 hidden acoustic domains were found in the acoustic model training data using the LDA model following the procedure in [31]; these domains were found in a unsupervised manner and internally structured the different acoustic conditions of the data. Afterwards, each segment in the training and development sets was assigned to one of these domains. In DNN training, 64 extra features were appended in the input layer, where the domain corresponding to the input frame was codified as a 1\u2013of\u2013N vector. Decoding is performed as usual, with the hidden domain corresponding to the input segment being also appended in the input layer."}, {"heading": "5.2. Dynamic noise adaptation of bottleneck systems", "text": "One of the advantages of tandem (DNN-GMM-HMM) systems is that techniques for adaptation such as Maximum A Posteriori (MAP) or MLLR [32] can be employed. In our previous works, a new HMM topology for asynchronous adaptation of GMM-HMM systems was proposed and shown to produce ASR improvement in the presence of dynamic background conditions [27].\nThis setup was applied to this task and expanded through the use of asynchronous Noise Adaptive Training (aNAT) [33, 27]. First, a global aCMLLR transformation with 8 parallel paths was trained on the whole training data in order to characterise the most common background conditions in this data. Then, the initial sMBR-trained Bottleneck model was retrained in an adaptive training fashion using this aCMLLR transformation. Finally, the global aCMLLR transformation was retrained into show-based aCMLLR transformations using an initial decoding stage in order to more finely characterise the types of noise and background existing in each show, and these transformations were used with the aNAT Bottleneck model to run the final noise-adapted system.\nThe results, including baseline results, for Hybrid systems with domain adaptation and Bottleneck systems with noise adaptation are shown in Table 7 using the manually defined segmentation and for systems trained on TRN2 data. The Hybrid baseline and Hybrid adapted systems were cross-entropy (CE) trained in this case, because sequence training for domain-adapted hybrid DNNs\ndid not complete in time. The domain adapted DNN in the Hybrid setup provided a significant improvement of 1.8% (5.9% relative), which showed the strength of the hidden domain found through the LDA model. For Bottleneck systems, the improvement over the baseline was 1% absolute (3.2% relative) in WER, with balanced improvement across the 8 genres. The experiments in Table 7 were carried out after the challenge and thus were not a part of the final submission."}, {"heading": "6. MULTI\u2013GENRE LANGUAGE MODELLING", "text": "Acoustic variation is not the only source of variability that can be found in multi-genre broadcasts. Lexical and linguistic variability is also present in this data, due to the large variety of topics that are covered in these shows. In order to tackle this linguistic variability, several experiments were designed to improve language modelling in this task.\nOne of the aspects explored in this work is the use of genrespecific LMs. While the subtitles in the LM2 language model training data were already categorised by genre, this information was not available in the much larger LM1 language model training data. In order to automatically derive genre labels for that dataset, genres were automatically inferred using an LDA based approach. First, hidden LDA topics were inferred from the LM2 data where genre labels are present. Given those, a Support Vector Machines (SVM) classifier could be trained that would allow classifying a show into one of the 8 genres using the distribution of LDA hidden topic posteriors as input. These SVMs were used to produce labels for separated chunks of the LM1 training data. The statistics of words assigned to each genre can be seen in Table 9.\nOnce all the data had been classified into genres, genre-based LMs were trained in two different configurations: The first one\nwas based on a Recurrent Neural Network (RNN) LM [34], initially trained on the full LM1 and LM2 training data. This initial RNNLM was then converted into 8 genre\u2013dependent RNNLMs by fine\u2013tuning each one of them to the genre-dependent data. These RNNLMs were used to rescore the lattices obtained by the Hybrid systems using the baseline 4-gram language model. The second one was based on genre-based 4-grams as the interpolation of the genre-independent 4-gram with each genre-dependent 4-gram and was used to rescore lattices in Bottleneck systems. Both systems used manual segmentation and were trained on TRN2.\nThe perplexity and recognition results obtained with the genrespecific LMs are shown in Table 8, along with the results using the baseline LMs. The results show a very significant drop in perplexity when using RNNLMs but only a modest improvement in word error rate of 0.7%. This is consistent with the experiments reported on the same BBC data in [35]. The main difference, however is that in [35], instead of LM1 as background language model, another corpus of 1 billion words was used for language modelling, and different topic models including LDA, were used to classify the text into a set of different genres. As noted above, the LM training data is noisy, both in word accuracy and genre labelling.\nUsing genre-specific n-gram language models yields an improvement of only 0.2% and the perplexity reductions are not as significant. This could be explained by the need to use longer contexts than 4-grams, in order to obtain improvements, which RNNLMs are able to achieve through the use of unrestrained context. It is also interesting to note that genre-specific RNNLMs perform worse than corresponding n-grams on some genres (e.g., comedy and drama). This seems to be related to data sparsity with these two genres having fewer words than the rest as shown in Table 9 and thus the RNNLM fine-tuning does not work very well. The experiments in Table 8 were carried out after the challenge and thus were not a part of the final submission."}, {"heading": "7. SYSTEM DESCRIPTION", "text": "The final system processing as submitted for the the MGB challenge followed the diagram pictured in Figure 1. Each node in the diagram was implemented as a composition of separate modules, each performing specific computation on the speech data.\nThe input audio was split into speech segments using a DNN segmenter based on the SNS2 strategy, as defined in Section 4. These segments were then decoded by an initial, unadapted Hybrid ASR system: ASR-P1, trained on TRN1. The segmentation was afterwards refined using confidence measures in the ASR output as described in section 4. After resegmentation, speaker clustering based on Bayesian Information Criterion (BIC) [36] was performed to assign each speech segment to a given speaker.\nFrom here onwards, three different decoding passes were deployed: ASR-P2-1, ASR-P2-2 and ASR-P2-3, which where based on complementary forms of dealing with speaker and noise variability. ASR-P2-1 was a Hybrid system where the features were normalised using speaker-based Cepstral Mean and Variance Normalisation (CMVN) without requiring any previous transcript. ASRP2-2 was also a Hybrid system, but in this case speaker variability was compensated through the use of fMLLR input features based on the transcript from ASR-P1. Finally, ASR-P2-3 was a Bottleneck\nsystem where asynchronous noise transformations were used as described in Section 5, and speaker-based MLLR transformations were trained on top of this for further speaker and noise factorisation. All these three systems were trained following the sMBR criterion using the TRN2 training data definition.\nThe output of these three passes was finally combined via a Recognition Output Voting Error Reduction (ROVER) [37] procedure."}, {"heading": "7.1. System implementation", "text": "The implementation of the system is based on the Resource Optimisation Toolkit (ROTK), which is developed by the team at the University of Sheffield and was presented initially in [25]. ROTK allows the formulation of functional modules that can be executed in asynchronous fashion using computing grid infrastructure. Systems are defined as a set of modules linked together by directed links transferring data of specific types. This is informally depicted in a graph in Figure 1; the actual modules used are more specific. The system uses metadata to organise how data is processed in an efficient parallelised way through the graph. Each module can split its own tasks into several subtasks based on data, which then can be processed in parallel. The overall dependency structure of these sub-tasks is then automatically inferred. Each module submits jobs on a grid system using the Sun Grid Engine (SGE). The ROTK system allows for simple repeatability of the experiments as the same graph can be executed on multiple datasets such as development and evaluation sets."}, {"heading": "8. RESULTS", "text": "The results of all intermediate passes and the final output are presented in Table 10. In this Table, the gains obtained by the 3 adapted systems in relation to the baseline can be seen, as well as the final gain obtained by the combination of the three outputs. Since the results that lead to the development of the proposed system have already been presented and discussed all through the paper, this Section only reviews the final results achieved by the full system on the development set.\nEvaluating the results per genre, the results vary significantly from News shows, with a 13.2% WER, to Comedy shows, with a 40.9% WER. This highlights the considerable impact of the acoustic variability present in broadcast shows. In terms of gain, Children\u2019s shows achieved the largest improvement from the initial unadapted system, 36.5%, to the final output, 27.7%. This shows how the different techniques proposed for compensating variability worked in complementary ways in one of the most challenging conditions, i.e., where children and adults may appear in the same show and large amounts of music and other backgrounds happen."}, {"heading": "9. CONCLUSION", "text": "In this paper we presented the complete system structure, model training and implementation of the University of Sheffield system for speech\u2013to\u2013text transcription of broadcast media. The system was designed for participation in Task 1 of the MGB challenge. The final result, 27.5% WER, reflects the complexity of the task, especially in the most challenging genres such as comedy or drama shows. It is important to note that these results are obtained without the availability of high quality training data, which is normally available for other related evaluation campaigns. The proposed system has made use of the complementarity of DNN-HMM and DNN-GMM-HMM systems using different adaptation strategies.\nSeveral techniques have been proposed and evaluated. In terms of data selection techniques for acoustic model training, results have shown that adding more data of more quality can provide improvements in both Hybrid and Bottleneck models. The refinement of automatic speech segmentation using the output of an ASR stage is a significant contribution of this system, with the results showing how this can be used to find speech segments that minimise error rates without necessarily minimising segmentation error rates. The two techniques proposed for domain and noise adaptation of acoustic models have shown how complementary techniques can be used successfully. In this work, domain\u2013based input features have been shown to reduce domain variability in Hybrid systems; while asynchronous adaptation with CMLLR transformations performs a similar effect in Bottleneck systems. Finally, language model adaptation to multi\u2013genre shows have been shown to produce slight improvements. In this case, the use of genre\u2013dependent 4\u2013grams does not achieve the gains obtained using genre information in RNNLMs, indicating that more work should be focused on adaptation of RNNs for language modelling."}, {"heading": "10. ACKNOWLEDGEMENTS AND DATA", "text": "We would like to thank others in the MINI research group at Sheffield that have helped to develop this system, with their advice and discussions. We would also like to thank our partners in the NST programme, at the Universities of Cambridge and Edinburgh, for the many discussions which helped us greatly in the development of systems.\nThe audio and subtitle data used for these experiments were distributed as part of the MGB Challenge (www.mgb-challenge.org) through a licence with the BBC. System output and results for the presented system are also available as part of the challenge results to participants."}, {"heading": "11. REFERENCES", "text": "[1] P. C. Woodland, M. J. F. Gales, D. Pye, and S. J. Young, \u201cBroadcast news transcription using HTK,\u201d in Proceedings of\nthe International Conference on Acoustics, Speech and Signal Processing (ICASSP), Munich, Germany, 1997, pp. 719\u2013722.\n[2] J. L. Gauvain, L. Lamel, and G. Adda, \u201cThe LIMSI broadcast news transcription system,\u201d Speech Communication, vol. 37, no. 1\u20132, pp. 89\u2013108, 2002.\n[3] M. J. F. Gales, D. Y. Kim, P. C. Woodland, H. Y. Chan, D. Mrva, R. Sinha, and S. E. Tranter, \u201cProgress in the CUHTK broadcast news transcription system,\u201d IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1513\u20131525, 2006.\n[4] P. Lanchantin, P. Bell, M. Gales, T. Hain, X. Liu, Y. Long, J. Quinnell, S. Renals, O. Saz, and M. Seigel, \u201cAutomatic transcription of multi\u2013genre media archives,\u201d in Proceedings of First Workshop on Speech, Language and Audio in Multimedia, Marseille, France, 2013.\n[5] M. Larson, X. Anguera, T. Reuter, G.J.F. Jones, B. Ionescu, M. Schedl, T. Piatrik, C. Hauff, and M. Soleymani (eds.), Proceedings of the MediaEval 2013 Multimedia Benchmark Workshop, CEUR Workshop Proceedings, 2013.\n[6] D. Pallett, J. Fiscus, J. Garofalo, and M. Przybocki, \u201c1995 Hub\u20134 dry run broadcast materials benchmark test,\u201d in Proceedings of 1996 DARPA Speech Recognition Workshop, 1996.\n[7] C. Cieri, D. Graff, M. Liberman, N. Martey, and S. Strassel, \u201cThe TDT-2 text and speech corpus,\u201d in Proceedings of the 1999 DARPA Broadcast News Workshop, Herndon, VA, 1999.\n[8] S. Galliano, E. Geoffrois, G. Gravier, J. F. Bonastre, D. Mostefa, and K. Choukri, \u201cCorpus description of the ESTER evaluation campaign for the rich transcription of french broadcast news,\u201d in Proceedings of Language Resources and Evaluation Conference, Genoa, Italy, 2006, pp. 139\u2013142.\n[9] M. Zelenak, H. Schulz, and J. Hernando, \u201cSpeaker diarization of broadcast news in albayzin 2010 evaluation campaign,\u201d EURASIP Journal on Audio, Speech and Music Processing, vol. 19, pp. 1\u20139, 2012.\n[10] P. Bell, M.J.F. Gales, T. Hain, J. Kilgour, P. Lanchantin, X. Liu, A. McParland, S. Renals, O. Saz, M. Webster, and P.C. Woodland, \u201cThe MGB challenge: Evaluating multi\u2013genre broadcast media transcription,\u201d in Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Scottsdale, AZ, 2015.\n[11] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, G. Ondrej, G. Nagendra, M. Hanneman, P. Motlicek, Q. Yanmin, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, \u201cThe Kaldi Speech Recognition Toolkit,\u201d in Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Big Island, HA, 2011.\n[12] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah, \u201cBoosted MMI for model and feature space discriminative training,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), Las Vegas, NV, 2008, pp. 4057\u20134060.\n[13] B. Kingsbury, T. N. Sainath, and H. Soltau, \u201cScalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization,\u201d in Proceeding of ISCA Interspeech, Portland, OR, 2012, pp. 10\u201313.\n[14] Matthew Gibson and Thomas Hain, \u201cHypothesis Spaces For Minimum Bayes Risk Training In Large Vocabulary Speech Recognition,\u201d in Proceedings of ISCA Interspeech, Pittsburgh, PA, 2006, pp. 2406\u20132409.\n[15] K. Vesely, L. Burget, and F. Grezl, \u201cParallel training of neural networks for speech recognition,\u201d in Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 2934\u20132937.\n[16] S. J. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw, L. Liu, G. Moore, J. J. Odell, D. G. Ollason, D. Povey, V. Valtchev, and P. C. Woodland, The HTK Book version 3.4, Cambridge University Engineering Department, 2006.\n[17] K. Richmond, R. Clark, and S. Fitt, \u201cOn generating combilex pronunciations via morphological analysis,\u201d in Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 1974\u20131977.\n[18] J.R. Novak, N. Minematsu, and K. Hirose, \u201cWSFT\u2013based grapheme\u2013to\u2013phoneme conversion: Open source tools for alignment, model\u2013building and decoding,\u201d in Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, San Sebastia\u0301n, Spain, 2012.\n[19] T. Hain, \u201cImplicit modelling of pronunciation variation in automatic speech recognition,\u201d Speech Communication, vol. 46, pp. 171\u2013188, 2005.\n[20] A. Stolcke, \u201cSRILM \u2013 An Extensible Language Modeling Toolkit,\u201d in Proceedings of International Conference on Spoken Language Processing (ICSLP), Denver, CO, 2002, pp. 901\u2013904.\n[21] Y. Long, M. J. F. Gales, P. Lanchantin, X. Liu, M. S. Seigel, and P. C. Woodland, \u201cImproving lightly supervised training for broadcast transcriptions,\u201d in Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 2187\u20132191.\n[22] P. Zhang, Y. Liu, and T. Hain, \u201cSemi\u2013supervised DNN training in meeting recognition,\u201d in Proceedings of IEEE Workshop on Spoken Language Technologies, South Lake Tahoe, CA, 2014.\n[23] G. Hinton, L. Deng, D. Yu, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, G. Dahl, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.\n[24] J. Dines, J. Vepa, and T. Hain, \u201cThe segmentation of multichannel meeting recordings for automatic speech recognition,\u201d in Proceedings of ISCA Interspeech, Pittsburgh, PA, 2006, pp. 1213\u20131216.\n[25] T. Hain, L. Burget, J. Dines, P.N. Garner, F. Grezl, A.E. Hannani, M. Huijbregts, M. Karafiat, M. Lincoln, and V. Wan, \u201cTranscribing meetings with the AMIDA systems,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 486\u2013498, 2012.\n[26] N. Ryant and M. Liberman, \u201cSpeech activity detection on youtube using deep neural networks,\u201d in Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 728\u2013731.\n[27] O. Saz and T. Hain, \u201cAsynchronous factorisation of speaker and background with feature transforms in speech recognition,\u201d in Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 1238\u20131242.\n[28] M.J.F. Gales, \u201cMaximum likelihood linear transformations for HMM-based speech recognition,\u201d Computer Speech & Language, vol. 12, no. 2, pp. 75 \u2013 98, 1998.\n[29] P. Karanasou, M. Gales, and P. Woodland, \u201cI\u2013vector estimation using informative priors for adaptation of deep neural networks,\u201d in Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 2872\u20132876.\n[30] Y. Liu, P. Karanasou, and T. Hain, \u201cAn investigation into speaker informaed DNN front\u2013end for LVCSR,\u201d in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 4300\u20134304.\n[31] M. Doulaty, O. Saz, and T. Hain, \u201cUnsupervised domain discovery using latent Dirichlet allocation for acoustic modelling in speech recognition,\u201d in Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 3640\u20133644.\n[32] M. J. F. Gales and P. C. Woodland, \u201cMean and variance adaptation within the MLLR framework,\u201d Computer, Speech and Language, vol. 10, no. 4, pp. 249\u2013264, 1996.\n[33] O. Kalinli, M.L. seltzer, J. Droppo, and A. Acero, \u201cNoise Adaptive Training for Robust Automatic Speech Recognition,\u201d IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 8, pp. 1889\u20131901, 2010.\n[34] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khundapur, \u201cRecurrent neural network based language model,\u201d in Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 1045\u20131048.\n[35] X. Chen, T. Tan, X. Liu, P. Lanchantin, M. Wan, M. J. F. Gales, and P. C. Woodland, \u201cRecurrent neural network language model adaptation for multi-genre broadcast speech recognition,\u201d in Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 3511\u20133515.\n[36] S. Shaobing Chen and P. S. Gopalakrishnan, \u201cClustering via the Bayesian information criterion with applications in speech recognition,\u201d in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seattle, WA, 1998, pp. 645\u2013648.\n[37] J. Fiscus, \u201cA post\u2013processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER),\u201d in Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Santa Barbara, CA, 1997, pp. 347\u2013 354."}], "references": [{"title": "Broadcast news transcription using HTK", "author": ["P.C. Woodland", "M.J.F. Gales", "D. Pye", "S.J. Young"], "venue": "Proceedings of  the International Conference on Acoustics, Speech and Signal Processing (ICASSP), Munich, Germany, 1997, pp. 719\u2013722.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "The LIMSI broadcast news transcription system", "author": ["J.L. Gauvain", "L. Lamel", "G. Adda"], "venue": "Speech Communication, vol. 37, no. 1\u20132, pp. 89\u2013108, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Progress in the CU- HTK broadcast news transcription system", "author": ["M.J.F. Gales", "D.Y. Kim", "P.C. Woodland", "H.Y. Chan", "D. Mrva", "R. Sinha", "S.E. Tranter"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1513\u20131525, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic transcription of multi\u2013genre media archives", "author": ["P. Lanchantin", "P. Bell", "M. Gales", "T. Hain", "X. Liu", "Y. Long", "J. Quinnell", "S. Renals", "O. Saz", "M. Seigel"], "venue": "Proceedings of First Workshop on Speech, Language and Audio in Multimedia, Marseille, France, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "1995 Hub\u20134 dry run broadcast materials benchmark test", "author": ["D. Pallett", "J. Fiscus", "J. Garofalo", "M. Przybocki"], "venue": "Proceedings of 1996 DARPA Speech Recognition Workshop, 1996.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "The TDT-2 text and speech corpus", "author": ["C. Cieri", "D. Graff", "M. Liberman", "N. Martey", "S. Strassel"], "venue": "Proceedings of the 1999 DARPA Broadcast News Workshop, Herndon, VA, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Corpus description of the ES- TER evaluation campaign for the rich transcription of french broadcast news", "author": ["S. Galliano", "E. Geoffrois", "G. Gravier", "J.F. Bonastre", "D. Mostefa", "K. Choukri"], "venue": "Proceedings of Language Resources and Evaluation Conference, Genoa, Italy, 2006, pp. 139\u2013142.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Speaker diarization of broadcast news in albayzin 2010 evaluation campaign", "author": ["M. Zelenak", "H. Schulz", "J. Hernando"], "venue": "EURASIP Journal on Audio, Speech and Music Processing, vol. 19, pp. 1\u20139, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The MGB challenge: Evaluating multi\u2013genre broadcast media transcription", "author": ["P. Bell", "M.J.F. Gales", "T. Hain", "J. Kilgour", "P. Lanchantin", "X. Liu", "A. McParland", "S. Renals", "O. Saz", "M. Webster", "P.C. Woodland"], "venue": "Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Scottsdale, AZ, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "G. Ondrej", "G. Nagendra", "M. Hanneman", "P. Motlicek", "Q. Yanmin", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Big Island, HA, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Boosted MMI for model and feature space discriminative training", "author": ["D. Povey", "D. Kanevsky", "B. Kingsbury", "B. Ramabhadran", "G. Saon", "K. Visweswariah"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), Las Vegas, NV, 2008, pp. 4057\u20134060.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization", "author": ["B. Kingsbury", "T.N. Sainath", "H. Soltau"], "venue": "Proceeding of ISCA Interspeech, Portland, OR, 2012, pp. 10\u201313.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Hypothesis Spaces For Minimum Bayes Risk Training In Large Vocabulary Speech Recognition", "author": ["Matthew Gibson", "Thomas Hain"], "venue": "Proceedings of ISCA Interspeech, Pittsburgh, PA, 2006, pp. 2406\u20132409.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel training of neural networks for speech recognition", "author": ["K. Vesely", "L. Burget", "F. Grezl"], "venue": "Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 2934\u20132937.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "On generating combilex pronunciations via morphological analysis", "author": ["K. Richmond", "R. Clark", "S. Fitt"], "venue": "Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 1974\u20131977.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "WSFT\u2013based grapheme\u2013to\u2013phoneme conversion: Open source tools for alignment, model\u2013building and decoding", "author": ["J.R. Novak", "N. Minematsu", "K. Hirose"], "venue": "Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, San Sebasti\u00e1n, Spain, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Implicit modelling of pronunciation variation in automatic speech recognition", "author": ["T. Hain"], "venue": "Speech Communication, vol. 46, pp. 171\u2013188, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "author": ["A. Stolcke"], "venue": "Proceedings of International Conference on Spoken Language Processing (ICSLP), Denver, CO, 2002, pp. 901\u2013904.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving lightly supervised training for broadcast transcriptions", "author": ["Y. Long", "M.J.F. Gales", "P. Lanchantin", "X. Liu", "M.S. Seigel", "P.C. Woodland"], "venue": "Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 2187\u20132191.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi\u2013supervised DNN training in meeting recognition", "author": ["P. Zhang", "Y. Liu", "T. Hain"], "venue": "Proceedings of IEEE Workshop on Spoken Language Technologies, South Lake Tahoe, CA, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "G. Dahl", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "The segmentation of multichannel meeting recordings for automatic speech recognition", "author": ["J. Dines", "J. Vepa", "T. Hain"], "venue": "Proceedings of ISCA Interspeech, Pittsburgh, PA, 2006, pp. 1213\u20131216.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Transcribing meetings with the AMIDA systems", "author": ["T. Hain", "L. Burget", "J. Dines", "P.N. Garner", "F. Grezl", "A.E. Hannani", "M. Huijbregts", "M. Karafiat", "M. Lincoln", "V. Wan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 486\u2013498, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech activity detection on youtube using deep neural networks", "author": ["N. Ryant", "M. Liberman"], "venue": "Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 728\u2013731.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Asynchronous factorisation of speaker and background with feature transforms in speech recognition", "author": ["O. Saz", "T. Hain"], "venue": "Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 1238\u20131242.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["M.J.F. Gales"], "venue": "Computer Speech & Language, vol. 12, no. 2, pp. 75 \u2013 98, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "I\u2013vector estimation using informative priors for adaptation of deep neural networks", "author": ["P. Karanasou", "M. Gales", "P. Woodland"], "venue": "Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 2872\u20132876.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "An investigation into speaker informaed DNN front\u2013end for LVCSR", "author": ["Y. Liu", "P. Karanasou", "T. Hain"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 4300\u20134304.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised domain discovery using latent Dirichlet allocation for acoustic modelling in speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 3640\u20133644.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Mean and variance adaptation within the MLLR framework", "author": ["M.J.F. Gales", "P.C. Woodland"], "venue": "Computer, Speech and Language, vol. 10, no. 4, pp. 249\u2013264, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Noise Adaptive Training for Robust Automatic Speech Recognition", "author": ["O. Kalinli", "M.L. seltzer", "J. Droppo", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 8, pp. 1889\u20131901, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1889}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khundapur"], "venue": "Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 1045\u20131048.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition", "author": ["X. Chen", "T. Tan", "X. Liu", "P. Lanchantin", "M. Wan", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 3511\u20133515.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering via the Bayesian information criterion with applications in speech recognition", "author": ["S. Shaobing Chen", "P.S. Gopalakrishnan"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seattle, WA, 1998, pp. 645\u2013648.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "A post\u2013processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER)", "author": ["J. Fiscus"], "venue": "Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Santa Barbara, CA, 1997, pp. 347\u2013 354.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "From the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3].", "startOffset": 187, "endOffset": 193}, {"referenceID": 1, "context": "From the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3].", "startOffset": 187, "endOffset": 193}, {"referenceID": 2, "context": "From the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3].", "startOffset": 276, "endOffset": 279}, {"referenceID": 3, "context": "The transcription of multigenre data is a complex task due to the large amounts of variability arising from multiple, diverse speakers, the variety of acoustic and recording conditions and the lexical and linguistic diversity of the topics covered [4].", "startOffset": 248, "endOffset": 251}, {"referenceID": 4, "context": "Evaluation series such as the NIST-organised Hub4 tasks [6] helped start the earlier efforts in broadcast news transcriptions in English, while the Topic Detection and Tracking (TDT) campaign [7] expanded this work to other tasks related to broadcast news.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "Evaluation series such as the NIST-organised Hub4 tasks [6] helped start the earlier efforts in broadcast news transcriptions in English, while the Topic Detection and Tracking (TDT) campaign [7] expanded this work to other tasks related to broadcast news.", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "More recently, the Ester campaigns [8] have created increased interest in the transcription of French broadcast news and the Albayzin campaigns [9] have pushed the efforts in audio processing of Spanish broadcast news.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "More recently, the Ester campaigns [8] have created increased interest in the transcription of French broadcast news and the Albayzin campaigns [9] have pushed the efforts in audio processing of Spanish broadcast news.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "Following these efforts, the Multi-Genre Broadcast (MGB) challenge [10] aimed to take on several tasks of an increasing complexity in broadcast media.", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "A full description of this and the other tasks in the challenge can be found in [10], but a brief description of the task is given here.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "The first types of systems used were Hybrid DNN-HMM systems, built using the Kaldi toolkit [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Features were transformed using a boosted Maximum Mutual Information (bMMI) discriminative transformation [12], unless otherwise stated.", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "State-level Minimum Bayes Risk (sMBR) [13, 14] as target functions, unless otherwise mentioned, and Stochastic Gradient Descent (SGD) was used as the optimisation method.", "startOffset": 38, "endOffset": 46}, {"referenceID": 12, "context": "State-level Minimum Bayes Risk (sMBR) [13, 14] as target functions, unless otherwise mentioned, and Stochastic Gradient Descent (SGD) was used as the optimisation method.", "startOffset": 38, "endOffset": 46}, {"referenceID": 13, "context": "The second system types used are so-called Bottleneck DNNGMM-HMM systems built using the TNet toolkit [15] for DNN training and the HTK toolkit [16] for Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) training and decoding.", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Pronunciations were obtained using the Combilex pronunciation dictionary[17], which was provided to the challenge participants.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "When a certain word was not contained in the lexicon, automatically generated pronunciations were obtained using the Phonetisaurus toolkit [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "These pronunciations were expanded to incorporate pronunciation probabilities, learnt from the alignment of the AM training data [19].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "LM training was performed with the SRILM toolkit [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "An aligned version of the subtitles was provided where the time stamps of the subtitles had been corrected in a lightly supervised manner [10, 21].", "startOffset": 138, "endOffset": 146}, {"referenceID": 18, "context": "An aligned version of the subtitles was provided where the time stamps of the subtitles had been corrected in a lightly supervised manner [10, 21].", "startOffset": 138, "endOffset": 146}, {"referenceID": 8, "context": "An initial selection strategy was based on selecting segments for training based on their Word Matching Error Rate (WMER), a by-product of the semi-supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [10, 21].", "startOffset": 299, "endOffset": 307}, {"referenceID": 18, "context": "An initial selection strategy was based on selecting segments for training based on their Word Matching Error Rate (WMER), a by-product of the semi-supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [10, 21].", "startOffset": 299, "endOffset": 307}, {"referenceID": 19, "context": "The scores were obtained from the posterior probabilities given by a 4-layer DNN trained on the initial selection of data whose targets were 144 monophone states [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "1, in this case using Cross-Entropy (CE) training [23].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "NNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26].", "startOffset": 67, "endOffset": 75}, {"referenceID": 22, "context": "NNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26].", "startOffset": 67, "endOffset": 75}, {"referenceID": 23, "context": "NNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "The first aimed to normalise the background variability in the input to DNNs for hybrid systems, while the second one aimed to use asynchronous Constrained Maximum Likelihood Linear Regression (aCMLLR) transformations [27] for the compensation of dynamic background noises in bottleneck systems.", "startOffset": 218, "endOffset": 222}, {"referenceID": 25, "context": "transforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "transforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].", "startOffset": 180, "endOffset": 188}, {"referenceID": 27, "context": "transforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].", "startOffset": 180, "endOffset": 188}, {"referenceID": 28, "context": "In [31], it was shown that LDA is a suitable model for structuring acoustic data from unknown origin, into unsupervised categories, that could be used to provide domain adaptation in ASR.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In this work, 64 hidden acoustic domains were found in the acoustic model training data using the LDA model following the procedure in [31]; these domains were found in a unsupervised manner and internally structured the different acoustic conditions of the data.", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "One of the advantages of tandem (DNN-GMM-HMM) systems is that techniques for adaptation such as Maximum A Posteriori (MAP) or MLLR [32] can be employed.", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "In our previous works, a new HMM topology for asynchronous adaptation of GMM-HMM systems was proposed and shown to produce ASR improvement in the presence of dynamic background conditions [27].", "startOffset": 188, "endOffset": 192}, {"referenceID": 30, "context": "This setup was applied to this task and expanded through the use of asynchronous Noise Adaptive Training (aNAT) [33, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 24, "context": "This setup was applied to this task and expanded through the use of asynchronous Noise Adaptive Training (aNAT) [33, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 31, "context": "was based on a Recurrent Neural Network (RNN) LM [34], initially trained on the full LM1 and LM2 training data.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "This is consistent with the experiments reported on the same BBC data in [35].", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "The main difference, however is that in [35], instead of LM1 as background language model, another corpus of 1 billion words was used for language modelling, and different topic models including LDA, were used to classify the text into a set of different genres.", "startOffset": 40, "endOffset": 44}, {"referenceID": 33, "context": "After resegmentation, speaker clustering based on Bayesian Information Criterion (BIC) [36] was performed to assign each speech segment to a given speaker.", "startOffset": 87, "endOffset": 91}, {"referenceID": 34, "context": "The output of these three passes was finally combined via a Recognition Output Voting Error Reduction (ROVER) [37] procedure.", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "The implementation of the system is based on the Resource Optimisation Toolkit (ROTK), which is developed by the team at the University of Sheffield and was presented initially in [25].", "startOffset": 180, "endOffset": 184}], "year": 2015, "abstractText": "We describe the University of Sheffield system for participation in the 2015 Multi\u2013Genre Broadcast (MGB) challenge task of transcribing multi\u2013genre broadcast shows. Transcription was one of four tasks proposed in the MGB challenge, with the aim of advancing the state of the art of automatic speech recognition, speaker diarisation and automatic alignment of subtitles for broadcast media. Four topics are investigated in this work: Data selection techniques for training with unreliable data, automatic speech segmentation of broadcast media shows, acoustic modelling and adaptation in highly variable environments, and language modelling of multi\u2013 genre shows. The final system operates in multiple passes, using an initial unadapted decoding stage to refine segmentation, followed by three adapted passes: a hybrid DNN pass with input features normalised by speaker\u2013based cepstral normalisation, another hybrid stage with input features normalised by speaker feature\u2013MLLR transformations, and finally a bottleneck\u2013based tandem stage with noise and speaker factorisation. The combination of these three system outputs provides a final error rate of 27.5% on the official development set, consisting of 47 multi\u2013genre shows.", "creator": "LaTeX with hyperref package"}}}