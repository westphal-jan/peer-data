{"id": "1601.00248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jan-2016", "title": "Contrastive Entropy: A new evaluation metric for unnormalized language models", "abstract": "Perplexity (per word) is the most widely used metric for evaluating language models. This is mainly due to its ease of calculation, lack of reliance on external tools such as speech recognition pipelines, and a good theoretical justification for why it should work. However, there is no shortage of criticism of this metric. At the heart of this criticism is the lack of correlation with extrinsic metrics such as word error rate (WHO), reliance on common vocabulary for model comparison, and inability to evaluate non-normalized language models. In this paper, we address the final problem of the inability to evaluate un-normalized models by introducing a new discriminatory evaluation metric that predicts the model's performance based on its ability to distinguish between test sets and their deformed version. Due to its discriminatory formulation, this approach can work with un-normalized probabilities while maintaining the ease of calculation.", "histories": [["v1", "Sun, 3 Jan 2016 05:47:42 GMT  (207kb,D)", "https://arxiv.org/abs/1601.00248v1", null], ["v2", "Thu, 31 Mar 2016 13:53:47 GMT  (84kb,D)", "http://arxiv.org/abs/1601.00248v2", "submitted to INTERSPEECH 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kushal arora", "anand rangarajan"], "accepted": false, "id": "1601.00248"}, "pdf": {"name": "1601.00248.pdf", "metadata": {"source": "CRF", "title": "Contrastive Entropy: A new evaluation metric for unnormalized language models", "authors": ["Kushal Arora", "Anand Rangarajan"], "emails": ["anand}@cise.ufl.edu"], "sections": [{"heading": "1. Introduction", "text": "There are two standard evaluation metrics for language models: perplexity and word error rate (WER). The simpler of these two, WER, is the percentage of erroneously recognized words E (deletions, insertions, substitutions) to the total number of words N in a speech recognition task i.e.\nWER = E\nN \u00d7 100%. (1)\nThe second metric, perplexity (per word), is an information theoretic measure that evaluates the similarity between the proposed probability distribution m and the original distribution p. It can be computed as an inverse of the (geometric) average probability of test set T\nPPL(T ) = 1 n \u221a m(T )\n(2)\nwhere n is the number of words in the test set T . In many ways, WER is a better metric. Any improvements on language modeling benchmarks is meaningful only if they translate to improvements in Automatic Speech Recognition (ASR) or Machine Translation. The problem with WER is that it needs a complete ASR pipeline to evaluate. Also, almost all benchmarking datasets are behind a pay-wall, hence not readily available for evaluation.\nPerplexity, on the other hand, is a theoretically elegant and easy to compute metric which correlates well with WER for simpler n-gram models. This makes PPL a good substitute for WER when evaluating n-grams models, but for more complex\nlanguage models the correlation is not so strong [1]. In addition to this, due to its reliance on exact probabilities, perplexity is an unsuitable metric to evaluate unnormalized models for which the partition function is intractable. Also, when comparing two models using perplexity, they must share the same vocabulary.\nMost of the previous work done to improve upon perplexity has been focused on achieving better correlation with WER. Iyer et al. [1] proposed a decision tree based metric that uses additional features like word length, POS tags and phonetic length of words to improve the WER correlation. Chen et al. [2] proposed a new metric M-ref which attempts to learn the likelihood curve between WER and perplexity. Clarkson et al. [3] attempted to use entropy in conjunction with perplexity\u2014 empirically learning the mixing coefficients.\nIn this paper we focus on a different problem, the problem of extending perplexity for unnormalized language models evaluation. We do so by introducing a discriminative approach to language model evaluation. Our approach is inspired by Contrastive Estimation [4] and stems from the philosophical starting point that a superior language model should be able to distinguish better between the sentence from the test set and its deformed version. While we use an unnormalized sentence level model as an example in this paper this technique should work for all models where partition function is intractable, for example unnormalized Model M and feed forward neural network language model (NNLM) from [5] or sentence level models like [6], [7] and [8].\nIn the next section, we give a sketch derivation of perplexity that highlights its word level model assumption. As we will be using a sentence level language model for evaluation, we then move the probability space to sentences and derive an expression for cross entropy rate for sentence level models. In Section 3, we introduce our new discriminative metric, Contrastive Entropy, which removes the normalization requirement associated with perplexity. In Section 4, we formulate recurrent neural networks as sentence level language models that we use for validation and in Section 5 we analyze this new metric across various models on the Pen-TreeBank section of the WSJ dataset. We conclude this paper by hypothesizing a better correlation between WER and contrastive entropy based on the fact they share the same goal of minimizing errors in prediction."}, {"heading": "2. Sentence level cross entropy rate", "text": "The Perplexity defined in equation (2) can also seen as exponentiated cross entropy rate, H(p,m), with cross entropy approximated as\nH(p,m;T ) = \u2212 1 n log(m(T )). (3)\nar X\niv :1\n60 1.\n00 24\n8v 2\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 6\nThis approximation can be derived viewing language as one continuous, infinite stream of words leading to the following expression for cross entropy rate:\nH(p,m) = lim l\u2192\u221e \u22121 l \u2211 wl1\u2208W l 1 p(wl1) log(m(w l 1)). (4)\nwhere W l1 is a set of all the sentences of length l Now, assuming the language to be ergodic and stationary, the Shannon-McMillan-Breiman Theorem [9] states that (4) can be approximated as a single sequence that is long enough, hence\nH(p,m) = \u2212 1 n log(m(wn1 )). (5)\nHere, wn1 is the test set T and n being the number of words in this test set.\nIn this derivation language was seen as an infinite stream of words. If instead, we build a sample space on sentences, then we can define the cross entropy of language as an infinite stream of sentences as\nH(p,m) = lim l\u2192\u221e \u22121 l \u2211 D\u2208Dl1 p(D) log(m(D)).\nDl1 here is a set of all documents containing l sentences. Now, applying the Shannon-McMillan-Breiman Theorem as we did in (5) and assuming that the sentences are independent and identically distributed, we can approximate the cross entropy rate of the sentence level model as\nH(p,m;T ) =\u2212 1 N log(m(T ))\n\u2212 1 N \u2211 Wn\u2208T log(m(Wn)). (6)\nwhere N is the number of sentences in the test set T . As the cross entropy still depends upon the exact probability, equation (6) is still intractable. In the next section, we overcome this problem by defining a discriminative evaluation metric which, instead of trying to minimize the distance between the original distribution p and the proposed distributionm, tries to maximize the discriminative ability of the model towards the test set from its distorted version."}, {"heading": "3. Contrastive Entropy and Contrastive Entropy Ratio", "text": "Let T be the test set. We pass this test set through a noisy channel and let the distorted version of this test set be T\u0302 . We now define the contrastive entropy rate as\nHC(T ; d) = H(T\u0302 ; d)\u2212H(T )\n= \u2212 1 N log\n( p(T\u0302 ; d)\np(T )\n)\n= \u2212 1 N log\n( p\u0303(T\u0302 ; d)\np\u0303(T )\n) . (7)\nHere, d is a measure of the distortion introduced in the test set, p\u0303 is the unnormalized probability and N is the size of the test set, which is the cardinality of words and sentences for word and sentence level models respectively.\nHC /HCR Higher or similar HCR\nLower HCR\nThe intuition behind our evaluation technique is that the distorted test set T\u0302 can be seen as an out of domain text, and that a superior language model should be able to better discriminate in-domain text from the language from the malformed set that are less likely to be generated by the same language source.\nThe metric proposed above still has a major drawback. It is not scale invariant. Let\u2019s say a model M generates a probability distributionm for test set T . We can simply cheat on this metric by proposing a model that exponentiates the probability by a factor of k, i.e. multiplies the entropy by factor of k. This limits the usefulness of the contrastive entropy to intra-model comparison for hyper-parameter optimization.\nWe overcome this issue by reporting an additional value for each model which we term the contrastive entropy ratio. The idea here is to choose a distortion level as baseline, let\u2019s say 10% and report the gain for a higher distortion levels, for example 30% over this baseline distortion :\nHCR(T ; db, d) = Hc(T, d)\nHc(T, db) . (8)\nNeither of the two numbers can provide a complete picture in isolation. Contrastive entropy can be cheated upon by scaling entropy, on the other hand, there is no guarantee that the contrastive entropy ratio would rise faster for a better discriminative model, but together, they balance each other out. Table 1 shows how to interpret these values. A model with higher contrastive entropy and a higher or similar contrastive entropy ratio would mean that it performs better at discriminating the good examples from the bad ones, whereas, a larger contrastive entropy with lower ratio would mean that models use different scales, and a higher ratio with lower cross entropy would not mean much while comparing the two models."}, {"heading": "4. Sentence-level RNNLM", "text": "As the metric we proposed here benchmarks the unnormalized level models, in this section we propose a simple sentence level language model that we can use to show the efficacy of our metric. This new model is simply an unfolded Recurrent Neural Network Language Model [10] build at sentence level and trained to maximize the margin between a valid sentence and its distorted version.\nThe Recurrent Neural Network based Language model can be defined recursively using the following equations\nx(t) = [ w(t\u2212 1)T s(t\u2212 1)T ]T , (9)\ns(t) = f(Ux(t)), and (10) y(t) = g(Ws(t)). (11)\nEquations (9) and (10) can be seen as building latent space representations of phrases using words and history and (11) can be seen as predicting the probability of this word given the context. This phrasal representation built in (9) and (10) then would be treated as the history for the next step. A standard sigmoidal\nnonlinearity is used for f and the probability distribution function g is a standard softmax.\nIf we limit the context to sentence levels and move the probability space to the sequence of the words or n-grams, equation (9) and (10) can be seen as composition function building phrase x(t), of the length n, from sub-phrase x(t \u2212 1), of the length n\u2212 1, and the nth word w(t). Equation (11) can be seen as building the unnormalized probability p\u0303 over the phrase x(t). We can rephrase the equations (9), (10) and (11) as\nx(t) = f ( U [ x(t\u2212 1) w(t) ]) , and (12)\ny(t) = g(Wx(t)). (13)\nHere we use the standard sigmoidal non linearity for the function f and the identity function for g.\nWe now define the score of a length N sentence W as\nS(W ) = N\u2211 t=1 y(t). (14)\nThe probability of the sentence can now be modeled as an exponential distribution\np(W ) = 1 Z e\u2212S(W ). (15)\nwhere Z is the partition function and the contrastive entropy from (7) can be calculated as\nHc(T ) = 1/N \u2211 W\u2208T ( S(W\u0302d)\u2212 S(W ) ) . (16)\nwhere W\u0302d is the distorted version ofW with distortion percentage d.\nTraining is done using a contrastive criterion where we try to maximize the distance between the in-domain sentence and its distorted version. This formulation is similar to one followed by Collobert et al. [8] and Okanohara et al. [7] for language modeling and by Smith and Eisner [4] for POS tagging. Mathematically, we can define this pseudo discriminative training objective as\n\u03b8? = argmin \u03b8 \u2211 d\u2208D max { 0, 1\u2212 S(Wd) + S(W\u0302d) } . (17)\nwhere W\u0302d is the distorted version of sentence Wd and \u03b8 = (U,X,W ) is the parameter of the model.\nThis simplistic sentence level recurrent neural network model is implemented in python using Theano [11] and is available at https://github.com/kushalarora/sentenceRNN.\n5. Experiments We use the Pen Treebank dataset with the following splits and preprocessing: Sections 0-20 were used as training data, sections 21-22 for validation and 23-24 for testing. The training, validation and testing token sizes are 930k, 74k and 82k respectively. The vocabulary is limited to 10k words with all words outside this set mapped to a special token < unk >.\nWe start by examining the distortion generation mechanism. As the evaluation includes the word level models, we need to preserve the word count. To do this, we restrict distortions to only two types: substitution and transpositions. For\nsubstitutions, we randomly swap the current word in the sentence with a random word from the vocabulary. For transposition, we randomly select a word from the same sentence and swap it with the current one. For each word in a sentence, there are three possible outcomes: no distortion with probability xN , substitution with probability xS and transposition with probability xT with xN + xS + xT = 1.\nNow, let\u2019s start by considering the sentence level RNN model proposed in section 4. For contrastive entropy to be a\ngood measure for sentence level models, the following assertions should be true: i) contrastive entropy should monotonically increase with distortions, ii) contrastive entropy of training set should go down with each epoch, and iii) contrastive entropy should increase with increase in training distortion margin. Figures 1, 2 and 3 show that the assertions made above empirically hold. We see a monotonic increase in contrastive entropy with distortion and training distortion margin in Figures 1 and 3 respectively. Figure 2 shows the contrastive entropy increase for training data with epochs. All sentence level RNN model referred above and elsewhere in this paper were trained using gradient descent with learning rate of 0.1 and `2 regularization coefficient of 10\u22123.\nFinally, we would like to compare the standard word level baseline models and our sentence level language model on this new metric. The objective here is to verify the hypothesis that between two language models, the superior one should be able to better distinguish the test sentence from their distorted versions. This is akin to saying that a better language model should have higher contrastive entropy value with similar or higher cross entropy ratio. Tables 2 and 3 shows the results for our experiments. The results were generated using the open source language modeling SRILM toolkit [12] for n-gram models and the RNNLM toolkit [13] for the RNN language model. The RNN model used had 200 hidden layers, with class size of 50. The sRNN-75(10) row in Tables 2 and 3 indicates that the sentence level RNN model was trained with latent space size of 75 and with training distortion level of 10%. All the results here were averaged over 10 runs.\nAs hypothesized, the contrastive entropy rises in Table 2\u2019s columns 2 to 4 and correlates negatively with perplexity for word level models\u2014i.e. the models expected to do better on perplexity do better on Contrastive entropy as well. Rows 4 to 6 compare sentence level RNN models. Here too, as expected, sRNN trained with distortion level of 10% outperforms sRNN trained with distortion margin of 50%. Now, let\u2019s compare word level models to our sentence level model. We can see that sRNN-75(50) performs worse compared to RNN for all levels and worse than 3-gram and 5-gram models for 10% and 30%. This can be attributed to the training distortion margin of 50% which encourages the sRNN to see anything with less than 50% distortion as in-domain sentences. On the other hand sRNN trained with distortion level of 10% performs the best as compared to all other models as it has been tuned to label slightly un-grammatical sentences or ones that have slightly un-natural structure as out of domain.\nTable 3 shows that scaling is not an issue for word level models as ratios are more or less the same. Sentence level models at 10% distortion do better than all the word-level models on both metrics which demonstrates their superior performance.\nsRNN-75(50) is an interesting case. At test distortion level of 30% it is clearly inferior to all word level models as it was trained on a distortion margin of 50%. With 50% test distortion the result is unclear as it does worse on contrastive entropy but better on contrastive ratio."}, {"heading": "6. Conclusion", "text": "In this paper we proposed a new evaluation criteria which can be used to evaluate unnormalized language models and showed, using examples, its efficacy in comparing sentence level models among themselves and to word level models. As both WER and contrastive entropy are discriminative measures, we hypothesize that contrastive entropy should have a better correlation with WER as compared to perplexity.\nWe also proposed a discriminatively trained sentence level formulation of recurrent neural networks which outperformed the current state of the art RNN models on our new metric. We hypothesize that this formulation of RNN does a better job at discriminative tasks like lattice re-scoring as compared to standard RNN and other traditional language modeling techniques. We conclude by restating that a metric is meaningful only if it can measure improvements in real world applications. Further experiments evaluating contrastive entropy\u2019s correlation with the WER and BLEU metrics over a wide range of datasets are required to unquestionably demonstrate the usefulness of this metric. Similarly, to establish superior discriminative ability of sentence level RNNs over standard RNNs, we must compare their performance on real word discriminative tasks like n-best list re-scoring."}, {"heading": "7. References", "text": "[1] R. Iyer, M. Ostendorf, and M. Meteer, \u201cAnalyzing and\npredicting language model improvements,\u201d in Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on. IEEE, 1997, pp. 254\u2013 261.\n[2] S. F. Chen, D. Beeferman, and R. Rosenfeld, \u201cEvaluation metrics for language models,\u201d 1998.\n[3] P. Clarkson, T. Robinson et al., \u201cTowards improved language model evaluation measures.\u201d in EUROSPEECH, 1999.\n[4] N. A. Smith and J. Eisner, \u201cContrastive estimation: Training log-linear models on unlabeled data,\u201d in Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2005, pp. 354\u2013362.\n[5] A. Sethy, S. Chen, E. Arisoy, and B. Ramabhadran, \u201cUnnormalized exponential and neural network lan-\nguage models,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5416\u20135420.\n[6] R. Rosenfeld, S. F. Chen, and X. Zhu, \u201cWhole-sentence exponential language models: a vehicle for linguisticstatistical integration,\u201d Computer Speech & Language, vol. 15, no. 1, pp. 55\u201373, 2001.\n[7] D. Okanohara and J. Tsujii, \u201cA discriminative language model with pseudo-negative samples.\u201d in ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, vol. 45, no. 1, 2007, p. 73.\n[8] R. Collobert and J. Weston, \u201cA unified architecture for natural language processing: Deep neural networks with multitask learning,\u201d in Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.\n[9] P. H. Algoet and T. M. Cover, \u201cA sandwich proof of the Shannon-McMillan-Breiman theorem,\u201d The annals of probability, pp. 899\u2013909, 1988.\n[10] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0300, and S. Khudanpur, \u201cRecurrent neural network based language model.\u201d in INTERSPEECH, 2010, pp. 1045\u20131048.\n[11] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \u201cTheano: a CPU and GPU math expression compiler,\u201d in Proceedings of the Python for Scientific Computing Conference (SciPy), Jun. 2010, oral Presentation.\n[12] A. Stolcke, \u201cSrilm-an extensible language modeling toolkit.\u201d in INTERSPEECH, 2002.\n[13] T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J. Cernocky, \u201cRNNLM-recurrent neural network language modeling toolkit,\u201d in Proc. of the 2011 ASRU Workshop, 2011, pp. 196\u2013201."}], "references": [{"title": "Analyzing and predicting language model improvements", "author": ["R. Iyer", "M. Ostendorf", "M. Meteer"], "venue": "Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on. IEEE, 1997, pp. 254\u2013 261.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Evaluation metrics for language models", "author": ["S.F. Chen", "D. Beeferman", "R. Rosenfeld"], "venue": "1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Towards improved language model evaluation measures.", "author": ["P. Clarkson", "T. Robinson"], "venue": "EUROSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N.A. Smith", "J. Eisner"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2005, pp. 354\u2013362.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Unnormalized exponential and neural network lan-  guage models", "author": ["A. Sethy", "S. Chen", "E. Arisoy", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5416\u20135420.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Whole-sentence exponential language models: a vehicle for linguisticstatistical integration", "author": ["R. Rosenfeld", "S.F. Chen", "X. Zhu"], "venue": "Computer Speech & Language, vol. 15, no. 1, pp. 55\u201373, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "A discriminative language model with pseudo-negative samples.", "author": ["D. Okanohara", "J. Tsujii"], "venue": "ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A sandwich proof of the Shannon-McMillan-Breiman theorem", "author": ["P.H. Algoet", "T.M. Cover"], "venue": "The annals of probability, pp. 899\u2013909, 1988.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1988}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), Jun. 2010, oral Presentation.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Srilm-an extensible language modeling toolkit.", "author": ["A. Stolcke"], "venue": "in INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "RNNLM-recurrent neural network language modeling toolkit", "author": ["T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. Cernocky"], "venue": "Proc. of the 2011 ASRU Workshop, 2011, pp. 196\u2013201.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "This makes PPL a good substitute for WER when evaluating n-grams models, but for more complex language models the correlation is not so strong [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "[1] proposed a decision tree based metric that uses additional features like word length, POS tags and phonetic length of words to improve the WER correlation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] proposed a new metric M-ref which attempts to learn the likelihood curve between WER and perplexity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] attempted to use entropy in conjunction with perplexity\u2014 empirically learning the mixing coefficients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Our approach is inspired by Contrastive Estimation [4] and stems from the philosophical starting point that a superior language model should be able to distinguish better between the sentence from the test set and its deformed version.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "While we use an unnormalized sentence level model as an example in this paper this technique should work for all models where partition function is intractable, for example unnormalized Model M and feed forward neural network language model (NNLM) from [5] or sentence level models like [6], [7] and [8].", "startOffset": 253, "endOffset": 256}, {"referenceID": 5, "context": "While we use an unnormalized sentence level model as an example in this paper this technique should work for all models where partition function is intractable, for example unnormalized Model M and feed forward neural network language model (NNLM) from [5] or sentence level models like [6], [7] and [8].", "startOffset": 287, "endOffset": 290}, {"referenceID": 6, "context": "While we use an unnormalized sentence level model as an example in this paper this technique should work for all models where partition function is intractable, for example unnormalized Model M and feed forward neural network language model (NNLM) from [5] or sentence level models like [6], [7] and [8].", "startOffset": 292, "endOffset": 295}, {"referenceID": 7, "context": "While we use an unnormalized sentence level model as an example in this paper this technique should work for all models where partition function is intractable, for example unnormalized Model M and feed forward neural network language model (NNLM) from [5] or sentence level models like [6], [7] and [8].", "startOffset": 300, "endOffset": 303}, {"referenceID": 8, "context": "where W l 1 is a set of all the sentences of length l Now, assuming the language to be ergodic and stationary, the Shannon-McMillan-Breiman Theorem [9] states that (4) can be approximated as a single sequence that is long enough, hence", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "This new model is simply an unfolded Recurrent Neural Network Language Model [10] build at sentence level and trained to maximize the margin between a valid sentence and its distorted version.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "[8] and Okanohara et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] for language modeling and by Smith and Eisner [4] for POS tagging.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[7] for language modeling and by Smith and Eisner [4] for POS tagging.", "startOffset": 50, "endOffset": 53}, {"referenceID": 10, "context": "This simplistic sentence level recurrent neural network model is implemented in python using Theano [11] and is available at https://github.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "The results were generated using the open source language modeling SRILM toolkit [12] for n-gram models and the RNNLM toolkit [13] for the RNN language model.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "The results were generated using the open source language modeling SRILM toolkit [12] for n-gram models and the RNNLM toolkit [13] for the RNN language model.", "startOffset": 126, "endOffset": 130}], "year": 2016, "abstractText": "Perplexity (per word) is the most widely used metric for evaluating language models. Despite this, there has been no dearth of criticism for this metric. Most of these criticisms center around lack of correlation with extrinsic metrics like word error rate (WER), dependence upon shared vocabulary for model comparison and unsuitability for unnormalized language model evaluation. In this paper, we address the last problem and propose a new discriminative entropy based intrinsic metric that works for both traditional word level models and unnormalized language models like sentence level models. We also propose a discriminatively trained sentence level interpretation of recurrent neural network based language model (RNN) as an example of unnormalized sentence level model. We demonstrate that for word level models, contrastive entropy shows a strong correlation with perplexity. We also observe that when trained at lower distortion levels, sentence level RNN considerably outperforms traditional RNNs on this new metric.", "creator": "LaTeX with hyperref package"}}}