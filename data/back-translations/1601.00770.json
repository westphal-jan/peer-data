{"id": "1601.00770", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures", "abstract": "We present a novel end-to-end neural model for extracting entities and relationships between them. Our relapsing neural network-based model stacks bidirectional sequential LSTM RNNs and bidirectional tree-structured LSTM RNNNNs to capture word sequence and dependency structure information, enabling our model to represent both entities and relationships with common parameters. In addition, we promote the detection of entities during training and the use of entity information related to extraction using curriculum learning and scheduled sampling. Our model improves over the state-of-the-art function-based model based on end-to-end relation extraction and achieves a relative error reduction of the F score in ACE2004 and ACE2005 by 3.5% and 4.8%, respectively. We also show improvements over the state-of-the art, function-based model based on the end-to-end relationship extraction (Task Extraction 2010) with a 2.5% error rating.", "histories": [["v1", "Tue, 5 Jan 2016 08:53:05 GMT  (91kb,D)", "http://arxiv.org/abs/1601.00770v1", "10 pages, 1 figure, 5 tables"], ["v2", "Sat, 19 Mar 2016 02:23:01 GMT  (94kb,D)", "http://arxiv.org/abs/1601.00770v2", "12 pages, 1 figure, 5 tables"], ["v3", "Wed, 8 Jun 2016 01:08:08 GMT  (94kb,D)", "http://arxiv.org/abs/1601.00770v3", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016. 13 pages, 1 figure, 6 tables"]], "COMMENTS": "10 pages, 1 figure, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["makoto miwa", "mohit bansal"], "accepted": true, "id": "1601.00770"}, "pdf": {"name": "1601.00770.pdf", "metadata": {"source": "CRF", "title": "End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures", "authors": ["Makoto Miwa", "Mohit Bansal"], "emails": ["makoto-miwa@toyota-ti.ac.jp", "mbansal@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information. For\ninstance, to learn that Toefting and Bolton have an Organization-Affiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. Previous joint models have employed manual feature based structured learning. An alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models.\nThere are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs fit well to NLP models, since they can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via end-to-end, combined modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures.\nWord sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words are not\nar X\niv :1\n60 1.\n00 77\n0v 1\n[ cs\n.C L\n] 5\nJ an\n2 01\nenough to predict that source and U.S. have an ORGAFF relation in the sentence \u201cThis is ...\u201d, one U.S. source said, and the context word said is required for this prediction. Many traditional, feature-based relation classification models extract features from both sequences and parse trees (Zhou et al., 2005). However, previous RNN-based models focus on only one of these linguistic structures (Socher et al., 2012).\nWe present a novel, end-to-end entity and relation extraction model based on both bidirectional sequential (left-to-right and right-to-left) and bidirectional tree-structured (bottom-up and top-down) LSTM-RNNs, to represent both word sequence and dependency tree structures, and to allow joint modeling of entities and relations in a single model. Our model also incorporates curriculum learning (Bengio et al., 2009) and scheduled sampling (Bengio et al., 2015) to alleviate the problem of lowperformance entity detection in early stages of training, as well as to allow entity information to further help downstream relation extraction. On end-toend entity and relation extraction, we improve over the state-of-the-art feature-based model, with 3.5% (ACE2004) and 4.8% (ACE2005) relative error reductions in F-score. On nominal relation classification (SemEval-2010 Task 8), our model gets a 2.5% relative error reduction in F-score over the state-ofthe-art CNN-based model. Finally, we also ablate and compare our various model components, which leads to some key findings about the contribution and effectiveness of different RNN structures, input dependency relation structures, and joint learning settings."}, {"heading": "2 Related Work", "text": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003). Recently, Huang et al. (2015) showed that building a conditional random field (CRF) layer on the top of bidirectional LSTM-RNNs performs comparably to the state-of-the-art methods in the part-of-speech (POS) tagging, chunking, and NER.\nFor relation classification, in addition to traditional feature/kernel-based approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), several neu-\nral models have been proposed in the SemEval2010 Task 8 (Hendrickx et al., 2010), including embedding-based models (Hashimoto et al., 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models. Xu et al. (2015b) also showed that LSTM-RNNs are useful for relation classification, but the performance was worse than CNN-based models. Li et al. (2015) compared separate sequence-based and tree-structured LSTMRNNs on relation classification, using basic RNN model structures.\nResearch on tree-structured LSTM-RNNs (Tai et al., 2015) fixes the direction of information propagation from bottom to top, and also cannot handle an arbitrary number of typed children as in a typed dependency tree. Furthermore, no RNNbased relation classification model simultaneously uses word sequence and dependency tree information. We propose several such novel model structures and training settings, investigating the simultaneous use of bidirectional sequential and bidirectional tree-structured LSTM-RNNs to jointly capture linear and dependency context for end-to-end extraction of relations between entities.\nAs for end-to-end (joint) extraction of relations between entities, all existing models are featurebased systems (and no NN-based model has been proposed). Such models include structured prediction (Li and Ji, 2014; Miwa and Sasaki, 2014), integer linear programming (Roth and Yih, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). Among these, structured prediction methods are state-of-the-art on several corpora. We present an improved, NN-based alternative for the end-to-end relation extraction."}, {"heading": "3 Model", "text": "We design our model with LSTM-RNNs that represent both word sequences and dependency tree structures, and perform end-to-end extraction of re-\nlations between entities on top of these RNNs. Fig. 1 illustrates the overview of the model. The model mainly consists of three representation layers: a word embeddings layer, a word sequence based LSTM-RNN layer, and finally a dependency subtree based LSTM-RNN layer."}, {"heading": "3.1 Embedding Layer", "text": "The embedding layer handles word embedding representations. nw, np, nd and ne-dimensional vectors v(w), v(p), v(d) and v(e) are embedded to words, partof-speech (POS) tags, dependency types, and entity labels, respectively."}, {"heading": "3.2 Sequence Layer", "text": "The sequence layer represents words in a linear sequence using the representations from the embedding layer. This layer represents sentential context information and maintains entities, as shown in bottom-left part of Fig. 1.\nWe employ bidirectional LSTM-RNNs (Zaremba and Sutskever, 2014) to represent the word sequence in a sentence. The LSTM unit at t-th word consists of a collection of d-dimensional vectors : an input gate it, a forget gate ft, an output gate ot, a memory cell ct, and a hidden state ht. The unit receives an n-dimensional input vector xt, the previous hidden state ht\u22121, and the memory cell ct\u22121, and calculates the new vectors using the following equations:\nit = \u03c3 ( W (i)xt + U (i)ht\u22121 + b (i) ) , (1)\nft = \u03c3 ( W (f)xt + U (f)ht\u22121 + b (f) ) ,\not = \u03c3\n(\nW (o)xt + U (o)ht\u22121 + b (o)\n)\n,\nut = tanh ( W (u)xt + U (u)ht\u22121 + b (u) ) ,\nct = it ut + ft ct\u22121, ht = ot tanh(ct),\nwhere \u03c3 denotes the logistic function, denotes element-wise multiplication, W and U are weight matrices, and b are bias vectors. The LSTM unit at tth word receives the concatenation of word and POS embeddings as its input vector: xt = [ v (w) t ; v (p) t ] . We also concatenate the hidden state vectors of the two directions\u2019 LSTM units corresponding to each word (denoted as \u2212\u2192 ht and \u2190\u2212 ht) as its output vector,\nst = [\u2212\u2192 ht ; \u2190\u2212 ht ] , and pass it to the subsequent layers."}, {"heading": "3.3 Entity Detection", "text": "We treat entity detection as a sequence labeling task. We assign an entity tag to each word using a commonly used encoding scheme BILOU (Begin, Inside, Last, Outside, Unit) (Ratinov and Roth, 2009), where each entity tag represents the entity type and the position of a word in the entity. For example, in Fig. 1, we assign B-PER and L-PER (which denote the beginning and last words of a person entity type, respectively) to each word in Sidney Yates to represent this phrase as a PER (person) entity type.\nWe realize entity detection on the top of the sequence layer. We employ a two-layered NN with an he-dimensional hidden layer h(e) and a softmax\noutput layer for entity detection.\nh (e) t = tanh ( W (eh)[st; v (e) t\u22121] + b (eh) ) (2)\nyt = softmax ( W (ey)h (e) t + b (ey) )\n(3)\nHere, W are weight matrices and b are bias vectors. We assign entity labels to words in a greedy, leftto-right manner.1 During this decoding, we use the predicted label of a word to predict the label of the next word so as to take label dependencies into account. The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word (Fig. 1)."}, {"heading": "3.4 Dependency Layer", "text": "The dependency layer represents a relation between a pair of target words in the dependency tree, and is in charge of relation-specific representations, as is shown in top-right part of Fig. 1. This layer mainly focuses on the shortest path between a pair of target words in the dependency tree (i.e., the path between the least common node and the two target words) since these paths are shown to be effective in relation classification (Xu et al., 2015a). For example, we show the shortest path between Yates and Chicago in the bottom of Fig. 1, and this path well captures the key phrase of their relation, i.e., born in.\nWe employ bidirectional tree-structured LSTMRNNs (i.e., bottom-up and top-down) to represent a relation candidate by capturing the dependency structure around the target word pair. This bidirectional structure propagates to each node not only the information from the leaves but also information from the root. This is especially important for relation extraction, which makes use of argument nodes near the bottom of the tree, and our topdown LSTM-RNN sends information from the top of the tree to such near-leaf nodes (unlike in standard bottom-up LSTM-RNNs).2 Note that the two variants of tree-structured LSTM-RNNs by Tai et al. (2015) are not able to represent our target structures which have a variable number of typed children: the Child-Sum Tree-LSTM does not deal with\n1We also tried beam search but this did not show improvements in initial experiments.\n2We also tried to use one LSTM-RNN by connecting the root (Paulus et al., 2014), but preparing two LSTM-RNNs showed slightly better performance in our initial experiments.\ntypes and the N -ary Tree assumes a fixed number of children. We thus propose a new variant of treestructured LSTM-RNN that shares weight matrices Us for same-type children and also allows variable number of children. For this variant, we calculate vectors in the LSTM unit at t-th node withC(t) children using following equations:\nit = \u03c3 W (i)xt + \u2211 l\u2208C(t) U (i) m(l)htl + b (i)  , (4) ftk = \u03c3\nW (f)xt + \u2211 l\u2208C(t) U (f) m(k)m(l)htl + b (f)  , ot = \u03c3\nW (o)xt + \u2211 l\u2208C(t) U (o) m(l)htl + b (o)  , ut = tanh\nW (u)xt + \u2211 l\u2208C(t) U (u) m(l)htl + b (u)  , ct = it ut +\n\u2211 l\u2208C(t) ftl ctl,\nht = ot tanh(ct),\nwhere m(\u00b7) is a type mapping function. To investigate appropriate structures to represent relations between target word pairs, we experiment with three structure options. We primarily employ the shortest path structure (SPTree), which captures the core dependency path between a target word pair and is widely used in relation extraction models, e.g., (Bunescu and Mooney, 2005; Xu et al., 2015a). We also try two other dependency structures: SubTree and FullTree. SubTree is the subtree under the lowest common ancestor of the target word pair. This provides additional modifier information to the path and the word pair in SPTree. FullTree is the full dependency tree. This captures context from the entire sentence. While we use one node type for SPTree, we define two node types for SubTree and FullTree, i.e., one for nodes on shortest paths and one for all other nodes. We use the type mapping function m(\u00b7) to distinguish these two nodes types."}, {"heading": "3.5 Stacking Sequence and Dependency Layers", "text": "We stack the dependency layer on top of the sequence layer to incorporate both word sequence and dependency tree structure information into the output. The dependency-layer LSTM unit at the t-th\nword receives as input xt = [ st; v (d) t ; v (e) t ] , i.e., the concatenation of its corresponding hidden state vectors st in the sequence layer, dependency type embedding v(d)t (denotes the type of dependency to the parent3), and label embedding v(e)t (corresponds to the predicted entity label). Next, the output relation candidate vector, which is passed to the subsequent relation classification softmax layer, is constructed as the concatenation dp = [\u2191hpA ; \u2193hp1 ; \u2193hp2 ], where \u2191hpA is the hidden state vector of the top LSTM unit in the bottom-up LSTM-RNN (representing the lowest common ancestor of the target word pair p), and \u2193hp1 , \u2193hp2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top-down LSTM-RNN.4 All the corresponding arrows are shown in Fig. 1."}, {"heading": "3.6 Relation Classification", "text": "We build relation candidates using the last words of entities, i.e., words with L or U labels in the BILOU scheme. For instance, in Fig. 1, we build a relation candidate using Yates with an L-PER label and Chicago with an U-LOC label. For each relation candidate, the NN receives the output of the dependency tree layer dp (described above) corresponding to the path between the word pair p in the candidate, and predicts its relation label.5 Similarly to the entity detection, we employ a two-layered NN with an hr-dimensional hidden layer h(r) and a softmax output layer (with weight matrices W , bias vectors b).\nh(r)p = tanh ( W (rh)dp + b (rh) )\n(5) yp = softmax ( W (ry)h (r) t + b (ry) ) (6)\nWe construct the input dp for relation classification from both sequence and tree-structured LSTMRNNs, but the contribution of sequence layer to the input is indirect. Furthermore, our model uses words for representing entities, so it cannot fully use the entity information. To alleviate these problems, we directly concatenate the average of hidden state vectors for each entity from the sequence\n3We use the dependency to the parent since the number of children varies. Dependency types can also be incorporated into m(\u00b7), but this did not help in initial experiments.\n4Note that the order of the target words corresponds to the direction of the relation, not the positions in the sentence.\n5We represent relation labels by type and direction, except for negative relations that have no direction.\nlayer to the input dp to relation classification, i.e., d\u2032p = [ dp;\n1 |Ip1 | \u2211 i\u2208Ip1 si; 1 |Ip2 | \u2211 i\u2208Ip2 si ]\n(Pair), where Ip1 and Ip2 represent sets of word indices in the first and second entities.6\nAlso, we assign two labels to each word pair in prediction since we consider both left-to-right and right-to-left directions. When the predicted labels are inconsistent, we select the positive and more confident label, similar to Xu et al. (2015a)."}, {"heading": "3.7 Training", "text": "We update the model parameters including weights, biases, and embeddings by back-propagation through time (BPTT) and Adam (Kingma and Ba, 2015) with gradient clipping, parameter averaging, and L2-regularization (we regularize weights W and U , not the bias terms b). We also apply dropout (Srivastava et al., 2014) to the embedding layer and to the final hidden layers for entity detection and relation classification.\nWe employ scheduled sampling (Bengio et al., 2015) in entity detection. In scheduled sampling, we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal. As for i, we choose the inverse sigmoid decay i = k/(k + exp(i/k)), where k(\u2265 1) is a hyper-parameter that adjusts how often we use the gold labels as prediction.\nWe also incorporate curriculum learning (Bengio et al., 2009), where we pretrain the entity detection model using the training data to encourage building positive relation instances from the detected entities in training."}, {"heading": "4 Results and Discussion", "text": ""}, {"heading": "4.1 Data and Task Settings", "text": "We evaluate on three datasets: ACE05 and ACE04 for end-to-end relation extraction, and SemEval2010 Task 8 for relation classification. We use the first two datasets as our primary target, and use the last one to thoroughly analyze and ablate the relation classification part of our model.\n6Note that we do not show this Pair in Fig.1 for simplicity.\nACE05 defines 7 coarse-grained entity types7 and 6 coarse-grained relation types between entities.8 We use the same data splits and preprocessing as Li and Ji (2014).9 We report the micro precision, recall, and F-scores on both entity and relation extraction to better explain model performance. We treat an entity as correct when its type and the region of its head are correct, and we treat a relation as correct when its type and argument entities are correct.\nACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.10 We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014)11, and the preprocessing and evaluation metrics of ACE05.\nSemEval-2010 Task 8 defines 9 relation types between nominals12 and a tenth type Other when two nouns have none of these relations (Hendrickx et al., 2010). The dataset consists of 8,000 training and 2,717 test sentences, and each sentence is annotated with a relation between two given nominals. We randomly selected 800 sentences from the training set as our development set. We followed the official task setting, and report the official macro-averaged F1 score (Macro-F1) on the 9 relation types."}, {"heading": "4.2 Experimental Settings", "text": "We implemented our model using the cnn library.13 We parsed the texts using the Stanford neural dependency parser (Chen and Manning, 2014) with the original Stanford Dependencies. Based on prelim-\n7Facility (FAC), Geo-Political Entities (GPE), Location (LOC), Organization (ORG), Person (PER), Vehicle (VEH) and Weapon (WEA).\n8Artifact (ART), Gen-Affiliation (GEN-AFF), OrgAffiliation (ORG-AFF), Part-Whole (PART-WHOLE), PersonSocial (PER-SOC) and Physical (PHYS).\n9We removed the cts, un subsets, and used a 351/80/80 train/dev/test split. We removed duplicated entities and relations, and resolved nested entities. We used head spans for entities. We use entities and relations to refer to entity mentions and relation mentions in ACE for brevity.\n10PYS, PER-SOC, Employment / Membership / Subsidiary (EMP-ORG), ART, PER/ORG affiliation (Other-AFF), GPE affiliation (GPE-AFF), and Discourse (DISC).\n11We removed DISC and did 5-fold CV on bnews and nwire subsets (348 documents).\n12 Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, Entity-Destination, Component-Whole, Member-Collection and Message-Topic\n13https://github.com/clab/cnn\ninary tuning, we fixed embedding dimensions nw to 200, np, nd, ne to 25, and dimensions of intermediate layers (d of LSTM-RNNs and he, hr of hidden layers) to 100. We initialized word vectors via word2vec (Mikolov et al., 2013) trained on Wikipedia14 and randomly initialized all other parameters. We tuned hyper-parameters using development sets for SemEval-2010 Task 8 and ACE05. For ACE04, we directly employed the best parameters for ACE05. Such hyper-parameters include the initial learning rate (5e-3, 2e-3, 1e-3, 5e-4, 2e-4, 1e4), the regularization parameter (1e-4, 1e-5, 1e-6, 1e-7), dropout probabilities (0.0, 0.1, 0.2, 0.3, 0.4, 0.5), the size of gradient clipping (1, 5, 10, 50, 100), scheduled sampling parameter k (1, 5, 10, 50, 100), and the number of epochs for training and pretraining in curriculum learning (\u2264 100).15 Our statistical significance results are based on the Approximate Randomization (AR) test (Noreen, 1989)."}, {"heading": "4.3 End-to-end Relation Extraction Results", "text": "Table 1 compares our model with the state-of-the-art feature-based model of Li and Ji (2014) on final test sets, and shows that our model performs better than the state-of-the-art model.\nTo analyze the contributions and effects of the various components of our end-to-end relation extraction model, we perform ablation tests on the ACE05 development set (Table 2). The performance slightly degraded without curriculum learning or scheduled sampling, and the performance significantly degraded when we removed both of them (p<0.05). This is reasonable because the model can only create relation instances when both of the entities are found and, without these enhancements, it may get too late to find some relations. Removing label embeddings did not affect the entity detection performance, but this degraded the recall in relation extraction. This indicates that entity label information is helpful in detecting relations.\nWe also show the performance without sharing parameters for detecting entities and relations (\u2212Shared parameters); we first train the entity de-\n14https://dumps.wikimedia.org/enwiki/20150901/ 15Here, numbers in parentheses show the range tried for the hyper-parameters. Also, for SemEval-2010 Task 8, we omitted the entity detection layer and label embeddings since only target nominals are annotated and the task defines no entity types.\ntection model, detect entities with the model, and build a separate relation extraction model on the top of the detected entities (note that we cannot use the curriculum learning in this case). Without the shared parameters, both the performance in entity detection and relation extraction drops slightly, although the differences are not significant. When we removed all the enhancements, i.e. scheduled sampling, curriculum learning, label embedding, and shared parameters, the performance is significantly worse than SpTree (p<0.001), showing that these enhancements provide complementary benefits to end-to-end relation extraction."}, {"heading": "4.4 Relation Classification Analysis Results", "text": "To thoroughly analyze the relation classification part alone, e.g., comparing different LSTM structures, architecture components such as hidden layers and input information, and classification task settings, we use the SemEval-2010 Task 8. This dataset, often used to evaluate NN models for relation classification, annotates only relation-related nominals\n(unlike ACE datasets), so we can focus cleanly and solely on the relation classification part.\nWe first report official test set results in Table 3, where our novel LSTM-RNN model is comparable to and better than both the state-of-the-art CNNbased models on this task, whereas the previous best LSTM-RNN model (even when using external WordNet) achieved only 0.837 (Xu et al., 2015b).16\nNext, we compare different LSTM-RNN structures in Table 4. We first compare the three input dependency structures (SPTree, SubTree, FullTree) for tree-structured LSTM-RNNs, which hints that the information outside of the shortest path signif-\n16We only show previous work\u2019s results without external knowledge sources (e.g., WordNet, NER) for fair comparison. With inclusion of such sources, previous work\u2019s best reported performance is 0.856 using WordNet (Xu et al., 2015a). The reported best performance with LSTM-RNNs is 0.837, but with using WordNet (Xu et al., 2015b). The authors did not provide results without using external resources.\nicantly hurts the performance (p<0.05), even if we distinguish the nodes in the shortest paths from other nodes. We also compare our tree-structured LSTMRNN (SPTree) with the Child-Sum tree-LSTM on the shortest path of Tai et al. (2015). Child-Sum performs worse than our SPTree model, but not with as big of a decrease as above. This may be because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child.\nWe further show results with two counterparts of sequence-based LSTM-RNNs using the shortest path. SPSeq is bidirectional LSTM-RNN on the shortest path. The LSTM unit receives input from the sequence layer concatenated with embeddings for surrounding dependency types and directions. We concatenate the outputs of the two RNNs for the relation candidate. SPXu is our adaptation of the shortest path LSTM-RNN proposed by Xu et al. (2015b) to match our sequence-layer based model.17 This has two LSTM-RNNs for the left and right subpaths of the shortest path. We first calculate the max pooling of the LSTM units for each of these two RNNs, and then concatenate the outputs of the pooling for the relation candidate. The comparison with these sequence-based LSTM-RNNs indicates that a tree-structured LSTM-RNN is comparable to sequence-based ones in representing shortest paths.\nOverall, the performance comparison of the LSTM-RNN structures in Table 4 shows that for relation classification, selecting the appropriate tree structure representation of the input (i.e., SPTree versus SubTree and FullTree) is more important than the choice of the LSTM-RNN structure on that input (i.e., sequential versus tree-based).\nTable 5 summarizes the contribution of several model components and training settings on SemEval relation classification. We first remove the hidden layer by directly connecting the LSTM-RNN layers to the softmax layers. We also skip the sequence layer and directly use the word and POS embeddings for the dependency layer.18 Removing one of hidden\n17This is different from the original one in that we use the sequence layer and we concatenate the embeddings for the input while the original one prepared individual LSTM-RNNs for different input and concatenated the outputs of the LSTM-RNNs.\n18Note that this setting still uses some sequence layer information since it uses the entity-related information (Pair).\nand sequence layers slightly degraded performance, and when removing of both layers, the performance significantly dropped (p<0.001). Removing entityrelated information from the sequence layer (\u2212Pair) did not affect the performance much, but on removing it together with the sequence layer, the performance dropped significantly (p<0.05). This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task.\nFinally, for the generation of relation candidates, generating only left-to-right candidates slightly degraded the performance, but the difference was small and hence the creation of right-to-left candidates was not critical. Treating the inverse relation candidate as a negative instance (Negative sampling) also performed comparably to other generation methods in our model (unlike Xu et al. (2015a), which showed a significance improvement over generating only left-to-right candidates)."}, {"heading": "5 Conclusion", "text": "We presented an end-to-end relation extraction model using bidirectional sequential and bidirectional tree-structured LSTM-RNNs to represent both word sequence and dependency tree structures. This allowed us to represent both entities and relations in a single model, achieving gains over the stateof-the-art, feature-based system on end-to-end relation extraction (ACE04 and ACE05), and over recent state-of-the-art CNN-based models on nominal relation classification (SemEval-2010 Task 8).\nOur evaluation and ablation led to three key findings. First, the use of both word sequence and dependency tree structures is effective. Second, training with the shared parameters improves relation extraction accuracy, especially when employed with\ncurriculum learning, scheduled sampling, and label embeddings. Finally, the shortest path, which has been widely used in relation classification, is also appropriate for representing tree structures in neural LSTM models."}, {"heading": "Acknowledgments", "text": "We thank Qi Li and Kevin Gimpel for dataset details and helpful discussions."}], "references": [{"title": "Curriculum learning", "author": ["Bengio et al.2009] Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005] Razvan C Bunescu", "Raymond Mooney"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Exploiting syntactico-semantic structures for relation extraction", "author": ["Chan", "Roth2011] Yee Seng Chan", "Dan Roth"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Chan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "The automatic content extraction (ace) program \u2013 tasks, data, and evaluation", "author": ["Alexis Mitchell", "Mark Przybocki", "Lance Ramshaw", "Stephanie Strassel", "Ralph Weischedel"], "venue": "In Proceedings of the Fourth International", "citeRegEx": "Doddington et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Doddington et al\\.", "year": 2004}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Clause identification with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the 2001 workshop on Computational Natural Language Learning-Volume", "citeRegEx": "Hammerton.,? \\Q2001\\E", "shortCiteRegEx": "Hammerton.", "year": 2001}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Task-oriented learning of word embeddings for semantic relation classification", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural", "citeRegEx": "Hashimoto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Joint entity and relation extraction using card-pyramid parsing", "author": ["Kate", "Mooney2010] Rohit J. Kate", "Raymond Mooney"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Kate et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Incremental joint extraction of entity mentions and relations", "author": ["Li", "Ji2014] Qi Li", "Heng Ji"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Li et al.2015] Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Modeling joint entity and relation extraction with table representation", "author": ["Miwa", "Sasaki2014] Makoto Miwa", "Yutaka Sasaki"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Miwa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Miwa et al\\.", "year": 2014}, {"title": "A survey of named entity recognition and classification", "author": ["Nadeau", "Sekine2007] David Nadeau", "Satoshi Sekine"], "venue": "Lingvisticae Investigationes,", "citeRegEx": "Nadeau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nadeau et al\\.", "year": 2007}, {"title": "ComputerIntensive Methods for Testing Hypotheses : An Introduction", "author": ["Eric W. Noreen"], "venue": null, "citeRegEx": "Noreen.,? \\Q1989\\E", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Global belief recursive neural networks", "author": ["Paulus et al.2014] Romain Paulus", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Paulus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Global Inference for Entity and Relation Identification via a Linear Programming Formulation", "author": ["Roth", "Yih2007] Dan Roth", "Wen-Tau Yih"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2007}, {"title": "Joint inference of entities, relations, and coreference", "author": ["Singh et al.2013] Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015a] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015b] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["Yang", "Cardie2013] Bishan Yang", "Claire Cardie"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach", "author": ["Yu", "Lam2010] Xiaofeng Yu", "Wai Lam"], "venue": "In Coling 2010: Posters,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Kernel methods for relation extraction", "author": ["Chinatsu Aone", "Anthony Richardella"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 33, "context": ", named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information.", "startOffset": 107, "endOffset": 148}, {"referenceID": 34, "context": ", named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information.", "startOffset": 107, "endOffset": 148}, {"referenceID": 8, "context": ", word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al.", "startOffset": 17, "endOffset": 34}, {"referenceID": 27, "context": ", word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015).", "startOffset": 68, "endOffset": 86}, {"referenceID": 16, "context": "Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al.", "startOffset": 155, "endOffset": 190}, {"referenceID": 34, "context": "Many traditional, feature-based relation classification models extract features from both sequences and parse trees (Zhou et al., 2005).", "startOffset": 116, "endOffset": 135}, {"referenceID": 25, "context": "However, previous RNN-based models focus on only one of these linguistic structures (Socher et al., 2012).", "startOffset": 84, "endOffset": 105}, {"referenceID": 0, "context": "Our model also incorporates curriculum learning (Bengio et al., 2009) and scheduled sampling (Bengio et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": ", 2009) and scheduled sampling (Bengio et al., 2015) to alleviate the problem of lowperformance entity detection in early stages of training, as well as to allow entity information to further help downstream relation extraction.", "startOffset": 31, "endOffset": 52}, {"referenceID": 8, "context": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003).", "startOffset": 87, "endOffset": 104}, {"referenceID": 9, "context": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003).", "startOffset": 164, "endOffset": 181}, {"referenceID": 8, "context": "LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003). Recently, Huang et al. (2015) showed that building a conditional random field (CRF) layer on the top of bidirectional LSTM-RNNs performs comparably to the state-of-the-art methods in the part-of-speech (POS) tagging, chunking, and NER.", "startOffset": 88, "endOffset": 213}, {"referenceID": 33, "context": "For relation classification, in addition to traditional feature/kernel-based approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), several neural models have been proposed in the SemEval2010 Task 8 (Hendrickx et al.", "startOffset": 88, "endOffset": 136}, {"referenceID": 11, "context": ", 2003; Bunescu and Mooney, 2005), several neural models have been proposed in the SemEval2010 Task 8 (Hendrickx et al., 2010), including embedding-based models (Hashimoto et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 10, "context": ", 2010), including embedding-based models (Hashimoto et al., 2015), CNN-based models (dos Santos et al.", "startOffset": 42, "endOffset": 66}, {"referenceID": 25, "context": ", 2015), and RNN-based models (Socher et al., 2012).", "startOffset": 30, "endOffset": 51}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al.", "startOffset": 31, "endOffset": 125}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models.", "startOffset": 31, "endOffset": 147}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models. Xu et al. (2015b) also showed that LSTM-RNNs are useful for relation classification, but the performance was worse than CNN-based models.", "startOffset": 31, "endOffset": 346}, {"referenceID": 6, "context": ", 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernelbased systems (Bunescu and Mooney, 2005), are also useful in NN-based models. Xu et al. (2015b) also showed that LSTM-RNNs are useful for relation classification, but the performance was worse than CNN-based models. Li et al. (2015) compared separate sequence-based and tree-structured LSTMRNNs on relation classification, using basic RNN model structures.", "startOffset": 31, "endOffset": 483}, {"referenceID": 27, "context": "Research on tree-structured LSTM-RNNs (Tai et al., 2015) fixes the direction of information propagation from bottom to top, and also cannot handle an arbitrary number of typed children as in a typed dependency tree.", "startOffset": 38, "endOffset": 56}, {"referenceID": 24, "context": "Such models include structured prediction (Li and Ji, 2014; Miwa and Sasaki, 2014), integer linear programming (Roth and Yih, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013).", "startOffset": 244, "endOffset": 282}, {"referenceID": 27, "context": "2 Note that the two variants of tree-structured LSTM-RNNs by Tai et al. (2015) are not able to represent our target structures which have a variable number of typed children: the Child-Sum Tree-LSTM does not deal with", "startOffset": 61, "endOffset": 79}, {"referenceID": 21, "context": "We also tried to use one LSTM-RNN by connecting the root (Paulus et al., 2014), but preparing two LSTM-RNNs showed slightly better performance in our initial experiments.", "startOffset": 57, "endOffset": 78}, {"referenceID": 28, "context": "When the predicted labels are inconsistent, we select the positive and more confident label, similar to Xu et al. (2015a).", "startOffset": 104, "endOffset": 122}, {"referenceID": 26, "context": "We also apply dropout (Srivastava et al., 2014) to the embedding layer and to the final hidden layers for entity detection and relation classification.", "startOffset": 22, "endOffset": 47}, {"referenceID": 1, "context": "We employ scheduled sampling (Bengio et al., 2015) in entity detection.", "startOffset": 29, "endOffset": 50}, {"referenceID": 0, "context": "We also incorporate curriculum learning (Bengio et al., 2009), where we pretrain the entity detection model using the training data to encourage building positive relation instances from the detected entities in training.", "startOffset": 40, "endOffset": 61}, {"referenceID": 5, "context": "ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.", "startOffset": 62, "endOffset": 87}, {"referenceID": 11, "context": "SemEval-2010 Task 8 defines 9 relation types between nominals12 and a tenth type Other when two nouns have none of these relations (Hendrickx et al., 2010).", "startOffset": 131, "endOffset": 155}, {"referenceID": 5, "context": "ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.10 We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014)11, and the preprocessing and evaluation metrics of ACE05.", "startOffset": 63, "endOffset": 199}, {"referenceID": 5, "context": "ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types.10 We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014)11, and the preprocessing and evaluation metrics of ACE05.", "startOffset": 63, "endOffset": 220}, {"referenceID": 17, "context": "We initialized word vectors via word2vec (Mikolov et al., 2013) trained on Wikipedia14 and randomly initialized all other parameters.", "startOffset": 41, "endOffset": 63}, {"referenceID": 20, "context": "15 Our statistical significance results are based on the Approximate Randomization (AR) test (Noreen, 1989).", "startOffset": 93, "endOffset": 107}, {"referenceID": 6, "context": "845 dos Santos et al. (2015) 0.", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": "845 dos Santos et al. (2015) 0.841 Xu et al. (2015a) 0.", "startOffset": 8, "endOffset": 53}, {"referenceID": 27, "context": "We also compare our tree-structured LSTMRNN (SPTree) with the Child-Sum tree-LSTM on the shortest path of Tai et al. (2015). Child-Sum performs worse than our SPTree model, but not with as big of a decrease as above.", "startOffset": 106, "endOffset": 124}, {"referenceID": 28, "context": "SPXu is our adaptation of the shortest path LSTM-RNN proposed by Xu et al. (2015b) to match our sequence-layer based model.", "startOffset": 65, "endOffset": 83}, {"referenceID": 28, "context": "Treating the inverse relation candidate as a negative instance (Negative sampling) also performed comparably to other generation methods in our model (unlike Xu et al. (2015a), which showed a significance improvement over generating only left-to-right candidates).", "startOffset": 158, "endOffset": 176}], "year": 2017, "abstractText": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model stacks bidirectional sequential LSTM-RNNs and bidirectional tree-structured LSTMRNNs to capture both word sequence and dependency tree substructure information. This allows our model to jointly represent both entities and relations with shared parameters. We further encourage detection of entities during training and use of entity information in relation extraction via curriculum learning and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 3.5% and 4.8% relative error reductions in F-score on ACE2004 and ACE2005, respectively. We also show improvements over the state-of-the-art convolutional neural network based model on nominal relation classification (SemEval-2010 Task 8), with 2.5% relative error reduction in F-score.", "creator": "LaTeX with hyperref package"}}}