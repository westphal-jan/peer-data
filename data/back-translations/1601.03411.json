{"id": "1601.03411", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2016", "title": "Analysis of Algorithms and Partial Algorithms", "abstract": "We present an alternative methodology for analyzing algorithms based on the concept of expected discounted reward. Naturally, this methodology deals with algorithms that do not always come to an end, so that it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in Artificial General Intelligence (AGI) and automated theory testing. We mention new approaches to improving AGI and the logical uncertainty that this methodology enables.", "histories": [["v1", "Wed, 13 Jan 2016 21:17:42 GMT  (14kb)", "https://arxiv.org/abs/1601.03411v1", null], ["v2", "Sun, 28 Feb 2016 18:30:51 GMT  (16kb)", "http://arxiv.org/abs/1601.03411v2", null], ["v3", "Tue, 1 Mar 2016 19:21:41 GMT  (16kb)", "http://arxiv.org/abs/1601.03411v3", null], ["v4", "Sun, 8 May 2016 00:52:51 GMT  (28kb)", "http://arxiv.org/abs/1601.03411v4", null], ["v5", "Mon, 7 Aug 2017 01:30:46 GMT  (28kb)", "http://arxiv.org/abs/1601.03411v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["andrew macfie"], "accepted": false, "id": "1601.03411"}, "pdf": {"name": "1601.03411.pdf", "metadata": {"source": "CRF", "title": "Analysis of Algorithms and Partial Algorithms", "authors": ["Andrew MacFie"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n03 41\n1v 5\n[ cs\n.A I]\n7 A\nug 2\n01 7"}, {"heading": "1 Introduction: Shortcomings of Traditional Analysis of Algorithms", "text": "Currently, the (running time) analysis of algorithms takes the following form. Given two algorithms A, B that solve the same problem, we find which is more efficient by asymptotically comparing the running time sequences (an), (bn) [4,15]. This could be using worst-case or average-case running times or even smoothed analysis [16]. We refer to this general method as traditional analysis of algorithms.\nAs with any model, traditional analysis of algorithms is not perfect. Authors have noted [1,9] that comparing sequence tails avoids the arbitrariness of any particular range of input lengths but leads us to say an = n\n100 is superior to bn = ( 1 + exp(\u22121010) )n\nwhich is false for practical purposes. A further issue with traditional analysis is illustrated by this situation: Say we have a function F : {0, 1}\u2217 \u2192 {0, 1} and an algorithm A that computes F such that for n \u2265 0, A takes (n!)! steps on the input 0n and n steps on any other input of length n. The algorithm A then has worst-case running time (n!)! and average-case running time slightly greater than 2\u2212n(n!)!, which are both terrible. However, if the inputs are generated according to a uniform distribution, the probability of taking more than n steps is 2\u2212n which is quickly negligible. We see that A should be considered an excellent algorithm but traditional analysis does not tell us that, unless we add \u201cwith high probability\u201d.\nThe same issue arises if A simply does not halt on 0n, in which case the worst-case and average-case running times are infinite. Indeed, this is not an esoteric phenomenon. For any problem with Turing degree 0\u2032 we cannot have an algorithm that halts on every input, but we develop partial solutions that work on a subset of inputs. Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]). E.g. in the case of automated\ntheorem proving, Buss, describing the main open problems in proof theory [3], states, \u201cComputerized proof search ... is widely used, but almost no mathematical theory is known about the effectiveness or optimality of present-day algorithms.\u201d\nDefinition 1. An algorithm A is a partial algorithm (a.k.a. computational method [12, p5]) for a given problem if on all inputs, A either outputs the correct value, or does not terminate.\nDefinition 2. We refer to partial algorithms for problems with Turing degree 0\u2032 as 0\u2032 algorithms.\nTo analyze 0\u2032 algorithms, and perhaps to better analyze normal terminating algorithms, we need a new approach that is not based on worst-case or average-case running time sequences. In Sect. 2 we present a new method for analyzing algorithms, called expected-reward analysis that avoids some of the issues mentioned above. Then in Sect. 3 we mention how this method can be used in self-improving AI systems. We give directions for further work in Sect. 4.\nNotation 1. Given a (possibly partial) algorithm A and an input \u03c9, we denote the number of steps taken by A on \u03c9 by cA(\u03c9), which takes the value \u221e if A does not halt on \u03c9."}, {"heading": "2 Expected-Reward Analysis of Algorithms", "text": ""}, {"heading": "2.1 Definition", "text": "Let A be a (possibly partial) algorithm with inputs in \u2126. We say the score of A is\nS(A) = \u2211\n\u03c9\u2208\u2126\nP ({\u03c9})r(\u03c9)D(cA(\u03c9)) = E(r \u00b7 (D \u25e6 cA)) ,\nwhere P is a probability measure on \u2126, D is a discount function [7], and r(\u03c9) is a reward (a.k.a. utility) value associated with obtaining the solution to \u03c9. The expression S(A) may be interpreted as the expected discounted reward that A receives if run on a random input, and the practice of comparing scores among algorithms we call expected-reward analysis. A higher score indicates a more efficient algorithm.\nThe functions D and r are arbitrary and are free to be set in the context of a particular application. E.g. in graphical user interface software we often desire near-instant responses, with utility rapidly dropping off with time. Assuming 0 \u2264 r \u2264 1, we immediately see that for all A, partial or not, we have\n0 \u2264 S(A) \u2264 1 .\nFor simplicity in this paper we assume r(\u03c9) = 1 and D is an exponential discount function, i.e.\nD(cA(\u03c9)) = exp(\u2212\u03bb cA(\u03c9)) ,\nwhere \u03bb > 0 is a discount rate.\nThe choice of P is also arbitrary; we remark on two special cases. If all inputs of a given length are weighted equally, P is determined by a probability mass function on Z0+. In this case any common discrete probability distribution may be used as appropriate. The measure P is also determined by a probability mass function on Z0+ if we weight equal-length inputs according to Solomonoff\u2019s universal distribution m [13], which is a particularly good general model, although computationally difficult.\nExpected-reward analysis is non-asymptotic, in the sense that all inputs potentially matter. Thus, while expected-reward analysis can be used on terminating algorithms, we expect it to give different results from traditional analysis, in general. Since particular inputs can make a difference to S(A), it may be advantageous to \u201chardcode\u201d initial cases into an algorithm. This practice certainly exists, e.g. humans may store the 12\u00d712 multiplication table as well as knowing a general integer multiplication algorithm.\nComputational complexity theory often works with classes of problems whose definitions are equivalent for all \u201creasonable\u201d models of computation [5]. However, even a varying constant factor could arbitrarily change a score. This is simply the price of concreteness, and outside of complexity theory, traditional analysis of algorithms generally selects a particular model of computation and gives precise results that do not necessarily apply to other models [6].\nUnlike traditional analysis, experimental data is relevant to score values in a statistical sense. If we are able to generate inputs according to P , either artificially or by sampling inputs found in practice, S(A) is a quantity amenable to statistical estimation. This suggests a form of experimental analysis of algorithms which focuses on a single real number rather than plotting the estimated running time for every input length, which, in the necessary absence of asymptotics in experimental analysis, may not conclusively rank two competing algorithms anyway.\nThe expected-reward paradigm already appears in the analysis of artificial agents, rather than algorithms [8]. As we see in Sect. 3, however, even in applications to AI, working in the more classical domain of algorithms brings benefits."}, {"heading": "2.2 Theory and Practice", "text": "Traditional analysis of algorithms has an established literature going back decades which provides a set of techniques for performing traditional analysis on algorithms developed for various problems. We do not significantly develop a mathematical theory of expected-reward analysis here, but we make some very brief initial remarks.\nBy way of introductory example, we consider expected-reward analysis applied to some well-known sorting algorithms. Let Sn be the set of permutations of [1..n] and let \u03a0n be a uniform random element of Sn. We denote the algorithms mergesort and quicksort by M and Q, as defined in [15], and set\nmn = E [exp(\u2212\u03bb cM (\u03a0n))] , qn = E [exp(\u2212\u03bb cQ(\u03a0n))] ,\nwhere cA(\u03c9) is the number of comparison operations used by an algorithm A to sort an input \u03c9.\nProposition 1. For n \u2265 1 we have\nmn = exp ( \u2212\u03bb(n\u2308lg(n)\u2309+ n\u2212 2\u2308lg(n)\u2309) ) , m0 = 1, (1)\nqn = e\u2212\u03bb(n+1)\nn\nn \u2211\nk=1\nqk\u22121qn\u2212k, q0 = 1 .\nProof. From [15], M makes the same number of comparisons for all inputs of length n \u2265 1:\ncM (\u03a0n) = n\u2308lg(n)\u2309+ n\u2212 2 \u2308lg(n)\u2309 ,\nso (1) is immediate. Now, when Q is called on \u03a0n, let \u03c1(\u03a0n) be the pivot element, and let \u03a0n, \u03a0n be the subarrays constructed for recursive calls to Q, where the elements in \u03a0n are less than \u03c1(\u03a0n), and the elements in \u03a0n are greater.\nWe have\nE[ exp(\u2212\u03bbcQ(\u03a0n))]\n= 1\nn\nn \u2211\nk=1\nE[exp(\u2212\u03bb(n+ 1 + cQ(\u03a0n) + cQ(\u03a0n)) ) | \u03c1(\u03a0n) = k]\n= e\u2212\u03bb(n+1)\nn\nn \u2211\nk=1\nE[exp(\u2212\u03bb(cQ(\u03a0n) + cQ(\u03a0n)) ) | \u03c1(\u03a0n) = k] .\nIt can be seen that given \u03c1(\u03a0n) = k, \u03a0n and \u03a0n are independent, thus\nE[ exp(\u2212\u03bbcQ(\u03a0n))]\n= e\u2212\u03bb(n+1)\nn\nn \u2211\nk=1\nE[exp(\u2212\u03bbcQ(\u03a0n)) | \u03c1(\u03a0n) = k] \u00b7\nE[exp(\u2212\u03bbcQ(\u03a0n)) | \u03c1(\u03a0n) = k]\n= e\u2212\u03bb(n+1)\nn\nn \u2211\nk=1\nE[exp(\u2212\u03bbcQ(\u03a0k\u22121))]E[exp(\u2212\u03bbcQ(\u03a0n\u2212k))] . \u2293\u2294\nFrom examining the best-case performance of Q, it turns out that cM (\u03a0n) \u2264 cQ(\u03a0n) for all n, so the expected-reward comparison of M and Q is easy: S(M) \u2265 S(Q) for any parameters. However, we may further analyze the absolute scores of M and Q to facilitate comparisons to arbitrary sorting algorithms. When performing expected-reward analysis on an individual algorithm, our main desideratum is a way to quickly compute the score value to within a given precision for each possible parameter value P, \u03bb. Proposition 1 gives a way of computing scores of M and Q for measures P that give equal length inputs\nequal weight, although it does not immediately suggest an efficient way in all cases. Bounds on scores are also potentially useful and may be faster to compute; in the next proposition, we give bounds on mn and qn which are simpler than the exact expressions above.\nProposition 2. For n \u2265 1,\ne\u22122\u03bb(n\u22121)\n(n\u2212 1)!\u03bb/ log(2) \u2264 mn \u2264\ne\u2212\u03bb(n\u22121)\n(n\u2212 1)!\u03bb/ log(2) . (2)\nFor all 0 < \u03bb \u2264 log(2) and n \u2265 0,\ne\u22122\u03b3\u03bb(n+1)\u2212\u03bb\n(n+ 1)!2\u03bb (2\u03c0(n+ 1))\u03bb < qn \u2264\ne\u22122\u03bbn\n(n!)\u03bb/ log(2) ,\nwhere \u03b3 is Euler\u2019s constant.\nProof. Sedgewick and Flajolet [15] give an alternative expression for the running time of mergesort:\ncM (\u03a0n) =\nn\u22121 \u2211\nk=1\n(\u230alg k\u230b+ 2) .\nStatement (2) follows from this because\nlog(k)/ log(2) + 1 < \u230alg k\u230b+ 2 \u2264 log(k)/ log(2) + 2 .\nWith 0 < \u03bb \u2264 log(2), we prove the upper bound\nqn \u2264 e\u22122\u03bbn\n(n!)\u03bb/ log(2) (3)\nfor all n \u2265 0 by induction. Relation (3) clearly holds for n = 0. We show that (3) can be proved for n = N (N > 0) on the assumption that (3) holds for 0 \u2264 n \u2264 N \u2212 1. Proposition 1 gives\nqN = e\u2212\u03bb(N+1)\nN\nN \u2211\nk=1\nqk\u22121qN\u2212k\n\u2264 e\u2212\u03bb(N+1)\nN\nN \u2211\nk=1\ne\u22122\u03bb(k\u22121) ((k \u2212 1)!)\u03bb/ log(2) e\u22122\u03bb(N\u2212k) ((N \u2212 k)!)\u03bb/ log(2)\n(by the assumption)\n= e\u22123\u03bbN+\u03bb\n(\n1\nN\nN \u2211\nk=1\n(\n1\n(k \u2212 1)!\n1\n(N \u2212 k)!\n)\u03bb/ log(2) )\n\u2264 e\u22123\u03bbN+\u03bb\n\n\n1\nN\u03bb/ log(2)\n(\nN \u2211\nk=1\n1\n(k \u2212 1)!\n1\n(N \u2212 k)!\n)\u03bb/ log(2) \n\n(by Jensen\u2019s inequality, since 0 < \u03bb/ log(2) \u2264 1)\n= e\u22123\u03bbN+\u03bb ( (2N\u22121)\u03bb/ log(2)\n(N !)\u03bb/ log(2)\n)\n= e\u22122\u03bbN\n(N !)\u03bb/ log(2) .\nThus (3) has been proved for all n \u2265 0.\nFor the lower bound on qn, we use the probabilistic form of Jensen\u2019s inequality,\nqn = E [exp(\u2212\u03bbcQ(\u03a0n))] \u2265 exp(\u2212\u03bbE [cQ(\u03a0n)]) ,\nnoting that average-case analysis of quicksort [15] yields\nE [cQ(\u03a0n)] = 2(n+ 1)(Hn+1 \u2212 1), n \u2265 0 ,\nwhere (Hn) is the harmonic sequence. For n \u2265 0, the bound\nHn+1 < log(n+ 1) + \u03b3 + 1\n2(n+ 1)\nholds [11] (sharper bounds exist), so we have\nqn > exp\n(\n\u22122\u03bb(n+ 1)\n(\nlog(n+ 1) + \u03b3 + 1\n2(n+ 1) \u2212 1\n))\n= e\u22122(\u03b3\u22121)\u03bb(n+1)\u2212\u03bb(n+ 1)\u22122\u03bb(n+1) .\nWe finish by applying Stirling\u2019s inequality\n(n+ 1)\u2212(n+1) \u2265 \u221a 2\u03c0(n+ 1) e\u2212(n+1)/(n+ 1)!, n \u2265 0 . \u2293\u2294\nFrom these results we may get a sense of the tasks involved in expectedreward analysis for typical algorithms.We note that with an exponential discount function, the independence of subproblems in quicksort is required for obtaining a recursive formula, whereas in traditional average-case analysis, linearity of expectation suffices.\nWe end this section by mentioning an open question relevant to a theory of expected-reward analysis.\nQuestion 1. If we fix a computational problem and parameters P, \u03bb, what is supA S(A), and is it attained?\nIf supA S(A) is not attained then the situation is similar to that in Blum\u2019s speedup theorem. Comparing supA S(A) among problems would be the expectedreward analog of computational complexity theory but because of the sensitivity of S to parameters and the model of computation, this is not useful."}, {"heading": "3 Self-Improving AI", "text": "The generality of 0\u2032 problems allows us to view design and analysis of 0\u2032 algorithms as a task which itself may be given to a 0\u2032 algorithm, bringing about recursive self-improvement. Here we present one possible concrete example of this notion and discuss connections with AI.\nComputational problems with Turing degree 0\u2032 are Turing-equivalent so without loss of generality in this section we assume 0\u2032 algorithms are automated theorem provers. Specifically, we fix a formal logic system, say ZFC (assuming it is consistent), and take the set of inputs to be ZFC sentences, and the possible outputs to be provable and not provable.\nLet a predicate \u03b2 be such that \u03b2(Z) holds iff Z is a 0\u2032 algorithm which is correct on provable inputs and does not terminate otherwise. In pseudocode we write the instruction to run some Z on input \u03c9 as Z(\u03c9), and if \u03c9 contains \u03b2 or S (the score function), their definitions are implicitly included.\nWe give an auxiliary procedure Search which takes as input a 0\u2032 algorithm Z and a rational number x and uses Z to obtain a 0\u2032 algorithm which satisfies \u03b2 and has score greater than x (if possible). Symbols in bold within a string literal get replaced by the value of the corresponding variable. We assume 0\u2032 algorithms are encoded as strings in a binary prefix code.\n1: procedure Search(x, Z) 2: u \u2190 the empty string 3: loop 4: do in parallel until one returns provable: 5: A: Z(\u201c\u2203v : (Z\u2217 = u0v =\u21d2 \u03b2(Z\u2217) \u2227 S(Z\u2217) > x)\u201d) 6: B: Z(\u201c\u2203v : (Z\u2217 = u1v =\u21d2 \u03b2(Z\u2217) \u2227 S(Z\u2217) > x)\u201d) 7: C: Z(\u201cZ\u2217 = u =\u21d2 \u03b2(Z\u2217) \u2227 S(Z\u2217) > x\u201d)\n8: if A returned provable then 9: u \u2190 u0 10: if B returned provable then 11: u \u2190 u1 12: if C returned provable then 13: return u\nWe remark that the mechanism of Search is purely syntactic and does not rely on consistency or completeness of ZFC, or the provability thereof. This would not be the case if we strengthened \u03b2 to require that \u03b2(Z) is true only if at most one of Z(\u03c9) and Z(\u00ac\u03c9) returns provable. Such a \u03b2 would never provably hold in ZFC.\nThe following procedure Improve takes an initial 0\u2032 algorithm Z0 and uses dovetailed calls to Search to output a sequence of 0\u2032 algorithms that tend toward optimality.\n1: procedure Improve(Z0) 2: best \u2190 Z0, pool \u2190 {}, score \u2190 0 3: for n \u2190 1 to \u221e do 4: an \u2190 nth term in Stern-Brocot enumeration of Q \u2229 (0, 1] 5: if an > score then 6: initialState\u2190 initial state of Search(an, best) 7: add (an, best, initialState) to pool\n8: improvementFound \u2190 false 9: for (a, Z, state) in pool do\n10: run Search(a, Z) one step starting in state state 11: newState \u2190 new current state of Search(a, Z) 12: if state is not a terminating state then 13: in pool, mutate (a, Z, state) into (a, Z, newState) 14: continue 15: improvementFound \u2190 true 16: best \u2190 output of Search(a, Z) 17: score \u2190 a 18: for (a\u0302, Z\u0302, \u02c6state) in pool where a\u0302 \u2264 score do 19: remove (a\u0302, Z\u0302, \u02c6state) from pool\n20: print best\n21: if improvementFound then 22: for (a, Z, state) in pool do 23: initialState\u2190 initial state of Search(a, best) 24: add (a, best, initialState) to pool\nThe procedure Improve has the following basic property.\nProposition 3. Let (Zn) be the sequence of 0 \u2032 algorithms printed by Improve. If \u03b2(Z0) holds, and if there is any 0 \u2032 algorithm Y and s \u2208 Q where \u03b2(Y ) and S(Y ) > s > 0 are provable, we have\nlim n\u2192\u221e S(Zn) \u2265 s .\nIf (Zn) is finite, the above limit can be replaced with the last term in (Zn).\nProof. The value s appears as some value an. For an = s, if an > score in line 5, then Search(s, best) will be run one step for each greater or equal value of n and either terminates (since Y exists) and score is set to s, or is interrupted if we eventually have score \u2265 s before Search(s, best) terminates. It suffices to note that when score attains any value x > 0, all further outputs Z satisfy S(Z) > x and there is at least one such output. \u2293\u2294\nThe procedure Improve also makes an attempt to use recently printed 0\u2032 algorithms in calls to Search. However, it is not true in general that S(Zn+1) \u2265 S(Zn). Checking if a particular output Zn is actually an improvement over Z0 or Zn\u22121 requires extra work.\nIn artificial general intelligence (AGI) it is desirable to have intelligent systems with the ability to make autonomous improvements to themselves [14]. If an AGI system such as an AIXI approximation [10] already uses a 0\u2032 algorithm Z to compute the universal distribution m, we can give the system the ability to improve Z over time by devoting some of its computational resources to running Improve. This yields a general agent whose environment prediction ability tends toward optimality."}, {"heading": "4 Future Work", "text": "We would like to be able to practically use expected-reward analysis with various parameter values, probability measures, and discount functions, on both terminating and non-terminating algorithms. Particularly, we would like to know whether 0\u2032 algorithms may be practically analyzed. It may be possible to develop general mathematical tools and techniques to enhance the practicality of these methods, such as exist for traditional analysis; this is a broad and open-ended research goal.\nAcknowledgements. The author wishes to thank Zhicheng Gao, Nima Hoda, Patrick LaVictoire, Saran Neti, and anonymous referees for helpful comments."}], "references": [{"title": "Why philosophers should care about computational complexity", "author": ["S. Aaronson"], "venue": "Computability: G\u00f6del, Turing, Church, and Beyond", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Looper: Lightweight detection of infinite loops at runtime", "author": ["J. Burnim", "N. Jalbert", "C. Stergiou", "K. Sen"], "venue": "In International Conference on Automated Software Engineering", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Re: Proof theory on the eve of year", "author": ["S. Buss"], "venue": "http://www.ihes.fr/~carbone/papers/proofsurveyFeferman2000.html", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "MIT Press, Cambridge, MA, third edn.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Handbook of Theoretical Computer Science, Vol", "author": ["P. van Emde Boas"], "venue": "A. pp. 1\u201366. MIT Press, Cambridge, MA, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Analytic combinatorics", "author": ["P. Flajolet", "R. Sedgewick"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Time discounting and time preference: A critical review", "author": ["S. Frederick", "G. Loewenstein", "T. O\u2019Donoghue"], "venue": "Journal of Economic Literature pp. 351\u2013401", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Toward a formal characterization of real-world general intelligence", "author": ["B. Goertzel"], "venue": "Proceedings of the 3rd Conference on Artificial General Intelligence, AGI. pp. 19\u201324", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Feasible functions", "author": ["Y. Gurevich"], "venue": "London Mathematical Society Newsletter 206, 6\u20137", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": "Springer, Berlin", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Gamma: Exploring Euler\u2019s Constant", "author": ["H. Julian"], "venue": "Princeton University Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "The Art of Computer Programming, Vol", "author": ["D.E. Knuth"], "venue": "1. Addison-Wesley, Reading, MA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": "Springer Science & Business Media", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "G\u00f6del machines: Fully self-referential optimal universal selfimprovers", "author": ["J. Schmidhuber"], "venue": "Artificial General Intelligence, pp. 199\u2013226. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "An Introduction to the Analysis of Algorithms", "author": ["R. Sedgewick", "P. Flajolet"], "venue": "AddisonWesley", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Smoothed analysis: an attempt to explain the behavior of algorithms in practice", "author": ["D.A. Spielman", "S.H. Teng"], "venue": "Communications of the ACM 52(10), 76\u201384", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The Mathematica Guidebook for Symbolics", "author": ["M. Trott"], "venue": "Springer Science & Business Media", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Given two algorithms A, B that solve the same problem, we find which is more efficient by asymptotically comparing the running time sequences (an), (bn) [4,15].", "startOffset": 153, "endOffset": 159}, {"referenceID": 14, "context": "Given two algorithms A, B that solve the same problem, we find which is more efficient by asymptotically comparing the running time sequences (an), (bn) [4,15].", "startOffset": 153, "endOffset": 159}, {"referenceID": 15, "context": "This could be using worst-case or average-case running times or even smoothed analysis [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Authors have noted [1,9] that comparing sequence tails avoids the arbitrariness of any particular range of input lengths but leads us to say an = n 100 is superior to bn = ( 1 + exp(\u221210) n which is false for practical purposes.", "startOffset": 19, "endOffset": 24}, {"referenceID": 8, "context": "Authors have noted [1,9] that comparing sequence tails avoids the arbitrariness of any particular range of input lengths but leads us to say an = n 100 is superior to bn = ( 1 + exp(\u221210) n which is false for practical purposes.", "startOffset": 19, "endOffset": 24}, {"referenceID": 1, "context": "Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]).", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]).", "startOffset": 260, "endOffset": 264}, {"referenceID": 2, "context": "theorem proving, Buss, describing the main open problems in proof theory [3], states, \u201cComputerized proof search .", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "where P is a probability measure on \u03a9, D is a discount function [7], and r(\u03c9) is a reward (a.", "startOffset": 64, "endOffset": 67}, {"referenceID": 12, "context": "The measure P is also determined by a probability mass function on Z0+ if we weight equal-length inputs according to Solomonoff\u2019s universal distribution m [13], which is a particularly good general model, although computationally difficult.", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "Computational complexity theory often works with classes of problems whose definitions are equivalent for all \u201creasonable\u201d models of computation [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "This is simply the price of concreteness, and outside of complexity theory, traditional analysis of algorithms generally selects a particular model of computation and gives precise results that do not necessarily apply to other models [6].", "startOffset": 235, "endOffset": 238}, {"referenceID": 7, "context": "The expected-reward paradigm already appears in the analysis of artificial agents, rather than algorithms [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 14, "context": "We denote the algorithms mergesort and quicksort by M and Q, as defined in [15], and set", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "From [15], M makes the same number of comparisons for all inputs of length n \u2265 1: cM (\u03a0n) = n\u2308lg(n)\u2309+ n\u2212 2 \u2308lg(n)\u2309 , so (1) is immediate.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "Sedgewick and Flajolet [15] give an alternative expression for the running time of mergesort:", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "For the lower bound on qn, we use the probabilistic form of Jensen\u2019s inequality, qn = E [exp(\u2212\u03bbcQ(\u03a0n))] \u2265 exp(\u2212\u03bbE [cQ(\u03a0n)]) , noting that average-case analysis of quicksort [15] yields", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "holds [11] (sharper bounds exist), so we have", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "In artificial general intelligence (AGI) it is desirable to have intelligent systems with the ability to make autonomous improvements to themselves [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "If an AGI system such as an AIXI approximation [10] already uses a 0 algorithm Z to compute the universal distribution m, we can give the system the ability to improve Z over time by devoting some of its computational resources to running Improve.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "We present an alternative methodology for the analysis of algorithms, based on the concept of expected discounted reward. This methodology naturally handles algorithms that do not always terminate, so it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in artificial general intelligence (AGI) and automated theorem proving. We mention an approach to self-improving AGI enabled by this methodology.", "creator": "LaTeX with hyperref package"}}}