{"id": "1601.03651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2016", "title": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation", "abstract": "Today, neural networks play an important role in classifying relationships. By developing different neural architectures, researchers have significantly improved performance compared to traditional methods. However, existing neural networks for classifying relationships tend to be flat architectures (e.g. single-layer convolution neural networks or recurrent networks) that may not explore the potential representation space at different levels of abstraction. In this paper, we propose deep recurring neural networks (DRNNs) to meet this challenge. In addition, we propose a method for data augmentation using the directivity of relationships. We evaluate our DRNs on SemEval 2010 task 8 and achieve a $F _ 1 $score of 85.81% that exceeds the state-of-the-art recorded results.", "histories": [["v1", "Thu, 14 Jan 2016 16:30:41 GMT  (279kb,D)", "http://arxiv.org/abs/1601.03651v1", null], ["v2", "Thu, 13 Oct 2016 07:11:46 GMT  (690kb,D)", "http://arxiv.org/abs/1601.03651v2", "Accepted by COLING-16"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yan xu", "ran jia", "lili mou", "ge li", "yunchuan chen", "yangyang lu", "zhi jin"], "accepted": false, "id": "1601.03651"}, "pdf": {"name": "1601.03651.pdf", "metadata": {"source": "CRF", "title": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation", "authors": ["Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "emails": ["jiaran1994@gmail.com", "doublepower.mou@gmail.com", "chenyunchuan11@mails.ucas.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "Classifying relations between two entities in a given context is an important task in natural language processing (NLP). Take the following sentence as an example: \u201cJewelry and other smaller [valuables]e1 were locked in a [safe]e2 or a closet with a deadbolt.\u201d The marked entities valuables and safe are of relation Content-Container(e1, e2). Relation classification plays a key role in various NLP applications, and has become a hot research topic in recent years.\nNowadays, neural network-based approaches have made significant improvement in relation clas-\n\u2217Equal contribution.\nsification, compared with traditional methods based on either human-designed features (Kambhatla, 2004; Hendrickx et al., 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification. Xu et al. (2015b) apply long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. Nguyen and Grishman (2015) build model ensembles with gated recurrent unit (GRU)based RNNs and CNNs.\nWe have noticed that these neural models are all designed in shallow architectures, e.g., one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014). A natural question is then whether such deep architectures are beneficial to the relation classification task.\nIn this paper, we propose deep recurrent neural networks (DRNNs) to classify relations. The deep RNN can explore the representation space in different levels of abstraction and granularity. By visualizing how RNN units are related to the ultimate classification, we demonstrate that different layers indeed learn different representations, and that high level layers are more capable of information integration. Following Xu et al. (2015b), we use the shortest dependency path (SDP) as the backbone of our deep RNNs (Figure 1a).\nWe further observe that the relationship between two entities are directed. Two sub-\nar X\niv :1\n60 1.\n03 65\n1v 1\n[ cs\n.C L\n] 1\n4 Ja\nn 20\npaths, separated by entities\u2019 common ancestor, can be mapped to subject-predicate and object-predicate components of a relation. By changing the order of these two sub-paths, we obtain a new data sample with the inversed relationship (Figure 1b). Such data augmentation technique further improves the performance without the need of leveraging additional data resources as in Xu et al. (2015a).\nWe evaluate our proposed method on the SemEval-2010 relation classification task, and achieve a state-of-the-art F1-score of 85.81%.\nThe rest of this paper is organized as follows. Section 2 reviews related work; Section 3 describes our DRNN model in detail. Section 4 presents experimental results. Finally, we have our conclusion in Section 5."}, {"heading": "2 Related Work", "text": "Traditional methods for relation classification mainly fall into two groups: feature-based or kernelbased. The former approaches extract different types of features and feed them into a classifier. Kambhatla (2004) uses a maximum entropy model for feature combination. Hendrickx et al. (2009) collect various features, including lexical, syntactic as well as semantic features. By contrast, kernel-based methods do not have explicit feature representations,\nbut require predefined similarity measure of two data samples. Bunescu and Mooney (2005) design a kernel along the shortest dependency path (SDP) between two entities by observing that the relation strongly relies on SDPs. Plank and Moschitti (2013) combine structural information and semantic information in a tree kernel.\nNowadays, neural networks are playing an important role in this task. Socher et al. (2011) design a recursive neural network along the constituency parse tree. Hashimoto et al. (2013), on the basis of recursive networks, emphasize more on important phrases to improve performance. Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP. Xu et al. (2015b) first introduce gated recurrent networks, in particular LSTM, to this task. Zeng et al. (2014), on the other hand, apply CNNs to relation classification. Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs. From the viewpoint of model ensembling, Liu et al. (2015) combine CNNs and recursive networks along the SDP, while Nguyen and Grishman (2015) incorporate CNNs with RNNs."}, {"heading": "3 The Proposed Methodology", "text": "In this section, we describe our methodology in detail. Subsection 3.1 provides an overall picture of our DRNN model. Subsections 3.2 and 3.3 describe deep recurrent neural networks. The proposed data augmentation technique is introduced in Subsection 3.4. Finally, we present our training objective in Subsection 3.5."}, {"heading": "3.1 Overview", "text": "Figure 2 depicts the overall architecture of the DRNNs model. Given a sentence and its dependency parse tree,1 we follow Xu et al. (2015b) and build our neural network on the shortest dependency path (SDP), which serves as a backbone. In particular, an RNN picks up information along each subpath, separated by the common ancestor of marked entities. Also, we take advantage of four information channels, namely, word embeddings, POS\n1We use the Stanford parser (de Marneffe et al., 2006).\nembeddings, grammatical relation embeddings, and WordNet embeddings.\nDifferent from Xu et al. (2015b), we design deep RNNs with up to four hidden layers. These layers are stacked in a hierarchical way so as to capture information in different levels of abstraction. For each RNN layer, max pooling gathers information from different recurrent nodes. Notice that the four channels (with eight sub-paths) are processed in a similar way. Then all pooling layers are concatenated and fed into a hidden layer for information integration. Finally, we have a softmax output layer for classification."}, {"heading": "3.2 Recurrent Neural Networks on Shortest Dependency Path", "text": "In this subsection, we introduce a single layer of RNN based on SDP, serving as a building block of our deep architecture.\nCompared with a raw word sequence or a whole parse tree, the shortest dependency path (SDP) between two entities has two main advantages. First, it reduces irrelevant information; second, grammatical relations between words focus on the action and agents in a sentence, which is naturally suitable for\nrelation classification. Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.\nConcentrated on the SDP, an RNN keeps a hidden state vector h, changing with the input word at each step accordingly. Conrectely, the hidden state ht, for the t-th word in the sub-path, depends on its previous state ht\u22121 and the current word\u2019s embedding xt. For the simplicity and without loss of generality, we use vanilla recurrent networks with preceptron-like interaction, that is, the input is linearly transformed by a weight matrix and non-linearly squashed by an activation function, i.e.,\nht = f(Winxt +Wrecht\u22121 + bh) (1)\nwhere Win and Wrec are weight matrices for the input and recurrent connections, respectively. bh is a bias term, and f is a non-linear activation function (ReLU in our experiment)."}, {"heading": "3.3 Deep Recurrent Neural Networks", "text": "Although an RNN, as described above, is suitable for picking information along a sequence (a subpath\nin our task) by its iterative nature, the machine learning community suggests that deep architectures may be more capable of information integration, and can capture different levels of abstraction.\nA single-layer RNN can be viewed that it is deep along time steps. When unfolded, however, the RNN has only one hidden layer to capture the current input, as well as to retain the information in its previous step. In this sense, single-layer RNNs are actually shallow in information processing (Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).\nIn the relation classification task, words along SDPs provide information from difference perspectives. On the one hand, the marked entities themselves are informative. On the other hand, the entities\u2019 common ancestor (typically verbs) tells how the two entities are related to each other. Such heterogeneous information might necessitate more complexity machinery than a single RNN layer.\nFollowing such intuition, we investigate deep RNNs by stacking multiple hidden layers on top of one another, that is, every layer of the deep RNNs treats its previous layer as input, and computes its activation similar to Equation 1. Formally, we have\nh (i) t = f(W (i\u22121) in h (i\u22121) t +W (i) rech (i) t\u22121\n+W (i\u22121) cross h (i\u22121) t\u22121 + b (i)) (2)\nwhere the subscripts refer to time steps, and superscripts indicate the number of layers. Note that, to enhance information propagation, we add a \u201ccross\u201d connection for hidden layers (i \u2265 2) from the lower layer in the previous time step, given by W (i\u22121) cross h (i\u22121) t\u22121 in Equation 2. (See also \u2197 and \u2196 arrows in Figure 2)."}, {"heading": "3.4 Data Augmentation", "text": "Neural networks, especially deep ones, are likely to be prone to overfitting. The SemEval-2010 relation classification dataset, we use, comprises only several thousand samples, which may not fully sustain the training of deep RNNs.\nTo mitigate this problem, we propose a data augmentation technique for relation classification by making use of the directionality of relationships.\nThe two sub-paths\n[valuables]e1 \u2192 jewelry\u2192 locked\nlocked\u2190 in\u2190 closet\u2190 [safe]e2\nin Figure 1, for example, can be mapped to the subject-predicate and objectpredicate components in the relation Content-Container(e1, e2). If we change the order of these two sub-paths, we obtain\n[safe]e1 \u2192 closet\u2192 in\u2192 locked\nlocked\u2190 jewelry\u2190 [valuables]e2\nThen the relationship becomes ContainerContent(e1, e2), which is exactly the inverse of Context-Container(e1, e2). In this way, we can augment the dataset without using additional resources."}, {"heading": "3.5 Training Objective", "text": "For each recurrent layer and embedding layer (over each sub-path for each channel), we apply a max pooling layers to gather information. In total, we have 40 pools, which are concatenated and fed to a hidden layer for information integration.\nFinally, a softmax layer outputs the estimated probability that two sub-paths (sleft and sright are of relation r. For a single data sample i, we apply the standard cross-entropy loss, denoted as J(slefti , s right i , ri). With the data augmentation technique, our overall training objective is\nJ = m\u2211 i=1 J(slefti , s right i , ri) + J(s right i , s left i , r \u22121 i )\n+\u03bb \u03c9\u2211\ni=1\n\u2016Wi\u2016F\nwhere r\u22121 refers to the inverse of relation r. m is the number of data samples in the original training set. \u03c9 is the number of weights in our DRNN model. \u03bb is a regularization coefficient, and \u2016 \u00b7 \u2016F denotes Frobenius norm of a matrix.\nFor decoding (predicting the relation of an unseen sample), the data augmentation techniques provide new opportunities, because we can use the probability of r(e1, e2), or r\u22121(e2, e1), or both. Section 2 provides detailed discussion."}, {"heading": "4 Experiments", "text": "In this section, we present our experiments in detail. Subsection 4.1 introduces the dataset; Subsection 4.2 describes hyperparameter settings. We have discussion of data augmentation in Subsection 4.3, and the rational for using RNNs in Subsection 4.4. Subsection 4.5 compares our DRNNs model with other methods in the literature. In Subsection 4.6, we have quantitative and qualitative analysis of how the depth affects our model."}, {"heading": "4.1 Dataset", "text": "We evaluated our DRNNs model on the SemEval2010 Task 8 dataset,2 which is an established benchmark for relation classification (Hendrickx et al., 2009). The dataset contains 8000 sentences for training, and 2717 for testing. We split 800 samples out of the training set for validation.\nThe dataset distinguishes 10 relations, as follows. \u2022 Cause-Effect \u2022 Component-Whole \u2022 Content-Container \u2022 Entity-Destination \u2022 Entity-Origin \u2022 Message-Topic \u2022 Member-Collection \u2022 Instrument-Agency \u2022 Product-Producer \u2022 Other The former 9 relations are directed, whereas the Other class is undirected. In total, we have 19 different classes."}, {"heading": "4.2 Hyperparameter Setting", "text": "This subsection presents hyperparameter setting of our proposed model. For a fair comparison, we basically followed the settings in Xu et al. (2015b). Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the Wikipedia corpus; embeddings in other channels were 50-dimensional initialized randomly. The hidden layers in each channel had the same number of units as their embeddings (either 200 or 50); the penultimate hidden layer was 100-dimensional. An `2 penalty of 10\u22125 was also applied as in Xu et al. (2015b), but we chose the dropout rate by validation\n2Implementation based on Mou et al. (2015).\nwith a granularity of 5% for our model variants (with different depths).\nWe also chose the depth of DRNNs by validation from the set {1, 2, \u00b7 \u00b7 \u00b7 , 5}, and 4-layer DRNNs yield the highest performance. Section 4.6 provides both quantitative and qualitative analysis regarding the effect of depth.\nWe applied stochastic gradient descent (with mini-batch 20) for optimization, where gradients are computed by standard back-propagation."}, {"heading": "4.3 Data Augmentation Details", "text": "As mentioned in Section 4.1, the SemEval-2010 task 8 dataset contains an undirected class Other in addition to 9 directed relations (18 classes). For data augmentation, it is natural that the inversed Other relation is also in the Other class itself. However, if we augment all the relations, we observe a performance degradation of 0.4% (Table 2). We deem that the Other class contains mainly noise, and is inimical to our model. Then we conducted another experiment where we only augmented the Other class. The result verifies our conjecture as we obtained an even larger degradation of 0.8% in this setting.\nThe pilot experiments suggest that we should take into consideration unfavorable noise when performing data augmentation. In this experiment, if we reverse the directed relations only and leave the Other class intact, the performance is improved by a large margin of 1.7%. This shows that our proposed data augmentation technique does help to mitigate the problem of data sparseness, if we carefully rule out the impact of noise.\nDuring validation and testing, we shall decode the target label of an unseen data sample (with two entities e1 and e2). Through data augmentation, we are equipped with the probability of r\u22121(e2, e1) in addition to r(e1, e2). In our experiment, we tried several settings and chose to use r\u22121(e2, e1) only, because it yields the highest the validation result. We think this is probably because the Other class brings more noise to r than r\u22121, as the Other class is not augmented (and hence asymmetric).\nWe would like to point out that our data augmentation method is a general technique for relation classification, which is not ad hoc to a specific dataset; that the methodology for dealing with noise is also potentially applicable to other datasets."}, {"heading": "4.4 RNNs v.s. CNNs", "text": "As both RNNs and CNNs are prevailing neural models for NLP, we are curious whether deep architecture is also beneficial to CNNs. We tried a CNN with a sliding window of size 3 like Xu et al. (2015a); other settings were as our DRNNs.\nThe result is shown in Table 3. We observe that a single layer of CNN is also effective, yielding an F1-score slightly worse than our RNN. But the deep architecture hurts the performance of CNNs in this task. We provide a plausible explanation as follows. When convolution is performed, the beginning and end of a sentence are typically padded with a special symbol or simply zero. However, the shortest dependency path between two entities is usually not very long (\u223c4 on average). Hence, sentence boundaries may play a large role in convolution, which makes CNNs vulnerable.\nOn the contrary, RNNs can deal with sentence boundaries smoothly, and the performance continues to increase with up to 4 hidden layers. Detailed analysis is deferred to Section 4.6."}, {"heading": "4.5 Results", "text": "Table 4 compares our DRNNs model with other state-of-the-art methods. The first entry in the table presents the highest performance achieved by traditional feature-based methods. Hendrickx et al.\n(2009) feed a variety of handcrafted features to the SVM classifier and achieve an F1-score of 82.2%.\nRecent performance improvements on this dataset are mostly achieved with the help of neural networks. Socher et al. (2012) build a recursive neural network on the constituency tree and achieve a comparable performance with Hendrickx et al. (2009). Further, they extend their recursive network with matrix-vector interaction and elevate the F1score to 82.4%. Ebrahimi and Dou (2015) restrict the network to SDP, which is slight better than a sentence-wide recursive network. Xu et al. (2015b) first introduce a type of gated recurrent neural network (LSTM) into this task and raise the F1-score to 83.7%.\nFrom the perspective of convolution, Zeng et al. (2014) construct a CNN on the word sequence; they also integrate word position embeddings, which help a lot on the CNN architecture. dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset. Doing so greatly improves the performance to a high F1-score of 85.6%. Besides, two representative hybrid models of neural networks are designed by Liu et al. (2015) and Nguyen and Grishman (2015).\nWithout the use of neural networks, Yu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM), which combines unlexicalized linguistic contexts and word embeddings. They achieve an F1-score of 83.0%.\nOur DRNNs model, along with data augmentation, achieves an F1-score of 85.8%. Without using additional data resources (Xu et al., 2015a), or ensembling methods (Nguyen and Grishman, 2015), we outperform previous state-of-the-art results. Even if we do not apply data augmentation, our DRNNs model yields 83.9% F1-score, which is also the highest score achieved by a single model trained on the dataset per se without dataset-specific denoising. The above results show the effectiveness of DRNNs, especially trained with a large (augmented) dataset."}, {"heading": "4.6 Analysis of DRNNs\u2019 Depth", "text": "In this subsection, we analyze the effect of depth in our DRNNs model. We have tested the depth from the set {1, 2, \u00b7 \u00b7 \u00b7 , 5}, and plot the result in Figure 3. As we see, the performance is increasing from depth 2 to depth 4, and reaches the highest point on depth 4. The performance on depth 5 has a little degradation, but is still at a relatively close level. Considering efficiency and model complexity, we did not try deeper models (e.g. \u2265 6), and chose 4 in our experiment.\nBesides, we investigate how RNN units\u2019 in different layers are related to the ultimate task of interest. This is accomplished by tracing back information from pooling layers. Noticing that the pooling layer takes maximum value in each dimension, we can compute how much a hidden layer\u2019s units are gathered by pooling for further processing. In this way, we are able to demonstrate the information flow in RNN hidden units. We plot three examples in Figure 4. Here, rectangles refer to RNN hidden layers, unfolded along time. (Rounded rectangles are word embeddings.) The shade of color reflects the ratio of the pooling proportion. \u2022 Sample 1: \u201cUntil 1864 [vessels]e1 in the service\nof certain UK public offices defaced the Red Ensign with the [badge]e2 of their office\u201d with label Instrument-Agency(e2, e1). Its two sub-paths of SDP are\n[vessels]e1 \u2192 until\u2192 defaced defaced\u2190 with\u2190 [badge]e2\nFrom Figure 4a, we can see that entities like\nvessels and badge are darker than the verb phrase defaced with on the embedding layer. When information is propagating horizontally and vertically, these entities are getting lighter, while the verb phrase becomes darker gradually. Intuitively, we think that, considering relation Instrument-Agency(e2, e1), it is hard to make judgements only by two entities vessels and badge. When adding the semantic of verb phrase defaced with, we are more aware of the target relation. \u2022 Sample 2: \u201cMost of the [verses]e1 of the planta-\ntion songs had some reference to [freedom]e2\u201d with label Message-Topic(e1, e2). Its two sub-paths of SDP are\n[verses]e1 \u2192 of\u2192 most\u2192 had had\u2190 reference\u2190 to\u2190 [freedom]e2\nSimilar to Sample 1, we can see from Figure 4b that the color of the \u201cpivot\u201d verb had is getting darker vertically, and becomes the darkest in the fourth RNN layer, indicating the highest pooling portion. This is probably because had links two ends of the relation, Message and Topic. \u2022 Sample 3: \u201cA more spare, less robust\nuse of classical [motifs]e1 is evident in a [ewer]e2 of 1784-85\u201d with label Component-Whole(e1, e2). Its two sub-paths of SDP are\n[motifs]e1 \u2192 of\u2192 use\u2192 evident evident\u2190 in\u2190 [ewer]e2\nDifferent from Figures 4a and 4b, higher layers pay more attention on entities rather than their ancestors. Considering the ComponentWhole relation in this sample, we observe that the entities, motifs and ewer, are more infor-\nmative than the other words like use, in, and evident, which contain mostly irrelevant information. Therefore, the pooling proportion of entities (motifs, ewer) is increasing, while other words\u2019 proportion is decreasing, as expected.\nWe summarize our findings as follows. (1) Pooled information typically peaks at one or a few words in the embedding layer. This makes sense because there is no information flow in this layer. (2) Information scatters over a wider range in hidden layers, showing that the recurrent propagation does mix information. (3) For a higher level layer, the network pays more attention on those words that are more relevant to the relation, either entities or the verb phrase, depending on the specific sentence."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed deep recurrent neural networks, named DRNNs, to improve the performance of relation classification. The DRNNs model, consisting of several RNN layers, explores the representation space of different abstraction levels. By visualizing the DRNNs\u2019 units, we demonstrated that high level layers are more capable of integrating information relevant to target relations. In addition, we have designed a data augmentation strategy by leveraging the directionality of relations.\nWhen evaluated on the SemEval dataset, our DRNNs model results in substantial performance boost. The F1-score generally improves as the depth increases, and reaches the highest point when the depth is 4. Our result also suggests potential future directions of neural network-based relation classification: building deep architectures as well as enriching datasets."}], "references": [{"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan C. Bunescu", "Raymond J. Mooney."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 724\u2013731.", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation, volume 6, pages 449\u2013454.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C\u0131cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics, pages 626\u2013634.", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Chain based rnn for relation classification", "author": ["Javid Ebrahimi", "Dejing Dou."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1244\u20131249.", "citeRegEx": "Ebrahimi and Dou.,? 2015", "shortCiteRegEx": "Ebrahimi and Dou.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["Kazuma Hashimoto", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Hashimoto et al\\.,? 2013", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen."], "venue": "Advances in Neural Information Processing Systems, pages 190\u2013198.", "citeRegEx": "Hermans and Schrauwen.,? 2013", "shortCiteRegEx": "Hermans and Schrauwen.", "year": 2013}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720\u2013728.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["Nanda Kambhatla."], "venue": "Proceedings of the ACL 2004 on Interactive Poster and Demonstration Ses-", "citeRegEx": "Kambhatla.,? 2004", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "A dependency-based neural network for relation classification", "author": ["Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1504.01106.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Combining neural networks and log-linear models to improve relation extraction", "author": ["Thien Huu Nguyen", "Ralph Grishman."], "venue": "arXiv preprint arXiv:1511.05926.", "citeRegEx": "Nguyen and Grishman.,? 2015", "shortCiteRegEx": "Nguyen and Grishman.", "year": 2015}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Barbara Plank", "Alessandro Moschitti."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498\u20131507.", "citeRegEx": "Plank and Moschitti.,? 2013", "shortCiteRegEx": "Plank and Moschitti.", "year": 2013}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "pages 536\u2013540.", "citeRegEx": "Xu et al\\.,? 2015a", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Xu et al\\.,? 2015b", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Factor-based compositional embedding models", "author": ["Mo Yu", "Matthew Gormley", "Mark Dredze."], "venue": "NIPS Workshop on Learning Semantics.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335\u2013", "citeRegEx": "Zeng et al\\.,? 2014", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "sification, compared with traditional methods based on either human-designed features (Kambhatla, 2004; Hendrickx et al., 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 86, "endOffset": 127}, {"referenceID": 6, "context": "sification, compared with traditional methods based on either human-designed features (Kambhatla, 2004; Hendrickx et al., 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 86, "endOffset": 127}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 19, "endOffset": 72}, {"referenceID": 14, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 19, "endOffset": 72}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al.", "startOffset": 20, "endOffset": 93}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification.", "startOffset": 20, "endOffset": 115}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification. Xu et al. (2015b) apply long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path.", "startOffset": 20, "endOffset": 221}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification. Xu et al. (2015b) apply long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. Nguyen and Grishman (2015) build model ensembles with gated recurrent unit (GRU)based RNNs and CNNs.", "startOffset": 20, "endOffset": 359}, {"referenceID": 4, "context": ", one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 168, "endOffset": 242}, {"referenceID": 7, "context": ", one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 168, "endOffset": 242}, {"referenceID": 8, "context": ", one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 168, "endOffset": 242}, {"referenceID": 17, "context": "Following Xu et al. (2015b), we use the shortest dependency path (SDP) as the backbone of our deep RNNs (Figure 1a).", "startOffset": 10, "endOffset": 28}, {"referenceID": 17, "context": "Such data augmentation technique further improves the performance without the need of leveraging additional data resources as in Xu et al. (2015a).", "startOffset": 129, "endOffset": 147}, {"referenceID": 7, "context": "Kambhatla (2004) uses a maximum entropy model for feature combination.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Hendrickx et al. (2009) collect various features, including lexical, syntactic as well as semantic features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Bunescu and Mooney (2005) design a kernel along the shortest dependency path (SDP) between two entities by observing that the relation strongly relies on SDPs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "Bunescu and Mooney (2005) design a kernel along the shortest dependency path (SDP) between two entities by observing that the relation strongly relies on SDPs. Plank and Moschitti (2013) combine structural information and semantic information in a tree kernel.", "startOffset": 0, "endOffset": 187}, {"referenceID": 10, "context": "Socher et al. (2011) design a recursive neural network along the constituency parse tree.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Hashimoto et al. (2013), on the basis of recursive networks, emphasize more on important phrases to improve performance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP. Xu et al. (2015b) first introduce gated recurrent networks, in particular LSTM, to this task.", "startOffset": 0, "endOffset": 113}, {"referenceID": 2, "context": "Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP. Xu et al. (2015b) first introduce gated recurrent networks, in particular LSTM, to this task. Zeng et al. (2014), on the other hand, apply CNNs to relation classification.", "startOffset": 0, "endOffset": 208}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model.", "startOffset": 21, "endOffset": 42}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs.", "startOffset": 21, "endOffset": 141}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs. From the viewpoint of model ensembling, Liu et al. (2015) combine CNNs and recursive networks along the SDP, while Nguyen and Grishman (2015) incorporate CNNs with RNNs.", "startOffset": 21, "endOffset": 248}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs. From the viewpoint of model ensembling, Liu et al. (2015) combine CNNs and recursive networks along the SDP, while Nguyen and Grishman (2015) incorporate CNNs with RNNs.", "startOffset": 21, "endOffset": 332}, {"referenceID": 17, "context": "Given a sentence and its dependency parse tree,1 we follow Xu et al. (2015b) and build our neural network on the shortest dependency path (SDP), which serves as a backbone.", "startOffset": 59, "endOffset": 77}, {"referenceID": 17, "context": "Different from Xu et al. (2015b), we design deep RNNs with up to four hidden layers.", "startOffset": 15, "endOffset": 33}, {"referenceID": 3, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 10, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 18, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 17, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 7, "context": "In this sense, single-layer RNNs are actually shallow in information processing (Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 80, "endOffset": 133}, {"referenceID": 8, "context": "In this sense, single-layer RNNs are actually shallow in information processing (Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 80, "endOffset": 133}, {"referenceID": 6, "context": "We evaluated our DRNNs model on the SemEval2010 Task 8 dataset,2 which is an established benchmark for relation classification (Hendrickx et al., 2009).", "startOffset": 127, "endOffset": 151}, {"referenceID": 11, "context": "Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the Wikipedia corpus; embeddings in other channels were 50-dimensional initialized randomly.", "startOffset": 74, "endOffset": 96}, {"referenceID": 16, "context": "For a fair comparison, we basically followed the settings in Xu et al. (2015b). Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al.", "startOffset": 61, "endOffset": 79}, {"referenceID": 11, "context": "Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the Wikipedia corpus; embeddings in other channels were 50-dimensional initialized randomly. The hidden layers in each channel had the same number of units as their embeddings (either 200 or 50); the penultimate hidden layer was 100-dimensional. An `2 penalty of 10\u22125 was also applied as in Xu et al. (2015b), but we chose the dropout rate by validation", "startOffset": 75, "endOffset": 409}, {"referenceID": 12, "context": "Implementation based on Mou et al. (2015). Variant of Data augmentation F1 No Augmentation 83.", "startOffset": 24, "endOffset": 42}, {"referenceID": 6, "context": "2 (Hendrickx et al., 2009) depdency parse, Levin classes, PropBank, FanmeNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner", "startOffset": 2, "endOffset": 26}, {"referenceID": 15, "context": "8 (Socher et al., 2011) + POS, NER, WordNet 77.", "startOffset": 2, "endOffset": 23}, {"referenceID": 16, "context": "1 (Socher et al., 2012) + POS, NER, WordNet 82.", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": "7 (Zeng et al., 2014) + position embeddings, WordNet 82.", "startOffset": 2, "endOffset": 21}, {"referenceID": 3, "context": "7 (Ebrahimi and Dou, 2015)", "startOffset": 2, "endOffset": 26}, {"referenceID": 19, "context": "6 (Yu et al., 2014) + dependency parsing, NER 83.", "startOffset": 2, "endOffset": 19}, {"referenceID": 18, "context": "(Xu et al., 2015b) Word + POS + GR + WordNet embeddings 83.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "0 (Liu et al., 2015) Word embeddings + NER 83.", "startOffset": 2, "endOffset": 20}, {"referenceID": 17, "context": "7 (Xu et al., 2015a) + negative sampling from NYT dataset 85.", "startOffset": 2, "endOffset": 20}, {"referenceID": 13, "context": "4 (Nguyen and Grishman, 2015) Word+POS+NER+WordNet embeddings, CNNs, RNNs + Voting 84.", "startOffset": 2, "endOffset": 29}, {"referenceID": 17, "context": "We tried a CNN with a sliding window of size 3 like Xu et al. (2015a); other settings were as our DRNNs.", "startOffset": 52, "endOffset": 70}, {"referenceID": 13, "context": "Socher et al. (2012) build a recursive neural network on the constituency tree and achieve a comparable performance with Hendrickx et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(2012) build a recursive neural network on the constituency tree and achieve a comparable performance with Hendrickx et al. (2009). Further, they extend their recursive network with matrix-vector interaction and elevate the F1score to 82.", "startOffset": 107, "endOffset": 131}, {"referenceID": 3, "context": "Ebrahimi and Dou (2015) restrict the network to SDP, which is slight better than a sentence-wide recursive network.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Ebrahimi and Dou (2015) restrict the network to SDP, which is slight better than a sentence-wide recursive network. Xu et al. (2015b) first introduce a type of gated recurrent neural network (LSTM) into this task and raise the F1-score to 83.", "startOffset": 0, "endOffset": 134}, {"referenceID": 15, "context": "From the perspective of convolution, Zeng et al. (2014) construct a CNN on the word sequence; they also integrate word position embeddings, which help a lot on the CNN architecture.", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset.", "startOffset": 4, "endOffset": 278}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset. Doing so greatly improves the performance to a high F1-score of 85.6%. Besides, two representative hybrid models of neural networks are designed by Liu et al. (2015) and Nguyen and Grishman (2015).", "startOffset": 4, "endOffset": 568}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset. Doing so greatly improves the performance to a high F1-score of 85.6%. Besides, two representative hybrid models of neural networks are designed by Liu et al. (2015) and Nguyen and Grishman (2015).", "startOffset": 4, "endOffset": 599}, {"referenceID": 17, "context": "Without using additional data resources (Xu et al., 2015a), or ensembling methods (Nguyen and Grishman, 2015), we outperform previous state-of-the-art results.", "startOffset": 40, "endOffset": 58}, {"referenceID": 13, "context": ", 2015a), or ensembling methods (Nguyen and Grishman, 2015), we outperform previous state-of-the-art results.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "Without the use of neural networks, Yu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM), which combines unlexicalized linguistic contexts and word embeddings.", "startOffset": 36, "endOffset": 53}], "year": 2017, "abstractText": "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent, compared with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolution neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluate our DRNNs on the SemEval-2010 Task 8, and achieve an F1score of 85.81%, outperforming state-of-theart recorded results.", "creator": "TeX"}}}