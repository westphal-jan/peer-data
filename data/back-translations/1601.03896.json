{"id": "1601.03896", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "abstract": "The auto-generation of descriptions from natural images is a challenging problem that has recently attracted a lot of interest from computer vision and natural language processing communities. In this survey, we classify existing approaches based on the way they conceptualize this problem, namely models that present description either as a generational problem or as a retrieval problem across a visual or multimodal representation space. We offer a detailed review of existing models, highlighting their pros and cons. In addition, we provide an overview of the benchmark image datasets and evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally, we extrapolate future directions in the field of auto-image description generation.", "histories": [["v1", "Fri, 15 Jan 2016 12:50:32 GMT  (4282kb)", "http://arxiv.org/abs/1601.03896v1", "To appear in JAIR"], ["v2", "Mon, 24 Apr 2017 09:47:20 GMT  (4282kb)", "http://arxiv.org/abs/1601.03896v2", "Journal of Artificial Intelligence Research 55, 409-442, 2016"]], "COMMENTS": "To appear in JAIR", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["raffaella bernardi", "ruket cakici", "desmond elliott", "aykut erdem", "erkut erdem", "nazli ikizler-cinbis", "frank keller", "adrian muscat", "barbara plank"], "accepted": false, "id": "1601.03896"}, "pdf": {"name": "1601.03896.pdf", "metadata": {"source": "CRF", "title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "authors": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank"], "emails": ["bernardi@disi.unitn.it", "ruken@ceng.metu.edu.tr", "d.elliott@uva.nl", "aykut@cs.hacettepe.edu.tr", "erkut@cs.hacettepe.edu.tr", "nazli@cs.hacettepe.edu.tr", "keller@inf.ed.ac.uk", "adrian.muscat@um.edu.mt", "bplank@cst.dk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n03 89"}, {"heading": "1. Introduction", "text": "Over the past two decades, the fields of natural language processing (NLP) and computer vision (CV) have seen great advances in their respective goals of analyzing and generating text, and of understanding images and videos. While both fields share a similar set of methods rooted in artificial intelligence and machine learning, they have historically developed separately, and their scientific communities have typically interacted very little.\nRecent years, however, have seen an upsurge of interest in problems that require a combination of linguistic and visual information. A lot of everyday tasks are of this nature, e.g., interpreting a photo in the context of a newspaper article, following instructions in conjunction with a diagram or a map, understanding slides while listening to a lecture. In\naddition to this, the web provides a vast amount of data that combines linguistic and visual information: tagged photographs, illustrations in newspaper articles, videos with subtitles, and multimodal feeds on social media. To tackle combined language and vision tasks and to exploit the large amounts of multimodal data, the CV and NLP communities have moved closer together, for example by organizing workshops on language and vision that have been held regularly at both CV and NLP conferences over the past few years.\nIn this new language-vision community, automatic image description has emerged as a key task. This task involves taking an image, analyzing its visual content, and generating a textual description (typically a sentence) that verbalizes the most salient aspects of the image. This is challenging from a CV point of view, as the description could in principle talk about any visual aspect of the image: it can mention objects and their attributes, it can talk about features of the scene (e.g., indoor/outdoor), or verbalize how the people and objects in the scene interact. More challenging still, the description could even refer to objects that are not depicted (e.g., it can talk about people waiting for a train, even when the train is not visible because it has not arrived yet) and provide background knowledge that cannot be derived directly from the image (e.g., the person depicted is the Mona Lisa). In short, a good image description requires full image understanding, and therefore the description task is an excellent test bed for computer vision systems, one that is much more comprehensive than standard CV evaluations that typically test, for instance, the accuracy of object detectors or scene classifiers over a limited set of classes.\nImage understanding is necessary, but not sufficient for producing a good description. Imagine we apply an array of state-of-the-art detectors to the image to localize objects (e.g., Felzenszwalb, Girshick, McAllester, & Ramanan, 2010; Girshick, Donahue, Darrell, & Malik, 2014), determine attributes (e.g., Lampert, Nickisch, & Harmeling, 2009; Berg, Berg, & Shih, 2010; Parikh & Grauman, 2011), compute scene properties (e.g., Oliva & Torralba, 2001; Lazebnik, Schmid, & Ponce, 2006), and recognize human-object interactions (e.g., Prest, Schmid, & Ferrari, 2012; Yao & Fei-Fei, 2010). The result would be a long, unstructured list of labels (detector outputs), which would be unusable as an image description. A good image description, in contrast, has to be comprehensive but concise (talk about all and only the important things in the image), and has to be formally correct, i.e., consists of grammatically well-formed sentences.\nFrom an NLP point of view, generating such a description is a natural language generation (NLG) problem. The task of NLG is to turn a non-linguistic representation into human-readable text. Classically, the non-linguistic representation is a logical form, a database query, or a set of numbers. In image description, the input is an image representation (e.g., the detector outputs listed in the previous paragraph), which the NLG model has to turn into sentences. Generating text involves a series of steps, traditionally referred to as the NLP pipeline (Reiter & Dale, 2006): we need to decide which aspects of the input to talk about (content selection), then we need to organize the content (text planning) and verbalize it (surface realization). Surface realization in turn requires choosing the right words (lexicalization), using pronouns if appropriate (referential expression generation), and grouping related information together (aggregation).\nIn other words, automatic image description requires not only full image understanding, but also sophisticated natural language generation. This is what makes it such an interesting\ntask that has been embraced by both the CV and the NLP communities.1 Note that the description task can become even more challenging when we take into account that good descriptions are often user-specific. For instance, an art critic will require a different description than a librarian or a journalist, even for the same photograph. We will briefly touch upon this issue when we talk about the difference between descriptions and captions in Section 3 and discuss future directions in Section 4.\nGiven that automatic image description is such an interesting task, and it is driven by the existence of mature CV and NLP methods and the availability of relevant datasets, a large image description literature has appeared over the last five years. The aim of this survey article is to give a comprehensive overview of this literature, covering models, datasets, and evaluation metrics.\nWe sort the existing literature into three categories based on the image description models used. The first group of models follows the classical pipeline we outlined above: they first detect or predict the image content in terms of objects, attributes, scene types, and actions, based on a set of visual features. Then, these models use this content information to drive a natural language generation system that outputs an image description. We will term these approaches direct generation models.\nThe second group of models cast the problem as a retrieval problem. That is, to create a description for a novel image, these models search for images in a database that are similar to the novel image. Then they build a description for the novel image based on the descriptions of the set of similar images that was retrieved. The novel image is described by simply reusing the description of the most similar retrieved image (transfer), or by synthesizing a novel description based on the description of a set of similar images. Retrieval-based models can be further subdivided based on what type of approach they use to represent images and compute similarity. The first subgroup of models uses a visual space to retrieve images, while the second subgroup uses a multimodal space that represents images and text jointly. For an overview of the models that will be reviewed in this survey, and which category they fall into, see Table 1.\nGenerating natural language descriptions from videos presents unique challenges over and above image-based description, as it additionally requires analyzing the objects and their attributes and actions in the temporal dimension. Models that aim to solve description generation from videos have been proposed in the literature (e.g., Khan, Zhang, & Gotoh, 2011; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, & Saenko, 2013; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013; Rohrbach, Qiu, Titov, Thater, Pinkal, & Schiele, 2013; Thomason, Venugopalan, Guadarrama, Saenko, & Mooney, 2014; Rohrbach, Rohrback, Tandon, & Schiele, 2015; Yao, Torabi, Cho, Ballas, Pal, Larochelle, & Courville, 2015; Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba, & Fidler, 2015). However, most existing work on description generation has used static images, and this is what we will focus on in this survey.2\nIn this survey article, we first group automatic image description models into the three categories outlined above and provide a comprehensive overview of the models in each\n1. Though some image description approaches circumvent the NLG aspect by transferring human-authored descriptions, see Sections 2.2 and 2.3. 2. An interesting intermediate approach involves the annotation of image streams with sequences of sentences, see the work of Park and Kim (2015).\ncategory in Section 2. We then examine the available multimodal image datasets used for training and testing description generation models in Section 3. Furthermore, we review evaluation measures that have been used to gauge the quality of generated descriptions in Section 3. Finally, in Section 4, we discuss future research directions, including possible new tasks related to image description, such as visual question answering."}, {"heading": "2. Image Description Models", "text": "Generating automatic descriptions from images requires an understanding of how humans describe images. An image description can be analyzed in several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). We follow Hodosh, Young, and Hockenmaier (2013) and assume that the descriptions that are of interest for this survey article are the ones that verbalize visual and conceptual information depicted in the image, i.e., descriptions that refer to the depicted entities, their attributes and relations, and the actions they are involved in. Outside the scope of automatic image description are non-visual descriptions, which give background information or refer to objects not depicted in the image (e.g., the location at which the image was taken or who took the picture). Also, not relevant for standard approaches to image description are perceptual descriptions, which capture the global low-level visual characteristics of images (e.g., the dominant color in the image or the type of the media such as photograph, drawing, animation, etc.).\nIn the following subsections, we give a comprehensive overview of state-of-the-art approaches to description generation. Table 1 offers a high-level summary of the field, using the three categories of models outlined in the introduction: direct generation models, retrieval models from visual space, and retrieval model from multimodal space."}, {"heading": "2.1 Description as Generation from Visual Input", "text": "The general approach of the studies in this group is to first predict the most likely meaning of a given image by analyzing its visual content, and then generate a sentence reflecting this meaning. All models in this category achieve this using the following general pipeline architecture:\n1. Computer vision techniques are applied to classify the scene type, to detect the objects present in the image, to predict their attributes and the relationships that hold between them, and to recognize the actions taking place.\n2. This is followed by a generation phase that turns the detector outputs into words or phrases. These are then combined to produce a natural language description of the image, using techniques from natural language generation (e.g., templates, n-grams, grammar rules).\nThe approaches reviewed in this section perform an explicit mapping from images to descriptions, which differentiates them from the studies described in Section 2.2 and 2.3, which incorporate implicit vision and language models. An illustration of a sample model is shown in Figure 1. An explicit pipeline architecture, while tailored to the problem at hand, constrains the generated descriptions, as it relies on a predefined sets of semantic classes of scenes, objects, attributes, and actions. Moreover, such an architecture crucially assumes\nthe accuracy of the detectors for each semantic class, an assumption that is not always met in practice.\nApproaches to description generation differ along two main dimensions: (a) which image representations they derive descriptions from, and (b) how they address the sentence generation problem. In terms of the representations used, existing models have conceptualized images in a number of different ways, relying on spatial relationships (Farhadi et al., 2010), corpus-based relationships (Yang et al., 2011), or spatial and visual attributes (Kulkarni et al., 2011). Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012). More recently, Yatskar et al. (2014) proposed to generate descriptions from denselylabeled images, which incorporate object, attribute, action, and scene annotations. Similar in spirit is the work by Fang et al. (2015), which does not rely on prior labeling of objects, attributes, etc. Rather, the authors train \u201cword detectors\u201d directly from images and their associated descriptions using multi-instance learning (a weakly supervised approach for the training of object detectors). The words returned by these detectors are then fed into a language model for sentence generation, followed by a re-ranking step.\nThe first framework to explicitly represent how the structure of an image relates to the structure of its description is the Visual Dependency Representations (VDR) method proposed by Elliott and Keller (2013). A VDR captures the spatial relations between the objects in an image in the form of a dependency graph. This graph can then be related to the syntactic dependency tree of the description of the image.3 While initial work using VDRs has relied on a corpus of manually annotated VDRs for training, more recent approaches induce VDRs automatically based on the output of an object detector (Elliott & de Vries, 2015) or the labels present in abstract scenes (Ortiz et al., 2015).4 The idea of explicitly representing image structure and using it for description generation has been picked up\n3. VDRs have proven useful not only for description generation, but also for image retrieval (Elliott, Lavrenko, & Keller, 2014). 4. Abstract scenes are schematic images, typically constructed using clip-art. They are employed to avoid the need for an object detector, as the labels and positions of all objects are know. An example is Zitnick and Parikh\u2019s (2013) dataset, see Section 3 for details.\nby Lin et al. (2015), who parse images into scene graphs, which are similar to VDRs and represent the relations between the objects in a scene. They then generate from scene graphs using a semantic grammar.5\nExisting approaches also vary along the second dimension, viz., in how they approach the sentence generation problem. At the one end of the scale, there are approaches that use n-gram-based language models. Examples include the works by Kulkarni et al. (2011) and Li et al. (2011), which both generate descriptions using n-gram language models trained on a subset of Wikipedia. These approaches first determine the attributes and relationships between regions in an image as region\u2013preposition\u2013region triples. The n-gram language model is then used to compose an image description that is fluent, given the language model. The approach of Fang et al. (2015) is similar, but uses a maximum entropy language model instead of an n-gram model to generate descriptions. This gives the authors more flexibility in handling the output of the word detectors that are at the core of their model.\nRecent image description work using recurrent neural networks (RNNs) can also be regarded as relying on language modeling. A classical RNN is a language model: it captures the probability of generating a given word in a string, given the words generated so far. In an image description setup, the RNN is trained to generate the next word given not only the string so far, but also a set of image features. In this setting, the RNN is therefore not purely a language model (as in the case of an n-gram model, for instance), but it is a hybrid model that relies on a representation that incorporates both visual and linguistic features. We will return to this in more detail in Section 2.3.\nA second set of approaches use sentence templates to generate descriptions. These are (typically manually) pre-defined sentence frames in which open slots need to be filled with labels for objects, relations, or attributes. For instance, Yang et al. (2011) fill in a sentence template by selecting the likely objects, verbs, prepositions, and scene types based on a Hidden Markov Model. Verbs are generated by finding the most likely pairing of object labels in the Gigaword external corpus. The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time.\nOther approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions. Another linguistically expressive model has recently been proposed by Ortiz et al. (2015). The authors model image description as machine translation over VDR\u2013sentence pairs and perform explicit content selection and surface realization using an integer linear program over linguistic constraints.\nThe systems presented so far aimed at directly generating novel descriptions. However, as argued by Hodosh et al. (2013), framing image description as a natural language generation (NLG) task makes it difficult to objectively evaluate the quality of novel descriptions\n5. Note that graphs are also used for image retrieval by Johnson, Krishna, Stark, Li, Shamma, Bernstein, and Fei-Fei (2015) and Schuster, Krishna, Chang, Fei-Fei, and Manning (2015).\nas it \u201cintroduces a number of linguistic difficulties that detract attention from the underlying image understanding problem\u201d (Hodosh et al., 2013). At the same time, evaluation of generation systems is known to be difficult (Reiter & Belz, 2009). Hodosh et al. therefore propose an approach that makes it possible to evaluate the mapping between images and sentences independently of the generation aspect. Models that follow this approach conceptualize image description as a retrieval problem: they associate an image with a description by retrieving and ranking a set of similar images with candidate descriptions. These candidate descriptions can then either be used directly (description transfer) or a novel description can be synthesized from the candidates (description generation).\nThe retrieval of images and ranking of their descriptions can be carried out in two ways: either from a visual space or from a multimodal space that combines textual and visual information space. In the following subsections, we will survey work that follows these two approaches."}, {"heading": "2.2 Description as a Retrieval in Visual Space", "text": "The studies in this group pose the problem of automatically generating the description of an image by retrieving images similar to the query image (i.e., the new image to be described); this is illustrated in Figure 2. In other words, these systems exploit similarity in the visual space to transfer descriptions to the query images. Compared to models that generate descriptions directly (Section 2.1), retrieval models typically require a large amount of training data in order to provide relevant descriptions.\nIn terms of their algorithmic components, visual retrieval approaches typically follow a pipeline of three main steps:\n1. Represent the given query image by specific visual features.\n2. Retrieve a candidate set of images from the training set based on a similarity measure in the feature space used.\n3. Re-rank the descriptions of the candidate images by further making use of visual and/or textual information contained in the retrieval set, or alternatively combine fragments of the candidate descriptions according to certain rules or schemes.\nOne of the first model to follow this approach was the Im2Text model of Ordonez et al. (2011). GIST (Oliva & Torralba, 2001) and Tiny Image (Torralba, Fergus, & Freeman, 2008)\ndescriptors are employed to represent the query image and to determine the visually similar images in the first retrieval step. Most of the retrieval-based models consider the result of this step as a baseline. For the re-ranking step, a range of detectors (e.g., object, stuff, pedestrian, action detectors) and scene classifiers specific to the entities mentioned in the candidate descriptions are first applied to the images to better capture their visual content, and the images are represented by means of these detector and classifier responses. Finally, the re-ranking is carried out via a classifier trained over these semantic features.\nThe model proposed by Kuznetsova et al. (2012) first runs the detectors and the classifiers used in the re-ranking step of the Im2Text model on a query image to extract and represent its semantic content. Then, instead of performing a single retrieval by combining the responses of these detectors and classifiers as the Im2Text model does, it carries out a separate image retrieval step for each visual entity present in the query image to collect related phrases from the retrieved descriptions. For instance, if a dog is detected in the given image, then the retrieval process returns the phrases referring to visually similar dogs in the training set. More specifically, this step is used to collect three different kinds of phrases. Noun and verb phrases are extracted from descriptions in the training set based on the visual similarity between object regions detected in the training images and in the query image. Similarly, prepositional phrases are collected for each stuff detection in the query image by measuring the visual similarity between the detections in the query and training images based on their appearance and geometric arrangements. Prepositional phrases are additionally collected for each scene context detection by measuring the global scene similarity computed between the query and training images. Finally, a description is generated from these collected phrases for each detected object via integer linear programming (ILP) which considers factors such as word ordering, redundancy, etc.\nThe method of Gupta et al. (2012) is another phrase-based approach. To retrieve visually similar images, the authors employ simple RGB and HSV color histograms, Gabor and Haar descriptors, GIST and SIFT (Lowe, 2004) descriptors as image features. Then, instead of using visual object detectors or scene classifiers, they rely only on the textual information in the descriptions of the visually similar images to extract the visual content of the input image. Specifically, the candidate descriptions are segmented into phrases of a certain type such as (subject, verb), (subject, prep, object), (verb, prep, object), (attribute, object), etc. Those that best describe the input image are determined according to a joint probability model based on image similarity and Google search counts, and the image is represented by triplets of the form {((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}. In the end, the description is generated using the three top-scoring triplets based on a fixed template. To increase the quality of the descriptions, the authors also apply syntactic aggregation and some subject and predicate grouping rules before the generation step.\nPatterson et al. (2014) were the first to present a large-scale scene attribute dataset in the computer vision community. The dataset includes 14,340 images from 707 scene categories, which are annotated with certain attributes from a list of 102 discriminative attributes related to materials, surface properties, lighting, affordances, and spatial layout. This allows them to train attribute classifiers from this dataset. In their paper, the authors also demonstrate that the responses of these attribute classifiers can be used as a global image descriptor which captures the semantic content better than the standard global image\ndescriptors such as GIST. As an application, they extended the baseline model of Im2Text by replacing the global features with automatically extracted scene attributes, giving better image retrieval and description results.\nMason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step. In particular, the authors represented images by using the scene attributes descriptor of Patterson et al. (2014). Once the visually similar images are identified from the training set, in the next step, the conditional probabilities of observing a word in the description of the query image are estimated via non-parametric density estimation using the descriptions of the retrieved images. The final output description is then determined by using two different extractive summarization techniques, one depending on the SumBasic model (Nenkova & Vanderwende, 2005) and the other based on Kullback-Leibler divergence between the word distributions of the query and the candidate descriptions.\nYagcioglu et al. (2015) proposed an average query expansion approach which is based on compositional distributed semantics. To represent images, they use features extracted from the recently proposed Visual Geometry Group convolutional neural network (VGG-CNN; Chatfield, Simonyan, Vedaldi, & Zisserman, 2014). These features are the activations of the last layer of a deep neural network trained on ImageNet, which have been proven to be effective in many computer vision problems. Then, the original query is expanded as the average of the distributed representations of retrieved descriptions, weighted by their similarity to the input image.\nThe approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image. It then selects a description from the candidate descriptions associated with the retrieved images that best describes the images that are similar to the query image, just like the approaches by Mason and Charniak (2014) and Yagcioglu et al. (2015). Their approach differs in terms of how they represent the similarity between description and how they select the best candidate over the whole set. Specifically, they propose to compute the description similarity based on the n-gram overlap F-score between the descriptions. They suggest to choose the output description by finding the description that corresponds to the description with the highest mean n-gram overlap with the other candidate descriptions (k-nearest neighbor centroid description) estimated via an n-gram similarity measure."}, {"heading": "2.3 Description as a Retrieval in Multimodal Space", "text": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). The intuition behind these models is illustrated in Figure 3, and the overall approach can be characterized as follows:\n1. Learn a common multimodal space for the visual and textual data using a training set of image\u2013description pairs.\n2. Given a query, use the joint representation space to perform cross-modal (image\u2013 sentence) retrieval.\nIn contrast to the retrieval models that work on a visual space (Section 2.2), where unimodal image retrieval is followed by ranking of the retrieved descriptions, here image and sentence features are projected into a common multimodal space. Then, the multimodal space is used to retrieve descriptions for a given image. The advantage of this approach is that it allows bi-directional models, i.e., the common space can also be used for the other direction, retrieving the most appropriate image for a query sentence.\nIn this section, we first discuss the seminal paper of Hodosh et al. (2013) on description retrieval, and then present more recent approaches that combine a retrieval approach with some form of natural language generation. Hodosh et al. (2013) map both images and sentences into a common space. The joint space can be used for both image search (find the most plausible image given a sentence) and image annotation (find the sentence that describes the image well), see Figure 3. In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009. The representation was thus limited to a set of pre-defined discrete slot fillers, which was given as training information. Instead, Hodosh et al. (2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space. CCA takes a training dataset of image-sentence pairs, i.e., Dtrain = {\u3008i, s\u3009}, thus input from two different feature spaces, and finds linear projections into a newly induced common space. In KCCA, kernel functions map the original items into higher-order space in order to capture the patterns needed to associate image and text. KCCA has been shown previously to be successful in associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) or image regions (Socher & Fei-Fei, 2010) with individual words or set of tags.\nHodosh et al. (2013) compare their KCCA approach to a nearest-neighbor (NN) baseline that uses unimodal text and image spaces, without constructing a joint space. A drawback of KCCA is that it is only applicable to smaller datasets, as it requires the two kernel matrices to be kept in memory during training. This becomes prohibitive for very large datasets.\nSome attempts have been made to circumvent the computational burden of KCCA, e.g., by resorting to linear models (Hodosh & Hockenmaier, 2013). However, recent work on description retrieval has instead utilized neural networks to construct a joint space for image description generation.\nSocher et al. (2014) use neural networks for building sentence and image vector representations that are then mapped into a common embedding space. A novelty of their work is that they use compositional sentence vector representations. First, image and word representations are learned in their single modalities, and finally mapped into a common multimodal space. In particular, they use a DT-RNN (Dependency Tree Recursive Neural Network) for composing language vectors to abstract over word order and syntactic difference that are semantically irrelevant. This results in 50-dimensional word embeddings. For the image space, the authors use a nine layer neural network trained on ImageNet data, using unsupervised pre-training. Image embeddings are derived by taking the output of the last layer (4,096 dimensions). The two spaces are then projected into a multi-modal space through a max-margin objective function that intuitively trains pairs of correct image and sentence vectors to have a high inner product. The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013).\nKarpathy et al. (2014) extend the previous multi-modal embeddings model. Rather than directly mapping entire images and sentences into a common embedding space, their model embeds more fine-grained units, i.e., fragments of images (objects) and sentences (dependency tree fragments), into a common space. Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014). A similar approach is pursued by Pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping between image representations and sentences. A constrained language model is then used to generate from this representation. A conceptually related approach is pursued by Ushiku et al. (2015): the authors use a common subspace model which maps all feature vectors associated with the same phrase into nearby regions of the space. For generation, a beamsearch based decoder or templates are used.\nDescription generation systems are difficult to evaluate, therefore the studies reviewed above treat the problem as a retrieval and ranking task (Hodosh et al., 2013; Socher et al., 2014). While such an approach has been valuable because it enables comparative evaluation, retrieval and ranking is limited by the availability of existing datasets with descriptions. To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).\nKiros et al. (2015) introduced a general encoder-decoder framework for image description ranking and generation, illustrated in Figure 4. Intuitively the method works as follows. The encoder first constructs a joint multimodal space. This space can be used to rank images and descriptions. The second stage (decoder) then uses the shared multimodal representation to generate novel descriptions. Their model, directly inspired by recent work in machine translation, encodes sentences using a Long\u2013Short Term Memory (LSTM) recurrent neural network, and image features using a deep convolutional network (CNN). LSTM is an extension of the recurrent neural network (RNN) that incorporates built-\nin memory to store information and exploit long range context. In Kiros et al.\u2019s (2015) encoder-decoder model, the vision space is projected into the embedding space of the LSTM hidden states; a pairwise ranking loss is minimized to learn the ranking of images and their descriptions. The decoder, a neural-network-based language model, is able to generate novel descriptions from this multimodal space.\nWork that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.\u2019s (2015a) model for the learning of novel visual concepts.\nKarpathy and Fei-Fei (2015) improve on previous models by proposing a deep visualsemantic alignment model with a simpler architecture and objective function. Their key insight is to assume that parts of the sentence refer to particular but unknown regions in the image. Their model tries to infer the alignments between segments of sentences and regions of images and is based on convolutional neural networks over image regions, bidirectional RNN over sentences and a structured objective that aligns the two modalities. Words and image regions are mapped into a common multimodal embedding. The multimodal recurrent neural network architecture uses the inferred alignments to learn and generate novel descriptions. Here, the image is used as condition for the first state in the recurrent neural network, which then generates image descriptions.\nAnother model that can generate novel sentences is proposed in Chen and Zitnick (2015). In contrast to the previous work, their model dynamically builds a visual representation of the scene as a description is being generated. That is, a word is read or generated and the\nvisual representation is updated to reflect the new information. They accomplish this with a simple RNN. The model achieves comparable or better results than most prior studies, except for the recently proposed deep visual-semantic alignment model (Karpathy & Fei-Fei, 2015). The model of Xu et al. (2015) is closely related in that it also uses an RNN-based architecture in which the visual representations are dynamically updated. Xu et al.\u2019s (2015) model incorporates an attentional component, which gives it a way of determining which regions in an image are salient, and it can focus its description on those regions. While resulting in an improvement in description accuracy, it also makes it possible to analyze model behavior by visualizing the regions that were attended to during each word that was generated by the model.\nThe general RNN-based ranking and generation approach is also followed by Lebret et al. (2015). Here, the main innovation is on the linguistic side: they employ a bilinear model to learn a common space of image features and syntactic phrases (noun phrases, verb phrases, and prepositional phrases). A Markov model is then utilized to generate sentences from these phrase embedding. On the visual side, standard CNN-based features are used. This results in an elegant modeling framework, whose performance is broadly comparable to the state of the art.\nFinally, two important directions that are less explored are: portability and weakly supervised learning. Verma and Jawahar (2014) evaluate the portability of a bi-directional model based on topic models, showing that performance significantly degrades. They highlight the importance of cross-dataset image description retrieval evaluation. Another interesting observation is that all of the above models require a training set of fully-annotated image-sentence pairs. However, obtaining such data in large quantities is prohibitively expensive. Gong et al. (2014) propose an approach based on weak supervision that transfers knowledge from millions of weakly annotated images to improve the accuracy of description retrieval."}, {"heading": "2.4 Comparison of Existing Approaches", "text": "The discussion in the previous subsections makes it clear that each approach to image description has its particular strengths and weaknesses. For example, the methods that cast the task as a generation problem (Section 2.1) have an advantage over other types of approaches in that they can produce novel sentences to describe a given image. However, their success relies heavily on how accurately they estimate the visual content and how well they are able to verbalize this content. In particular, they explicitly employ computer vision techniques to predict the most likely meaning of a given image; these methods have limited accuracy in practice, hence if they fail to identify the most important objects and their attributes, then no valid description can be generated. Another difficulty lies in the final description generation step; sophisticated natural language generation is crucial to guarantee fluency and grammatical correctness of the generated sentences. This can come at the price of considerable algorithmic complexity.\nIn contrast, image description methods that cast the problem as a retrieval from a visual space problem and transfer the retrieved descriptions to a novel image (Section 2.2) always produce grammatically correct descriptions. This is guaranteed by design, as these systems fetch human-generated sentences from visually similar images. The main issue with\nthis approach is that it requires large amounts of images with human-written descriptions. That is, the accuracy (but not the grammaticality) of the descriptions reduces as the size of the training set decreases. The training set also needs to be diverse (in addition to being large), in order for visual retrieval-based approaches to produce image descriptions that are adequate for novel test images (Devlin et al., 2015). Though this problem can be mitigated by re-synthesizing a novel description from the retrieved ones (see Section 2.2).\nApproaches that cast image description as a retrieval from a multimodal space problem (Section 2.3) also have the advantage of generating human-like descriptions as they are able to retrieve the most appropriate ones from a pre-defined large pool of descriptions. However, ranking these descriptions requires a cross-modal similarity metric that compares images and sentences. Such metrics are difficult to define, compared to the unimodal image-to-image similarity metrics used by retrieval models that work on a visual space. Additionally, training a common space for images and sentences requires a large training set of images annotated with human-generated descriptions. On the plus side, such a multimodal embedding space can also be used for the reverse problem, i.e., for retrieving the most appropriate image for a query sentence. This is something generation-based or visual retrieval-based approaches are not capable of."}, {"heading": "3. Datasets and Evaluation", "text": "There is a wide range of datasets for automatic image description research. The images in these datasets are associated with textual descriptions and differ from each other in certain aspects such as in size, the format of the descriptions and in how the descriptions were collected. Here we review common approaches for collecting datasets, the datasets themselves, and evaluation measures for comparing generated descriptions with ground-truth texts. The datasets are summarized in Table 2, and examples of images and descriptions are given in Figure 5. The readers can also refer to the dataset survey by Ferraro, Mostafazadeh, Huang, Vanderwende, Devlin, Galley, and Mitchell (2015) for an analysis similar to ours. It provides a basic comparison of some of the existing language and vision datasets. It is not limited to automatic image description, and it reports some simple statistics and quality metrics such as perplexity, syntactic complexity, and abstract to concrete word ratios."}, {"heading": "3.1 Image-Description Datasets", "text": "The Pascal1K sentence dataset (Rashtchian et al., 2010) is a dataset which is commonly used as a benchmark for evaluating the quality of description generation systems. This medium-scale dataset, consists of 1,000 images that were selected from the Pascal 2008 object recognition dataset (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010) and includes objects from different visual classes, such as humans, animals, and vehicles. Each image is associated with five descriptions generated by humans on Amazon Mechanical Turk (AMT) service.\nThe Visual and Linguistic Treebank (VLT2K; Elliott & Keller, 2013) makes use of images from the Pascal 2010 action recognition dataset. It augments these images with three, twosentence descriptions per image. These descriptions were collected on AMT with specific\n7. Kuznetsova et al. (2014) ran a human judgments study on 1,000 images from this dataset.\ninstructions to verbalize the main action depicted in the image and the actors involved (first sentence), while also mentioning the most important background objects (second sentence). For a subset of 341 images of the Visual and Linguistic Treebank, object annotation is available (in the form of polygons around all objects mentioned in the descriptions). For this subset, manually created Visual Dependency Representations (see Section 2.1) are also included (three VDRs per images, i.e., a total of 1023).\nThe Flickr8K dataset (Hodosh et al., 2013) and its extended version Flickr30K dataset (Young et al., 2014) contain images from Flickr, comprising approximately 8,000 and 30,000 images, respectively. The images in these two datasets were selected through user queries for specific objects and actions. These datasets contain five descriptions per image which were collected from AMT workers using a strategy similar to that of the Pascal1K dataset.\nThe Abstract Scenes dataset (Zitnick & Parikh, 2013; Zitnick, Parikh, & Vanderwende, 2013) consists of 10,000 clip-art images and their descriptions. The images were created through AMT, where workers were asked to place a fixed vocabulary of 80 clip-art objects into a scene of their choosing. The descriptions were then sourced for these worker-created scenes. The authors provided these descriptions in two different forms. While the first group contains a single sentence description for each image, the second group includes two alternative descriptions per image. Each of these two descriptions consist of three simple sentences with each sentence describing a different aspect of the scene. The main advantage of this dataset is it affords the opportunity to explore image description generation without the need for automatic object recognition, thus avoiding the associated noise. A more recent version of this dataset has been created as a part of the visual question-answering (VQA) dataset (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, & Parikh, 2015). It contains 50,000 different scene images with more realistic human models and with five single-sentence descriptions.\nThe IAPR-TC12 dataset introduced by Grubinger et al. (2006) is one of the earliest multi-modal datasets and contains 20,000 images with descriptions. The images were originally retrieved via search engines such as Google, Bing and Yahoo, and the descriptions were produced in multiple languages (predominantly English and German). Each image is associated with one to five descriptions, where each description refers to a different aspect of the image, where applicable. The dataset also contains complete pixel-level segmentation of the objects.\nThe MS COCO dataset (Lin et al., 2014) currently consists of 123,287 images with five different descriptions per image. Images in this dataset are annotated for 80 object categories, which means that bounding boxes around all instances in one of these categories are available for all images. The MS COCO dataset has been widely used for image description, something that is facilitated by the standard evaluation server that has recently become available13 . Extensions of MS COCO are currently under development, including the addition of questions and answers (Antol et al., 2015).\nOne paper (Lin et al., 2015) uses an the NYU dataset (Silberman, Kohli, Hoiem, & Fergus, 2012), which contains 1,449 indoor scenes with 3D object segmentation. This dataset has been augmented with five descriptions per image by Lin et al."}, {"heading": "3.2 Image-Caption Datasets", "text": "Image descriptions verbalize what can be seen in the image, i.e., they refer to the objects, actions, and attributes depicted, mention the scene type, etc. Captions, on the other hand, are typically texts associated with images that verbalize information that cannot be seen in the image. A caption provides personal, cultural, or historical context for the image (Panofsky, 1939). Images shared through social networking or photo-sharing websites can be accompanied by descriptions or captions, or a mixtures of both types of text. The images in a newspaper or a museum will typically contain cultural or historical texts, i.e., captions not descriptions.\nThe BBC News dataset (Feng & Lapata, 2008) was one of the earliest collections of images and co-occurring texts. Feng and Lapata (2008) harvested 3,361 news articles from the British Broadcasting Corporation News website, with the constraint that the article includes an image and a caption.\nThe SBU1M Captions dataset introduced by Ordonez et al. (2011) differs from the previous datasets in that it is a web-scale dataset containing approximately one million captioned images. It is compiled from data available on Flickr with user-provided image descriptions. The images were downloaded and filtered from Flickr with the constraint that an image contained at least one noun and one verb on predefined control lists. The resulting dataset is provided as a CSV file of URLs.\n8. Source http://nlp.cs.illinois.edu/HockenmaierGroup/pascal-sentences/index.html 9. Source http://github.com/elliottd/vlt\n10. Source https://illinois.edu/fb/sec/1713398 11. Source http://imageclef.org/photodata 12. Source http://research.microsoft.com/en-us/um/people/larryz/clipart/SemanticClassesRender\n/Classes_v1.html\n13. Source http://mscoco.org/explore 13. Source http://mscoco.org/dataset/#captions-eval\nThe De\u0301ja\u0300-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images with 180,000 near-identical captions harvested from Flickr. 760 million images were downloaded from Flickr during the calendar year 2013 using a set of 693 nouns as queries. The image captions are normalized through lemmatization and stop word removal to create a corpus of the near-identical texts. For instance, the sentences the bird flies in blue sky and a bird flying into the blue sky were normalized to bird fly IN blue sky (Chen et al., 2015). Image\u2013 caption pairs are retained if the captions are repeated by more than one user in normalized form."}, {"heading": "3.3 Collecting Datasets", "text": "Collecting new image\u2013text datasets is typically performed through crowd-sourcing or harvesting data from the web. The images for these datasets have either been sourced from an existing task in the computer vision community \u2013 the Pascal challenge (Everingham et al., 2010) was used to the Pascal1K and VLT2K datasets \u2013 directly from Flickr, in the case of Flickr8K/30K, MS COCO, SBU1M Captions, and De\u0301ja\u0300-Image Captions datasets, or crowdsourced, in the case of the Abstract Scenes dataset. The texts in image\u2013description datasets are usually crowd-sourced from Amazon Mechanical Turk or Crowdflower; whereas the texts in image\u2013caption datasets have been harvested from photo-sharing sites, such as Flickr, or from news providers. Captions are usually collected without financial incentive because they are written by the people sharing their own images, or by journalists.\nCrowd-sourcing the descriptions of images involves defining a simple task that can be performed by untrained workers. Examples of the task guidelines used by Hodosh et al. (2013) and Elliott and Keller (2013) are given in Figure 6. In both instances, care was taken to clearly inform the potential workers about the expectations for the task. In particular, explicit instructions were given on how the descriptions should be written, and examples of good texts were provided. In addition, Hodosh et al. provided more extensive examples to explain what would constitute unsatisfactory texts. Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013).\nThe issue of remuneration for crowd-sourced workers is controversial, and higher payments do not always lead to better quality in a crowd-sourced environment (Mason &Watts, 2009). Rashtchian et al. (2010) paid $0.01/description, Elliott and Keller (2013) paid $0.04 for an average of 67 seconds of work to produce a two-sentence description. To the best of our knowledge, such information is not available for the other datasets."}, {"heading": "3.4 Evaluation Measures", "text": "Evaluating the output of a natural language generation (NLG) system is a fundamentally difficult task (Dale & White, 2007; Reiter & Belz, 2009). The most common way to assess the quality of automatically generated texts is the subjective evaluation by human experts. NLG-produced text is typically judged in terms of grammar and content, indicating how syntactically correct and how relevant the text is, respectively. Fluency of the generated\n15. Source Appendix of the work by Hodosh et al. (2013)\ntext is sometimes tested as well, especially when a surface realization technique is involved during the generation process. Automatically generated descriptions for images can be evaluated using the same NLG techniques. Typically, judges are provided with the image as well as with the description during evaluation tasks. Subjective human evaluations of machine generated image descriptions are often performed on Mechanical Turk with the help\nof questions. So far, the following Likert-scale questions have been used to test datasets and user groups of various sizes.\n\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).\n\u2022 The description is grammatically correct (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013, inter alia).\n\u2022 The description has no incorrect information (Mitchell et al., 2012).\n\u2022 The description is relevant for this image (Li et al., 2011; Yang et al., 2011).\n\u2022 The description is creatively constructed (Li et al., 2011).\n\u2022 The description is human-like (Mitchell et al., 2012).\nAnother approach for evaluating descriptions is to use automatic measures, such as BLEU (Papineni, Roukos, Ward, & Zhu, 2002), ROUGE (Lin & Hovy, 2008), Translation Error Rate (Feng & Lapata, 2013), Meteor (Denkowski & Lavie, 2014), or CIDEr (Vedantam, Lawrence Zitnick, & Parikh, 2015). These measures were originally developed to evaluate the output of machine translation engines or text summarization systems, with the exception of CIDEr, which was developed specifically for image description evaluation. All these measures compute a score that indicates the similarity between the system output and one or more human-written reference texts (e.g., ground truth translations or summaries). This approach to evaluation has been subject to much discussion and critique (Kulkarni et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014). Kulkarni et al. found weakly negative or no correlation between human judgments and unigram BLEU on the Pascal 1K Dataset (Pearson\u2019s \u03c1 = -0.17 and 0.05). Hodosh et al. studied the Cohen\u2019s \u03ba correlation of expert human judgments and binarized unigram BLEU and unigram ROUGE of retrieved descriptions on the Flickr8K dataset. They found the best agreement between humans and BLEU (\u03ba = 0.72) or ROUGE (\u03ba = 0.54) when the system retrieved the sentences originally associated with the images. Agreement dropped when only one reference sentence was available, or when the reference sentences were disjoint from the proposal sentences. They concluded that neither measure was appropriate for image description evaluation and subsequently proposed image\u2013sentence ranking experiments, discussed in more detail below. Elliott and Keller analyzed the correlation between human judgments and automatic evaluation measures for retrieved and system-generated image descriptions in the Flickr8K and VLT2K datasets. They showed that sentence-level unigram BLEU, which at that point in time was the de facto standard measure for image description evaluation, is only weakly correlated with human judgments. Meteor (Banerjee & Lavie, 2005), a less frequently used translation evaluation measure, exhibited the highest correlation with human judgments. However, Kuznetsova et al. (2014) found that unigram BLEU was more strongly correlated with human judgments than Meteor for image caption generation.\nThe first large-scale image description evaluation took place during the MS COCO Captions Challenge 2015,15 featuring 15 teams with a dataset of 123,716 training images\n15. Source http://mscoco.org/dataset/cap2015\nand 41,000 images in a withheld test dataset. The number of reference texts for each testing image was either five or 40, based on the insight that some measures may benefit from larger reference sets (Vedantam et al., 2015). When automatic evaluation measures were used, some of the image description systems outperformed a human\u2013human upper bound,16 whether five or 40 reference descriptions were provided. However, none of the systems outperformed human\u2013human evaluation when a judgment elicitation task was used. Meteor was found to be the most robust measure, with the systems beating the human text on one and two submissions (depending on the number of references); the systems outperformed humans seven or five times measured with CIDEr; according to ROUGE and BLEU, the system nearly always outperformed the humans, further confirming the unsuitability of these evaluation measures.\nThe models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above. This evaluation paradigm was first proposed by Hodosh et al., who reported high correlation with human judgments for image\u2013sentence based ranking evaluations.\nIn Table 3, we summarize all the image description approaches discussed in this survey, and list the datasets and evaluation measures employed by each of these approaches. It can be seen that more recent systems (starting in 2014) have converged on the use of large description datasets (Flickr8K/30K, MS COCO) and employ evaluation measures that perform well in terms of correlation with human judgments (Meteor, CIDEr). However, the use of BLEU, despite its limitations, is still widespread; also the use of human evaluation is by no means universal in the literature."}, {"heading": "4. Future Directions", "text": "As this survey demonstrates, the CV and NLP communities have witnessed an upsurge in interest in automatic image description systems. With the help of recent advances in deep learning models for images and text, substantial improvements in the quality of automatically generated descriptions has been registered. Nevertheless, a series of challenges for image description research remain. In the following, we discuss future directions that this line of research is likely to benefit from."}, {"heading": "4.1 Datasets", "text": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, the introduction of Flickr30K, MS COCO and other large datasets has enabled the training of more complex models such as neural networks. Still, the area is likely to benefit from larger and diversified datasets that share a common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue that\n16. Calculated by collecting an additional human-written description, which was then compared to the reference descriptions.\nthe collection process and the quality of the descriptions in the datasets affect performance significantly, and make transfer learning between datasets not as effective as expected. They show that learning a model from MS COCO and applying it to datasets collected in different settings such as SBU1M Captions or Pascal1K, leads to a degradation in BLEU performance. This is surprising, since MS COCO offers a much larger amount of training data than Pascal1K. As Vinyals et al. put it, this is largely due to the differences in vocabulary and in the quality of descriptions. Most learning approaches are likely to suffer from such situations. Collecting larger and comprehensive datasets and developing more generic approaches that are capable of generating naturalistic descriptions across domains therefore is an open challenge.\nWhile supervised algorithms are likely to take advantage of carefully collected large datasets, lowering the amount of supervision in exchange of access to larger unsupervised data is also an interesting avenue for future research. Leveraging unsupervised data for building richer representations and description models is another open research challenge in this context."}, {"heading": "4.2 Measures", "text": "Designing automatic measures that can mimic human judgments in evaluating the suitability of image descriptions is perhaps the most urgent need in the area of image description (Elliott & Keller, 2014). This need can be dramatically observed at the latest evaluation results of MS COCO Challenge. According to existing measures, including the latest CIDEr measure (Vedantam et al., 2015), several automatic methods outperform the human upper bound (this upper bound indicates how similar human descriptions are to each other). The counterintuitive nature of this result is confirmed by the fact that when human judgments are used for evaluation, the output of even the best system is judged as worse than a human generated description for most of the time (Fang et al., 2015). However, since conducting human judgment experiments is costly, there is a major need for improved automatic measures that are more highly correlated with human judgments. Figure 7 plots the Epanechnikov probability density estimate (a non-parametric optimal estimator) for BLEU, Meteor, ROUGE, and CIDEr scores per subjective judgment in Flickr8K dataset. The human judgments were obtained from human experts (Hodosh et al., 2013). BLEU is once again confirmed to be unable to sufficiently discriminate between the lowest three human judgments, while Meteor and CIDEr show signs of moving towards a useful separation."}, {"heading": "4.3 Diversity and Originality", "text": "Current algorithms often rely on direct representations of the descriptions they see at training time, making the descriptions generated at test time very similar. This results in many repetitions and limits the diversity of the generated descriptions, making it difficult to reach human levels of performance. This situation has been demonstrated by Devlin et al. (2015), who show that their best model is able to generate only 47.0% of unique descriptions. Systems that generate diverse and original descriptions that do not just repeat what is already seen, but also infer the underlying semantics therefore remain as an open challenge. Chen and Zitnick (2015) and related approaches take a step towards addressing such limitations by coupling description and visual representation generation.\nJas and Parikh (2015) introduces the notion of image specificity, arguing that the domain of image descriptions is not uniform, certain images being more specific than others. Descriptions of non-specific images tend to vary a lot as people tend to describe a nonspecific scene from different aspects. This notion and its effects to description systems and measures should be investigated in further detail."}, {"heading": "4.4 Further Tasks", "text": "Another open challenge is visual question-answering (VQA). While natural language question-answering based on text has been a significant goal of NLP research for a long time (e.g., Liang, Jordan, & Klein, 2012; Fader, Zettlemoyer, & Etzioni, 2013; Richardson, Burges, & Renshaw, 2013; Fader, Zettlemoyer, & Etzioni, 2014), answering questions about images is a task that has recently emerged. Towards achieving this goal, Malinowski and Fritz (2014a) propose a Bayesian framework that connects natural language questionanswering with the visual information extracted from image parts. More recently, image question answering methods based on neural networks have been developed (Gao, Mao, Zhou, Huang, & Yuille, 2015; Ren, Kiros, & Zemel, 2015; Malinowski, Rohrbach, & Fritz, 2015; Ma, Lu, & Li, 2016). Following this effort, several datasets on this task are being released: DAQUAR (Malinowski & Fritz, 2014a) was compiled from scene depth images and mainly focuses on questions about the type, quantity and color of objects; COCOQA (Ren et al., 2015) was constructed by converting image descriptions to VQA format over a subset of images from the MS COCO dataset; the Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park, Berg, & Berg, 2015) and the VQA dataset (Antol et al., 2015), were again built for images from MS COCO, but this time question-answer pairs are collected via human annotators in a freestyle paradigm. Research in this emerging field is likely to flourish in the near future. The ultimate goal of VQA is to build systems that can pass the (recently developed) Visual Turing Test by being able to answer arbitrary questions about images with the same precision as a human observer (Malinowski & Fritz, 2014b; Geman, Geman, Hallonquist, & Younes, 2015).\nHaving multilingual repositories for image description is an interesting direction to explore. Currently, among the available benchmark datasets, only the IAPR-TC12 dataset (Grubinger et al., 2006) has multilingual descriptions (in English and German). Future work should investigate whether transferring multimodal features between monolingual description models results in improved descriptions compared to monolingual baselines. It would be interesting to study different models and new tasks in a multilingual multimodal setting using larger and more syntactically diverse multilingual description corpora.17\nOverall, image understanding is the ultimate goal of computer vision and natural language generation is one of the ultimate goals of NLP. Image description is where these both goals are interconnected and this topic is therefore likely to benefit from individual advances in each of these two fields.\n17. The Multimodal Translation Shared Task at the 2016 Workshop on Machine Translation will use an English and German translated version of the Flickr30K corpora. See http://www.statmt.org/wmt16/ multimodal-task.html for more details."}, {"heading": "5. Conclusions", "text": "In this survey, we discuss recent advances in automatic image description and closely related problems. We review and analyze a large body of the existing work by highlighting common characteristics and differences between existing research. In particular, we categorize the related work into three groups: (i) direct description generation from images, (i) retrieval of images from a visual space, and (iii) retrieval of images from multimodal (joint visual and linguistic) space. In addition, we provided a brief review of the existing corpora and automatic evaluation measures, and discussed some future directions for vision and language research.\nCompared to traditional keyword-based image annotation (using object recognition, attribute detection, scene labeling, etc.), automatic image description systems produce more human-like explanations of visual content, providing a more complete picture of the scene. Advancements in this field could lead to more intelligent artificial vision systems, which can make inferences about the scenes through the generated grounded image descriptions and therefore interact with their environments in a more natural manner. They could also have a direct impact on technological applications from which visually impaired people can benefit through more accessible interfaces.\nDespite the remarkable increase in the number of image description systems in recent years, experimental results suggest that system performance still falls short of human performance. A similar challenge lies in the automatic evaluation of systems using reference descriptions. The measures and the tools currently in use are not sufficiently highly correlated with human judgments, indicating a need for measures that can deal with the complexity of the image description problem adequately."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": null, "citeRegEx": "Banerjee and Lavie,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "Automatic attribute discovery and characterization from noisy web data", "author": ["T.L. Berg", "A.C. Berg", "J. Shih"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Berg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2010}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["J. Chen", "P. Kuznetsova", "D. Warren", "Y. Choi"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Chen and Zitnick,? \\Q2015\\E", "shortCiteRegEx": "Chen and Zitnick", "year": 2015}, {"title": "Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation: Position Papers", "author": ["R. Dale", "White", "M.E. (Eds"], "venue": null, "citeRegEx": "Dale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dale et al\\.", "year": 2007}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "In Conference of the European Chapter of the Association for Computational Linguistics Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski and Lavie,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie", "year": 2014}, {"title": "Language Models for Image Captioning: The Quirks and What Works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Describing images using inferred visual dependency representations", "author": ["D. Elliott", "A.P. de Vries"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Elliott and Vries,? \\Q2015\\E", "shortCiteRegEx": "Elliott and Vries", "year": 2015}, {"title": "Image Description using Visual Dependency Representations", "author": ["D. Elliott", "F. Keller"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Elliott and Keller,? \\Q2013\\E", "shortCiteRegEx": "Elliott and Keller", "year": 2013}, {"title": "Comparing Automatic Evaluation Measures for Image Description", "author": ["D. Elliott", "F. Keller"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Elliott and Keller,? \\Q2014\\E", "shortCiteRegEx": "Elliott and Keller", "year": 2014}, {"title": "Query-by-Example Image Retrieval using Visual Dependency Representations", "author": ["D. Elliott", "V. Lavrenko", "F. Keller"], "venue": "In International Conference on Computational Linguistics", "citeRegEx": "Elliott et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2014}, {"title": "The PASCAL Visual Object Classes (VOC) Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Paraphrase-driven learning for open question answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Automatic Image Annotation Using Auxiliary Text Information", "author": ["Y. Feng", "M. Lapata"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Feng and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Feng and Lapata", "year": 2008}, {"title": "Automatic caption generation for news images", "author": ["Y. Feng", "M. Lapata"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Feng and Lapata,? \\Q2013\\E", "shortCiteRegEx": "Feng and Lapata", "year": 2013}, {"title": "A survey of current datasets for vision and language research", "author": ["F. Ferraro", "N. Mostafazadeh", "T. Huang", "L. Vanderwende", "J. Devlin", "M. Galley", "M. Mitchell"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "A. Yuille"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "The IAPR TC-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P. Clough", "H. Muller", "T. Deselaers"], "venue": "In International Conference on Language Resources and Evaluation", "citeRegEx": "Grubinger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Choosing linguistics over vision to describe images", "author": ["A. Gupta", "Y. Verma", "C.V. Jawahar"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Sentence-based image description with scalable, explicit models", "author": ["M. Hodosh", "J. Hockenmaier"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition Workshops", "citeRegEx": "Hodosh and Hockenmaier,? \\Q2013\\E", "shortCiteRegEx": "Hodosh and Hockenmaier", "year": 2013}, {"title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": null, "citeRegEx": "Hotelling,? \\Q1936\\E", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "A conceptual framework for indexing visual information at multiple levels", "author": ["A. Jaimes", "Chang", "S.-F"], "venue": "In IST SPIE Internet Imaging", "citeRegEx": "Jaimes et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jaimes et al\\.", "year": 2000}, {"title": "Image specificity", "author": ["M. Jas", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Jas and Parikh,? \\Q2015\\E", "shortCiteRegEx": "Jas and Parikh", "year": 2015}, {"title": "Guiding the long-short term memory model for image caption generation", "author": ["X. Jia", "E. Gavves", "B. Fernando", "T. Tuytelaars"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Jia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2015}, {"title": "Image retrieval using scene graphs", "author": ["J. Johnson", "R. Krishna", "M. Stark", "Li", "L.-J", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Towards coherent natural language description of video streams", "author": ["M.U.G. Khan", "L. Zhang", "Y. Gotoh"], "venue": "In International Conference on Computer Vision Workshops", "citeRegEx": "Khan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2011}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "In Advances in Neural Information Processing Systems Deep Learning Workshop", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Generating Natural-Language Video Descriptions Using Text-Mined Knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R. Mooney", "K. Saenko", "S. Guadarrama"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Krishnamoorthy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamoorthy et al\\.", "year": 2013}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Collective Generation of Natural Image Descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kuznetsova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "TREETALK: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonezz", "T.L. Berg", "Y. Choi"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Lazebnik et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Phrase-based image captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "In The SIGNLL Conference on Computational Natural Language Learning", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2012}, {"title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "author": ["Lin", "C.-Y", "E. Hovy"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "Generating multi-sentence natural language descriptions of indoor scenes", "author": ["D. Lin", "S. Fidler", "C. Kong", "R. Urtasun"], "venue": "In British Machine Vision Conference", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Advances in Neural Information Processing Systems Workshop on Learning Semantics", "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["J. Mao", "X. Wei", "Y. Yang", "J. Wang", "Z. Huang", "A.L. Yuille"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Nonparametric Method for Data-driven Image Captioning", "author": ["R. Mason", "E. Charniak"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Mason and Charniak,? \\Q2014\\E", "shortCiteRegEx": "Mason and Charniak", "year": 2014}, {"title": "Financial incentives and the \u201dperformance of crowds", "author": ["W.A. Mason", "D.J. Watts"], "venue": "In ACM SIGKDD Workshop on Human Computation", "citeRegEx": "Mason and Watts,? \\Q2009\\E", "shortCiteRegEx": "Mason and Watts", "year": 2009}, {"title": "Midge: generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A.C. Berg", "K. Yamaguchi", "T.L. Berg", "K. Stratos", "III Daume"], "venue": "In Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "The impact of frequency on summarization", "author": ["A. Nenkova", "L. Vanderwende"], "venue": "Tech. rep., Microsoft Research", "citeRegEx": "Nenkova and Vanderwende,? \\Q2005\\E", "shortCiteRegEx": "Nenkova and Vanderwende", "year": 2005}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Oliva and Torralba,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2001}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Learning to Interpret and Describe Abstract Scenes", "author": ["L.M.G. Ortiz", "C. Wolff", "M. Lapata"], "venue": "In Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "Ortiz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ortiz et al\\.", "year": 2015}, {"title": "Studies in Iconology", "author": ["E. Panofsky"], "venue": null, "citeRegEx": "Panofsky,? \\Q1939\\E", "shortCiteRegEx": "Panofsky", "year": 1939}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Expressing an image stream with a sequence of natural sentences", "author": ["C. Park", "G. Kim"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Park and Kim,? \\Q2015\\E", "shortCiteRegEx": "Park and Kim", "year": 2015}, {"title": "The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding", "author": ["G. Patterson", "C. Xu", "H. Su", "J. Hays"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Patterson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2014}, {"title": "Simple image description generator via a linear phrase-based model", "author": ["P. Pinheiro", "R. Lebret", "R. Collobert"], "venue": "In International Conference on Learning Representations Workshop", "citeRegEx": "Pinheiro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pinheiro et al\\.", "year": 2015}, {"title": "Weakly supervised learning of interactions between humans and objects", "author": ["A. Prest", "C. Schmid", "V. Ferrari"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Prest et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Prest et al\\.", "year": 2012}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies Workshop on Creating Speech and Language Data with Amazon\u2019s", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "An investigation into the validity of some metrics for automatically evaluating natural language generation systems", "author": ["E. Reiter", "A. Belz"], "venue": "Computational Linguistics,", "citeRegEx": "Reiter and Belz,? \\Q2009\\E", "shortCiteRegEx": "Reiter and Belz", "year": 2009}, {"title": "Building Natural Language Generation Systems", "author": ["E. Reiter", "R. Dale"], "venue": null, "citeRegEx": "Reiter and Dale,? \\Q2006\\E", "shortCiteRegEx": "Reiter and Dale", "year": 2006}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "In International Conference on Machine Learningt Deep Learning Workshop", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["M. Richardson", "C.J. Burges", "E. Renshaw"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A dataset for movie description", "author": ["A. Rohrbach", "M. Rohrback", "N. Tandon", "B. Schiele"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Translating Video Content to Natural Language Descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["S. Schuster", "R. Krishna", "A. Chang", "L. Fei-Fei", "C.D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing Vision and Language Workshop", "citeRegEx": "Schuster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 2015}, {"title": "Analyzing the subject of a picture: A theoretical approach", "author": ["S. Shatford"], "venue": "Cataloging & Classification Quarterly,", "citeRegEx": "Shatford,? \\Q1986\\E", "shortCiteRegEx": "Shatford", "year": 1986}, {"title": "Indoor segmentation and support inference from RGBD images", "author": ["N. Silberman", "P. Kohli", "D. Hoiem", "R. Fergus"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of im- ages using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Socher and Fei.Fei,? \\Q2010\\E", "shortCiteRegEx": "Socher and Fei.Fei", "year": 2010}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A. Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "In International Conference on Computational Linguistics", "citeRegEx": "Thomason et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Common subspace for model and similarity: Phrase learning for caption generation from images", "author": ["Y. Ushiku", "M. Yamaguchi", "Y. Mukuta", "T. Harada"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Ushiku et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ushiku et al\\.", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval", "author": ["Y. Verma", "C.V. Jawahar"], "venue": "In British Machine Vision Conference", "citeRegEx": "Verma and Jawahar,? \\Q2014\\E", "shortCiteRegEx": "Verma and Jawahar", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "A Distributed Representation Based Query Expansion Approach for Image Captioning", "author": ["S. Yagcioglu", "E. Erdem", "A. Erdem", "R. Cakici"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yagcioglu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yagcioglu et al\\.", "year": 2015}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "III Daume", "Y. Aloimonos"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Grouplet: A structured image representation for recognizing human and object interactions", "author": ["B. Yao", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Yao and Fei.Fei,? \\Q2010\\E", "shortCiteRegEx": "Yao and Fei.Fei", "year": 2010}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images", "author": ["M. Yatskar", "M. Galley", "L. Vanderwende", "L. Zettlemoyer"], "venue": "In Joint Conference on Lexical and Computation Semantics", "citeRegEx": "Yatskar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Zitnick and Parikh,? \\Q2013\\E", "shortCiteRegEx": "Zitnick and Parikh", "year": 2013}], "referenceMentions": [{"referenceID": 70, "context": "An interesting intermediate approach involves the annotation of image streams with sequences of sentences, see the work of Park and Kim (2015).", "startOffset": 123, "endOffset": 143}, {"referenceID": 82, "context": "An image description can be analyzed in several different dimensions (Shatford, 1986; Jaimes & Chang, 2000).", "startOffset": 69, "endOffset": 107}, {"referenceID": 82, "context": "An image description can be analyzed in several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). We follow Hodosh, Young, and Hockenmaier (2013) and assume that the descriptions that are of interest for this survey article are the ones that verbalize visual and conceptual information depicted in the image, i.", "startOffset": 70, "endOffset": 157}, {"referenceID": 17, "context": "Elliott and de Vries (2015) X Fang et al. (2015) X", "startOffset": 30, "endOffset": 49}, {"referenceID": 43, "context": "Figure 1: The automatic image description generation system proposed by Kulkarni et al. (2011).", "startOffset": 72, "endOffset": 95}, {"referenceID": 18, "context": "In terms of the representations used, existing models have conceptualized images in a number of different ways, relying on spatial relationships (Farhadi et al., 2010), corpus-based relationships (Yang et al.", "startOffset": 145, "endOffset": 167}, {"referenceID": 93, "context": ", 2010), corpus-based relationships (Yang et al., 2011), or spatial and visual attributes (Kulkarni et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 43, "context": ", 2011), or spatial and visual attributes (Kulkarni et al., 2011).", "startOffset": 42, "endOffset": 65}, {"referenceID": 18, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 93, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 43, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 49, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 63, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 67, "context": "While initial work using VDRs has relied on a corpus of manually annotated VDRs for training, more recent approaches induce VDRs automatically based on the output of an object detector (Elliott & de Vries, 2015) or the labels present in abstract scenes (Ortiz et al., 2015).", "startOffset": 253, "endOffset": 273}, {"referenceID": 15, "context": "In terms of the representations used, existing models have conceptualized images in a number of different ways, relying on spatial relationships (Farhadi et al., 2010), corpus-based relationships (Yang et al., 2011), or spatial and visual attributes (Kulkarni et al., 2011). Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012). More recently, Yatskar et al. (2014) proposed to generate descriptions from denselylabeled images, which incorporate object, attribute, action, and scene annotations.", "startOffset": 146, "endOffset": 672}, {"referenceID": 15, "context": "Similar in spirit is the work by Fang et al. (2015), which does not rely on prior labeling of objects, attributes, etc.", "startOffset": 33, "endOffset": 52}, {"referenceID": 11, "context": "The first framework to explicitly represent how the structure of an image relates to the structure of its description is the Visual Dependency Representations (VDR) method proposed by Elliott and Keller (2013). A VDR captures the spatial relations between the objects in an image in the form of a dependency graph.", "startOffset": 184, "endOffset": 210}, {"referenceID": 101, "context": "An example is Zitnick and Parikh\u2019s (2013) dataset, see Section 3 for details.", "startOffset": 14, "endOffset": 42}, {"referenceID": 43, "context": "by Lin et al. (2015), who parse images into scene graphs, which are similar to VDRs and represent the relations between the objects in a scene.", "startOffset": 3, "endOffset": 21}, {"referenceID": 39, "context": "Examples include the works by Kulkarni et al. (2011) and Li et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 39, "context": "Examples include the works by Kulkarni et al. (2011) and Li et al. (2011), which both generate descriptions using n-gram language models trained on a subset of Wikipedia.", "startOffset": 30, "endOffset": 74}, {"referenceID": 15, "context": "The approach of Fang et al. (2015) is similar, but uses a maximum entropy language model instead of an n-gram model to generate descriptions.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "The approach of Fang et al. (2015) is similar, but uses a maximum entropy language model instead of an n-gram model to generate descriptions. This gives the authors more flexibility in handling the output of the word detectors that are at the core of their model. Recent image description work using recurrent neural networks (RNNs) can also be regarded as relying on language modeling. A classical RNN is a language model: it captures the probability of generating a given word in a string, given the words generated so far. In an image description setup, the RNN is trained to generate the next word given not only the string so far, but also a set of image features. In this setting, the RNN is therefore not purely a language model (as in the case of an n-gram model, for instance), but it is a hybrid model that relies on a representation that incorporates both visual and linguistic features. We will return to this in more detail in Section 2.3. A second set of approaches use sentence templates to generate descriptions. These are (typically manually) pre-defined sentence frames in which open slots need to be filled with labels for objects, relations, or attributes. For instance, Yang et al. (2011) fill in a sentence template by selecting the likely objects, verbs, prepositions, and scene types based on a Hidden Markov Model.", "startOffset": 16, "endOffset": 1210}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates.", "startOffset": 24, "endOffset": 50}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar.", "startOffset": 24, "endOffset": 506}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions.", "startOffset": 24, "endOffset": 689}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions. Another linguistically expressive model has recently been proposed by Ortiz et al. (2015). The authors model image description as machine translation over VDR\u2013sentence pairs and perform explicit content selection and surface realization using an integer linear program over linguistic constraints.", "startOffset": 24, "endOffset": 934}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions. Another linguistically expressive model has recently been proposed by Ortiz et al. (2015). The authors model image description as machine translation over VDR\u2013sentence pairs and perform explicit content selection and surface realization using an integer linear program over linguistic constraints. The systems presented so far aimed at directly generating novel descriptions. However, as argued by Hodosh et al. (2013), framing image description as a natural language generation (NLG) task makes it difficult to objectively evaluate the quality of novel descriptions", "startOffset": 24, "endOffset": 1263}, {"referenceID": 66, "context": "Figure 2: The description model based on retrieval from visual space proposed by Ordonez et al. (2011).", "startOffset": 81, "endOffset": 103}, {"referenceID": 32, "context": "as it \u201cintroduces a number of linguistic difficulties that detract attention from the underlying image understanding problem\u201d (Hodosh et al., 2013).", "startOffset": 126, "endOffset": 147}, {"referenceID": 66, "context": "One of the first model to follow this approach was the Im2Text model of Ordonez et al. (2011). GIST (Oliva & Torralba, 2001) and Tiny Image (Torralba, Fergus, & Freeman, 2008)", "startOffset": 72, "endOffset": 94}, {"referenceID": 54, "context": "To retrieve visually similar images, the authors employ simple RGB and HSV color histograms, Gabor and Haar descriptors, GIST and SIFT (Lowe, 2004) descriptors as image features.", "startOffset": 135, "endOffset": 147}, {"referenceID": 43, "context": "The model proposed by Kuznetsova et al. (2012) first runs the detectors and the classifiers used in the re-ranking step of the Im2Text model on a query image to extract and represent its semantic content.", "startOffset": 22, "endOffset": 47}, {"referenceID": 29, "context": "The method of Gupta et al. (2012) is another phrase-based approach.", "startOffset": 14, "endOffset": 34}, {"referenceID": 29, "context": "The method of Gupta et al. (2012) is another phrase-based approach. To retrieve visually similar images, the authors employ simple RGB and HSV color histograms, Gabor and Haar descriptors, GIST and SIFT (Lowe, 2004) descriptors as image features. Then, instead of using visual object detectors or scene classifiers, they rely only on the textual information in the descriptions of the visually similar images to extract the visual content of the input image. Specifically, the candidate descriptions are segmented into phrases of a certain type such as (subject, verb), (subject, prep, object), (verb, prep, object), (attribute, object), etc. Those that best describe the input image are determined according to a joint probability model based on image similarity and Google search counts, and the image is represented by triplets of the form {((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}. In the end, the description is generated using the three top-scoring triplets based on a fixed template. To increase the quality of the descriptions, the authors also apply syntactic aggregation and some subject and predicate grouping rules before the generation step. Patterson et al. (2014) were the first to present a large-scale scene attribute dataset in the computer vision community.", "startOffset": 14, "endOffset": 1232}, {"referenceID": 60, "context": "Mason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step.", "startOffset": 0, "endOffset": 28}, {"referenceID": 60, "context": "Mason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step. In particular, the authors represented images by using the scene attributes descriptor of Patterson et al. (2014). Once the visually similar images are identified from the training set, in the next step, the conditional probabilities of observing a word in the description of the query image are estimated via non-parametric density estimation using the descriptions of the retrieved images.", "startOffset": 0, "endOffset": 411}, {"referenceID": 60, "context": "Mason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step. In particular, the authors represented images by using the scene attributes descriptor of Patterson et al. (2014). Once the visually similar images are identified from the training set, in the next step, the conditional probabilities of observing a word in the description of the query image are estimated via non-parametric density estimation using the descriptions of the retrieved images. The final output description is then determined by using two different extractive summarization techniques, one depending on the SumBasic model (Nenkova & Vanderwende, 2005) and the other based on Kullback-Leibler divergence between the word distributions of the query and the candidate descriptions. Yagcioglu et al. (2015) proposed an average query expansion approach which is based on compositional distributed semantics.", "startOffset": 0, "endOffset": 1014}, {"referenceID": 8, "context": "The approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image.", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "The approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image. It then selects a description from the candidate descriptions associated with the retrieved images that best describes the images that are similar to the query image, just like the approaches by Mason and Charniak (2014) and Yagcioglu et al.", "startOffset": 16, "endOffset": 453}, {"referenceID": 8, "context": "The approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image. It then selects a description from the candidate descriptions associated with the retrieved images that best describes the images that are similar to the query image, just like the approaches by Mason and Charniak (2014) and Yagcioglu et al. (2015). Their approach differs in terms of how they represent the similarity between description and how they select the best candidate over the whole set.", "startOffset": 16, "endOffset": 481}, {"referenceID": 32, "context": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).", "startOffset": 120, "endOffset": 185}, {"referenceID": 85, "context": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).", "startOffset": 120, "endOffset": 185}, {"referenceID": 39, "context": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).", "startOffset": 120, "endOffset": 185}, {"referenceID": 32, "context": "Figure 3: Image descriptions as a retrieval task as proposed in Hodosh et al. (2013); Socher et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 32, "context": "Figure 3: Image descriptions as a retrieval task as proposed in Hodosh et al. (2013); Socher et al. (2014); Karpathy et al.", "startOffset": 64, "endOffset": 107}, {"referenceID": 32, "context": "Figure 3: Image descriptions as a retrieval task as proposed in Hodosh et al. (2013); Socher et al. (2014); Karpathy et al. (2014). (Image Source http://nlp.", "startOffset": 64, "endOffset": 131}, {"referenceID": 18, "context": "In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009.", "startOffset": 73, "endOffset": 95}, {"referenceID": 33, "context": "(2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space.", "startOffset": 77, "endOffset": 94}, {"referenceID": 31, "context": "In this section, we first discuss the seminal paper of Hodosh et al. (2013) on description retrieval, and then present more recent approaches that combine a retrieval approach with some form of natural language generation.", "startOffset": 55, "endOffset": 76}, {"referenceID": 31, "context": "In this section, we first discuss the seminal paper of Hodosh et al. (2013) on description retrieval, and then present more recent approaches that combine a retrieval approach with some form of natural language generation. Hodosh et al. (2013) map both images and sentences into a common space.", "startOffset": 55, "endOffset": 244}, {"referenceID": 18, "context": "In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009. The representation was thus limited to a set of pre-defined discrete slot fillers, which was given as training information. Instead, Hodosh et al. (2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space.", "startOffset": 74, "endOffset": 325}, {"referenceID": 18, "context": "In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009. The representation was thus limited to a set of pre-defined discrete slot fillers, which was given as training information. Instead, Hodosh et al. (2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space. CCA takes a training dataset of image-sentence pairs, i.e., Dtrain = {\u3008i, s\u3009}, thus input from two different feature spaces, and finds linear projections into a newly induced common space. In KCCA, kernel functions map the original items into higher-order space in order to capture the patterns needed to associate image and text. KCCA has been shown previously to be successful in associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) or image regions (Socher & Fei-Fei, 2010) with individual words or set of tags. Hodosh et al. (2013) compare their KCCA approach to a nearest-neighbor (NN) baseline that uses unimodal text and image spaces, without constructing a joint space.", "startOffset": 74, "endOffset": 982}, {"referenceID": 85, "context": "Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014).", "startOffset": 157, "endOffset": 178}, {"referenceID": 32, "context": "Description generation systems are difficult to evaluate, therefore the studies reviewed above treat the problem as a retrieval and ranking task (Hodosh et al., 2013; Socher et al., 2014).", "startOffset": 145, "endOffset": 187}, {"referenceID": 85, "context": "Description generation systems are difficult to evaluate, therefore the studies reviewed above treat the problem as a retrieval and ranking task (Hodosh et al., 2013; Socher et al., 2014).", "startOffset": 145, "endOffset": 187}, {"referenceID": 9, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 41, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 48, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 90, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 91, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 76, "context": "Socher et al. (2014) use neural networks for building sentence and image vector representations that are then mapped into a common embedding space.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al.", "startOffset": 86, "endOffset": 116}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al. (2014) extend the previous multi-modal embeddings model.", "startOffset": 86, "endOffset": 140}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al. (2014) extend the previous multi-modal embeddings model. Rather than directly mapping entire images and sentences into a common embedding space, their model embeds more fine-grained units, i.e., fragments of images (objects) and sentences (dependency tree fragments), into a common space. Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014). A similar approach is pursued by Pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping between image representations and sentences.", "startOffset": 86, "endOffset": 658}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al. (2014) extend the previous multi-modal embeddings model. Rather than directly mapping entire images and sentences into a common embedding space, their model embeds more fine-grained units, i.e., fragments of images (objects) and sentences (dependency tree fragments), into a common space. Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014). A similar approach is pursued by Pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping between image representations and sentences. A constrained language model is then used to generate from this representation. A conceptually related approach is pursued by Ushiku et al. (2015): the authors use a common subspace model which maps all feature vectors associated with the same phrase into nearby regions of the space.", "startOffset": 86, "endOffset": 915}, {"referenceID": 9, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015). Kiros et al. (2015) introduced a general encoder-decoder framework for image description ranking and generation, illustrated in Figure 4.", "startOffset": 195, "endOffset": 363}, {"referenceID": 41, "context": "Figure 4: The encoder-decoder model proposed by Kiros et al. (2015).", "startOffset": 48, "endOffset": 68}, {"referenceID": 85, "context": "(2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 37, "context": "In Kiros et al.\u2019s (2015) encoder-decoder model, the vision space is projected into the embedding space of the LSTM hidden states; a pairwise ranking loss is minimized to learn the ranking of images and their descriptions.", "startOffset": 3, "endOffset": 25}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture.", "startOffset": 106, "endOffset": 128}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM.", "startOffset": 106, "endOffset": 490}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al.", "startOffset": 106, "endOffset": 594}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al.", "startOffset": 106, "endOffset": 683}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval.", "startOffset": 106, "endOffset": 731}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al.", "startOffset": 106, "endOffset": 850}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al.", "startOffset": 106, "endOffset": 876}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets.", "startOffset": 106, "endOffset": 941}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.", "startOffset": 106, "endOffset": 1042}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.\u2019s (2015a) model for the learning of novel visual concepts.", "startOffset": 106, "endOffset": 1099}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.\u2019s (2015a) model for the learning of novel visual concepts. Karpathy and Fei-Fei (2015) improve on previous models by proposing a deep visualsemantic alignment model with a simpler architecture and objective function.", "startOffset": 106, "endOffset": 1176}, {"referenceID": 5, "context": "Another model that can generate novel sentences is proposed in Chen and Zitnick (2015). In contrast to the previous work, their model dynamically builds a visual representation of the scene as a description is being generated.", "startOffset": 63, "endOffset": 87}, {"referenceID": 88, "context": "The model of Xu et al. (2015) is closely related in that it also uses an RNN-based architecture in which the visual representations are dynamically updated.", "startOffset": 13, "endOffset": 30}, {"referenceID": 88, "context": "The model of Xu et al. (2015) is closely related in that it also uses an RNN-based architecture in which the visual representations are dynamically updated. Xu et al.\u2019s (2015) model incorporates an attentional component, which gives it a way of determining which regions in an image are salient, and it can focus its description on those regions.", "startOffset": 13, "endOffset": 176}, {"referenceID": 47, "context": "The general RNN-based ranking and generation approach is also followed by Lebret et al. (2015). Here, the main innovation is on the linguistic side: they employ a bilinear model to learn a common space of image features and syntactic phrases (noun phrases, verb phrases, and prepositional phrases).", "startOffset": 74, "endOffset": 95}, {"referenceID": 47, "context": "The general RNN-based ranking and generation approach is also followed by Lebret et al. (2015). Here, the main innovation is on the linguistic side: they employ a bilinear model to learn a common space of image features and syntactic phrases (noun phrases, verb phrases, and prepositional phrases). A Markov model is then utilized to generate sentences from these phrase embedding. On the visual side, standard CNN-based features are used. This results in an elegant modeling framework, whose performance is broadly comparable to the state of the art. Finally, two important directions that are less explored are: portability and weakly supervised learning. Verma and Jawahar (2014) evaluate the portability of a bi-directional model based on topic models, showing that performance significantly degrades.", "startOffset": 74, "endOffset": 683}, {"referenceID": 26, "context": "Gong et al. (2014) propose an approach based on weak supervision that transfers knowledge from millions of weakly annotated images to improve the accuracy of description retrieval.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "The training set also needs to be diverse (in addition to being large), in order for visual retrieval-based approaches to produce image descriptions that are adequate for novel test images (Devlin et al., 2015).", "startOffset": 189, "endOffset": 210}, {"referenceID": 74, "context": "The Pascal1K sentence dataset (Rashtchian et al., 2010) is a dataset which is commonly used as a benchmark for evaluating the quality of description generation systems.", "startOffset": 30, "endOffset": 55}, {"referenceID": 44, "context": "Kuznetsova et al. (2014) ran a human judgments study on 1,000 images from this dataset.", "startOffset": 0, "endOffset": 25}, {"referenceID": 74, "context": "Pascal1K (Rashtchian et al., 2010) 1,000 5 No Partial VLT2K (Elliott & Keller, 2013) 2,424 3 Partial Partial Flickr8K (Hodosh & Hockenmaier, 2013) 8,108 5 Yes No Flickr30K (Young et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 97, "context": ", 2010) 1,000 5 No Partial VLT2K (Elliott & Keller, 2013) 2,424 3 Partial Partial Flickr8K (Hodosh & Hockenmaier, 2013) 8,108 5 Yes No Flickr30K (Young et al., 2014) 31,783 5 No No Abstract Scenes (Zitnick & Parikh, 2013) 10,000 6 No Complete IAPR-TC12 (Grubinger et al.", "startOffset": 145, "endOffset": 165}, {"referenceID": 27, "context": ", 2014) 31,783 5 No No Abstract Scenes (Zitnick & Parikh, 2013) 10,000 6 No Complete IAPR-TC12 (Grubinger et al., 2006) 20,000 1\u20135 No Segmented MS COCO (Lin et al.", "startOffset": 95, "endOffset": 119}, {"referenceID": 53, "context": ", 2006) 20,000 1\u20135 No Segmented MS COCO (Lin et al., 2014) 164,062 5 Soon Partial", "startOffset": 40, "endOffset": 58}, {"referenceID": 66, "context": "BBC News (Feng & Lapata, 2008) 3,361 1 No No SBU1M Captions (Ordonez et al., 2011) 1,000,000 1 Possibly No D\u00e9j\u00e0-Image Captions (Chen et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 4, "context": ", 2011) 1,000,000 1 Possibly No D\u00e9j\u00e0-Image Captions (Chen et al., 2015) 4,000,000 Varies No No", "startOffset": 52, "endOffset": 71}, {"referenceID": 32, "context": "The Flickr8K dataset (Hodosh et al., 2013) and its extended version Flickr30K dataset (Young et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 97, "context": ", 2013) and its extended version Flickr30K dataset (Young et al., 2014) contain images from Flickr, comprising approximately 8,000 and 30,000 images, respectively.", "startOffset": 51, "endOffset": 71}, {"referenceID": 53, "context": "The MS COCO dataset (Lin et al., 2014) currently consists of 123,287 images with five different descriptions per image.", "startOffset": 20, "endOffset": 38}, {"referenceID": 0, "context": "Extensions of MS COCO are currently under development, including the addition of questions and answers (Antol et al., 2015).", "startOffset": 103, "endOffset": 123}, {"referenceID": 52, "context": "One paper (Lin et al., 2015) uses an the NYU dataset (Silberman, Kohli, Hoiem, & Fergus, 2012), which contains 1,449 indoor scenes with 3D object segmentation.", "startOffset": 10, "endOffset": 28}, {"referenceID": 26, "context": "The IAPR-TC12 dataset introduced by Grubinger et al. (2006) is one of the earliest multi-modal datasets and contains 20,000 images with descriptions.", "startOffset": 36, "endOffset": 60}, {"referenceID": 68, "context": "A caption provides personal, cultural, or historical context for the image (Panofsky, 1939).", "startOffset": 75, "endOffset": 91}, {"referenceID": 20, "context": "Feng and Lapata (2008) harvested 3,361 news articles from the British Broadcasting Corporation News website, with the constraint that the article includes an image and a caption.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Feng and Lapata (2008) harvested 3,361 news articles from the British Broadcasting Corporation News website, with the constraint that the article includes an image and a caption. The SBU1M Captions dataset introduced by Ordonez et al. (2011) differs from the previous datasets in that it is a web-scale dataset containing approximately one million captioned images.", "startOffset": 0, "endOffset": 242}, {"referenceID": 4, "context": "The D\u00e9j\u00e0-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images with 180,000 near-identical captions harvested from Flickr.", "startOffset": 32, "endOffset": 51}, {"referenceID": 4, "context": "For instance, the sentences the bird flies in blue sky and a bird flying into the blue sky were normalized to bird fly IN blue sky (Chen et al., 2015).", "startOffset": 131, "endOffset": 150}, {"referenceID": 14, "context": "The images for these datasets have either been sourced from an existing task in the computer vision community \u2013 the Pascal challenge (Everingham et al., 2010) was used to the Pascal1K and VLT2K datasets \u2013 directly from Flickr, in the case of Flickr8K/30K, MS COCO, SBU1M Captions, and D\u00e9j\u00e0-Image Captions datasets, or crowdsourced, in the case of the Abstract Scenes dataset.", "startOffset": 133, "endOffset": 158}, {"referenceID": 32, "context": "Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013).", "startOffset": 255, "endOffset": 276}, {"referenceID": 12, "context": "The images for these datasets have either been sourced from an existing task in the computer vision community \u2013 the Pascal challenge (Everingham et al., 2010) was used to the Pascal1K and VLT2K datasets \u2013 directly from Flickr, in the case of Flickr8K/30K, MS COCO, SBU1M Captions, and D\u00e9j\u00e0-Image Captions datasets, or crowdsourced, in the case of the Abstract Scenes dataset. The texts in image\u2013description datasets are usually crowd-sourced from Amazon Mechanical Turk or Crowdflower; whereas the texts in image\u2013caption datasets have been harvested from photo-sharing sites, such as Flickr, or from news providers. Captions are usually collected without financial incentive because they are written by the people sharing their own images, or by journalists. Crowd-sourcing the descriptions of images involves defining a simple task that can be performed by untrained workers. Examples of the task guidelines used by Hodosh et al. (2013) and Elliott and Keller (2013) are given in Figure 6.", "startOffset": 134, "endOffset": 938}, {"referenceID": 11, "context": "(2013) and Elliott and Keller (2013) are given in Figure 6.", "startOffset": 11, "endOffset": 37}, {"referenceID": 11, "context": "(2013) and Elliott and Keller (2013) are given in Figure 6. In both instances, care was taken to clearly inform the potential workers about the expectations for the task. In particular, explicit instructions were given on how the descriptions should be written, and examples of good texts were provided. In addition, Hodosh et al. provided more extensive examples to explain what would constitute unsatisfactory texts. Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013). The issue of remuneration for crowd-sourced workers is controversial, and higher payments do not always lead to better quality in a crowd-sourced environment (Mason &Watts, 2009). Rashtchian et al. (2010) paid $0.", "startOffset": 11, "endOffset": 902}, {"referenceID": 11, "context": "(2013) and Elliott and Keller (2013) are given in Figure 6. In both instances, care was taken to clearly inform the potential workers about the expectations for the task. In particular, explicit instructions were given on how the descriptions should be written, and examples of good texts were provided. In addition, Hodosh et al. provided more extensive examples to explain what would constitute unsatisfactory texts. Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013). The issue of remuneration for crowd-sourced workers is controversial, and higher payments do not always lead to better quality in a crowd-sourced environment (Mason &Watts, 2009). Rashtchian et al. (2010) paid $0.01/description, Elliott and Keller (2013) paid $0.", "startOffset": 11, "endOffset": 952}, {"referenceID": 32, "context": "Source Appendix of the work by Hodosh et al. (2013)", "startOffset": 31, "endOffset": 52}, {"referenceID": 43, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 49, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 63, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 44, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 32, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 63, "context": "\u2022 The description has no incorrect information (Mitchell et al., 2012).", "startOffset": 47, "endOffset": 70}, {"referenceID": 49, "context": "\u2022 The description is relevant for this image (Li et al., 2011; Yang et al., 2011).", "startOffset": 45, "endOffset": 81}, {"referenceID": 93, "context": "\u2022 The description is relevant for this image (Li et al., 2011; Yang et al., 2011).", "startOffset": 45, "endOffset": 81}, {"referenceID": 49, "context": "\u2022 The description is creatively constructed (Li et al., 2011).", "startOffset": 44, "endOffset": 61}, {"referenceID": 63, "context": "\u2022 The description is human-like (Mitchell et al., 2012).", "startOffset": 32, "endOffset": 55}, {"referenceID": 43, "context": "This approach to evaluation has been subject to much discussion and critique (Kulkarni et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014).", "startOffset": 77, "endOffset": 145}, {"referenceID": 32, "context": "This approach to evaluation has been subject to much discussion and critique (Kulkarni et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014).", "startOffset": 77, "endOffset": 145}, {"referenceID": 11, "context": "Elliott and Keller analyzed the correlation between human judgments and automatic evaluation measures for retrieved and system-generated image descriptions in the Flickr8K and VLT2K datasets. They showed that sentence-level unigram BLEU, which at that point in time was the de facto standard measure for image description evaluation, is only weakly correlated with human judgments. Meteor (Banerjee & Lavie, 2005), a less frequently used translation evaluation measure, exhibited the highest correlation with human judgments. However, Kuznetsova et al. (2014) found that unigram BLEU was more strongly correlated with human judgments than Meteor for image caption generation.", "startOffset": 0, "endOffset": 560}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al.", "startOffset": 33, "endOffset": 59}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al.", "startOffset": 33, "endOffset": 109}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al.", "startOffset": 33, "endOffset": 192}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al.", "startOffset": 33, "endOffset": 250}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al.", "startOffset": 33, "endOffset": 328}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al.", "startOffset": 33, "endOffset": 391}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al.", "startOffset": 33, "endOffset": 446}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al. (2014) MultRetrieval Pascal1K mRank, R@k Verma and Jawahar (2014) MultRetrieval IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k Yatskar et al.", "startOffset": 33, "endOffset": 491}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al. (2014) MultRetrieval Pascal1K mRank, R@k Verma and Jawahar (2014) MultRetrieval IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k Yatskar et al.", "startOffset": 33, "endOffset": 550}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al. (2014) MultRetrieval Pascal1K mRank, R@k Verma and Jawahar (2014) MultRetrieval IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k Yatskar et al. (2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al.", "startOffset": 33, "endOffset": 625}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al.", "startOffset": 39, "endOffset": 225}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al.", "startOffset": 39, "endOffset": 284}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al.", "startOffset": 39, "endOffset": 343}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al.", "startOffset": 39, "endOffset": 411}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al.", "startOffset": 39, "endOffset": 482}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al.", "startOffset": 39, "endOffset": 567}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al.", "startOffset": 39, "endOffset": 619}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al.", "startOffset": 39, "endOffset": 677}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al.", "startOffset": 39, "endOffset": 717}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al. (2015) Generation Abstract Scenes Human, BLEU, Meteor Pinheiro et al.", "startOffset": 39, "endOffset": 790}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al. (2015) Generation Abstract Scenes Human, BLEU, Meteor Pinheiro et al. (2015) MultRetrieval COCO BLEU Ushiku et al.", "startOffset": 39, "endOffset": 860}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al. (2015) Generation Abstract Scenes Human, BLEU, Meteor Pinheiro et al. (2015) MultRetrieval COCO BLEU Ushiku et al. (2015) Generation Pascal1K, IAPR, SBU1M, COCO BLEU", "startOffset": 39, "endOffset": 905}, {"referenceID": 88, "context": "The number of reference texts for each testing image was either five or 40, based on the insight that some measures may benefit from larger reference sets (Vedantam et al., 2015).", "startOffset": 155, "endOffset": 178}, {"referenceID": 32, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 85, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 26, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 39, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 18, "context": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013).", "startOffset": 70, "endOffset": 139}, {"referenceID": 43, "context": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013).", "startOffset": 70, "endOffset": 139}, {"referenceID": 18, "context": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, the introduction of Flickr30K, MS COCO and other large datasets has enabled the training of more complex models such as neural networks. Still, the area is likely to benefit from larger and diversified datasets that share a common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue that", "startOffset": 71, "endOffset": 440}, {"referenceID": 88, "context": "According to existing measures, including the latest CIDEr measure (Vedantam et al., 2015), several automatic methods outperform the human upper bound (this upper bound indicates how similar human descriptions are to each other).", "startOffset": 67, "endOffset": 90}, {"referenceID": 17, "context": "The counterintuitive nature of this result is confirmed by the fact that when human judgments are used for evaluation, the output of even the best system is judged as worse than a human generated description for most of the time (Fang et al., 2015).", "startOffset": 229, "endOffset": 248}, {"referenceID": 32, "context": "The human judgments were obtained from human experts (Hodosh et al., 2013).", "startOffset": 53, "endOffset": 74}, {"referenceID": 7, "context": "This situation has been demonstrated by Devlin et al. (2015), who show that their best model is able to generate only 47.", "startOffset": 40, "endOffset": 61}, {"referenceID": 5, "context": "Chen and Zitnick (2015) and related approaches take a step towards addressing such limitations by coupling description and visual representation generation.", "startOffset": 0, "endOffset": 24}, {"referenceID": 77, "context": "Following this effort, several datasets on this task are being released: DAQUAR (Malinowski & Fritz, 2014a) was compiled from scene depth images and mainly focuses on questions about the type, quantity and color of objects; COCOQA (Ren et al., 2015) was constructed by converting image descriptions to VQA format over a subset of images from the MS COCO dataset; the Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al.", "startOffset": 231, "endOffset": 249}, {"referenceID": 23, "context": ", 2015) was constructed by converting image descriptions to VQA format over a subset of images from the MS COCO dataset; the Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park, Berg, & Berg, 2015) and the VQA dataset (Antol et al.", "startOffset": 190, "endOffset": 208}, {"referenceID": 0, "context": ", 2015), Visual Madlibs dataset (Yu, Park, Berg, & Berg, 2015) and the VQA dataset (Antol et al., 2015), were again built for images from MS COCO, but this time question-answer pairs are collected via human annotators in a freestyle paradigm.", "startOffset": 83, "endOffset": 103}, {"referenceID": 54, "context": "Towards achieving this goal, Malinowski and Fritz (2014a) propose a Bayesian framework that connects natural language questionanswering with the visual information extracted from image parts.", "startOffset": 29, "endOffset": 58}, {"referenceID": 27, "context": "Currently, among the available benchmark datasets, only the IAPR-TC12 dataset (Grubinger et al., 2006) has multilingual descriptions (in English and German).", "startOffset": 78, "endOffset": 102}], "year": 2016, "abstractText": "Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}