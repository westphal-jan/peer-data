{"id": "1601.07124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization", "abstract": "This work aims to introduce a new algorithm for the automatic summary of speech to text based on statistical deviations of probabilities and graphs. Input is a text from voice conversations with noise, and output is a compact text summary. Our results on the CCCS Multiling 2015 French corpus pilot task are very encouraging.", "histories": [["v1", "Tue, 26 Jan 2016 18:19:00 GMT  (47kb,D)", "http://arxiv.org/abs/1601.07124v1", "7 pages, 2 figures, CCCS Multiling 2015 Workshop"]], "COMMENTS": "7 pages, 2 figures, CCCS Multiling 2015 Workshop", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["elvys linhares pontes", "juan-manuel torres-moreno", "r\\'ea carneiro linhares"], "accepted": false, "id": "1601.07124"}, "pdf": {"name": "1601.07124.pdf", "metadata": {"source": "CRF", "title": "LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization", "authors": ["Elvys Linhares Pontes", "Juan-Manuel Torres-Moreno", "Andr\u00e9a Carneiro Linhares"], "emails": ["@gmail.com", "@univ-avignon.fr", "andrea.linhares@ufc.br"], "sections": [{"heading": null, "text": "Keywords: Automatic Text Summarization, Jensen-Shannon\u2019s divergence of probabilities, Speech-to-text summarization, Graph model."}, {"heading": "1 Introduction", "text": "Nowadays, a lot of information is daily generated. It is necessary to have available memory storage because each datum must be processed and the information contained therein analyzed. The manual analysis is impossible because it is necessary a huge number of persons to analyze this information in an available time. The summary is a short text with main ideas of original text (Torres-Moreno, 2014) and reduces the read time to analyze these data.\nAudio is widely used in daily life on the radio and on the internet, in news, interviews and conversations. A Call Centre Conversation creates a lot of conversations every day. These centers has issues and tasks. It is essential the control of the discussed topics and the results obtained by customers in these calls. One way to analyze and accelerate the data processing is speech summarization, that is different from traditional text summarization because there are other problems in these texts as speech errors, sentences of different sizes and colloquialisms.\n\u201cMultiling is a community-driven initiative for benchmarking multilingual summarization systems, nurturing further research, and pushing the state-of-the-art in the area\u201d 1. The MultiLing 2015 initiative features the following tasks: Multilingual Multi-document Summarization, Multilingual Single-document Summarization, Online Forum Summarization and Call Centre Conversation Summarization (CCCS). The CCCS pilot task consists in \u201ccreating systems that can analyze call centres conversations and generate written summaries reflecting why the customer is calling, how the agent answers that query, what are the steps to solve the problem and what is the resolution status of the problem\u201d (Favre et al., 2015).\nWe developed the LIA-RAG summarization system based on the RAG system (Pontes et al., 2015), coupled with some post-processing rules in order to generate a final summary. LIA-RAG uses a graph model to analyze and verify a set of documents (e.g., the conversation transcription) for MultiLing\u201915 CCCS pilot task. LIA-RAG creates a summary computing the relevance of the words and the similarity among the sentences. The system uses a simple post-processing to improve the quality of the final summary.\nThe rest of the paper is organized as follows: section 2 describes related work on automatic summarization of texts and conversations. Sections 3 and 4 analyze the graph model and the system used in this work. Section 5 describes the results obtained for Multiling/DECODA French corpus and section 6 concludes this work.\n1http://multiling.iit. demokritos.gr/pages/view/1517/ multiling-2015-call-for-participation\nar X\niv :1\n60 1.\n07 12\n4v 1\n[ cs\n.C L\n] 2\n6 Ja\nn 20\n16"}, {"heading": "2 Related Works", "text": "Automatic Text Summarization (ATS) aims to creates a summary containing the main ideas of a textual document (Mani and Mayburi, 1999; Mani, 2001; Torres-Moreno, 2014). The summary can be an extraction or abstraction of a single document or multi-document. The extraction process identifies the most informative sentences of a document and creates a summary by assembling of these sentences (Luhn, 1958; Torres-Moreno, 2014). Extraction may be guided (by a query). In this case, the algorithm selects the most relevant information follow a particular topic. The abstraction algorithms create new (or reformulate) sentences from original texts (Seno, 2010; Seno and Nunes, 2008) and the extraction methods use the key sentences of texts (Barzilay and McKeown, 2005; Torres-Moreno, 2014).\nWorks about abstraction usually uses syntactic and semantic knowledge of a language to create the summary. This procedure verifies the best construction of a sentence (Barzilay et al., 1999). This type of summarization uses fusion to help the review of information. (Seno, 2010) proposed a method to fusion similar sentences in Brazilian Portuguese based on a symbolic and domain-independent approach. This method allows the fusion by union and by intersection of a document cluster. Fusion by union preserves the overall message of the cluster while fusion by intersection analyses the redundant information considered most important in the cluster. (Seno and Nunes, 2008) described how to identify common information between sentences in Brazilian Portuguese using lexical knowledge, syntactic and semantic rules of paraphrasing.\n(Jorge et al., 2010) developed a summarizer system based on the CST model (Cross-document Structure Theory). The system proposed analyses redundancy and contradiction among different information sources in Brazilian Portuguese.\n(Barzilay et al., 1999) developed a method to generate automatic summaries by identifying and synthesizing similar elements in a cluster of documents. This method creates the summary based on similarity between the sentences and topic. (Barzilay and McKeown, 2005) described an approach to fusion sentences through the text-to-text technique, to synthesize repeated information from multiple documents. This method uses a syntactic alignment in sentences\nto identify common information. After the identification step, sentences are processed and a new text is generated with the same content.\nA way to calculate the similarity between sentences is to use co-occurrence of words. (He et al., 2008) proposed a fusion method using similarity metrics, co-occurrence skip-bigram and information density to evaluate sentences and to select the most relevant ones. (Hennig and Albayrak, 2010) developed a multi-document model to summarize by analyzing the co-occurrence of sentence-term and sentence-bigram using the Jensen-Shannon (JS) divergence.\nAnother method to obtain relevant sentences uses compression, as reported in (Pitler, 2010). Pitler uses approaches based on syntactic trees, sentences and discourse. (Filippova, 2010) describes a multi-sentence compression method using a word-based graph.\nThe summarization by extraction does not have the same quality as the summaries produced by abstraction because it uses surface methods based on statistical calculations to verify the sentence relevance. However, the extraction is general and do not require deep analysis of the language (Barzilay and McKeown, 2005; Pontes et al., 2014).\n(Pontes et al., 2014) use Graph theory concomitant with JS divergence to create multi-document summaries by extraction. Their system describes a text model as a graph where the sentences are represented by vertices and the edges connect two similar sentences. Their approach calculates the stable set of the graph aiming creating the summary containing sentences with general information of the cluster and without redundancy. (Linhares et al., 2013) model the text as graph model and use a heuristic (greedy algorithm) to obtain the relevant sentences in the text.\nThe speech summarization task is more complex and it involves other problems. It is more difficult to identify utterance boundaries because it may be fragmented, contain disfluencies and also because speech recognition introduces errors. Meetings involve multi-party conversation with overlapping speakers. The language used is informal and utterances tend to be partial, fragmentary, ungrammatical and include many ellipses and pronouns. However, the speech\nsignal may provides additional information that emphasizes a piece of text as prosody (Murray et al., 2005).\n(Mckeown et al., 2005) described some ways to use a text summarization as a speech summarization. They described some work about summarization of broadcast news and meetings. (Murray et al., 2005) analyzed extractive summarization of multiparty meetings. They described Maximal Marginal Relevance and Latent Semantic Analysis to create the summary based on prosodic and lexical features."}, {"heading": "3 Modeling the problem", "text": "This paper aims to design a system to summarize several documents by extraction its most important sentences. Statistical techniques were used to build a language independent system. The proposed methods are based on a specific preprocessing of words, a weighting function of sentences and a bag-of-words model to represent the text content.\nThis model uses K matrices represented by SK[m\u00d7n] and constructed fromK documents, where ma is the number of sentences and na is the number of distinct words in the document a (a \u2208 K). The cell saij of the matrix represents the frequency of word j in the sentence i (FPij) of the document a. This stage was constructed using the libraries and algorithms from Cortex summarization system (Torres-Moreno et al., 2002; Torres-Moreno et al., 2001).\nSa =  sa11 s a 12 . . . s a 1n sa21 s a 22 . . . s a 2n ... ...\n... sam1 s a m2 . . . s a mn  , a \u2208 K saij = { FPij , if \u2203 word j in sentence i 0, otherwise (1)"}, {"heading": "3.1 Jensen-Shannon divergence", "text": "We use Jensen-Shannon (JS) divergence to measure the similarity between sentences. Let w be a words\u2019 set in P and Q. P and Q represent the probability distribution between two objects: two individuals sentences or a sentence and a set of sentences. The divergence will then calculated among these two objects. The JS divergence is symmetric and provides a stable way to measure the difference between two distributions (equation 2).\nDJS(P ||Q) = 1\n2 \u2211 w\u2208W\n[ Pw log ( 2\u00d7 Pw Pw +Qw )\n+ Qw log ( 2\u00d7Qw Pw +Qw )] (2)\nThe JS divergence value ranges from [0,\u221e+). It is closer to zero when the distributions are similar and they differ in another case.\nIn the case there is a word in a sentence that is missing in another one, a smooth (different weighting) will be used to avoid null values and have a smoother distribution (Hiemstra, 2009). If a word w is not present in the sentence Q, then the smooth is calculated by the equation 3, where \u03b2 = 1.5 \u00d7 voc, which voc is the number of distinct words in R, \u03b3 is the variable that controls the relevance of the missing word in the sentence and N is the number of words in R (Louis and Nenkova, 2013).\nQw =\n( Pw + \u03b3\nN + \u03b3 \u00d7 \u03b2\n) (3)"}, {"heading": "3.2 Term Frequency-Inverse Sentence Frequency (TF-ISF)", "text": "One way to verify the initial relevance of a word and a sentence to the text is through the TF-ISF. This metric is based on term frequency in the text and it is calculated by the equation 4.\ntf isf(w) = tf(w)\u00d7 log ( n\nnw\n) (4)\nwhere tf(w) is frequency of term w, n is total number of documents and nw is number of documents that contain the term w."}, {"heading": "4 The LIA-RAG system", "text": "In general lines, a text consists of several sentences with different topics. The text can be divided into several groups and each of them describes one step/idea in the text. If a group is large, then it is relevant to the text. It is possible to choose the sentences of the largest group and obtains the most relevant content.\nThe main ideas of a text are generally analyzed and discussed several times. The vertices with higher degree have more similar sentences and then, are important to the text. However, it is not\nnecessary to have a lot of similar sentences to be a relevant one.\nRe\u0301sumeur Audio-texte a\u0300 base de Graphes (RAG) is a summarizer system by sentence extraction, which selects the main sentences of a text and uses a post-processing to remove some errors and make the text more concise and compact."}, {"heading": "4.1 The RAG algorithm", "text": "RAG uses Graph theory and divergence metrics to calculate the similarity and to group the sentences. Initially, the system performs a filtering process to remove the brackets. Then, it performs a segmentation, filtering and stemming processes to remove stopwords and reduce the words to their roots. RAG accomplished this preprocessing and matrix transformation based on (Torres-Moreno et al., 2001). It calculates the relevance of each sentence based on TF-ISF metric (equation 4) and removes the less relevant sentences.\nThe system creates a graphGwhich each vertex represents a sentence previously selected. The text is analyzed and modeled as a sentence graph (vertices). Based on equation 4, it calculates the similarity between sentences. If the similarity between two sentences is less than 0.16 (threshold obtained by empirical testing), then the system creates an edge between them. So, the vertices with higher degrees have the most relevant content of the text. However, some sentences may have a small degree, but they may contain important information.\nRAG combines the TF-ISF and degree sentences to analyze the relevance of them. The relevance of the sentence i is defined by:\nrel(i) = degree(i)\u00d7 tf isf(i) (5)\nwhere degree(i) is the degree of vertex i and rel(i) is the relevance of the sentence i. After, the system creates a summary with the higher score sentences, excluding similar (or redundant) sentences based on Dice\u2019s coefficient (Bai et al., 2012).\nThe figure 1 describes the RAG system."}, {"heading": "4.2 LIA-RAG: RAG with a specific speech post-processing", "text": "The speech recognition process produces a text that contains several grammatical problems (slang, colloquialisms, expressions and speech\nrecognition errors). An extraction summary algorithm selects the relevant sentences, however the sentences may have some grammatical problems. So, it is necessary to perform a treatment of this summary.\nThe main analyzed aspects in this process are:\n\u2022 Colloquialisms,\n\u2022 Speech expressions and\n\u2022 Dates.\nLIA-RAG system receives the summary as an input. In this input, some speech expressions are used to connect ideas or concepts in oral conversations. LIA-RAG removes these expressions, because often they are incorrectly transcripted (a noise source). Also, the system eliminates several colloquialisms and the duplicated words. The system replaces some mistaken words by its correct form. The figure 2 shows the architecture of the LIA-RAG system."}, {"heading": "5 Results", "text": "The tests were carried on a computer with i5@2.6 GHz processor and 4 GB of RAM on GNU/Linux\nDebian 64-bit operating system. The algorithms of RAG were implemented using the Perl language.\nWe used the French DECODA corpus (Bechet et al., 2012). The systems have to generate textual summaries with the main idea of each conversation belonging to the corpus. \u201cThe conversations topics range from itinerary and schedule requests, to lost and found, to complaints (the calls were recorded during strikes)\u201d (Favre et al., 2015). Each summary has 7% of the number of words of each conversation transcription. We compared LIA-RAG and RAG systems with two baseline systems (random and first lead base).\nIn order to evaluate the quality of the summaries, Multiling CCCS used the system Recall-Oriented Understudy for Gisting Evaluation (ROUGE)2, which determines the quality of an automatic summary based on the intersection of the n-grams of a candidate summary and the n-grams of a set of reference summaries. More specifically, we used ROUGE-N and ROUGE-SU measures. ROUGE-N, N \u2208 [1, 2]. ROUGE is an n-gram recall measure (Lin, 2004)3. The values of these metrics belongs to [0, 1], 1 for the best result.\nThe table 1 shows the results obtained using the systems over the training corpus. This corpus contains 50 conversations transcription with 23,363 words and 115 summaries. Both versions of RAG provided the best results. The RAG system identified the main sentences discussed in conversations. However, the errors and speech expressions decreased the informativeness. The post-processing of LIA-RAG allowed to improve the results. This process reduces errors and generates a more informative and concise summary.\nThe French test corpus has 100 conversations\n2The options for running ROUGE 1.5.5 are -a -l 10000 -n 4 -x -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0\n3http://www.berouge.com/Pages/default. aspx\ntranscription with 42,130 words and 212 summaries. The ROUGE-2 official performance for the systems participating to CCCS pilot task is showed in table 2 (Favre et al., 2015). The LIA-RAG system obtained the best results."}, {"heading": "6 Conclusion and perspectives", "text": "Divergence of probabilities in a graph model to extract key sentences in French speech-to-text summarization was very interesting. LIA-RAG system uses very few language resources (stopwords and stemming) and has achieved good results. Nevertheless, the system is easily adaptable to other languages with only some modifications in the preprocessing stage.\nAn interesting perspective of this work consists in the utilization of the speech TAGs markers to improve the computation of the sentences score. In addition, it is necessary to improve the post-processing in order to increase the quality of the final summary. Finally, the verification of the grammaticality and readability of the extracted key sentences can help to produce more realistic abstracts."}, {"heading": "Acknowledgments", "text": "This project was partially founded by a scholarship from FUNCAP-CE (Brazil)."}], "references": [{"title": "and Jason S", "author": ["Ming-Hong Bai", "Yu-Ming Hsieh", "Keh-Jiann Chen"], "venue": "Chang.", "citeRegEx": "Bai et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Sentence fusion for multidocument news summarization", "author": ["Barzilay", "McKeown2005] Regina Barzilay", "Kathleen R. McKeown"], "venue": "Comput. Linguist.,", "citeRegEx": "Barzilay et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2005}, {"title": "McKeown", "author": ["Regina Barzilay", "Kathleen R"], "venue": "and Michael Elhadad.", "citeRegEx": "Barzilay et al.1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Renato De Mori", "author": ["Frederic Bechet", "Benjamin Maza", "Nicolas Bigouroux", "Thierry Bazillon", "Marc El-Beze"], "venue": "and Eric Arbillot.", "citeRegEx": "Bechet et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "2015", "author": ["Benoit Favre", "Evgeny Stepanov", "J\u00e9r\u00e9my Trione", "Fr\u00e9d\u00e9ric B\u00e9chet", "Giuseppe Riccardi"], "venue": "Call centre conversation summarization: A pilot task at multiling", "citeRegEx": "Favre et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-sentence compression: Finding shortest paths in word graphs", "author": ["Katja Filippova"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Filippova.,? \\Q2010\\E", "shortCiteRegEx": "Filippova.", "year": 2010}, {"title": "Jinguang Chen", "author": ["Tingting He", "Fang Li", "Wei Shao"], "venue": "and Liang Ma.", "citeRegEx": "He et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Personalized multi-document summarization using n-gram topic model fusion", "author": ["Hennig", "Albayrak2010] L. Hennig", "S. Albayrak"], "venue": "In Proceedings of LREC\u201910,", "citeRegEx": "Hennig et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hennig et al\\.", "year": 2010}, {"title": "Probability smoothing", "author": ["D. Hiemstra"], "venue": "In Encyclopedia of Database Systems,", "citeRegEx": "Hiemstra.,? \\Q2009\\E", "shortCiteRegEx": "Hiemstra.", "year": 2009}, {"title": "Maria Luc\u0131\u0301a del Rosario", "author": ["Castro Jorge"], "venue": "and Thiago Alexandre Salgueiro Pardo.", "citeRegEx": "Jorge et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Proc. ACL workshop on Text Summarization Branches Out,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Juan-Manuel Torres-Moreno", "author": ["Andr\u00e9a Carneiro Linhares"], "venue": "and Javier Ramirez.", "citeRegEx": "Linhares et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatically assessing machine summary content without a gold standard", "author": ["Louis", "Nenkova2013] Annie Louis", "Ani Nenkova"], "venue": "Computational Linguistics,", "citeRegEx": "Louis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2013}, {"title": "The automatic creation of literature abstracts", "author": ["H.P. Luhn"], "venue": "IBM J. Res. Dev.,", "citeRegEx": "Luhn.,? \\Q1958\\E", "shortCiteRegEx": "Luhn.", "year": 1958}, {"title": "2005", "author": ["Kathleen Mckeown", "Julia Hirschberg", "Michel Galley", "Sameer Maskey"], "venue": "From text to speech summarization. In ICASSP.", "citeRegEx": "Mckeown et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Steve Renals", "author": ["Gabriel Murray"], "venue": "and Jean Carletta.", "citeRegEx": "Murray et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Methods for sentence compression", "author": ["E. Pitler"], "venue": "Technical report,", "citeRegEx": "Pitler.,? \\Q2010\\E", "shortCiteRegEx": "Pitler.", "year": 2010}, {"title": "Andr\u00e9a Carneiro Linhares", "author": ["Elvys Linhares Pontes"], "venue": "and Juan-Manuel Torres-Moreno.", "citeRegEx": "Pontes et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Juan-Manuel Torres-Moreno", "author": ["Elvys Linhares Pontes"], "venue": "and Andr\u00e9a Carneiro Linhares.", "citeRegEx": "Pontes et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Some experiments on clustering similar sentences", "author": ["Seno", "Mariadas Gra\u00e7as Volpe Nunes"], "venue": null, "citeRegEx": "Seno et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Seno et al\\.", "year": 2008}, {"title": "Um m\u00e9todo para a fus\u00e3o autom\u00e1tica de senten\u00e7as similares em portugu\u00eas", "author": ["Eloize Rossi Marques Seno"], "venue": "Ph.D. thesis, Instituto de Cie\u0302ncias Matema\u0301ticas e de Computac\u0327a\u0303o,", "citeRegEx": "Seno.,? \\Q2010\\E", "shortCiteRegEx": "Seno.", "year": 2010}, {"title": "Patricia Vel\u00e1zquez-Morales", "author": ["Juan-Manuel Torres-Moreno"], "venue": "and Jean-Guy Meunier.", "citeRegEx": "Torres.Moreno et al.2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Patricia Vel\u00e1zquez-Morales", "author": ["Juan-Manuel Torres-Moreno"], "venue": "and Jean-Guy Meunier.", "citeRegEx": "Torres.Moreno et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic Text Summarization", "author": ["Juan-Manuel Torres-Moreno"], "venue": null, "citeRegEx": "Torres.Moreno.,? \\Q2014\\E", "shortCiteRegEx": "Torres.Moreno.", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "This paper aims to introduces a new algorithm for automatic speech-to-text summarization based on statistical divergences of probabilities and graphs. The input is a text from speech conversations with noise, and the output a compact text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus are very encouraging.", "creator": "LaTeX with hyperref package"}}}