{"id": "1601.07969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2016", "title": "Zipf's law is a consequence of coherent language production", "abstract": "The task of text segmentation or \"chunking\" can occur on many levels in text analysis, depending on whether it is most advantageous to break it down by paragraphs of a book, sentences of a paragraph, etc. In the past, we have focused on a fine-grained segmentation task, which we call text partitioning, in which each space assumes a binary state of fixed (word binding) or broken (word hyphenation) texts with some probability. In this work, we have closely examined perhaps the most naive version of this process: random or, uniform stochastic partitioning, in which all word-word gaps have a uniform (word binding) or broken (word hyphenation) probability.", "histories": [["v1", "Fri, 29 Jan 2016 02:39:56 GMT  (1437kb,D)", "http://arxiv.org/abs/1601.07969v1", "4 pages, 3 figures"], ["v2", "Fri, 5 Aug 2016 22:13:18 GMT  (988kb,D)", "http://arxiv.org/abs/1601.07969v2", "5 pages, 4 figures"]], "COMMENTS": "4 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jake ryland williams", "james p bagrow", "rew j reagan", "sharon e alajajian", "christopher m danforth", "peter sheridan dodds"], "accepted": false, "id": "1601.07969"}, "pdf": {"name": "1601.07969.pdf", "metadata": {"source": "CRF", "title": "Selection models of language production support informed text partitioning: an intuitive and practical, bag-of-phrases framework for text analysis", "authors": ["Jake Ryland Williams", "James P. Bagrow", "Andrew J. Reagan", "Sharon E. Alajajian", "Christopher M. Danforth", "Peter Sheridan Dodds"], "emails": [], "sections": [{"heading": null, "text": "Selection models of language production support informed text partitioning: an intuitive and practical, bag-of-phrases framework for text analysis.\nJake Ryland Williams,1, \u2217 James P. Bagrow,2, \u2020 Andrew J. Reagan,2, \u2021 Sharon\nE. Alajajian,1, \u00a7 Christopher M. Danforth,2, \u00b6 and Peter Sheridan Dodds2, \u2217\u2217\n1School of Information, University of California, Berkeley 102 South Hall #4600 Berkeley, CA 94720-4600.\n2Department of Mathematics & Statistics, Vermont Complex Systems Center, Computational Story Lab, & the Vermont Advanced Computing Core,\nThe University of Vermont, Burlington, VT 05401. (Dated: September 29, 2017)\nThe task of text segmentation, or \u2018chunking,\u2019 may occur at many levels in text analysis, depending on whether it is most beneficial to break it down by paragraphs of a book, sentences of a paragraph, etc. Here, we focus on a fine-grained segmentation task, which we refer to as text partitioning, where we apply methodologies to segment sentences or clauses into phrases, or lexical constructions of one or more words. In the past, we have explored (uniform) stochastic text partitioning\u2014a process on the gaps between words whereby each space assumes one from a binary state of fixed (word binding) or broken (word separating) by some probability. In that work, we narrowly explored perhaps the most na\u0308ive version of this process: random, or, uniform stochastic partitioning, where all word-word gaps are prescribed a uniformly-set breakage probability, q. Under this framework, the breakage probability is a tunable parameter, and was set to be pure-uniform: q = 1/2. In this work, we explore phrase frequency distributions under variation of the parameter q, and define non-uniform, or informed stochastic partitions, where q is a function of surrounding information. Using a crude but effective function for q, we go on to apply informed partitions to over 20, 000 English texts from the Project Gutenberg eBooks database. In these analyses, we connect selection models to generate a notion of fit goodness for the \u2018bag-of-terms\u2019 (words or phrases) representations of texts, and find informed (phrase) partitions to be an improvement over the q = 1 (word) and q = 1/2 (phrase) partitions in most cases. This, together with the scalability of the methods proposed, suggests that the bag-of-phrases model should more often than not be implemented in place of the bag-of-words model, setting the stage for a paradigm shift in feature selection, which lies at the foundation of text analysis methodology.\nPACS numbers: 89.65.-s, 89.75.-k, 89.70.-a\nIn our previous work on text partitioning [1], and again in our study on the effects of text mixing [2], we have observed the relevance of the selection model proposed long ago by Simon [3]. We will use this work again, together with Zipf\u2019s law [4, 5], as the basis for a model of frequency data of text.\nThe defining aspects of Simon\u2019s model for language generation hold that as the words of a text appear, they do so independently, and in proportion to their historic frequencies of occurrence. The first assumption (of wordword independence) is simultaneously the basis for the \u2018bag-of-words\u2019 model that is pervasive in the computational text analysis community. It is clear that this assumption fails when one considers the clear dependence of words that are bound, like \u201cNew,\u201d \u201cYork,\u201d and \u201cCity.\u201d As we have seen in [2], dependence can also be caused by mixing, where the rate of a word\u2019s occurrence varies in a (global) mixed text as one transitions from document to document. These two types of dependence then indicate that for the Simon model to hold, and consequently, for the \u2018bag-of-words\u2019 model to hold, texts should be analyzed on the homogeneous (unmixed) scales of production, and the words of texts should be bound as phrases, according to their local dependence. The first criterion\ncan be relatively easy to satisfy, and in our analyses will come for free by the curation of the Project Gutenberg eBooks [6] database. The second, however, poses a much greater challenge, as the meanings that bind words into idioms and other irreducible forms are only known a priori to the writers who generate text for their meanings\u2014 all else must interpret.\nIn our work on stochastic text partitioning [1] we proposed a simple mechanism for the fine-grained chunking of text into phrases. Under this framework, sentences were broken down stochastically, splitting on whitespace with a uniform bond-breakage probability, q. While this (uniform) framework is clearly not the ideal mechanism to isolate phrases for, say, dictionary lookups, we did observe interesting improvements over word distributions (which too arise from stochastic text partitioning, setting q = 1) in relation to Simon\u2019s model.\nSince we cannot ask all writers to resolve their texts with their intended partitions, the next best step may then be to explore informed stochastic partitions, defining q\u2019s variation either empirically or through some model. While a strictly empirical approach might be best, it would likely require intensive data collection and surveying, and for this reason we will move forward in this\nTypeset by REVTEX\nar X\niv :1\n60 1.\n07 96\n9v 1\n[ cs\n.C L\n] 2\n9 Ja\nn 20\n16\n2 development with a hybrid approach. Here, we define q to be a function of two variables\u2014the immediately adjacent (left and right) words framing a given whitespace (gap) in text. Considering the example clause\nHot dog doctor!\nq0 q1 q2 q3\nwe will define q1 to be a function of the ordered pair, (Hot,dog), and the clause-edge partition probabilities (q0, and q3) to be constant and equal to 1. For simplicity, we will assume in general that qi are independent of one another, and approximate them with quasi-empirical breakage rates.\nSince we possess no wealth of hand-partitioned text for training, we instead turn to bond-breakage information held latently inside of the Wiktionary [7] by its trove of larger-than-word entries. We artificially construct a partitioned text by enumerating all N2 ordered pairs of phrases listed under the dictionary of N . Each pair of phrases is then considered a sentence, with one broken bond (between the two phrases), and potentially many unbroken bonds (internal to the phrases drawn). For an ordered pair (wl, wr) of left and right-framing words, we then define q(wl, wr) to be the probability that wl and wr came from separate phrases, given their concurrent appearance in a sentence in our artificial text.\nWhile our construction for the partition function q is highly artificial, it has substantial benefits in not requiring surveys, or having bias toward content types. However, if one wanted to implement these methods for analysis of a particular medium, e.g., Twitter, it might be best to survey that particular medium for partition probabilities, as the lay of the land for Twitter may look different than for Moby Dick. We also note here that q-defining partitions may be constructed by entirely different means and for different purposes. For example, one could bias q by pairwise word similarity, e.g., by edit distances [8], and make repetitions like \u201cha ha\u201d or \u201cla la\u201d, etc. more or less often partitioned.\nArmed with the dictionary-informed partition function defined above, we now turn to our measure of partition quality. This descends directly from what we refer to as the Zipf/Simon, single-parameter power-law model for rank-frequency distributions. Zipf\u2019s law succinctly formulates a relationship between the frequencies of occurrence of words, and their ranks by frequency (descending):\nf = C \u00b7 r\u2212\u03b8, (1)\nand results when languages are generated by Simon\u2019s model.\nThe main caveat for the compatibility of Zipf\u2019s empirical law with Simon\u2019s theoretical model arises from the\nfact that Simon\u2019s model is only able to generate singlescaling rank-frequency distributions with scaling parameters \u03b8 in [0, 1). While many texts exhibit multiple scalings and values of \u03b8 larger than 1, we have seen that these anomalies are often the result word dependence. Consequently, we view the degree to which a particular Zipf/Simon model aligns with empirical rank-frequency data as a measure of the goodness of fit for the \u2018bag-ofterms\u2019 representation of a text (tokenization) that generated the rank-frequency data (regardless of how the terms are defined, i.e., words or phrases).\nThe Zipf/Simon fit for a text of N unique and M total words is defined as follows. Assuming the text was generated precisely according to Simon\u2019s model, the constant word introduction rate is known by the text-wide average: \u03b1 = N/M , from which it can be shown [3] that the scaling parameter emerges to be \u03b8mod = 1\u2212\u03b1. The exact form of the model\u2019s fit is then obtained by computing the constant of proportionality: Cmod = N\n\u03b8mod , whereupon we may take the coefficient of determination, or R2, as a\n3\nmeasure of goodness of fit for the Zipf/Simon model.\nSince the connection of Simon\u2019s model to the parameter \u03b8 occurs naturally in the complimentary cumulative distribution function (CCDF) of term frequencies, we measure and regress all quantities along CCDFs, while we present all results in the intuitive and familiar rankfrequency (Zipf) representations.\nIn Fig 1, we plot an example informed partition for Herman Melville\u2019s well-known book \u201cMoby Dick\u201d (black line, main axes), and behind it, the spectrum of uniform stochastic partitions ranging from q = 0 (green) sentences/clauses, to q = 1 (pink) words. One can see the distributions steepen along the gradient as q is increased (which is generally the trend throughout the eBooks), and as much can be seen in the lower inset, where a regressed scaling parameter \u03b8 is compared with \u03b8mod = 1 \u2212 \u03b1. Also within the main axes lies the Zipf/Simon fit for the the informed partition (blue, dashed), which appears reasonable by inspection, and outperforms all uniform partitions (including the q = 1 word partition) by goodness of fit of the Zipf/Simon model (upper inset), where the informed partition is indicated by the black point (in both insets). Note that this increased quality of fit by the Zipf/Simon model is also observed readily in the lower inset, where \u03b8 and 1\u2212 \u03b1 nearly align.\nZooming out to the larger collection of English eBooks,\nwe focus on comparing two partitions for each text\u2014the q = 1 (word) partitions, and the informed (phrase) partitions. In Fig. 2, we compare the goodness of fit by the Zipf/Simon model between the two partition types. This comparison divides the collection of books into two subsets that we will refer to as the word-and phrase-based texts. Our demarcation is given by the line R2q=1 = R 2 inf (red, dashed), and shows that most texts were better processed under the informed (phrase-based, R2q=1 < R 2 inf) partition framework. While a good number of texts (\u2248 30%) are word-based, we note that these texts tend to have weaker R2 values, which can be seen in Fig. 3 where we apply an R2 cutoff to the books and measure the portion of the dataset removed. This quantifies the numbers of phrase-and word-based texts that are strong fits (R2 > 0.7) to be in a proportion of roughly 10 : 1, respectively.\nFrom Fig. 3, we can see that R2 \u2248 0.7 appears as a critical value for the cutoff, which, if applied as a threshold for analysis, results in approximately a 35% loss (red line/stars) for the dataset (largely due to the low-R2 word-based texts). By allowing the books to be either phrase- or word-based (according to their R2 values), we are then able to accommodate a strong (R2 \u2265 0.7) \u2018bag\u2019 analysis for more than 65% of the dataset, which is a large improvement over the conventional, blanket usage of words (dashed blue line/open circles), which only accommodates strong fits for approximately 15% of the collection.\nWhile one can view the (roughly) 35% of poorly-fit books as a loss to quality analysis, it is possible that refinement of the partition function (through better data or modeling) would allow one to obtain better tokenizations/fits for all books. However, one can also look at these 35% in a different way\u2014as potentially being unfit for the \u2018bag-of-terms\u2019 framework. Sorting the eBooks according to max{R2q=1, R2inf}, we find the ten poorest fits:\n(0.00) A Spelling-Book for Advanced Classes (0.00) One Thousand and One Initial Letters (0.00) English-Esperanto Dictionary (0.00) A Pocket Dictionary (0.01) Footsteps on the Road to Learning; (0.01) Infant\u2019s Cabinet of Birds & Beasts (0.01) Emperor Quartet op.76 no.3., 2nd mov. (0.01) ABC of Fox Hunting (0.02) The Works Of Charles James Lever (0.02) Price/Cost Indexes from 1875 to 1989\nto include dictionaries, spelling books, and books of extremely small size (often as placeholders for other media), indicating that low R2 values appropriately identify texts unfit for analyses.\nWe have shown that informed stochastic text partitioning and fit-goodness tests are capable of improving the basic methodologies for feature selection in text analysis,\nallowing for better adherence to assumptions (specifically, term-term independence) that are present in a vast collection of algorithms currently in practice throughout industry and academia. Additionally, phrase-based text analysis improves the independent interpretability of features\u2014in the soft sense, at the level of user experience. With phrase-based text analysis, end-point users\n(e.g., policy makers or product users interpreting a list of phrase-feature topics from a topic model readout) who may not understand the machinery of an algorithm will be better able to interpret results, as phrases provide critical context for interpretation.\nTo make these tools both explorable and available for the community, we have with this letter released the first version of a Python package for text partitioning (for detailed information, see https://github.com/jakerylandwilliams/partitioner) as well as an explorable online appendix (http://jakerylandwilliams.github.io/partitioner/) that also aims to gather empirical partition data from the public, using the model for scientific research proposed by Volunteer Science [9] (https://volunteerscience.com/) that will enable us to improve informed partitions for future work.\n\u2217 Electronic address: jakerylandwilliams@berkeley.edu \u2020 Electronic address: james.bagrow@uvm.edu \u2021 Electronic address: Andrew.Reagan@uvm.edu \u00a7 Electronic address: sharon@ischool.berkeley.edu \u00b6 Electronic address: chris.danforth@uvm.edu \u2217\u2217 Electronic address: peter.dodds@uvm.edu [1] J. R. Williams, P. R. Lessard, S. Desu, E. M. Clark,\nJ. P. Bagrow, C. M. Danforth, and P. S. Dodds, CoRR abs/1406.5181 (2015), http://arxiv.org/abs/1406.5181. [2] J. R. Williams, J. P. Bagrow, C. M. Danforth, and P. S. Dodds, CoRR (2015), http://arxiv.org/abs/1409.3870. [3] H. A. Simon, Biometrika 42, 425 (1955). [4] G. K. Zipf, The Psycho-Biology of Language (Houghton-\nMifflin, 1935). [5] G. K. Zipf, Human Behaviour and the Principle of Least-\nEffort (Addison-Wesley, 1949). [6] Project Gutenberg (2010), http://www.gutenberg.org. [7] Wiktionary (2014), http://dumps.wikimedia.org/enwiktionary/2014. [8] V. I. Levenshtein, Soviet Physics Doklady 10, 707 (1966). [9] V. Science (2016), https://volunteerscience.com/."}], "references": [{"title": "and P", "author": ["J.R. Williams", "P.R. Lessard", "S. Desu", "E.M. Clark", "J.P. Bagrow", "C.M. Danforth"], "venue": "S. Dodds, CoRR abs/1406.5181 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "and P", "author": ["J.R. Williams", "J.P. Bagrow", "C.M. Danforth"], "venue": "S. Dodds, CoRR ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Biometrika 42", "author": ["H.A. Simon"], "venue": "425 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1955}, {"title": "Human Behaviour and the Principle of Least", "author": ["G.K. Zipf"], "venue": "Effort (Addison-Wesley,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1949}, {"title": "Soviet Physics Doklady 10", "author": ["V.I. Levenshtein"], "venue": "707 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1966}], "referenceMentions": [{"referenceID": 0, "context": "In our previous work on text partitioning [1], and again in our study on the effects of text mixing [2], we have observed the relevance of the selection model proposed long ago by Simon [3].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "In our previous work on text partitioning [1], and again in our study on the effects of text mixing [2], we have observed the relevance of the selection model proposed long ago by Simon [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "In our previous work on text partitioning [1], and again in our study on the effects of text mixing [2], we have observed the relevance of the selection model proposed long ago by Simon [3].", "startOffset": 186, "endOffset": 189}, {"referenceID": 3, "context": "together with Zipf\u2019s law [4, 5], as the basis for a model of frequency data of text.", "startOffset": 25, "endOffset": 31}, {"referenceID": 1, "context": "As we have seen in [2], dependence can also be caused by mixing, where the rate of a word\u2019s occurrence varies in a (global) mixed text as one transitions from document to document.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "In our work on stochastic text partitioning [1] we proposed a simple mechanism for the fine-grained chunking of text into phrases.", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": ", by edit distances [8], and make repetitions like \u201cha ha\u201d or \u201cla la\u201d, etc.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "age: \u03b1 = N/M , from which it can be shown [3] that the scaling parameter emerges to be \u03b8mod = 1\u2212\u03b1.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "edu [1] J.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "[2] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[8] V.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Jake Ryland Williams, \u2217 James P. Bagrow, \u2020 Andrew J. Reagan, \u2021 Sharon E. Alajajian, \u00a7 Christopher M. Danforth, \u00b6 and Peter Sheridan Dodds \u2217\u2217 School of Information, University of California, Berkeley 102 South Hall #4600 Berkeley, CA 94720-4600. Department of Mathematics & Statistics, Vermont Complex Systems Center, Computational Story Lab, & the Vermont Advanced Computing Core, The University of Vermont, Burlington, VT 05401. (Dated: September 29, 2017)", "creator": "LaTeX with hyperref package"}}}