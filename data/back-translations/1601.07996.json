{"id": "1601.07996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2016", "title": "Feature Selection: A Data Perspective", "abstract": "The selection of features as a data pre-processing strategy has proven effective and efficient in the preparation of high-dimensional data for data mining and machine learning. Objectives of feature selection include: building simpler and more comprehensible models; improving data mining performance; and preparing clean, comprehensible data. The recent proliferation of big data has highlighted some significant challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by the current challenges and opportunities in the big data age, we look at feature selection research from the perspective of data and review representative feature selection algorithms for generic data, structured data, heterogeneous data, and streaming data. To emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: selectivity-based methodologies, statistical algorithms to facilitate the presentation of selectivity-based learning methods; and, statistical algorithmic methods to facilitate the selection of generic data.", "histories": [["v1", "Fri, 29 Jan 2016 08:32:10 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v1", null], ["v2", "Fri, 26 Feb 2016 00:29:42 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v2", null], ["v3", "Fri, 4 Mar 2016 22:24:56 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v3", null], ["v4", "Mon, 26 Sep 2016 10:32:12 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jundong li", "kewei cheng", "suhang wang", "fred morstatter", "robert p trevino", "jiliang tang", "huan liu"], "accepted": false, "id": "1601.07996"}, "pdf": {"name": "1601.07996.pdf", "metadata": {"source": "CRF", "title": "Feature Selection: A Data Perspective", "authors": ["Jundong Li", "Kewei Cheng", "Suhang Wang", "Robert P. Trevino", "Jiliang Tang", "Huan Liu"], "emails": ["jundongl@asu.edu", "kcheng18@asu.edu", "swang187@asu.edu", "fmorstat@asu.edu", "rptrevin@asu.edu", "jlt@yahoo-inc.com", "huanliu@asu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n07 99\n6v 1\nKeywords: Feature Selection"}, {"heading": "1. Introduction", "text": "We are now in the era of big data, where massive amounts of high dimensional data has become ubiquitous in our daily life, such as social media, e-commerce, health care, bioinformatics, transportation, online education, etc. Figure (1) shows an example by plotting the growth trend of UCI machine learning repository (Bache and Lichman, 2013). Rapid growth of data presents challenges for effective and efficient data management. Therefore, it is desirable and of great importance to apply data mining and machine learning techniques to automatically discover knowledge from these data.\nWhen applying data mining and machine learning algorithms on high dimensional data, a critical issue is known as curse of dimensionality (Hastie et al., 2005). It refers to the phenomenon that data becomes sparser in high dimensional space, adversely affecting algorithms designed for low dimensional space. In addition, with the existence of a large number of features, learning models tend to overfit which may cause performance degradation on unseen data. Moreover, data of high dimension significantly increases the memory storage requirements and computational costs for data analytics.\nDimensionality reduction is one of the most powerful tools to address the previously described issues. It can be categorized mainly into into two main components: feature extraction and feature selection. Feature extraction projects original high dimensional feature space to a new feature space with low dimensionality. The new constructed feature space is usually a linear or nonlinear combination of the original feature space. Examples of feature extraction methods include Principle Component Analysis (PCA) (Jolliffe, 2002), Linear Discriminant Analysis (LDA) (Scholkopft and Mullert, 1999), Canonical Correlation Analysis (CCA) (Hardoon et al., 2004), Singular Value Decomposition (Golub and Van Loan, 2012), ISOMAP (Tenenbaum et al., 2000) and Locally Linear Embedding (LLE) (Roweis and Saul, 2000). Feature selection, on the other hand, directly selects a subset of relevant features for the use model construction. Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al., 2005), Fisher Score (Duda et al., 2012), Laplacian Score (He et al., 2005), and SPEC (Zhao and Liu, 2007) are some of the well known feature selection techniques.\nBoth feature extraction and feature selection have the advantage of improving learning performance, increasing computational efficiency, decreasing memory storage requirements, and building better generalization models. However, since feature extraction builds a set of new features, further analysis is problematic as we cannot get the physical meaning of these features in the transformed space. In contrast, by keeping some original features, feature selection maintains physical meanings of original features, and gives models better readability and interpretability. Therefore, feature selection is often preferred in many realworld applications such as text mining and genetic analysis compared to feature extraction.\nReal-world data is usually imperfect, containing some irrelevant and redundant features. Removing these features by feature selection reduces storage and computational cost while avoiding significant loss of information or negative degradation of learning performance. For example, in Figure (2(a)), feature f1 is a relevant feature which is able to discriminate two classes (clusters). However, given feature f1, feature f2 in Figure (2(b)) is redundant as f2 is strongly correlated with f1. In Figure (2(c)), feature f3 is an irrelevant feature as it cannot separate two classes (clusters) at all. Therefore, the removal of f2 and f3 will not negatively impact the learning performance.\nIn the following subsections, we first review traditional categorizations of feature selection algorithms from the availability of labels and from the search strategy perspectives in Section 1.1. In Section 1.2, we revisit feature selection from a data perspective motivated by challenges and opportunities from big data. Meanwhile, we discuss the necessity for a comprehensive and structured overview of current advances on feature selection, and our efforts to build a open source machine learning repository to cover state-of-the-art feature selection algorithms. In Section 1.3, we give an outline and organization of the survey."}, {"heading": "1.1 Traditional Categorizations of Feature Selection Algorithms", "text": ""}, {"heading": "1.1.1 Label Perspective", "text": "According to the availability of label information, feature selection algorithms can be broadly classified as supervised, unsupervised and semi-supervised methods.\nSupervised Feature Selection Supervised feature selection is generally designed for classification or regression problems. It aims to select a subset of features that are able to discriminate samples from different classes. With the existence of class labels, the feature relevance is usually assessed via its correlation with class labels. A general framework of supervised feature selection is illustrated in Figure (3). The training phase of the classification highly depends on feature selection. After splitting the data into training and testing sets, classifiers are trained based on a subset of features selected by supervised feature selection. Note that the feature selection phase can either be independent of the learning algorithm (filter methods), or it may iteratively take advantage of the learning performance of a classifier to assess the quality of selected features so far (wrapper methods). Finally, the trained classifier predicts class labels of samples in the test set on the selected features.\nUnsupervised Feature Selection Unsupervised feature selection is generally designed for clustering problems. Since acquiring labeled data is particularly expensive in both time and effort, unsupervised feature selection on unlabeled data has recently gained considerable attention recently. Due to the lack of label information to evaluate the importance of features, unsupervised feature selection methods seek alternative criteria to define the relevance of features such as data similarity and local discriminative information. A general framework of unsupervised feature selection is illustrated in Figure (4). Different from supervised feature selection, unsupervised feature\nselection usually uses all instances are available in the feature selection phase. The feature selection phase is either be independent of the unsupervised learning algorithms (filter methods), or it relies on the learning algorithms to iteratively improve the quality of selected features (wrapper methods). After the feature selection phase, it outputs the cluster structure of all data samples on the selected features by using a typical clustering algorithm.\nSemi-Supervised Feature Selection Supervised feature selection works when sufficient label information is available while unsupervised feature selection algorithms do not require any label information. However, in many real-world applications, we usually have a small number of labeled samples and a large number of unlabeled samples. Both supervised and unsupervised feature selection algorithms cannot fully take advantage of all samples in this scenario. For supervised methods, the small number of labeled samples may be insufficient to provide correlation information of features; while unsupervised methods totally ignore class labels which could provide useful information to discriminate different classes. Therefore, it is desirable to develop semi-supervised methods by exploiting both labeled and unlabeled samples. We provide a general framework of semi-supervised feature selection in Figure (5), it is similar to the framework of supervised feature selection except that in semi-supervised methods only partial label information is available."}, {"heading": "1.1.2 Search Strategy Perspective", "text": "With respect to different selection strategies, feature selection methods can be categorized as wrapper, filter and embedded methods.\nWrapper Methods Wrapper methods rely on the predictive performance of a predefined learning algorithm to evaluate the quality of selected features. Given a specific learning algorithm, a typical wrapper method performs two steps: (1) Searches for a subset of features and (2) evaluate selected features. It repeats (1) and (2) until some stopping criteria are satisfied or the desired learning performance is obtained. The workflow of wrapper methods is illustrated in Figure (6). It can be observed that the feature set search component first generates a subset of features, then the learning algorithm acts as a black box to evaluate the quality of these features based on the learning performance. The whole process\nworks iteratively until the highest learning performance is achieved. The feature subset that gives the highest learning performance is output as the selected features. Unfortunately, a known issue of wrapper methods is that the search space for d features is 2d, which makes the exhaustive search impractical when d is large. Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance. However, the search space is still extremely large for high dimensional datasets. As a result, wrapper methods are seldom used in practice.\nFilter Methods Filter methods are independent of any learning algorithms. They rely on certain characteristics of data to assess the importance of features. Filter methods are typically more efficient than wrapper methods. However, due to the lack of a specific learning algorithm guiding the feature selection phase, the selected features may not be optimal for the target learning algorithms. A typical filter method consists of two steps. In the first step, feature importance is ranked by a feature score according to some feature evaluation criteria. The feature importance evaluation process can be either univariate or multivariate. In the univariate scheme, each feature is ranked individually regardless of other features, while the multivariate scheme ranks multiple features in a batch way. In the second step of a typical filter method, low ranking features are filtered out and the remaining features are selected. In the past decades, many different evaluation criteria for filter methods have been proposed. Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-S\u030cikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al., 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al., 2010; Farahat et al., 2011).\nEmbedded Methods Filter methods select features that are independent of any learning algorithms, therefore they are computational efficient. However, they fail to consider the bias of the learning algorithms, and the selected features may not be optimal for the learning\ntasks. On the contrast, wrapper methods evaluate the importance of features by the given learning algorithms iteratively and can obtain better predictive accuracy for that specific learning algorithm. However, due to the exponential search space, it is computational intractable in many applications when the feature dimension is high. Embedded methods provide a trade-off solution between filter and wrapper methods which embed the feature selection with the model learning, thus they inherit the merits of wrapper and filter methods \u2013 (1) they include the interactions with the learning algorithm; and (2) they are far more efficient than the wrapper methods since they do not need to evaluate feature sets iteratively. The most widely used embedded methods are the regularization models which targets to fit a learning model by minimizing the fitting errors and forcing the feature coefficients to be small (or exact zero) simultaneously. Afterwards, both the regularization model and selected feature sets are output as results."}, {"heading": "1.2 Feature Selection Algorithms from A Data Perspective", "text": "The recent popularity of big data presents some challenges for the traditional feature selection task. Meanwhile, some characteristics of big data like velocity and variety tend to promote the development of novel feature selection algorithms. Here we briefly present and discuss some major concerns when we apply feature selection algorithms.\nStreaming Data and Features Streaming data and features have become more prevalent in real world applications. This poses a significant challenge to traditional feature selection algorithms, which assume static datasets with fixed features. For example in Twitter, new data like posts and new features like slang words are continuously being generated. It is impractical to apply traditional batch-mode feature selection algorithms to find relevant features at each round when new data or new feature arrives. In addition, the volume data may sometimes be too large to be loaded into memory directly with a single data scan. This is especially a problem when a second pass is either unavailable or very\nexpensive. Due to aforementioned reasons, it is more appealing to apply feature selection in a streaming fashion to dynamically maintain a best set of feature from all features and data seen up to that point.\nHeterogeneous Data Most existing algorithms of feature selection are designed to handle tasks with single data source and always assume that the data is independent and identically distributed (i.i.d.). However, multi-source data is quite prevalent in many domains. For example, in social media, data come from heterogeneous sources such as text, images, tags. In addition, linked data is ubiquitous and presents itself in various forms such as user-post relations and user-user relations. The availability of multiple data sources brings unprecedented opportunities as we can leverage shared intrinsic characteristics and correlations to find more relevant features. However, challenges are also unequivocally presented with additional data sources. For instance, with the existence of link information, the widely adopted i.i.d. assumption in most machine learning algorithms does not hold. How to appropriately utilize link information for feature selection is still a challenging problem.\nStructures Between Features Sometimes, features can exhibit certain types of structures in many real-world applications. Some well-known structures among features are group structure, tree structure, graph structure, etc. When performing feature selection, if the feature structure is not taken into consideration, the intrinsic dependencies may not be captured and the selected features may not be suitable for the data. Incorporating the prior knowledge of feature structures can possibly help select relevant features to greatly improve the learning performance.\nThe aforementioned reasons motivate us to investigate feature selection algorithms from a different view to include recent advances and frontiers about feature selection. In this survey, we revisit feature selection algorithms from a data perspective, the categorization is illustrated in Figure (7). It is shown that data consists of static data and streaming data. First, for static data, it can be further grouped into generic data and heterogeneous data. In generic data, features can either be flat or possess some intrinsic structures. Traditional feature selection algorithms are proposed to deal with these flat features in which features are considered to be independent. The past few decades have witnessed hundreds of feature selection algorithms. Based on their technical characteristics, we propose to classify them\ninto four groups, i.e., similarity based methods, information-theoretic based methods, sparse learning based methods and statistical based methods as shown in Figure (8). It should be noted that this categorization only involves filter methods and embedded methods while the wrapper methods are excluded. The reason for excluding wrapper methods is that they are computationally expensive and, therefore, are seldom used in practice. More details about these four categories will be investigated later. When features express some structures, specific feature selection algorithms for structural features are more desirable. On the other hand, data can be heterogeneous, in many real-world applications we are often encompassed by linked, multi-source or multi-view data, we also show how well-designed feature selection algorithms are designed to deal with these situations. Second, in the streaming settings, data arrive sequentially in a streaming fashion where the size of data instances is unknown, feature selection algorithms that make only one pass over the data is proposed to tackle streaming data. Similarly, in a orthogonal setting, features can also be generated dynamically \u2013 new features are sequentially added and the size of features is even unknown in some cases. Streaming feature selection algorithms are designed to determine if accepting the newly added features and if removing existing but outdated features.\nCurrently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014). These surveys either focus on traditional feature selection algorithms or detailed learning task like classification and clustering. However, none of them provide a comprehensive and structured overview of traditional feature selection algorithms in conjunction with recent advances in feature selection from a data perspective. In this survey, we will introduce representative feature selection algorithms to cover all components mentioned in Figure (7) and Figure (8). We also release a feature selection repository in Python named scikit-feast which is built upon the widely used machine learning package scikitlearn1 and two scientific computing packages Numpy2 and Scipy3. It includes more than 40 representative feature selection algorithms. The website of the repository is available at http://featureselection.asu.edu/scikit-feast/."}, {"heading": "1.2.1 Organization of the Survey", "text": "We present this survey in five parts and the covered topics are listed as follows:\n1. Feature Selection with Generic Data (Section 2)\n(a) Similarity based Feature Selection Methods\n(b) Information Theoretical based Feature Selection Methods\n(c) Sparse Learning based Feature Selection Methods\n(d) Statistical based Feature Selection Methods\n2. Feature Selection with Structure Features (Section 3)\n(a) Feature Selection Algorithms with Group Structure Features\n1. http://scikit-learn.org/stable/ 2. http://www.numpy.org/ 3. http://www.scipy.org/\n(b) Feature Selection Algorithms with Tree Structure Features\n(c) Feature Selection Algorithms with Graph Structure Features\n3. Feature Selection with Heterogeneous Data (Section 4)\n(a) Feature Selection Algorithms with Linked Data\n(b) Feature Selection Algorithms with Multi-Source Data\n(c) Feature Selection Algorithms with Multi-View Data\n4. Feature Selection with Streaming Data (Section 5)\n(a) Feature Selection Algorithms with Data Streams\n(b) Feature Selection Algorithms with Feature Streams\n5. Performance Evaluation (Section 6)\n6. Open Problems and Challenges (Section 7)\n7. Summary of the Survey (Section 8)"}, {"heading": "2. Feature Selection on Generic Data", "text": "Over the past two decades, hundreds of feature selection algorithms have been proposed. In this Section, we broadly group traditional feature selection algorithms for generic data into four categories: similarity based, information theoretical based, sparse learning based and statistical based methods according to the techniques they adopt during the feature selection process. In the following subsections, we will briefly review each category with some representative algorithms.\nWe summarize some common symbols used throughout this survey in Table 1. We use bold uppercase characters for matrices (e.g. A), bold lowercase characters for vectors (e.g. a), calligraphic fonts for sets (e.g. F). We follow the matrix settings in Matlab to represent i-th row of matrix A as A(i, :), j-th column of A as A(:, j), (i, j)-th entry of A as A(i, j), transpose of A as A\u2032, and the trace of A as tr(A). For any matrix A \u2208 Rn\u00d7d, its Frobenius norm is defined as ||A||F = \u221a \u2211n i=1 \u2211d j=1A(i, j) 2, and its \u21132,1-\nnorm is ||A||2,1 = \u2211n\ni=1\n\u221a\n\u2211d j=1A(i, j) 2. For any vector a = [a1, a2, ..., an] \u2032, its \u21132-norm is\ndefined as ||a||2 = \u221a \u2211n i=1 a 2 i , and its \u21131-norm is ||a||1 = \u2211n i=1 |ai|. I is an identity matrix and 1 is a vector whose elements are all 1."}, {"heading": "2.1 Similarity based Methods", "text": "Different feature selection algorithms exploit various types of criteria to define the relevance of features such as distance, separability, information, correlation, dependency, and reconstruction error. Among them, there is a family of methods assessing the importance of features by its ability to preserve data similarity. We call these kinds of methods to be similarity based feature selection methods. For supervised feature selection, data similarity\ncan be derived from label information; while for unsupervised feature selection methods, most methods take advantage of different distance metric measures to obtain data similarity.\nGiven a dataset X \u2208 Rn\u00d7d with n instances and d features, the pairwise similarity among instances can be encoded in an affinity matrix S \u2208 Rn\u00d7n. The affinity matrix S is symmetric and its (i, j)-th entry indicates the similarity between the i-th instance xi and the j-th instance xj , the larger the value of Si,j is, the more similarity xi and xj share. Suppose we want to select k most relevant features from F , then the utility of these k features is maximized as follows:\nmax F\n\u2211\nf\u2208F\nSC(f) = max F\n\u2211\nf\u2208F\nf\u0302 \u2032S\u0302f\u0302 , (1)\nwhere SC is a function that measures the utility of feature f , f\u0302 \u2032 and S\u0302 are the normalized feature and refined affinity matrix obtained from f and S, respectively. The maximization problem in Eq. (1) shows that we would select a subset of features from F such that they can well preserve the data similarity structures defined in S\u0302. This problem is usually solved by greedily selecting the top k features that maximize their individual utility f\u0302 \u2032Sf\u0302 . Methods in this category vary in the way the similarity matrix S is designed. We subsequently discuss about the original formulations of some representative algorithms in this group and then introduce how they can be reformulated under the unified framework."}, {"heading": "2.1.1 Laplacian Score (He et al., 2005) (Unsupervised)", "text": "Laplacian Score is an unsupervised feature selection algorithm which selects features that can best preserve the data manifold structure. It consists of three phases. First, it construct\na nearest neighbor graph G with n nodes where the i-th node corresponds to xi. If xi is among the p nearest neighbors of xj or xj is among the p nearest neighbors of xi, nodes i and j are connected in G (p is a predefined number). Second, if nodes i and j are connected, the entry in the affinity matrix Sij is S(i, j) = e \u2212 ||xi\u2212xj || 2\nt , where t is a constant, otherwise S(i, j) = 0. The diagonal matrix D is defined as D(i, i) =\n\u2211n j=1 S(i, j) and the laplacian\nmatrix L is L = D \u2212 S (Chung, 1997). Lastly, the Laplacian Score of each feature fi is computed as:\nlaplacian score(fi) = f\u0303 \u2032iLf\u0303i\nf\u0303 \u2032iDf\u0303i , where f\u0303i = fi \u2212\nf \u2032iD1 1\u2032D1 1. (2)\nSince Laplacian Score evaluates the importance of each feature individually, the task of selecting the k features can be solved by greedily picking the top k features with the smallest Laplacian Scores.\nThe Laplacian Score of each feature can be reformulated as follows:\nlaplacian score(fi) = f\u0303 \u2032iLf\u0303i\nf\u0303 \u2032iDf\u0303i = f\u0303 \u2032i(D\u2212 S)f\u0303i f\u0303 \u2032iDf\u0303i = 1\u2212 f\u0303 \u2032 iSf\u0303i f\u0303 \u2032iDf\u0303i\n= 1\u2212 (\nf\u0303i\n||D 12 f\u0303i||\n)\u2032\nS\n(\nf\u0303i\n||D 12 f\u0303i||\n)\n.\n(3)\nSince f\u0303i \u2032 Df\u0303i is the weighted data variance of feature fi (denoted as \u03c3 2 i ), ||D 1 2 f\u0303i|| is the standard data variance (denoted as \u03c3i), and the term f\u0303i/||D 1 2 f\u0303i|| is interpreted as a normalized feature vector f\u0302i = (fi \u2212 \u00b5i1)/\u03c3i. Therefore, Laplacian Score feature selection can be reformulated by maximizing the following term:\nmax F\n\u2211\nf\u2208F\nf\u0302 \u2032S\u0302f\u0302 , (4)\nand it is a special case of the unified framework of similarity based feature selection."}, {"heading": "2.1.2 SPEC (Zhao and Liu, 2007) (Unsupervised and Supervised)", "text": "SPEC is an extension of Laplacian Score that work for both supervised and unsupervised scenarios. For example, in the unsupervised scenario, without label information, the data similarity is measured by the RBF kernel function:\nS(i, j) = e\u2212 ||xi\u2212xj || 2\u03c32 , (5)\nin the supervised scenario, using label information, data similarity can be defined by:\nS(i, j) =\n{ 1 nl if yi = yj = l\n0 otherwise, (6)\nwhere nl is the number of data samples in the class l. Afterwards the construction of affinity matrix S, the diagonal matrix D is defined as D(i, i) =\n\u2211n j=1 S(i, j) and the normalized\nlaplacian matrix Lnorm is Lnorm = D \u2212 1 2 (D \u2212 S)D\u2212 12 (Chung, 1997). The basic idea of\nSPEC is similar to Laplacian Score, a feature that is consistent with the data manifold structure assigns similar values to instances that are near each other. The feature relevance are measured by three different criteria:\nSPEC score1(fi) = f\u0302i \u2032 \u03b3(Lnorm)f\u0302i =\nn \u2211\nj=1\n\u03b12j\u03b3(\u03bbj)\nSPEC score2(fi) = f\u0302i \u2032 \u03b3(Lnorm)f\u0302i\n1\u2212 (f\u0302i \u2032 \u03be1)2\n=\n\u2211n j=2 \u03b1 2 j\u03b3(\u03bbj)\n\u2211n j=2 \u03b1 2 j\nSPEC score3(fi) =\nm \u2211\nj=1\n(\u03b3(2) \u2212 \u03b3(\u03bbj))\u03b12j .\n(7)\nIn the above three equations, f\u0302i = D 1 2 fi/||D 1 2 fi||; (\u03bbj , \u03bej) is the j-th eigenpair of the normalized laplacian matrix Lnorm; \u03b1j = cos \u03b8j, \u03b8j is the angle between \u03bej and fi; \u03b3(.) is an increasing function to penalize high frequency components of the eigensystem to reduce noise. If the data is noise free, the function \u03b3(.) can be removed and \u03b3(x) = x. When the second evaluation criteria SPEC score2(fi) is used, SPEC is equivalent to Laplacian Score. For SPEC score3(fi), it uses the top m eigenpairs to evaluate the importance of feature fi.\nNext we show how SPEC can be reduced to the generalized similarity based feature selection framework. Selecting top k features in SPEC with three different criteria in Eq. (7) can be reformulated as:\nmax F\n\u2211\nf\u2208F\nf\u0302 \u2032i S\u0302f\u0302i\nin SPEC score1 : f\u0302i = fi||D 1 2 fi||, S\u0302 = D 1 2U(I \u2212 \u03b3(\u03a3))U\u2032D 12 in SPEC score2 : f\u0302i = (fi \u2212 \u00b51)/||D 1 2 fi||, S\u0302 = D 1 2U(I\u2212 \u03b3(\u03a3))U\u2032D 12 in SPEC score3 : f\u0302i = fi||D 1 2 fi||, S\u0302 = D 1 2Um(\u03b3(2I) \u2212 \u03b3(\u03a3m))U\u2032mD 1 2 ,\n(8)\nwhere U and \u03a3 contain the singular vectors and singular values of the normalized laplacian matrix Lnorm, Lnorm = U\u03a3U \u2032."}, {"heading": "2.1.3 Fisher Score (Duda et al., 2012) (Supervised)", "text": "Fisher Score is a supervised feature selection algorithm. Suppose the class labels of n samples y = {y1, y2, ..., yn} come from c classes, Fisher Score selects the features such that the feature values of the samples within the same class are small while the feature values of the samples from different classes are large. The Fisher score of each feature fi is evaluated as follows:\nfisher score(fi) = \u2211c j=1 nj(\u00b5i,j \u2212 \u00b5i)2 \u2211c\nj=1 nj\u03c3(i, j) 2\n, (9)\nwhere nj, \u00b5i, \u00b5i,j and \u03c3 2 i,j indicate the number of samples in class j, mean value of feature fi, mean value of feature fi for samples in class j, variance value of feature fi for samples in class j, respectively. Similar to Laplacian Score, the top k features can be obtained by greedily selecting the features with the largest Fisher Scores.\nAccording to (He et al., 2005), Fisher Score can be considered as a special case of Laplacian Score as long as the affinity matrix is as follows:\nS(i, j) =\n{ 1 nl if yi = yj = l\n0 otherwise, (10)\nwhere nl is the number of data samples in the class l. With this specific affinity matrix, we can get the following relationship between Fisher Score and Laplacian Score as follows:\nfisher score(fi) = 1\u2212 1\nlaplacian score(fi) . (11)\nTherefore, the computation of Fisher Score can be reduced to the unified framework of similarity based feature selection.\nFisher Score measures the relevance of each feature individually as Laplacian Score and SPEC. This leads to a suboptimal subset of features that is incapable of removing redundant features. To tackle this issue, a Generalized Fisher Score method Gu et al. (2011) is proposed to jointly select features. It aims to find a subset of features by maximizing the lower bound of Fisher Score, which results in the following objective function:\nmin W,p\n1 2 ||Xdiag(p)W \u2212H||2F + \u03b1 2 ||W||2F\ns.t. p \u2208 {0, 1}d,p\u20321 = k. (12)\np is a feature indicator vector to indicate whether the feature is selected or not, \u03b1 is a regularization parameter, H \u2208 Rn\u00d7c is a label matrix, its (i, j)-th entry is given by:\nH(i, j) =\n\n\n\n\u221a\nn nj\n\u2212 \u221a\nnj n if yi = j\n\u2212 \u221a\nnj n\notherwise, (13)\nwhere nj is the number of data instances in class j."}, {"heading": "2.1.4 Trace Ratio Criterion (Nie et al., 2008) (Supervised)", "text": "Recently, the trace ratio criterion has been proposed to directly select the global optimal feature subset based on the corresponding score, which is computed in a trace ratio norm. It builds two affinity matrices Sw and Sb to characterize within-class (local affinity) and between-class (global affinity) data similarity. Their corresponding diagonal matrices and laplacian matrices are defined as Dw(i, i) = \u2211n j=1 Sw(i, j), Db(i, i) = \u2211n j=1 Sb(i, j), Lw = Dw \u2212 Sw, Lb = Db \u2212 Sb, respectively. Let W = [wi1 ,wi2 , ...,wik ] \u2208 Rd\u00d7k be the selection indicator matrix such that only the ij-th entry in wij is 1 and all the other entries are 0. With these, the trace ratio criterion of all k features in F is:\ntrace ratio(F) = tr(W \u2032X\u2032LbXW)\ntr(W\u2032X\u2032LwXW) . (14)\nThe basic idea is to maximize the data similarity for the instances from the same class (or close to each other) while minimizing the data similarity for the instances from different\nclasses (or far away from each other). The larger the score, the more important the feature set is.\nThe trace ratio criterion score in Eq. (14) provides a general framework for feature selection. Different between-class and within-class similarity matricse Sb and Sw lead to different feature selection algorithms such as batch-mode Lalpacian Score and batch-mode Fisher Score; all of them can be considered as special cases of the general similarity based feature selection framework. For example, in batch-mode Fisher Score, the within-class data similarity and the between-class data similarity are defined as follows:\nSw(i, j) =\n{\n1/nl if yi = yj = l 0 otherwise,\n(15)\nSb(i, j) =\n{\n1/n\u2212 1/nl if yi = yj = l 1/n otherwise,\n(16)\nwhere nl is the number of instances in class l. The trace ratio criterion for the feature subset F can be calculated as:\ntrace ratio fisher(F) = tr(W \u2032X\u2032LbXW)\ntr(W\u2032X\u2032LwXW) =\n\u2211k s=1 f \u2032 is Swfis\n\u2211k s=1 f \u2032 is (I\u2212 Sw)fis\n. (17)\nMaximizing the score in above equation is also equivalent to maximize the following term: \u2211k\ns=1 f \u2032 is Swfis\n\u2211k s=1 f \u2032 is fis\n= X\u2032FSwXF\nX\u2032FXF . (18)\nSince X\u2032FXF is constant and the above maximization problem can be reduced to the unified similarity based feature selection framework:\nmax F\n\u2211\nf\u2208F\nf\u0302 \u2032S\u0302f\u0302 , where f\u0302 = f/||f || and S\u0302 = Sw. (19)\nIn batch-mode Laplacian Score, the within-class data similarity and the between-class data similarity are defined as follows:\nSw(i, j) =\n{\ne\u2212 ||xi\u2212xj ||\n2\nt if xi \u2208 Np(xj) or xj \u2208 Np(xi) 0 otherwise,\n(20)\nSb = (1 \u2032Dw1) \u22121Dw11 \u2032D. (21)\nThe trace ratio criterion score can be computed by:\ntrace ratio laplacian(F) = tr(W \u2032X\u2032LbXW)\ntr(W\u2032X\u2032LwXW) =\n\u2211k s=1 f \u2032 is Dwfis\n\u2211k s=1 f \u2032 is (Dw \u2212 Sw)fis\n. (22)\nMaximizing the above trace ratio criterion score is also equivalent to solve the following problem:\nmax F\n\u2211\nf\u2208F\nf\u0302 \u2032S\u0302f\u0302 , where f\u0302 = f/||D 12 f || and S\u0302 = Sw, (23)\ntherefore it is a special case of the unified framework."}, {"heading": "2.1.5 ReliefF (Robnik-S\u030cikonja and Kononenko, 2003) (Supervised)", "text": "Relief and its multi-class variant ReliefF are supervised filter algorithms that select features to separate instances from different classes. Assume that l data instances are randomly selected among all n instances, then the feature score of fi in Relief is defined as follows:\nRelief score(fi) = 1\n2\nl \u2211\nj=1\nd(X(j, i) \u2212X(NM(j), i)) \u2212 d(X(j, i) \u2212X(NH(j), i)), (24)\nwhere NM(j) and NH(j) indicates the nearest data instances to xj with the same class label and different class, respectively. d(.) is a distance metric which is usually set to be Euclidean distance. Relief only works for binary classification task. To tackle the multiclass classification problem, the feature score in Eq. (24) is extended in ReliefF:\nReliefF score(fi) = 1\nc\nl \u2211\nj=1\n\n\u2212 1 mj\n\u2211\nxr\u2208NH(j)\nd(X(j, i) \u2212X(r, i)) (25)\n+ \u2211\ny 6=yj\n1\nhjy\np(y)\n1\u2212 p(y) \u2211\nxr\u2208NM(j,y)\nd(X(j, i) \u2212X(r, i))\n\n , (26)\nwhere NH(j) and NM(j, y) indicate the nearest data instances to xj in the same class and a different class y, respectively, and their sizes are hjy and mj. p(y) is the ratio of instances with class label y.\nReliefF is equivalent to selecting features that preserve a special form of data similarity matrix which can be derived from the class labels. Assume that the dataset has the same number of instances in each of the c classes, there are q instances in both NM(j) and NH(j, y), the Euclidean distance is used and each feature vector has been normalized. Then according to (Zhao and Liu, 2007), the criterion of ReliefF is equivalent to the following with above assumptions:\nReliefF score(fi) =\nn \u2211\nj=1\n(\nq \u2211\ns=1\n1 q (X(j, i) \u2212X(NM(j)s))2 (27)\n\u2212 \u2211\ny 6=yj\n\u2211q s=1X(j, i) \u2212X(NH(j, y)s)2\n(c\u2212 1)q\n\n , (28)\nwhere NM(j)s denote the s-th nearest hit of xj and NH(j, y)s denote the s-th nearest miss of xj in class y. It is easy to show that maximizing the ReliefF score in Eq. (28) is equivalent to the following optimization problem:\nmax F\n\u2211\nf\u2208F\n\u22121 + f \u2032S\u0302f , (29)\nwhere the affinity matrix is defined as:\nS\u0302(i, j) =\n\n \n \n1 if i = j \u22121\nq xj \u2208 NH(i)\n1 (c\u22121)q xj \u2208 NH(i, y).\n(30)\nThe optimization can be solved in a greedy manner by selecting the top k features with the highest ReliefF scores."}, {"heading": "2.2 Information Theoretical based Methods", "text": "A large family of existing feature selection algorithms are information theoretical based methods. Algorithms in this family mainly exploit different heuristic filter criteria to measure the importance of features. As indicated in (Duda et al., 2012), many handdesigned information theoretic criteria are proposed to maximize feature relevance and minimize feature redundancy. Since the relevance of a feature is usually measured by its correlation with class labels, most algorithms in this family are performed in a supervised way. In addition, most information theoretic concepts can only be applied on discrete variables. Therefore, feature selection algorithms in this family can only work with discrete data. For numeric feature values, some data discretization techniques (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006) are required. Two decades of research on information theoretic criteria can be unified in a conditional likelihood maximization framework, and most algorithms can be reduced to be a specific case of the unified framework (Brown et al., 2012) . In this subsection, we introduce some representative algorithms in this family. First, we show the general forms of the algorithms, and then we show how they can be reduced to the unified framework. Before presenting the detailed algorithms, we first give a brief introduction about basic information theoretic concepts.\nThe first concept is called entropy, it is a measure of the uncertainty of a discrete random variable. The entropy of a discrete random variable X is defined as follows:\nH(X) = \u2212 \u2211\nxi\u2208X\nP (xi)log(P (xi)), (31)\nwhere xi denotes a specific value of random variable X, P (xi) denotes the probability of xi over all possible values of X which can be estimated from the data.\nThe second concept is the conditional entropy of X given another discrete random variable Y :\nH(X|Y ) = \u2211\nyj\u2208Y\nP (yj) \u2211\nxi\u2208X\nP (xi|yj)log(P (xi|yj)), (32)\nwhere P (yi) is the prior probability of yi, while P (xi|yj) is the conditional probability of xi given yj. The measure of conditional entropy shows the uncertainty of X given another discrete random variable Y .\nThe concept of information gain (Shannon, 2001) between X and Y is used to measure their dependency with entropy and conditional entropy. Since it measures the amount of information shared by X and Y together, it is often referred as mutual information, which is calculated as:\nI(X;Y ) =H(X)\u2212H(X|Y )\n= \u2211\nxi\u2208X\n\u2211\nyj\u2208Y\nP (xi, yj)log P (xi, yj) P (xi)P (yj) , (33)\nwhere P (xi, yj) is the joint probability of xi and yj. It can be noticed that information gain is symmetric such that I(X;Y ) = I(Y ;X), and is zero if the discrete variables X and Y are independent.\nSimilar to the concept of entropy, the conditional information gain (or conditional mutual information) of discrete variables X and Y given the third discrete variable Z is given as follows:\nI(X;Y |Z) =H(X|Z)\u2212H(X|Y,Z)\n= \u2211\nzk\u2208Z\nP (zk) \u2211\nxi\u2208X\n\u2211\nyj\u2208Y\nP (xi, yj|zk)log P (xi, yj |zk)\nP (xi|zk)P (yj |zk) . (34)\nIt shows the amount of mutual information shared by X and Y when the third discrete variable Z is given.\nSearching for the global best set of features is NP-hard, thus, most algorithms in this family exploit heuristic sequential search approaches to add/remove features one by one. In this survey, we explain the feature selection problem by forward sequential search such that features are added into the selected feature set one by one. We denote S as the current selected feature set that is initially empty. Y represents the class labels. Xj \u2208 S is a specific feature in the current S. J(.) is a feature selection criterion (score) where, generally, the higher the value of J(Xk), the more important the feature Xk is. In the unified conditional likelihood maximization feature selection framework, the selection criterion (score) for a new unselected feature Xk is given as follows:\nJcmi(Xk) = I(Xk;Y ) + \u2211\nXj\u2208S\ng[I(Xj ;Xk), I(Xj ;Xk|Y )], (35)\nwhere g(.) is a function w.r.t. two variables I(Xj ;Xk) and I(Xj ;Xk|Y ). If g(.) is a nonlinear function w.r.t. these two variables, it is referred as a criterion by linear combinations of Shannon information terms such that:\nJCMI(Xk) = I(Xk;Y )\u2212 \u03b2 \u2211\nXj\u2208S\nI(Xj ;Xk) + \u03bb \u2211\nXj\u2208S\nI(Xj ;Xk|Y ). (36)\nwhere \u03b2 and \u03bb are two nonnegative parameters between zero and one. On the other other hand, if g(.) is a linear function w.r.t. these two variables, it is referred as a criterion by non-linear combinations of Shannon information terms."}, {"heading": "2.2.1 Mutual Information Maximization (or Information Gain) (Lewis, 1992)", "text": "Mutual Information Maximization (MIM)(also known as Information Gain) measures the importance of a feature by its correlation with the class label. It assumes that when a feature has a strong correlation with the class label, it can help achieve good classification performance. The Mutual Information score for a new unselected feature Xk is:\nJMIM (Xk) = I(Xk;Y ). (37)\nIt can be observed that in MIM, the scores of features are assessed individually which is independent of other features. Therefore, in MIM, only the feature correlation is considered\nwhile the feature redundancy property is completely ignored. After it obtains the MIM feature score for all unselected features, we choose the feature with the highest feature score and add it to the selected feature set. The process repeats until the desired number of selected features are obtained.\nIt can also be observed that MIM is a special case of linear combination of Shannon information terms such that:\nJCMI(Xk) = I(Xk;Y )\u2212 \u03b2 \u2211\nXj\u2208S\nI(Xj ;Xk) + \u03bb \u2211\nXj\u2208S\nI(Xj ;Xk|Y ), (38)\nwhere both \u03b2 and \u03bb are equal to zero."}, {"heading": "2.2.2 Mutual Information Feature Selection (Battiti, 1994)", "text": "A limitation of MIM feature selection criterion is that it assumes that features are independent of each other. However, in reality, good features should not only be strongly correlated with class labels, but also should not be highly correlated with each other. In other words, the correlation between features should be minimized. Mutual Information Feature Selection (MIFS) criterion considers both the feature relevance and feature redundancy in the feature selection phase, the feature score for a new unselected feature Xk can be formulated as follows:\nJMIFS(Xk) = I(Xk;Y )\u2212 \u03b2 \u2211\nXj\u2208S\nI(Xk;Xj). (39)\nIn MIFS, the feature relevance of the new feature is evaluated by the first term I(Xk;Y ), while the second term tries to penalize the feature that has a high mutual information with the current selected features such that feature redundancy is minimized. In the original paper, the parameter \u03b2 is empirically set to be one.\nMIFS can also be reduced to be a special case of the linear combination of Shannon information terms:\nJCMI(Xk) = I(Xk;Y )\u2212 \u03b2 \u2211\nXj\u2208S\nI(Xj ;Xk) + \u03bb \u2211\nXj\u2208S\nI(Xj ;Xk|Y ), (40)\nwhere \u03b2 is between zero and one, and \u03bb is set to be zero."}, {"heading": "2.2.3 Minimum Redundancy Maximum Relevance (Peng et al., 2005)", "text": "Unlike MIFS that empirically sets \u03b2 to be one, (Peng et al., 2005) proposed a Minimum Redundancy Maximum Relevance (MRMR) criterion to set the value of \u03b2 the reverse of the number of selected features:\nJMRMR(Xk) = I(Xk;Y )\u2212 1 |S| \u2211\nXj\u2208S\nI(Xk;Xj). (41)\nWith more selected features, the effect of feature redundancy is gradually reduced. The intuition is that with more non-redundant features selected, it is becoming more difficult for new features to be redundant to the features that have already been in S. In (Brown et al.,\n2012), it gives another interpretation that the pairwise independence between features becomes stronger as more features are added to S, possibly because of noise information in the data.\nThe MRMR criterion is strongly linked to the Conditional likelihood maximization framework:\nJCMI(Xk) = I(Xk;Y )\u2212 \u03b2 \u2211\nXj\u2208S\nI(Xj ;Xk) + \u03bb \u2211\nXj\u2208S\nI(Xj ;Xk|Y ), (42)\nif we iteratively revise the value of \u03b2 to be 1|S| , and set the other parameter \u03bb to be zero."}, {"heading": "2.2.4 Conditional Infomax Feature Extraction (Lin and Tang, 2006)", "text": "MIFS and MRMR consider both feature relevance and feature redundancy at the same time. However, some studies Lin and Tang (2006); El Akadi et al. (2008); Guo and Nixon (2009) show that in contrast to minimize the feature redundancy, the conditional redundancy between unselected features and already selected features given class labels should be maximized. In other words, as long as the feature redundancy given class labels is stronger than the intra feature redundancy, the feature selection will be affected negatively. A typical feature selection under this argument is Conditional Infomax Feature Extraction (CIFE) criterion, in which the feature score for a new unselected feature Xk is:\nJCIFE(Xk) = I(Xk;Y )\u2212 \u2211\nXj\u2208S\nI(Xj ;Xk) + \u2211\nXj\u2208S\nI(Xj ;Xk|Y ). (43)\nIt can be observed that compared with MIFS, it adds a third term \u2211 Xj\u2208S I(Xj ;Xk|Y ) to maximize the conditional redundancy. CIFE is also a special case of the linear combination of Shannon information terms:\nJCMI(Xk) = I(Xk;Y )\u2212 \u03b2 \u2211\nXj\u2208S\nI(Xj ;Xk) + \u03bb \u2211\nXj\u2208S\nI(Xj ;Xk|Y ), (44)\nby setting \u03b2 and \u03b3 to be one."}, {"heading": "2.2.5 Joint Mutual Information (Yang and Moody, 1999)", "text": "MIFS and MRMR reduce feature redundancy in the feature selection process. An alternative criterion, Joint Mutual Information (Yang and Moody, 1999; Meyer et al., 2008) was proposed to increase complementary information that are shared between new unselected feature and selected features given the class labels. The feature selection criterion is listed as follows:\nJJMI(Xk) = \u2211\nXj\u2208S\nI(Xk,Xj ;Y ). (45)\nThe basic idea of JMI is that we should include new features that are complementary to the existing features given the class labels.\nUnlike previous mentioned approaches that can be directly represented by the linear combination of Shannon information terms, JMI can not be directly reduced to the condition\nlikelihood maximization framework. In (Brown et al., 2012), the authors demonstrate that with simple manipulations, the JMI criterion can be re-written as:\nJJMI(Xk) = I(Xk;Y )\u2212 1 |S| \u2211\nXj\u2208S\nI(Xj ;Xk) + 1 |S| \u2211\nXj\u2208S\nI(Xj ;Xk|Y ). (46)\nTherefore, it is also a special case of the linear combination of Shannon information terms by iteratively setting \u03b2 and \u03bb to be 1|S| ."}, {"heading": "2.2.6 Conditional Mutual Information Maximization (Fleuret, 2004)", "text": "Previously mentioned information theoretic feature selection criterion can be reduced to a linear combination of Shannon information terms. Next we show some other algorithms that can only reduce to a non-linear combination of Shannon information terms. Among them, Conditional Mutual Information Maximization (CMIM) (Vidal-Naquet and Ullman, 2003; Fleuret, 2004) is a criterion which iteratively selects features which maximize the mutual information with the class labels given the selected features so far. In other words, CMIM does not select the feature that is similar to previous selected ones even though its predictable power for the class labels is strong. Mathematically, during the selection phase, the feature score for each new unselected feature Xk can be formulated as follows:\nJCMIM (Xk) = min Xj\u2208S\n[I(Xk;Y |Xj)]. (47)\nNote that the value of I(Xk;Y |Xj) is small if Xk is not strongly correlated with the class label Y or if Xk is redundant when S is known. By selecting the feature that maximizes this minimum value, it can guarantee that the selected feature has strong predictive ability as well as reducing redundancy to the selected features.\nThe CMIM criterion is equivalent to the following form after some derivations:\nJCMIM (Xk) = I(Xk;Y )\u2212 max Xj\u2208S [I(Xj ;Xk)\u2212 I(Xj ;Xk|Y )]. (48)\nTherefore, CMIM is also a special case of the conditional likelihood maximization framework:\nJcmi(Xk) = I(Xk;Y ) + \u2211\nXj\u2208S\ng[I(Xj ;Xk), I(Xj ;Xk|Y )]. (49)"}, {"heading": "2.2.7 Informative Fragments (Vidal-Naquet and Ullman, 2003)", "text": "In (Vidal-Naquet and Ullman, 2003), the authors propose a feature selection criterion called Informative Fragments (IG). The feature score of each new unselected features is given as:\nJIF (Xk) = min Xj\u2208S\n[I(XjXk;Y )\u2212 I(Xj ;Y )]. (50)\nThe intuition behind Informative Fragments is that the addition of the new feature Xk should maximize the value of conditional mutual information between Xk and existing features in S over the mutual information between Xj and Y . An interesting phenomenon of Informative Fragments is that with the chain rule that I(XkXj;Y ) = I(Xj ;Y ) + I(Xk;Y |Xj), Informative Fragments has the equivalent form as CMIM, therefore, it can also be reduced to the conditional likelihood maximization framework."}, {"heading": "2.2.8 Interaction Capping (Jakulin, 2005)", "text": "Interaction Capping is a similar feature selection criterion as CMIM in Eq. (48), but instead of restricting the term I(Xj ;Xk)\u2212 I(Xj ;Xk|Y ) to be nonnegative:\nJCMIM (Xk) = I(Xk;Y )\u2212 \u2211\nXj\u2208S\nmax[0, I(Xj ;Xk)\u2212 I(Xj ;Xk|Y )]. (51)\nApparently, it is a special case of non-linear combination of Shannon information terms by setting the function g(.) to be max[0, I(Xj ;Xk)\u2212 I(Xj ;Xk|Y )]."}, {"heading": "2.2.9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006)", "text": "Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al., 2008):\nJDISR(Xk) = \u2211\nXj\u2208S\nI(XjXk;Y ) H(XjXkY ) . (52)\nIt is easy to validate that DISR is a non-linear combination of Shannon information terms and can be reduced to the conditional likelihood maximization framework."}, {"heading": "2.2.10 Fast Correlation Based Filter (Yu and Liu, 2003)", "text": "There are other information theoretical based feature selection methods that can not be simply be reduced to the unified conditional likelihood maximization framework. Here, we introduce one algorithm named Fast Correlation Based Filter (FCBF) (Yu and Liu, 2003). It is a filter method that exploits feature-class correlation and feature-feature correlation simultaneously. The algorithm works as follows: (1) given a predefined threshold \u03b4, it selects a subset of features S that is highly correlated with the class label with SU \u2265 \u03b4, where SU is the symmetric uncertainty, the SU between a set of features XS and the class label Y is given as follows:\nSU(XS , Y ) = 2 I(XS ;Y )\nH(XS) +H(Y ) . (53)\nA specific feature Xk is called predominant iff SU(Xk, Y ) \u2265 \u03b4 and there does not exist a feature Xj \u2208 S(j 6= k) such that SU(Xj ,Xk) \u2265 SU(Xk, Y ). Feature Xj is considered to be redundant to feature Xk if SU(Xj ,Xk) \u2265 SU(Xk, Y ); (2) the set of redundant features is denoted as SPi , which will be further split into S+Pi and S \u2212 Pi\nwhere they contain redundant features to feature Xk with SU(Xj , Y ) > SU(Xk, Y ) and SU(Xj , Y ) < SU(Xk, Y ), respectively; and (3) different heuristics are applied on SP , S+Pi and S \u2212 Pi\nto remove reundant features and keep the features that are most relevant features to the class label. Different from feature weighting methods that assign a score to each feature and select the features with the highest score, FCBF is a subset search algorithm which cannot determine the number of selected features."}, {"heading": "2.3 Sparse Learning based Methods", "text": "Filter feature selection methods select features that are independent of any learning algorithms. However, they do not take into account the bias of the learning algorithms such that the selected features may not be optimal for a specific learning task. To tackle this issue, embedded methods embed the feature selection phase into the learning algorithm construction where these two phases compliment each other. The selected features are suitable for that learning algorithm which can be used for further analysis. There are three main types of embedded feature selection methods: The first type of embedded methods are pruning methods. At the very beginning, they use the whole set of features to train a learning model and then attempt to remove some features by setting the feature coefficients to zero, while maintaining the model performance, one example method in this category is recursive feature elimination methods using support vector machine (SVM) (Guyon et al., 2002). The second type of embedded methods contain a built-in feature selection mechanism such as ID3 (Quinlan, 1986) and C4.5 (Quinlan, 1993). The third type of methods are sparse learning based methods which aim to minimize the fitting errors along with some sparse regularization terms. The sparse regularizer forces some feature coefficients to be small or exactly zero, and then the corresponding features can be simply eliminated. Sparse learning based methods have received great attention in recent years due to their good performance and interpretability. In the following subsections, we review the sparse learning based feature selection methods in both supervised and unsupervised perspective. We first cover some representative supervised sparse learning based methods and then introduce some unsupervised sparse learning based methods."}, {"heading": "2.3.1 Feature Selection with \u21131-norm Regularizer (Supervised) (Tibshirani,", "text": "1996; Hastie et al., 2015)\nFirst, we consider the binary classification (yi is either 0 or 1) or regression problem with only one regression target. Without loss of generality, we only consider linear classification or linear regression model, but it can be easily extended to non-linear problems. The classification label or regression target y can be considered as a linear combination of data instances X like SVM (Cortes and Vapnik, 1995) and logistic regression (Hosmer Jr and Lemeshow, 2004). To achieve feature selection, the \u21131-norm penalty term is added on the classification or regression model. One main advantage of \u21131-norm regularization (Lasso) (Tibshirani, 1996; Hastie et al., 2015) is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero. This property makes it suitable for feature selection, as we can select features with corresponding non-zero coefficients. Specifically, let w denote the model parameter (feature coefficient) that can be obtained by solving following optimization problem:\nw = argmin w loss(w;X,y) + \u03b1 penalty(w), (54)\nwhere loss(.) is a loss function which denotes a classification or regression model, penalty(w) is a sparse regularization term for feature selection, and \u03b1 is a regularization parameter to balance the contribution of the loss function and the regularization term.\nSome widely used loss functions loss(.) include least squares, hinge loss and logistic loss. They are defined as follows:\n\u2022 Least Square Loss:\nloss(w;X,y) =\nn \u2211\ni=1\n(yi \u2212w\u2032xi)2, (55)\n\u2022 Hinge Loss:\nloss(w;X,y) =\nn \u2211\ni=1\nmax(0, 1 \u2212 yiw\u2032xi), (56)\n\u2022 Logistic Loss:\nloss(w;X,y) = n \u2211\ni=1\nlog(1 + exp(\u2212yiw\u2032xi)), (57)\nAs mentioned above, the most widely used sparse regularization is \u21131-norm regularization, but there are also different types of sparse regularization such as adaptive lasso and elastic net. Next, we briefly introduce these sparse regularization terms.\n\u2022 Lasso Regularization (Tibshirani, 1996): Lasso is short for least absolute shrinkage and selection operator, it is based on the \u21131-norm regularization term on the feature coefficient w:\npenalty(w) = ||w||1 = d \u2211\ni=1\n|wi|. (58)\nThe \u21131-norm regularization term forces some feature coefficient to be zero and the corresponding features can be simply eliminated since the elimination of these features will not greatly affect the learning performance. After obtaining the feature weight w by some optimization algorithms, we can sort the feature importance according to the feature weight \u2013 the higher the feature weight, the more important the feature is.\n\u2022 Adaptive Lasso Regularization (Zou, 2006): The Lasso variable selection phase is consistent if it satisfies non-trivial solutions. However, this condition is difficult to satisfy in some scenarios (Zhao and Yu, 2006). Another critical issue of Lasso is that the lasso shrinkage produces biased estimates for the large coefficients, and thus it could be suboptimal in terms of estimation risk (Fan and Li, 2001). To tackle these problems, the adaptive Lasso regularization is proposed:\npenalty(w) =\nd \u2211\ni=1\n|wi| bi , (59)\nwhere b is a given weight vector to control the contribution of each feature coefficient in the \u21131-norm penalty term. It is easy to see that adaptive Lasso is a weighted version of Lasso. In (Zou, 2006), the authors show that the adaptive lasso enjoys the oracle properties and can be solved by the same efficient algorithm for solving the Lasso.\n\u2022 Elastic Net Regularization (Zou and Hastie, 2005): In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications. In addition, in many applications such as bioinformatics,\nimage processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other. However, Lasso tends to randomly select features from a group and discards the others. To handle features with high correlations, Elastic Net regularization (Zou and Hastie, 2005) is proposed as:\npenalty(w) = d \u2211\ni=1\n|wi|\u03b3 + ( d \u2211\ni=1\nw2i ) \u03bb, (60)\nwith 0 < \u03b3 \u2264 1 and \u03bb \u2265 0. The parameters \u03b3 and \u03bb are usually set to be 1 such that Elastic Net regularization is simply a combination of \u21131-norm and \u21132-norm regularization.\nIn addition to the different variations of Lasso regularization, another way to obtain a sparse representation of feature coefficients w is Dantzig selector; it is based on the normal score equations and controls the correlation of residuals with X as:\nmin w ||w||1 s.t.||X\u2032(y \u2212Xw)||\u221e \u2264 \u03bb,\n(61)\nwhere ||.||\u221e denotes the infinity norm of a vector. Dantzig selector was designed for linear regression models. In (Candes and Tao, 2007; James et al., 2009), the authors show that the errors of Dantzig selector is up to a logarithmic factor log(d) (d is the feature dimensionality). Strong theoretical results in (James et al., 2009) show that LASSO and Dantzig selector are closely related.\n2.3.2 Feature Selection with \u21132,1-norm Regularizer (Supervised) (Liu et al., 2009b)\nAs mentioned above, for binary classification and regression with one target, we can achieve feature selection via \u21131-norm regularization since it will force some feature coefficient to be exact zero. Here, we discuss how to perform feature selection for the general classification problems. The problem is more difficult because of multiple classification or regression targets since we would like the feature selection phase to be consistent over multiple targets. In other words, we want multiple predictive models for different targets to share the similar parameter sparsity patterns \u2013 each feature either has small scores for all data points or has large scores over all data points. This problem can be solved by the \u21132,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008). Similar to \u21131-norm regularization, \u21132,1-norm regularization is also convex and a global optimal solution can be achieved. The \u21132,1-norm regularization has strong connections with group lasso (Yuan and Lin, 2006) which will be explained later.\nAssume that X denotes the data matrix, and y denotes the label vector such that it contains s different class labels {c1, c2, ..., cs}. First, we can transform the feature vector y to one-hot label matrix Y such that if yi = cj then the only the j-th element in the corresponding row vector Y(i, :) is 1, other elements are 0. We further assume that the\nlinear classification problem is parameterized by a weight matrix W such that the j-th column of W contains the feature coefficient for the j-th class label. If the least square loss function is specified, then the model is formulated as follows:\nmin W\n||XW \u2212Y||2F + \u03b1||W||2,1, (62)\nwhere the parameter \u03b1 is used to control the contribution from the loss function and the regularization term. By solving this optimization problem, we can obtain a sparse matrix W where many rows are exact zero or small numbers. The features corresponding to these rows can then be eliminated. By selecting a few number of rows, it achieves joint feature selection among different classification labels.\nSimilar to Lasso, we can also exploit different loss functions such as hinge loss and logistic loss. Meanwhile, the \u21132,1-norm regularization can also be modified as adaptive Lasso and elastic net. Generally, the \u21132,1-norm regularization problem can be solved efficiently by the state-of-the-art proximal gradient descent methods (Liu et al., 2009a). After we obtain the feature coefficient matrix W \u2208 Rd\u00d7s, we can compute the \u21132-norm of each row vector ||W(i, :)||2 which corresponds to the i-th feature \u2013 the larger the value of the \u21132-norm, the more important the feature is."}, {"heading": "2.3.3 Efficient and Robust Feature Selection (Supervised) (Nie et al., 2010)", "text": "In (Nie et al., 2010), the authors propose an efficient and robust feature selection (REFS) method by employing a joint \u21132,1-norm minimization on both the loss function and the regularization. Their argument is that the \u21132-norm based loss function is sensitive to noisy data while the \u21132,1-norm based loss function is more robust to noise. The reason is that \u21132,1norm loss function has a rotational invariant property (Ding et al., 2006). Consistent with \u21132,1-norm regularized feature selection model, a \u21132,1-norm regularizer is added to the \u21132,1norm loss function to achieve group sparsity. The objective function of REFS is formulated as follows:\nmin W\n||XW \u2212Y||2,1 + \u03b1||W||2,1, (63)\nThe objective function of REFS is difficult to solve since both terms are convex but nonsmooth. In (Nie et al., 2010), an efficient algorithm is proposed to solve this optimization problem with strict convergence analysis."}, {"heading": "2.3.4 Multi-Cluster Feature Selection (Unsupervised) (Cai et al., 2010)", "text": "Most of existing sparse learning based approaches build a learning model with the supervision of class labels. The feature selection phase is derived afterwards on the sparse feature coefficients. However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015). Multi-Cluster Feature Selection (MCFS) (Cai et al., 2010) is one of the first unsupervised feature selection algorithm using sparse learning techniques. Without class labels to guide the feature selection process, MCFS proposes to select features that can cover the multi-cluster structure of the data where spectral analysis is used to measure the correlation between different features.\nMCFS consists of three steps: (1) the spectral clustering step, (2) sparse coefficient learning step and (3) feature selection step. In the first step, spectral clustering (Chan et al., 1994; Ng et al., 2002) is applied on the dataset to detect the cluster structure of the data. It first constructs a k-nearest neighbor graph to capture the local geometric structure of the data and gets the graph affinity matrix S, where k is a predefined parameter. There are many different ways to define the affinity matrix. Typical ways include 0-1 weighting, heart kernel weighting and dot-product weighting. For example, if the affinity matrix is built by the heart kernel weighting scheme, S is computed as follows:\nS(i, j) = e \u2212||xi\u2212xj ||\n2\n\u03c3 , (64)\nwhere xi and xj are connected instances in the k-nearest neighbor graph and the bandwidth \u03c3 is a predefined parameter. Then, the graph Laplacian matrix is defined as L = D \u2212W, where D is a diagonal matrix with its diagonal element D(i, i) = \u2211\nj W(i, j). With the graph Laplacian matrix, the flat embedding that unfolds the data manifold can be obtained by solving the following generalized eigen-problem:\nLe = \u03bbDe. (65)\nLet E = {e1, e2, ..., eK} denote the eigenvectors of the above eigen-problem w.r.t. the smallest K eigenvalues. Each column of E, i.e., ei denotes the i-th embedding of the data X, and K denotes the intrinsic dimensionality of the data that is usually set to the number of clusters if the number of clusters is known in advance.\nIn the second step, since the embedding of the data X is known, MCFS takes advantage of them to measure the importance of a feature by a regression model with a \u21131-norm regularization. Specifically, given the i-th embedding ei, MCFS regards it as a regression target to minimize the following objective function:\nmin wi\n||Xwi \u2212 ei||22 + \u03b1||wi||1, (66)\nwhere wi denotes the feature coefficient vector for the i-th embedding. By solving all K sparse regression problems, MCFS obtains K sparse feature coefficient vectors W = [w1, ...,wK ] and each vector corresponds to one embedding of X. In the third step, for each feature fj, the MCFS score for that feature can be computed as:\nMCFS(j) = max i\n|W(j, i)|. (67)\nThe higher the MCFS score, the more important the feature is."}, {"heading": "2.3.5 \u21132,1-norm Regularized Discriminative Feature Selection (Unsupervised) (Yang et al., 2011)", "text": "To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010). An alternative way is to exploit the discriminative information encoded in the data that has been proven to be effective in many learning tasks (Fukunaga, 2013). In (Yang et al., 2011), the authors propose a new unsupervised feature selection algorithm\n(UDFS) to select the most discriminative features by exploiting both the discriminative information and feature correlations.\nFirst, we briefly introduce discriminative information. Suppose n instances come from s classes and there are ni instances in the i-th class. Y \u2208 {0, 1}n\u00d7s denotes the class label matrix for n instances such that Y(i, j) = 1 if xi belongs to the j-th class, otherwise Y(i, j) = 0. Let Hn = In \u2212 1n1n1\u2032n, then the total scatter matrix St and between class scatter matrix Sb are defined as follows:\nSt =\nn \u2211\ni=1\n(xi \u2212 \u00b5)(xi \u2212 \u00b5)\u2032 = X\u0303\u2032X\u0303\nSb = c \u2211\ni=1\nni(\u00b5i \u2212 \u00b5)(\u00b5i \u2212 \u00b5)\u2032 = X\u0303\u2032GG\u2032X\u0303, (68)\nwhere \u00b5 = x1+...+xn n is the mean of all data instances, \u00b5i is the mean of all instances in the i-th class, X\u0303 is the centered data matrix such X\u0303 = HnX and G = [G1,G1, ...,Gn] \u2032 = Y(Y\u2032Y)\u2212 1 2 is the weighted label indicator matrix. Linear discriminant analysis aims to obtain a linear transformation matrix W \u2208 Rd\u00d7s that projects X from a d-dimensional space to a low dimensional space such that St is minimized and Sb is maximized.\nInstead of using global discriminative information, authors in (Yang et al., 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al., 2010) to select discriminative features. The advantage of using local discriminative information are two folds. First, it has been demonstrated to be more important than global discriminative information in many classification and clustering tasks. Second, when it considers the local discriminative information, the data manifold structure is also taken into account. For each data instance xi, it constructs a k-nearest neighbor set for that instance Nk(xi) = {xi1 , xi2 , ..., xik}. Let XNk(i) = [xi,xi1 , ...,xik ] denote the local data matrix around xi, then the local total scatter matrix S\n(i) t and local between class scatter matrix S (i) b are defined as:\nS (i) t = X\u0303i \u2032 X\u0303i S (i) b = X\u0303i \u2032 GiG \u2032 iX\u0303i,\n(69)\nwhere X\u0303i = Hk+1Xi and G(i) = [Gi,Gi1 , ...,Gik ] \u2032. Note that G(i) is a subset from G and G(i) can be obtained by a selection matrix Pi \u2208 {0, 1}n\u00d7(k+1) such that G(i) = P\u2032(i)G. Without label information in unsupervised feature selection, UDFS assumes that there is a linear classifier W \u2208 Rd\u00d7s to map each data instance xi \u2208 Rd to a low dimensional space Gi \u2208 Rs. Following the definition of global discriminative information (Yang et al., 2010; Fukunaga, 2013), the local discriminative score for each instance xi is computed as:\nDSi = tr[(S (i) t + \u03bbId) \u22121S (i) b ]\n= tr[W\u2032X\u2032P(i)X\u0303i \u2032 (X\u0303iX\u0303i \u2032 + \u03bbId) \u22121X\u0303iP \u2032 (i)XW],\n(70)\nwhere \u03bb is a small number to make X\u0303iX\u0303i \u2032 invertible. If the instance has a high local discriminative score, it indicates that the linear classifier W can discriminate these instances\nwell. Therefore, UDFS tends to trainW which obtains the highest local discriminative score for all instances in X; it incorporates a \u21132,1-norm regularizer to achieve feature selection, the objective function is formulated as follows:\nmin W\u2032W=Id\nn \u2211\ni=1\n{tr[G\u2032(i)Hk+1G(i) \u2212DSi]}+ \u03b1||W||2,1, (71)\nwhereG\u2032(i)Hk+1G(i) is added to avoid overfit and ||W||2,1 will make W to be sparse in rows, which is suitable for feature selection, \u03b1 is a regularization parameter to control the sparsity of the model. With some mathematical derivation, the objective function in Eq. (71) can be reformulated as follows:\nmin W\u2032W=Id\ntr(W\u2032MW) + \u03b1||W||2,1, (72)\nwhere M = X\u2032[ \u2211n i=1(PiHk+1(X\u0303iX\u0303i + \u03bbIk+1) \u22121Hk+1S \u2032 i)]X. After we obtain the sparse coefficient matrix, we can rank the features according to its \u21132-norm value and return the top ranked ones."}, {"heading": "2.3.6 Feature Selection Using Nonnegative Spectral Analysis (Unsupervised) (Li et al., 2012)", "text": "In addition to UDFS, there are some other ways to exploit discriminative information for unsupervised feature selection. Nonnegative Discriminative Feature Selection (NDFS) (Hou et al., 2011) is an algorithm which performs spectral clustering and feature selection simultaneously in a joint framework to select a subset of discriminative features. NDFS assumes that pseudo class label indicators can be obtained by spectral clustering techniques. Different from most existing spectral clustering techniques, NDFS imposes nonnegative and orthogonal constraints during the spectral clustering phase. The argument is that with these constraints, the learnt pseudo class labels are more close to real cluster results. These nonnegative pseudo class labels then act as regression constraints to guide the feature selection phase. Instead of performing these two tasks separately, NDFS incorporates these two phases into a joint framework.\nSuppose that these n data instances come from s classes {c1, ..., cs} and Y \u2208 {0, 1}n\u00d7s denotes the class label matrix such that Y(i, j) = 1 if xi belongs to j-th class, otherwise Y(i, j) = 0. Similar to the definition in UDFS, we use G = [G1,G1, ...,Gn] \u2032 = Y(Y\u2032Y)\u2212 1 2 to denote the weight cluster indicator matrix. It is easy to show that for the weight cluster indicator matrix G, we have GG\u2032 = In. NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved (Shi and Malik, 2000; Yu and Shi, 2003). Since the local geometric structure of the dataset can be modeled by a k-nearest neighbor graph, the affinity matrix S is defined as:\nS(i, j) = e \u2212||xi\u2212xj ||\n2\n\u03c3 . (73)\nxi and xj are connected instances in the k-nearest neighbor graph, where the bandwidth \u03c3 is a predefined parameter. Then the local geometric structure can be fully used by minimizing\nthe following normalized graph Laplacian:\nmin G\n1\n2\nn \u2211\ni=1\nn \u2211\nj=1\nS(i, j)|| Gi\u221a D(i, i) \u2212 Gj\u221a D(j, j) ||22\n= tr(G\u2032LG),\n(74)\nwhereD is a diagonal matrix with its diagonal elementD(i, i) = \u2211n j=1 S(i, j), L = D \u2212 1 2 (D\u2212 S)D\u2212 1 2 is the normalized Laplacian matrix. Given the pseudo labels G, NDFS assumes that there exists a linear transformation matrix W \u2208 Rd\u00d7s between the data instances X and pseudo labels G. These pseudo class labels are utilized as constraints to guide the feature selection process by minimizing the following objective function:\nmin W ||XW \u2212G||2F + \u03b1||W||2,1 s.t. GG\u2032 = In,G \u2265 0,\n(75)\nwhere \u03b1 is a parameter to control the sparsity of the model. ||W||2,1 ensures that NDFS achieves group sparsity among all s psuedo class labels. Note that NDFS imposes a nonnegative constraint on G because with both the orthogonal and nonnegative constraint, there is only one positive element in each row of G and all the other elements are zero. In this way, the learnt pseudo class labels G are more accurate and can better help to select discriminative features.\nBy combining the objective functions in Eq. (74) and Eq. (75), the objective function of NDFS is formulated as follows:\nmin G,W\ntr(G\u2032LG) + \u03b2(||XW \u2212G||2F + \u03b1||W||2,1)\ns.t. GG\u2032 = In,G \u2265 0. (76)\nThe objective function of NDFS in Eq. (76) can be solved in an alternating way. After obtaining the feature coefficient matrix W, NDFS takes the same strategy as UDFS to rank the features according to its \u21132-norm value in W and return the top ranked ones."}, {"heading": "2.3.7 Feature selection via joint embedding learning and sparse regression (Unsupervised) (Hou et al., 2011)", "text": "Feature selection via joint embedding learning and sparse regression (JELSR) (Hou et al., 2011) is an unsupervised feature selection that is similar to NDFS. They both embed the pseudo class label learning process into the sparse regression for feature selection. The difference is that NDFS uses graph Laplacian to learn the pseudo class labels, while JELSR utilizes local linear embedding. In addition, NDFS imposes both orthogonal and nonnegative constraints on the pseudo class labels while JELSR only considers the orthogonal constraint.\nIn the first step, JELSR builds a k-nearest neighbor graph. In the second step, instead of using some explicit affinity matrix S like MCFS and NDFS, JELSR takes advantage of the local linear embedding (Roweis and Saul, 2000) to learn the local approximation matrix, i.e., affinity matrix. Specifically, if the i-th instance xi and the j-th instance xj\nare connected in the k-nearest neighbor graph; its entry in the affinity matrix is positive S(i, j) > 0, otherwise S(i, j) = 0. The nonzero entry can be learnt by solving the following optimization problems:\nmin S\nn \u2211\ni=1\n||xi \u2212 n \u2211\nj=1\nS(i, j)xj ||22\ns.t.\nn \u2211\nj=1\nS(i, j) = 1.\n(77)\nThe basic idea is that each instance in X can be approximated by a linear combination of some of its k-nearest neighbors. JELSR assumes that the low dimensional representation, i.e., pesudo class labels, shares the same local structure as the instances X in the high dimensional space, therefore it aims to minimize the following:\nmin G\nn \u2211\ni=1\n||Gi \u2212 n \u2211\nj=1\nS(i, j)Gj ||22 = tr(G\u2032LG)\ns.t. GG\u2032 = In.\n(78)\nBy integrating the embedding learning phase into the feature selection phase, the objective function of JELSR is formulated as follows:\nmin G,W\ntr(G\u2032LG) + \u03b2(||XW \u2212G||2F + \u03b1||W||2,1)\ns.t. GG\u2032 = In. (79)\nwhere \u03b1 and \u03b2 are two balance parameters. The \u21132,1-norm regularization term ensures that W is sparse in rows. Similar to UDFS and NDFS, after deriving W by some optimization algorithms, it uses \u21132-norm, i.e., W(i, :) to rank features. The higher ranked features are relatively more important."}, {"heading": "2.4 Statistical based Methods", "text": "Another category of feature selection algorithms is based on different statistical measures; we group them as statistical based methods in this survey. Since they rely on some statistical measures instead of learning algorithms to evaluate the relevance of features, most of them are filter based methods. In addition, most statistical based algorithms analyze features individually. Hence, feature redundancies is inevitably ignored during the selection phase. We introduce some representative feature selection algorithms in this category. Note that the vast majority algorithms in this category works with discrete data, numerical datasets have to perform discretization first."}, {"heading": "2.4.1 Low Variance (Pedregosa et al., 2011) (Unsupervised)", "text": "Low Variance is a simple feature selection algorithm which eliminates the feature whose variance is below some threshold. For example, for the features that have the same values on all instances, the variance is 0 and should be removed since it cannot help to discriminate instances from different classes. Suppose that the dataset consists of only boolean features,\ni.e., the feature values are either 0 and 1, since the boolean features are Bernoulli random variables, its variance value can be computed as:\nvariance score(fi) = p(1\u2212 p), (80) where p denotes the percentage of instances that take the feature value of 1. After the variance of features are obtained, the feature with a variance score below a predefined threshold can be directly pruned."}, {"heading": "2.4.2 T-score (Davis and Sampson, 1986) (Supervised)", "text": "t-score is used for binary classification problems. For each feature fi, suppose that \u00b51 and \u00b52 are the mean feature values for the instances from the first class and the second class, \u03c31 and \u03c32 are the corresponding standard deviation values, n1 and n2 denote the number of instances from these two classes. Then the t-score for the feature fi can be computed as:\nt score(fi) = |\u00b51 \u2212 \u00b52| \u221a\n\u03c321 n1 + \u03c322 n2\n. (81)\nThe basic idea of t-score is to assess whether the feature can make the means of two classes to be different statistically by computing the ratio between the mean difference and the variance of two classes. Usually, the higher the t-score, the more important the feature is."}, {"heading": "2.4.3 F-score (Wright, 1965) (Supervised)", "text": "t-score can only be applied for binary classification task, therefore it has some limitations. f -score can handle the multi-class situation by testing if a feature is able to well separate samples from different classes. Considering both the within class variance and between class variance, the f -score can be computed as follows:\nf score(fi) =\n\u2211 j nj c\u22121(\u00b5j \u2212 \u00b5)2\n1 n\u2212c\n\u2211 j(nj \u2212 1)\u03c32j . (82)\nGiven feature fi, nj, \u00b5, \u00b5j , \u03c3j denote the number of instances from class j, the mean feature value, the mean feature value on class j, the standard deviation of feature value on class j, respectively. Similar to t-score, the higher the t-score, the more important the feature is."}, {"heading": "2.4.4 Chi-Square Score (Liu and Setiono, 1995) (Supervised)", "text": "Chi-square score utilizes the test of independence to assess whether the feature is independent of the class label. Given a particular feature with r different feature values, the Chi-square score of that feature can be computed as:\nChi square score(fi) =\nr \u2211\nj=1\nc \u2211\ns=1\n(njs \u2212 \u00b5js)2 \u00b5js , (83)\nwhere njs is the number of instances with the j-th feature value given feature fi. In addition, \u00b5js = n\u2217snj\u2217 n\n, where nj\u2217 indicates the number of data instances with the j-th feature value given feature fi, n\u2217s denotes the number of data instances in class r. A higher Chi-square score indicates that the feature is relatively more important."}, {"heading": "2.4.5 Gini Index (Gini, 1912) (Supervised)", "text": "Gini index is a statistical measure to quantify if the feature is able to separate instances from different classes. Given a feature fi with r different feature values, for the j-th feature value, let W denote the set of instances with the feature value smaller than or equal to the j-th feature value, let W denote the set of instances with the feature value larger than the j-th feature value. In other words, the j-th feature value can separate the dataset into W and W, then the Gini index score for the feature fi is given as follows:\ngini index score(fi) = min W\n(\np(W)(1 \u2212 c \u2211\ns=1\np(Cs|W)2) + p(W)(1\u2212 c \u2211\ns=1\np(Cs|W)2) ) , (84)\nwhere Cs indicates that the class label is s. p(.) denotes the probability, for instance, p(Cs|W) indicates the conditional probability of class s given the set of W. In Eq. (84), the gini index score is obtained by going through all the possible split W. Usually for binary classification problem, it can take a maximum value of 0.5, but it can also be used in multi-classification problems. Unlike previous statistical measures, the lower the gini index value, the more relevant the feature is."}, {"heading": "2.4.6 CFS (Hall and Smith, 1999) (Supervised)", "text": "The basic idea of CFS is to use a correlation based heuristic to evaluate the worth of feature subset F :\nCFS score(F) = krcf\u221a k + k(k \u2212 1)rff , (85)\nwhere the CFS score shows the heuristic \u201cmerit\u201d of the feature subset F with k features. rcf is the mean feature class correlation and rff is the average feature-feature intercorrelation. In Eq. (85), the numerator indicates the predictive power of the feature set while the denominator shows how much feature redundancy the feature set has. The basic idea of CFS is that a good feature subset should have strong correlation with class labels and are weakly intercorrelated. In order to get the feature-class correlation and feature-feature correlation, CFS uses symmetrical uncertainty (Vetterling et al., 1992) to estimate the degree of associations between two attributes. Since finding the global optimal feature subset is computational prohibitive, CFS adopts a best-search strategy to find a local optimal feature subset. At the very beginning, it computes the utility of each feature by considering both feature-class and feature-feature correlation with the symmetrical uncertainty measure. It then starts with an empty set and expand the set by the feature with the highest utility until it satisfies a stopping criteria."}, {"heading": "3. Feature Selection with Structure Features", "text": "Existing feature selection methods for generic data are based on a strong assumption that features are independent of each other while completely ignoring the intrinsic structures among features. For example, these feature selection methods may select the same subset of features even though the features are reshuffled (Ye and Liu, 2012). However, in many real applications features also exhibit various kinds of structures, e.g., spatial or temporal smoothness, disjoint groups, overlap groups, trees and graphs (Tibshirani et al., 2005;\nJenatton et al., 2011; Yuan et al., 2011; Huang et al., 2011; Zhou et al., 2012). With the existence of these intrinsic feature structures, feature selection algorithms which incorporate knowledge about the structures of features may help select more relevant features and therefore improve some post-learning tasks such as classification and clustering. One motivating example is from bioinformatics. In the study of arrayCGH such features (the DNA copy numbers along the genome) have some natural spatial order, incorporating such spatial structure can help select more meaningful features and achieve more accurate classification accuracy. Therefore, we discuss some representative feature selection algorithms which explicitly consider feature structures. Specifically, we will focus on group structure, tree structure and graph structure.\nSince most existing algorithms in this family are supervised algorithm for binary classification and regression task. Without loss of generality, we focus on linear classification or regression problems such that the class label or regression target y can be considered as a linear combination of data instances X, the linear combination (feature coefficient) is encoded in a vector w. A popular and successful approach to achieve feature selection with structural features is to minimize a empirical error penalized by a structural regularization term, which can be formulated as:\nw = argmin w\nloss(w;X,y) + \u03b1 penalty(w,G), (86)\nwhere G denotes the structures among features and \u03b1 is a trade-off parameter between the loss function and the sparse regularization term. To achieve feature selection, penalty(w,G) is usually set to be a sparse regularization term. Note that the above formulation is similar to that in Eq. (54), the only difference is that for feature selection with structural features, we explicitly consider the structural information G among features in the sparse regularization term."}, {"heading": "3.1 Feature Selection with Group Feature Structures", "text": "In many real-world applications, features exhibit group structures. One of the most common example is that in multifactor analysis-of-variance (ANOVA), each factor is associated with several groups and can be expressed by a set of dummy features (Yuan and Lin, 2006). Some other examples include different frequency bands represented as groups in signal processing (McAuley et al., 2005) and genes with similar functionalities acting as groups in bioinformatics (Ma et al., 2007). Therefore, when performing feature selection, it is more appealing to explicitly take into consideration the group structure among features."}, {"heading": "3.1.1 Group Lasso (Supervised) (Yuan and Lin, 2006)", "text": "Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem. In other words, it selects or does not select a group of features as a whole. The difference between Lasso and Group Lasso is shown by the toy example in Figure (9). Assume that a total of 25 features come from 4 different groups G = {G1, G2, G3, G4} (each column denotes a feature) and there is no overlap between these groups. Considering the explicit feature structure, we can reorganize the feature coefficients for these 25 features into 4 parts w = {w1;w2;w3;w4}, where wi denotes the feature coefficients for the features from the ith group Gi. Lasso completely ignores the group structures among features and the selected features (dark column) are across four different groups. In contrast, Group Lasso tends to select or not select the features from different groups as a whole. In the toy example, Group Lasso only selects the second and the fourth group G2 and G4, features in other two groups G1 and G3 are not selected. Mathematically, Group Lasso first uses a \u21132-norm regularization term for feature coefficients wi in each group Gi, then it performs a \u21131-norm regularization for previous \u21132-norm terms. The objective function of group lasso is formulated as follows:\nmin w loss(w;X,y) + \u03b1\ng \u2211\ni=1\nhi||wGi ||2, (87)\nwhere g is the total number of groups, hi is a weight for the i-th group wGi which can be considered as a prior to measure the contribution of the i-th group in the feature selection process. Through optimizing Eq. (89), we obtain the feature coefficients for all the features, these feature coefficients are ranked in a descending order, the higher the value, the more the important the feature is.\n3.1.2 Sparse Group Lasso (Supervised) (Friedman et al., 2010; Peng et al., 2010)\nOnce Group Lasso selects a group, all the features in the group will be selected. However, for some applications that require the diversity of selected features, Group Lasso is not appropriate anymore. In other words, it is desirable to consider the intrinsic feature structures and select features from different selected groups simultaneously. Sparse Group Lasso (Friedman et al., 2010) takes advantage of both Lasso and Group Lasso, and it produces a solution with simultaneous intra-group and inter-group sparsity. The sparse regularization term of Sparse Group Lasso is a combination of the penalty term of Lasso\nand Group Lasso. The formulation of Sparse Group Lasso is formulated as follows:\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1) g \u2211\ni=1\nhi||wGi ||2, (88)\nwhere \u03b1 is parameter between 0 and 1 to balance the contribution of inter-group sparsity and intra-group sparsity for feature selection.\nThe differences between Lasso, Group Lasso and Sparse Group Lasso are illustrated in Figure (9):\n\u2022 Lasso does not consider the group structures among features and selects a subset of relevant features among all groups;\n\u2022 Group Lasso performs group selection and selects or not selects a whole group of features;\n\u2022 Sparse Group Lasso performs group selection and selects a subset of relevant features in each group."}, {"heading": "3.1.3 Overlapping Sparse Group Lasso (Supervised) (Jacob et al., 2009)", "text": "Above methods consider the disjoint group structures among features. However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009). One motivating example is the usage of biologically meaningful gene/protein groups mentioned in (Ye and Liu, 2012). Different groups of genes may overlap, i.e., one protein/gene may belong to multiple groups. Under this scenario, Group Lasso and Sparse Group Lasso are not applicable. A general overlapping Sparse Group Lasso regularization is similar to the regularization term of Sparse Group Lasso:\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1) g \u2211\ni=1\nhi||wGi ||2. (89)\nThe only difference for overlapping Sparse Group Lasso is that different feature groups Gi can have a overlap, i.e., there exists at least two groups Gi and Gj such that Gi \u22c2 Gj 6= \u2205."}, {"heading": "3.2 Feature Selection with Tree Feature Structures", "text": "In addition to group structures, features can also exhibit other kinds of structures such as tree structures. For example, in image processing such as face images, different pixels (features) can be represented a tree, where the root node indicates the whole face, its child nodes can be the different organs, and each specific pixel is considered as a leaf node. In other words, these pixels enjoy a spatial locality structure. Another motivating example is that genes/proteins may form certain hierarchical tree structures (Liu and Ye, 2010). Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010)."}, {"heading": "3.2.1 Tree-guided Group Lasso (Supervised) (Liu and Ye, 2010)", "text": "In Tree-guided Group Lasso, the structure over the features can be represented as a tree with leaf nodes as features. Each internal node denotes a group of features such that the internal node is considered as a root of a subtree and the group of features are considered as leaf nodes. Each internal node in the tree is associated with a weight that represents the height of its subtree, or how tightly the features in this subtree are correlated.\nWe follow the definition from (Liu and Ye, 2010) to define Tree-guided Group Lasso, for an index tree G with a depth of d, Gi = {Gi1, Gi2, ..., Gini} denotes the whole set of nodes (features) in the i-th level (the root node is defined as the level 0), and ni denotes the number of nodes in the i-th level. With these, the nodes in Tree-guided Group Lasso have to satisfy the following two conditions: (1) internal nodes from the same depth level have non-overlapping indices, i.e., Gij \u22c2\nGik = \u2205, \u2200i = 1, 2, ..., d, j 6= k, i \u2264 j, k \u2264 ni; and (2) if Gi\u22121m is the parent node of G i j , G i j \u2286 Gi\u22121m .\nWe explain these conditions via a toy example in Figure (10). In the figure, we can observe that 8 features are organized in an indexed tree of depth 3. For the internal nodes in each level, we have:\nG01 = {f1, f2, f3, f4, f5, f6, f7, f8} G11 = {f1, f2}, G21 = {f3, f4, f5, f6, f7}, G31 = {f8} G21 = {f1, f2}, G22 = {f3, f4}, G22 = {f5, f6, f7}.\n(90)\nIt can be observed from the figure that G01 is the root node of the index tree which includes 8 features. In addition, internal nodes from the same level do not overlap while\nthe parent node and the child node have some overlap such that the features of the child node is a subset of these of the parent node.\nWith the definition of the index tree, the objective function of Tree-guided Group Lasso is formulated as follows:\nmin w loss(w;X,y) + \u03b1\nd \u2211\ni=0\nni \u2211\nj=1\nhij ||wGij ||2, (91)\nwhere \u03b1 \u2265 0 is a regularization parameter and hij \u2265 0 is pre-defined parameter to measure the contribution of the internal node Gij . Since parent node is a superset of its child nodes, thus, if a parent node is not selected (i.e., the corresponding model coefficient is zero), all of its child nodes will not be selected. For example, as illustrated in Figure (10), if the internal node G12 is not selected, both of its child nodes G 2 2 and G 2 3 will not be selected."}, {"heading": "3.3 Feature Selection with Graph Feature Structures", "text": "In many real-world applications, features may have strong dependencies. For example, in natural language processing, if we take each word as a feature, we have synonyms and antonyms relationships between different words (Fellbaum, 1998). Moreover, many biological studies show that genes tend to work in groups according to their biological functions, and there are strong dependencies between some genes. Since features show some dependencies in these cases, we can model the features by an undirected graph, where nodes represent features and edges among nodes show the pairwise dependencies between features. Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).\nWhen there exists some dependencies among features, we can use a undirected graph G(N,E) to encode these dependencies. Assume that there are n nodes N = {N1, N2, ..., Nn} and a set of E edges {E1, E2, ..., Ee} in G(N,E). Then node Ni corresponds to the i-th feature and the pairwise feature dependencies can be represented by an adjacency matrix A \u2208 RNn\u00d7Nn of G(N,E). Figure (11) shows an example of the graph with 7 features {f1, f2, ..., f7} and its pairwise dependence information encoded in an adjacency matrix A. Note that entries in A does not necessarily have to be 0 or 1, it can be any numerical numbers that can reflect the correlations between features."}, {"heading": "3.3.1 Laplacian Lasso (Supervised) (Ye and Liu, 2012)", "text": "Since features exhibit graph structures, when two nodes (features) Ni and Nj are connected by an edge in G(N,E), the features fi and fj are more likely to be selected together, and they should have similar feature coefficients. One way to achieve this target is via Graph Lasso \u2013 adding a graph regularizer on the feature graphs on the basis of Lasso. The formulation is as follows:\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1) \u2211\ni,j\nA(i, j)(wi \u2212wj)2, (92)\nwhere the first regularization term \u03b1||w||1 is the same as the regularization term as Lasso while the second term enforces that if a pair of features shows strong dependencies, i.e., large A(i, j), their feature coefficients should also be close to each other. We can reformulate above loss function into a matrix format:\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1)w\u2032Lw, (93)\nwhere L = D \u2212 A is the Laplacian matrix and D is a diagonal matrix with D(i, i) = \u2211Ni\nj A(i, j). The Laplacian matrix in Eq. (93) is positive semi-definite and captures the local geometric structure of features. It can be observed that when the Laplacian matrix is the identity matrix I, w\u2032Lw = ||w||22, the penalty term in Eq. (93) reduces to the elastic net penalty (Zou and Hastie, 2005). Since the graph regularization term w\u2032Lw is convex and differentiable, existing efficient algorithms like LARS (Efron et al., 2004) and proximal gradient descent methods (Liu and Ye, 2009) can be directly applied."}, {"heading": "3.3.2 GFLasso (Supervised) (Kim and Xing, 2009)", "text": "In Eq. (93), graph feature structures are represented by an unsigned graph, and it encourages features connected together with similar feature coefficients. However, in many cases, features can also be negatively correlated. In this case, the feature graph G(N,E) is represented by a signed graph, with both positive and negative edges. Recently, GFLasso is proposed to explicitly consider both the positive and negative feature correlations, the objective function of GFLasso is formulated as follows:\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1) \u2211\ni,j\nA(i, j)(wi \u2212 sign(ri,j)wj)2, (94)\nwhere ri,j indicates the correlation between two features fi and fj. When two features are positively correlation, we have A(i, j) = 1 and ri,j > 0, and the penalty term forces\nthe feature coefficients wi and wj to be similar; on the other hand, if two features are negatively correlated, we have A(i, j) = 1 and ri,j < 0, and the penalty term makes the feature coefficients wi and wj to be dissimilar. An major limitation of GFLasso is that it uses pairwise sample correlations to measure feature dependencies, which may lead to additional estimation bias. The feature dependencies cannot be correctly estimated when the sample size is small."}, {"heading": "3.3.3 GOSCAR (Supervised) (Yang et al., 2012)", "text": "To address the limitations of GFLasso, (Yang et al., 2012) proposed a GOSCAR algorithm by putting a \u2113\u221e-norm regularization to enforce the pairwise feature coefficients to be equal if they are connected over the feature graph G(N,E). The formulation of GOSCAR is defined as:\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1) \u2211\ni,j\nA(i, j)max(|wi|, |wj |). (95)\nIn the above formulation, the \u21131-norm regularization is used for feature selection while the pairwise \u2113\u221e-norm term penalizes large coefficients. The pairwise \u2113\u221e-norm term can be decomposed as:\nmax(|wi|, |wj |) = 1\n2 (|wi +wj|+ |wi \u2212wj|)\n= |u\u2032w|+ |v\u2032w|, (96)\nwhere u and v are sparse vectors with only two non-zero entries such that ui = uj = 1 2 , vi = \u2212vj = 12 . The formulation of the GOSCAR is more general than the OSCAR algorithm which is proposed in (Bondell and Reich, 2008). OSCAR algorithm assumes that all features form a complete graph whose entries in the adjacency matrix A are all 1. In contrast, GOSCAR can deal with an arbitrary undirected graph as long as the adjacency matrix A is symmetric.\nDifferent signs of feature coefficients can introduce additional penalty in the objective function in Eq. (93). Even though GOSCAR is designed to mitigate this side effect, it may still over penalize the feature coefficient wi or wj due to the property of the max operator. Therefore, a new formulation with non-convex grouping penalty is also proposed in (Yang et al., 2012):\nmin w\nloss(w;X,y) + \u03b1||w||1 + (1\u2212 \u03b1) \u2211\ni,j\nA(i, j)\u2016|wi| \u2212 |wj|\u20161, (97)\nwhere the grouping penalty term controls only magnitudes of differences of coefficients ignoring their signs over the feature graph G(N,E).\nFor feature selection problem with graph structured features, a subset of highly connected features in the graph is more likely to be selected or not selected as a whole. For example, in the toy example in Figure (11), features {f5, f6, f7} are selected while features {f1, f2, f3, f4} are not selected."}, {"heading": "4. Feature Selection with Heterogeneous Data", "text": "Traditional feature selection algorithms can only work with generic data from a single data source which is based on the data independent and identically distributed (i.i.d.) assumption. However, heterogeneous data are becoming more prevalent in the era of big data. For example, in the medical domain, high dimensional gene features are often considered associated with different types clinical features. Since data of each source can be noisy, partial, or redundant, how to select relevant sources and how to use them together for effective feature selection is a challenging problem. Another example is in social media platforms, instances with high-dimensional features are often linked together, how to integrate link information to guide feature selection is another difficult problem. In this section, we review current feature selection algorithms for heterogeneous data from three aspects: (1) feature selection for linked data; (2) feature selection for multi-source data; and (3) feature selection for multi-view data. Note that multi-source feature selection is similar to multi-view feature selection, but they are different in two ways. First, multisource feature selection aims select features from the original feature space by integrating multiple sources while multi-view feature selection select features from different feature spaces for all views simultaneously. Second, multi-source feature selection normally ignores the correlations among sources while multi-view feature selection exploits the relations among features from different views."}, {"heading": "4.1 Feature Selection Algorithms with Linked Data", "text": "Linked data has become ubiquitous in real-world applications such as Twitter4 (tweets linked through hyperlinks), social networks in Facebook5 (people connected by friendships) and biological networks (protein interaction networks). Since linked data are related to each other by different types of links, they are distinct from traditional attribute value data (or \u201cflat\u201d data). Figure (12) illustrates a toy example of linked data and its two representations. Figure (12(a)) shows 8 linked instances (u1 to u8) while Figure (12(b)) is a conventional representation of attribute-value data such that each row corresponds to one instance and each column corresponds to one feature6. As mentioned above, in addition to feature information, linked data provides an extra source of information in the form of links, which can be represented by an adjacency matrix, illustrated in Figure (12(c)). Many linked data related learning tasks are proposed such as collective classification (Macskassy and Provost, 2007; Sen et al., 2008), relational learning (Long et al., 2006, 2007), and active learning (Bilgic et al., 2010; Hu et al., 2013), but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information. Until recently, feature selection for linked data receives some attention. Next, we introduce some representative algorithms which leverage link information for feature selection.\n4. https://twitter.com/ 5. https://www.facebook.com/ 6. The data can either be labeled or unlabeled. In the example in Figure (12), the data is unlabeled."}, {"heading": "4.1.1 Feature Selection on Networks (Supervised) (Gu and Han, 2011)", "text": "In (Gu and Han, 2011), authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS). In detail, they propose to use linear classifier to capture the relationship between content information and class labels, and incorporate link information by graph regularization. Suppose that X \u2208 Rn\u00d7d denotes the content matrix with n instances and each instance is associated with a d-dimensional feature vector; Y \u2208 Rn\u00d7c denotes the class labels matrix such that Y(i, k) = 1 if the class label for the i-th instance is k, Y(i, k) = 0 otherwise; A denotes the adjacency matrix for all n linked instances. The network can either be directed and undirected. With these notations, LapRLS first attempts to learn a linear classifier W \u2208 Rd\u00d7c to map X to Y:\nmin W\n\u2016XW \u2212Y\u20162F + \u03b1\u2016W\u20162,1 + \u03b2\u2016W\u20162F . (98)\nThe regularization term \u2016W\u20162,1 is included to achieve feature selection. It makes W be sparse in rows, which makes the feature sparse across all c class labels. The other regularization term \u2016W\u20162F presents the overfitting of the model and makes the model more robust in the same time.\nTill now, FSNet has modeled the content information for feature selection. However, link information is not considered yet. To capture the correlation between link information and\ncontent information to select more relevant features, FSNet uses the graph regularization and the basic assumption is that if two instances are linked, their class labels are likely to be similar. Taking into consideration the graph regularization, the objective function of FSNet can be mathematically formulated as:\nmin W\n\u2016XW \u2212Y\u20162F + \u03b1\u2016W\u20162,1 + \u03b2\u2016W\u20162F + \u03b3tr(W\u2032X\u2032LXW), (99)\nwhere tr(W\u2032X\u2032LXW) is the graph regularization, \u03b3 controls the contribution of content information and link information. For undirected network, L = D \u2212 A is the laplacian matrix where D is a diagonal matrix with its diagonal entry as D(i, i) =\n\u2211n j=1A(i, j). For\ndirected network, the laplacian matrix L can be obtained either by transforming the network to be undirected A = max(A,A\u2032) or by applying the random walk method in (Zhou et al., 2005a). The objective function in Eq. (99) can be solved by proximal gradient descent methods (Nesterov, 2004). Afterwards, NetFS calculates the \u21132-norm value for each feature coefficient vector in W and return the top ranked ones.\n4.1.2 Feature Selection for Social Media Data (Supervised) (Tang and Liu, 2012a, 2014a)\nIn (Tang and Liu, 2012a, 2014a), the authors investigate feature selection problem on social media data by evaluating the effects of user-user and user-post relationships as illustrated in Figure (13). The target is to perform feature selection for high dimensional posts, and take into account the user-user and user-post relationships manifested in the linked data.\nIn the proposed supervised feature selection framework (LinkedFS), different social relationships are extracted to enhance the feature selection performance. As illustrated in Figure (14), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user u2 can have multiple posts (p3, p4 and p5) and these posts are more similar than those randomly selected; (b) CoFollowing - two users u1 and u3 follow a user u4, its counterpart in citation analysis is bibliographic coupling (Morris, 2005), and their posts are more likely to be of similar topics; (c) CoFollowed - two users u2 and u4 are followed by a third user u1, similar to co-citation relation (Morris, 2005) in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user u1 follows another user u2, and their posts (e.g., {p1, p2} and {p3, p4, p5} are more likely similar in\nterms of topics. These four hypotheses are supported by social correlation theories such as homophily (McPherson et al., 2001) and social influence (Marsden and Friedkin, 1993) in explaining the existence of similarity as what these relations suggest. For example, homophily indicates that people with similar interests are more likely to be linked.\nNext, we use the CoPost relation as an example to illustrate how these relations can be integrated into feature selection. A comprehensive report of other three relations can be referred to the original paper (Tang and Liu, 2012a, 2014a). Let p = {p1, p2, ..., pN} be the post set and X \u2208 RN\u00d7d be the matrix representation of these posts where N is the number of posts and each post is associated with a d dimensional feature vector; Y \u2208 Rn\u00d7c denotes the class labels matrix where Y(i, k) = 1 if the i-th post belongs to class cj , otherwise zero; u = {u1, u2, ..., un} denotes the set of n users and their link information is encoded in an adjacency matrix A; P \u2208 Rn\u00d7N denotes the user-post relationships such that P(i, j) = 1 if ui posts pj, otherwise 0. To integrate the CoPost relations among users into the feature selection framework, the authors propose to add a regularization term enforces the hypothesis that the class labels (i.e., topics) of posts by the same user are similar. Hence, feature selection with the CoPost hypothesis can be formulated as the following optimization problem:\nmin W\n\u2016XW \u2212Y\u20162F + \u03b1\u2016W\u20162,1 + \u03b2 \u2211\nu\u2208u\n\u2211\n{pi,pj}\u2208pu\n\u2016X(i, :)W \u2212X(j, :)W\u201622, (100)\nwhere pu denotes the whole set of posts by user u. The parameter \u03b1 controls the sparseness of W in rows across all class labels and \u03b2 controls the contribution of the CoPost relations. The CoPost regularization term indicates that if a user posts multiple posts, the class labels of these posts should be similar. After optimizing the objective function in Eq. (100), we can obtain the sparse matrix W and get the ranking of all d features thereby."}, {"heading": "4.1.3 Unsupervised Feature Selection for Linked Data (Unsupervised) (Tang and Liu, 2012b, 2014b)", "text": "Linked Unsupervised Feature Selection (LUFS) is an unsupervised feature selection framework for linked data and in essence, LUFS investigates how to take advantage of link information for unsupervised feature selection. Generally speaking, feature selection aims to select a subset of relevant features with some constraints, while in supervised feature selection, the class labels play the role of constraints such that distances of instances with the\nsame class labels should be closer than these of instances from different classes. However, in unsupervised feature selection, without class labels to assess feature relevance, some alternative criteria has to be exploited. The problem is formulated as follows: given n linked instances {x1, x2, ..., xn}, their feature information and link information can be represented a feature matrix X \u2208 Rn\u00d7d and an adjacency matrix A \u2208 Rn\u00d7n, respectively, where d denotes the feature dimension. The task is to select a subset of relevant features from all d features by utilizing both feature information X and link information A. LUFS introduces the concept of pseudo-class labels to guide the unsupervised feature selection. Particularly, LUFS assumes the pseudo class labels come from c classes, and uses Y \u2208 Rn\u00d7c to denote the label matrix such each row of Y has only one nonzero entry. Like most feature selection algorithms, LUFS assumes a linear mapping matrix W \u2208 Rd\u00d7c exists between feature matrix X and pseudo-class label matrix Y. With these notations, LUFS seeks the pseudoclass labels by extracting constraints from link information and attribute-value information through social dimension approach and spectral analysis, respectively.\nFirst, to consider the constraints from link information, LUFS employs social dimension approach to exploit the hidden factors that incur the interdependency among instances. Particularly, it uses modularity maximization (Newman and Girvan, 2004) to extract hidden factor matrix H. Since the extracted hidden factor matrix H indicates some affiliations among linked instances, according to the Linear Discriminative Analysis, within, between and total hidden factor scatter matrix Sw, Sb and St are defined as Sw = Y\u2032Y \u2212Y\u2032FF\u2032Y, Sb = Y\u2032FF\u2032Y, St = Y\u2032Y, where F = H(H\u2032H)\u2212 1 2 is the weighted hidden factor matrix. Considering the fact that instances with similar hidden factors are similar and instances with different hidden factors are dissimilar, the constraint from link information can be incorporated by solving the following maximization problem:\nmax W\ntr((St) \u22121Sb). (101)\nSecond, to take advantage of information from attribute-value part, LUFS obtains the constraints by the spectral analysis (Von Luxburg, 2007):\nmin tr(Y\u2032LY), (102)\nwhere L = D \u2212 S is the laplacian matrix and D is the diagonal matrix with its diagonal entry as D(i, i) =\n\u2211n j=1 S(i, j). S denotes the affinity matrix from X and LUFS adopts the\nRBF kernel to get the affinity matrix. Incorporating the constraints from Eq. (101) and Eq. (102), the objective function of LUFS is formulated as follows:\nmin W\ntr(Y\u2032LY)\u2212 \u03b1tr((St)\u22121Sb), (103)\nwhere \u03b1 is a regularization parameter to balance the contribution from these two constraints. To achieve feature selection, LUFS further adds a \u21132,1-norm regularization term on W, and with spectral relaxation of the pseudo-class label matrix, the objective function in Eq. (103) can be eventually represented as:\nmin W tr(W\u2032(X\u2032LX+ \u03b1X\u2032(In \u2212 FF\u2032))W) + \u03b2\u2016W\u20162,1 s.t. W\u2032(X\u2032X+ \u03bbId)W = Ic,\n(104)\nwhere \u03b2 is a parameter to control the sparseness of W in rows and \u03bbId is included to make X\u2032X+\u03bbId invertible. Similar to previous mentioned feature selection algorithms, the ranking of features can be obtained from the sparse matrix W."}, {"heading": "4.2 Feature Selection Algorithms with Multi-Source Data", "text": "Over the past few decades, many feature selection algorithms are proposed and they have proven to be effective in handling high dimensional data. However, most of them are designed for single source of data. In many data mining and machine learning tasks, we may have multiple data sources for the same set of data instances. For example, recent advancement in bioinformatics reveal the existence of some non-coding RNA species in addition widely used messenger RNA, these non-coding RNA species functions across a variety of biological process. The availability of multiple data sources makes it possible to solve some problems unsolvable using a single source since the multi-faceted representations of data can help depict some intrinsic patterns hidden in a single source of data. The task of multi-source feature selection is formulated as follows: given m sources of data depicting the same set of n instances, and their matrix representations X1 \u2208 Rn\u00d7d1 ,X2 \u2208 Rn\u00d7d2 , ...,Xm \u2208 R n\u00d7dm (where d1, ..., dm denote the feature dimensions), select a subset of relevant features from a target source (e.g., Xi) by taking advantage of all information in all m sources."}, {"heading": "4.2.1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008)", "text": "To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al., 2004). They introduce a concept of geometry-dependent covariance that enables the usage of the global geometric pattern in covariance analysis for feature selection. Given multiple local geometric patterns in an affinity matrix Si where i denotes the i-th data source, a global pattern can be obtained by linearly combining all affinity matrices as S =\n\u2211m i=1 \u03b1iSi, where \u03b1i controls the contribution\nof the i-th source. With the global geometric pattern obtained from multiple data sources, one can build a geometry-dependent sample covariance matrix for the target source Xi as follows:\nC = 1\nn\u2212 1\u03a0X \u2032 i(S\u2212\nS11\u2032S\n1\u2032S1 )Xi\u03a0, (105)\nwhere \u03a0 is a diagonal matrix with \u03a0(j, j) = \u2016D 12Xi(:, j)\u2016\u22121, and D is also a diagonal matrix from S with D(k, k) =\n\u2211n j=1 S(k, j).\nAfter getting a geometry-dependent sample covariance matrix, a subsequent question is how to use it effectively for feature selection. Basically, two methods are proposed. The first method, GPCOVvar sorts the diagonal of the covariance matrix and selects the features that have the biggest variances. Selecting features based on this method is equivalent to choose features that are consistent with the global geometry pattern. The basic idea of the first method is similar to Laplacian score (He et al., 2005) and SPEC (Zhao and Liu, 2007) mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually. While on the other hand, the second method, GPCOVspca, applies sparse principle component analysis (SPCA) (d\u2019Aspremont et al.,\n2007) to select features that is able to retain the total variance maximally, and hence considers the interactions among features and is able to select features with less redundancy."}, {"heading": "4.3 Feature Selection Algorithms with Multi-View Data", "text": "Multi-view sources represent different facets of data instances via different feature spaces. These feature spaces are naturally dependent and also high dimensional, which suggests that feature selection is necessary to prepare these sources for effective data mining tasks such as multi-view clustering. A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations. For example, selecting pixels, tags, and terms about images in Flickr7 simultaneously. Since multi-view feature selection is designed to select features across multiple views by using their relations, they are naturally different from multi-source feature selection. The difference between multi-source feature selection and multi-view feature selection is illustrated in Figure (15). For supervised multi-view feature selection, the most common approach is Sparse Group Lasso (Friedman et al., 2010; Peng et al., 2010). In this subsection, we review some representative algorithms for unsupervised multi-view feature selection.\n4.3.1 Adaptive Multi-view Feature Selection (Unsupervised) (Feng et al., 2013)\nAdaptive unsupervised multi-view feature selection (AUMFS) takes advantages of data cluster structure, data similarity and correlations among views simultaneously for feature selection. More specifically, let X1 \u2208 Rn\u00d7d1 ,X2 \u2208 Rn\u00d7d2 , ...,X1 \u2208 Rn\u00d7dm denote the description of n instances from m different views, X = [X1,X2, ...,Xm] \u2208 Rd denotes the concatenated data, where d = d1+ d2+ ...+ dm. AUMFS first builds a feature selection model by using a \u21132,1-norm regularized least square loss function:\nmin W,F\n\u2016XW \u2212 F\u20162,1 + \u03b1\u2016W\u20162,1, (106)\n7. https://www.flickr.com/\nwhere F \u2208 Rn\u00d7c is the pseudo class label matrix. The \u21132,1-norm loss function is imposed since it is robust to outliers in the data instances and \u21132,1-norm regularization selects features across all c pseudo class labels with joint sparsity. Then AUMFS uses spectral clustering on an affinity matrix from different views to learning the shared pseudo class labels. For the data matrix Xi in each view, they first build an affinity matrix Si based on the data similarity on that view and get the the corresponding laplacian matrix Li. Then it aims to learn the pseudo class label matrix by considering the spectral clustering from all views:\nmin F,\u03bb\nm \u2211\ni=1\n\u03bbitr(F \u2032LiF) = min tr(F\n\u2032 m \u2211\ni=1\n\u03bbiLiF)\ns.t. F\u2032F = Ic,F \u2265 0, m \u2211\ni=1\n\u03bbi = 1, \u03bbi \u2265 0, (107)\nwhere the contribution of each feature for the joint spectral clustering is balanced by a nonnegative weight \u03bbi and the summation of all \u03bbi equals 1. By combing the objective function in Eq. (106) and Eq. (107) together, the final objective function of AUMFS which jointly performs pseudo class label learning and feature selection is as follows:\nmin tr(F\u2032 m \u2211\ni=1\n\u03bbiLiF) + \u03b2(\u2016XW \u2212 F\u20162,1 + \u03b1\u2016W\u20162,1)\ns.t. F\u2032F = Ic,F \u2265 0, m \u2211\ni=1\n\u03bbi = 1, \u03bbi \u2265 0. (108)\nThrough optimizing the objective function in Eq. (109) and obtaining the feature coefficient matrix W. AUMFS takes a commonly accepted way to rank all features according to the value of \u2016W(i, :)\u201622 in a descending order and return the top ranked ones."}, {"heading": "4.3.2 Unsupervised Feature Selection for Multi-View Data (Unsupervised) (Tang et al., 2013)", "text": "AUMFS (Feng et al., 2013) learns one feature weight matrix for all features from different views to approximate the pseudo class labels. In (Tang et al., 2013), the authors propose a novel unsupervised feature selection method called Multi-view Feature Selection (MVFS). Similar to AUMFS, MVFS also uses spectral clustering with the affinity matrix from different views to learn the pseudo class labels. But it differs from AUMFS that it learns one feature weight matrix for each view to fit the pesudo class labels by the joint least squared loss and \u21132,1-norm regularization. The optimization problem of MVFS can be formulated as follows:\nmin tr(F\u2032 m \u2211\ni=1\n\u03bbiLiF) +\nm \u2211\ni=1\n\u03b2(\u2016XiWi \u2212 F\u20162,1 + \u03b1\u2016Wi\u20162,1)\ns.t. F\u2032F = Ic,F \u2265 0, m \u2211\ni=1\n\u03bbi = 1, \u03bbi \u2265 0. (109)\nSimilar to AUMFS, the orthogonal and nonnegative constraints on the pseudo class label matrix F is imposed to guarantee there is one and only one positive entry in each row and\nother entries are all zero. The parameter \u03bbi is employed to control the contribution of each view and\n\u2211m i=1 \u03bbi = 1."}, {"heading": "4.3.3 Multi-view Clustering and Feature Learning via Structured Sparsity (Wang et al., 2013a)", "text": "In some cases, it is possible that features from a certain view contains more discriminative information than features from other views for different clusters. According to (Wang et al., 2013a), one example is that in image processing, the color features are more useful than other types of features in identifying stop signs. To address this issue in multi-view feature selection, a novel feature selection algorithm is proposed in (Wang et al., 2013a) with a joint group \u21131-norm and \u21132,1-norm regularization.\nFor the feature weight matrix W1, ...,Wm from m different views, the group \u21131-norm is defined as \u2016W\u2016G1 = \u2211c j=1 \u2211m i=1 \u2016Wi(:, j)\u2016. Crucially, the group \u21131-norm regularization term is able to capture the global relations among different views and is able to achieve view-wise sparsity such that only a few views are selected. In addition to group \u21131-norm, a \u21132,1-norm regularizer on W is also included to achieve feature sparsity among selected views. It can be observed that the basic idea is very similar to sparse group lasso (Friedman et al., 2010; Peng et al., 2010) which also requires intra-group sparsity and inter-group sparsity. Hence, the objective function of the proposed method is formulated as follows:\nmin W,F \u2016XW \u2212 F\u20162F + \u03b1\u2016W\u20162,1 + \u03b2\u2016W\u2016G1 s.t. F\u2032F = Ic,F \u2265 0,\n(110)\nwhere \u03b1 and \u03b2 are two parameters to control the inter-view sparsity and intra-view sparsity. Through the proposed two regularizer, many features in the discriminative views and a small number of features in the non-discriminative views will output as the final set of features to discriminate cluster structures."}, {"heading": "5. Feature Selection with Streaming Data", "text": "Methods introduced in the previous sections assume that all data instances and features are known in advance. However, it is not the case in many real-world applications that we are more likely facing with dynamic data streams and feature streams. In the worst cases, the size of data or the features are unknown or even infinite, thus it is not practical to wait until all data instances or features are available to perform feature selection. For streaming data, one motivating example is that in online spam email detection problem, new emails are constantly arriving, it is not easy to employ batch-mode feature selection methods to select relevant feature in a time manner. On a orthogonal setting, feature selection for streaming features also have its practical significances. For example, Twitter produces more than 320 millions of tweets everyday and a large amount of slang words (features) are continuously being generated. These slang words promptly grab users\u2019 attention and become popular in a short time. Therefore, it is more preferable to perform streaming feature selection to rapidly adapt to the changes. Recently, there exists some work trying to combine these two dual problems together, the problem is referred as feature selection on Trapezoidal data streams (Zhang et al., 2015). In the following two sections, we will review\nsome representative algorithms for these two orthogonal problems, i.e., feature selection for streaming data and feature selection for streaming features."}, {"heading": "5.1 Feature Selection Algorithms with Feature Streams", "text": "For the feature selection problem with streaming features, the number of instances is considered to be constant while candidate features arrive one at a time, the task is to timely select a subset of relevant features from all features seen so far. Instead of searching for the whole feature space which is costly, streaming feature selection (SFS) processes a new feature upon its arrival. A general framework of streaming feature selection is presented in Figure (16). At each time step, a typical SFS algorithm first determines whether to accept the most recently arrived feature; if the feature is added to the selected feature set, it then determines whether to discard some existing features from the selected feature set. The process repeats until no new features show up anymore. Different algorithms have different implementations in the first step that checks newly arrived features. The second step which checks existing features is an optional step for some algorithms."}, {"heading": "5.1.1 Grafting Algorithm (Supervised) (Perkins and Theiler, 2003)", "text": "The first attempt to perform streaming feature selection is credited to (Perkins and Theiler, 2003). They proposed a streaming feature selection framework based on stagewise gradient descent regularized risk framework (Perkins et al., 2003). Grafting is a general technique that can deal with a variety of models that are parameterized by a feature weight vector w subject to \u21131-norm regularization, such as Lasso:\nmin w\nloss(w;X,y) + \u03b1||w||1. (111)\nIn general, Grafting can work with the following objective function in which the features are assumed to arrive one at a time, the key challenge is how to efficiently update the parameter w when new features are continuously coming. The basic idea of Grafting streaming feature selection algorithm is based on the observation \u2013 incorporating a new feature into the model in Eq. (111) involves in adding a new penalty term into the model. For example, at the time step j, when a new feature fj arrives, it incurs a regularization\npenalty of \u03b1|wj |. Therefore, the addition of the new feature fj reduces the objective function value in Eq. (111) only when the reduction in the loss function part loss(w;X,y) outweighs the increase in the \u21131-norm regularization. With this observation, the condition of accepting the new feature fj is as follows:\n\u2223 \u2223 \u2223 \u2223 \u2202loss(w;X,y)\n\u2202wj\n\u2223 \u2223 \u2223 \u2223 > \u03b1. (112)\nOtherwise, the Grafting algorithm will set the feature coefficient wj of the new feature fj to be zero. In the second step, when new features like fj are accepted and included in the model, Grafting adopts a conjugate gradient (CG) procedure to optimize the model with respect to all current parameters."}, {"heading": "5.1.2 Alpha-investing Algorithm (Supervised) (Zhou et al., 2005b)", "text": "Alpha-investing (Zhou et al., 2005b) is an adaptive complexity penalty method which dynamically changes the threshold of error reduction that is required to accept a new feature. It is motivated from a desire to control the false discovery rate (FDR) of newly arrived features such that a small portion of spurious features does not greatly affect the model accuracy. The detailed algorithm works as follows:\n\u2022 Initialize w0 = 0 (probability of false positives), i = 0 (index of features), selected features in the model SF = \u2205\n\u2022 Step 1 : Get a new feature fi \u2022 Step 2 : Set \u03b1i = wi/(2i)\n\u2022 Step 3 :\nwi+1 = wi \u2212 \u03b1i, SF = SF if p value(fi, SF ) \u2265 \u03b1i wi+1 = wi + \u03b1\u2206 \u2212 \u03b1i, SF = SF \u222a fi if p value(fi, SF ) < \u03b1i\n\u2022 Step 4 : i = i+ 1\n\u2022 Step 5 : Repeat Step 1 to Step 4.\nThe threshold \u03b1i corresponds to the probability of selecting a spurious feature at the time step i. The threshold \u03b1i is adjusted by the wealth wi, which denotes acceptable number of false positivly detected features at the current moment. The wealth wi is increased when a feature is added to the model, otherwise it is decreased when a feature is not included to save for future features. More precisely, at each time step, the method calculates the p-value by using the fact that \u2206Logliklohood is equivalent to t-statistics, the p-value denotes the probability that a feature coefficient could be set to nonzero when it is not (false positively detected). The basic idea of alpha-investing is to adaptively adjust the threshold such that when new features are selected and included into the model, it allows a higher chance of including incorrect features in the future. On the opposite side, each time when a new feature is not included (found to be not statistically significant), the wealth\nis wasted and lowers the chance of finding more spurious features. However, one major limitation of this method is that it only tests newly arrived features while failing to take into consideration of the feature redundancy for old existing features. In (Dhillon et al., 2010), authors extended the alpha-investing algorithm and proposed a proposed a multiple streamwise feature selection algorithm to the case where there are multiple feature streams."}, {"heading": "5.1.3 Online Streaming Feature Selection Algorithm", "text": "(Supervised) (Wu et al., 2010, 2013)\nDifferent from grafting and alpha-investing, authors studied the streaming feature selection problem from an information theoretic perspective by using the concept of Markov blanket (Wu et al., 2010, 2013). According to their definition, the whole feature set consists of four types of features: irrelevant features, redundant feature, weakly relevant but nonredundant features, and strongly relevant features. An optimal feature selection should select non-redundant and strongly relevant features. But features are dynamically arrived in a streaming fashion, it is difficult to find all strongly relevant and non-redundant features. The proposed method, OSFS is able to capture these non-redundant and strongly relevant features via two steps: (1) online relevance analysis, and (2) online redundancy analysis. A general framework of OSFS is listed as follows:\n\u2022 Initialize selected features in the model SF = \u2205\n\u2022 Step 1 : Get a new feature fi \u2022 Step 2 : Online relevance analysis\nDiscard feature fi if fi is relevant to the class label\nSF = SF \u222a fi otherwise\n\u2022 Step 3 : Online Redundancy Analysis\n\u2022 Step 4 : Repeat Step 1 to Step 3 until some stopping criteria are satisfied.\nIn step 2, the online relevance analysis step, OSFS discovers weakly relevant and strongly relevant features, and these features are added into the best candidate features (BCF). Otherwise, if the newly arrived feature is not relevant to the class label, it is discarded and not considered in the future step. In step 3, the online redundancy analysis step, OSFS dynamically eliminates redundant features in the selected subset using Markov Blanket. For each feature fj in the best candidate set BCF , if there exists a subset of BCF making fj and the class label conditionally independent, then fj is removed from BCF . The most time consuming part of OSFS is the redundancy analysis phase, therefore, a fast-OSFS is proposed to improve efficiency. Fast-OSFS further divides this phase into inner-redundancy analysis part and outer-redundancy analysis part. In the inner-redundancy analysis part, fast-OSFS only re-examines the feature newly added into BCF, while the outer-redundancy analysis part re-examines each feature of BCF only when the process of generating a feature is stopped."}, {"heading": "5.1.4 Streaming Feature Selection with Group Structures", "text": "(Supervised) (Wang et al., 2013b; Li et al., 2013)\nPrevious mentioned streaming feature selection algorithms evaluate new features individually. However, streaming features may also exhibit group structures and current group feature selection algorithms such as Group Lasso cannot handle online processing.\nTherefore, in (Wang et al., 2013b, 2015), authors propose an streaming group feature selection algorithm (OGFS) which consists of two parts: online intra-group selection and online inter-group selection. In the online intra-group selection phase, for the streaming features in a specific group, OGFS uses spectral feature selection techniques to assess if the newly arrived feature will increase the ratio between between-class distances and withinclass distances or it is a significant feature with discriminative power. If the inclusion of this new feature will increase this ratio or is statistically significant, the new feature is included, otherwise it is discarded. After all feature groups are processed, in the online inter-group step, for the features from different feature groups, OGFS uses Lasso to select a subset of features to obtain an ultimate subset.\nIn addition to OGFS, a similar algorithm is proposed in (Li et al., 2013). The proposed algorithm, GFSSF also contains two steps: the feature level selection and group level selection. The difference is that it performs feature level selection and group level selection from an information theoretic perspective. In the feature level selection, it only processes features from the same group, and seeks for the best feature subset from the arrived features so far via relevance and redundancy analysis. Then in the group selection phase, it seeks for a set of feature groups that can cover as much uncertainty of the class labels as possible with a minimum cost. Afterwards, it obtains a subset of relevant features that is sparse at both the group level and the individual feature level."}, {"heading": "5.1.5 Unsupervised Streaming Feature Selection in Social Media (Unsupervised) (Li et al., 2015)", "text": "Vast majority of streaming feature selection methods are supervised which utilize label information to guide feature selection process. However, in social media, it is easy to amass vast quantities of unlabeled data, while it is time and labor consuming to obtain labels. To deal with large-scale unlabeled data in social media, authors in (Li et al., 2015) proposed an USFS algorithm to study unsupervised streaming feature selection. The key idea of USFS is to utilize source information such as link information to enable unsupervised streaming feature selection. The work flow of the proposed framework USFS is shown in Figure 2. USFS first uncovers hidden social factors from link information by mixed membership stochastic blockmodel (Airoldi et al., 2009). Suppose that the number of instances is n and each instance is associated with a k dimensional latent factors. After obtaining the social latent factors \u03a0 \u2208 Rn\u00d7k for each linked instance, USFS takes advantage of them as a constrain to perform selection. At a specific time step t, let X(t), W(t) denote the corresponding feature matrix, feature coefficient matrix. To model feature information, USFS constructs a graph G to represent feature similarity and A(t) denotes the adjacency matrix of the graph, L(t) is the corresponding laplacian matrix. Then the objective function\nto achieve feature selection at the time step t is given as follows:\nmin W(t)\n1 2 ||X(t)W(t)\u2212\u03a0||2F +\u03b1\nk \u2211\ni=1\n\u2016(w(t))i\u20161+ \u03b2\n2 ||W(t)||2F +\n\u03b3 2 ||(X(t)W(t))T (L(t)) 12 ||2F , (113)\nwhere \u03b1 is a sparse regularization parameter, \u03b2 controls the robustness of the model and \u03b3 balances link information and feature information.\nAssume at the next time step t + 1 a new feature arrives, to test new features, USFS takes a similar strategy as Grafting to perform gradient test. Specifically, if the inclusion of the new feature is going to reduce the objective function in Eq. (113) at the new time step, the feature is accepted, otherwise the new feature can be removed. When new features are continuously being generated, they may take place of some existing features and some existing features may become outdated, therefore, USFS also investigates if it is necessary to remove any existing features by re-optimizing the model through A Broyden-FletcherGoldfarb-Shanno (BFGS) quasinewton method (Boyd and Vandenberghe, 2004)."}, {"heading": "5.2 Feature Selection Algorithms with Data Streams", "text": "In this subsection, we review the problem of feature selection with data streams which is considered as a dual problem of streaming feature selection. Most existing feature selection algorithms assume that all the data instances are available before performing feature selection. However, such assumptions are not always true in real-world applications that data instances are dynamically generated and arrive in a sequential manner. Therefore, it is necessary and urgent to come up with some solutions to deal with sequential data of high dimensionality."}, {"heading": "5.2.1 Online Feature Selection (Supervised) (Wang et al., 2014)", "text": "In (Wang et al., 2014), an online feature selection algorithm (OFS) for binary classification is proposed. Let {x1,x2, ...,xt...} and {y1, y2, ..., yt...} denote a sequence of input data instances and input class labels, where each data instance xi \u2208 Rd is in a d-dimensional\nspace and class label yi \u2208 {\u22121,+1}. The task of OFS is to learn a linear classifier w(t) \u2208 Rd that can be used to classify each instance xi by a linear function sign(w\n(t)xi). To achieve the feature selection purpose, it requires that the linear classifier w(t) has at most B-nonzero elements such that \u2016w(t)\u20160 \u2264 B. It indicates that at most B features will be used for classification. With a regularization parameter \u03bb and a step size \u03b7, the algorithm of OFS works as follows:\n\u2022 Step 1 : Get a new data instance xt and its class label yt \u2022 Step 2 : Make a class label prediction sign(w(t)xt) for the new instance\n\u2022 Step 3(a): If xt is misclassified such that yiw(t)xt < 0\nw\u0303t+1 = (1\u2212 \u03bb\u03b7)wt + \u03b7ytxt w\u0302t+1 = min{1, 1/ \u221a \u03bb\u2016w\u0303t+1\u20162}w\u0303t+1\nwt+1 = Truncate(w\u0302t+1, B)\n\u2022 Step 3(b): wt+1 = (1\u2212 \u03bb\u03b7)wt \u2022 Step 4 : Repeat Step 1 to Step 3(a) or Step 3(b) until no new data instances arrive.\nIn Step 3(a), each time when a training instance xt is misclassified, wt is first updated by online gradient descent and then it is projected to a \u21132-norm ball to ensure that the classifier is bounded. After that, the new classifier w\u0302t+1 is truncated by taking the most important B features. A subset of B features is output at each time step. The process repeats until there are no new data instances arrive anymore."}, {"heading": "5.2.2 Unsupervised Feature Selection on Data Streams (Unsupervised) (Huang et al., 2015)", "text": "OFS assumes that the class labels of continuously generated data streams are available. However, it is not the case in many real-world applications that label information is costly to obtain. To timely select a subset of relevant features when unlabeled data are continuously being generated, authors in (Huang et al., 2015) propose a novel unsupervised feature selection method (FSDS) that is able to perform feature selection timely with only one pass of the data and utilize limited storage. The basic idea of FSDS is to use matrix sketching to efficiently maintain a low-rank approximation of the current observed data and then apply regularized regression to obtain the feature coefficients, which can further be used to obtain the importance of features. The authors empirically show that when some orthogonality conditions are satisfied, the ridge regression can replace the Lasso for feature selection, which is more computational efficient. Assume at a specific time step t, X(t) \u2208 Rnt\u00d7d denotes the data matrix at that time step, its rank-k approximation is X\n(t) k = \u2211k i=1 \u03b4iuiv \u2032 i,\nwhere \u03b4i (i \u2264 k) are the top-k singular values, ui and v\u2032i are the corresponding left and right singular vectors, respectively. With these, the feature coefficients can be obtained by minimizing the following ridge regression problem:\nmin W(t)\n\u2016X(t)W(t) \u2212U(t)k \u20162F + \u03b1\u2016W(t)\u20162F , (114)\nwhereU (t) k \u2208 Rnt\u00d7k are the top-k left singular vectors ofX(t), \u03b1 is a regularization parameter to avoid overfitting. The bottleneck of Eq. (114) is that the singular value decomposition of X(t) is computational expensive, especially when nt is very large. Therefore, FSDS utilizes the matrix sketching strategy from (Liberty, 2013) to maintain a low-rank approximation of X(t). Let B(t) \u2208 R\u2113\u00d7d denote the matrix sketch of X(t) (\u2113). After some mathematical derivations, the objective of ridge regression is reformulated as follows:\nmin W(t)\n\u2016B(t)W(t) \u2212 {e1, ..., ek}\u20162F + \u03b1\u2016W(t)\u20162F , (115)\nwhere ei \u2208 R\u2113 is a vector with its i-th location as 1 and other locations as 0. By solving the optimization problem in Eq. (115), the importance of each feature fi can be computed as:\nscore(j) = max i\n|W(t)(j, i)|. (116)\nThe higher the feature score, the more important the feature is."}, {"heading": "6. Performance Evaluation", "text": "In this section, we discuss the evaluation of feature selection algorithms, focusing on feature selection algorithms for generic data. We first introduce the developed feature selection repository, then we introduce the algorithms to be evaluated and some publicly available benchmark datasets we collect. At last, we introduce some widely adopted evaluation metrics and present the empirical experimental results."}, {"heading": "6.1 Feature Selection Repository", "text": "First, we introduce our efforts in developing a feature selection repository \u2013 scikit-feast. The purpose of this feature selection repository is to collect some widely used feature selection algorithms that have been developed in the feature selection research to serve as a platform for facilitating their application, comparison and joint study. The feature selection repository also effectively assists researchers to achieve more reliable evaluation in the process of developing new feature selection algorithms.\nWe develop the open source feature selection repository scikit-feast by one of the most popular programming language \u2013 python. It contains more than 40 popular feature selection algorithms, including most traditional feature selection algorithms mentioned in this survey and some structural and streaming feature selection algorithms. It is built upon one widely used machine learning package scikit-learn and two scientific computing packages Numpy and Scipy. At the same time, we also maintain a website (http://featureselection.asu.edu/scikit-feast/) for this project which offers several sources such as public available benchmark datasets, performance evaluation of algorithms, test cases to run each algorithm. The source code of this repository is available at Github (https://github.com/jundongl/scikit-feast)."}, {"heading": "6.2 Algorithms", "text": "We empirically evaluate the performance of feature selection algorithms for generic data provided in the repository. Next, we will provide detailed information how these algorithms\nare evaluated, including the datasets, evaluation criteria and experimental setup. The selected feature selection algorithms that will be evaluated are listed as follows. We list the following information of each algorithm: (1) supervised or unsupervised; (2) similarity based, information theoretical based, sparse learning based, or statistical based; (3) output: feature weighting or subset selection; (4) feature type: numerical or categorical. The first two items have been mentioned previously. The third item categorizes these algorithms based on the output. Feature weighing algorithms basically give each feature a score for ranking and feature subset algorithms only show which features are selected. The last item shows the feature types the feature selection method can handle with, continuous or discrete. For supervised feature selection methods, we also list if the method can tackle binary-class or multi-class classification problem.\n1. Fisher Score: supervised, similarity, feature weight, continuous and discrete(multi-class)\n2. ReliefF: supervised, similarity, feature weight, continuous and discrete(multi-class)\n3. Trace Ratio: supervised, similarity, feature weight, continuous and discrete(multi-class)\n4. Laplacian Score: unsupervised, similarity, feature weight, continuous and discrete\n5. SPEC: unsupervised, similarity, feature weight, continuous and discrete\n6. MIM: supervised, information theoretic, feature weight, discrete(multi-class)\n7. MIFS: supervised, information theoretic, feature weight, discrete(multi-class)\n8. MRMR: supervised, information theoretic, feature weight, discrete(multi-class)\n9. CIFE: supervised, information theoretic, feature weight, discrete(multi-class)\n10. JMI: supervised, information theoretic, feature weight, discrete(multi-class)\n11. CMIM: supervised, information theoretic, feature weight, discrete(multi-class)\n12. ICAP: supervised, information theoretic, feature weight, discrete(multi-class)\n13. DISR: supervised, information theoretic, feature weight, discrete(multi-class)\n14. FCBF: supervised, information theoretic, feature subset, discrete(multi-class)\n15. RFS: supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n16. Least square loss (\u21132,1): supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n17. Logistic loss (\u21132,1): supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n18. MCFS: unsupervised, sparse learning, feature weight, continuous and discrete\n19. UDFS: unsupervised, sparse learning, feature weight, continuous and discrete\n20. NDFS: unsupervised, sparse learning, feature weight, continuous and discrete\n21. Low variance: unsupervised, statistical, feature subset, discrete(binary-class)\n22. T-score: supervised, statistical, feature weight, continuous and discrete(binary-class)\n23. F-score: supervised, statistical, feature weight, continuous and discrete(multi-class)\n24. Chi-square: supervised, statistical, feature weight, discrete(multi-class)\n25. Gini Index: supervised, statistical, feature weight, discrete(multi-class)"}, {"heading": "6.3 Datasets", "text": "To test different algorithms, we collect 25 public available benchmark datasets to evaluate the performance of feature selection algorithms. We list the detailed information of each dataset in Table 2. We carefully select datasets from different categories, e.g., text data, image data, biological data, and some others. The features in these datasets are either in numerical or categorical values. We also present the number of features, number of instances and the number of classes for each dataset. The heterogeneity of the data is important for exposing the strength and weakness of algorithms in different applications."}, {"heading": "6.4 Evaluation Metrics", "text": "Next, we introduce the widely adopted way to evaluate the performance of feature selection algorithms. We have different evaluation metrics for supervised and unsupervised methods. For algorithms of different output types, different evaluation strategies are used:\n1. If it is a feature weighting method that outputs the feature score for each feature, then the quality of the first {5, 10, 15, ..., 295, 300} features are evaluated respectively.\n2. If it is a feature subset selection method that only outputs which features are selected, then we use all the selected features to perform the evaluation.\nSupervised Methods: To test performance of the supervised feature selection algorithms, the evaluation framework introduced in Figure (3) is used. The whole dataset is usually divided into two parts - the training set T and test set U . Feature selection algorithms will be first applied on the training set T to obtain a subset of relevant features F . Then the test set on the selected features are acting as input to a classification model for the testing purpose. In the experiments, we use classification accuracy to evaluate the classification performance, and three classification models, Linear SVM, Decision Tree, Na\u0308\u0131ve Bayes are used. To get more reliable results, we adopt 10-fold cross validation, and the final classification performance are reported as an average over 10 folds. The higher the average classification accuracy, the better the feature selection algorithm is.\nUnsupervised Methods: Following the standard way to assess unsupervised feature selection, we evaluate the unsupervised feature selection algorithms in terms of clustering performance. Two commonly used clustering performance metrics, i.e., normalized mutual information (NMI) and accuracy (ACC) are used.\nSpecifically, Let C and C \u2032 denote the clustering results from ground truth class labels and the predicted cluster labels, respectively. The mutual information between two clusters C and C \u2032 is:\nMI(C,C \u2032) = \u2211\nci\u2208C,c\u2032j\u2208C \u2032\np(ci, c \u2032 j)log\np(ci, c \u2032 j)\np(ci)p(c \u2032 j)\n(117)\nwhere p(ci) and p(c \u2032 j) are the probabilities of instances in cluster ci and c \u2032 j , respectively. p(ci, c \u2032 j) indicates the probability of instances in cluster ci and in c \u2032 j at the same time. Then, NMI is defined as:\nNMI(C,C \u2032) = MI(C,C \u2032)\nmax(H(C),H(C \u2032) (118)\nwhere H(C) and H(C \u2032) represent the entropies of clusterings C and C \u2032, respectively. Let pi and qi be the clustering result and the ground truth label for instance ui, respectively. Then, accuracy (ACC) is defined as:\nACC = 1\nn\nn \u2211\ni=1\n\u03b4(qi,map(pi)) (119)\nwhere n is the total number of data instances, \u03b4(.) is an indicator function such that \u03b4(x, y) = 1 if x = y, otherwise \u03b4(x, y) = 0. map(x) permutes the predicted cluster labels to match the ground truth as much as possible.\nEach feature selection algorithm is first applied to select features, then K-means clustering is performed based on the selected features. We repeat the K-means algorithm 20 times and report the average clustering results since K-means may converge to local optimal."}, {"heading": "6.5 Experimental Results", "text": "Due to space limit, we do not list the evaluation results here. The experimental results are shown in http://featureselection.asu.edu/scikit-feast/datasets.php). For each dataset, we list all applicable feature selection algorithms along with its evaluation on either classification or clustering task."}, {"heading": "7. Open Problems and Challenges", "text": "Over the past two decades, there are tremendous amount of work in developing feature selection algorithms for both theoretical analysis and real-world applications. However, we still believe there are more work can be done in this community. Here are several challenges and concerns that we need to mention and discuss."}, {"heading": "7.1 Scalability", "text": "With the tremendous growth of dataset sizes, the scalability of most current feature selection algorithms may be jeopardized. In many scientific and business applications, data are usually measured in terabyte (1TB = 1012 bytes). Normally, datasets in the scale of terabytes cannot be loaded into the memory directly and therefore limits the usability of most feature selection algorithms. Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014). In addition, most feature selection algorithms proposed so far require time complexity proportional to O(d2) or even O(d3), where d is the feature dimension. Recently, big data of ultrahigh dimensionality has emerged in many real-world applications such as text mining and information retrieval. Most feature selection algorithms does not scale well on the ultrahigh dimensional data, its efficiency deteriorates quickly or is even computational infeasible. In this case, well-designed feature selection algorithms in linear or sub-linear running time is more preferred (Fan et al., 2009; Tan et al., 2014). Moreover, in some online classification or online clustering tasks, the scalability of feature selection algorithms is also a big issue. For example, the data streams or feature streams may be infinite and cannot be loaded into the memory, therefore we can only make one pass of the data where the second pass is either unavailable or computational expensive. Even though feature selection algorithms can reduce the issue of scalability for online classification or clustering, these methods either require to keep full dimensionality in the memory or require iterative processes to visit data instances more than once, which limit their practical usages. In conclusion, even though there are preliminary work to increase the scalability of feature selection algorithms, we believe that the scalability problem should be given more attention to keep pace with the rapid growth of very large-scale and fast streaming data."}, {"heading": "7.2 Stability", "text": "For supervised feature selection algorithms, their performance are usually evaluated by the classification accuracy. In addition to accuracy, the stability of these algorithms is also an important consideration when developing new feature selection algorithms. A motivating example from the field of bioinformatics shows that domain experts would like to see the same set or similar set of genes (features) to be selected each time when they obtain new samples in the small amount of perturbation. Otherwise domain experts would not trust these algorithms when they get quite different sets of features with small data perturbation. Considering its importance in practical usage, stability of feature selection algorithms has received increasing attention in the community (Kalousis et al., 2007; He and Yu, 2010). It is observed that many well-known feature selection algorithms suffer from the low stability\nproblem after the small data perturbation is introduced in the training set. It is also found in (Alelyani et al., 2011) that the underlying characteristics of data may greatly affect the stability of feature selection algorithms and the stability issue may also be data dependent. These factors include the dimensionality of feature, number of data instances, etc.\nIn against with supervised feature selection, stability of unsupervised feature selection algorithms has not be well studied yet. Studying stability for unsupervised feature selection is much more difficult than that of the supervised methods. The reason is that in unsupervised feature selection, we do not have enough prior knowledge about the cluster structure of the data. Thus we are uncertain that if the new data instance, i.e., the perturbation belongs to any existing clusters or will introduce new clusters. While in supervised feature selection, we have the prior knowledge about the label of each data instance, and a new sample that does not belong to any existing classes will be considered as an outlier and we do not need to modify the selected feature set to adapt to the outliers. In other words, unsupervised feature selection is more sensitive to noise and the noise will affects the stability of the algorithm."}, {"heading": "7.3 Model Selection", "text": "For most feature selection algorithms especially for feature weighting methods, we have to specify the number of selected features. However, it is often unknown what is the optimal number of selected features. With too large number of selected features, it may increase the risk in including some noisy, redundant and irrelevant features which may jeopardize the learning performance. On the other hand, it is also not good to include too small number of selected features, since some relevant features may be eliminated. In practice, we usually adopt a heuristic way to grid search the number of selected features and pick the number that has the best classification or clustering performance, but the whole process is computational expensive. It is still an open and challenging problem to determine the optimal number of selected features.\nIn addition to the optimal number of selected features, we also need to specify the number of clusters or pseudo classes for unsupervised feature selection algorithms. In real world problems, we usually have limited knowledge about the clustering structure of the data. Choosing different number of clusters may lead to merging totally different small clusters into one big cluster or splitting one big cluster into smaller ones. As a consequence, it may result in finding totally different subsets of features. Hence, determining the optimal number of clusters is almost impossible. Some work has been done to estimate these tricky parameters. For instance, in (Tibshirani et al., 2001), a principled way to estimate the number of suitable clusters in a dataset is proposed. However, it is still not clear how to find the best number of clusters for unsupervised feature selection. All in all, we believe that the model selection problem should be paid more attention."}, {"heading": "8. Conclusion", "text": "Feature selection is effective in preprocessing data and reducing data dimensionality that is an essential to successful data mining and machine learning applications. Meanwhile, it has been a hot research topic with practical significance in many areas such as statistics, pattern recognition, machine learning, and data mining (including web, text, image, and\nmicroarrays). The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. The past few years have witnessed the development of hundreds of new feature selection methods. This survey article aims to provide a comprehensive overview about recent advances in feature selection. We first introduce basic concepts of feature selection and emphasis the importance of applying feature selection algorithms to solve practical problems. Then, we classify traditional feature selection algorithms from label perspective and search strategy perspective. Since current categorization cannot meet the rapid development in feature selection, we propose to review recent advances in feature selection algorithms from a data perspective. Following the taxonomy in Figure (7), we surveyed the family of feature selection algorithms in four parts: (1) feature Selection with generic Data; (2) feature selection with structure features; (3) feature selection with heterogeneous data; and (4) feature selection with streaming data. Specifically, we further classify feature selection algorithms with generic data into similarity based, information theoretical based, sparse learning based and statistical based methods from their properties. For feature selection with structure features, we consider three types of structural features, namely group, tree and graph features. The third part feature selection with heterogeneous data consists of feature selection algorithms for linked data, multi-source and multi-view data. The fourth part consists of feature selection for streaming data and streaming features. To facilitate the research on feature selection, in this survey, we also present a feature selection repository - scikit-feast, which includes some of the most popular feature selection algorithms that have been developed in the past few decades. We also provide some suggestions on how to evaluate these feature selection algorithms, either supervised or unsupervised methods. At the end of the survey, we present some open problems and challenges that need to be paid more attention in the future feature selection research.\nIt also should be mentioned that the aim of the survey is not to claim the superiority of some feature selection algorithms over others. On the other hand, our goal is to provide a comprehensive structured list of recent advances of feature selection algorithms and a feature selection repository to promote the research in this community. As a matter of fact, it is at the discretion of users to decide which algorithms or which tools to use in practice."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M Airoldi", "David M Blei", "Stephen E Fienberg", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Airoldi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2009}, {"title": "The effect of the characteristics of the dataset on the selection stability", "author": ["Salem Alelyani", "Huan Liu", "Lei Wang"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Alelyani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alelyani et al\\.", "year": 2011}, {"title": "Feature selection for clustering: A review", "author": ["Salem Alelyani", "Jiliang Tang", "Huan Liu"], "venue": "Data Clustering: Algorithms and Applications,", "citeRegEx": "Alelyani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alelyani et al\\.", "year": 2013}, {"title": "Consistency of the group lasso and multiple kernel learning", "author": ["Francis R Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bach.,? \\Q2008\\E", "shortCiteRegEx": "Bach.", "year": 2008}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["Roberto Battiti"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Battiti.,? \\Q1994\\E", "shortCiteRegEx": "Battiti.", "year": 1994}, {"title": "An improved multitask learning approach with applications in medical diagnosis", "author": ["Jinbo Bi", "Tao Xiong", "Shipeng Yu", "Murat Dundar", "R Bharat Rao"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2008}, {"title": "Active learning for networked data", "author": ["Mustafa Bilgic", "Lilyana Mihalkova", "Lise Getoor"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Bilgic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bilgic et al\\.", "year": 2010}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with oscar", "author": ["Howard D Bondell", "Brian J Reich"], "venue": null, "citeRegEx": "Bondell and Reich.,? \\Q2008\\E", "shortCiteRegEx": "Bondell and Reich.", "year": 2008}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Brown et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Brown et al\\.", "year": 2012}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["Deng Cai", "Chiyuan Zhang", "Xiaofei He"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "The dantzig selector: statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2007}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["Pak K Chan", "Martine DF Schlag", "Jason Y Zien"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on,", "citeRegEx": "Chan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Chan et al\\.", "year": 1994}, {"title": "A survey on feature selection methods", "author": ["Girish Chandrashekar", "Ferat Sahin"], "venue": "Computers & Electrical Engineering,", "citeRegEx": "Chandrashekar and Sahin.,? \\Q2014\\E", "shortCiteRegEx": "Chandrashekar and Sahin.", "year": 2014}, {"title": "Spectral graph theory, volume 92", "author": ["Fan RK Chung"], "venue": "American Mathematical Soc.,", "citeRegEx": "Chung.,? \\Q1997\\E", "shortCiteRegEx": "Chung.", "year": 1997}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["Alexandre d\u2019Aspremont", "Laurent El Ghaoui", "Michael I Jordan", "Gert RG Lanckriet"], "venue": "SIAM review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Statistics and data analysis in geology, volume 646", "author": ["John C Davis", "Robert J Sampson"], "venue": null, "citeRegEx": "Davis and Sampson.,? \\Q1986\\E", "shortCiteRegEx": "Davis and Sampson.", "year": 1986}, {"title": "Feature selection using multiple streams", "author": ["Paramveer S Dhillon", "Dean P Foster", "Lyle H Ungar"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Dhillon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2010}, {"title": "1-pca: rotational invariant l 1-norm principal component analysis for robust subspace factorization", "author": ["Chris Ding", "Ding Zhou", "Xiaofeng He", "Hongyuan Zha. R"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Ding et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2006}, {"title": "Supervised and unsupervised discretization of continuous features", "author": ["James Dougherty", "Ron Kohavi", "Mehran Sahami"], "venue": "In Machine learning: proceedings of the twelfth international conference,", "citeRegEx": "Dougherty et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dougherty et al\\.", "year": 1995}, {"title": "Unsupervised feature selection with adaptive structure learning", "author": ["Liang Du", "Yi-Dong Shen"], "venue": "arXiv preprint arXiv:1504.00736,", "citeRegEx": "Du and Shen.,? \\Q2015\\E", "shortCiteRegEx": "Du and Shen.", "year": 2015}, {"title": "Pattern classification", "author": ["Richard O Duda", "Peter E Hart", "David G Stork"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duda et al\\.", "year": 2012}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "A powerful feature selection approach based on mutual information", "author": ["Ali El Akadi", "Abdeljalil El Ouardighi", "Driss Aboutajdine"], "venue": "International Journal of Computer Science and Network Security,", "citeRegEx": "Akadi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Akadi et al\\.", "year": 2008}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Evgeniou and Pontil.,? \\Q2007\\E", "shortCiteRegEx": "Evgeniou and Pontil.", "year": 2007}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Jianqing Fan", "Runze Li"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Fan and Li.,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li.", "year": 2001}, {"title": "Ultrahigh dimensional feature selection: beyond the linear model", "author": ["Jianqing Fan", "Richard Samworth", "Yichao Wu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2009}, {"title": "An efficient greedy method for unsupervised feature selection", "author": ["Ahmed K Farahat", "Ali Ghodsi", "Mohamed S Kamel"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Farahat et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farahat et al\\.", "year": 2011}, {"title": "Adaptive unsupervised multiview feature selection for visual concept recognition", "author": ["Yinfu Feng", "Jun Xiao", "Yueting Zhuang", "Xiaoming Liu"], "venue": "In Computer Vision\u2013ACCV", "citeRegEx": "Feng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2012}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["Fran\u00e7ois Fleuret"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fleuret.,? \\Q2004\\E", "shortCiteRegEx": "Fleuret.", "year": 2004}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": "arXiv preprint arXiv:1001.0736,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Introduction to statistical pattern recognition", "author": ["Keinosuke Fukunaga"], "venue": "Academic press,", "citeRegEx": "Fukunaga.,? \\Q2013\\E", "shortCiteRegEx": "Fukunaga.", "year": 2013}, {"title": "Variability and mutability, contribution to the study of statistical distribution and relaitons", "author": ["CW Gini"], "venue": "Studi Economico-Giuricici della R,", "citeRegEx": "Gini.,? \\Q1912\\E", "shortCiteRegEx": "Gini.", "year": 1912}, {"title": "Genetic algorithms in search, optimization, and machine learning", "author": ["David E Golberg"], "venue": "Addion wesley,", "citeRegEx": "Golberg.,? \\Q1989\\E", "shortCiteRegEx": "Golberg.", "year": 1989}, {"title": "Towards feature selection in network", "author": ["Quanquan Gu", "Jiawei Han"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "Gu and Han.,? \\Q2011\\E", "shortCiteRegEx": "Gu and Han.", "year": 2011}, {"title": "Generalized fisher score for feature selection", "author": ["Quanquan Gu", "Zhenhui Li", "Jiawei Han"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Gu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2011}, {"title": "Gait feature subset selection by mutual information. Systems, Man and Cybernetics, Part A: Systems and Humans", "author": ["Baofeng Guo", "Mark S Nixon"], "venue": "IEEE Transactions on,", "citeRegEx": "Guo and Nixon.,? \\Q2009\\E", "shortCiteRegEx": "Guo and Nixon.", "year": 2009}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Isabelle Guyon", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning,", "citeRegEx": "Guyon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "Feature extraction: foundations and applications, volume", "author": ["Isabelle Guyon", "Steve Gunn", "Masoud Nikravesh", "Lofti A Zadeh"], "venue": null, "citeRegEx": "Guyon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2008}, {"title": "Feature selection for machine learning: Comparing a correlation-based filter approach to the wrapper", "author": ["Mark A Hall", "Lloyd A Smith"], "venue": "In FLAIRS conference,", "citeRegEx": "Hall and Smith.,? \\Q1999\\E", "shortCiteRegEx": "Hall and Smith.", "year": 1999}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "The elements of statistical learning: data mining, inference and prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman", "James Franklin"], "venue": "The Mathematical Intelligencer,", "citeRegEx": "Hastie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2005}, {"title": "Statistical Learning with Sparsity: The Lasso and Generalizations", "author": ["Trevor Hastie", "Robert Tibshirani", "Martin Wainwright"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Laplacian score for feature selection", "author": ["Xiaofei He", "Deng Cai", "Partha Niyogi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "He et al\\.,? \\Q2005\\E", "shortCiteRegEx": "He et al\\.", "year": 2005}, {"title": "Stable feature selection for biomarker discovery", "author": ["Zengyou He", "Weichuan Yu"], "venue": "Computational biology and chemistry,", "citeRegEx": "He and Yu.,? \\Q2010\\E", "shortCiteRegEx": "He and Yu.", "year": 2010}, {"title": "Applied logistic regression", "author": ["David W Hosmer Jr.", "Stanley Lemeshow"], "venue": null, "citeRegEx": "Jr and Lemeshow.,? \\Q2004\\E", "shortCiteRegEx": "Jr and Lemeshow.", "year": 2004}, {"title": "Feature selection via joint embedding learning and sparse regression", "author": ["Chenping Hou", "Feiping Nie", "Dongyun Yi", "Yi Wu"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "Hou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2011}, {"title": "Actnet: Active learning for networked texts in microblogging", "author": ["Xia Hu", "Jiliang Tang", "Huiji Gao", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "Hu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Unsupervised feature selection on data streams", "author": ["Hao Huang", "Shinjae Yoo", "S Kasiviswanathan"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Learning with structured sparsity", "author": ["Junzhou Huang", "Tong Zhang", "Dimitris Metaxas"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Group lasso with overlap and graph lasso", "author": ["Laurent Jacob", "Guillaume Obozinski", "Jean-Philippe Vert"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Machine learning based on attribute interactions", "author": ["Aleks Jakulin"], "venue": "PhD thesis, Univerza v Ljubljani,", "citeRegEx": "Jakulin.,? \\Q2005\\E", "shortCiteRegEx": "Jakulin.", "year": 2005}, {"title": "Dasso: connections between the dantzig selector and lasso", "author": ["Gareth M James", "Peter Radchenko", "Jinchi Lv"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "James et al\\.,? \\Q2009\\E", "shortCiteRegEx": "James et al\\.", "year": 2009}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["Rodolphe Jenatton", "Julien Mairal", "Francis R Bach", "Guillaume R Obozinski"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Jenatton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2010}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["Rodolphe Jenatton", "Jean-Yves Audibert", "Francis Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Stability of feature selection algorithms: a study on high-dimensional spaces", "author": ["Alexandros Kalousis", "Julien Prados", "Melanie Hilario"], "venue": "Knowledge and information systems,", "citeRegEx": "Kalousis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kalousis et al\\.", "year": 2007}, {"title": "Statistical estimation of correlated genome associations to a quantitative trait network", "author": ["Seyoung Kim", "Eric P Xing"], "venue": "PLoS Genet,", "citeRegEx": "Kim and Xing.,? \\Q2009\\E", "shortCiteRegEx": "Kim and Xing.", "year": 2009}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["Seyoung Kim", "Eric P Xing"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Kim and Xing.,? \\Q2010\\E", "shortCiteRegEx": "Kim and Xing.", "year": 2010}, {"title": "The feature selection problem: Traditional methods and a new algorithm", "author": ["Kenji Kira", "Larry A Rendell"], "venue": "In AAAI,", "citeRegEx": "Kira and Rendell.,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell.", "year": 1992}, {"title": "A practical approach to feature selection", "author": ["Kenji Kira", "Larry A Rendell"], "venue": "In Proceedings of the ninth international workshop on Machine learning,", "citeRegEx": "Kira and Rendell.,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell.", "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John"], "venue": "Artificial intelligence,", "citeRegEx": "Kohavi and John.,? \\Q1997\\E", "shortCiteRegEx": "Kohavi and John.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["Daphne Koller", "Mehran Sahami"], "venue": "In In 13th International Conference on Machine Learning,", "citeRegEx": "Koller and Sahami.,? \\Q1995\\E", "shortCiteRegEx": "Koller and Sahami.", "year": 1995}, {"title": "Discretization techniques: A recent survey", "author": ["Sotiris Kotsiantis", "Dimitris Kanellopoulos"], "venue": "GESTS International Transactions on Computer Science and Engineering,", "citeRegEx": "Kotsiantis and Kanellopoulos.,? \\Q2006\\E", "shortCiteRegEx": "Kotsiantis and Kanellopoulos.", "year": 2006}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert RG Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Feature selection and feature extraction for text categorization", "author": ["David D Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Lewis.,? \\Q1992\\E", "shortCiteRegEx": "Lewis.", "year": 1992}, {"title": "Group feature selection with streaming features", "author": ["Haiguang Li", "Xindong Wu", "Zhao Li", "Wei Ding"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Unsupervised streaming feature selection in social media", "author": ["Jundong Li", "Xia Hu", "Jiliang Tang", "Huan Liu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Zechao Li", "Yi Yang", "Jing Liu", "Xiaofang Zhou", "Hanqing Lu"], "venue": "In AAAI,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Simple and deterministic matrix sketching", "author": ["Edo Liberty"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Liberty.,? \\Q2013\\E", "shortCiteRegEx": "Liberty.", "year": 2013}, {"title": "Conditional infomax learning: an integrated framework for feature extraction and fusion", "author": ["Dahua Lin", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin and Tang.,? \\Q2006\\E", "shortCiteRegEx": "Lin and Tang.", "year": 2006}, {"title": "Chi2: Feature selection and discretization of numeric attributes", "author": ["Huan Liu", "Rudy Setiono"], "venue": "In tai,", "citeRegEx": "Liu and Setiono.,? \\Q1995\\E", "shortCiteRegEx": "Liu and Setiono.", "year": 1995}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Moreau-yosida regularization for grouped tree structure learning", "author": ["Jun Liu", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu and Ye.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2010}, {"title": "Multi-task feature learning via efficient l 2, 1-norm minimization", "author": ["Jun Liu", "Shuiwang Ji", "Jieping Ye"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Global and local structure preservation for feature selection", "author": ["Xinwang Liu", "Lei Wang", "Jian Zhang", "Jianping Yin", "Huan Liu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Sparse logistic regression with lp penalty for biomarker identification", "author": ["Zhenqiu Liu", "Feng Jiang", "Guoliang Tian", "Suna Wang", "Fumiaki Sato", "Stephen J Meltzer", "Ming Tan"], "venue": "Statistical Applications in Genetics and Molecular Biology,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Spectral clustering for multi-type relational data", "author": ["Bo Long", "Zhongfei Mark Zhang", "Xiaoyun Wu", "Philip S Yu"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Long et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Long et al\\.", "year": 2006}, {"title": "A probabilistic framework for relational clustering", "author": ["Bo Long", "Zhongfei Mark Zhang", "Philip S Yu"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Long et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Long et al\\.", "year": 2007}, {"title": "Supervised group lasso with applications to microarray data analysis", "author": ["Shuangge Ma", "Xiao Song", "Jian Huang"], "venue": "BMC bioinformatics,", "citeRegEx": "Ma et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2007}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["Sofus A Macskassy", "Foster Provost"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Macskassy and Provost.,? \\Q2007\\E", "shortCiteRegEx": "Macskassy and Provost.", "year": 2007}, {"title": "Network studies of social influence", "author": ["Peter V Marsden", "Noah E Friedkin"], "venue": "Sociological Methods & Research,", "citeRegEx": "Marsden and Friedkin.,? \\Q1993\\E", "shortCiteRegEx": "Marsden and Friedkin.", "year": 1993}, {"title": "Convex principal feature selection", "author": ["Mahdokht Masaeli", "Yan Yan", "Ying Cui", "Glenn Fung", "Jennifer G Dy"], "venue": "In SDM,", "citeRegEx": "Masaeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Masaeli et al\\.", "year": 2010}, {"title": "Subband correlation and robust speech recognition", "author": ["James McAuley", "Ji Ming", "Darryl Stewart", "Philip Hanna"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "McAuley et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2005}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook"], "venue": "Annual review of sociology,", "citeRegEx": "McPherson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McPherson et al\\.", "year": 2001}, {"title": "The group lasso for logistic regression", "author": ["Lukas Meier", "Sara Van De Geer", "Peter B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Meier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2008}, {"title": "On the use of variable complementarity for feature selection in cancer classification", "author": ["Patrick E Meyer", "Gianluca Bontempi"], "venue": "In Applications of Evolutionary Computing,", "citeRegEx": "Meyer and Bontempi.,? \\Q2006\\E", "shortCiteRegEx": "Meyer and Bontempi.", "year": 2006}, {"title": "Information-theoretic feature selection in microarray data using variable complementarity", "author": ["Patrick Emmanuel Meyer", "Colas Schretter", "Gianluca Bontempi"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "Meyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 2008}, {"title": "Unsupervised feature selection using feature similarity", "author": ["Pabitra Mitra", "CA Murthy", "Sankar K. Pal"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Mitra et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2002}, {"title": "Manifestation of emerging specialties in journal literature: A growth model of papers, references, exemplars, bibliographic coupling, cocitation, and clustering coefficient distribution", "author": ["Steven A Morris"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Morris.,? \\Q2005\\E", "shortCiteRegEx": "Morris.", "year": 2005}, {"title": "A branch and bound algorithm for feature subset selection", "author": ["Patrenahalli M Narendra", "Keinosuke Fukunaga"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Narendra and Fukunaga.,? \\Q1977\\E", "shortCiteRegEx": "Narendra and Fukunaga.", "year": 1977}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Finding and evaluating community structure in networks", "author": ["Mark EJ Newman", "Michelle Girvan"], "venue": "Physical review E,", "citeRegEx": "Newman and Girvan.,? \\Q2004\\E", "shortCiteRegEx": "Newman and Girvan.", "year": 2004}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Trace ratio criterion for feature selection", "author": ["Feiping Nie", "Shiming Xiang", "Yangqing Jia", "Changshui Zhang", "Shuicheng Yan"], "venue": "In AAAI,", "citeRegEx": "Nie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2008}, {"title": "Efficient and robust feature selection via joint 2, 1-norms minimization", "author": ["Feiping Nie", "Heng Huang", "Xiao Cai", "Chris H Ding"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Nie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2010}, {"title": "Joint covariate selection for grouped classification", "author": ["Guillaume Obozinski", "Ben Taskar", "Michael Jordan"], "venue": "Technical report,", "citeRegEx": "Obozinski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer", "author": ["Jie Peng", "Ji Zhu", "Anna Bergamaschi", "Wonshik Han", "Dong-Young Noh", "Jonathan R Pollack", "Pei Wang"], "venue": "The annals of applied statistics,", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "Online feature selection using grafting", "author": ["Simon Perkins", "James Theiler"], "venue": "In ICML, pages 592\u2013599,", "citeRegEx": "Perkins and Theiler.,? \\Q2003\\E", "shortCiteRegEx": "Perkins and Theiler.", "year": 2003}, {"title": "Grafting: Fast, incremental feature selection by gradient descent in function space", "author": ["Simon Perkins", "Kevin Lacker", "James Theiler"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Perkins et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Perkins et al\\.", "year": 2003}, {"title": "Robust unsupervised feature selection", "author": ["Mingjie Qian", "Chengxiang Zhai"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "Qian and Zhai.,? \\Q2013\\E", "shortCiteRegEx": "Qian and Zhai.", "year": 2013}, {"title": "Induction of decision trees", "author": ["J. Ross Quinlan"], "venue": "Machine learning,", "citeRegEx": "Quinlan.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan.", "year": 1986}, {"title": "5: programs for machine learning", "author": ["J Ross Quinlan. C"], "venue": null, "citeRegEx": "C4.,? \\Q1993\\E", "shortCiteRegEx": "C4.", "year": 1993}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine learning,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko.", "year": 2003}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": null, "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Regularized learning with networks of features", "author": ["Ted Sandler", "John Blitzer", "Partha P Talukdar", "Lyle H Ungar"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sandler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sandler et al\\.", "year": 2009}, {"title": "Mullert. Fisher discriminant analysis with kernels", "author": ["Bernhard Scholkopft", "Klaus-Robert"], "venue": "Neural networks for signal processing IX,", "citeRegEx": "Scholkopft and Klaus.Robert,? \\Q1999\\E", "shortCiteRegEx": "Scholkopft and Klaus.Robert", "year": 1999}, {"title": "Regression approaches for microarray data analysis", "author": ["Mark R Segal", "Kam D Dahlquist", "Bruce R Conklin"], "venue": "Journal of Computational Biology,", "citeRegEx": "Segal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Segal et al\\.", "year": 2003}, {"title": "Collective classification in network data", "author": ["Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Galligher", "Tina Eliassi-Rad"], "venue": "AI magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications Review,", "citeRegEx": "Shannon.,? \\Q2001\\E", "shortCiteRegEx": "Shannon.", "year": 2001}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shi and Malik.,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik.", "year": 2000}, {"title": "Parallel large scale feature selection for logistic regression", "author": ["Sameer Singh", "Jeremy Kubica", "Scott Larsen", "Daria Sorokina"], "venue": "In SDM,", "citeRegEx": "Singh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "Local fisher discriminant analysis for supervised dimensionality reduction", "author": ["Masashi Sugiyama"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Sugiyama.,? \\Q2006\\E", "shortCiteRegEx": "Sugiyama.", "year": 2006}, {"title": "Towards ultrahigh dimensional feature selection for big data", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Feature selection with linked data in social media", "author": ["Jiliang Tang", "Huan Liu"], "venue": "In SDM, pages 118\u2013128", "citeRegEx": "Tang and Liu.,? \\Q2012\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2012}, {"title": "Unsupervised feature selection for linked social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Tang and Liu.,? \\Q2012\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2012}, {"title": "Feature selection for social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "citeRegEx": "Tang and Liu.,? \\Q2014\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2014}, {"title": "An unsupervised feature selection framework for social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Tang and Liu.,? \\Q2014\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2014}, {"title": "Unsupervised feature selection for multiview data in social media", "author": ["Jiliang Tang", "Xia Hu", "Huiji Gao", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Feature selection for classification: A review", "author": ["Jiliang Tang", "Salem Alelyani", "Huan Liu"], "venue": "Data Classification: Algorithms and Applications,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["Robert Tibshirani", "Guenther Walther", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tibshirani et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2001}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["Robert Tibshirani", "Michael Saunders", "Saharon Rosset", "Ji Zhu", "Keith Knight"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tibshirani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2005}, {"title": "Numerical recipes: example book (C)", "author": ["William T Vetterling", "Saul A Teukolsky", "William H Press"], "venue": "Press Syndicate of the University of Cambridge,", "citeRegEx": "Vetterling et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Vetterling et al\\.", "year": 1992}, {"title": "Object recognition with informative features and linear classification", "author": ["Michel Vidal-Naquet", "Shimon Ullman"], "venue": "In ICCV,", "citeRegEx": "Vidal.Naquet and Ullman.,? \\Q2003\\E", "shortCiteRegEx": "Vidal.Naquet and Ullman.", "year": 2003}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Multi-view clustering and feature learning via structured sparsity", "author": ["Hua Wang", "Feiping Nie", "Heng Huang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Online feature selection and its applications", "author": ["Jialei Wang", "Peilin Zhao", "Steven CH Hoi", "Rong Jin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Online group feature selection", "author": ["Jing Wang", "Zhong-Qiu Zhao", "Xuegang Hu", "Yiu-Ming Cheung", "Meng Wang", "Xindong Wu"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Online feature selection with group structure analysis", "author": ["Jing Wang", "Meng Wang", "Peipei Li", "Luoqi Liu", "Zhongqiu Zhao", "Xuegang Hu", "Xindong Wu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The interpretation of population structure by f-statistics with special regard to systems of mating", "author": ["Sewall Wright"], "venue": "Evolution, pages 395\u2013420,", "citeRegEx": "Wright.,? \\Q1965\\E", "shortCiteRegEx": "Wright.", "year": 1965}, {"title": "Online streaming feature selection", "author": ["Xindong Wu", "Kui Yu", "Hao Wang", "Wei Ding"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Online feature selection with streaming features", "author": ["Xindong Wu", "Kui Yu", "Wei Ding", "Hao Wang", "Xingquan Zhu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "N3lars: Minimum redundancy maximum relevance feature selection for large and high-dimensional data", "author": ["Makoto Yamada", "Avishek Saha", "Hua Ouyang", "Dawei Yin", "Yi Chang"], "venue": "arXiv preprint arXiv:1411.2331,", "citeRegEx": "Yamada et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2014}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["Howard Hua Yang", "John E Moody"], "venue": "In NIPS,", "citeRegEx": "Yang and Moody.,? \\Q1999\\E", "shortCiteRegEx": "Yang and Moody.", "year": 1999}, {"title": "Feature grouping and selection over an undirected graph", "author": ["Sen Yang", "Lei Yuan", "Ying-Cheng Lai", "Xiaotong Shen", "Peter Wonka", "Jieping Ye"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Yi Yang", "Dong Xu", "Feiping Nie", "Shuicheng Yan", "Yueting Zhuang"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Yi Yang", "Heng Tao Shen", "Zhigang Ma", "Zi Huang", "Xiaofang Zhou"], "venue": "In IJCAI ProceedingsInternational Joint Conference on Artificial Intelligence,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Sparse methods for biomedical data", "author": ["Jieping Ye", "Jun Liu"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Ye and Liu.,? \\Q2012\\E", "shortCiteRegEx": "Ye and Liu.", "year": 2012}, {"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution", "author": ["Lei Yu", "Huan Liu"], "venue": "In ICML,", "citeRegEx": "Yu and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2003}, {"title": "Multiclass spectral clustering", "author": ["Stella X Yu", "Jianbo Shi"], "venue": "In Computer Vision,", "citeRegEx": "Yu and Shi.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Shi.", "year": 2003}, {"title": "Efficient methods for overlapping group lasso", "author": ["Lei Yuan", "Jun Liu", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yuan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2011}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "Flexible latent variable models for multi-task learning", "author": ["Jian Zhang", "Zoubin Ghahramani", "Yiming Yang"], "venue": "Machine Learning,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Towards mining trapezoidal data streams", "author": ["Qin Zhang", "Peng Zhang", "Guodong Long", "Wei Ding", "Chengqi Zhang", "Xindong Wu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zhao and Yu.,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu.", "year": 2006}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["Peng Zhao", "Guilherme Rocha", "Bin Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Zhao and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Zhao and Liu.", "year": 2007}, {"title": "Multi-source feature selection via geometry-dependent covariance analysis", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In FSDM,", "citeRegEx": "Zhao and Liu.,? \\Q2008\\E", "shortCiteRegEx": "Zhao and Liu.", "year": 2008}, {"title": "Massively parallel feature selection: an approach based on variance preservation", "author": ["Zheng Zhao", "Ruiwen Zhang", "James Cox", "David Duling", "Warren Sarle"], "venue": "Machine learning,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Learning from labeled and unlabeled data on a directed graph", "author": ["Dengyong Zhou", "Jiayuan Huang", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Modeling disease progression via fused sparse group lasso", "author": ["Jiayu Zhou", "Jun Liu", "Vaibhav A Narayan", "Jieping Ye"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Streaming feature selection using alpha-investing", "author": ["Jing Zhou", "Dean Foster", "Robert Stine", "Lyle Ungar"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "The adaptive lasso and its oracle properties", "author": ["Hui Zou"], "venue": "Journal of the American statistical association,", "citeRegEx": "Zou.,? \\Q2006\\E", "shortCiteRegEx": "Zou.", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 42, "context": "When applying data mining and machine learning algorithms on high dimensional data, a critical issue is known as curse of dimensionality (Hastie et al., 2005).", "startOffset": 137, "endOffset": 158}, {"referenceID": 56, "context": "Examples of feature extraction methods include Principle Component Analysis (PCA) (Jolliffe, 2002), Linear Discriminant Analysis (LDA) (Scholkopft and Mullert, 1999), Canonical Correlation Analysis (CCA) (Hardoon et al.", "startOffset": 82, "endOffset": 98}, {"referenceID": 41, "context": "Examples of feature extraction methods include Principle Component Analysis (PCA) (Jolliffe, 2002), Linear Discriminant Analysis (LDA) (Scholkopft and Mullert, 1999), Canonical Correlation Analysis (CCA) (Hardoon et al., 2004), Singular Value Decomposition (Golub and Van Loan, 2012), ISOMAP (Tenenbaum et al.", "startOffset": 204, "endOffset": 226}, {"referenceID": 123, "context": ", 2004), Singular Value Decomposition (Golub and Van Loan, 2012), ISOMAP (Tenenbaum et al., 2000) and Locally Linear Embedding (LLE) (Roweis and Saul, 2000).", "startOffset": 73, "endOffset": 97}, {"referenceID": 107, "context": ", 2000) and Locally Linear Embedding (LLE) (Roweis and Saul, 2000).", "startOffset": 43, "endOffset": 66}, {"referenceID": 124, "context": "Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al.", "startOffset": 6, "endOffset": 24}, {"referenceID": 14, "context": "Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 99, "context": "Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al., 2005), Fisher Score (Duda et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 21, "context": ", 2005), Fisher Score (Duda et al., 2012), Laplacian Score (He et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 44, "context": ", 2012), Laplacian Score (He et al., 2005), and SPEC (Zhao and Liu, 2007) are some of the well known feature selection techniques.", "startOffset": 25, "endOffset": 42}, {"referenceID": 151, "context": ", 2005), and SPEC (Zhao and Liu, 2007) are some of the well known feature selection techniques.", "startOffset": 18, "endOffset": 38}, {"referenceID": 37, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 70, "endOffset": 97}, {"referenceID": 62, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 138, "endOffset": 161}, {"referenceID": 91, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 187, "endOffset": 216}, {"referenceID": 33, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 237, "endOffset": 252}, {"referenceID": 106, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al.", "startOffset": 88, "endOffset": 149}, {"referenceID": 63, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al.", "startOffset": 171, "endOffset": 223}, {"referenceID": 37, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al.", "startOffset": 171, "endOffset": 223}, {"referenceID": 143, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al., 2005), feature ability to preserve data manifold structure (He et al.", "startOffset": 244, "endOffset": 281}, {"referenceID": 99, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al., 2005), feature ability to preserve data manifold structure (He et al.", "startOffset": 244, "endOffset": 281}, {"referenceID": 44, "context": ", 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al.", "startOffset": 61, "endOffset": 115}, {"referenceID": 35, "context": ", 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al.", "startOffset": 61, "endOffset": 115}, {"referenceID": 151, "context": ", 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al.", "startOffset": 61, "endOffset": 115}, {"referenceID": 83, "context": ", 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al., 2010; Farahat et al., 2011).", "startOffset": 82, "endOffset": 126}, {"referenceID": 27, "context": ", 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al., 2010; Farahat et al., 2011).", "startOffset": 82, "endOffset": 126}, {"referenceID": 37, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 2, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 12, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 122, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 44, "context": "1 Laplacian Score (He et al., 2005) (Unsupervised) Laplacian Score is an unsupervised feature selection algorithm which selects features that can best preserve the data manifold structure.", "startOffset": 18, "endOffset": 35}, {"referenceID": 13, "context": "The diagonal matrix D is defined as D(i, i) = \u2211n j=1 S(i, j) and the laplacian matrix L is L = D \u2212 S (Chung, 1997).", "startOffset": 101, "endOffset": 114}, {"referenceID": 151, "context": "2 SPEC (Zhao and Liu, 2007) (Unsupervised and Supervised) SPEC is an extension of Laplacian Score that work for both supervised and unsupervised scenarios.", "startOffset": 7, "endOffset": 27}, {"referenceID": 13, "context": "Afterwards the construction of affinity matrix S, the diagonal matrix D is defined as D(i, i) = \u2211n j=1 S(i, j) and the normalized laplacian matrix Lnorm is Lnorm = D \u2212 1 2 (D \u2212 S)D 12 (Chung, 1997).", "startOffset": 184, "endOffset": 197}, {"referenceID": 21, "context": "3 Fisher Score (Duda et al., 2012) (Supervised) Fisher Score is a supervised feature selection algorithm.", "startOffset": 15, "endOffset": 34}, {"referenceID": 44, "context": "According to (He et al., 2005), Fisher Score can be considered as a special case of Laplacian Score as long as the affinity matrix is as follows:", "startOffset": 13, "endOffset": 30}, {"referenceID": 35, "context": "To tackle this issue, a Generalized Fisher Score method Gu et al. (2011) is proposed to jointly select features.", "startOffset": 56, "endOffset": 73}, {"referenceID": 95, "context": "4 Trace Ratio Criterion (Nie et al., 2008) (Supervised) Recently, the trace ratio criterion has been proposed to directly select the global optimal feature subset based on the corresponding score, which is computed in a trace ratio norm.", "startOffset": 24, "endOffset": 42}, {"referenceID": 106, "context": "5 ReliefF (Robnik-\u0160ikonja and Kononenko, 2003) (Supervised) Relief and its multi-class variant ReliefF are supervised filter algorithms that select features to separate instances from different classes.", "startOffset": 10, "endOffset": 46}, {"referenceID": 151, "context": "Then according to (Zhao and Liu, 2007), the criterion of ReliefF is equivalent to the following with above assumptions:", "startOffset": 18, "endOffset": 38}, {"referenceID": 21, "context": "As indicated in (Duda et al., 2012), many handdesigned information theoretic criteria are proposed to maximize feature relevance and minimize feature redundancy.", "startOffset": 16, "endOffset": 35}, {"referenceID": 19, "context": "For numeric feature values, some data discretization techniques (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006) are required.", "startOffset": 64, "endOffset": 124}, {"referenceID": 64, "context": "For numeric feature values, some data discretization techniques (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006) are required.", "startOffset": 64, "endOffset": 124}, {"referenceID": 8, "context": "Two decades of research on information theoretic criteria can be unified in a conditional likelihood maximization framework, and most algorithms can be reduced to be a specific case of the unified framework (Brown et al., 2012) .", "startOffset": 207, "endOffset": 227}, {"referenceID": 112, "context": "The concept of information gain (Shannon, 2001) between X and Y is used to measure their dependency with entropy and conditional entropy.", "startOffset": 32, "endOffset": 47}, {"referenceID": 66, "context": "1 Mutual Information Maximization (or Information Gain) (Lewis, 1992) Mutual Information Maximization (MIM)(also known as Information Gain) measures the importance of a feature by its correlation with the class label.", "startOffset": 56, "endOffset": 69}, {"referenceID": 4, "context": "2 Mutual Information Feature Selection (Battiti, 1994) A limitation of MIM feature selection criterion is that it assumes that features are independent of each other.", "startOffset": 39, "endOffset": 54}, {"referenceID": 99, "context": "3 Minimum Redundancy Maximum Relevance (Peng et al., 2005) Unlike MIFS that empirically sets \u03b2 to be one, (Peng et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 99, "context": ", 2005) Unlike MIFS that empirically sets \u03b2 to be one, (Peng et al., 2005) proposed a Minimum Redundancy Maximum Relevance (MRMR) criterion to set the value of \u03b2 the reverse of the number of selected features: JMRMR(Xk) = I(Xk;Y )\u2212 1 |S| \u2211", "startOffset": 55, "endOffset": 74}, {"referenceID": 71, "context": "4 Conditional Infomax Feature Extraction (Lin and Tang, 2006) MIFS and MRMR consider both feature relevance and feature redundancy at the same time.", "startOffset": 41, "endOffset": 61}, {"referenceID": 69, "context": "4 Conditional Infomax Feature Extraction (Lin and Tang, 2006) MIFS and MRMR consider both feature relevance and feature redundancy at the same time. However, some studies Lin and Tang (2006); El Akadi et al.", "startOffset": 42, "endOffset": 191}, {"referenceID": 23, "context": "However, some studies Lin and Tang (2006); El Akadi et al. (2008); Guo and Nixon (2009) show that in contrast to minimize the feature redundancy, the conditional redundancy between unselected features and already selected features given class labels should be maximized.", "startOffset": 46, "endOffset": 66}, {"referenceID": 23, "context": "However, some studies Lin and Tang (2006); El Akadi et al. (2008); Guo and Nixon (2009) show that in contrast to minimize the feature redundancy, the conditional redundancy between unselected features and already selected features given class labels should be maximized.", "startOffset": 46, "endOffset": 88}, {"referenceID": 138, "context": "5 Joint Mutual Information (Yang and Moody, 1999) MIFS and MRMR reduce feature redundancy in the feature selection process.", "startOffset": 27, "endOffset": 49}, {"referenceID": 138, "context": "An alternative criterion, Joint Mutual Information (Yang and Moody, 1999; Meyer et al., 2008) was proposed to increase complementary information that are shared between new unselected feature and selected features given the class labels.", "startOffset": 51, "endOffset": 93}, {"referenceID": 88, "context": "An alternative criterion, Joint Mutual Information (Yang and Moody, 1999; Meyer et al., 2008) was proposed to increase complementary information that are shared between new unselected feature and selected features given the class labels.", "startOffset": 51, "endOffset": 93}, {"referenceID": 8, "context": "In (Brown et al., 2012), the authors demonstrate that with simple manipulations, the JMI criterion can be re-written as: JJMI(Xk) = I(Xk;Y )\u2212 1 |S| \u2211", "startOffset": 3, "endOffset": 23}, {"referenceID": 29, "context": "6 Conditional Mutual Information Maximization (Fleuret, 2004) Previously mentioned information theoretic feature selection criterion can be reduced to a linear combination of Shannon information terms.", "startOffset": 46, "endOffset": 61}, {"referenceID": 128, "context": "Among them, Conditional Mutual Information Maximization (CMIM) (Vidal-Naquet and Ullman, 2003; Fleuret, 2004) is a criterion which iteratively selects features which maximize the mutual information with the class labels given the selected features so far.", "startOffset": 63, "endOffset": 109}, {"referenceID": 29, "context": "Among them, Conditional Mutual Information Maximization (CMIM) (Vidal-Naquet and Ullman, 2003; Fleuret, 2004) is a criterion which iteratively selects features which maximize the mutual information with the class labels given the selected features so far.", "startOffset": 63, "endOffset": 109}, {"referenceID": 128, "context": "7 Informative Fragments (Vidal-Naquet and Ullman, 2003) In (Vidal-Naquet and Ullman, 2003), the authors propose a feature selection criterion called Informative Fragments (IG).", "startOffset": 24, "endOffset": 55}, {"referenceID": 128, "context": "7 Informative Fragments (Vidal-Naquet and Ullman, 2003) In (Vidal-Naquet and Ullman, 2003), the authors propose a feature selection criterion called Informative Fragments (IG).", "startOffset": 59, "endOffset": 90}, {"referenceID": 52, "context": "8 Interaction Capping (Jakulin, 2005) Interaction Capping is a similar feature selection criterion as CMIM in Eq.", "startOffset": 22, "endOffset": 37}, {"referenceID": 87, "context": "9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006) Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al.", "startOffset": 37, "endOffset": 63}, {"referenceID": 87, "context": "9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006) Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al.", "startOffset": 169, "endOffset": 195}, {"referenceID": 39, "context": "9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006) Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al., 2008):", "startOffset": 262, "endOffset": 282}, {"referenceID": 143, "context": "10 Fast Correlation Based Filter (Yu and Liu, 2003) There are other information theoretical based feature selection methods that can not be simply be reduced to the unified conditional likelihood maximization framework.", "startOffset": 33, "endOffset": 51}, {"referenceID": 143, "context": "Here, we introduce one algorithm named Fast Correlation Based Filter (FCBF) (Yu and Liu, 2003).", "startOffset": 76, "endOffset": 94}, {"referenceID": 38, "context": "At the very beginning, they use the whole set of features to train a learning model and then attempt to remove some features by setting the feature coefficients to zero, while maintaining the model performance, one example method in this category is recursive feature elimination methods using support vector machine (SVM) (Guyon et al., 2002).", "startOffset": 323, "endOffset": 343}, {"referenceID": 104, "context": "The second type of embedded methods contain a built-in feature selection mechanism such as ID3 (Quinlan, 1986) and C4.", "startOffset": 95, "endOffset": 110}, {"referenceID": 124, "context": "1 Feature Selection with l1-norm Regularizer (Supervised) (Tibshirani, 1996; Hastie et al., 2015) First, we consider the binary classification (yi is either 0 or 1) or regression problem with only one regression target.", "startOffset": 58, "endOffset": 97}, {"referenceID": 43, "context": "1 Feature Selection with l1-norm Regularizer (Supervised) (Tibshirani, 1996; Hastie et al., 2015) First, we consider the binary classification (yi is either 0 or 1) or regression problem with only one regression target.", "startOffset": 58, "endOffset": 97}, {"referenceID": 124, "context": "One main advantage of l1-norm regularization (Lasso) (Tibshirani, 1996; Hastie et al., 2015) is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero.", "startOffset": 53, "endOffset": 92}, {"referenceID": 43, "context": "One main advantage of l1-norm regularization (Lasso) (Tibshirani, 1996; Hastie et al., 2015) is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero.", "startOffset": 53, "endOffset": 92}, {"referenceID": 124, "context": "\u2022 Lasso Regularization (Tibshirani, 1996): Lasso is short for least absolute shrinkage and selection operator, it is based on the l1-norm regularization term on the feature coefficient w: penalty(w) = ||w||1 = d", "startOffset": 23, "endOffset": 41}, {"referenceID": 157, "context": "\u2022 Adaptive Lasso Regularization (Zou, 2006): The Lasso variable selection phase is consistent if it satisfies non-trivial solutions.", "startOffset": 32, "endOffset": 43}, {"referenceID": 149, "context": "However, this condition is difficult to satisfy in some scenarios (Zhao and Yu, 2006).", "startOffset": 66, "endOffset": 85}, {"referenceID": 25, "context": "Another critical issue of Lasso is that the lasso shrinkage produces biased estimates for the large coefficients, and thus it could be suboptimal in terms of estimation risk (Fan and Li, 2001).", "startOffset": 174, "endOffset": 192}, {"referenceID": 157, "context": "In (Zou, 2006), the authors show that the adaptive lasso enjoys the oracle properties and can be solved by the same efficient algorithm for solving the Lasso.", "startOffset": 3, "endOffset": 14}, {"referenceID": 158, "context": "\u2022 Elastic Net Regularization (Zou and Hastie, 2005): In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications.", "startOffset": 29, "endOffset": 51}, {"referenceID": 89, "context": "image processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other.", "startOffset": 49, "endOffset": 107}, {"referenceID": 110, "context": "image processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other.", "startOffset": 49, "endOffset": 107}, {"referenceID": 77, "context": "image processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other.", "startOffset": 49, "endOffset": 107}, {"referenceID": 158, "context": "To handle features with high correlations, Elastic Net regularization (Zou and Hastie, 2005) is proposed as:", "startOffset": 70, "endOffset": 92}, {"referenceID": 10, "context": "In (Candes and Tao, 2007; James et al., 2009), the authors show that the errors of Dantzig selector is up to a logarithmic factor log(d) (d is the feature dimensionality).", "startOffset": 3, "endOffset": 45}, {"referenceID": 53, "context": "In (Candes and Tao, 2007; James et al., 2009), the authors show that the errors of Dantzig selector is up to a logarithmic factor log(d) (d is the feature dimensionality).", "startOffset": 3, "endOffset": 45}, {"referenceID": 53, "context": "Strong theoretical results in (James et al., 2009) show that LASSO and Dantzig selector are closely related.", "startOffset": 30, "endOffset": 50}, {"referenceID": 97, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 24, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 5, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 147, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 146, "context": "The l2,1-norm regularization has strong connections with group lasso (Yuan and Lin, 2006) which will be explained later.", "startOffset": 69, "endOffset": 89}, {"referenceID": 96, "context": "3 Efficient and Robust Feature Selection (Supervised) (Nie et al., 2010) In (Nie et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 96, "context": ", 2010) In (Nie et al., 2010), the authors propose an efficient and robust feature selection (REFS) method by employing a joint l2,1-norm minimization on both the loss function and the regularization.", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "The reason is that l2,1norm loss function has a rotational invariant property (Ding et al., 2006).", "startOffset": 78, "endOffset": 97}, {"referenceID": 96, "context": "In (Nie et al., 2010), an efficient algorithm is proposed to solve this optimization problem with strict convergence analysis.", "startOffset": 3, "endOffset": 21}, {"referenceID": 9, "context": "4 Multi-Cluster Feature Selection (Unsupervised) (Cai et al., 2010) Most of existing sparse learning based approaches build a learning model with the supervision of class labels.", "startOffset": 49, "endOffset": 67}, {"referenceID": 9, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 141, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 47, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 69, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 103, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 76, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 20, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 9, "context": "Multi-Cluster Feature Selection (MCFS) (Cai et al., 2010) is one of the first unsupervised feature selection algorithm using sparse learning techniques.", "startOffset": 39, "endOffset": 57}, {"referenceID": 11, "context": "In the first step, spectral clustering (Chan et al., 1994; Ng et al., 2002) is applied on the dataset to detect the cluster structure of the data.", "startOffset": 39, "endOffset": 75}, {"referenceID": 94, "context": "In the first step, spectral clustering (Chan et al., 1994; Ng et al., 2002) is applied on the dataset to detect the cluster structure of the data.", "startOffset": 39, "endOffset": 75}, {"referenceID": 141, "context": "5 l2,1-norm Regularized Discriminative Feature Selection (Unsupervised) (Yang et al., 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 44, "context": ", 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010).", "startOffset": 157, "endOffset": 212}, {"referenceID": 151, "context": ", 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010).", "startOffset": 157, "endOffset": 212}, {"referenceID": 9, "context": ", 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010).", "startOffset": 157, "endOffset": 212}, {"referenceID": 31, "context": "An alternative way is to exploit the discriminative information encoded in the data that has been proven to be effective in many learning tasks (Fukunaga, 2013).", "startOffset": 144, "endOffset": 160}, {"referenceID": 141, "context": "In (Yang et al., 2011), the authors propose a new unsupervised feature selection algorithm", "startOffset": 3, "endOffset": 22}, {"referenceID": 141, "context": "Instead of using global discriminative information, authors in (Yang et al., 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 115, "context": ", 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al., 2010) to select discriminative features.", "startOffset": 64, "endOffset": 99}, {"referenceID": 140, "context": ", 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al., 2010) to select discriminative features.", "startOffset": 64, "endOffset": 99}, {"referenceID": 140, "context": "Following the definition of global discriminative information (Yang et al., 2010; Fukunaga, 2013), the local discriminative score for each instance xi is computed as: DSi = tr[(S (i) t + \u03bbId) S (i) b ] = tr[WXP(i)X\u0303i \u2032 (X\u0303iX\u0303i \u2032 + \u03bbId) X\u0303iP \u2032 (i)XW], (70)", "startOffset": 62, "endOffset": 97}, {"referenceID": 31, "context": "Following the definition of global discriminative information (Yang et al., 2010; Fukunaga, 2013), the local discriminative score for each instance xi is computed as: DSi = tr[(S (i) t + \u03bbId) S (i) b ] = tr[WXP(i)X\u0303i \u2032 (X\u0303iX\u0303i \u2032 + \u03bbId) X\u0303iP \u2032 (i)XW], (70)", "startOffset": 62, "endOffset": 97}, {"referenceID": 69, "context": "6 Feature Selection Using Nonnegative Spectral Analysis (Unsupervised) (Li et al., 2012) In addition to UDFS, there are some other ways to exploit discriminative information for unsupervised feature selection.", "startOffset": 71, "endOffset": 88}, {"referenceID": 47, "context": "Nonnegative Discriminative Feature Selection (NDFS) (Hou et al., 2011) is an algorithm which performs spectral clustering and feature selection simultaneously in a joint framework to select a subset of discriminative features.", "startOffset": 52, "endOffset": 70}, {"referenceID": 113, "context": "NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved (Shi and Malik, 2000; Yu and Shi, 2003).", "startOffset": 132, "endOffset": 171}, {"referenceID": 144, "context": "NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved (Shi and Malik, 2000; Yu and Shi, 2003).", "startOffset": 132, "endOffset": 171}, {"referenceID": 47, "context": "7 Feature selection via joint embedding learning and sparse regression (Unsupervised) (Hou et al., 2011) Feature selection via joint embedding learning and sparse regression (JELSR) (Hou et al.", "startOffset": 86, "endOffset": 104}, {"referenceID": 47, "context": ", 2011) Feature selection via joint embedding learning and sparse regression (JELSR) (Hou et al., 2011) is an unsupervised feature selection that is similar to NDFS.", "startOffset": 85, "endOffset": 103}, {"referenceID": 107, "context": "In the second step, instead of using some explicit affinity matrix S like MCFS and NDFS, JELSR takes advantage of the local linear embedding (Roweis and Saul, 2000) to learn the local approximation matrix, i.", "startOffset": 141, "endOffset": 164}, {"referenceID": 98, "context": "1 Low Variance (Pedregosa et al., 2011) (Unsupervised) Low Variance is a simple feature selection algorithm which eliminates the feature whose variance is below some threshold.", "startOffset": 15, "endOffset": 39}, {"referenceID": 16, "context": "2 T-score (Davis and Sampson, 1986) (Supervised) t-score is used for binary classification problems.", "startOffset": 10, "endOffset": 35}, {"referenceID": 134, "context": "3 F-score (Wright, 1965) (Supervised) t-score can only be applied for binary classification task, therefore it has some limitations.", "startOffset": 10, "endOffset": 24}, {"referenceID": 72, "context": "4 Chi-Square Score (Liu and Setiono, 1995) (Supervised) Chi-square score utilizes the test of independence to assess whether the feature is independent of the class label.", "startOffset": 19, "endOffset": 42}, {"referenceID": 32, "context": "5 Gini Index (Gini, 1912) (Supervised) Gini index is a statistical measure to quantify if the feature is able to separate instances from different classes.", "startOffset": 13, "endOffset": 25}, {"referenceID": 40, "context": "6 CFS (Hall and Smith, 1999) (Supervised) The basic idea of CFS is to use a correlation based heuristic to evaluate the worth of feature subset F : CFS score(F) = krcf", "startOffset": 6, "endOffset": 28}, {"referenceID": 127, "context": "In order to get the feature-class correlation and feature-feature correlation, CFS uses symmetrical uncertainty (Vetterling et al., 1992) to estimate the degree of associations between two attributes.", "startOffset": 112, "endOffset": 137}, {"referenceID": 142, "context": "For example, these feature selection methods may select the same subset of features even though the features are reshuffled (Ye and Liu, 2012).", "startOffset": 124, "endOffset": 142}, {"referenceID": 146, "context": "One of the most common example is that in multifactor analysis-of-variance (ANOVA), each factor is associated with several groups and can be expressed by a set of dummy features (Yuan and Lin, 2006).", "startOffset": 178, "endOffset": 198}, {"referenceID": 84, "context": "Some other examples include different frequency bands represented as groups in signal processing (McAuley et al., 2005) and genes with similar functionalities acting as groups in bioinformatics (Ma et al.", "startOffset": 97, "endOffset": 119}, {"referenceID": 80, "context": ", 2005) and genes with similar functionalities acting as groups in bioinformatics (Ma et al., 2007).", "startOffset": 82, "endOffset": 99}, {"referenceID": 146, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 146, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 3, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 51, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 86, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 30, "context": "2 Sparse Group Lasso (Supervised) (Friedman et al., 2010; Peng et al., 2010) Once Group Lasso selects a group, all the features in the group will be selected.", "startOffset": 34, "endOffset": 76}, {"referenceID": 100, "context": "2 Sparse Group Lasso (Supervised) (Friedman et al., 2010; Peng et al., 2010) Once Group Lasso selects a group, all the features in the group will be selected.", "startOffset": 34, "endOffset": 76}, {"referenceID": 30, "context": "Sparse Group Lasso (Friedman et al., 2010) takes advantage of both Lasso and Group Lasso, and it produces a solution with simultaneous intra-group and inter-group sparsity.", "startOffset": 19, "endOffset": 42}, {"referenceID": 51, "context": "3 Overlapping Sparse Group Lasso (Supervised) (Jacob et al., 2009) Above methods consider the disjoint group structures among features.", "startOffset": 46, "endOffset": 66}, {"referenceID": 51, "context": "However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 55, "context": "However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 150, "context": "However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 142, "context": "One motivating example is the usage of biologically meaningful gene/protein groups mentioned in (Ye and Liu, 2012).", "startOffset": 96, "endOffset": 114}, {"referenceID": 74, "context": "Another motivating example is that genes/proteins may form certain hierarchical tree structures (Liu and Ye, 2010).", "startOffset": 96, "endOffset": 114}, {"referenceID": 59, "context": "Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010).", "startOffset": 132, "endOffset": 193}, {"referenceID": 74, "context": "Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010).", "startOffset": 132, "endOffset": 193}, {"referenceID": 54, "context": "Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010).", "startOffset": 132, "endOffset": 193}, {"referenceID": 74, "context": "1 Tree-guided Group Lasso (Supervised) (Liu and Ye, 2010) In Tree-guided Group Lasso, the structure over the features can be represented as a tree with leaf nodes as features.", "startOffset": 39, "endOffset": 57}, {"referenceID": 74, "context": "We follow the definition from (Liu and Ye, 2010) to define Tree-guided Group Lasso, for an index tree G with a depth of d, Gi = {G1, G2, .", "startOffset": 30, "endOffset": 48}, {"referenceID": 108, "context": "Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).", "startOffset": 149, "endOffset": 210}, {"referenceID": 58, "context": "Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).", "startOffset": 149, "endOffset": 210}, {"referenceID": 139, "context": "Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).", "startOffset": 149, "endOffset": 210}, {"referenceID": 142, "context": "1 Laplacian Lasso (Supervised) (Ye and Liu, 2012) Since features exhibit graph structures, when two nodes (features) Ni and Nj are connected by an edge in G(N,E), the features fi and fj are more likely to be selected together, and they should have similar feature coefficients.", "startOffset": 31, "endOffset": 49}, {"referenceID": 158, "context": "(93) reduces to the elastic net penalty (Zou and Hastie, 2005).", "startOffset": 40, "endOffset": 62}, {"referenceID": 22, "context": "Since the graph regularization term w\u2032Lw is convex and differentiable, existing efficient algorithms like LARS (Efron et al., 2004) and proximal gradient descent methods (Liu and Ye, 2009) can be directly applied.", "startOffset": 111, "endOffset": 131}, {"referenceID": 58, "context": "2 GFLasso (Supervised) (Kim and Xing, 2009) In Eq.", "startOffset": 23, "endOffset": 43}, {"referenceID": 139, "context": "3 GOSCAR (Supervised) (Yang et al., 2012) To address the limitations of GFLasso, (Yang et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 139, "context": ", 2012) To address the limitations of GFLasso, (Yang et al., 2012) proposed a GOSCAR algorithm by putting a l\u221e-norm regularization to enforce the pairwise feature coefficients to be equal if they are connected over the feature graph G(N,E).", "startOffset": 47, "endOffset": 66}, {"referenceID": 7, "context": "The formulation of the GOSCAR is more general than the OSCAR algorithm which is proposed in (Bondell and Reich, 2008).", "startOffset": 92, "endOffset": 117}, {"referenceID": 139, "context": "Therefore, a new formulation with non-convex grouping penalty is also proposed in (Yang et al., 2012):", "startOffset": 82, "endOffset": 101}, {"referenceID": 81, "context": "Many linked data related learning tasks are proposed such as collective classification (Macskassy and Provost, 2007; Sen et al., 2008), relational learning (Long et al.", "startOffset": 87, "endOffset": 134}, {"referenceID": 111, "context": "Many linked data related learning tasks are proposed such as collective classification (Macskassy and Provost, 2007; Sen et al., 2008), relational learning (Long et al.", "startOffset": 87, "endOffset": 134}, {"referenceID": 6, "context": ", 2006, 2007), and active learning (Bilgic et al., 2010; Hu et al., 2013), but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 35, "endOffset": 73}, {"referenceID": 48, "context": ", 2006, 2007), and active learning (Bilgic et al., 2010; Hu et al., 2013), but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 35, "endOffset": 73}, {"referenceID": 34, "context": "1 Feature Selection on Networks (Supervised) (Gu and Han, 2011) In (Gu and Han, 2011), authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS).", "startOffset": 45, "endOffset": 63}, {"referenceID": 34, "context": "1 Feature Selection on Networks (Supervised) (Gu and Han, 2011) In (Gu and Han, 2011), authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS).", "startOffset": 67, "endOffset": 85}, {"referenceID": 92, "context": "(99) can be solved by proximal gradient descent methods (Nesterov, 2004).", "startOffset": 56, "endOffset": 72}, {"referenceID": 90, "context": "As illustrated in Figure (14), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user u2 can have multiple posts (p3, p4 and p5) and these posts are more similar than those randomly selected; (b) CoFollowing - two users u1 and u3 follow a user u4, its counterpart in citation analysis is bibliographic coupling (Morris, 2005), and their posts are more likely to be of similar topics; (c) CoFollowed - two users u2 and u4 are followed by a third user u1, similar to co-citation relation (Morris, 2005) in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user u1 follows another user u2, and their posts (e.", "startOffset": 339, "endOffset": 353}, {"referenceID": 90, "context": "As illustrated in Figure (14), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user u2 can have multiple posts (p3, p4 and p5) and these posts are more similar than those randomly selected; (b) CoFollowing - two users u1 and u3 follow a user u4, its counterpart in citation analysis is bibliographic coupling (Morris, 2005), and their posts are more likely to be of similar topics; (c) CoFollowed - two users u2 and u4 are followed by a third user u1, similar to co-citation relation (Morris, 2005) in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user u1 follows another user u2, and their posts (e.", "startOffset": 514, "endOffset": 528}, {"referenceID": 85, "context": "These four hypotheses are supported by social correlation theories such as homophily (McPherson et al., 2001) and social influence (Marsden and Friedkin, 1993) in explaining the existence of similarity as what these relations suggest.", "startOffset": 85, "endOffset": 109}, {"referenceID": 82, "context": ", 2001) and social influence (Marsden and Friedkin, 1993) in explaining the existence of similarity as what these relations suggest.", "startOffset": 29, "endOffset": 57}, {"referenceID": 93, "context": "Particularly, it uses modularity maximization (Newman and Girvan, 2004) to extract hidden factor matrix H.", "startOffset": 46, "endOffset": 71}, {"referenceID": 152, "context": "1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008) To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 152, "context": "1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008) To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al.", "startOffset": 171, "endOffset": 191}, {"referenceID": 65, "context": "1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008) To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al., 2004).", "startOffset": 328, "endOffset": 352}, {"referenceID": 44, "context": "The basic idea of the first method is similar to Laplacian score (He et al., 2005) and SPEC (Zhao and Liu, 2007) mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually.", "startOffset": 65, "endOffset": 82}, {"referenceID": 151, "context": ", 2005) and SPEC (Zhao and Liu, 2007) mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually.", "startOffset": 17, "endOffset": 37}, {"referenceID": 30, "context": "For supervised multi-view feature selection, the most common approach is Sparse Group Lasso (Friedman et al., 2010; Peng et al., 2010).", "startOffset": 92, "endOffset": 134}, {"referenceID": 100, "context": "For supervised multi-view feature selection, the most common approach is Sparse Group Lasso (Friedman et al., 2010; Peng et al., 2010).", "startOffset": 92, "endOffset": 134}, {"referenceID": 121, "context": "2 Unsupervised Feature Selection for Multi-View Data (Unsupervised) (Tang et al., 2013) AUMFS (Feng et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 121, "context": "In (Tang et al., 2013), the authors propose a novel unsupervised feature selection method called Multi-view Feature Selection (MVFS).", "startOffset": 3, "endOffset": 22}, {"referenceID": 30, "context": "It can be observed that the basic idea is very similar to sparse group lasso (Friedman et al., 2010; Peng et al., 2010) which also requires intra-group sparsity and inter-group sparsity.", "startOffset": 77, "endOffset": 119}, {"referenceID": 100, "context": "It can be observed that the basic idea is very similar to sparse group lasso (Friedman et al., 2010; Peng et al., 2010) which also requires intra-group sparsity and inter-group sparsity.", "startOffset": 77, "endOffset": 119}, {"referenceID": 148, "context": "Recently, there exists some work trying to combine these two dual problems together, the problem is referred as feature selection on Trapezoidal data streams (Zhang et al., 2015).", "startOffset": 158, "endOffset": 178}, {"referenceID": 101, "context": "1 Grafting Algorithm (Supervised) (Perkins and Theiler, 2003) The first attempt to perform streaming feature selection is credited to (Perkins and Theiler, 2003).", "startOffset": 34, "endOffset": 61}, {"referenceID": 101, "context": "1 Grafting Algorithm (Supervised) (Perkins and Theiler, 2003) The first attempt to perform streaming feature selection is credited to (Perkins and Theiler, 2003).", "startOffset": 134, "endOffset": 161}, {"referenceID": 102, "context": "They proposed a streaming feature selection framework based on stagewise gradient descent regularized risk framework (Perkins et al., 2003).", "startOffset": 117, "endOffset": 139}, {"referenceID": 17, "context": "In (Dhillon et al., 2010), authors extended the alpha-investing algorithm and proposed a proposed a multiple streamwise feature selection algorithm to the case where there are multiple feature streams.", "startOffset": 3, "endOffset": 25}, {"referenceID": 67, "context": "4 Streaming Feature Selection with Group Structures (Supervised) (Wang et al., 2013b; Li et al., 2013) Previous mentioned streaming feature selection algorithms evaluate new features individually.", "startOffset": 65, "endOffset": 102}, {"referenceID": 67, "context": "In addition to OGFS, a similar algorithm is proposed in (Li et al., 2013).", "startOffset": 56, "endOffset": 73}, {"referenceID": 68, "context": "5 Unsupervised Streaming Feature Selection in Social Media (Unsupervised) (Li et al., 2015) Vast majority of streaming feature selection methods are supervised which utilize label information to guide feature selection process.", "startOffset": 74, "endOffset": 91}, {"referenceID": 68, "context": "To deal with large-scale unlabeled data in social media, authors in (Li et al., 2015) proposed an USFS algorithm to study unsupervised streaming feature selection.", "startOffset": 68, "endOffset": 85}, {"referenceID": 0, "context": "USFS first uncovers hidden social factors from link information by mixed membership stochastic blockmodel (Airoldi et al., 2009).", "startOffset": 106, "endOffset": 128}, {"referenceID": 131, "context": "1 Online Feature Selection (Supervised) (Wang et al., 2014) In (Wang et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 131, "context": ", 2014) In (Wang et al., 2014), an online feature selection algorithm (OFS) for binary classification is proposed.", "startOffset": 11, "endOffset": 30}, {"referenceID": 49, "context": "2 Unsupervised Feature Selection on Data Streams (Unsupervised) (Huang et al., 2015) OFS assumes that the class labels of continuously generated data streams are available.", "startOffset": 64, "endOffset": 84}, {"referenceID": 49, "context": "To timely select a subset of relevant features when unlabeled data are continuously being generated, authors in (Huang et al., 2015) propose a novel unsupervised feature selection method (FSDS) that is able to perform feature selection timely with only one pass of the data and utilize limited storage.", "startOffset": 112, "endOffset": 132}, {"referenceID": 70, "context": "Therefore, FSDS utilizes the matrix sketching strategy from (Liberty, 2013) to maintain a low-rank approximation of X(t).", "startOffset": 60, "endOffset": 75}, {"referenceID": 114, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014).", "startOffset": 171, "endOffset": 231}, {"referenceID": 153, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014).", "startOffset": 171, "endOffset": 231}, {"referenceID": 137, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014).", "startOffset": 171, "endOffset": 231}, {"referenceID": 26, "context": "In this case, well-designed feature selection algorithms in linear or sub-linear running time is more preferred (Fan et al., 2009; Tan et al., 2014).", "startOffset": 112, "endOffset": 148}, {"referenceID": 116, "context": "In this case, well-designed feature selection algorithms in linear or sub-linear running time is more preferred (Fan et al., 2009; Tan et al., 2014).", "startOffset": 112, "endOffset": 148}, {"referenceID": 57, "context": "Considering its importance in practical usage, stability of feature selection algorithms has received increasing attention in the community (Kalousis et al., 2007; He and Yu, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 45, "context": "Considering its importance in practical usage, stability of feature selection algorithms has received increasing attention in the community (Kalousis et al., 2007; He and Yu, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 1, "context": "It is also found in (Alelyani et al., 2011) that the underlying characteristics of data may greatly affect the stability of feature selection algorithms and the stability issue may also be data dependent.", "startOffset": 20, "endOffset": 43}, {"referenceID": 125, "context": "For instance, in (Tibshirani et al., 2001), a principled way to estimate the number of suitable clusters in a dataset is proposed.", "startOffset": 17, "endOffset": 42}], "year": 2016, "abstractText": "Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/scikit-feast/). At the end of this survey, we also have a discussion about some open problems and challenges that need to be paid more attention in future research.", "creator": "LaTeX with hyperref package"}}}