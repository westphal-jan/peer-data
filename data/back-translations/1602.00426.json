{"id": "1602.00426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection", "abstract": "The results are evaluated against metrics and corpora proposed in the Zero Resource Speech Challenge of Interspeech 2015. A multi-layered acoustic tokenizer (MAT) has been proposed to automatically detect multiple sets of acoustic tokens from the given corpus. Each set of acoustic tokens is specified by a series of hyperparameters describing the model configuration. These sets of acoustic tokens have different characteristics of the given corpus and the underlying language and can therefore be mutually reinforced. Multiple sets of tokits are then used as targets of a Multi-Target Deep Neural Network (MDNN) with low-level acoustic characteristics. Bottle features extracted from the MDNN are then used as feedback input for the MAT and the MDNN itself in the next framework.", "histories": [["v1", "Mon, 1 Feb 2016 08:37:56 GMT  (633kb,D)", "http://arxiv.org/abs/1602.00426v1", "arXiv admin note: text overlap witharXiv:1506.02327"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1506.02327", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["cheng-tao chung", "cheng-yu tsai", "hsiang-hung lu", "chia-hsiang liu", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1602.00426"}, "pdf": {"name": "1602.00426.pdf", "metadata": {"source": "CRF", "title": "AN ITERATIVE DEEP LEARNING FRAMEWORK FOR UNSUPERVISED DISCOVERY OF SPEECH FEATURES AND LINGUISTIC UNITS WITH APPLICATIONS ON SPOKEN TERM DETECTION", "authors": ["Cheng-Tao Chung", "Cheng-Yu Tsai", "Hsiang-Hung Lu", "Chia-Hsiang Liu", "Hung-yi Lee", "Lin-shan Lee"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 zero resource, unsupervised learning, dnn, hmm"}, {"heading": "1. INTRODUCTION", "text": "In the era of big data, huge quantities of raw speech data is easy to obtain, but annotated speech data remains hard to acquire. This leads to the increased importance of zero resource applications where annotated data is not required, such as query-by-example spoken term detection. With the dominant paradigm of automatic speech recognition (ASR) technologies being supervised learning [1], speech technologies under the zero resource scenario is a relatively less explored topic. The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5]. In this work we develop new approaches for unsupervised discovery of speech features and linguistic unit, and in the tests use the evaluation metrics and the corpora defined by the Challenge for easier comparison of the results. Track 1 of the Challenge was to construct framewise speech features representing the speech sounds that is more robust to within-speaker and across-speaker variation. Track 2 of the Challenge then focuses\non the discovery of word linguistic units and extracting timing information for such units from the speech corpus. In both tracks a complete set of evaluation metrics as well as a set of standard corpora were defined in order to analyze the quality of the discovered framewise speech features and linguistic units in an different aspects without considering the backend applications. In addition to the metrics defined by the Challenge, a whole set of experiments on query b spoken term detection was performed to demonstrate that discovered acoustic tokens and speech features work well in a real applications .\nIn this paper, we propose a completely unsupervised iterative deep learning framework for the task. A Multi-layered Acoustic Tokenizer (MAT) is used to generate multiple sets of acoustic tokens, each with a specific model configuration referred to as a layer. The different layer of the tokens carry complementary knowledge about the corpus and the language behind [6], thus can be further mutually reinforced [7]. The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network [8] (MDNN) to learn the framewise bottleneck features [9] (BNFs). The BNFs are then used as feedback input to both the MAT and the MDNN in the next iteration. The whole framework is referred to as a Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN). In addition to evaluating the results with the metrics defined in the Challenge mentioned above we perform an additional set of experiments focused on query-by-example spoken term detection using the acoustic tokens discovered."}, {"heading": "2. PROPOSED APPROACH", "text": ""}, {"heading": "2.1. Overview of the proposed framework", "text": "The framework of the approach is shown in Fig1. In the left part, the Multi-layered Acoustic Tokenizer (MAT) produces many sets of acoustic tokens using unsupervised HMMs, each describing some aspects of the given corpus. These tokens are specified by two hyperparameters \u03c8 = (m,n) describing HMM configurations which will be explained below. Each set of acoustic tokens for each configuration is obtained by iteratively optimizing the token models and the token labels on the given acoustic corpus. Multiple pairs of hyperparameters were selected producing multi-layered token labels for the given corpus to be used as the training targets of the Multitarget Deep Neural Network (MDNN) on the right part of Fig.1 , so the knowledge carried by different token sets on different layers are fused. Bottleneck features are then extracted from this MDNN. In the first iteration, some initial acoustic features are used for both the MAT and the MDNN. This gives the first set of bottleneck fea-\nar X\niv :1\n60 2.\n00 42\n6v 1\n[ cs\n.C L\n] 1\nF eb\n2 01\ntures. These bottleneck features are then used as feedback to both the MAT (to replace the initial acoustic features) and the MDNN (to be concatenated with the initial acoustic features to produce tandem features) in the second iteration. Such feedback can be continued iteratively. The complete framework is referred to as Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN). The output of the MDNN (bottleneck features) is evaluated with metrics of Track 1 of the Challenge, while the time intervals for the acoustic token labels at the output of the MAT are evaluated with metrics of Track 2 of the Challenge. Both the acoustic tokens and acoustic features are further examined in a query by example spoken term detection experiment in the end."}, {"heading": "2.2. Multi-layered Acoustic Tokenizer(MAT)", "text": ""}, {"heading": "2.2.1. Unsupervised Token Discovery for Each layer of MAT", "text": "The goal here is to obtain multiple sets of acoustic tokens in a completely unsupervised way, each defined by the hyperparameters \u03c8 = (m,n). It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14]. This can be achieved by first finding an initial label set \u03c90 based on a set of assumed tokens for all features in the corpus X as in (1) [13]. Then in each iteration t the HMM parameters \u03b8\u03c8t can be trained with the label set \u03c9t\u22121 obtained in the previous iteration as in (2), and the new label set \u03c9t can be obtained by token decoding with the obtained parameters \u03b8\u03c8t as in (3).\n\u03c90 = initialization(X), (1)\n\u03b8\u03c8t = argmax \u03b8\u03c8 P (X|\u03b8\u03c8, \u03c9t\u22121), (2)\n\u03c9t = argmax \u03c9\nP (X|\u03b8\u03c8t , \u03c9). (3)\nThe training process can be repeated with enough number of iterations until a converged set of token HMMs is obtained. The processes (2),(3) are respectively referred to as token model optimization and token label optimization in the left part of Fig.1."}, {"heading": "2.2.2. Granularity Space of Multi-layered Acoustic Token Sets", "text": "The process explained above can be performed with different HMM configurations, each characterized by two hyperparameters\u03c8: the\nnumber of states m in each acoustic token HMM, and the total number of distinct acoustic tokens n during initialization. The token labels of a signal can be considered as a temporal segmentation, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all distinct acoustic tokens can be considered as a segmentation of the phonetic space, so the total number n of distinct acoustic tokens represents the phonetic granularity. This gives a two-dimensional representation in terms of temporal and phonetic granularities as in Fig.2. The points in this two-dimensional space in Fig.2 correspond to acoustic token configurations with different model granularities, carrying complementary knowledge about the corpus and the language. Although the best selection of the hyperparameters in the above two-dimensional space is not known, we can simply select M temporal granularities (m=m1,m2,...mM ) and N phonetic granularities (n=n1,n2,...nN ), forming a two-dimensional array ofM \u00d7N hyperparameter pairs in the granularity space."}, {"heading": "2.3. Mutual Reinforcement(MR) of Multi-layered Tokens", "text": "Because all layers of acoustic tokens obtained in the MAT above are learned in an unsupervised fashion, they may not be very precise. But we have many layers, each for a distinct pair of hyperparameters \u03c8 = (m,n), so they can be mutually reinforced(MR). This is explained here and shown in Fig.3, including token boundary fusion and LDA-based token label re-initialization as in Fig.3(a)."}, {"heading": "2.3.1. Token Boundary Fusion", "text": "Fig.3(b) shows the token boundary when a part of an utterance is segmented into acoustic tokens on different layers with different hyperparameter pairs \u03c8 = (m,n). We define a boundary function bm,n(j) on each layer with \u03c8 = (m,n) for the possible boundary between every pair of two adjacent frames within the utterance, where j is the time index for such possible boundaries. On each layer bm,n(j)=1 if boundary j is a token boundary and 0 otherwise. All these boundary functions bm,n(j) for all different layers are then weighted and averaged to give a joint boundary function B(j). The weights consider the fact that smaller m or shorter HMMs generate more boundaries, so those boundaries should weigh less. The peaks of B(j) are then selected based on the second derivatives and some filtering and thresholding process. This gives the new segmentation of the utterance as shown at the bottom of Fig.3(b)."}, {"heading": "2.3.2. LDA-based Token Label Re-initialization", "text": "As shown in Fig.3(c), each new segment obtained above usually consists of a sequence of acoustic tokens on each layer based on the tokens defined on that layer. We now consider all the tokens on all the\ndifferent layers as different words, so we have a vocabulary of MN\u2211 i=1 ni words, i.e., there are ni words on the i-th layer and there are a total of MN layers. A new segment here is thus considered as a document (bag-of-words) composed of words (tokens) collected from all different layers. Latent Dirichlet Allocation [15] (LDA) is preformed for topic modeling, and then each document (new segment) is labeled with the most probable topic. Because in LDA a topic is characterized by a word distribution, here a token distribution across different layers may also represent a certain acoustic characteristics or a certain acoustic token. By setting the number of topics in LDA as the number of distinct tokens n (n=n1,n2,...nN ) as in subsection 2.2.2, we have a new initial label set \u03c90 as in (1) of subsection 2.2.1, in which each new segment obtained here is a new acoustic token whose ID is the topic ID obtained by LDA. This new initial label set \u03c90 is then used to re-train all the acoustic tokens on all layers of MAT as in (1)(2)(3)."}, {"heading": "2.4. The Multi-target DNN (MDNN)", "text": "As shown in the right part of Fig.1, token label sequence from a layer (with hyperparameters \u03c8 = (m,n)) is a valid target for supervised framewise training, although obtained in an unsupervised way. In the initial work here, we simply take the token label rather than the HMM state as the training target. As shown in Fig.1, there are multilayered token labels with different hyperparameters \u03c8 = (m,n) for each utterance offered by MAT on the left, so we jointly consider all the multi-layered token labels by learning the parameters for a single DNN with a uniformly weighted cross-entropy objective at the output layer. As a result, the bottleneck feature (BNF) extracted from this DNN automatically fuse all knowledge about the corpus and the language from the different sets of acoustic tokens."}, {"heading": "2.5. The Iterative Learning Framework for MAT-DNN", "text": "Once the BNFs are extracted from the MDNN in iteration 1, they can be taken as the input to the MAT on the left of Fig.1 replacing the initial acoustic features. The MAT then generates updated sets of multi-layered token labels and these updated sets of multi-layered token labels can be used as the updated training targets of the MDNN. The input features of the MDNN can also be updated by concatenating the initial acoustic features with the newly extracted BNFs as the concatenated features, and this process can be repeated. The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC. Although different from the conventional recurrent neural network (RNN) in which the recurrent structure is included in back propagation training, the concatenation of the bottleneck features with other features in the next iteration in MDNN is a kind of recurrent structure."}, {"heading": "2.6. Spoken Term Detection", "text": "Let {pr, r = 1, 2, 3, .., n} denote the n acoustic tokens in the set of \u03c8=(m,n). We first construct a distance matrix S of size n \u00d7 n off-line for every token set \u03c8=(m,n), for which the element S(i, j)\nis the distance between any two token HMMs pi and pj in the set.\nS(i, j) = KL(i, j). (4)\nThe KL-divergence KL(i, j) between two token HMMs in (4) is defined as the symmetric KL-divergence between the states based on the variational approximation [19] summed over the states.\nIn the on-line phase, we perform the following for each entered spoken query q and each document (utterance) d in the archive for each token set \u03c8=(m,n). Assume for a given pattern set a document d is decoded into a sequence of D acoustic patterns with indices (d1, d2, ..., dD) and the query q into a sequence of Q patterns with indices (q1, ..., qQ). We thus construct a matching matrix W of size D \u00d7Q for every document-query pair, in which each entry (i, j) is the distance between acoustic tokens with indices di and qj as in (5), where S(i, j) is defined in (4),\nW (i, j) = S(di, qj). (5)\nWe perform token-based DTW on this matching matrix W by summing the distance between token pairs along the optimal path and return the minimal distance as the distance between document d and query q."}, {"heading": "3. EXPERIMENTAL SETUP", "text": "The MAT-DNN presented above allows flexible configurations, but here we train the MAT-DNN in the following manner. We set m=3, 5, 7, 9 states per token HMM and n=50, 100, 300, 500 distinct tokens in the MAT, which gave a total of 16 layers(m=11,13 were added in some tests as mentioned below).\nIn the first iteration, we used the 39 dimension Mel-frequency Cepstral Coefficients (MFCC) with energy, delta and double delta as the initial acoustic features for the input to both the MAT and the MDNN. We concatenated the MFCC with a window of 4 frames before and after (39x9 dimensions), and an i-vector (400 dimensions) trained on the MFCC of each evaluation interval used as the input of the MDNN. The topology of the MDNN is set to be 751(input)256(hidden)-256(hidden)-39(bottleneck)-(target) with 3 hidden layers. and we kept the dimensionality of these features to be 39 for a fair comparison. For the Deep Boltzmann Machine(DBM), we used the 39-dimension MFCC with a window of 5 frames before and after as the input. The configuration we used for the DBM is 429(visible)256(hidden)-256(hidden)-39(hidden). We also extracted another set of LSTM-RNN autoencoder bottleneck features but found the performance was slightly worse than MFCC.\nIn the second iteration, we concatenated the original MFCC, the BNF extracted from the first iteration, the DBM posteriorgrams, and\nthe i-vector forming a (39x9+39x9+39x9+400=1453) dimension input to the MDNN. We used the updated token labels as the target and extracted the BNF as the features.\nThe MAT was trained using the zrst [20], a python wrapper for the HTK toolkit [21] and srilm [22] that we developed for training unsupervised HMMs with varying model granularity. The LDA model we used in the Mutual Reinforcement was trained by MALLET [23]. The MFCC were extracted using the HTK toolkit [21]. The i-vectors were extracted using Kaldi [24]. The DBM posteriorgram was extracted using libdnn [25]. The MDNN was trained using Caffe [26]. The two corpora used in the Challenge were used here: the Buckeye corpus [27] in English and the NCHLT Xitsonga Speech corpus in Tsonga."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "4.1. Feature Quality in Metrics of Track 1", "text": "The evaluation was based on the ABX discriminability test [28] including across-speaker and within-speaker tests. The warping distance obtained by performing Dynamic Time Warping on feature sequences of predefined phone pairs was used as the distance metric for the ABX discriminability test. The results in error percentage(the lower the better) are listed in Table 1.\nRows (1) and (13) are the official baseline MFCC features and the official topline supervised phone posteriorgrams provided by the Challenge respectively. Row (2) is our baseline of the MFCC features, the initial acoustic features used to train all systems in this work. Row (3) is for the DBM posteriorgrams extracted from the MFCC of row (2), serving as a strong unsupervised baseline. The results in rows (4), (5) and (6) are the performance of the bottleneck features(BNF-1st) extracted in the first iteration of the MATDNN without applying Mutual Reinforcement (MR) (4), applying MR once(MR-1) (5), and twice(MR-2) (6) respectively. Rows (7) and (8) are the same as rows (4), (5) except the bottleneck features were extracted in the second iteration of the MAT-DNN (BNF-2nd) and the MAT of the MAT-DNN was trained using the BNF of row(5). Row (9),(10) and (11) is similar to row (5), except we use a wider bottleneck layer with 64, 128, 256 dimensions instead of 39. Row (12) is similar to row (8), except only the MFCC and i-vectors were concatenated as input without other features.\nAll the features from row (2) to (10) except for (9) are confined to 39 dimensions for fair comparison. We observe that as a standalone feature extractor without any iterations, the MAT-DNN in row (5) outperformed the DBM baseline in (3). The effect of Mutual Reinforcement can be seen in the improvement from rows (4) to (5)(6) and (7) to (8). We observe that a single iteration of Mutual\nReinforcement is enough to bring huge improvement. The effect of iterations in the MAT-DNN can be seen by comparing rows (2), (5), (8), respectively corresponding to 0, 1, and 2 iterations. Although the performance improvement from row (2) to row (5) is notable, it dropped in the second iteration in (8). To investigate reasons of this performance drop, we widened the bottleneck feature to 256 dimensions in row (9) and observed a dramatic improvement. It is possible that we have not yet explored the full potential of the MATDNN as comparison between algorithms was the original goal when we designed the experiments. For a better tuned set of parameters, improvement in following iterations is to be expected on Track 1. Nonetheless, the benefit of the second iteration is better observed in Track 2."}, {"heading": "4.2. Quality of the Discovered Units in Metrics of Track 2", "text": "Track 2 of the Challenge defined a total of 7 evaluation metrics for 3 tasks [29] describing different aspects of the quality of the linguistic unit discovered from the corpora: coverage, Normalized Edit Distance(NED) and matching F-score for the matching task; grouping F-score and type F-score for clustering task; token F-score, boundary F-score for the parsing task. Except for coverage and NED whose values are indicators of system characteristic rather than a system performance, the higher the value the better for the other five metrics. Except for coverage, the other six scores are shown in the six subfigures in Fig.4(a) and (b). We omit coverage because it is always 100% in all cases. In each subfigure, the results for four cases are shown in four sections from left to right, corresponding to the\nfour sets of tokens obtained in MAT after the first and second iterations of MAT-DNN (TOK-1st or TOK-2nd) with MR performed or not (MR-0,1,2). The corresponding bottleneck features for them are in front of those listed in rows (4), (5), (6) and (8) of Table 1, as marked at the bottom of each section. For each of these section, the three or six groups of bars correspond to different values of m (m=3, 5, 7 orm=3, 5, 7, 9, 11, 13), while in each group the four bars correspond to the four values of n (n=50, 100, 300, 500 from left to right), where \u03c8 = (m,n) are the parameters for the token sets. The bars in blue and yellow are those better or equal to the JHU baseline offered by the Challenge, while those in white are worse. Only the results jointly considering both within and across talker conditions are shown.\nFrom Fig.4(a) for English, it can be seen that the proposed token sets perform well in type, token and boundary scores, although much worse in matching and grouping scores. We see in many cases the benefits brought by MR (e.g. MR-2 in (6) vs MR-1 in (5) in type of Fig.4(a)) and the second iteration (e.g. TOK-2nd in (8) vs TOK1st in (5) in boundary of Fig.4(a)), especially for small values of m. In many groups for a given m, smaller values of n seemed better, probably because n=50 is close to the total number of phonemes in the language. Also, a general trend is that larger values of m were better, probably because HMMs with more states were better in modeling the relatively long units; this may directly lead to the higher type, token and boundary scores.\nSimilar observations can be made for Tsonga in Fig.4(b), and the overall performance seemed to be even better as the proposed token sets performed well even in matching scores. The improve-\nments brought by MR (e.g. MR-1 vs MR-0), the bottleneck features (compared to JHU baseline) and the second iteration (TOK-2nd vs TOK-1st) are better observed here, which gave the best cases for all the five main scores. This is probably due to the fact that more sets of tokens were available for MR and MAT-DNN on Tsonga than English. We can conclude from this observation that more token sets introduces more robustness and that leads to better token sets for the next iteration. When m goes to 13, we see that for MR-0 in the left section of Fig.4(b)) almost all metrics degraded except for matching scores, but with MR-1, MR-2 almost all the scores consistently increases (except for NED) when m became larger. This suggests that MR can prevent degradation from happening while detecting relatively long units.\nWe selected three typical example token sets (A)(B)(C) out of the many proposed here, and compared them with the JHU baseline [30] in Table 2 including Precision (P), Recall (R) and F-scores (F). These three example sets are also marked in Fig.4. In Table 2 those better than JHU baseline are in bold. The much higher NED and coverage scores suggest that the proposed approach is a highly permissive matching algorithm. The much higher parsing scores (type, token and boundary scores), especially the Recall and F-scores, imply the proposed approach is more successful in discovering wordlike units. However, the matching and grouping scores were much worse probably because the discovered tokens covered almost the whole corpus, including short pauses or silence, and therefore many tokens were actually noises. Another possible reason might be that the values of n used were much smaller than the size of the real word vocabulary, making the same token label used for signal segments of varying characteristics and this degenerated the grouping qualities."}, {"heading": "4.3. Unsupervised Spoken Term Detection", "text": "Although the discovered speech features (BNFs) and linguistic units (tokens) were evaluated to be of high quality in Tables 1,2 and Fig. 4 in various aspects in terms of the metrics defined in the Challenge, in this paper we wish to investigate if the proposed MAT-DNN is good for a real application, i.e. spoken term detection. Separate query by example spoken term detection experiments were conducted on the two corpora, English (Eng) and Tsonga (Xit). For English/Tsonga, spoken instances of 5/10 query words randomly selected from the data set were used as the spoken query to search for other instances in the spoken archive. Both the selected queries and the corpora were first labeled as sequences of the multi-layered tokens. The distance between the document token sequences and query token sequence is evaluated by the token DTW distance as defined in section 2.6. A total of 5 collections of multi-layered token sets were tested here, which are (TOK-1st, MR-0), (TOK-1st, MR-1), (TOK-1st, MR-2), (TOK-2nd, MR-0), (TOK-2nd, MR-1). For English, each collection consists of 3\u00d74 sets of acoustic tokens with granularity m = 3, 5, 7 and n = 50, 100, 300, 500, so we obtained 12 scores for every query-document pair on every collection. For Tsonga, m = 3, 5, 7, 9 and n = 50, 100, 300, 500, thus we had 16 scores for every query-document pair. We averaged the 12, or 16 distances in every collection and obtained the results. Mean Average Precision(MAP), the higher the better, was used as our evaluation metric, and dynamic time warping on the feature sequences was taken as the baseline.\nThe results for the 5 collections of tokens are in row (a) to (e) in 3. The benefit of the iterative framework of Mutual Reinforcement (MR) can be observed by comparing rows (a) to (b), (b) to (c) and (b) to (d) (MR-0 vs MR-1, MR-1 vs MR-2). The benefit of the iterative framework of the MAT-DNN can be observed by comparing row (a) to (d) and (b) to (e) (TOK-1st vs TOK-2nd). We then aver-\naged all token DTW distances in (a) to (e) in row(h), and obtained better results, showing that the information obtained in each collection is complimentary to each other as well. We then compared these results with two cases of DTW performed on frame-level features: 39-dim MFCC in row (f) and bottleneck features (BNF-1st, MR-1) in row (g). By comparing rows (g) to (f), we observe that the features obtained by MAT-DNN performed significantly better than the MFCC from which they were derived. We further fused the information from both the feature based DTW and token based DTW by averaging all scores in rows (a) to (g) in row (i), producing even better results indicating frame-level and token-level information are complementary."}, {"heading": "5. CONCLUSION", "text": "In this paper we propose an iterative deep learning framework, MATDNN, to discover high quality features and multi-layer acoustic token sets on a completely unsupervised way. These features and tokens are evaluated by the metrics and corpora defined in the Zero Resource Speech Challenge in Interspeech 2015. We fuse the information obtained from different token sets in the spoken term detection experiments and obtain good initial results. We hope that these results serve as good references for future investigations."}, {"heading": "6. ACKNOWLEDGMENT", "text": "We would like to thank Yuan-ming Liou, Yen-Chen Wu, and Yen-Ju Lu for providing the DBM posteriorgrams and i-vectors used in the MAT-DNN of this work."}, {"heading": "7. REFERENCES", "text": "[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep Neural Networks For Acoustic Modeling In Speech Recognition: The Shared Views Of Four Research Groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.\n[2] Chia-ying Lee and James Glass, \u201cA Nonparametric Bayesian Approach To Acoustic Model Discovery,\u201d in Proceedings Of The 50th Annual Meeting Of The Association For Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.\n[3] Man-hung Siu, Herbert Gish, Arthur Chan, William Belfield, and Steve Lowe, \u201cUnsupervised Training Of An HMM-based Self-Organizing Unit Recognizer With Applications To Topic Classification And Keyword Discovery,\u201d Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.\n[4] Herman Kamper, Micha Elsner, Aren Jansen, and Sharon Goldwater, \u201cUnsupervised Neural Network Based Feature Extraction Using Weak Top-Down Constraints,\u201d .\n[5] Keith Levin, Aren Jansen, and Benjamin Van Durme, \u201cSegmental Acoustic Indexing For Zero Resource Keyword Search,\u201d in Proc. ICASSP, 2015.\n[6] Cheng-Tao Chung, Chun-an Chan, and Lin-shan Lee, \u201cUnsupervised Spoken Term Detection With Spoken Queries By Multi-level Acoustic Patterns With Varying Model Granularity,\u201d in Acoustics, Speech And Signal Processing (ICASSP), 2014 IEEE International Conference On. IEEE, 2014.\n[7] Cheng-Tao Chung, Wei-Ning Hsu, Cheng-Yi Lee, and LinShan Lee, \u201cEnhancing Automatically Discovered Multi-level Acoustic PatternsConsidering Context Consistency with Applications in Spoken Term Detection,\u201d in Acoustics, Speech And Signal Processing (ICASSP), 2015 IEEE International Conference On. IEEE, 2015.\n[8] Ngoc Thang Vu, Jochen Weiner, and Tanja Schultz, \u201cInvestigating The Learning Effect Of Multilingual Bottle-Neck Features For ASR,\u201d in Fifteenth Annual Conference Of The International Speech Communication Association, 2014.\n[9] Karel Vesely, Martin Karafia\u0301t, Frantisek Grezl, Marcel Janda, and Ekaterina Egorova, \u201cThe Language-Independent Bottleneck Features,\u201d in Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.\n[10] Aren Jansen and Kenneth Church, \u201cTowards Unsupervised Training Of Speaker Independent Acoustic Models.,\u201d in INTERSPEECH, 2011, pp. 1693\u20131692.\n[11] Herbert Gish, Man-hung Siu, Arthur Chan, and William Belfield, \u201cUnsupervised Training Of An Hmm-based Speech Recognizer For Topic Classification.,\u201d in INTERSPEECH, 2009, pp. 1935\u20131938.\n[12] Man-Hung Siu, Herbert Gish, Arthur Chan, and William Belfield, \u201cImproved Topic Classification And Keyword Discovery Using An Hmm-based Speech Recognizer Trained Without Supervision.,\u201d in INTERSPEECH, 2010, pp. 2838\u2013 2841.\n[13] Cheng-Tao Chung, Chun-an Chan, and Lin-shan Lee, \u201cUnsupervised Discovery Of Linguistic Structure Including Twolevel Acoustic Patterns Using Three Cascaded Stages Of Iterative Optimization,\u201d in Acoustics, Speech And Signal Processing (ICASSP), 2013 IEEE International Conference On. IEEE, 2013, pp. 8081\u20138085.\n[14] Mathias Creutz and Krista Lagus, \u201cUnsupervised Models For Morpheme Segmentation And Morphology Learning,\u201d ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.\n[15] David M Blei, Andrew Y Ng, and Michael I Jordan, \u201cLatent dirichlet allocation,\u201d the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.\n[16] Ruslan Salakhutdinov and Geoffrey E Hinton, \u201cDeep Boltzmann Machines,\u201d in International Conference On Artificial Intelligence And Statistics, 2009, pp. 448\u2013455.\n[17] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[18] Ahilan Kanagasundaram, Robbie Vogt, David B Dean, Sridha Sridharan, and Michael W Mason, \u201cI-vector Based Speaker Recognition On Short Utterances,\u201d in Proceedings Of The 12th Annual Conference Of The International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u20132344.\n[19] John R Hershey and Peder A Olsen, \u201cApproximating The Kullback Leibler Divergence Between Gaussian Mixture Models,\u201d in Acoustics, Speech And Signal Processing, 2007. ICASSP 2007. IEEE International Conference On. IEEE, 2007, vol. 4, pp. IV\u2013317.\n[20] Cheng-Tao Chung, \u201czrst,\u201d https://github.com/C2Tao/zrst, 2014.\n[21] Steve Young, Gunnar Evermann, Mark Gales, Thomas Hain, Dan Kershaw, Xunying Liu, Gareth Moore, Julian Odell, Dave Ollason, Dan Povey, et al., The HTK Book, vol. 2, Entropic Cambridge Research Laboratory Cambridge, 1997.\n[22] Andreas Stolcke et al., \u201cSRILM-an Extensible Language Modeling Toolkit.,\u201d in INTERSPEECH, 2002.\n[23] Andrew K McCallum, \u201c{MALLET: A Machine Learning For Language Toolkit},\u201d 2002.\n[24] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely, \u201cThe Kaldi Speech Recognition Toolkit,\u201d in IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society, IEEE Catalog No.: CFP11SRW-USB.\n[25] Po-Wei Chou, \u201clibdnn,\u201d https://github.com/botonchou/libdnn, 2014.\n[26] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell, \u201cCaffe: Convolutional Architecture for Fast Feature Embedding,\u201d arXiv preprint arXiv:1408.5093, 2014.\n[27] Mark A Pitt, Laura Dilley, Keith Johnson, Scott Kiesling, William Raymond, Elizabeth Hume, and Eric Fosler-Lussier, \u201cBuckeye Corpus Of Conversational Speech (2nd Release),\u201d Columbus, OH: Department of Psychology, Ohio State University, 2007.\n[28] Thomas Schatz, Vijayaditya Peddinti, Francis Bach, Aren Jansen, Hynek Hermansky, and Emmanuel Dupoux, \u201cEvaluating Speech Features With The Minimal-Pair ABX Task: Analysis Of The Classical MFC/PLP Pipeline,\u201d in INTERSPEECH 2013: 14th Annual Conference Of The International Speech Communication Association, 2013, pp. 1\u20135.\n[29] Bogdan Ludusan, Maarten Versteegh, Aren Jansen, Guillaume Gravier, Xuan-Nga Cao, Mark Johnson, and Emmanuel Dupoux, \u201cBridging The Gap Between Speech Technology And Natural Language Processing: An Evaluation Toolbox For Term Discovery Systems,\u201d in Language Resources And Evaluation Conference, 2014.\n[30] Aren Jansen and Benjamin Van Durme, \u201cEfficient Spoken Term Discovery Using Randomized Algorithms,\u201d in Automatic Speech Recognition And Understanding (ASRU), 2011 IEEE Workshop On. IEEE, 2011, pp. 401\u2013406."}], "references": [{"title": "Deep Neural Networks For Acoustic Modeling In Speech Recognition: The Shared Views Of Four Research Groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A Nonparametric Bayesian Approach To Acoustic Model Discovery", "author": ["Chia-ying Lee", "James Glass"], "venue": "Proceedings Of The 50th Annual Meeting Of The Association For Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Training Of An HMM-based Self-Organizing Unit Recognizer With Applications To Topic Classification And Keyword Discovery", "author": ["Man-hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield", "Steve Lowe"], "venue": "Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised Neural Network Based Feature Extraction Using Weak Top-Down Constraints", "author": ["Herman Kamper", "Micha Elsner", "Aren Jansen", "Sharon Goldwater"], "venue": ".", "citeRegEx": "4", "shortCiteRegEx": null, "year": 0}, {"title": "Segmental Acoustic Indexing For Zero Resource Keyword Search", "author": ["Keith Levin", "Aren Jansen", "Benjamin Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised Spoken Term Detection With Spoken Queries By Multi-level Acoustic Patterns With Varying Model Granularity", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech And Signal Processing (ICASSP), 2014 IEEE International Conference On. IEEE, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhancing Automatically Discovered Multi-level Acoustic PatternsConsidering Context Consistency with Applications in Spoken Term Detection", "author": ["Cheng-Tao Chung", "Wei-Ning Hsu", "Cheng-Yi Lee", "Lin- Shan Lee"], "venue": "Acoustics, Speech And Signal Processing (ICASSP), 2015 IEEE International Conference On. IEEE, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating The Learning Effect Of Multilingual Bottle-Neck Features For ASR", "author": ["Ngoc Thang Vu", "Jochen Weiner", "Tanja Schultz"], "venue": "Fifteenth Annual Conference Of The International Speech Communication Association, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "The Language-Independent Bottleneck Features", "author": ["Karel Vesely", "Martin Karafi\u00e1t", "Frantisek Grezl", "Marcel Janda", "Ekaterina Egorova"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards Unsupervised Training Of Speaker Independent Acoustic Models", "author": ["Aren Jansen", "Kenneth Church"], "venue": "IN- TERSPEECH, 2011, pp. 1693\u20131692.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Training Of An Hmm-based Speech Recognizer For Topic Classification", "author": ["Herbert Gish", "Man-hung Siu", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2009, pp. 1935\u20131938.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved Topic Classification And Keyword Discovery Using An Hmm-based Speech Recognizer Trained Without Supervision", "author": ["Man-Hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2010, pp. 2838\u2013 2841.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised Discovery Of Linguistic Structure Including Twolevel Acoustic Patterns Using Three Cascaded Stages Of Iterative Optimization", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech And Signal Processing (ICASSP), 2013 IEEE International Conference On. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised Models For Morpheme Segmentation And Morphology Learning", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep Boltzmann Machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "International Conference On Artificial Intelligence And Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "I-vector Based Speaker Recognition On Short Utterances", "author": ["Ahilan Kanagasundaram", "Robbie Vogt", "David B Dean", "Sridha Sridharan", "Michael W Mason"], "venue": "Proceedings Of The 12th Annual Conference Of The International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u20132344.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximating The Kullback Leibler Divergence Between Gaussian Mixture Models", "author": ["John R Hershey", "Peder A Olsen"], "venue": "Acoustics, Speech And Signal Processing, 2007. ICASSP 2007. IEEE International Conference On. IEEE, 2007, vol. 4, pp. IV\u2013317.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "zrst", "author": ["Cheng-Tao Chung"], "venue": "https://github.com/C2Tao/zrst, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "SRILM-an Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke"], "venue": "INTERSPEECH, 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "MALLET: A Machine Learning For Language Toolkit", "author": ["Andrew K McCallum"], "venue": "2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society, IEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "libdnn", "author": ["Po-Wei Chou"], "venue": "https://github.com/botonchou/libdnn, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Buckeye Corpus Of Conversational Speech (2nd Release)", "author": ["Mark A Pitt", "Laura Dilley", "Keith Johnson", "Scott Kiesling", "William Raymond", "Elizabeth Hume", "Eric Fosler-Lussier"], "venue": "Columbus, OH: Department of Psychology, Ohio State University, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluating Speech Features With The Minimal-Pair ABX Task: Analysis Of The Classical MFC/PLP Pipeline", "author": ["Thomas Schatz", "Vijayaditya Peddinti", "Francis Bach", "Aren Jansen", "Hynek Hermansky", "Emmanuel Dupoux"], "venue": "INTERSPEECH 2013: 14th Annual Conference Of The International Speech Communication Association, 2013, pp. 1\u20135.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Bridging The Gap Between Speech Technology And Natural Language Processing: An Evaluation Toolbox For Term Discovery Systems", "author": ["Bogdan Ludusan", "Maarten Versteegh", "Aren Jansen", "Guillaume Gravier", "Xuan-Nga Cao", "Mark Johnson", "Emmanuel Dupoux"], "venue": "Language Resources And Evaluation Conference, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient Spoken Term Discovery Using Randomized Algorithms", "author": ["Aren Jansen", "Benjamin Van Durme"], "venue": "Automatic Speech Recognition And Understanding (ASRU), 2011 IEEE Workshop On. IEEE, 2011, pp. 401\u2013406.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "With the dominant paradigm of automatic speech recognition (ASR) technologies being supervised learning [1], speech technologies under the zero resource scenario is a relatively less explored topic.", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 2, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 3, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 4, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 5, "context": "The different layer of the tokens carry complementary knowledge about the corpus and the language behind [6], thus can be further mutually reinforced [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "The different layer of the tokens carry complementary knowledge about the corpus and the language behind [6], thus can be further mutually reinforced [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network [8] (MDNN) to learn the framewise bottleneck features [9] (BNFs).", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network [8] (MDNN) to learn the framewise bottleneck features [9] (BNFs).", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 10, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 11, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 12, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 13, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 12, "context": "This can be achieved by first finding an initial label set \u03c90 based on a set of assumed tokens for all features in the corpus X as in (1) [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "Latent Dirichlet Allocation [15] (LDA) is preformed for topic modeling, and then each document (new segment) is labeled with the most probable topic.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC.", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC.", "startOffset": 260, "endOffset": 264}, {"referenceID": 17, "context": "The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC.", "startOffset": 323, "endOffset": 327}, {"referenceID": 18, "context": "The KL-divergence KL(i, j) between two token HMMs in (4) is defined as the symmetric KL-divergence between the states based on the variational approximation [19] summed over the states.", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "The MAT was trained using the zrst [20], a python wrapper for the HTK toolkit [21] and srilm [22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "The MAT was trained using the zrst [20], a python wrapper for the HTK toolkit [21] and srilm [22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "The LDA model we used in the Mutual Reinforcement was trained by MALLET [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "The i-vectors were extracted using Kaldi [24].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "The DBM posteriorgram was extracted using libdnn [25].", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "The MDNN was trained using Caffe [26].", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "The two corpora used in the Challenge were used here: the Buckeye corpus [27] in English and the NCHLT Xitsonga Speech corpus in Tsonga.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "The evaluation was based on the ABX discriminability test [28] including across-speaker and within-speaker tests.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Track 2 of the Challenge defined a total of 7 evaluation metrics for 3 tasks [29] describing different aspects of the quality of the linguistic unit discovered from the corpora: coverage, Normalized Edit Distance(NED) and matching F-score for the matching task; grouping F-score and type F-score for clustering task; token F-score, boundary F-score for the parsing task.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "We selected three typical example token sets (A)(B)(C) out of the many proposed here, and compared them with the JHU baseline [30] in Table 2 including Precision (P), Recall (R) and F-scores (F).", "startOffset": 126, "endOffset": 130}], "year": 2016, "abstractText": "In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.", "creator": "LaTeX with hyperref package"}}}