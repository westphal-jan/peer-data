{"id": "1602.01925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Massively Multilingual Word Embeddings", "abstract": "Our evaluation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC +, has been shown to correlate better with two downstream tasks (text categorization and parsing) than previous ones. In this and other evaluation methods, our evaluation methods outperform existing ones. We also describe an evaluation web portal that will facilitate further research in this area as well as open source publications of all our methods.", "histories": [["v1", "Fri, 5 Feb 2016 04:26:38 GMT  (25kb)", "http://arxiv.org/abs/1602.01925v1", null], ["v2", "Sat, 21 May 2016 08:08:21 GMT  (32kb)", "http://arxiv.org/abs/1602.01925v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["waleed ammar", "george mulcaire", "yulia tsvetkov", "guillaume lample", "chris dyer", "noah a smith"], "accepted": false, "id": "1602.01925"}, "pdf": {"name": "1602.01925.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample Chris Dyer", "Noah A. Smith"], "emails": ["wammar@cs.cmu.edu,", "gmulc@uw.edu,", "ytsvetko@cs.cmu.edu", "glample@cs.cmu.edu,", "cdyer@cs.cmu.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 92\n5v 1\n[ cs\n.C L\n] 5\nF eb"}, {"heading": "1 Introduction", "text": "Vector-space representations of words are widely used in statistical models of natural language. In addition to improvements on standard monolingual NLP tasks (Collobert and Weston, 2008), shared representation of words across languages offer intriguing possibilities (Klementiev et al., 2012). For example, in machine translation, translating a word never seen in parallel data may be overcome by seeking its vector-space neighbors, provided the embeddings are learned from both plentiful monolingual corpora and more limited parallel data. A second opportunity comes from transfer learning, in which models trained in one language can be deployed in other languages. While previous work has used hand-engineered features that are cross-linguistically stable as the basis model transfer (Zeman and Resnik, 2008;\nMcDonald et al., 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016). We therefore conjecture that developing estimation methods for \u201cmassively\u201d multilingual word embeddings (i.e., embeddings for words in a large number of languages) will play an important role in the future of multilingual NLP.\nThis paper makes the following contributions to this area. First, we articulate the desiderata for multilingual embeddings and propose two new estimation methods that fulfill these (\u00a72). These methods are designed to use only monolingual data in each language and pairwise parallel dictionaries (no parallel corpora are required), and they scale to any number of languages (a number of previous models have been limited to only pairs of languages). Second, we propose an automatic evaluation methodology designed to test how well these goals are fulfilled (\u00a73). This includes multiQVEC+, an inexpensive to compute evaluation which correlates well with performance on two downstream multilingual tasks, cross-lingual document categorization and cross-lingual parsing. Although intrinsic evaluations will never be perfect,1 a standard set of evaluation metrics will help drive research. We evaluate our two proposed methods and two existing methods on various sets of languages consisting of 3, 12, and 59 languages (\u00a75). Our proposed methods outperform existing methods on existing intrinsic metrics, the two extrinsic tasks, and our newly proposed\n1Goodhart\u2019s eponymous law warns that \u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d\nmultiQVEC+. Finally, in addition to an open-source implementation of our methods, we include a link to a public web portal for uploading arbitrary multilingual embeddings and evaluating them automatically using a suite of intrinsic and extrinsic evaluation methods (\u00a74)."}, {"heading": "2 Estimating Multilingual Embeddings", "text": "Let L be a set of languages, and let Vm be the set of surface forms (word types) in m \u2208 L. Let V = \u22c3 m\u2208L V\nm. Our goal is to estimate a partial embedding function E : L \u00d7 V 7\u2192 Rd (allowing a surface form that appears in two languages to have different vectors in each). We would like to estimate this function such that: (i) semantically similar words in the same language are nearby, (ii) translationally equivalent words in different languages are nearby, and (iii) the domain of the function covers as many words in V as possible.\nWe use distributional similarity in a monolingual corpus Mm to model semantic similarity between words in the same language. For cross-lingual similarity, either a parallel corpus Pm,n or a bilingual dictionary Dm,n \u2282 Vm \u00d7 Vn can be used. Our methods focus on the latter, in some cases extracting Dm,n from a parallel corpus.2\nWith three notable exceptions (see \u00a72.3, \u00a72.4, \u00a76), previous work on multilingual embeddings only considered the bilingual case, | L |= 2. In this section, we focus on estimating multilingual embeddings for | L |> 2 and describe two novel methods (multiCluster and multiCCA), then review the translation-invariance matrix factorization method (Gardner et al., 2015) and a variant of the multiSkip method (Guo et al., 2016).3\n2To do this, we align the corpus using fast align (Dyer et al., 2013) in both directions. The estimated parameters of the word translation distributions are used to select pairs: Dm,n = {\n(u, v) | u \u2208 Vm, v \u2208 Vn, pm|n(u | v)\u00d7 pn|m(v | u) > \u03c4 }\n, where the threshold \u03c4 trades off dictionary recall and precision. We fixed \u03c4 = 0.1 early on based on manual inspection of the resulting dictionaries.\n3We developed the multiSkip method independently of Guo et al. (2016)."}, {"heading": "2.1 Multilingual cluster (multiCluster) embeddings", "text": "In this approach, we decompose the problem into two simpler subproblems: E = Eembed \u25e6 Ecluster, where Ecluster : L \u00d7 V 7\u2192 C deterministically maps words to multilingual clusters C, and Eembed : C \u2192 R\nd assigns a vector to each cluster. We use a bilingual dictionary to find clusters of translationally equivalent words, then use distributional similarities of the clusters in monolingual corpora from all languages in L to estimate an embedding for each cluster. By forcing words from different languages in a cluster to share the same embedding, we create anchor points in the vector space to bridge languages.\nMore specifically, we define the clusters as the connected components in a graph where nodes are (language, surface form) pairs and edges correspond to translation entries in Dm,n. We assign arbitrary IDs to the clusters and replace each word token in each monolingual corpus with the corresponding cluster ID, and concatenate all modified corpora. The resulting corpus consists of multilingual cluster ID sequences. We can then apply any monolingual embedding estimator; here, we use the skipgram model from Mikolov et al. (2013a)."}, {"heading": "2.2 Multilingual CCA (multiCCA) embeddings", "text": "Faruqui and Dyer (2014) proposed a bilingual embedding estimation method based on canonical correlation analysis (CCA) and showed that the resulting embeddings for English words outperform monolingually-trained English embeddings on word similarity tasks. First, they use monolingual corpora to train monolingual embeddings for each language independently (Em and En), capturing semantic similarity within each language separately. Then, using a bilingual dictionary Dm,n, they use CCA to estimate linear projections from the ranges of the monolingual embeddings Em and En, yielding a bilingual embedding Em,n. The linear projections are defined by Tm\u2192m,n and Tn\u2192m,n \u2208 Rd\u00d7d; they are selected to maximize the correlation between Tm\u2192m,nEm(u) and Tn\u2192m,nEn(v) where (u, v) \u2208 Dm,n. The bilingual embedding is then defined as ECCA(m, u) = Tm\u2192m,nEm(u) (and likewise for ECCA(n, v)).\nIn this work, we use this method as a building block to construct multilingual embeddings for more\nlanguages. We let the vector space of the initial (monolingual) English embeddings serve as the multilingual vector space (since English typically offers the largest corpora and wide availability of bilingual dictionaries). We then estimate projections from the monolingual embeddings of the other languages into the English space.\nWe start by estimating, for each m \u2208 L \\ {en}, the two projection matrices: Tm\u2192m,en and Ten\u2192m,en; these are guaranteed to be non-singular. We then define the multilingual embedding as ECCA(en, u) = Een(u) for u \u2208 Ven, and ECCA(m, v) = T\u22121en\u2192m,enTm\u2192m,enE\nm(v) for v \u2208 Vm,m \u2208 L \\ {en}.\nThough not explored here, this approach generalizes even without a single \u201chub\u201d language (English) with which every other language shares a bilingual dictionary. If the languages are all connected by bilingual dictionaries, then we can select any spanning tree of the \u201clanguage graph\u201d induced by the bilingual dictionaries, and any language as the \u201croot.\u201d Words in any language can be iteratively projected into the vector spaces along the path to the root using the technique described above. In future work, non-linear transformations might be explored as well."}, {"heading": "2.3 MultiSkip embeddings", "text": "Luong et al. (2015b) proposed a method for estimating a bilingual embedding which only makes use of parallel data; it extends the skipgram model of Mikolov et al. (2013a). The skipgram model defines a distribution over words u that occur in a context window (of size K) of a word v:\np(u | v) = expEskipgram(m, v)\u22a4Econtext(m, u)\u2211\nu\u2032\u2208Vm expEskipgram(m, v) \u22a4Econtext(m, u\u2032)\nIn practice, this distribution can be estimated using a noise contrastive estimation approximation (Gutmann and Hyva\u0308rinen, 2012) while maximizing the log-likelihood:\n\u2211\ni\u2208pos(Mm)\n\u2211\nk\u2208{\u2212K,...,\u22121,1,...,K}\nlog p(ui+k | ui)\nwhere pos(Mm) are the indices of words in the monolingual corpus Mm.\nTo establish a bilingual embedding, with a parallel corpus Pm,n of source language m and target language n, Luong et al. (2015b) estimate conditional models of words in both source and target positions. The source positions are selected as sentential contexts (similar to monolingual skipgram), and the bilingual contexts come from aligned words. The bilingual objective is to maximize:\n\u2211\ni\u2208m-pos(Pm,n)\n\u2211\nk\u2208{\u2212K,...,\u22121,1,...,K}\nlog p(ui+k | ui) + log p(va(i)+k | ui)\n+ \u2211\nj\u2208n-pos(Pm,n)\n\u2211\nk\u2208{\u2212K,...,\u22121,1,...,K}\nlog p(vj+k | vj) + log p(ua(j)+k | vj)\n(1)\nwhere m-pos(Pm,n) and n-pos(Pm,n) are the indeces of the source and target tokens in the parallel corpus respectively, a(i) and a(j) are the positions of words that align to i and j in the other language. It is easy to see how this method can be extended for more than two languages by summing up the bilingual objective in Eq. 1 for all available parallel corpora."}, {"heading": "2.4 Translation-invariant matrix factorization", "text": "Gardner et al. (2015) proposed that multilingual embeddings should be translation invariant. Consider a matrix X \u2208 R|V|\u00d7|V| which summarizes the pointwise mutual information statistics between pairs of words in monolingual corpora, and let UV\u22a4 be a low-rank decomposition of X where U,V \u2208 R|V|\u00d7d. Now, consider another matrix A \u2208 R|V|\u00d7|V| which summarizes bilingual alignment frequencies in a parallel corpus. Gardner et al. (2015) solves for a low-rank decomposition UV\u22a4 which both approximates X as well as its transformations A\u22a4X, XA and A\u22a4XA by defining the following objective:\nminU,V \u2016X \u2212 UV \u22a4\u20162 + \u2016XA \u2212 UV\u22a4\u20162 + \u2016A\u22a4X \u2212 UV\u22a4\u20162 + \u2016A\u22a4XA \u2212 UV\u22a4\u20162\nThe multilingual embeddings are then taken to be the rows of the matrix U."}, {"heading": "3 Evaluating Multilingual Embeddings", "text": "One of our main contributions is to streamline the evaluation of multilingual embeddings. In addition\nto assessing goals (i\u2013iii) stated in \u00a72, a good evaluation metric should also (iv) show good correlation with performance in downstream applications and (v) be computationally efficient.\nIt is easy to evaluate the coverage (iii) by counting the number of words covered by an embedding function in a closed vocabulary. Intrinsic evaluation metrics are generally designed to be computationally efficient (v) but may or may not meet the goals (i, ii, iv). By design, standard (monolingual) word similarity tasks meet (i) while crosslingual word similarity tasks and the word translation tasks meet (ii). We propose another evaluation method (multiQVEC+), designed to simultaneously assess goals (i, ii). MultiQVEC+ extends QVEC (Tsvetkov et al., 2015), a recently proposed monolingual evaluation method, addressing fundamental flaws and extending it to multiple languages. To assess the degree to which these evaluation metrics meet (iv), in \u00a75 we perform a correlation analysis looking at which intrinsic metrics are best correlated with downstream task performance\u2014i.e., we evaluate the evaluation metrics."}, {"heading": "3.1 Word similarity", "text": "Word similarity datasets such as WS-353-SIM (Agirre et al., 2009) and MEN (Bruni et al., 2014) provide human judgments of semantic similarity. By ranking words by cosine similarity and by their empirical similarity judgments, a ranking correlation can be computed that assesses how well the estimated vectors capture human intuitions about semantic relatedness.\nSome of previous work on bilingual and multilingual embeddings has focused on monolingual word similarity to evaluate embeddings (e.g., Faruqui and Dyer., 2014). This approach is limited because it cannot measure the degree to which embeddings from different languages are similar (ii). For this paper, we report results on an English word similarity task, the Stanford RW dataset (Luong et al., 2013), as well as a combination of several cross-lingual word similarity datasets (Camacho-Collados et al., 2015)."}, {"heading": "3.2 Word translation", "text": "This task directly assesses the degree to which translationally equivalent words in different lan-\nguages are nearby in the embedding space. The evaluation data consists of word pairs which are known to be translationally equivalent. The score for one word pair (l1,w1), (l2,w2) both of which are covered by an embedding E is 1 if cosine(E(l1,w1),E(l2,w2)) \u2265 cosine(E(l1,w1),E(l2,w\u20322))\u2200w \u2032 2 \u2208 G l2 where Gl2 is the set of words of language l2 in the evaluation dataset, and cosine is the cosine similarity function. Otherwise, the score for this word pair is 0. The overall score is the average score for all word pairs covered by the embedding function. This is a variant of the method used by Mikolov et al. (2013b) to evaluate bilingual embeddings."}, {"heading": "3.3 Correlation-based evaluation", "text": "We introduce QVEC+\u2014an intrinsic evaluation measure of the quality of monolingual and multilingual word embeddings. Our method is a monolingual improvement and a multilingual extension of QVEC\u2014a recently proposed monolingual evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource (Tsvetkov et al., 2015). We review QVEC, and then describe QVEC+.\nQVEC. The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manually-annotated linguistic resource. Let the number of common words in the vocabulary of the word embeddings and the linguistic resource be N. To quantify the semantic content of embeddings, a semantic linguistic matrix S \u2208 RP\u00d7N is constructed from a semantic database, with a column vector for each word. Each word vector is a distribution of the word over P linguistic properties, based on annotations of the word in the database. Let X \u2208 RD\u00d7N be embedding matrix with every row as a dimension vector x \u2208 R1\u00d7N . D denotes the dimensionality of word embeddings. Then, S and X are aligned to maximize the cumulative correlation between the aligned dimensions of the two matrices. Specifically, let A \u2208 {0, 1}D\u00d7P be a matrix of alignments such that aij = 1 iff xi is aligned to sj, otherwise aij = 0. If r(xi, sj) is the Pearson\u2019s correlation between vectors xi and sj, then\nQVEC is defined as:\nQVEC = maxA:\u2211j aij\u22641\nX\u2211\ni=1\nS\u2211\nj=1\nr(xi, sj)\u00d7 aij\nThe constraint \u2211\nj aij \u2264 1, warrants that one distributional dimension is aligned to at most one linguistic dimension.\nQVEC has been shown to correlate strongly with downstream semantic tasks. However, it suffers from two major weaknesses. First, it is not invariant to linear transformations of the embeddings\u2019 basis, whereas the bases in word embeddings are generally arbitrary (Szegedy et al., 2014). Second, a sum of correlations produces an unnormalized score: the more dimensions in the embedding matrix the higher the score. This precludes comparison of models of different dimensionality. QVEC+ simultaneously addresses both problems.\nQVEC+. To measure correlation between the embedding matrix X and the linguistic matrix S, instead of cumulative dimension-wise correlation we employ CCA. CCA finds two sets of basis vectors, one for X\u22a4 and the other for S\u22a4, such that the correlations between the projections of the matrices onto these basis vectors are maximized. Formally, CCA finds a pair of basis vectors v and w such that\nQVEC+ = CCA(X\u22a4,S\u22a4) = maxv,w r(X \u22a4v,S\u22a4w)\nThus, QVEC+ ensures invariance to the matrices bases rotation, and since it is a single correlation, it produces a score in [\u22121, 1]. Both QVEC and QVEC+ rely on a matrix of linguistic properties constructed from a manually crafted linguistic resource. In this paper, instead of only constructing the linguistic matrix based on monolingual annotations, we use supersense tag annotations for English (Miller et al., 1993), Danish (Mart\u0131nez Alonso et al., 2015) and Italian (Montemagni et al., 2003) to create extensions of QVEC and QVEC+ for the multilingual case; henceforth, multiQVEC and multiQVEC+."}, {"heading": "3.4 Extrinsic tasks", "text": "In order to evaluate how useful the word embeddings are for a downstream task, we use the embedding vector as a dense feature representation of each word\nin the input, and deliberately remove any other feature available for this word (e.g., prefixes, suffixes, part-of-speech). For each task, we train one model on the aggregate training data available for several languages, and evaluate on the aggregate evaluation data in the same set of languages. We apply this for multilingual document classification and multilingual dependency parsing.\nFor document classification, we follow Klementiev et al. (2012) in using the RCV corpus of newswire text, and train a classifier which differentiates between four topics. While most previous work which used this data only in a bilingual setup, we simultaneously train the classifier on documents in seven languages,4 and evaluate on the development/test section of those languages. For this task, we report the average classification accuracy on the test set.\nFor dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) on a subset of the languages in the universal dependencies v1.15, and test on the same languages, reporting unlabeled attachment scores. We remove all part-of-speech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the provided (pretrained) embeddings as the token representation."}, {"heading": "4 Evaluation Portal", "text": "In order to facilitate future research on multilingual word embeddings, we developed a web portal6 to enable researchers who develop new estimation methods to evaluate them using a suite of evaluation tasks. The portal serves the following purposes:\n\u2022 Download the monolingual and bilingual data we used to estimate multilingual embeddings in this paper,\n\u2022 Download standard development/test data sets for each of the evaluation metrics to help researchers working in this area report trustwor-\n4Danish, German, English, Spanish, French, Italian and Swedish.\n5http://hdl.handle.net/11234/LRT-1478 6 http://128.2.220.95/multilingual\nmetric language ISO 639-1 codes\ndocument classification da, de, en, it, fr, sv dependency parsing bg, cs, da, de, el, en, es, fi, fr, hu, it, sv (multi)QVEC+/(multi)QVEC da, en, it word similarity de, en, es, fa, fr, it, pt\nword translation bg, cs, da, de, el, en, es, fi, fr, hu, it, sv, zh, af, ca, iw, cy, ar, ga, zu,\net, gl, id, ru, nl, pt, la, tr, ne, lv, lt, tg, ro, is, pl, yi, be, hy, hr, jw, ka, ht, fa, mi, bs, ja, mg, tl, ms, uz, kk, sr, mn, ko, mk, so, uk, sl, sw\nTable 1 lists the evaluation metrics used on the web portal along with the languages currently available."}, {"heading": "5 Experiments", "text": "Our experiments are designed to show two primary sets of results: (i) how well the proposed intrinsic evaluation metrics correlate with downstream tasks that use multilingual word vectors (\u00a75.1) and (ii) which estimation methods work best (\u00a75.2)."}, {"heading": "5.1 Correlations between intrinsic vs. extrinsic evaluation metrics", "text": "In this experiment, we consider four intrinsic evaluation metrics (cross-lingual word similarity, word translation, multiQVEC and multiQVEC+) and two extrinsic evaluation metrics (multilingual document classification and multilingual parsing).\nData: All evaluation data sets we used are available for download on the evaluation portal. For the cross-lingual word similarity task, we use the 307 English-Italian word pairs in the multilingual\n7Except for the original RCV documents, which are restricted by the Reuters license and cannot be republished. All other data is available for download.\nMWS353 dataset (Leviant and Reichart, 2015). For the word translation task, we use a subset of 647 translation pairs from Wiktionary in English, Italian and Danish. For multiQVEC and multiQVEC+, we used the 41 supersense tag annotations (26 for nouns and 15 for verbs) as described in \u00a73. For the downstream tasks, we use the English, Italian and Danish subsets of the RCV corpus and the universal dependencies v1.1.\nSetup: Estimating correlations between the proposed intrinsic evaluation metrics and downstream task performance requires a sample of different vector embeddings with their intrinsic and extrinsic task scores. To create this sample, we trained a total of 17 different multilingual embeddings8 for three languages (English, Italian and Danish).\nResults: Table 2 shows the correlations of the four intrinsic metrics against the performance of the vectors on the two downstream tasks. We establish (i) that intrinsic methods used in the literature (crosslingual word similarity and word translation) are poorly correlated with downstream tasks, and (ii) that both intrinsic methods we propose for evaluating multilingual word embeddings (i.e., multiQVEC and multiQVEC+) strongly correlate with both multilingual document classification and multilingual dependency parsing."}, {"heading": "5.2 Evaluating multilingual estimation methods", "text": "We now turn to evaluating multilingual embeddings obtained using the estimation methods in \u00a72.\nLanguages: We compare the four estimation methods in \u00a72 on three language sets of {3, 12, 59}\n817 = 12 multiCluster embeddings +1 multiCCA embeddings +1 multiSkip embeddings +2 translation-invariance embeddings.\nlanguages.9 Since the multiSkip and translationinvariance methods require word translation probabilities, we were only able to use them with the {3, 12}-language sets for which we have parallel corpora.\nData: As mentioned in \u00a72, the multiCluster and multiCCA estimation methods only require monolingual corpora and bilingual dictionaries, while the multiSkip and translation-invariance methods require parallel data. Details and pointers for downloading the data used to estimate and evaluate embeddings in each set of languages can be found on the evaluation portal.\nSetup: All embeddings trained for this evaluation have 40 dimensions. We used the development section of the evaluation methods (see \u00a74) for tuning hyperparameters. All skipgram-based models (multiCCA, multiSkip, and multiCluster) were trained using 10 epochs of stochastic gradient descent. We used a context window size of 3 for the translationinvariance method.10 For the other methods, we used a context window size of 3 (for the 3-language and 59-language embeddings) and 5 (for the 12- language embeddings). We only estimated embeddings for words/clusters which occur 5 times or more in the monolingual corpora.\nResults: We report results of this experiment separately for each set of languages in Tables 3, 4, and 5. Looking at the performance of downstream tasks in Tables 3, 4, we establish that both our proposed dictionary-based methods (multiCCA and multiCluster) outperform the multiSkip and translationinvariance methods. This is consistent for both classification and parsing. It is also clear that multiCCA outperforms multiCluster on most tasks. However, the intrinsic metrics do not always agree with the extrinsic results.\nAlthough the results are not always compa-\n9The ISO 639-1 codes of the three language sets we used are: {da, en, it}, {bg, cs, da, de, el, en, es, fi, fr, hu, it, sv}, and { bg, cs, da, de, el, en, es, fi, fr, hu, it, sv, zh, af, ca, iw, cy, ar, ga, zu, et, gl, id, ru, nl, pt, la, tr, ne, lv, lt, tg, ro, is, pl, yi, be, hy, hr, jw, ka, ht, fa, mi, bs, ja, mg, tl, ms, uz, kk, sr, mn, ko, mk, so, uk, sl, sw }.\n10Constructing the pointwise mutual information matrix for larger context window sizes was computaionally challenging.\nrable across the three tables,11 some of them are. For instance, the results of (multi)QVEC and (multi)QVEC+ are comparable across the three tables, since the semantic annotations required for computing the score are only available in Danish, English and Italian. We can see that the performance tends to decline as we go from three to twelve to fifty-nine languages. This is especially true for the multiCluster method because using bilingual dictionaries for more languages result in conflating more and more words in the same cluster, and all words in the same cluster share the same embedding. To avoid this problem, it may be worth exploring better ways of constructing multilingual word clustering from bilingual dictionaries (e.g., spectral clustering)."}, {"heading": "6 Previous Work", "text": "We focused on methods for training multilingual embeddings for many languages, but there is a rich body of literature on bilingual embeddings, including work on machine translation (Zou et al., 2013; Hermann and Blunsom, 2014; Cho et al., 2014; Luong et al., 2015b; Luong et al., 2015a, inter alia),12 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisky\u0300 et al., 2014). Word clusters is a related form of distributional representation; in clustering, cross-lingual distributional representations were proposed as well (Ta\u0308ckstro\u0308m et al., 2012)."}, {"heading": "7 Conclusion", "text": "We introduced two estimation methods for multilingual word embeddings, multiCCA and multiCluster, which only require bilingual dictionaries and monolingual corpora, and used them to train embeddings for 59 languages. We found the embeddings esti-\n11For example, the evaluation set used for word translation task in Tables 3, 4, and 5 uses word pairs in 3, 12, and 59 languages, respectively.\n12Hermann and Blunsom (2014) showed that the bicvm method can be extended to more than two languages, but the released software library only supports bilingual embeddings. We tried following the first author\u2019s recommendation at https://github.com/karlmoritz/bicvm/issues/4, but we were not able to reproduce their results.\nmated using our dictionary-based methods to outperform those estimated using other methods for two downstream tasks: multilingual dependency parsing and multilingual document classification. We also developed a new intrinsic method to evaluate multilingual embeddings and showed that it strongly correlates with downstream tasks (and runs faster). Finally, in order to help future research in this area, we created a web portal for users to upload their multilingual embeddings and easily evaluate them on nine evaluation metrics, with two modes of operation (development and test) to encourage sound experimentation practices."}, {"heading": "Acknowledgments", "text": "Waleed Ammar is supported by the Google fellowship in natural language processing. Part of this material is based upon work supported by a subcontract with Raytheon BBN Technologies Corp. under DARPA Prime Contract No. HR0011-15-C0013. This work was supported in part by the National Science Foundation through award IIS1526745. We thank Manaal Faruqui, Wang Ling, Kazuya Kawakami, Matt Gardner and Benjamin Wilson for helpful comments."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "In Proc. of JAIR", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A framework for the construction of monolingual and cross-lingual word similarity datasets", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proc. of ACL", "citeRegEx": "Camacho.Collados et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. of EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "Proc. of EACL. Association for Computational Linguistics", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Translation invariant word embeddings", "author": ["Gardner et al.2015] Matt Gardner", "Kejun Huang", "Evangelos Papalexakis", "Xiao Fu", "Partha Talukdar", "Christos Faloutsos", "Nicholas Sidiropoulos", "Tom Mitchell"], "venue": "In Proc. of EMNLP", "citeRegEx": "Gardner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments. arXiv preprint arXiv:1410.2455", "author": ["Gouws et al.2014] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": null, "citeRegEx": "Gouws et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Crosslingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of ACL", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of AAAI", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Hyv\u00e4rinen2012] Michael U Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In Proc. of JMLR", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Multilingual Models for Compositional Distributional Semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proc. of ACL", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proc. of COLING", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning bilingual word representations by marginalizing alignments", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In arXiv preprint arXiv:1405.0947", "citeRegEx": "Kocisk\u1ef3 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kocisk\u1ef3 et al\\.", "year": 2014}, {"title": "Judgment language matters: Towards judgment language informed vector space modeling", "author": ["Leviant", "Reichart2015] Ira Leviant", "Roi Reichart"], "venue": "In arXiv preprint arXiv:1508.00106", "citeRegEx": "Leviant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leviant et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of CoNLL, Sofia, Bulgaria", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proc. of ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015b] Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proc. of the 1st Workshop on Vector Space Modeling for Natural Language Processing", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Supersense tagging for danish", "author": ["Anders Johannsen", "Sussi Olsen", "Sanni Nimb", "Nicolai Hartvig Srensen", "Anna Braasch", "Anders Sgaard", "Bolette Sandford Pedersen"], "venue": "In Proc. of NODALIDA,", "citeRegEx": "Alonso et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2015}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proc. of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": "In arXiv preprint arXiv:1309.4168v1", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A semantic concordance", "author": ["Claudia Leacock", "Randee Tengi", "Ross T. Bunker"], "venue": "In Proc. of HLT,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "Building the italian", "author": ["Francesco Barsotti", "Marco Battista", "Nicoletta Calzolari", "Ornella Corazzari", "Alessandro Lenci", "Antonio Zampolli", "Francesca Fanciulli", "Maria Massetani", "Remo Raffaelli"], "venue": null, "citeRegEx": "Montemagni et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Montemagni et al\\.", "year": 2003}, {"title": "Intriguing properties of neural networks", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In Proc. of ICLR", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proc. of NAACL,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Cross-language parser adaptation between related languages", "author": ["Zeman", "Resnik2008] Daniel Zeman", "Philip Resnik"], "venue": "In Proc. of IJCNLP,", "citeRegEx": "Zeman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zeman et al\\.", "year": 2008}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "NLP tasks (Collobert and Weston, 2008), shared representation of words across languages offer intriguing possibilities (Klementiev et al., 2012).", "startOffset": 119, "endOffset": 144}, {"referenceID": 21, "context": "While previous work has used hand-engineered features that are cross-linguistically stable as the basis model transfer (Zeman and Resnik, 2008; McDonald et al., 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al.", "startOffset": 119, "endOffset": 166}, {"referenceID": 14, "context": ", 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016).", "startOffset": 99, "endOffset": 169}, {"referenceID": 11, "context": ", 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016).", "startOffset": 99, "endOffset": 169}, {"referenceID": 8, "context": "(Gardner et al., 2015) and a variant of the multiSkip method (Guo et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": ", 2015) and a variant of the multiSkip method (Guo et al., 2016).", "startOffset": 46, "endOffset": 64}, {"referenceID": 5, "context": "To do this, we align the corpus using fast align (Dyer et al., 2013) in both directions.", "startOffset": 49, "endOffset": 68}, {"referenceID": 10, "context": "We developed the multiSkip method independently of Guo et al. (2016). 2.", "startOffset": 51, "endOffset": 69}, {"referenceID": 22, "context": "We can then apply any monolingual embedding estimator; here, we use the skipgram model from Mikolov et al. (2013a).", "startOffset": 92, "endOffset": 115}, {"referenceID": 17, "context": "To establish a bilingual embedding, with a parallel corpus Pm,n of source language m and target language n, Luong et al. (2015b) estimate conditional models of words in both source and target positions.", "startOffset": 108, "endOffset": 129}, {"referenceID": 8, "context": "Gardner et al. (2015) solves for a low-rank decomposition UV\u22a4 which both approximates X as well as its transformations A\u22a4X, XA and A\u22a4XA by defining the following objective:", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "MultiQVEC+ extends QVEC (Tsvetkov et al., 2015), a recently proposed monolingual evaluation method, addressing fundamental flaws and extending it to multiple languages.", "startOffset": 24, "endOffset": 47}, {"referenceID": 0, "context": "Word similarity datasets such as WS-353-SIM (Agirre et al., 2009) and MEN (Bruni et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 1, "context": ", 2009) and MEN (Bruni et al., 2014) provide human judgments of semantic similarity.", "startOffset": 16, "endOffset": 36}, {"referenceID": 17, "context": "For this paper, we report results on an English word similarity task, the Stanford RW dataset (Luong et al., 2013), as well as a combination of several cross-lingual word similarity datasets (Camacho-Collados et al.", "startOffset": 94, "endOffset": 114}, {"referenceID": 2, "context": ", 2013), as well as a combination of several cross-lingual word similarity datasets (Camacho-Collados et al., 2015).", "startOffset": 84, "endOffset": 115}, {"referenceID": 22, "context": "This is a variant of the method used by Mikolov et al. (2013b) to evaluate bilingual embeddings.", "startOffset": 40, "endOffset": 63}, {"referenceID": 28, "context": "Our method is a monolingual improvement and a multilingual extension of QVEC\u2014a recently proposed monolingual evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource (Tsvetkov et al., 2015).", "startOffset": 214, "endOffset": 237}, {"referenceID": 26, "context": "First, it is not invariant to linear transformations of the embeddings\u2019 basis, whereas the bases in word embeddings are generally arbitrary (Szegedy et al., 2014).", "startOffset": 140, "endOffset": 162}, {"referenceID": 24, "context": "In this paper, instead of only constructing the linguistic matrix based on monolingual annotations, we use supersense tag annotations for English (Miller et al., 1993), Danish (Mart\u0131nez Alonso et al.", "startOffset": 146, "endOffset": 167}, {"referenceID": 25, "context": ", 2015) and Italian (Montemagni et al., 2003) to create extensions of", "startOffset": 20, "endOffset": 45}, {"referenceID": 14, "context": "For document classification, we follow Klementiev et al. (2012) in using the RCV corpus of newswire text, and train a classifier which differentiates between four topics.", "startOffset": 39, "endOffset": 64}, {"referenceID": 5, "context": "For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) on a subset of the languages in the universal dependencies v1.", "startOffset": 58, "endOffset": 77}, {"referenceID": 10, "context": ", 2015a, inter alia),12 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev et al.", "startOffset": 57, "endOffset": 93}, {"referenceID": 11, "context": ", 2015a, inter alia),12 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev et al.", "startOffset": 57, "endOffset": 93}, {"referenceID": 14, "context": ", 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisk\u1ef3 et al., 2014).", "startOffset": 51, "endOffset": 118}, {"referenceID": 9, "context": ", 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisk\u1ef3 et al., 2014).", "startOffset": 51, "endOffset": 118}, {"referenceID": 15, "context": ", 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisk\u1ef3 et al., 2014).", "startOffset": 51, "endOffset": 118}, {"referenceID": 27, "context": "Word clusters is a related form of distributional representation; in clustering, cross-lingual distributional representations were proposed as well (T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 148, "endOffset": 172}], "year": 2017, "abstractText": "We introduce new methods for estimating and evaluating embeddings of words from dozens of languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC+, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). On this evaluation and others, our estimation methods outperform existing ones. We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.", "creator": "LaTeX with hyperref package"}}}