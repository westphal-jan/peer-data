{"id": "1602.02196", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits", "abstract": "We present efficient algorithms for the problem of contextual bandits with i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of strategies. Our BISTRO algorithm requires d calls to the oracle of empirical risk minimization (ERM) per turn, where d is the number of actions. It uses unlabeled data to make the problem mathematically simple. If the ERM problem itself is mathematically difficult, we expand the approach by using multiplicative approximation algorithms for the ERM. The integrity gap of relaxation occurs only in the limit of regret and not as a benchmark. Finally, we show that the contextual version of the bandit problem is learnable (and efficient) when the fully information-supervised online learning problem has a non-trivial regret guarantee (and efficient).", "histories": [["v1", "Sat, 6 Feb 2016 00:34:59 GMT  (22kb)", "http://arxiv.org/abs/1602.02196v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexander rakhlin", "karthik sridharan"], "accepted": true, "id": "1602.02196"}, "pdf": {"name": "1602.02196.pdf", "metadata": {"source": "CRF", "title": "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits", "authors": ["Alexander Rakhlin", "Karthik Sridharan"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 19\n6v 1\n[ cs\n.L G\n] 6\nF eb"}, {"heading": "1 Introduction", "text": "A multi-armed bandit with covariates (also known as a contextual bandit) is a generalization of the classical multi-armed bandit problem [LR85]. As the name suggests, in this natural formulation the quality of the arms may depend on the observed set of covariates. Contextual bandits arise in many application areas, from ad placement and news recommendation to personalized medical care and clinical trials. In recent years, there has been a strong push to develop computationally efficient regret minimization methods with respect to a given set of policies [LZ08, DHK+11, BLL+11, AHK+14]. The grand goal here would be to develop efficient and statistically optimal methods for large (and possibly uncountable) sets of policies, just as machine learning and statistics succeeded in developing methods that perform well relative to rich classes of predictors (linear separators, SVMs, and so forth). Compared to batch learning, however, the state of affairs at the moment is quite poor. It appears to be difficult to develop scalable methods even for a finite set of policies, as witnessed by the papers mentioned earlier. To some extent, the reason is not surprising: while in statistical learning the batch nature of the problem suggests the empirical objective to optimize, the scope of algorithms for contextual bandits is not at all clear.\n[AHK+14] exhibit a computationally attractive method for a finite class of policies, given an ERM (empirical risk minimization) oracle for the class. The oracle model allows one to address the question of how much more difficult (computationally) the bandit problem is in comparison to the batch learning problem.\nIn the present paper, we introduce a family of efficient methods (and, more generally, a new algorithmic approach based on relaxations) for minimizing regret against a potentially uncountable\nclass F , given that the value of the ERM objective can be computed. In addition, we require access to i.i.d. draws of contexts (e.g. unlabeled data) \u2014 a realistic assumption in many application areas mentioned earlier. Our method requires only d oracle calls per round, irrespective of the size of the policy class. Furthermore, the results hold in the hybrid scenario where the contexts are i.i.d. but rewards evolve according to an arbitrary process.\nLet us now describe the scenario in more detail. On each round t = 1, . . . , n, we observe covariates xt \u2208 X , select an action y\u0302t \u2208 {1, . . . , d} \u225c [d], and observe the cost ct(y\u0302t) of the chosen action. Here ct \u2208 [0,1]d is a cost assignment to all actions, chosen by Nature independently of y\u0302t. This cost vector remains unknown to us, except for the coordinate ct(y\u0302t). Since we include randomized prediction methods, we denote the distribution over the d choices on round t by qt \u2208 \u2206d, and draw y\u0302t \u223c qt. The goal is to design a prediction method with small expected cumulative cost \u2211nt=1 qTt ct.\nWe assume that x1, . . . , xn are drawn i.i.d. from some unknown distribution Px on X . At the same time, we do not place any assumption on the sequence of costs c1, . . . , cn, which may evolve according to some arbitrary stochastic process, or be an \u201cindividual sequence,\u201d or even be chosen adaptively and adversarially. As such, our setting may be termed \u201chybrid i.i.d.-adversarial.\u201d Our results also hold in the so-called transductive setting, where the side information is presented ahead of time.1\nWe have in mind machine learning applications such as online ad or product placement, whereby the contextual information x1, . . . , xn of website visitors may be viewed as an i.i.d. sequence, yet the decisions made by these customers might be too complex to be described in a probabilistic form.\nA common way to encode the prior knowledge about the problem is to take a class F of functions (or, deterministic policies) X \u2192 [d], with the hope that one of the functions will incur small cost on the presented contexts. With this \u201cinductive bias,\u201d we then aim to make predictions as to minimize regret\nReg = n\u2211 t=1 qTt ct \u2212 inf f\u2208F n\u2211 t=1 f(xt)Tct, (1) where henceforth we abuse the notation by identifying the value f(x) \u2208 [d] with the standard basis vector ef(x). This regret formulation encodes the prior knowledge of the practitioner. If the modeling choice F is good and (1) is small, the algorithm is guaranteed to incur small loss\u2211nt=1 qTt ct. Modeling the set of solutions F to the problem is a more direct approach (in the spirit of statistical learning) as compared to the harder problem of positing distributional assumptions on the relationship between contexts and the rewards. (The latter approach typically suffers from the curse of dimensionality.)\nThe difficulty of the problem arises from the form of the feedback. The customer seeking to buy a product different from what is presented by the recommendation engine may leave the site without revealing her valuation for all the items. Similarly, in personalized care, we may only observe the effect of the drug choice selected for the given patient. It is well recognized that exploration\u2014or randomization\u2014is required in these problems. Yet, in the contextual bandit setting the explorationexploitation trade-off is not simple, as the quality of the arms changes with the context in a way that is only indirectly captured by the benchmark term.\nOnline multiclass classification with one bit (correct-or-not) feedback can be seen as an example of our setting. In that case ct is a standard basis vector eyt for some class yt \u2208 [d], and the feedback\n1In Section 6 we also discuss the fully-adversarial case (see [ACBFS02, MS09] for the famous EXP4 algorithm for finite F).\nis ct(y\u0302t) = I{y\u0302t \u2260 yt}. Unlike [KSST08], we posit that side information is i.i.d.\u2014an assumption that will play a key role in developing computationally efficient methods, even for the indicator (rather than the easier hinge) loss.\nThe hybrid i.i.d.-adversarial scenario has been studied in both the full information and contextual bandit settings in [LM09]. Their algorithm, as well as the algorithm of [BLL+11], maintain distributions over the set of functions and, hence, computation can be linear in the size of F .\nFor the case whenF is finite, the upper bound for BISTRO provided in Theorem 2 isO(n3/4(log \u2223F \u2223)1/4). The work of [AHK+14] gives a better O(n1/2(log \u2223F \u2223)1/2) rate for the case when rewards are i.i.d. On the other hand, our results hold for\n\u2022 arbitrary F and arbitrary reward sequences,\n\u2022 approximate ERM values and a way to address the computational problem associated to ERM.\nWe remark that if contexts are arbitrary as well, our setting subsumes the problem of multiclass prediction with bandit feedback and indicator loss, as described above. Even for the multiclass hinge loss, it is still unclear (at least to the authors) whether the rate O(n2/3) for the linear classifier considered in [KSST08] can be improved.2 It is, therefore, an open question whether the O(n3/4) rates achieved by our method for the hybrid scenario for arbitrary classes F can be improved.\nThere are several new techniques that make it possible to develop computationally feasible prediction methods with nontrivial regret guarantees:\n\u2022 First is the idea of relaxations, presented in [RSS12] for the full-information setting. An extension to partial information case has been a big roadblock for developing new bandit methods. We present this extension here.\n\u2022 Second is the idea of a random playout, also employed in [RS15]. We show that by having access to unlabeled contexts, the computational (and statistical) difficulty of integrating with respect to the unknown distribution simply disappears.\n\u2022 We extend the notion of classical Rademacher averages to the case of vector-valued functions. The symmetrization technique in this case is of independent interest.\n\u2022 In many cases, the offline ERM optimization problem (which we assume away as an \u201coracle call\u201d) may be NP hard. Building on the technique of [RS15], we employ optimization-based relaxations for integer programs. We prove that the regret bound of the resulting algorithm only worsens by a multiplicative factor that is related to the ratio of average widths of the relaxed and the original sets.\nIt is worth emphasizing again that the family of prediction methods presented in this work is drived from the partial-information extension of the relaxation framework, and the resulting algorithms are distinct from the ones appearing in the literature. We believe that this approach is systematic and can partially fill the gap in our understanding of the algorithmic possibilities for contextual bandits.\n2The O(n1/2) rate in [HK11] is only proved for the case of log-loss."}, {"heading": "2 Notation", "text": "We denote [d] \u225c {1, . . . , d} and a1\u2236t \u225c {a1, . . . , at}. Let \u2206d be the probability simplex over d coordinates. The vector of ones is denoted by 1 and an indicator of event A by I{A}. For a matrix M , we use Mt to refer to its t-th column."}, {"heading": "3 Setup", "text": "Let us recall the online protocol. On each round t \u2208 [n], we observe side information xt \u2208 X , predict y\u0302t \u223c qt \u2208\u2206d, and observe feedback ct(y\u0302t) for some ct \u2208 [0,1]d.\nGiven x1\u2236n, it is convenient to work with a matrix representation of the class F projected on these data. Each f \u2208 F yields sequence (f(x1), . . . , f(xn)), which we collect as a d \u00d7 n matrix Mf , defined as Mf(j, t) = I{f(xt) = j} . (2) Let M\u0302 = M\u0302[x1\u2236n] = {Mf \u2236 f \u2208 F} denote the collection of matrices. (The hat on M\u0302 will remind us of the dependence of this set on x1\u2236n, even if not explicitly mentioned).\nWe may now define the oracle employed by the prediction method:\nDefinition 1. Given a class F of policies X \u2192 [d], a set of covariates x1\u2236n, and a real-valued d \u00d7n matrix Y , a value-of-ERM oracle returns the value\ninf M\u2208M\u0302[x1\u2236n] n\u2211 t=1 MTt Yt . (3)\nThe oracle is called \u03b4-approximate if the reported value is within \u03b4 from the minimum.\nWe may express the comparator term in (1) as an ERM objective (3) with Y = [c1, . . . , cn]. Closely related to this expression is a new (to the best of our knowledge) definition of Rademacher averages for vector-valued functions: given x1\u2236n, define\nR(F ;x1\u2236n) \u225cR(M\u0302) \u225c E\u01eb1\u2236n sup M\u2208M\u0302 n\u2211 t=1 MTt \u01ebt (4)\nwhere \u01eb1, . . . ,\u01ebn are d-dimensional vectors with independent Rademacher random variables. We observe that Rademacher complexity is nothing but a (negative of) the ERM objective with the random matrix [\u2212\u01eb1, . . . ,\u2212\u01ebn]. Indeed, as in the classical case, correlation of the vector valued function class F with noise measures its complexity."}, {"heading": "4 Relaxations for Partial Information", "text": "Let us write the information obtained on round t as a tuple It(xt, qt, y\u0302t, ct) = (xt, qt, y\u0302t, ct(y\u0302t)), keeping in mind that xt is revealed before qt is chosen. In full information problems, It contains the vector ct, but not so in our bandit case. For partial information problems, it turns out to be crucial to include qt in the definition of It, in addition to the value ct(y\u0302t).\nA partial-information relaxation Rel () is a function that maps (I1, . . . , It) to a real value, for any t \u2208 [n]. We say that the partial-infromation relaxation Rel (I1, . . . , It) is admissible if for any t \u2208 [n], for all I1, . . . , It\u22121,\nE xt inf qt max ct E y\u0302t\u223cqt {ct(y\u0302t) +Rel (I1\u2236t\u22121, It(xt, qt, y\u0302t, ct))} \u2264Rel (I1\u2236t\u22121) (5) and for all x1\u2236n,c1\u2236n, and q1\u2236n,\nE\ny\u03021\u2236n\u223cq1\u2236n Rel (I1\u2236n) \u2265 \u2212 inf f\u2208F n\u2211 t=1 f(xt)Tct . (6) In the above expressions, xt follows the (unknown) distribution Px, qt ranges over distributions on[d], and ct over [0,1]d.\nAny randomized strategy (qt)nt=1 that certifies the inequalities (5) and (6) is called an admissible strategy. Lemma 1. Let Rel () be an admissible relaxation and (qt)nt=1 an admissible strategy. Then for any c1\u2236n,\nE[Reg] \u2264Rel (\u2205) . The above partial-information relaxation setup appears to be \u201cthe right\u201d analogue of the fullinformation relaxation framework. While we do not present it here, one may recover the EXP4 algorithm through the above approach, with the correct regret bound.\nWe will now present an admissible strategy for the contextual bandit problem, assuming we can sample from the distribution Px, or have access to unlabeled data."}, {"heading": "5 The BISTRO Algorithm", "text": "For any t \u2208 [n], define a d \u00d7 n matrix Y (t) as Y (t) = [c1, . . . , ct\u22121, ct,2\u01ebt+1, . . . ,2\u01ebn] with \u01ebs \u2208 {\u00b11}d a vector of independent Rademacher random variables. At each step t \u2208 [n], the randomized method presented below calculates a distribution qt \u2208\u2206d with each coordinate at least \u03b3 and defines an unbiased estimate c\u0303t of ct in a usual manner as c\u0303t(j) = I{y\u0302t = j} \u00d7 ct(y\u0302t)/qt(j). It is standard to verify that Ey\u0302t\u223cqt c\u0303t = ct. We then define Y\u0303 (t) = [c\u03031, . . . , c\u0303t\u22121, c\u0303t,2\u03b3\u22121\u01ebt+1, . . . ,2\u03b3\u22121\u01ebn], (7) and recall that Y\u0303 (t) s denotes the s-th column of this matrix. The next theorem is the main result of the paper.\nTheorem 2. The partial-information relaxation\nRel (I1\u2236t) = E (x,\u01eb)t+1\u2236n sup M\u2208M\u0302 {\u2212 n\u2211 s=1 MTs Y\u0303 (t) s } + (n \u2212 t)\u03b3 (8)\nis admissible. An admissible randomized strategy for this relaxation is given by BISTRO (Algorithm 1). The expected regret of the algorithm with \u03b3 = \u221a 2ER(F ;x1\u2236n)/(nd) is upper bounded by 2 \u221a 2d \u22c5 n \u22c5ER(F ;x1\u2236n).\nAlgorithm 1 BISTRO: BandItS wiTh RelaxatiOns input Parameter \u03b3 \u2208 (0,1/d) 1: for t = 1, . . . , n do 2: Observe xt. Draw xt+1\u2236n \u223c Px and \u01ebt+1\u2236n . 3: Construct Y\u0303 (t) and define q\u2217t to be a minimizer of\nmax j\u2208[d] \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9q T ej \u2212 min M\u2208M\u0302[x1\u2236n] {\u2211 s\u2260t \u03b3MTs Y\u0303 (t) s +M T t ej}\u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad over q \u2208\u2206d and set\nqt = (1 \u2212 \u03b3d)q\u2217t + \u03b31. (9) 4: Predict y\u0302t \u223c qt and observe ct(y\u0302t). 5: Create an estimate c\u0303t:\nc\u0303t(j) = I{y\u0302t = j} \u00d7 ct(y\u0302t)/qt(j). 6: end for\nThe draw xt+1\u2236n \u223c Px can be realized by drawing from a pool of unlabeled data. The random signs comprising the matrix Y\u0303 provide a form of \u201cregularization\u201d. We remark that in experiments, one may obtain better performance by replacing the factor 2 in (7) with a smaller value, or even with zero. A theoretical justification for this (which is related to using a surrogate loss) is beyond the scope of this paper.\nLemma 3. The calculation of q\u2217t in BISTRO 3 can be done by a water-filling argument and requires d calls to the ERM oracle.\nProof of Lemma 3. The optimization problem in Algorithm 1 is of the form\nmin q\u2208\u2206d max j\u2208[d] {qj \u2212 \u03c8j} where \u03c8j is the value of the infimum over M\u0302 corresponding to ej , and it is solved by a water-filling argument which we describe next. Each value \u03c8j is a value-of-ERM oracle call. Let \u03c8(1) \u2265 . . . \u2265 \u03c8(d) be a sorted order of these values, and let q(1) = . . . = q(d) = 0 be the initial values of the corresponding coordinates of the solution q. Start with a unit amount and assign q(1) = \u03c8(1) \u2212 \u03c8(2). Then add \u03c8(2)\u2212\u03c8(3) to both q(1) and q(2), and proceed until either the unit mass is exhausted, or the smallest coordinate (d) in the ordering is reached and filled. In the former case, q is the solution, and the latter case requires us to uniformly fill all the coordinates of q until they sum to one. It is easy to see that this procedure minimizes the maximum difference.\n3\u2018Bistro\u2019 means \u2018fast\u2019 in Russian.\nThe algorithm only requires the value of the ERM objective, not the solution. Furthermore, this value can be \u03b4-approximate, and the additional error is O(n\u03b4) over the n rounds. This provides extra flexibility, since approximate ERM values may be obtained via optimization methods.\nPerhaps the most unusual aspect of the algorithm is the use of unlabeled data. It is an example of a general random playout idea. In the setting of online linear optimization, the Follow-thePerturbed-Leader method is an example of such a random playout, yet the idea extends well beyond this scenario. As shown in [RSS12], the random playout technique can be applied when a certain worst-case-choice can be replaced with a known bad-enough distribution. However, when side information xt is i.i.d., the step is not even required. Furthermore, an inspection of the proof shows that we may deal with x\u2019s coming from a non-i.i.d. stochastic process, as long as we are able to draw future samples from it.\nWe also remark that (9) may be applied only to the coordinates that are close to zero, if any. The potential suboptimality of the O(n3/4) bound stems from the uniform exploration. It is an open question whether this can be improved systematically for all classes F , or whether there is a different structural property that allows one to avoid this form of exploration."}, {"heading": "6 Extensions", "text": "In this section, we outline several extensions of BISTRO. Specifically, we show how to incorporate additional data-based constraints, and how to use further optimization-based relaxations (such as LP or SDP), to obtain polynomial time methods for the ERM (or regularized ERM) solution. We show that one obtains a regret bound that only worsens by a factor related to the integrality gap of the integer program relaxation. With an eye on both computation and prediction performance, these techniques expand the applicability of BISTRO."}, {"heading": "6.1 Data-dependent policy classes", "text": "An inspection of the proof reveals that all the steps go through if define regret in (1) with respect to a data-dependent class F[x1\u2236n]:\nn\u2211 t=1 qTt ct \u2212 inf f\u2208F[x1\u2236n] n\u2211 t=1 f(xt)Tct. (10) In this case, given x1\u2236n, to each f \u2208 F[x1\u2236n] we associate Mf as defined in (2), and take M\u0302 = {Mf \u2236 f \u2208 F[x1\u2236n]}. The BISTRO algorithm is then identical, while the regret upper bound of Theorem 2 now replaces ER(F ;x1\u2236n) with ER(F[x1\u2236n];x1\u2236n).\nThe ability to change the set of policies according to the actual data allows an extra degree of flexibility. This flexibility can be realized via additional global constraints in terms of x1\u2236n, as we show in the next few sections. We also discuss a concrete example."}, {"heading": "6.2 Data-based constraints", "text": "A particular way to define a data-dependent subset of F is via constraints. Suppose we let C(f ;x1\u2236n) be the degree to which f \u2208 F violates constraints with respect to the given data x1\u2236n. We then\ndefine FK[x1\u2236n] = {f \u2208 F \u2236 C(f ;x1\u2236n) \u2264K}, (11) a pruning of the original class that keeps only those policies that do not violate the constraints by more than K. Let us give an example.\nExample: Product Recommendation Suppose at each time step we are asked to recommend one of d products to a person, based on her covariate information xt. Let F be a set of policies that map xt to the particular choice of the product (e.g. the label achieving maximum projection of xt onto d vectors wj; here F may consist of all such unit vector tuples). The payoff is whether the person decided to buy the recommended product. However, suppose xt also encodes the location (physical, or within a network), and we believe it is a good idea to focus recommendations such that near-by people are targeted with the same product. The marketing motivation here is two-fold: first, the recommendations would reinforce each other when individuals communicate, or if one of them buys the product; second, in a social network near-by individuals (friends) tend to have similar tastes, and thus a good policy would suggest similar items.\nThe objective of enforcing similarity of recommendations is a global constraint that can only be checked once we know all the x1, . . . , xn. We can easily incorporate the constraint into the definition of FK[x1\u2236n] as follows. Let w(xs, xr) be the cost of providing different recommendations to xs and xr (which is smaller if the two individuals are \u201cfar\u201d). In the case of a network, we may set, for instance, w(xs, xr) = 0 if the sth person is more than a hop away from the rth person. Define\nC(f ;x1\u2236n) = \u2211 s,r\u2208[n] w(xs, xr)I{f(xs) \u2260 f(xr)} , (12) the constraint violation by f in assigning products to the given set of individuals. Let FK[x1\u2236n] be defined as in (11). Note that the constraint is not on the behavior of the recommendation engine, but on the set of policies that we hope will do well for the problem. If there is indeed the effect of reinforcement of recommendations or similarity of tastes within the local neighborhood, the restriction to a smaller set FK[x1\u2236n] is justified.\nWithin the same setting of product recommendation, we might instead take a set of policies ensuring that within each neighborhood at least k individuals receive each particular product recommendation. This constraint, which roughly corresponds to \u201ccoverage\u201d of the relevant population, can be written as\nC(f ;x1\u2236n) =\u2211 \u2113 \u2211 j\u2208[d] \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3k \u2212 \u2211s\u2208T\u2113 f(xs)[j] \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6+\nwhere {T\u2113}\u2113 is a partition of [n] into neighborhoods according to information contained in x1\u2236n. The above two examples give a flavor of the constraints that can be encoded \u2014 the framework is flexible enough to fit a wealth of scenarios.\nFrom the computational point of view, it might be difficult to obtain the ERM value over a constrained set FK[x1\u2236n]. Instead, we consider an additional form of relaxation, where the constraint is subtracted off as a Lagrangian term. We will then employ certain linear programming relaxations to solve the product recommendation problem. Notably, by going to a regularized version of relaxations we are not changing the regret definition, which is still with respect to the constrained set."}, {"heading": "6.3 Regularized relaxation", "text": "Let FK[x1\u2236n] = {f \u2208 F \u2236 C(f ;x1\u2236n) \u2264 K} be the constrained set for some value K and a constraint function C, as in the previous section. Let us write C(M ;x1\u2236n) for the matrix representation the corresponding f \u2208 F . The following form of a relaxation may be better suited for approximation algorithms than the one where the constraint is strictly enforced.\nLemma 4. For any \u03bb,K > 0, the partial-information relaxation\nE\n(x,\u01eb)t+1\u2236n sup M\u2208M\u0302 {\u2212 n\u2211 s=1 MTs Y\u0303 (t) s \u2212 \u03bbC(M ;x1\u2236n)}\n+ \u03bbK + (n \u2212 t)\u03b3 (13) is admissible, where M\u0302 denotes the matrix representation of the original (unconstrained) set F of policies.\nProof of Lemma 4. We check that the initial condition is satisfied. For this purpose, let M\u0302K be the set of matrices corresponding to the constrained set FK[x1\u2236n]. Similarly to (18) in the proof of Theorem 2,\n\u2212 inf f\u2208FK[x1\u2236n] n\u2211 t=1 f(xt)Tct \u2264 E sup M\u2208M\u0302K n\u2211 t=1 \u2212MTt Y\u0303 (n) t \u2264 E sup M\u2208M\u0302 { n\u2211 t=1 \u2212MTt Y\u0303 (n) t \u2212 \u03bbC(M ;x1\u2236n)} + \u03bbK.\nThe second inequality holds since all the matrices in the former supremum have the constraint value bounded by K. The recursive condition argument follows exactly as in the proof of Theorem 2.\nThe only change required for BISTRO is to define the optimization objective in terms of regularized ERM values\nmin M\u2208M\u0302 {\u2211 s\u2260t \u03b3MTs Y\u0303 (t) s +M T t ej + \u03b3 \u22121\u03bbC(M ;x1\u2236n)} (14)\nover the unconstrained set of matrices corresponding to F . While the required minimization problem is over an unconstrained set of policies, we can control the expected regret\nn\u2211 t=1 qTt ct \u2212 inf f\u2208FK[x1\u2236n] n\u2211 t=1 f(xt)Tct. (15) of the modified BISTRO with respect to the constrained set FK[x1\u2236n], which is the original goal. The regret is given by Rel (\u2205), which is at most\nE sup M\u2208M\u0302 {\u2212\u03b3\u22121 n\u2211 t=1 MTt \u01ebt \u2212 \u03bbC(M ;x1\u2236n)} + nd\u03b3 + \u03bbK. It is possible to optimally balance \u03bb with respect toK and the Rademacher averages in a data-driven manner, but we omit this step for brevity.\nAs we illustrate in the next section, optimization problems of the form (14) may admit a linear programming (or other) relaxation, offering an alternative to the optimization problem over the constrained set."}, {"heading": "6.4 Optimization-based relaxations", "text": "To make the algorithm of this paper more applicable, we discuss here the situation where the ERM oracle or the regularized ERM oracle for the class FK[x1\u2236n] (or the unconstrained set F) is a difficult or even an NP-hard integer program. The idea is to choose a superset M\u0303 \u2287 M\u0302 for which the linear optimization problem is easier. Lemma 5. Let M\u0303 \u2287 M\u0302 be a set of matrices such that the column sum \u2211dj=1Mt(j) \u2264 1 for any M \u2208 M\u0303 and t \u2208 [n]. Then the partial information relaxation\nRel (I1\u2236t) = E (x,\u01eb)t+1\u2236n sup M\u2208M\u0303 {\u2212 n\u2211 s=1 MTs Y\u0303 (t) s } + (n \u2212 t)\u03b3\nis admissible. BISTRO (with ERM over M\u0303 rather than M) is an admissible strategy for this relaxation and the expected regret is upper bounded by\n2 \u221a\n2d \u22c5 n \u22c5ER(M\u0303). Similarly, using M\u0303 in (13) yields an admissible relaxation, and BISTRO with the corresponding regularized ERM is an admissible strategy.\nThe set M\u0303[x1\u2236n] may be defined via linear programming or SDP relaxations for integer programs, or via Lasserre/Parrilo hierarchies [Las01, Par03]. There is a large body of literature that aims at understanding the integrality gap in relaxing the integer program. These results are directly applicable to the present problem.\nAs a concrete example, consider the product recommendation example in the previous section, and consider the cost (12) for each policy and the restriction FK[x1\u2236n] in (11). We assume here that F is the set of all possible labelings, since in general the optimization problem will depend on the structure of F and its description. Let us phrase the regularized ERM integer program (14) as a Metric Labeling Constraint [KT02] problem. The general form of this integer program is given for z \u2208 [d]n by\ng(z) = \u2211 v\u2208V d1(v, zv) + \u2211 (u,v)\u2208E W(u,v)d2(zu, zv) (16) where G = (V,E,W ) is a graph with nonnegative weights, \u2223V \u2223 = n, the value d1 \u2236 V \u00d7 [d] \u2192 R is a cost of assigning a label to a node, and the separation cost d2 \u2236 [d] \u00d7 [d] \u2192 R\u22650 on the edges is a metric on the space of labels. The Metric Labeling Constraint problem asks for a solution that minimizes g(z) over [d]n.\nFor our application to product recommendation we convert the regularized minimization objective of (14) with the constraint (12) into the above form (16) by matching the assignment costs to the linear part and the separation costs to the constraint part (12). More precisely, let G be a fully connected graph with weights W(s,r) = \u03b3\n\u22121\u03bb \u22c5w(xs, xr) between nodes corresponding to xs and xr. The indices of vertices correspond to time steps in [n], and zv corresponds to the coordinate chosen by the particular M at time v. We take d1(v, zv) to be the value \u03b3eTzv Y\u0303 (t)v if v \u2260 t and eTzvej if v = t. Define d2(a, b) = I{a \u2260 b} to be the uniform metric. We may also define a metric on the space of products, assigning smaller distance to similar items.\n[KT02] give an LP relaxation for the Metric Labeling Constraint problem. The set that defines the relaxation is precisely the set M\u0303 we seek. Furthermore, the authors prove a 2-approximation ratio for the uniform metric, which is the case here. ([CKNZ04] prove an integrality gap of O(log k) for the general case).\nGiven the 2-approximation ratio result, we conclude that the regret bound for BISTRO with the LP program as the relaxation of the regularized ERM is only a constant worse than the bound with the constrained set FK[x1\u2236n]. The exact optimization over the latter set may be computationally intractable, while we provide an efficient method to achieve a bound, optimal to within a constant. As already noted in [RS15], such an approach that fuses approximation algorithms and online relaxations is able to produce polynomial-time methods with regret defined as 1\u00d7 the benchmark, while the benchmark itself may be NP-hard. This phenomenon can be attributed to the improper nature of the predictions, which need not be consistent with any particular policy in F .\nMore generally, by obtaining a multiplicative approximation of gap for the integer program, one may derive ER(M\u0303[x1\u2236n]) \u2264 O(gap) \u00d7 ER(M[x1\u2236n]). (17) Then one obtains a method with better computational properties and a regret bound which is only O(\u221agap) worse. Once again, the factor in front of the comparator in the definition (1) of regret is still one when using M\u0303 as a relaxation.\nFinally, we remark that (17) is comparing an average width of M\u0303 (largest projection onto noise) with an average width of M. Such a comparison of average widths (and, therefore, \u201caverage gap\u201d) for useful sets of contextual bandit policies F appears to be an interesting area of further investigation. We refer to [RS15], where some of these ideas have been developed in the context of cut-based constraints for node prediction on graphs."}, {"heading": "6.5 Adversarial contexts", "text": "Suppose we place no assumption on the evolution of xt\u2019s, which may now be treated as worst-case. This problem subsumes the full information online classification setting, and, hence, one cannot hope to have nontrivial regret against policy classes F with infinite Littlestone dimension. More generally, the best one can hope for is to say that the adversarial contextual bandit problem can be solved whenever the corresponding full information problem may be solved. We now present essentially this result: if there is a full-information relaxation, then one may use it to solve the adversarial contextual bandit problem. Moreover, based on the work of [RSS12, FRS15], all the known online learning methods appear to be relaxation based. Hence, we essentially prove below that\nIf a problem is online learnable in the full-information adversarial setting, then it is learnable in the adversarial contextual bandit setting. Furthermore, if the former is computationally tractable, then so is the latter.\nTo be precise, the full information version of contextual problem is as follows. On round t, we observe xt \u2208 X , predict y\u0302t \u2208 [d], and observe ct \u2208 [0,1]d. The regret is defined as before, with our cumulative cost being \u2211 ct(y\u0302t).\nA full information relaxation Rel\u2020 (c1, . . . , ct) is admissible if sup xt inf qt max ct E y\u0302t\u223cqt {ct(y\u0302t) +Rel\u2020 (c1\u2236t)} \u2264Rel\u2020 (c1\u2236t\u22121)\nand\nRel\u2020 (c1\u2236n) \u2265 \u2212 inf f\u2208F n\u2211 t=1 f(xt)Tct . Similarly, a partial information relaxation is admissible in this adversarial case when c1\u2236t are replaced with I1\u2236t in the above admissibility definition, as in Section 4. Lemma 6. If Rel\u2020 () is an admissible full-information relaxation for the adversarial scenario, then Rel (I1\u2236t) \u225c \u03b3\u22121Rel\u2020 (\u03b3c\u03031, . . . , \u03b3c\u0303t) + (n \u2212 t)d\u03b3 is admissible for the partial information scenario. Prediction qt is obtained as qt = (1 \u2212 d\u03b3)q\u2217t + \u03b31 where q\u2217t is computed by solving for a full-information strategy with the scaled unbiased estimates of costs. The resulting regret upper bound is\n2 \u221a d \u22c5 n \u22c5Rel\u2020 (\u2205).\nProof of Lemma 6. Let us first check the initial condition. We have that\nE y\u03021\u2236n\u223cq1\u2236n Rel (I1\u2236n) = E y\u03021\u2236n\u223cq1\u2236n \u03b3\u22121Rel\u2020 (\u03b3c\u03031, . . . , \u03b3c\u0303n) \u2265 E\ny\u03021\u2236n\u223cq1\u2236n \u2212 inf f\u2208F n\u2211 t=1 f(xt)Tc\u0303t \u2265 \u2212 inf f\u2208F n\u2211 t=1 f(xt)Tct where the first inequality is due to admissibility of the full-information relaxation, and the second is due to Jensen\u2019s inequality and unbiasedness of c\u0303t. For the recursive part, we follow the proof of Theorem 2 and note that all the statements, until the end, are done conditionally on xt. Define the strategy q\u2217t as\nq\u2217t = argmin q\u2208\u2206d sup c\u0303\u2208\u03b3\u22121[0,1]d {qT(\u03b3c\u0303t) +Rel\u2020 (\u03b3c\u03031, . . . , \u03b3c\u0303t)} and let qt = (1 \u2212 d\u03b3)q\u2217t + \u03b31. Given xt, (22) tells us\nmax ct\u2208[0,1]d Ey\u0302t\u223cqt{ct(y\u0302t) +Rel (I1, . . . , It) } \u2264 sup c\u0303t\u2208\u03b3\u22121[0,1]d {(q\u2217t )Tc\u0303t +Rel (I1, . . . , It)} + d\u03b3 which is equal to\n\u03b3\u22121 sup c\u0303t {(q\u2217t )T(\u03b3c\u0303t) +Rel\u2020 (\u03b3c\u03031, . . . , \u03b3c\u0303t)} + (n \u2212 t + 1)d\u03b3 \u2264 \u03b3\u22121Rel\u2020 (\u03b3c\u03031, . . . , \u03b3c\u0303t\u22121) + (n \u2212 t + 1)d\u03b3\nby admissibility of the full-information relaxation. Observe that the use of the full-information relaxation on \u03b3c\u0303t\u2019s is warranted since these vectors are in [0,1]d. This concludes the proof.\nWe remark that the time complexity of the adversarial contextual bandit solution in Lemma 6 is the same as the time complexity of the corresponding full information procedure."}, {"heading": "7 Open Problems and Future Directions", "text": "The main open problem is whether the regret upper bound for BISTRO or a related method can be improved. In the inequality (22) we decouple the distribution q\u2032t from qt, and this appears to be the source of the loseness, at least in the analysis. A more precise analysis at this step might resolve the issue. It is unclear what kind of structure of F may be used to improve computation and/or regret guarantees of BISTRO.\nUnder structural assumptions on F one may come up with sufficient statistics for the information I1\u2236t and, therefore, avoid keeping around all the estimates c\u0303t. Of course, this is the case in noncontextual bandits, where the sum \u2211 c\u0303t is sufficient (at least as evidenced by existing near-optimal bandit methods).\nAn interesting avenue of investigation is to study the more general case when x\u2019s are drawn from a stochastic process with a parametrized form. One may then attempt to estimate the parameters of the process on-the-go and use the estimate to hallucinate future data for random playout."}, {"heading": "8 Proofs", "text": "Proof of Lemma 1. In the proof, we use the shorthand \u27ea. . .\u27ebnt=1 do denote repeated application of the operators within the brackets from t = 1 to n. As an example, the sequence of operators\nE x1 max c1 E x2 max c2 [G(x1, c1, x2, c2)] acting on the function G is abbreviated as \u27eaExt maxct\u27eb2t=1 [G(x1, c1, x2, c2)].\nLet q1, . . . , qn be an admissible strategy. The expected regret of this strategy can be upper bounded by\nE[Reg] \u2264 sup c1\u2236n E[Reg] \u2264 \u27eaE xt sup ct \u27ebn t=1 [ n\u2211 t=1 qTt ct \u2212 inf f\u2208F n\u2211 t=1 f(xt)Tct] by Jensen\u2019s inequality (pulling Ext out of multiple suprema until its t-th position). The last expression is further upper bounded by\n\u27eaE xt sup ct \u27ebn t=1 [ n\u2211 t=1 qTt ct + E y\u03021\u2236n\u223cq1\u2236n Rel (I1\u2236n)] by admissibility of the partial information relaxation. By linearity of expectation for Ey\u0302t and Jensen\u2019s inequality (to pull it out through multiple suprema as before), we obtain an upper bound of\n\u27eaE xt sup ct E y\u0302t\u223cqt \u27ebn t=1 [ n\u2211 t=1 ct(y\u0302t) +Rel (I1\u2236n)] . We now start from step n and observe that \u2211n\u22121t=1 ct(y\u0302t) does not depend on xn, cn, y\u0302n, and thus we rewrite the preceding expression as\n\u27eaE xt sup ct E y\u0302t\u223cqt \u27ebn\u22121 t=1 [n\u22121\u2211 t=1 ct(y\u0302t) + E xt sup ct E y\u0302t\u223cqt {cn(y\u0302n) +Rel (I1\u2236n)}] .\nBy admissibility of qt and (5), we pass to the upper bound of\n\u27eaE xt sup ct E y\u0302t\u223cqt \u27ebn\u22121 t=1 [n\u22121\u2211 t=1 ct(y\u0302t) +Rel (I1\u2236n\u22121)] . Continuing in this fashion leads to a bound of Rel (\u2205)."}, {"heading": "A Proof of Theorem 2", "text": "Admissibility: initial condition For any c1\u2236n, q1\u2236n, x1\u2236n, it holds that\n\u2212 inf f\u2208F n\u2211 t=1 f(xt)Tct = sup M\u2208M[x1\u2236n] \u2212 n\u2211 t=1 MTt Y (n) t \u2264 Ey\u03021\u2236n\u223cq1\u2236n sup M\u2208M[x1\u2236n] \u2212 n\u2211 s=1 MTs Y\u0303 (n) s = Ey\u03021\u2236n\u223cq1\u2236nRel (I1\u2236n) .\n(18)\nIn the remainder of the proof we will often write M instead of M[x1\u2236n] for brevity. Admissibility: recursion Let D \u225c {\u03b3\u22121ej \u2236 j \u2208 [d]} \u222a {0}, the set of scaled standard basis vectors, together with the origin. Observe that c\u0303t \u2208 conv(D) by our definition of unbiased estimates (in fact, it is only a scaling of one coordinate).\nWe now reason conditionally on xt. As before, let \u01ebs \u2208 {\u00b11}d denote a vector of independent Rademacher random variables. Let us abbreviate by \u03c1 = (\u01ebt+1\u2236n, xt+1\u2236n), a draw of independent Rademacher variables and covariates from Px for the \u201cfuture rounds\u201d, as part of the random playout procedure. Together with the estimates c\u0303s for s < t, we may now construct Y\u0303\n(t) and M matrices and define the randomized prediction algorithm as\nq\u2217t (\u03c1) = argmin q\u2208\u2206d sup c\u0303\u2208D \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9q Tc\u0303 + sup M\u2208M[x1\u2236n] \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303 \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad (19) = argmin\nq\u2208\u2206d sup y\u0302t,q \u2032 t max ct \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9q Tc\u0303t(ct, q\u2032t, y\u0302t) + sup M\u2208M[x1\u2236n] \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t(ct, q\u2032t, y\u0302t)\u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad (20) We remark that xt enters the above definition of q \u2217 t (\u03c1), but we leave this dependence implicit until the end of the proof. For the purposes of the proof also define\nqt(\u03c1) = (1 \u2212 d\u03b3) \u22c5 q\u2217t (\u03c1) + \u03b31, (21)\na version of q\u2217t (\u03c1) that is shifted away from the boundary of the simplex (a step that allows for estimation of ct). Also define qt = E\u03c1[qt(\u03c1)] and q\u2217 = E\u03c1[q\u2217t (\u03c1)]. Observe that Ey\u0302t\u223cqt[ct(y\u0302t)] = qTt ct \u2264 (q\u2217t )Tct + \u03b31Tct \u2264 Ey\u0302t\u223cqt[(q\u2217t )Tc\u0303t(ct, qt, y\u0302t)] + d\u03b3 Hence,\nmax ct\u2208[0,1]d Ey\u0302t\u223cqt{ct(y\u0302t) +Rel (I1, . . . , It) } \u2264 max\nct\u2208[0,1]d Ey\u0302t\u223cqt{(q\u2217t )Tc\u0303t(ct, qt, y\u0302t) +Rel (I1\u2236t\u22121, It(xt, qt, y\u0302t, ct)) } + d\u03b3\n\u2264 sup y\u0302t\u2208[d],q\u2032t max ct\u2208[0,1]d {(q\u2217t )Tc\u0303t(ct, q\u2032t, y\u0302t) +Rel (I1\u2236t\u22121, It(xt, q\u2032t, y\u0302t, ct))} + d\u03b3. (22) In the last expression, the supremum is over q\u2032t of the form (1\u2212 d\u03b3) \u22c5 q + \u03b31, q \u2208\u2206d. This last upper bound holds because qt is one of such distributions. The importance of this upper bound is that it decouples the q\u2217t from q \u2032 t in the first term, a step that yields a simple optimization problem that\ndefines q\u2217t (\u03c1). Writing out the form of the relaxation, the last expression is equal to sup y\u0302t,q\u2032t max ct {(q\u2217t )Tc\u0303t(ct, q\u2032t, y\u0302t) + E\u03c1 sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T\nt c\u0303t(ct, q\u2032t, y\u0302t)} + (n \u2212 t + 1)d\u03b3 \u2264 sup\nc\u0303t\u2208conv(D)\n{(q\u2217t )Tc\u0303t + E\u03c1 sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t} + (n \u2212 t + 1)d\u03b3 since c\u0303t(ct, q\u2032t, y\u0302t) \u2208 conv(D). The expression inside the supremum is a convex function of c\u0303t, and thus the supremum is achieved at a vertex, an element of D. Since q\u2217t = E\u03c1[q\u2217t (\u03c1)], we upper bound the last expression via Jensen\u2019s inequality (omitting (n \u2212 t + 1)d\u03b3 to simplify the exposition) by\nE\u03c1 sup c\u0303t\u2208D {q\u2217t (\u03c1)Tc\u0303t + sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t} (23) Since q\u2217t (\u03c1) is precisely defined to be the minimizer (given \u03c1) of the supremum in (23), the preceding expression is equal to\nE\u03c1 inf q\u2208\u2206d sup c\u0303t\u2208D {qTc\u0303t + sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t} The rest of the upper bounds will be derived conditionally on \u03c1. Observe that\ninf q\u2208\u2206d sup c\u0303t\u2208D {qTc\u0303t + sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t} = sup pt inf q Ec\u0303t\u223cpt {qTc\u0303t + sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t} by the minimax theorem, where pt ranges over the set of distributions on D. By linearity of expectation, the preceding expression is equal to\nsup pt inf q {qTEc\u0303t\u223cpt[c\u0303t] + Ec\u0303t\u223cpt sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t} = sup\npt \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9minj\u2208[d]e T jEc\u0303t\u223cpt[c\u0303t] + Ec\u0303t\u223cpt sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s \u2212M T t c\u0303t \u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad . (24)\nObserve that for any M \u2208M, \u2211dj=1Mj,t = 1 and the elements of Mt are nonnegative. Thus min j e T\njEc\u0303t\u223cpt[c\u0303t] \u2264MTt Ec\u0303t\u223cpt[c\u0303t] Therefore, (24) is equal to\nsup pt \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9Ec\u0303t\u223cpt supM\u2208M\u2212\u2211s\u2260tM T s Y\u0303 (t) s +min j\u2208[d] e T jEc\u0303t\u223cpt[c\u0303t] \u2212MTt c\u0303t\u23ab\u23aa\u23aa\u23ac\u23aa\u23aa\u23ad \u2264 sup\npt {Ec\u0303t\u223cpt sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s +M T t Ec\u0303t\u223cpt[c\u0303t] \u2212MTt c\u0303t} = sup\npt {Ec\u0303t,c\u0303\u2032t\u223cpt sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s +M T t (c\u0303\u2032t \u2212 c\u0303t)} Since exchanging c\u0303t and c\u0303 \u2032 t switches the sign in the last term, we may introduce an independent Rademacher random variable \u03b4t via the standard technique of symmetrization. The last expression is then equal to\nsup pt {Ec\u0303t,c\u0303\u2032t\u223cptE\u03b4t sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s + \u03b4tM T t (c\u0303\u2032t \u2212 c\u0303t)} \u2264 sup\npt {Ec\u0303t\u223cptE\u03b4t sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s + 2\u03b4tM T t c\u0303t} The above inequality follows by splitting the supremum into two parts equal parts. Let us now reason conditionally on c\u0303t. There are two cases: either c\u0303t = 0 or c\u0303t = \u03b3 \u22121 ej for some coordinate\nj \u2208 [d]. Let us consider the second case, and the first follows from the same reasoning. Take Z to be a random vector with independent coordinates and values in {\u2212\u03b3\u22121, \u03b3\u22121}d. For the jth coordinate, Zj is identically \u03b3\n\u22121, while for all other coordinates i \u2260 j the distribution Zi is symmetric. Clearly, EZ = c\u0303t. By Jensen\u2019s inequality,\nE\u03b4t sup M\u2208M {\u2212\u2211 s\u2260t MTs Y\u0303 (t) s + 2\u03b4tM T t c\u0303t} \u2264 E\u03b4tEZ sup M\u2208M {\u2212\u2211 s\u2260t MTs Y\u0303 (t) s + 2\u03b4tM T t Z} It is not hard to see that the distribution of \u03b4tZ is uniform on {\u2212\u03b3\u22121, \u03b3\u22121}d, and we can write it as \u03b3\u22121\u01ebt, a scaled vector of independent Rademacher random variables. The overall bound (together with the omitted term (n \u2212 t + 1)d\u03b3) is then max\nct\u2208[0,1]d Ey\u0302t\u223cqt{ct(y\u0302t) +Rel (I1, . . . , It) } \u2264 E\u03c1 sup pt {Ec\u0303t\u223cptE\u01ebt sup M\u2208M \u2212\u2211 s\u2260t MTs Y\u0303 (t) s + 2\u03b3 \u22121MTt \u01ebt} + (n \u2212 t + 1)d\u03b3 = E\u03c1E\u01ebt sup\nM\u2208M {\u2212\u2211\ns\u2260t\nMTs Y\u0303 (t) s + 2\u03b3 \u22121MTt \u01ebt} + (n \u2212 t + 1)d\u03b3 since the expression no longer depends on pt and c\u0303t. The above inequality holds for any xt. Hence, we may take expectation on both sides, yielding\nExt max ct\u2208[0,1]d Ey\u0302t\u223cqt{ct(y\u0302t) +Rel (I1, . . . , It) } \u2264 E\u01ebt\u2236n,xt\u2236n sup M\u2208M[x1\u2236n] {\u2212\u2211 s\u2260t MTs Y\u0303 (t) s + 2\u03b3 \u22121MTt \u01ebt} + (n \u2212 t + 1)d\u03b3 =Rel (I1\u2236t\u22121) because \u03c1 = (\u01ebt+1\u2236n, xt+1\u2236n). This proves admissibility.\nOmitting 0 from objective Examining the algorithm in (19), we note that the optimization problem may be taken over c\u0303 \u2208 {e1, . . . ,ed}; that is, the argmin over q does not change upon the removal of 0. To see this, suppose that q\u2217t (\u03c1) is the optimal response when c\u0303 \u2208 {e1, . . . ,ed}. Then it is also an optimal response to c\u0303 \u2208 {e1, . . . ,ed} \u222a {0} since for c\u0303 = 0 the value of q does not make any difference in terms of the value. This proves our claim, and is reflected in the definition of Algorithm 1.\nRegret bound The final bound is given by\nRel (\u2205) = ExE\u01eb sup M\u2208M[x1\u2236n] \u2212 n\u2211 t=1 MTt Y\u0303 (0) t + nd\u03b3 = 2 \u03b3 ER(F ;x1\u2236n) + nd\u03b3 = 2\u221a2dnER(F ;x1\u2236n)"}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R.E. Schapire"], "venue": "arXiv preprint arXiv:1402.0555,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["A. Beygelzimer", "J. Langford", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "A linear programming formulation and approximation algorithms for the metric labeling problem", "author": ["C. Chekuri", "S. Khanna", "J. Naor", "L. Zosin"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Chekuri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chekuri et al\\.", "year": 2004}, {"title": "Efficient optimal learning for contextual bandits", "author": ["M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang"], "venue": "arXiv preprint arXiv:1106.2369,", "citeRegEx": "Dudik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2011}, {"title": "Adaptive online learning", "author": ["D. Foster", "A. Rakhlin", "K. Sridharan"], "venue": "In NIPS,", "citeRegEx": "Foster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2015}, {"title": "Newtron: an efficient bandit algorithm for online multiclass prediction", "author": ["E. Hazan", "S. Kale"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields", "author": ["J. Kleinberg", "E. Tardos"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Kleinberg and Tardos.,? \\Q2002\\E", "shortCiteRegEx": "Kleinberg and Tardos.", "year": 2002}, {"title": "Global optimization with polynomials and the problem of moments", "author": ["J. B Lasserre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lasserre.,? \\Q2001\\E", "shortCiteRegEx": "Lasserre.", "year": 2001}, {"title": "Hybrid stochastic-adversarial on-line learning", "author": ["A. Lazaric", "R. Munos"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Lazaric and Munos.,? \\Q2009\\E", "shortCiteRegEx": "Lazaric and Munos.", "year": 2009}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Tighter bounds for multi-armed bandits with expert advice", "author": ["H. B McMahan", "M. J Streeter"], "venue": "In COLT,", "citeRegEx": "McMahan and Streeter.,? \\Q2009\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2009}, {"title": "Semidefinite programming relaxations for semialgebraic problems", "author": ["P.A. Parrilo"], "venue": "Mathematical programming,", "citeRegEx": "Parrilo.,? \\Q2003\\E", "shortCiteRegEx": "Parrilo.", "year": 2003}, {"title": "Hierarchies of relaxations for online prediction problems with evolving constraints", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2015\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2015}, {"title": "Relax and randomize: From value to algorithms", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "We present efficient algorithms for the problem of contextual bandits with i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of policies. Our algorithm BISTRO requires d calls to the empirical risk minimization (ERM) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the ERM problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the ERM. The integrality gap of the relaxation only enters in the regret bound rather than the benchmark. Finally, we show that the adversarial version of the contextual bandit problem is learnable (and efficient) whenever the full-information supervised online learning problem has a non-trivial regret guarantee (and efficient).", "creator": "LaTeX with hyperref package"}}}