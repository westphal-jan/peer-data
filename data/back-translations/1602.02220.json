{"id": "1602.02220", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Improved Dropout for Shallow and Deep Learning", "abstract": "Dropout has been observed with great success in forming deep neural networks by randomly setting the results of neurons to zero. There has also been a steep increase in interest in shallow learning, e.g. logistical regression. However, independent scanning for dropouts may be suboptimal for convergence's sake. To show the optimal probabilities of failure, we suggest using multinomial scanning for dropouts, i.e., scanning of traits or neurons according to a multinomical distribution with different probabilities for different traits / neurons. To show the optimal probabilities of failure, we analyze shallow learning with multinomical failure phenomena and set the risk limits for stochastic optimization. By minimizing a sampling-dependent factor in the risk range, we obtain a distributional probability of failure with sampling probabilities that depend on the second order of the data distribution, and propose the problem of emerging risk factor based only on learning in depth distributions.", "histories": [["v1", "Sat, 6 Feb 2016 05:41:57 GMT  (145kb,D)", "http://arxiv.org/abs/1602.02220v1", null], ["v2", "Sun, 4 Dec 2016 05:31:19 GMT  (454kb)", "http://arxiv.org/abs/1602.02220v2", "In NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zhe li", "boqing gong", "tianbao yang"], "accepted": true, "id": "1602.02220"}, "pdf": {"name": "1602.02220.pdf", "metadata": {"source": "META", "title": "Improved Dropout for Shallow and Deep Learning ", "authors": ["Zhe Li", "Boqing Gong", "Tianbao Yang"], "emails": ["ZHE-LI-1@UIOWA.EDU", "BGONG@CRCV.UCF.EDU", "TIANBAO-YANG@UIOWA.EDU"], "sections": [{"heading": "1. Introduction", "text": "Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters (Krizhevsky et al., 2012; Srivastava et al., 2014), which usually identically and independently at random samples neurons and sets their outputs to be zeros. Extensive experiments (Hinton et al., 2012) have shown that dropout can help obtain the state-of-the-art performance on a range of benchmark data sets. Recently, dropout has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition.\nIn this paper, instead of identically and independently at random zeroing out features or neurons, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. Intuitively, it makes more sense to use non-uniform multinomial sampling than identical and independent sampling for different features/neurons. For example, in shallow learning if there are some features that have zero variance, we can dropout these features more frequently or completely allowing the training to focus on more important features and consequentially enabling faster convergence. To justify the multinomial sampling for dropout and reveal the optimal sampling probabilities, we conduct a rigorous analysis on the risk bound of shallow learning by stochastic optimization with multinomial dropout, and demonstrate that a distribution-dependent dropout leads to a smaller expected risk (i.e., faster convergence and smaller generalization error).\nInspired by the distribution-dependent dropout, we propose a data-dependent dropout for shallow learning, and an evolutional dropout for deep learning. For shallow learning, the sampling probabilities are computed from the second\nar X\niv :1\n60 2.\n02 22\n0v 1\n[ cs\n.L G\n] 6\nF eb\norder statistics of features of the training data. For deep learning, the sampling probabilities of dropout for a layer are computed on-the-fly from the second-order statistics of the layer\u2019s outputs based on a mini-batch of examples. This is particularly suited for deep learning because (i) the distribution of each layer\u2019s outputs is evolving over time, which is known as internal covariate shift (Ioffe & Szegedy, 2015); (ii) passing through all the training data in deep neural networks (in particular deep convolutional neural networks) is much more expensive than through a mini-batch of examples. For a mini-batch of examples, we can leverage parallel computing architectures to accelerate the computation of sampling probabilities.\nWe note that the proposed evolutional dropout achieves similar effect to the batch normalization technique (Znormalization based on a mini-batch of examples) (Ioffe & Szegedy, 2015) but with different flavors. Both approaches can be considered to tackle the issue of internal covariate shift for accelerating the convergence. Batch normalization tackles the issue by normalizing the output of neurons to zero mean and unit variance and then performing dropout independently 1. In contrast, our proposed evolutional dropout tackles this issue from another perspective by exploiting a distribution-dependent dropout, which adapts the sampling probabilities to the evolving distribution of a layer\u2019s outputs. In other words, it uses normalized sampling probabilities based on the second order statistics of internal distributions. Indeed, we notice that for shallow learning with Z-normalization (normalizing each feature to zero mean and unit variance) the proposed datadependent dropout reduces to uniform dropout that acts similar to the standard dropout. Because of this connection, the presented theoretical analysis also sheds some lights on the power of batch normalization from the angle of theory. Compared to batch normalization, the proposed distribution-dependent dropout is still attractive because (i) it is rooted in theoretical analysis of the risk bound; (ii) it introduces no additional parameters and layers without complicating the back-propagation and the inference; (iii) it facilitates further research because its shares the same mathematical foundation as standard dropout (e.g., equivalent to a form of data-dependent regularizer) (Wager et al., 2013).\nWe summarize the main contributions of the paper below.\n\u2022 We propose a multinomial dropout and demonstrate that a distribution-dependent dropout leads to a faster convergence and a smaller generalization error through the risk bound analysis for shallow learning.\n\u2022 We propose an efficient evolutional dropout for deep 1The author also reported that in some cases dropout is even\nnot necessary\nlearning based on the distribution-dependent dropout.\n\u2022 We justify the proposed dropouts for both shallow learning and deep learning by experimental results on several benchmark datasets.\nIn the reminder, we first review some related work and preliminaries. We present the main results in Section 4 and experimental results in Section 5."}, {"heading": "2. Related Work", "text": "In this section, we review some related work on dropout, optimization algorithms for deep learning and distributiondependent or sampling-dependent error bounds.\nDropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer. The most simple form of dropout is to multiply hidden units by i.i.d Bernoulli noise. Several recent works also found that using other types of noise works as well as Bernoulli noise (e.g., Gaussian noise), which could lead to a better approximation of the marginalized loss (Wang & Manning, 2013; Kingma et al., 2015). Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015). Graham et al. (2015) used the same noise across a batch of examples in order to speed-up the computation. Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014). It has been applied to improve the performance of logistic regression, support vector machine and other single-layer models for natural language tasks such as document classification, named entity recognition, etc. The present work proposes a new dropout with noise sampled according to distribution-dependent sampling probabilities. To the best of our knowledge, this is the first work that rigorously studies this type of dropout with theoretical analysis of the risk bound. It is shown that the new dropout improves the performance in convergence speed dramatically.\nStochastic gradient descent with back-propagation has been used a lot in optimizing deep neural networks. However, it is notorious for its slow convergence especially for deep learning. Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from\ndifferent perspectives. Among them, we notice that developed evolutional dropout for deep learning achieves similar effect as batch normalization (Ioffe & Szegedy, 2015) addressing the internal covariate shift issue (i.e., evolving distributions of internal hidden units). However, there also exist significant differences from the batch normalization that have been highlighted in the previous section.\nThe distribution-dependent sampling and distributiondependent risk bounds presented in this work share similar spirit to some recent works in shallow learning and other areas. Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities. Kukliansky & Shamir (2015) proposed a similar distributiondependent sampling for attribute-efficient learning in linear regression. However, our work has a very different focus. They focused on minimizing the standard expected risk and using sampled features to compute an unbiased stochastic gradient of the risk. Here, we focus on minimizing the dropout risk (the risk defined over corrupted features) and tackle both shallow learning and deep learning. Distribution-dependent risk bounds have also been examined in several studies (Sabato et al., 2013)."}, {"heading": "3. Preliminaries", "text": "In this section, we present some preliminaries, including the framework of risk minimization in machine learning and learning with dropout noise. We also introduce the multinomial dropout, which allows us to construct a distribution-dependent dropout as revealed in next section.\nLet (x, y) denote a feature vector and a label, where x \u2208 Rd and y \u2208 Y . Denote by P the joint distribution of (x, y) and denote by D the marginal distribution of x. The goal of risk minimization is to learn a prediction function f(x) that minimizes the expected loss, i.e.,\nmin f\u2208H EP [`(f(x), y)]\nwhere `(z, y) is a loss function (e.g., the logistic loss) that measures the inconsistency between z and y. In deep learning, the prediction function f(x) is determined by a deep neural network. In shallow learning, one might be interested in learning a linear model f(x) = w>x. In the following presentation, the analysis will focus on the risk minimization of a linear model, i.e.,\nmin w\u2208Rd\nL(w) , EP [`(w>x, y)] (1)\nIn this paper, we are interested in learning with dropout, i.e., the feature vector x is corrupted by a dropout noise. In particular, let \u223c M denote a dropout noise vector of\ndimension d, and the corrupted feature vector is given by x\u0302 = x\u25e6 , where he operator \u25e6 represents the element-wise multiplication. Let P\u0302 denote the joint distribution of the new data (x\u0302, y) and D\u0302 denote the marginal distribution of x\u0302. With the corrupted data, the risk minimization becomes\nmin w\u2208Rd\nL\u0302(w) , EP\u0302 [`(w >(x \u25e6 ), y)] (2)\nIn standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i.e., features are dropped with a probability \u03b4 and scaled by 11\u2212\u03b4 with a probability 1 \u2212 \u03b4. We can also write j = bj 1\u2212\u03b4 , where bj \u2208 {0, 1}, j \u2208 [d] are i.i.d Bernoulli random variables with Pr(bj = 1) = 1\u2212 \u03b4. The scaling factor 11\u2212\u03b4 is added to ensure that E [x\u0302] = x. It is obvious that using the standard dropout different features will have equal probabilities to be dropped out or to be selected independently. However, in practice some features could be more informative than the others for learning purpose. Therefore, it makes more sense to assign different sampling probabilities for different features and make the features compete with each other.\nTo this end, we introduce the following multinomial dropout.\nDefinition 1. (Multinomial Dropout) A multinomial dropout is defined as x\u0302 = x \u25e6 , where i = mikpi , i \u2208 [d] and {m1, . . . ,md} follow a multinomial distribution Mult(p1, . . . , pd; k) with \u2211d i=1 pi = 1 and pi \u2265 0.\nRemark: The multinomial dropout allows us to use nonuniform sampling probabilities p1, . . . , pd for different features. The value of mi is the number of times that the i-th feature is selected in k independent trials of selection. In each trial, the probability that the i-th feature is selected is given by pi. As in the standard dropout, the normalization by kpi is to ensure that E [x\u0302] = x. The parameter k plays the same role as the parameter 1 \u2212 \u03b4 in standard dropout, which controls the number of features to be dropped. In particular, the expected total number of the kept features using multinomial dropout is k and that using standard dropout is d(1 \u2212 \u03b4). In the sequel, to make fair comparison between the two dropouts, we let k = d(1 \u2212 \u03b4). In this case, when a uniform distribution pi = 1/d is used in multinomial dropout to which we refer as uniform dropout, then i = mi1\u2212\u03b4 , which acts similarly to the standard dropout using i.i.d Bernoulli random variables. Note that another choice to make the sampling probabilities different is still using i.i.d Bernoulli random variables but with different probabilities for different features. However, multinomial dropout is more suitable because (i) it is easy to control the level of dropout by varying the value of k; (ii) it gives rise to natural competition among features because of the\nconstraint \u2211 i pi = 1; (iii) it allows us to minimize the sampling dependent risk bound for obtaining a better distribution than uniform sampling.\nDropout is a data-dependent regularizer Dropout as a regularizer has been studied in (Wager et al., 2013; Baldi & Sadowski, 2013) for logistic regression, which is stated in the following proposition for ease of discussion later.\nProposition 1. If `(z, y) = log(1 + exp(\u2212yz)), then\nEP\u0302 [`(w >x\u0302, y)] = EP [`(w >x, y)] +RD,M(w) (3)\nwhereM denotes the distribution of and\nRD,M(w) = ED,M\n[ log exp(w> x\u25e6 2 ) + exp(\u2212w > x\u25e6 2 )\nexp(w>x/2) + exp(\u2212w>x/2)\n]\nRemark: It is notable that RD,M \u2265 0 due to the Jensen inequality. Using the second order Taylor expansion, (Wager et al., 2013) showed that the following approximation of RD,M(w) is easy to manipulate and understand:\nR\u0302D,M(w) = ED[q(w >x)(1\u2212 q(w>x))w>CM(x \u25e6 )w] 2\n(4)\nwhere q(w>x) = 1 1+exp(\u2212w>x/2) , and CM denotes the covariance matrix in terms of . In particular, if is the standard dropout noise, then\nCM[x \u25e6 ] = diag(x21\u03b4/(1\u2212 \u03b4), . . . , x2d\u03b4/(1\u2212 \u03b4))\nwhere diag(s1, . . . , sn) denotes a d \u00d7 d diagonal matrix with the i-th entry equal to si. If is the multinomial dropout noise in Definition 1, we have\nCM[x \u25e6 ] = 1\nk diag(x2i /pi)\u2212\n1 k xx> (5)"}, {"heading": "4. Learning with Multinomial Dropout", "text": "In this section, we analyze a stochastic optimization approach for minimizing the dropout loss in (2). Assume the sampling probabilities are known. We first obtain a risk bound of learning with multinomial dropout for stochastic optimization. Then we try to minimize the factors in the risk bound that depend on the sampling probabilities. We would like to emphasize that our goal here is not to show that using dropout would render a smaller risk than without using dropout, but rather focus on the impact of different sampling probabilities on the risk. Let the initial solution be w1. At the iteration t, we sample (xt, yt) \u223c P and t \u223cM as in Definition 1 and then update the model by\nwt+1 = wt \u2212 \u03b7t\u2207`(w>t (xt \u25e6 t), yt) (6)\nwhere \u2207` denotes the (sub)gradient in terms of wt and \u03b7t is a step size. Suppose we run the stochastic optimization by n steps (i.e., using n examples) and compute the final solution as\nw\u0302n = 1\nn n\u2211 t=1 wt.\nWe note that another approach of learning with dropout is to minimize the empirical risk by marginalizing out the dropout noise, i.e., replacing the true expectations EP and ED in (3) with empirical expectations over a set of samples (x1, y1), . . . , (xn, yn) denoted by EPn and EDn . Since the data dependent regularizer RDn,M(w) is difficult to compute, one usually uses an approximation R\u0302Dn,M(w) (e.g., as in (4)) in place of RDn,M(w). However, the resulting problem is a non-convex optimization, which together with the approximation error would make the risk analysis much more involved. In contrast, the update in (6) can be considered as a stochastic gradient descent update for solving the convex optimization problem in (2), allowing us to establish the risk bound based on previous results of stochastic gradient descent for risk minimization (ShalevShwartz et al., 2009; Srebro et al., 2010). Nonetheless, this restriction does not loss the generality. Indeed, stochastic optimization is usually employed for solving empirical loss minimization in big data and deep learning.\nThe following theorem establishes a risk bound of w\u0302n in expectation.\nTheorem 1. Let L(w) be the expected risk of w defined in (1). Assume ED\u0302[\u2016x \u25e6 \u2016 2 2] \u2264 B2 and `(z, y) is GLipschitz continuous. For any \u2016w\u2217\u20162 \u2264 r, by appropriately choosing \u03b7, we can have\nE[L(w\u0302n) +RD,M(w\u0302n)]\n\u2264 L(w\u2217) +RD,M(w\u2217) + GBr\u221a n\nwhere E[\u00b7] is taking expectation over the randomness in (xt, yt, t), t = 1, . . . , n.\nRemark: In the above theorem, we can choose w\u2217 to be the best model that minimizes the expected risk in (1). Since RD,M (w) \u2265 0, the upper bound in the theorem above is also the upper bond of the risk of w\u0302n, i.e., L(w\u0302n), in expectation. The proof of the above theorem follows the standard analysis of stochastic gradient descent. The detailed proof of theorem is included in the appendix."}, {"heading": "4.1. Distribution Dependent Dropout", "text": "Next, we consider the sampling dependent factors in the risk bounds. From Theorem 1, we can see that there are two terms that depend on the sampling probabilities, i.e.,\nB2 - the upper bound of ED\u0302[\u2016x \u25e6 \u2016 2 2], and RD,M(w\u2217) \u2212 RD,M(w\u0302n) \u2264 RD,M(w\u2217). We note that the second term also depend on w\u2217 and w\u0302n, which is more difficult to optimize. We first try to minimize ED\u0302[\u2016x \u25e6 \u2016 2 2] and present the discussion on minimizing RD,M(w\u2217) later. From Theorem 1, we can see that minimizing ED\u0302[\u2016x \u25e6 \u2016 2 2] would lead to not only a smaller risk (given the same number of total examples, smaller ED\u0302[\u2016x \u25e6 \u2016 2 2] gives a smaller risk bound) but also a faster convergence (with the same number of iterations, smaller ED\u0302[\u2016x \u25e6 \u2016 2 2] gives a smaller optimization error).\nThe following proposition simplifies the expectation ED\u0302[\u2016x \u25e6 \u2016 2 2]. Proposition 2. Let follow the distributionM defined in Definition 1. Then\nED\u0302[\u2016x \u25e6 \u2016 2 2] =\n1\nk d\u2211 i=1 1 pi ED[x 2 i ] + k \u2212 1 k d\u2211 i=1 ED[x 2 i ]\n(7)\nProof. We have\nED\u0302\u2016x \u25e6 \u2016 2 2 = ED [ d\u2211 i=1 x2i k2p2i E[m2i ] ] Since {m1, . . . ,md} follows a multinomial distribution Mult(p1, . . . , pd; k), we have\nE[m2i ] = var(mi) + (E[mi]) 2 = kpi(1\u2212 pi) + k2p2i\nThe result in the Proposition follows by combining the above two equations.\nGiven the expression of ED\u0302[\u2016x \u25e6 \u2016 2 2] in Proposition 2, we can minimize it over p, leading to the following result.\nProposition 3. The solution to\np\u2217 = arg min p\u22650,p>1=1\nED\u0302[\u2016x \u25e6 \u2016 2 2]\nis given by\np\u2217i = \u221a ED[x2i ]\u2211d\nj=1 \u221a ED[x2j ] , i = 1, . . . , d (8)\nProof. Note that only the first term in the R.H.S of (7) depends on pi. Thus, p\u2217 = arg minp\u22650,p>1=1 \u2211d i=1 ED[x 2 i ] pi . The result then follows the KKT conditions.\nNext, we examine RD,M(w\u2217). Since direct manipulation on RD,M(w\u2217) is difficult, we try to minimize the second order Taylor expansion R\u0302D,M(w\u2217) for logistic loss. The following theorem establishes an upper bound of R\u0302D,M(w\u2217).\nProposition 4. Let follow the distributionM defined in Definition 1. We have\nR\u0302D,M(w\u2217) \u2264 1\n8k \u2016w\u2217\u201622 ( d\u2211 i=1 ED[x 2 i ] pi \u2212 ED[\u2016x\u201622] )\nRemark: The proof is included in the appendix. By minimizing the relaxed upper bound in Proposition 4, we obtain the same sampling probabilities as in (8). We note that a tighter upper bound can be established, however, which will yield sampling probabilities dependent on the unknown w\u2217.\nIn summary, using the probabilities in (8), we can reduce both ED\u0302[\u2016x \u25e6 \u2016 2 2] and RD,M(w\u2217) in the risk bound, leading to a faster convergence and a smaller generalization error. In practice, we can use empirical second-order statistics to compute the probabilities, i.e.,\npi =\n\u221a 1 n \u2211n j=1[[xj ]\n2 i ]\u2211d\ni\u2032=1 \u221a 1 n \u2211n j=1[[xj ] 2 i\u2032 ]\n(9)\nwhere [xj ]i denotes the i-th feature of the j-th example, which gives us a data-dependent dropout. We state it formally in the following definition.\nDefinition 2. (Data-dependent Dropout) Given a set of training examples (x1, y1), . . . , (xn, yn). A datadependent dropout is defined as x\u0302 = x \u25e6 , where i = mi kpi , i \u2208 [d] and {m1, . . . ,md} follow a multinomial distribution Mult(p1, . . . , pd; k) with pi given by (9).\nRemark: Note that if the data is normalized such that each feature has zero mean and unit variance (i.e., according to Z-normliazation), the data-dependent dropout reduces to uniform dropout. It implies that the data-dependent dropout achieves similar effect as Z-normalization plus uniform dropout. In this sense, our theoretical analysis also explains why Z-normalization usually speeds up the training (Ranzato et al., 2010)."}, {"heading": "4.2. Evolutional Dropout for Deep Learning", "text": "Next, we discuss how to implement the distributiondependent dropout for deep learning. In training deep neural networks, the dropout is usually added to the intermediate layers (e.g., fully connected layers and convolutional layers). Let xl = (xl1, . . . , x l d) denote the outputs of the l-th layer (with the index of data omitted). Adding dropout to this layer is equivalent to multiplying xl by a dropout noise vector l, i.e., feeding x\u0302l = xl \u25e6 l as the input to the next layer. Inspired by the data-dependent dropout, we can generate l according to a distribution given in Definition 1 with sampling probabilities pli computed from {xl1, . . . ,xln} similar to that (9). However, deep\nlearning is usually trained with big data and a deep neural network is optimized by mini-batch stochastic gradient descent. Therefore, at each iteration it would be too expensive to afford the computation to pass through all examples. To address this issue, we propose to use a mini-batch of examples to calculate the second-order statistics similar to what was done in batch normalization. Let X l = (xl1, . . . ,x l m) denote the outputs of the l-th layer for a mini-batch of m examples. Then we can calculate the probabilities for dropout by\npli =\n\u221a 1 m \u2211m j=1[[x\nl j ] 2 i ]\u2211d\ni\u2032=1 \u221a 1 m \u2211m j=1[[x l j ] 2 i\u2032 ] , i = 1, . . . , d (10)\nwhich define the evolutional dropout named as such because the probabilities pli will also evolve as the the distribution of the layer\u2019s outputs evolve. We describe the evolutional dropout as applied to a layer of a deep neural network in Figure 1.\nFinally, we would like to compare the evolutional dropout with batch normalization. Similar to batch normalization, evolutional dropout can also address the internal covariate shift issue by adapting the sampling probabilities to the evolving distribution of layers\u2019 outputs. However, different from batch normalization, evolutional dropout is a randomized technique, which enjoys many benefits as standard dropout including (i) the back-propagation is simple to implement (just multiplying the gradient of X\u0302 l by the dropout mask to get the gradient of X l); (ii) the inference (i.e., testing) remains the same 2; (iii) it is equivalent to a datadependent regularizer with a clear mathematical explanation; (iv) it prevents units from co-adapting and allows combining exponentially many different neural network ar-\n2Different from some implementations for standard dropout which doest no scale by 1/(1 \u2212 \u03b4) in training but scale by 1 \u2212 \u03b4 in testing, here we do scale in training and thus do not need any scaling in testing.\nchitectures efficiently (Srivastava et al., 2014), which facilitate generalization. Moreover, the evolutional dropout has its root in distribution-dependent dropout, which has theoretical guarantee to accelerate the convergence and improve the generalization for shallow learning."}, {"heading": "5. Experimental Results", "text": "In the section, we present some experimental results to justify the proposed dropouts. In all experiments, we set \u03b4 = 0.5 in the standard dropout and k = 0.5d in the proposed dropouts for fair comparison, where d represents the number of features or neurons of the layer that dropout is applied to. For the sake of clarity, we divided the experiments into three parts. In the first part, we compare the performance of the data-dependent dropout (d-dropout) to the standard dropout (s-dropout) for logistic regression. In the second part, we compare the performance of evolutional dropout (e-dropout) to the standard dropout for training deep convolutional neural networks. Finally, we compare e-dropout with batch normalization."}, {"heading": "5.1. Shallow Learning", "text": "We implement the presented stochastic optimization algorithm. To evaluate the performance of data-dependent dropout for shallow learning, we used the three data sets: real-sim, news20 and RCV1 3. Table 1 summarizes the statistic of the three data sets. In this experiment, we use a fixed step size and tune the step size in [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001] and report the best results in terms of convergence speed on the training data for both standard dropout and data-dependent dropout. Figure 2 shows the obtained results on these three data sets. In each figure, we plot both the training error and the testing error. We can see that both the training and testing errors using the proposed data-dependent dropout decrease much faster than using the standard dropout and also a smaller testing error is achieved by using the datadependent dropout."}, {"heading": "5.2. Evolutional Dropout for Deep Learning", "text": "We would like to emphasize that we are not aiming to obtain better prediction performance by trying different net-\n3https://www.csie.ntu.edu.tw/\u02dccjlin/ libsvmtools/datasets/\nwork structures and different engineering tricks such as data augmentation, whitening, etc., but rather focus on the comparison of the proposed dropout to the standard dropout using Bernoulli noise on the same network structure.\nIn the next three subsections, we present the results on three benchmark data sets for comparing e-dropout and s-dropout: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). We use the same or similar network structure as in the literature for the three data sets, which will be described separately for different data sets. In general, the networks consist of convolution layers, pooling layers, fully connected layers, softmax layers and a cost layer. For the detailed neural network structures and their parameters, please refer to the appendix. The dropout is added to the fully connected layers. The rectified linear activation function is used for all neurons. All the experiments are conducted using the cuda-convnet library 4. The training procedure is similar to (Krizhevsky et al., 2012), that is using mini-batch SGD with momentum (0.9). The size of mini-batch is fixed to 128. The weights are initialized based on the Gaussian distribution with mean zero and standard deviation 0.01. The learning rate (i.e., step size) is decreased after a number of epochs similar to what was done in previous works (Krizhevsky et al., 2012). In terms of the initial learning rate, we find that the evolutional dropout allows us to use a larger initial learning rate similar to batch normalization, which will be revealed in the next three subsections."}, {"heading": "5.2.1. MNIST", "text": "The MNIST (LeCun et al., 1998) data set has 60,000 training images and 10,000 testing images. Each image with size 28\u00d7 28 in this dataset represents a hand written digits\n4https://code.google.com/archive/p/ cuda-convnet/\n0 \u223c 9. The task is to classify each image in the test data set into 10 categories. We used the similar neural network structure to (Wan et al., 2013): two convolution layers, two fully connected layers, a softmax layer and a cost layer at the end. The dropout is added to the first fully connected layer. The initial learning rate for standard dropout is set to 0.01 the same to (Wan et al., 2013), and that for evolutional dropout is set to 0.1. Figure 3 (a) shows the training error and the testing error on MNIST data set using the standard dropout and the evolutional dropout. We can see that by using the evolutional dropout both the training error and the testing error decrease significantly faster than those using the standard dropout."}, {"heading": "5.2.2. CIFAR-10", "text": "The CIFAR-10 (Krizhevsky & Hinton, 2009) data set has 50,000 training images and 10,000 test images, which belong to one of 10 classes. Each class has in total 6,000 images. Every image in this data set has the RGB format and has a size 32 \u00d7 32. The neural network structure is adopted from (Krizhevsky & Hinton, 2009), which consists of 2 convolution layers, 2 max pooling layers, 2 local response normalization layers, 2 locally connected layers, 2 fully connected layers, and a softmax and a cost layer. The dropout is added to the first fully connected layer. The initial learning rate for the standard dropout is set to 0.001 the same to what is used in cuda-convnet 4, and that for the evolutional dropout is set to 0.01. Figure 3 (b) shows the training and testing errors on CIFAR-10 data set using the standard dropout and the evolutional dropout. Again, we can see that the evolutional dropout is much faster than the standard dropout."}, {"heading": "5.2.3. CIFAR-100", "text": "CIFAR-100 (Krizhevsky & Hinton, 2009) data set is similar to CIFAR-10, except that there are 100 classes. The network structure for this data set is similar to that for CIFAR10, expect that the size of input and output in the last fully\nconnected layer and remaining layers are different. The initial learning rate for the standard dropout is set to 0.001 similar to CIFAR-10, and that for the evolutional dropout is set to 0.01. Figure 3 (c) shows the training and testing errors on CIFAR-100 data set. The results show that evolutional dropout not only accelerates the convergence substantially but also decreases the testing error by a large margin.\nWe also report the relative improvement of the evolutional dropout on the testing error and the convergence speed compared to the standard dropout. We use the number of iterations for convergence to measure the convergence speed. First, we obtain the minimum testing error achieved by the two dropouts. The number of iterations for convergence is taken as the smallest iteration number that achieves a testing error within 10\u22124 precision of the minimum testing error. Then, we compute the relative improvement of the evolutional dropout on both metrics by dividing the absolute improvement by the measures of the standard dropout. The results are reported in Table 2. We can see that both the testing performance and the convergence speed are improved substantially."}, {"heading": "5.3. Comparison with the Batch Normalization (BN)", "text": "Finally, we make a comparison between the evolutional dropout and the batch normalization. For batch normal-\nization, we use the implementation in Caffe 5. We compare the evolutional dropout with the batch normalization on CIFAR-10 data set. The network structure is from the Caffe package and can be found in the appendix, which is different from the one used in the previous experiment. It contains three convolutional layers and one fully connected layer. Each convolutional layer is followed by a pooling layer. We compare three methods: (i) No BN and No Dropout - without using batch normalization and dropout; (ii) BN; (iii) Evolutional Dropout. For BN, three batch normalization layers are inserted before or after each pooling layer following the architecture given in Caffe package. For the evolutional dropout training, only one layer of dropout is added to the the last convolutional layer. The sigmoid activation function is used in BN because it gives better performance than the rectified linear activation function (Ioffe & Szegedy, 2015). For the other two methods, we use the rectified linear activation function. The minibatch size is set to 100, the default value in Caffe. The initial learning rates for the three methods are set to the same value (0.001), and they are decreased once by ten times. The testing accuracy versus the number of iterations is plotted in Figure 4, from which we can see that the evolutional\n5https://github.com/BVLC/caffe/\ndropout training achieves significant convergence speed-up and even better prediction performance than BN."}, {"heading": "6. Conclusion", "text": "In this paper, we have proposed a distribution-dependent dropout for both shallow learning and deep learning. Theoretically, we proved that the new dropout achieves a smaller risk and faster convergence. Based on the distributiondependent dropout, we developed an efficient evolutional dropout for training deep neural networks that adapts the sampling probabilities to the evolving distributions of layers\u2019 outputs. Experimental results on various data sets verified that the proposed dropouts can dramatically improve the convergence and also reduce the testing error."}, {"heading": "A. Proof of Theorem 1", "text": "The update given by\nwt+1 = wt \u2212 \u03b7\u2207`(w>t (xt \u25e6 t), yt)\ncan be considered as the stochastic gradient descent (SGD) update of the following problem\nmin w {L\u0302(w) , EP\u0302 [`(w\n>(x \u25e6 ), y)]}\nDefine gt as\ngt = \u2207`(w>t (xt \u25e6 t), yt) = `\u2032(w>t (xt \u25e6 t), yt)xt \u25e6 t\nwhere `\u2032(z, y) denotes the derivative in terms of z. Since the loss function is G-Lipschitz continuous, therefore \u2016gt\u20162 \u2264 G\u2016xt \u25e6 t\u20162. According to the analysis of SGD (Zinkevich, 2003), we have the following lemma.\nLemma 1. Let wt+1 = wt \u2212 \u03b7gt and w1 = 0. Then for any \u2016w\u2217\u20162 \u2264 r we have\nn\u2211 t=1 g>t (wt \u2212w\u2217) \u2264 r2 2\u03b7 + \u03b7 2 n\u2211 t=1 \u2016gt\u201622 (11)\nBy taking expectation on both sides over the randomness in (xt, yt, t) and noting the bound on \u2016gt\u20162, we have\nE[n] [ n\u2211 t=1 g>t (wt \u2212w\u2217) ] \u2264 r 2 2\u03b7 + \u03b7 2 n\u2211 t=1 G2E[n][\u2016xt \u25e6 t\u201622]\nwhere E[t] denote the expectation over (xi, yi, i), i = 1, . . . , t. Let Et[\u00b7] denote the expectation over (xt, yt, t) with (xi, yi, i), i = 1, . . . , t\u2212 1 given. Then we have\nn\u2211 t=1 E[t][g > t (wt \u2212w\u2217)] \u2264 r2 2\u03b7 + \u03b7 2 n\u2211 t=1 G2Et[\u2016xt \u25e6 t\u201622]\nSince\nE[t][g > t (wt \u2212w\u2217)] = E[t\u22121][Et[gt]>(wt \u2212w\u2217)]\n= E[t\u22121][\u2207L\u0302(wt)>(wt \u2212w\u2217)] \u2265 E[t\u22121][L\u0302(wt)\u2212 L\u0302(w\u2217)]\nAs a result\nE[n] [ n\u2211 t=1 (L\u0302(wt)\u2212 L\u0302(w\u2217)) ]\n\u2264 r 2 2\u03b7 + \u03b7 2 n\u2211 t=1 G2ED\u0302[\u2016xt \u25e6 t\u2016 2 2] \u2264 r2 2\u03b7 + \u03b7 2 G2B2n\nwhere the last inequality follows the assumed upper bound of ED\u0302[\u2016xt \u25e6 t\u2016 2 2]. Following the definition of w\u0302n and the convexity of L(w) we have\nE[n][L\u0302(w\u0302n)\u2212 L\u0302(w\u2217)] \u2264 E[n]\n[ 1\nn n\u2211 t=1 (L(wt)\u2212 L(w\u2217))\n]\n\u2264 r 2 2\u03b7n + \u03b7 2 G2B2\nBy minimizing the upper bound of in terms of \u03b7, we have\nE[n][L(w\u0302n)\u2212 L(w\u2217)] \u2264 GBr\u221a n\nAccording to Proposition 1 in the paper,\nL\u0302(w) = L(w) +RD,M(w)\nTherefore\nE[n][L(w\u0302n) +RD,M(w\u0302n)]\n\u2264 L(w\u2217) +RD,M(w\u2217) + GBr\u221a n\nA.1. Proof of Lemma 1\n1 2 \u2016wt+1 \u2212w\u2217\u201622 = 1 2 \u2016wt \u2212 \u03b7gt \u2212w\u2217\u201622\n= 1\n2 \u2016wt \u2212w\u2217\u201622 +\n\u03b72\n2 \u2016gt\u201622 \u2212 \u03b7(wt \u2212w\u2217)>gt\nThen\n(wt \u2212w\u2217)>gt \u2264 1\n2\u03b7 \u2016wt \u2212w\u2217\u201622 \u2212\n1\n2\u03b7 \u2016wt+1 \u2212w\u2217\u201622\n+ \u03b7\n2 \u2016gt\u201622\nBy summing the above inequality over t = 1, . . . , n, we obtain\nn\u2211 t=1 g>t (wt \u2212w\u2217) \u2264 \u2016w\u2217 \u2212w1\u201622 2\u03b7 + \u03b7 2 n\u2211 t=1 \u2016gt\u201622\nBy noting that w1 = 0 and \u2016w\u2217\u20162 \u2264 r, we obtain the inequality in Lemma 1."}, {"heading": "B. Proof of Proposition 4", "text": "We prove the first upper bound first. From Eqn. (4) in the paper, we have\nR\u0302D,M(w\u2217) \u2264 1\n8 ED[w\n> \u2217 CM(x \u25e6 )w\u2217]\nwhere we use the fact \u221a ab \u2264 a+b2 for a, b \u2265 0. Using Eqn. (5) in the paper, we have\nED[w > \u2217 CM(x \u25e6 )w\u2217]\n= ED [ w>\u2217 ( 1\nk diag(x2i /pi)\u2212\n1 k xx>\n) w\u2217 ] = 1\nk ED [ d\u2211 i=1 w2\u2217ix 2 i pi \u2212 (w>\u2217 x)2 ] This gives a tight bound of R\u0302D,M(w\u2217), i.e.,\nR\u0302D,M(w\u2217) \u2264 1\n8k { d\u2211 i=1 w2\u2217iED[x 2 i ] pi \u2212 ED(w>\u2217 x)2 } By minimizing the above upper bound over pi, we obtain following probabilities\np\u2217i =\n\u221a w2\u2217iED[x\n2 i ]\u2211d\nj=1 \u221a w2\u2217iED[x 2 j ]\n(12)\nwhich depend on unknown w\u2217. We address this issue, we derive a relaxed upper bound. We note that\nCM(x \u25e6 ) = EM[(x \u25e6 \u2212 x)(x \u25e6 \u2212 x)>] \u2264 EM\u2016x \u25e6 \u2212 x\u201622 \u00b7 Id = ( EM[\u2016x \u25e6 \u201622]\u2212 \u2016x\u201622 ) Id\nwhere Id denotes the identity matrix of dimension d. Thus\nED[w > \u2217 CM(x \u25e6 )w\u2217] \u2264 \u2016w\u2217\u201622 ( ED\u0302[\u2016x \u25e6 \u2016 2 2]\u2212 ED[\u2016x\u201622] ) By noting the result in Proposition 2 in the paper, we have\nED[w > \u2217 CM(x \u25e6 )w\u2217]\n\u2264 1 k \u2016w\u2217\u201622 ( d\u2211 i=1 ED[x 2 i ] pi \u2212 ED[\u2016x\u201622] ) which proves the upper bound in Proposition 4."}, {"heading": "C. Neural Network Structures", "text": "Tables 3, 4 and 5 present the neural network structures and the number of filters, filter size, padding and stride parameters for MNIST, CIFAR-10 and CIFAR-100, respectively. Tables 6 and 7 present the network structures of different methods in subsection 5.3 in the paper. Note that in Table 4 and Table 5, the rnorm layer is the local response normalization layer and the local layer is the locally-connected layer with unshared weights. The layer pool(ave) in Table 6 and Table 7 represents the average pooling layer."}], "references": [{"title": "Understanding dropout", "author": ["Baldi", "Pierre", "Sadowski", "Peter J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Baldi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2013}, {"title": "Dropout training for support vector machines", "author": ["Chen", "Ning", "Zhu", "Jun", "Jianfei", "Zhang", "Bo"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Efficient batchwise dropout training using submatrices", "author": ["Graham", "Benjamin", "Reizenstein", "Jeremy", "Robinson", "Leigh"], "venue": "CoRR, abs/1502.02478,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "On the inductive bias of dropout", "author": ["Helmbold", "David P", "Long", "Philip M"], "venue": "CoRR, abs/1412.4736,", "citeRegEx": "Helmbold et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Helmbold et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "CoRR, abs/1506.02557,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Attribute efficient linear regression with distribution-dependent sampling", "author": ["Kukliansky", "Doron", "Shamir", "Ohad"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Kukliansky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kukliansky et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["Martens", "James", "Grosse", "Roger"], "venue": "arXiv preprint arXiv:1503.05671,", "citeRegEx": "Martens et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2015}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["Neyshabur", "Behnam", "Salakhutdinov", "Ruslan R", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Ranzato", "Marc\u2019Aurelio", "Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Stochastic convex optimization", "author": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Srebro", "Nathan", "Sridharan", "Karthik"], "venue": "In The 22nd Conference on Learning Theory (COLT),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Altitude training: Strong bounds for singlelayer dropout", "author": ["Wager", "Stefan", "Fithian", "William", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Fast dropout training", "author": ["Wang", "Sida", "Manning", "Christopher"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "An explicit sampling dependent spectral error bound for column subset selection", "author": ["Yang", "Tianbao", "Zhang", "Lijun", "Jin", "Rong", "Zhu", "Shenghuo"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Deep learning with elastic averaging sgd", "author": ["Zhang", "Sixin", "Choromanska", "Anna", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1412.6651,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Adaptive dropout rates for learning with corrupted features", "author": ["Zhuo", "Jingwei", "Zhu", "Jun", "Zhang", "Bo"], "venue": "In IJCAI, pp", "citeRegEx": "Zhuo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhuo et al\\.", "year": 2015}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters (Krizhevsky et al., 2012; Srivastava et al., 2014), which usually identically and independently at random samples neurons and sets their outputs to be zeros.", "startOffset": 108, "endOffset": 158}, {"referenceID": 4, "context": "Extensive experiments (Hinton et al., 2012) have shown that dropout can help obtain the state-of-the-art performance on a range of benchmark data sets.", "startOffset": 22, "endOffset": 43}, {"referenceID": 18, "context": ", equivalent to a form of data-dependent regularizer) (Wager et al., 2013).", "startOffset": 54, "endOffset": 74}, {"referenceID": 7, "context": ", Gaussian noise), which could lead to a better approximation of the marginalized loss (Wang & Manning, 2013; Kingma et al., 2015).", "startOffset": 87, "endOffset": 130}, {"referenceID": 24, "context": "Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015).", "startOffset": 102, "endOffset": 142}, {"referenceID": 7, "context": "Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework (Zhuo et al., 2015; Kingma et al., 2015).", "startOffset": 102, "endOffset": 142}, {"referenceID": 19, "context": "Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014).", "startOffset": 59, "endOffset": 121}, {"referenceID": 1, "context": "Other studies focus on shallow learning with dropout noise (Wager et al., 2014; Helmbold & Long, 2014; Chen et al., 2014).", "startOffset": 59, "endOffset": 121}, {"referenceID": 12, "context": "Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.", "startOffset": 101, "endOffset": 264}, {"referenceID": 12, "context": "Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks (Srivastava et al., 2014). It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. (2013); Baldi & Sadowski (2013) have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.", "startOffset": 101, "endOffset": 289}, {"referenceID": 1, "context": "Graham et al. (2015) used the same noise across a batch of examples in order to speed-up the computation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from", "startOffset": 99, "endOffset": 233}, {"referenceID": 13, "context": "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from", "startOffset": 99, "endOffset": 233}, {"referenceID": 23, "context": "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning (Sutskever et al., 2013; Neyshabur et al., 2015; Zhang et al., 2014; Martens & Grosse, 2015; Ioffe & Szegedy, 2015; Kingma & Ba, 2014), which tackle the problem from", "startOffset": 99, "endOffset": 233}, {"referenceID": 22, "context": "Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "Yang et al. (2015) developed sampling-dependent approximation error bound for column subset selection problem and tried to optimize the sampling-dependent error bound to obtain optimal sampling probabilities. Kukliansky & Shamir (2015) proposed a similar distributiondependent sampling for attribute-efficient learning in linear regression.", "startOffset": 0, "endOffset": 236}, {"referenceID": 18, "context": "In standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i.", "startOffset": 20, "endOffset": 61}, {"referenceID": 4, "context": "In standard dropout (Wager et al., 2013; Hinton et al., 2012), the entries of the noise vector are sampled independently according to Pr( j = 0) = \u03b4 and Pr( j = 1 1\u2212\u03b4 ) = 1 \u2212 \u03b4, i.", "startOffset": 20, "endOffset": 61}, {"referenceID": 18, "context": "Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in (Wager et al., 2013; Baldi & Sadowski, 2013) for logistic regression, which is stated in the following proposition for ease of discussion later.", "startOffset": 85, "endOffset": 129}, {"referenceID": 18, "context": "Using the second order Taylor expansion, (Wager et al., 2013) showed that the following approximation of RD,M(w) is easy to manipulate and understand:", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "In this sense, our theoretical analysis also explains why Z-normalization usually speeds up the training (Ranzato et al., 2010).", "startOffset": 105, "endOffset": 127}, {"referenceID": 11, "context": "In the next three subsections, we present the results on three benchmark data sets for comparing e-dropout and s-dropout: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).", "startOffset": 128, "endOffset": 148}, {"referenceID": 9, "context": "The training procedure is similar to (Krizhevsky et al., 2012), that is using mini-batch SGD with momentum (0.", "startOffset": 37, "endOffset": 62}, {"referenceID": 9, "context": ", step size) is decreased after a number of epochs similar to what was done in previous works (Krizhevsky et al., 2012).", "startOffset": 94, "endOffset": 119}, {"referenceID": 11, "context": "The MNIST (LeCun et al., 1998) data set has 60,000 training images and 10,000 testing images.", "startOffset": 10, "endOffset": 30}, {"referenceID": 20, "context": "We used the similar neural network structure to (Wan et al., 2013): two convolution layers, two fully connected layers, a softmax layer and a cost layer at the end.", "startOffset": 48, "endOffset": 66}, {"referenceID": 20, "context": "01 the same to (Wan et al., 2013), and that for evolutional dropout is set to 0.", "startOffset": 15, "endOffset": 33}], "year": 2017, "abstractText": "Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10% on the prediction performance and over 50% on the convergence speed compared to the standard dropout.", "creator": "LaTeX with hyperref package"}}}