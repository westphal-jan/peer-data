{"id": "1602.02672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks", "abstract": "We propose deeply distributed, recurring Q-networks (DDRQN), which allow teams of agents to solve communication-based coordination tasks, where agents do not receive a pre-designed communication protocol. Therefore, in order to be able to communicate successfully, they must first automatically develop and agree on their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known puzzles, and show that DDRQN can successfully solve such tasks by discovering elegant communication protocols. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning communication protocols. Furthermore, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.", "histories": [["v1", "Mon, 8 Feb 2016 18:01:35 GMT  (3732kb,D)", "http://arxiv.org/abs/1602.02672v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["jakob n foerster", "yannis m assael", "nando de freitas", "shimon whiteson"], "accepted": false, "id": "1602.02672"}, "pdf": {"name": "1602.02672.pdf", "metadata": {"source": "META", "title": "Learning to Communicate to Solve Riddles  with Deep Distributed Recurrent Q-Networks", "authors": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson"], "emails": ["JAKOB.FOERSTER@CS.OX.AC.UK", "YANNIS.ASSAEL@CS.OX.AC.UK", "NANDODEFREITAS@GOOGLE.COM", "SHIMON.WHITESON@CS.OX.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al., 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).\nThe above-mentioned problems all involve only a single learning agent. However, recent work has begun to address multi-agent deep RL. In competitive settings, deep learn\u2020These authors contributed equally to this work.\ning for Go (Maddison et al., 2015; Silver et al., 2016) has recently shown success. In cooperative settings, Tampuu et al. (2015) have adapted deep Q-networks (Mnih et al., 2015) to allow two agents to tackle a multi-agent extension to ALE. Their approach is based on independent Qlearning (Shoham et al., 2007; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), in which all agents learn their own Q-functions independently in parallel.\nHowever, these approaches all assume that each agent can fully observe the state of the environment. While DQN has also been extended to address partial observability (Hausknecht & Stone, 2015), only single-agent settings have been considered. To our knowledge, no work on deep reinforcement learning has yet considered settings that are both partially observable and multi-agent.\nSuch problems are both challenging and important. In the cooperative case, multiple agents must coordinate their behaviour so as to maximise their common payoff while facing uncertainty, not only about the hidden state of the environment but about what their teammates have observed and thus how they will act. Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).\nIn this paper, we propose deep distributed recurrent Qnetworks (DDRQN) to enable teams of agents to learn effectively coordinated policies on such challenging problems. We show that a naive approach to simply training independent DQN agents with long short-term memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) is inadequate for multi-agent partially observable problems.\nTherefore, we introduce three modifications that are key to DDRQN\u2019s success: a) last-action inputs: supplying each agent with its previous action as input on the next time step so that agents can approximate their action-observation histories; b) inter-agent weight sharing: a single network\u2019s\nar X\niv :1\n60 2.\n02 67\n2v 1\n[ cs\n.A I]\n8 F\neb 2\nweights are used by all agents but that network conditions on the agent\u2019s unique ID, to enable fast learning while also allowing for diverse behaviour; and c) disabling experience replay, which is poorly suited to the non-stationarity arising from multiple agents learning simultaneously.\nTo evaluate DDRQN, we propose two multi-agent reinforcement learning problems that are based on well-known riddles: the hats riddle, where n prisoners in a line must determine their own hat colours; and the switch riddle, in which n prisoners must determine when they have all visited a room containing a single switch. Both riddles have been used as interview questions at companies like Google and Goldman Sachs.\nWhile these environments do not require convolutional networks for perception, the presence of partial observability means that they do require recurrent networks to deal with complex sequences, as in some single-agent works (Hausknecht & Stone, 2015; Ba et al., 2015) and languagebased (Narasimhan et al., 2015) tasks. In addition, because partial observability is coupled with multiple agents, optimal policies critically rely on communication between agents. Since no communication protocol is given a priori, reinforcement learning must automatically develop a coordinated communication protocol.\nOur results demonstrate that DDRQN can successfully solve these tasks, outperforming baseline methods, and discovering elegant communication protocols along the way. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success."}, {"heading": "2. Background", "text": "In this section, we briefly introduce DQN and its multiagent and recurrent extensions."}, {"heading": "2.1. Deep Q-Networks", "text": "In a single-agent, fully-observable, reinforcement learning setting (Sutton & Barto, 1998), an agent observes its current state st \u2208 S at each discrete time step t, chooses an action at \u2208 A according to a potentially stochastic policy \u03c0, observes a reward signal rt, and transitions to a new state st+1. Its objective is to maximize an expectation over the discounted return, Rt\nRt = rt + \u03b3rt+1 + \u03b3 2rt+2 + \u00b7 \u00b7 \u00b7 , (1)\nwhere \u03b3 \u2208 [0, 1) is a discount factor. The Q-function of a policy \u03c0 is:\nQ\u03c0(s, a) = E [Rt|st = s, at = a] . (2)\nThe optimal action-value function Q\u2217(s, a) = max\u03c0 Q \u03c0(s, a) obeys the Bellman optimality equation:\nQ\u2217(s, a) = Es\u2032 [ r + \u03b3max\na\u2032 Q\u2217(s\u2032, a\u2032) | s, a\n] . (3)\nDeep Q-networks (Mnih et al., 2015) (DQNs) use neural networks parameterised by \u03b8 to representQ(s, a; \u03b8). DQNs are optimised by minimising the following loss function at each iteration i:\nLi(\u03b8i) = Es,a,r,s\u2032 [( yDQNi \u2212Q(s, a; \u03b8i) )2] , (4)\nwith target\nyDQNi = r + \u03b3max a\u2032 Q(s\u2032, a\u2032; \u03b8\u2212i ). (5)\nHere, \u03b8\u2212i are the weights of a target network that is frozen for a number of iterations while updating the online networkQ(s, a; \u03b8i) by gradient descent. DQN uses experience replay (Lin, 1993; Mnih et al., 2015): during learning, the agent builds a datasetDt = {e1, e2, . . . , et} of experiences et = (st, at, rt, st+1) across episodes. The Q-network is then trained by sampling mini-batches of experiences from D uniformly at random. Experience replay helps prevent divergence by breaking correlations among the samples. It also enables reuse of past experiences for learning, thereby reducing sample costs."}, {"heading": "2.2. Independent DQN", "text": "DQN has been extended to cooperative multi-agent settings, in which each agent m observes the global st, selects an individual action amt , and receives a team reward, rt, shared among all agents. Tampuu et al. (2015) address this setting with a framework that combines DQN with independent Q-learning, applied to two-player pong, in which all agents independently and simultaneously learn their own Q-functions Qm(s, am; \u03b8mi ). While independent Q-learning can in principle lead to convergence problems (since one agent\u2019s learning makes the environment appear non-stationary to other agents), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014)."}, {"heading": "2.3. Deep Recurrent Q-Networks", "text": "Both DQN and independent DQN assume full observability, i.e., the agent receives st as input. By contrast, in partially observable environments, st is hidden and instead the agent receives only an observation ot that is correlated with st but in general does not disambiguate it.\nHausknecht & Stone (2015) propose the deep recurrent Qnetwork (DRQN) architecture to address single-agent, partially observable settings. Instead of approximatingQ(s, a)\nwith a feed-forward network, they approximate Q(o, a) with a recurrent neural network that can maintain an internal state and aggregate observations over time. This can be modelled by adding an extra input ht\u22121 that represents the hidden state of the network, yielding Q(ot, ht\u22121, a; \u03b8i). Thus, DRQN outputs both Qt, and ht, at each time step. DRQN was tested on a partially observable version of ALE in which a portion of the input screens were blanked out."}, {"heading": "2.4. Partially Observable Multi-Agent RL", "text": "In this work, we consider settings where there are both multiple agents and partial observability: each agent receives its own private omt at each time step and maintains an internal state hmt . However, we assume that learning can occur in a centralised fashion, i.e., agents can share parameters, etc., during learning so long as the policies they learn condition only on their private histories. In other words, we consider centralised learning of decentralised policies.\nWe are interested in such settings because it is only when multiple agents and partial observability coexist that agents have the incentive to communicate. Because no communication protocol is given a priori, the agents must first automatically develop and agree upon such a protocol. To our knowledge, no work on deep RL has considered such settings and no work has demonstrated that deep RL can successfully learn communication protocols."}, {"heading": "3. DDRQN", "text": "The most straightforward approach to deep RL in partially observable multi-agent settings is to simply combine DRQN with independent Q-learning, in which case each agent\u2019s Q-network represents Qm(omt , h m t\u22121, a\nm; \u03b8mi ), which conditions on that agent\u2019s individual hidden state as well as observation. This approach, which we call the naive method, performs poorly, as we show in Section 5.\nInstead, we propose deep distributed recurrent Q-networks (DDRQN), which makes three key modifications to the naive method. The first, last-action input, involves providing each agent with its previous action as input to the next time step. Since the agents employ stochastic policies for the sake of exploration, they should in general condition their actions on their action-observation histories, not just their observation histories. Feeding the last action as input allows the RNN to approximate action-observation histories.\nThe second, inter-agent weight sharing, involves tying the weights of all agents networks. In effect, only one network is learned and used by all agents. However, the agents can still behave differently because they receive different observations and thus evolve different hidden states. In addition, each agent receives its own index m as input, mak-\nAlgorithm 1 DDRQN\nInitialise \u03b81 and \u03b8\u22121 for each episode e do hm1 = 0 for each agent m s1 = initial state, t = 1 while st 6= terminal and t < T do\nfor each agent m do With probability pick random amt else amt = argmaxaQ(o m t , h m t\u22121,m, a m t\u22121, a; \u03b8i)\nGet reward rt and next state st+1, t = t+ 1 \u2207\u03b8 = 0 . reset gradient for j = t\u2212 1 to 1, \u22121 do\nfor each agent m do ymj = { rj , if sj terminal, else rj + \u03b3maxaQ(o m j+1, h m j ,m, a m j , a; \u03b8 \u2212 i )\nAccumulate gradients for: (ymj \u2212Q(omj , hmj\u22121,m, amj\u22121, amj ; \u03b8i))2\n\u03b8i+1 = \u03b8i + \u03b1\u2207\u03b8 . update parameters \u03b8\u2212i+1 = \u03b8 \u2212 i + \u03b1 \u2212(\u03b8i+1 \u2212 \u03b8\u2212i ) . update target network\ning it easier for agents to specialise. Weight sharing dramatically reduces the number of parameters that must be learned, greatly speeding learning.\nThe third, disabling experience replay, simply involves turning off this feature of DQN. Although experience replay is helpful in single-agent settings, when multiple agents learn independently the environment appears nonstationary to each agent, rendering its own experience obsolete and possibly misleading.\nGiven these modifications, DDRQN learns a Q-function of the form Q(omt , h m t\u22121,m, a m t\u22121, a m t ; \u03b8i). Note that \u03b8i does not condition on m, due to weight sharing, and that amt\u22121 is a portion of the history while amt is the action whose value the Q-network estimates.\nAlgorithm 1 describes DDRQN. First, we initialise the target and Q-networks. For each episode, we also initialise the state, s1, the internal state of the agents, hm1 , and a m 0 . Next, for each time step we pick an action for each agent -greedily w.r.t. the Q-function. We feed in the previous action, amt\u22121, the agent index, m, along with the observation omt and the previous internal state, h m t\u22121. After all agents have taken their action, we query the environment for a state update and reward information.\nWhen we reach the final time step or a terminal state, we proceed to the Bellman updates. Here, for each agent, m, and time step, j, we calculate a target Q-value, ymj , using the observed reward, rj , and the discounted target network. We also accumulate the gradients, \u2207\u03b8, by regressing the Q-value estimate, Q(omj , h m j\u22121,m, a m j\u22121, a; \u03b8i), against the target Q-value, ymj , for the action chosen, a m j .\nLastly, we conduct two weight updates, first \u03b8i in the direction of the accumulated gradients, \u2207\u03b8, and then the target network, \u03b8\u2212i , in the direction of \u03b8i."}, {"heading": "4. Multi-Agent Riddles", "text": "In this section, we describe the riddles on which we evaluate DDRQN."}, {"heading": "4.1. Hats Riddle", "text": "The hats riddle can be described as follows: \u201cAn executioner lines up 100 prisoners single file and puts a red or a blue hat on each prisoner\u2019s head. Every prisoner can see the hats of the people in front of him in the line - but not his own hat, nor those of anyone behind him. The executioner starts at the end (back) and asks the last prisoner the colour of his hat. He must answer \u201cred\u201d or \u201cblue.\u201d If he answers correctly, he is allowed to live. If he gives the wrong answer, he is killed instantly and silently. (While everyone hears the answer, no one knows whether an answer was right.) On the night before the line-up, the prisoners confer on a strategy to help them. What should they do?\u201d (Poundstone, 2012). Figure 1 illustrates this setup.\nAn optimal strategy is for all prisoners to agree on a communication protocol in which the first prisoner says \u201cblue\u201d if the number of blue hats is even and \u201cred\u201d otherwise (or vice-versa). All remaining prisoners can then deduce their hat colour given the hats they see in front of them and the responses they have heard behind them. Thus, everyone except the first prisoner will definitely answer correctly.\nTo formalise the hats riddle as a multi-agent RL task, we define a state space s = (s1, . . . , sn, a1, . . . , an), where n is the total number of agents, sm \u2208 {blue, red} is the mth agent\u2019s hat colour and am \u2208 {blue, red} is the action it took on the m-th step. At all other time steps, agent m can only take a null action. On the m-th time step, agent m\u2019s observation is om = (a1, . . . , am\u22121, sm+1, . . . , sn). Reward is zero except at the end of the episode, when it is the total number of agents with the correct action: rn =\u2211 m I(a\nm = sm). We label only the relevant observation om and action am of agent m, omitting the time index.\nAlthough this riddle is a single action and observation problem it is still partially observable, given that none of the agents can observe the colour of their own hat."}, {"heading": "4.2. Switch Riddle", "text": "The switch riddle can be described as follows: \u201cOne hundred prisoners have been newly ushered into prison. The warden tells them that starting tomorrow, each of them will be placed in an isolated cell, unable to communicate amongst each other. Each day, the warden will choose one of the prisoners uniformly at random with replacement, and place him in a central interrogation room containing only a light bulb with a toggle switch. The prisoner will be able to observe the current state of the light bulb. If he wishes, he can toggle the light bulb. He also has the option of announcing that he believes all prisoners have visited the interrogation room at some point in time. If this announcement is true, then all prisoners are set free, but if it is false, all prisoners are executed. The warden leaves and the prisoners huddle together to discuss their fate. Can they agree on a protocol that will guarantee their freedom?\u201d (Wu, 2002).\nDay 1\n3 2 3 1\nOff On\nOff On\nOff On\nDay 2 Day 3 Day 4\nSwitch:\nAction: On None None Tell\nOff On\nPrisoner in IR :\nFigure 2. Switch: Every day one prisoner gets sent to the interrogation room where he can see the switch and choose between actions \u201cOn\u201d, \u201cOff\u201d, \u201cTell\u201d and \u201cNone\u201d.\nA number of strategies (Song, 2012; Wu, 2002) have been analysed for the infinite time-horizon version of this problem in which the goal is to guarantee survival. One wellknown strategy is for one prisoner to be designated the counter. Only he is allowed to turn the switch off while each other prisoner turns it on only once. Thus, when the counter has turned the switch off n\u22121 times, he can \u201cTell\u201d.\nTo formalise the switch riddle, we define a state space s = (SWt, IRt, s\n1, . . . , sn), where SWt \u2208 {on, off} is the position of the switch, IR \u2208 {1 . . . n} is the current visitor in the interrogation room and s1, . . . , sn \u2208 {0, 1} tracks which agents have already been to the interrogation room. At time step t, agent m observes omt = (irt, swt), where irt = I(IRt = m), and swt = SWt if the agent is in the interrogation room and null otherwise. If agent m is in the interrogation room then its actions are amt \u2208 {\u201cOn\u201d, \u201cOff\u201d, \u201cTell\u201d, \u201cNone\u201d}; otherwise the only action is \u201dNone\u201d. The episode ends when an agent chooses \u201cTell\u201d or when the maximum time step is reached. The reward rt\nis 0 except unless an agent chooses \u201cTell\u201d, in which case it is 1 if all agents have been to the interrogation room and \u22121 otherwise."}, {"heading": "5. Experiments", "text": "In this section, we evaluate DDRQN on both multi-agent riddles. In our experiments, prisoners select actions using an -greedy policy with = 1\u22120.5 1n for the hats riddle and = 0.05 for the switch riddle. For the latter, the discount factor was set to \u03b3 = 0.95, and the target networks, as described in Section 3, update with \u03b1\u2212 = 0.01, while in both cases weights were optimised using Adam (Kingma & Ba, 2014) with a learning rate of 1\u00d7 10\u22123. The proposed architectures make use of rectified linear units, and LSTM cells. Further details of the network implementations are described in the Supplementary Material and source code will be published online."}, {"heading": "5.1. Hats Riddle", "text": "Figure 3 shows the architecture we use to apply DDRQN to the hats riddle. To select am, the network is fed as input om = (a1, . . . , am\u22121, sm+1, . . . , sn), as well as m and n. The answers heard are passed through two single-layer MLPs, zka = MLP[1 \u00d7 64](ak) \u2295 MLP[2 \u00d7\n64](m,n), and their outputs are added element-wise. Subsequently, zka is passed through an LSTM network yka , h k a = LSTMa[64](z k a , h k\u22121 a ). We follow a similar procedure for the n \u2212 m hats observed defining yks , hks = LSTMs[64](zks , h k\u22121 s ). Finally, the last values of the two LSTM networks ym\u22121a and y n s are used to approximate the Q-Values for each action Qm = MLP[128 \u00d7 64, 64 \u00d7 64, 64 \u00d7 1](ym\u22121a ||yns ). The network is trained with minibatches of 20 episodes.\nFurthermore, we use an adaptive variant of curriculum learning (Bengio et al., 2009) to pave the way for scalable strategies and better training performance. We sample examples from a multinomial distribution of curricula, each corresponding to a different n, where the current bound is raised every time performance becomes near optimal. The probability of sampling a given n is inversely proportional to the performance gap compared to the normalised maximum reward. The performance is depicted in Figure 5.\nWe first evaluate DDRQN for n = 10 and compare it with tabular Q-learning. Tabular Q-learning is feasible only with few agents, since the state space grows exponentially with n. In addition, separate tables for each agent precludes generalising across agents.\nFigure 4 shows the results, in which DDRQN substantially outperforms tabular Q-learning. In addition, DDRQN also comes near in performance to the optimal strategy described in Section 4.1. This figure also shows the results of an ablation experiment in which inter-agent weight sharing has been removed from DDRQN. The results confirm that inter-agent weight sharing is key to performance.\nSince each agent takes only one action in the hats riddle, it is essentially a single step problem. Therefore, last-action\ninputs and disabling experience replay do not play a role and do not need to be ablated. We consider these components in the switch riddle in Section 5.2.\nWe compare the strategies DDRQN learns to the optimal strategy by computing the percentage of trials in which the first agent correctly encodes the parity of the observed hats in its answer. Table 1 shows that the encoding is almost perfect for n \u2208 {3, 5, 8}. For n \u2208 {12, 16, 20}, the agents do not encode parity but learn a different distributed solution that is nonetheless close to optimal. We believe that qualitatively this solution corresponds to more the agents communicating information about other hats through their answers, instead of only the first agent."}, {"heading": "5.2. Switch Riddle", "text": "Figure 6 illustrates the model architecture used in the switch riddle. Each agent m is modelled as a recurrent neural network with LSTM cells that is unrolled for Dmax time-steps, where d denotes the number of days of the episode. In our experiments, we limit d to Dmax = 4n\u2212 6 in order to keep the experiments computationally tractable.\nThe inputs, omt , a m t\u22121,m and n, are processed through a 2-layer MLP zmt = MLP[(7 + n) \u00d7 128, 128 \u00d7\n128](omt ,OneHot(a m t\u22121),OneHot(m), n). Their embedding zmt is then passed an LSTM network, y m t , h m t = LSTM[128](zmt , h m t\u22121), which is used to approximate the agent\u2019s action-observation history. Finally, the output ymt of the LSTM is used at each step to approximate the Q-values of each action using a 2-layer MLP Qmt = MLP[128 \u00d7 128, 128 \u00d7 128, 128 \u00d7 4](ymt ). As in the hats riddle, curriculum learning was used for training.\nFigure 7, which shows results for n = 3, shows that DDRQN learns an optimal policy, beating the naive method and the hand coded strategy, \u201ctell on last day\u201d. This verifies that the three modifications of DDRQN substantially improve performance on this task. In following paragraphs we analyse the importance of the individual modifications.\nWe analysed the strategy that DDRQN discovered for n = 3 by looking at 1000 sampled episodes. Figure 8 shows a decision tree, constructed from those samples, that corresponds to an optimal strategy allowing the agents to collectively track the number of visitors to the interrogation room. When a prisoner visits the interrogation room after day two, there are only two options: either one or two prisoners may have visited the room before. If three prisoners had been, the third prisoner would have already finished the game. The two remaining options can be encoded via the \u201con\u201d and \u201coff\u201d position respectively. In order to carry out this strategy each prisoner has to learn to keep track of whether he has visited the cell before and what day it currently is.\nFigure 9 compares the performance of DDRQN to a variant in which the switch has been disabled. After around 3,500 episodes, the two diverge in performance. Hence, there is a clearly identifiable point during learning when the prison-\ners start learning to communicate via the switch. Note that only when the switch is enabled can DDRQN outperform the hand-coded \u201ctell on last day\u201d strategy. Thus, communication via the switch is required for good performance.\nFigure 10 shows performance for n = 4. On most runs, DDRQN clearly beats the hand-coded \u201ctell on last day\u201d strategy and final performance approaches 90% of the oracle. However, on some of the remaining runs DDRQN fails to significantly outperform the hand-coded strategy. Analysing the learned strategies suggests that prisoners typically encode whether 2 or 3 prisoners have been to the room via the \u201con\u201d and \u201coff\u201d positions of the switch, respectively. This strategy generates no false negatives, i.e., when the 4th prisoner enters the room, he always \u201cTells\u201d, but generates false positives around 5% of the time. Example strategies are included in the Supplementary Material.\nFurthermore, Figure 11 shows the results of ablation experiments in which each of the modifications in DDRQN is removed one by one. The results show that all three\nmodifications contribute substantially to DDRQN\u2019s performance. Inter-agent weight sharing is by far the most important, without which the agents are essentially unable to learn the task, even for n = 3. Last-action inputs also play a significant role, without which performance does not substantially exceed that of the \u201ctell on last day\u201d strategy. Disabling experience replay also makes a difference, as performance with replay never reaches optimal, even after 50,000 episodes. This result is not surprising given the non-stationarity induced by multiple agents learning in parallel. Such non-stationarity arises even though agents can track their action-observation histories via RNNs within a given episode. Since their memories are reset between episodes, learning performed by other agents appears as non-stationary from their perspective.\nHowever, it is particularly important in communicationbased tasks like these riddles, since the value function for communication actions depends heavily on the interpretation of these messages by the other agents, which is in turn set by their Q-functions."}, {"heading": "6. Related Work", "text": "There has been a plethora of work on multi-agent reinforcement learning with communication, e.g., (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013). However, most of this work assumes a pre-defined communication protocol. One exception is the work of Kasai et al. (2008), in which the tabular Q-learning agents have to learn the content of a message to solve a predator-prey task. Their approach is similar to the Q-table benchmark used in Section 5.1. By contrast, DDRQN uses recurrent neural networks that allow for memory-based communication and generalisation across agents.\nAnother example of open-ended communication learning in a multi-agent task is given in (Giles & Jim, 2002). However, here evolutionary methods are used for learning communication protocols, rather than RL. By using deep RL with shared weights we enable our agents to develop distributed communication strategies and to allow for faster learning via gradient based optimisation.\nFurthermore, planning-based RL methods have been employed to include messages as an integral part of the multi-agent reinforcement learning challenge (Spaan et al., 2006). However, so far this work has not been extended to\ndeal with high dimensional complex problems.\nIn ALE, partial observability has been artificially introduced by blanking out a fraction of the input screen (Hausknecht & Stone, 2015). Deep recurrent reinforcement learning has also been applied to textbased games, which are naturally partially observable (Narasimhan et al., 2015). Recurrent DQN was also successful in the email campaign challenge (Li et al., 2015).\nHowever, all these examples apply recurrent DQN in single-agent domains. Without the combination of multiple agents and partial observability, there is no need to learn a communication protocol, an essential feature of our work."}, {"heading": "7. Conclusions & Future Work", "text": "This paper proposed deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In order to successfully communicate, agents in these tasks must first automatically develop and agree upon their own communication protocol. We presented empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. In addition, we presented ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.\nFuture work is needed to fully understand and improve the scalability of the DDRQN architecture for large numbers of agents, e.g., for n > 4 in the switch riddle. We also hope to further explore the \u201clocal minima\u201d structure of the coordination and strategy space that underlies these riddles. Another avenue for improvement is to extend DDRQN to make use of various multi-agent adaptations of Q-learning (Tan, 1993; Littman, 1994; Lauer & Riedmiller, 2000; Panait & Luke, 2005).\nA benefit of using deep models is that they can efficiently cope with high dimensional perceptual signals as inputs. In the future this can be tested by replacing the binary representation of the colour with real images of hats or applying DDRQN to other scenarios that involve real world data as input.\nWhile we have advanced a new proposal for using riddles as a test field for multi-agent partially observable reinforcement learning with communication, we also hope that this research will spur the development of further interesting and challenging domains in the area."}, {"heading": "8. Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship and the EPSRC."}], "references": [{"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["Assael", "J.-A. M", "N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "Deisenroth", "M. p"], "venue": "arXiv preprint arXiv:1510.02173,", "citeRegEx": "Assael et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Assael et al\\.", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "In AAAI,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "An overview of recent progress in the study of distributed multi-agent coordination", "author": ["Y. Cao", "W. Yu", "W. Ren", "G. Chen"], "venue": "IEEE Transactions on Industrial Informatics,", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Probabilistic approach to collaborative multi-robot localization", "author": ["D. Fox", "W. Burgard", "H. Kruppa", "S. Thrun"], "venue": "Autonomous Robots,", "citeRegEx": "Fox et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2000}, {"title": "A formal analysis and taxonomy of task allocation in multi-robot systems", "author": ["B.P. Gerkey", "M.J. Matari"], "venue": "International Journal of Robotics Research,", "citeRegEx": "Gerkey and Matari,? \\Q2004\\E", "shortCiteRegEx": "Gerkey and Matari", "year": 2004}, {"title": "Learning communication for multi-agent systems", "author": ["C.L. Giles", "K.C. Jim"], "venue": "In Innovative Concepts for AgentBased Systems,", "citeRegEx": "Giles and Jim,? \\Q2002\\E", "shortCiteRegEx": "Giles and Jim", "year": 2002}, {"title": "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Deep recurrent Qlearning for partially observable MDPs", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht and Stone,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Learning of communication codes in multi-agent reinforcement learning problem", "author": ["T. Kasai", "H. Tenmoto", "A. Kamiya"], "venue": "In IEEE Conference on Soft Computing in Industrial Applications, pp", "citeRegEx": "Kasai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kasai et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "An algorithm for distributed reinforcement learning in cooperative multi-agent systems", "author": ["M. Lauer", "M. Riedmiller"], "venue": "In ICML,", "citeRegEx": "Lauer and Riedmiller,? \\Q2000\\E", "shortCiteRegEx": "Lauer and Riedmiller", "year": 2000}, {"title": "End-toend training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Recurrent reinforcement learning: A hybrid approach", "author": ["X. Li", "L. Li", "J. Gao", "X. He", "J. Chen", "L. Deng", "J. He"], "venue": "arXiv preprint 1509.03044,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.J. Lin"], "venue": "PhD thesis,", "citeRegEx": "Lin,? \\Q1993\\E", "shortCiteRegEx": "Lin", "year": 1993}, {"title": "Markov games as a framework for multiagent reinforcement learning", "author": ["M.L. Littman"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Littman,? \\Q1994\\E", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Maddison et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2015}, {"title": "Coordination of communication in robot teams by reinforcement learning", "author": ["D. Maravall", "J. De Lope", "R. Domnguez"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Maravall et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maravall et al\\.", "year": 2013}, {"title": "Reinforcement learning in the multi-robot domain", "author": ["M.J. Matari"], "venue": "Autonomous Robots,", "citeRegEx": "Matari,? \\Q1997\\E", "shortCiteRegEx": "Matari", "year": 1997}, {"title": "QueryPOMDP: POMDP-based communication in multiagent systems", "author": ["F.S. Melo", "M. Spaan", "S.J. Witwicki"], "venue": "In Multi-Agent Systems, pp", "citeRegEx": "Melo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2011}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "In EMNLP,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in Atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Consensus and cooperation in networked multi-agent systems", "author": ["R. Olfati-Saber", "J.A. Fax", "R.M. Murray"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Olfati.Saber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Olfati.Saber et al\\.", "year": 2007}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Panait and Luke,? \\Q2005\\E", "shortCiteRegEx": "Panait and Luke", "year": 2005}, {"title": "Are You Smart Enough to Work at Google?: Fiendish Puzzles and Impossible Interview Questions from the World\u2019s Top Companies", "author": ["W. Poundstone"], "venue": "Oneworld Publications,", "citeRegEx": "Poundstone,? \\Q2012\\E", "shortCiteRegEx": "Poundstone", "year": 2012}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown", "year": 2009}, {"title": "If multi-agent learning is the answer, what is the question", "author": ["Y. Shoham", "R. Powers", "T. Grenager"], "venue": "Artificial Intelligence,", "citeRegEx": "Shoham et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shoham et al\\.", "year": 2007}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. sabis"], "venue": "search. Nature,", "citeRegEx": "sabis,? \\Q2016\\E", "shortCiteRegEx": "sabis", "year": 2016}, {"title": "100 prisoners and a light bulb", "author": ["Y. Song"], "venue": "Technical report, University of Washington,", "citeRegEx": "Song,? \\Q2012\\E", "shortCiteRegEx": "Song", "year": 2012}, {"title": "Decentralized planning under uncertainty for teams of communicating agents", "author": ["M. Spaan", "G.J. Gordon", "N. Vlassis"], "venue": "In International joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Spaan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Spaan et al\\.", "year": 2006}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["A. Tampuu", "T. Matiisen", "D. Kodelja", "I. Kuzovkin", "K. Korjus", "J. Aru", "R. Vicente"], "venue": "arXiv preprint arXiv:1511.08779,", "citeRegEx": "Tampuu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tampuu et al\\.", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["M. Tan"], "venue": "In ICML,", "citeRegEx": "Tan,? \\Q1993\\E", "shortCiteRegEx": "Tan", "year": 1993}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint 1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "In NIPS,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "100 prisoners and a lightbulb", "author": ["W. Wu"], "venue": "Technical report, OCF, UC Berkeley,", "citeRegEx": "Wu,? \\Q2002\\E", "shortCiteRegEx": "Wu", "year": 2002}, {"title": "Empirically evaluating multiagent learning algorithms", "author": ["E. Zawadzki", "A. Lipson", "K. Leyton-Brown"], "venue": "arXiv preprint 1401.8074,", "citeRegEx": "Zawadzki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zawadzki et al\\.", "year": 2014}, {"title": "Coordinating multi-agent reinforcement learning with limited communication", "author": ["C. Zhang", "V. Lesser"], "venue": null, "citeRegEx": "Zhang and Lesser,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Lesser", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al.", "startOffset": 180, "endOffset": 243}, {"referenceID": 0, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al.", "startOffset": 180, "endOffset": 243}, {"referenceID": 38, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al.", "startOffset": 180, "endOffset": 243}, {"referenceID": 1, "context": ", 2015), visual attention (Ba et al., 2015), and the Atari learning environment (ALE) (Guo et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 7, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 32, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 37, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 26, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 22, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 2, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 17, "context": "ing for Go (Maddison et al., 2015; Silver et al., 2016) has recently shown success.", "startOffset": 11, "endOffset": 55}, {"referenceID": 28, "context": "Their approach is based on independent Qlearning (Shoham et al., 2007; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), in which all agents learn their own Q-functions independently in parallel.", "startOffset": 49, "endOffset": 122}, {"referenceID": 40, "context": "Their approach is based on independent Qlearning (Shoham et al., 2007; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), in which all agents learn their own Q-functions independently in parallel.", "startOffset": 49, "endOffset": 122}, {"referenceID": 17, "context": "ing for Go (Maddison et al., 2015; Silver et al., 2016) has recently shown success. In cooperative settings, Tampuu et al. (2015) have adapted deep Q-networks (Mnih et al.", "startOffset": 12, "endOffset": 130}, {"referenceID": 19, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 4, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 23, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 3, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 1, "context": "While these environments do not require convolutional networks for perception, the presence of partial observability means that they do require recurrent networks to deal with complex sequences, as in some single-agent works (Hausknecht & Stone, 2015; Ba et al., 2015) and languagebased (Narasimhan et al.", "startOffset": 225, "endOffset": 268}, {"referenceID": 21, "context": ", 2015) and languagebased (Narasimhan et al., 2015) tasks.", "startOffset": 26, "endOffset": 51}, {"referenceID": 15, "context": "DQN uses experience replay (Lin, 1993; Mnih et al., 2015): during learning, the agent builds a datasetDt = {e1, e2, .", "startOffset": 27, "endOffset": 57}, {"referenceID": 28, "context": "While independent Q-learning can in principle lead to convergence problems (since one agent\u2019s learning makes the environment appear non-stationary to other agents), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014).", "startOffset": 204, "endOffset": 276}, {"referenceID": 40, "context": "While independent Q-learning can in principle lead to convergence problems (since one agent\u2019s learning makes the environment appear non-stationary to other agents), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014).", "startOffset": 204, "endOffset": 276}, {"referenceID": 33, "context": "Tampuu et al. (2015) address this setting with a framework that combines DQN with independent Q-learning, applied to two-player pong, in which all agents independently and simultaneously learn their own Q-functions Q(s, a; \u03b8 i ).", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "What should they do?\u201d (Poundstone, 2012).", "startOffset": 22, "endOffset": 40}, {"referenceID": 39, "context": "Can they agree on a protocol that will guarantee their freedom?\u201d (Wu, 2002).", "startOffset": 65, "endOffset": 75}, {"referenceID": 30, "context": "A number of strategies (Song, 2012; Wu, 2002) have been analysed for the infinite time-horizon version of this problem in which the goal is to guarantee survival.", "startOffset": 23, "endOffset": 45}, {"referenceID": 39, "context": "A number of strategies (Song, 2012; Wu, 2002) have been analysed for the infinite time-horizon version of this problem in which the goal is to guarantee survival.", "startOffset": 23, "endOffset": 45}, {"referenceID": 35, "context": ", (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013).", "startOffset": 2, "endOffset": 98}, {"referenceID": 20, "context": ", (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013).", "startOffset": 2, "endOffset": 98}, {"referenceID": 18, "context": ", (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013).", "startOffset": 2, "endOffset": 98}, {"referenceID": 10, "context": "One exception is the work of Kasai et al. (2008), in which the tabular Q-learning agents have to learn the content of a message to solve a predator-prey task.", "startOffset": 29, "endOffset": 49}, {"referenceID": 31, "context": "Furthermore, planning-based RL methods have been employed to include messages as an integral part of the multi-agent reinforcement learning challenge (Spaan et al., 2006).", "startOffset": 150, "endOffset": 170}, {"referenceID": 21, "context": "Deep recurrent reinforcement learning has also been applied to textbased games, which are naturally partially observable (Narasimhan et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 14, "context": "Recurrent DQN was also successful in the email campaign challenge (Li et al., 2015).", "startOffset": 66, "endOffset": 83}, {"referenceID": 35, "context": "Another avenue for improvement is to extend DDRQN to make use of various multi-agent adaptations of Q-learning (Tan, 1993; Littman, 1994; Lauer & Riedmiller, 2000; Panait & Luke, 2005).", "startOffset": 111, "endOffset": 184}, {"referenceID": 16, "context": "Another avenue for improvement is to extend DDRQN to make use of various multi-agent adaptations of Q-learning (Tan, 1993; Littman, 1994; Lauer & Riedmiller, 2000; Panait & Luke, 2005).", "startOffset": 111, "endOffset": 184}], "year": 2016, "abstractText": "We propose deep distributed recurrent Qnetworks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multiagent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.", "creator": "LaTeX with hyperref package"}}}