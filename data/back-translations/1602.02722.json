{"id": "1602.02722", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "PAC Reinforcement Learning with Rich Observations", "abstract": "We propose and investigate a new tractable model of high-dimensional observation amplification learning called Contextual MDPs, and generalize contextual bandits into a sequential decision framework. These models require an agent to take action based on high-dimensional observations (characteristics) with the goal of achieving long-term performance that competes with a large set of strategies. As the size of the observation space is a primary barrier to sample-efficient learning, we assume that Contextual MDPs can be summarized by a small number of hidden states. In this environment, we are designing a new reinforcement learning algorithm that engages with global exploration, while using a functional class to approximate future performance.", "histories": [["v1", "Mon, 8 Feb 2016 20:12:50 GMT  (43kb,D)", "http://arxiv.org/abs/1602.02722v1", null], ["v2", "Tue, 1 Mar 2016 15:16:12 GMT  (50kb,D)", "http://arxiv.org/abs/1602.02722v2", null], ["v3", "Tue, 24 May 2016 13:20:29 GMT  (40kb,D)", "http://arxiv.org/abs/1602.02722v3", null], ["v4", "Fri, 28 Oct 2016 15:37:17 GMT  (45kb,D)", "http://arxiv.org/abs/1602.02722v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "alekh agarwal", "john langford"], "accepted": true, "id": "1602.02722"}, "pdf": {"name": "1602.02722.pdf", "metadata": {"source": "CRF", "title": "Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations", "authors": ["Akshay Krishnamurthy", "Alekh Agarwal", "John Langford"], "emails": ["akshaykr@cs.cmu.edu", "alekha@microsoft.com", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The Atari Reinforcement Learning research program [20] has highlighted a critical deficiency of reinforcement learning algorithms: they cannot effectively solve problems that require systematic exploration. How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore?\nIn RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24]. Why do these results not apply?\nAn easy response is, \u201cbecause the hard games are not MDPs.\u201d This may be true for some of the hard games, but it is misleading\u2014the algorithms used do not even engage in minimal planning and global exploration1 as is required to solve MDPs efficiently. MDP-optimized global exploration has also been avoided because of a polynomial dependence on the number of unique observations which is intractably large with observations from a visual sensor. \u2217akshaykr@cs.cmu.edu \u2020alekha@microsoft.com \u2021jcl@microsoft.com 1We use \u201cglobal exploration\u201d to distinguish the structural exploration strategies required to solve an MDP efficiently from exponentially less efficient alternative such as -greedy.\nar X\niv :1\n60 2.\n02 72\n2v 1\n[ cs\n.L G\n] 8\nF eb\nIn contrast, supervised and contextual bandit learning algorithms have no dependence on the number of observations and at most a logarithmic dependence on the size of the underlying policy set. Approaches to RL with a weak dependence on these quantities exist [13], but suffer from an exponential dependence on the time horizon\u2014with K actions and a horizon of H , they require KH samples. Examples show this dependence is necessary, although such examples require a large number of states. Can we find an RL algorithm with no dependence on the number of unique observations and a polynomial dependence on the number of actionsK, the number of necessary statesM , the horizonH , and the policy complexity log(|\u03a0|)?\nTo begin answering this question we consider a simplified setting by assuming:\n1. episodic reinforcement learning. 2. the policy space can represent the exact-best solution. 3. state transition dynamics are deterministic.\nThese simplifications make the problem significantly more tractable without trivializing the core goal of designing a Poly(K,M,H, log(|\u03a0|))) algorithm. To this end, our contributions are:\n1. A new class of models (Contextual-MDPs) for the design and analysis of reinforcement learning algorithms. Contextual-MDPs generalize both contextual bandits and MDPs, but, unlike Partially Observable MDPs (POMDPs), the optimal policy in a Contextual-MDP depends only on the most recent observation rather than the entire trajectory.\n2. A new reinforcement learning algorithm and a guarantee that it PAC-learns Contextual-MDPs (with the above assumptions) using O(MK2H3 log(|\u03a0|)) samples. This is done by combining ideas from contextual bandits with a novel state equality test, yielding the first Poly(K,M,H, log(|\u03a0|)) reinforcement learning algorithm with no dependence on the number of unique observations. Like initial contextual bandit approaches, the algorithm is computationally inefficient since it requires enumeration of the policy class, an aspect we hope to address in future work.\nOur algorithm uses a function class to approximate future rewards, and thus lends theoretical backing for reinforcement learning with function approximation, which is the empirical state-of-the-art."}, {"heading": "2 The Model", "text": "In this section, we introduce the model we study throughout the paper, which we call episodic ContextualMDPs. We first setup basic notation. LetH \u2208 N be a time horizon,X denote a high-dimensional observation space,A a finite set of actions, and let S denote a finite set of latent states. Let K = |A|. We partition S into H disjoint groups S1, . . . ,SH , each of size at most M . For a set P , \u2206(P ) denotes the set of distributions over P ."}, {"heading": "2.1 Basic Definitions", "text": "An episodic Contextual-MDP is defined by the tuple (\u0393H ,\u0393, D) where H \u2208 N is the episode length, \u0393H \u2208 \u2206(SH) denotes a starting state distribution, \u0393 : (S \u00d7 A) \u2192 \u2206(S) denotes the transition dynamics, and D : S \u2192 \u2206(X \u00d7 [0, 1]K) associates a distribution over (observation, reward) pairs with each state. We use Ds \u2208 \u2206(X \u00d7 [0, 1]K) to denote the (observation, reward) distribution associated with state s and also the marginal distribution over observations (usage will be clear from context). We useDs|x to denote conditional distribution of the reward given the observation x in state s. The marginal and conditional distributions are referred to as Ds(x) and Ds|x(r).\nWe assume that the process is layered (also known as loop-free or acyclic in the literature) so that for a state sh \u2208 Sh and for any action a \u2208 A, \u0393(sh, a) \u2208 \u2206(Sh\u22121). Since the state space is partitioned into disjoint sets, each state is available only at a single time point, and the environment transitions from the state space SH down to S1 via a sequence of actions. Layered structure allows us to avoid indexing policies and Q-functions with time, which enables more concise notation but is mathematically equivalent to an alternative reformulation without layered structure.\nAn episode proceeds as follows. The environment chooses sH \u223c \u0393H , (xH , rH) \u223c DsH , and xH is revealed to the learner, who chooses an action aH . The learner observes rH(aH) and the environment transitions to state sH\u22121 \u223c \u0393(sH , aH), draws (xH\u22121, rH\u22121) \u223c DsH\u22121 and reveals xH\u22121 to the learner. The learner chooses an action aH\u22121 and the process continues for a total of H rounds of interaction, at which point the episode ends.\nOver the course of an episode, the reward obtained by the learner is \u2211H h=1 rh(ah), and the goal is to\nmaximize the expected cumulative reward,\nR = E[ H\u2211 h=1 rh(ah)], (1)\nwhere the expectation accounts for all randomness in the model and the learner. We assume that almost surely \u2211H h=1 rh(ah) \u2208 [0, 1] for any action sequence.\nThe record of interaction observed by the learner is (xH , aH , rH(aH), . . . , x1, a1, r1(a1)). The full record of interaction for a single episode is the tuple (sH , xH , rH , aH , . . . s1, x1, r1, a1) where sH \u223c \u0393H , sh \u223c \u0393(sh+1, ah+1), (xh, rh) \u223c Dsh and all actions ah are chosen by the learner. Notice that all state information and rewards for alternative actions are unobserved by the learning algorithm. Figure 1 illustrates the observed and unobserved quantities over one round of interaction.\nA policy \u03c0 : X \u2192 A is a strategy for navigating the search space by taking actions \u03c0(x) given observation x. A policy generates a sequence of interactions as (xH , \u03c0(xH), rH(\u03c0(xH)), . . . , x1, \u03c0(x1), r1(\u03c0(x1))) with expected reward defined recursively through\nV (\u03c0) = Es\u223c\u0393H [V (s, \u03c0)] and, V (s, \u03c0) = E(x,r)\u223cDs [ r(\u03c0(x)) + Es\u2032\u223c\u0393(s,\u03c0(x))V (s \u2032, \u03c0) ] .\nAs the base case, we assume that for states s \u2208 S1, all actions transition deterministically to a terminal state s0 with V (s0, \u03c0) = 0 for all \u03c0.\nThe optimal expected reward achievable can be similarly computed recursively as\nV ? = Es\u223c\u0393H [V ?(s)] and, (2)\nV ?(s) = Ex\u223cDs max a\nEr\u223cDs|x [ r(a) + Es\u2032\u223c\u0393(s,a)V ?(s\u2032) ] .\nFor each (s, x) pair such that Ds(x) > 0 we can also define a Q? function as\nQ?s(x, a) = Er\u223cDs|x [ r(a) + Es\u2032\u223c\u0393(s,a)V ?(s\u2032) ] . (3)\nThis function captures the optimal choice of action given this (state, observation) pair and therefore encodes optimal behavior in the model. With no further assumptions, the above model is a layered episodic Partially Observable Markov Decision Process (POMDP)."}, {"heading": "2.2 The Contextual-MDP Model", "text": "The Contextual-MDP is as described above, but with an important restriction on the structure of the Q? function defined in Equation (3).\nDefinition 1 (Contextual-MDP). Let (S,A,X ,\u0393H ,\u0393, D) be a layered episodic POMDP. Let Q? be correspondingly defined as in Equation 3 and a?(s, x) = argmaxa\u2208AQ ? s(x, a). The POMDP is called a Contextual-MDP if for any two states s, s\u2032 such that Ds(x), Ds\u2032(x) > 0 we have a?(s, x) = a?(s\u2032, x).\nRestated, a Contextual-MDP requires the optimal action for maximizing long-term reward to be dependent solely on the observation x irrespective of the state. This is depicted in Figure 1, where the optimal action a? depends only on the current observation. In the following section, we describe how this condition relates to other reinforcement learning models in the literature. However, we first describe some examples where the condition holds.\nExample 1 (Disjoint contexts). The simplest example for a Contextual-MDP is one where each state s can be identified with a subset Xs so that Ds(x) > 0 only for x \u2208 Xs and where Xs \u2229 Xs\u2032 = \u2205 when s 6= s\u2032. In this case, a realized context x uniquely identifies the underlying state s so that the function Q?s(x, a) need not explicitly depend on the state s. On the other hand, this underlying mapping from s to Xs is unknown to the learning agent so the problem cannot be easily reduced to a classical tabular MDP with a small number of states. Our algorithm will not try to explicitly learn this mapping as the sample complexity could be prohibitive, but resolve it implicitly using the learner\u2019s policy class. The setting naturally extends to scenarios where any tuple of \u03c4 successive contexts have non-overlapping support for some \u03c4 \u2265 1. In this case, we can define a new state space that consists of all concatenations of \u03c4 states in the underlying state space and new observation distributions that concatenate corresponding observations.\nExample 2 (Path-augmented contexts). If the transition function \u0393 is deterministic, one can associate each state swith a sequence of actions (a path) that ends at state s and augment the context with some featurization of this sequence, including a featurization of the states visited along this path. As paths uniquely identify states in this case, this is an instance of the disjoint context scenario in Example 1.\nMore generally, Contextual-MDPs provides a convenient framework to reason about reinforcement learning with function approximation as we will see. This is highly desirable as such approaches are the empirical state-of-the-art, but the limited supporting theory provides little advice on systematic global exploration."}, {"heading": "2.3 Connections to Other Models", "text": "Our model is closely related to several well-studied models in the literature, namely:\nContextual Bandits: IfH = 1, then Contextual-MDPs reduce to stochastic contextual bandits [14, 6], a well-studied simplification of the general reinforcement learning problem. In contextual bandits, the learning algorithm repeatedly takes an action on the basis of a context (or observation), and accumulates reward for the chosen action. The main difference is that the choice of action does not influence the future observations; in the stochastic contextual bandits problem all (observation, reward) pairs are drawn independently and identically from some distribution. Thus Contextual-MDPs force the learning algorithm to use long-term decision making, which is not required for contextual bandits.\nMarkov Decision Processes: If X = S and the distribution over observations for each state s is concentrated on s, then Contextual-MDPs reduce to Markov Decision Processes (MDPs). MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24]. The main difference in our setting is that the observation space X is extremely large or infinite and the underlying state is unobserved, so tabular approaches are not viable. Thus, ContextualMDPs force the learning algorithm to generalize across observations, which is not required in MDPs.\nSample complexity bounds for reinforcement learning with large state or observation spaces do exist, but the results require unrealistic assumptions and/or the bounds are rather weak. One example is the metricE3 algorithm of Kakade et al. [10] (see also [8]) that has sample complexity independent of the number of unique states, but assumes the ability to cover the state space in a metric a priori known to the learner. Moreover, the sample complexity scales linearly in the cover size as opposed to a more typical logarithmic dependence as in supervised learning. The sparse sampling planner of Kearns et al. [13] also implies a sample complexity bound that is independent of the observation space size for episodic MDPs, but grows as O(KH). More recently, Abbasi-Yadkori and Neu [1] propose a model for MDPs with side-information, but this model requires mapping side-information to a small-state MDP, rather than mapping an observation to an action as in Contextual-MDPs.\nPolicy gradient methods that apply (stochastic) optimization methods to find a parameterized policy with high value can also be applied to large-state MDPs and to Contextual-MDPs. However, these methods use local search techniques and consequently do not achieve global optimality [25, 9] in theory as well as empirically, unlike our algorithm which is guaranteed to find the globally optimal policy.\nPOMDPs: By definition a Contextual-MDP is a Partially Observable Markov Decision Process (POMDP) where the optimal action at any state depends only on the current observation. Thus in ContextualMDPs, the learning algorithm does not have to reason over belief states as is required in POMDPs.\nBorrowing terminology, a Contextual-MDP is precisely a POMDP where a reactive policy, which uses only the current observation, is optimal. While there are POMDP methods for learning reactive policies, or more generally policies with bounded memory [19], they are based on policy gradient techniques, which suffer both theoretical and empirical drawbacks as we mentioned.\nThere are some sample complexity guarantees for learning in arbitrarily complex POMDPs, but the bounds we are aware of are quite weak as they scale linearly with |\u03a0| [12, 18].\nPredictive State Representations (PSRs): PSRs [17] encode states as a a collection of tests, a test being a sequence of (a, x) pairs observed in the history. Representationally, PSRs are even more powerful than POMDPs [23] which make them also more general than Contextual-MDPs. However, we are not aware of finite sample bounds for learning PSRs."}, {"heading": "2.4 Connections to Other Techniques", "text": "State Abstraction: Our work is closely related to the literature on state abstraction (See [16] for a survey), which primarily focuses on understanding what optimality properties are preserved in an MDP after the state space is compressed. However, Contextual-MDPs do not necessarily admit non-trivial state abstraction functions that are easy to discover (i.e. that do not amount to learning the optimal behavior) as the optimal behavior can depend on the observation in an arbitrary manner. Moreover, while there are finite sample results for learning state abstractions, they all make strong assumptions that limit the scope of application. A recent example is the work of Jiang et al. [7] which finds a good abstraction from a set of successively finer ones, but cannot search over the exponentially many abstractions functions.\nFunction Approximation: Our solution uses function approximation to address the generalization problem implicit in Contextual-MDPs. Function approximation is the empirical state-of-the-art in reinforcement learning [20], but theoretical analysis has been quite limited. Several authors have studied linear function approximation (See [26, 21]) but none of these results give finite sample bounds, as they do not address the exploration question. Baird [3] analyzes more general function approximation for predicting the value function in a Markov Chain, but does not show convergence when the agent is also selecting actions. More closely to our work, Li and Littman [15] do give finite sample bounds for RL with function approximation, but they assume access to a particular \u201cKnows-what-it-knows\u201d oracle, which cannot exist even for simple problems. We are not aware of finite sample results for approximating Q? with a function class, which is precisely what we do here."}, {"heading": "3 Our Approach", "text": "In this paper, we consider the task of probably approximately correct (PAC) learning Contextual-MDPs. Given a policy class \u03a0, we say that an algorithm PAC learns a Contextual-MDP if for any , \u03b4 \u2208 (0, 1), the algorithm outputs a policy \u03c0\u0302 with V (\u03c0\u0302) \u2265 max\u03c0\u2208\u03a0 V (\u03c0) \u2212 with probability at least 1 \u2212 \u03b4. The sample complexity of the algorithm is the number of episodes of the Contextual-MDP that the algorithm executes before returning an -suboptimal policy. Formally, the sample complexity is a function n : (0, 1)2 \u2192 N such that for any , \u03b4 \u2208 (0, 1), the algorithm returns an -suboptimal policy with probability at least 1 \u2212 \u03b4 using only n( , \u03b4) episodes."}, {"heading": "3.1 Additional Assumptions for the Result", "text": "Our algorithm operates on Contextual-MDPs with two additional assumptions. The first assumption posits the ability to approximate the Q? function (3) well and seems essential for a function approximation based approach.\nAssumption 1 (Realizability). We identify our set of policies \u03a0 with a set of regression functions F \u2282 (X \u00d7A)\u2192 [0, 1]. Specifically, we set \u03a0 = {\u03c0f : f \u2208 F} where \u03c0f = argmaxa f(x, a). We assume that F is available to the learner and make a realizability assumption, meaning that there exists a function f? \u2208 F , such that for every x \u2208 X and a \u2208 A, f?(x, a) = Q?s(x, a), for any state s such that Ds(x) > 0. We use N to denote |F| = |\u03a0|.\nNote that the above assumption tacitly forces the function Q?s(x, a) to be consistent across all states s withDs(x) > 0. This is stronger than only assuming the consistency of the argmax ofQ? as in Definition 1, but Q? may still be a complex function of the observation x and action a.\nThe regressor class induces a family of value functions defined for each f \u2208 F , s \u2208 S, and for any policy \u03c0 : X \u2192 A,\nV f (s, \u03c0) = Ex\u223cDsf(x, \u03c0(x)).\nWorking through definitions, it is easy to see that,\nV ?(s) = V f ?\n(s, \u03c0f?),\nfor all s so that V ? = Es\u223c\u0393H [V f?(s, \u03c0f?(s))]. Recalling the earlier definition, PAC-learning in the realizable setting requires finding a policy \u03c0\u0302 with V (\u03c0\u0302) \u2265 V ? \u2212 . Assumption 2 (Deterministic Transitions). We further assume that the transition model is deterministic. This means that the starting distribution \u0393H is a point-mass on some state sH and the transition dynamics map state action pairs deterministically to future states, i.e. \u0393 : (S \u00d7 A) \u2192 S , preserving the layered structure.\nEven with deterministic transitions, PAC-learning Contextual-MDPs requires systematic exploration that is unaddressed in previous work."}, {"heading": "3.2 Algorithm", "text": "We seek an algorithm that can PAC-learn realizable deterministic-transition Contextual-MDPs with sample complexity that is Poly(M,K,H, , log(N), log(1/\u03b4)), and we refer to such a sample complexity bound as polynomial in all relevant parameters. Notably, the algorithm should have no dependence on |X | which can be infinite. In this section, we develop such an algorithm, and we prove the sample complexity bound in Section 4. Our focus is on statistical efficiency, so we ignore computational considerations in the present work.\nBefore turning to the algorithm, it is worth clarifying some additional notation. Since we are focused on the deterministic transition setting, it is natural to think about the Contextual-MDP as an exponentially large search tree with fan-out K and depth H . Each node in the search tree is labeled with a state s \u2208 S, and each edge is labeled with an action a \u2208 A, both of which are consistent with the transition model. A path p \u2208 A? corresponds to a sequence of actions from the root of the search tree, and we also use p to denote the state reached after executing the corresponding sequence of actions from the root. We often call such a path a roll-in, in line with existing terminology. For a roll-in p, we use p \u25e6 a to denote a path formed by executing all actions in p and then execution action a.\nPseudocode for our algorithm is displayed in Algorithm 1. The algorithm is tail-recursive in nature and mimics depth first search, starting at the root of the search tree. The first call to the algorithm should be cMDPLearn(\u2205,F , , \u03b4) where \u2205 denotes the empty path (i.e., the root of the search tree), F is the given class of regression functions, is the target accuracy and \u03b4 is the target failure probability.\nThe Elimination Component: At a high level, the algorithm aims to maintain only the regressors that approximate the Q? function well, and it makes progress by discarding regressors that have a poor fit to the Q? function. This is achieved by training in a bottom-up fashion, so that when training at some path p, we ensure convergence at all descendants. At path p, we train by retaining only the regressors that have low excess risk on a carefully constructed regression problem (See TD-Elim, displayed in Algorithm 3). The regression problem in TD-Elim is motivated by Assumption 1 and the definition of Q? in Eq. (3), which imply that for any state s generating observation x,\nf?(x, a) = Er\u223cDs|xr(a) + V (\u0393(s, a), \u03c0f?)\n=Er\u223cDs|xr(a) + Ex\u2032\u223cD\u0393(s,a)f ?(x\u2032, \u03c0f?(x \u2032)). (4)\nAlgorithm 1 cMDPLearn (p,F , , \u03b4) Set \u03c6 =\n40H \u221a K , test = (H\u2212|p|\u22121/4) 2H .\nif StateLearned(p,F , test, \u03c6, \u03b4/2MKH ) then Return F . # Learned in state p already end if for a \u2208 A do F \u2190 cMDPLearn(p \u25e6 a,F , , \u03b4). # Recurse end for F\u0302 \u2190 TD-Elim ( p,F , \u03c6, \u03b4/2MH ) . # Learn in state p\nReturn F\u0302 .\nThus f? is consistent between its estimate at the current state s and the future state s\u2032 = \u0393(s, a). The regression problem we create is essentially a finite sample version of this identity. However some care must be taken as the target for each regression function f , V f (s\u2032, \u03c0f ), is the value of the future as predicted by f . This target differs for each function but can be estimated from samples. To ensure correct behavior of the regression problem, we must obtain high-quality estimates of these future value predictions. Nevertheless if constructed carefully, these regression problems ensure that the algorithm retains only good regressors, which induce good policies.\nTD-Elim is inspired by the RegressorElimination algorithm of Agarwal et al. [2] for contextual bandit learning in the realizable setting. Apart from the differences between the regression problem, motivated by the discussion above, the other main difference between the algorithms is the choice of action-selection distribution. RegressorElimination must carefully choose actions to balance exploration and exploitation which leads to an optimal regret bound. In contrast, we are pursuing a PAC-guarantee here, for which it suffices to focus exclusively on exploration.\nThe Exploration Component: The other main component of the algorithm, which is crucial for obtaining polynomial sample complexity, is a global exploration technique (See Algorithm 2). This is based on testing if the learning mechanism has already converged in a state (path), which can be done by estimating the value predictions for all surviving regressors. Specifically, if this test returns true, then all the surviving regressors agree on the value of the current state. As shown below, this condition is sufficient to learn at ancestor states. Furthermore, if we have already trained on the (observation, reward) distribution induced by the path p, then this test returns true with high probability, thus implicitly performing a state equality test at a level needed by the class F . The first property ensures that we learn properly, while the second property gives a strong sample complexity guarantee.\nThis test is performed at each path visited by the algorithm before making the recursive call on descendant paths, and if the learning process has converged, the algorithm does not visit the descendant paths. Thus, the algorithm does not traverse a large fraction the entire search tree provided that the convergence test succeeds often enough. The number of times the test does not report true at each level is upper bounded by the number of states M leading to a polynomial sample complexity bound."}, {"heading": "4 Theoretical Analysis", "text": "In this section we prove a PAC-learning guarantee for cMDPLearn on Contextual-MDPs.\nTheorem 1 (PAC bound). For any , \u03b4 \u2208 (0, 1) and any Contextual-MDP (Definition 1) with deterministic\nAlgorithm 2 StateLearned(p,F , test, \u03c6, \u03b4) Set ntest = 2 log(2N/\u03b4)/\u03c62. Collect ntest observations xi \u223c Dp. Compute Monte-Carlo estimates for each value function,\nV\u0302 f (p, \u03c0f ) = 1\nntest ntest\u2211 i=1 f(xi, \u03c0f (xi)) \u2200f \u2208 F\nif |V\u0302 f (p, \u03c0f )\u2212 V\u0302 g(p, \u03c0g)| \u2264 test for all f, g \u2208 F then return true end if Return false.\nAlgorithm 3 TD-Elim(p,F , \u03c6, \u03b4) Require estimates V\u0302 f (p \u25e6 a, \u03c0f ),\u2200f \u2208 F , a \u2208 A. Set ntrain = 24 log(2N/\u03b4)/\u03c62\nCollect ntrain observations (xi, ai, ri) where xi \u223c Dp, ai is chosen uniformly at random, and ri = ri(ai). Update F to {\nf \u2208 F : R\u0303(f) \u2264 min f \u2032\u2208F\nR\u0303(f) + 2\u03c62 + 22 log(N/\u03b4)\nntrain\n} ,\nwith R\u0303(f) = 1\nntrain ntrain\u2211 i=1 (f(xi, ai)\u2212 ri \u2212 V\u0302 f (p \u25e6 ai, \u03c0f ))2\nReturn F .\ntransitions for whichQ? \u2208 F , with probability at least 1\u2212\u03b4, any policy returned by cMDPLearn(\u2205,F , , \u03b4) is at most -suboptimal. Moreover, cMDPLearn(\u2205,F , , \u03b4) requires at most O ( MH3K2 2 log ( NMHK\n\u03b4 )) episodes.\nThis theorem states that cMDPLearn learns a policy that is at most -suboptimal for a ContextualMDP using a number of episodes that is polynomial in all relevant parameters. Looking a little closer into the result, our overall sample complexity is shown to beMHntrain +MHKntest. Since ntrain and ntest are set to be of the same order, this reveals that we lose a factor of K more samples while testing for state equality in the StateLearned routine. Consequently, the sample complexity can be improved by a factor of K whenever we do not need to test for state equality, or if collecting several exploration observations x for a path p without observing reward signal is cheap.\nSince Contextual-MDPs generalize both contextual bandits and MDPs, it is worth comparing the results.\n1. In contextual bandits, we have M = H = 1 so that the sample complexity of cMDPLearn is O\u0303(K2/ 2), in contrast with the optimal O\u0303(K/ 2) sample complexity for contextual bandit learning. As discussed above, the gap is due to the Kntest factor, which goes away in contextual bandits since there is only one state. Thus with minor modification, cMDPLearn matches the optimal sample complexity for contextual bandits.\n2. Assumptions vary with the paper, but broadly prior results establish the sample complexity for learning layered episodic MDPs with deterministic transitions is O\u0303(MKpoly(H)/ 2) [5, 22]. Again the discrepancy is the additional factor ofK present in Theorem 1, which can be avoided given that the states are known in an MDP. In this setting, cMDPLearn can easily be modified to have O\u0303(MKH3/ 2) sample complexity for layered episodic MDPs."}, {"heading": "4.1 Preliminaries for the Proof", "text": "The proof of the theorem hinges on analysis of the two subroutines. We turn first to the TD-Elim routine, for which we show the following guarantee.\nTheorem 2. Consider running TD-Elim at path p with regressors F , parameters \u03c6, \u03b4 and with ntrain = 24 log(2N/\u03b4)/\u03c62. Suppose that the following are true:\n1. Estimation Precondition: We have access to estimates V\u0302f (p \u25e6 a, \u03c0f ) for all f \u2208 F , a \u2208 A such that,|V\u0302 f (p \u25e6 a, \u03c0f )\u2212 V f (p \u25e6 a, \u03c0f )| \u2264 \u03c6.\n2. Bias Precondition: For all f, g \u2208 F and for all a \u2208 A, |V f (p \u25e6 a, \u03c0f )\u2212 V g(p \u25e6 a, \u03c0g)| \u2264 \u03c41.\nThen the following hold simultaneously with probability at least 1\u2212 \u03b4:\n1. f? is retained by the algorithm. 2. Estimation Bound:\n|V\u0302 f (p, \u03c0f )\u2212 V f (p, \u03c0f )| \u2264 \u03c6\u221a 12\n(5)\n3. Bias Bound:\n|V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 8\u03c6 \u221a K + 2\u03c6+ \u03c41 (6)\n4. Risk Bound:\nV ?(p)\u2212 V (p, \u03c0f ) \u2264 4\u03c6 \u221a 2K + 2\u03c6+ 2\u03c41 (7)\nThe last three bounds hold for all surviving f, g \u2208 F .\nThe proof is intricate so we sketch a proof here and defer details to Appendix A.\nProof Sketch. The proof involves relating the value predictions V f (p, \u03c0f ) to the empirical squared loss quantity controlled explicitly by the algorithm. The first step is similar to Agarwal et al. [2] and shows that R\u0303(f) is close to its expectation, which implies that f\u2217 is never eliminated and all surviving regressors have small expected squared error.\nThe second step relates the differences V f (p, \u03c0f ) \u2212 V g(p, \u03c0g) to E[R\u0303(f) \u2212 R\u0303(g)] as well as the bias and estimation precondition bounds at the descendants p \u25e6 a. It is crucial that the bias bound (6) retains a constant of 1 on \u03c41 which leads to additive growth of the bias and avoids an exponential dependence onH in our later analysis. Similar ideas are applied for the risk bound where a multiplicative factor is unavoidable, but fortunately we will only use this bound once at the root.\nAnalysis of the StateLearned subroutine requires only standard concentration-of-measure arguments.\nTheorem 3. Consider running StateLearned on path p with ntest = 2 log(2N/\u03b4)/\u03c62 and test \u2265 2\u03c6+\u03c42.\n1. If |V f (p, \u03c0f )\u2212V g(p, \u03c0g)| \u2264 \u03c42,\u2200f, g \u2208 F , then with probability at least 1\u2212 \u03b4, the algorithm returns true. 2. If the algorithm returns true, then with probability at least 1\u2212 \u03b4, we have (a) value estimates V\u0302 f (p, \u03c0f ) with |V\u0302 f (p, \u03c0f )\u2212 V f (p, \u03c0f )| \u2264 \u03c6 \u2200f \u2208 F , and (b) |V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 2\u03c6+ test \u2200f, g \u2208 F .\nAppendix B provides the proof."}, {"heading": "4.2 Proof of Theorem 1", "text": "For now assume that all calls to TD-Elim and StateLearned succeed. Later we will bound the total number of calls and hence the total failure probability.\nLet be the error parameter passed to cMDPLearn and recall that \u03c6 = 40H \u221a K . We first argue that in all calls to TD-Elim the estimation precondition is satisfied. To see this, notice that by design, the algorithm only calls TD-Elim at path p after the recursive step, which means for each p \u25e6 a either we ran TD-Elim or StateLearned returned true. Since both Theorems 2 and 3 guarantee estimation error of order \u03c6, the estimation precondition for path p holds. This argument applies to all paths p for which we call TD-Elim so that the estimation precondition is always satisfied.\nWe next analyze the bias term, for which we prove the following claim by induction. Inductive Claim: For all paths pwith h actions remaining and any pair f, g \u2208 F of surviving regressors,\n|V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 20h \u221a K \u03c6.\nBase Case: The claim clearly holds at time point 0, since all regressors estimate future reward as zero. Inductive Step: Assume that the inductive claim holds for all accessed paths with h\u2212 1 actions remaining. Consider any path p with h actions remaining that we access; we want to show that the claim holds for p.\nSince we access the path p, either we call TD-Elim or StateLearned returns true. If we call TD-Elim, then by the inductive hypothesis, we have already filtered the regressor class so that for all, a \u2208 A, f, g \u2208 F , we have\n|V f (p \u25e6 a, \u03c0f )\u2212 V g(p \u25e6 a, \u03c0g)| \u2264 20(h\u2212 1) \u221a K\u03c6.\nWe will therefore instantiate \u03c41 = 20(h\u2212 1) \u221a K\u03c6 in the bias precondition of Theorem 2. Of course we also have that the estimation precondition is satisfied with parameter \u03c6. Therefore, the bias bound in Theorem 2 shows that, for all f, g \u2208 F retained by the algorithm,\n|V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 8\u03c6 \u221a K + 2\u03c6+ \u03c41\n\u2264 10 \u221a K\u03c6+ 20(h\u2212 1) \u221a K\u03c6 \u2264 20 ( h\u2212 1\n2\n)\u221a K\u03c6. (8)\nThus, the inductive step holds in this case. The other case we must consider is if StateLearned returns true. In this case, we call StateLearned at level h with parameter test = (h\u22121/4) 2H = 20 ( h \u2212 14 )\u221a K\u03c6 recalling the setting of\n\u03c6 and noting that H \u2212 |p| is exactly the number of actions remaining after rolling in with p. Then by Theorem 3, if it returns true, we have the bias bound,\n|V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 2\u03c6+ 20 ( h\u2212 1\n4\n)\u221a K\u03c6\n\u2264 20h \u221a K\u03c6. (9)\nThus we have established the inductive claim. Establishing the PAC guarantee: Since the inductive claim holds for all paths accessed at level H \u2212 1, we may apply the risk bound in Theorem 2 at the root of the tree. The estimation and bias preconditions are satisfied with parameters \u03c6 and \u03c41 = 20(H \u2212 1) \u221a K\u03c6, respectively, so Theorem 2 implies that for any surviving regressor f \u2208 F ,\nV ?(sH)\u2212 V (sH , \u03c0f )\n\u2264 \u03c6(4 \u221a 2K + 2) + 40(H \u2212 1) \u221a K\u03c6 \u2264 40H \u221a K\u03c6 = , (10)\nby our choice for \u03c6. Thus, we satisfy the PAC guarantee, since all surviving policies \u03c0f are at most - suboptimal.\nWorking through definitions, this means if we set\nntrain = 24 log(2N/\u03b4) \u03c62 \u2265 (4\u00d7 10 4)H2K 2 log (2N/\u03b4)\nntest = 2 log(2N/\u03b4) \u03c62 \u2265 3200H\n2K\nlog(2N/\u03b4),\nthen we obtain the PAC guarantee. Bounding the Sample Complexity: We now bound the number of calls to each subroutine, which reveals how to allocate the failure probability and gives the sample complexity bound. Again, assume that all calls succeed.\nFirst notice that if we call StateLearned on some state s \u2208 Sh for which we have already called TD-Elim, then StateLearned returns true (assuming all calls to subroutines succeed). This follows because TD-Elim guarantees that the population predicted values for this state distribution are at most 20(h\u2212 1/2) \u221a K\u03c6 apart (Eq. (8)), which becomes the choice of \u03c42 in application of Theorem 3. This is legal since 2\u03c6+ 20(h\u2212 1/2) \u221a K\u03c6 \u2264 20(h\u2212 1/4) \u221a K\u03c6 = test,\nso that precondition for Theorem 3 holds. Therefore, at any level h, we can call TD-Elim at most one time per state s \u2208 Sh. In total, this yields MH calls to TD-Elim.\nNext, since we only make recursive calls when we execute TD-Elim, we expand at most M paths per level. This means that we call StateLearned on at most MK paths per level, since the fan-out of the tree is K. Thus, the number of calls to StateLearned is at most MKH .\nBy the setting of \u03b4 in the calls to these subroutines (i.e. \u03b4/(2MKH) in all calls to StateLearned and \u03b4/(2MH) in all calls to TD-Elim), and by Theorems 2 and 3, the total failure probability is therefore at most \u03b4.\nFinally, the number of episodes (trajectories) executed is\nMHntrain +MKHntest = O ( MH3K2\n2 log\n( NMHK\n\u03b4\n)) ,\nwhich concludes the proof."}, {"heading": "5 Discussion", "text": "This paper introduces a new model, Contextual-MDPs, in which it is possible to design and analyze principled reinforcement learning algorithms that engage in global exploration. As a first step, we develop cMDPLearn and show that it learns near-optimal behavior in Contextual-MDPs with polynomial sample complexity. To our knowledge, this is the first polynomial sample complexity bound for reinforcement learning with general function approximation.\nHowever, there are many avenues for future work:\n1. cMDPLearn has two main undesirable properties. Firstly, it requires a deterministic transition model which is unrealistic in some practical settings. Secondly, the algorithm involves enumerating the class of regression functions, so while its sample complexity is logarithmic in the function class size, its running time is linear, which is typically intractably slow. Resolving both of these deficiencies may lead to a new practical reinforcement learning algorithm.\n2. Our algorithm also crucially relies on the realizability assumption, which on one hand is implicitly assumed by state-of-the-art reinforcement learning algorithms, but is known to be unnecessary in the contextual bandit setting. Is it possible to design completely agnostic algorithms for learning in Contextual-MDPs?\nWe look forward to pursuing these directions."}, {"heading": "Acknowledgements", "text": "We thank Akshay Balsubramani and Hal Daume\u0301 III for formative discussions, and we thank Tzu-Kuo Huang for a careful reading of an early draft of this paper."}, {"heading": "A Proof of Theorem 2", "text": "The proof of Theorem 2 is quite technical, and we compartmentalize into several components. We begin with several technical lemmas. Throughout we will use the preconditions of the theorem, which we reproduce here.\nCondition 1. For all f \u2208 F and a \u2208 A, we have estimates V\u0302 f (p \u25e6 a, \u03c0f ) such that,\n|V\u0302 f (p \u25e6 a, \u03c0f )\u2212 V f (p \u25e6 a, \u03c0f )| \u2264 \u03c6.\nCondition 2. For all f, g \u2208 F and a \u2208 A we have,\n|V f (p \u25e6 a, \u03c0f )\u2212 V g(p \u25e6 a, \u03c0g)| \u2264 \u03c41.\nWe will make frequent use of the parameters \u03c6 and \u03c41 which are specified by these two conditions, and explicit in the theorem statement.\nRecall the notation,\nV f (p, \u03c0g) = Ex\u223cDpf(x, \u03c0g(x))\nwhich will be used heavily throughout the proof. As notational convenience, we will suppress dependence on the distributionDp, since we are considering one invocation of TD-Elim and we always roll into path p. This means that all (observation, reward) tuples will be drawn from Dp. Secondly it will be convenient to introduce the shorthand V f (p) = V f (p, \u03c0f ) and similarly for the estimates. Finally, we will further shorten the value functions for paths p \u25e6 a by defining,\nV fa = Ex\u223cDp\u25e6af(x, \u03c0f (x)) = V f (p \u25e6 a, \u03c0f ).\nWe will also use V\u0302 fa to denote the estimated versions which we have access to according to Condition 1. Lastly, our proof makes extensive use of the following random variable, which is defined for a particular regressor, although we omit the explicit dependence.\nY , (f(x, a)\u2212 r(a)\u2212 V\u0302 f (p \u25e6 a))2 \u2212 (f?(x, a)\u2212 r(a)\u2212 V\u0302 f ? (p \u25e6 a))2.\nHere (x, r) \u223c Dp and a \u2208 A is drawn uniformly at random as prescribed by Algorithm 3. When multiple regressors are involved, we use Y (f) to denote the random variable associated with regressor f .\nTo proceed, we first compute the expectation and variance of this random variable.\nLemma 1 (Properties of TD Squared Loss). Assume Conditions 1 and 2 hold. Then for any f \u2208 F , the random variable Y satisfies,\nEx,a,r[Y ] = Ex,a [ (f(x, a)\u2212 V\u0302 f (p \u25e6 a)\u2212 f?(x, a) + V f ? (p \u25e6 a))2 ] \u2212 Ex,a [ (V\u0302 f ? (p \u25e6 a)\u2212 V f ? (p \u25e6 a))2 ]\nVar x,a,r\n[Y ] \u2264 32Ex,a[Y ] + 64\u03c62\nProof. For further shorthand, denote f = f(x, a), f? = f?(x, a) and recall the definition of V fa and V\u0302 f a .\nEx,a,rY = Ex,a,r [ (f \u2212 V\u0302 fa \u2212 r(a))2 \u2212 (f? \u2212 V\u0302 f ? a \u2212 r(a))2 ]\n= Ex,a,r [ (f \u2212 V\u0302 fa )2 \u2212 2r(a)(f \u2212 V\u0302 fa \u2212 f? + V\u0302 f ? a )\u2212 (f? \u2212 V\u0302 f ? a ) 2 ]\nNow recall that E[r(a)|x, a] = f\u2217(x, a)\u2212 V f?a by the definition of f\u2217, which allows us to further obtain\nEx,a,rY = Ex,a [ (f \u2212 V\u0302 fa )2 \u2212 2(f? \u2212 V f ? a )(f \u2212 V\u0302 fa ) + 2(f? \u2212 V\u0302 f ? a + V\u0302 f? a \u2212 V f ? a )(f ? \u2212 V\u0302 f ? a )\u2212 (f? \u2212 V\u0302 f ? a ) 2 ]\n= Ex,a [ (f \u2212 V\u0302 fa )2 \u2212 2(f? \u2212 V f ? a )(f \u2212 V\u0302 fa ) + (f? \u2212 V\u0302 f ? a ) 2 + 2(V\u0302 f ? a \u2212 V f ? a )(f ? \u2212 V\u0302 f ? a ) ]\n= Ex,a [ (f \u2212 V\u0302 fa )2 \u2212 2(f? \u2212 V f ? a )(f \u2212 V\u0302 fa ) + (f? \u2212 V f ? a + V f? a \u2212 V\u0302 f ? a ) 2 + 2(V\u0302 f ? a \u2212 V f ? a )(f ? \u2212 V\u0302 f ? a ) ]\n= Ex,a [ (f \u2212 V\u0302 fa \u2212 f? + V f ? a ) 2 + 2(V f ? a \u2212 V\u0302 f ? a )(f ? \u2212 V f ? a ) + (V f? a \u2212 V\u0302 f ? a ) 2 + 2(V\u0302 f ? a \u2212 V f ? a )(f ? \u2212 V\u0302 f ? a ) ]\n= Ex,a [ (f \u2212 V\u0302 fa \u2212 f? + V f ? a ) 2 \u2212 (V f ? a \u2212 V\u0302 f ? a ) 2 ]\nFor the second claim, notice that we can write,\nY = (f \u2212 V\u0302 fa \u2212 f? + V\u0302 f ? a )(f \u2212 V\u0302 fa + g \u2212 V\u0302 f ? a \u2212 2r(a)),\nso that,\nY 2 \u2264 16(f \u2212 V\u0302 fa \u2212 f? + V\u0302 f ? a ) 2.\nThis holds because all quantities in the second term are bounded in [0, 1]. Therefore,\nVar(Y ) \u2264 E[Y 2] \u2264 16Ex,a [ (f(x, a)\u2212 V\u0302 fa \u2212 f?(x, a) + V\u0302 f ? a ) 2 ]\n= 16Ex,a [ (f(x, a)\u2212 V\u0302 fa \u2212 f?(x, a) + V f ? a + V\u0302 f? a \u2212 V f ? a ) 2 ]\n\u2264 32Ex,a [ (f(x, a)\u2212 V\u0302 fa \u2212 f?(x, a) + V f ? a ) 2 ] + 32\u03c62\n\u2264 32Ex,aY + 64\u03c62\nThe first inequality is straightforward, while the second inequality is from the argument above. The third inequality uses the fact that (a+ b)2 \u2264 2a2 + 2b2 and the fact that for each a, the estimate V\u0302 f?a has absolute error at most \u03c6 (By Condition 1). The last inequality adds and subtracts the term involving (V f ?\na \u2212 V\u0302 f ? a ) 2\nto obtain Ex,aY .\nThe next step is to relate the empirical squared loss to the population squared loss, which is done by application of Bernstein\u2019s inequality.\nLemma 2 (Squared Loss Deviation Bounds). Assume Conditions 1 and 2 hold. With probability at least 1\u2212 \u03b4/2, where \u03b4 is a parameter of the algorithm, f? survives the filtering step of Algorithm 3 and moreover, any surviving f satisfies,\nEY (f) \u2264 10\u03c62 + 120 log(2N/\u03b4) ntrain .\nProof. We will apply Bernstein\u2019s inequality on the centered random variable, ntrain\u2211 i=1 Yi(f)\u2212 EYi(f),\nand then take a union bound over all f \u2208 F . Here the expectation is over the ntrain samples (xi, ai, ri) where (xi, r) \u223c Dp, ai is chosen uniformly at random, and ri = r(ai). Notice that since actions are chosen uniformly at random, all terms in the sum are identically distributed, so that EYi(f) = EY (f).\nTo that end, fix one f \u2208 F and notice that |Y \u2212EY | \u2264 8 almost surely, as each quantity in the definition of Y is bounded in [0, 1], so each of the four terms can be at most 4, but two are non-positive and two are non-negative in Y \u2212 EY . We will use Lemma 1 to control the variance. Bernstein\u2019s inequality implies that, with probability at least 1\u2212 \u03b4,\nntrain\u2211 i=1 EYi \u2212 Yi \u2264 \u221a 2 \u2211 i Var(Yi) log(1/\u03b4) + 16 log(1/\u03b4) 3\n\u2264 \u221a 64 \u2211 i (E(Yi) + 2\u03c62) log(1/\u03b4) + 16 log(1/\u03b4) 3\nThe first inequality here is Bernstein\u2019s inequality while the second is based on the variance bound in Lemma 1. Now letting X = \u221a\u2211\ni(E(Yi) + 2\u03c6 2), Z = \u2211 i Yi and C = \u221a log(1/\u03b4), the inequality above is\nequivalent to,\nX2 \u2212 2ntrain\u03c62 \u2212 Z \u2264 8XC + 16\n3 C2\n\u21d2 X2 \u2212 8XC + 16C2 \u2212 Z \u2264 2ntrain\u03c62 + 22C2\n\u21d2 (X \u2212 4C2)\u2212 Z \u2264 2ntrain\u03c62 + 22C2\n\u21d2 \u2212Z \u2264 2ntrain\u03c62 + 22C2\nUsing the definition of \u2212Z, this last inequality implies that, ntrain\u2211 i=1 (f?(xi, ai)\u2212 ri(ai)\u2212 V\u0302 f ? (p \u25e6 ai))2 \u2264 ntrain\u2211 i=1 (f(xi, ai)\u2212 ri(ai)\u2212 V\u0302 f (p \u25e6 ai))2 + 2ntrain\u03c62 + 22 log(1/\u03b4)\nVia a union bound over all f \u2208 F , and rebinding \u03b4 \u2190 \u03b4/(2N), we have,\nR\u0303(f?) \u2264 min f\u2208F\nR\u0303(f) + 2\u03c62 + 22 log(N/\u03b4)\nntrain\nSince this is precisely the threshold used in filtering regressors, we ensure that f? survives. Now for any other surviving regressor f , we are ensured that Z is upper bounded. Specifically we have,\n(X \u2212 4C)2 \u2264 Z + 2ntrain\u03c62 + 22C2 \u2264 4ntrain\u03c62 + 44C2 \u21d2 X2 \u2264 ( \u221a 4ntrain\u03c62 + 44C2 + 4C) 2\n\u2264 8ntrain\u03c62 + 120C2\nThis proves the claim sinceX2 = ntrainEY (f)+2ntrain\u03c62 (Recall that the Yis are identically distributed).\nThis deviation bound allows us to establish the three claims in Theorem 2. We start with the estimation error claim, which is straightforward.\nLemma 3 (Estimation Error). Assume Conditions 1 and 2 hold. Let \u03b4 \u2208 (0, 1). Then with probablity at least 1\u2212 \u03b4, for all f \u2208 F that are retained by the Algorithm 3, we have estimates V\u0302 f (p, \u03c0f ) with,\n|V\u0302 f (p, \u03c0f )\u2212 V f (p, \u03c0f )| \u2264\n\u221a 2 log(N/\u03b4)\nntrain .\nProof. The proof is a consequence of Hoeffding\u2019s inequality and a union bound. Clearly the Monte Carlo estimate,\nV\u0302 f (p, \u03c0f ) = 1\nntrain ntrain\u2211 i=1 f(xi, \u03c0f (xi)),\nis unbiased for V f (p, \u03c0f ) and the centered quantity is bounded in [\u22121, 1]. Thus Hoeffding\u2019s inequality gives precisely the bound in the lemma.\nNext we turn to the claim regarding bias.\nLemma 4 (Bias Accumulation). Assume Conditions 1 and 2 hold. In the same 1\u2212 \u03b4 event in Lemma 2, for any pair f, g \u2208 F retained by Algorithm 3, we have,\nV f (p, \u03c0f )\u2212 V g(p, \u03c0g) \u2264 2 \u221a K \u221a 11\u03c62 + 120 log(N/\u03b4)\nntrain + 2\u03c6+ \u03c41\nProof. We start by expanding definitions,\nV f (p, \u03c0f )\u2212 V g(p, \u03c0g) = Ex\u223cDp [f(x, \u03c0f (x))\u2212 g(x, \u03c0g(x))]\nNow, since g prefers \u03c0g(x) to \u03c0f (x), it must be the case that g(x, \u03c0g(x)) \u2265 g(x, \u03c0f (x)), so that,\nV f (p, \u03c0f )\u2212 V g(p, \u03c0g) \u2264Ex\u223cDpf(x, \u03c0f (x))\u2212 g(x, \u03c0f (x)) = Ex\u223cDp [f(x, \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 f?(x, \u03c0f (x)) + V\u0302 f ?\n(p \u25e6 \u03c0f (x), \u03c0f?)] \u2212 Ex\u223cDp [g(x, \u03c0f (x))\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)\u2212 f?(x, \u03c0f (x)) + V\u0302 f ?\n(p \u25e6 \u03c0f (x), \u03c0f?)] + Ex\u223cDp [V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)]\nThis last equality is just based on adding and subtracting several terms. The first two terms look similar, and we will relate them to the squared loss. For the first, by Lemma 1, we have that for each x \u2208 X ,\nEr,a|x[Y (f)] + Ea|x[(V\u0302 f?(p \u25e6 a, \u03c0f?)\u2212 V f ? (p \u25e6 a, \u03c0f?))2] = Ea|x [ (f(x, a)\u2212 V\u0302 f (p \u25e6 a, \u03c0f )\u2212 f?(x, a) + V f ? (p \u25e6 a, \u03c0f?))2 ]\n\u2265 1 K\n[ (f(x, \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 f?(x, \u03c0f (x)) + V f ? (p \u25e6 \u03c0f (x), \u03c0f?))2 ]\nNow by Jensen\u2019s inequality the first term can be upper bounded as,\nEx\u223cDp [f(x, \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 f?(x, \u03c0f (x)) + V\u0302 f ? (p \u25e6 \u03c0f (x), \u03c0f?)] \u2264 \u221a\nEx\u223cDp [(f(x, \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 f?(x, \u03c0f (x)) + V\u0302 f ?(p \u25e6 \u03c0f (x), \u03c0f?))2]\n= \u221a KEx\u223cDp [ 1\nK (f(x, \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 f?(x, \u03c0f (x)) + V\u0302 f?(p \u25e6 \u03c0f (x), \u03c0f?))2 ] \u2264 \u221a K ( Ex,a,r[Y (f)] + Ex,a[(V\u0302 f ?(p \u25e6 a, \u03c0f?)\u2212 V f?(p \u25e6 a, \u03c0f?))2]\n) \u2264 \u221a K \u221a EY (f) + \u03c62\n\u2264 \u221a K \u221a 11\u03c62 + 120 log(N/\u03b4)\nntrain ,\nwhere the last step follows from Lemma 2. This bounds the first term in the expansion of V f (p, \u03c0f ) \u2212 V g(p, \u03c0g). Now for the term involving g, we can apply essentially the same argument,\n\u2212 Ex\u223cDp [g(x, \u03c0f (x))\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)\u2212 f?(x, \u03c0f (x)) + V\u0302 f ? (p \u25e6 \u03c0f (x), \u03c0f?)] \u2264 \u221a\nEx\u223cDp [(g(x, \u03c0f (x))\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)\u2212 f?(x, \u03c0f (x)) + V\u0302 f ?(p \u25e6 \u03c0f (x), \u03c0f?))2]\n\u2264 \u221a K \u221a 11\u03c62 + 120 log(N/\u03b4)\nntrain\nSummarizing, the current bound we have is,\nV f (p, \u03c0f )\u2212 V g(p, \u03c0g) \u2264 2 \u221a K \u221a 11\u03c62 + 120 log(N/\u03b4)\nntrain + Ex\u223cDp [V\u0302\nf (p \u25e6 \u03c0f (x), \u03c0f )\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)]\nThe last term is easily bounded by the preconditions in the statement of Theorem 2. For each a, we have,\nV\u0302 f (p \u25e6 a, \u03c0f )\u2212 V\u0302 g(p \u25e6 a, \u03c0g) \u2264 |V\u0302 f (p \u25e6 a, \u03c0f )\u2212 V f (p \u25e6 a, \u03c0f )|+ |V f (p \u25e6 a, \u03c0f )\u2212 V g(p \u25e6 a, \u03c0g)|+ |V g(p \u25e6 a, \u03c0g)\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)| \u2264 2\u03c6+ \u03c41,\nfrom Conditions 1 and 2. Consequently\nEx\u223cDp [V\u0302 f (p \u25e6 \u03c0f (x), \u03c0f )\u2212 V\u0302 g(p \u25e6 \u03c0f (x), \u03c0g)] = \u2211 a\u2208A Ex [ 1[\u03c0f (x) = a](V\u0302 f (p \u25e6 a, \u03c0f )\u2212 V\u0302 g(p \u25e6 a, \u03c0g)) ]\n\u2264 2\u03c6+ \u03c41\nThis proves the claim.\nLastly, we must show how the squared loss relates to the risk, which establishes the last claim of the Theorem. The proof is similar to that of the bias bound but has subtle differences that require reproducing the argument.\nLemma 5 (Regret Bound). Assume Conditions 1 and 2 hold. In the same 1 \u2212 \u03b4 event in Lemma 2, for any regressor f \u2208 F retained by Algorithm 3, we have,\nV f ? (p, \u03c0f?)\u2212 V f ? (p, \u03c0f ) \u2264 \u221a 2K \u221a 11\u03c62 + 120 log(N/\u03b4)\nntrain + 2(\u03c6+ \u03c41).\nProof.\nV f ? (p, \u03c0f?)\u2212 V f ? (p, \u03c0f ) = Ex[f ?(x, \u03c0f?(x))\u2212 f?(x, \u03c0f (x))]\n\u2264 Ex[f?(x, \u03c0f?(x))\u2212 f(x, \u03c0f?(x)) + f(x, \u03c0f (x))\u2212 f?(x, \u03c0f (x))]\nThis follows since f prefers its own action to that of f?, so that f(x, \u03c0f (x)) \u2265 f(x, \u03c0f?(x)). For any observation x \u2208 X and action a \u2208 A, define,\n\u2206x,a = (f(x, a)\u2212 V\u0302 f (p \u25e6 a, \u03c0f )\u2212 f?(x, a) + V f ? (p \u25e6 a, \u03c0f?))\nso that we can write,\nV f ? (p, \u03c0f?)\u2212 V f ? (p, \u03c0f )\n\u2264 Ex[\u2206x,\u03c0f (x) \u2212\u2206x,\u03c0f? (x) + (V\u0302 f (p \u25e6 \u03c0f (x))\u2212 V f\n? (p \u25e6 \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f?(x)) + V f ? (p \u25e6 \u03c0f?(x)))]\nThe term involving both \u2206s can be bounded as in the proof of Lemma 4. For any x \u2208 X\nEr,a|xY (f) + Ea|x[(V\u0302 f?(p \u25e6 a)\u2212 V f ? (p \u25e6 a))2] = Ea|x [ (f(x, a)\u2212 V\u0302 f (p \u25e6 a)\u2212 f?(x, a) + V f ? (p \u25e6 a))2 ]\n\u2265 \u22062x,\u03c0f (x) + \u2206 2 x,\u03c0f? (x)\nK \u2265\n(\u2206x,\u03c0f? (x) \u2212\u2206x,\u03c0f (x))2\n2K\nThus,\nEx[\u2206x,\u03c0f (x) \u2212\u2206x,\u03c0f? (x)] \u2264\n\u221a 2KE (\u2206x,\u03c0f (x) \u2212\u2206x,\u03c0f? (x))2\n2K\n\u2264 \u221a 2K \u221a EY (f) + \u03c62 \u2264 \u221a 2K \u221a 11\u03c62 + 120 log(N/\u03b4)\nntrain\nWe are left to bound the residual term,\n(V\u0302 f (p \u25e6 \u03c0f (x))\u2212 V f ? (p \u25e6 \u03c0f (x))\u2212 V\u0302 f (p \u25e6 \u03c0f?(x)) + V f ?\n(p \u25e6 \u03c0f?(x))) \u2264 (V f (p \u25e6 \u03c0f (x))\u2212 V f ? (p \u25e6 \u03c0f (x))\u2212 V f (p \u25e6 \u03c0f?(x)) + V f ?\n(p \u25e6 \u03c0f?(x))) + 2\u03c6 \u2264 2(\u03c6+ \u03c41)\nProof of Theorem 2: Equipped with the above Lemmas, we can proceed to prove the theorem. By assumption of the theorem, Conditions 1 and 2 hold, so all lemmas are applicable. Apply Lemma 3 with failure probability \u03b4/2, where \u03b4 is the parameter in the algorithm, and apply Lemma 2, which also fails with\nprobability at most \u03b4/2. A union bound over these two events implies that the failure probability of the algorithm is at most \u03b4.\nOutside of this failure probability event, all three of Lemmas 3, 4, and 5 hold. If we set ntrain = 24 log(2N/\u03b4)/\u03c62 then these three bounds give,\n|V\u0302 f (p, \u03c0f )\u2212 V f (p, \u03c0f )| \u2264 \u03c6\u221a 12 |V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 8\u03c6 \u221a K + 2\u03c6+ \u03c41\nV f ? (p, \u03c0f?)\u2212 V f ? (p, \u03c0f ) \u2264 4\u03c6 \u221a 2K + 2\u03c6+ 2\u03c41.\nThese bounds hold for all f, g \u2208 F that are retained by the algorithm. Of course by Lemma 2, we are also ensured that f? is retained by the algorithm. This proves the four claims in the theorem."}, {"heading": "B Proof of Theorem 3", "text": "This result is a straightforward application of Hoeffding\u2019s inequality. We collect ntest observations xi \u223c Dp by rolling into p and use the Monte Carlo estimates,\nV\u0302 f (p, \u03c0f ) = 1\nntest ntest\u2211 i=1 f(xi, \u03c0f (xi))\nBy Hoeffding\u2019s inequality, via a union bound over all f \u2208 F , we have that with probability at least 1\u2212 \u03b4,\n\u2223\u2223\u2223V\u0302 f (p, \u03c0f )\u2212 V f (p, \u03c0f )\u2223\u2223\u2223 \u2264 \u221a 2 log(2N/\u03b4)\nntest\nSetting ntest = 2 log(2N/\u03b4)/\u03c62, gives that our empirical estimates are at most \u03c6 away from the population versions.\nNow for the first claim, if the population versions are already within \u03c42 of each other, then the empirical versions are at most 2\u03c6+ \u03c42 apart by the triangle inequality,\n|V\u0302 f (p, \u03c0f )\u2212 V\u0302 g(p, \u03c0g)| \u2264 |V\u0302 f (p, \u03c0f )\u2212 V f (p, \u03c0f )|+ |V f (p, \u03c0f )\u2212 V g(p, \u03c0g)|+ |V g(p, \u03c0g)\u2212 V\u0302 g(p, \u03c0g)| \u2264 2\u03c6+ \u03c42.\nThis applies for any pair f, g \u2208 F whose population value predictions are within \u03c42 of each other. Since the threshold in Algorithm 2 is set to 2\u03c6+ \u03c42, this implies that the procedure returns true.\nFor the second claim, if the procedure returns true, then all empirical value predictions are at most test apart, so the population versions are at most 2\u03c6 + test apart, again by the triangle inequality. Specifically, for any pair f, g \u2208 F we have,\n|V f (p, \u03c0f )\u2212 V g(p, \u03c0g)| \u2264 |V f (p, \u03c0f )\u2212 V\u0302 f (p, \u03c0f )|+ |V\u0302 f (p, \u03c0f )\u2212 V\u0302 g(p, \u03c0g)|+ |V\u0302 g(p, \u03c0g)\u2212 V g(p, \u03c0g)| \u2264 2\u03c6+ test.\nBoth arguments apply for all pairs f, g \u2208 F , which proves the claim."}], "references": [{"title": "Online learning in mdps with side information", "author": ["Yasin Abbasi-Yadkori", "Gergely Neu"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Contextual bandit learning with predictable rewards", "author": ["Alekh Agarwal", "Miroslav Dud\u0131\u0301k", "Satyen Kale", "John Langford", "Robert E Schapire"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "R-max-a general polynomial time algorithm for nearoptimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Christoph Dann", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Abstraction selection in model-based reinforcement learning", "author": ["Nan Jiang", "Alex Kulesza", "Satinder Singh"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Model-based exploration in continuous state spaces", "author": ["Nicholas K Jong", "Peter Stone"], "venue": "In Abstraction, Reformulation, and Approximation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Sham Kakade", "John Langford"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Exploration in metric state spaces", "author": ["Sham Kakade", "Michael Kearns", "John Langford"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Approximate planning in large pomdps via reusable trajectories", "author": ["Michael J Kearns", "Yishay Mansour", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "A sparse sampling algorithm for near-optimal planning in large markov decision processes", "author": ["Michael J. Kearns", "Yishay Mansour", "Andrew Y. Ng"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Reducing reinforcement learning to kwik online regression", "author": ["Lihong Li", "Michael L Littman"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Towards a unified theory of state abstraction for mdps", "author": ["Lihong Li", "Thomas J Walsh", "Michael L Littman"], "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Predictive representations of state", "author": ["Michael L Littman", "Richard S Sutton", "Satinder P Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Reinforcement learning and mistake bounded algorithms", "author": ["Yishay Mansour"], "venue": "In Conference on Computational Learning Theory (COLT),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["Nicolas Meuleau", "Leonid Peshkin", "Kee-Eung Kim", "Leslie Pack Kaelbling"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "A convergent form of approximate policy iteration", "author": ["Theodore J Perkins", "Doina Precup"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Efficient pac learning for episodic tasks with acyclic state spaces", "author": ["Spyros Reveliotis", "Theologos Bountourelis"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Predictive state representations: A new theory for modeling dynamical systems. In Uncertainty in Artificial Intelligence (UAI)", "author": ["Satinder Singh", "Michael R James", "Matthew R Rudary"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Pac model-free reinforcement learning", "author": ["Alexander L Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L Littman"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}], "referenceMentions": [{"referenceID": 19, "context": "The Atari Reinforcement Learning research program [20] has highlighted a critical deficiency of reinforcement learning algorithms: they cannot effectively solve problems that require systematic exploration.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore? In RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24].", "startOffset": 193, "endOffset": 204}, {"referenceID": 3, "context": "How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore? In RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24].", "startOffset": 193, "endOffset": 204}, {"referenceID": 23, "context": "How can we construct Reinforcement Learning (RL) algorithms which effectively plan and plan to explore? In RL theory, this is an effectively solved problem for Markov Decision Processes (MDPs) [11, 4, 24].", "startOffset": 193, "endOffset": 204}, {"referenceID": 12, "context": "Approaches to RL with a weak dependence on these quantities exist [13], but suffer from an exponential dependence on the time horizon\u2014with K actions and a horizon of H , they require K samples.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "1 Basic Definitions An episodic Contextual-MDP is defined by the tuple (\u0393H ,\u0393, D) where H \u2208 N is the episode length, \u0393H \u2208 \u2206(SH) denotes a starting state distribution, \u0393 : (S \u00d7 A) \u2192 \u2206(S) denotes the transition dynamics, and D : S \u2192 \u2206(X \u00d7 [0, 1]) associates a distribution over (observation, reward) pairs with each state.", "startOffset": 237, "endOffset": 243}, {"referenceID": 0, "context": "We use Ds \u2208 \u2206(X \u00d7 [0, 1]) to denote the (observation, reward) distribution associated with state s and also the marginal distribution over observations (usage will be clear from context).", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "We assume that almost surely \u2211H h=1 rh(ah) \u2208 [0, 1] for any action sequence.", "startOffset": 45, "endOffset": 51}, {"referenceID": 13, "context": "3 Connections to Other Models Our model is closely related to several well-studied models in the literature, namely: Contextual Bandits: IfH = 1, then Contextual-MDPs reduce to stochastic contextual bandits [14, 6], a well-studied simplification of the general reinforcement learning problem.", "startOffset": 207, "endOffset": 214}, {"referenceID": 5, "context": "3 Connections to Other Models Our model is closely related to several well-studied models in the literature, namely: Contextual Bandits: IfH = 1, then Contextual-MDPs reduce to stochastic contextual bandits [14, 6], a well-studied simplification of the general reinforcement learning problem.", "startOffset": 207, "endOffset": 214}, {"referenceID": 10, "context": "MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 3, "context": "MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 23, "context": "MDPs with small state spaces can be efficiently solved by tabular approaches that maintain and update statistics about each state [11, 4, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 9, "context": "[10] (see also [8]) that has sample complexity independent of the number of unique states, but assumes the ability to cover the state space in a metric a priori known to the learner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[10] (see also [8]) that has sample complexity independent of the number of unique states, but assumes the ability to cover the state space in a metric a priori known to the learner.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "[13] also implies a sample complexity bound that is independent of the observation space size for episodic MDPs, but grows as O(K).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "More recently, Abbasi-Yadkori and Neu [1] propose a model for MDPs with side-information, but this model requires mapping side-information to a small-state MDP, rather than mapping an observation to an action as in Contextual-MDPs.", "startOffset": 38, "endOffset": 41}, {"referenceID": 24, "context": "However, these methods use local search techniques and consequently do not achieve global optimality [25, 9] in theory as well as empirically, unlike our algorithm which is guaranteed to find the globally optimal policy.", "startOffset": 101, "endOffset": 108}, {"referenceID": 8, "context": "However, these methods use local search techniques and consequently do not achieve global optimality [25, 9] in theory as well as empirically, unlike our algorithm which is guaranteed to find the globally optimal policy.", "startOffset": 101, "endOffset": 108}, {"referenceID": 18, "context": "While there are POMDP methods for learning reactive policies, or more generally policies with bounded memory [19], they are based on policy gradient techniques, which suffer both theoretical and empirical drawbacks as we mentioned.", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "There are some sample complexity guarantees for learning in arbitrarily complex POMDPs, but the bounds we are aware of are quite weak as they scale linearly with |\u03a0| [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 17, "context": "There are some sample complexity guarantees for learning in arbitrarily complex POMDPs, but the bounds we are aware of are quite weak as they scale linearly with |\u03a0| [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 16, "context": "Predictive State Representations (PSRs): PSRs [17] encode states as a a collection of tests, a test being a sequence of (a, x) pairs observed in the history.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "Representationally, PSRs are even more powerful than POMDPs [23] which make them also more general than Contextual-MDPs.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "4 Connections to Other Techniques State Abstraction: Our work is closely related to the literature on state abstraction (See [16] for a survey), which primarily focuses on understanding what optimality properties are preserved in an MDP after the state space is compressed.", "startOffset": 125, "endOffset": 129}, {"referenceID": 6, "context": "[7] which finds a good abstraction from a set of successively finer ones, but cannot search over the exponentially many abstractions functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Function approximation is the empirical state-of-the-art in reinforcement learning [20], but theoretical analysis has been quite limited.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Several authors have studied linear function approximation (See [26, 21]) but none of these results give finite sample bounds, as they do not address the exploration question.", "startOffset": 64, "endOffset": 72}, {"referenceID": 2, "context": "Baird [3] analyzes more general function approximation for predicting the value function in a Markov Chain, but does not show convergence when the agent is also selecting actions.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "More closely to our work, Li and Littman [15] do give finite sample bounds for RL with function approximation, but they assume access to a particular \u201cKnows-what-it-knows\u201d oracle, which cannot exist even for simple problems.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "We identify our set of policies \u03a0 with a set of regression functions F \u2282 (X \u00d7A)\u2192 [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "[2] for contextual bandit learning in the realizable setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Assumptions vary with the paper, but broadly prior results establish the sample complexity for learning layered episodic MDPs with deterministic transitions is \u00d5(MKpoly(H)/ ) [5, 22].", "startOffset": 175, "endOffset": 182}, {"referenceID": 21, "context": "Assumptions vary with the paper, but broadly prior results establish the sample complexity for learning layered episodic MDPs with deterministic transitions is \u00d5(MKpoly(H)/ ) [5, 22].", "startOffset": 175, "endOffset": 182}, {"referenceID": 1, "context": "[2] and shows that R\u0303(f) is close to its expectation, which implies that f\u2217 is never eliminated and all surviving regressors have small expected squared error.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We propose and study a new tractable model for reinforcement learning with high-dimensional observation called Contextual-MDPs, generalizing contextual bandits to a sequential decision making setting. These models require an agent to take actions based on high-dimensional observations (features) with the goal of achieving long-term performance competitive with a large set of policies. Since the size of the observation space is a primary obstacle to sample-efficient learning, Contextual-MDPs are assumed to be summarizable by a small number of hidden states. In this setting, we design a new reinforcement learning algorithm that engages in global exploration while using a function class to approximate future performance. We also establish a sample complexity guarantee for this algorithm, proving that it learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. This represents an exponential improvement on the sample complexity of all existing alternative approaches and provides theoretical justification for reinforcement learning with function approximation.", "creator": "LaTeX with hyperref package"}}}