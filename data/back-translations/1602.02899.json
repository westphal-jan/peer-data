{"id": "1602.02899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data", "abstract": "The success of these classification methods depends on the effectiveness of the learning methods.The classification algorithm of the Extreme Learning Machine (ELM) is a relatively new learning method based on an upstream neural network.The ELM classification algorithm is a simple and fast way to build a model out of high-dimensional datasets.The traditional ELM learning algorithm implicitly requires full access to the entire dataset.This is a major data protection problem in most cases. Disclosure of private data (e.g. medical records) is prevented due to security considerations.In this research, we propose an efficient and secure data protection learning algorithm for the ELM classification, which is split vertically between several party.The new learning method preserves privacy on numerical attributes, builds a classification model without passing on the data of each party to others.", "histories": [["v1", "Tue, 9 Feb 2016 08:37:26 GMT  (431kb)", "http://arxiv.org/abs/1602.02899v1", "22nd International Conference, ICONIP 2015"]], "COMMENTS": "22nd International Conference, ICONIP 2015", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["ferhat \\\"ozg\\\"ur \\c{c}atak"], "accepted": false, "id": "1602.02899"}, "pdf": {"name": "1602.02899.pdf", "metadata": {"source": "CRF", "title": "Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data", "authors": ["Ferhat \u00d6zg\u00fcr \u00c7atak"], "emails": ["ozgur.catak@tubitak.gov.tr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 89\n9v 1\n[ cs\n.C R\n] 9\nKeywords: extreme learning machine, privacy preserving data analysis, secure multi-party computation"}, {"heading": "1 Introduction", "text": "The main purpose of machine learning can be expressed as to find the patterns and summarize the data form of high-dimensional data sets. The classification algorithms [1,2] is one of the most widely used method of machine learning in real-life problems. Data sets used in real-life problems are high dimensional as a result, analysis of them is a complicated process.\nExtreme Learning Machine (ELM) was proposed by [3] based on generalized Single-hidden Layer Feed-forward Networks (SLFNs). Main characteristics of ELM are small training time compared to traditional gradient-based learning methods, high generalization property on predicting unseen examples with multiclass labels and parameter free with randomly generated hidden nodes.\nBackground knowledge attack uses quasi-identifier attributes of a dataset and reduce the possible values of output sensitive information. A well-known\nexample about background knowledge attack is the personal health information about of Massachusetts governor William Weld using a anonymized data set [4]. In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].\nAlthough the anonymization methods are applied to data sets to protect sensitive data, though, the sensitive data is still accessed by an attacker in various ways [7]. Also, data anonymization methods are not applicable in some cases. In another scenario, consider the situation when two or more hospitals wants to analyze patient data [8] through collaborative processes that require using each other\u2019s databases. In such cases, it is necessary to find a secure training method that can run jointly on private union databases, without revealing or pooling their sensitive data. Privacy-preserving ELM learning systems are one of the methods that the only information learned by the different parties is the output model of learning method.\nIn this research, we propose a privacy-preserving ELM training model that constructs the global ELM classification model from the distributed data sets in multiple parties. The training data set is vertically partitioned among the parties, and the final distributed model is constructed at an independent party to securely predict the correct label for the new input data.\nThe content of this paper is as follows: Related work is reviewed in Section 2. In Section 3, ELM, secure multi-party computation and the secure addition are explained. In section 4, our new privacy-preserving ELM for vertically partitioned data is proposed. Section 5 emprically shows the timing results of our method with different public data sets."}, {"heading": "2 Related Works", "text": "In this section, we review the existing works that have been developed for different machine learning methods. The major differences between our learning model and existing work are highlighted.\nRecently, there has been significant contributions in privacy-preserving machine learning. Secretans et al. [9] presents a probabilistic neural network (PNN) model. The PNN is an approximation of the theoretically optimal classifier, known as the Bayesian optimal classifier. There are at least three parties involved in the computation of the secure matrix summation to add the partial class conditional probability vectors together. Aggarwal et al. [10] developed condensation based learning method. They show that an anonymized data closely matches the characteristics of the original data. Samet et al. [11] present new privacy-preserving protocols for both the back-propagation and ELM algorithms among several parties. The protocols are presented for perceptron learning algorithm and applied only single layer models. Oliveria et al. [12] proposed methods distort confidential numerical features to protect privacy for clustering analysis. Guang et al. [13] proposed a privacy-preserving back-propagation algorithm for horizontally partitioned databases for multi-party case. They use secure sum in their protocols. Yu et al. [14] proposed a privacy-preserving solution for support\nvector machine classification. Their approach constructs the global SVM classification model from the data distributed at multiple parties, without disclosing the data of each party to others."}, {"heading": "3 Preliminaries", "text": "In this section, we introduce preliminary knowledge of ELM, secure multi-party computation and secure addition briefly."}, {"heading": "3.1 Extreme learning machine", "text": "ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] . Then, ELM was extended to the generalized single-hidden layer feed-forward networks where the hidden layer may not be neuron like [17,18]. Main advantages of ELM classification algorithm is that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [19,20]. The most noticeable feature of ELM is that its hidden layer parameters are selected randomly.\nGiven a set of training data D = {(xi, yi) | i = 1, ..., n},xi \u2208 R p, yi \u2208 {1, 2, ...,K}} sampled independently and identically distributed (i.i.d.) from some unknown distribution. The goal of a neural network is to learn a function f : X \u2192 Y where X is instances and Y is the set of all possible labels. The output label of an single hidden-layer feed-forward neural networks (SLFNs) with N hidden nodes can be described as\nfN (x) = N \u2211\ni=1\n\u03b2iG(ai, bi,x), x \u2208 R n, ai \u2208 R n (1)\nwhere ai and bi are the learning parameters of hidden nodes and \u03b2i is the weight connecting the ith hidden node to the output node. The output function of ELM for generalized SLFNs can be identified by\nfN (x) =\nN \u2211\ni=1\n\u03b2iG(ai, bi,x) = \u03b2 \u00d7 h(x) (2)\nFor the binary classification applications, the decision function of ELM becomes\nfN(x) = sign\n(\nN \u2211\ni=1\n\u03b2iG(ai, bi,x)\n)\n= sign (\u03b2 \u00d7 h(x)) (3)\nEquation 2 can be written in another form as\nH\u03b2 = T (4)\nH and T are respectively hidden layer matrix and output matrix.\n\u03b2 = H\u2020T (5)\nH\u2020 is the Moore-Penrose generalized inverse of matrix H. Hidden layer matrix can be described as\nH(a\u0303, b\u0303, x\u0303) =\n\n  G(a1, b1, x1) \u00b7 \u00b7 \u00b7 G(aL, bL, x1) ... . . . ...\nG(a1, b1, xN ) \u00b7 \u00b7 \u00b7 G(aL, bL, xN )\n\n \nN\u00d7L\n(6)\nwhere a\u0303 = a1, ..., aL, b\u0303 = b1, ..., bL, x\u0303 = x1, ..., xN . Output matrix can be described as\nT = [ t1 . . . tN ]T\n(7)\nThe hidden nodes of SLFNs can be randomly generated. They can be independent of the training data."}, {"heading": "3.2 Secure Multi Party Computation", "text": "In vertically partitioned data, each party holds different attributes of same data set. Let\u2019s have n input instances, D = {(xi, yi) | xi \u2208 R p, yi \u2208 R} n\ni=1. The partition strategy is shown in Figure 1.\nSecure Multi-Party Addition In secure multi-party addition (SMA), each party, Pi, has a private local value, xi. At the end of the computation, we obtain the sum, x = \u2211k\u22121\ni=0 . For this works, we applied the Yu et al. [21] secure addition procedure. Their approach is a generalization of the existing works [22] that uses secure communication and trusted party. Canonical order based , P0, \u00b7 \u00b7 \u00b7 , Pk\u22121, protocol is applied. The SMA method is show in Algorithm 1. This protocol calculates the required sum in secure manner.\nAlgorithm 1 Secure multi-party addition\n1: procedure SMA(P) 2: P0 : R\u2190 rand(F) \u22b2 P0 randomly chooses a number R 3: V \u2190 R + x0 mod F 4: P0 sends V to node P1 5: for i = 1, \u00b7 \u00b7 \u00b7 , k \u2212 1 do 6: Pi receives V = R + \u2211i\u22121\nj=0 xj mod F\n7: Pi computes V = ( R + \u2211i\nj=1 xj mod F\n)\n= ((xi + V ) mod F)\n8: Pi sends V to node Pi+1 9: end for 10: P0 : V \u2190 (V \u2212R) = (V \u2212R mod F) \u22b2 Actual addition result 11: end procedure"}, {"heading": "4 Privacy-preserving ELM over vertically partitioned data", "text": "The data set that one wants to find a classifier for consists of m instances in n-dimensional space is shown with D \u2208 Rm\u00d7n. Each instances of the data set has values for n features. Matrix D is vertically partitioned into i parties of P0, P1, \u00b7 \u00b7 \u00b7 , Pi and each features of instances owned by a party that is private shown as Figure 1 illustrates. As shown in Equation 6, at second stage of ELM learning, hidden layer output matrix, H is calculated using randomly assigned hidden node parameters w, b and \u03b2. ELM calculates the matrix H and output weight vector, \u03b2, is obtained by multiplying H and T. Each member of H is computed with an activation function g such that G(wi,xi, bi) = g(xi \u00b7wi + bi) for sigmoid or G(wi,xi, bi) = g(bi \u2212 ||xi \u2212wi||) for radial based functions. An (i, j)th element of H is\nG(wi,xi, bi) = sign(xi \u00b7wi + bi) (8)\nwhere xi is the ith instances of data set and wi is hidden node input weight of ith instance and xi,wi \u2208 R\nn. Let x1i , \u00b7 \u00b7 \u00b7x k i be vertically partitioned vectors of input instance xi andw 1 j , \u00b7 \u00b7 \u00b7w k j\nbe vertically partitioned vectors of jth hidden node input weight wj , b 0 j , \u00b7 \u00b7 \u00b7 , b k j be jth node input bias over k different parties. Then the output of jth input node with ith instance of input data set using k sites is\nsign(xi \u00b7wj + bj) = sign (( x0i \u00b7w 0 j + b 0 j) + \u00b7 \u00b7 \u00b7+ (x k j \u00b7w k j + b k j ))\n(9)\nFrom Equation 9, calculation of hidden layer output matrix, H can be decomposed into k different parties using secure sum of matrices, such that\nH = sign (T1 + \u00b7 \u00b7 \u00b7+Tk) (10)\nwhere\nTi =\n\n \n(\nxi1 \u00b7w i 1 + b i 1\n) \u00b7 \u00b7 \u00b7 (\nxi1 \u00b7w i L + b i L\n)\n... . . . ... (\nxiN \u00b7w i 1 + b i 1\n) \u00b7 \u00b7 \u00b7 (\nxiL \u00b7w i N + b i N\n)\n\n \nN\u00d7L\n(11)\nPrivacy Preserving ELM Algorithm Let D \u2208 RM\u00d7N , and the number of input layer size be L, the number of parties be k, then the our training model becomes:\n1. Master party creates weight matrix, W \u2208 RL\u00d7N 2. Master party distributes partition W with same feature size for each parties.\n3. Party P0 creates a random matrix, R =\n\n  rand1,1(F) \u00b7 \u00b7 \u00b7 rand1,L(F) ... . . . ...\nrandN,1(F) \u00b7 \u00b7 \u00b7 randN,L(F)\n\n \nN\u00d7L\n4. Party P0 creates perturbated output,V = R+\n\n \n(\nx01 \u00b7w 0 1 + b 0 1\n) \u00b7 \u00b7 \u00b7 (\nx01 \u00b7w 0 L + b 0 L\n)\n... . . . ... (\nx0N \u00b7w 0 1 + b 0 1\n) \u00b7 \u00b7 \u00b7 (\nx0N \u00b7w 0 L + b 0 L\n)\n\n \n5. for i = 1, \u00b7 \u00b7 \u00b7 , k \u2212 1\n\u2013 Pi computes V = V +\n\n \n(\nxi1 \u00b7w i 1 + b i 1\n) \u00b7 \u00b7 \u00b7 (\nxi1 \u00b7w i L + b i L\n)\n... . . . ... (\nxiN \u00b7w i 1 + b i 1\n) \u00b7 \u00b7 \u00b7 (\nxiL \u00b7w i N + b i N\n)\n\n \n\u2013 Pi sends V to Pi+1 6. P0 subtracts random matrix, R, from the received matrix V. H = (V \u2212R)\nmod F\n7. Hidden layer node weight vector, \u03b2, is calculated. \u03b2 = H\u2020 \u00b7T"}, {"heading": "5 Experiments", "text": "In this section, we perform experiments on real-world data sets from the public available data set repositories. Public data sets are used to evaluate the proposed learning method. Classification models of each data set are compared for accuracy results without using secure multi-party computation.\nExperimental setup : In this section, our approach is applied to six different data sets to verify model affectivity and efficiency. The data sets are summarized in Table 1, including australian, colon-cancer, diabetes, duke, heart, ionosphere. For each data set in Table 1, we vary number of party size, k from 2 to number\nof feature, n, of the data set. For instance, when our party size is three, k = 3, and attribute size fourteen, n = 14, then the first two party have 5 attributes, and last party has 4 attributes.\nSimulation Results: The accuracy of secure multi-party computation based ELM is exactly same for the traditional ELM training algorithm. Figure 2 shows results of our simulations. As shown in figure, time scale becomes its steady state position when number of parties, k, moves closer to number of attributes, k."}, {"heading": "6 Conclusion and Future Works", "text": "ELM learning algorithm, a new method compared to other classification algorithms. ELM outperforms traditional Single Layer Feed-forward Neural-networks and Support Vector Machines for big data [29]. The ELM is applied in many fields. Almost, in all fields that ELM is applied (i.e. medical records, business, government), privacy is a major concern.\nA new privacy-preserving learning model is proposed for ELM in vertically partitioned data in multi-party partitioning without sharing the data of each site to the others. In order to save the privacy of input data set, master party divides weight vector, and each party calculates the activation function result with its data and weight vector. Extending the privacy-preserving ELM to horizontally distributed data set is a future work for this approach."}], "references": [{"title": "Machine learning: An artificial intelligence approach", "author": ["J.R. Anderson", "R.S. Michalski", "J.G. Carbonell", "T.M. Mitchell"], "venue": "Volume 2. Morgan Kaufmann", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Database management systems", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": "Osborne/McGrawHill", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Extreme learning machine: Theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing 70(13)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "k-anonymity: A model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10(05)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "L-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "D. Kifer", "J. Gehrke", "M. Venkitasubramaniam"], "venue": "ACM Trans. Knowl. Discov. Data 1(1)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["N. Li", "T. Li", "S. Venkatasubramanian"], "venue": "Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Differential privacy and machine learning: a survey and review", "author": ["Z. Ji", "Z.C. Lipton", "C. Elkan"], "venue": "arXiv preprint arXiv:1412.7584", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Secure multiparty computation for privacy-preserving data mining", "author": ["Y. Lindell", "B. Pinkas"], "venue": "Journal of Privacy and Confidentiality 1(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A privacy preserving probabilistic neural network for horizontally partitioned databases", "author": ["J. Secretan", "M. Georgiopoulos", "J. Castro"], "venue": "Neural Networks, 2007. IJCNN 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A condensation approach to privacy preserving data mining", "author": ["C. Aggarwal", "P. Yu"], "venue": "In Bertino, E., Christodoulakis, S., Plexousakis, D., Christophides, V., Koubarakis, M., Bhm, K., Ferrari, E., eds.: Advances in Database Technology - EDBT 2004. Volume 2992 of Lecture Notes in Computer Science. Springer Berlin Heidelberg", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Privacy-preserving back-propagation and extreme learning machine algorithms", "author": ["S. Samet", "A. Miri"], "venue": "Data Knowl. Eng. 79-80", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Privacy preserving clustering by data transformation", "author": ["S.R. Oliveira", "O.R. Zaiane"], "venue": "Journal of Information and Data Management 1(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A privacy preserving neural network learning algorithm for horizontally partitioned databases", "author": ["L. Guang", "W. Ya-Dong", "S. Xiao-Hong"], "venue": "Inform. Technol. J 9", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Privacy-preserving svm using nonlinear kernels on horizontally partitioned data", "author": ["H. Yu", "X. Jiang", "J. Vaidya"], "venue": "Proceedings of the 2006 ACM Symposium on Applied Computing. SAC \u201906, New York, NY, USA, ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks", "author": ["G. bin Huang", "Q. yu Zhu", "C. kheong Siew"], "venue": "IN PROC. INT. JOINT CONF. NEURAL NETW.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.B. Huang", "L. Chen", "C.K. Siew"], "venue": "Neural Networks, IEEE Transactions on 17(4)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing 70(1618)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhanced random search based incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing 71(1618)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressed-domain ship detection on spaceborne optical image using deep neural network and extreme learning machine", "author": ["J. Tang", "C. Deng", "G.B. Huang", "B. Zhao"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 53(3)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental extreme learning machine with fully complex hidden nodes", "author": ["G.B. Huang", "M.B. Li", "L. Chen", "C.K. Siew"], "venue": "Neurocomputing 71(46)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Privacy-preserving svm classification on vertically partitioned data", "author": ["H. Yu", "J. Vaidya", "X. Jiang"], "venue": "Advances in Knowledge Discovery and Data Mining. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiparty computation for randomly ordering players and making random selections", "author": ["L. Sweeney", "M. Shamos"], "venue": "Technical report, Carnegie Mellon University", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "International journal of man-machine studies 27(3)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1987}, {"title": "Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays", "author": ["U. Alon", "N. Barkai", "D.A. Notterman", "K. Gish", "S. Ybarra", "D. Mack", "A.J. Levine"], "venue": "Proceedings of the National Academy of Sciences 96(12)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Using the adap learning algorithm to forecast the onset of diabetes mellitus", "author": ["J.W. Smith", "J. Everhart", "W. Dickson", "W. Knowler", "R. Johannes"], "venue": "Proceedings of the Annual Symposium on Computer Application in Medical Care, American Medical Informatics Association", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1988}, {"title": "Predicting the clinical status of human breast cancer by using gene expression profiles", "author": ["M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J.A. Olson", "J.R. Marks", "J.R. Nevins"], "venue": "Proceedings of the National Academy of Sciences 98(20)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Statlog (heart) data set", "author": ["UCI"], "venue": "https://archive.ics.uci.edu/ml/datasets/Statlog+(Heart)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Classification of radar returns from the ionosphere using neural networks", "author": ["V.G. Sigillito", "S.P. Wing", "L.V. Hutton", "K.B. Baker"], "venue": "Johns Hopkins APL Technical Digest 10", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1989}, {"title": "Extreme learning machines [trends controversies", "author": ["E. Cambria", "Huang", "G.B.e.a."], "venue": "Intelligent Systems, IEEE 28(6)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The classification algorithms [1,2] is one of the most widely used method of machine learning in real-life problems.", "startOffset": 30, "endOffset": 35}, {"referenceID": 1, "context": "The classification algorithms [1,2] is one of the most widely used method of machine learning in real-life problems.", "startOffset": 30, "endOffset": 35}, {"referenceID": 2, "context": "Extreme Learning Machine (ELM) was proposed by [3] based on generalized Single-hidden Layer Feed-forward Networks (SLFNs).", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "example about background knowledge attack is the personal health information about of Massachusetts governor William Weld using a anonymized data set [4].", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "Although the anonymization methods are applied to data sets to protect sensitive data, though, the sensitive data is still accessed by an attacker in various ways [7].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "In another scenario, consider the situation when two or more hospitals wants to analyze patient data [8] through collaborative processes that require using each other\u2019s databases.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "[9] presents a probabilistic neural network (PNN) model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] developed condensation based learning method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] present new privacy-preserving protocols for both the back-propagation and ELM algorithms among several parties.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposed methods distort confidential numerical features to protect privacy for clustering analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a privacy-preserving back-propagation algorithm for horizontally partitioned databases for multi-party case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed a privacy-preserving solution for support", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] .", "startOffset": 112, "endOffset": 121}, {"referenceID": 15, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] .", "startOffset": 112, "endOffset": 121}, {"referenceID": 2, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] .", "startOffset": 112, "endOffset": 121}, {"referenceID": 16, "context": "Then, ELM was extended to the generalized single-hidden layer feed-forward networks where the hidden layer may not be neuron like [17,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 17, "context": "Then, ELM was extended to the generalized single-hidden layer feed-forward networks where the hidden layer may not be neuron like [17,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 18, "context": "Main advantages of ELM classification algorithm is that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [19,20].", "startOffset": 320, "endOffset": 327}, {"referenceID": 19, "context": "Main advantages of ELM classification algorithm is that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [19,20].", "startOffset": 320, "endOffset": 327}, {"referenceID": 20, "context": "[21] secure addition procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Their approach is a generalization of the existing works [22] that uses secure communication and trusted party.", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 100, "endOffset": 104}, {"referenceID": 25, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 154, "endOffset": 158}, {"referenceID": 27, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 179, "endOffset": 183}], "year": 2016, "abstractText": "Especially in the Big Data era, the usage of different classification methods is increasing day by day. The success of these classification methods depends on the effectiveness of learning methods. Extreme learning machine (ELM) classification algorithm is a relatively new learning method built on feed-forward neural-network. ELM classification algorithm is a simple and fast method that can create a model from high-dimensional data sets. Traditional ELM learning algorithm implicitly assumes complete access to whole data set. This is a major privacy concern in most of cases. Sharing of private data (i.e. medical records) is prevented because of security concerns. In this research, we propose an efficient and secure privacy-preserving learning algorithm for ELM classification over data that is vertically partitioned among several parties. The new learning method preserves the privacy on numerical attributes, builds a classification model without sharing private data without disclosing the data of each party to others.", "creator": "LaTeX with hyperref package"}}}