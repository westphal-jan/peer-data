{"id": "1602.03265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Simple Search Algorithms on Semantic Networks Learned from Language Use", "abstract": "An interesting interplay arises between the richness of representations in semantic memory and the complexity of the algorithms required to process it. It remains an open question whether representations of words and their relationships, learned from linguistic usage, can enable a simple search algorithm to mimic the observed behavior in the fluid task. At this point, we show that it is plausible to learn rich representations from naturalistic data, for which a very simple search algorithm (a random path) can replicate human patterns. We suggest that explicit structuring of knowledge of words into a semantic network plays a crucial role in modeling human behavior in memory search and recovery; this is also the case in a number of semantic information sources.", "histories": [["v1", "Wed, 10 Feb 2016 04:54:15 GMT  (183kb,D)", "https://arxiv.org/abs/1602.03265v1", null], ["v2", "Thu, 11 Feb 2016 04:46:21 GMT  (172kb,D)", "http://arxiv.org/abs/1602.03265v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aida nematzadeh", "filip miscevic", "suzanne stevenson"], "accepted": false, "id": "1602.03265"}, "pdf": {"name": "1602.03265.pdf", "metadata": {"source": "CRF", "title": "Simple Search Algorithms on Semantic Networks Learned from Language Use", "authors": ["Aida Nematzadeh", "Filip Miscevic", "Suzanne Stevenson"], "emails": ["aida@cs.toronto.edu", "miscevic@cs.toronto.edu", "suzanne@cs.toronto.edu"], "sections": [{"heading": null, "text": "Keywords: semantic networks; semantic search; semantic memory; computational modeling"}, {"heading": "Introduction", "text": "Semantic memory plays a significant role in cognition because it is the locus of storage for concepts and their relations. There are a number of competing hypotheses for the representation of semantic memory, such as semantic networks (e.g., Collins & Loftus, 1975; Steyvers & Tenenbaum, 2005), vector space models (e.g., Landauer & Dumais, 1997), and topic models (Griffiths, Steyvers, & Tenenbaum, 2007). The content and structure of semantic memory is of great interest because it impacts how effectively people can store, search for, and retrieve information.\nRecent work in computational modeling has illustrated in an interesting way the trade-off between the representation of semantic memory and the nature of the algorithms required to process it (Hills, Jones, & Todd, 2012; Abbott, Austerweil, & Griffiths, 2015). The models in question focused on the semantic fluency task, in which people name as many members of a cue category as they can in a certain amount of time. This task is informative about representation and processing of semantic memory because it requires people to access semantically-related words. Based on their empirical data in such a task, Hills et al. (2012) argue that people follow an optimal foraging pattern that is similar to animals searching for food: a semantic patch is exploited until the rate of word retrieval is less than the long-term average rate of retrieval, and then a new patch of related words is explored.\nHills et al. (2012) and Abbott et al. (2015) suggest that very different computational approaches are required to model this empirical behavior. Hills et al. (2012) adopted a vector space representation of semantic memory \u2013 one that encodes word\u2013 word co-occurrence patterns. Using this representation, they\nshowed that the best match to human behavior required a twostage algorithm with an explicit strategy to switch from exploiting the current semantic patch to exploring a new patch. In contrast, Abbott et al. (2015) showed that a simple random walk that operates uniformly was sufficient to model the pattern of behavior. To achieve this, their model used a semantic network representation that encoded relations among words from free association norms. These results clearly demonstrate the interplay of representation and algorithm in replicating the same empirical data on semantic memory.\nHaving a semantic memory that is appropriately structured to support efficient real-time access might constitute a good balance in the representation/process trade-off. But people must learn such a structure. Creating a semantic network by directly encoding human association norms, as Abbott et al. (2015) do, avoids the statistical learning problem that people face (Jones, Hills, & Todd, 2015). It has thus remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task.\nOur first contribution here is to show that this is indeed possible: we create a semantic network using learned meanings of words from a cognitively plausible computational model, and show that a simple, uniform random walk exhibits the observed foraging pattern of search. Moreover, we also show that if an explicit semantic network is created from the vectorspace semantic information of Hills et al. (2012), the same random walk algorithm on that network shows the desired match with human behavior. We thus conclude that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling observed behavior in memory search and retrieval; moreover, this is the case across a range of semantic information sources (not solely in the case of free association data). We also perform structural analyses of the networks to consider the relation between their connectivity properties and their behavior."}, {"heading": "Semantic Fluency Data and Models", "text": "Hills et al. (2012) argue that search through semantic memory is guided by the same strategy as that used by animals foraging for food. In support of this view, they found that participant responses in a semantic fluency task (i.e., \u2018name as many animals as you can in 3 minutes\u2019) came in bursts of semantically related \u201cpatches\u201d (animal categories as defined by Troyer, Moscovitch, and Winocur (1997), such as \u2018pets\u2019 or \u2018farm animals\u2019). Moreover, the timing of these responses was consistent with the marginal value theorem of optimal foraging in physical space (Charnov, 1976). Specifically, the time\nar X\niv :1\n60 2.\n03 26\n5v 2\n[ cs\n.C L\n] 1\n1 Fe\nb 20\n16\nit took for participants to retrieve the next novel item relative to the last one \u2013 referred to as the inter-item retrieval time (IRT) \u2013 increased with each item within a patch. When the IRT exceeded the participant\u2019s average IRT across the entire trial, a switch into a different patch of semantically-related words occurred, and the IRT then decreased. This pattern can be seen in Figure 1a in the Results section.\nHills et al. (2012) investigated the ability of different search algorithms to model this empirical data, using semantic representations of words learned by a vector space model, BEAGLE, on the Wikipedia corpus (Jones & Mewhort, 2007). They show that a two-stage algorithm best replicates the data, using local cues (word\u2013word similarity) to find the next item within a patch, along with an explicit strategy to switch to a global cue (word frequency) to guide exploration of a new patch. Moreover, they showed that a simpler search algorithm \u2013 a random walk that used only the word\u2013word similarities \u2013 could not capture the observed foraging pattern.\nIn contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al. to replicate the empirical data.\nIn the remainder of the paper, we explore whether a structured representation that results in a simpler search and retrieval algorithm can be learned from the kind of data that people are naturally exposed to. Similarly to Abbott et al. (2015) and in contrast to Hills et al. (2012), we construct a semantic network to explicitly encode the appropriate relations among words. However, unlike Abbott et al., our model learns these relations from a language corpus rather than simply encoding human association norms. Moreover, unlike Hills et al., we use a corpus of child-directed speech to reflect more naturalistic language input, and use a semantic representation that explicitly draws on conceptual knowledge."}, {"heading": "Our Semantic Representation", "text": "We briefly review our computational word learner, then describe the process for constructing semantic networks."}, {"heading": "The Word Learner", "text": "We use an incremental and probabilistic cross-situational learner shown to mimic a range of child and adult behaviors in vocabulary learning (Fazly, Alishahi, & Stevenson, 2010). The model takes as input a sequence of utterance\u2013scene pairs, U\u2013S, where U represents the linguistic input to a child, and S represents the non-linguistic data a child perceives in language learning. The input is highly ambiguous, as the mapping between individual words in U and the relevant semantics in S is not explicitly indicated \u2013 U is represented as a set of words, and S as a set of semantic features:\nU : {crocodile, float, in, the, river} S: { . . . , REPTILE, VERTEBRATE, . . . , BODY-OF-WATER, . . .}\nFrom such input, the model uses an incremental version of expectation-maximization to learn a probability distribution P (.|w) for each word w over all observed features.\nThe utterances in the input are taken from a corpus of childdirected speech. To create the associated scene representations, each word in the corpus is entered into a gold-standard lexicon. (This lexicon is never seen by the model.) Each word in the lexicon has a set of semantic features representing its gold-standard meaning. The features for each animal word (and nouns in general) are the names of each ancestor node (hypernym) of the word\u2019s first sense in WordNet1. Each noun is thus represented by definitional features that reflect conceptual knowledge: general features such as OBJECT, which appear with many words, and more specific features such as REPTILE, which appear with fewer words. For example:\ncrocodile: { CROCODILIAN REPTILE, DIAPSID, REPTILE, VERTEBRATE, \u00b7 \u00b7 \u00b7 , WHOLE, OBJECT, \u00b7 \u00b7 \u00b7 }\nScene S for utterance U is formed by taking the union of the gold-standard semantic features for all words in U . Thus the semantic input to the model represents naturalistic features that are distributed realistically across related entities, and reflect a conceptual hierarchy intended to approximate the type of conceptual categories children are forming.\nAn interesting property of the learner is that the learned meaning probabilities for a word w, P (f |w) for observed features f , reflects not only the co-occurrences of w with its gold-standard features: The probabilities importantly capture the influence of contextual features in the input as well. For example, crocodile and hippopotamus will be distinguished by high probabilities for the definitional features P (REPTILE|crocodile) and P (MAMMAL|hippopotamus), but are both likely to have a higher than chance value for the feature BODY-OF-WATER since both animals live in rivers. Thus the learned semantic representation in the model captures both definitional and contextual similarities of words."}, {"heading": "Constructing a Semantic Network", "text": "Other recent research has used free-association norms or conceptual hierarchies like WordNet as the basis for a semantic network; two words are connected by an edge in the network if there is a direct connection between them in the representation (Steyvers & Tenenbaum, 2005; Abbott et al., 2015). By contrast, for our meaning representation (as for BEAGLE data), the appropriate network connections among the words must be determined by considering how related any pair of words is in that representation (since all words are implicitly more or less related). We follow Nematzadeh, Fazly, and Stevenson (2014b) in their approach to creating a semantic network over our model\u2019s learned meaning representations. Since we aim to model the empirical data from Hills et al. (2012) that looked at semantic fluency in the category of animals, we focus on the subset of words in our training data that\n1http://wordnet.princeton.edu\noccur in the dataset of Hills et al. (2012). Each such word is represented as a node in the network, and pairs of nodes are connected if the cosine similarity of their associated meaning probability vectors exceeds a certain threshold \u03c4 . The meaning similarity serves as the weight on an inserted edge.\nWe experiment with various values for the edge-threshold \u03c4 , and at higher values, the resulting network becomes somewhat disconnected: groupings of very similar words form sets of connected components, usually animals of a similar subcategory (e.g. \u2018farm animals\u2019 or \u2018pets\u2019). This reflects the fine-grained differences in word meaning that the learner has acquired. Because these learned representations do not completely capture taxonomic knowledge \u2013 i.e., that \u2018animal\u2019 is a subsuming category of those groupings naturally occurring in the network \u2013 we treat the word animal differently in deciding on its network connections.2 Specifically, we use a lower threshold, \u03c4a, to determine when to add edges including the node for animal. This ensures that animal is connected to a number of the groupings of animals, and increases the connectivity of the network. (Future work will look at mechanisms as in Nematzadeh et al. (2015) for adequately capturing the meanings of hierarchically organized entities.)\nThe resulting graph may not be fully connected. Since the fluency task starts with the cue word animal and can only reach nodes that are directly or indirectly connected to it, we take the semantic network for our purposes to be the connected component of the graph that includes animal. The number of nodes in the semantic network may thus be smaller than the number of observed animal words."}, {"heading": "Experimental Methods", "text": ""}, {"heading": "The Semantic Networks", "text": "The child-directed speech that forms the basis for the input to our word learner is the Manchester corpus (Theakston, Lieven, Pine, & Rowland, 2001) of CHILDES (MacWhinney, 2000). Of the 518 unique animals classified by Hills et al. (2012) using the categories described by Troyer et al. (1997), 111 of these are present in the full corpus and thus in our gold standard lexicon. However, only 93 of these appear in the 481K-word corpus (120K utterances) we use for training. Thus, a semantic network of learned meanings \u2013 called a Learner network \u2013 will have a maximum of 94 nodes (93 words from the animal subcategories plus animal itself).\nRecall that the learned representations from our model reflect both definitional and contextual aspects of word meaning; this contextualization of meaning has been shown to influence the structure of resulting semantic networks (Nematzadeh et al., 2014b). For comparison, we create \u201cgold-standard\u201d semantic networks, called Gold, whose edge connections are determined using the gold-standard (definitional) meanings rather than the learned meanings. These networks enable us to see the impact of having the hierarchi-\n2The calculation of model probabilities P (f |w) entails that general features like ANIMAL (that many words share) have lower probability than specific features that distinguish the words.\ncal semantics from WordNet without the contextually learned aspects of meaning. The Gold networks have a maximum of 112 nodes (111 animal terms+animal).\nFinally, we used the same method to create BEAGLE semantic networks using the data reported by Hills et al. (2012). BEAGLE consists of word co-occurrence data that encodes contextualized meanings; however, some hierarchical conceptual knowledge is reflected in the 400M-word Wikipedia corpus it was trained on. The BEAGLE data contains 364 animal words that appear in Hills et al. (2012), and thus these networks have a maximum of 365 nodes.\nFor all semantic networks, we use cosine similarity as the (potential) edge weights, and consider various levels of the thresholds \u03c4a (for edges that include the word animal) and \u03c4 (for all other edges) on these weights for inclusion of edges.3"}, {"heading": "Simulating Behavior with Random Walks", "text": "Our goal is to see whether the structure of our semantic networks is sufficient to obtain the observed foraging behavior using a simple, uniform search algorithm. To that end, we perform random walks with variations as discussed by Abbott et al. (2015). Each random walk begins at the word animal to simulate the fact that animal is the cue for the fluency task (i.e., \u201cname as many animals as you can\u201d). Each step in a random walk \u2013 i.e., the move from the current node nc to the next node nn \u2013 is determined by a probabilistic selection over the edges incident on nc. The selection process may choose the edge to follow in proportion to the edge weights (a weighted walk), or use a uniform distribution over all edges connected to nc (an unweighted walk). (A further variation in which there is a probability p of jumping back to the word animal after any step in the random walk had no appreciable impact on our results, so we do not report that method here.) Due to the probabilistic nature of the algorithm (in selecting edges to traverse), we report results averaged over 282 random walks for each network under parameter settings of interest.\nTo reflect the time limit in the semantic fluency task, Abbott et al. (2015) fix the number of steps in the random walks to produce approximately the same number of words as human participants. Because this walk length is dependent on properties of the graph being traversed, Abbott et al. set this for each network, using walk lengths of 45 with the BEAGLE data and 2000 on their own semantic network. We take an alternative approach: Instead of picking one walk length to produce a certain number of words, we explore the interaction of different walk lengths with parameters of the networks to see which combinations lead to an appropriate number of words produced. We aim for a range of number of words produced around that of people \u2013 i.e., 37\u00b1 5.\nEvaluating IRTs and Patch Switches In assessing the fit of the random walks to human data, we use the same mapping of steps in the walk to the IRT as used\n3Our code and data are available at https://github.com/ FilipMiscevic/random walk.git.\nby Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.e., the IRT is the number of steps in the walk between a node ni the first time it is visited and the next node nj in the walk that has not been previously visited. Any nodes revisited between such an ni and nj increase the IRT between them.\nPatch switches occur in the fluency task when participants switch from listing animals in one subcategory (such as \u2018farm animals\u2019) to another (such as \u2018pets\u2019). Motivated by findings in Hills, Todd, and Jones (2009), we use a \u201cfluid patch model\u201d with the Troyer et al. (1997) categories of animals in analyzing our results. This approach takes into account that animals may belong to multiple categories: a patch switch is considered to have occurred whenever the current novel word and the next novel word in the walk do not have some category in common. Patch switches are used to determine the patch entry position in analyzing the match of the random walks to human data (e.g., \u201c1\u201d represents the first item in a patch; \u201c-1\u201d is the last word before a patch switch; see Figure 1).\nTo assess whether our networks match the human IRT pattern, we consider specific thresholds for the ratio of the IRT at certain points to the overall mean IRT of the random walk. For the patch entry point (1), this ratio for the human data is around 1.2 (cf. Figure 1a); we consider a minimum threshold 1.1 as achieving a fit, with a stricter ratio of 1.2 indicating a better match to human data.4 For the IRT at position 2, where there is a decrease following the patch switch, we similarly set a maximum threshold of 0.80 of the mean IRT over all. For all other positions, the ratio of IRT to mean IRT must be less than or equal to 1.0. We report walks as matching human data when they meet all these thresholds (and note when the stricter of the patch entry thresholds is met)."}, {"heading": "Experimental Results", "text": ""}, {"heading": "Parameter Search and Selection", "text": "Several parameters influence both the number of words produced in a random walk on our networks, and the precise pattern of IRTs and patch switches. The thresholds \u03c4 and \u03c4a used in determining the edges to include in the networks (for non-animal and animal nodes, respectively) affect both how connected the network is and the actual pattern of connectivity (e.g., all over loosely connected, or a disjoint set of connected components). For example, having fewer edges does not necessarily lead to less connectivity, but might increase the path length between a given pair of words.\nSimilarly, the number of steps the random walk is allowed \u2013 the \u201cwalk length\u201d L \u2013 clearly influences the number of words produced, but it affects the patterning as well. For example, longer walks have more opportunity to explore more subcategories of words, which can affect the patch switching. Also, a longer walk does not necessarily mean that more\n4Hills et al. (2012) note that to mimic foraging the value simply needs to be higher than the average IRT.\nwords are produced \u2013 it might simply raise the IRTs by spending more time revisiting nodes.\nGiven that the structure of the network and the random walk length interact to produce both a certain number of words and a given IRT pattern, we perform a parameter search over pairs of reasonable values of the edge threshold \u03c4 and the walk length L. (We fix \u03c4a for connecting animal at 0.40, which we found to give good results across all networks.) We vary \u03c4 in increments of 0.05, with the range chosen for each type of network based on preliminary experimentation. We vary L from 35 (the approximate number of words produced by people) to 135 (within which all networks showed humanlike behavior for some value of \u03c4 ).\nWe search over \u03c4 \u00d7 L to find the combinations that yield walks over Learner, Gold, and BEAGLE that match the human pattern of responses. In particular, we looked for parameters that: (i) produce a range of number of words similar to that of people, and (ii) produce an IRT pattern that matches that of people (as detailed in Methods). Instead of simply finding one parameter combination that achieves these goals, we consider the number of such walks across a range of parameter settings to indicate the robustness of an approach to semantic representation."}, {"heading": "Overall Patterns Observed", "text": "Generally \u03c4 and L work together to produce the desired output patterns \u2013 i.e., the higher \u03c4 \u2019s need higher L\u2019s to produce the right number of words. We select a range of four \u03c4 values (Learner [.70\u2013.85], Gold [.75\u2013.90], BEAGLE [.40\u2013.55]) and nine settings of L (60\u2013100) that exhibit the best performance in showing human behavior (as in (i) and (ii) above). This yields a set of 36 walks in each of the weighted and unweighted settings to analyze; see Table 1.\nOverall, BEAGLE performs somewhat better with weighted walks and our networks somewhat better with unweighted walks. The high \u03c4 in our networks means edge weights have a small range and are thus very similar \u2013 i.e., they are not much more informative than picking uniformly. Also, BEAGLE is trained on a corpus over 800 times the size of ours, so our Learner weights may simply be noisier.\nWe find that the best performance for BEAGLE (weighted) only matches the target human pattern for 19% of the walks; the best for Gold does so for 33% and the Learner for 64% (both unweighted). Even with weighted walks, our Learner achieves the pattern in 44% of the walks, over twice the number of BEAGLE. We believe that our learned representations, which encode both conceptual knowledge from\nWordNet coupled with contextual influences from corpus cooccurrences, more robustly reflect the nature of the similarity relations among words for this task. Thus, Learner performs better than both Gold and BEAGLE that each only (primarily) capture one of these types of knowledge.\nInterestingly, we get these patterns with walk lengths in the range of 60\u2013100, where Abbott et al. (2015) used lengths of 2000 to produce words at the rate of people. Perhaps word co-occurrence data more directly captures relations amongst a wide variety of words compared to the association norms of their data. Future analysis of their network compared to ours may reveal why their walks apparently revisit nodes much more frequently."}, {"heading": "Comparing Best Results", "text": "To look more closely at specific patterns, we compared the networks under the best \u03c4 parameter for each (Learner: 0.80, Gold: 0.85, BEAGLE: 0.50), with the full range of L = 35 \u2212 135; see Table 2. For these settings, we found all networks did the same or slightly better using a weighted walk compared to unweighted. All networks perform very similarly, with the primary difference that the Learner network matches the human target behavior in more walks. Moreover, both Gold and Learner meet the stricter IRT ratio of 1.2 in most cases of weighted walks, while BEAGLE only meets the less strict ratio of 1.1. See Figure 1 for the results of a sample walk (L = 95 [Learner], 85 [Gold], 80 [BEAGLE]).\nIn summary, human-like IRT patterns were observed for random walks on each of the three networks. Importantly, this includes random walks using the BEAGLE data, which Hills et al. (2012) previously showed could not produce such a pattern when used directly. This demonstrates that creating a semantic network from the BEAGLE representation imposes important structure on the raw co-occurrence data, helping the network to focus on meaningful word\u2013word connections. Moreover, the fact that our Learner network shows a very good match to human behavior demonstrates that appropriate representations for a semantic network can be acquired by a cognitively-plausible word learner."}, {"heading": "Analyzing the Structure and Semantics of Networks", "text": "Previous research suggests that a small-world network \u2013 a sparse graph with highly-connected sub-networks organized around \u201chubs\u201d \u2013 enables efficient access to semantic information (Steyvers & Tenenbaum, 2005). The idea is similar to foraging: first the hubs are explored, and then a new subnetwork connecting to the matched hub is exploited. Indeed,\na semantic network created from the association norms used by Abbott et al. (2015) has been shown to have a small-world structure (Steyvers & Tenenbaum, 2005).\nAs in Nematzadeh et al. (2014b), we calculate a \u201csmallworldness\u201d score (\u03c3) for each of our networks, using wellknown graph metrics; when \u03c3 > 1, the network conforms to a small-world structure. See Table 3 for the best networks as in Figure 1. We find that all the networks that exhibit the target IRT pattern have a small-world structure; in other words, a small-world structure may be necessary in producing the human pattern. However, having a small-world structure is not sufficient: most of the networks under the wide range of parameter settings we examined have small-world structure, but not all exhibit the foraging behavior.\nWe observe that an appropriate graph structure on its own cannot guarantee efficient search and retrieval: For that, the content of the sub-networks need to appropriately link semantically-related words. Indeed, Abbott et al. (2015) also find that their network captures appropriate groupings of animals. We considered whether our networks also reflect the structure of animal subcategories. For the Learner and Gold networks, we can do this by removing the animal node and its edges (which we added as the cue word for the random walk), and then labeling each connected component of the network\nwith the most frequently occurring category from Troyer et al. (1997). We take a mean of precision and recall for each such cluster, weighted by its size, and compute the F-score (see Table 3). Although not all subcategories of animals are connected to each other (lower recall), the sub-networks have mostly animals from the same subcategory (high average precision), supporting the observed human-like patch switching.\nUnfortunately, the networks from BEAGLE do not form such connected components, making this approach to clustering analysis inappropriate. We note here that Abbott et al. (2015) claim the BEAGLE data shows only a \u201cweak signature of animal clusters\u201d. We also observe that the small-worldness value is overall larger in our networks than that of BEAGLE; these properties of BEAGLE networks may explain why they do not perform as robustly as our networks in replicating the behavioral data."}, {"heading": "Discussion and Future Work", "text": "There is an interesting interplay between the richness of representations in semantic memory and the complexity of algorithms required to process it. We show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) is enough to replicate the patterns observed in people. Two key factors play a role in the success of our approach: (1) Our learned representations capture the hierarchical relations among words as well as their contextual similarities. (2) We explicitly impose a structure onto our learned representations by creating a semantic network in which words are connected only if their similarity exceeds a certain threshold.\nOur work builds on recent research by Hills et al. (2012) and Abbott et al. (2015) in which different representation\u2013 algorithm pairs (vectors of co-occurrence statistics and strategic search vs. association norms and random search) replicate the same behavioral data from a fluency task: people name animal words from a subcategory (e.g., pets) until their rate of retrieval is less than the long-term average rate of retrieval, and then they switch to a new subcategory (e.g., farm animals). Importantly, our approach has the advantage that our representations are learned from naturalistic language learning data. Although here we created the semantic networks using the final learned representations of the model, these networks can also be acquired incrementally during word learning (Nematzadeh et al., 2014a).\nWe further demonstrate that a random walk on a semantic network created from the vector representations of Hills et al. (2012) can produce the observed human pattern. This shows that the co-occurrence statistics learned from a large corpus encodes the required semantic information; however, the explicit structure of a semantic network is needed to simplify the search process. Moreover, our analysis reveals that to replicate the behavioral data all semantic networks (using the various representations) need to have certain connectivity properties \u2013 i.e., they consist of highly-connected components, and most nodes are reachable from other nodes via\nrelatively short paths."}], "references": [{"title": "Random walks on semantic networks can resemble optimal foraging", "author": ["J.T. Abbott", "J.L. Austerweil", "T.L. Griffiths"], "venue": "Psyc. Rev.,", "citeRegEx": "Abbott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abbott et al\\.", "year": 2015}, {"title": "Optimal foraging, the marginal value theorem", "author": ["E.L. Charnov"], "venue": "Theoretical Population Biology,", "citeRegEx": "Charnov,? \\Q1976\\E", "shortCiteRegEx": "Charnov", "year": 1976}, {"title": "A spreading-activation theory of semantic processing", "author": ["A.M. Collins", "E.F. Loftus"], "venue": "Psyc. Rev.,", "citeRegEx": "Collins and Loftus,? \\Q1975\\E", "shortCiteRegEx": "Collins and Loftus", "year": 1975}, {"title": "A probabilistic computational model of cross-situational word learning", "author": ["A. Fazly", "A. Alishahi", "S. Stevenson"], "venue": "Cog. Sci.,", "citeRegEx": "Fazly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fazly et al\\.", "year": 2010}, {"title": "Optimal foraging in semantic memory", "author": ["T.T. Hills", "M.N. Jones", "P.M. Todd"], "venue": "Psyc. Rev.,", "citeRegEx": "Hills et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hills et al\\.", "year": 2012}, {"title": "Optimal foraging in semantic memory", "author": ["T.T. Hills", "P.M. Todd", "M.N. Jones"], "venue": "In CogSci Proceedings", "citeRegEx": "Hills et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hills et al\\.", "year": 2009}, {"title": "Hidden processes in structural representations: A reply", "author": ["M.N. Jones", "T.T. Hills", "P.M. Todd"], "venue": null, "citeRegEx": "Jones et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2015}, {"title": "Representing word meaning and order information in a composite holographic lexicon", "author": ["M.N. Jones", "D.J. Mewhort"], "venue": "Psyc. Rev.,", "citeRegEx": "Jones and Mewhort,? \\Q2007\\E", "shortCiteRegEx": "Jones and Mewhort", "year": 2007}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psyc. Rev.,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "The CHILDES project: Tools for analyzing talk", "author": ["B. MacWhinney"], "venue": "(3rd ed.,", "citeRegEx": "MacWhinney,? \\Q2000\\E", "shortCiteRegEx": "MacWhinney", "year": 2000}, {"title": "The University of South Florida free association, rhyme, and word fragment norms", "author": ["D.L. Nelson", "C.L. McEvoy", "T.A. Schreiber"], "venue": null, "citeRegEx": "Nelson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Nelson et al\\.", "year": 1998}, {"title": "A cognitive model of semantic network learning", "author": ["A. Nematzadeh", "A. Fazly", "S. Stevenson"], "venue": "In Proceed. Conf. on Empirical Methods in Natural Lang. Processing", "citeRegEx": "Nematzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2014}, {"title": "Structural differences in the semantic networks of simulated word learners", "author": ["A. Nematzadeh", "A. Fazly", "S. Stevenson"], "venue": "In CogSci Proceedings (pp. 1072\u20131077)", "citeRegEx": "Nematzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2014}, {"title": "A computational cognitive model of novel word generalization", "author": ["A. Nematzadeh", "E. Grant", "S. Stevenson"], "venue": "In Proceed. Conf. on Empirical Methods in Natural Lang. Processing (pp. 1795\u20131804)", "citeRegEx": "Nematzadeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2015}, {"title": "The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth", "author": ["M. Steyvers", "J.B. Tenenbaum"], "venue": "Cog. Sci.,", "citeRegEx": "Steyvers and Tenenbaum,? \\Q2005\\E", "shortCiteRegEx": "Steyvers and Tenenbaum", "year": 2005}, {"title": "The role of performance limitations in the acquisition of verb\u2013argument structure: An alternative account", "author": ["A.L. Theakston", "E.V. Lieven", "J.M. Pine", "C.F. Rowland"], "venue": "Journal of Child Language,", "citeRegEx": "Theakston et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Theakston et al\\.", "year": 2001}, {"title": "Clustering and switching as two components of verbal fluency: Evidence from younger and older healthy adults", "author": ["A.K. Troyer", "M. Moscovitch", "G. Winocur"], "venue": "Neuropsychology,", "citeRegEx": "Troyer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Troyer et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 4, "context": "Based on their empirical data in such a task, Hills et al. (2012) argue that people follow an optimal foraging pattern that is similar to animals searching for food: a semantic patch is exploited until the rate of word retrieval is less than the long-term average rate of re-", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "(2012) and Abbott et al. (2015) suggest that very", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "Hills et al. (2012) adopted a vector space representation of semantic memory \u2013 one that encodes word\u2013 word co-occurrence patterns.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk that operates uniformly was sufficient to model the pattern of behavior.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "Creating a semantic network by directly encoding human association norms, as Abbott et al. (2015) do, avoids the statistical learning problem that people face (Jones, Hills, & Todd, 2015).", "startOffset": 77, "endOffset": 98}, {"referenceID": 4, "context": "Moreover, we also show that if an explicit semantic network is created from the vectorspace semantic information of Hills et al. (2012), the same random walk algorithm on that network shows the desired match with human behavior.", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": "Moreover, the timing of these responses was consistent with the marginal value theorem of optimal foraging in physical space (Charnov, 1976).", "startOffset": 125, "endOffset": 140}, {"referenceID": 3, "context": "Hills et al. (2012) investigated the ability of different search algorithms to model this empirical data, using semantic representations of words learned by a vector space model, BEAGLE, on the Wikipedia corpus (Jones & Mewhort, 2007).", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998).", "startOffset": 13, "endOffset": 187}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al.", "startOffset": 13, "endOffset": 326}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al. to replicate the empirical data. In the remainder of the paper, we explore whether a structured representation that results in a simpler search and retrieval algorithm can be learned from the kind of data that people are naturally exposed to. Similarly to Abbott et al. (2015) and in contrast to Hills et al.", "startOffset": 13, "endOffset": 825}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al. to replicate the empirical data. In the remainder of the paper, we explore whether a structured representation that results in a simpler search and retrieval algorithm can be learned from the kind of data that people are naturally exposed to. Similarly to Abbott et al. (2015) and in contrast to Hills et al. (2012), we construct a semantic network to explicitly encode the appropriate relations among words.", "startOffset": 13, "endOffset": 864}, {"referenceID": 0, "context": "Other recent research has used free-association norms or conceptual hierarchies like WordNet as the basis for a semantic network; two words are connected by an edge in the network if there is a direct connection between them in the representation (Steyvers & Tenenbaum, 2005; Abbott et al., 2015).", "startOffset": 247, "endOffset": 296}, {"referenceID": 4, "context": "Since we aim to model the empirical data from Hills et al. (2012) that looked at semantic fluency in the category of animals, we focus on the subset of words in our training data that", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "occur in the dataset of Hills et al. (2012). Each such word is represented as a node in the network, and pairs of nodes are connected if the cosine similarity of their associated meaning probability vectors exceeds a certain threshold \u03c4 .", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": "(Future work will look at mechanisms as in Nematzadeh et al. (2015) for adequately capturing the meanings of hierarchically organized entities.", "startOffset": 43, "endOffset": 68}, {"referenceID": 9, "context": "The child-directed speech that forms the basis for the input to our word learner is the Manchester corpus (Theakston, Lieven, Pine, & Rowland, 2001) of CHILDES (MacWhinney, 2000).", "startOffset": 160, "endOffset": 178}, {"referenceID": 4, "context": "Of the 518 unique animals classified by Hills et al. (2012) using the categories described by Troyer et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 4, "context": "Of the 518 unique animals classified by Hills et al. (2012) using the categories described by Troyer et al. (1997), 111 of these are present in the full corpus and thus in our gold standard lexicon.", "startOffset": 40, "endOffset": 115}, {"referenceID": 4, "context": "Finally, we used the same method to create BEAGLE semantic networks using the data reported by Hills et al. (2012).", "startOffset": 95, "endOffset": 115}, {"referenceID": 4, "context": "The BEAGLE data contains 364 animal words that appear in Hills et al. (2012), and thus these networks have a maximum of 365 nodes.", "startOffset": 57, "endOffset": 77}, {"referenceID": 0, "context": "To that end, we perform random walks with variations as discussed by Abbott et al. (2015). Each random walk begins at the word animal to simulate the fact that animal is the cue for the fluency task (i.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": "To reflect the time limit in the semantic fluency task, Abbott et al. (2015) fix the number of steps in the random walks to produce approximately the same number of words", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "by Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "by Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.e., the IRT is the number of steps in the walk between a node ni the first time it is visited and the next node nj in the walk that has not been previously visited. Any nodes revisited between such an ni and nj increase the IRT between them. Patch switches occur in the fluency task when participants switch from listing animals in one subcategory (such as \u2018farm animals\u2019) to another (such as \u2018pets\u2019). Motivated by findings in Hills, Todd, and Jones (2009), we use a \u201cfluid patch model\u201d with the Troyer et al.", "startOffset": 3, "endOffset": 655}, {"referenceID": 0, "context": "by Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.e., the IRT is the number of steps in the walk between a node ni the first time it is visited and the next node nj in the walk that has not been previously visited. Any nodes revisited between such an ni and nj increase the IRT between them. Patch switches occur in the fluency task when participants switch from listing animals in one subcategory (such as \u2018farm animals\u2019) to another (such as \u2018pets\u2019). Motivated by findings in Hills, Todd, and Jones (2009), we use a \u201cfluid patch model\u201d with the Troyer et al. (1997) categories of animals in analyzing our results.", "startOffset": 3, "endOffset": 715}, {"referenceID": 0, "context": "Interestingly, we get these patterns with walk lengths in the range of 60\u2013100, where Abbott et al. (2015) used lengths of 2000 to produce words at the rate of people.", "startOffset": 85, "endOffset": 106}, {"referenceID": 4, "context": "Importantly, this includes random walks using the BEAGLE data, which Hills et al. (2012) previously showed could not produce such a pattern when used directly.", "startOffset": 69, "endOffset": 89}, {"referenceID": 4, "context": "Figure 1: (a) Human IRTs reproduced from Hills et al. (2012). (b\u2013d) Modeling IRTs in weighted random walks using the parameters described in Comparing Best Results.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "a semantic network created from the association norms used by Abbott et al. (2015) has been shown to have a small-world structure (Steyvers & Tenenbaum, 2005).", "startOffset": 62, "endOffset": 83}, {"referenceID": 11, "context": "As in Nematzadeh et al. (2014b), we calculate a \u201csmallworldness\u201d score (\u03c3) for each of our networks, using wellknown graph metrics; when \u03c3 > 1, the network conforms to a small-world structure.", "startOffset": 6, "endOffset": 32}, {"referenceID": 0, "context": "Indeed, Abbott et al. (2015) also find that their network captures appropriate groupings of animals.", "startOffset": 8, "endOffset": 29}, {"referenceID": 16, "context": "with the most frequently occurring category from Troyer et al. (1997). We take a mean of precision and recall for each such cluster, weighted by its size, and compute the F-score (see Table 3).", "startOffset": 49, "endOffset": 70}, {"referenceID": 0, "context": "We note here that Abbott et al. (2015) claim the BEAGLE data shows only a \u201cweak signature of animal clusters\u201d.", "startOffset": 18, "endOffset": 39}, {"referenceID": 3, "context": "Our work builds on recent research by Hills et al. (2012) and Abbott et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": "(2012) and Abbott et al. (2015) in which different representation\u2013 algorithm pairs (vectors of co-occurrence statistics and strategic search vs.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "We further demonstrate that a random walk on a semantic network created from the vector representations of Hills et al. (2012) can produce the observed human pattern.", "startOffset": 107, "endOffset": 127}], "year": 2016, "abstractText": "Recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory. An interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it. It has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task. Here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) can replicate the human patterns. We suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval; moreover, this is the case across a range of semantic information sources.", "creator": "TeX"}}}