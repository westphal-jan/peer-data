{"id": "1602.04278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Signer-independent Fingerspelling Recognition with Deep Neural Network Adaptation", "abstract": "We are investigating the problem of recognizing finger letter sequences in the American sign language in a sign-independent environment. Finger letter sequences are both challenging and important to detect as they are used for many substantive words such as correct nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracy in finger letter recognition in a sign-dependent environment. However, the more realistic sign-independent environment presents challenges due to significant differences between signers coupled with the lack of available training data. We are investigating this problem with approaches inspired by automatic speech recognition. We are starting with the most powerful approaches from previous work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network classifiers of letters (DNN) and phonological characteristics. Using DNs matching, we determine that a gap is possible between the independent word bridge and a large part of the word bridge.", "histories": [["v1", "Sat, 13 Feb 2016 03:30:34 GMT  (148kb,D)", "http://arxiv.org/abs/1602.04278v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.NE", "authors": ["taehwan kim", "weiran wang", "hao tang", "karen livescu"], "accepted": false, "id": "1602.04278"}, "pdf": {"name": "1602.04278.pdf", "metadata": {"source": "CRF", "title": "SIGNER-INDEPENDENT FINGERSPELLING RECOGNITION WITH DEEP NEURAL NETWORK ADAPTATION", "authors": ["Taehwan Kim", "Weiran Wang", "Hao Tang", "Karen Livescu"], "emails": ["taehwan@ttic.edu", "weiranwang@ttic.edu", "haotang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014 American Sign Language, fingerspelling, deep neural network, adaptation, segmental CRF\n1. INTRODUCTION\nAutomatic sign language recognition is a nascent technology that has the potential to improve the ability of Deaf and hearing individuals to communicate, as well as Deaf individuals\u2019 ability to take full advantage of modern information technology. For example, online sign language video blogs and news1 are currently almost completely unindexed and unsearchable as they include little accompanying annotation.\nResearch on this problem has included both speechinspired approaches and computer vision-based techniques, using either/both video and depth sensor input [1, 2, 3, 4, 5, 6,\n\u2217This research was supported by NSF grant NSF-1433485. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.\n1E.g., http://ideafnews.com, http://aslized.org.\n7, 8, 9]. We focus on recognition from video, for applicability to existing recordings. Before the technology can be applied \u201cin the wild\u201d, it must overcome challenges posed by visual nuisance parameters (e.g., lighting, occlusions) and signer variation. Annotated data sets for this problem are scarce, in part due to the need to recruit signers and skilled annotators.\nWe consider American Sign Language (ASL), in particular the fingerspelling component: the spelling out of a word as a sequence of handshapes or hand trajectories corresponding to individual letters. Fig. 1 gives example fingerspelling sequences. Fingerspelling accounts for roughly 12-35% of ASL [10] and is typically used for proper nouns or borrowings from English, which can often be the most important content words. Some aspects of fingerspelling can be characterized through the phonology of handshape [11, 12], which can be described in terms of phonological features. Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19]. For the unconstrained letter sequence recognition problem, Kim et al. [7, 8] obtained \u223c 90% average letter accuracies in a signer-dependent setting, using either tandem hidden Markov models (HMMs) or segmental conditional random fields (SCRFs), with features from neural network classifiers of letters and phonological features. That work used the largest video data set of which we are aware containing unconstrained, connected fingerspelling, consisting of four signers each signing 600 word tokens for a total of \u223c 350k image frames.\nIn this paper we consider the problem of signer-independence in unconstrained fingerspelling sequence recognition, in the context of limited training data. Prior work has address signer adaptation for large-vocabulary German Sign Language recognition [9], but to our knowledge this paper is the first to address adaptation for fingerspelling. We investigate approaches to signer-independence including speed normalization and neural network adaptation. The adaptation techniques are largely borrowed from speech recognition research, but the application is quite different in that the overall amount of data is much smaller and the types of variation are different. We find that the simple signer normalization is ineffective, while DNN adaptation is very effective.\nar X\niv :1\n60 2.\n04 27\n8v 1\n[ cs\n.C L\n] 1\n3 Fe\nb 20\n16\n2. METHODS\nThe task is to convert a video (a sequence of images), as in Fig. 1, to a sequence of letters. The segmentation into letters is unknown, so this is a sequence prediction task analogous to connected phone or word recognition. We start with the recognition approaches that have achieved the best prior results on this task [7, 8], with updates for improved performance with deeper neural networks. We next briefly describe the recognizers, the neural network classifiers and adaptation."}, {"heading": "2.1. Recognizers", "text": "The first recognizer is a tandem model [20] based on [7]. Frame-level features are fed to neural network classifiers, one of which predicts the frame\u2019s letter label and six others which predict handshape phonological features.2 Classifier outputs are concatenated with the image features, after a dimensionality reduction, and input to a hidden Markov model (HMM) recognizer with Gaussian mixture observation densities.\nThe second recognizer is a segmental CRF (SCRF) model based on [8]. SCRFs [21, 22] are conditional log-linear models with feature functions that can be based on variablelength segments of input frames, allowing for great flexibility in defining feature functions. As in [8], we use an SCRF to rescore lattices produced by a baseline frame-based recognizer (in this case, the tandem model). We use the same feature functions as in [8], which include language model features, a feature that measures agreement with the baseline recognizer, means of letter/phonological feature neural network classifier outputs over each segment, and \u201cpeak detection\u201d features that measure the dynamics of each segment.\nFinally, we also use a first-pass decoding SCRF from Tang et al. [23], which is independent of any frame-based recognizer. We use the same feature functions as in [23], namely average DNN outputs over each segment, samples of DNN outputs within the segment, duration and bias, all lexicalized.\n2See [11, 12, 7] for details of the phonological features."}, {"heading": "2.2. DNN adaptation", "text": "The DNNs are first trained in a signer-independent way on all but the test signer, using an L2-regularized cross-entropy loss. The inputs are the image features concatenated over a multiframe window, which are fed through several fully connected layers followed by a softmax output layer. Inspection of data such as Fig. 1 reveals the main sources of signer variation: speed, hand appearance, and non-signing motion variation before/after signing. The speed variation is large, with a factor of 1.8 between the fastest and slowest signers. In the absence of adaptation data, we consider a simple speed normalization: We augment the training data with resampled image features, at 0.8x and 1.2x the original frame rate.\nIf we have access to some labeled data from the test signer, but not a sufficient amount for training full signerspecific DNNs, we can apply adaptation. A number of DNN adaptation approaches have been developed (e.g., [24, 25, 26, 27]). We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig. 2. Most of the network parameters are fixed; only a limited set of weights at the input and output layers are learned. In the first approach (LIN+UP in Fig. 2), we apply a single affine transformation WLIN to the static features at each frame (before concatenation) and feed the result to the trained signer-independent DNNs. We jointly learn WLIN and adapt the last (softmax) layer weights by minimizing the same cross-entropy loss on the adaptation data, and \u201cwarm-start\u201d the softmax layer with the learned signerindependent weights. The second approach (LIN+LON in Fig. 2) uses the same input adaptation layer, but rather than adapting the softmax weights, it removes the softmax output activation and adds a new softmax output layer WLON for the test signer, trained jointly with the same cross-entropy loss. Finally, we also consider adaptation by fine-tuning; that is, updating all of the DNN weights on adaptation data starting from the signer-independent weights. The adaptation can\nuse either ground-truth frame-level letter labels (using human annotation as described in [7]) or labels obtained by forced alignment if only word labels are available.\n3. EXPERIMENTS\nWe use the ASL video data set of [8], comprising four signers each fingerspelling 600 word tokens consisting of two repetitions of a 300-word list, including common English words, names, and foreign words. Annotators marked the peak of articulation of each letter, and the annotations were converted to a \u201cground-truth\u201d frame labeling by assuming that the letter boundaries occur mid-way between peaks. Following [8], the hand portion of each image is extracted via hand detection and segmentation using a signer-specific Gaussian color model, followed by suppression of irrelevant pixels. The extracted hand images are resized to 128 \u00d7 128 and Histogram of Gradient (HoG) [31] features are extracted using multiple spatial grids (4 \u00d7 4, 8 \u00d7 8, and 16 \u00d7 16), followed by dimensionality reduction with principal components analysis (PCA)."}, {"heading": "3.1. Frame classification", "text": "The initial unadapted signer-independent DNNs are trained on all but the test signer for each of the seven tasks (letters and the six phonological features). The input is the 128-dimensional HoG features concatenated over a 21-frame window, and the networks have three hidden layers of 3000 ReLUs [32]. Cross-entropy training is done with a weight decay penalty of 10\u22125 via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [33] at a rate of 0.5 at each hidden layer, fixed momentum of 0.95, and initial learning rate of 0.01, which is halved when held-out accuracy stops improving. These hyperparameters were tuned on held-out (signer-independent) data in initial experiments, not reported here in the interest of space. We pick the best-performing epoch on held-out data.\nWe next consider DNN normalization and adaptation with different types and amounts of supervision. For LIN+UP and LIN+LON, we adapt by running SGD over minibatches of 100 samples with a fixed momentum of 0.9 for up to 20 epochs, with initial learning rate of 0.02 (which is halved when accuracy stops improving on the adaptation data). For\nfine-tuning, we use the same SGD procedure as for the signerindependent DNNs. We pick the epoch with the highest accuracy on the adaptation data. The resulting frame accuracies are given in Fig. 3. In addition, Fig. 3 includes the result of speed normalization for the case of letter classification. Speed normalization provides consistent but very small improvements, while adaptation gives large improvements in all settings. LIN+UP slightly outperforms LIN+LON, and fine-tuning outperforms both LIN+UP and LIN+LON. For letter sequence recognition in the next section, we adapt via fine-tuning using 20% of the test signer\u2019s data.\nFig. 4 further analyzes the DNNs via confusion matrices. One of the main effects is the large number of incorrect predictions of the non-signing classes (<s>, </s>). We observe the same effect with the phonological feature classifiers. This may be due to the previously mentioned fact that non-linguistic gestures are variable and easy to confuse with signing when given a new signer\u2019s image frames. The confusion matrices show that, as the DNNs are adapted, this is the main type of error that is corrected."}, {"heading": "3.2. Connected letter recognition", "text": "In connected letter recognition, we measure performance via the letter accuracy, analogously to the word or phone accuracy in speech recognition. Table 1 shows the letter accuracies obtained with the tandem, rescoring SCRF, and first-pass SCRF models with DNN adaptation via fine-tuning, using different types of adaptation data. For all models, we do not retrain the models with the adapted DNNs, but tune several hyperparameters3 on 10% of the test signer\u2019s data. The tuned models are evaluated on an unseen 10% of the test signer\u2019s remaining data; finally, we repeat this for eight choices of tuning and test sets, covering the 80% of the test signer\u2019s data that we do not use for adaptation, and report the mean letter accuracy over the test sets.\nAs shown in Table 1, without adaptation both tandem and SCRF models do poorly, achieving only roughly 40% letter accuracies, with the rescoring SCRF slightly outperforming the others (recall that signer-dependent recognition achieves about 90% letter accuracies [8]). With adaptation, however, performance jumps to up to 69.7% letter accuracy with forced-alignment adaptation labels and up to 82.7% accuracy with ground-truth adaptation labels. All of the adapted models perform similarly, but interestingly, the first-pass SCRF is slightly worse than the others before adaptation and better (by 4.4% absolute) after ground-truth adaptation. One hypothesis\n3See [7, 8, 23] for details of the tuning parameters.\nis that the first-pass SCRF is more dependent on the DNN performance, while the tandem model uses the original image features and the rescoring SCRF uses the tandem model hypotheses and scores. Once the DNNs are adapted, however, the first-pass SCRF outperforms the other models.\n4. CONCLUSION\nIn this study of signer-independent and adapted ASL fingerspelling recognition, we have seen that fingerspelling has great variability in speed, hand appearance, and appearance of non-signing gestures. We have improved performance on new signers via adaptation of DNNs in tandem and SCRF recognizers. Several DNN adaptation approaches are successful, with the largest improvements coming from simple fine-tuning on adaptation data. This approach improves letter accuracies from around 40% (unadapted) to up to 69.7% with weak word-level supervision and up to 82.7% with groundtruth frame labels for the adaptation data. While the models perform similarly, the best adapted model is a first-pass SCRF. The main DNN improvements come from resolving confusions between actual letters and the non-signing (\u201csilence\u201d) class. Future work will continue to improve the models and adaptation approaches, as well as address other types of variability that are needed to port the models to video data \u201cin the wild\u201d.\n5. REFERENCES\n[1] P. Dreuw, D. Rybach, T. Deselaers, M. Zahedi, and H. Ney, \u201cSpeech recognition techniques for a sign language recognition system,\u201d in Proc. Interspeech, 2007.\n[2] M. M. Zaki and S. I. Shaheen, \u201cSign language recognition using a combination of new vision based features,\u201d Pattern Recognition Letters, pp. 3397\u20133415, 2010.\n[3] R. Bowden, D. Windridge, T. Kadir, A. Zisserman, and M. Brady, \u201cA linguistic feature vector for the visual interpretation of sign language,\u201d in Proc. ECCV, 2004.\n[4] S. Liwicki and M. Everingham, \u201cAutomatic recognition of fingerspelled words in British Sign Language,\u201d in Proc. 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis, 2009.\n[5] S. Theodorakis, V. Pitsikalis, and P. Maragos, \u201cModel-level data-driven sub-units for signs in videos of continuous sign language,\u201d in Proc. ICASSP, 2010.\n[6] C. Vogler and D. Metaxas, \u201cToward scalability in ASL recognition: Breaking down signs into phonemes,\u201d in Proc. Gesture Workshop, 1999.\n[7] T. Kim, K. Livescu, and G. Shakhnarovich, \u201cAmerican Sign Language fingerspelling recognition with phonological feature-based tandem models,\u201d in Proc. SLT, 2012.\n[8] T. Kim, G. Shakhnarovich, and K. Livescu, \u201cFingerspelling recognition with semi-Markov conditional random fields,\u201d in Proc. ICCV, 2013.\n[9] J. Forster, O. Koller, C. Oberdo\u0308rfer, Y. Gweth, and H. Ney, \u201cImproving continuous sign language recognition: Speech recognition techniques and system design,\u201d in Proc. SLPAT, 2013.\n[10] C. Padden and D. C. Gunsauls, \u201cHow the alphabet came to be used in a sign language,\u201d Sign Language Studies, 2004.\n[11] D. Brentari, A Prosodic Model of Sign Language Phonology, MIT Press, 1998.\n[12] R. E. Johnson and S. K. Liddell, \u201cToward a phonetic representation of signs: sequentiality and contrast,\u201d Sign Language Studies, vol. 11, no. 2, pp. 241\u2013274, 2010.\n[13] V. Athitsos, J. Alon, S. Sclaroff, and G. Kollios, \u201cBoostMap: A method for efficient approximate similarity rankings,\u201d in Proc. CVPR, 2004.\n[14] S. Ricco and C. Tomasi, \u201cFingerspelling recognition through classification of letter-to-letter transitions,\u201d in Proc. ACCV, 2009.\n[15] G. Tsechpenakis, D. Metaxas, and C. Neidle, \u201cLearningbased dynamic coupling of discrete and continuous trackers,\u201d Computer Vision and Image Understanding, vol. 104, no. 2\u20133, 2006.\n[16] R. Bowden and M. Sarhadi, \u201cA non-linear model of shape and motion for tracking finger spelt American sign language,\u201d Image and Vision Computing, vol. 20, no. 9\u201310, 2002.\n[17] P. Goh and E. Holden, \u201cDynamic fingerspelling recognition using geometric and motion features,\u201d in Proc. ICIP, 2006.\n[18] A. Roussos, S. Theodorakis, V. Pitsikalis, and P. Maragos, \u201cDynamic affine-invariant shape-appearance handshape features and classification in sign language videos,\u201d Journal of Machine Learning Research, vol. 14, 2013.\n[19] N. Pugeault and R. Bowden, \u201cSpelling it out: Real-time ASL fingerspelling recognition,\u201d in Proc. ICCV, 2011.\n[20] D. P. W. Ellis, R. Singh, and S. Sivadas, \u201cTandem acoustic modeling in large-vocabulary recognition,\u201d in Proc. ICASSP, 2001.\n[21] S. Sarawagi and W. W. Cohen, \u201cSemi-Markov conditional random fields for information extraction,\u201d in NIPS, 2004.\n[22] G. Zweig and P. Nguyen, \u201cA segmental CRF approach to large vocabulary continuous speech recognition,\u201d in Proc. ASRU, 2009.\n[23] H. Tang, W. Wang, K. Gimpel, and K. Livescu, \u201cDiscriminative segmental cascades for feature-rich phone recognition,\u201d in Proc. ASRU, 2015.\n[24] H. Liao, \u201cSpeaker adaptation of context dependent deep neural networks,\u201d in Proc. ICASSP, 2013.\n[25] O. Abdel-Hamid and H. Jiang, \u201cFast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code,\u201d in Proc. ICASSP, 2013.\n[26] P. Swietojanski and S. Renals, \u201cLearning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models,\u201d in Proc. SLT, 2014.\n[27] R. Doddipatla, M. Hasan, and T. Hain, \u201cSpeaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition,\u201d in Proc. Interspeech, 2014.\n[28] J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, and T. Robinson, \u201cSpeaker-adaptation for hybrid HMM-ANN continuous speech recognition system,\u201d in Proc. Eurospeech, 1995.\n[29] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, and Y. Gong, \u201cAdaptation of context-dependent deep neural networks for automatic speech recognition,\u201d in Proc. SLT, 2012.\n[30] B. Li and K. C. Sim, \u201cComparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems,\u201d in Proc. Interspeech, 2010.\n[31] N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human detection,\u201d in Proc. CVPR, 2005.\n[32] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and G. E. Hinton, \u201cOn rectified linear units for speech processing,\u201d in Proc. ICASSP, 2013.\n[33] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014."}], "references": [{"title": "Speech recognition techniques for a sign language recognition system", "author": ["P. Dreuw", "D. Rybach", "T. Deselaers", "M. Zahedi", "H. Ney"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Sign language recognition using a combination of new vision based features", "author": ["M.M. Zaki", "S.I. Shaheen"], "venue": "Pattern Recognition Letters, pp. 3397\u20133415, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A linguistic feature vector for the visual interpretation of sign language", "author": ["R. Bowden", "D. Windridge", "T. Kadir", "A. Zisserman", "M. Brady"], "venue": "Proc. ECCV, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic recognition of fingerspelled words in British Sign Language", "author": ["S. Liwicki", "M. Everingham"], "venue": "Proc. 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Model-level data-driven sub-units for signs in videos of continuous sign language", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "Proc. ICASSP, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward scalability in ASL recognition: Breaking down signs into phonemes", "author": ["C. Vogler", "D. Metaxas"], "venue": "Proc. Gesture Workshop, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "American Sign Language fingerspelling recognition with phonological feature-based tandem models", "author": ["T. Kim", "K. Livescu", "G. Shakhnarovich"], "venue": "Proc. SLT, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fingerspelling recognition with semi-Markov conditional random fields", "author": ["T. Kim", "G. Shakhnarovich", "K. Livescu"], "venue": "Proc. ICCV, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving continuous sign language recognition: Speech recognition techniques and system design", "author": ["J. Forster", "O. Koller", "C. Oberd\u00f6rfer", "Y. Gweth", "H. Ney"], "venue": "Proc. SLPAT, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "How the alphabet came to be used in a sign language", "author": ["C. Padden", "D.C. Gunsauls"], "venue": "Sign Language Studies, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A Prosodic Model of Sign Language Phonology", "author": ["D. Brentari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Toward a phonetic representation of signs: sequentiality and contrast", "author": ["R.E. Johnson", "S.K. Liddell"], "venue": "Sign Language Studies, vol. 11, no. 2, pp. 241\u2013274, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "BoostMap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "Proc. CVPR, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Fingerspelling recognition through classification of letter-to-letter transitions", "author": ["S. Ricco", "C. Tomasi"], "venue": "Proc. ACCV, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learningbased dynamic coupling of discrete and continuous trackers", "author": ["G. Tsechpenakis", "D. Metaxas", "C. Neidle"], "venue": "Computer Vision and Image Understanding, vol. 104, no. 2\u20133, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "A non-linear model of shape and motion for tracking finger spelt American sign language", "author": ["R. Bowden", "M. Sarhadi"], "venue": "Image and Vision Computing, vol. 20, no. 9\u201310, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Dynamic fingerspelling recognition using geometric and motion features", "author": ["P. Goh", "E. Holden"], "venue": "Proc. ICIP, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic affine-invariant shape-appearance handshape features and classification in sign language videos", "author": ["A. Roussos", "S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "Journal of Machine Learning Research, vol. 14, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Spelling it out: Real-time ASL fingerspelling recognition", "author": ["N. Pugeault", "R. Bowden"], "venue": "Proc. ICCV, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Tandem acoustic modeling in large-vocabulary recognition", "author": ["D.P.W. Ellis", "R. Singh", "S. Sivadas"], "venue": "Proc. ICASSP, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "NIPS, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A segmental CRF approach to large vocabulary continuous speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Proc. SLT, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. Eurospeech, 1995.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Proc. SLT, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 9, "context": "Fingerspelling accounts for roughly 12-35% of ASL [10] and is typically used for proper nouns or borrowings from English, which can often be the most important content words.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "Some aspects of fingerspelling can be characterized through the phonology of handshape [11, 12], which can be described in terms of phonological features.", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "Some aspects of fingerspelling can be characterized through the phonology of handshape [11, 12], which can be described in terms of phonological features.", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 13, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 14, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 15, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 16, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 17, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 18, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 6, "context": "[7, 8] obtained \u223c 90% average letter accuracies in a signer-dependent setting, using either tandem hidden Markov models (HMMs) or segmental conditional random fields (SCRFs), with features from neural network classifiers of letters and phonological features.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[7, 8] obtained \u223c 90% average letter accuracies in a signer-dependent setting, using either tandem hidden Markov models (HMMs) or segmental conditional random fields (SCRFs), with features from neural network classifiers of letters and phonological features.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "Prior work has address signer adaptation for large-vocabulary German Sign Language recognition [9], but to our knowledge this paper is the first to address adaptation for fingerspelling.", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "We start with the recognition approaches that have achieved the best prior results on this task [7, 8], with updates for improved performance with deeper neural networks.", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "We start with the recognition approaches that have achieved the best prior results on this task [7, 8], with updates for improved performance with deeper neural networks.", "startOffset": 96, "endOffset": 102}, {"referenceID": 19, "context": "The first recognizer is a tandem model [20] based on [7].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "The first recognizer is a tandem model [20] based on [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "The second recognizer is a segmental CRF (SCRF) model based on [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "SCRFs [21, 22] are conditional log-linear models with feature functions that can be based on variablelength segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 14}, {"referenceID": 21, "context": "SCRFs [21, 22] are conditional log-linear models with feature functions that can be based on variablelength segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 14}, {"referenceID": 7, "context": "As in [8], we use an SCRF to rescore lattices produced by a baseline frame-based recognizer (in this case, the tandem model).", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "We use the same feature functions as in [8], which include language model features, a feature that measures agreement with the baseline recognizer, means of letter/phonological feature neural network classifier outputs over each segment, and \u201cpeak detection\u201d features that measure the dynamics of each segment.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "[23], which is independent of any frame-based recognizer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "We use the same feature functions as in [23], namely average DNN outputs over each segment, samples of DNN outputs within the segment, duration and bias, all lexicalized.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "2See [11, 12, 7] for details of the phonological features.", "startOffset": 5, "endOffset": 16}, {"referenceID": 11, "context": "2See [11, 12, 7] for details of the phonological features.", "startOffset": 5, "endOffset": 16}, {"referenceID": 6, "context": "2See [11, 12, 7] for details of the phonological features.", "startOffset": 5, "endOffset": 16}, {"referenceID": 23, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 24, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 25, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 26, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 27, "context": "We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig.", "startOffset": 110, "endOffset": 122}, {"referenceID": 28, "context": "We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig.", "startOffset": 110, "endOffset": 122}, {"referenceID": 29, "context": "We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig.", "startOffset": 110, "endOffset": 122}, {"referenceID": 6, "context": "use either ground-truth frame-level letter labels (using human annotation as described in [7]) or labels obtained by forced alignment if only word labels are available.", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "We use the ASL video data set of [8], comprising four signers each fingerspelling 600 word tokens consisting of two repetitions of a 300-word list, including common English words, names, and foreign words.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "Following [8], the hand portion of each image is extracted via hand detection and segmentation using a signer-specific Gaussian color model, followed by suppression of irrelevant pixels.", "startOffset": 10, "endOffset": 13}, {"referenceID": 30, "context": "The extracted hand images are resized to 128 \u00d7 128 and Histogram of Gradient (HoG) [31] features are extracted using multiple spatial grids (4 \u00d7 4, 8 \u00d7 8, and 16 \u00d7 16), followed by dimensionality reduction with principal components analysis (PCA).", "startOffset": 83, "endOffset": 87}, {"referenceID": 31, "context": "The input is the 128-dimensional HoG features concatenated over a 21-frame window, and the networks have three hidden layers of 3000 ReLUs [32].", "startOffset": 139, "endOffset": 143}, {"referenceID": 32, "context": "Cross-entropy training is done with a weight decay penalty of 10\u22125 via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [33] at a rate of 0.", "startOffset": 167, "endOffset": 171}, {"referenceID": 7, "context": "As shown in Table 1, without adaptation both tandem and SCRF models do poorly, achieving only roughly 40% letter accuracies, with the rescoring SCRF slightly outperforming the others (recall that signer-dependent recognition achieves about 90% letter accuracies [8]).", "startOffset": 262, "endOffset": 265}, {"referenceID": 6, "context": "3See [7, 8, 23] for details of the tuning parameters.", "startOffset": 5, "endOffset": 15}, {"referenceID": 7, "context": "3See [7, 8, 23] for details of the tuning parameters.", "startOffset": 5, "endOffset": 15}, {"referenceID": 22, "context": "3See [7, 8, 23] for details of the tuning parameters.", "startOffset": 5, "endOffset": 15}], "year": 2016, "abstractText": "We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signerdependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7% with framelevel adaptation labels and 69.7% with only word labels.", "creator": "LaTeX with hyperref package"}}}