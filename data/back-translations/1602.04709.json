{"id": "1602.04709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies", "abstract": "Exploring social conversations to satisfy the needs of patients is an important analytical task to which many scientific publications contribute to fill the knowledge gap in this area. However, the greatest difficulty remains the inability to translate such contributions into pragmatic processes that the pharmaceutical industry can use to gain insights from social media data, which may be considered one of the most sophisticated sources of information available today due to its sheer volume and noise. This study is based on the work of Scott Spangler and Jeffrey Kreulen and uses it to identify social media structure by extracting an up-to-date taxonomy that is capable of capturing the latent knowledge in social conversations on health-related websites. The mechanism for automatically identifying and generating a taxonomy from social conversations is being developed and tested against public data from media sites that focus on the needs of cancer patients and their families. In addition, a novel method of generating the category and determining an optimal number of meaningful categories will be used by readers familiar with what Jeffrey and Scott are doing in their research.", "histories": [["v1", "Fri, 12 Feb 2016 19:56:49 GMT  (593kb)", "http://arxiv.org/abs/1602.04709v1", "7 pages, 7 figures, 1 table"]], "COMMENTS": "7 pages, 7 figures, 1 table", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["giancarlo crocetti", "amir a delay", "fatemeh seyedmendhi"], "accepted": false, "id": "1602.04709"}, "pdf": {"name": "1602.04709.pdf", "metadata": {"source": "CRF", "title": "Identifying Structures in Social Conversations in NSCLC Patients through the Semi-Automatic extraction of Topical Taxonomies", "authors": ["Giancarlo Crocetti", "Amir A. Delay", "Fatemeh Seyedmendhi"], "emails": [], "sections": [{"heading": null, "text": "www.ijera.com 20|P a g e\nmany scholarly publications are contributing to fill the knowledge gap in this area. The main difficulty remains the inability to turn such contributions into pragmatic processes the pharmaceutical industry can leverage in order to generate insight from social media data, which can be considered as one of the most challenging source of information available today due to its sheer volume and noise. This study is based on the work by Scott Spangler and Jeffrey Kreulen and applies it to identify structure in social media through the extraction of a topical taxonomy able to capture the latent knowledge in social conversations in health-related sites. The mechanism for automatically identifying and generating a taxonomy from social conversations is developed and pressured tested using public data from media sites focused on the needs of cancer patients and their families. Moreover, a novel method for generating the category\u2019s label and the determination of an optimal number of categories is presented which extends Scott and Jeffrey\u2019s research in a meaningful way. We assume the reader is familiar with taxonomies, what they are and how they are used.\nKeywords\u2013Taxonomy, Social Media, Text Mining, Clustering, Knowledge Management\nI. Introduction The availability of taxonomies varies dramatically from domain to domain. In life sciences, for example, we have an abundance of well-curated and up-to-date taxonomies (e.g. MeSH, MeDRA, Entrez Gene, etc.), from medical to genetic to disease terminologies. However, the situation is very different in other domains. With the exception of the financial area, it is very hard to come by domainspecific taxonomies, and when available, they are often proprietary with a hefty price tag. The problem is further exacerbated when we are interested in ascertaining the context around social conversation through the use of topical taxonomies which are domain specific and practically inexistent on the market for purchasing\nIn this situation there are not many options. You might develop your own taxonomy - after all there is a well-defined framework that can guide you through the process -, adopt an automatic strategy, or use a hybrid approach with the initial generation of terms using data mining techniques which are further reviewed by a professional taxonomist. The latter is an approach taken in this case study, in which the term taxonomy is used in its broader sense to reference any means of organizing concepts of knowledge.\nThe results presented in this study are based on the work done by Scott Spangler and Jeffrey Kreulen [1] and extended by deriving two methods for:\n1. Identifying a possible solution for the number of categories in the extracted taxonomy. 2. labeling each category using descriptors within the cluster\nThis work is focused on the extraction of a taxonomy on social data, but it can easily be extended to any other domain.\nThe development of a new taxonomy requires a significant effort thatnot only ends with the result of the initial taxonomy, but will also require continual maintenance and governance. Several authors such as Patrick Lambe [2] and Heather Hedden [3]outline a process for the development of a taxonomy. Other processes exist in the industry, and together they define a multi-step procedure for developing an inhouse taxonomy which usually requires:\n1. The definition of a business case 2. The engagement of stakeholders 3. Having in place a strong communication plan 4. The identification of the design 5. The definition of a governance model.\nWhen compared to other designs, the automatic extraction of a draft taxonomy might appeal for several reasons:\nwww.ijera.com 21|P a g e\n1. Domain experts might not be readily available in your company; therefore, data mining\napproaches might represent a viable solution.\n2. You might have time constraints on your project that does not allow the running of workshops or\ninterviews.\n3. Budget constraints.\nII. Literature Review Social media analysis is an interest for researchers because sites related to healthcare societies are in need to compare previous medical records. This can be used by practitioners for getting information related to patient care and overall productivity. Other areas that maybe of interest are promotional information, addressing confusing terminologies, increasing communication between patients, families, and having more of a variety of people who participate in health research and clinical trials. Therefore, we can use this to raise the bidirectional stream of communication between the doctors and their patients. This can be used to update the physician\u2019s use of social media as a proficient way to share new medical information within the medical community and also develop healthcare quality[4].\nIn order to discover important topics in forums related to health information and patients\u2019 information needs for healthcare knowledge, we need to analyze content identification. We can use methods, such as surveys based on questionnaires and statistical analysis. In previous studies, analysis of information that are shared in medical support blogs were based on the number of people who used it and the frequency of postings. Using this information from the surveys they were able to evaluate different user groups from the data[5].\nHowever, there were some problems in these statistical methods; the sample populations that researchers chose were too narrow and bias, so it affected the accuracy of their model. Another problem that arose was the fast development of these health forums and websites; they needed another method that can handle these lager amounts of data and be able to process at optimal speeds[5].\nPrevious studies demonstrated that the most frequent themes with which patients are concerned are prevention, diagnosis, support, treatments and the long-term side effects of those treatments. However, statistical content analysis is based on human defined content, which requires significant amounts of effort. This can be a time consuming process which is often error-prone and time consuming. In fact, when using traditional statistical methods, we have difficulty discovering relationships within data. Thus these methods are unpractical due to time and storage constraints[5].\nTo solve this, clustering methods seems to provide a good alternative to purely statistical methods. Clustering is primarily used to find unique relationships and patterns within large datasets that previously have no organization. This technique has been used in complicated task such as, pattern recognition, image analysis, and facility location. They are able to use clustering in order to partition the data into homogenous clusters, which are based on the content of the data, and give us an un-biased approach to looking at our content[6].\nIn fact, cluster analysis refers to an area of multivariate statistics that involves the grouping of objects based on some measure of proximity defined amongst those objects. Unlike discriminant analysis, which assumes that the group memberships are known, cluster analysis generates group memberships based on the proximities of data. Cluster analysis also differs markedly from principal component analysis and factor analysis. Whereas principal component analysis and factor analysis typically focus on reducing dimensionality by establishing linear combinations of variables; cluster analysis centers on classification of the objects based on their proximity with respect to variable measurements. Lu and colleagues [5] proposed a method based on clustering to explore health-related discussions in online health communities automatically instead of using the statistical approaches employed in previous studies. By integrating medical domain-specific knowledge, they have constructed a medical topic analysis model. The application of clustering for the extraction of taxonomies was successful also in other areas such as the automotive industry[7]. Ringsquandl presented a novel approach to semi-automatically learn concept Hierarchies from natural language requirements of the automotive industry. They extract taxonomies by using clustering techniques in combination with general thesauri. Evaluation shows that this taxonomy extraction approach outperforms common hierarchical clustering techniques[7]. Han, proposed a hierarchical clustering algorithm based on an asymmetric distance metric to explore hierarchical folksonomy for social media[8]."}, {"heading": "III. Methodology", "text": "Without considering types of taxonomies you need to focus on relations and the fact they can contain different structures and different kinds of relationships between terms.The basic building blocks in any taxonomy remain the set of terms; therefore, the first step is to identify such set of terms.\nIt shows from the literature review that clustering algorithms have a proven record on the identification of descriptive terms; it is not a coincidence that this was the method of choice for Spangler and Kreulen [1]. Consequently, we used the same k-Means\nwww.ijera.com 22|P a g e\nalgorithm as a starting point so to create a baseline that could be further improved with the use of other clustering approaches which are outside the scope of this study.\nThe decision to use the k-Means was based on the ability of this algorithm to create centroids which can be considered as a summary of all conversations contained in a cluster, generated as the arithmetic average of its elements. These centroids, together with a measure of similarity, provide a good platform to identify key terminology in the grouping of similar conversations. Despite its simplicity, we have to be aware of some of its major drawback related to the identification of sub-optimal solutions. Because, at this point, we only interested in testing a candidate process, we are not particularly concerned to its optimization tasks and the use of an algorithm that is very transparent in its execution and result generation it is preferable to other, more opaque, methods.\nThe goal of this process is to create a possible grouping of terms, organized in a one-level hierarchy, from a topic-specific collection of social media posts in order to generate a draft taxonomy able to capture the important knowledge expressed in these social conversations. Each level is represented by a single cluster and the associated descriptors are extracted using the centroid information generated for each cluster.\nWe are well aware of the current impossibility to generate taxonomies using completely automated means and the support of an expert taxonomist is of paramount importance. Consequently, the entire process has been developed so that the time needed to review the resulting \u201cdraft\u201d taxonomy by a domain expert, wound not take more than one hour. This can only be achieved but setting constraints on the number of categories and descriptors included in the taxonomy, yet, it is not easy to determine such thresholds A good number for categories is 30 which allows the analyst to visualize them at once in tabular form. With this first constraint in place we can set the maximum number of descriptors to 2,000. This number is derived considering 30 categories containing no more than 29 unique descriptors for a total of 870 features. To be on the safe side, Spangler and Kreulen doubled this number to 1740 and round it up to a maximum of 2,000 features which is a very reasonable number.\nA taxonomy of this size can be easily reviewed by an expert taxonomist within one hour. We will also see that, in particular, the constraints on 30 categories will allow the optimization on the number of clusters for a given solution."}, {"heading": "3.1 Source of data", "text": "In this study we crawled conversations from\nsocial sites that do not require login and with open access to their forums. In particular we collected data\nfrom: www.cancerforums.net, www.lunglovelink.org, and www.lungevity.org. Among the forums available for crawling we collected social posts related to Small Cell Lung Cancer using an in-house crawler developed using Python, based on the open source Beautiful Soup, and stored the social data in the form of XML documents with a canonical structure independent of any particular site. Once the crawling was completed the XML data was converted into a comma delimited file for further processing.\nThe final dataset contained a total of posts with\nthe breakdown shown in Figure 1."}, {"heading": "3.2 Extracting and preparing the data", "text": "The data mining tool used in this experiment is RapidMiner with the addition of the TextMining addon that is freely available and downloadable within the tool itself. The process of text preprocessing in social posts is described in Figure 2.\nAs in any text mining pre-processing task, we began the processing by transforming the social conversations into the well-known Vector Space Model by using the \u201cProcess Document\u201d operator in RapidMiner using TF-IDF measures of frequency.\nWe first utilized tokenization, which is a text processing methods that separates text into a sequence of tokens (single words) using the spaces between words and removing any punctuation\nwww.ijera.com 23|P a g e\ncharacter. Next we transformed the tokens to contain all lowercase letters so that word frequency can be accounted for.\nSocial conversations represent very noisy data, with the use of many embellishing terms that are weakly related to the topic of the conversation. Therefore, special attention must be paid to the removal of such noise. To this end, we applied four stop-wording dictionaries that are specific to remove the high frequency English words such as words that are commonly used in cancer discussions, the list of usernames, and the first names of people in the discussion. 2-grams are therefore generated to capture pair of terms that have special meaning when used together. For small documents like social interactions it does not make much sense to go beyond 2-grams. Only the 2-grams with a minimum document frequency of 3 (they must appear in at least three documents) were kept, all the other were filtered out. The resulting vocabulary contained terms like \u201clung_cancer\u201d, \u201ccat_scan\u201d, or \u201canti_nausea\u201d which are all examples of a meaningful terminology in the context of this experiment.\nThe pre-processing step generated what is commonly called the Vector Space Model (VSM) which is a matrix containing as rows the documents representing the social posts and as columns the terms extracted during the process as shown in Figure 3. Each cell in the matrix represents the TF-IDF measure of frequency associated to that term. A discussion on TF-IDF weighting is outside the scope of this book, but this should suffice to say that this score is higher for important terms, that is, terms that appear on a restricted number of documents and possibly providing context to the social conversations in which they appear."}, {"heading": "3.3 k-Means clustering", "text": "Organizing the data into meaningful categories is a fundamental task in the generation of a taxonomy and many approaches exists today. Clustering analysis is the formal study of methods for grouping objects based on similar characteristics so to explore their intrinsic structure. However, clustering methods do not generate labels and this shortcoming was addressed in this study.\nThe k-Means clustering algorithm was proposed over 50 years ago and still widely used today, even though many other algorithms have been proposed since its introduction [9]. Because our study is based on the work of Spangler and Kreulen [1], we will use the same approach that utilizes the k-Means algorithm."}, {"heading": "3.4 Determining the number of categories", "text": "The application of the k-Means clustering algorithm requires the user to specify the initial number k of categories which, probably, is not something that is known a priori. In this particular case the help of a subject matter expert might not provide a useful insight in inferring the number of taxonomical categories contained in the social conversations under analysis.\nLuckily, we can put the constraint of generating no more than 30 categories to good use. Before doing that we need to define a score that is able to measure the quality of a clustering solution so to be able to determine when a solution is better than another. In general, we can characterize a \u201cgood solution\u201d as a set of clusters that are far apart from each other with points within the same cluster that are as close as possible. This characterization allows us to define the following quantities [10]:\nwww.ijera.com 24|P a g e\nWithin Cluster Variation (WCV): measures how close or similar are elements within the clusters. We want this quantity to be as small as possible.\nBetween Cluster Variation (BCV): measures how far are the clusters from each other. We want this quantity to be as large as possible. The WCV and BCV can be represented graphically as in Figure 4.\nInstead of analyzing these two measures independently we can consider the quality ratio:\n(1) where WCV is:\n(2) the sum of the square\ndistances of each point in the cluster from the\nrespective centroid . We can consider this as a\ngood approximation for the overall WCV.\nSimilarly, we can approximate BCV as:\n(3) with the centroid\nfor cluster . The distant function used for the\ncalculation of WCV and BCV is the cosine similarity which works particularly well with text [11]. In general we want to have a small WCV and a large BCV, consequently we want to maximize the quality ratio .\nFrom the constraint on the number of categories we know that our optimal value of k is between 2 and 30. We can, therefore, run the k-Means algorithm for each of the possible value of k between 2 and 30 and calculate the associate value, with i=2, \u2026, 30. Our\noptimal taxonomy, under such constraints, will be the one having k categories so that\n.\nIn Figure 5 we see the values of Qk for different values of k. The highest value of q is associated with the solution comprised of k=12 clusters.\nThis is an interesting finding: the same solution was identified by a domain experts when considering the grouping for the different values of k. It would be interesting to support this finding with more experiments."}, {"heading": "3.5 Extraction of the taxonomy", "text": "We have to identify the clustering solution for which we are getting the best separation between conversations belonging to different clusters while trying to increase similarity in conversations belonging to the same group.\nIt is possible, now, to generate the category label and assign the descriptors within the category. To this end we will use the centroid vector in each of the 12 clusters as in Figure 6, where we are showing the centroids for first 5 clusters and the associated TFIDF weighting.\nwww.ijera.com 25|P a g e\n3.5.1 Extraction category labels\nThe label for each category is created through the concatenation of terms within each centroid, which poses the following issues:\n1. Which terms should we pick? 2. How many terms should we consider?\nTo address these issues we consider the centroid vector for each of the cluster and sort its terms\nby the TF-IDF values in descending order as shown in Figure 7.\nThe procedure to create the category label is then\nthe following:\n1. Include the first term in the label\n2. Consider the ratio of the TF-IDF\nweights of two subsequent terms and\n3. Add the term to the label if\n4. Repeat step 2 and 3 until it is possible to add items without discontinuity\nThe application of this procedure to the cluster in Figure 7, generates the label \u201cNausea & Chemo & Anti_Nausea\u201d, and in Table 1, we reported the labels generated for the entire cluster solution.\nNotice how the algorithm was able to identify conversation topics around specific drugs like Alimta, Zometa, and Tarceva, which are used in the treatment of Non-Small Lung Cancer and might represent an important insight in the analysis of conversations around this topic, especially if we are interested in patient\u2019s perception on the treatment of NSLC.\n3.5.2 Filling the category with descriptors With the category labels in place we can now add the remaining list of terms in the centroid vectors as descriptors to further detail the conversation\u2019s topics identified previously. In considering the example we used before, related to cluster 3 (see Figure 7), we have the following category.\nNausea & Chemo & Anti_Nausea Chemo Drug Medication Treatment anti-nausea nausea medication nausea \u2026\nThis is a well-defined concept in which patients are clearly conversing on the side effect of chemotherapy and possible solutions. Let\u2019s look at another example as in cluster 8: \u201cChemo & Treatment & Radat\u201d:\nChemo & Treatment & Radiat Chemo Treatment Radiat Tarceva Doctor Trial \u2026\nwww.ijera.com 26|P a g e\nWhich relates to conversation around trials for the use of Tarceva in addition to radiation therapy. The topical insight is quite strong in this case as well.\nThe resulting on-level categorization generated under the constraints highlighted in this methodology can be easily reviewed by a domain expert within one hour and the resulting curated taxonomy used to identify specific topics in social conversation.\nIV. Conclusions and further work The interesting work of Spangler and Kreulen [1] related to the analysis of social conversations has been extended in important ways through the addition of several improvements:\n1. A better insight of the number of clusters k inputted in the k-Means clustering algorithm 2. Generation of categorical labels related to the grouping of conversations\nThe labels generated by the procedure illustrated in this study can provide an immediate insight on the topics discussed in social media posts without the need of reading this vast amount of text data. Moreover, the curated taxonomy outputted by this procedure can be used by annotation services able to capture important terminology in social media data sources related to the topics described in the taxonomy.\nThe methodology in this study was based on the same clustering approach, using k-Means, by Spangler and Kreulen and it would be interesting to have comparative study using other algorithmic models. Moreover the resulting taxonomy at the end of the process is one-level deep, which could be extended with the application of recursive methods and improved through the use of lexicon resources so to derive broader/narrower relationships and the addition of synonyms.\nV. Aknowledgments This research was not possible without the help and support from the following graduate students at the Seidenberg School of Computer Science and Information Systems of Pace University: Mr. Suhail Bandhari, Ms. Trishna Mitra, Ms. Manvi Mohta, and Daniel Rings. Their help in identifying the social media sites and extracting the data was invaluable. A special thanks also to Prof. Catherine Dwyer for her support in this project and for coordinating the students."}], "references": [{"title": "Mining the Talk: Unlocking the Business Value in Unstructured Information (Boston", "author": ["S. Spangler", "J. Kreulen"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Organising Knowledge: Taxonomies, Knowledge and Organisational Effectiveness (Oxford", "author": ["P. Lambe"], "venue": "UK: Chandos Publishing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "The Accidental Taxonomist (Medford", "author": ["H. Hedden"], "venue": "NJ: Information Today,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Health-Related Hot Topic Detection in Online Communities", "author": ["Y. Lu", "P. Zhang", "J. Liu", "J. Deng", "S. Li"], "venue": "Using Text Clustering,PLOS ONE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Accelerated Simplified Swarm Optimization with Exploitation Search Scheme for Data Custering", "author": ["W. Yeh", "C. Lai"], "venue": "PLOS ONE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Taxonomy Extraction from Automotive Natural Language Requirements Using Unsupervised  Learning, International,Journal on", "author": ["M. Ringsquandl", "M. Schraps"], "venue": "Natural Language Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Contructing taxonomy by hierarchical clustering in online social bookmarking", "author": ["Z. Han", "Y Q. Mo", "Liu", "M. Zuo"], "venue": "Educational and Information Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Data Clustering: 50 years beyond k-Means", "author": ["A.K. Jain"], "venue": "Pattern Recognitions letter,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Discovering Knowledge in Data (Hoboken", "author": ["D. Larose", "C. Larose"], "venue": "NJ: Wiley,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Similarity Measures for Text Document Clustering", "author": ["A. Huang"], "venue": "Proceeding of the New Zealand Computer Science Research Student,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "The results presented in this study are based on the work done by Scott Spangler and Jeffrey Kreulen [1] and extended by deriving two methods for: 1.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Several authors such as Patrick Lambe [2] and Heather Hedden [3]outline a process for the development of a taxonomy.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "Several authors such as Patrick Lambe [2] and Heather Hedden [3]outline a process for the development of a taxonomy.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "Using this information from the surveys they were able to evaluate different user groups from the data[5].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "Another problem that arose was the fast development of these health forums and websites; they needed another method that can handle these lager amounts of data and be able to process at optimal speeds[5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 3, "context": "methods are unpractical due to time and storage constraints[5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "They are able to use clustering in order to partition the data into homogenous clusters, which are based on the content of the data, and give us an un-biased approach to looking at our content[6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "Lu and colleagues [5] proposed a method based on clustering to explore health-related discussions in online health communities automatically instead of using the statistical approaches employed in previous studies.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "The application of clustering for the extraction of taxonomies was successful also in other areas such as the automotive industry[7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "Evaluation shows that this taxonomy extraction approach outperforms common hierarchical clustering techniques[7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "asymmetric distance metric to explore hierarchical folksonomy for social media[8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "of descriptive terms; it is not a coincidence that this was the method of choice for Spangler and Kreulen [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "The k-Means clustering algorithm was proposed over 50 years ago and still widely used today, even though many other algorithms have been proposed since its introduction [9].", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "on the work of Spangler and Kreulen [1], we will use the same approach that utilizes the k-Means", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "This characterization allows us to define the following quantities [10]:", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "calculation of WCV and BCV is the cosine similarity which works particularly well with text [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Conclusions and further work The interesting work of Spangler and Kreulen [1] related to the analysis of social conversations has been extended in important ways through the addition of several improvements:", "startOffset": 74, "endOffset": 77}], "year": 2016, "abstractText": "The exploration of social conversations for addressing patient\u2019s needs is an important analytical task in which many scholarly publications are contributing to fill the knowledge gap in this area. The main difficulty remains the inability to turn such contributions into pragmatic processes the pharmaceutical industry can leverage in order to generate insight from social media data, which can be considered as one of the most challenging source of information available today due to its sheer volume and noise. This study is based on the work by Scott Spangler and Jeffrey Kreulen and applies it to identify structure in social media through the extraction of a topical taxonomy able to capture the latent knowledge in social conversations in health-related sites. The mechanism for automatically identifying and generating a taxonomy from social conversations is developed and pressured tested using public data from media sites focused on the needs of cancer patients and their families. Moreover, a novel method for generating the category\u2019s label and the determination of an optimal number of categories is presented which extends Scott and Jeffrey\u2019s research in a meaningful way. We assume the reader is familiar with taxonomies, what they are and how they are used. Keywords\u2013Taxonomy, Social Media, Text Mining, Clustering, Knowledge Management", "creator": "Microsoft\u00ae Word 2016"}}}