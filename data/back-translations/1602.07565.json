{"id": "1602.07565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Stochastic Shortest Path with Energy Constraints in POMDPs", "abstract": "We look at partially observable Markov decision-making processes (POMDPs) with a range of target states and positive integer costs associated with each transition. The traditional optimization goal (stochastic shortest path) calls for minimizing the expected total cost until the target is reached. We extend the traditional framework of POMDPs to energy consumption, which is a hard constraint. Energy levels can rise and fall with transitions, and the hard constraint requires that the energy level must remain positive in all steps until the target is reached. First, we present a new algorithm for solving POMDPs with energy levels, which is developed on existing POMDPs and uses RTDP as the main method. Our second contribution refers to political representation. For larger POMDP cases, the strategies calculated by existing solvers are too effective to be comprehensible. We present an automated process based on machine learning techniques that automatically extract important decisions from POMDP, which allow us to make POMDP-level-based, human-based decisions.", "histories": [["v1", "Wed, 24 Feb 2016 15:41:22 GMT  (99kb,D)", "https://arxiv.org/abs/1602.07565v1", "Technical report accompanying a paper published in proceedings of AAMAS 2016"], ["v2", "Wed, 11 May 2016 16:26:20 GMT  (86kb,D)", "http://arxiv.org/abs/1602.07565v2", "Technical report accompanying a paper published in proceedings of AAMAS 2016"]], "COMMENTS": "Technical report accompanying a paper published in proceedings of AAMAS 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom\\'a\\v{s} br\\'azdil", "krishnendu chatterjee", "martin chmel\\'ik", "anchit gupta", "petr novotn\\'y"], "accepted": false, "id": "1602.07565"}, "pdf": {"name": "1602.07565.pdf", "metadata": {"source": "CRF", "title": "Stochastic Shortest Path with Energy Constraints in POMDPs", "authors": ["Tom\u00e1\u0161 Br\u00e1zdil"], "emails": ["xbrazdil@fi.muni.cz", "kchatterjee@ist.ac.at", "mchmelik@ist.ac.at", "anchit@iitb.ac.in", "pnovotny@ist.ac.at"], "sections": [{"heading": null, "text": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and positive integer costs associated with every transition. The traditional optimization objective (stochastic shortest path) asks to minimize the expected total cost until the target set is reached. We extend the traditional framework of POMDPs to model energy consumption, which represents a hard constraint. There are energy levels that may increase and decrease with transitions, and the hard constraint requires that the energy level must remain positive in all steps till the target is reached. Our contribution is twofold. First, we present a novel algorithm for solving POMDPs with energy levels, developing on existing POMDP solvers and using real-time dynamic programming as its main method. Our second contribution is related to policy representation. For larger POMDP instances the policies computed by existing solvers are too large to be understandable. We present an automated procedure based on machine learning techniques that automatically extracts important decisions of a policy and computes its succinct, human readable representation. Finally, we show experimentally that our algorithm performs well and computes succinct policies on a number of POMDP instances from the literature that were naturally enhanced with energy levels."}, {"heading": "1 Introduction", "text": "Motion and task planning for autonomous agents are one of the classical problems studied in AI and robotics. One of the main challenges that make the problem difficult is the presence of uncertainty about the state of the agent and its environment [33], caused for instance by the agent\u2019s unreliable sensors. To account for these issues, powerful abstract frameworks for solving\nplanning problems under uncertainty were developed, among which the Partially observable Markov decision processes (POMDPs) play a crucial role.\nEach POMDP describes a discrete, typically finitestate system that exhibits both probabilistic and nondeterministic behaviour [54, 41]. Probabilities are useful for modelling sensor errors, hardware failures, and similar events whose rate of occurrence can be established empirically, while non-determinism represents the freedom of the agent\u2019s controller to choose appropriate control input. The imperfection of agent\u2019s sensors is represented by observations. In every step the controller receives an observation but not the current state itself. Policies (policies), i.e. rules for resolving non-determinism in POMDPs, can be viewed as blueprints for implementing concrete controllers of the agent. Hence, given a POMDP modelling an agent and its environment the usual task is to find a policy ensuring that the behaviour of the system conforms to a given specification or objective.\nVarious types of objectives in POMDPs have been studied. Typically it is assumed that there is a reward (resp. cost) function that assigns rewards (resp. costs) to transitions of the system. The goal of the agent is to maximize (resp. minimize) the reward (resp. cost) over a finite-horizon [52], or over an infinite horizon [55], where the sequence of rewards (resp. costs) can be aggregated by considering the discounted reward [42, 53] or the average reward [43, 21], etc. Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.e., the expected sum of costs of all transitions traversed before reaching T . Typically T is such that reaching a state of T corresponds to the agent completing some assigned task.\nar X\niv :1\n60 2.\n07 56\n5v 2\n[ cs\n.A I]\n1 1\nEnergy and Soft vs. Hard Constraints Most autonomous robotic devices operate under certain energy constraints, i.e. they need a steady supply of some resource (in the form of, e.g. fuel, electricity, etc.) to operate correctly. While the stochastic shortest path (SSP) objective can in principle express specifications of the form \u201dcomplete the task while minimizing the expected consumption of some resource,\u201d this approach is not suitable for modelling of resource-constrained systems, as the SSP objective only talks about the expected cost, without giving any guarantees on the cost of concrete executions of the modelled system, i.e., it is an example of a soft constraint. In particular, to use costs in SSP objectives to model resource consumption we would need to assume that the amount of a resource consumed by making a transition (represented by the transition\u2019s cost) is always available. This is not always realistic. An autonomous robot typically has a battery of a finite capacity (or a fuel tank of finite volume) which is continually depleted as the robot operates. The total amount of a resource required to complete the task can exceed this capacity, prompting the robot to periodically recharge the battery (or refuel the tank) at special charging points (petrol stations). When the resource is depleted, no action remains available to the robot, i.e., keeping the energy level positive is a hard constraint that must hold along every single execution of the system, no matter the outcomes of stochastic choices. (The issue of expectation-based vs. executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].)\nIn this paper we address this issue and extend POMDPs with energy constraints. That is, to a POMDPM with a given objective we assign a positive integer capacity cap and to each transition of M we assign an integer update representing the amount of a resource consumed or reloaded by this transition. Such a POMDP starts with some initial level of a resource (say cap, i.e. the resource is loaded to full capacity) which is then modified as the system evolves: whenever a transition with some update u is traversed, the resource level changes from ` to min{` + u, cap} (discarding any quantity exceeding cap captures the fact that the robot\u2019s storage capacity cannot be exceeded). The task is to find a policy ensuring the original objective and at the same time ensuring that the resource level stays positive till the target is reached.\nOur Results on POMDPs with Energy Constraints We study energy-reachability problem for POMDPs. In the qualitative version of the problem we ask to find a policy that ensures that the expected total cost is finite before reaching the target state and at the same time keeps the resource level positive. In the quantitative version we additionally seek for a pol-\nicy that, on top of above two conditions, minimizes the expected total cost till the target is reached. We show how to solve both these problems by reducing them to corresponding problems in POMDPs without energy constraints. In particular, we show that the qualitative energy-reachability problem is EXPTIMEcomplete, i.e. it has the same complexity as unconstrained qualitative reachability. We experimentally evaluate our approach on standard POMDP models of robot planning.\nRepresentation of Policies Solving POMDPs with energy-reachability objectives highlights another relevant issue: the representation the computed policies. Policies in POMDPs are often represented in a form of a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20]. Size of these structures can become very large and not very readable by humans. For instance, policies ensuring that a target state is reached with probability 1 might require table or plan graph of size exponential in the size of the POMDP [19].\nThere are two reasons why size and representation of policies matter. First, as offline-computed policies have to be implemented on real-world devices, it is advisable to reduce their memory requirements so that they fit into the device\u2019s memory and do not cause delays through inefficient memory access. The second issue, to which we devote a particular attention in this paper, is the one of human readability. From the engineering point of view it is vital to be able to visualize the policy and understand its behaviour. This is reflected in numerous informal rules for safety-critical system design that enforce \u201dsimplicity\u201d and \u201dreadability\u201d [29, 57] as well as in academic treatments of the subject [35, Chapter 2 on \u201dSimplicity\u201d]. Although many methods for policy computation in POMDPs produce results that are correct by design, the behaviour induced by the computed policy in an actual device might not be reasonable, due to either using an improper model of the system, or too weak specification that does not rule out all undesirable behaviours. In such a case, comprehension of the computed policy can lead the system designer to refine the model or specification in an appropriate way. Easily understandable descriptions of policies can be also interesting for type approval authorities.\nReadability of policies is relevant for POMDPs in general, but the issue is especially pronounced in energyconstrained POMDPs, as the standard representation does not reveal which decisions depend on states and which depend on current resource level, an information useful for identifying bottlenecks caused by insufficient storage capacity or exploiting the fact that policy\u2019s dependency on resource levels might not be complex (e.g. \u201dwhen low on fuel, go to a gas station\u201d).\nOur Results on Policy Representation We study succinct representation of policies via decision trees.1 A decision tree (DT) is an easily visualisable data structure in which leaves represents actions prescribed by the policy and branching in internal nodes represents decisions that the policy makes in order to select a suitable action. To obtain a DT-representation from the corresponding table representation we utilize machine learning techniques for learning DTs. There advantage of this approach is that learning algorithms are often able to exploit the structure of the model, identify the crucial decisions made by the policy and encode only these decisions in the DT. This typically results in much more succinct representation without significant loss of the policy\u2019s performance. To support this claim, we present experimental results on learning DTrepresented policies using several well-known learning tools. As discussed in the previous paragraph, our approach can be seen as a generic technique for POMDPs which is particularly apt for use in the presence of energy constraints."}, {"heading": "2 Related Work", "text": "Our model of POMDPs with energy constraints, where the goal is to optimize the expected total cost while ensuring constraints on resource consumption, resembles the standard framework of constrained POMDPs [30, 59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs). The crucial difference between constrained POMDPs and our energy constraints is that the constraints in constrained POMDPs are soft, i.e. they are bounds on the expectation of some quantity, while we require that the resource level stays always positive (not just on average) in all runs (see also a discussion in Section 1).\nAs mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints. Our notion of energy constraints is similar to the one used in verification, in particular to so called energy games and MDPs [16, 18] and consumption games [11], although none of these concepts was considered in a partially observable setting so far. DTs have already been successfully used to represent policies in verification of perfectly observable MDPs modelled in the well-known PRISM tool [10]. For POMDPs, in [9] they consider a situation where the POMDP itself is encoded succinctly using DTs and similar structures, and they use this assumption to design a specific algorithm\n1These should not be confused with policy trees that represent a complete behaviour of a POMDP under a fixed policy [33].\ncomputing a desired policy (which itself is not encoded as a DT). In contrast, we assume that the model is given explicitly and use generic machine learning methods to infer succinct representations of policies. In [7] they study relationship of DTs and POMDPs from an inverse perspective, POMDPs are used as a tool for learning decision trees from generic datasets. DTs were also used to represent policies in a reinforcement-learning setting [25], where the agent has no a priori model of the environment.\nThe need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24]. Intuitively, the approach is based on direct search for a small policy represented as a finite (possibly stochastic) transducer whose transitions are labelled by observations. In every step, the state of the transducer changes according to the transition function and latest observation received, and the controller then outputs an action to be performed based on the current state of the transducer. While this approach was shown to produce small and well-performing policies, we argue that our approach, while having similar goal, is conceptually different and offers an orthogonal set of advantages. The main difference is that FSCs represent a function whose domain are histories of actions and observations: each state of the finite transducer implicitly carries an information about the set of histories that lead the transducer to this state. The transducer thus captures an operational aspect of a policy, i.e. the way in which it is executed as a program. On the other hand, DTs represent functions whose domain is the set of beliefs: given a belief, a single root-leaf traversal of a DT is used to establish an action to be performed. Thus, DTs capture the logic of agent\u2019s decision in a concrete time instant; it is up to the agent to keep an (accurate or approximate) representation of its belief (which can be done using standard computations) and thus to take care of the history-dependent aspect of decision making. The latter approach more explicitly captures the decision making process as a human-like inference of suitable action from available information, and thus we believe that it provides better readability. Another advantage of our approach, which is validated by our experiments, is that the machine learning techniques we use are able to automatically identify the \u201dmost important\u201d decisions that amount for the majority of optimization efforts. Finally, we show that FSCs can be prone to storing an unnecessary amount of data about resource levels. We further explain differences between the two formalism on a concrete example at the end of Section 5.\nOne crucial difference between previous approaches to policy succinctness in both POMDP [22] and other settings [25] is that in previous work they concurrently optimize both the performance of a policy and its size,\nwhich requires dedicated algorithms, while we separate these tasks: first we search for a well-performing, though possibly \u201dugly\u201d policy, and then learn its succinct representation (similar approach was used in [23], where policies computed by point-based methods were \u201dcompiled\u201d into FSCs). Thus, we present a framework for obtaining succinct representations in which various state-of-the art algorithms for POMDP solving and DT learning can be used. On the POMDP side, this allows us to keep up with advances in solving of large POMDPs. On the DT side, we can use well-developed machine learning tools that already offer a selection of methods for tree pruning and visualization, which is important for readability."}, {"heading": "3 Preliminaries", "text": "Notation We use N0,N,Z to denote the sets of nonnegative, positive, and all integers, respectively. For n \u2208 N we denote by [n] the set {1, . . . , n}. Let X, Y be finite sets. For a function f : X \u2192 Y and sets X \u2032 \u2286 X, Y \u2032 \u2286 Y we denote by f(X \u2032) the image of X \u2032 under f , i.e. the set {y \u2208 Y | \u2203x \u2208 X \u2032 : f(x) = y} and by f\u22121(Y \u2032) the pre-image of Y \u2032 under f , i.e. the set {x \u2208 X | f(x) \u2208 Y \u2032}. We denote by D(X) the set of all probability distributions on X, i.e. of all functions f : X \u2192 [0, 1] s.t. \u2211 x\u2208X f(x) = 1. For f \u2208 D(X) we denote by supp(f) the support of f , i.e. the set {x \u2208 X | f(x) > 0}. A probability distribution f is Dirac if |supp(f)| = 1. An encoding size of an object O (i.e. the number of bits needed to represent O) is denoted by ||O||.\nPOMDPs A Partially Observable Markov Decision Process (POMDP) is a tuple M = (S,A, \u03b4, Z,O, \u03bb) where: S is a finite set of states; A is a finite alphabet of actions; \u03b4 : S\u00d7A\u2192 D(S) is a probabilistic transition function assigning to every state-action pair a probability distribution over the successor states (i.e. \u03b4(s, a)(s\u2032) denotes the probability of making a transition from s to s\u2032 under action a); Z is a finite set of observations; O : S\u00d7A\u2192 D(Z) is a probabilistic observation function assigning a probability distribution over observations to every state-action pair; and \u03bb is an initial probability distribution over the states ofM. We write \u03b4(s\u2032|s, a) as a shorthand for \u03b4(s, a)(s\u2032).\nRemark 1 (Deterministic observation function). We remark that deterministic observation functions of type O : S \u2192 Z are sufficient in POMDPs. Informally, the probabilistic aspect of the observation function is captured in the transition function, and by enlarging the state space with the product with the observations, we obtain an observation function only on states [17]. Thus in the sequel without loss of generality we will always\nconsider observation function of type O : S \u2192 Z which greatly simplifies the notation.\nRuns and Histories A run (finite or infinite) in a POMDP is an alternating sequence of states and actions s0, a1, s1, a2, s2, . . . such that s0 \u2208 supp(\u03bb) and for every i \u2265 0 it holds \u03b4(si+1|si, ai+1) > 0. To a run w = s0, a1, s1, a2, . . . we assign an observed run, i.e. a corresponding observation-action sequence O(w) = O(s0), a1, O(s1), a2, . . . . A history (finite or infinite) is an alternating sequence of observations and actions denoted as \u03c1 = z0, a1, z1, a2, z2, . . . , such that there exists a run w for which \u03c1 = O(w) (note that we already assume that function O is deterministic as noted in Remark 1).\nThe length of a finite run w = s0, a1, . . . , sk is the number len(w) = k, i.e. the number of actions performed along w. The length of an infinite run is \u221e, and the lengths of (finite or infinite) histories are defined likewise. We denote by RunM and FHistM the sets of all runs and finite histories in M, respectively.\nPolicies A policy (or a policy) in POMDP M is a function \u03c3 of type FHistM \u2192 D(A). Intuitively, policies are abstractions of controllers for the system modelled byM: the control is exerted by choosing a suitable action in every decision step, depending on the history of the system\u2019s evolution. A run w = s0, a1, s1, . . . conforms to a policy \u03c3 if for all 0 \u2264 i < len(w) the distribution \u03c3(O(s0, a1, . . . , si)) assigns positive probability to action ai+1.\nSemantics of POMDPs The behaviour of M under a policy \u03c3 can be intuitively described as follows: first, an initial state s0 is sampled according the initial distribution \u03bb. Then the system evolves in discrete steps. In a step i \u2265 0, let wi = s0, a1, s1, a2, . . . ai, si be the current finite run, i.e. the sequence of traversed states and chosen actions up to the i-th step (we have w0 = s0). An action ai is sampled according to the distribution \u03c3(O(wi)), and then a successor state si+1 is sampled according to the distribution \u03b4(si, ai). In the next step the same procedure is performed with run wi+1 = s0, a1, s1, a2, . . . ai+1, si+1, etc. The process evolves in this manner ad infinitum. This intuitive description can be formalized by constructing a suitable probability measure P\u03c3 assigning probabilities to sets of infinite runs in M. The construction of P\u03c3 is standard [6]. We denote by E\u03c3 the expected value operator induced by P\u03c3.\nObjectives An objective is a mathematical formalization of a desired behaviour of a system modeled by a POMDP. In this paper we study POMDPs that com-\nbine reachability, stochastic shortest path, and energy objectives. \u2022 A reachability objective is given by a set T \u2286 S\nof target states. A run s0, a1, s1, . . . satisfies such an objective if it eventually reaches a state from T , i.e. if si \u2208 T for some i \u2265 0. We denote by ReachT the set of all infinite runs that satisfy a reachability objective with target set T . \u2022 A total cost objective is given by a tuple (T, c),\nwhere T is again a set of target states and c : S \u00d7 A \u2192 N is a cost function assigning a positive integer cost to every state-action pair. Total cost is a quantitative objective, i.e. instead of saying that a run satisfies the objective or not, we measure the \u201dquality\u201d of a run by assigning a number to it. Here we assign to an infinite run w = s0, a1, s1, . . . its total cost TC cT (w) = \u2211m i=1 c(si\u22121, ai), where\nm = inf{j \u2265 0 | sj \u2208 T}. (We stipulate that an empty sum equals zero. Note that if m =\u221e, then TC cT (w) =\u221e.) \u2022 An energy objective is given by a tuple (E, cap, T ),\nwhere E : A \u00d7 Z \u2192 Z is a function assigning a resource change to every action-observation pair, cap \u2208 N0 is a non-negative capacity, and T is a set of target states. For (s, a, n) \u2208 S \u00d7 A \u00d7 [cap] we define a one-step resource update EnUpcap(s, a, n) = min{cap, n + E(a,O(s))}. For a run w = s0, a1, s1, . . . we put an energy level after i \u2208 N0 steps along w, where 0 \u2264 i \u2264 len(w), to be a number ELcapE (w, i) defined inductively as follows: ELcapE (w, 0) = cap and for i \u2265 1 we put ELcapE (w, i) = EnUp cap(si\u22121, ai,EL cap E (w, i \u2212 1)). In other words, we assume that the resource level is initially at full capacity and is then changed by performing various actions. Should the resource level rise above cap, the excess amount is immediately discarded. An infinite run w = s0, a1, s1, . . . satisfies an energy objective given by (E, cap, T ) if ELcapE (w, i) > 0 for all 0 < i \u2264 m, where m = inf{j \u2265 0 | sj \u2208 T}. We denote the set of all such satisfying infinite runs by SafecapE,T .\nRemark 2. Note that per our definition the resource level at every step is perfectly observable. This is a reasonable assumption whenever the modelled energyconstrained agent is equipped with sufficiently precise charge/fuel sensors. In this our model resembles mixedobservability POMDPs [40, 2], and indeed in the next section we will present a transformation of POMDPs with energy constraints into standard POMDPs in which resource levels are a fully observable component of each state. However, mixed observability is used to enhance the performance of exact and point-based algorithms, while we aim for solution via simulation-based techniques, namely RTDP-Bel. Since, in the words of [2], online techniques cannot be probably adapted\nto benefit from mixed observability, we stick to standard POMDP formulations. For further applications of mixed observability, see, e.g. [15, 13].\nComputational Tasks Given a POMDP M, a set of states T , an resource change function E, and a capacity cap, we define the set of energy-safe policies EnSafeMT (E, cap). A policy \u03c3 belongs to the set EnSafeMT (E, cap) if for all infinite runs w conforming to policy \u03c3 we have w \u2208 SafecapE,T . Given a policy \u03c3 \u2208 EnSafeMT (E, cap) we define the value of \u03c3 as the expectation Val(\u03c3) = E\u03c3[TC cT ].\nWe are interested in minimizing the expected cost till the target set T is reached while keeping the energy level positive, i.e., we are interested in approximating the following value:\noptCost = inf \u03c3\u2208EnSafeMT (E,cap) Val(\u03c3)\nWe aim to solve the following computational problems:\n1. The qualitative energy-reachability problem asks whether optCost <\u221e. 2. The quantitative energy-reachability problem asks for a policy \u03c3 such that Val(\u03c3) approximates the value optCost.\nRemark 3. We remark about POMDPs without energy constraints and the restrictions of the cost function:\n1. The problem of approximating optimal cost optCost in POMDPs without energy constraints for positive costs was shown to be decidable in [17]. 2. The problem of approximating optimal cost optCost for general costs (positive and negative) was proved to be undecidable in [17] already for POMDPs without energy constraints.\nAs policies are per definition infinite objects, in both the qualitative and quantitative problems we aim to compute their finite representations. One of the primary aims of this paper is to address the efficiency of such representations, our goal being to find succinct and/or human-readable ways to encode the computed policies."}, {"heading": "4 Solving Energy-Reachability", "text": "Problems\nFor the rest of the section let us fix a POMDPM, target set T , functions c, E and a capacity cap. We will evaluate the complexity of presented algorithms in terms of ||M|| and ||cap||. We assume that cap is represented in binary, i.e. cap is at most exponential in ||cap||.\nTo solve both types of energy-reachability problems we construct a product POMDP M\u00d7 by encoding the\nresource levels inM\u00d7 directly into the states. Formally, M\u00d7 has a set of states S\u00d7 = S \u00d7 [cap] \u222a {\u22a5}, where \u22a5 is a newly added sink state, and the same set of actions asM. A transition function \u03b4\u00d7 ofM\u00d7 is defined as follows: for all (s, n) \u2208 S \u00d7 [cap], all a \u2208 A s.t. EnUpcap(s, a, n) \u2265 1, and all s\u0303 \u2208 S\u00d7 we have\n\u03b4\u00d7(s\u0303|(s, n), a) = { \u03b4(s\u2032|s, a) if s\u0303 = (s\u2032,EnUpcap(s, a, n)) 0 otherwise;\nand for every other s\u0302 \u2208 S\u00d7, a \u2208 A the distribution \u03b4\u00d7(s\u0302, a) is Dirac, assigning 1 to state \u22a5. The set of observations in M\u00d7 is Z\u00d7 = Z \u222a {\u22a5}, and observation function O\u00d7 is such that O\u00d7(s, n) = O(s) for each (s, n) \u2208 S \u00d7 [cap] and O\u00d7(\u22a5) = \u22a5. Finally, the initial distribution \u03bb\u00d7 assigns to each tuple of the form (s, cap) probability \u03bb(s), and 0 to all other states. We also extend the reachability and total cost objectives to M\u00d7 by defining a new target set T\u00d7 = T \u00d7 [cap] and cost function c\u00d7 such that c\u00d7((s, n), a) = c(s, a) and c\u00d7(\u22a5, a) = 1, for every action a.\nIt is straightforward to verify that POMDP M\u00d7 can be automatically constructed in time polynomial in ||M|| and exponential in ||cap||.\nThere is a natural correspondence between runs, histories, and policies in M and M\u00d7 which preserves the properties related to our objectives. In particular, for every policy \u03c3 \u2208 EnSafeMT (E, cap) that almost surely reaches T one can construct a policy \u03c3\u0303 inM\u00d7 such that \u03c3\u0303 almost surely reaches T\u00d7 and E\u03c3\u0303[TC c\u00d7 T\u00d7\n] = E\u03c3[TC cT ]; and vice versa, any policy in M\u00d7 that almost surely reaches target, can be transformed into a policy of the same expected cost in EnSafeMT (E, cap). If the policy \u03c3 to be transformed is finitely represented, the finite representation of the transformed policy \u03c3\u0303 can be computed in time polynomial in ||M\u00d7|| and ||\u03c3||.\nIt follows that to solve the qualitative energyreachability problem forM it suffices to solve the qualitative reachability problem for M\u00d7, i.e. compute a policy \u03c3\u0303 such that P\u03c3\u0303(ReachT\u00d7) = 1. Algorithm solving the qualitative reachability problem based on belief supports was presented in [17, 3]. We briefly recall the approach: A belief support of a finite history \u03c1 = z0, a1, z1, . . . , zn is a set B(\u03c1) of states in which the POMDP can be with positive probability after the sequence \u03c1 is observed, i.e. B(\u03c1) = {s \u2208 S | \u2203w = s0, a1, s1..., sn \u2208 RunM\u00d7 : \u03c1 = O(w) \u2227 s = sn}. The algorithm computes for each U \u2208 BS(M\u00d7) = {U \u2286 S | \u2203\u03c1 \u2208 FHistM\u00d7 : B(\u03c1) = U} a set of so-called allowed actions in U : intuitively, action a is allowed in U if playing action a in any situation where the observed finite history has belief support U results into situation in which the target set can still be reached with probability 1 by some policy. One can show that if there is a state s \u2208 supp(\u03bb0) such that B(O(s)) admits no allowed action, then no policy in M\u00d7 can reach the\ntarget almost surely. Otherwise the algorithm outputs a policy \u03c3all which for each finite history \u03c1 plays all actions allowed in B(\u03c1) with uniform probability. It can be proves that for \u03c3all it holds P\u03c3all (ReachT\u00d7) = 1.\nThe running time of the algorithm and the space needed to represent \u03c3all is dominated by a polynomial in the the size of BS(M\u00d7), i.e. in the number of reachable belief supports. This number can be trivially bounded by 2|S\u00d7|, which is an expression doubly exponential in ||cap||. However, from the construction of M\u00d7 we get the following improved bound:\nLemma 1. It holds |BS(M\u00d7)| \u2264 2|S|\u00b7cap.\nAs a consequence we get the following.\nTheorem 1. The qualitative energy-reachability problem for POMDPs is EXPTIME-complete.\nProof. The upper bound follows from Lemma 1, and from the complexity of constructing M\u00d7 and translating its policies to M. The lower bound follows from EXPTIME-hardness of qualitative reachability in POMDPs [19].\nTo solve the quantitative energy-reachability problem, we again use an algorithm for POMDPs without energy, namely the one from [17], and apply it to M\u00d7. The algorithm, which assumes that the sets of allowed actions were already computed via the aforementioned method, finds a policy of small cost that almost surely reaches T\u00d7, using a modified version of RTDP-Bel [8]. RTDP-Bel is an adaptation of the real-time dynamic programming value iteration [4] to POMDPs. It is an approximative method which does not guarantee convergence to optimum, but it is known to produce nearoptimal policies on many instances where optimal costs can be computed using exact methods [17]. Hence, results produced by RTDP-Bel are a useful yardstick against which policies obtained by other methods can be compared. Due to the absence of guarantees we do not investigate the theoretical complexity of the algorithm. Its experimental evaluation can be found in Section 6.\nThe policy output by modified RTDP-Bel bases its decision in every step on the current belief, i.e. the probability distribution over the set of states representing the likelihood of being in particular states given the current history of states and observations [33]. As the space of beliefs is continuous, the policy operates on its discretized version, which allows it to be represented by a finite table storing one action per discretized belief. Not all beliefs have to be stored in the table: RTDP-Bel can converge to optimum without considering all reachable beliefs. Still, memory required to store the table might be too large for the policy to be understandable. In the next section we present a framework for converting table-represented policies into a more succinct and human-readable form."}, {"heading": "5 Succinct Representation of", "text": "Policies\nA policy in M\u00d7 computed by the RTDP-Bel is a function which to every belief assigns an action to be taken. Formally, a belief is a probability distribution b on S\u00d7 such that supp(b) \u2286 O\u22121\u00d7 (z) for some observation z.\nAs indicated above, RTDP-Bel considers only discretized beliefs, that is beliefs whose probabilities are rounded to a finite mesh. For technical reasons, the RTDP-Bel represents such discretized beliefs as vectors of non-negative integers from an interval [0, B] where B is a bound which determines the precision of the approximation.\nThus each belief inM\u00d7 can be represented as a vector b \u2208 ZS+1 whose first |S| components are integers from [0, B], and whose last component is in [cap]. Given such a belief b, the true probability of being in a state s with the energy n is (approximately) equal to bs/B, where bs is the component corresponding to state s.\nFor simplicity, we assume that actions are named in such a way that A = {0, 1, . . . , kA}."}, {"heading": "5.1 Decision Trees", "text": "There are numerous possibilities of succinctly representing sets of vectors of numbers (and functions on such sets) in a human readable form. One of the most popular formalisms suitable for this purpose are decision trees (DT see [44, 38]). We use DTs to represent functions of beliefs in POMDPs. For convenience, we follow closely the definition of DT used in [10]. Let V = {v1, . . . , vd} be a set of variable names.\nDefinition 1. A decision tree over the set of variables V is a tuple T = (Tr, \u03c1, \u03b8) where Tr is a finite rooted binary (ordered) tree with a set of inner nodes N and a set of leaves L, \u03c1 assigns to every inner node a predicate of the form [vi \u223c const ] where vi \u2208 V , const \u2208 Z, \u223c \u2208 {\u2264, <,\u2265, >,=}, and \u03b8 assigns to every leaf a nonnegative integer.\nA DT T over V determines a function f : Zd \u2192 Z as follows: For a vector ~v = (v\u03041, . . . , v\u0304n) \u2208 Zd, we find a path p from the root to a leaf ` such that for each inner node n on the path, the predicate \u03c1(n) is satisfied by substitution vi = v\u0304i iff the first child of n is on p. Then we put f(~v) = f(v\u03041, . . . , v\u0304n) = \u03b8(`). In our setting the set of variable names is chosen so as to suitably characterize the current belief. Typically, one can put V = S \u222a {Energy}, although different sets can be used as well. The domain of values assigned to leaves is the set of actions A = {0, 1, . . . , kA}.\nTraining DT.\nWe describe the process of learning a training set, which can also be understood as storing the input/output behaviour of a function described by data. Assume that we are given a training sequence \u03c4 = (~v1, f1), . . . , (~v\nk, fk) (repetitions allowed!) that specifies the desired input/output behaviour, i.e. each ~vi = (vi1, . . . , v i n) \u2208 Zd is a training input and fi \u2208 Z is the expected output. The goal is to learn a DT which exhibits the input/output behaviour prescribed by the training sequence.\nA standard process of learning according to the algorithm ID3 [44, 38] proceeds as follows:\n1. Start with a single node (root), and assign to it the whole training sequence. 2. For a node n with a sequence \u03c4 = (~v1, f1), . . . , (~v\nk, fk), (a) if all training examples in \u03c4 have the same\nexpected output value (i.e. there is x such that fi = x for all i), set \u03b8(n) = x and stop; (b) otherwise, \u2022 choose a predicate with the \u201chighest in-\nformation gain\u201d (with lowest entropy, see e.g. [38, Sections 3.4.1, 3.7.2]), \u2022 split \u03c4 (according to the inputs) into sequences satisfying and not satisfying the predicate, assign them to the first and the second child, respectively,\n\u2022 go to step 2 for each child. Intuitively, the predicate with the highest information gain tends to make a clear cut among the classes.\nIn addition, the final tree can be pruned. This means that some leaves are merged, resulting in a smaller tree at the cost of some imprecision of storing. The pruning phase is quite sophisticated, hence for the sake of simplicity and brevity, we omit the details here. We use the standard C4.5 algorithm and refer to [45, 38]. In Section 6, we comment on effects of parameters used in pruning. We also use the CART algorithm [12] with so called Gini index instead of the information gain to select the best splits (there are also differences in pruning)."}, {"heading": "5.2 Learning a DT Policy", "text": "Our goal is to train a DT to succinctly represent a policy computed by RTDP-Bel. We use RTDP-Bel to generate a training set using the following procedure:\n\u2022 Compute a policy \u03c3 using RTDP-Bel that solves the quantitative energy-reachability problem. \u2022 Run a specified number m of simulations of \u03c3, each of a fixed length `. In every step of each simulation produce a new training instance (b, \u03c3(b)) where\n\u2013 b is the current belief,\n\u2013 \u03c3(b) is the action chosen by \u03c3 in the current step.\nThe above procedure generates a training sequence of pairs \u03c4 = (b1, \u03c3(b1)), . . . , (bk, \u03c3(bk)) where k = m \u00b7 `. We feed this sequence into a learning algorithm for DT and obtain a decision tree approximating behaviour of \u03c3 on beliefs visited by the simulations.\nProducing \u03c4 using RTDP-Bel should serve as a tool for detecting the most important decisions of \u03c3. The intuition is that whenever a decision in a belief is made repeatedly in many simulations, it is worth remembering in the DT.\nExecution of a DT Policy. The agent maintains the current belief over the state-space. The decision tree T represents a function from beliefs to recommended actions as described in Section 5.1. In every step, the agent computes the value of the function for the current belief. Whenever a non-allowed action is recommended by the decision tree, we detect this and play all allowed actions uniformly at random, taking advantage of the precomputed set of allowed actions. Note that similar situation may also arise when executing a policy computed by RTDP-Bel: such a policy does not typically store decisions for all beliefs (see Section 4) and hence it might happen during its execution that no entry for the current belief can be found. Whatever action the implementation chooses in such a situation, the action must be allowed in the current belief-support, because after playing a non-allowed action, the set of target states would not be reached with probability 1. It is thus reasonable to assume that the precomputed information on allowed actions must be stored when executing any of the two types of policies. Hence, we view the difference between the size of the RTDP-Bel policy (i.e. the number of entries in its table) and the size of the DT learned from this policy (i.e. the number of its nodes) to be the primary measure of how much succinctness can be achieved by using decision trees.\nComparison with Finite-state Controllers. We illustrate the conceptual differences between finite-state controllers and decision trees on a toy POMDP example. Figure 1 depicts the Energy-constrained Tiger POMDP which is an extension of the famous Tiger problem introduced in [32]. Imagine an agent standing in front of two closed doors. Behind one of the doors is a tiger and behind the other is a treasure. If the agent opens the door with the tiger a huge cost is received. In all other situations no cost is received. After opening any of the doors the POMDP reaches a terminal configuration. The agent can also listen, in order to gain some information about the location of the tiger. Unfortunately, listening is not entirely accurate. There is a 15% chance that the agent will hear a tiger be-\nhind the left-hand door when the tiger is really behind the right-hand door, and vice versa. We consider two cases in the first one the listening action has no resource consumption and in the second case it has a resource consumption associated with the action. In the second case if the agent is running out of the resource, it may decide to recharge in a neighbouring location.\nIn the case without energy consumption, the optimal cost is 0, as for every \u03b5 \u2265 0 there is a number T such that after performing L listening actions the robot builds enough confidence about the position of the tiger so as to choose the good door with probability at least 1 \u2212 \u03b5. At the same time, L listening actions are necessary to build such a confidence, and L \u2192 \u221e as \u03b5\u2192 0. To represent a policy that waits until enough confidence is built and then decides we can employ both FSCs and DTs. In the case of FSC the corresponding finite transducer needs to have number of states proportional to L, as it needs to count the number of listening actions so far as well as results of these actions. In a DT case the policy can be represented using a DT with 5 nodes, which is depicted in Figure 2. We use two variables Tiger \u2212 Left and Tiger \u2212 Right, that represent the probability that the tiger is behind the respective door. In each step the robot straightforwardly extracts these probabilities from its current belief and uses the decision tree to select an appropriate action. (A solid line to a successor is taken if the condition inside an inner node is satisfied, otherwise the dashed line is taken. The parameter \u03b4 represents the confidence level sufficient for door selection.) In particular, the size of the tree is independent of \u03b5.\nIn the case with energy consumption, the FSC either needs to store the information on the current resource level in the transducer\u2019s state, or, if the resource levels are contained in observations (as is the case in our reduction in the previous section), the FSC needs to have, in each of its states, at least one outgoing transition per each energy level (as there must be at least one outgoing transition for each possible observation). Thus, the size of such a controller is proportional also to the capacity cap. On the other hand, using a DT it suffices to slightly modify the tree in Figure 2. We need to use an additional variable representing the current resource level (which we assume is precisely known to the robot), and add a new root node which tests whether\nthe energy is greater than 1: if it is, the robot goes to a sub-tree depicted in Figure 2, otherwise it recharges in the neighbouring location."}, {"heading": "6 Experimental results", "text": "We have extended the POMDP file format introduced in [14] with the constructs necessary to model resource consumption. We have implemented Algorithm 1 that implements the product construction of Section 4. We use a modified version of the RTDP-Bel POMDP solver [8] to solve these energy-constrained POMDPs and generate training data for machine learning tools (see the previous section).\nAlgorithm 1\nInput: POMDP M with energy-reachability objective Output: A succinct policy \u03c3 if there exists one\nM\u00d7 \u2190 constructProduct(M) . Section 4 \u03c3r \u2190 RTDP\u2212Bel(M\u00d7) \u03c4 \u2190 trainingData(\u03c3r) . Section 5 \u03c3 \u2190 trainTree(\u03c4) . Section 5 return: \u03c3\nGenerating Training Data. In all the examples we consider we choose appropriate variables to characterize the beliefs , e.g. in grid-like environments we have variables for the current row, column, and current resource level (recall that resource level is perfectly observable). We parse the policy produced by RTDP-Bel to obtain a training sequence (~v1, a1), (~v2, a2), . . . where each ~vi is a vector of variable values characterizing a belief to which the RTDP-Bel policy assigns action ai.\nDecision Tree Learning. We use three different methods to obtain decision trees for our examples:\n1. In the first scenario decision trees are constructed using the Weka machine learning package [27]. The Weka suite offers various decision tree classifiers. We use the J48 classifier, which is an implementation of the C4.5 algorithm [45]; 2. We use the package rpart [58] from R [46] which implements the CART algorithms of [12]. We use the Gini index as default for selecting the best splits. We have experimented with tree pruning using complexity parameters (see [58]).\n3. Finally, we constructed trees using the package tree [47] of R which implements algorithms of [48]. In this case, the default measure for selecting splits is the deviance (see [48]). Note that the results of tree and rpart packages are usually very different.\nWe experimented on two well-known examples of POMDPs naturally extended with resource levels. Typically, we assume that all the actions of the agent decrease the resource level and there are specific recharging locations in the area that restore the resource level to the maximum capacity. The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8]. (B) We consider the RockSample example from [8, 53]."}, {"heading": "6.1 Succinct Policies: Example", "text": "In this part we discuss an example of a succinct human readable policy for the Hallway examples\nExample 1. Figure 3 shows a decision tree computed via Tree package for an instance of a Hallway example, which models a robot navigation in a maze. We use variable names x1, . . . , x8, y1, . . . , y8,Energy, where the values xn, yn represent the probability that the x or y coordinate, respectively, of the robot is equal to n (we have B = 20, i.e. value 20 represents probability 1). For better readability, predicates are not contained within nodes, they label edges instead. To execute a policy represented by the tree, the robot looks, in every step, on its current belief. If the current resource level is at least 3 (recall that resource levels are integers), it performs action 0 (\u201dmove forward\u201d). Otherwise the robot checks the probability of the current y coordinate being 7. If it is smaller than 12 , the robot turns left (action 1), otherwise it turns right (action 2). If at any point the action recommended by the tree is not allowed, the robot chooses an allowed action uniformly at random."}, {"heading": "6.2 Discussion on Experimental results.", "text": "We present the results of our approach in Table 1. Every entry contains the following information: (i) the name of the benchmark; (ii) the size of the state space; (iii) the maximum resource level cap; (iv) the size of the the product state space S\u00d7 after a preprocessing\nstep which removes unreachable states; (v) the value of the policy \u03c3all that plays all allowed actions uniformly at random; (vi) RTDP-Bel entries, that present the size of the computed policy and its corresponding value; (vii) for Weka, RPart, Tree we present the size of the computed decision tree and the value of the corresponding policy. The entries labelled with \u201d-\u201d did not have a run that reaches the set of target states T , entries in italics do not reach the set of target states T with all the runs within the run cut-off length of 1000, i.e., the expected cost for the policies may be higher. The bold-faced entries present the best result among the three considered DT-learning tools.\nThe entries show that removing the unreachable states from the product POMDP M\u00d7 is efficient and allows scaling to larger instances, e.g., a naive product construction in entry Hallway 10x10 would yield a product POMDP with 3885 states compared to the 2437 reachable states.\nIn most of the cases our approach succeeded and computed a significantly smaller policy than the standard policy computed by RTDP-Bel. The computed succinct policies usually perform slightly worse than the optimal explicit policy, however still overwhelmingly outperform the naive policy \u03c3all that plays all actions uniformly at random. For instance, the tree for Hallway8x8 presented in Example 1 identifies two crucial decisions (one based on current resource level, and one on crossing a certain \u201dlatitude\u201d) with which the policy already significantly outperforms \u03c3all and achieves performance relatively close to the RTDP-Bel policy of size 537. Interestingly, this suggests that POMDPs with SSP objectives exhibit a phenomenon known as Pareto principle [39], where a small fraction of decisions accounts for majority of optimization effort. Moreover, the learning techniques we used are typically able to identify such decisions. Even for RockSample examples, where there is not much room for imprecision (\u03c3all incurs only the double the cost incurred by the corresponding RTDPBel policy) the DT policies performed relatively well on\nsome instances.\nInterestingly, no approach significantly outperforms any other and for each of the three considered approaches there are entries where it dominates. On a fraction of examples, such as Hallway 5x5 or on the large RockSample instances, the learned policies do not perform well. One way of improving the performance of DT policies would be to use more expressive variants of DTs, such as linear DTs, that can capture general linear dependencies between variables. Testing this conjecture would require extending the aforementioned DT-learning tools with the capability of learning linear DTs, which we deem to be a viable direction of future work."}, {"heading": "7 Conclusion", "text": "In this work we have considered POMDPs with a set of target states, positive integer costs associated with every transition, and resource levels. We present a novel algorithm for solving POMDPs enhanced with resource levels based on the existing POMDP solvers and the RTDP method. We consider three different approaches to obtain succinct and human-readable policies. On two scalable domains from the existing literature we present succinct policies that perform only slightly worse than the optimal policy, while being significantly smaller."}], "references": [{"title": "Constrained Markov Decision Processes", "author": ["Eitan Altman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "A Closer Look at MOMDPs", "author": ["Mauricio Araya-L\u00f3pez", "Vincent Thomas", "Olivier Buffet", "Francois Charpillet"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Probabilistic \u03c9-automata", "author": ["Christel Baier", "Marcus Gr\u00f6sser", "Nathalie Bertrand"], "venue": "J. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning to act using real-time dynamic programming", "author": ["Andrew G Barto", "Steven J Bradtke", "Satinder P Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Dynamic Programming and Optimal Control", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Probability and Measure", "author": ["Patrick Billingsley"], "venue": "Wiley, 3rd edition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Learning Sorting and Decision Trees with POMDPs", "author": ["Blai Bonet", "H\u00e9ctor Geffner"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Solving POMDPs: RTDP-Bel vs. Point-based Algorithms", "author": ["Blai Bonet", "H\u00e9ctor Geffner"], "venue": "In IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Computing optimal policies for partially observable decision processes using compact representations", "author": ["Craig Boutilier", "David Poole"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Counterexample Explanation by Learning Small Strategies in Markov Decision Processes", "author": ["Tom\u00e1\u0161 Br\u00e1zdil", "Krishnendu Chatterjee", "Martin Chme\u013a\u0131k", "Andreas Fellner", "Jan Kret\u0301\u0131nsk\u00fd"], "venue": "In Proceedings of Computer Aided Verification - 27th International Conference (CAV 2015),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Efficient Controller Synthesis for Consumption Games with Multiple Resource Types", "author": ["Tom\u00e1\u0161 Br\u00e1zdil", "Krishnendu Chatterjee", "Anto\u0144\u0131n Ku\u010dera", "Petr Novotn\u00fd"], "venue": "Computer Aided Verification 2012,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Classification and Regression Trees", "author": ["Leo Breiman", "Jerome H. Friedman", "Charles J. Stone", "Richard A. Olshen"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1984}, {"title": "Multi-Robot Coordinated Decision Making under Mixed Observability through Decentralized Data Fusion", "author": ["Jesus Capit\u00e1n", "Luis Merino", "Anibal Ollero"], "venue": "In Proceedings of the 11th International Conference on Mobile Robots and Competitions (Robotica", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "MOMDPs: a solution for modelling adaptive management problems", "author": ["Iadine Chades", "Josie Carwardine", "Tara Martin", "Samuel Nicol", "R\u00e9gis Sabbadin", "Olivier Buffet"], "venue": "In Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Optimal Cost Almost- Sure Reachability in POMDPs", "author": ["Krishnendu Chatterjee", "Martin Chme\u013a\u0131k", "Raghav Gupta", "Ayush Kanodia"], "venue": "In AAAI 2015,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Energy and Mean-Payoff Parity Markov Decision Processes", "author": ["Krishnendu Chatterjee", "Laurent Doyen"], "venue": "In Proceedings of MFCS 2011,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Qualitative Analysis of Partially-Observable Markov Decision Processes", "author": ["Krishnendu Chatterjee", "Laurent Doyen", "Thomas A. Henzinger"], "venue": "In MFCS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A survey of partial observation stochastic parity games", "author": ["Krishnendu Chatterjee", "Laurent Doyen", "Thomas A Henzinger"], "venue": "Formal Methods in System Design,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Incremental Policy Iteration with Guaranteed Escape from Local Optima in POMDP Planning", "author": ["Marek Grzes", "Pascal Poupart"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Controller Compilation and Compression for Resource Constrained Applications", "author": ["Marek Grzes", "Pascal Poupart", "Jesse Hoey"], "venue": "editors, ADT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Isomorph-Free Branch and Bound Search for Finite State Controllers", "author": ["Marek Grzes", "Pascal Poupart", "Jesse Hoey"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Policy tree: Adaptive representation for policy gradient", "author": ["Ujjwal Das Gupta", "Erik Talvitie", "Michael Bowling"], "venue": "In Proceedings of the Twenty- Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Simplicity considered fundamental to design for predictability", "author": ["Wolfgang A Halang"], "venue": "In Design of Systems with Predictable Behaviour,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "The WEKA data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "ACM SIGKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "An Improved Policy Iteration Algorithm for Partially Observable MDPs", "author": ["Eric A. Hansen"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "The power of 10: rules for developing safety-critical code", "author": ["Gerard J. Holzmann"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Piecewise Linear Dynamic Programming for Constrained POMDPs", "author": ["Joshua D. Isom", "Sean P. Meyn", "Richard D. Braatz"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Finding Optimal Observation-based Policies for Constrained POMDPs under the Expected Average Reward Criterion", "author": ["Xiaofeng Jiang", "Hongsheng Xi", "Xiaodong Wang", "Falin Liu"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie P. Kaelbling", "Michael L. Littman", "Anthony R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "Point-Based Value Iteration for Constrained POMDPs", "author": ["Dongho Kim", "Jaesong Lee", "Kee-Eung Kim", "Pascal Poupart"], "venue": "IJCAI/AAAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Real-time systems: design principles for distributed embedded applications", "author": ["Hermann Kopetz"], "venue": "Springer Science & Business Media,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Learning policies for partially observable environments: Scaling up", "author": ["Michael L. Littman", "Anthony R. Cassandra", "Leslie P. Kaelbling"], "venue": "In ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1995}, {"title": "Solving POMDPs by searching the space of finite policies", "author": ["Nicolas Meuleau", "Kee-Eung Kim", "Leslie Pack Kaelbling", "Anthony R. Cassandra"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Machine Learning", "author": ["Thomas M. Mitchell"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Power laws, Pareto distributions and Zipf\u2019s law", "author": ["Mark E.J. Newman"], "venue": "Contemporary physics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Planning under Uncertainty for Robotic Tasks with Mixed Observability", "author": ["Sylvie C.W. Ong", "Shao Wei Png", "David Hsu", "Wee Sun Lee"], "venue": "I. J. Robotic Res.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "The complexity of Markov decision processes", "author": ["Christos H. Papadimitriou", "John N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1987}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["Joelle Pineau", "Geoff Gordon", "Sebastian Thrun", "Others"], "venue": "In IJCAI,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2003}, {"title": "Induction of decision trees", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1986}, {"title": "Programs for Machine Learning", "author": ["J. Ross Quinlan. C"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1993}, {"title": "A Language and Environment for Statistical Computing", "author": ["R R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "tree: Classification and Regression Trees, 2015. R package version 1.0-36", "author": ["Brian Ripley"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Pattern recognition and neural networks", "author": ["Brian D. Ripley"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1996}, {"title": "Optimal scheduling of interactive and noninteractive traffic in telecommunication systems", "author": ["Keith W. Ross", "Bintong Chen"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1988}, {"title": "Markov decision processes with sample path constraints: the communicating case", "author": ["Keith W. Ross", "Ravi Varadarajan"], "venue": "Operations Research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1989}, {"title": "Multichain Markov Decision Processes with a Sample Path Constraint: A Decomposition", "author": ["Keith W. Ross", "Ravi Varadarajan"], "venue": "Approach. Math. Oper. Res.,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1991}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["Richard D Smallwood", "Edward J. Sondik"], "venue": "Operations Research,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1973}, {"title": "Heuristic search value iteration for POMDPs", "author": ["Trey Smith", "Reid Simmons"], "venue": "In UAI,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "The Optimal Control of Partially Observable Markov Processes over the Infinite Horizon: Discounted Costs", "author": ["Edward J. Sondik"], "venue": "Operations Research,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1978}, {"title": "A pointbased POMDP algorithm for robot planning", "author": ["Matthijs T.J. Spaan", "Nikos Vlassis"], "venue": "In ICRA. IEEE,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2004}, {"title": "The Ten Commandments for C Programmers (Annotated Edition)", "author": ["Henry Spencer"], "venue": "https://www. lysator.liu.se/c/ten-commandments.html. Accessed: November", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2016}, {"title": "rpart: Recursive Partitioning and Regression Trees, 2015. R package version 4.1-10", "author": ["Terry Therneau", "Beth Atkinson", "Brian Ripley"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "An online algorithm for constrained POMDPs", "author": ["Aditya Undurti", "Jonathan P. How"], "venue": "In ICRA,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "A decentralized approach to multi-agent planning in the presence of constraints and uncertainty", "author": ["Aditya Undurti", "Jonathan P. How"], "venue": "In ICRA,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Sample-Based Policy Iteration for Constrained DEC-POMDPs", "author": ["Feng Wu", "Nicholas R. Jennings", "Xiaoping Chen"], "venue": "ECAI, volume 242 of Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2012}], "referenceMentions": [{"referenceID": 29, "context": "One of the main challenges that make the problem difficult is the presence of uncertainty about the state of the agent and its environment [33], caused for instance by the agent\u2019s unreliable sensors.", "startOffset": 139, "endOffset": 143}, {"referenceID": 37, "context": "Each POMDP describes a discrete, typically finitestate system that exhibits both probabilistic and nondeterministic behaviour [54, 41].", "startOffset": 126, "endOffset": 134}, {"referenceID": 47, "context": "cost) over a finite-horizon [52], or over an infinite horizon [55], where the sequence of rewards (resp.", "startOffset": 28, "endOffset": 32}, {"referenceID": 49, "context": "cost) over a finite-horizon [52], or over an infinite horizon [55], where the sequence of rewards (resp.", "startOffset": 62, "endOffset": 66}, {"referenceID": 38, "context": "costs) can be aggregated by considering the discounted reward [42, 53] or the average reward [43, 21], etc.", "startOffset": 62, "endOffset": 70}, {"referenceID": 48, "context": "costs) can be aggregated by considering the discounted reward [42, 53] or the average reward [43, 21], etc.", "startOffset": 62, "endOffset": 70}, {"referenceID": 7, "context": "Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.", "startOffset": 120, "endOffset": 130}, {"referenceID": 4, "context": "Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.", "startOffset": 120, "endOffset": 130}, {"referenceID": 14, "context": "Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.", "startOffset": 120, "endOffset": 130}, {"referenceID": 0, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 45, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 46, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 44, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 7, "context": "a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20].", "startOffset": 8, "endOffset": 11}, {"referenceID": 29, "context": "a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "the size of the POMDP [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "merous informal rules for safety-critical system design that enforce \u201dsimplicity\u201d and \u201dreadability\u201d [29, 57] as well as in academic treatments of the subject [35, Chapter 2 on \u201dSimplicity\u201d].", "startOffset": 100, "endOffset": 108}, {"referenceID": 51, "context": "merous informal rules for safety-critical system design that enforce \u201dsimplicity\u201d and \u201dreadability\u201d [29, 57] as well as in academic treatments of the subject [35, Chapter 2 on \u201dSimplicity\u201d].", "startOffset": 100, "endOffset": 108}, {"referenceID": 55, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 137, "endOffset": 149}, {"referenceID": 27, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 137, "endOffset": 149}, {"referenceID": 54, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 137, "endOffset": 149}, {"referenceID": 0, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "As mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints.", "startOffset": 82, "endOffset": 92}, {"referenceID": 4, "context": "As mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints.", "startOffset": 82, "endOffset": 92}, {"referenceID": 14, "context": "As mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints.", "startOffset": 82, "endOffset": 92}, {"referenceID": 15, "context": "Our notion of energy constraints is similar to the one used in verification, in particular to so called energy games and MDPs [16, 18] and consumption games [11], although none of these concepts was considered in a partially observable setting so far.", "startOffset": 126, "endOffset": 134}, {"referenceID": 10, "context": "Our notion of energy constraints is similar to the one used in verification, in particular to so called energy games and MDPs [16, 18] and consumption games [11], although none of these concepts was considered in a partially observable setting so far.", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "DTs have already been successfully used to represent policies in verification of perfectly observable MDPs modelled in the well-known PRISM tool [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "For POMDPs, in [9] they consider a situation where the POMDP itself is encoded succinctly using DTs and similar structures, and they use this assumption to design a specific algorithm", "startOffset": 15, "endOffset": 18}, {"referenceID": 29, "context": "1These should not be confused with policy trees that represent a complete behaviour of a POMDP under a fixed policy [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "In [7] they", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "DTs were also used to represent policies in a reinforcement-learning setting [25], where the agent has no a priori model of the environment.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 24, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 33, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 20, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 18, "context": "One crucial difference between previous approaches to policy succinctness in both POMDP [22] and other settings [25] is that in previous work they concurrently optimize both the performance of a policy and its size,", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "One crucial difference between previous approaches to policy succinctness in both POMDP [22] and other settings [25] is that in previous work they concurrently optimize both the performance of a policy and its size,", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "which requires dedicated algorithms, while we separate these tasks: first we search for a well-performing, though possibly \u201dugly\u201d policy, and then learn its succinct representation (similar approach was used in [23],", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "of all functions f : X \u2192 [0, 1] s.", "startOffset": 25, "endOffset": 31}, {"referenceID": 14, "context": "Informally, the probabilistic aspect of the observation function is captured in the transition function, and by enlarging the state space with the product with the observations, we obtain an observation function only on states [17].", "startOffset": 227, "endOffset": 231}, {"referenceID": 5, "context": "The construction of P is standard [6].", "startOffset": 34, "endOffset": 37}, {"referenceID": 36, "context": "In this our model resembles mixedobservability POMDPs [40, 2], and indeed in the next section we will present a transformation of POMDPs with energy constraints into standard POMDPs in which resource levels are a fully observable component of each state.", "startOffset": 54, "endOffset": 61}, {"referenceID": 1, "context": "In this our model resembles mixedobservability POMDPs [40, 2], and indeed in the next section we will present a transformation of POMDPs with energy constraints into standard POMDPs in which resource levels are a fully observable component of each state.", "startOffset": 54, "endOffset": 61}, {"referenceID": 1, "context": "Since, in the words of [2], online techniques cannot be probably adapted to benefit from mixed observability, we stick to standard POMDP formulations.", "startOffset": 23, "endOffset": 26}, {"referenceID": 13, "context": "[15, 13].", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[15, 13].", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "The problem of approximating optimal cost optCost in POMDPs without energy constraints for positive costs was shown to be decidable in [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "The problem of approximating optimal cost optCost for general costs (positive and negative) was proved to be undecidable in [17] already for POMDPs with-", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "Algorithm solving the qualitative reachability problem based on belief supports was presented in [17, 3].", "startOffset": 97, "endOffset": 104}, {"referenceID": 2, "context": "Algorithm solving the qualitative reachability problem based on belief supports was presented in [17, 3].", "startOffset": 97, "endOffset": 104}, {"referenceID": 16, "context": "POMDPs [19].", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "To solve the quantitative energy-reachability problem, we again use an algorithm for POMDPs without energy, namely the one from [17], and apply it to M\u00d7.", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "method, finds a policy of small cost that almost surely reaches T\u00d7, using a modified version of RTDP-Bel [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "RTDP-Bel is an adaptation of the real-time dynamic programming value iteration [4] to POMDPs.", "startOffset": 79, "endOffset": 82}, {"referenceID": 14, "context": "optimal policies on many instances where optimal costs can be computed using exact methods [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "the probability distribution over the set of states representing the likelihood of being in particular states given the current history of states and observations [33].", "startOffset": 163, "endOffset": 167}, {"referenceID": 39, "context": "One of the most popular formalisms suitable for this purpose are decision trees (DT see [44, 38]).", "startOffset": 88, "endOffset": 96}, {"referenceID": 34, "context": "One of the most popular formalisms suitable for this purpose are decision trees (DT see [44, 38]).", "startOffset": 88, "endOffset": 96}, {"referenceID": 9, "context": "For convenience, we follow closely the definition of DT used in [10].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "A standard process of learning according to the algorithm ID3 [44, 38] proceeds as follows:", "startOffset": 62, "endOffset": 70}, {"referenceID": 34, "context": "A standard process of learning according to the algorithm ID3 [44, 38] proceeds as follows:", "startOffset": 62, "endOffset": 70}, {"referenceID": 40, "context": "5 algorithm and refer to [45, 38].", "startOffset": 25, "endOffset": 33}, {"referenceID": 34, "context": "5 algorithm and refer to [45, 38].", "startOffset": 25, "endOffset": 33}, {"referenceID": 11, "context": "We also use the CART algorithm [12] with so called Gini index instead of the information gain to select the best splits (there are also differences in pruning).", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "Figure 1 depicts the Energy-constrained Tiger POMDP which is an extension of the famous Tiger problem introduced in [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "solver [8] to solve these energy-constrained POMDPs and generate training data for machine learning tools (see the previous section).", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": "In the first scenario decision trees are constructed using the Weka machine learning package [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "5 algorithm [45]; 2.", "startOffset": 12, "endOffset": 16}, {"referenceID": 52, "context": "We use the package rpart [58] from R [46] which implements the CART algorithms of [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "We use the package rpart [58] from R [46] which implements the CART algorithms of [12].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "We use the package rpart [58] from R [46] which implements the CART algorithms of [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 52, "context": "We have experimented with tree pruning using complexity parameters (see [58]).", "startOffset": 72, "endOffset": 76}, {"referenceID": 42, "context": "Finally, we constructed trees using the package tree [47] of R which implements algorithms of [48].", "startOffset": 53, "endOffset": 57}, {"referenceID": 43, "context": "Finally, we constructed trees using the package tree [47] of R which implements algorithms of [48].", "startOffset": 94, "endOffset": 98}, {"referenceID": 43, "context": "In this case, the default measure for selecting splits is the deviance (see [48]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 50, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 48, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 7, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 7, "context": "RockSample example from [8, 53].", "startOffset": 24, "endOffset": 31}, {"referenceID": 48, "context": "RockSample example from [8, 53].", "startOffset": 24, "endOffset": 31}, {"referenceID": 2, "context": "RockSample[3,4] 435 7 2403 12.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[3,4] 435 7 2403 12.", "startOffset": 10, "endOffset": 15}, {"referenceID": 2, "context": "638 RockSample[3,5] 1011 7 5425 12.", "startOffset": 14, "endOffset": 19}, {"referenceID": 4, "context": "638 RockSample[3,5] 1011 7 5425 12.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "275 RockSample[4,4] 771 7 4297 14.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "275 RockSample[4,4] 771 7 4297 14.", "startOffset": 14, "endOffset": 19}, {"referenceID": 4, "context": "644 RockSample[5,4] 1803 6 5679 16.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "644 RockSample[5,4] 1803 6 5679 16.", "startOffset": 14, "endOffset": 19}, {"referenceID": 35, "context": "Interestingly, this suggests that POMDPs with SSP objectives exhibit a phenomenon known as Pareto principle [39], where a small fraction of decisions accounts for majority of optimization effort.", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and positive integer costs associated with every transition. The traditional optimization objective (stochastic shortest path) asks to minimize the expected total cost until the target set is reached. We extend the traditional framework of POMDPs to model energy consumption, which represents a hard constraint. There are energy levels that may increase and decrease with transitions, and the hard constraint requires that the energy level must remain positive in all steps till the target is reached. Our contribution is twofold. First, we present a novel algorithm for solving POMDPs with energy levels, developing on existing POMDP solvers and using real-time dynamic programming as its main method. Our second contribution is related to policy representation. For larger POMDP instances the policies computed by existing solvers are too large to be understandable. We present an automated procedure based on machine learning techniques that automatically extracts important decisions of a policy and computes its succinct, human readable representation. Finally, we show experimentally that our algorithm performs well and computes succinct policies on a number of POMDP instances from the literature that were naturally enhanced with energy levels.", "creator": "LaTeX with hyperref package"}}}