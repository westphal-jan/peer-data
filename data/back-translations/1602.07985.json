{"id": "1602.07985", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "How effective can simple ordinal peer grading be?", "abstract": "Upon completion of an exam, each student is asked to qualitatively evaluate a set of test assignments taken by their fellow students. An aggregation rule then combines the individual rankings into a global rule that covers all students. We define a broad class of simple aggregation rules and present a theoretical framework for assessing their effectiveness. If statistical information about students \"grading behavior is available, the framework can be used to calculate the optimal rule from that class in terms of a set of performance goals. For example, a natural rule called Borda proves optimal if students grade correctly. In addition, we present extensive simulations and a field experiment that confirm our theory and prove that it is extremely accurate when there is only rough information about grading behavior.", "histories": [["v1", "Thu, 25 Feb 2016 16:37:57 GMT  (434kb,D)", "http://arxiv.org/abs/1602.07985v1", "23 pages, 5 figures, 4 tables"]], "COMMENTS": "23 pages, 5 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.AI cs.DS cs.LG", "authors": ["ioannis caragiannis", "george a krimpas", "alexandros a voudouris"], "accepted": false, "id": "1602.07985"}, "pdf": {"name": "1602.07985.pdf", "metadata": {"source": "CRF", "title": "How effective can simple ordinal peer grading be?\u2217", "authors": ["Ioannis Caragiannis", "George A. Krimpas", "Alexandros A. Voudouris"], "emails": ["caragian@ceid.upatras.gr.", "krimpas@ceid.upatras.gr.", "voudouris@ceid.upatras.gr."], "sections": [{"heading": "1 Introduction", "text": "Educational platforms such as Coursera, Udacity, and EdX provide easy access to high level education to everyone who has a decent Internet access. In September 2015, these platforms had more than 24 million users \u2014essentially, students attending the offered courses\u2014 and this number is expected to further increase in the near future. The term \u201cmassive open online course\u201d, or simply MOOC, is very descriptive of the service these platforms offer. A MOOC is the result of their partnership with a faculty member in a top university, whose role is to design the course and organize the course material so that it takes advantage of the most popular Internet apps that the platform utilizes. Courses offered include literally everything.\nEven though the service provided is certainly useful, the viability of MOOCs will strongly depend on their sources of revenues. Currently, investments from VCs have secured their survival for a short term, but what about their future? A feature that could be the main source of revenue for MOOCs is the so-called verified certificate which the students can get at a cost of a few dozens of dollars. The verified certificate keeps information about the performance of a student in a course (or in a chain of courses) and can be used to justify a student\u2019s quality to potential employers. So, the verified certificate should \u2217Emails: {caragian,krimpas,voudouris}@ceid.upatras.gr. This work was partially supported by COST Action IC1205 on \u201cComputational Social Choice\u201d and by the Caratheodory research grant E.114 from the University of Patras.\nar X\niv :1\n60 2.\n07 98\n5v 1\n[ cs\n.A I]\n2 5\nFe b\n20 16\nhave reliable information about the student performance in the courses she has participated in. Even though the means to guarantee this in the traditional University system is well-established, achieving this in a MOOC is a challenge.\nThe big issue is in the massive student participation. Of course, the Internet provides tools so that organizing exams with huge numbers of students is possible. But what about assessment and grading? As the most popular courses attract 50 000 students or more and the vision of MOOCs enthusiasts is for millions of students per course, is grading of assignments or exams possible? No doubt, professional graders would be extremely costly. Organizing the material using multiple-choice questions and answers that could be graded automatically cannot be an option when the students are asked to prepare an essay or a formal mathematical proof or express their critical thinking over some issue. Grading is a typical example of a human computation [11] task in these cases.\nThe only solution that seems consistent to the MOOCs vision is known as peer grading [10, 13, 16], according to which the grading task is outsourced to the students that participated to the exam themselves. This approach has been already implemented in some MOOCs and standalone experimental tools such as crowdgrader.org [8], peergrading.org [14], and our own co-rank1 [3] are already available. Even though the approach seems straightforward, there are subtle implementation issues. For example, allowing the students to use cardinal scores is problematic, since they participate both in the exam and in grading and they may have incentives to assign low grades in order to improve their personal relative performance. Even if we assume that they grade honestly, their experience in doing so is very limited and the result will most probably be unreliable.\nIn this paper, we focus our attention on ordinal peer grading, which has recently received attention in the AI and machine learning community [4, 14, 15]. Following the setting that we considered in our previous work [4], each student gets a bundle of a small number (our favourite number that we have extensively used recently is 6) of exam papers so that each exam paper is given to the same number of students. Each student has to rank the exam papers in her bundle (in terms of quality) and an aggregation rule will then combine the (partial) rankings submitted by the students and come up with a final ranking of all exam papers; this will be the grading outcome. Information about the position of a student in the final ranking (e.g., top 10% out of 33 000 students) can be included in her verified certificate.\nIn [4], we formally proved that a simple aggregation rule, inspired from Borda\u2019s rule from social choice theory [2], recovers correctly an expected fraction of 1 \u2212 O(1/k) of the pairwise relations in the underlying ground truth ranking, when bundles of size k are used and students make no mistakes when grading. The assumption for a ground truth and the comparison of the grading outcome to it is similar in spirit to recent approaches that combine voting and learning [5, 6, 7]. The new aspect in [4] (as well as in the current paper) is the relaxed requirement of recovering the ground truth only approximately. Experimental results show that this rule has very good performance in an imperfect grading scenario inspired by a noisy model of generating random rankings that has been proposed by Mallows [12]. Note that, unlike other studies [14, 15], we investigate the potential of applying ordinal peer grading exclusively, without involving any professionals in grading.\nIn this paper, we follow a different approach. To explain our rationale, we remark that theoretical analysis requires to handle with extra care dependencies between several random variables that appear due to the distribution of exam papers to bundles. The analysis of Borda was possible only due to its particular definition; we have not managed to extend the analysis to any other aggregation rule. Also, the O notation in the theoretical\n1Available at co-rank.ceid.upatras.gr.\nguarantee for Borda above hides large constant terms that constitute the bound of theoretical interest only. We would like to develop a \u201ctheory\u201d for determining the performance of Borda with the highest possible accuracy. And, of course, why should we restrict our study only to Borda?\nInstead, we define and study a large class of simple aggregation rules, which we call type-ordering aggregation rules; the class includes Borda. We present a theoretical framework for assessing the performance of each member of this class with respect to a series of performance objectives. A crucial step in our study is that we have completely neglected the dependencies between the random variables that make the rigorous analysis difficult. This sacrifice of mathematical rigor is formally incorrect unless the number of students tends to infinity; this can be justified by the massive participation in MOOCs. But the best justification of our approach is that the theoretical predictions of performance are experimentally shown to be exact, which means that the dependencies have no positive or negative impact on performance. Furthermore, once (statistical) information about the grading behaviour of students and the desired performance objectives are known, our framework can serve as an optimization toolkit for selecting the optimal type-ordering aggregation rule. This requires an exact solution to an instance of the feedback arc set problem which, albeit NP-hard in general, can be solved exactly for the instances that do arise.\nOur theoretical framework allows us to obtain a series of results. For example, we establish that Borda is the optimal type-ordering aggregation rule when students act as perfect graders. This is rather surprising, since Borda is among the simplest aggregation rules in the class we consider. Even though it was not observed to be optimal in any other scenario we considered, its performance is always extremely close to optimality. Furthermore, as mentioned above, the optimization task of deciding the optimal aggregation rule strongly depends on the information about grading behaviour. We study how inaccuracies of this information affect the choice of the optimal aggregation rule and its performance for the Mallows model. The results suggest a very minor impact and, essentially, a tiny sample of a student population is enough for building a fairly accurate model of grading behaviour.\nOverall, our approach combines theory, simulations, and experimentation and is presented graphically in Figure 1. The lower chain of the figure describes what one would expect from a simulated exam. There is a student population and some of them participate in an exam. The preparation level of the participating students that determines their performance in the exam is a random variable following a uniform probability distribution. After the exam, each student acts as the grader of a small number of exam papers submitted by other students. The grading performance can depend on the preparation level as well. The grades are combined using the aggregation rule and the final ranking is compared to the ground truth to come up with the observed performance.\nThe most interesting part of Figure 1 is the upper chain. First, a field experiment can be used to extract information about the student population, translated into a noise model. We have performed such a field experiment with students in our home institution; we describe it in detail and present the collected data later in the paper. These data are used to build a noise model which, together with the desired performance objective, are then given as input to the optimization engine which constructs the optimal aggregation rule for the particular noise model and performance objective, and reports a theoretical prediction about the performance the rule is expected to have. The optimal aggregation rule can also be applied to the grades from our simulated exams (hence, the vertical arrow in Figure 1) and a comparison of the theoretically predicted performance with the observed performance of the simulated exam will validate our theory.\nThe rest of the paper is structured as follows. We begin with preliminary definitions and useful notation in Section 2. The type-ordering aggregation rules and our theoretical framework are presented in Section 3. The field experiment and the validation of our framework are discussed in Section 4."}, {"heading": "2 Preliminaries", "text": "We assume that n students have participated in an exam and have submitted their exam papers. Our approach to ordinal peer grading has three distinct tasks: the distribution of papers to students, the grading task by each student, and the aggregation of the grades into a final result. We describe these tasks in detail here and give definitions that will be useful later."}, {"heading": "2.1 Distributing the exam papers", "text": "All students that participated in the exam will have to participate in grading as well. The goal of the first task is to balance their grading load. This is done by distributing (copies of) each paper to the students so that each exam paper is given to exactly k students and each student receives exactly k (distinct) exam papers. The k papers that a student receives form her bundle. These are the exam papers which the student has to grade. Crucially, the bundle of a student should not contain her own exam paper.\nA k-regular bipartite graph G = (U, V,E) with n nodes on each side of the bipartition (called an (n, k)-bundle graph in [4]) can be used to represent the distribution of exam papers to students. Each node of sets U and V represents a student. An edge of the graph G between a node u \u2208 U and a node v \u2208 V indicates that the exam paper of the student corresponding to node u is in the bundle of the student corresponding to node v. The restriction on the degree of the nodes of set U means that each exam paper is given to exactly k students and the restriction on the degree of the nodes of V means that all bundles have size k.\nIn our previous work, we considered bundle graphs that satisfy a particular structural property, namely they contain no cycle of length 4. This was a technical constraint that was required only in our theoretical analysis. Experimental results in that paper indicate that uniformly random k-regular bipartite graphs are almost as good as bundle graphs. These are the bundle graphs we considered in the current work. A random k-regular graph can be built as follows. Starting from the complete bipartite graph Kn,n with node sets U and V , first remove the edges between nodes corresponding to the same students in U and V . Then, draw a perfect matching uniformly at random among all perfect matchings of Kn,n that do not include previously removed edges. Repeat this step k times; this is possible\ndue to Hall\u2019s matching theorem since, at the beginning of every step, the edges that have not previously been removed form a regular bipartite graph. The union of the edges in the k perfect matchings forms the bundle graph."}, {"heading": "2.2 Modelling the grading task", "text": "Throughout the paper, we assume that there is an underlying strict ranking of the exam papers, the ground truth, which we aim to recover. As it will shortly become apparent, the setting we consider is so restrictive that we should not expect to recover the ground truth exactly. Instead, we would like to recover the ground truth approximately.\nA restriction of our setting is that each student is given only k exam papers to grade. Another restriction is that the grading task for each student is simply to rank the exam papers in her bundle, in decreasing order of quality. We consider different scenarios for the grading behaviour of the students. In a first scenario, assume that, after the end of the exam, the instructor announces indicative solutions and gives detailed instructions that the students can use during grading. Here, we assume that students will act as perfect graders. Admittedly, this is an unrealistic assumption but we include it as an extreme case in our study together with many others.\nOur most realistic scenario poses another challenging restriction. In general, we assume that students make mistakes when grading. For modelling purposes, we assume that a student grades as follows. When she receives a bundle of papers, she draws a random ranking according to a probability distribution that characterizes the grading behaviour of all students participating in the exam. So, the behaviour of a student in an imperfect grading scenario is characterized by a k \u00d7 k noise matrix P = (pi,j)i,j\u2208[k], where pi,j denotes the probability that the exam paper with correct rank j among the k exam papers in a bundle is ranked at position i by the grader. Clearly, a noise matrix is doubly stochastic, i.e., the sum of the entries in any column and any row is equal to 1. Observe that the corresponding noise matrix for perfect grading is the k \u00d7 k identity matrix. The term \u201cnoise model\u201d is often used as a synonym for the term \u201cnoise matrix\u201d.\nNote that a noise matrix provides only aggregate information over all students of a population. Actually, it is not hard to see that a doubly stochastic matrix may correspond to many different probability distributions over rankings. This kind of information will be the only tool we will use in our theoretical analysis. On the other hand, in our simulations, we will use more refined models of grader behaviour, assuming that it strongly depends on the level of preparation of the student for the exam and her success in it. A particular such model is the following. Each student has a quality drawn uniformly at random from the interval [1/2, 1] and affects her position in the ground truth and her ability to grade as well. The ground truth is the ranking of the students in decreasing order of quality. A student b of quality q performs the grading task as follows: she considers every pair of exam papers x and y in her bundle, such that x appears ahead of y in the ground truth, and temporarily determines x b y with probability q and y b x with probability 1 \u2212 q; the pairwise relation b will evolve into her ranking of the exam papers in her bundle. If, after considering all pairs of exam papers in the bundle, the pairwise relation b is cyclic, the whole process is repeated from scratch. Otherwise, the ranking of the exam papers in the bundle induced by b is the grading outcome of student b.\nThe above process has been implemented and is used in our simulations with bundles of size k = 6. In our theoretical analysis, we have computed the corresponding noise matrix by sampling 109 students with uniform qualities and simulating the grading behaviour\ndescribed above. The resulted noise matrix Pmallows is presented here for ease of reference.\nPmallows = \n0.6337 0.1753 0.0824 0.0494 0.0339 0.0253 0.1753 0.5112 0.1549 0.0768 0.0479 0.0339 0.0824 0.1549 0.4865 0.1500 0.0768 0.0494 0.0494 0.0768 0.1500 0.4865 0.1549 0.0824 0.0339 0.0479 0.0768 0.1549 0.5112 0.1753 0.0253 0.0339 0.0494 0.0824 0.1753 0.6337\n (1)\nDue to its similarities with the well-known Mallows model [12] for generating random rankings, we refer to this grading behaviour as Mallows grading."}, {"heading": "2.3 Aggregation rules", "text": "The third important task is to aggregate the partial rankings provided by the graders into a final output ranking. This is done using an aggregation rule. A simple but very compelling aggregation rule is inspired by the Borda count voting rule. In our context, Borda computes a score for each exam paper by examining the positions it has in the rankings of the graders that have this exam paper in their bundles. A first position by an exam paper contributes k points to its score, a second position contributes k \u2212 1 points, and so on. The outcome of Borda is a ranking of the exam papers in non-increasing order of their Borda scores. When we use Borda, we assume that ties are broken uniformly at random but other tie-breaking schemes could be considered as well. More generally, a positional scoring aggregation rule could use a different score vector (a1, a2, ..., ak) with a1 \u2265 a2 \u2265 ... \u2265 ak (and a1 > ak) in order to increase the score of each exam paper by ai whenever it is ranked i-th by a grader.\nIn our previous paper [4], we also considered several other aggregation rules such as a rule that we call Random Serial Dictatorship (RSD) as well as rules that are based on appropriately defined Markov chains. RSD is very slow in the computation of the final outcome and, even though it performs remarkably well with perfect graders, it has a poor performance in simulated exams with Mallows graders. We will not consider it in the current paper; actually, applying it with input from 10 000 graders, which is the typical scenario we consider in this paper, is a computational challenge. The aggregation rules that are based on Markov chains were defined in an unsuccessful attempt to distinguish between high and low quality graders and put more weight on the partial rankings of the former. These ideas are not considered in this work either. Instead, we focus on much simpler aggregation rules."}, {"heading": "3 Type-ordering aggregation rules", "text": "We will use the term type to refer to the grading result of an exam paper. The grading result for it consists of the ranks it gets from the k graders that have it in their bundles. So, the type is a vector of k integers from [k] = {1, 2, ..., k}. We follow the convention that the k entries in types appear in monotone non-decreasing order. We use"}, {"heading": "Tk = {\u03c3 = (\u03c31, \u03c32, ..., \u03c3k)|1 \u2264 \u03c31 \u2264 \u03c32 \u2264 ... \u2264 \u03c3k \u2264 k}", "text": "to denote the set of all types for bundle size k. It is not hard to see that Tk contains ( 2k\u22121 k ) different types.\nAs an example with k = 6, an exam paper of type (1, 2, 2, 2, 2, 5) is ranked first by one of its graders, second by four graders, and fifth by one grader. Now, consider another exam\npaper of type (2, 2, 2, 2, 3, 3) and observe that Borda would give to both exam papers the same Borda score of 28. Is there some particular reason for which these two exam papers should be very close in the final ranking? Now, consider the two types (1, 1, 1, 2, 5, 6) and (2, 2, 2, 3, 3, 3) of Borda scores 26 and 27, respectively. Borda indicates that an exam paper with the second type is better. But looking carefully at the ranks, we could come up with the following interpretation. The first exam paper is very good (and most probably in one of the two top positions in any bundle) and the two low ranks are due to poor judgement by the graders. In contrast, the second exam paper is just above average and this is reflected in all grades. Of course, such interpretations are valid only when they can be supported by information about the graders. But, certainly, there are cases where such interpretations are indeed valid.\nSo, it seems that Borda is restrictive; then, one would think that this is due to the particular scores that Borda uses. We will not consider the task of investigating whether different scores could yield better results but will instead define a much broader class of aggregation rules. A type-ordering aggregation rule uses a strict ordering of the types in Tk. Then, the final ranking of the exam papers follows the ordering of their types, breaking ties uniformly at random. In general, rules of this class seem to be very powerful. Compared to Borda which partitions the set of exam papers into only k2 different scores, a type-ordering aggregation rule can distinguish between exponentially many (in terms of k) different types. In the following, we use the term Borda ordering to refer to any ordering of the types in non-increasing order of Borda score. We also use B(\u03c3) to denote the Borda score of an exam paper with type \u03c3 = (\u03c31, ..., \u03c3k). Clearly, B(\u03c3) = \u2211k i=1 (k + 1\u2212 \u03c3i) =\nk2 + k \u2212 \u2211k\ni=1 \u03c3i. We remark that the use of types in the definition of a broad class of aggregation rules has been possible due to the regularity that we imposed on the bundles and the distribution of exam papers to them. Of course, this creates issues related to the theoretical analysis of these rules (such as dependencies between the random variables involved in the distribution to bundles and in grading) and it should be expected that their rigorous analysis will be much more involved than the analysis of Borda (which is already quite complicated; see [4]). In the next section, we discuss how to overcome such issues."}, {"heading": "3.1 A framework for theoretical analysis", "text": "For the analysis of type-ordering aggregation rules, we will assume an infinite number of students. This is close to the vision of MOOCs with huge numbers of enrolled students and is the important assumption that constitutes the theoretical analysis possible. So, the positions of students in the ground truth ranking can be thought of as occupying the continuum of the interval [0, 1]. We will usually identify an exam paper as a real number x \u2208 [0, 1], i.e., by its rank in the ground truth ranking. Furthermore, we will assume that in each of the k bundles to which exam paper x belongs, the remaining k\u2212 1 exam papers are selected uniformly at random from the student population. Our assumption of infinitely many students allows us to ignore subtleties such as the requirement that all students in a bundle should be distinct and also different than the student that acts as the grader of the bundle (the probability that this requirement will not be satisfied in some bundle is zero).\nConsider an aggregation rule that uses an ordering of the types defined by bundles of size k and is applied to partial rankings provided by graders whose behaviour follows the noise model P . Let us focus on computing the expected number of pairwise relations in the ground truth ranking that are correctly recovered in the outcome of the rule. It suffices to\nconsider every pair of exam papers x, y \u2208 [0, 1] with x < y (i.e., exam paper x has a better rank in the ground truth compared to exam paper y) and add one point if x has a better type than y according to the ordering , and half a point if both exam papers have the same type. In this last case, the tie is resolved uniformly at random and the probability that the correct pairwise relation will be recovered is 1/2. Hence, denoting the expected fraction of pairwise relations recovered by the rule by C (we will refine this notation in a while) and the event that exam paper x gets type \u03c3 after grading by xB \u03c3, we have\nC = \u222b 1 0 \u222b 1 x  \u2211 \u03c3,\u03c3\u2032:\u03c3 \u03c3\u2032 Pr[xB \u03c3 and y B \u03c3\u2032] + 1 2 \u2211 \u03c3 Pr[xB \u03c3 and y B \u03c3]  dy dx =\n\u2211 \u03c3,\u03c3\u2032:\u03c3 \u03c3\u2032 \u222b 1 0 \u222b 1 x Pr[xB \u03c3 and y B \u03c3\u2032] dy dx+ 1 2 \u2211 \u03c3 \u222b 1 0 \u222b 1 x Pr[xB \u03c3 and y B \u03c3] dy dx\nThe first sum runs over all pairs of different types \u03c3, \u03c3\u2032 with order \u03c3 \u03c3\u2032 and the second sum runs over all types. The scary (at first glance) double integral can be hidden under the notation W (\u03c3, \u03c3\u2032) to obtain\nC = \u2211\n\u03c3,\u03c3\u2032:\u03c3 \u03c3\u2032 W (\u03c3, \u03c3\u2032) +\n1\n2 \u2211 \u03c3 W (\u03c3, \u03c3). (2)\nWe will use the term weight to refer to the quantity W (\u03c3, \u03c3\u2032). Our assumption for an infinite number of students nullifies any dependencies between the rank vectors that exam papers x and y get after grading. So, the events xB \u03c3 and y B \u03c3\u2032 are independent and the definition of the weight W (\u03c3, \u03c3\u2032) becomes\nW (\u03c3, \u03c3\u2032) = \u222b 1 0 \u222b 1 x Pr[xB \u03c3] \u00b7 Pr[y B \u03c3\u2032] dy dx. (3)\nLet us now compute the probability that exam paper x gets type \u03c3 = (\u03c31, ..., \u03c3k). By considering all ways to distribute the entries of the type vector as ranks of an exam paper by the graders that handle it (ignoring symmetries), there are\nN(\u03c3) = k!\nd1! \u00b7 ... \u00b7 dk!\nways that the exam paper can get type \u03c3, where di is the number of graders that have the exam paper ranked i-th. Again, due to our assumption for infinitely many students and the uniform inclusion of them into bundles, the quality of each exam paper included in a bundle does not affect the quality of other exam papers (in the same or different bundles). Clearly, the grading by different students is performed without dependencies either. Denoting by E(x, \u03c3i) the event that exam paper x is ranked \u03c3i-th in a bundle, the probability that x is of type \u03c3 is\nPr[xB \u03c3] = N(\u03c3) k\u220f i=1 Pr[E(x, \u03c3i)].\nTo compute Pr[E(x, \u03c3i)], it suffices to consider all possible true ranks that exam paper x may have in a bundle and account for the probability of having such a rank and being\nranked \u03c3i-th by the grader handling the bundle. Let us denote by E\u2217(x, j) the event that the true rank of x in a bundle is j. Then,\nPr[xB \u03c3] = N(\u03c3) k\u220f i=1 k\u2211 j=1 p\u03c3i,j Pr[E\u2217(x, j)].\nNow, the probability Pr[E\u2217(x, j)] is equal to the number of ways we can choose j \u2212 1 exam papers to be ahead of x times the probability that all of them will indeed be ahead of x in the bundle times the probability that the rest k \u2212 j exam papers in the bundle will have true ranks worse than j. We use Lk to denote the set of all k-entry vectors ` = (`1, ..., `k) with `i \u2208 [k] and, for compactness of notation, we abbreviate \u2211k i=1 `i by |`|1. We have\nPr[xB \u03c3] = N(\u03c3) k\u220f i=1 k\u2211 j=1 p\u03c3i,j ( k \u2212 1 j \u2212 1 ) xj\u22121(1\u2212 x)k\u2212j (4)\n= N(\u03c3) \u2211 `\u2208Lk k\u220f i=1 p\u03c3i,`i ( k \u2212 1 `i \u2212 1 ) x`i\u22121(1\u2212 x)k\u2212`i\n= N(\u03c3) \u2211 `\u2208Lk ( k\u220f i=1 p\u03c3i,`i ( k \u2212 1 `i \u2212 1 )) x|`|1\u2212k(1\u2212 x)k2\u2212|`|1 ,\nwhere the second equality is obtained by exchanging the sum and product operators. Using the fact that (1\u2212 x)m = \u2211m j=0 ( m j ) (\u22121)jxj for m = k2 \u2212 |`|1, we obtain\nPr[xB \u03c3] = N(\u03c3) \u2211 `\u2208Lk ( k\u220f i=1 p\u03c3i,`i ( k \u2212 1 `i \u2212 1 )) x|`|1\u2212k k2\u2212|`|1\u2211 j=0 ( k2 \u2212 |`|1 j ) (\u22121)jxj\n= N(\u03c3) \u2211 `\u2208Lk k2\u2212|`|1\u2211 j=0 ( k\u220f i=1 p\u03c3i,`i ( k \u2212 1 `i \u2212 1 ))( k2 \u2212 |`|1 j ) (\u22121)jx|`|1\u2212k+j . (5)\nInterestingly, Pr[x B \u03c3] is a univariate polynomial of degree k2 \u2212 k. Then, the double integral can be computed analytically. The computation is tedious but straightforward. In Appendix A, we show how to compute such integrals."}, {"heading": "3.2 Computing optimal type-ordering aggregation rules", "text": "The approach in Section 3.1 suggests a general way of evaluating the performance of any type-ordering aggregation rule. In order to compute the expected number of correctly recovered pairwise relations, it suffices to use equations (2), (3), and (5). Equation (5) can be used to obtain Pr[xB \u03c3], which is then used in equation (3) to compute the weights (for any possible pair of types \u03c3 and \u03c3\u2032). Finally, equation (2) returns the expected number of correctly recovered pairwise relations.\nOf course, the expected number of correctly recovered pairwise relations is not the only performance objective one would like to measure. For example, we could simply ignore exam papers that are very close in the ground truth ranking. The ground truth ranking is mostly a modelling assumption and it should not be very restrictive in the evaluation of an aggregation rule. So, we could just measure the expected number of correctly recovered pairwise relations between pairs of exam papers with ranks in the ground truth that differ\nby at least a% (for small values such as 5%). Another possibility would be to ignore pairwise relations between pairs of exam papers that have both very low rank in the ground truth. For example, why is it important to recover correctly the pairwise relation between the students that have true ranks 80% and 95%? A general objective in this direction would be to measure the correctly recovered relations between pairs of exam papers that involve one with true rank in the top a% (e.g., 20%).\nOur theoretical framework can be easily extended to handle such cases. In general, a performance objective is defined by a bivariate function f : [0, 1]2 \u2192 [0, 1] which returns the importance of measuring a correctly recovered relation between two students x and y with x \u2264 y. In the presentation of our framework in Section 3.1, we have assumed such a function with f(x, y) = 1 for every student pair. The two scenarios of the previous paragraph can be captured by the function (i) f(x, y) = 1 when y \u2212 x \u2265 a% and f(x, y) = 0 otherwise, and (ii) f(x, y) = 1 when x \u2264 a% (and x \u2264 y) and f(x, y) = 0 otherwise. Many other performance objectives can be defined including ones in which the function f returns fractional values between 0 and 1.\nThe only modification in the computation of Section 3.1 is in the computation of the weights which should become\nW (\u03c3, \u03c3\u2032) = \u222b 1 0 \u222b 1 x f(x, y) Pr[xB \u03c3] \u00b7 Pr[y B \u03c3\u2032] dy dx. (6)\nIn order to capture the generality of the scenarios considered, we overload the notation for the performance measure C to specify the bundle size k, the aggregation rule , the noise matrix P describing the grading behaviour, and the performance objective f .\nTheorem 1. Consider a type-ordering aggregation rule that is applied on k-sized partial rankings from an infinite population of students with grading behaviour that follows a noise matrix P . Then, the fraction of correctly recovered pairwise relations that satisfy the performance objective given by the bivariate function f is\nC(k, , P, f) = \u2211\n\u03c3,\u03c3\u2032:\u03c3 \u03c3\u2032 W (\u03c3, \u03c3\u2032) +\n1\n2 \u2211 \u03c3 W (\u03c3, \u03c3), (7)\nwhere W (\u03c3, \u03c3\u2032) is given by (6) and Pr[xB \u03c3] is in turn given by (5).\nNote that the weights do not depend on the aggregation rule at all. They depend on the grading behaviour and the performance objective. Instead, the aggregation rule determines only the particular weights that should be summed up in order to compute C(k, , P, f). This means that, once we have information about the bundle size, the grading behaviour, and the desired performance objective, we can seek for the type-ordering aggregation rule that is optimal for this particular scenario. All we have to do is to compute the type-ordering aggregation rule that maximizes C(k, , P, f) which, actually, translates to computing an ordering of the types so that the leftmost summation in the definition (7) is maximized.\nIt is not hard to see that the problem is equivalent to solving the feedback arc set problem on an edge-weighted complete directed graph. In particular, the input is a complete directed graph that has a node for each type \u03c3 \u2208 Tk. A directed edge from a node corresponding to type \u03c3 towards a node corresponding to type \u03c3\u2032 has weight W (\u03c3, \u03c3\u2032). Now, the objective is to find an ordering of the nodes so that the total weight of \u201cconsistently directed\u201d edges from a node to a node of higher rank in the ordering is maximized. The next statement should now be obvious.\nTheorem 2. Computing the optimal type-ordering aggregation rule for a scenario involving specific bundle size, grading behaviour, and desired performance objective is equivalent to solving feedback arc set on an edge-weighted complete directed graph.\nFeedback arc set (FAS) is NP-hard even in its very simple variant on unweighted tournaments [1]. The particular weighted version we consider here admits a PTAS [9]. Unfortunately, the solutions that such a PTAS can guarantee in reasonable time are sufficiently far from optimality and the resulting type-ordering aggregation rule will consequently have highly suboptimal performance. Fortunately, the FAS instances that we had to solve in order to compute optimal rules have a very nice structure for all the scenarios considered. This structure allows us to compute the optimal FAS solution (almost) exactly by a straightforward algorithm that we present in the following. We strongly believe that this nice property holds in any scenario that can appear in practice.\nLet us assume that we would like to solve FAS on an edge-weighted complete directed graphG = (V,E,w) and to compute an ordering of the nodes of V so that the total weight of edges in the direction that is consistent to the ordering is as high as possible. First observe that if two opposite directed edges have the same weight, the ordering of its endpoints does not affect the contribution of the consistently directed edge. So, the decision about the relative order of such non-critical node pairs can be postponed until the very end of the algorithm and any decision about them will be just fine. Now, consider two nodes u and v of G such that w(u, v) > w(v, u); then, the consistently directed edge that we would like to have in the final solution is (u, v). We will call such pairs of nodes critical pairs. Decisions about the ordering of critical pairs of nodes have to be taken first. An ideal situation would be if after deciding the critical node pairs, we came up with a partial ordering of all nodes that participate in at least one critical pair. The ordering could then be completed by appropriate decisions about non-critical node pairs. And, luckily, this process would have resulted in an optimal solution for FAS since every pair of nodes has the maximum possible contribution to the objective. Of course, things are not as easy in general since the decisions about critical pairs may lead to cycles of nodes, which cannot be part of the final ordering.\nOur algorithm proceeds as follows. It takes as input an edge-weighted complete directed graph G = (Tk, E,W ) with Tk as the node set and weight W (\u03c3, \u03c3\u2032) (computed using (6)) for every directed edge from type \u03c3 to type \u03c3\u2032. Our algorithm builds an auxiliary unweighted directed graph H = (T\u2016, A) again over the types. For every critical pair of types \u03c3, \u03c3\u2032 with W (\u03c3, \u03c3\u2032) > W (\u03c3\u2032, \u03c3), the auxiliary graph has a directed edge from type \u03c3 to type \u03c3\u2032. The next step is to compute all strongly connected components of H; two types \u03c3 and \u03c3\u2032 belong to the same strongly connected component if H contains a directed path from \u03c3 to \u03c3\u2032 and a directed path from \u03c3\u2032 to \u03c3. This computation can be easily done by computing breadth first search trees rooted at every node of H. After this step, the ordering of the types in different strongly connected components is irrevocably decided. In order to decide the ordering of types within the same strongly connected component, we use brute force on the corresponding subgraph of G. If the size of a strongly connected component is so large that brute forcing is prohibitive, we just order the types within the component according to a Borda ordering (breaking ties uniformly at random). As a final step, we decide the order of non-critical node pairs.\nThe approach to use Borda ordering when brute forcing is very costly in terms of running time might give the impression that the outcome of the above algorithm is always very close to a Borda ordering. Surprisingly, our algorithm returns Borda orderings (or orderings that are very close to Borda) only when this is absolutely necessary. One such situation is presented in the next section where we show that Borda is indeed the opti-\nmal aggregation rule in all scenarios that involve perfect graders. For imperfect graders, brute forcing has been proved extremely useful as the vast majority of strongly connected components are very small. We report statistical information from the size distribution of strongly connected components in Section 4."}, {"heading": "3.3 Borda is optimal for perfect graders", "text": "We will now exploit our theoretical framework to obtain our first concrete result.\nTheorem 3. For every scenario that involves perfect graders, Borda (with any tie breaking) is the optimal type-ordering aggregation rule.\nProof. Assume that we have a scenario with a bundle size of k, perfect grading (i.e., a k \u00d7 k identity noise matrix), and a bivariate function f that represents the performance objective.\nWe first compute the probability that exam paper x gets type \u03c3 using (4) and the fact that p\u03c3i,` = 1 if \u03c3i = ` and p\u03c3i,` = 0 otherwise. Hence,\nPr[xB \u03c3] = N(\u03c3) k\u220f i=1 ( k \u2212 1 \u03c3i \u2212 1 ) x\u03c3i\u22121(1\u2212 x)k\u2212\u03c3i\n= N(\u03c3) ( k\u220f i=1 ( k \u2212 1 \u03c3i \u2212 1 )) xk 2\u2212B(\u03c3)(1\u2212 x)B(\u03c3)\u2212k.\nNow, consider two exam papers with ranks x and y in the ground truth such that x < y and let \u03c3 and \u03c3\u2032 be two types. Using the above equality, we obtain\nPr[xB \u03c3] Pr[y B \u03c3\u2032] Pr[xB \u03c3\u2032] Pr[y B \u03c3] = ( y(1\u2212 x) x(1\u2212 y) )B(\u03c3)\u2212B(\u03c3\u2032) . (8)\nSince y > x, it is also 1 \u2212 x > 1 \u2212 y and the right hand side of the last equation is above, equal, or below 1 if and only if the quantity B(\u03c3) \u2212 B(\u03c3\u2032) is positive, zero, or negative. Hence, the quantity\nPr[xB \u03c3] Pr[y B \u03c3\u2032]\u2212 Pr[xB \u03c3\u2032] Pr[y B \u03c3]\nand the Borda score difference B(\u03c3)\u2212B(\u03c3\u2032) between the two types \u03c3 and \u03c3\u2032 have the same sign. Now, let sgn : R\u2192 {\u22121, 0, 1} be the signum function. Then, we have that\nsgn ( W (\u03c3, \u03c3\u2032)\u2212W (\u03c3\u2032, \u03c3) ) = sgn\n(\u222b 1 0 \u222b 1 x f(x, y)(Pr[xB \u03c3] Pr[y B \u03c3\u2032]\u2212 Pr[xB \u03c3\u2032] Pr[y B \u03c3]) dy dx ) = sgn ( B(\u03c3)\u2212B(\u03c3\u2032)\n) This implies that any Borda ordering of the types maximizes the quantity\u2211\n\u03c3,\u03c3\u2032:\u03c3 \u03c3\u2032W (\u03c3, \u03c3 \u2032) and, hence, the quantity C(k, , I, f), and the theorem follows."}, {"heading": "4 Validation of our framework", "text": ""}, {"heading": "4.1 Building a realistic noise model using a field experiment", "text": "We have run a field experiment with the students that attended the course on Computational Complexity in our home institution during the Spring 2015 semester. This is a\ncourse that the first author teaches during the last few years and usually includes an optional mid term exam. As it is typically the case in Greek universities, cardinal integer and half-integer scores between 0 and 10 are used in such exams and they represent how correct the answers of the students to the exam questions are. Hence, these cardinal scores represent the success of the students in the exam in absolute terms.\nIn our experiment, our goal has been to investigate how effective the students can be in ordinal grading. For this purpose, we created a hypothetical exam with three questions and prepared several answers for them. In particular, we prepared 16 different answers to question 1, 12 answers to question 2, and 8 answers to question 3. Combinations of these answers into all different ways resulted in a pool of 1536 different exam papers. We created bundles of size 6 from this pool. Each student was given a bundle of exam papers which was asked to rank (for a bonus grade). Note that the selection of papers in each bundle was not arbitrary. The answers for the questions belonged to different levels of correctness and included excellent ones, almost excellent ones with a minor issue not fully resolved, answers in the right direction but with sloppy write-up, completely incorrect answers, no answer at all, etc. Specifically, we had 7, 6, and 5 different levels of correctness for the answers in questions 1, 2, and 3, respectively. When bundles were formed, we imposed the following constraint for any pair of exam papers A and B in a bundle: if the correctness level of paper A in an answer is strictly higher than that in paper B, then paper B cannot have a strictly higher correctness level than A in any other answer. Furthermore, there was at least one question for which the answers had different levels of correctness. This guaranteed a strict ranking of the exam papers in each bundle and, furthermore, that this ranking would be well-defined and independent of any assumptions about the importance of the different questions.\nIn addition to this ranking exercise, the students also participated in the traditional mid term exam. This allowed us to quantify the correlation between their grading behaviour and their success in the traditional exam. The results are depicted in Figure 2(a). Even though students that were excellent in the traditional exam did very well as graders, the grading performance of the majority of the remaining students seems to be uniformly distributed between average and excellent, with just a few under-performing outliers.2\nFor comparison, we have plotted the same small number of randomly chosen students according to the Mallows distribution in Figure 2(b). Here, the correlation between student quality (to be thought of as equivalent to the success in the traditional exam) and grading performance is clear. The data depicted in Figure 2(a) have been used extensively in \u201crealistic\u201d exams. In these exams, we simulate a large population of students, whose quality and grading behaviour is determined by randomly drawing a bubble from those in Figure 2(a) with probability proportional to the area of the bubbles. Essentially, each of our 136 students in the experiment serves as the support of the realistic distribution while the quality is slightly perturbed to result in a strict ground truth ranking.\nThe above information has also been distilled in the noise matrix Preal = (pi,j)i,j\u2208[k] 2An explanation for this grading behaviour is that, even though the students have participated in many exams like the mid term in the past and have a very good idea of what they are expected to do, this was the very first time they were asked to rank. We plan to make the whole information (questions, answers, indicative bundles, etc.) available to the students attending our Computational Complexity course during the current semester (Spring 2016) so that they are well-prepared for the next field experiment. It will be very interesting to see how this preparation will affect their grading behaviour.\nwith\nPreal = \n0.463 0.257 0.102 0.058 0.058 0.058 0.205 0.316 0.227 0.110 0.066 0.073 0.161 0.191 0.257 0.205 0.132 0.051 0.102 0.117 0.191 0.242 0.279 0.066 0.044 0.066 0.139 0.220 0.301 0.227 0.022 0.051 0.080 0.161 0.161 0.522\n (9)\nwhich represents a \u201crealistic\u201d noise model. The information in the matrix was obtained by measuring the frequency that the i-th ranked exam paper by students should be correctly ranked at position j. For example, 28 out of 136 students ranked third an exam paper in their bundles which should have been ranked fourth; thus, p3,4 = 0.205. This matrix is used in the computation of the optimal type-ordering aggregation rules for realistic scenarios according to the methodology developed in Sections 3.1 and 3.2. Observe that matrix Preal does not include any information about the correlation between the grading behaviour and the quality of the student that acts as grader; actually, this is a feature that our framework completely neglects. Surprisingly, the experimental results that we present in the following indicate that storing information about this relation is not necessary for making accurate performance predictions."}, {"heading": "4.2 On the accuracy of theoretical performance predictions", "text": "Our next step was to compute the optimal type-ordering aggregation rules for several scenarios. We have kept the same bundle size of k = 6 in all scenarios and distinguish between the realistic and Mallows noise models by using the corresponding matrices Preal and Pmallows defined in equations (9) and (1), respectively. As performance objectives, we have considered the following:\n\u2022 all2all: the total number of all correctly recovered pairwise relations;\n\u2022 th-10% and th-50%: the total number of correctly recovered relations between pairs that include an exam paper that is ranked in the top 10% and top 50% in the ground truth, respectively;\n\u2022 acc-2% and acc-5%: the total number of correctly recovered relations between pairs with positions that differ by at least 2% and 5% in the ground truth, respectively.\nWe have used the theoretical framework presented in Sections 3.1 and 3.2; of course, computations have been automated. All the computational results that we report in the following have been obtained using an Intel 12-core i7 machine with 32Gb of RAM running Windows 7. Our methods have been implemented in C using the GNU Multiple Precision Arithmetic Library (GMP) and in Matlab R2013a. In particular, high precision is absolutely necessary in order to compute the weights even for bundles of size 6 since, by inspecting equations (5) and (6) carefully, we can see that there are products with more than 30 factors and factorials of integers up to 30 that are involved in the computations.\nA first remark is that the algorithm that we have used to solve FAS exactly is very fast. This is due to the fact that strongly connected components have very small size. In all cases we considered, among the 462 different types that we can have for bundles of size 6, more than 97% of them form singleton components and the maximum component size never exceeded 26 (for the 100-sample approximation of Mallows that we discuss in Section 4.3). Their distribution for scenarios with realistic and Mallows noise models for all performance objectives is depicted in Table 1.\nRecall that our theoretical framework assumes an infinite number of students and, hence, neglects apparent dependencies between random variables that certainly exist in scenarios with finite number of students. So, the information in Table 2 is rather surprising and shows that, besides our assumptions, our theory provides extremely accurate predictions for the performance of aggregation rules in practice. Note that the values in Table 2 are percentages and we never observed differences beyond the second decimal point between the theoretically predicted value and the experimental one. Also, note that we have used 10 000 students in our experiments. This number is lower than the vision for the most popular courses that will be offered by MOOCs in the near future; the predictions become even more accurate for higher numbers of students.\nTable 2 contains the expected values (from 1000 simulations) of the performance measure for each grading scenario and corresponding optimal aggregation rule (as well as for Borda). Even though Borda is never optimal in any imperfect grading scenario that we considered, its performance is always very close to the optimal rule. In particular, Borda is never more than 0.55% worse than the optimal rule in realistic scenarios (for the acc5% performance objective); such differences can be up to 1.53% in scenarios with Mallows\nnoise (and the th-10% objective). Figure 3 reports detailed information for all these simulations. Clearly, the performance of the aggregation rules for all the objectives that we considered is sharply concentrated around the expected values. The clouds of points are very close to the diagonal (corresponding to values with equal coordinates) in the realistic scenarios and only marginally further from it in the Mallows scenarios.\n78.8 79 79.2 79.4 79.6 79.8 80 80.2 80.4 80.6 80.8 78.8\n79\n79.2\n79.4\n79.6\n79.8\n80\n80.2\n80.4\n80.6\n80.8\nOptimal rule\nB or\nda\n(a) realistic, all2all\n86 86.5 87 87.5 88 88.5\n86\n86.5\n87\n87.5\n88\n88.5\nOptimal rule\nB or\nda\n(b) realistic, th-10%\n81.6 81.8 82 82.2 82.4 82.6 82.8 83 83.2 83.4 83.6\n81.6\n81.8\n82\n82.2\n82.4\n82.6\n82.8\n83\n83.2\n83.4\n83.6\nOptimal rule\nB or\nda\n(c) realistic, acc-5%\n83.8 84 84.2 84.4 84.6 84.8 85 85.2 85.4 85.6 83.8\n84\n84.2\n84.4\n84.6\n84.8\n85\n85.2\n85.4\n85.6\nOptimal rule\nB or\nda\n(d) mallows, all2all\n89.5 90 90.5 91 91.5 92 92.5 93\n89.5\n90\n90.5\n91\n91.5\n92\n92.5\n93\nOptimal rule\nB or\nda\n(e) mallows, th-10%\n87 87.2 87.4 87.6 87.8 88 88.2 88.4 88.6 88.8 89 87\n87.2\n87.4\n87.6\n87.8\n88\n88.2\n88.4\n88.6\n88.8\n89\nOptimal rule\nB or\nda\n(f) mallows, acc-5%\nFigure 3: Performance of Borda compared to the optimal rule for the realistic and Mallows noise scenarios and the performance objectives all2all, acc-5%, and th-10%.\nA final comment on the performance of the optimal type-ordering aggregation rules is that they are extremely robust. Even though they have been optimized with respect to a particular performance objective, they perform very well with respect to other objectives as well. Figure 4 shows measurements of properties that cannot be expressed as performance objectives under our framework. Each plot shows data about Borda and the optimal (under the all2all objective) type-ordering aggregation rules in scenarios with perfect, realistic, and Mallows grading. Borda in the perfect grading scenario achieves the best performance with respect to these objectives as well. Actually, its performance in this scenario can\nserve as the optimistic barrier for every type-ordering aggregation rule in any (imperfect) grading scenario. More interestingly, Borda has performance that is very close to the optimal rule for realistic grading (the corresponding curves almost coincide in Figure 4) and slightly worse for Mallows grading. These results are in sync to those in Table 2, Figure 3, and Figure 4 and showcases the robustness of Borda in different scenarios."}, {"heading": "4.3 The effect of inaccuracies in the noise model", "text": "The realistic noise model that we built in Section 4.1 is, by definition, an approximation of the students in our home institution. Besides limitations that have to do with our modelling assumptions, it has the obvious drawback that it has been built using a very small fraction of our students. So far, the reader should have been convinced that the aggregation rules we have built are indeed optimal for a large population that inherits the quality and grading performance of this small fraction of students; this has been the focus of our experiments with realistic grading. What is far from clear is whether these aggregation rules will perform equally well for the whole population of the students in our home institution. To see the importance of this question, imagine it in the planetary scale that MOOCs envision. Can we make safe predictions for huge student populations by sampling a tiny fraction of them, building a noise model as we did in Section 4.1, and then select optimal type-ordering aggregation rules as we did in Section 4.2?\nWe give a positive answer to this question by considering Mallows grading scenarios. With Mallows, we have the luxury of a well-defined noise model for the grading behaviour of a huge student population and we have used it in order to compute optimal aggregation rules for this model. This information will be used only for assessing the approach presented in the following. Instead, we will pretend that no information about grading behaviour is available and all we can do is to apply (actually, to simulate) a field experiment like the one we presented in Section 4.1 on a tiny fraction of the students in order to come up with a noise matrix. In this way, we will compute an approximation of the true noise model.\nWe have followed this approach using samples of Mallows graders of size 100 and 1000.\nThe two noise model matrices we obtained are as follows:\nP100 = \n0.59 0.19 0.07 0.08 0.06 0.01 0.19 0.44 0.18 0.09 0.04 0.06 0.10 0.19 0.43 0.19 0.07 0.02 0.05 0.05 0.15 0.45 0.19 0.11 0.06 0.10 0.09 0.14 0.46 0.15 0.01 0.03 0.08 0.05 0.18 0.65  and\nP1000 = \n0.639 0.186 0.066 0.058 0.031 0.020 0.193 0.534 0.150 0.055 0.032 0.036 0.073 0.149 0.501 0.147 0.076 0.054 0.039 0.075 0.155 0.497 0.147 0.087 0.033 0.038 0.071 0.163 0.517 0.178 0.023 0.018 0.057 0.080 0.197 0.625  The matrices have been used to compute the optimal type-ordering aggregation rules for the five performance objectives. Interestingly, the instances of FAS that we had to solve were slightly harder now, particularly for the 100-sample noise model, where strongly connected components of size up to 26 emerged in the auxiliary graph. Still, our methodology was applied smoothly and allowed us to compute optimal rules.\nTable 3 shows the theoretical prediction and observed performance values of these rules (by applying the rules on 1000 simulated exams with 10 000 Mallows students). The performance of the aggregation rules that were computed using the 100-sample approximation of Mallows are already amazingly close to those for Mallows. For the rules that we computed using the 1000-sample approximation, it is almost impossible to distinguish them from the Mallows-optimal ones in terms of performance.\nA more refined graphical representation of these findings is given in Figure 5 (best viewed in color). Each plot contains a blue and a red cloud of 1000 points each corresponding to a single simulated exam with 10 000 students. The blue points (respectively, red points) measure the performance of the optimal rule for the 1000-sample (respectively, 100-sample) Mallows approximation versus the Mallows-optimal rule. The blue cloud almost coincides with the diagonal in each plot, indicating an optimal approximation of the Mallows-optimal rule. The red cloud is distinct (there is a negligible \u201cintersection\u201d of blue and red points only for the th-10% performance objective) but very close. To realize how close the two clouds are, almost the whole cloud of points for Borda (from Figures 3(d), 3(e), and 3(f)) would be located outside the plot area of Figure 5 (if we attempted to plot it).\nWe conclude the presentation of our experimental results with a comparison of the type orderings of the optimal aggregation rules for Mallows and its 100-sample and 1000-sample\napproximations; these are presented in Table 4. Therein, we can see that the optimal rule for the 1000-sample noise model according to the all2all performance objective is very close (but not identical) to the Mallows-optimal rule. The optimal rule for the 100-sample noise model is substantially different (this difference is more apparent in lower positions of the ordering which cannot be included here). An interesting characteristic of optimal rules for Mallows and its approximations is that the orderings of types are not monotonic. For example, type (1, 1, 1, 1, 1, 5) is always ahead of (1, 1, 1, 1, 1, 2). This justifies our decision to study type-ordering aggregation rules and ignore positional scoring rules; clearly, no positional scoring rule would come up with non-monotonic orderings of types."}, {"heading": "A Computing the weights", "text": "We now elaborate on how to analytically compute the weight W (\u03c3, \u03c3\u2032); the following computations are implemented in Algorithm 1. Recall that W (\u03c3, \u03c3\u2032) is given by equation (6), i.e.,\nW (\u03c3, \u03c3\u2032) = \u222b 1 0 \u222b 1 x f(x, y) Pr[xB \u03c3] \u00b7 Pr[y B \u03c3\u2032] dy dx,\nand that the performance objective bivariate function f indicates whether a correctly recovered pairwise relation between two students x and y (with x < y) should be accounted for or not. All perfomance objectives we consider in this paper can generically be described by such a function f with f(x, y) = 1 when x \u2208 [\u03b1, \u03b2] and y \u2208 [x + \u03b3, \u03b4] for appropriate values of \u03b1, \u03b2, \u03b3, \u03b4 \u2208 [0, 1], and f(x, y) = 0 otherwise. In particular, all2all can be expressed with the tuple (\u03b1, \u03b2, \u03b3, \u03b4) = (0, 1, 0, 1), th-10% and th-50% with the tuples (0, 0.1, 0, 1) and (0, 0.5, 0, 1), and acc-2% and acc-5% with the tuples (0, 0.98, 0.02, 1) and (0, 0.95, 0.05, 1).\nThen, W (\u03c3, \u03c3\u2032) is equal to\nW (\u03c3, \u03c3\u2032) = \u222b \u03b2 \u03b1 \u222b \u03b4 x+\u03b3 Pr[xB \u03c3] \u00b7 Pr[y B \u03c3\u2032] dy dx\n= \u222b \u03b2 \u03b1 Pr[xB \u03c3] \u222b \u03b4 x+\u03b3 Pr[y B \u03c3\u2032] dy dx. (10)\nNow, recall that Pr[x B \u03c3] is given by equation (5) and is a univariate polynomial of degree k2 \u2212 k. Hence, it can be written as\nPr[xB \u03c3] = k2\u2212k\u2211 s=0 cs(\u03c3)x s, (11)\nwhere the coefficient cs(\u03c3) with s = 0, ..., k2 \u2212 k are computed by equation (5), and have been included for completeness in the first part of Algorithm 1.\nThe inner integral in equation (10) is computed as follows:\u222b \u03b4 x+\u03b3 Pr[y B \u03c3\u2032] dy = \u222b \u03b4 x+\u03b3 k2\u2212k\u2211 s=0 cs(\u03c3 \u2032)ys dy\n= k2\u2212k\u2211 s=0 cs(\u03c3 \u2032) \u222b \u03b4 x+\u03b3 ys dy\n= k2\u2212k\u2211 s=0 cs(\u03c3 \u2032) s+ 1 ( \u03b4s+1 \u2212 (x+ \u03b3)s+1 ) =\nk2\u2212k\u2211 s=0 cs(\u03c3 \u2032)\u03b4s+1 s+ 1 \u2212 k2\u2212k+1\u2211 s=1 cs(\u03c3 \u2032)(x+ \u03b3)s s .\nUsing the fact that (z + w)m = \u2211m\ni=0 ( m i ) zm\u2212iwi for z = \u03b3, w = x, and m = s, we obtain\u222b \u03b4\nx+\u03b3 Pr[y B \u03c3\u2032] dy = k2\u2212k\u2211 s=0 cs(\u03c3 \u2032)\u03b4s+1 s+ 1 \u2212 k2\u2212k+1\u2211 s=1 cs(\u03c3 \u2032) s\u2211 i=0 1 s ( s i ) \u03b3s\u2212ixi. (12)\nObserve that the inner integral is also a univariate polynomial of degree k2 \u2212 k + 1 and it can be written as \u222b \u03b4\nz+\u03b3 Pr[y B \u03c3\u2032] dy = k2\u2212k+1\u2211 t=0 dt(\u03c3)x t, (13)\nwhere the coefficient dt(\u03c3\u2032) with t = 0, ..., k2 \u2212 k + 1 are computed by equation (12) at the second part of Algorithm 1.\nBy substituting equations (11) and (13) in equation (10), we obtain\nW (\u03c3, \u03c3\u2032) = \u222b \u03b2 \u03b1 Pr[xB \u03c3] (\u222b \u03b4 x+\u03b3 Pr[y B \u03c3\u2032] dy ) dx\n= \u222b \u03b2 \u03b1 k2\u2212k\u2211 s=0 cs(\u03c3)x s k2\u2212k+1\u2211 t=0 dt(\u03c3 \u2032)xt dz\n= k2\u2212k\u2211 s=0 k2\u2212k+1\u2211 t=0 cs(\u03c3)dt(\u03c3 \u2032) \u222b \u03b2 \u03b1 xs+t dz\n= k2\u2212k\u2211 s=0 k2\u2212k+1\u2211 t=0 cs(\u03c3)dt(\u03c3 \u2032) s+ t+ 1 ( \u03b2s+t+1 \u2212 \u03b1s+t+1 ) .\nThis computation is described in the last part of Algorithm 1.\nAlgorithm 1: Computing W (\u03c3, \u03c3\u2032) // Compute the coefficient vector c(\u03c3) for s := 0 . . . k2 \u2212 k do\nset cs(\u03c3) := 0 end for `1 := 1 . . . k do\n. . . for `k := 1 . . . k do\nset |`|1 := \u2211k\ni=1 `i for j := 0 . . . k2 \u2212 |`|1 do set c|`|1\u2212k+j(\u03c3) := c|`|1\u2212k+j(\u03c3) +N(\u03c3) (\u220fk i=1 p\u03c3i,`i ( k\u22121 `i\u22121 ))( k2\u2212|`|1 j ) (\u22121)j\nend end\nend // Compute the coefficient vector d(\u03c3\u2032)\nset d0(\u03c3\u2032) := \u2211k2\u2212k s=0 cs(\u03c3\u2032)\u03b4s+1 s+1 for t := 1 . . . k2 \u2212 k + 1 do set dt(\u03c3\u2032) := 0 end for s := 1 . . . k2 \u2212 k + 1 do\nfor i := 0 . . . s do set di(\u03c3\u2032) := di(\u03c3\u2032)\u2212 cs(\u03c3 \u2032) s ( s i ) \u03b3s\u2212i\nend end // Compute W (\u03c3, \u03c3\u2032) set W (\u03c3, \u03c3\u2032) := 0 for s := 0 . . . k2 \u2212 k do\nfor t := 0 . . . k2 \u2212 k + 1 do set W (\u03c3, \u03c3\u2032) :=W (\u03c3, \u03c3\u2032) + cs(\u03c3)dt(\u03c3\n\u2032) s+t+1\n( \u03b2s+t+1 \u2212 \u03b1s+t+1 ) end\nend"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Ordinal peer grading has been proposed as a simple and scalable solution for com-<lb>puting reliable information about student performance in massive open online courses.<lb>The idea is to outsource the grading task to the students themselves as follows. After<lb>the end of an exam, each student is asked to rank \u2014in terms of quality\u2014 a bundle of<lb>exam papers by fellow students. An aggregation rule will then combine the individ-<lb>ual rankings into a global one that contains all students. We define a broad class of<lb>simple aggregation rules and present a theoretical framework for assessing their ef-<lb>fectiveness. When statistical information about the grading behaviour of students is<lb>available, the framework can be used to compute the optimal rule from this class with<lb>respect to a series of performance objectives. For example, a natural rule known as<lb>Borda is proved to be optimal when students grade correctly. In addition, we present<lb>extensive simulations and a field experiment that validate our theory and prove it to<lb>be extremely accurate in predicting the performance of aggregation rules even when<lb>only rough information about grading behaviour is available.", "creator": "LaTeX with hyperref package"}}}