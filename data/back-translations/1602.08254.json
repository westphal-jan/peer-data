{"id": "1602.08254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Theoretical Analysis of the $k$-Means Algorithm - A Survey", "abstract": "The $k $mean algorithm is one of the most widely used cluster heuristics. Despite its simplicity, analyzing its runtime and the quality of the approach is surprisingly difficult and can lead to deep insights that can be used to improve the algorithm. In this paper, we examine the most recent results in this direction as well as several extensions of the basic $k $mean method.", "histories": [["v1", "Fri, 26 Feb 2016 09:39:50 GMT  (38kb)", "http://arxiv.org/abs/1602.08254v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["johannes bl\\\"omer", "christiane lammersen", "melanie schmidt", "christian sohler"], "accepted": false, "id": "1602.08254"}, "pdf": {"name": "1602.08254.pdf", "metadata": {"source": "CRF", "title": "Theoretical Analysis of the k-Means Algorithm \u2013 A Survey", "authors": ["Johannes Bl\u00f6mer", "Christiane Lammersen", "Melanie Schmidt", "Christian Sohler"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n08 25\n4v 1\n[ cs\n.D S]\n2 6\nFe b\n20 16\nTheoretical Analysis of the k-Means Algorithm\n\u2013 A Survey\nJohannes Blo\u0308mer\u2217 Christiane Lammersen\u2020 Melanie Schmidt\u2021\nChristian Sohler\u00a7\nFebruary 29, 2016\nThe k-means algorithm is one of the most widely used clustering heuristics. Despite its simplicity, analyzing its running time and quality of approximation is surprisingly difficult and can lead to deep insights that can be used to improve the algorithm. In this paper we survey the recent results in this direction as well as several extension of the basic k-means method."}, {"heading": "1 Introduction", "text": "Clustering is a basic process in data analysis. It aims to partition a set of objects into groups called clusters such that, ideally, objects in the same group are similar and objects in different groups are dissimilar to each other. There are many scenarios where such a partition is useful. It may, for example, be used to structure the data to allow efficient information retrieval, to reduce the data by replacing a cluster by one or more representatives or to extract the main \u2018themes\u2019 in the data. There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47]. Notice that the title of [47] is Data clustering: 50 years beyond K-means in reference to the k-means algorithm, the probably most widely used clustering algorithm of all time. It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey.\nThe k-means algorithm solves the problem of clustering to minimize the sum of squared errors (SSE). In this problem, we are given a set of points P \u2282 Rd in a Euclidean space, and the goal is to find a set C \u2282 Rd of k points (not necessarily included in P ) such that the sum of the squared distances of the points in P to their nearest center in C is minimized. Thus, the objective function to be minimized is\ncost(P,C) := \u2211\np\u2208P min c\u2208C\n\u2016p \u2212 c\u20162 ,\n\u2217Department of Computer Science, University of Paderborn, Germany \u2020School of Computing Science, Simon Fraser University, Burnaby, B.C., Canada \u2021Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA \u00a7Department of Computer Science, TU Dortmund University, Germany\nwhere \u2016\u00b7\u20162 is the squared Euclidean distance. The points in C are called centers. The objective function may also be viewed as the attempt to minimize the variance of the Euclidean distance of the points to their nearest cluster centers. Also notice that when given the centers, the partition of the data set is implicitly defined by assigning each point to its nearest center.\nThe above problem formulation assumes that the number of centers k is known in advance. How to choose k might be apparent from the application at hand, or from a statistical model that is assumed to be true. If it is not, then the k-means algorithm is typically embedded into a search for the correct number of clusters. It is then necessary to specify a measure that allows to compare clusterings with different k (the SSE criterion is monotonically decreasing with k and thus not a good measure). A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39]. In this survey, we assume that k is provided with the input.\nAs Jain [47] also notices, the k-means algorithm is still widely used for clustering and in particular for solving the SSE problem. That is true despite a variety of alternative options that have been developed in fifty years of research, and even though the k-means algorithm has known drawbacks.\nIn this survey, we review the theoretical analysis that has been developed for the k-means algorithm. Our aim is to give an overview on the properties of the k-means algorithm and to understand its weaknesses, but also to point out what makes the k-means algorithm such an attractive algorithm. In this survey we mainly review theoretical aspects of the k-means algorithm, i.e. focus on the deduction part of the algorithm engineering cycle, but we also discuss some implementations with focus on scalability for big data.\n1.1 The k-means algorithm\nIn order to solve the SSE problem heuristically, the k-means algorithm starts with an initial candidate solution {c1, . . . , ck} \u2282 Rd, which can be chosen arbitrarily (often, it is chosen as a random subset of P ). Then, two steps are alternated until convergence: First, for each ci, the algorithm calculates the set Pi of all points in P that are closest to ci (where ties are broken arbitrarily). Then, for each 1 \u2264 i \u2264 k, it replaces ci by the mean of Pi. Because of this calculation of the \u2018means\u2019 of the sets Pi, the algorithm is also called the k-means algorithm.\nThe k-Means Algorithm Input: Point set P \u2286 Rd number of centers k 1. Choose initial centers c1, . . . , ck of from R d"}, {"heading": "2. repeat", "text": "3. P1, . . . , Pk \u2190 \u2205 4. for each p \u2208 P do 5. Let i = argmini=1,...,k \u2016p \u2212 ci\u20162 6. Pi \u2190 Pi \u222a {p} 7. for i = 1 to k do 8. if Pi 6= \u2205 then ci = 1|Pi| \u2211 p\u2208Pi p 9. until the centers do not change\nThe k-means algorithm is a local improvement heuristic, because replacing the center of a set Pi by its mean can only improve the solution (see Fact 1 below), and then reassigning the points to their closest center in C again only improves the solution. The algorithm converges, but the first important question is how many iterations are necessary until an optimal or good solution is found. The second natural question is how good the solution will be when the algorithm stops. We survey upper and lower bounds on running time and quality in Section 2. Since the quality of the computed solution depends significantly on the starting solution, we discuss ways to choose the starting set of centers in a clever way in Section 3. Then, we survey variants of the basic k-means algorithm in Section 4 and alternatives to the k-means algorithm in Section 5. In Section 6, we consider the complexity of the SSE problem. Finally, we describe results on the k-means problem and algorithm for Bregman divergences Section 7. Bregman divergences have numerous applications and constitute the largest class of dissimilarity measure for which the k-means algorithm can be applied.\n2 Running Time and Quality of the basic k-Means Algorithm\nIn this section, we consider the two main theoretical questions about the k-means algorithm: What is its running time, and does it provide a solution of a guaranteed quality? We start with the running time."}, {"heading": "2.1 Analysis of Running Time", "text": "The running time of the k-means algorithm depends on the number of iterations and on the running time for one iteration. While the running time for one iteration is clearly polynomial in n, d and k, this is not obvious (and in general not true) for the number of iterations. Yet, in practice, it is often observed that the k-means algorithm does not significantly improve after a relatively small number of steps. Therefore, one often performs only a constant number of steps. It is also common to just stop the algorithm after a given maximum number of iterations, even if it has not converged. The running time analysis thus focuses on two things. First, what the asymptotic running time of one iteration is and how it can be accelerated for benign inputs. Second, whether there is a theoretical explanation on why the algorithm tends to converge fast in practice."}, {"heading": "2.1.1 Running Time of One Iteration", "text": "A straightforward implementation computes \u0398(nk) distances in each iteration in time \u0398(ndk) and runs over the complete input point set. We denote this as the \u2018naive\u2019 implementation. Asymptotically, the running time for this is dominated by the number of iterations, which is in general not polynomially bounded in n in the worst case (see next subsection for details). However, in practice, the number of iterations is often manually capped, and the running time of one iteration becomes the important factor. We thus want to mention a few practical improvements.\nThe question is whether and how it can be avoided to always compute the distances between all points and centers, even if this does not lead to an asymptotic improvement. Imagine the following pruning rule: Let ci be a center in the current iteration. Compute the minimum\ndistance \u2206i between ci and any other center in time \u0398(kd). Whenever the distance between a point p and ci is smaller than \u2206i/2, then the closest center to p is ci and computing the other k \u2212 1 distances is not necessary. A common observation is that points often stay with the same cluster as in the previous iteration. Thus, check first whether the point is within the safe zone of its old center. More complicated pruning rules take the movement of the points into account. If a point has not moved far compared to the center movements, it keeps its center allocation. Rules like this aim at accelerating the k-means algorithm while computing the same clustering as a na\u0308\u0131ve implementation. The example pruning rules are from [50].\nAccelerating the algorithm can also be done by assigning groups of points together using sufficient statistics. Assume that a subset P \u2032 of points is assigned to the same center. Then finding this center and later updating it based on the new points can be done by only using three statistics on P \u2032. These are the sum of the points (which is a point itself), the sum of the squared lengths of the points (and thus a constant) and the number of points. However, this is only useful if the statistic is already precomputed. For low-dimensional data sets, the precomputation can be done using kd-trees. These provide a hierarchical subdivision of a point set. The idea now is to equip each inner node with sufficient statistics on the point set represented by it. When reassigning points to centers, pruning techniques can be used to decide whether all points belonging to an inner node have the same center, or whether it is necessary to proceed to the child nodes to compute the assignment. Different algorithms based on this idea are given in [10, 54, 69]. Notice that sufficient statistics are used in other contexts, too, e.g. as a building block of the well-known data stream clustering algorithm BIRCH [77].\nThere are many ways more that help to accelerate the k-means algorithm. For an extensive overview and more pointers to the literature, see [41]."}, {"heading": "2.1.2 Worst-Case Analysis", "text": "Now we we take a closer look at the worst-case number of iterations, starting with (large) general upper bounds and better upper bounds in special cases. Then we review results for lower bounds on the number of iterations and thus on the running time of the basic k-means algorithm. In the next section, we have a look into work on smoothed analysis for the k-means algorithm which gives indications on why the k-means algorithm often performs so well in practice.\nUpper Bounds The worst-case running time to compute a k-clustering of n points in Rd by applying the k-means algorithm is upper bounded by O(ndk \u00b7 T ), where T is the number of iterations of the algorithm. It is known that the number of iterations of the algorithm is bounded by the number of partitionings of the input points induced by a Voronoi-diagramm of k centers. This number can be bounded by O(ndk2) because given a set of k centers, we can move each of the O(k2) bisectors such that they coincide with d linearly independent points without changing the partition. For the special case of d = 1 and k < 5, Dasgupta [31] proved an upper bound of O(n) iterations. Later, for d = 1 and any k, Har-Peled and Sadri [44] showed an upper bound of O(n\u22062) iterations, where \u2206 is the ratio between the diameter and the smallest pairwise distance of the input points.\nIn the following, we will explain the idea to obtain the upper bound given in [44]. The input\nis a set P of n points with spread \u2206 from the Euclidean line R. W.l.o.g., we can assume that the minimum pairwise distance in P is 1 and the diameter of P is \u2206. For any natural number k and for any partition of P into k sets, the clustering cost of P with the means of the subsets as centers is bounded by O(n\u22062). In particular, this holds for the solution of the k-means algorithm after the first iteration. Additionally, the clustering cost of P certainly is \u03c9(1) as we assumed that the minimum pairwise distance in P is 1. Thus, if we can show that each following iteration decreases the cost by at least some constant amount, then we are done. Let us now consider the point of time in any iteration of the k-means algorithm when the cluster centers have been moved to the means of their respective clusters and the next step is to assign each point to the new closest cluster center. In this step, there has to be a cluster that is extended or shrunk from its right end. W.l.o.g. and as illustrated in Figure 1, let us assume that the leftmost cluster Q1 is extended from its right end. Let S be the set of points that join cluster Q1 to obtain cluster Q \u2032 1. Since the minimum pairwise distance is 1, the distance of the mean of S to the leftmost point in S is at least (|S| \u2212 1)/2. Similarly, the distance of the mean of Q1 to the rightmost point in Q1 is at least (|Q1| \u2212 1)/2. Furthermore, the distance between any point in Q1 and any point in S is at least 1. Let \u00b5(X) be the mean of any point set X. Then, we have \u2016\u00b5(Q1)\u2212 \u00b5(S)\u2016 \u2265 (|Q1| \u2212 1)/2 + (|S| \u2212 1)/2 + 1 = (|Q1|+ |S|)/2. The movement of the mean of the leftmost cluster is at least\n\u2016\u00b5(Q1)\u2212 \u00b5(Q\u20321)\u2016 = \u2225 \u2225 \u2225\n\u2225\n\u00b5(Q1)\u2212 |Q1|\u00b5(Q1) + |S|\u00b5(S)\n|Q1|+ |S|\n\u2225 \u2225 \u2225 \u2225\n= |S|\n|Q1|+ |S| \u2016\u00b5(Q1)\u2212 \u00b5(S)\u2016 \u2265 |S| 2 \u2265 1 2 .\nWe will now need the following fact, which is proved in Section 6.\nFact 1. Let\n\u00b5 := 1 |P | \u2211\np\u2208P p\nbe the mean of a point set P , and let y \u2208 Rd be any point. Then, we have \u2211\np\u2208P \u2016p\u2212 y\u20162 =\n\u2211 p\u2208P \u2016p \u2212 \u00b5\u20162 + |P | \u00b7 \u2016y \u2212 \u00b5\u20162 .\nDue to this fact, the result of the above calculation is an improvement of the clustering cost of at least 1/4, which shows that in each iteration the cost decreases at least by some constant amount and hence there are at most O(n\u22062) iterations.\nLower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73]. Dasgupta [31] proved that the k-means algorithm has a worstcase running time of \u2126(n) iterations. Using a construction in some \u2126( \u221a n)-dimensional space, Arthur and Vassilvitskii [13] were able to improve this result to obtain a super-polynomial worst-case running time of 2\u2126( \u221a n) iterations. This has been simplified and further improved by Vattani [73] who proved an exponential lower bound on the worst-case running time of the k-means algorithm showing that k-means requires 2\u2126(n) iterations even in the plane. A modification of the construction shows that the k-means algorithm has a worst-case running time that, besides being exponential in n, is also exponential in the spread \u2206 of the ddimensional input points for any d \u2265 3.\nIn the following, we will give a high-level view on the construction presented in [73]. Vattani uses a special set of n input points in R2 and a set of k = \u0398(n) cluster centers adversarially chosen among the input points. The points are arranged in a sequence of t = \u0398(n) gadgets G0, G1, . . . , Gt\u22121. Except from some scaling, the gadgets are identical. Each gadget contains a constant number of points, has two clusters and hence two cluster centers, and can perform two stages reflected by the positions of the two centers. In one stage, gadget Gi, 0 \u2264 i < t, has one center in a certain position c\u2217i , and, in the other stage, the same center has left the position c\u2217i and has moved a little bit towards gadget Gi+1. Once triggered by gadget Gi+1, Gi performs both of these stages twice in a row. Performing these two stages happens as follows. The two centers of gadget Gi+1 are assigned to the center of gravity of their clusters, which results in some points of Gi+1 are temporarily assigned to the center c \u2217 i of Gi. Now, the center of Gi located at c \u2217 i and the centers of Gi+1 move, so that the points temporarily assigned to a center of Gi are again assigned to the centers of Gi+1. Then, again triggered by Gi+1, gadget Gi performs the same two stages once more. There is only some small modification in the arrangement of the two clusters of Gi+1. Now, assume that all gadgets except Gt\u22121 are stable and the centers of Gt\u22121 are moved to the centers of gravity of their clusters. This triggers a chain reaction, in which the gadgets perform 2\u2126(t) stages in total. Since, each stage of a gadget corresponds to one iteration of the k-means algorithm, the algorithm needs 2\u2126(n) iterations on the set of points contained in the gadgets."}, {"heading": "2.1.3 Smoothed Analysis", "text": "Concerning the above facts, one might wonder why k-means works so well in practice. To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63]. This model is especially useful when both worst-case and average-case analysis are not realistic and reflects the fact that real-world datasets are likely to contain measurement errors or imprecise data. In case an algorithm has a low time complexity in the smoothed setting, it is likely to have a small running time on real-world datasets as well.\nNext, we explain the model in more detail. For given parameters n and \u03c3, an adversary chooses an input instance of size n. Then, each input point is perturbed by adding some small amount of random noise using a Gaussian distribution with mean 0 and standard deviation \u03c3. The maximum expected running time of the algorithm executed on the perturbed input points is measured.\nArthur and Vassilvitskii [15] showed that, in the smoothed setting, the number of iterations\nof the k-means algorithm is at most poly(nk, \u03c3\u22121). This was improved by Manthey and Ro\u0308glin [63] who proved the upper bounds poly(n \u221a k, 1/\u03c3) and kkd \u00b7poly(n, 1/\u03c3) on the number of iterations. Finally, Arthur et al. [12] showed that k-means has a polynomial-time smoothed complexity of poly(n, 1/\u03c3).\nIn the following, we will give a high-level view on the intricate analysis presented in [12]. Arthur et al. show that after the first iteration of k-means, the cost of the current clustering is bounded by some polynomial in n, k and d. In each further iteration, either some cluster centers move to the center of gravity of their clusters or some points are assigned to a closer cluster center or even both events happen. Obviously, the clustering cost is decreased after each iteration, but how big is this improvement? Arthur et al. prove that, in expectation, an iteration of k-means decreases the clustering cost by some amount polynomial in 1/n and \u03c3. This results in a polynomial-time smoothed complexity.\nThe key idea to obtain the above lower bound on the minimum improvement per iteration is as follows. Let us call a configuration of an iteration, defined by a partition into clusters and a set of cluster centers, good if in the successive iteration either a cluster center moves significantly or reassigning a point decreases the clustering cost of the point significantly. Otherwise, the configuration is called bad. Arthur et al. show an upper bound on the probability that a configuration is bad. The problem is now that there are many possible configurations. So we cannot take the union bound over all of these possible configurations to show that the probability of the occurrence of any bad configuration during a run of k-means is small. To avoid this problem, Arthur et al. group all configurations into a small number of subsets and show that each subset contains either only good configurations or only bad configurations. Finally, taking the union bound over all subsets of configurations leads to the desired result, i.e., proving that the occurrence of a bad configuration during a run of k-means is small."}, {"heading": "2.2 Analysis of Quality", "text": "As mentioned above, the k-means algorithm is a local improvement heuristic. It is known that the k-means algorithm converges to a local optimum [70] and that no approximation ratio can be guaranteed [55]. Kanungo et al. [55] illustrate the latter fact by the simple example given in Figure 2. In this example, we are given four input points on the Euclidean line depicted by the first dashed line in Figure 2. The distances between the first and second, second and third and third and fourth point are named x, y and z, respectively. We assume that x < y < z, so x is the smallest distance and placing two centers in the first two points and one between the third and fourth costs 2 \u00b7 x2/4 = x2/2, and this is the (unique) optimal solution depicted on the second dashed line.\nOn the third dashed line, we see a solution that is clearly not optimal because it costs y2/2 and y > x. The approximation ratio of this solution is y2/x2, which can be made arbitrarily bad by moving the first point to the left and thus increasing y.\nIf we choose the initial centers randomly, it can happen that the k-means algorithm encounters this solution (for example when we pick the first, third and fourth point as initial centers and keep y < z while increasing y). When finding the solution, the k-means algorithm will terminate because the assignment of points to the three centers is unique and every center is the mean of the points assigned to it.\nThus, the worst-case approximation guarantee of the k-means algorithm is unbounded.\n3 Seeding Methods for the k-means Algorithm\nThe k-means algorithm starts with computing an initial solution, which can be done in a number of different ways. Since the k-means algorithm is a local improvement strategy we can, in principle, start with an arbitrary solution and then the algorithms runs until it converges to a local optimum. However, it is also known that the algorithm is rather sensible to the choice of the starting centers. For example, in the situation in Figure 2, no problem occurs if we choose the first, second and third point as the starting centers.\nOften one simply chooses the starting centers uniformly at random, but this can lead to problems, for example, when there is a cluster that is far away from the remaining points and that is so small that it is likely that no point of it is randomly drawn as one of the initial centers. In such a case one must hope to eventually converge to a solution that has a center in this cluster as otherwise we would end up with a bad solution. Unfortunately, it is not clear that this happens (in fact, one can assume that it will not).\nTherefore, a better idea is to start with a solution that already satisfies some approximation guarantees and let the k-means algorithm refine the solution. In this section we will present methods that efficiently pick a relatively good initial solution. As discussed later in Section 6 there are better approximation algorithms, but they are relatively slow and the algorithms presented in this section present a better trade-off between running time and quality of the initial solution."}, {"heading": "3.1 Adaptive Sampling", "text": "Arthur and Vassilvitskii [14] proposed a seeding method for the k-means algorithm which applies adaptive sampling. They construct an initial set C of k centers in the following way: The first center is sampled uniformly at random. For the ith center, each input point p is sampled with probability D2(p)/ \u2211\nq\u2208P D 2(q), where P is the input point set, D2(p) =\nminc1,...,ci\u22121 ||p \u2212 ci||2 is the cost of p in the current solution and c1, . . . ci\u22121 are the centers chosen so far. The sampling process is referred to asD2-sampling, and the algorithm consisting of D2-sampling followed by the k-means algorithm is called k-means++.\nWe study the progress ofD2-sampling in comparison to a fixed optimal solution. An optimal set of centers partitions P into k optimal clusters. If we could sample a center from each cluster uniformly at random, we would in expectation obtain a constant approximation. Since taking a point uniformly at random can also be described as first choosing the cluster and then picking the point uniformly at random, we know that the first point will be uniformly from one (unknown) cluster, which is fine. We want to make sure that this will also approximately be\nthe case for the remaining clusters. The main problem is that there is a significant probability to sample points from a cluster which we already hit (especially, if these clusters contain a lot of points). In order to avoid this, we now sample points with probability proportional to the squared distance from the previously chosen cluster centers. In this way, it is much more likely to sample points from the remaining clusters since the reason that these points belong to a different cluster is that otherwise they would incur a high cost. One can show that in a typical situation, when one of the remaining clusters is far away from the clusters we already hit, then conditioned on the fact that we hit this cluster, the new center will be approximately uniformly distributed within the cluster. In the end, this process leads to a set of k centers that is an expected O(log k)-approximation [14].\nThus, D2-sampling is actually an approximation algorithm by itself (albeit one with a worse approximation guarantee than other approximations). It has a running time of O(kdn) and is easy to implement. In addition, it serves well as a seeding method. Arthur and Vassilvitskii obtain experimental results indicating that k-means++ outperforms the k-means algorithm in practice, both in quality and running time. It also leads to better results than just using D2-sampling as an independent algorithm.\nIn follow-up work, Aggarwal et al. [7] show that when sampling O(k) centers instead of k centers, one obtains a constant-factor approximation algorithm for SSE. This is a bicriteria approximation because in addition to the fact that the clustering cost might not be optimal, the number of clusters is larger than k.\nAdaptive Sampling under Separation Conditions Clustering under separation conditions is an interesting research topic on its own. The idea is that the input to a clustering problem should have some structure, otherwise, clustering it would not be meaningful. Separation conditions assume that the optimal clusters cannot have arbitrary close centers or a huge overlap.\nWe focus on initialization strategies for the k-means algorithm. In this paragraph, we will see a result on adaptive sampling that uses a separation condition. In Section 3.2, we will see another example for the use of separation conditions. Other related work includes the paper by Balcan et al. [18], who proposed the idea to recover a \u2018true\u2019 (but not necessarily optimal) clustering and introduced assumptions under which this is possible. Their model is stronger than the model by Ostrovsky et al. [68] that we will describe next and triggered a lot of follow-up work on other clustering variants.\nOstrovsky et al. [68] analyze adaptive sampling under the following \u03b5-separability: The input is \u03b5-separated if clustering it (optimally) with k \u2212 1 instead of the desired k clusters increases the cost by a factor of at least 1/\u03b52. Ostrovsky et al. show that under this separation condition, an approach very similar to the above k-means++ seedings performs well1. In their seeding method, the first center is not chosen uniformly at random, but two centers are chosen simultaneously, and the probability for each pair of centers is proportional to their distance. Thus, the seeding starts by picking two centers with rather high distance instead of choosing one center uniformly at random and then picking a center with rather high distance to the first center. Ostrovsky et al. show that if the input is \u03b5-separated, this seeding achieves a (1 + f(\u03b5))-approximation for SSE where f(\u03b5) is a function that goes to zero if \u03b5 does so. The\n1Notice that though we present these results after [14] and [7] for reasons of presentation, the work of Ostrovsky et al. [68] appeared first.\nsuccess probability of this algorithm decreases exponentially in k (because there is a constant chance to miss the next cluster in every step), so Ostrovsky et al. enhance their algorithm by sampling O(k) clusters and using a greedy deletion process to reduce the number back to k. Thereby, they gain a linear-time constant-factor approximation algorithm (under their separation condition) that can be used as a seeding method.\nLater, Awasthi et al. [16] improved this result by giving an algorithm where the approximation guarantee and the separation condition are decoupled, i. e., parameterized by different parameters. Braverman et al. [25] developed a streaming algorithm.\nNote that \u03b5-separability scales with the number of clusters. Imagine k optimal clusters with the same clustering cost C, i. e., the total clustering cost is k \u00b7 C. Then, \u03b5-separability requires that clustering with k\u2212 1 clusters instead of k clusters costs at least k \u00b7 C/\u03b52. Thus, for more clusters, the pairwise separation has to be higher."}, {"heading": "3.2 Singular Value Decomposition and Best-Fit Subspaces", "text": "In the remainder of this section, we will review a result from a different line of research because it gives an interesting result for the SSE problem when we make certain input assumptions.\nLearning Mixtures of Gaussians In machine learning, clustering is often done from a different perspective, namely as a problem of learning parameters of mixture models. In this setting, a set of observations X is given (in our case, points) together with a statistical model, i. e., a family of density functions over a set of parameters \u0398 = {\u03981, . . . ,\u0398\u2113}. It is assumed that X was generated by the parameterized density function for one specific parameter set and the goal is to recover these parameters. Thus, the desired output are parameters which explain X best, e. g., because they lead to the highest likelihood that X was drawn.\nFor us, the special case that the density function is a mixture of Gaussian distributions on R d is of special interest because it is very related to SSE. Here, the set of observations X is a point set which we denote by P . On this topic, there has been a lot of research lately, which started by Dasgupta [30] who analyzed the problem under separation conditions. Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66]. The main reason why this work cannot be directly applied to SSE is the assumption that the input data X is actually drawn from the parameterized density function so that properties of these distributions can be used and certain extreme examples become unlikely and can be ignored. However, in [56], the authors prove a result which can be decoupled from this assumption, and the paper proposes an initialization method for the k-means algorithm. So, we take a closer look at this work.\nKumar and Kannan [56] assume a given target clustering which is to be recovered and then show the following. If (1 \u2212 \u03b5) \u00b7 |P | points in P satisfy a special condition which they call proximity condition (which depends on the target clustering), then applying a certain initialization method and afterwards running the k-means algorithm leads to a partitioning of the points that misclassifies at most O(k2\u03b5n) points. Kumar and Kannan also show that in many scenarios like learning of Gaussian mixtures, points satisfy their proximity condition with high probability.\nNotice that for \u03b5 = 0 their result implies that all points are correctly classified, i. e., the optimal partitioning is found. This in particular implies a result for the k-means algorithm\nwhich is the second step of the algorithm by Kumar and Kannan: It converges to the \u2018true\u2019 centers provided that the condition holds for all points. We take a closer look at the separation condition.\nSeparation Condition To define the proximity condition, consider the |P |\u00d7d matrix A which has the points of P in its rows. Also define the matrix C by writing the optimal center of the point in row i of A in row i of C (this implies that there are only k different rows vectors in C). Now, let T1, . . . , Tk be the target clustering, let \u00b5i be the mean of Ti, and let ni be the number of points in Ti. Then, define\n\u2206rs :=\n(\nck\u221a nr + ck\u221a ns\n)\n\u2016A\u2212 C\u2016S\nfor each r 6= s with r, s \u2208 {1, . . . , k}, where c is some constant. The term \u2016A \u2212 C\u2016S is the spectral norm of the matrix A\u2212 C, defined by\n\u2016A\u2212C\u2016S := max v\u2208Rd,\u2016v\u2016=1 \u2016(A\u2212 C) \u00b7 v\u20162 .\nA point p from cluster Tr satisfies the proximity condition if, for any s 6= r, the projection of p onto the line between \u00b5r and \u00b5s is at least \u2206rs closer to \u00b5r than to \u00b5s.\nWe have a closer look at the definition. The term A \u2212 C is the matrix consisting of the difference vectors, i. e., it gives the deviations of the points to their centers. The term \u2016(A \u2212 C) \u00b7 v\u20162 is the projection of these distance vectors into direction v, i. e., a measure on how much the data is scattered in this direction. Thus, \u2016A \u2212 C\u2016S/n is the largest average distance to the mean in any direction. It is an upper bound on the variance of the optimal clusters. Assume that ni = n/k for all i. Then, \u2206 2 rs = (2c)\n2k2\u2016A\u2212C\u20162S/ni is close to being the maximal average variance of the two clusters in any direction. It is actually larger, because \u2016A \u2212 C\u2016S includes all clusters, so \u2206rs and thus the separation of the points in Tr and Ts depends on all clusters even though it differs for different r, s.\nSeeding Method Given an input that is assumed to satisfy the above separation condition, Kumar and Kanan compute an initial solution by projecting the points onto a lowerdimensional subspace and approximately solving the low-dimensional instance. The computed centers form the seed to the k-means method.\nThe lower-dimensional subspace is the best-fit subspace Vk, i. e., it minimizes the expression \u2211\np\u2208P minv\u2208V \u2016p \u2212 v\u20162 among all k-dimensional subspaces V . It is known that Vk is the subspace spanned by the first k eigenvectors of A, which can be calculated by singular value decomposition (SVD)2, and that projecting points to Vk and solving the SSE optimally on the projected points yields a 2-approximation. Any constant-factor approximation thus gives a constant approximation for the original input.\nIn addition to these known facts, the result by Kumar and Kannan shows that initializing the k-means algorithm with this solution even yields an optimal solution as long as the optimal partition satisfies the proximity condition.\n2The computation of the SVD is a well-studied field of research. For an in-depth introduction to spectral algorithms and singular value decompositions, see [52].\n4 Variants and Extensions of the k-means Algorithm\nThe k-means algorithm is a widely used algorithm, but not always in the form given above. Naming all possible variations of the algorithm is beyond the scope of this survey and may be impossible to do. We look at two theoretically analyzed modifications.\nSingle Point Assignment Step We call a point in a given clustering misclassified if the distance to the cluster center it is currently assigned to is longer than the distance to at least one of the other cluster centers. Hence, in each iteration of the k-means algorithm, all misclassified points are assigned to their closest cluster center and then all cluster centers are moved to the means of the updated clusters. Har-Peled and Sadri [44] study a variant of the k-means algorithm in which the assignment step assigns only one misclassified point to the closest cluster center instead of all misclassified points at once as done in the original algorithm. After such an assignment step, the centers of the two updated clusters are moved to the means of the clusters. The algorithm repeats this until no misclassified points exist. Har-Peled and Sadri call their variant SinglePnt. Given a number of clusters k and a set P of n points with spread \u2206 from a Euclidean space Rd, they show that the number of iterations of SinglePnt is upper bounded by some polynomial in n, \u2206, and k.\nIn the following, we will describe the proof given in [44]. W.l.o.g., we can assume that the minimum pairwise distance in P is 1 and the diameter of P is \u2206. As we have seen for the classical k-means algorithm, the cost of P is O(n\u22062) after the first iteration of SinglePnt. The main idea is now to show that, in each following iteration of SinglePnt, the improvement of the clustering cost is lower bounded by some value dependent on the ratio between the distance of the reassigned point to the two involved cluster centers and the size of the two clusters. Based on this fact, we will prove that O(kn) iterations are sufficient to decrease the clustering cost by some constant amount, which results in O(kn2\u22062) iterations in total.\nLet Qi and Qj be any two clusters such that, in an assignment step, a point q \u2208 Qj moves from cluster Qj to cluster Qi, i. e., after this step we obtain the two clusters Q \u2032 i = Qi \u222a {q} and Q\u2032j = Qj\\{q}. Let \u00b5(X) be the mean of any point set X \u2282 Rd. Then, the movement of the first cluster center is\n\u2016\u00b5(Qi)\u2212 \u00b5(Q\u2032i)\u2016 = \u2225 \u2225 \u2225\n\u2225 \u00b5(Qi)\u2212 ( |Qi| |Qi|+ 1 \u00b5(Qi) + 1 |Qi|+ 1 q\n)\u2225\n\u2225 \u2225 \u2225 = \u2016\u00b5(Qi)\u2212 q\u2016 |Qi|+ 1 .\nSimilarly, we have \u2016\u00b5(Qj)\u2212 \u00b5(Q\u2032j)\u2016 = \u2016\u00b5(Qj)\u2212 q\u2016/(|Qj | \u2212 1). Due to Fact 1, the movement of the first cluster center decreases the clustering cost of Q\u2032i by (|Qi|+ 1)\u2016\u00b5(Qi)\u2212 \u00b5(Q\u2032i)\u20162 = \u2016\u00b5(Qi)\u2212q\u2016/(|Qi|+1), and the movement of the second cluster center decreases the clustering cost of Q\u2032j by (|Qj | \u2212 1)\u2016\u00b5(Qj)\u2212 \u00b5(Q\u2032j)\u20162 = \u2016\u00b5(Qj)\u2212 q\u2016/(|Qj | \u2212 1). It follows that the total decrease in the clustering cost is at least (\u2016\u00b5(Qi)\u2212 q\u2016+ \u2016\u00b5(Qj)\u2212 q\u2016)2/(2(|Qi|+ |Qj|)).\nThe reassignment of a point q \u2208 P is called good if the distance of q to at least one of the two centers of the involved clusters is bigger than 1/8. Otherwise, the reassignment is called bad. If a reassignment is good, then it follows from the above that the improvement of the clustering cost is at least (1/8)2/(2n) = 1/(128n). Thus, O(n) good reassignments are sufficient to improve the clustering cost by some constant amount. Next, we show that one out of k + 1 reassignments must be good, which then completes the proof.\nFor each i \u2208 {1, . . . , k}, let Bi be the ball with radius 1/8 whose center is the i-th center in the current clustering. Since the minimum pairwise distance in P is 1, each ball can contain at most one point of P . Observe that a point q \u2208 P can only be involved in a bad reassignment if it is contained in more than one ball. Let us consider the case that, due to a bad reassignment, a ball Bi loses its point q \u2208 P and so has been moved a distance of at most 1/8 away from q. Since the minimum pairwise distance in P is 1, Bi needs a good reassignment, so that it can again contain a point from P . Next, observe that, while performing only bad reassignments, a cluster Qi is changed by gaining or losing the point q contained in Bi. Hence, if a cluster Bi loses q, it cannot gain it back. Otherwise, the clustering cost would be increased. It follows that the total number of consecutive bad reassignments is at most k.\nGeneralization of Misclassification Har-Peled and Sadri [44] study another variant of the k-means algorithm, which they call Lazy-k-Means. This variant works exactly like the original algorithm except that each iteration reassigns only those points which are significantly misclassified. More precisely, given a k-clustering of a set P of n points from a Euclidean space Rd and a precision parameter \u03b5, 0 \u2264 \u03b5 \u2264 1, we call a point q \u2208 P (1+ \u03b5)-misclassified if q belongs to some cluster Qj and there is some other cluster Qi with \u2016q\u2212\u00b5(Qj)\u2016 > (1+ \u03b5)\u2016q\u2212\u00b5(Qi)\u2016, where \u00b5(X) is the mean of some set X \u2282 Rd. Each iteration of Lazy-k-Means reassigns all (1+\u03b5)-misclassified points to their closest cluster center and then moves each cluster center to the mean of its updated cluster. This process is repeated until there are no (1+\u03b5)-misclassified points. Note that, for \u03b5 = 0, Lazy-k-Means is equal to the k-means algorithm. For 0 < \u03b5 \u2264 1, Har-Peled and Sadri prove that the number of iteration of Lazy-k-Means is upper bounded by some polynomial in n, \u2206, and \u03b5\u22121, where \u2206 is the spread of the point set P .\nIn the following, we will sketch the proof given in [44]. W.l.o.g., we can assume that the minimum pairwise distance in P is 1 and the diameter of P is \u2206, so the clustering cost is O(n\u22062) after the first iteration of Lazy-k-Means. The idea is now to show that every two consecutive iterations lead to a cost improvement of \u2126(\u03b53), which results in O(n\u22062\u03b5\u22123) iterations in total. The proof of the lower bound on the cost improvement is based on the following known fact (see also Figure 3).\nFact 2. Given two points c, c\u2032 \u2208 Rd with \u2016c \u2212 c\u2032\u2016 = \u2113, all points q \u2208 Rd with \u2016q \u2212 c\u2016 > (1 + \u03b5)\u2016q \u2212 c\u2032\u2016 are contained in the open ball whose radius is R = \u2113(1 + \u03b5)/(\u03b5(2 + \u03b5)) and whose center is on the line containing the segment cc\u2032 at distance R+ \u2113\u03b5/(2(2 + \u03b5)) from the bisector of cc\u2032 and on the same side of the bisector as c\u2032. The ball is called \u03b5-Apollonius ball\nfor c\u2032 with respect to c.\nLet q \u2208 P be any (1 + \u03b5)-misclassified point that switches its assignment from a center c to another center c\u2032 with \u2113 = \u2016c \u2212 c\u2032\u2016. We also say that c and c\u2032 are the switch centers of q. Then, based on the fact that the distance of q to the bisector of cc\u2032 is at least \u2113\u03b5/(2(2+\u03b5)) (see Fact 2 and Figure 3) and by using Pythagorean equality, one can show that the improvement of the clustering cost for q is at least\n\u2016q \u2212 c\u20162 \u2212 \u2016q \u2212 c\u2032\u20162 \u2265 \u2113 2\u03b5\n2 + \u03b5 .\nWe call any (1+ \u03b5)-misclassified point q \u2208 P strongly misclassified if the distance between its switch centers is at least \u21130 := \u03b5(2 + \u03b5)/(16(1 + \u03b5)). Otherwise, a (1+ \u03b5)-misclassified point is called weakly misclassified. It follows from the above inequality that the improvement of the clustering cost caused by reassigning a strongly misclassified point is at least \u211320\u03b5/(2 + \u03b5) = \u2126(\u03b53) for 0 < \u03b5 \u2264 1. Thus, if we can show that at least every second iteration of Lazy-k-Means reassigns some strongly misclassified point, then we are done.\nLet us assume that there are only weakly misclassified points, and q is one of these points with switch centers c and c\u2032. We know that the distance \u2113 between c and c\u2032 is less than \u21130, which is less than 1/8 for 0 < \u03b5 \u2264 1. Furthermore, it follows from \u2113 < \u21130 that the radius of the \u03b5-Apollonius ball for c\u2032 with respect to c is less than 1/16 (see also Figure 4). Since q is contained in this \u03b5-Apollonius ball, the distance between c\u2032 and q is less than 1/8. Hence, both switch centers have a distance of less than 1/4 from q. Since the minimum pairwise distance in P is 1, every center can serve as a switch center for at most one weakly misclassified point.\nLet us consider any weakly misclassified point q with switch centers c and c\u2032, where c belongs to the cluster that loses q and c\u2032 belongs to the cluster that gains q. As explained above, both centers have a distance of less than 1/4 from q. Hence, due to reassigning q, center c is moved by a distance of less than 1/4. It follows that, after the considered iteration, the distance between c and q is less than 1/2. Since the minimum pairwise distance in P is 1, every other point in P has a distance of more than 1/2 to c. Thus, c can only be a switch\ncenter for strongly misclassified points in the next iteration. Furthermore, due to reassigning q, the gaining center c\u2032 is moved towards q. Since the distance of q to all the other points in P is at least 1, no other center can move closer to q than c\u2032 due to a reassignment of a weakly misclassified point. This means in the next iteration c\u2032 will still be the closest cluster center to q and q will not be (1+\u03b5)-misclassified. As a result, either there are no (1+\u03b5)-misclassified points left and the algorithm terminates or there are some strongly misclassified points. Thus, at least every second iteration reassigns some strongly misclassified points, which completes the proof.\n5 Alternatives to the k-means algorithm for big data\nAgain, naming all alternative clustering algorithms that have been proposed is beyond the scope of this survey. However, we will take a short look at algorithms, that are developed starting from a theoretical analysis (with respect to the SSE problem), but that are also implemented and shown to be viable in practice. We have already discussed one prime example for this type of algorithm, the k-means++ algorithm by Arthur and Vassilvitskii [14]. The running time of the seeding is comparable to one iteration of the k-means algorithm (when assuming that drawing random numbers is possible in constant time), so using it as a seeding method does not have a significant influence on the running time asymptotically or in practice. However, it turns the k-means algorithm into an expected O(log k)-approximation algorithm. A similar example is the local search algorithm by Kanungo et al. [54] that we describe in more detail in Section 6. It has a polynomial worst case running time and provides a constant approximation. Additionally, it was implemented and showed very good practical behavior when combined with the k-means algorithm.\nHowever, the research we have discussed in Section 2.1.1 aiming at accelerating the iterations of the k-means algorithm shows that there is interest in being faster than the k-means algorithm (and the constant approximation algorithms), and this interest increases with the availability of larger and larger amounts of data. The problem of solving the SSE problem for big data has been researched from a practical as well as from a theoretical side and in this section, we are interested in the intersection.\nThe theoretical model of choice is streaming. The data stream model assumes that the data can only be read once and in a given order, and that the algorithm is restricted to small space, e.g. polylogarithmic in the input it processes, but still computes an approximation. One-pass algorithms and low memory usage are certainly also desirable from a practical point of view, since random access to the data is a major slowdown for algorithms, and small memory usage might mean that all stored information actually fits into the main memory. The k-means algorithm reads the complete data set in each iteration, and a straightforward implementation of the k-means++ reads the data about k times for the seeding alone, and these are reasons why the algorithms do not scale so well for large inputs.\nAn old variant of the k-means algorithm, proposed independently of Lloyd\u2019s work by MacQueen [60], gives a very fast alternative to the k-means algorithm. It processes the data once, assigns each new data point to its closest center and updates this center to be the centroid of the points assigned to it. Thus, it never reassigns points. MacQueen\u2019s k-means algorithm clearly satisfies the first two requirements for a streaming algorithm, but not the third. Indeed, it is not surprising that MacQueen\u2019s algorithm does not necessarily converge to a good\nsolution, and that the solution depends heavily on the start centers and the order of the input points. The famous streaming algorithm BIRCH [77] is also very fast and is perceived as producing better clusterings, yet, it still shares the property that there is no approximation guarantee [37].\nVarious data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms). We now look at algorithms which lie in between practical and theoretical results.\nLocal search and the Stream framework\nGuha et al. [40] develop a framework for clustering algorithms in the data stream setting that they call Stream. They combine it with a constant factor approximation based on local search. The resulting algorithm is named StreamLS3. It computes a constant approximation in the data stream setting. StreamLS has originally been designed for the variant of the SSE problem where the distances are not squared (also called the k-median problem), but it is stated to work for the SSE problem as well with worse constants.\nThe Stream framework reads data in blocks of size m. For each block, it computes a set of c \u00b7k centers that are a constant factor approximation for the SSE problem with k centers (c is a constant) by using an approximation algorithm A. It thus reduces m points to c \u00b7 k points, where m is at least n\u03b5 for some \u03b5 > 0. This is repeated until the number of computed centers reaches m, i.e. it is repeated for m/(ck) blocks. Then, m2/(ck) points have been be reduced to m points. These are then again reduced to ck points, i.e. the computed centers are treated like as input to the same procedure, one level higher in a computation tree. On the ith level of this tree, ck points represent (m/ck)i input blocks. Thus, the height of the computation tree is at most O(logm/(ck) n/m) \u2208 O(logn\u03b5 n). This is actually a constant, since\nlogn\u03b5/(ck) n = log n\nlog n\u03b5 =\n1 \u03b5 .\nThus, the computation tree has constant height. It stores at most m points on each level, so the storage requirement of the algorithm is \u0398(m) = O(n\u03b5) under the assumption that A requires space that is linear in its input size. The running time of the algorithm is O(ndk) under the assumption that A has linear running time. Whenever an actual solution to the SSE problem is queried, it can be produced from the O(m) stored centers by computing a constant factor approximation by a different algorithm A\u2032. Guha et al. show that the result is a constant factor approximation for the original input data.\nGuha et al. also develop the algorithm LSEARCH which they use as the algorithm A within their framework. The algorithm StreamLS is the combination of the Stream framework with the algorithm LSEARCH. LSEARCH is a local search based algorithm that is based on algorithms for a related problem, the facility location problem. It is allowed to computed more than k centers, but additional centers are penalized. The main purpose of LSEARCH is an expected speed-up compared to other local search based methods with O(n2) running time.\n3http://infolab.stanford.edu/~loc/\nThe experiments included in [40] actually use the SSE criterion to evaluate their results, since the intention is to compare with the k-means algorithm, which is optimized for SSE. The data sets are around fifty thousand points and forty dimensions. First, LSEARCH is compared to the k-means algorithm and found to be about three times slower than the kmeans algorithm while producing results that are much better. Then, StreamLS is compared to BIRCH and to StreamKM, the algorithm resulting from embedding the k-means algorithm into the Stream framework. StreamLS and StreamKM compute solutions of much higher quality than BIRCH, with StreamLS computing the best solutions. BIRCH on the other hand is significantly faster, in particular, its running time per input point increases much less with increasing stream length.\nAdaptions of k-means++\nAilon, Jaiswal and Monteleoni [8] use the Stream framework and combine it with different approximation algorithms. The main idea is to extend the seeding part of the k-means++ algorithm to an algorithm called k-means# and to use this algorithm within the above Stream framework description. Recall that the seeding in k-means++ is done by D2-sampling. This method iteratively samples k centers. The first one is sampled uniformly at random. For the ith center, each input point p is sampled with probability D2(p)/ \u2211\nq\u2208P D 2(q), where P is\nthe input point set, D2(p) = minc1,...,ci\u22121 ||p\u2212 ci||2 is the cost of p in the current solution and c1, . . . ci\u22121 are the centers chosen so far. A set of k centers chosen in this way is an expected O(log k)-approximation.\nThe algorithm k-means# starts with choosing 3 log k centers uniformly at random and then performs k \u2212 1 iterations, each of which samples 3 log k centers according to the above given probability distribution. This is done to ensure that for an arbitrary optimal clustering of the points, each of the clusters is \u2018hit\u2019 with constant probability by at least one center. Ailon et al. show that the O(k log k) centers computed by k-means# are a constant factor approximation for the SSE criterion with high probability4.\nTo obtain the final algorithm, the Stream framework is used. Recall that the framework uses two approximation algorithms A and A\u2032. While A can be a bicriteria approximation that computes a constant factor approximation with c \u00b7 k centers, A\u2032 has to compute an approximative solution with k centers. The approximation guarantee of the final algorithm is the guarantee provided by A\u2032.\nAilon et al. sample k centers by D2-sampling for A\u2032, thus, the overall result is an expected O(log k) approximation. For A, k-means# is ran 3 log n times to reduce the error probability sufficiently and then the best clustering is reported. The overall algorithm needs n\u03b5 memory for a constant \u03b5 > 0.\nThe overall algorithm is compared to the k-means algorithm and to MacQueen\u2019s k-means algorithm on data sets with up to ten thousand points in up to sixty dimensions. While it produces solutions of better quality than the two k-means versions, it is slower than both.\nAckermann et al. [6] develop a streaming algorithm based on k-means++ motivated from a different line of work5. The ingredients of their algorithms look very much alike the basic\n4As briefly discussed in Section 3.1, it is sufficient to sample O(k) centers to obtain a constant factor approximation as later discovered by Aggarwal et al [7]. 5Project website online can be found at http://www.cs.uni-paderborn.de/fachgebiete/ag-bloemer/forschung/abgeschlossene/c\nbuilding blocks of the algorithm by Ailon et al.: sampling more than k points according to the k-means++ sampling method, organizing the computations in a binary tree and computing the final clustering with k-means++. There are key differences, though.\nFirstly, their work is motivated from the point of view of coresets for the SSE problem. A coreset S for a point set P is a smaller and weighted set of points that has approximately the same clustering cost as P for any choice of k centers. It thus satisfies a very strong property. Ackermann et al. show that sampling sufficiently many points according to the k-means++ sampling results in a coreset. For constant dimension d, they show that O(k \u00b7 (log n)O(1)) points guarantee that the clustering cost of the sampled points is within an \u03b5-error from the true cost of P for any set of k centers6.\nCoresets can be embedded into a streaming setting very nicely by using a technique called merge-and-reduce. It works similar as the computation tree of the Stream framework: It reads blocks of data, computes a coreset for each block and merges and reduces these coresets in a binary computation tree. Now the advantage is that this tree can have superconstant height since this can be cancelled out by adjusting the error \u03b5 of each coreset computation. A maximum height of \u0398(log n) means that the block size on the lowest level can be much smaller than above (recall that in the algorithm by Ailon et al., the block size was n\u03b5). For the above algorithm, a height of \u0398(log n) would mean that the approximation ratio would be \u2126(clogn) \u2208 \u2126(n). By embedding their coreset construction into the merge-and-reduce technique, Ackermann et al. provide a streaming algorithm that needs O(k \u00b7 (log n)O(1)) space and computes a coreset of similar size for SSE problem. They obtain a solution for the problem by running k-means++ on the coreset. Thus, the solution is an expected O(log k)approximation.\nSecondly, Ackermann et al. significantly speed up the k-means++ sampling approach. Since the sampling is applied again and again, this has a major impact on the running time. Notice that it is necessary for the sampling to compute D(p) for all p and to update this after each center that was drawn. When computing a coreset of m points for a point of \u2113 points, a vanilla implementation of this sampling needs \u0398(dm\u2113) time. Ackermann et. al. develop a data structure called coreset tree which allows to perform the sampling much faster. It does, however, change the sampling procedure slightly, such that the theoretically proven bound does not necessarily hold any more.\nIn the actual implementation, the sample size and thus the coreset size is set to 200k and thus much smaller than it is supported by the theoretical analysis. However, experiments support that the algorithm still produces solutions of high quality, despite these two heuristic changes. The resulting algorithm is called StreamKM++.\nAckermann et al. test their algorithm on data sets with up to eleven million points in up to 68 dimensions and compare the performance to BIRCH, StreamLS, the k-means algorithm and k-means++. They find that StreamLS and StreamKM++ compute solutions of comparable quality, and much better than BIRCH. BIRCH is the fastest algorithm. However, StreamKM++ beats the running time of StreamLS by far and can e.g. compute a solution for the largest data set and k = 30 in 27% of the running time of StreamLS. For small dimensions or higher k, the speed up is even larger. The k-means algorithm and k-means++ are much slower than StreamLS and thus also than StreamKM++. It is to be expected that\n6This holds with constant probability and for any constant \u03b5.\nStreamKM++ is faster than the variant by Ailon et al. as well.\nSufficient statistics\nThe renown algorithm BIRCH7 [77] computes a clustering in one pass over the data by maintaining a preclustering. It uses a data structure called clustering feature tree, where the term clustering feature denotes the sufficient statistics for the SSE problem. The leaves of the tree represent subsets of the input data by their sufficient statistics. At the arrival of each new point, BIRCH decides whether to add the point to an existing subset or not. If so, then it applies a rule to choose one of the subsets and to add the point to it by updating the sufficient statistics. This can be done in constant time. If not, then the tree grows and represents a partitioning with one more subset.\nBIRCH has a parameter for the maximum size of the tree. If the size of the tree exceeds this threshold, then it rebuilds the tree. Notice that a subset represented by its sufficient statistics cannot be split up. Thus, rebuilding means that some subsets are merged to obtain a smaller tree. After reading the input data, BIRCH represents each subset in the partitioning by a weighted point (which is obtained from the sufficient statistics) and then runs a clustering algorithm on the weighted point set.\nThe algorithm is very fast since updating the sufficient statistics is highly efficient and rebuilding does not occur too often. However, the solutions computed by BIRCH are not guaranteed to have a low cost with respect to the SSE cost function.\nFichtenberger et al. [37] develop the algorithm BICO8. The name is a combination of the words BIRCH and coreset. BICO also maintains a tree which stores a representation of a partitioning. Each node of this tree represents a subset by its sufficient statistics.\nThe idea of BICO is to improve the decision if and where to add a point to a subset in order to decrease the error of the summary. For this, BICO maintains a maximum error value T . A subset is forbidden to induce more error than T . The error of a subset is measured by the squared distances of all points in the subset to the centroid because in the end of the computation, the subset will be represented by the centroid.\nFor a new point, BICO searches for the subset whose centroid is closest to the point. BICO first checks whether the new point lies within a certain radius of this centroid since it wants to avoid to use all the allowed error of a subset for one point. If the point lies outside of the radius, a new node is created directly beneath the root of the tree for the new point. Otherwise, the point is added to this subset if the error keeps being bounded by T . If the point does not pass this check, then it is passed on to the child node of the current node whose centroid is closest. If no child node exists or the point lies without the nodes radius, then a new child node is created based on the new point.\nIf the tree gets too large, then T is doubled and the tree is rebuilt by merging subsets whose error as a combined subset is below the new T .\nFor constant dimension d, Fichtenberger et al. show that the altered method is guaranteed to compute a summary that satisfies the coreset property for a threshold value that lies in \u0398(k \u00b7 log n). Combined with k-means++, BICO gives an expected O(log k)-approximation.\n7http://pages.cs.wisc.edu/ vganti/birchcode/ 8http://ls2-www.cs.uni-dortmund.de/bico\nThe implementation of BICO faces the same challenge as StreamKM++, k-means or kmeans++, namely, it needs to again and again compute the distance between a point and its closest neighbor in a stored point set. BICO has one advantage, though, since it is only interested in this neighbor if it lies within a certain radius of the new point. This helps in developing heuristics to speed up the insertion process. The method implemented in BICO has the same worst case behavior as iterating through all stored points but can be much faster.\nFichtenberger et al. compare BICO to StreamKM++, BIRCH and MacQueen\u2019s k-means algorithm on the same data sets as in [6] and one additional 128-dimensional data set. In all experiments, the summary size of BICO is set to 200k, thus the summary is not necessarily a coreset. The findings are that BICO and StreamKM++ compute the best solutions, while BIRCH and MacQueen are the fastest algorithms. However, for small k, the running time of BICO is comparable to BIRCH and MacQueen. The running time of BICO is O(ndm), where m is the chosen summary size, thus, the increase in the running time for larger k stems from the choice m = 200k. For larger k, the running time can be decreased to lie below the running time of BIRCH by reducing m at the cost of worse solutions. In the tested instances, the quality was then still higher than for BIRCH and MacQueen."}, {"heading": "6 Complexity of SSE", "text": "Before we consider variants of the k-means algorithm that deal with objective functions different from SSE, we conclude our SSE related study by looking at the complexity of SSE in general. We start by delivering a proof to the following fact which we already used above. We also reflect on the insights that it gives us on the structure of optimal solutions of the SSE problem.\nFact 3. Let \u00b5 := 1|P | \u2211 p\u2208P p be the mean of a point set P , and let y \u2208 Rd be any point. Then, we have\n\u2211 p\u2208P \u2016p\u2212 y\u20162 = \u2211 p\u2208P \u2016p \u2212 \u00b5\u20162 + |P | \u00b7 \u2016y \u2212 \u00b5\u20162 .\nProof. The result is well known and the proof is contained in many papers. We in particular follow [55]. First note that\n\u2211 p\u2208P \u2016p \u2212 y\u20162 = \u2211 p\u2208P \u2016p \u2212 \u00b5+ \u00b5\u2212 y\u20162\n= \u2211\np\u2208P \u2016p \u2212 \u00b5\u20162 + 2(\u00b5 \u2212 y)T\n\u2211 p\u2208P (p\u2212 \u00b5) + |P | \u00b7 \u2016y \u2212 \u00b5\u20162 .\nThus, the statement follows from\n\u2211 p\u2208P (p \u2212 \u00b5) = \u2211 p\u2208P p\u2212 |P | \u00b7 \u00b5 = \u2211 p\u2208P p\u2212 |P | 1|P | \u2211 p\u2208P p = 0 .\nThe first consequence of Fact 3 is that the SSE problem can be solved analytically for k = 1: The mean \u00b5 minimizes the cost function, and the optimal cost is \u2211\np\u2208P \u2016p\u2212 \u00b5\u20162. For\nk \u2265 2, the optimal solution induces a partitioning of the input point set P into subsets of P with the same closest center. These subsets are called clusters. The center of a cluster is the mean of the points contained in the cluster (otherwise, exchanging the center by the mean would improve the solution). At the same time, every partitioning of the point set induces a feasible solution by computing the mean of each subset of the partitioning. This gives a new representation of an optimal solution as a partitioning of the input point set that minimizes the induced clustering cost.\nNotice that we cannot easily enumerate all possible centers as there are infinitely many possibilities. By our new view on optimal solutions, we can instead iterate over all possible partitionings. However, the number of possible partitionings is exponential in n for every constant k \u2265 2. We get the intuition that the problem is hard, probably even for small k. Next, we see a proof that this is indeed the case. Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al. [9].\nNP-Hardness of SSE We reduce the following problem to SSE with k = 2. Given a graph G = (V,E), a cut is a partitioning of the nodes V into subsets X \u2282 V and V \\X \u2282 V . By the density of a cut (X,V \\X), we mean the ratio |E(X)|/(|X| \u00b7 |V \\X|), where E(X) is the set of edges having one endpoint in X and the other endpoint in V \\X. Now, our version of the densest cut problem asks for the cut with the highest density. This problem is NP-hard because it is equivalent to finding the cut with minimal density in the complement graph, which is known to be NP-hard due to [65].\nWe define a type of incidence matrix M in the following way. In a |V | \u00d7 |E|-matrix, the entry in row i and column j is 0 if edge j is not incident to vertex i. Otherwise, let i\u2032 be the other vertex to which j is incident. Then, we arbitrarily set one of the two entries (i, j) and (i\u2032, j) to 1 and the other one to \u22121. For an example, see Figure 5(a) and 5(b). We interpret the rows of M as points in R|E| and name the set of these points P (V ). Each subset X \u2286 V then corresponds to a subset P (X) \u2286 P (V ), and a cut (X,V \\X) corresponds to a partitioning (P (X), P (X\\V )) of these points and thus to a 2-clustering. We take a closer look at the cost of cluster P (X) which is the sum of the costs of all points in it. For each point, the cost is the squared distance to the mean of P (X), and this cost can be calculated by summing up the squared differences in each coordinate. Remember that the coordinates correspond to edges in E. Thus, one way to analyze the cost is to figure out how much cost is caused by a specific edge. For each edge ej = (x, y), there are three possibilities for the\nclustering cost: If x, y \u2208 X, then the mean of P (X) has a 0 in the jth coordinate, and thus the squared distance is 0 for all coordinates except those corresponding to x and y, and it is 1 for these two. If x, y /\u2208 X, then the mean of P (X) also has a 0 in the jth coordinate, and as all points in P (X) also have 0 at the jth coordinate, this coordinate contributes nothing to the total cost. If either x \u2208 X, y /\u2208 X or x /\u2208 X, y \u2208 X and thus ej \u2208 E(X), then the mean has \u00b11/|X| as its jth coordinate, which induces a squared distance of (0\u22121/|X|)2 for |X|\u22121 of the points, and a squared distance of (1\u2212 1/|X|)2 for the one endpoint that is in X. Thus, the total cost of P (X) is\n\u2211\nej=(x,y)\u2208E,x,y\u2208X 2 +\n\u2211\nej=(x,y)\u2208E(X)\n[\n(|X| \u2212 1) 1|X|2 + (1\u2212 1/|X|) 2\n]\n= \u2211\nej=(x,y)\u2208E,x,y\u2208X 2 + |E(X)|\n(\n1\u2212 1|X|\n)\n.\nThis analysis holds for the clustering cost of P (V \\X) analogously. Additionally, every edge is either in E(X), or it has both endpoints in either P (X) or P (V \\V ). Thus, the total cost of the 2-clustering induced by X is\n2(|E| \u2212 |E(X)|) + |E(X)| ( 2\u2212 1|X| \u2212 1 |V \\X| ) = 2|E| \u2212 |E(X)| \u00b7 |V ||X| \u00b7 |V \\X| .\nFinding the optimal 2-clustering means that we minimize the above term. As 2|E| and |V | are the same for all possible 2-clusterings, this corresponds to finding the clustering which maximizes |E(X)|/(|X| \u00b7 |V \\X|). Thus, finding the best 2-clustering is equivalent to maximizing the density.\nNotice that the above transformation produces inputs which are |E|-dimensional. Thus, SSE is hard for constant k and arbitrary dimension. It is also hard for constant dimension d and arbitrary k [61]. For small dimension and a small number of clusters k, the problem can be solved in polynomial time by the algorithm of Inaba et al. [46].\nApproximation Algorithms This section is devoted to the existence of approximation algorithms for SSE. First, we convince ourselves that there is indeed hope for approximation algorithms with polynomial running time even if k or d is large. Above, we stated that we cannot solve the problem by enumerating all possible centers as there are infinitely many of them. But what if we choose centers only from the input point set? This does not lead to an optimal solution: Consider k = 1 and a point set lying on the boundary of a circle. Then the optimal solution is inside the circle (possibly its center) and is definitely not in the point set. However, the solution cannot be arbitrarily bad. Let k = 1 and let c \u2208 P be a point p \u2208 P which minimizes \u2016p \u2212 \u00b5\u20162, i. e., it is the point closest to the optimal center (breaking ties arbitrarily). Then,\ncost(P, {c}) = \u2211p\u2208P \u2016p\u2212 c\u20162 Fact 1 = \u2211 p\u2208P ( \u2016p \u2212 \u00b5\u20162 + \u2016c\u2212 \u00b5\u20162 )\n\u2264 \u2211p\u2208P ( \u2016p \u2212 \u00b5\u20162 + \u2016p \u2212 \u00b5\u20162 ) = 2cost(P, {\u00b5}) .\nThus, a 2-approximated solution to the 1-means problem can be found in quadratic time by iterating through all input points. For k > 1, the calculation holds for each cluster in the optimal solution, and thus there exists a 2-approximate solution consisting of k input points. By iterating through all O(nk) possible ways to choose k points from P , this gives a polynomial-time approximation algorithm for constant k.\nFor arbitrary k, we need a better way to explore the search space, i. e., the possible choices of centers out of P to gain a constant-factor approximation algorithm with polynomial running time. Kanungo et al. [55] show that a simple swapping algorithm suffices. Consider a candidate solution, i. e., a set C \u2286 P with |C| = k. The swapping algorithm repeatedly searches for points c \u2208 C and p \u2208 P\\C with cost(P,C) > cost(P,C \u222a {p}\\{c}), and then replaces c by p. Kanungo et al. prove that if no such swapping pair is found, then the solution is a 25- approximation of the best possible choice of centers from P . Thus, the swapping algorithm converges to a 50-approximation9. In addition, they show that in polynomial time by always taking swaps that significantly improve the solution, one only loses a (1 + \u03b5)-factor in the approximation guarantee. This gives a very simple local search algorithm with constant approximation guarantee. Kanungo et al. also refine their algorithm in two ways: First, they use a result by Matous\u030cek [64] that says that one can find a set S of size O(n\u03b5\u2212d log(1/\u03b5)) in time O(n log n + n\u03b5\u2212d log(1/\u03b5)) such that the best choice of centers from S is a (1 + \u03b5)approximation of the best choice of centers from Rd. This set is used to choose the centers from instead of simply using P . Second, they use q-swaps instead of the 1-swaps described before. Here, q\u2032 \u2264 q centers are simultaneously replaced by a set of q\u2032 new centers. They show that this leads to a (9 + \u03b5)-approximation and also give a tight example showing that 9 is the best possible approximation ratio for swapping-based algorithms.\nThe work of Kanungo et al. is one step in a series of papers developing approximation algorithms for SSE. The first constant approximation algorithm was given by Jain and Vazirani [49] who developed a primal dual approximation algorithm for a related problem and extended it to the SSE setting. Inaba et al. [46] developed the first polynomial-time (1 + \u03b5)approximation algorithm for the case of k = 2 clusters. Matus\u030cek [64] improved this and obtained a polynomial-time (1 + \u03b5)-approximation algorithm for constant k and d with running time O(n logk n) if \u03b5 is also fixed. Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57]. Notice that all cited (1 + \u03b5)-approximation algorithms are exponential in the number of clusters k and in some cases additionally in the dimension d.\nInapproximability results Algorithms with a (1 + \u03b5)-guarantee are only known for the case that k is a constant (and \u03b5 has to be a constant, too). Recently, Awasthi, Charikar, Krishnaswamy and Sinop [17] showed that there exists an \u03b5 such that it is NP-hard to approximate SSE within a factor of (1 + \u03b5) for arbitrary k and d. Their proof holds for a very small value of \u03b5, and a larger inapproximability result is not yet known.\n7 k-means with Bregman divergences\nThe k-means problem can be defined for any dissimilarity measure. An important class of dissimilarity measures are Bregman divergences. Bregman divergences have numerous appli-\n9Note that Kanungo et al. use a better candidate set and thus give a (25 + \u03b5)-approximation.\ncations in machine learning, data compression, speech and image analysis, data mining, or pattern recognition. We review mainly results known for the k-means algorithm when applied to Bregman divergences. As we will see, for Bregman divergences the k-means method can be applied almost without modifications to the algorithm.\nTo define Bregman divergences, let D \u2286 Rd, and let \u03a6 : D \u2192 R be a strictly convex function that is differentiable on the relative interior ri(D). The Bregman divergence d\u03a6 : D\u00d7 ri(D) \u2192 R\u22650 \u222a {\u221e} is defined as\nd\u03a6(x, c) = \u03a6(x)\u2212 \u03a6(c)\u2212 (x\u2212 c)T\u2207\u03a6(c),\nwhere \u2207\u03a6(c) is the gradient of \u03a6 at c. The squared Euclidean distance is a Bregman divergence. Other Bregman divergences that are used in various applications are shown on\nTable 6. Bregman divergences have a simple geometric interpretation that is shown in Figure 7. For c fixed, let fc : R d \u2192 R be defined by fc(x) : \u03a6(c)+(x\u2212c)T\u2207\u03a6(c). The function fc is a linear approximation to \u03a6 at point c. Then d\u03a6(x, c) is the difference between the true function value \u03a6(x) and the value fc(x) of the linear approximation to \u03a6 at c. Bregman divergences usually are asymmetric and violate the triangle inequality. In fact, the only symmetric Bregman divergences are the Mahalanobis divergences (see Table 6).\nAs one can see from Table 6, for some Bregman divergences d\u03a6 there exist points x, c such that d\u03a6(x, c) = \u221e. We call these pairs of points singularities. In most results and algorithms that we describe these singularities require special treatment or have to be defined away.\nk-means with Bregman divergences. Similar to SSE we can define the minimum sum-ofBregman-errors clustering problem (SBE). In this problem we are given a fixed Bregman divergence d\u03a6 with domain D and a set of points P \u2282 D. The aim is to find a set C \u2282 ri(D) of\nk points (not necessarily included in P ) such that the sum of the Bregman divergences of the points in P to their nearest center in C is minimized. Thus, the cost function to be minimized is\ncost\u03a6(P,C) := \u2211\np\u2208P min c\u2208C d\u03a6(p, c) .\nThe points in C are called centers. Because of the (possible) asymmetry of d\u03a6 the order of arguments in d\u03a6(x, c) is important.\nFor any Bregman divergence the optimal solution for k = 1 is given by the mean of the points in P . More precisely, Fact 1 completely carries over to Bregman divergences (see [20]).\nFact 4. Let d\u03a6 : D\u00d7 ri(D) \u2192 R\u22650 \u222a {\u221e} be a Bregman divergence and P \u2282 D, |P | < \u221e and let\n\u00b5 = 1 |P | \u2211\np\u2208P\nbe the mean of set P . For any y \u2208 ri(D): \u2211\np\u2208P d\u03a6(p, y) =\n\u2211 p\u2208P d\u03a6(p, \u00b5) + |P | \u00b7 d\u03a6(\u00b5, y).\nProof. It suffices to show the final statement of the Fact.\n\u2211 p\u2208P d\u03a6(p, y) = \u2211 p\u2208P \u03a6(p)\u2212 \u03a6(y)\u2212 (x\u2212 s)T\u2207\u03a6(y)\n= \u2211\np\u2208P \u03a6(p)\u2212 \u03a6(\u00b5) + \u03a6(\u00b5)\u2212 \u03a6(y)\u2212 (x\u2212 s)T\u2207\u03a6(y)\n= \u2211\np\u2208P (\u03a6(p)\u2212 \u03a6(\u00b5)) + |P |(\u03a6(\u00b5)\u2212 \u03a6(y))\u2212\n\n\n\u2211 p\u2208P (p \u2212 y)\n\n\nT\n\u2207\u03a6(y)\n= \u2211\np\u2208P (\u03a6(p)\u2212 \u03a6(\u00b5)) + |P |\n( \u03a6(\u00b5)\u2212 \u03a6(y)\u2212 (\u00b5\u2212 y)T\u2207\u03a6(y) )\n= \u2211\np\u2208P d\u03a6(p, \u00b5) + |P | \u00b7 d\u03a6(\u00b5, y),\nwhere the last equality follows from\n\u2211 p\u2208P (p\u2212 \u00b5)T = 0 and \u2211 p\u2208P (p\u2212 \u00b5)T\u2207\u03a6(\u00b5) = 0.\nMoreover, for all Bregman divergences, any set of input points P , and any set of k centers {\u00b51, . . . , \u00b5k}, the optimal partitions for SBE induced by the centers \u00b5j can be separated by hyperplanes. This was first explicitly stated in [20]. More precisely, the Bregman bisector {\nx \u2208 D | d\u03a6(x, c1) = d\u03a6(x, c2) }\nbetween any two points c1, c2 \u2208 D \u2286 Rd is always a hyperplane. i.e. for any pair of points c1, c2 there are a \u2208 Rd, b \u2208 R such that\n{ x \u2208 D | d\u03a6(x, c1) = d\u03a6(x, c2) } = { x \u2208 D | aTx = b } . (1)\nAs a consequence, SBE can be solved for any Bregman divergence in time O(nk 2d). Hence for fixed k and d, SBE is solvable in polynomial time. However, in general SBE is an NP-hard problem. This was first observed in [4] and can be shown in two steps. First, let the Bregman divergence d\u03a6 be a Mahalanobis divergence for a symmetric, positive definite matrix A. Then there is a unique symmetric, positive definite matrix B such that A = BTB, i.e. for any p, q\nd\u03a6(p, q) = (p \u2212 q)TA(p\u2212 q) = \u2016Bp\u2212Bq\u20162. (2)\nTherefore, SBE with d\u03a6 is just SSE for a linearly transformed input set. This immediately implies that for Mahalanobis divergences SBE is NP-hard. Next, if \u03a6 is sufficiently smooth, the Hessian \u22072\u03a6t of \u03a6 at point t \u2208 ri(D) is a symmetric, positive definite matrix. Therefore, d\u03a6 locally behaves like a Mahalanobis divergence. This can used to show that with appropriate restriction on the strictly convex function \u03a6 SBE is NP-hard.\nApproximation Algorithms and \u00b5-similarity. No provable approximation algorithms for general Bregman divergences are known. Approximation algorithms either work for specific Bregman divergences or for restricted classes of Bregman divergences. Chaudhuri and McGregor [27] give an O(log(n)) approximation algorithm for the Kullback-Leibler divergence (n is the size of the input set P ). They obtain this result by exploiting relationships between the Kullback-Leibler divergence and the so-called Hellinger distortion and between the Hellinger distortion and the squared Euclidean distance.\nThe largest subclass of Bregman divergences for which approximation algorithms are known to exist consists of \u00b5-similar Bregman divergences. A Bregman divergence d\u03a6 defined on domain D\u00d7 ri(D) is called \u00b5-similar if there is a symmetric, positive definite matrix A and a constant 0 < \u00b5 \u2264 1 such that for all (x, y) \u2208 D\u00d7 ri(D)\n\u00b5 \u00b7 dA(x, y) \u2264 d\u03a6(x, y) \u2264 dA(x, y). (3)\nSome Bregman divergences are (trivially) \u00b5-similar. Others, like the Kullback-Leibler divergence or the Itakura-Saito divergence become \u00b5-similar if one restricts the domain on which they are defined. For example, if we restrict the Kullback-Leibler divergence to D = [\u03bb, \u03bd]d for 0 < \u03bb < \u03bd \u2264 1, then the Kullback-Leibler divergence is \u03bb\u03bd -similar. This can be shown by looking at the first order Taylor series expansion of the negative Shannon entropy \u03a6(x1, . . . , xd) = \u2211\nxi ln(xi). \u00b5-similar Bregman divergences approximately behave like Mahalanobis divergences. Due to (2) Mahalanobis divergences behave like the squared Euclidean distance. Hence, one can hope that \u00b5-similar Bregman divergences behave roughly like the squared Euclidean distance. In fact, it is not too difficult to show that the swapping algorithm of Kanungo et al. [55] can be generalized to \u00b5-similar Bregman divergences to obtain approximation algorithms with approximation factor 18/\u00b52 + \u01eb for arbitrary \u01eb > 0. Whether one can combine the technique of Kanungo et al. with Matous\u030cek\u2019s technique [64] to obtain better constant factor approximation algorithms is not known.\nIn the work of Ackermann et al. [5], \u00b5-similarity has been used to obtain a probabilistic (1 + \u01eb)-approximation algorithm for SBE, whose running time is exponential in k, d, 1/\u01eb, and 1/\u00b5, but linear in |P |. Building upon results in [57], Ackermann at al. describe and analyze an algorithm to solve the k-median problem for metric and non-metric distance measures D that satisfy the following conditions.\n(1) For k = 1, optimal solutions to the k-median problem with respect to distance D can be computed efficiently.\n(2) For every \u03b4, \u03b3 > 0 there is a constant m\u03b4,\u03b3 such that for any set P , with probability 1 \u2212 \u03b4 the optimal 1-median of a random sample S of size m\u03b4,\u03b3 from P is a (1 + \u03b3)approximation to the 1-median for set P .\nTogether, (1) and (2) are called the [\u03b3, \u03b4]-sampling property. Using the same algorithm as in [57] but a combinatorial rather than geometric analysis, Ackermann et al. show that for any distance measure D satisfying the [\u03b3, \u03b4]-sampling property and any \u01eb > 0 there is an algorithm that with constant probability returns a (1 + \u01eb)-approximation to the k-median problem with distance measure D. The running time of the algorithm is linear in n, the number of input points, and exponential in k, 1/\u01eb, and the parameter m\u03b4,\u01eb/3 from the sampling\nproperty. Finally, Ackermann et al. show that any \u00b5-similar Bregman divergence satisfies the [\u03b4, \u03b3]-sampling property with parameter m\u03b4,\u03b3 = 1 \u03b3\u03b4\u00b5 . Overall, this yields a (1 + \u01eb) algorithm for SBE for \u00b5-similar Bregman divergences with running time linear in n, and exponential in k, 1/\u01eb, 1/\u00b5.\nThe k-means algorithm for Bregman divergences. The starting point for much of the recent research on SBE for Bregman divergences is the work by Banerjee et al. [20]. They were the first to explicitly state Fact 4 and describe the k-means algorithm (see page 2) as a generic algorithm to solve SBE for arbitrary Bregman divergences. Surprisingly, the kmeans algorithm cannot be generalized beyond Bregman divergences. In [19] it is shown, that under some mild smoothness conditions, any divergence that satisfies Fact 4 is a Bregman divergence. Of course, this does not imply that variants or modifications of the k-means algorithm cannot be used for distance measures other than Bregman divergences. However, in these generalizations cluster centroids cannot be used as optimizers in the second step, the re-estimation step.\nBanerjee et al. already showed that for any Bregman divergence the k-means algorithm terminates after a finite number of steps. In fact, using the linear separability of intermediate solutions computed by the k-means algorithm (see Eq. 1), for any Bregman divergence the number of iterations of the k-means algorithm can be bounded by O(nk2d). Since the squared Euclidean distance is a Bregman divergence it is clear that no approximation guarantees can be given for the solutions the k-means algorithm finds for SBE.\n1. Lower bounds. Manthey and Ro\u0308glin extended Vattani\u2019s exponential lower bound for the running time of the k-means algorithm to any Bregman divergence d\u03a6 defined by a sufficiently smooth function \u03a6. In their proof they use an approach similar to the approach used by Ackerman et al. to show that SBE is NP-hard. Using (2) Manthey and Ro\u0308glin first extend Vattani\u2019s lower bound to any Mahalanobis divergence. Then, using the fact that any Bregman divergence d\u03a6 with sufficiently smooth \u03a6 locally resembles some Mahalanobis divergence dA, Manthey and Ro\u0308glin show that a lower bound for the Mahalanobis divergence dA carries over to a lower bound for the Bregman divergence d\u03a6. Hence, for any smooth Bregman divergence the k-means algorithm has exponential running time. Moreover, Manthey and Ro\u0308glin show that for the k-means algorithm the squared Euclidean distance, and more generally Mahalanobis divergences, are the easiest Bregman divergences.\n2. Smoothed analysis. Recall that the smoothed complexity of the k-means algorithm is polynomial in n and 1/\u03c3, when each input point is perturbed by random noise generated using a Gaussian distribution with mean 0 and standard deviation \u03c3, a result due to Arthur, Manthey, and Ro\u0308glin [12]. So far, this result has not been generalized to Bregman divergences. For almost any Bregman divergence d\u03a6 Manthey and Ro\u0308glin [62] prove two upper bounds on the smoothed complexity of the k-means algorithm. The first bound is of the form poly(n \u221a k, 1/\u03c3), the second is of the form kkd \u00b7poly(n, 1/\u03c3). These bounds match bounds that Manthey and Ro\u0308gin achieved for the squared Euclidean distance in [63]. Instead of reviewing their proofs, we will briefly review two technical difficulties Manthey and Ro\u0308glin had to account for.\nBregman divergences d\u03a6 : D \u00d7 ri(D) \u2192 R\u22650 \u222a {\u221e} like the Kullback-Leibler divergence are defined on a bounded subset of some Rd. Therefore perturbing a point in D may yield a point for which the Bregman divergence is not defined. Moreover, whereas the Gaussian noise is natural for the squared Euclidean distance this is by no means clear for all Bregman divergences. In fact, Banerjee et al. [20] already showed a close connection between Bregman divergences and exponential families, indicating that noise chosen according to an exponential distribution may be appropriate for some Bregman divergences. Manthey and Ro\u0308glin deal with these issues by first introducing a general and abstract perturbation model parametrized by some \u03c3 \u2208 (0, 1]. Then Manthey and Ro\u0308glin give a smoothed analysis of the k-means algorithm for Bregman divergences with respect to this abstract model. It is important to note that as in the squared Euclidean case, the parameter \u03c3 measures the amount of randomness in the perturbation. Finally, for Bregman divergences like the Mahalanobis divergences, the Kullback-Leibler divergence, or the Itakura-Saito Manthey and Ro\u0308glin instantiate the abstract perturbation model with some perturbations schemes using explicit distributions.\nSingularities of Bregman divergences are the second technical difficulty that Manthey and Ro\u0308glin have to deal with. For each Bregman divergence d\u03a6 they introduce two parameters 0 < \u03b6 \u2264 1 and \u03be \u2265 1 that in some sense measures how far away d\u03a6 is from being a Mahalanobis divergence. This resembles the \u00b5-similarity introduced by Ackermann et al. [5]. Whereas for many Bregman divergences the parameter \u00b5 can only be defined by restricting the domain of the divergence, this is not necessary in the approach by Manthey and Ro\u0308gin. However, their upper bounds on the smoothed complexity of the k-means algorithm for Bregman divergences are not uniform, instead for any specific Bregman divergence the bound depends (polynomially) on the values \u03be and 1/\u03b6.\nIt is still an open problem whether the polynomial bound of Arthur et al. [12] on the smoothed complexity of the k-means algorithm can be generalized to Bregman divergences. Surprisingly, even for general Mahalanobis divergences this is not known. As Manthey and Ro\u0308glin mention, at this point polynomial bounds on the smoothed complexity of the k-means algorithm can only be achieved for Mahalanobis divergences dA and input sets P , where the largest eigenvalue of A is bounded by a polynomial in |P |.\n3. Seeding methods. In [2] the k-means++ randomized seeding algorithm by Arthur and Vassilvitskii [14] is generalized to \u00b5-similar Bregman divergences. Ackermann and Blo\u0308mer show that for a \u00b5-similar Bregman divergence this generalization, called Bregman++, yields a O ( \u00b5\u22122 log(k) )\n-approximation for SBE. In [3] Ackermann and Blo\u0308mer generalize the result by Ostrovsky et a. [68] on adaptive sampling for \u01eb-separable instances to Bregman divergences.\nNock et al. [67] generalize k-means++ to certain symmetrized versions of Bregman divergences d\u03a6, called mixed Bregman divergences. They prove approximation factors of the form O ( \u03c1\u03c8 log k )\n, where \u03c1\u03c8 is some parameter depending on d\u03a6, that roughly measures how much d\u03a6 violates the triangle inequality. Note, however, that the mixed Bregman divergences introduced by Nock et al. are not proper Bregman divergences."}], "references": [{"title": "On spectral learning of mixtures of distributions. In: COLT", "author": ["D. Achlioptas", "F. McSherry"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Coresets and approximate clustering for bregman divergences", "author": ["M.R. Ackermann", "J. Bl\u00f6mer"], "venue": "Proceedings of the 20th Annual ACM- SIAM Symposium on Discrete Algorithms (SODA", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Bregman clustering for separable instances", "author": ["M.R. Ackermann", "J. Bl\u00f6mer"], "venue": "Proceedings of the 12th Scandinavian Symposium and Workshop on Algorithm Theory (SWAT", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Hardness and non-approximability of bregman clustering problems", "author": ["M.R. Ackermann", "J. Bl\u00f6mer", "C. Scholz"], "venue": "Electronic Colloquium on Computational Complexity (ECCC) 18(15),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Clustering for metric and non-metric distance measures", "author": ["M.R. Ackermann", "J. Bl\u00f6mer", "C. Sohler"], "venue": "ACM Transactions on Algorithms 6(4),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Streamkm++: A clustering algorithm for data streams", "author": ["M.R. Ackermann", "M. M\u00e4rtens", "C. Raupach", "K. Swierkot", "C. Lammersen", "C. Sohler"], "venue": "ACM Journal of Experimental Algorithmics 17,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Adaptive sampling for k-means clustering", "author": ["A. Aggarwal", "A. Deshpande", "R. Kannan"], "venue": "In: APPROX-RANDOM. pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "Proceedings of the 22nd Annual Conference on Neural Information Processing Systems. pp", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "NP-hardness of Euclidean sum-ofsquares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "An efficient k-means clustering algorithm", "author": ["K. Alsabti", "S. Ranka", "V. Singh"], "venue": "Proceeding of the First Workshop on High-Performance Data Mining", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Learning mixtures of separated nonspherical gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "Annals of Applied Probability", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "k-means has polynomial smoothed complexity", "author": ["D. Arthur", "B. Manthey", "H. R\u00f6glin"], "venue": "Proceedings of the 50th Annual IEEE Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "How slow is the k-means method", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the 22nd ACM Symposium on Computational Geometry (SoCG", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Worst-case and smoothed analysis of the ICP algorithm, with an application to the k-means method", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SIAM Journal on Computing", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["P. Awasthi", "A. Blum", "O. Sheffet"], "venue": "In: FOCS. pp", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "The hardness of approximation of euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "SoCG", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Approximate clustering without the approximation", "author": ["M.F. Balcan", "A. Blum", "A. Gupta"], "venue": "In: SODA. pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "On the optimality of conditional expectation as a bregman predictor", "author": ["A. Banerjee", "X. Guo", "H. Wang"], "venue": "Information Theory, IEEE Transactions on 51(7),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Clustering with bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Toward learning gaussian mixtures with arbitrary separation", "author": ["M. Belkin", "K. Sinha"], "venue": "COLT. pp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Learning gaussian mixtures with arbitrary separation", "author": ["M. Belkin", "K. Sinha"], "venue": "CoRR abs/0907.1054", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Polynomial learning of distribution families", "author": ["M. Belkin", "K. Sinha"], "venue": "FOCS. pp. 103\u2013", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "Grouping Multidimensional Data,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Streaming k-means on well-clusterable data", "author": ["V. Braverman", "A. Meyerson", "R. Ostrovsky", "A. Roytman", "M. Shindler", "B. Tagiku"], "venue": "SODA. pp", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Isotropic pca and affine-invariant clustering", "author": ["S.C. Brubaker", "S. Vempala"], "venue": "In: FOCS. pp", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Finding metric structure in information theoretic clustering. In: COLT", "author": ["K. Chaudhuri", "A. McGregor"], "venue": "Citeseer", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "COLT. pp", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM Journal on Computing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "In: FOCS. pp", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "How fast is k-means", "author": ["S. Dasgupta"], "venue": "In: COLT. p", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "The hardness of k-means clustering", "author": ["S. Dasgupta"], "venue": "Tech. Rep. CS2008-0916,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["S. Dasgupta", "L.J. Schulman"], "venue": "Journal of Machine Learning Research", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "A unified framework for approximating and clustering data", "author": ["D. Feldman", "M. Langberg"], "venue": "Proceedings of the 43th Annual ACM Symposium on Theory of Computing (STOC). pp", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "A ptas for k-means clustering based on weak coresets", "author": ["D. Feldman", "M. Monemizadeh", "C. Sohler"], "venue": "Proceedings of the 23rd ACM Symposium on Computational Geometry (SoCG). pp", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["J. Feldman", "R. O\u2019Donnell", "R.A. Servedio"], "venue": "SIAM J. Comput", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "BICO: BIRCH Meets Coresets for k-Means Clustering", "author": ["H. Fichtenberger", "M. Gill\u00e9", "M. Schmidt", "C. Schwiegelshohn", "C. Sohler"], "venue": "Proceedings of the 21st European Symposium on Algorithms (ESA). pp", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Coresets in dynamic geometric data streams", "author": ["G. Frahling", "C. Sohler"], "venue": "Proceedings of the 37th STOC. pp", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Null models in cluster validation. In: From data to knowledge : theoretical and practical aspects of classification, data analysis, and knowledge organization, pp", "author": ["A. Gordon"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering 15(3),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Accelerating lloyd\u2019s algorithm for k-means clustering", "author": ["G. Hamerly", "J. Drake"], "venue": "Partitional Clustering Algorithms, pp", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Smaller coresets for k-median and k-means clustering", "author": ["S. Har-Peled", "A. Kushal"], "venue": "Discrete & Computational Geometry 37(1),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Proceedings of the 36th Annual ACM Symposium on Theory of Computing (STOC", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "How fast is the k-means method", "author": ["S. Har-Peled", "B. Sadri"], "venue": "In: SODA. pp", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2005}, {"title": "Clustering Algorithms", "author": ["J.A. Hartigan"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1975}, {"title": "Applications of weighted voronoi diagrams and randomization to variance-based k-clustering (extended abstract)", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "In: Symposium on Computational Geometry (SoCG", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1994}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters 31(8),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys 31(3),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1999}, {"title": "Approximation algorithms for metric facility location and kmedian problems using the primal-dual schema and lagrangian relaxation", "author": ["K. Jain", "V.V. Vazirani"], "venue": "J. ACM 48(2),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2001}, {"title": "Large-scale parallel data clustering", "author": ["D. Judd", "P.K. McKinley", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20(8),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1998}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "In: STOC. pp", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Spectral algorithms. Foundations and Trends in Theoretical Computer Science", "author": ["R. Kannan", "S. Vempala"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "SIAM Journal Comput", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "IEEE Transactions on pattern analysis and machine intelligence 24(7),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2002}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2004}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["A. Kumar", "R. Kannan"], "venue": "Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2010}, {"title": "Linear-time approximation schemes for clustering problems in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "Journal of the ACM", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2010}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "Bell Laboratories Technical Memorandum", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1957}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory 28(2),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1967}, {"title": "The planar k-means problem is nphard", "author": ["M. Mahajan", "P. Nimbhorkar", "K.R. Varadarajan"], "venue": "In: WALCOM. pp", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2009}, {"title": "Worst-case and smoothed analysis of k-means clustering with Bregman divergences", "author": ["B. Manthey", "H. R\u00f6glin"], "venue": "JoCG 4(1),", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Improved smoothed analysis of the k-means method", "author": ["B. Manthey", "H. R\u00f6lin"], "venue": "Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms. pp. 461\u2013470. Society for Industrial and Applied Mathematics", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2009}, {"title": "On approximate geometric k-clustering", "author": ["J. Matou\u0161ek"], "venue": "Discrete & Computational Geometry 24(1),", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2000}, {"title": "Sparsest cuts and bottlenecks in graphs", "author": ["D.W. Matula", "F. Shahrokhi"], "venue": "Discrete Applied Mathematics 27,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1990}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}, {"title": "Mixed bregman clustering with approximation guarantees. In: Machine Learning and Knowledge Discovery in Databases", "author": ["R. Nock", "P. Luosto", "J. Kivinen"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2008}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In: FOCS. pp", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "Accelerating exact k -means algorithms with geometric reasoning", "author": ["D. Pelleg", "A.W. Moore"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 1999}, {"title": "k-means-type algorithms: A generalized convergence theorem and characterization of local optimality", "author": ["S.Z. Selim", "M.A. Ismail"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1984}, {"title": "Sur la division des corps mat\u00e9riels en parties", "author": ["H. Steinhaus"], "venue": "Bulletin de l\u2019Acade\u0301mie Polonaise des Sciences IV(12),", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1956}, {"title": "Estimating the number of clusters in a dataset via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2001}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Proceedings of the 25th ACM Symposium on Computational Geometry (SoCG", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2009}, {"title": "Approximation schemes for clustering problems", "author": ["W.F. de la Vega", "M. Karpinski", "C. Kenyon", "Y. Rabani"], "venue": "Proceedings of the 35th Annual ACM Symposium on Theory of Computing (STOC", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2003}, {"title": "A spectral algorithm for learning mixture models", "author": ["S. Vempala", "G. Wang"], "venue": "J. Comput. Syst. Sci. 68(4),", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "Choosing the number of clusters I-III", "author": ["S. Venkatasubramanian"], "venue": "http://blog.geomblog.org/p/conceptual-view-of-clustering.html", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}, {"title": "BIRCH: A New Data Clustering Algorithm and Its Applications", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "Data Mining and Knowledge Discovery 1(2),", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1997}], "referenceMentions": [{"referenceID": 44, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 79, "endOffset": 87}, {"referenceID": 47, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 79, "endOffset": 87}, {"referenceID": 23, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 109, "endOffset": 117}, {"referenceID": 46, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 109, "endOffset": 117}, {"referenceID": 46, "context": "Notice that the title of [47] is Data clustering: 50 years beyond K-means in reference to the k-means algorithm, the probably most widely used clustering algorithm of all time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 57, "context": "It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey.", "startOffset": 33, "endOffset": 37}, {"referenceID": 70, "context": "It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey.", "startOffset": 78, "endOffset": 82}, {"referenceID": 75, "context": "A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39].", "startOffset": 71, "endOffset": 75}, {"referenceID": 71, "context": "A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39].", "startOffset": 145, "endOffset": 149}, {"referenceID": 38, "context": "A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39].", "startOffset": 176, "endOffset": 180}, {"referenceID": 46, "context": "As Jain [47] also notices, the k-means algorithm is still widely used for clustering and in particular for solving the SSE problem.", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "The example pruning rules are from [50].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Different algorithms based on this idea are given in [10, 54, 69].", "startOffset": 53, "endOffset": 65}, {"referenceID": 53, "context": "Different algorithms based on this idea are given in [10, 54, 69].", "startOffset": 53, "endOffset": 65}, {"referenceID": 68, "context": "Different algorithms based on this idea are given in [10, 54, 69].", "startOffset": 53, "endOffset": 65}, {"referenceID": 76, "context": "as a building block of the well-known data stream clustering algorithm BIRCH [77].", "startOffset": 77, "endOffset": 81}, {"referenceID": 40, "context": "For an extensive overview and more pointers to the literature, see [41].", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "For the special case of d = 1 and k < 5, Dasgupta [31] proved an upper bound of O(n) iterations.", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Later, for d = 1 and any k, Har-Peled and Sadri [44] showed an upper bound of O(n\u22062) iterations, where \u2206 is the ratio between the diameter and the smallest pairwise distance of the input points.", "startOffset": 48, "endOffset": 52}, {"referenceID": 43, "context": "In the following, we will explain the idea to obtain the upper bound given in [44].", "startOffset": 78, "endOffset": 82}, {"referenceID": 43, "context": "Figure 1: Illustration of the upper bound for the k-means algorithm [44].", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Lower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73].", "startOffset": 103, "endOffset": 115}, {"referenceID": 30, "context": "Lower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73].", "startOffset": 103, "endOffset": 115}, {"referenceID": 72, "context": "Lower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73].", "startOffset": 103, "endOffset": 115}, {"referenceID": 30, "context": "Dasgupta [31] proved that the k-means algorithm has a worstcase running time of \u03a9(n) iterations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "Using a construction in some \u03a9( \u221a n)-dimensional space, Arthur and Vassilvitskii [13] were able to improve this result to obtain a super-polynomial worst-case running time of 2\u03a9( \u221a n) iterations.", "startOffset": 81, "endOffset": 85}, {"referenceID": 72, "context": "This has been simplified and further improved by Vattani [73] who proved an exponential lower bound on the worst-case running time of the k-means algorithm showing that k-means requires 2\u03a9(n) iterations even in the plane.", "startOffset": 57, "endOffset": 61}, {"referenceID": 72, "context": "In the following, we will give a high-level view on the construction presented in [73].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63].", "startOffset": 117, "endOffset": 129}, {"referenceID": 14, "context": "To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63].", "startOffset": 117, "endOffset": 129}, {"referenceID": 62, "context": "To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63].", "startOffset": 117, "endOffset": 129}, {"referenceID": 14, "context": "Arthur and Vassilvitskii [15] showed that, in the smoothed setting, the number of iterations", "startOffset": 25, "endOffset": 29}, {"referenceID": 62, "context": "This was improved by Manthey and R\u00f6glin [63] who proved the upper bounds poly(n \u221a k, 1/\u03c3) and kkd \u00b7poly(n, 1/\u03c3) on the number of iterations.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "[12] showed that k-means has a polynomial-time smoothed complexity of poly(n, 1/\u03c3).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In the following, we will give a high-level view on the intricate analysis presented in [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 69, "context": "It is known that the k-means algorithm converges to a local optimum [70] and that no approximation ratio can be guaranteed [55].", "startOffset": 68, "endOffset": 72}, {"referenceID": 54, "context": "It is known that the k-means algorithm converges to a local optimum [70] and that no approximation ratio can be guaranteed [55].", "startOffset": 123, "endOffset": 127}, {"referenceID": 54, "context": "[55] illustrate the latter fact by the simple example given in Figure 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "Figure 2: Example illustrating the fact that no approximation guarantee can be given for the k-means algorithm [55].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Arthur and Vassilvitskii [14] proposed a seeding method for the k-means algorithm which applies adaptive sampling.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "In the end, this process leads to a set of k centers that is an expected O(log k)-approximation [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "[7] show that when sampling O(k) centers instead of k centers, one obtains a constant-factor approximation algorithm for SSE.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18], who proposed the idea to recover a \u2018true\u2019 (but not necessarily optimal) clustering and introduced assumptions under which this is possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "[68] that we will describe next and triggered a lot of follow-up work on other clustering variants.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "[68] analyze adaptive sampling under the following \u03b5-separability: The input is \u03b5-separated if clustering it (optimally) with k \u2212 1 instead of the desired k clusters increases the cost by a factor of at least 1/\u03b52.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The Notice that though we present these results after [14] and [7] for reasons of presentation, the work of Ostrovsky et al.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "The Notice that though we present these results after [14] and [7] for reasons of presentation, the work of Ostrovsky et al.", "startOffset": 63, "endOffset": 66}, {"referenceID": 67, "context": "[68] appeared first.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] improved this result by giving an algorithm where the approximation guarantee and the separation condition are decoupled, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] developed a streaming algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "On this topic, there has been a lot of research lately, which started by Dasgupta [30] who analyzed the problem under separation conditions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 10, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 25, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 27, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 32, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 52, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 74, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 21, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 20, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 22, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 35, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 50, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 65, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 55, "context": "However, in [56], the authors prove a result which can be decoupled from this assumption, and the paper proposes an initialization method for the k-means algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 55, "context": "Kumar and Kannan [56] assume a given target clustering which is to be recovered and then show the following.", "startOffset": 17, "endOffset": 21}, {"referenceID": 51, "context": "For an in-depth introduction to spectral algorithms and singular value decompositions, see [52].", "startOffset": 91, "endOffset": 95}, {"referenceID": 43, "context": "Har-Peled and Sadri [44] study a variant of the k-means algorithm in which the assignment step assigns only one misclassified point to the closest cluster center instead of all misclassified points at once as done in the original algorithm.", "startOffset": 20, "endOffset": 24}, {"referenceID": 43, "context": "In the following, we will describe the proof given in [44].", "startOffset": 54, "endOffset": 58}, {"referenceID": 43, "context": "Generalization of Misclassification Har-Peled and Sadri [44] study another variant of the k-means algorithm, which they call Lazy-k-Means.", "startOffset": 56, "endOffset": 60}, {"referenceID": 43, "context": "In the following, we will sketch the proof given in [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "We have already discussed one prime example for this type of algorithm, the k-means++ algorithm by Arthur and Vassilvitskii [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 53, "context": "[54] that we describe in more detail in Section 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "An old variant of the k-means algorithm, proposed independently of Lloyd\u2019s work by MacQueen [60], gives a very fast alternative to the k-means algorithm.", "startOffset": 92, "endOffset": 96}, {"referenceID": 76, "context": "The famous streaming algorithm BIRCH [77] is also very fast and is perceived as producing better clusterings, yet, it still shares the property that there is no approximation guarantee [37].", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "The famous streaming algorithm BIRCH [77] is also very fast and is perceived as producing better clusterings, yet, it still shares the property that there is no approximation guarantee [37].", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 33, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 34, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 37, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 41, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 42, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 39, "context": "[40] develop a framework for clustering algorithms in the data stream setting that they call Stream.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "The experiments included in [40] actually use the SSE criterion to evaluate their results, since the intention is to compare with the k-means algorithm, which is optimized for SSE.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Ailon, Jaiswal and Monteleoni [8] use the Stream framework and combine it with different approximation algorithms.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "[6] develop a streaming algorithm based on k-means++ motivated from a different line of work5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "1, it is sufficient to sample O(k) centers to obtain a constant factor approximation as later discovered by Aggarwal et al [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 76, "context": "Sufficient statistics The renown algorithm BIRCH7 [77] computes a clustering in one pass over the data by maintaining a preclustering.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "[37] develop the algorithm BICO8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "compare BICO to StreamKM++, BIRCH and MacQueen\u2019s k-means algorithm on the same data sets as in [6] and one additional 128-dimensional data set.", "startOffset": 95, "endOffset": 98}, {"referenceID": 54, "context": "We in particular follow [55].", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al.", "startOffset": 74, "endOffset": 85}, {"referenceID": 31, "context": "Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al.", "startOffset": 74, "endOffset": 85}, {"referenceID": 60, "context": "Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al.", "startOffset": 74, "endOffset": 85}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 64, "context": "This problem is NP-hard because it is equivalent to finding the cut with minimal density in the complement graph, which is known to be NP-hard due to [65].", "startOffset": 150, "endOffset": 154}, {"referenceID": 60, "context": "It is also hard for constant dimension d and arbitrary k [61].", "startOffset": 57, "endOffset": 61}, {"referenceID": 45, "context": "[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[55] show that a simple swapping algorithm suffices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "also refine their algorithm in two ways: First, they use a result by Matou\u0161ek [64] that says that one can find a set S of size O(n\u03b5\u2212d log(1/\u03b5)) in time O(n log n + n\u03b5\u2212d log(1/\u03b5)) such that the best choice of centers from S is a (1 + \u03b5)approximation of the best choice of centers from Rd.", "startOffset": 78, "endOffset": 82}, {"referenceID": 48, "context": "The first constant approximation algorithm was given by Jain and Vazirani [49] who developed a primal dual approximation algorithm for a related problem and extended it to the SSE setting.", "startOffset": 74, "endOffset": 78}, {"referenceID": 45, "context": "[46] developed the first polynomial-time (1 + \u03b5)approximation algorithm for the case of k = 2 clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "Matu\u0161ek [64] improved this and obtained a polynomial-time (1 + \u03b5)-approximation algorithm for constant k and d with running time O(n log n) if \u03b5 is also fixed.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 73, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 33, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 37, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 42, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 56, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 16, "context": "Recently, Awasthi, Charikar, Krishnaswamy and Sinop [17] showed that there exists an \u03b5 such that it is NP-hard to approximate SSE within a factor of (1 + \u03b5) for arbitrary k and d.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "Shannon entropy Kullback-Leibler divergence [0, 1] \u2211", "startOffset": 44, "endOffset": 50}, {"referenceID": 19, "context": "More precisely, Fact 1 completely carries over to Bregman divergences (see [20]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "This was first explicitly stated in [20].", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "This was first observed in [4] and can be shown in two steps.", "startOffset": 27, "endOffset": 30}, {"referenceID": 26, "context": "Chaudhuri and McGregor [27] give an O(log(n)) approximation algorithm for the Kullback-Leibler divergence (n is the size of the input set P ).", "startOffset": 23, "endOffset": 27}, {"referenceID": 54, "context": "[55] can be generalized to \u03bc-similar Bregman divergences to obtain approximation algorithms with approximation factor 18/\u03bc2 + \u01eb for arbitrary \u01eb > 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "with Matou\u0161ek\u2019s technique [64] to obtain better constant factor approximation algorithms is not known.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "[5], \u03bc-similarity has been used to obtain a probabilistic (1 + \u01eb)-approximation algorithm for SBE, whose running time is exponential in k, d, 1/\u01eb, and 1/\u03bc, but linear in |P |.", "startOffset": 0, "endOffset": 3}, {"referenceID": 56, "context": "Building upon results in [57], Ackermann at al.", "startOffset": 25, "endOffset": 29}, {"referenceID": 56, "context": "Using the same algorithm as in [57] but a combinatorial rather than geometric analysis, Ackermann et al.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In [19] it is shown, that under some mild smoothness conditions, any divergence that satisfies Fact 4 is a Bregman divergence.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Recall that the smoothed complexity of the k-means algorithm is polynomial in n and 1/\u03c3, when each input point is perturbed by random noise generated using a Gaussian distribution with mean 0 and standard deviation \u03c3, a result due to Arthur, Manthey, and R\u00f6glin [12].", "startOffset": 262, "endOffset": 266}, {"referenceID": 61, "context": "For almost any Bregman divergence d\u03a6 Manthey and R\u00f6glin [62] prove two upper bounds on the smoothed complexity of the k-means algorithm.", "startOffset": 56, "endOffset": 60}, {"referenceID": 62, "context": "These bounds match bounds that Manthey and R\u00f6gin achieved for the squared Euclidean distance in [63].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "[20] already showed a close connection between Bregman divergences and exponential families, indicating that noise chosen according to an exponential distribution may be appropriate for some Bregman divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] on the smoothed complexity of the k-means algorithm can be generalized to Bregman divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In [2] the k-means++ randomized seeding algorithm by Arthur and Vassilvitskii [14] is generalized to \u03bc-similar Bregman divergences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In [2] the k-means++ randomized seeding algorithm by Arthur and Vassilvitskii [14] is generalized to \u03bc-similar Bregman divergences.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "In [3] Ackermann and Bl\u00f6mer generalize the result by Ostrovsky et a.", "startOffset": 3, "endOffset": 6}, {"referenceID": 67, "context": "[68] on adaptive sampling for \u01eb-separable instances to Bregman divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 66, "context": "[67] generalize k-means++ to certain symmetrized versions of Bregman divergences d\u03a6, called mixed Bregman divergences.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Clustering is a basic process in data analysis. It aims to partition a set of objects into groups called clusters such that, ideally, objects in the same group are similar and objects in different groups are dissimilar to each other. There are many scenarios where such a partition is useful. It may, for example, be used to structure the data to allow efficient information retrieval, to reduce the data by replacing a cluster by one or more representatives or to extract the main \u2018themes\u2019 in the data. There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47]. Notice that the title of [47] is Data clustering: 50 years beyond K-means in reference to the k-means algorithm, the probably most widely used clustering algorithm of all time. It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey. The k-means algorithm solves the problem of clustering to minimize the sum of squared errors (SSE). In this problem, we are given a set of points P \u2282 Rd in a Euclidean space, and the goal is to find a set C \u2282 Rd of k points (not necessarily included in P ) such that the sum of the squared distances of the points in P to their nearest center in C is minimized. Thus, the objective function to be minimized is", "creator": "LaTeX with hyperref package"}}}