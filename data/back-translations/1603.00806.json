{"id": "1603.00806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Hybrid Collaborative Filtering with Autoencoders", "abstract": "Collaborative filtering aims to exploit user feedback to make personalized recommendations, and such algorithms look for latent variables in a large sparse matrix of ratings, and can be improved by adding ancillary information to address the well-known cold-start problem. While neural networks have tremendous success in image and speech recognition, collaborative filtering pays less attention to them, which is all the more surprising since neural networks are able to detect latent variables in large and heterogeneous datasets. In this paper, we present a Collaborative Filtering Neural Network Architecture (CFN) that calculates a nonlinear matrix factorization from sparse rating inputs and page information. We demonstrate experimentally, using MovieLens and Douban datasets, that CFN maps the state of the art and benefits from page information. We offer a neural algorithm for reusable toral plugin.", "histories": [["v1", "Wed, 2 Mar 2016 17:48:25 GMT  (239kb,D)", "http://arxiv.org/abs/1603.00806v1", null], ["v2", "Wed, 9 Mar 2016 19:18:09 GMT  (303kb,D)", "http://arxiv.org/abs/1603.00806v2", null], ["v3", "Tue, 19 Jul 2016 08:10:08 GMT  (379kb,D)", "http://arxiv.org/abs/1603.00806v3", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.NE", "authors": ["florian strub", "jeremie mary", "romaric gaudel"], "accepted": false, "id": "1603.00806"}, "pdf": {"name": "1603.00806.pdf", "metadata": {"source": "META", "title": "Hybrid Collaborative Filtering with Neural Networks", "authors": ["Florian Strub", "Romaric Gaudel"], "emails": ["FLORIAN.STRUB@INRIA.FR", "JEREMIE.MARY@INRIA.FR", "ROMARIC.GAUDEL@INRIA.FR"], "sections": [{"heading": "1. Introduction", "text": "Recommendation systems advise users on which items (movies, musics, books etc.) they are more likely to be interested in. A good recommendation system may dramatically increase the amount of sales of a firm or retain customers. For instance, 80% of movies watched on Netflix come from the recommender system of the company (Gomez-Uribe & Hunt, 2015). One efficient way to design such algorithm is to predict how a user would rate a given item. Two key methods co-exist to tackle this issue: Content-Based Filtering and Collaborative Filtering (CF).\nPreliminary work. Under review by the under review by the International Conference on Machine Learning (ICML),\nContent-Based Filtering explicitly uses the user/item knowledge to estimate a new rating. For instance, user information can be the age, gender or graph of friends etc. Item information can be the movie genre, a short description or the tags. CF uses the ratings history of users and items. The feedback of one user on some items is combined with the feedback of all other users on all items to predict a new rating. For instance, if someone rated a few books, Collaborative Filtering aims at estimating the ratings he would have given to thousands of other books by using the ratings of all the other readers. CF is often preferred to Content-Based Filtering because it wins the agnostic vs. studied contest: CF only relies on the ratings of the users while Content-Based Filtering requires advanced engineering on items to perform well (Lops et al., 2011).\nThe most successful approach in CF is to retrieve potential latent factors from the sparse matrix of ratings. Book latent factors are likely to encapsulate the book genre (spy novel, fantasy, etc.) or some writing styles. Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent (Koren et al., 2009) or Regularized Alternative Least Square algorithm (Zhou et al., 2008). However, these methods are linear and cannot catch subtle factors. Newer algorithms were explored to face those constraints such as Non Linear Probabilistic Matrix Factorization (Lawrence & Urtasun, 2009), Factorization Machines (Rendle, 2010) or Local Low Rank Matrix Approximation (Lee et al., 2013).\nAnother limitation of CF is known as the cold start problem: how to recommend an item to a user when no rating exists for neither the user nor the item ? To overcome this issue, one idea is to build a hybrid model mixing CF and Content Based Filtering where side information is integrated into the training process. The goal is to supplant the lack of ratings through side information. A successful approach (Adams & Murray, 2010; Porteous & A. U. Asuncion, 2010) extends the Bayesian Probabilistic Matrix Factorization Framework (Salakhutdinov & Mnih, 2008) to in-\nar X\niv :1\n60 3.\n00 80\n6v 1\n[ cs\n.I R\n] 2\nM ar\n2 01\ntegrate side information. However, recent algorithms outperform them in the general case (Lee et al., 2012).\nIn this paper we introduce a CF approach based on Stacked Denoising Autoencoders (Vincent et al., 2010) which tackles both challenges: learning a non-linear representation of users and items, and alleviating the cold start problem by integrating side information. Compared to previous attempts in that direction (Salakhutdinov et al., 2007; Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015), our framework integrates the sparse matrix of ratings and side information in a unique Network. This joint model leads to a scalable and robust approach which beats state-of-the-art results in CF. Reusable source code is provided in Lua/Torch to reproduce the results. Last but not least, we show that CF approaches based on Matrix Factorization have a strong link with our approach.\nThe paper is organized as follows. First, Sec. 2 summarizes the state-of-the-art in CF and Neural Networks. Then, our model is described in Sec. 3 and 4 and its relation with Matrix Factorization is characterized in Sec. 3.2. Finally, experimental results are given and discussed in Sec. 5 and Sec. 6 discusses algorithmic aspects."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Denoising Autoencoders", "text": "The proposed approach builds upon Autoencoders which are feed-forward Neural Networks popularized by Kramer (1991). They are unsupervised Networks where the output of the Network aims at reconstructing the initial input. The Network is constrained to use narrow hidden layers, forcing a dimensionality reduction on the data. The Network is trained by back-propagating the squared error loss on the reconstruction. Such Networks are divided into two parts:\n\u2022 the encoder : f(x) = \u03c3(W1x + b1), \u2022 the decoder : g(y) = \u03c3(W2y + b2),\nwith x \u2208 RN the input, y \u2208 RK the output, K the size of the Autoencoder\u2019s bottleneck (K << N ), W1 \u2208 RN\u00d7K and W2 \u2208 RK\u00d7N the weight matrices, b1 \u2208 RK and b2 \u2208 RN the bias vectors, and \u03c3(.) a non-linear transfer function.\nRecent work in Deep Learning advocates to stack pretrained encoders to initialize Deep Neural Networks (Glorot & Bengio, 2010). This process enables the lowest layers of the Network to find low-dimensional representations. It experimentally increases the quality of the whole Network. Yet, classic Autoencoders often degenerate into identity Networks and they fail to learn the latent relationship between data. (Vincent et al., 2010) tackle this issue by corrupting inputs, pushing the Network to denoise the final outputs. One method is to add Gaussian noise on a random\nfraction \u03bd of the input. Another method is to mask a random fraction \u03bd of the input by replacing them with zero. In this case, the Denoising AutoEncoder (DAE) loss function is modified to emphasize the denoising aspect of the Network. The loss is based on two main hyperparameters \u03b1, \u03b2. They balance whether the Network would focus on denoising the input (\u03b1) or reconstructing the input (\u03b2):\nL2,\u03b1,\u03b2(x, x\u0303) = \u03b1  \u2211 j\u2208C(x\u0303) [nn(x\u0303)j \u2212 xj ]2 +\n\u03b2  \u2211 j 6\u2208C(x\u0303) [nn(x\u0303)j \u2212 xj ]2  ,\nwhere x\u0303 is the corrupted input x \u2208 RN , C are the indices of the corrupted elements of x, nn(x)j is the jth output of the Network."}, {"heading": "2.2. Matrix Factorization", "text": "One of the most successful approach of Collaborative Filtering is Matrix Factorization (Koren et al., 2009). This method retrieves latent factors from the ratings of items made by the users. The underlying idea is that key features are hidden in the ratings themselves. Given N users and M items, the rating rij is the rating given by the ith user for the jth item. It entails a sparse matrix of ratings R \u2208 RN\u00d7M . In Collaborative Filtering, sparsity is originally produced by missing values rather than zero values. The goal of Matrix Factorization is to find a K low rank matrix R\u0302 \u2208 RN\u00d7M where R\u0302 = UVT with U \u2208 RN\u00d7K and V \u2208 RM\u00d7K are two matrices of rank K encoding a dense representation of the users/items with\narg min U,V \u2211 (i,j)\u2208K(R) (rij\u2212u\u0304Ti v\u0304j)2 +\u03bb(\u2016u\u0304i\u20162Fro+\u2016v\u0304j\u20162Fro),\nwhere K(R) is the set of indices of known ratings, u\u0304i, v\u0304j are the corresponding line vectors of U ,V and \u2016u\u0304i\u2016Fro is the Frobenius norm."}, {"heading": "2.3. Related Work", "text": "Neural Networks have attracted little attention in the CF community. In a preliminary work, (Salakhutdinov et al., 2007) tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow (Truyen et al., 2009). While Deep Learning has tremendous success in image and speech recognition (LeCun et al., 2015), sparse data has received less attention and remains a challenging problem for Neural Networks.\nNevertheless, Neural Networks are able to discover nonlinear latent variables with heterogeneous data (LeCun et al., 2015) which makes them a promising tool for CF.\n(Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015) directly train Autoencoders to provide the best predicted ratings. Those methods report excellent results in the general case. However, the cold start initialization problem is ignored. For instance, AutoRec (Sedhain et al., 2015) replaces unpredictable ratings by an arbitrary selected score. In our case, we apply a training loss designed for sparse rating inputs and we integrate side information to lessen the cold start effect.\nOther contributions deal with this cold start problem by using Neural Networks properties for Content-Based Filtering: Neural Networks are first trained to learn a feature representation from the item which is then processed obtain a CF approach such as Probabilistic Matrix Factorization (Mnih & Salakhutdinov, 2007) to provide the final rating. For instance, (Glorot et al., 2011; Wang et al., 2014a) respectively auto-encode bag-of-words from restaurant reviews and movie plots, (Li et al., 2015) auto-encode heterogeneous side information from users and items. Finally, (den Oord et al., 2013; Wang et al., 2014b) use Convolutional Networks on music samples. In our case, side information and ratings are used together without any unsupervised pretreatment."}, {"heading": "2.4. Notation", "text": "In the rest of the paper, we will use the following notations:\n\u2022 ui, vj are the sparse lines/columns of R\n\u2022 u\u0303i, v\u0303j are corrupted versions of ui, vj\n\u2022 u\u0302i, v\u0302j are dense estimates of R\u0302\n\u2022 u\u0304i, v\u0304j are dense low rank representations of ui, vj."}, {"heading": "3. Autoencoders for Collaborative Filtering", "text": "User preferences are encoded as a sparse matrix of ratings R. A user is represented by a sparse line ui \u2208 RN and an item is represented by a sparse column vj \u2208 RM . The Collaborative Filtering objective can be formulated as: turn the sparse vectors ui/vj , into dense vectors u\u0302i/v\u0302j .\nWe propose to perform this conversion with Autoencoders. To do so, we need to define two types of Autoencoders:\n\u2022 U-CFN is defined as nn(ui) = u\u0302i, \u2022 V-CFN is defined as nn(vj) = v\u0302j .\nThe encoding part of these Autoencoder aims at building a low-rank dense representation of the sparse input of ratings. The decoding part aims at predicting a dense vector of ratings from the low-rank dense representation of the encoder. This new approach differs from classic Autoencoders which only aim at reconstructing/denoising the input. As we will see later, the training loss will then differ from the evaluation one."}, {"heading": "3.1. Sparse Inputs", "text": "There is no standard approach for using sparse vectors as inputs of Neural Networks. Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values (Tresp et al., 1994; Bishop, 1995). In our case, we want the Autoencoder to handle this prediction issue by itself. Such problems have already been studied in industry (Miranda et al., 2012) where 5% of the values are missing. However in Collaborative Filtering we often face datasets with more than 95% missing values. Furthermore, missing values are not known during training in Collaborative Filtering which makes the task even more difficult.\nOur approach includes three ingredients to handle the training of sparse Autoencoders:\n\u2022 inhibit the edges of the input layers by zeroing out values in the input,\n\u2022 inhibit the edges of the output layers by zeroing out back-propagated values,\n\u2022 use a denoising loss to emphasize rating prediction over rating reconstruction.\nOne way to inhibit the input edges is to turn missing values to zero. To keep the Autoencoder from always returning zero, we also use an empirical loss that disregards the loss of unknown values. No error is back-propagated for missing values. Therefore, the error is back-propagated for actual zero values while it is discarded for missing values. In other words, missing values do not bring information to the Network. This operation is equivalent to removing the neurons with missing values described in (Salakhutdinov et al., 2007; Sedhain et al., 2015). However, Our method has important computational advantages because only one Neural Networks is trained whereas other techniques has to share the weights among thousands of Networks.\nFinally, we take advantage of the masking noise from the Denoising AutoEncoders (DAE) empirical loss. By simulating missing values in the training process, Autoencoders are trained to predict them. In Collaborative Filtering, this prediction aspect is actually the final target. Thus, emphasizing the prediction criterion turns the classic unsupervised training of Autoencoders into a simulated supervised learning. By mixing both the reconstruction and prediction criteria, the training can be thought as a pseudo-semisupervised learning. This makes the DAE loss a promising objective function. After regularization, the final training loss is:\nL2,\u03b1,\u03b2(x, x\u0303) = \u03b1  \u2211 j\u2208C(x\u0303)\u2229K(x) [nn(x\u0303)j \u2212 xj ]2 +\n\u03b2  \u2211 j 6\u2208C(x\u0303)\u2229K(x) [nn(x\u0303)j \u2212 xj ]2 + \u03bb|W|2Fro,\nwhere K(x) are the indices of known values of x, W is the flatten vector of weights and \u03bb is the regularization hyperparameter. The full forward/backward process is explained in Figure 1. Importantly, Autoencoders with sparse inputs differs from sparse-Autoencoders (Lee et al., 2006) or Dropout regularization (Srivastava et al., 2014) in the sense that Sparse Autoencoders and Droupout inhibit the hidden neurons while inputs/outputs are actually known."}, {"heading": "3.2. Autoencoder and Low Rank Matrix Factorization", "text": "Autoencoders are actually strongly linked with Matrix Factorization. For an Autoencoder with only one hidden layer\nand no output transfer function, the response of the network can be written as\nnn(x) = W2 \u03c3(W1x + b1) + b2,\nwhere W1,W2 are the weights matrices and b1,b2 the bias terms. Up to the bias term b2, if we replace x by the representation ui of the user i, we recover a predicted vector u\u0302i of the form Vu\u0304i:\nu\u0302i = nn(ui) = W2\ufe38\ufe37\ufe37\ufe38 V \u03c3(W1ui + b1)\ufe38 \ufe37\ufe37 \ufe38 u\u0304i +b2.\nSymmetrically, v\u0302j has the form Uv\u0304j:\nv\u0302j = nn(vj) = W \u2032 2\ufe38\ufe37\ufe37\ufe38\nU\n\u03c3(W\u20321vj + b \u2032 1)\ufe38 \ufe37\ufe37 \ufe38\nv\u0304j\n+b\u20322.\nFor the Matrix Factorization by ALS ; R\u0302 is iteratively built by solving for each row i of U (resp. column j of VT) a penalized least square regression using the known values of the ith row of R (resp. jth column of R) as observations of a scalar product in dimension k of u\u0304i and a column of VT (resp v\u0304i and a row of U). An Autoencoder replaces the regularization introduced by the alternate scheme by a projection in dimension k composed with the non linearity \u03c3. This process corresponds to a non linear matrix factorization.\nNote that CFN breaks the symmetry between U and V. For example, while Matrix Factorization approaches learn both U and V, U-CFN learns V and only indirectly learns U: U-CFN targets the function to build u\u0304i whatever the row ui. A direct benefit is that the learned Autoencoder is able to fill in every vector ui, even if that vector was not in the training data.\nFinally, both non-linear decompositions on rows and columns are done independently, which means that the matrix V learned by U-CFN from rows can differ from the concatenation of vectors v\u0304j predicted by V-CFN from columns."}, {"heading": "4. Integrating side information", "text": "Collaborative Filtering only relies on the feedback of users regarding a set of items. When additional information is available for the users and the items, this can sound restrictive. One would think that adding more information can help in several ways: increasing the prediction accuracy, speeding up the training, increasing the robustness of the model, etc. Furthermore, pure Collaborative Filtering suffers from the cold start problem: when very little information is available on an item, Collaborative Filtering will have difficulties recommending it. When bad recommendation are provided, the probability to receive valuable feedback is lowered leading to a vicious circle for new items.\nA common way to tackle this problem is to add some side information to ensure a better initialization of the system. This is known in the recommendation community as hybridization.\nThe simplest approach to integrate side information is to append additional user/item bias to the rating prediction (Koren et al., 2009):\nr\u0302ij = u\u0304 T i v\u0304j + bu,i + bu,j + b \u2032,\nwhere bu,i, bu,j , b\u2032 are respectively the user, item, and global bias of the Matrix Factorization. Computing these bias can be done through hand-crafted engineering or Collaborative Filtering technique. For instance, one method is to extend the dense feature vectors by directly appending side information on them (Porteous & A. U. Asuncion, 2010). Therefore, the estimated rating is computed by:\nr\u0302ij = {u\u0304i,xi} \u2297 {v\u0304j ,yj} def = u\u0304T[1:k],iv\u0304[1:k],j + u\u0304\nT [k+1:k+p],iyj\ufe38 \ufe37\ufe37 \ufe38\nbv,j\n+ v\u0304T[k+1:k+p],jxi\ufe38 \ufe37\ufe37 \ufe38 bu,i ,\nwhere xi \u2208 RP and yj \u2208 RQ are respectively a vector representation of side information for the user and the item. If we decompose the previous scalar product, u\u0304T[1:k],iv\u0304[1:k],j is the classic Matrix Factorization term, u\u0304T[k+1:k+p],iyj is the linear regression of the dense user representation and the item side information, v\u0304T[k+1:k+p],jxj is the linear regression of the dense item representation and the user side information. For users, they can encode the genre, the age, job, circle of friends and so on. For items, they can be tags, a bag of words etc.\nUnfortunately, those methods cannot be directly applied to Neural Networks because Autoencoders optimize U and V independently. New strategies must be designed to incorporate side information. One notable example for bitext word alignment was recently made by (Ammar et al., 2014).\nIn our case, the first idea would be to append the side information to the sparse input vector. For simplicity purpose, the next equations will only focus on shallow U-Autoencoders with no output transfer functions. Yet, this can be extended to more complex Networks and VAutoencoders. Therefore, we would get:\nnn({ui,xi}) = V \u03c3(W\u20321{ui,xi}+ b1) + b2 = u\u0302i,\nwhere W\u20321 \u2208 Rk\u00d7(n+p) is a weight matrix . When no previous rating exist, it enables the Neural Networks to have at an input to predict new ratings. With this scenario, side information is assimilated to pseudo-ratings that will always\nexist for every items. However, when the dimension of the Neural Network input is far greater than the dimension of the side information, the Autoencoder may have difficulties to use it efficiently.\nYet, common Matrix Factorization would append side information to dense feature representations {u\u0304i,xi} rather than sparse feature representation as we just proposed {ui,xi}. Thus, one way to reproduce this idea is to inject the side information to every layer inputs of the Network.\nnn({ui,xi}) = V\u2032 { u\u0304\u2032\ufe37 \ufe38\ufe38 \ufe37 \u03c3(W\u20321{ui,xi}+ b1),xi}+ b2\n= V\u2032 {u\u0304\u2032i,xi}+ b2 = V\u2032[1:k]u\u0304 \u2032 i + V\n\u2032 [k+1:k+p]xi\ufe38 \ufe37\ufe37 \ufe38\nbu,i\n+b2,\nwhere V\u2032 \u2208 R(n\u00d7k+p) is a weight matrix, V\u2032[1:k] \u2208 Rn\u00d7k,V\u2032[k+1:k+p] \u2208 R\nn\u00d7p are respectively the submatrices of V\u2032 that contain the columns from 1 to k and k+ 1 to k + p.\nBy injecting the side information in every layer, the dynamic Autoencoders representation is forced to integrate this new data. In addition, the output layer computes a linear regression of the side information. However, we do not want the side information to overstep the dense rating representation. Thus, we enforced the following constraint. The dimension of the sparse input must be greater than the dimension of the Autoencoder bottleneck which must be greater than the dimension of the side information 1:"}, {"heading": "P << Ku << N and Q << Kv << M.", "text": "We finally obtain an Autoencoder which can incorporate side information and be trained through backpropagation.\n1When side information is sparse, the dimension of the side information can be assimilated to the number of non-zero parameters"}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Benchmark Models", "text": "We benchmark CFN with two SVD techniques that are broadly used in the industry.\nAlternating Least Squares with Weighted-\u03bb-Regularization (ALS-WR) (Zhou et al., 2008) solves the low-rank matrix factorization problem by alternatively fixing U and V and solving the resulting linear problem. Tikhonov regularization is used as it empirically provides excellent results:\nL2 = \u2211\n(i,j)\u2208K(R)\n(rij\u2212uTi vj)2+\u03bb(ni\u2016ui\u20162Fro+nj\u2016vj\u20162Fro),\nwhere ni is the number of rating for the ith user and nj is the number of rating for the jth item. Experiments are run with the Apache Mahout Software. 2\nSVDFeature (Chen et al., 2012) is a Machine Learning Toolkit for feature-based Collaborative Filtering. He won the KDD Cup for two consecutive years. Ratings are given by the following equation:\nr\u0302 = N+P\u2211 p xpb (u) p + M+Q\u2211 q yqb (v) q + \u2016R\u2016\u2211 r zrb (g) r + ( N+P\u2211 p xpup )T (M+Q\u2211 q yqvq )\nwhere b(u) \u2208 RN+P , b(i) \u2208 RM+Q, b(g) \u2208 R\u2016R\u2016 are the the side information bias, and U \u2208 RN+P\u00d7K , V \u2208 RM+Q\u00d7K encode the latent factors. The model parameters are computed by gradient descent."}, {"heading": "5.2. Data", "text": "Experiments are conducted on MovieLens and Douban datasets.\nThe MovieLens-1M, MovieLens-10M and MovieLens20M datasets respectively provide 1/10/20 millions discrete ratings from 6/72/138 thousands users on 4/10/27 thousands movies. Side information for MovieLens-1M is the age, sex and gender of the user and the movie category (action, thriller etc.). Side information for MovieLens-10/20M is a matrix of tags T where Tij is the occurrence of the jth tag for the ith movie and the movie category. No side information is provided for users.\nThe Douban dataset (Ma et al., 2011) provides 17 million discrete ratings from 129 thousands users on 58 thousands movies. Side information is the bi-directional user/friend relations for the user. The user/friend relation are treated\n2http://mahout.apache.org/\nlike the matrix of tags from MovieLens. No side information is provided for items.\nPreprocessing For each of these datasets, the full dataset is considered and the ratings are normalized from -1 to 1. We split them into random 80%-20% train-test datasets and inputs are unbiased before the training process: denoting \u00b5 the mean over the training set, bui the mean of the i\nth user and bvi the mean of the v\nth item, SVD algorithms, U-CFN and V-CFN respectively learn from runbiasedij = rij + \u00b5\u2212 bui \u2212 bvj , runbiasedij = rij \u2212 bui and runbiasedij = rij \u2212 bvi .\nPostprocessing The bias computed on the training set is added back while evaluating the final RMSE.\nSide Information In order to enforce the side information constraint, Q Kv M , Principal Component Analysis is performed on the matrix of tags. We keep the 50 greatest eigenvectors3 and normalize them by the square root of their respective eigenvalue: given T = PDQT with D the diagonal matrix of eigenvalues sorted in descending order, the movie tags are represented by Y = PJ\u00d7K\u2032D 0.5 K\u2032\u00d7K\u2032 with K\n\u2032 the number of kept eigenvectors. Binary representation such as the movie category is then concatenated to Y."}, {"heading": "5.3. Error Function", "text": "Two Mean Square Errors (MSE) co-exist for Autoencoders and one must be careful to use the right estimator for benchmarking. The classic Autoencoder loss estimates the quality of the input reconstruction:\nMSErec(Xtr) = 1 |Rtr| \u2211\nx\u2208Xtr \u2211 k\u2208K(x) [nn(x)k \u2212 xk]2,\nwhere |Rtr| is the number of ratings in the training dataset.\nIn Collaborative Filtering context, while this loss provides useful information during the training phase, the quality of the learned Autoencoder is given by its prediction accuracy for unknown inputs. This prediction accuracy is summed up couples (xte,xtr) of training-testing examples:\nMSEpred(Xte,Xtr) =\n1 |Rte| \u2211\n(xte,xtr)\u2208(Xte,Xtr) \u2211 k\u2208K(xte) [nn(xtr)k \u2212 xte,k]2.\nWe use MSEpred to evaluate both the baselines and CFN.\n3The number of eigenvalues is arbitrary selected. We do not focus on optimizing the quality of this representation."}, {"heading": "5.4. Training Settings", "text": "We train 4-layers Autoencoders for MovieLens-1M and 2-layers Autoencoders for MovieLens-10/20M and the Douban datasets. The first and second layers have from 500 to 700 hidden neurons. The decoding layers have the same dimension in the reverse order. Weights are initialized using the fan-in rule Wij \u223c U [ \u2212 1\u221a\nn ,\u2212 1\u221a n\n] (LeCun et al.,\n1998). Transfer functions are hyperbolic tangents. The Neural Network is optimized with stochastic backpropagation with minibatch of size 30 and a weight decay is added for regularization. Hyperparameters 4 are tuned by a simple genetic algorithm already used by (Teytaud et al., 2007) in a different context."}, {"heading": "5.5. Results", "text": "Table 1 summarizes the RMSE on MovieLens and Douban datasets. Reported results are computed through k-fold cross-validation and confidence intervals correspond to a 95% range. To the best of our knowledge, the best results published regarding MovieLens-1M and MovieLens-10M are reported by both (Lee et al., 2013; Sedhain et al., 2015) with a final RMSE of 0.831 \u00b1 0.003 and 0.782 \u00b1 0.003. These scores are obtained with a training ratio of 90%/10% and without side information. (Kim & Choi, 2014) use a scalable version of BPMF with side information and they report respectively 0.844 and 0.6856 RMSE score on MovieLens-10M and Douban with a training ratio of 80%/20% while V-CFN++ returns 0.782 and 0.694 with the same experimental conditions."}, {"heading": "5.6. Discussion", "text": "V-CFNs have excellent performance in our experiments for every dataset we run. It is competitive compared to the state-of-the-art Collaborative Filtering algorithms and clearly outperforms them for MovieLens-10M. For instance, recent works still reports a global RMSE above 0.8 even using some side information with MovieLens-10M\n4Hyperparameters used for the experiments are provided with the source code.\n(Kim & Choi, 2014; Kumar et al., 2014). The confidence interval of CFN is also low on large datasets which make him a robust algorithm for Collaborative Filtering.\nIn the experiments, V-CFN outperforms U-CFN. It suggests that the structure on the items is stronger than the one on users i.e. it is easier to guess tastes based on movies you liked than to find some users similar to you. Of course, the behavior could be different on some other data .\nAt first sight, the use of side information has a limited impact on the RMSE. This statement has to be mitigated: as the repartition of known entries in the dataset is not uniform, the estimates are biased towards users and items with a lot of ratings. For theses users and movies, the dataset already contains a lot of information, thus having some extra information will have a marginal effect. Users and items with few ratings should benefit more from some side information but the estimation biais hides them.\nIn order to exhibit the utility of side information, we report in Table 3 the RMSE conditionally to the number of missing values for items. As expected, the fewer number of ratings for an item, the more important the side information. A more careful analysis of the RMSE improvement in this setting shows that the improvement is uniformly distributed over the users whatever their number of ratings. This corresponds to the fact that the available side information is only about items. This is very desirable for a real system: the ef-\nfective use of side information to the new items is crucial to deal with the flow of new products.\nIn the end, we trained V-CFN on MovieLens-10M with either the movie genre or the matrix of tags. Both side information increase the global RMSE by 0.1% and concatenating them increase the final score by a small margin of 0.15%. Therefore, V-CFN could handle the heterogeneity of side information. However, the U-CFN failed to use the friendship relationship to increase the RMSE."}, {"heading": "6. Remarks", "text": ""}, {"heading": "6.1. Source code", "text": "Torch is a powerful framework written in Lua to quickly prototype Neural Networks. It is a widely used (Facebook, Deep Mind) industry standard. However, Torch lacks some important basic tools to deal with sparse inputs. Thus, we develop several new modules to deal with DAE loss, sparse DAE loss and sparse inputs on both CPU and GPU. They can easily be plugged into existing code. An out-of-the-box tutorial is available to directly run the experiments. The code is freely available on Github and Luarocks. 5"}, {"heading": "6.2. Scalability", "text": "One major problem that most Collaborative Filtering have to resolve is scalability since dataset often have hundred of thousands users and items. An efficient algorithm must be trained in a reasonable amount of time and provide quick feedback during evaluation time.\nRecent advances in GPU computation managed to reduce the training time of Neural Networks by several orders of magnitude. However, Collaborative Filtering deals with sparse data and GPUs are designed to perform well on dense data. (Salakhutdinov et al., 2007; Sedhain et al., 2015) face this sparsity constraint by building small dense Networks with shared weights. Yet, this approach may lead to important synchronisation latencies. In our case, we tackle the issue by selectively densifying the inputs just before sending them to the GPUs cores without modification of the result of the computation. It introduces an overhead on the computational complexity but this implementation\n5https://github.com/*******\nallows the GPUs to work at their full strength. In practice, vectorial operations overtake the extra cost. Such approach is an efficient strategy to handle sparse data which achieves a balance between memory footprint and computational time. We are able to train Large Neural Networks within a few minutes as shown in Table 4. At the time of writing, alternative strategies to train networks with sparse inputs on GPUs are under development."}, {"heading": "6.3. Future Works", "text": "Implicit feedback may greatly enhance the quality of Collaborative Filtering algorithms (Koren et al., 2009; Rendle, 2010). For instance, Implicit feedback would be incorporated to CFN by feeding the Network with an additional binary input. By doing so, (Salakhutdinov et al., 2007) enhance the quality of prediction for Restricted Boltzmann Machine on the Netflix Dataset. Additionally, Content Based Technique with Deep learning such as (den Oord et al., 2013; Wang et al., 2014b) would be plugged to CFN. The idea would be to train a joint Network that would directly link the raw item features to the ratings such as music, pictures or word representations. As a different topic, V-CFN and U-CFN does not always report the same type of errors. This is more likely to happen when they are fed with side information. One interesting work would be to combine a suitable Network that mix them both. Finally, other metrics exist to estimate the quality of Collaborative Filtering to fit other real-world constraints. Normalized Discounted Cumulative Gain (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2002) or F-score are often sometimes preferred to RMSE/MAE and should be benchmarked."}, {"heading": "7. Conclusion", "text": "In this paper, we have introduced a Neural Network architecture, aka CFN, to perform Collaborative Filtering with side information. Contrary to other attempts with Neural Networks, this joint Network integrate side information and learn a non-linear representation of users or items into a unique Neural Network. This approach manages to both beats state of the art results in CF and ease the cold start problem on the MovieLens and Douban datasets. CFN is also scalable and robust to deal with large size dataset. We made several claims that Autoencoders are closely linked\nto low-rank Matrix Factorization in Collaborative Filtering. Finally, a reusable source code is provided in Torch and hyperparameters are provided to reproduce the results."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors would like to acknowledge the stimulating environment provided by SequeL research group, Inria and CRIStAL. This work was supported by French Ministry of Higher Education and Research, by CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, the Projet CHIST-ERA IGLU and by FUI Herme\u0300s. Experiments were carried out using Grid\u20195000 tested, supported by Inria, CNRS, RENATER and several universities as well as other organizations."}], "references": [{"title": "Incorporating side information in probabilistic matrix factorization with gaussian processes", "author": ["R.P. Adams", "Murray", "G.E. Dahland I"], "venue": "arXiv preprint arXiv:1003.4944,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["W. Ammar", "C. Dyer", "N.A. Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press,", "citeRegEx": "Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Svdfeature: a toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Deep content-based music recommendation", "author": ["den Oord", "A. Van", "S. Dieleman", "B. Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2013}, {"title": "Neural network matrix factorization", "author": ["G.K. Dziugaite", "D.M. Roy"], "venue": "arXiv preprint arXiv:1511.06443,", "citeRegEx": "Dziugaite and Roy,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite and Roy", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The netflix recommender system: Algorithms, business value, and innovation", "author": ["C. Gomez-Uribe", "N. Hunt"], "venue": "ACM Trans. Manage. Inf. Syst.,", "citeRegEx": "Gomez.Uribe and Hunt,? \\Q2015\\E", "shortCiteRegEx": "Gomez.Uribe and Hunt", "year": 2015}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "K. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen", "year": 2002}, {"title": "Scalable variational bayesian matrix factorization with side information", "author": ["Kim", "Yong-Deok", "Choi", "Seungjin"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), Reykjavik,", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Nonlinear principal component analysis using autoassociative neural networks", "author": ["M.A. Kramer"], "venue": "AIChE journal,", "citeRegEx": "Kramer,? \\Q1991\\E", "shortCiteRegEx": "Kramer", "year": 1991}, {"title": "Social popularity based svd++ recommender system", "author": ["R. Kumar", "B.K. Verma", "S.S. Rastogi"], "venue": "International Journal of Computer Applications,", "citeRegEx": "Kumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2014}, {"title": "Non-linear matrix factorization with gaussian processes", "author": ["N.D. Lawrence", "R. Urtasun"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lawrence and Urtasun,? \\Q2009\\E", "shortCiteRegEx": "Lawrence and Urtasun", "year": 2009}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.R. Muller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "A comparative study of collaborative filtering algorithms", "author": ["J. Lee", "M. Sun", "G. Lebanon"], "venue": "arXiv preprint arXiv:1205.3193,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Local lowrank matrix approximation", "author": ["J. Lee", "S. Kim", "G. Lebanon", "Y. Singerm"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Deep collaborative filtering via marginalized denoising auto-encoder", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["P. Lops", "Gemmis", "M. De", "G. Semeraro"], "venue": "In Recommender systems handbook,", "citeRegEx": "Lops et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lops et al\\.", "year": 2011}, {"title": "Recommender systems with social regularization", "author": ["H. Ma", "D. Zhou", "C. Liu", "M.R. Lyu", "I. King"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "Ma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2011}, {"title": "Reconstructing Missing Data in State Estimation With Autoencoders", "author": ["V. Miranda", "J. Krstulovic", "H. Keko", "C. Moreira", "J. Pereira"], "venue": "IEEE Transactions on Power Systems,", "citeRegEx": "Miranda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miranda et al\\.", "year": 2012}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih and Salakhutdinov,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Salakhutdinov", "year": 2007}, {"title": "Bayesian matrix factorization with side information and dirichlet process mixtures", "author": ["I. Porteous", "A.U. Asuncion", "M. Welling"], "venue": "In AAAI,", "citeRegEx": "Porteous et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Porteous et al\\.", "year": 2010}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Rendle,? \\Q2010\\E", "shortCiteRegEx": "Rendle", "year": 2010}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov and Mnih,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Mnih", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Autorec: Autoencoders meet collaborative filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "L. Xie"], "venue": "In Proceedings of the 24th International Conference on World Wide Web Companion,", "citeRegEx": "Sedhain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sedhain et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N Srivastava", "G. Hinton", "A. Krizhevsk", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs", "author": ["F. Strub", "J. Mary"], "venue": "In NIPS Workshop on Machine Learning for eCommerce,", "citeRegEx": "Strub and Mary,? \\Q2015\\E", "shortCiteRegEx": "Strub and Mary", "year": 2015}, {"title": "Active learning in regression, with application to stochastic dynamic programming", "author": ["O. Teytaud", "J. Mary"], "venue": "In International Conference On Informatics in Control, Automation and Robotics (eds.), ICINCO and CAP,", "citeRegEx": "Teytaud et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teytaud et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 20, "context": "studied contest: CF only relies on the ratings of the users while Content-Based Filtering requires advanced engineering on items to perform well (Lops et al., 2011).", "startOffset": 145, "endOffset": 164}, {"referenceID": 11, "context": "Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent (Koren et al., 2009) or Regularized Alternative Least Square algorithm (Zhou et al.", "startOffset": 131, "endOffset": 151}, {"referenceID": 25, "context": "Newer algorithms were explored to face those constraints such as Non Linear Probabilistic Matrix Factorization (Lawrence & Urtasun, 2009), Factorization Machines (Rendle, 2010) or Local Low Rank Matrix Approximation (Lee et al.", "startOffset": 162, "endOffset": 176}, {"referenceID": 18, "context": "Newer algorithms were explored to face those constraints such as Non Linear Probabilistic Matrix Factorization (Lawrence & Urtasun, 2009), Factorization Machines (Rendle, 2010) or Local Low Rank Matrix Approximation (Lee et al., 2013).", "startOffset": 216, "endOffset": 234}, {"referenceID": 17, "context": "However, recent algorithms outperform them in the general case (Lee et al., 2012).", "startOffset": 63, "endOffset": 81}, {"referenceID": 27, "context": "Compared to previous attempts in that direction (Salakhutdinov et al., 2007; Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015), our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 141}, {"referenceID": 28, "context": "Compared to previous attempts in that direction (Salakhutdinov et al., 2007; Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015), our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 141}, {"referenceID": 12, "context": "The proposed approach builds upon Autoencoders which are feed-forward Neural Networks popularized by Kramer (1991). They are unsupervised Networks where the output of the Network aims at reconstructing the initial input.", "startOffset": 101, "endOffset": 115}, {"referenceID": 11, "context": "One of the most successful approach of Collaborative Filtering is Matrix Factorization (Koren et al., 2009).", "startOffset": 87, "endOffset": 107}, {"referenceID": 27, "context": "In a preliminary work, (Salakhutdinov et al., 2007) tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow (Truyen et al.", "startOffset": 23, "endOffset": 51}, {"referenceID": 28, "context": "(Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015) directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 65}, {"referenceID": 28, "context": "For instance, AutoRec (Sedhain et al., 2015) replaces unpredictable ratings by an arbitrary selected score.", "startOffset": 22, "endOffset": 44}, {"referenceID": 7, "context": "For instance, (Glorot et al., 2011; Wang et al., 2014a) respectively auto-encode bag-of-words from restaurant reviews and movie plots, (Li et al.", "startOffset": 14, "endOffset": 55}, {"referenceID": 19, "context": ", 2014a) respectively auto-encode bag-of-words from restaurant reviews and movie plots, (Li et al., 2015) auto-encode heterogeneous side information from users and items.", "startOffset": 88, "endOffset": 105}, {"referenceID": 2, "context": "Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values (Tresp et al., 1994; Bishop, 1995).", "startOffset": 108, "endOffset": 142}, {"referenceID": 22, "context": "Such problems have already been studied in industry (Miranda et al., 2012) where 5% of the values are missing.", "startOffset": 52, "endOffset": 74}, {"referenceID": 27, "context": "This operation is equivalent to removing the neurons with missing values described in (Salakhutdinov et al., 2007; Sedhain et al., 2015).", "startOffset": 86, "endOffset": 136}, {"referenceID": 28, "context": "This operation is equivalent to removing the neurons with missing values described in (Salakhutdinov et al., 2007; Sedhain et al., 2015).", "startOffset": 86, "endOffset": 136}, {"referenceID": 16, "context": "Importantly, Autoencoders with sparse inputs differs from sparse-Autoencoders (Lee et al., 2006) or Dropout regularization (Srivastava et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 11, "context": "The simplest approach to integrate side information is to append additional user/item bias to the rating prediction (Koren et al., 2009):", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": "One notable example for bitext word alignment was recently made by (Ammar et al., 2014).", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": "SVDFeature (Chen et al., 2012) is a Machine Learning Toolkit for feature-based Collaborative Filtering.", "startOffset": 11, "endOffset": 30}, {"referenceID": 21, "context": "The Douban dataset (Ma et al., 2011) provides 17 million discrete ratings from 129 thousands users on 58 thousands movies.", "startOffset": 19, "endOffset": 36}, {"referenceID": 15, "context": "Weights are initialized using the fan-in rule Wij \u223c U [ \u2212 1 \u221a n ,\u2212 1 \u221a n ] (LeCun et al., 1998).", "startOffset": 75, "endOffset": 95}, {"referenceID": 31, "context": "Hyperparameters 4 are tuned by a simple genetic algorithm already used by (Teytaud et al., 2007) in a different context.", "startOffset": 74, "endOffset": 96}, {"referenceID": 18, "context": "To the best of our knowledge, the best results published regarding MovieLens-1M and MovieLens-10M are reported by both (Lee et al., 2013; Sedhain et al., 2015) with a final RMSE of 0.", "startOffset": 119, "endOffset": 159}, {"referenceID": 28, "context": "To the best of our knowledge, the best results published regarding MovieLens-1M and MovieLens-10M are reported by both (Lee et al., 2013; Sedhain et al., 2015) with a final RMSE of 0.", "startOffset": 119, "endOffset": 159}, {"referenceID": 13, "context": "(Kim & Choi, 2014; Kumar et al., 2014).", "startOffset": 0, "endOffset": 38}, {"referenceID": 27, "context": "(Salakhutdinov et al., 2007; Sedhain et al., 2015) face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 50}, {"referenceID": 28, "context": "(Salakhutdinov et al., 2007; Sedhain et al., 2015) face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 50}, {"referenceID": 11, "context": "Implicit feedback may greatly enhance the quality of Collaborative Filtering algorithms (Koren et al., 2009; Rendle, 2010).", "startOffset": 88, "endOffset": 122}, {"referenceID": 25, "context": "Implicit feedback may greatly enhance the quality of Collaborative Filtering algorithms (Koren et al., 2009; Rendle, 2010).", "startOffset": 88, "endOffset": 122}, {"referenceID": 27, "context": "By doing so, (Salakhutdinov et al., 2007) enhance the quality of prediction for Restricted Boltzmann Machine on the Netflix Dataset.", "startOffset": 13, "endOffset": 41}], "year": 2017, "abstractText": "Collaborative Filtering aims at exploiting the feedback of users to provide personalised recommendations. Such algorithms look for latent variables in a large sparse matrix of ratings. They can be enhanced by adding side information to tackle the well-known cold start problem. While Neural Networks have tremendous success in image and speech recognition, they have received less attention in Collaborative Filtering. This is all the more surprising that Neural Networks are able to discover latent variables in large and heterogeneous datasets. In this paper, we introduce a Collaborative Filtering Neural network architecture aka CFN which computes a non-linear Matrix Factorization from sparse rating inputs and side information. We show experimentally on the MovieLens and Douban dataset that CFN outperforms the state of the art and benefits from side information. We provide an implementation of the algorithm as a reusable plugin for Torch, a popular Neural Network framework.", "creator": "LaTeX with hyperref package"}}}