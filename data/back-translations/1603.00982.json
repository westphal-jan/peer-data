{"id": "1603.00982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder", "abstract": "The model we use is called Sequence-to-Sequence Autoencoder (SA) and consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps the input sequence into a fixed-dimensionality vector representation, and the second RNN acts as a decoder that reverts the representation back to the input sequence, and the two RNN are then trained together to minimize reconstruction error. Furthermore, we introduce Denoising Sequence-to-Sequence Autoencoder (DSA), which shows that the learned DNs are displayed better than the learned DW models.", "histories": [["v1", "Thu, 3 Mar 2016 05:44:51 GMT  (205kb,D)", "http://arxiv.org/abs/1603.00982v1", null], ["v2", "Tue, 15 Mar 2016 14:16:28 GMT  (196kb,D)", "http://arxiv.org/abs/1603.00982v2", null], ["v3", "Thu, 17 Mar 2016 06:11:47 GMT  (196kb,D)", "http://arxiv.org/abs/1603.00982v3", null], ["v4", "Sat, 11 Jun 2016 03:40:23 GMT  (201kb,D)", "http://arxiv.org/abs/1603.00982v4", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["yu-an chung", "chao-chung wu", "chia-hao shen", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1603.00982"}, "pdf": {"name": "1603.00982.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Recurrent Neural Networks", "authors": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-Yi Lee"], "emails": ["hungyilee}@ntu.edu.tw"], "sections": [{"heading": null, "text": "length acoustic feature sequences as fixed-length feature vectors is usually needed in many speech applications, including speaker identification, audio emotion classification and spoken term detection (STD). In this paper, we apply and extend sequence-to-sequence learning framework to learn representations for audio segments without any supervision. The model we used is called Sequence-to-sequence Autoencoder (SA), which consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps the input sequence into a vector representation of fixed dimensionality, and the second RNN acts as a decoder that maps the representation back to the input sequence. The two RNNs are then jointly trained by minimizing the reconstruction error. We further propose Denoising Sequence-to-sequence Autoencoder (DSA) that improves the learned representations. The vector representations learned by SA and DSA are shown to be very helpful for query-by-example STD. The experimental results have shown that the proposed models achieved better retrieval performance than using audio segment representation designed heuristically and the classical Dynamic Time Warping (DTW) approach."}, {"heading": "1. Introduction", "text": "Representing variable-length audio segments as fixed-length feature vectors is important for many speech applications. In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries. In query-by-example STD, by representing each word segment as a feature vector, indexing the segments can be easier, which makes retrieval much more efficient [6].\nAudio segment representation is still an open problem. It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5]. Deep learning is also used for extracting representations [7, 8]. By learning Recurrent Neural Network (RNN) with an audio segment as input and the word corresponding to the segment as target, the outputs of the hidden layer at the last few time steps can be used as the representation of the input segment [8]. However, this approach needs a large amount of audio segments with word labeling as training data.\nOn the other hand, Autoencoder has been one of the most prominent machine learning techniques for extracting representations in an unsupervised way [9, 10], but its input should be vectors of fixed dimensionality. Such limitation is dreadful be-\ncause sentences, speech and video are intrinsically expressed in arbitrary length sequences. Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation. This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.\nIn this paper, we propose to use Sequence-to-sequence Autoencoder (SA) to represent variable-length audio segments by fixed-length feature vectors. The audio segments that sound alike would have feature vectors nearby in the space. Different from the previous work [7, 8], learning SA does not need any supervision. That is, only the audio segments without any labeling are needed. This makes it very suitable for the applications in the low resource scenario. Inspired from denoising Autoencoder [15, 16], we further propose Denoising Sequenceto-sequence Autoencoder (DSA) that improves the learned representations. The learned representations can be used in a wide variety of speech applications. In this preliminary study, we show that the audio segment representations from SA can be very helpful in query-by-example STD. Here the audio database is first segmented by the word boundaries, and then each segment is transformed into a vector by the SA. When a spoken query is entered, it is also transformed into a vector by the same SA, and then the segments in the database are ranked according to the similarities with the spoken query based on their vectors. Query-by-example STD using audio segment representation is much more efficient than the Dynamic Time Warping (DTW) based approaches because instead of computing the similarities between sequences by DTW, only the similarities between single vectors are needed. The retrieval performance using the learned representations outperformed using representations designed heuristically and the DTW approach."}, {"heading": "2. Proposed Approach", "text": "In this section, we describe the approach for learning audio segment representations without any supervision. An audio segment is usually expressed with a variable-length input x = (x1, x2, ..., xT ), where each symbol xt is an acoustic feature such as MFCC. The goal here is to automatically encode any audio segment sequence x with different T into a fixedlength vector representation z \u2208 Rd, where d is the dimension of the encoded space. The learned representation z can be used in a wide variety of applications, for example, query-byexample STD as described in Section 3. Below we first give a brief recap on the RNN Encoder-Decoder framework [17, 18], which is widely used to realize the sequence-to-sequence learning in Section 2.1. In Section 2.2, formal introduction to the\nar X\niv :1\n60 3.\n00 98\n2v 1\n[ cs\n.S D\n] 3\nM ar\n2 01\n6\nSequence-to-sequence Autoencoder (SA) is presented. Finally, the denoising criterion that could further make the representations learned more robust is presented in Section 2.3."}, {"heading": "2.1. RNN Encoder-Decoder framework", "text": "RNN are neural networks whose hidden neurons form a directed cycle. Given a sequence x = (x1, x2, ..., xT ), RNN updates its hidden state ht according to the current input xt and the hidden state at the last time step ht\u22121. The hidden state ht acts as an internal memory at time step t that enables the network to capture dynamic temporal information and also allows the network to process variable-length of sequence. In practice, RNN does not seem to learn long-term dependencies [19], so LSTM [20] is designed to conquer such difficulty. Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].\nRNN Encoder-Decoder [18, 17] consists of an Encoder RNN and a Decoder RNN. The Encoder RNN reads the input sequence x = (x1, x2, ..., xM ) sequentially and the hidden state ht of the RNN is updated accordingly. After the last symbol xM is processed, the hidden state hM is interpreted as the learned representation of the whole input sequence. Then, by taking hM as input, the Decoder RNN generates the output sequence y = (y1, y2, ..., xN ) sequentially, where M and N can be different. Such RNN Encoder-Decoder framework is able to handle variable-length of input. Although there may exists a considerable time lag between the input symbols and their corresponding output symbols, LSTM units in RNN are able to handle such case well due to their powerfulness on modeling long-term dependencies.\n2.2. Sequence-to-sequence Autoencoder (SA)\nIn this section, we describe the Sequence-to-sequence Autoencoder (SA) which combines the RNN Encoder-Decoder framework with Autoencoder for unsupervised learning of audio segment representations. Figure 1 depicts its structure. SA consists of an Encoder RNN (the left part of Figure 1) and a Decoder RNN (the right part). Given an audio segment represented as an acoustic feature sequence x = (x1, x2, ..., xT ) with any length T , the Encoder RNN reads each acoustic feature xt sequentially and the hidden state ht is updated accordingly. After\nthe last acoustic feature xT has been read and processed, the hidden state hT of the Encoder RNN is viewed as the learned representation z of the input sequence (the small black block in Figure 1). The Decoder RNN takes hT as the first input and generates its first output y1. Then it takes y1 as input to generate y2, and so on. Following the philosophy of Autoencoder [9, 10], the target of the output sequence y = (y1, y2, ..., yT ) is the input sequence x = (x1, x2, ..., xT ). In other words, the RNN Encoder and Decoder are jointly trained by minimizing the reconstruction error, measured by the general mean squared error\u2211T\nt=1 \u2016xt \u2212 yt\u2016 2. Because the input sequence is taken as the learning target, the training process does not need any labelled data. The fixed-length vector representation z will be meaningful representation for the input audio segment x because if the whole input sequence x can be reconstructed from z by RNN Decoder, z must contain the information of the whole input sequence x."}, {"heading": "2.3. Denoising Sequence-to-sequence Autoencoder (DSA)", "text": "To learn more robust and meaningful representation, we further apply denoising criterion to SA learning. The input acoustic feature sequence x is randomly added noise, yielding a corrupted version x\u0303. Here the input of SA is x\u0303, and SA has to output (reconstruct) the original x from the corrupted input x\u0303. The SA learned with denoising criterion is referred to as Denoising SA (DSA) in the following paper."}, {"heading": "3. Application: Query-by-example STD", "text": "The audio segment representation z learned in the last section is applied on unsupervised query-by-example STD. The target of unsupervised query-by-example STD is to locate the occurrence regions of the term in the input spoken query without speech recognition. Figure 2 shows how we used the representation z in unsupervised query-by-example. This framework is inspired from the previous work [6], but completely different approach is used to represent the audio segments here. In the upper half of Figure 2, the audio archives in the database are segmented based on word boundaries into a set of variablelength segments, and then the system exploits the RNN encoder in Figure 1 to encode the word audio segments to fixed-length vectors. Given the query audio (the left lower corner of Figure 2), it is also encoded by the RNN encoder into a vector.\nThe system then calculates the cosine similarities between the query vector and all the vectors of the word audio segments in the database, and then return a list of searched audio segments ranked according to the similarities."}, {"heading": "4. Experiments", "text": "In this section, we analyze the learned representations and apply them in query-by-example STD. In Section 4.1, we first describe the experimental setup. Then, in Section 4.2, we analyze the vector representations learned by SA and DSA quantitatively. Finally, in Section 4.3, we demonstrate that SA and DSA are useful in query-by-example STD."}, {"heading": "4.1. Experimental Setup", "text": "We used LibriSpeech1 corpus [27], which is derived from read audiobooks from the LibriVox2 project, as data for experiments. For the lack of computation power, only the 5.4 hours dev-clean subset was used for training SA and DSA. The 5.4 hours testclean subset was the testing set. Both the training and testing sets contain 40 different speakers. MFCCs of 13-dim were used as the acoustic features. Both the training and testing sets were segmented according to the word boundaries obtained by forced alignment with respect to the reference transcriptions. Although the oracle word boundaries were used, since the baselines used the same segmentation as well, the comparison in this paper was fair.\nSA and DSA were implemented with Theano [28, 29]. The network structure and hyper parameters were set as below without further tuning if no specified:\n\u2022 Both RNN Encoder and Decoder consisted one hidden layer with 100 LSTM units. That is, each input segment was mapped into a 100-dim vector representation by SA or DSA. We adopted the LSTM version described in the previous work [30], and the peephole connections [31] were added.\n\u2022 The networks were trained by stochastic gradient descent without momentum for 500 epochs, with a fixed learning rate of 0.3 for the first 400 epochs and 0.35 for the rest 100 epochs.\n\u2022 For DSA, zero-masking [16] was used to generate the noisy input x\u0303, which randomly wiped out some elements in each input acoustic feature and set them to zero. The possibility to be wiped out was set to 0.3.\nFor query-by-example STD, the testing set also served as the audio database to be searched through. Each word segment in the testing set was taken as a spoken query once. When a word segment was taken as a query, it would be excluded from the database. There were 5557 queries in total. Mean Average Precision (MAP) [32] was used as the evaluation measure for query-by-example STD as the previous work [33]."}, {"heading": "4.2. Analysis of the Learned Representations", "text": "In this section, we want to validate that the words with similar pronunciations would have close SA and DSA vector representations. We used the SA and DSA learned from the training set to encode the segments in the testing set, which were never seen in the training stage, and then computed the cosine similarity between each segment pair in the testing set. The results\n1http://www.openslr.org/12/ 2https://librivox.org\nare in Table 1, in which the average cosine similarity of the segment pairs corresponding to the word pairs with different phoneme edit distances are displayed in each row. It is clear that when two segments are words with larger phoneme edit distances (that is, more distinct pronunciations), on an average, the cosine similarity of their vector representations is smaller. We also found that SA and DSA can even distinguish the words only with one different phoneme because the similarity of the case that edit distance is 0 (same pronunciation) is remarkably larger than the case that edit distance equals 1. The results show that SA and DSA can encode the acoustic information in their vector representations. However, it is difficult to know whether the performance of DSA is superior to SA from Table 1. Their performance will be further compared with other baselines in Section 4.3.\nBecause RNN Encoder in Figure 1 reads the input acoustic sequence sequentially, it is possible that the last few acoustic features dominate the vector representation, and the words with the same suffixes are hard to be distinguished by the learned representation. To justify the hypothesis, we analyze the following two sets of words with the same suffixes:\n\u2022 Set #1: father, mother, another\n\u2022 Set #2: ever, never, however\nSame with the previous analysis, we used cosine similarity to measure the differences between two encoded vectors. The result is displayed in Table 2. From Set #1, we found the vector\nrepresentations of audio segments of \u201cfather\u201d are much more similar to each other than the vectors of \u201cmother\u201d and \u201canother\u201d (0.5047 v.s. 0.2639, 0.1964 for SA, and 0.5209 v.s. 0.2748, 0.1861 for DSA) even though \u201cfarther\u201d, \u201cmother\u201d and\n\u201canother\u201d have the same suffix \u201c-ther\u201d. Set #2 (\u201c ever\u201d, \u201cnever\u201d and \u201chowever\u201d) show the same phenomenon. The other examples also show similar phenomenon, but we just display two sets of words here due to space limitation. The results show that although the RNN Encoder read the acoustic sequence sequentially, the words with the same suffix are still distinguishable from the learned vectors."}, {"heading": "4.3. Query-by-example STD", "text": "Here we compared the performance of using the representations from SA and DSA in query-by-example with two baselines. The first baseline used DTW [34] which is a common algorithm used in current query-by-example application. Here we used DTW to directly compute the similarities between the input query and the audio segments, and ranked the segments according to the similarities. Only the vanilla version of DTW was used here, and the Euclidean distance was used to evaluate the difference between acoustic features.\nThe second baseline used the same framework in Figure 2, except that the RNN Encoder is replaced by a manually designed encoder. We use the example in Figure 3 to illustrate this approach. The approach contains three procedures: di-\nvide, average, and concatenate. Assume there is an input sequence x = (x1, x2, ..., x18), where each symbol xt contains 13 elements, and the goal is to compress it into a 39- dim vector representation. The approach first divides x into 39/13 = 3 groups: (x1, x2, ..., x6), (x7, x8, ..., x12), and (x13, x14, ..., x18). Then, for each group, the method computes their average, yielding three vectors x(1) = 1\n6 \u22116 t=1 xt,\nx(2) = 1 6 \u221112 t=7 xt, and x (3) = 1 6 \u221118 t=13 xt, respectively. Finally, the three vectors are concatenated together and form a 39-dim vector representation [x(1);x(2);x(3)]. We will refer to this approach as Na\u0131\u0308ve Encoder (NE) in the following paper. Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5]. In Table 3, we show the MAP obtained by NE with different encoded space dimensionality, which is the subscript number after each word \u201cNE\u201d in the table. Because MFCCs of 13 dim are the acous-\ntic features, the dimensions of NE was always the multiple of\n13. We found that NE with 52 and 104 dimensions, or NE52 and NE104, yielded the best results, so we will compare the proposed approaches with NE52 and NE104 in the following paper 3.\nFigure 4 shows the retrieval performances of DTW, NE52, NE104, SA and DSA. The y-axis is for MAP, and the x-axis is the number of epochs. The number of epochs is only related to SA and DSA, and have nothing to do with DTW, NE52 and NE104. Besides the DTW approach, the other four approaches mapped the variable-length audio segments to fixed-length vectors, but different approaches were used. DTW is not comparable with NE52 and NE104 probably because only the vanilla version was used, and NE was able to consider the temporal information. As the training epochs increased, both of SA and DSA resulted in higher MAP. The retrieval performance of SA is comparable with NE52 and NE104 after 450 epochs. DSA outperformed SA in most cases, and defeated NE52 and NE104 since 360 epochs, and its performance still increased after that. The experimental results shows that learned representations can be much better than the manually designed ones, and verified that the learned representations were better when learning with denoising criterion."}, {"heading": "5. Conclusions and Future Work", "text": "We apply and extend Sequence-to-sequence Autoencoder (SA) for unsupervised learning of audio segment representations, and show that SA can learn meaningful representations. The learned representations can have many applications. In this preliminary study, we used them in query-by-example STD. The experimental results have shown that SA achieved higher MAP than the DTW-based approach and audio segment representation designed heuristically, and denoising training criterion further enhanced the learned representations. For the future work, we are training the SA on larger corpora with different dimensionality, and training and testing the proposed approaches on the audio segments obtained by unsupervised segmentation. We will use the indexing approaches to make the framework in Figure 2 more efficient, and compare the proposed approaches with more state-of-the-art query-by-example STD methods.\n3We did not tune the dimensionality of the learned representations, which was always fixed to 100."}, {"heading": "6. References", "text": "[1] N. Dehak, R. Dehak, P. Kenny, N. Brummer, P. Ouellet, and P. Du-\nmouchel, \u201cSupport vector machines versus fast scoring in the lowdimensional total variability space for speaker verification,\u201d in INTERSPEECH, 2009.\n[2] B. Schuller, S. Steidl, and A. Batliner, \u201cThe INTERSPEECH 2009 emotion challenge,\u201d in INTERSPEECH, 2009.\n[3] H.-Y. Lee and L.-S. Lee, \u201cEnhanced spoken term detection using support vector machines and weighted pseudo examples,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 6, pp. 1272\u20131284, 2013.\n[4] I.-F. Chen and C.-H. Lee, \u201cA hybrid HMM/DNN approach to keyword spotting of short words,\u201d in INTERSPEECH, 2013.\n[5] A. Norouzian, A. Jansen, R. Rose, and S. Thomas, \u201cExploiting discriminative point process models for spoken term detection,\u201d in INTERSPEECH, 2012.\n[6] K. Levin, A. Jansen, and B. V. Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in ICASSP, 2015.\n[7] S. Bengio and G. Heigold, \u201cWord embeddings for speech recognition,\u201d in INTERSPEECH, 2014.\n[8] G. Chen, C. Parada, and T. N. Sainath, \u201cQuery-by-example keyword spotting using long short-term memory networks,\u201d in ICASSP, 2015.\n[9] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[10] P. Baldi, \u201cAutoencoders, unsupervised learning, and deep architectures,\u201d Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, p. 43, 2012.\n[11] A. M. Dai and Q. V. Le, \u201cSemi-supervised sequence learning,\u201d in arXiv preprint arXiv: 1511.01432, 2015.\n[12] J. Li, M.-T. Luong, and D. Jurafsky, \u201cA hierarchical neural autoencoder for paragraphs and documents,\u201d in arXiv preprint arXiv: 1506.01057, 2015.\n[13] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler, \u201cSkip-thought vectors,\u201d in arXiv preprint arXiv: 1506.06726, 2015.\n[14] N. Srivastava, E. Mansimov, and R. Salakhutdinov, \u201cUnsupervised learning of video representations using LSTMs,\u201d arXiv preprint arXiv:1502.04681, 2015.\n[15] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, \u201cExtracting and composing robust features with denoising autoencoders,\u201d in ICML, 2008.\n[16] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, \u201cStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,\u201d JMLR, vol. 11, pp. 3371\u20133408, 2010.\n[17] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in NIPS, 2014.\n[18] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.\n[19] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.\n[20] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[21] J. Schmidhuber, D. Wierstra, M. Gagliolo, and F. Gomez, \u201cTraining recurrent networks by evolino,\u201d Neural Computation, vol. 19, no. 3, pp. 757\u2013779, 2007.\n[22] J. Bayer, D. Wierstra, J. Togelius, and J. Schmidhuber, \u201cEvolving memory cell structures for sequence learning,\u201d in ICANN, 2009.\n[23] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in INTERSPEECH, 2014.\n[24] P. Doetsch, M. Kozielski, and H. Ney, \u201cFast and robust training of recurrent neural networks for offline handwriting recognition,\u201d in ICFHR, 2014.\n[25] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[26] K. Greff, R. K. Srivastava, J. Koutn\u0131\u0301k, B. R. Steunebrink, and J. Schmidhuber, \u201cLstm: A search space odyssey,\u201d arXiv preprint arXiv:1503.04069, 2015.\n[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an ASR corpus based on public domain audio books,\u201d in ICASSP, 2015.\n[28] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \u201cTheano: a CPU and GPU math expression compiler,\u201d in SciPy, 2010.\n[29] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio, \u201cTheano: new features and speed improvements,\u201d CoRR, vol. abs/1211.5590, 2012.\n[30] A. Graves, \u201cGenerating sequences with recurrent neural networks,\u201d CoRR, vol. abs/1308.0850, 2013.\n[31] F. Gers, J. Schmidhuber et al., \u201cRecurrent nets that time and count,\u201d in IJCNN, 2000.\n[32] C. D. Manning, P. Raghavan, and H. Sch?tze, Introduction to Information Retrieval. Cambridge University Press, 2008, ch. 8.\n[33] C.-T. Chung, C.-A. Chan, and L.-s. Lee, \u201cUnsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity,\u201d in ICASSP, 2014.\n[34] H. Sakoe and S. Chiba, \u201cDynamic programming algorithm optimization for spoken word recognition,\u201d Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43\u2013 49, 1978."}], "references": [{"title": "Support vector machines versus fast scoring in the lowdimensional total variability space for speaker verification", "author": ["N. Dehak", "R. Dehak", "P. Kenny", "N. Brummer", "P. Ouellet", "P. Dumouchel"], "venue": "IN- TERSPEECH, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "The INTERSPEECH 2009 emotion challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner"], "venue": "INTERSPEECH, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples", "author": ["H.-Y. Lee", "L.-S. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 6, pp. 1272\u20131284, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A hybrid HMM/DNN approach to keyword spotting of short words", "author": ["I.-F. Chen", "C.-H. Lee"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting discriminative point process models for spoken term detection", "author": ["A. Norouzian", "A. Jansen", "R. Rose", "S. Thomas"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B.V. Durme"], "venue": "ICASSP, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Word embeddings for speech recognition", "author": ["S. Bengio", "G. Heigold"], "venue": "INTERSPEECH, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example keyword spotting using long short-term memory networks", "author": ["G. Chen", "C. Parada", "T.N. Sainath"], "venue": "ICASSP, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, p. 43, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "arXiv preprint arXiv: 1511.01432, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "arXiv preprint arXiv: 1506.01057, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "arXiv preprint arXiv: 1506.06726, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "ICML, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "JMLR, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Training recurrent networks by evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez"], "venue": "Neural Computation, vol. 19, no. 3, pp. 757\u2013779, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolving memory cell structures for sequence learning", "author": ["J. Bayer", "D. Wierstra", "J. Togelius", "J. Schmidhuber"], "venue": "ICANN, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "INTERSPEECH, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and robust training of recurrent neural networks for offline handwriting recognition", "author": ["P. Doetsch", "M. Kozielski", "H. Ney"], "venue": "ICFHR, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Librispeech: an ASR corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "ICASSP, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "SciPy, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "CoRR, vol. abs/1211.5590, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, vol. abs/1308.0850, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent nets that time and count", "author": ["F. Gers", "J. Schmidhuber"], "venue": "IJCNN, 2000.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Sch?tze, Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity", "author": ["C.-T. Chung", "C.-A. Chan", "L.-s. Lee"], "venue": "ICASSP, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43\u2013 49, 1978.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 97, "endOffset": 106}, {"referenceID": 3, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 97, "endOffset": 106}, {"referenceID": 4, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 97, "endOffset": 106}, {"referenceID": 5, "context": "In query-by-example STD, by representing each word segment as a feature vector, indexing the segments can be easier, which makes retrieval much more efficient [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 0, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 2, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 3, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 4, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 6, "context": "Deep learning is also used for extracting representations [7, 8].", "startOffset": 58, "endOffset": 64}, {"referenceID": 7, "context": "Deep learning is also used for extracting representations [7, 8].", "startOffset": 58, "endOffset": 64}, {"referenceID": 7, "context": "By learning Recurrent Neural Network (RNN) with an audio segment as input and the word corresponding to the segment as target, the outputs of the hidden layer at the last few time steps can be used as the representation of the input segment [8].", "startOffset": 241, "endOffset": 244}, {"referenceID": 8, "context": "On the other hand, Autoencoder has been one of the most prominent machine learning techniques for extracting representations in an unsupervised way [9, 10], but its input should be vectors of fixed dimensionality.", "startOffset": 148, "endOffset": 155}, {"referenceID": 9, "context": "On the other hand, Autoencoder has been one of the most prominent machine learning techniques for extracting representations in an unsupervised way [9, 10], but its input should be vectors of fixed dimensionality.", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 11, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 12, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 13, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 11, "context": "This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.", "startOffset": 71, "endOffset": 79}, {"referenceID": 12, "context": "This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.", "startOffset": 71, "endOffset": 79}, {"referenceID": 13, "context": "This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Different from the previous work [7, 8], learning SA does not need any supervision.", "startOffset": 33, "endOffset": 39}, {"referenceID": 7, "context": "Different from the previous work [7, 8], learning SA does not need any supervision.", "startOffset": 33, "endOffset": 39}, {"referenceID": 14, "context": "Inspired from denoising Autoencoder [15, 16], we further propose Denoising Sequenceto-sequence Autoencoder (DSA) that improves the learned representations.", "startOffset": 36, "endOffset": 44}, {"referenceID": 15, "context": "Inspired from denoising Autoencoder [15, 16], we further propose Denoising Sequenceto-sequence Autoencoder (DSA) that improves the learned representations.", "startOffset": 36, "endOffset": 44}, {"referenceID": 16, "context": "Below we first give a brief recap on the RNN Encoder-Decoder framework [17, 18], which is widely used to realize the sequence-to-sequence learning in Section 2.", "startOffset": 71, "endOffset": 79}, {"referenceID": 17, "context": "Below we first give a brief recap on the RNN Encoder-Decoder framework [17, 18], which is widely used to realize the sequence-to-sequence learning in Section 2.", "startOffset": 71, "endOffset": 79}, {"referenceID": 18, "context": "In practice, RNN does not seem to learn long-term dependencies [19], so LSTM [20] is designed to conquer such difficulty.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "In practice, RNN does not seem to learn long-term dependencies [19], so LSTM [20] is designed to conquer such difficulty.", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 21, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 22, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 23, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 17, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 24, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 25, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 17, "context": "RNN Encoder-Decoder [18, 17] consists of an Encoder RNN and a Decoder RNN.", "startOffset": 20, "endOffset": 28}, {"referenceID": 16, "context": "RNN Encoder-Decoder [18, 17] consists of an Encoder RNN and a Decoder RNN.", "startOffset": 20, "endOffset": 28}, {"referenceID": 8, "context": "Following the philosophy of Autoencoder [9, 10], the target of the output sequence y = (y1, y2, .", "startOffset": 40, "endOffset": 47}, {"referenceID": 9, "context": "Following the philosophy of Autoencoder [9, 10], the target of the output sequence y = (y1, y2, .", "startOffset": 40, "endOffset": 47}, {"referenceID": 5, "context": "This framework is inspired from the previous work [6], but completely different approach is used to represent the audio segments here.", "startOffset": 50, "endOffset": 53}, {"referenceID": 26, "context": "We used LibriSpeech corpus [27], which is derived from read audiobooks from the LibriVox project, as data for experiments.", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "SA and DSA were implemented with Theano [28, 29].", "startOffset": 40, "endOffset": 48}, {"referenceID": 28, "context": "SA and DSA were implemented with Theano [28, 29].", "startOffset": 40, "endOffset": 48}, {"referenceID": 29, "context": "We adopted the LSTM version described in the previous work [30], and the peephole connections [31] were added.", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "We adopted the LSTM version described in the previous work [30], and the peephole connections [31] were added.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "\u2022 For DSA, zero-masking [16] was used to generate the noisy input x\u0303, which randomly wiped out some elements in each input acoustic feature and set them to zero.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "Mean Average Precision (MAP) [32] was used as the evaluation measure for query-by-example STD as the previous work [33].", "startOffset": 29, "endOffset": 33}, {"referenceID": 32, "context": "Mean Average Precision (MAP) [32] was used as the evaluation measure for query-by-example STD as the previous work [33].", "startOffset": 115, "endOffset": 119}, {"referenceID": 33, "context": "The first baseline used DTW [34] which is a common algorithm used in current query-by-example application.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 3, "context": "Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 4, "context": "Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5].", "startOffset": 101, "endOffset": 110}], "year": 2017, "abstractText": "Representing audio segments expressed with variablelength acoustic feature sequences as fixed-length feature vectors is usually needed in many speech applications, including speaker identification, audio emotion classification and spoken term detection (STD). In this paper, we apply and extend sequence-to-sequence learning framework to learn representations for audio segments without any supervision. The model we used is called Sequence-to-sequence Autoencoder (SA), which consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps the input sequence into a vector representation of fixed dimensionality, and the second RNN acts as a decoder that maps the representation back to the input sequence. The two RNNs are then jointly trained by minimizing the reconstruction error. We further propose Denoising Sequence-to-sequence Autoencoder (DSA) that improves the learned representations. The vector representations learned by SA and DSA are shown to be very helpful for query-by-example STD. The experimental results have shown that the proposed models achieved better retrieval performance than using audio segment representation designed heuristically and the classical Dynamic Time Warping (DTW) approach.", "creator": "LaTeX with hyperref package"}}}