{"id": "1603.02514", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "Variational Autoencoders for Semi-supervised Text Classification", "abstract": "We propose a new semi-supervised learning method for sequence classification tasks. Our work is based on both a deep generative model for semi-supervised learning\\ cite {kingma2014semi} and a variable auto-encoder for sequence modeling\\ cite {bowman2015generating}. We found that the introduction of Sc-LSTM is critical to the success of our method. We have obtained some preliminary experimental results on IMDB sentiment classification data sets, which show that the proposed model improves classification accuracy compared to purely supervised classification data sets.", "histories": [["v1", "Tue, 8 Mar 2016 13:24:45 GMT  (169kb,D)", "http://arxiv.org/abs/1603.02514v1", "6 pages, 1 figure"], ["v2", "Mon, 23 May 2016 14:33:50 GMT  (365kb,D)", "http://arxiv.org/abs/1603.02514v2", "6 pages, 1 figure"], ["v3", "Thu, 24 Nov 2016 08:18:31 GMT  (735kb,D)", "http://arxiv.org/abs/1603.02514v3", "8 pages, 4 figure"]], "COMMENTS": "6 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["weidi xu", "haoze sun", "chao deng", "ying tan"], "accepted": true, "id": "1603.02514"}, "pdf": {"name": "1603.02514.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Variational Autoencoders for Sequence Classification", "authors": ["Weidi Xu", "Haoze Sun"], "emails": ["hsu@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Semi-supervised learning makes use of both labeled and unlabeled data for training. Classification using semi-supervised learning is a critical problem to be tackled since the data size nowadays is increasing much more faster than before, while only a limited subset of data samples has their corresponding labels. Hence lots of attention has been drawn from researchers over machine learning and deep learning communities, leading to many semi-supervised learning methods. Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.e. pre-training, followed by normal supervised fine-tuning phase. Unsupervised learning are applied to get better initial weight parameters by trying to extract the hidden features which are beneficial for data reconstruction. However these features may be irrele-\nvant to determine the class label in certain classification tasks. For sequence classification problem, Socher et al. (Socher et al., 2013) proposed a similar model using semi-supervised method. The model jointly optimize both reconstruct error and prediction accuracy.\nRecently, variational auto-encoder (Kingma and Welling, 2013) is proposed and it has shown impressive performance in image generation tasks. It has also been applied in semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) and achieved the best results in several image semisupervised learning tasks . We refer to these kind of models as SemiVAEs. Different from traditional semi-supervised learning method, this method consists of two cost functions for both labeled data and unlabelled data. The cost for unlabelled data is the expectation of conditional generation likelihood on classification probability of each category. Hence minimizing this cost optimizes conditional generation model as well as classification model. Adoption of conditional generation models and independent classifier alleviates the problem with previous pre-training methods, i.e. extracting features only relevant to data reconstruct.\nSequence classification tasks take sequence as input to predict category labels, whose research hotspot including sentiment classification problem, text categorization, etc. However, SemiVAE can not be applied to sequence classification tasks directly. Sequence classification is different from common image classification problem in several aspects, a) the length of datasamples are variable b) the data distribution of same category is more diverse and c) the category information is more abstract. We combines the variational autoencoders for semi-supervised learning (Kingma et al., 2014) and its application in sequence generation (Bowman et al., 2015) for sequence classifiar X\niv :1\n60 3.\n02 51\n4v 1\n[ cs\n.C L\n] 8\nM ar\n2 01\n6\ncation problem. In our implementation the generation is modelled by a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). To model conditional generation (main component necessary in SemiVAE), one obvious option is to take the init hidden state and category feature as the first input and then predict each word sequentially using recurrent neural networks, e.g LSTM. Nonetheless we found this implementation would confuse the model to ignore the label input and hence failed to improve the classification accuracy. To force the conditional generation model to be aware of category feature, we utilize the similar structure with semantically controlled LSTM (Sc-LSTM), which is proprosed in (Wen et al., 2015). Using Sc-LSTM, the category feature is fed to generation model at each step, explicitly informing the generation model to be conditioned on given category. The resulting model is capable of apply this semi-supervised method in sequence domain successfully.\nIn the experiment we show our results on IMDB dataset, which is a common dataset for sentiment classification task. We compare our model with pure supervised learning models and illustrate the importance of several key components in our method.\nThe article is organized as follows. In the next section, we introduce several background works. And then our model is presented in details. In section 4, we obtain both quantitive results and qualitative analysis about our models. At last we conclude our paper with a detailed discussion."}, {"heading": "2 Backgrounds", "text": ""}, {"heading": "2.1 Variational Autoencoder", "text": "Recently variaitonal autoencoder (VAE) have drawn a lot of attentions due to its impressive results reported in (Kingma and Welling, 2013) and (Gregor et al., 2015). It is equipped with a top-down generative network \u03b8 and a bottomup recognition network \u03c6. Both networks are jointly trained to maximize the variational lower bound of data likelihood. Given dataset X = {x1, x2, ......, xN}, the variational auto-encoder aims to maximize the loglikelihood of all datapoints logp\u03b8(x1, x2, ..., xN ) = \u2211N i=1 logp\u03b8(xi). Different form traditiontal auto-encoder architecture, the VAE at first samples latent representation z from a learned posterior distribution p\u03c6(z|x), using reparameterization tricks (Kingma\nand Welling, 2013). Then x is reconstructed from the latent space of z by generative model p\u03b8(x|z). VAEs encode each datapoint x into a small region (but not a point) in latent space. With this trait, the model has a more smooth latent space than previous autoencoders. The variational lower bound is:\nlogp\u03b8(x) \u2265 Eq\u03c6 [logp\u03b8(x|z)] \u2212KL[q\u03c6(z|x)||p(z)]\n(1)\nGenerally the first term approximate the reconstruction probability. The distribution of p(x|z) takes the form of Gaussian or Bernoulli distribution, for continuous or binary valued data respectively. The second term acts as the regulariser by minimizing the difference between the prior p(z) and the learned posterior p\u03c6(z|x), usually both distributions are diagonal Gaussian. The model is specified as follows:\nlogp\u03b8(x) \u2265 Eq\u03c6 [logp\u03b8(x|z)] \u2212KL[q\u03c6(z|x)||p(z)]\n(2)\np(z) = N(Z|0, I) (3)\np\u03b8(x|z) =N (x|\u00b5\u03b8(z), \u03b4\u03b8(z)) or B(x|\u00b5\u03b8(z))\n(4)\nwhere \u00b5\u03c6(x), \u03b4\u03c6(x), \u00b5\u03b8(z), \u03b4\u03b8(z) are represented as multi-layer perceptron networks."}, {"heading": "2.2 Semi-supervised Learning Using VAEs", "text": "Conditional generative models can generate data according to certain attributions of given label. Kingma et al.(Kingma et al., 2014) firstly proposed a conditional variational auto-encoder (CVAE) to successfully separate the image style and content information. Several other works (Maal\u00f8e et al., 2016; Yan et al., 2015) also proved CVAE to be powerful in image generation tasks. In addition, semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) can help improving the discriminative results by employing unlabeled data. Given labeled dataset X,Y = {(x1, y1), (x2, y2)......, (xN , yN )} the lower bound of CVAE is:\nlogp\u03b8(x, y) \u2265 Eq\u03c6(z|x,y)[logp\u03b8(x|y, z) + logp\u03b8(y) + logp(z)\u2212 logq\u03c6(z|x, y)] = \u2212L(x, y)\n(5)\nFor the unlabeled dataset, the unobserved label y is predicted form the recognition model with a learned classifier q\u03c6(y|x). The lower bound is hence: logp\u03b8(x) \u2265 \u2211 y q\u03c6(y|x)(\u2212L(x, y)) +H(q\u03c6(y|x))\n= \u2212U(x) (6)\nThis equation is the key for semi-supervised learning. Note that the first term is the expection of labeled lower bound on q\u03c6(y|x) and intuitively we can explain the procedure as follows: 1) once we can roughly infer the log likelihood of a certain sample x with ground-true label yp, i.e. p\u03b8(x|yp, z) and wrong label yn. p\u03b8(x|yp, z) is likely to be greater than p\u03b8(x|yn, z). And hence the gradient computed for q\u03c6(yp|x) has a greater coefficient for q\u03c6(yp|x), and such reinforce the classifier. 2) if the classifier can predict the right label for most time, the generation model will be strengthened in a similar way. This is a positive feedback in the training. We will show how this mechanism can be applied in this sequence classification tasks in our method.\nActually the classification loss term is also added to the objective function, so that the classifier q\u03c6(y|x) can be learned from labeled dateset. Hence the overall objective for entire dataset is now:\nJ = \u2211\n(x,y)\u2208Sl L(x, y) + \u2211 x\u2208Su U(x)\n+ \u03b1E(x,y)\u2208Sl [\u2212logq\u03c6(y|x)] (7)"}, {"heading": "2.3 Variational Autoencoder for Sentence Generation", "text": "Recurrent neural networks (RNNs) are the most successful methods for sequence generation tasks such as machine translation and image captioning. However, one-by-one generation mechanism in RNNs can\u2019t extract high level features like topic, style and sentiment properties. To solve the problem, variational recurrent autoencoders (VRAEs) have been employed in modeling global features for sequential data like sentences(Bowman et al., 2015) and music(Fabius and van Amersfoort, 2014). Similar with the aforementioned models, VRAEs use encoder-decoder structure. In recognitive model, sequence x is processed by encoder\nRNNs to extract the global information, while in generative model latent variables z are used to initialize the hidden state for decoder RNNs."}, {"heading": "3 Model", "text": "In this section, we introduce our work in details. The model is based on the aforementioned networks and is sketched in figure 1.\nSpecifically the inference model of our method is described as following equation.\nq\u03c6(z|y, x) = N (z|\u00b5(x, y), diag(\u03c32(x, y))) (8)\nx\u0302 = LSTM(x) (9)\n\u00b5(x, y) =W ([x\u0302; y]) (10)\nlog\u03c32(x, y) = softplus(W ([x\u0302; y])) (11)\nwhere sequence x is encoded by LSTM network, and the output is concatenated with y for setting this diagonal Gaussian distribution, y is represented as one-hot vectors. Here we use the notation b = W (a) to denote a linear weight matrix with bias from vector a to vector b for simplicity ."}, {"heading": "3.1 Sc-LSTM", "text": "To model the sequence generation conditioned on both hidden state z and class label y, we originally simply concatenate y and z as the initial state for LSTM. However during experiments we found this simple implementation failed to improve the classification performance since the model figures out that ignoring the class feature and minimizing the generation likelihood according to language model (i.e. predicting next word according to a small context windows) is the best strat-\negy to optimize the objective function. This is because the category information is passed to generation network only at first time-step and the conditional generation probability is maximized over all kinds of categories (each multiplied with a coefficient produced by classifier q\u03c6(y|x)). If the conditional generation model can not distinguish between different category features, the model will be incapable to take advantage of positive feedback mechanism described in section 2.2, and hence fail to boost the performance using this semi-supervised learning method.\nConsidering this, we force the conditional generation model to generate sequence at each timestep to be aware of given labels. Simplified semantically controlled LSTM (Sc-LSTM) network proposed in (Wen et al., 2015) is adopted to achieve this goal. The Sc-LSTM is defined by the following equations:\nit = \u03c3(Wwiwt +Whiht\u22121) (12)\nft = \u03c3(Wwfwt +Whfht\u22121) (13)\not = \u03c3(Wwowt +Whoht\u22121) (14)\nc\u0302t = tanh(Wwcwt +Whiht\u22121) (15)\nht = ot \u2297 tanh(ct) (16)\nct = ft \u2297 ct\u22121 + it \u25e6 c\u0302t + tanh(Wycy) (17)\nwhere the first six equations are the same as in standard LSTM networks and the last equation has an extra term about y. Therefore the model is explicitly informed of the existence of category feature all the steps. By using this setting the conditional generation model has to distinguish between the different labels. The details analysis will be made in section 4."}, {"heading": "3.2 Cost Annealing", "text": "Cost annealing is a training trick introduced in (Bowman et al., 2015; Husza\u0301r, 2015), which gradually increasing the weight of KL cost from zero to one. Without this the model tend to ignore the input x and most training likely yields models consistently set q(z|x) approximate to q(z). This technique is also considered in our implementation."}, {"heading": "4 Experimental Results", "text": "This section will show several preliminary experimental results in Large Movie Review Dataset, sometimes know as IMDB dataset. The dataset\nconsists of 25K labelled datasamples for training, 25K labelled datasamples for testing and 50K unlabelled datasamples used for semi-supervised learning. The dataset has adequate samples to training classifier as well as generation model.\nFor device set, we randomly sample 5K samples from training set. We adopt a common implementation for classification model, i.e. using the averaged hidden state generated by standard LSTM network to predict the label. Averaging the hidden states is more easier for training reported in (Hong and Fang, ). Although there are some other more sophisticated methods for sequence classification, we use this standard model for simplicity in following experiments. Our results may not supass the state-of-art results for IMDB dataset yet due to the limitation of classifier, but still has demonstrated the sufficiency of our semi-supervised method for sequential data.\nWe compare our method (denoted as SemiSeq) with pure-supervised LSTM classifier, with different size of labelled datasamples. To split the labelled data for each experiment we sample from training set with two categories balanced and leave the others unlabelled. The results is listed in table 4. We draw the results of testset with best results on devset in each single experiment.\nIn all experiments our method is able to improve the classification accuracy for about 1\u223c2%. Note that the accuracy using 25K samples and carefully chosen hyper-parameters is about 89.1%, which can be regarded as the performance limit of LSTM network for IMDB sentiment classification task. With less labelled samples more improvement can be obtained.\nIn the future other datasets may be considered."}, {"heading": "4.1 Implementation Details", "text": ""}, {"heading": "4.1.1 Length Sampling", "text": "When using datasets consists of long sequence samples, it is not efficient to use the whole sequence for generation. Assuming sub-sequence has enough information to infer the category, trun-\ncated sequence will suffice to differ the generation likelihood of various class feature. On the other hand, the recurrent neural network, even LSTM, is incapable of model very long sequence generation. This trick alleviate the difficulty for conditional generation model.\nIn the implementation for IMDB dataset where average sequence length is approximately 300, we randomly draw sub-sequence uniformly from each data sample to speed up training. In experiments we found that this trick works well for the purpose of fast training. The training speed is about 3 times faster comparing to using whole sequence."}, {"heading": "4.1.2 Dropout and Word-dropout", "text": "To improve generalization ability word dropout method is utilized. In conditional sequence generation model, we randomly drop out some words and replace with blanks. This method introduces noise into networks and help the network to be more general. However we found it is inappropriate to use the word-dropout mechanism in equation 6. Since the word-dropout make the generation probability unstable and hence influence the gradient computed for sequence classifier. Hence in our implementation, we use this mechanism only for labelled data, i.e. equation 5, but not in equation 6."}, {"heading": "4.2 Qualitative Analysis", "text": "Here we examine the capability of generation model. Even though the classification performance is improved using our method, the generation power still remains unknown. To check it out we randomly sample several sentences using trained conditional generation model. The sampled sentences are shown in table 4.2. We show several cases when using the same initial state z and different labels.\nFrom the samples we can tell that both of sentence generated by same z share the similar sentence structure and words, but the sentimental implication is totally different with each other. On the other hand the model is still unable to capture the high level meaning of sentiment but tried to remember the frequency of words for each category."}, {"heading": "5 Conclusion", "text": "In this paper we present a new semi-supervised learning method for sequence classification. We explained the reason for using Sc-LSTM as conditional generative model. The resulting model is\ncapable of improve the classification performance remarkably."}, {"heading": "Acknowledgments", "text": "This work was supported by the Natural Science Foundation of China (NSFC) under grant no. 61375119 and the Beijing Natural Science Foundation under grant no. 4162029, and partially supported by National Key Basic Research Development Plan (973 Plan) Project of China under grant no. 2015CB352302."}], "references": [{"title": "Greedy layer-wise training of deep networks. Advances in neural information processing", "author": ["Bengio et al.2007] Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349", "author": ["Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Variational recurrent auto-encoders", "author": ["Fabius", "van Amersfoort2014] Otto Fabius", "Joost R van Amersfoort"], "venue": "arXiv preprint arXiv:1412.6581", "citeRegEx": "Fabius et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fabius et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101", "author": ["Ferenc Husz\u00e1r"], "venue": null, "citeRegEx": "Husz\u00e1r.,? \\Q2015\\E", "shortCiteRegEx": "Husz\u00e1r.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Welling2013] Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auxiliary deep generative models. arXiv preprint arXiv:1602.05473", "author": ["Maal\u00f8e et al.2016] Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": null, "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of the confer-", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Attribute2image: Conditional image generation from visual attributes. arXiv preprint arXiv:1512.00570", "author": ["Yan et al.2015] Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Our work is based on both deep generative model for semisupervised learning (Kingma et al., 2014) and variational auto-encoder for sequence modeling (Bowman et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": ", 2014) and variational auto-encoder for sequence modeling (Bowman et al., 2015).", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": "Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.", "startOffset": 33, "endOffset": 97}, {"referenceID": 11, "context": "Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.", "startOffset": 33, "endOffset": 97}, {"referenceID": 0, "context": "Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.", "startOffset": 33, "endOffset": 97}, {"referenceID": 10, "context": "(Socher et al., 2013) proposed a similar model using semi-supervised method.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "It has also been applied in semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) and achieved the best results in several image semisupervised learning tasks .", "startOffset": 53, "endOffset": 95}, {"referenceID": 9, "context": "It has also been applied in semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) and achieved the best results in several image semisupervised learning tasks .", "startOffset": 53, "endOffset": 95}, {"referenceID": 8, "context": "We combines the variational autoencoders for semi-supervised learning (Kingma et al., 2014) and its application in sequence generation (Bowman et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 1, "context": ", 2014) and its application in sequence generation (Bowman et al., 2015) for sequence classifiar X iv :1 60 3.", "startOffset": 51, "endOffset": 72}, {"referenceID": 12, "context": "To force the conditional generation model to be aware of category feature, we utilize the similar structure with semantically controlled LSTM (Sc-LSTM), which is proprosed in (Wen et al., 2015).", "startOffset": 175, "endOffset": 193}, {"referenceID": 3, "context": "Recently variaitonal autoencoder (VAE) have drawn a lot of attentions due to its impressive results reported in (Kingma and Welling, 2013) and (Gregor et al., 2015).", "startOffset": 143, "endOffset": 164}, {"referenceID": 8, "context": "(Kingma et al., 2014) firstly proposed a conditional variational auto-encoder (CVAE) to successfully separate the image style and content information.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Several other works (Maal\u00f8e et al., 2016; Yan et al., 2015) also proved CVAE to be powerful in image generation tasks.", "startOffset": 20, "endOffset": 59}, {"referenceID": 13, "context": "Several other works (Maal\u00f8e et al., 2016; Yan et al., 2015) also proved CVAE to be powerful in image generation tasks.", "startOffset": 20, "endOffset": 59}, {"referenceID": 8, "context": "In addition, semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) can help improving the discriminative results by employing unlabeled data.", "startOffset": 38, "endOffset": 80}, {"referenceID": 9, "context": "In addition, semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) can help improving the discriminative results by employing unlabeled data.", "startOffset": 38, "endOffset": 80}, {"referenceID": 1, "context": "To solve the problem, variational recurrent autoencoders (VRAEs) have been employed in modeling global features for sequential data like sentences(Bowman et al., 2015) and music(Fabius and van Amersfoort, 2014).", "startOffset": 146, "endOffset": 167}, {"referenceID": 12, "context": "Simplified semantically controlled LSTM (Sc-LSTM) network proposed in (Wen et al., 2015) is adopted to achieve this goal.", "startOffset": 70, "endOffset": 88}, {"referenceID": 1, "context": "Cost annealing is a training trick introduced in (Bowman et al., 2015; Husz\u00e1r, 2015), which gradually increasing the weight of KL cost from zero to one.", "startOffset": 49, "endOffset": 84}, {"referenceID": 6, "context": "Cost annealing is a training trick introduced in (Bowman et al., 2015; Husz\u00e1r, 2015), which gradually increasing the weight of KL cost from zero to one.", "startOffset": 49, "endOffset": 84}], "year": 2016, "abstractText": "Semi-supervised learning becomes one of the most significant problems nowadays since the size of datasets is increasing tremendously while labeled data is limited. We propose a new semisupervised learning method for sequence classification tasks. Our work is based on both deep generative model for semisupervised learning (Kingma et al., 2014) and variational auto-encoder for sequence modeling (Bowman et al., 2015). We found the introduction of Sc-LSTM is critical to the success in our method. We have obtained some preliminary experimental results on IMDB sentiment classification dataset, showing that the proposed model improves the classification accuracy comparing to pure supervised classifier.", "creator": "LaTeX with hyperref package"}}}