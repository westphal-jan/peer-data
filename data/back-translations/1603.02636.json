{"id": "1603.02636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data", "abstract": "We introduce the DROW detector, a deep-learning-based detector for 2D range data. Laser scanners are light-immutable, deliver accurate range data and typically cover a wide field of view, making them interesting sensors for robotic applications. So far, laser range data detection has been dominated by handcrafted features and improved classifiers that may lose performance due to suboptimal design decisions. We propose a detector based on the Convolutional Neural Network (CNN) for this task. We show how to effectively apply CNNs for 2D range data detection and propose a depth pre-processing step and a tuning scheme that significantly improves CNN's performance. We demonstrate our approach to wheelchairs and walkers, obtaining state-of-the-art detection results. Apart from the training data, however, none of our design decisions restrict the detector to the ROS-4K data node for our 24S-46k classroom.", "histories": [["v1", "Tue, 8 Mar 2016 19:39:19 GMT  (886kb,D)", "https://arxiv.org/abs/1603.02636v1", "Lucas Beyer and Alexander Hermans contributed equally"], ["v2", "Mon, 5 Dec 2016 18:06:28 GMT  (881kb,D)", "http://arxiv.org/abs/1603.02636v2", "Lucas Beyer and Alexander Hermans contributed equally"]], "COMMENTS": "Lucas Beyer and Alexander Hermans contributed equally", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG cs.NE", "authors": ["lucas beyer", "alexander hermans", "bastian leibe"], "accepted": false, "id": "1603.02636"}, "pdf": {"name": "1603.02636.pdf", "metadata": {"source": "CRF", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data", "authors": ["Lucas Beyer", "Alexander Hermans", "Bastian Leibe"], "emails": ["last@vision.rwth-aachen.de"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMany autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18]. Laser scanners are widely used due to their typically large field of view and their invariance to lighting and environmental conditions. While early detection methods used simple heuristics such as fitting lines and circles [35], in the past few years hand crafted features, coupled with learned classifiers, have dominated laser based detection. Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28]. Even though the aforementioned models obtain respectable results, the general consensus seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking. We show that detection based on single 2D range scan can actually perform well.\nIn this paper, we specifically focus on the detection of wheelchairs and walkers. This is motivated by a service robot application in an elderly care facility, where many people rely on their mobility aids. Since the presence of a mobility aid can significantly change a person\u2019s appearance, both in laser and in camera data, they are less reliably detected by existing people detectors. However, especially people relying\n\u2020Equal contribution. Ordering determined by a last minute coin flip. The work in this paper was funded by the EU project STRANDS (ICT2011-600623). All authors are at the Visual Computing Institute, RWTH Aachen University. e-mail: last@vision.rwth-aachen.de\non those aids will have a harder time avoiding an approaching robot, making a reliable detection all the more important. Weinrich et al. [34] face a similar task and propose the Gandalf detector for detecting people, wheelchairs, and walkers in individual laser scans. They introduce a new feature set for laser segments and classify these with an AdaBoost classifier. The resulting detector performs reasonably well and we include it as a baseline.\nIn this paper, we introduce the DROW (Distance RObust Wheelchair/Walker) detector. Driven by our application scenario, we focus on the detection of wheelchairs and walkers. However, there are no design decisions which restrict our approach to these objects and we are confident it will generalize to detection of persons or other objects, given sufficient annotated training data. Code and data needed to train and run our detector, as well as code to annotate data for new tasks, will be made available upon publication.\nIn computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5]. Specifically, Convolutional Neural Networks (CNNs) have been very successful at challenging\nar X\niv :1\n60 3.\n02 63\n6v 2\n[ cs\n.R O\n] 5\nD ec\n2 01\n6\ntasks. In this paper, we show how CNNs can be applied for object detection in laser data, alleviating the need for feature engineering and enabling drastic improvements.\nWhile CNN-based image-level detectors like MultiBox [16] and YOLO [24] could in principle be applied to a 2D laser scan, we found that doing so naively is not effective. Since the spatial density of laser points varies greatly with distance, the fixed perceptive field of a CNNs covers widely different scales, making learning difficult. To make use of the spatial information a laser sensor provides, we propose a preprocessing stage that cuts out and normalizes a fixed realworld extent window around each laser point. All of those windows are fed through a CNN which can cast votes for object locations. These votes are then turned into individual detections using a non-maximum suppression scheme. We show that both depth preprocessing and voting are essential components of our approach, an overview of which can be seen in Fig. 1. Our approach does not require background subtraction and runs with a frame rate of \u223c75 fps on a modern desktop machine using a single core and a GPU. On our robot, it easily keeps up with the laser frame rate (\u223c13 fps) on a laptop GPU.\nTo summarize, we make the following contributions: \u2022 We introduce the DROW detector, a CNN based\nwheelchair and walker detector for 2D range data which, by effectively making use of the provided depthinformation, achieves state of the art results. \u2022 We publish our dataset, containing 464k raw scans of which 24k have been annotated with wheelchair and walker centroids. \u2022 We provide ROS components of our detector and related service modules, including trained models."}, {"heading": "II. APPROACH", "text": "Our approach consists of three steps: preprocessing, which cuts out a resampled window around every laser point and computes detection locations in a local coordinate system, a CNN classifying said windows and predicting relative detection locations, and finally a voting and non-maximum suppression scheme turning predictions into detections.\nA. 2D Range Data Preprocessing\nIn order to apply a CNN for detection, the network\u2019s receptive field must cover a large part of the object. The problem with laser scans is that nearby objects cover a large amount of laser beams, whereas distant objects are only hit by a handful of beams. This means the CNN\u2019s receptive field\nmust cover most of the laser, which makes it very prone to overfitting to the training scenes\u2019 backgrounds.\nTo circumvent this problem, and at the same time make use of the real-world scale information that laser data provides, we propose to evaluate the CNN in a depth-guided slidingwindow fashion. This means that we preprocess the data such that objects have approximately the same representation at every distance, thus alleviating the need to implicitly learn completely different representations at varying distances: Around each beam, we cut out a window of real-world extent `, thus spanning an angle \u03b1 = 2 sin\u22121( `2r ) and containing a variable amount of measurements depending on the distance r at which the current beam hits an obstacle. We then resample the measurements inside this window linearly at n fixed samples. When applied to such a window, the network\u2019s receptive field always covers the same real-world extent, regardless of the distance r. In addition, we center the window around the current point, clamp any values outside a \u00b1Hr hull in order to remove distant clutter and finally project the values into [\u22121, 1]. In total, this means max(\u2212Hr,min(x \u2212 r,Hr))/Hr is applied to each point x of the window around the point at depth r, as illustrated in Fig. 1. A detailed analysis regarding which of these preprocessing operations contribute the most to DROW\u2019s performance can be found in Section III-E."}, {"heading": "B. Prediction", "text": "For each window, and thus for each laser point with its context, a CNN both classifies whether this window belongs to an object class of interest through a SoftMax output and, if so, votes for the center location of that object through a regression output. As the 2D range data is inherently rotationinvariant, we do not want to perform voting in absolute (x, y) coordinates. Instead, we learn offsets (\u2206x,\u2206y) in a coordinate system centered and aligned at the current laser point, as shown in Fig. 1.\nC. Voting and Non-Maximum Supression\nThe predictions for every window need to be consolidated into detection centers. This is achieved by making the CNN\u2019s predictions vote into regular grids spanning the laser\u2019s field of view. Let p(O|w) = \u2211 c\u2208C p(c|w) be the total probability of the window w seeing an object of interest, where C are the classes to be detected.\nIf p(O|w) exceeds a predefined voting threshold T , the window will cast a vote into a class-agnostic grid with weight p(O|w) as well as into each class-specific grid with weight p(c|w). After all windows have potentially cast votes, each\ngrid is blurred with a Gaussian filter and non-maximum suppression is performed on the class-agnostic grid. For each maximum found in that grid, a detection is predicted at the cell\u2019s center using the class which has the highest sum of votes in said cell. The reason for this voting scheme, as opposed to treating each class separately, is to avoid detections of both classes at the same position. Fig. 2 shows an example of the different steps involved. Votes cast from the raw laser points in (a) are shown in (b); (c) - (e) show the three voting grids and (f) shows the two resulting detections."}, {"heading": "III. EXPERIMENTAL EVALUATION", "text": "For the evaluation of our approach we first introduce our new dataset. We outline the details of our training procedure and of the evaluation methods. After the general evaluation of DROW and a comparison to baselines, we perform several ablation studies to show how much individual parts of our detector contribute to the overall performance."}, {"heading": "A. Dataset", "text": "Although Weinrich et al. [34] provide their datasets, their recordings are limited to scenes with a single wheelchair or walker. While this might suffice for learning the few parameters of fairly constrained features and classifiers, we want to learn features from scratch and thus require more general and varied data. This is why we recorded a little over 10 hours of data at an elderly care facility.\nthe video streams for privacy\nreasons. The software infrastructure is based on ROS and all data was stored in rosbags. To ensure seeing enough wheelchairs and walkers, one person permanently roamed around using either while the robot was recording. Our recordings consist of both (1) natural scenes where we recorded the everyday life in the facility including all kinds of clutter such as flower pots, chairs, furniture, rolling beds, . . . , and (2) artificial recordings where we drove around in certain patterns as to bring variation to the dataset. Apart from the resident\u2019s wheelchairs, we included as many wheelchair and walker models as possible, including one motorized wheelchair.\n2) Dataset Statistics: We split the resulting dataset into a train, validation and test set. To create these sets, we split the care facility into four non-overlapping areas, three of which were assigned to the train, test and validation sets, respectively. The fourth, the entrance hall, was split into temporally disjoint sequences which were distributed over the train and validation sets. Based on this split, we can show how well the approach generalizes to never before seen areas. Table I shows an overview of our dataset, as well as statistics of the subset we annotated. The wheelchair and walker counts refer to individual detections as opposed to instances, and the bar plots show their distribution over the distance. Each bar represents a 1 m slice in the distance (15 bars for up to 15 m), clearly showing that the majority of observed mobility aids are encountered within 1 m to 6 m of the robot.\n3) Annotation: We annotate our data with wheelchair and walker centers. Since the dataset contains 464k raw laser scans, we devise an annotation scheme that keeps effort manageable while still covering the full extent of the sequences as well as allowing temporal approaches to be developed in the future. Instead of all the laser scans, we annotate small batches throughout every sequence as follows: A batch consists of 100 frames, out of which we annotate every 5th frame, resulting in 20 annotated frames per batch.1 Within a sequence, we only annotate every 4th batch, leading to a total of 5 % of the laser scans being annotated. We wrote a custom annotation tool based on matplotlib [11], which\n1This allows experimenting with interpolation between the annotations, even though we didn\u2019t do so in this work.\nloads sequences of scans with corresponding RGB images from the head camera and automatically finds the batches that need to be annotated. To aid the user in annotation, we show the first, current, and last image of the current batch. An example view of this annotation tool can be seen in Fig. 4. A 1.2 m circle around the mouse pointer, indicating the average wheelchair size, helps the user click on locations of wheelchair or walker centers. By using all of this supportive information to get an understanding of the scene, we were able to annotate most mobility aids, but based on the limited camera view, we likely still missed a few."}, {"heading": "B. Training Procedure", "text": "We train a CNN which, for each preprocessed window, predicts whether it is indicative of a nearby wheelchair or walker and, if it is, predicts the offset of its center. We do so in a single pass by using a network with two output layers: a three-way SoftMax differentiating between background, wheelchair and walker, and a two-dimensional linear regression output. We optimize the network by minimizing the sum of a negative log-likelihood criterion on the SoftMax output and a root-mean-square error on the regression output. The regression targets are computed as (\u2206x,\u2206y) in each window\u2019s local coordinate system as shown in Fig. 1. The class-labels are determined by the type of the closest detection to the window\u2019s center-point, with a maximal Euclidean distance of 0.6 m for wheelchairs and 0.4 m for walkers. When there is no nearby annotation, no error is backpropagated for the regression output and the network is thus free to predict any offset.\nThe architecture of our network, inspired by the popular VGGnet [27]2, is as follows: Conv 5@64, Conv 5@64, Max 2, Conv 5@128, Conv 3@128, Max 2, Conv 5@256, Conv 3@5, where Convn@c represents a convolution with c filters of size n and Max p the maximum operation on a window of p values. Batch-normalization [12], dropout [31] of 0.25, and ReLU nonlinearities [9] are applied between all convolutional layers. For an input window resampled to 48 values, the network outputs a vector of length five, three of which are sent to a SoftMax and the other two are the\n2VGG is a well-performing architecture in vision. Other architectures may perform better, but that\u2019s beyond the scope of this paper.\nregression outputs \u2206x and \u2206y. All weights are initialized similarly to the scheme proposed in [26], except for those of the output layer, which are initialized to 0. For training the network, we use the AdaDelta optimizer with \u03c1 = 0.95 and = 10\u22127 and train until approximate convergence.3 During training, we add small multiplicative random noise to the regression targets and we flip each window and its target with probability 0.5. We implemented our CNN in Theano [2].\nFor the voting scheme, we add multiplicative class-weights to the votes and re-normalize them, thus voting for class c with wcp(c|w)\u2211\ni\u2208C wip(i|w) instead of p(c|w). We then optimize\nall voting hyper-parameters (class-weights w, grid resolution b and blur-size \u03c3) using hyperopt [4] to maximize maxT fwheelchair(T ) + fwalker(T ) on our validation set, fc(T ) being the F1-score of class c using detection threshold T . Interestingly, the best values, wbg = 0.38, wwheelchair = 0.60, wwalker = 0.49, b = 0.1 m and \u03c3 = 2.93 are not far off from our initial guesses."}, {"heading": "C. Approach Evaluation", "text": "We trained our model on our training set and computed hyperparameters on our validation set as described in the previous section. In order to evaluate real-world performance of DROW, we now look at the precision and recall curves on our own test set and on the publicly available Reha test set of [34]4 recorded with a similar robot. Recall that our test set was recorded in a never before seen part of the care facility, thus a method which learns location priors or background models will fail. The result shown in Fig. 5 demonstrates that DROW generalizes very well and has significantly higher precision and recall than Gandalf [34], both on our test set and on the Reha test set.\nIn our application scenario, the actual detection of a mobility aid is more important than their correct classification, hence we also evaluate a class-agnostic performance. For this, we ignore the class of all detections and annotations during evaluation.\n3This means that we started training in the evening and stopped it the next morning. The loss-curve was almost flat in log-space.\n4Detection annotations for the Reha dataset were provided by the authors. We skip the Home test set as it does not contain mobility aids.\nThe precision and recall curves for each class are computed by varying the voting threshold T . We use an evaluation radius of 0.5 m, meaning a detection is matched to a ground-truth if it falls within 0.5 m of that groundtruth\u2019s center and has the correct class. An annotation can be matched with at most one detection, all remaining detections of that class are false positives and all unmatched annotations of that class are false negatives. To see how well localized DROW\u2019s detections are, we show precision-recall curves for varying evaluation radii in Fig. 6. The cluttering of all curves for acceptance radii of 0.3 m and above mean that our detections are well localized. During annotation, it became clear to us that the precision of the annotations is limited, meaning that evaluation at radii as small as 0.1 m is strongly affected by labeling noise.\nWe also analyze how well DROW behaves with respect to the distance from the laser scanner. For this, we start by ignoring all detections beyond 0.1 m from the laser. We then slowly grow that radius, taking into account more and more detections, and plot how the precision and recall at a fixed threshold evolve in Fig. 7. DROW performs very well across the board, but especially so in mid-range which is crucial for navigation and planning. Unsurprisingly, the curves do not change much beyond a distance of 10 m, as only few mobility aids where observed that far."}, {"heading": "D. Baselines", "text": "We compare our proposed method to two baselines in Fig. 5: the publicly available Gandalf detector [34] and a naive deep learning baseline.\nNone of the precision-recall curves in [34] quantify Gandalf\u2019s actual detection performance, they only focus on parts of the system independently. To obtain comparable detection precision-recall values, we evaluate Gandalf5 using the evaluation protocol described above. As provided, the Gandalf detector has no single threshold to be tuned and thus results in a single point in our plots. For the classagnostic case, we plot the results for the cases when Gandalf person detections are kept ( ) and discarded ( ), respectively, showing a trade-off between precision and recall.\n5https://github.com/neurob/gandalf_detector\nhistogram ( , y axis on the right) shows the percentage of all annotations at a certain range.\nNote that the detector performance we obtained with the code from [34] is significantly lower than the one reported in the original paper. The performance could be improved by an additional covariance-based merging step briefly mentioned in [34] which is, however, neither described in detail in the paper, nor included in the provided detector code. We were thus not able to reproduce the results presented in [34]. However, extrapolating from the false positives and missing detections we observed using the provided code, even an optimistic bound on this method\u2019s performance would place it below our detector\u2019s curve.\nAs a naive deep learning baseline, we train another CNN, similarly to YOLO [24], that directly predicts up to two detections (sufficient for 95.0 % of the frames) in normalized (x, y) coordinates, based on a full scan. The results of this baseline experiment are shown as dotted lines in Fig. 5 (a), but are missing from the Reha test set as the scans have a different amount of laser beams. It should be noted that we spent a considerable amount of time getting the naive CNN baseline to work as well as possible and follow best-practice: carefully chosen receptive-fields, batch-normalization, careful initialization, AdaDelta optimizer, etc. However, as can easily be seen, just applying deep learning to 2D range data in such a naive way leads to abysmal results."}, {"heading": "E. Ablation Studies", "text": "In this section, we analyze how each of our design decisions affects detection by systematically removing or substituting each part that defines our approach. Fig. 8 summarizes these experiments. Overall, the complete DROW detector ( ) outperforms all other configurations, i.e. each single decision contributes to the high performance. For each of the following experiments, we retrain the full network from scratch.\n1) Preprocessing: Our depth preprocessing can be dissected into three operations: centering, clamping and resampling. Not centering the input window ( ) has the smallest, albeit non-negligible, effect. Removing the clamping ( ) hurts overall performance as much as removing the centering. Without the resampling step ( ) which ensures that the network\u2019s receptive field keeps a constant real-world\nsize, performance drops significantly, especially for walkers. Considering that the CNN has to learn completely different representations across different distances, the overall performance is still surprisingly good. The fact that walkers are almost undetected and yet the agnostic performance is decent suggests a high confusion. Finally, dropping all of the above steps ( ) performs the worst in all measures, showing that our preprocessing is indeed crucial.\nIn total, all these experiments clearly suggest that each of the three preprocessing steps are important.\n2) Voting: The comparison to the naive deep learning baseline in Fig. 5a already showed that voting is essential for proper detection results.\nIn order to see the effect of regressing local offsets (\u2206x,\u2206y), we train a version of the network which regresses (\u2206\u03d5, r): an angular offset and an absolute distance to the laser scanner ( ). For this experiment, we also had to remove centering (c.f . ), as otherwise r is impossible to predict. Without centering, the network could in principle learn to \u201cpass through\u201d the centerpoint\u2019s distance and add it as a bias in the last layer. The drastically degraded results suggest that this is a much more difficult problem that the learning procedure fails to solve.\nTo ensure that this difficulty is not caused by a bad interaction with our preprocessing, we also train a network to make these predictions on raw, unprocessed data ( ). This model performing worst of all shows that even a plain voting-based network alone does not perform well if no care is taken on both the input and output.\n3) Model: Since every window is sent through the model individually, and all windows have the same size due to the resampling step, there is no inherent reason for the model to\nbe a CNN. To show that the spatial prior encoded in a CNN is useful, we train a three hidden-layer perceptron ( ) with 2048 hidden units each, ReLU non-linearities, dropout, and batch-normalization. As in the other experiments, we leave all other design decisions unchanged. The result shows that, although the MLP is a viable alternative, the CNN clearly outperforms it.\nAs an additional baseline we trained a regression forest ( ) that regresses the three class probabilities and the local offsets (\u2206x,\u2206y). It was trained with the same training data as the CNN and the MLP (including the flip augmentation). We used the scikit-learn [23] implementation of the regression forest with all default settings and 50 trees. Training took 7.5 hours (using 5 threads) and resulted in a model of 6.9GB compared to our CNN model of 1.1MB. Nevertheless, the forest model performs even slightly worse than the MLP in all cases.\n4) Window Size: So far, all evaluations have been performed with a real-world window spanning ` = 1.66 m, motivated by the typical extent of wheelchairs being 1.2 m. To verify that this is a reasonable choice, we trained a network on both larger (2.20 m) and smaller (1.10 m) windows. The results of this experiment can be seen in Fig. 9. The classagnostic prediction performance is best for our choice of ` = 1.66 m, but wheelchairs alone could be detected considerably better by using a window of 2.2 m. These results suggest that further improvements might be obtained by providing the network with multiple scales of input windows."}, {"heading": "IV. RELATED WORK", "text": "Most related to our approach is that of Weinrich et al. [34] who propose a distance invariant feature for laser based detection. They detect wheelchairs, walkers, and persons in 2D range data by first detecting larger jumps in the distance. Once such a jump is found, they create a window with a fixed real-world extent, covering subsequent scan points. The window is discretized into equally sized segments, for each of which a clamped distance relative to the window depth is computed. Each segment is characterized by the min, max and average depth which are concatenated for all segments to form the feature vector. These are then classified by an AdaBoost classifier. While their feature design has a high similarity to our depth preprocessing step, their windows are only created when a jump in distance is found, where we\ncreate a window for every scan point. The biggest difference, however, is that they directly predict a detection center and object class for every window, while we vote for centers, a step in our detector which we have shown to be essential.\nA few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work. The solution of creating a re-sampled depth image from a 3D point-cloud by ray-tracing [7], however, involves multiple significantly more complex operations than our proposed simple pre-processing and no timings were provided. The other commonly-used solution is the creation of a 3D occupancy-grid [14], [17]. Such an approach effectively uses N + 1-dimensional inputs for data on an N - dimensional manifold, while our approach keeps the input N -dimensional. Arguably, the latter makes learning easier and predictions more robust [3]. In addition, multiple different heuristics exist for \u201cfilling holes\u201d in those occupancygrids [17], whereas our pre-processing doesn\u2019t produce holes in the first place. In the end, both data representations may be made to work, but we believe our lower-dimensional representation is simpler and more effective.\nOthers have used voting for detection in laser scans, Mozos et al. [20] in a multi-height laser setup and Spinello et al. [29] in a layered fashion based on 3D range data. Both, related to [15], learn shape models from data and cast votes from the different laser layers to detection centroids. However, similar to Weinrich et al. [34], they rely on jump distance segmentation and only the segments can cast votes for detections. In [33], Wang et al. prove that detection using a voting scheme corresponds to sliding-window detection on a sparse grid for linear models. While they achieve good detections on point clouds using hand-crafted features and a linear SVM, it has been shown time and again in the computer vision literature (and in this paper) that learned non-linear features and classifiers outperform handcrafted features and linear classifiers by a large margin. In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36]. It would thus be interesting to establish a relationship between deep, non-linear voting detectors such as DROW and sliding-window detectors on sparse inputs as shown in [33].\nDetection in range data is not limited to persons or mobility aids. Around the time at which we uploaded the preprint6 of this paper, Ondruska et al. [22] shows how to do \u201ctracking\u201d of pedestrians, cyclists, buses, cars, and road obstacles in 2D range data. Although seemingly similar to our work, their input is an occupancy-grid (i.e. N + 1- dimensional) for which they predict a labelled occupancygrid, as opposed to discrete detections or tracks with IDs. Based on the biases for static grid cells in their RNN, it remains to be seen how well their approach works on a mobile robot.\nMerdrignac et al. [18] also detect cars, bicyclists and static road obstacles in 2D range data. They hypothesize that more\n6https://arxiv.org/abs/1603.02636v1\ninformation can be extracted from range data than done before and aim to achieve this by designing a large set of hand-crafted features. We agree, but we instead learn feature representations from the data directly."}, {"heading": "V. DISCUSSION", "text": "A few interesting aspects of our detector should be highlighted. Firstly, given the experimental evaluation it becomes clear that we perform well with respect to wheelchairs, but our walker performance leaves some room for improvement. One reason for this could be the fact that our training set is rather biased towards wheelchairs. However, the classagnostic precision-recall curves are very similar to the ones of wheelchairs in most of our experiments. This suggests that one significant problem for walker detection is a high confusion with wheelchairs. The fact that whenever DROW detects a mobility aid of either class, it is well localized, leads us to the conclusion that the network \u201cunderstood\u201d walkers but needs to see more of them.\nSecondly, laser-based detection is often dismissed as too sparse or too difficult in a single frame. We hope our results refute this fear, even though we do not dismiss the performance gain that could likely be achieved by tracking our detections over time.\nA commonly suggested alternative to laser-based detections is the use of a vision-based detector and RGB(-D) cameras, which produce much richer data. Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios. They typically rely on geometric primitives [21] or very specific camera setups [6]. Furthermore, commonly used RGB-D sensors such as the Asus Xtion, mounted on a human sized robot, have a too narrow field of view to perceive wheelchairs both far away, as well as when they come close to the robot. Vision-based detectors would thus need many cameras to cover the same field of view that is covered by a single laser scanner. Additionally, since mobility aids themselves come in many different appearances, training such a detector robustly would require a vastly larger dataset. Thus, a laserbased detector, as we have shown to be feasible here, is the more effective and efficient approach."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper we introduced the DROW detector, a fast deep learning based detector for wheelchairs and walkers from 2D range data. We have proposed a depth preprocessing and a voting scheme, both of which enable CNNs to vastly outperform naive CNN detection baselines and obtain state of the art results compared to a previous method. We performed a thorough experimental evaluation, justifying all our major design choices. To achieve all this, we recorded a large dataset in which we annotated wheelchair and walker centroids, and which we believe will enable further research. We are convinced that our detector generalizes to other classes given training data and will thus be useful to the community. Upon acceptance of the paper, we will publish our code, including a ROS node and the annotated dataset."}], "references": [{"title": "Using Boosted Features for the detection of People in 2D Range Data", "author": ["Kai O Arras", "\u00d3scar Mart\u0131\u0301nez Mozos", "Wolfram Burgard"], "venue": "In ICRA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures", "author": ["James Bergstra", "Daniel Yamins", "David Daniel Cox"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Biternion Nets: Continuous Head Pose Regression from Discrete Training Labels", "author": ["Lucas Beyer", "Alexander Hermans", "Bastian Leibe"], "venue": "In GCPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Robotic Assistance: an Automatic Wheelchair Tracking and Following Functionality by Omnidirectional Vision", "author": ["Cyril Cauchois", "Fabrice De Chaumont", "Bruno Marhic", "Laurent Delahoche", "M\u00e9lanie Delafosse"], "venue": "In IROS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Unsupervised Feature Learning for Classification of Outdoor 3D Scans", "author": ["Mark De Deuge", "A Quadros", "C Hung", "B Douillard"], "venue": "ACRA,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Real-time Multisensor People Tracking for Human-Robot Spatial Interaction", "author": ["Christian Dondrup", "Nicola Bellotto", "Ferdian Jovan", "Marc Hanheide"], "venue": "In ICRA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Sparse Rectifier", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Neural Networks. In AISTATS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Wheelchair detection using cascaded decision", "author": ["Chun-Rong Huang", "Pau-Choo Chung", "Kuo-Wei Lin", "Sheng-Chieh Tseng"], "venue": "tree. T-ITB,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Matplotlib: A 2D graphics environment", "author": ["J.D. Hunter"], "venue": "CS&E, 9(3):90\u2013 95", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Unsupervised Feature Learning for 3D Scene Labeling", "author": ["Kevin Lai", "Liefeng Bo", "Dieter Fox"], "venue": "In ICRA,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Robust Object Detection with Interleaved Categorization and Segmentation", "author": ["Bastian Leibe", "Ale\u0161 Leonardis", "Bernt Schiele"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "SSD: Single Shot MultiBox Detector", "author": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott Reed"], "venue": "arXiv preprint arXiv:1512.02325,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition", "author": ["Daniel Maturana", "Sebastian Scherer"], "venue": "In IROS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "2D Laser Based Road Obstacle Classification for Road Safety Improvement", "author": ["Pierre Merdrignac", "Evangeline Pollard", "Fawzi Nashashibi"], "venue": "In ARSO,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound", "author": ["Fausto Milletari", "Seyed-Ahmad Ahmadi", "Christine Kroll", "Annika Plate", "Verena Rozanski", "Juliana Maiostre", "Johannes Levin", "Olaf Dietrich", "Birgit Ertl-Wagner", "Kai B\u00f6tzel"], "venue": "arXiv preprint arXiv:1601.07014,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Multi-Part People Detection Using 2D Range Data. IJSR", "author": ["Oscar Martinez Mozos", "Ryo Kurazume", "Tsutomu Hasegawa"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Wheelchair Detection in a Calibrated Environment", "author": ["Ashish Myles", "Niels Da Vitoria Lobo", "Mubarak Shah"], "venue": "In ACCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks", "author": ["Peter Ondruska", "Julie Dequaire", "Dominic Zeng Wang", "Ingmar Posner"], "venue": "In RSS, Workshop on Limits and Potentials of Deep Learning in Robotics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "JMLR, 12:2825\u20132830", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "You Only Look Once: Unified, Real-Time Object Detection", "author": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Hough Networks for Head Pose Estimation and Facial Feature Localization", "author": ["Gernot Riegler", "David Ferstl", "Matthias R\u00fcther", "Horst Bischof"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Looking at Vehicles on the Road: A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior Analysis. TITS", "author": ["Sayanan Sivaraman", "Mohan Manubhai Trivedi"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A Layered Approach to People Detection in 3D Range Data", "author": ["Luciano Spinello", "Kai Oliver Arras", "Rudolph Triebel", "Roland Siegwart"], "venue": "In AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Human Detection using Multimodal and Multidimensional Features", "author": ["Luciano Spinello", "Roland Siegwart"], "venue": "In ICRA,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1929}, {"title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation", "author": ["Jonathan J Tompson", "Arjun Jain", "Yann LeCun", "Christoph Bregler"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Voting for voting in online point cloud object detection", "author": ["Dominic Zeng Wang", "Ingmar Posner"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "People Detection and Distinction of their Walking Aids in 2D Laser Range Data based on Generic Distance-Invariant Features", "author": ["Christoph Weinrich", "Tim Wengefeld", "Christof Schroeter", "Horst- Michael Gross"], "venue": "RO-MAN,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Fast Line, Arc/Circle and Leg Detection from Laser Scan Data in a Player Driver", "author": ["Jodo Xavier", "Marquidia Pacheco", "Daniel Castro", "Ant\u00f3nio Ruano", "Urbano Nunes"], "venue": "In ICRA,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Deep Voting: A Robust Approach Toward Nucleus Localization in Microscopy Images", "author": ["Yuanpu Xie", "Xiangfei Kong", "Fuyong Xing", "Fujun Liu", "Hai Su", "Lin Yang"], "venue": "In MICCAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Many autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18].", "startOffset": 139, "endOffset": 142}, {"referenceID": 33, "context": "Many autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "Many autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "While early detection methods used simple heuristics such as fitting lines and circles [35], in the past few years hand crafted features, coupled with learned classifiers, have dominated laser based detection.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 51, "endOffset": 54}, {"referenceID": 29, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 174, "endOffset": 177}, {"referenceID": 27, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 201, "endOffset": 205}, {"referenceID": 29, "context": "seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking.", "startOffset": 172, "endOffset": 176}, {"referenceID": 19, "context": "seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking.", "startOffset": 205, "endOffset": 209}, {"referenceID": 28, "context": "seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking.", "startOffset": 211, "endOffset": 215}, {"referenceID": 33, "context": "[34] face a similar task and propose the Gandalf detector for detecting people, wheelchairs, and walkers in individual laser scans.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5].", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "In computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5].", "startOffset": 182, "endOffset": 186}, {"referenceID": 4, "context": "In computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 15, "context": "While CNN-based image-level detectors like MultiBox [16] and YOLO [24] could in principle be applied to a 2D laser scan, we found that doing so naively is not effective.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "While CNN-based image-level detectors like MultiBox [16] and YOLO [24] could in principle be applied to a 2D laser scan, we found that doing so naively is not effective.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "[34] provide their datasets, their recordings are limited to scenes with a single wheelchair or walker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We wrote a custom annotation tool based on matplotlib [11], which", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "(b) Performance on the Reha test set [34].", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "Gandalf [34]", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "5: Performance comparison of DROW, the Gandalf detector [34], and a naive deep learning baseline on two test sets (a) and (b).", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "The architecture of our network, inspired by the popular VGGnet [27]2, is as follows: Conv 5@64, Conv 5@64, Max 2, Conv 5@128, Conv 3@128, Max 2, Conv 5@256, Conv 3@5, where Convn@c represents a convolution with c filters of size n and Max p the maximum operation on a", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "Batch-normalization [12], dropout [31] of 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "Batch-normalization [12], dropout [31] of 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "25, and ReLU nonlinearities [9] are applied between all convolutional layers.", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "All weights are initialized similarly to the scheme proposed in [26], except for those of the output layer, which are initialized to 0.", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "We implemented our CNN in Theano [2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "We then optimize all voting hyper-parameters (class-weights w, grid resolution b and blur-size \u03c3) using hyperopt [4] to maximize maxT fwheelchair(T ) + fwalker(T ) on our validation set, fc(T ) being the F1-score of class c using detection threshold T .", "startOffset": 113, "endOffset": 116}, {"referenceID": 33, "context": "our own test set and on the publicly available Reha test set of [34]4 recorded with a similar robot.", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "precision and recall than Gandalf [34], both on our test set and on the Reha test set.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "5: the publicly available Gandalf detector [34] and a", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "None of the precision-recall curves in [34] quantify Gandalf\u2019s actual detection performance, they only focus on parts of the system independently.", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "Note that the detector performance we obtained with the code from [34] is significantly lower than the one reported in the original paper.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "The performance could be improved by an additional covariance-based merging step briefly mentioned in [34] which is, however, neither described in detail in the paper, nor included in the provided detector code.", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "We were thus not able to reproduce the results presented in [34].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "As a naive deep learning baseline, we train another CNN, similarly to YOLO [24], that directly predicts up to two detections (sufficient for 95.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "We used the scikit-learn [23] implementation of the regression forest with all default settings and 50 trees.", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "[34] who propose a distance invariant feature for laser based detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "A few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "A few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "A few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "image from a 3D point-cloud by ray-tracing [7], however, involves multiple significantly more complex operations than our proposed simple pre-processing and no timings were provided.", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "The other commonly-used solution is the creation of a 3D occupancy-grid [14], [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "The other commonly-used solution is the creation of a 3D occupancy-grid [14], [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Arguably, the latter makes learning easier and predictions more robust [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 16, "context": "In addition, multiple different heuristics exist for \u201cfilling holes\u201d in those occupancygrids [17], whereas our pre-processing doesn\u2019t produce holes in the first place.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "[20] in a multi-height laser setup and Spinello et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] in a layered fashion based on 3D range data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Both, related to [15], learn shape models from data and cast votes from the different laser layers to detection centroids.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "[34], they rely on jump distance segmentation and only the segments can cast votes for detections.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "In [33], Wang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36].", "startOffset": 136, "endOffset": 140}, {"referenceID": 35, "context": "In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36].", "startOffset": 142, "endOffset": 146}, {"referenceID": 32, "context": "It would thus be interesting to establish a relationship between deep, non-linear voting detectors such as DROW and sliding-window detectors on sparse inputs as shown in [33].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": "[22] shows how to do \u201ctracking\u201d of pedestrians, cyclists, buses, cars, and road obstacles in 2D range data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] also detect cars, bicyclists and static road obstacles in 2D range data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios.", "startOffset": 54, "endOffset": 57}, {"referenceID": 20, "context": "They typically rely on geometric primitives [21] or very specific camera setups [6].", "startOffset": 44, "endOffset": 48}, {"referenceID": 5, "context": "They typically rely on geometric primitives [21] or very specific camera setups [6].", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We introduce the DROW detector, a deep learning based object detector operating on 2D range data. Laser scanners are lighting invariant, provide accurate 2D range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser 2D range data has been dominated by hand-crafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a Convolutional Neural Network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2D range data, and propose a depth preprocessing step and a voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464k laser scans, out of which 24k were annotated.", "creator": "LaTeX with hyperref package"}}}