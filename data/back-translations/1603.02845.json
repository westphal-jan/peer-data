{"id": "1603.02845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2016", "title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "abstract": "In environments where only blank language data is available, language technology must be developed without transcriptions, pronunciation dictionaries, or language modeling text. A similar problem arises in modeling the acquisition of children's languages. In these cases, the categorical linguistic structure must be discovered directly from the voice tone. We present a novel, unattended Bayesian model that segments blank language and groups the segments into hypothetical groups of words, resulting in a complete unattended tokenization of the input language with respect to discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed dimensional acoustic vector space. The model implemented as a Gibbs sampler then forms a full-word acoustic model in that space while performing a segmentation together.", "histories": [["v1", "Wed, 9 Mar 2016 11:14:23 GMT  (1054kb,D)", "http://arxiv.org/abs/1603.02845v1", "11 pages, 8 figures; Accepted to the IEEE/ACM Transactions on Audio, Speech, and Language Processing"]], "COMMENTS": "11 pages, 8 figures; Accepted to the IEEE/ACM Transactions on Audio, Speech, and Language Processing", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["herman kamper", "aren jansen", "sharon goldwater"], "accepted": false, "id": "1603.02845"}, "pdf": {"name": "1603.02845.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings", "authors": ["Herman Kamper", "Sharon Goldwater"], "emails": ["aren@jhu.edu).", "sgwater@inf.ed.ac.uk)."], "sections": [{"heading": null, "text": "Index Terms\u2014unsupervised speech processing, word discovery, speech segmentation, word acquisition, unsupervised learning.\nI. INTRODUCTION\nGREAT advances have been made in speech recognition inthe last few years. However, most of these improvements have come from supervised techniques, relying on large corpora of transcribed speech audio data, texts for language modelling, and pronunciation dictionaries. For under-resourced languages, only limited amounts of these resources are available. In the extreme zero-resource case, only raw speech audio is available for system development. In this setting, unsupervised methods are required to discover linguistic structure directly from audio. Similar techniques are also necessary to model how infants acquire language from speech input in their native language.\nResearchers in the speech processing community have recently started to use completely unsupervised techniques to build zero-resource technology directly from unlabelled speech data. Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection. Few studies, however, have considered\nH. Kamper is with the School of Informatics, University of Edinburgh, UK (email: see http://www.kamperh.com).\nA. Jansen performed this work while with the Human Language Technology Center of Excellence at Johns Hopkins University, USA (email: aren@jhu.edu).\nS. J. Goldwater is with the School of Informatics, University of Edinburgh, UK (email: sgwater@inf.ed.ac.uk).\nHK is funded by a Commonwealth Scholarship. This work was supported in part by a James S. McDonnell Foundation Scholar Award to SG.\nan unsupervised system able to perform a full-coverage segmentation of speech into word-like units\u2014the goal of this paper. Such a system would perform fully unsupervised speech recognition, allowing downstream applications, such as queryby-example search and speech indexing (grouping together related utterances in a corpus), to be developed in a manner similar to when supervised systems are available.\nAnother community that would have significant interest in such a system is the scientific cognitive modelling community. Here, researchers are interested in the problems faced during early language learning: infants have to learn phonetic categories and a lexicon for their native language using speech audio as input [7]. In this community, unsupervised models have been developed that perform full-coverage word segmentation of data into a sequence of words, proposing word boundaries for the entire input. However, these models take transcribed symbol sequences as input, rather than continuous speech [8].\nA few recent studies [9]\u2013[12], summarized in detail in Section II-C, share our goal of full-coverage speech segmentation. Most of these follow an approach of phone-like subword discovery with subsequent or joint word discovery, working directly on the frame-wise acoustic speech features.\nThe model we present is a novel Bayesian model that jointly segments speech data into word-like segments and then clusters these segments, each cluster representing a discovered word type.1 Instead of operating directly on acoustic frames, our model uses a fixed-dimensional representation of whole segments: any potential word segment of arbitrary length is mapped to a fixed-length vector, its acoustic embedding. Because the model has no subword level of representation and models whole segments directly, we refer to the model as segmental.2 Using these fixed-dimensional acoustic embeddings, we extend the Bayesian segmentation model of Goldwater et al. [8] (which took symbolic input) to the continuous speech domain. In an evaluation on an unsupervised digit recognition task using the TIDigits corpus, we show that our model outperforms the unsupervised HMM-based model of Walter et al. [11], without specifying the vocabulary size and without relying on a UTD system for model initialization.\nThe main contribution of this work is to introduce a novel segmental Bayesian model for unsupervised segmentation and clustering of speech into hypothesized words\u2014an approach\n1\u2018Word type\u2019 refers to distinct words, i.e. the entries in a lexicon, while \u2018word token\u2019 refers to different realizations of a particular word.\n2 \u2018Segmental\u2019 is used here, as in [13], to distinguish approaches operating on whole units of speech from those doing frame-wise modelling. This is different from the traditional linguistic usage of \u2018segment\u2019 to refer to phone-sized units.\nc\u00a9 2016 IEEE\nar X\niv :1\n60 3.\n02 84\n5v 1\n[ cs\n.C L\n] 9\nM ar\n2 01\n6\nwhich is distinct from any presented before. Our preliminary work in this direction was presented in [14]. Here we present a complete mathematical description of the model and much more extensive experiments and discussion. In particular, we provide a thorough analysis of the discovered structures and model errors, investigate the effects of model hyperparameters, and discuss the challenges involved in scaling our approach to larger-vocabulary tasks."}, {"heading": "II. RELATED WORK", "text": "In the following we describe relevant studies from both the speech processing and cognitive modelling communities."}, {"heading": "A. Discovery of words in speech", "text": "Unsupervised term discovery (UTD), sometimes referred to as \u2018lexical discovery\u2019 or \u2018spoken term discovery\u2019, is the task of finding meaningful repeated word- or phrase-like patterns in raw speech audio. Most state-of-the-art UTD systems are based on the seminal work of Park and Glass [5], who proposed a method to find pairs of similar audio segments and then cluster them into hypothesized word types. The pattern matching step uses a variant of dynamic time warping (DTW) called segmental DTW, which allows similar sub-sequences within two vector time series to be identified, rather than comparing entire sequences as in standard DTW. Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].\nLike our own system, many of these UTD systems operate on whole-word representations, with no subword level of representation. However, each word is represented as a vector time series with variable dimensionality (number of frames), requiring DTW for comparisons. Since our own system uses fixed-dimensional word representations, we can define an acoustic model over these embeddings and make comparisons without requiring any alignment. In addition, UTD systems aim to find and cluster repeated, isolated acoustic segments, leaving much of the input data as background. In contrast, we aim for full-coverage segmentation of the entire speech input into hypothesized words."}, {"heading": "B. Word segmentation of symbolic input", "text": "Cognitive scientists have long been interested in how infants learn to segment words and discover the lexicon of their native language, with computational models seen as one way to specify and test particular theories (see [7], [8] for reviews). In this community, most computational models of word segmentation perform full-coverage segmentation of the data into a sequence of words. However, these models generally take phonemic or phonetic strings as input, rather than continuous speech.\nEarly word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17]. The model presented here is based on the non-parametric Bayesian approach of Goldwater et al. [8], which was shown to yield more accurate segmentations than previous work. Their approach learns a\nlanguage model over the tokens in its inferred segmentation, incorporating priors that favour predictable word sequences and a small vocabulary.3 The original method uses a Gibbs sampler to sample individual boundary positions; our own sampler is based on the later work of Mochihashi et al. [18] who presented a blocked sampler that uses dynamic programming to resample the segmentation of a full utterance at once.\nGoldwater et al.\u2019s original model assumed that every instance of a word is represented by the same sequence of phonemes; later studies [19]\u2013[21] proposed noisy-channel extensions in order to deal with variation in word pronunciation. Our model can also be viewed as a noisy-channel extension to the original model, but with a different type of channel model. In [19]\u2013 [21], variability is modeled symbolically as the conditional probability of an output phone given the true phoneme (so the input to the models is a sequence or lattice of phones), whereas our channel model is a true acoustic model (the input is the speech signal). As in the phonetic noisy channel model of [20], we learn the language model and channel model jointly."}, {"heading": "C. Full-coverage segmentation of speech", "text": "We highlight four recent studies that share our goal of fullcoverage word segmentation of speech.\nSun and Van hamme [9] developed an approach based on non-negative matrix factorization (NMF). NMF is a technique which allows fixed-dimensional representations of speech utterances (typically co-occurrence statistics of acoustic events) to be factorized into lower-dimensional parts, corresponding to phones or words [22]. To capture temporal information, Sun and Van hamme [9] incorporated NMF in a maximum likelihood training procedure for discrete-density HMMs. They applied this approach to an 11-word unsupervised connected digit recognition task using the TIDigits corpus. They learnt 30 unsupervised HMMs, each representing a discovered word type. They found that the discovered word clusters corresponded to sensible words or subwords: average cluster purity was around 85%. Although NMF itself relies on a fixed-dimensional representation (as our system does) the final HMMs of their approach still perform frame-by-frame modelling (as also in the studies below). Our approach, in contrast, operates directly on a fixed-dimensional representation of speech segments.\nChung et al. [10] used an HMM-based approach which alternates between subword and word discovery. Their system models discovered subword units as continuous-density HMMs and learns a lexicon in terms of these units by alternating between unsupervised decoding and parameter re-estimation. For evaluation, the output from their unsupervised system was compared to the ground truth transcriptions and every discovered word type was mapped to the ground truth label that resulted in the smallest error rate. This allowed their system to be evaluated in terms of unsupervised WER; on a four-hour Mandarin corpus with a vocabulary size of about 400, they achieved WERs around 60%.\n3 They experimented with learning either a unigram or bigram language model, and found that the proposed boundaries of both models were very accurate, but the unigram model proposed too few boundaries.\nLee et al. [12, Ch. 3], [23] developed a non-parametric hierarchical Bayesian model for full-coverage speech segmentation. Using adaptor grammars (a generalized framework for defining such Bayesian models), an unsupervised subword acoustic model developed in earlier work [24] was extended with syllable and word layers, as well as a noisy channel model for capturing phonetic variability in word pronunciations. When applied to speech from single speakers in the MIT Lecture corpus, most of the words with highest TF-IDF scores were successfully discovered, and Lee et al. showed that joint modelling of subwords, syllables and words improved term discovery performance. In [23], although unsupervised WER was not reported, the full-coverage segmentation of the system was evaluated in terms of word boundary F -score. As in these studies, we also follow a Bayesian approach. However, our model operates directly at the whole-word level instead of having a hierarchy of layers from words down to acoustic features. In addition, we evaluate on a small-vocabulary multispeaker corpus rather than large-vocabulary single-speaker data.\nThe work that is most directly comparable to our own is that of Walter et al. [11]. They developed a fully unsupervised system for connected digit recognition, also using the TIDigits corpus. As in [10], they followed a two-step iterative approach of subword and word discovery. For subword discovery, speech is partitioned into subword-length segments and clustered based on DTW similarity. For every subword cluster, a continuousdensity HMM is trained. Word discovery takes as input the subword tokenization of the input speech. Every word type is modelled as a discrete-density HMM with multinomial emission distributions over subword units, accounting for noise and pronunciation variation. HMMs are updated in an iterative procedure of parameter estimation and decoding. Eleven of the whole-word HMMs were trained, one for each of the digits in the corpus. Using a random initialization, their system achieved an unsupervised WER of 32.1%; using UTD [5] to provide initial word identities and boundaries, 18.1% was achieved. In a final improvement, the decoded output was used to train from scratch standard continuous-density whole-word HMMs. This led to further improvements by leveraging the well-developed HMM tools used for supervised speech recognition.\nThis study of Walter et al. shows that unsupervised multispeaker speech recognition on a small-vocabulary task is possible. It also provides useful baselines on a standard dataset, and gives a reproducible evaluation method in terms of the standard WER. Our model is comparable to Walter et al.\u2019s word discovery system before the refinement using a traditional HMM-GMM recognizer. We therefore use the results they obtained before refinement as baselines in our experiments. It would be possible to apply the same refinement step to our model, but we have not done so here.\nMost of the above studies perform explicit subword modelling, while our approach operates on fixed-dimensional embeddings of whole-word segments. We do not argue that the latter is necessarily superior, but rather see our approach as a new contribution; direct whole-word modelling has both advantages and disadvantages. On the positive side, it is often easier to identify cross-speaker similarities between words than between subwords [25], which is why most UTD systems focus\non longer-spanning patterns. And from a cognitive perspective, there is evidence that infants are able to segment whole words from continuous speech before phonetic contrasts in their native language have been fully learned [26], [27]. On the other hand, direct whole-word modelling in our approach makes it more difficult to explicitly include intermediate modelling layers (phones, syllables, morphemes) as Lee et al. did. Furthermore, our whole-word approach is completely reliant on the quality of the embeddings; in Section V we show that the embedding function we use deals poorly with short segments. Improved embedding techniques are the subject of current research [28] and it would be straightforward to replace the current embedding approach with any other (including one that incorporates subword modelling)."}, {"heading": "III. THE SEGMENTAL BAYESIAN MODEL", "text": "In our approach, any potential word segment (of arbitrary length) is mapped to a vector in a fixed-dimensional space RD. The goal of this acoustic word embedding procedure is that word instances of the same type should lie close together in this space. The different hypothesized word types are then modelled in this D-dimensional space using a Gaussian mixture model (GMM) with Bayesian priors. Every mixture component of the GMM corresponds to a discovered type; the component mean can be seen as an average embedding for that word. However, since the model is unsupervised, we do not know the identities of the true word types to which the components correspond.\nAssume for the moment such an ideal GMM exists. This Bayesian GMM is the core component in our overall approach, which is illustrated in Fig. 1(a). Given a new unsegmented unlabelled utterance of acoustic feature frames y1:M = y1,y2, . . . ,yM , the aim is to hypothesize where words start and end in the stream of features, and to which word type (GMM mixture component) every word segment belongs. Given a proposed segmentation hypothesis (Fig. 1(a) bottom), we can calculate the acoustic embedding vector for every proposed word segment (Fig. 1(a) middle), calculate a likelihood score for each embedding under the current GMM (Fig. 1(a) top), and obtain an overall score for the current segmentation hypothesis. The aim then is to find the optimal segmentation under the current GMM, which can be done using dynamic programming. In our model, we sample a likely segmentation with a dynamic programming Gibbs sampling algorithm using the probabilities we obtain from the Bayesian GMM. The result is a complete segmentation of the input utterance and a prediction of the component to which every word segment belongs.\nIn our actual model, the Bayesian GMM is built up jointly while performing segmentation: the GMM provides the likelihood terms required for segmentation, while the segmentation hypothesizes the boundaries for the word segments which are then clustered using the GMM. The GMM (details in Section III-B) can thus be seen as an acoustic model which discovers the underlying word types of a language, while the segmentation component (Section III-C) discovers where words start and end. Below we provide complete details of the model."}, {"heading": "A. Fixed-dimensional representation of speech segments", "text": "Our model requires that any acoustic speech segment in an utterance be embedded in a fixed-dimensional space. In principle, any approach that is able to map an arbitrary-length vector time series to a fixed-dimensional vector can be used. Based on previous results, we follow the embedding approach developed by Levin et al. [29], as summarized below.\nThe notation Y = y1:T is used to denote a vector time series, where each yt is the frame-level acoustic features (e.g. MFCCs). We need a mapping function f(Y ) that maps time series Y into a space RD in which proximity between mappings indicates similar linguistic content, so embeddings of word tokens of the same type will be close together. In [29], the mapping f is performed as follows. For a target speech segment, a reference vector is constructed by calculating the DTW alignment cost to every exemplar in a reference set Yref = {Yi}Nrefi=1. Applying dimensionality reduction to the reference vector yields the embedding in RD. Dimensionality reduction is performed using Laplacian eigenmaps [30].\nIntuitively, Laplacian eigenmaps tries to find an optimal nonlinear mapping such that the k-nearest neighbouring speech segments in the reference set Yref are mapped to similar regions in the target space RD. To embed an arbitrary segment Y which is not an element of Yref, a kernel-based out-of-sample extension is used [31]. This performs a type of interpolation using the exemplars in Yref that are similar to target segment Y .\nIn all experiments we use a radial basis function kernel:\nK(Yi, Yj) = exp\n{ \u2212 [DTW(Yi, Yj)] 2\n2\u03c32K\n} (1)\nwhere DTW(Yi, Yj) denotes the DTW alignment cost between segments Yi and Yj , and \u03c3K is the kernel width parameter. In [31], it was shown that the optimal projection to the jth dimension in the target space is given by\nhj(Y ) = Nref\u2211 i=1 \u03b1 (j) i K(Yi, Y ) (2)\nThe \u03b1(j)i terms are the solutions to the generalized eigenvector problem (LK+ \u03beI)\u03b1 = \u03bbK\u03b1, with L the normalized graph Laplacian, K the Gram matrix with elements Kij = K(Yi, Yj) for Yi, Yj \u2208 Yref, and \u03be a regularization parameter. An arbitrary speech segment Y is then mapped to the embedding x \u2208 RD given by x = f(Y ) = [h1(Y ), h2(Y ), . . . , hd(Y )] T.\nWe have given only a brief outline of the embedding method here; complete details can be found in [29]\u2013[31]."}, {"heading": "B. Acoustic modelling: discovering word types", "text": "Given a segmentation hypothesis of a corpus (indicating where words start and end), the acoustic model needs to cluster the hypothesized word segments (represented as fixeddimensional vectors) into groups of hypothesized word types. Note again that acoustic modelling is performed jointly with word segmentation (next section), but here we describe the acoustic model under the current segmentation hypothesis. Formally, given the embedded word vectors X = {xi}Ni=1 from the current segmentation hypothesis, the acoustic model needs to assign each vector xi to one of K clusters.\nWe choose for the acoustic model a Bayesian GMM with fixed spherical covariance. This model treats its mixture weights and component means as random variables rather than point estimates as is done in a regular GMM. In [32] we showed that the Bayesian GMM performs significantly better in clustering word embeddings than a regular GMM trained with expectation-maximization. The former also fits naturally within the sampling framework of our complete model.\nThe Bayesian GMM is illustrated in Fig. 1(b). For each observed embedding xi, latent variable zi indicates the component to which xi belongs. The prior probability that xi belongs to component k is \u03c0k = P (zi = k). Given zi = k, xi is generated by the kth Gaussian mixture component with mean vector \u00b5k. All components share the same fixed covariance matrix \u03c32I; preliminary experiments, based on [32], indicated that it is sufficient to only model component means while keeping covariances fixed. Formally, the model is then defined as:\n\u03c0 \u223c Dir (a/K1) (3) zi \u223c \u03c0 (4)\n\u00b5k \u223c N (\u00b50, \u03c320I) (5) xi \u223c N (\u00b5zi , \u03c32I) (6)\nWe use a symmetric Dirichlet prior in (3) since it is conjugate to the categorical distribution in (4) [33, p. 171], and a sphericalcovariance Gaussian prior in (5) since it is conjugate to the Gaussian distribution in (6) [34]. We use \u03b2 = (\u00b50, \u03c3 2 0 , \u03c3\n2) to denote all the hyperparameters of the mixture components.\nGiven X , we infer the component assignments z = (z1, z2, . . . , zN ) using a collapsed Gibbs sampler [35]. Since we chose conjugate priors, we can marginalize over \u03c0 and {\u00b5k}Kk=1 and only need to sample z. This is done in turn\nfor each zi conditioned on all the other current component assignments:\nP (zi = k|z\\i,X ; a,\u03b2) \u221d P (zi = k|z\\i; a)p(xi|X\\i, zi = k, z\\i;\u03b2) (7)\nwhere z\\i is all latent component assignments excluding zi and X\\i is all embedding vectors apart from xi.\nBy marginalizing over \u03c0, the first term on the right hand side of (7) can be calculated as:\nP (zi = k|z\\i; a) = Nk\\i + a/K\nN + a\u2212 1 (8)\nwhere Nk\\i is the number of embedding vectors from mixture component k without taking xi into account [36, p. 843]. This term can be interpreted as a discounted unigram language modelling probability. Similarly, it can be shown that by marginalizing over \u00b5k, the second term\np(xi|X\\i, zi = k, z\\i;\u03b2) = p(xi|Xk\\i;\u03b2) (9) is the posterior predictive of xi for a Gaussian distribution with known spherical covariance and a conjugate prior over its means, which is itself a spherical covariance Gaussian distribution [34]. Here, Xk\\i is the set of embedding vectors assigned to component k without taking xi into account. Since the multivariate distributions in (5) and (6) have known spherical covariances, the probability density function (PDF) of the multivariate posterior predictive simply decomposes into the product of univariate PDFs; for a single dimension xi of vector xi, this PDF is given by\np(xi|Xk\\i) = N (xi|\u00b5Nk\\i , \u03c32Nk\\i + \u03c3 2) (10)\nwhere\n\u03c32Nk\\i = \u03c32\u03c320\nNk\\i\u03c320 + \u03c3 2\n, \u00b5Nk\\i = \u03c3 2 Nk\\i ( \u00b50 \u03c320 + Nk\\ixk\\i \u03c32 ) (11)\nand xk\\i is component k\u2019s sample mean for this dimension [34]. Although we use a model with a fixed number of components K, Bayesian models that marginalize over their parameters have been shown to prefer sparser solutions than maximumlikelihood models with the same structure [37]. Thus, our Bayesian GMM tends towards solutions where most of the data are clustered into just a few components, and we can find good minimally constrained solutions by setting K to be much larger than the expected true number of types and letting the model decide how many of those components to use."}, {"heading": "C. Joint segmentation and clustering", "text": "The acoustic model of the previous section can be used to cluster existing segments. Our joint segmentation and clustering system works by first sampling a segmentation of the current utterance based on the current acoustic model (marginalizing over cluster assignments for each potential segment), and then resampling the clusters of the newly created segments. The inference algorithm is a blocked Gibbs sampler using dynamic programming, based on the work of Mochihashi et al. [18].\nMore formally, given acoustic data {si}Si=1, where every utterance si consists of acoustic frames y1:Mi , we need to\nhypothesize word boundary locations and a word type (mixture component) for each hypothesized segment. X (si) denotes the embedding vectors under the current segmentation for utterance si. Pseudo-code for the blocked Gibbs sampler, which samples a segmentation utterance-wide, is given in Fig. 2. An utterance si is randomly selected; the embeddings from the current segmentation X (si) are removed from the Bayesian GMM; a new segmentation is sampled; and finally the embeddings from this new segmentation are added back into the Bayesian GMM.\nFor each utterance si a new set of embeddings X (si) is sampled in line 6 of Fig. 2. This is done using the forward filtering backward sampling dynamic programming algorithm [38]. Forward variable \u03b1[t] is defined as the density of the frame sequence y1:t, with the last frame the end of a word: \u03b1[t] , p(y1:t|h\u2212). The embeddings and component assignments for all words not in si, and the hyperparameters of the GMM, are denoted as h\u2212 = (X\\s, z\\s; a,\u03b2). To derive recursive equations for \u03b1[t], we use a variable qt to indicate the number of acoustic observation frames in the hypothesized word that ends at frame t: if qt = j, then yt\u2212j+1:t is a word. The forward variables can then be recursively calculated as:\n\u03b1[t] = p(y1:t|h\u2212) = t\u2211\nj=1\np(y1:t, qt = j|h\u2212)\n= t\u2211 j=1 p(yt\u2212j+1:t|h\u2212)p(y1:t\u2212j , qt = j|h\u2212)\n= t\u2211 j=1 p(yt\u2212j+1:t|h\u2212)\u03b1[t\u2212 j] (12)\nstarting with \u03b1[0] = 1 and calculating (12) for 1 \u2264 t \u2264M \u2212 1. The p(yt\u2212j+1:t|h\u2212) term in (12) is the value of a joint PDF over acoustic frames yt\u2212j+1:t. In a frame-based supervised setting, this term would typically be calculated as the product of the PDF values of a GMM (or prior-scaled posteriors of a deep neural network) for the frames involved. However, we work at a whole-word segment level, and our acoustic model is defined over a whole segment, which means we need to define this term explicitly. Let x\u2032 = f(yt\u2212j+1:t) be the word embedding calculated on the acoustic frames yt\u2212j+1:t (the hypothesized word). We then treat the term as:\np(yt\u2212j+1:t|h\u2212) , [ p ( x\u2032|h\u2212 )]j (13)\nThus, as in the frame-based supervised case, each frame is\nassigned a PDF score. But instead of having a different PDF value for each frame, all j frames in the segment yt\u2212j+1:t are assigned the PDF value of the whole segment under the current acoustic model. Another interpretation is to see j as a language model scaling factor, used to combine the continuous embedding and discrete unigram spaces. In initial experiments we found that without this factor, severe over-segmentation occurred. The marginal term in (13) can be calculated as:\np(x\u2032|h\u2212) = K\u2211 k=1 p(x\u2032, zh = k|X\\h, z\\h; a,\u03b2)\n= K\u2211 k=1 P (zh = k|z\\h; a)p(x\u2032|Xk\\h;\u03b2) (14)\nThe two terms in (14) are provided by the Bayesian GMM acoustic model, as given in equations (8) and (9), respectively.\nOnce all \u03b1\u2019s have been calculated, a segmentation can be sampled backwards [18]. Starting from the final positition t =M , we sample the preceding word boundary position using\nP (qt = j|y1:t, h\u2212) \u221d p(yt\u2212j+1:t|h\u2212)\u03b1[t\u2212 j] (15) We calculate (15) for 1 \u2264 j \u2264 t and sample while t\u2212 j \u2265 1.\nFig. 2 gives the complete sampler for our model, showing how segmentation and clustering of speech is performed jointly. The inner part of Fig. 2 is also illustrated in Fig. 1(a): lines 4 to 6 perform word segmentation which proceeds from top to bottom in Fig. 1(a), while lines 7 to 9 perform acoustic modelling which proceeds from bottom to top.\nD. Iterating the model\nAs explained in Section III-A, the fixed-dimensional embedding extraction relies on a reference set Yref. In [29], this set was composed of true word segments. In this unsupervised setting, we do not have such a set. We therefore start with exemplars extracted randomly from the data. Using this set, we extract embeddings and then run our sampler in an unconstrained setup where it is free to discover an order of magnitude more clusters than the true number of word types. From the biggest clusters discovered in this first iteration (those that cover 90% of the data), we extract a new exemplar set, which is used to recalculate embeddings. We repeat this procedure for a number of iterations, resulting in a refined exemplar set Yref."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Evaluation setup", "text": "We evaluate using the TIDigits connected digit corpus [39], which has a vocabulary of 11 English digits: \u2018oh\u2019 and \u2018zero\u2019 through \u2018nine\u2019. Using this simple small-vocabulary task, we are able to thoroughly analyze the discovered units and report results on the same corpus as several previous unsupervised studies [9], [11], [40], [41]. In particular, we use the recent results of Walter et al. [11] as baselines in our own experiments.\nTIDigits consists of an official training set with 112 speakers (male and female) and 77 digit sequences per speaker, and a comparable test set. Each set contains about 3 hours of speech. Our model is unsupervised, which means that the concepts of\ntraining and test data become blurred. We run our model on both sets separately\u2014in each case, unsupervised modelling and evaluation is performed on the same set. To avoid confusion with supervised regimes, we relabel the official TIDigits training set as \u2018TIDigits1\u2019 and the test set as \u2018TIDigits2\u2019. TIDigits1 was used during development for tuning hyperparameters (see Section IV-B); TIDigits2 was treated as unseen final test set.\nFor evaluation, the unsupervised decoded output of a system is compared to the ground truth transcriptions. From this comparison a mapping matrix G is constructed: Gij is the number of acoustic frames that are labelled as digit i in the ground truth transcript and labelled as discovered word type j by the model. We then use three quantitative evaluation metrics:\n\u2022 Average cluster purity: Every discovered word type (cluster) is mapped to the most common ground truth digit in that cluster, given by i\u2032 = argmaxiGij for cluster j. Average purity is then defined as the total proportion of the correctly mapped frames: \u2211 j maxiGij/ \u2211 i,j Gij . If the number of\ndiscovered types is more than the true number of types, more than one cluster may be mapped to a single ground truth type (i.e. a many-to-one mapping, as in [9]). \u2022 Unsupervised WER: Discovered types are again mapped, but here at most one cluster is mapped to a ground truth digit [11]. By then aligning the mapped decoded output from a system to the ground truth transcripts, we calculate WER = S+D+IN , with S the number of substitutions, D deletions, I insertions, and N the tokens in the ground truth. In cases where the number of discovered types is greater than the true number, some clusters will be left unassigned and counted as errors. \u2022 Word boundary F -score: By comparing the word boundary positions proposed by a system to those from forced alignments of the data (falling within 40 ms), we calculate word boundary precision and recall, and report the F -scores.\nWe consider two system initialization strategies, which were also used in [11]: (i) random initialization; and (ii) initialization from a separate UTD system. In the UTD condition, the boundary positions and cluster assignments for the words discovered by a UTD system can be used. Walter et al. used both the boundaries and assignments, while we use only the boundaries for initialization (we didn\u2019t find any gain by using the cluster identities as well). We use the UTD system of [6].\nAs mentioned in Section II-C, Walter et al. constrained their system to only discover 11 clusters (the true number). For our model we consider two scenarios: (i) in the constrained setting, we fix the number of components of the model to K = 15; (ii) in the unconstrained setting, we allow the model to discover up to K = 100 clusters. For the first, we use K = 15 instead of 11 since we found that more consistent performance is achieved when allowing some variation in cluster discovery. In the second setting, K = 100 allows the model to discover many more clusters than the true number of types. Since the Bayesian GMM is able to (and does) empty out some of its components (not all 100 clusters need to be used) this represents the case where we do not know vocabulary size upfront and the model itself is required to find a suitable number of clusters."}, {"heading": "B. Model implementation and hyperparameters", "text": "The hyperparameters of our model are set mainly based on previous work on other tasks [32]. However, some parameters were changed by hand during development. These changes were made based on performance on TIDigits1. Below, we also note the changes we made from our own preliminary work [14]. The hyperparameters we used in [14] led to far less consistent performance over multiple sampling runs: WER standard deviations were in the order of 9% absolute, compared to the deviations of less than 1% that we obtain in Section IV-C.\nFor the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u00b50, a = 1, \u03c3 2 = 0.005, \u03c320 = \u03c3 2/\u03ba0 and \u03ba0 = 0.05. Based on [29], [32] we use the following parameters for the fixed-dimensional embedding extraction (Section III-A): dimensionality D = 11, k = 30, \u03c3K = 0.04, \u03be = 2.0 and Nref = 8000. The embedding dimensionality for this smallvocabulary task is less than that typically used for other largervocabulary unsupervised tasks (e.g. D = 50 in [32]). In our preliminary work on TIDigits [14], we used D = 15 with Nref = 5000, but here we found that using D = 11 with a bigger reference set Nref = 8000 gave more consistent performance on TIDigits1. For embedding extraction, speech is parameterized as 15-dimensional frequency-domain linear prediction features [43] at a frame rate of 10 ms, and cosine distance is used as similarity metric in DTW alignments.\nAs in [32], embeddings are normalized to the unit sphere. We found that some embeddings were close to zero, causing issues in the sampler. We therefore add low-variance zero-mean Gaussian noise before normalizing: the standard deviation of the noise is set to 0.05 \u00b7 \u03c3E , where \u03c3E is the sample standard deviation of all possible embeddings. Changing the 0.05 factor within the range [0.01, 0.1] made little difference.\nIn Section III-D we explained that to find the reference set Yref for embedding extraction, we start with exemplars extracted randomly from the data, and then iteratively refine the set by using the decoded output from our model. In the first iteration we use Nref = 8000 random exemplars. In subsequent iterations, we use terms from the biggest discovered clusters that cover at least 90% of the data: we use the word tokens with the highest marginal densities as given by (14) in each of these clusters to yield 4000 discovered exemplars which we use in addition to 4000 exemplars again extracted randomly from the data, to give a total set of size Nref = 8000. We found that performance was more consistent when still using some random exemplars in Yref after the first iteration.\nTo make the search problem in Fig. 2 tractable, we require potential words to be between 200 ms and 1 s in duration, and we only consider possible word boundaries at 20 ms intervals. By doing this, the number of possible embeddings is greatly reduced. Although embedding comparisons are fast, the calculation of the embeddings is not, and this is the main bottleneck of our approach. In our implementation, all allowed embeddings are pre-computed. The sampler can then look up a particular embedding without the need to compute it on the fly.\nTo improve sampler convergence, we use simulated annealing [8], by raising the boundary probability in (15) to the\npower 1\u03b3 before sampling, where \u03b3 is a temperature parameter. We also found that convergence is improved by first running the sampler in Fig. 2 without sampling boundaries. In all experiments we do this for 25 iterations. Subsequently, the complete sampler is run for J = 25 Gibbs sampling iterations with 5 annealing steps in which 1\u03b3 is increased linearly from 0.01 to 1. In all cases we run 5 sampling chains in parallel [35], and report average performance and standard deviations."}, {"heading": "C. Results and analysis", "text": "Unconstrained model evaluation: As explained, we use our model to iteratively rediscover the embedding reference set Yref. Table I shows the performance of the unconstrained segmental Bayesian model on TIDigits1 as the reference set is refined. Random initialization is used throughout. Unconstrained modelling represents the most realistic setting where vocabulary size is not known upfront. Standard deviations were less than 0.3% absolute for all metrics.\nDespite being allowed to discover many more clusters (up to 100) than the true number of word types (11), the model achieves a WER of 35.4% in the first iteration, which improves to around 21% in iterations 3 and 4. Error rate increases slightly in iteration 5. Cluster purity over all iterations is above 86.5%, which is higher than the scores of around 85% reported by Sun and Van hamme [9]. Word boundary F -scores are around 70% over all iterations. As mentioned, the Bayesian GMM is biased not to use all of its 100 components. Despite this, none of the models empty out any of their components. However, most of the data is covered by only a few components: the last row in Table I shows that in the first iteration, 90% of the data is covered by the 20 biggest mixture components, while this number drops to 13 clusters in subsequent iterations.\nIn order to analyze the type of errors that are made, we visualize the mapping matrix G, which gives the number of frames of overlap between the ground truth digits and the discovered word types (Section IV-A). Fig. 3 shows the mappings for the 15 biggest clusters of the unconstrained models of iterations 3 and 5 of Table I, respectively.\nConsider the mapping in Fig. 3(a) for iteration 3. Qualitatively we observe a clear correspondence between the ground truth and discovered word types, which coincides with the high average purity of 89.2%. Apart from cluster 66, all other clusters overlap mainly with a single digit. Listening to cluster 66 reveals that most tokens correspond to [ay v] from the end of the digit \u2018five\u2019 and tokens of [ay n] from the end of the \u2018nine\u2019, both dominated by the diphthong. Correspondingly, most of the tokens in cluster 14 are the beginning [f ay] of \u2018five\u2019, while cluster 92 is mainly the beginning [n ay] of \u2018nine\u2019.\nThe digit \u2018eight\u2019 is split across two clusters: cluster 51 mainly contains \u2018eight\u2019 tokens where the final [t] is not pronounced, while in cluster 89 the final [t] is explicitly produced.\nTable I shows that performance deteriorates slightly in iteration 5. By comparing Figures 3(a) and (b), the source of the extra errors can be observed: overall the mapping in the fifth iteration (b) looks similar to that of the third (a), except the digit \u2018five\u2019 is now also partly covered by a third cluster (73). This cluster mainly contains beginning portions of \u2018five\u2019 and \u2018nine\u2019, again dominated by the diphthong [ay]. Cluster 62 in this case mainly contains tokens of the fricative [f] from \u2018five\u2019. Note that both WER and boundary F -score penalize the splitting of digits, although the discovered clusters correspond to consistent partial words. Below we discuss this issue further.\nOne might suspect from the analysis in Fig. 3 that some of the discovered word types are bi-modal, i.e. that when a single component of the Bayesian GMM contains two different true types (e.g. cluster 66 in Fig. 3(a), with tokens of both \u2018five\u2019 and \u2018nine\u2019), there might be two relatively distinct sub-clusters of embeddings within that component. However, this is not the case. Fig. 4 shows the embeddings of the discovered word types for a single speaker from the model in iteration 3 of Table I; embeddings are ordered and stacked by discovered type along the y-axis, with the embedding values coloured along\nthe x-axis. The embeddings for cluster 66 appear uni-modal, despite containing both [ay v] and [ay n] tokens; yet they are distinct from the embeddings in cluster 92 ([n ay] tokens) and cluster 14 ([f ay]). This analysis suggests that the model is finding sensible clusters given the embedding representation it has, and to consistently improve results we would need to focus on developing more discriminative embeddings.\nConstrained model evaluation and comparison: To compare with the discrete HMM-based system of Walter et al. [11], we use the exemplar set discovered in iteration 3 of Table I (using an unconstrained setup up to this point) and then constrain the Bayesian segmental model to 15 components. Table II shows WERs achieved on TIDigits1. Under random initialization, the constrained segmental Bayesian model performs 12.7% absolute better than the discrete HMM. When using UTD for initialization, the discrete HMM does better by 1.3% absolute. The WER of the third-iteration unconstrained model in Table I is repeated in the last row of Table II. Despite only mapping 11 out of 100 clusters to true labels, this unconstrained model still yields 10.6% absolute lower WER than the randomly-initialized discrete HMM with the correct number of clusters. By comparing rows 2 and 3, we observe that there is only a 2.1% absolute gain in WER by constraining the Bayesian model to a stricter number of types.\nGeneralization and hyperparameters: As noted in Section IV-B, some development decisions were made based on performance on TIDigits1. TIDigits2 was kept as unseen data up to this point. Using the setup developed on TIDigits1, we repeated exemplar extraction and segmentation separately on TIDigits2. Three iterations of exemplar refinement were used. Table III shows the performance of randomlyinitialized systems on both TIDigits1 and TIDigits2, with the performance on TIDigits1 repeated from Table II.\nTABLE III PERFORMANCE OF THE BAYESIAN SEGMENTAL MODEL ON TIDIGITS1 AND TIDIGITS2, WITH RANDOM INITIALIZATION\nModel TIDigits1 (%) TIDigits2 (%)\nWER Cluster purity Boundary F -score WER Cluster purity Boundary F -score\nConstrained segmental Bayesian 19.4\u00b1 0.3 88.4\u00b1 0.06 70.6\u00b1 0.2 13.2\u00b1 1.0 91.2\u00b1 0.2 76.7\u00b1 0.7 Unconstrained segmental Bayesian 21.5\u00b1 0.1 89.2\u00b1 0.1 71.8\u00b1 0.2 17.6\u00b1 0.2 92.5\u00b1 0.1 77.6\u00b1 0.3\n7 5 4 2 6 12 13 10 1 0 14 3 8 11 9 Cluster ID\none\ntwo\nthree\nfour\nfive\nsix\nseven\neight\nnine\noh\nzero\nG ro\nun d\ntr ut\nh ty\np e\nThis analysis and our previous discussion of Fig. 3 indicate that unsupervised WER is a particularly harsh measure of unsupervised word segmentation performance: the model may discover consistent units, but if these units do not coincide with whole words, the system will be penalized. This is also the case for word boundary F -score. Average cluster purity is less affected since a many-to-one mapping is performed; Table III shows that purity changes the least of the three metrics when moving from TIDigits1 to TIDigits2.\nIn a final set of experiments, we considered the effect of model hyperparameters. We found that performance is most sensitive to changes in the maximum number of allowed Gaussian components K and the component variance \u03c32. Fig. 6 shows the effect on WER when changing these hyperparameters.\n0.001 0.0025 0.005 0.01 0.02 0.05 0.1 Gaussian component variance \u03c32\n15\n20\n25\n30\n35\n40\n45\n50\n55\nW E\nR (%\n)\nK = 15 K = 25 K = 100\nFig. 6. WERs of the segmental Bayesian model on TIDigits1 as the number of Gaussian components K and variance \u03c32 is varied (log-scale on x-axis).\nResults are reasonably stable for \u03c32 in the range [0.0025, 0.02], with WERs below 25%. When allowing many components (K = 100) and using a small variance, as on the left of the figure, fragmentation takes place with digits being separated into several clusters. On the right side of the figure, where large variances are used, a few garbage clusters start to capture the majority of the data, leading to poor performance. The figure also shows that lower WER could be achieved by using a \u03c32 = 0.02 instead of 0.005 (which we used in the experiments above, based on [32]). The reason for the three curves meeting at this \u03c32 setting is that, for all three settings of K, more than 90% of the data are captured by only the 11 biggest clusters.\nWe similarly varied the target embedding dimensionality D using a constrained setup (K = 15), as shown in Fig. 7. For D = 6, garbage clusters start to capture the majority of the tokens at lower settings of \u03c32 than for D = 11 and D = 20. Much more stable performance is achieved in the latter two cases. The slightly worse performance of the D = 20 setting compared to the others is mainly due to a cluster containing the diphthong [ay], which is present in both \u2018five\u2019 and \u2018nine\u2019."}, {"heading": "V. CHALLENGES IN SCALING TO LARGER VOCABULARIES", "text": "We evaluated our system on a small-vocabulary dataset in order to compare to previous work and to allow us to thoroughly analyze the discovered structures. Our long-term aim (shared by many of the researchers mentioned in Section II-C) is to scale our system to more realistic multi-speaker corpora with larger vocabularies. Here we discuss the challenges in doing so.\nThe fixed-dimensional embedding calculations are the main bottleneck in our overall approach, since embeddings must be computed for each of the very large number of potential word segments. The embeddings also limit accuracy; one case in particular where the embedding function produces poor\nembeddings is for very short speech segments. An example is given in Fig. 8. The first embedding is from cluster 33 in Fig. 4, which is reliably mapped to the digit \u2018one\u2019. The bottom three embeddings are from short segments not overlapping with the true digit \u2018one\u2019, with respective durations 20 ms, 40 ms and 80 ms. Although these three speech segments have little similarity to the segments in cluster 33, Fig. 8 shows that their embeddings are a good fit to this cluster. This is possibly due to the aggressive warping in the DTW alignment of these short segments, together with artefacts from normalizing the embeddings to the unit sphere. This failure-mode is easily dealt with by setting a minimum duration constraint (Section IV-B), but again shows our model\u2019s reliance on accurate embeddings.\nTo scale to larger corpora, both the efficiency and accuracy of the embeddings would therefore need to be improved (see [28] for recent supervised efforts in this direction). More importantly, the above discussion highlights a shortcoming of our approach: the sampler considers potential word boundaries at any position, without regard to the original acoustics or any notion of a minimal unit. Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions. This implicitly defines a minimal unit: the pseudo-phones or pseudo-syllables segmented in the first pass. By using such a first-pass method in our system, the number of embedding calculations would greatly be reduced and it would provide a more principled way to deal with artefacts from short segments.\nAnother challenge when dealing with larger vocabularies is the choice of the number of clusters K. An upper-bound of K = 100, as we use for our unconstrained model, would not be sufficient for realistic vocabularies. However, the Bayesian framework would allow us to make our model non-parametric: the Bayesian GMM could be replaced by an infinite GMM [45] which infers the number of clusters automatically.\nFinally, in this study we made a unigram word predictability assumption (Section III-C) since the digit sequences do not have any word-word dependencies. However, in a realistic corpus, such dependencies will exist and could prove useful (even essential) for segmentation and lexicon discovery. In particular, [20] showed that for joint segmentation and clustering of noisy phone sequences, a bigram model was needed to improve\nWord embedding from cluster 33 (\u2192 one)\nEmbedding dimensions\nEmbeddings close to the above (non-word segments)\nFig. 8. Four embeddings from the same speaker as in Fig. 4: the top one is from cluster 33, the bottom three are from short non-word speech segments.\nclustering accuracy. Following [8], [18] it is mathematically straightforward to extend the algorithm of Section III-C to more complex language models. Exact computation of the extended model will be slow (e.g. the bigram extension of equation (14) requires marginalizing over the cluster assignment of both the current and preceding embeddings) but we anticipate that reasonable approximations will be possible (e.g. only marginalizing over a handful of the most probable clusters). The development of these extensions and approximations is an important part of our future work on larger vocabularies."}, {"heading": "VI. CONCLUSION", "text": "We introduced a novel Bayesian model, operating on fixed-dimensional embeddings of speech, which segments and clusters unlabelled continuous speech into hypothesized word units\u2014an approach which is very different from any presented before. We applied our model to a small-vocabulary digit recognition task and compared performance to a more traditional HMM-based approach of a previous study. Our model outperformed the baseline by more than 10% absolute in unsupervised word error rate (WER), without being constrained to a small number of word types (as the HMM was). Analysis showed that our model is reliant on the whole-word fixeddimensional segment representation: when partial words are consistently mapped to a similar region in embedding space, the model proposes these as separate word types. Most of the errors of the model were therefore due to consistent splitting of particular digits into partial-word clusters, or separate clusters for the same digit based on pronunciation variation. The model, however, is not restricted to a particular embedding method. Future work will investigate more accurate and efficient embedding approaches and unsupervised language modelling."}], "references": [{"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task at MediaEval 2012", "author": ["F. Metze", "X. Anguera", "E. Barnard", "M. Davel", "G. Gravier"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013 197, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.  ACCEPTED TO THE IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2016  11", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A Bayesian framework for word segmentation: Exploring the effects of context", "author": ["S.J. Goldwater", "T.L. Griffiths", "M. Johnson"], "venue": "Cognition, vol. 112, no. 1, pp. 21\u201354, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering linguistic structures in speech: Models and applications", "author": ["C.-y. Lee"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "SCARF: a segmental conditional random field toolkit for speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "Interspeech, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Fully unsupervised smallvocabulary speech recognition using a segmental Bayesian model", "author": ["H. Kamper", "S.J. Goldwater", "A. Jansen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient, probabilistically sound algorithm for segmentation and word discovery", "author": ["M.R. Brent"], "venue": "Mach. Learn., vol. 34, no. 1-3, pp. 71\u2013105, 1999.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning to segment speech using multiple cues: A connectionist model", "author": ["M.H. Christiansen", "J. Allen", "M.S. Seidenberg"], "venue": "Lang. Cognitive Proc., vol. 13, no. 2-3, pp. 221\u2013268, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A statistical model for word discovery in transcribed speech", "author": ["A. Venkataraman"], "venue": "Comput. Linguist., vol. 27, no. 3, pp. 351\u2013372, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling", "author": ["D. Mochihashi", "T. Yamada", "N. Ueda"], "venue": "Proc. ACL, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning a language model from continuous speech", "author": ["G. Neubig", "M. Mimura", "S. Mori", "T. Kawahara"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A joint learning model of word segmentation, lexical acquisition and phonetic variability", "author": ["M. Elsner", "S.J. Goldwater", "N. Feldman", "F. Wood"], "venue": "Proc. EMNLP, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised word segmentation from noisy input", "author": ["J. Heymann", "O. Walter", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering phone patterns in spoken utterances by non-negative matrix factorization", "author": ["V. Stouten", "K. Demuynck", "H. Van hamme"], "venue": "IEEE Signal Proc. Let., vol. 15, pp. 131\u2013134, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Mommy and me: familiar names help launch babies into speech-stream segmentation", "author": ["H. Bortfeld", "J.L. Morgan", "R.M. Golinkoff", "K. Rathbun"], "venue": "Psychol. Sci., vol. 16, no. 4, pp. 298\u2013304, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phonetic categories by learning a lexicon", "author": ["N.H. Feldman", "T.L. Griffiths", "J.L. Morgan"], "venue": "Proc. CCSS, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "arXiv preprint arXiv:1510.01032, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings", "author": ["H. Kamper", "A. Jansen", "S. King", "S.J. Goldwater"], "venue": "Proc. SLT, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution", "author": ["K.P. Murphy"], "venue": "2007. [Online]. Available: http://www.cs.ubc.ca/\u223cmurphyk/mypapers. html", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Gibbs sampling for the uninitiated", "author": ["P. Resnik", "E. Hardisty"], "venue": "University of Maryland, College Park, MD, Tech. Rep., 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S.J. Goldwater", "T.L. Griffiths"], "venue": "Proc. ACL, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian methods for hidden Markov models", "author": ["S.L. Scott"], "venue": "J. Am. Stat. Assoc., vol. 97, no. 457, pp. 337\u2013351, 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "A database for speaker-independent digit recognition", "author": ["R.G. Leonard"], "venue": "Proc. ICASSP, 1984.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1984}, {"title": "A computational model for unsupervised word discovery", "author": ["L. ten Bosch", "B. Cranen"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Pattern discovery in continuous speech using block diagonal infinite HMM", "author": ["N. Vanhainen", "G. Salvi"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparametric Bayesian alternative to spike sorting", "author": ["F. Wood", "M.J. Black"], "venue": "J. Neurosci. Methods, vol. 173, no. 1, pp. 1\u201312, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Frequency-domain linear prediction for temporal features", "author": ["M. Athineos", "D.P.W. Ellis"], "venue": "Proc. ASRU, 2003.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllable-like units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "Here, researchers are interested in the problems faced during early language learning: infants have to learn phonetic categories and a lexicon for their native language using speech audio as input [7].", "startOffset": 197, "endOffset": 200}, {"referenceID": 7, "context": "However, these models take transcribed symbol sequences as input, rather than continuous speech [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "A few recent studies [9]\u2013[12], summarized in detail in Section II-C, share our goal of full-coverage speech segmentation.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "A few recent studies [9]\u2013[12], summarized in detail in Section II-C, share our goal of full-coverage speech segmentation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "[8] (which took symbolic input) to the continuous speech domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11], without specifying the vocabulary size and without relying on a UTD system for model initialization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "2 \u2018Segmental\u2019 is used here, as in [13], to distinguish approaches operating on whole units of speech from those doing frame-wise modelling.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Our preliminary work in this direction was presented in [14].", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "Most state-of-the-art UTD systems are based on the seminal work of Park and Glass [5], who proposed a method to find pairs of similar audio segments and then cluster them into hypothesized word types.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "language, with computational models seen as one way to specify and test particular theories (see [7], [8] for reviews).", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "language, with computational models seen as one way to specify and test particular theories (see [7], [8] for reviews).", "startOffset": 102, "endOffset": 105}, {"referenceID": 14, "context": "Early word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "Early word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "Early word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "[8], which was shown to yield more accurate segmentations than previous work.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] who presented a blocked sampler that uses dynamic programming to resample the segmentation of a full utterance at once.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "\u2019s original model assumed that every instance of a word is represented by the same sequence of phonemes; later studies [19]\u2013[21] proposed noisy-channel extensions in order to deal with variation in word pronunciation.", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "\u2019s original model assumed that every instance of a word is represented by the same sequence of phonemes; later studies [19]\u2013[21] proposed noisy-channel extensions in order to deal with variation in word pronunciation.", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "In [19]\u2013 [21], variability is modeled symbolically as the conditional probability of an output phone given the true phoneme (so the input to the models is a sequence or lattice of phones), whereas our channel model is a true acoustic model (the input is the speech signal).", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [19]\u2013 [21], variability is modeled symbolically as the conditional probability of an output phone given the true phoneme (so the input to the models is a sequence or lattice of phones), whereas our channel model is a true acoustic model (the input is the speech signal).", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "As in the phonetic noisy channel model of [20], we learn the language model and channel model jointly.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "Sun and Van hamme [9] developed an approach based on non-negative matrix factorization (NMF).", "startOffset": 18, "endOffset": 21}, {"referenceID": 21, "context": "NMF is a technique which allows fixed-dimensional representations of speech utterances (typically co-occurrence statistics of acoustic events) to be factorized into lower-dimensional parts, corresponding to phones or words [22].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "To capture temporal information, Sun and Van hamme [9] incorporated NMF in a maximum likelihood training procedure for discrete-density HMMs.", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "[10] used an HMM-based approach which", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "3], [23] developed a non-parametric hierarchical Bayesian model for full-coverage speech segmentation.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "Using adaptor grammars (a generalized framework for defining such Bayesian models), an unsupervised subword acoustic model developed in earlier work [24] was extended with syllable and word layers, as well as a noisy channel model for capturing phonetic variability in word pronunciations.", "startOffset": 149, "endOffset": 153}, {"referenceID": 22, "context": "In [23], although unsupervised WER was not reported, the full-coverage segmentation of the system was evaluated in terms of word boundary F -score.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As in [10], they followed a two-step iterative approach of subword and word discovery.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "1%; using UTD [5] to provide initial word identities and boundaries, 18.", "startOffset": 14, "endOffset": 17}, {"referenceID": 24, "context": "On the positive side, it is often easier to identify cross-speaker similarities between words than between subwords [25], which is why most UTD systems focus on longer-spanning patterns.", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "And from a cognitive perspective, there is evidence that infants are able to segment whole words from continuous speech before phonetic contrasts in their native language have been fully learned [26], [27].", "startOffset": 195, "endOffset": 199}, {"referenceID": 26, "context": "And from a cognitive perspective, there is evidence that infants are able to segment whole words from continuous speech before phonetic contrasts in their native language have been fully learned [26], [27].", "startOffset": 201, "endOffset": 205}, {"referenceID": 27, "context": "Improved embedding techniques are the subject of current research [28] and it would be straightforward to replace the current embedding approach with any other (including one that incorporates subword modelling).", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "[29], as summarized below.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], the mapping f is performed as follows.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Dimensionality reduction is performed using Laplacian eigenmaps [30].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "To embed an arbitrary segment Y which is not an element of Yref, a kernel-based out-of-sample extension is used [31].", "startOffset": 112, "endOffset": 116}, {"referenceID": 30, "context": "In [31], it was shown that the optimal projection to the jth dimension in the target space is given by", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "We have given only a brief outline of the embedding method here; complete details can be found in [29]\u2013[31].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "We have given only a brief outline of the embedding method here; complete details can be found in [29]\u2013[31].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "In [32] we showed that the Bayesian GMM performs significantly better in clustering word embeddings than a regular GMM trained with expectation-maximization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "All components share the same fixed covariance matrix \u03c3I; preliminary experiments, based on [32], indicated that it", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "171], and a sphericalcovariance Gaussian prior in (5) since it is conjugate to the Gaussian distribution in (6) [34].", "startOffset": 112, "endOffset": 116}, {"referenceID": 34, "context": ", zN ) using a collapsed Gibbs sampler [35].", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "is the posterior predictive of xi for a Gaussian distribution with known spherical covariance and a conjugate prior over its means, which is itself a spherical covariance Gaussian distribution [34].", "startOffset": 193, "endOffset": 197}, {"referenceID": 33, "context": "(11) and xk\\i is component k\u2019s sample mean for this dimension [34].", "startOffset": 62, "endOffset": 66}, {"referenceID": 36, "context": "Although we use a model with a fixed number of components K, Bayesian models that marginalize over their parameters have been shown to prefer sparser solutions than maximumlikelihood models with the same structure [37].", "startOffset": 214, "endOffset": 218}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "This is done using the forward filtering backward sampling dynamic programming algorithm [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Once all \u03b1\u2019s have been calculated, a segmentation can be sampled backwards [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": "In [29], this set was composed of true word segments.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "We evaluate using the TIDigits connected digit corpus [39], which has a vocabulary of 11 English digits: \u2018oh\u2019 and \u2018zero\u2019 through \u2018nine\u2019.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 73, "endOffset": 77}, {"referenceID": 39, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 40, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "[11] as baselines in our own experiments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "a many-to-one mapping, as in [9]).", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "\u2022 Unsupervised WER: Discovered types are again mapped, but here at most one cluster is mapped to a ground truth digit [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "We consider two system initialization strategies, which were also used in [11]: (i) random initialization; and (ii) initialization from a separate UTD system.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "We use the UTD system of [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 31, "context": "The hyperparameters of our model are set mainly based on previous work on other tasks [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "Below, we also note the changes we made from our own preliminary work [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "The hyperparameters we used in [14] led to far less consistent performance over multiple sampling runs: WER standard deviations were in the order of 9% absolute, compared to the deviations of less than 1% that we obtain in Section IV-C.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "For the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u03bc0, a = 1, \u03c3 2 = 0.", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": "For the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u03bc0, a = 1, \u03c3 2 = 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 41, "context": "For the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u03bc0, a = 1, \u03c3 2 = 0.", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "Based on [29], [32] we use the following parameters for the fixed-dimensional embedding extraction (Section III-A): dimensionality D = 11, k = 30, \u03c3K = 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "Based on [29], [32] we use the following parameters for the fixed-dimensional embedding extraction (Section III-A): dimensionality D = 11, k = 30, \u03c3K = 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "D = 50 in [32]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "In our preliminary work on TIDigits [14], we used D = 15 with Nref = 5000, but here we found that using D = 11 with a bigger reference set Nref = 8000 gave more consistent performance on TIDigits1.", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "For embedding extraction, speech is parameterized as 15-dimensional frequency-domain linear prediction features [43] at a frame rate of 10 ms, and cosine distance is used as similarity metric in DTW alignments.", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "As in [32], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "To improve sampler convergence, we use simulated annealing [8], by raising the boundary probability in (15) to the power 1 \u03b3 before sampling, where \u03b3 is a temperature parameter.", "startOffset": 59, "endOffset": 62}, {"referenceID": 34, "context": "In all cases we run 5 sampling chains in parallel [35],", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "5%, which is higher than the scores of around 85% reported by Sun and Van hamme [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "[11], we use the exemplar set discovered in iteration 3 of Table I (using an unconstrained setup up to this point) and then constrain the Bayesian segmental model to 15 components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] AND THE SEGMENTAL BAYESIAN MODEL", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Discrete HMM [11] yes 32.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "005 (which we used in the experiments above, based on [32]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "To scale to larger corpora, both the efficiency and accuracy of the embeddings would therefore need to be improved (see [28] for recent supervised efforts in this direction).", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "In particular, [20] showed that for joint segmentation and clustering of noisy phone sequences, a bigram model was needed to improve Word embedding from cluster 33 (\u2192 one)", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "Following [8], [18] it is mathematically straightforward to extend the algorithm of Section III-C to more complex language models.", "startOffset": 10, "endOffset": 13}, {"referenceID": 17, "context": "Following [8], [18] it is mathematically straightforward to extend the algorithm of Section III-C to more complex language models.", "startOffset": 15, "endOffset": 19}], "year": 2016, "abstractText": "In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.", "creator": "LaTeX with hyperref package"}}}