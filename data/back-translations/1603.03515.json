{"id": "1603.03515", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting", "abstract": "In this paper we look at the problem of active learning of a linear classifier by query synthesis, in which the learner can construct artificial queries to estimate the true limits of decision-making, a problem that has lately attracted great interest in automated science and opposing reverse engineering, for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to obtain information (e.g. automated science) or to avoid detection at minimal cost (e.g. reverse engineering).", "histories": [["v1", "Fri, 11 Mar 2016 04:18:48 GMT  (7790kb)", "http://arxiv.org/abs/1603.03515v1", null], ["v2", "Sat, 12 Nov 2016 17:39:47 GMT  (6921kb)", "http://arxiv.org/abs/1603.03515v2", "Accepted by AAAI 2017"]], "reviews": [], "SUBJECTS": "cs.AI cs.IT cs.LG math.IT", "authors": ["lin chen 0003", "seyed hamed hassani", "amin karbasi"], "accepted": true, "id": "1603.03515"}, "pdf": {"name": "1603.03515.pdf", "metadata": {"source": "CRF", "title": "Dimension Coupling: Optimal Active Learning of Halfspaces via Query Synthesis", "authors": ["Lin Chen", "Hamed Hassani", "Amin Karbasi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n03 51\n5v 1\n[ cs\n.A I]\nWe develop a general framework, called dimension coupling (DC), that 1) reduces a d-dimensional learning problem to d \u2212 1 low-dimensional sub-problems, 2) solves each sub-problem efficiently, and 3) appropriately aggregates the results and outputs a linear classifier. We consider the three most common scenarios in the literature: idealized noise-free, independent noise realizations, and agnostic settings. We show that the DC framework avoids the curse of dimensionality: its computational complexity in all three cases scales linearly with the dimension. Moreover, in the noiseless and noisy cases, we show that the query complexity of DC is near optimal (within a constant factor of the optimum algorithm). We also develop an agnostic variant of DC for which we provide strong theoretical guarantees. To further support our theoretical analysis, we compare the performance of DC with the existing work in all three settings. We observe that DC consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster."}, {"heading": "Introduction", "text": "In contrast to the passive model of supervised learning, where all the labels are provided without any interactions with the learning mechanism, the key insight in active learning is that the learning algorithm can perform significantly better if it is allowed to choose which data points to label. This approach has found far-reaching applications, including the classical problems in AI (e.g., classification [17], information retrieval [16], speech recognition [8]) as well as the modern ones (e.g., interactive recommender systems [10], optimal decision making [9]).\nIn statistical learning theory, one assumes that a set of hypotheses H along with a set of unlabeled data points X are given, where each data point x \u2208 X is drawn i.i.d. from some distribution D. Classical probably approximately correct (PAC) bounds then yield the sample complexity (i.e., the number of required i.i.d. examples) from D to output a hypothesis h \u2208 H that will have estimation error at most \u01eb with probability at least 1\u2212\u03b4, for some fixed \u01eb, \u03b4 > 0. Here, the estimation error is defined as error(h) = Prx\u223cD[h(x) \u2260 h\u2217(x)], where h\u2217 is the unknown true hypothesis. In the realizable case of learning a halfspace, i.e., when h\u2217 \u2208 Rd perfectly separates the data points into positive and negative labels, it is known that with O\u0303(d/\u01eb)1 i.i.d. samples one can find a linear separator with an estimation error \u01eb. In contrast, a simple counting argument based on sphere packing shows that any algorithm needs \u2126(d log(1/\u01eb)) examples to achieve an estimation error of \u01eb [6].\nIf one is not careful, active learning may require more samples than passive learning to achieve the same estimation error. One setting that is guaranteed to perform at least as well as passive learning is pool-based active learning [12]: a set of unlabeled examples are drawn i.i.d. where instead of obtaining all labels at once, the learning algorithm sequentially decides which labels to request and which ones to discard. The key challenge is to develop an algorithm that requests informative labels from the pool in such a way that the remaining labels can be inferred as quickly as possible. A principled way is to define a version space V containing all hypotheses consistent with the labels obtained so far and try to shrink it significantly by requesting new labels from the pool. An effective but computationally expensive method is the halving algorithm, also referred to as generalized binary search (GBS), where at each iteration the next example to label is chosen in such a way that it approximately cuts the version space by one half. The new version space is constructed and the process is repeated until the estimation error is less than \u01eb. In many cases, computing the version space is computationally prohibitive. Dasgupta [6], in the idealized case that the labels are noise-free and there exists a realizable linear separator, showed that one requires O\u0303(d log2(1/\u01eb)) labels, an exponential improvement over O\u0303(d/\u01eb). Unfortunately,the algorithm achieving this bound is generally intractable. Perhaps the most common query methods that approximate the idealized halving algorithm are uncertainty sampling [12] and queryby-committee (QBC) [7]. Freund et al. [7] showed that QBC requests O\u0303(d log(1/\u01eb)) labels, but comes at the price of prohibitive computational cost in each iteration. Similarly, Balcan et al. [4] showed that uncertainty sampling also achieves an exponential reduction in sample complexity, requiring O\u0303(d2 log(1/\u01eb)) labels. Unfortunately, most approximation methods are only guaranteed to work if the pool size grows exponentially fast in each iteration. Hence, one needs to always store a large amount of data in addition to computing a complex version space.\nAn attractive alternative to the pool-based framework is query synthesis [3]: a learner can request for any unlabeled data instance from the input space, including queries that the learner synthesizes from scratch. This way the pool size limitation is entirely eliminated. In many recent applications, ranging from automated science [11], to robotics [5], and to adversarial reverse engineering [13], query synthesis is the appropriate model. For instance, in security-sensitive applications (e.g., spam filters\n1We use the O\u0303 notation to ignore terms that are logarithmic or dependent on \u03b4.\nand intrusion detection systems) that routinely use machine learning tools, a growing concern is the ability of adversarial attacks to identify the blind spots of the learning algorithms. Concretely, classifiers are commonly deployed to detect miscreant activities. However, they are attacked by adversaries who generate exploratory queries to elicit information that in return allows them to evade detection [14]. In this work, we show how an adversary can use active learning methods by making synthetically de novo queries and thus identify the linear separator used for classification.\nWe should emphasize that in active learning via synthesized queries the learning algorithm can query the label of any points (irrespective of the underlying distribution D) in order to explore the hypothesis space. However, for evaluating the performance of the algorithm in terms of the estimation error, one may use the distribution D. This is quite different from pool-based framework where queries should be among the data points drawn i.i.d. from the underlying distribution D.\nThere is little known about the theoretical performance of actively learning a linear classifier via query synthesis. The closest work to our efforts is [2] where they propose a heuristic algorithm that approximates the version space by a convex body with promising empirical results in the noise-free setting. Their method is not appropriate for high dimensional data, as its computational complexity scales cubically with the dimension. Moreover, their algorithm fails in the presence of noise.\nIn this paper, we develop a framework, called Dimension Coupling (DC), with the following guarantees. In the noiseless setting, we show that its computational complexity is O\u0303(d log 1 \u01eb\u0303 ) and its query complexity is O\u0303(d log 1 \u01eb\u0303 ). Similarly, in the noisy\nsetting, we show that the computational complexity of DC is O\u0303(d(log 1 \u01eb\u0303 + log 1 \u03b4\u0303 )2) and\nits query complexity is O\u0303(d(log 1 \u01eb\u0303 + log 1 \u03b4\u0303 )). Note that in both settings the computational complexity scales linearly with the dimension. Moreover, the query complexity in both settings is near-optimal. Finally, we generalize our results to the agnostic case. Our empirical experiments demonstrate that DC runs orders of magnitude faster than the existing methods."}, {"heading": "Problem Formulation", "text": "Our objective is to estimate an unknown halfspace H\u2217 = {x \u2208 Rd \u2236 \u27e8h\u2217, x\u27e9 > 0}, using as few queries as possible. Here, \u27e8\u22c5, \u22c5\u27e9 is the standard inner product of the Euclidean space (also \u2225 \u22c5 \u2225 denotes the Euclidean norm), h\u2217 is some (hidden) unit vector that we want to estimate, and a generic query is of the form sign(\u27e8h\u2217, x\u27e9) where x is selected in Rd. Let us emphasize here that the only information we obtain from a query is the sign of the inner product and not the value. E.g., the queries of the form sign(\u27e8h\u2217, ei\u27e9), where ei is the ith standard basis vector, will only reveal the sign of the ith component of h\u2217 (and nothing further about its value). In the noiseless setting, we observe the true outcome of the query, i.e. sign \u27e8h\u2217, x\u27e9 \u2208{1,\u22121}. In the noisy setting, the outcome is a flipped version of the true sign with independent flip probability \u03c1. That is, denoting the outcome by Y we have Y \u2208 {\u22121,1} and Pr[Y \u2260 sign \u27e8h\u2217, x\u27e9] \u2236= \u03c1 < 1/2.\nSince the length of the selected vector x will not affect the outcome of the query, we only query the points on the unit sphere Sd\u22121 = {x \u2208 Rd \u2236 \u2225x\u2225 = 1}. Hence, we term\nX = Sd\u22121 as the query space. All possible unit vectors also reside on the unit sphere Sd\u22121. Therefore, the initial version space, denoted by H, is also Sd\u22121.\nAs aforementioned, to evaluate the performance of halfspace learning algorithms, we assume that unlabeled data points are drawn from the distribution D with a continuous probability density function fD. The estimation error is defined as error(h) = Prx\u223cD[h(x) \u2260 h\u2217(x)] = \u222b 1{h(x) \u2260 h\u2217(x)}fD(x)dx \u2264 4MD arcsin(\u2225h \u2212 h\u2217\u2225/2), where MD = maxx fD(x). To make sure the estimation error error(h) \u2264 \u01eb , it suffices to ensure \u2225h \u2212 h\u2217\u2225 < \u01ebD, where \u01ebD = 2 sin[\u01eb/(4MD)]; e.g., when D is uniform, \u01ebD = 2 sin(\u03c0\u01eb/2). Therefore, hereinafter we only discuss how to guarantee\u2225h \u2212 h\u2217\u2225 < \u01ebD with high probability (say, at least 1 \u2212 \u03b4). We report all the results in terms of \u2225h \u2212 h\u2217\u2225 but it is easy to convert them to the estimation error. Thus, given \u01eb, \u03b4 > 0, we seek an algorithm that (i) adaptively selects vectors x1, x2,\u22ef, (ii) observes the (noisy) responses to each query sign\u27e8h\u2217, xi\u27e9, (iii) and outputs, using as few queries as possible, an estimate h\u0302 of h\u2217 such that \u2225h\u0302 \u2212 h\u2217\u2225 < \u01eb with probability at least 1 \u2212 \u03b4."}, {"heading": "Dimension Coupling Based Framework", "text": "Suppose that h\u2217 has the form h\u2217 = \u2211di=1 ciei, where {ei}di=1 is an arbitrarily chosen orthonormal basis for Rd. We assume w.l.o.g. that h\u2217 is normalised (i.e., \u2211di=1 c2i = 1). Our objective is then to learn the coefficients {ci}di=1 within a given precision by using the (noisy) responses to the selected sign queries. The key insight here is that this task can be partitioned in a divide-and-conquer fashion into many smaller tasks, each involving a few dimensions. The final answer (the values of {ci}di=1) will then be obtained by aggregating the answers of these subproblems.\nExample 1. Assume h\u2217 = c1e1 + c2e2 + c3e3 + c4e4, where ei\u2019s are the standard basis vectors for R4. Define\ne\u03021 = c1e1 + c2e2\u221a\nc21 + c 2 2\n, e\u03022 = c3e3 + c4e4\u221a\nc23 + c 2 4\nNote here that e\u03021 is the (normalised) orthogonal projection of h\u2217 onto span{e1, e2} and e\u03022 is the (normalised) orthogonal projection of h\u2217 onto span{e3, e4}. Consider the following procedure to learn h\u2217: first find out what e\u03021 and e\u03022 are, and then use the relation h\u2217 = \u221a c21 + c 2 2e\u03021 + \u221a c23 + c 2 4e\u03022 to find h\n\u2217 based on the orthonormal vectors e\u03021, e\u03022. By this procedure, the original \u201cfour-dimensional\u201d problem has been broken into three \u201ctwo-dimensional\u201d problems.\nFor general d, the idea is similar: We break the problem into at most d \u2212 1 \u201ctwodimensional\u201d problems that each can be solved efficiently. More formally, let us assume that we have an algorithm, called DC2 (e1, e2, \u01eb, \u03b4), that takes as input two orthonormal vectors e1, e2 and outputs with probability at least 1 \u2212 \u03b4 a vector e\u0302 with the following three properties:\ne\u0302 \u2208 span{e1, e2}, \u2225e\u0302\u2225 = 1, \u2225e\u0302 \u2212 \u27e8h\u2217,e1\u27e9e1+\u27e8h\u2217,e2\u27e9e2\u2225\u27e8h\u2217,e1\u27e9e1+\u27e8h\u2217,e2\u27e9e2\u2225\u2225 < \u01eb. In other words, the unit vector e\u0302 is within a distance \u01eb to the (normalised) projection of h\u2217 onto the subspace span{e1, e2}. In the next section, we will explain in detail how\nto design an optimal candidate for DC2 that uses (noisy) responses to queries of the form sign \u27e8x,h\u2217\u27e9. In the current section, we explain a framework DC that estimates h\u2217 using at most d \u2212 1 calls to DC2 (a formal description is given in Algorithm 1). Consider the decomposition h\u2217 = \u2211di=1 ciei. For simplicity assume that d is an even number. We can write\nh\u2217 = d\n\u2211 i=1 ciei = d/2 \u2211 j=1 c\u0302j c2j\u22121e2j\u22121 + c2je2j\u221a\nc22j\u22121 + c 2 2j\n, (1)\nwhere in the last step we have taken c\u0302j \u225c \u221a c22j\u22121 + c 2 2j . Now, note that\nc2j\u22121e2j\u22121+c2je2j\u221a c2 2j\u22121 +c2 2j\nis the (normalised) orthogonal projection of h\u2217 onto span{e2j\u22121, e2j}. Hence, by using DC2 (e2j\u22121, e2j , \u01eb, \u03b4) we can obtain, with probability at least 1\u2212 \u03b4, a good approximation e\u0302j (within a distance \u01eb) of this projection. Therefore, for small enough \u01eb we have h\u2217 \u2248 \u2211d/2j=1 c\u0302j e\u0302j . Since h \u2217 is now expressed (approximately) in terms of d/2 known orthonormal vectors {e\u0302j}d/2j=1, we have effectively reduced the dimensionality of problem from d to d/2. The idea is then to repeat the same procedure as in (1) to the newly obtained representation of h\u2217. Hence, by repeating this procedure at most log2 d times we will reach a vector which is the final approximation of h\u2217.\nAlgorithm 1 Dimension Coupling (DC) Input: an orthonormal basis E = {e1, e2, . . . , ed} of Rd. while d > 2\n1. for j = 1 to \u230ad/2\u230b Replace the two vectors e2j\u22121, e2j in E with the vector DC2 (e2j\u22121, e2j , \u01eb, \u03b4). end for\n2. Set d \u2190 \u2308d/2\u2309 . end while return DC2 (e1, e2, \u01eb, \u03b4). Theorem 2. For DC (outlined in Algorithm 1) we have:\n1. DC will call the two-dimensional subroutine DC2 at most d \u2212 1 times.\n2. Provided that the output of DC2 is with probability 1 \u2212 \u03b4 within distance \u01eb of the true value and \u01eb \u2264 1\n6d , DC ensures an estimation error of at most 6\u01ebd with\nprobability at least 1 \u2212 \u03b4d.\nWe defer the proof of all theorems to the appendix. As a result of Theorem 2, if we desire the framework DC to estimate h\u2217 within distance \u01eb\u0303 and with probability at least 1\u2212 \u03b4\u0303, then it is enough to fix the corresponding parameters of DC2 to \u01eb = \u01eb\u0303\n6d and \u03b4 = \u03b4\u0303 d .\nDC2: Solving in 2 Dimensions\nBefore illustrating the algorithm DC2, let us review some notation. Given two orthonormal vectors e1, e2 we denote the (normalised) projection of h\u2217 onto span{e1, e2} by h\u22a5, i.e.,\nh\u22a5 = \u27e8h\u2217, e1\u27e9 e1 + \u27e8h\u2217, e2\u27e9 e2\u2225\u27e8h\u2217, e1\u27e9 e1 + \u27e8h\u2217, e2\u27e9 e2\u22252 . (2)\nThe objective of DC2 (e1, e2, \u01eb, \u03b4) is to find a unit vector e\u0302 \u2208 span{e1, e2} such that\u2225e\u0302 \u2212 h\u22a5\u2225 < \u01eb. In fact, we require the latter to hold with probability at least 1 \u2212 \u03b4. Any unit vector inside span{e1, e2}, e.g., h\u22a5, can equivalently be represented as a\npair (c1, c2) on the two-dimensional unit circle S1 (e.g., h\u22a5 = c1e1+ c2e2 and c21 + c22 = 1). To simplify notation, we use a point (c1, c2) \u2208 S1 and its corresponding unit vector c1e1+c2e2 interchangeably. In this setting, it is easy to see that for any x \u2208 span{e1, e2} sign \u27e8x,h\u2217\u27e9 = sign \u27e8x,h\u22a5\u27e9 . (3) We take a Bayesian approach. In the beginning, when no queries have been performed, DC2 assumes no prior information about the vector h\u22a5. Therefore, it takes the uniform distribution on S1 (with pdf p0(h) = 12\u03c0 ) as its prior belief about h\u22a5. After performing each query, the posterior (belief) about h\u22a5 will be updated according to the observation. We let pm(h) denote the (pdf of the) posterior after performing the first m queries. In this manner, DC2 runs in total of T\u01eb,\u03b4 rounds, where in each round a specific query is selected and posed to the oracle. The number T\u01eb,\u03b4 will be specified later (see Theorems 3 and 4). Upon the completion of round T\u01eb,\u03b4, the algorithm returns as its final output a vector e\u0302 \u2208 S1 that maximises the posterior pdf pT\u01eb,\u03b4(h). If there are multiple such maximisers, it picks one arbitrarily. We now proceed with a detailed description of DC2 (a formal description is provided in Algorithm 2). We first consider the simpler noise-free case and the other settings will follow afterwards.\nNoise-Free Case: We explain DC2 (outlined in Algorithm 2) with the help of a running example given in Figure 1. As we will see, after each round of DC2 the possible region that h\u22a5 can belong to will be \u201chalved\u201d.\nWe first note that as the initial distribution p0 is assumed to be the uniform distribution on S1, the vector x1 (see step 2-(a) of Algorithm 2) can indeed be any point on the unit circle S1. Thus, DC2 chooses x1 arbitrarily on S1. By (3), using the query sign \u27e8x1, h\u2217\u27e9 will also give us the value of sign \u27e8x1, h\u22a5\u27e9. Depending on this value, it is easy to verify that only half of S1 can possibly contain h\u22a5 (see Figure 1). Let us denote this region by R1. Hence, the probability distribution p1(h) (which is our current belief about h\u22a5) is updated as follows: for h \u2209 R1 we have that p1(h) = 0, and as all the points inside the half-circle R1 are equiprobable, we have for h \u2208 R1 that p1(h) = 1/\u03c0. In other words, at time m = 0 the vector h\u22a5 could have been anywhere on the unit circle, but, after round m = 1 it can only belong to the half-circle R1. Thus, after the first round, DC2 \u201chalves\u201d the admissible region of h\u22a5. Continuing in this theme, it is not hard to verify that (see Figure 1) at round m = 2 the value of p2(h) is non-zero and uniform only on a region R2 which is a quarter-circle. In an inductive manner, letting Rm\u22121 denote the admissible region (sector) at round m \u2212 1 (see Figure 1) and assuming that pm\u22121 is only non-zero and uniform on the sector Rm\u22121, then xm at round m is\nAlgorithm 2 DC2\nInput: orthonormal vectors e1, e2, estimation error at most \u01eb, success probability at least 1 \u2212 \u03b4.\n1. Set p0(h) to be uniform, i.e., \u2200h \u2208 S1 \u2236 p0(h) = 1/2\u03c0. 2. for m = 1 to T\u01eb,\u03b4\n(a) find a vector xm \u2208 S1 which is a solution to the following equation: \u222bS1 sign \u27e8x,h\u27e9 pm\u22121(h)dh = 0. If there are multiple solutions, choose one arbitrarily. (b) ask from the oracle the value of sign \u27e8xm, h\u2217\u27e9. (c) based on the response obtained from the oracle, update the distribution\npm\u22121(h) to pm(h). end for\n3. return e\u0302 = argmaxh\u2208S1 pT\u01eb,\u03b4(h).\nprecisely the vector that is orthogonal to the midpoint of the sector Rm\u22121. Therefore, after observing the value of sign \u27e8xm, h\u2217\u27e9, the admissible region Rm is the better half of Rm\u22121 that is compatible with the observation (i.e., it contains h\u22a5). Also, Rm is again a sector and pm will be uniform on Rm and zero outside. It is also easy to see that the circular angle for the sector Rm is \u03c02m . The following statement is now immediate. Theorem 3. Consider DC in the absence of noise (\u03c1 = 0). If we let T\u01eb,\u03b4 = \u2308log2 \u03c0\u01eb \u2309, then it outputs a vector that is within a distance \u01eb of h\u22a5.\nA few comments are in order: The above guarantee for DC2 holds with probability one and thus the parameter \u03b4 is irrelevant in the noiseless setting. Furthermore, during each round of DC2, the distribution pm can be represented by only two numbers (the starting and ending points of the sector Rm), and the vector xm can be computed efficiently (it is the orthogonal vector to the midpoint of Rm). Therefore, assuming one unit of complexity for performing the queries, DC2 can be implemented with complexity O(T\u01eb,\u03b4). Finally, by using Theorem 2, we conclude that DC requires O\u0303(d log 1\u01eb ) queries with computational complexity O\u0303(d log 1 \u01eb ). Noisy Case: In general, DC2 follows a similar procedure as in the noiseless case except that the distributions pm does not look as simple. However, as we now discuss, these distributions can still be stored efficiently and as a result the vector xm can be computed efficiently. Indeed, (the pdf of) pm is piecewise constant on the unit circle (see Figure 2). More precisely, at any round m, there are at most 2m points u1, u2,\u22ef, u2m that are ordered clock-wise on the unit-circle and pm is constant when restricted to each of the sectors [ui, ui+1). At round m + 1, in order to find xm+1 (see step 2-(a) of Algorithm 2), DC2 first finds a line that passes through the centre of S1 and cuts S1 into two \u201chalves\u201d which have the same measure with respect to pm. Note that finding such a line can be done in O(m) steps because pm has the piecewise constant property. Once such a line is found, it is then easy to see that xm+1 can be any of the two points orthogonal to the line. As a result, DC2 at round m + 1 can find xm+1 in O(m) operations. We denote the half-circle containing xm+1 by R+ and the other half by R\u2212. We refer to Figure 2 for a schematic illustration. Once a noisy response to the query sign \u27e8xm+1, h\u2217\u27e9 is obtained, the probability distribution pm will be updated to pm+1 in the following way. First, consider the event that the outcome of sign \u27e8xm+1, h\u2217\u27e9 is +1. We have pm(sign \u27e8xm+1, h\u2217\u27e9 = +1) = (1\u2212\u03c1) pm(R+)+\u03c1 pm(R\u2212) = 1/2, and similarly pm(sign \u27e8xm+1, h\u2217\u27e9 = +1) = 1/2. Therefore, by Bayes theorem we obtain the following update rules for pm+1. If we observe that sign \u27e8xm+1, h\u2217\u27e9 = +1, then for h \u2208 R+ we have pm+1(h) = 2(1\u2212\u03c1)pm(h) and for h \u2208 R\u2212 we have pm+1(h) = (2\u03c1)pm(h). Also, if we observe that sign \u27e8xm+1, h\u2217\u27e9 = \u22121, then for h \u2208 R+ \u2236 pm+1(h) = (2\u03c1)pm(h) and for h \u2208 R\u2212 \u2236 pm+1(h) = 2(1 \u2212 \u03c1)pm(h). (note that the factor of 2 here is due to the normalization.) It is easy to verify that pm+1 is also a piecewise constant distribution (now on 2(m + 1) sectors; see Fig. 2). Theorem 4. In the noisy setting (with independent flip probability \u03c1), having\nT\u01eb,\u03b4 \u2265M +max{T0, T1, T2, T3} = O(log 1 \u01eb + log 1 \u03b4 ) (4)\nis sufficient to guarantee that DC2 outputs with probability at least 1 \u2212 \u03b4 a vector that is within a distance \u01eb of h\u22a5. Here, we have M = \u2308 2 log 2\u03b4\u2212 log(4\u03c1(1\u2212\u03c1)) \u2309, T0 = 8 log 2\u03b4log(2(1\u2212\u03c1)) , T1 = 8 log 1\n8\u03c0\u01eb log(2(1\u2212\u03c1)) , T2 = 8 log(2(1\u2212\u03c1)) (log(2M) + log( 4log(2(1\u2212\u03c1)))) andT3 = 24\u03c1 log2 1\u2212\u03c1\u03c1log2(2(1\u2212\u03c1)) (log(M) + log( 4\u03b4 )). Theorem 2 indicates that DC requires O\u0303(d(log 1\n\u01eb + log 1 \u03b4 )) queries. Also, as dis-\ncussed above, the computational complexity of DC2 is O(T 2\u01eb,\u03b4). Hence, DC has computational complexity O\u0303(d(log 1\n\u01eb + log 1 \u03b4 )2).\nAgnostic Case: A common approach used in the agnostic setting is the empirical risk minimization (ERM), which generates queries by independently sampling from the marginal distribution of X , i.e., PX , and then selects a unit vector that minimizes the number of errors made on these n queries. We follow a similar approach as in [15]. Suppose that we are given a query budget of n. We allocate n/3 queries to DC and let h1 denote the unit vector selected by DC. Then we allocate n/3 queries to ERM and let h2 denote the unit vector proposed by ERM using its n/3 queries. Consider the region \u2206 that h1 and h2 disagree on, i.e., \u2206 = {x \u2208 Sd\u22121 \u2236 h1(x) \u2260 h2(x)}. Let P\u2206 be the restriction of the probability measure PX on \u2206. Here we sample the remaining n/3 queries from \u2206 according to P\u2206. We write R\u0302\u2206(hi) for the average number of errors made by hi on the sampled n/3 queries. Finally, we set R\u2206(hi) = E[R\u0302\u2206(hi)]. The algorithm will output h\u0302 = argminh\u2208{h1,h2} R\u0302\u2206(h) as the final result. Theorem 5. Using the above procedure we have the following bounded error proba-\nbility in expectation E[R(h\u0302)] \u2264 min{E[R(h1),E[R(h2)]} + 2\u221a 3ne , where n is the number of queries and e is the base of the natural logarithm."}, {"heading": "Empirical Results", "text": "In this section, we extensively evaluate the performance of DC against the following baselines:\n\u2022 RANDOM-SAMPLING: Queries are generated by sampling uniformly at random from the unit sphere Sd\u22121.\n\u2022 UNCERTAINTY-SAMPLING: Queries are sampled uniformly at random from the orthogonal complement of w, where w is the vector learned by linear SVM.\n\u2022 QUERY-BY-BAGGING: The bag size is set to 20 and 1000 queries are generated at each iteration. The query with the largest disagreement is picked [1].\n\u2022 SPECTRAL: The version space is approximated by the largest ellipsoid consistent with all previous query-label pairs. Then, at each iteration a query is selected to approximately halve the ellipsoid [2].\n\u2022 REPETITIVE-DC: In the noisy setting, one easy way to apply DC is to query each point R times and use the majority rule to determine its label.\nOur metrics to compare different algorithms are: a) estimation error, b) query complexity, and c) execution time. In particular, as we increase the number of queries we\nmeasure the average estimation errors and execution times for all the baselines (with 90% confidence intervals). By nature, in active learning via query synthesis, all data points and queries are generated synthetically. For all the baselines, we used the fastest available implementations in MATLAB.\nNoise-free setting: Figures 3a, 3b and 3c (with dimension d = 25,50,100, respectively) show that in terms of estimation error, DC outperforms all other baselines, and significantly outperforms RANDOM-SAMPLING, UNCERTAINTY-SAMPLING and QUERY-BY-BAGGING. Note that the estimation errors are plotted in log-scales. In terms of execution times, we see in Fig. 3d that DC runs three orders of magnitude faster than other baselines. Training an SVM at each iteration for RANDOMSAMPLING, UNCERTAINTY-SAMPLING and QUERY-BY-BAGGING comes with a huge computational cost. Similarly, SPECTRAL requires solving a convex optimization problem at each iteration; thus its performance drastically deteriorates as the dimension increases, which makes it infeasible for many practical problems.\nNoisy setting: We set the noise level to \u03c1 = 0.1 and compare the performance of DC against RANDOM-SAMPLING, UNCERTAINTY-SAMPLING, QUERY-BY-BAGGING, and REPETITIVE-DC (for R = 5). As mentioned in [2], and we have also observed in our experiments, SPECTRAL does not work even for small amounts of noise as it incorrectly shrinks the version space and misses the true linear separator. We see again in Figures 3e and 3f (for d = 25,50) that DC significantly outperforms all other methods in terms of estimation error as we increase the number of queries. Figure 3g shows that DC still runs \u223c100 times faster than RANDOM-SAMPLING, UNCERTAINTYSAMPLING, and QUERY-BY-BAGGING. Clearly, DC has a higher computational cost than REPETITIVE-DC, as DC performs a Bayesian update after each query. Finally, as we increase the dimension to d = 1000, RANDOM-SAMPLING, UNCERTAINTYSAMPLING, and QUERY-BY-BAGGING become significantly slower. Hence, in Fig. 3h we only show how the estimation error (for noise levels \u03c1 = 0.01,0.1,0.2) decreases for DC and REPETITIVE-DC with more queries.\nAgnostic setting: We sample a set of size 1000 from the Von Mises\u2013Fisher distribution (the analog of the normal distribution on Sd\u22121) and compute the empirical error . The noise level at each point is drawn independently from the truncated normal N (0.1,0.5) on [0,0.5]. Fig. 3i shows the error probability of RANDOMSAMPLING, UNCERTAINTY-SAMPLING, QUERY-BY-BAGGING and AGNOSTIC-DC (for d = 25). Both AGNOSTIC-DC and UNCERTAINTY-SAMPLING achieve an error probability \u223c0.1 after 700 queries. However, as shown by Theorem 5, AGNOSTIC-DC has a strong theoretical guarantee which, by contrast, other baselines do not have."}, {"heading": "Appendix A: Proof of Theorem 2", "text": "At each round of DC, the value of d is replaced by \u2308d 2 \u2309. It is thus not hard to verify\nthat DC runs in \u230alog2 d\u230b rounds until d \u2264 2 and in total there are at most d \u2212 1 usages of DC2. As a result, if the probability of success for DC2 is at least 1 \u2212 \u03b4, then by the union bound the probability of success of DC is at least 1 \u2212 (d \u2212 1)\u03b4.\nFor the last part of the theorem, we prove a more general statement: Assume that we run DC with an input being an orthonormal set {e1, e2,\u22ef, ed} where ei, h\u2217 \u2208 RK for some K \u2265 d. We prove that DC outputs a vector that is close to the (normalised) orthogonal projection of h\u2217 into span{e1, e2,\u22ef, ed}. More precisely, we define\nh\u22a5 = \u2211 d i=1\u27e8ei, h\u2217\u27e9ei\u2225\u27e8ei, h\u2217\u27e9ei\u2225 . (5)\nThen, DC runs in log2 d rounds, calls DC 2 d \u2212 1 times, and outputs with probability at\nleast 1 \u2212 \u03b4d a vector h\u0302 for which \u2225h\u22a5 \u2212 h\u0302\u2225 < 6\u01ebd. In exactly similar way as discussed above, we can conclude that DC runs in log2 d times and uses DC\n2 d \u2212 1 times. Also, again by the union bound, with probability at least 1 \u2212 \u03b4d all the outputs of DC2 are a close estimate (within distance \u01eb) of their corresponding objective. Thus, by assuming that all the calls of DC2 have been successful (which happens w.p. at least 1 \u2212 \u03b4d), we use an inductive argument to prove that \u2225h\u22a5 \u2212 h\u0302\u2225 < 6\u01ebd. We use induction on d. For d = 2 the result is clear. We now prove the result when d = k assuming that it holds for all d < k. For simplicity, we assume that k is an even number, i.e, k = 2t (the proof follows very similarly for k being odd). We can then write\nh\u22a5 = d\n\u2211 i=1\nciei = t\n\u2211 j=1\nc\u0302jh \u22a5 j , (6)\nwhere c\u0302j = \u221a c22j\u22121 + c 2 2j and h \u22a5 j =\nc2j\u22121ej+c2je2j\u221a c2 2j\u22121 +c2 2j . Note that h\u22a5j = c2j\u22121ej+c2je2j\u221a c2 2j\u22121 +c2 2j is pre-\ncisely the (normalised) orthogonal projection of h\u2217 (and also h\u22a5) onto span{e2j\u22121, e2j}. As we explained in Section , in the first round of DC each vector h\u22a5j will be replaced by the output of DC2 (e2j\u22121, e2j , \u01eb, \u03b4) which we denote by e\u0302j . Let us now define the vector h\u0302\u22a5 as\nh\u22a51 = \u2211tj=1\u27e8h\u22a5, e\u0302j\u27e9e\u0302j\u221a \u2211tj=1\u27e8h\u22a5, e\u0302j\u27e92\nIt is easy to verify that \u2223\u2223h\u22a51 \u2223\u2223 = 1 as {e\u03021, e\u03022,\u22ef, e\u0302t} is an orthonormal set. By the assumption of the induction, the final output of DC, which we denote by h\u0302\u22a5, will be within the distance 6\u01ebt of h\u0302\u22a5. That is, \u2223\u2223h\u22a51 \u2212 h\u0302\u22a5\u2223\u2223 < 6\u01ebt. (7) We now prove that \u2223\u2223h\u22a5 \u2212 h\u22a51 \u2223\u2223 < 6\u01ebt. (8)\nFrom (7) and (8) the induction hypothesis will be immediate as we can write \u2223\u2223h\u22a5 \u2212 h\u0302\u22a5\u2223\u2223 \u2264 \u2223\u2223h\u22a5 \u2212 h\u22a51 \u2223\u2223 + \u2223\u2223h\u22a51 \u2212 h\u0302\u22a5\u2223\u2223 < 6\u01ebt + 6\u01ebt = 6\u01ebd. It thus remains to prove (8).\nFirstly, we define \u03b2 \u225c \u221a \u2211tj=1\u27e8h\u22a5, e\u0302j\u27e92. We have\n\u2223\u2223h\u22a5 \u2212 h\u22a51 \u2223\u2223 = \u2223\u2223h\u22a5 \u2212 \u2211tj=1\u27e8h\u22a5, e\u0302j\u27e9e\u0302j \u03b2\n\u2223\u2223 = \u2223\u2223\u03b2h\u22a5 \u2212\u2211tj=1\u27e8h\u22a5, e\u0302j\u27e9e\u0302j\n\u03b2 \u2223\u2223\n= \u2223\u2223 (\u03b2 \u2212 1)h\u22a5 + h\u22a5 \u2212\u2211tj=1\u27e8h\u22a5, e\u0302j\u27e9e\u0302j \u03b2\n\u2223\u2223 \u2264 \u22231 \u2212 \u03b2\n\u03b2 \u2223 + \u2223\u2223h\u22a5 \u2212\u2211tj=1\u27e8h\u22a5, e\u0302j\u27e9e\u0302j \u03b2 \u2223\u2223. (9)\nSecondly, we have\n\u2223\u03b22 \u2212 1\u2223 = \u2223 t\u2211 j=1 \u27e8h\u22a5, e\u0302j\u27e92 \u2212 \u27e8h\u22a5, h\u22a5j \u27e92\u2223 \u2264 t\n\u2211 j=1 \u2223\u27e8h\u22a5, e\u0302j\u27e9 \u2212 \u27e8h\u22a5, h\u22a5j \u27e9\u2223\u2223\u27e8h\u22a5, e\u0302j\u27e9 + \u27e8h\u22a5, h\u22a5j \u27e9\u2223 \u2264 t\n\u2211 j=1 \u2223\u2223h\u22a5\u2223\u2223 \u22c5 \u2223\u2223e\u0302j \u2212 h\u22a5j \u2223\u2223 \u22c5 (\u2223\u2223h\u22a5\u2223\u2223 \u22c5 \u2223\u2223e\u0302j \u2223\u2223 + \u2223\u2223h\u22a5\u2223\u2223 \u22c5 \u2223\u2223h\u22a5j \u2223\u2223) \u2264 2\u01ebt,\nwhere the last step follows from \u2223\u2223h\u22a5\u2223\u2223 = \u2223\u2223h\u22a5j \u2223\u2223 = \u2223\u2223e\u0302j \u2223\u2223 = 1 and \u2223\u2223e\u0302j \u2212 h\u22a5j \u2223\u2223 \u2264 \u01eb. Hence, by noting the fact that 2\u01ebt = \u01ebd \u2264 1\n6 we obtain\n\u03b2 \u2208 [\u221a1 \u2212 2\u01ebt,\u221a1 + 2\u01ebt], (10) and \u22231 \u2212 \u03b2\n\u03b2 \u2223 \u2264max{ 1\u221a 1 \u2212 2\u01ebt \u2212 1,1 \u2212 1\u221a 1 + 2\u01ebt } \u2264 2\u01ebt. (11) Fourthly, similar as above we can show that\n\u2223\u2223h\u22a5 \u2212 t\u2211 j=1 \u27e8h\u22a5, e\u0302j\u27e9e\u0302j \u2223\u2223 = \u2223\u2223 t\u2211 j=1 \u27e8h\u22a5, h\u22a5j \u27e9h\u22a5j \u2212 \u27e8h\u22a5, e\u0302j\u27e9e\u0302j \u2223\u2223 \u2264 2\u01ebt. (12) Now, by plugging (10), (11) and (12) into (9) we get (8)."}, {"heading": "Appendix B: Proof of Theorem 4", "text": "Let {\u03b6n, n \u2265 1} be a sequence of independent and identically distributed (iid) Bernoulli(\u03c1) random variables. Denote by (F ,\u2126,Pr) the probability space generated by this sequence. At the m-th round of DC2, if \u03b6m = 1 (which takes place with independent probability \u03c1) then we observe a flipped version of sign\u27e8xm, h\u2217\u27e9. Also, if \u03b6m = 0 we observe the correct version of sign\u27e8xm, h\u2217\u27e9.\nConsider a query of the form sign\u27e8x,h\u2217\u27e9. This query divides the unit circle into two parts (half-circles) depending on the sign of \u27e8x,h\u2217\u27e9 (see Figure 4). The two parts are: (i) Preferred part: all h such that sign\u27e8x,h\u27e9 = sign\u27e8x,h\u22a5\u27e9, and (ii) Unpreferred part: all h such that sign\u27e8x,h\u27e9 = \u2212 sign\u27e8x,h\u22a5\u27e9. The two parts can be separated by a line \u2113x that passes through the origin. We refer to Figure 4 for a schematic explanation.\nIn this setting, we say that the query sign\u27e8x,h\u2217\u27e9 prefers a point z if z belongs to the preferred part of the query. Otherwise, we say that the query does not prefer z. Also, we frequently use the line \u2113x rather than the query sign\u27e8x,h\u2217\u27e9 when it causes no ambiguity. Finally, for a region A on the unit circle say that the query sign\u27e8x,h\u2217\u27e9 cuts the region A if and only if the line \u2113x passes through region A. Otherwise, we say that the query does not cut A. If \u2113x does not cut A, then \u2113x prefers A if A is in the preferred part and does not prefer A otherwise (see Figure 4). Finally, for two points x, y we define the distance d(x, y) to be the length of the (smaller) sector between them (see Figure 4). Clearly, we have d(x, y) \u2265 \u2225x \u2212 y\u22252.\nAt round m of DC2 a vector xm is chosen and the (noisy) outcome of sign\u27e8xm, h\u2217\u27e9 is observed. As explained in Section ??, xm is chosen in a way that the preferred and unpreferred parts have equal measures under pm\u22121, i.e., pm\u22121(Fxm) = pm\u22121(Uxm) = 1 2\n. Let us see what happens to pm (the posterior belief about h\u22a5 at round m) after we conduct the query sign\u27e8xm, h\u2217\u27e9. As the result of the query is noisy, we have two\ndifferent update rules depending on each of the following cases: (i) \u03b6m = 0, i.e., we observe the correct value sign\u27e8xm, h\u2217\u27e9. In this case, the measure pm is updated as follows\npm+1(h) = \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 2(1 \u2212 \u03c1)pm(h) if h \u2208 Fxm ,(2\u03c1)pm(h) if h \u2208 Uxm .\n(ii) \u03b6m = 1, i.e., we observe the flipped value \u2212 sign\u27e8xm, h\u2217\u27e9. In this case, the measure pm is updated as follows\npm+1(h) = \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 (2\u03c1)pm(h) if h \u2208 Fxm , 2(1 \u2212 \u03c1)pm(h) if h \u2208 Uxm .\nConsider the number T\u01eb,\u03b4 given in (4). Our goal is to show that\nPr [\u2203y \u2208 S1 \u2236 d(y, h\u22a5) > \u03c1 and pT\u01eb,\u03b4(y) \u2265 pT\u01eb,\u03b4(h\u22a5)] < \u03b4. (13) Clearly, the result of the theorem follows from (13). For better illustration, we assume w.l.o.g that h\u22a5 = (0,1). Consider a point y on the right-hand side of the unit circle such that d(y, h\u22a5) > \u01eb 2 . Also, Consider points z0, zK such that d(z0, h\u22a5) = \u01eb/4 and\nd(h\u22a5, zK) = \u01eb/2. We now divide the sector starting with z0 and ending with zK into K \u2236= T\u01eb,\u03b4+1 pints. That is, for i = 1,2,\u22ef,K we denote by zi the point that d(h\u22a5, zi) = \u01eb 4 + i \u01eb 4(T\u01eb,\u03b4+1) (see Figure 5). Also, for i \u2265 1, we let the sector starting with zi\u22121 and\nending with zi be denoted by Ai. Note that in the very beginning of the algorithm when we have uniform measure on the unit circle, each of the regions Ai has p0(Ai) =\n\u01eb 8\u03c0\u22c5(T\u01eb,\u03b4+1) (as \u2223Ai\u2223 = \u01eb4(T\u01eb,\u03b4+1) ).\nDC2 has in total T\u01eb,\u03b4 rounds and in each round m it conducts a query with an associated line \u2113xm . We let M \u2236= \u2308 2 log 2\u03b4log 4(\u03c1(1\u2212\u03c1))\u2309 and consider the following events: \u2022 E1: There is at least M lines which separate zK from h\u22a5 or equivalently, there\nis at least M lines that cut the region (h\u22a5, zK). \u2022 E2,j (1 \u2264 j \u2264K): The region Aj is not cut by any of the lines \u21131, \u21132, . . . , \u2113T\u01eb,\u03b4 .\n\u2022 E3: \u2203y such that d(y, h\u22a5) > \u03c12 and pT\u01eb,\u03b4(y) \u2265 pT\u01eb,\u03b4(h\u22a5). It is easy to see that Pr [\u22c3Kj=1E2,j] = 1 as we have T\u01eb,\u03b4 queries and hence by the pigeon-hole principle there is always a region Aj that is not cut by any of the lines. We can write:\nPr [E3] = Pr [E3 \u2229E1] +Pr [E3 \u2229Ec1] \u2264 Pr [E3 \u2223 E1] + T\u01eb,\u03b4+1\u2211\nj=1 Pr [E3 \u2229Ec1 \u2229E2,j] . (14)\nNow using Lemma 6 (stated below), we have\nPr [E3 \u2223 E1] \u2264 Pr [E3 \u2223 E1] \u2264 (4\u03c1(1 \u2212 \u03c1))M2 \u2264 \u03b4 2 . (15)\nLet us now bound Pr [E3 \u2229Ec1 \u2229E2,j] . We have Pr [E3 \u2229Ec1 \u2229E2,j] \u2264 Pr [E2,j \u2229Ec1] , and using the fact that \u2223E2,j \u2223 = \u03c14(T\u01eb,\u03b4+1) we obtain from Lemma 7 that Pr [E2,j \u2229Ec1] \u2264 (M \u2212 1)(\u03b81 + \u03b82),\nand thus T\u01eb,\u03b4+1\n\u2211 j=1 Pr [E2,j \u2229Ec1] \u2264 (T\u01eb,\u03b4 + 1)M(\u03b81 + \u03b82), (16) where \u03b81 and \u03b82 are given in Lemma 7 with m \u2190 T\u03c1,\u03b4 and k \u2190M . Now, we show that the above expression is upper bounded by \u03b4/2, and hence by using relations (14) and (15), we get the proof of the main theorem.\nThe value of T0 is chosen in such a way that we have\n2 log(T\u01eb,\u03b4 + 1) T\u01eb,\u03b4 \u2212M \u2264 log(2(1 \u2212 \u03c1)) 4 . (17)\nT1 ensures that 2\nT\u01eb,\u03b4 \u2212M log\n8\u03c0 \u01eb \u2264 log(2(1 \u2212 \u03c1)) 4 . (18)\nT2 and ensures that 2M T\u01eb,\u03b4 \u2212M log(2\u03c1) \u2264 log(1 \u2212 2\u03c1) 2 . (19)\nFinally, T3 ensures that\n(T\u01eb,\u03b4 + 1)M exp \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9\u2212\u03c1 T \u2212M 6 \u239b\u239d log(2(1 \u2212 \u03c1))2\u03c1 log 1\u2212\u03c1 \u03c1 \u239e\u23a0 2\u23ab\u23aa\u23aa\u23aa\u23ac\u23aa\u23aa\u23aa\u23ad \u2264 \u03b4 4 . (20)\nNow, by plugging in (17)-(20) into the values of \u03b81 and \u03b82 in (16) we conclude that the right side of (16) is bounded by \u03b4\n2 .\nLemma 6. Let x1, x2,\u22ef, xm be the vectors chosen by DC 2 up to round m with Fxi\nand Uxi being their associated preferred and unpreferred parts (i.e. pi\u22121(Fxi) = pi\u22121(Uxi) = 1/2). Consider two points h1, h2 such that h1 \u2208 \u2229mi=1Fxi and h2 \u2208 \u2229mi=1Uxi . We have for \u03b2 > 0 that Pr [pm(x) < pm(y)] \u2264 (4\u03c1(1 \u2212 \u03c1))m . Proof. For i \u2208 [m], define the random variable Zi as Zi \u225c log pi(x)pi(y) . Using the update rules of pi that we explained above, it is easy to see that for i \u2265 1: Zi = Zi\u22121 +(1 \u2212 2\u03b6i) log 1\u2212\u03c1\u03c1 . Also, as p0 is uniform over S1 we have Z0 = 0. We thus have Zm = \u2211mi=1(1 \u2212 2\u03b6i) log 1\u2212\u03c1\u03c1 . Hence,\nPr [Zm \u2264 0] = Pr [log 1 \u2212 \u03c1\n\u03c1\nm\n\u2211 i=1 (1 \u2212 2\u03b6i) \u2264 0] = Pr [m\u2211\ni=1 \u03b6i \u2265\n1 2 ]\n\u2264 (4\u03c1(1 \u2212 \u03c1))m2 , where the last step follows directly from the so called Chernoff bound.\nWe note that the vector h\u22a5 is always a member of the preferred part of any test. As a result, at any round of DC2 we have that h\u22a5 \u2208 \u2229mi=1Fxi .\nLemma 7. Consider a region A on the unit circle which does not contain h\u22a5. Assume we are at round m of DC2 where a sequence of queries with associated lines \u2113x1 , \u2113x2 , . . . , \u2113xm have been conducted. We define events E1 and E2 as\n\u2022 E1 \u225c None of the lines \u2113xi cuts A;\n\u2022 E2 \u225c At most k of the lines do not prefer A,\nwhere k is an an integer. We have Pr [E1 \u2229E2] \u2264 k(\u03b81 + \u03b82), where\n\u03b81 = exp \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9\u2212\u03c1 m \u2212 k 6 \u239b\u239d log(2(1 \u2212 \u03c1)) \u2212 2 m\u2212k log 2\u03c0\u2223A\u2223) \u03c1 log( 1\u2212\u03c1 \u03c1 ) \u239e\u23a0 2\u23ab\u23aa\u23aa\u23aa\u23ac\u23aa\u23aa\u23aa\u23ad ,\nand\n\u03b82 = exp \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9\u2212\u03c1 m \u2212 k 6 \u239b\u239d log(2(1 \u2212 \u03c1)) + 2k m\u2212k log(2\u03c1) \u03c1 log( 1\u2212\u03c1 \u03c1 ) \u239e\u23a0 2\u23ab\u23aa\u23aa\u23aa\u23ac\u23aa\u23aa\u23aa\u23ad .\nProof. We have\nPr [E1 \u2229E2] \u2264 Pr [E2 \u2223 E1] \u2264 k\u2211 j=1 Pr [E2,j \u2223 E1] , (21) where we define\nE2,j \u225c Exactly j lines do not prefer A.\nWe will now calculate Pr [E2,j \u2223 E1] . In the beginning, p0 puts a uniform measure on A and hence p0(A) = \u2223A\u22232\u03c0 . Let us first investigate the dynamics of pi\u22121(A) when we conduct the i-th query and condition on event E1 (i.e. given that none of the lines cut A). In this setting, we define the random variables Zi = log pi(A). At time i, assuming that the line \u2113xi does not cut A, Zi has different update rules depending on the two cases whether the line \u2113xi prefers A or does not prefer A. (i) first case: if the line \u2113xi prefers A, then we know that either with probability 1 \u2212 \u03c1 (if \u03b6i = 0) we have pi(A) = 2(1\u2212\u03c1)pi\u22121(A) and with probability \u03c1 (if \u03b6i = 1) we have pi(A) = (2\u03c1)pi\u22121(A). Thus, we can write Zi = Zi\u22121+Fi, where Fi \u225c \u03b6i log(2\u03c1)+(1\u2212\u03b6i) log(2(1\u2212\u03c1)). (ii) second case: if \u2113xi does not prefer A, then using a similar argument we obtain Zi = Zi\u22121 +Ui, where Ui \u225c \u03b6i log(2(1 \u2212 \u03c1)) + (1 \u2212 \u03b6i) log(2\u03c1). Now, in order to find an upper bound on Pr [E2,j \u2223 E1], we assume without loss of generality that in the first m \u2212 j rounds we the lines are as in the first case and in the last j rounds the lines are as in the second case (note that any other given order of the lines is statistically equivalent to this simple order that we consider).\nZm = Z0 + m\u2212j\n\u2211 i=1\nFi + m\n\u2211 i=m\u2212j+1 Ui\n= log2 \u2223A\u2223 2\u03c0 + m\u2212j \u2211 i=1 Fi + m \u2211 i=m\u2212j+1 Ui.\nNow, noting that pm(A) \u2264 1 and hence logpm(A) \u2264 0, we obtain Pr [E2,j \u2223 E1]\n\u2264 Pr \u23a1\u23a2\u23a2\u23a2\u23a3log2 p0(A) + m\u2212j \u2211 i=1 Fi + m \u2211 i=m\u2212j+1 Ui \u2264 0 \u23a4\u23a5\u23a5\u23a5\u23a6 = Pr \u23a1\u23a2\u23a2\u23a2\u23a3 m\u2212j \u2211 i=1 Fi + m \u2211 i=m\u2212j+1 Ui \u2264 log2 2\u03c0\u2223A\u2223\n\u23a4\u23a5\u23a5\u23a5\u23a6 Let us now define\n\u03b11 = Pr \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 m\u2212j 2 \u2211 i=1 Fi \u2264 log 2\u03c0 A \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 and\n\u03b12 = Pr \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 m\u2212j \u2211 i=1+m\u2212j 2 Fi + m \u2211 i=m\u2212j+1 Ui \u2264 0 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6\nUsing the union bound, we have Pr [E2,j \u2223 E1] \u2264 \u03b11 + \u03b12. (22) Now, to bound \u03b11 we obtain after some simplifications that\n\u03b11 = Pr \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 m\u2212j 2 \u2211 i=1 \u03b6i \u2265 \u03c1 \u00d7 m \u2212 j 2 \u00d7 log(2(1 \u2212 \u03c1)) \u2212 2 m\u2212j log 2\u03c0\u2223A\u2223 \u03c1 log 1\u2212\u03c1 \u03c1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 , and by using the Chernoff bound we get\n\u03b11 \u2264 exp \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9\u2212\u03c1 m \u2212 j 6 \u239b\u239d log(2(1 \u2212 \u03c1)) \u2212 2 m\u2212j log 2\u03c0\u2223A\u2223) \u03c1 log( 1\u2212\u03c1 \u03c1 ) \u239e\u23a0 2\u23ab\u23aa\u23aa\u23aa\u23ac\u23aa\u23aa\u23aa\u23ad . (23)\nTo bound \u03b12 we can similarly write after some simple steps that\n\u03b12 \u2264 Pr \u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 m\u2212j \u2211 i=1+m\u2212j 2 \u03b6i \u2265 \u03c1 \u00d7 m \u2212 j 2 log(2(1 \u2212 \u03c1)) + 2j m\u2212j log(2\u03c1) \u03c1 log 1\u2212\u03c1 \u03c1 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 ,\nand using the Chernoff bound we get\n\u03b12 \u2264 exp \u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23a9\u2212\u03c1 m \u2212 j 6 \u239b\u239d log(2(1 \u2212 \u03c1)) + 2j m\u2212j log(2\u03c1) \u03c1 log( 1\u2212\u03c1 \u03c1 ) \u239e\u23a0 2\u23ab\u23aa\u23aa\u23aa\u23ac\u23aa\u23aa\u23aa\u23ad . (24)\nWe further note that both of the upper bounds on \u03b11 and \u03b12 decrease when we increase j. Hence, the proof of the theorem follows by letting j = k in (23) and (24), and also plugging these bounds into (21)."}, {"heading": "Appendix C: Proof of Theorem 5", "text": "First of all, we consider Pr[R(h\u0302) >min{R(h1),R(h2)}]. Let \u03b4 \u225c R\u2206(h1)\u2212R\u2206(h2) and \u03b4\u0302 \u225c R\u0302\u2206(h1) \u2212 R\u0302\u2206(h2). By Hoeffding\u2019s inequality, we have\nPr[\u2223\u03b4\u0302 \u2212 \u03b4\u2223 \u2265 \u03b3] \u2264 2 exp(\u2212n\u03b32 6 ) . If \u03b4 > 0, then letting \u03b3 = \u03b4 yields that Pr[\u03b4\u0302 < 0] \u2264 2 exp(\u2212n\u03b42/6). Therefore Pr[R\u0302\u2206(h1) < R\u0302\u2206(h2)] \u2264 2 exp(\u2212n\u03b42/6). Since we know that R\u2206(h1) > R\u2206(h2) (\u03b4 > 0), we have R(h1) > R(h2) because h1 and h2 agree on the complement of \u2206. Thus R(h\u0302) > min{R(h1),R(h2)} with probability at most 2 exp(\u2212n\u03b42/6). Similarly if \u03b4 < 0, then we have Pr[\u03b4\u0302 > 0] \u2264 2 exp(\u2212n\u03b42/6). Therefore Pr[R\u0302\u2206(h1) > R\u0302\u2206(h2)] \u2264 2 exp(\u2212n\u03b42/6). SinceR(h1) < R(h2), henceR(h\u0302) >min{R(h1),R(h2)}\nwith probability at most 2 exp(\u2212n\u03b42/6). In sum, we have R(h\u0302) >min{R(h1),R(h2)} with probability at most 2 exp(\u2212n\u2223R\u2206(h1) \u2212R\u2206(h2)\u22232/6).\nLet \u03b4n denote 2 exp(\u2212n\u2223R\u2206(h1)\u2212R\u2206(h2)\u22232/6). Now we want to bound E[R(h\u0302) \u2223 h1, h2]. We have\nE[R(h\u0302) \u2223 h1, h2] \u2264 (1 \u2212 \u03b4n)min{R(h1),R(h2)} + \u03b4nmax{R(h1),R(h2)} = min{R(h1),R(h2)} + \u03b4n\u2223R(h1) \u2212R(h2)\u2223 = min{R(h1),R(h2)} + 2\u2223R(h1) \u2212R(h2)\u2223\nexp(\u2212n\u2223R\u2206(h1) \u2212R\u2206(h2)\u22232/6 \u2264 min{R(h1),R(h2)} + 2\u2223R(h1) \u2212R(h2)\u2223\nexp(\u2212n\u2223R(h1) \u2212R(h2)\u22232/6, where the last inequality holds because \u2223R(h1) \u2212R(h2)\u2223 \u2264 \u2223R\u2206(h1) \u2212R\u2206(h2)\u2223.\nWhen u = \u221a 3/n, the function f(u) = 2ue\u2212nu2/6 achieves its maximum 2\u221a 3\nne .\nThus we obtain that\nE[R(h\u0302) \u2223 h1, h2] \u2264min{R(h1),R(h2)} + 2 \u221a 3\nne .\nBy Jensen\u2019s inequality, we conclude that\nE[R(h\u0302)] \u2264 E[min{R(h1),R(h2)] + 2 \u221a 3\nne \u2264 min{E[R(h1),E[R(h2)]} + 2 \u221a 3\nne ."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>In this paper, we consider the problem of actively learning a linear classifier<lb>through query synthesis where the learner can construct artificial queries in order<lb>to estimate the true decision boundaries. This problem has recently gained a lot of<lb>interest in automated science and adversarial reverse engineering for which only<lb>heuristic algorithms are known. In such applications, queries can be constructed<lb>de novo to elicit information (e.g., automated science) or to evade detection with<lb>minimal cost (e.g., adversarial reverse engineering).<lb>We develop a general framework, called dimension coupling (DC), that 1) re-<lb>duces a d-dimensional learning problem to d \u2212 1 low-dimensional sub-problems,<lb>2) solves each sub-problem efficiently, and 3) appropriately aggregates the results<lb>and outputs a linear classifier. We consider the three most common scenarios in<lb>the literature: idealized noise-free, independent noise realizations, and agnostic<lb>settings. We show that the DC framework avoids the curse of dimensionality: its<lb>computational complexity in all three cases scales linearly with the dimension.<lb>Moreover, in the noiseless and noisy cases, we show that the query complexity of<lb>DC is near optimal (within a constant factor of the optimum algorithm). We also<lb>develop an agnostic variant of DC for which we provide strong theoretical guaran-<lb>tees. To further support our theoretical analysis, we compare the performance of<lb>DC with the existing work in all three settings. We observe that DC consistently<lb>outperforms the prior arts in terms of query complexity while often running orders<lb>of magnitude faster.", "creator": "LaTeX with hyperref package"}}}