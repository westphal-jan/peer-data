{"id": "1603.05614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Streaming Algorithms for News and Scientific Literature Recommendation: Submodular Maximization with a d-Knapsack Constraint", "abstract": "Submodular maximization problems belong to the family of combinatorial optimization problems and are widely applicable. In this paper, we focus on the problem of maximizing a monotonous submodular function subject to a $d $knapsack constraint, for which we propose a streaming algorithm that achieves a $\\ left (\\ frac {1} {1 + d} -\\ epsilon\\ right) $approximation of the optimal value while requiring only a single run through the data set without storing all the data in memory. In our experiments, we evaluate the effectiveness of our proposed algorithm extensively through two applications: message recommendation and scientific literature recommendation. It is observed that the proposed streaming algorithm achieves both execution acceleration and memory savings by several orders of magnitude compared to existing approaches.", "histories": [["v1", "Thu, 17 Mar 2016 19:01:12 GMT  (337kb,D)", "https://arxiv.org/abs/1603.05614v1", "11 pages, 5 figures"], ["v2", "Mon, 4 Jul 2016 16:15:56 GMT  (337kb,D)", "http://arxiv.org/abs/1603.05614v2", "11 pages, 5 figures"], ["v3", "Tue, 5 Jul 2016 00:43:45 GMT  (337kb,D)", "http://arxiv.org/abs/1603.05614v3", "11 pages, 5 figures"]], "COMMENTS": "11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["qilian yu", "easton li xu", "shuguang cui"], "accepted": false, "id": "1603.05614"}, "pdf": {"name": "1603.05614.pdf", "metadata": {"source": "CRF", "title": "Streaming Algorithms for News and Scientific Literature Recommendation: Submodular Maximization with a d-Knapsack Constraint", "authors": ["Qilian Yu", "Shuguang Cui"], "emails": ["cui}@tamu.edu)."], "sections": [{"heading": null, "text": "1 1+2d\n\u2212 )\n-approximation of the optimal value, while it only needs one single pass through the dataset without storing all the data in the memory. In our experiments, we extensively evaluate the effectiveness of our proposed algorithm via two applications: news recommendation and scientific literature recommendation. It is observed that the proposed streaming algorithm achieves both execution speedup and memory saving by several orders of magnitude, compared with existing approaches.\nI. INTRODUCTION As our society enters the big data era, the main problem that data scientists are facing is how to process the unprecedented large datasets. Besides, data sources are heterogenous, comprising documents, images, sounds, and videos. Such challenges require the data processing algorithms to be more computationally efficient. The concept of submodularity plays an important role in pursuing efficient solutions for combinatorial optimization, since it has rich theoretical and practical features. Hence submodular optimization has been adopted to preprocess massive data in order to reduce the computational complexity. For example, in the kernel-based machine learning [1], [2], the most representative subset of data is first selected in order to decrease the dimension of the feature space, by solving a submodular maximization problem under a cardinality constraint. Besides, such submodular optimization models have been extended to address data summarization problems [3].\nAlthough maximizing a submodular function under a cardinality constraint is a typical NP-hard problem, a simple greedy algorithm developed in [4] achieves a (1 \u2212 e\u22121)approximation of the optimal solution with a much lower computation complexity. When the main memory can store the whole dataset, such a greedy algorithm can be easily applied for various applications. However, as it requires the full access to the whole dataset, large-scale problems prevent the greedy algorithm from being adequate due to practical computation resource and memory limitations. Even in the case when the memory size is not an issue, it is possible that the number of\nQ. Yu, E. L. Xu, and S. Cui are with the Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX 77843 USA (e-mails: {yuql216, eastonlixu, cui}@tamu.edu).\ndata samples grows rapidly such that the main memory is not able to read all of them simultaneously.\nUnder the scenarios discussed above, processing data in a streaming fashion becomes a necessity, where at any time point, the streaming algorithm needs to store just a small portion of data into the main memory, and produces the solution right at the end of data stream. A streaming algorithm does not require the full access to the whole dataset, thus only needs limited computation resource. In [5], the authors introduced a streaming algorithm to maximize a submodular function under a cardinality constraint, where the cardinality constraint is just a special case of a d-knapsack constraint [6] with each weight being one. When each element has multiple weights or there are more than one knapsack constraints, the algorithm proposed in [5] is no longer applicable.\nIn this paper, we develop a new streaming algorithm to maximize a monotone submodular function, subject to a general d-knapsack constraint. It requires only one single pass through the data, and produces a ( 1 1+2d \u2212 )\n-approximation of the optimal solution, for any > 0. In addition, the algorithm only requires O ( b log b d ) memory (independent of the dataset\nsize) and O ( log b ) computation per element with b being\nthe standardized d-knapsack capacity. To our knowledge, it is the first streaming algorithm that provides a constant-factor approximation guarantee with only monotone submodularity assumed. In our experiments, compared with the classical greedy algorithm developed in [7], the proposed streaming algorithm achieves over 10,000 times running time reduction with a similar performance.\nThe rest of this paper is organized as follows. In Section II we introduce the formulation and related existing results. In Section III we describe the proposed algorithms. In Section IV we present two applications in news and scientific literature recommendations. We draw the conclusions in Section V."}, {"heading": "II. FORMULATION AND MAIN RESULTS", "text": ""}, {"heading": "A. Problem Formulation", "text": "Let V = {1, 2, . . . , n} be the ground set and f : 2V \u2192 [0,\u221e) be a nonnegative set function on the subsets of V . For any subset S of V , we denote the characteristic vector of S by xS = (xS,1, xS,2, . . . , xS,n), where for 1 \u2264 j \u2264 n, xS,j = 1, if j \u2208 S; xS,j = 0, otherwise. For S \u2286 V and r \u2208 V , the marginal gain of f with respect to S and r is defined to be\n\u2206f (r|S) , f(S \u222a {r})\u2212 f(S),\nar X\niv :1\n60 3.\n05 61\n4v 3\n[ cs\n.L G\n] 5\nJ ul\n2 01\n6\n2 which quantifies the increase in the utility function f(S) when r is added into subset S. A function f is submodular if it satisfies that for any A \u2286 B \u2286 V and r \u2208 V \\ B, the diminishing returns condition holds:\n\u2206f (r|B) \u2264 \u2206f (r|A).\nAlso, f is said to be a monotone function, if for any S \u2286 V and r \u2208 V , \u2206f (r|S) \u2265 0. For now, we adopt the common assumption that f is given in terms of a black box that computes f(S) for any S \u2286 V . In Sections III-A, III-B, III-C, we will discuss the case when the submodular function is independent [8] of the ground set V (i.e., for any S \u2286 V , f(S) depends on only S, not V \\S), and in Section III-D, we will discuss the setting where the value of f(S) depends on not only the subset S but also the ground set V .\nNext, we introduce the d-knapsack constraint. Let b = (b1, b2, . . . , bd)\nT be a d-dimensional budget vector, where for 1 \u2264 i \u2264 d, bi > 0 is the budget corresponding to the i-th resource. Let C = (ci,j) denote a d\u00d7n matrix, whose (i, j)-th entry ci,j > 0 is the weight of the element j \u2208 V with respect to the i-th knapsack resource constraint. Then the d-knapsack constraint can be expressed by CxS \u2264 b. The problem for maximizing a monotone submodular function f : 2V \u2192 [0,\u221e) subject to a d-knapsack constraint can be formulated as\nmaximize S\u2286V f(S) subject to CxS \u2264 b. (1)\nWe aim to MAximize a monotone Submodular set function subject to a d-Knapsack constraint, which is called d-MASK for short. Without loss of generality, for 1 \u2264 i \u2264 d, 1 \u2264 j \u2264 n, we assume that ci,j \u2264 bi. That is, no entry in C has a larger weight than the corresponding knapsack budget, since otherwise the corresponding element is never selected into S.\nFor the sake of simplicity, we here standardize Problem (1). Let\nb , max 1\u2264i\u2264d bi and c\u2032 , min 1\u2264i\u2264d,1\u2264j\u2264n bci,j/bi.\nFor 1 \u2264 i \u2264 d, 1 \u2264 j \u2264 n, we replace each ci,j with bci,j/bic\n\u2032 and bi with b/c\u2032. We then create a new matrix D by concatenating C and b over columns. That is, D = (di,j) is a d\u00d7 (n+ 1) matrix, such that, for 1 \u2264 i \u2264 d, di,j = ci,j \u2265 1 if 1 \u2264 j \u2264 n; di,j = b if j = n + 1. The standardized problem has the same optimal solution as Problem (1). In the rest of the paper, we only consider the standardized version of the d-MASK problem."}, {"heading": "B. Related Work and Main Results", "text": "Submodular optimization has been regarded as a powerful tool for combinatorial massive data mining and machine learning, for which a streaming algorithm processes the dataset piece by piece and then produces an approximate solution right at the end of the data stream. This makes it quite suitable to process a massive dataset in many applications.\nWhen d = 1 and all entries of C are ones, Problem (1) is equivalent to maximizing a monotone submodular function under a cardinality constraint. This optimization problem has\nbeen proved to be NP-hard [4], and people have developed many approximation algorithms to solve this problem, among which the greedy algorithm [4] is the most popular one. Specifically, the greedy algorithm selects the element with the maximum marginal value at each step and produces a (1 \u2212 e\u22121)-approximation guarantee with O(kn) computation complexity, where k is the maximum number of elements that the solution set can include, and n is the number of elements in the ground set V . Recently, some accelerated algorithms were proposed in [9], [10]. Unfortunately, neither of them can be applied to the case when the size of the dataset is over the capacity of the main memory. A streaming algorithm was developed in [5] with a (1/2\u2212 )-approximation of the optimal value, for any > 0. This streaming algorithm does not require the full access to the dataset, and needs only one pass through the dataset. Thus it provides a practical way to process a large dataset on the fly with a low memory requirement, but not applicable under a general d-knapsack constraint.\nFurther, the authors in [11] dealt with the case when d = 1 and each entry of C can take any positive values. Maximizing a monotone submodular function under a single knapsack constraint is also called a budgeted submodular maximization problem. This problem is also NP-hard, and the authors in [7] suggested a greedy algorithm, which produces a (1 \u2212 e\u22121)approximation of the optimal value with O(n5) computation complexity. Specifically, it first enumerates all the subsets of cardinalities at most three, then greedily adds the elements with maximum marginal values per weight to every subset starting with three elements, and finally outputs the suboptimal subset. Although the solution has a (1\u2212e\u22121)-approximation guarantee, the O(n5) computation cost prevents this greedy algorithm from being widely used in practice. Hence some modified versions of the greedy algorithm have been developed. The authors in [11] applied it to document summarization with a (1 \u2212 e\u22121/2) performance guarantee. In [12], the so-called cost effective forward (CEF) algorithm for outbreak detection was proposed, which produces a solution with a (1\u2212 e\u22121)/2approximation guarantee and requires only O(Mn) computation complexity, where M is the knapsack budget when d = 1.\nThe considered d-MASK problem is a generalization of the above problems to maximize a submodular function under more than one budgeted constraints. A framework was proposed in [6] for maximizing a submodular function subject to a d-knapsack constraint, which yields a (1 \u2212 e\u22121 \u2212 )- approximation for any > 0. However, it is hard to implement this algorithm, since it involves some high-order terms with respect to the number of budgets, making it inappropriate for processing large datasets [13]. Later, an accelerated algorithm was developed in [14]. It runs for O (1/\u03b4) rounds in MapReduce [15] for a constant \u03b4, and provides an \u2126 (1/d)approximation. However, this algorithm needs an O(log n) blowup in communication complexity among various parts. As observed in [16], such a blowup decreases its applicability in practice. Note that the authors in [14] mentioned that the MapReduce method with an \u2126 (1/d)-approximation can be extended to execute in a streaming fashion, but did not provide any concrete algorithms and the associated analysis.\nTable I shows the comparison among the approximation\n3\nguarantees and computation costs of the aforementioned algorithms against our proposed algorithm.\nTo our best knowledge, this paper is the first to propose an efficient streaming algorithm for maximizing a monotone submodular function under a d-knapsack constraint, with 1) a constant-factor approximation guarantee, 2) no assumption on full access to the dataset, 3) execution of a single pass, 4) O(b log b) memory requirement, 5) O(log b) computation complexity per element, and 6) only assumption on monotonicity and submodularity of the objective function. In the following section, we describe the proposed algorithm in details."}, {"heading": "III. STREAMING ALGORITHMS FOR MAXIMIZING MONOTONE SUBMODULAR FUNCTIONS", "text": ""}, {"heading": "A. Special Case: One Cardinality Constraint", "text": "We first consider a special case of the d-MASK problem: maximizing a submodular function subject to one cardinality constraint:\nmaximize S\u2286V f(S) subject to |S| \u2264 k. (2)\nIn [4], the authors proved this problem is NP-hard and proposed a classical greedy algorithm. At each step of the algorithm, as we explained earlier, the element with the largest marginal value is added to the solution set. This operation, in fact, reduces the \u201cgap\u201d to the optimal solution by a significant amount. Formally, if element j is added to the current solution set S by the greedy algorithm, the marginal value \u2206f (j|S) of this picked element should be at least above certain threshold. In [5], the authors developed the so-called Sieve-Streaming algorithm, where the threshold for the marginal value is set to be (OPT/2\u2212f(S))/(k\u2212|S|), where S is the current solution set, k is the maximum allowed number of elements in S, and OPT is the optimal value of the optimization problem. In our paper, for this submodular maximization problem under a single cardinality constraint, we first introduce a simple streaming algorithm under the assumption that we have the knowledge of the optimal value of the problem.\nAlgorithm 1 Simple Streaming Algorithm 1: Input: v such that \u03b1OPT \u2264 v \u2264 OPT, for some \u03b1 \u2208 (0, 1]. 2: S := \u2205. 3: for j := 1 to n 4: if f(S \u222a {j})\u2212 f(S) \u2265 v2k and |S| \u2264 k then 5: S := S \u222a {j}. 6: end if 7: end for 8: return S.\nTheorem 1. The simple streaming algorithm (Algorithm 1) produces a solution S such that\nf(S) \u2265 \u03b1 2 OPT.\nProof: Given v \u2208 [\u03b1OPT,OPT], let us discuss the following two cases.\nCase 1: |S| = k. For 1 \u2264 i \u2264 k, let ai be the element added to S in the i-th iteration of the for-loop. Then we obtain\nf(S) = f({a1, a2, . . . , ak}) \u2265 f({a1, a2, . . . , ak})\u2212 f(\u2205)\n= k\u2211 i=1 [ f({a1, a2, . . . , ai})\u2212 f({a1, a2, . . . , ai\u22121}) ] .\nBy the condition in Line 4 of Algorithm 1, for 1 \u2264 i \u2264 k, we have\nf({a1, a2, . . . , ai})\u2212 f({a1, a2, . . . , ai\u22121}) \u2265 v\n2k ,\nand hence\nf(S) \u2265 v 2k \u00b7 k \u2265 \u03b1 2 OPT.\nCase 2: |S| < k. Let S\u0304 = S\u2217\\S, where S\u2217 is the optimal solution to the Problem (2). For each element a \u2208 S\u0304, we have\nf(S \u222a {a})\u2212 f(S) < v 2k .\nSince f is monotone submodular, we obtain\nf(S\u2217)\u2212 f(S) = f(S \u222a S\u0304)\u2212 f(S) \u2264 \u2211 a\u2208S\u2032 [f(S \u222a {a})\u2212 f(S)] < v 2k \u00b7 k \u2264 1 2 f(S\u2217),\nwhich implies that\nf(S) > 1\n2 f(S\u2217) =\n1 2 OPT \u2265 \u03b1 2 OPT.\nThis simple streaming algorithm produces a solution by visiting every element in the ground set only once. But it requires the knowledge of the optimal value of the problem. Besides, when the elements have non-uniform weights, this algorithm does not work. To deal with the problem with nonuniform weights and more than one constraint, we are going to modify the greedy rule and take the weight-dependent marginal values into account in a streaming fashion.\n4"}, {"heading": "B. General Case: Multiple Knapsack Constraints", "text": "In order to get the desirable output, in this subsection, we first assume we have some knowledge of OPT, and then remove this assumption by estimating OPT based on the maximum value per weight of any single element. At the end, we will remove all assumptions to develop the final version of the streaming algorithm for the general case of a d-MASK problem.\nSuppose that we know a value v such that \u03b1OPT \u2264 v \u2264 OPT for some 0 < \u03b1 \u2264 1. That is, we know an approximation of OPT up to a constant factor \u03b1. We then construct the following algorithm to choose a subset S with the knowledge of the optimal value of the problem.\nAlgorithm 2 OPT-KNOWN-d-MASK 1: Input: v such that \u03b1OPT \u2264 v \u2264 OPT, for some \u03b1 \u2208 (0, 1]. 2: S := \u2205. 3: for j := 1 to n 4: if ci,j \u2265 b2 and f({j}) ci,j\n\u2265 2vb(1+2d) for some i \u2208 [1, d] then\n5: S := {j}. 6: return S. 7: end if 8: if \u2211 l\u2208S\u222a{j} ci,l \u2264 b and \u2206f (j|S) ci,j\n\u2265 2vb(1+2d) for all i \u2208 [1, d] then\n9: S := S \u222a {j}. 10: end if 11: end for 12: return S.\nAt the beginning of the algorithm, the solution set S is set to be an empty set. The algorithm will terminate when either we find an element j \u2208 V satisfying\nci,j \u2265 b\n2 and f({j}) ci,j \u2265 2v b(1 + 2d) for some i \u2208 [1, d], (3)\nor we finish one pass through the dataset. Here we define that an element j \u2208 V is a big element if it satisfies (3). When the algorithm finds a big element a, it simply outputs {a} and terminates. The following lemma shows that {a} is already a good enough solution.\nLemma 1. Assume the input v satisfies \u03b1OPT \u2264 v \u2264 OPT, and V has at least one big element. The output S of Algorithm 2 satisfies\nf(S) \u2265 \u03b1 1 + 2d OPT.\nProof: Let a be the first big element that Algorithm 2 finds. Then {a} is output and the algorithm terminates. Therefore, by (3), we have\nf(S) = f({a}) \u2265 2v b(1 + 2d) \u00b7 b 2 = v 1 + 2d \u2265 \u03b1 1 + 2d OPT.\nWhen V does not contain any big elements, during the data streaming, an element j is added to the solution set S if 1)\nthe marginal value per weight for each knapsack constraint \u2206f (j|S)/ci,j is at least \u03b2v/b for 1 \u2264 i \u2264 d, and 2) the overall d-knapsack constraint is still satisfied. In this paper, we set \u03b2 = 2b1+2d , which gives us the best approximation guarantee as shown in the proof of Theorem 2. The following lemma shows the property of the output of Algorithm 2.\nLemma 2. Assume that V has no big elements. The output S of Algorithm 2 has the following two properties:\n1) There exists an ordering a1, a2, . . . , a|S| of the elements in S, such that for all 0 \u2264 t < |S| and 1 \u2264 i \u2264 d, we have\n\u2206f (at+1|St) ci,at \u2265 2v b(1 + 2d) , (4)\nwhere St = {a1, a2, . . . , at}. 2) Assume that for 1 \u2264 i \u2264 d, \u2211|S| t=1 ci,at \u2264 b/2. Then\nfor each aj \u2208 V , there exists an index \u00b5(aj), with 1 \u2264 \u00b5(aj) \u2264 d such that\n\u2206f (aj |S) c\u00b5(aj),aj < 2v b(1 + 2d) .\nProof: 1) For 0 \u2264 t < |S|, at the (t + 1)-th step of the algorithm, assume that at+1 is the element added to the current solution set St = {a1, a2, . . . , at}. Then a1, a2, . . . , a|S| forms an ordering satisfying (4).\n2) By contradiction, assume that there exists j \u2208 V such that for 1 \u2264 i \u2264 d, we have\nf(S \u222a {j})\u2212 f(S) ci,j \u2265 2v b(1 + 2d) .\nSince j is not a big element and f is submodular, we have ci,j < b/2, for 1 \u2264 i \u2264 d. Then j can be added into S, where a contradiction occurs.\nWe then establish the following theorem to show that Algorithm 2 produces an ( \u03b1\n1+2d\n) -approximation of the optimal\nsolution to Problem (1).\nTheorem 2. Assuming that the input v satisfies \u03b1OPT \u2264 v \u2264 OPT, Algorithm 2 has the following properties: \u2022 It outputs S that satisfies f(S) \u2265 \u03b11+2dOPT; \u2022 It only goes one pass over the dataset, stores at most\nO(b) elements, and has O(d) computation complexity per element.\nProof: If V contains at least one big element, by Lemma 1, we have\nf(S) \u2265 \u03b1 1 + 2d OPT;\notherwise, we discuss the following two cases: Case 1: \u2211 j\u2208S ci,j \u2265 b/2, for some i \u2208 [1, d]. By the submodularity of f and Property 1) in Lemma 2, we have\nf(S) \u2265 2v b(1 + 2d) \u2211 j\u2208S ci,j \u2265 v 1 + 2d \u2265 \u03b1 1 + 2d OPT.\nCase 2: \u2211 j\u2208S ci,j < b/2, for all i \u2208 [1, d]. Let S\u2217i be the set of elements aj \u2208 S\u2217\\S such that \u00b5(aj) = i, for 1 \u2264 i \u2264 d.\n5 Then we have S\u2217 \\ S = \u22c3\n1\u2264i\u2264d S \u2217 i . With the help of the\nsubmodularity of f and Property 2) in Lemma 2, we obtain\nf(S \u222a S\u2217i )\u2212 f(S) \u2264 2v\nb(1 + 2d) \u2211 aj\u2208S\u2217i c\u00b5(aj),aj < 2v 1 + 2d ,\nfor 1 \u2264 i \u2264 d. Then we have f(S\u2217)\u2212 f(S) = f(S \u222a (S\u2217 \\ S))\u2212 f(S)\n\u2264 \u2211\n1\u2264i\u2264d\n[f(S \u222a S\u2217i )\u2212 f(S)] < 2dv\n1 + 2d ,\nand further,\nf(S) > f(S\u2217)\u2212 2dv 1 + 2d \u2265 1 1 + 2d OPT.\nIn both cases, we conclude\nf(S) \u2265 \u03b1 1 + 2d OPT.\nSince we have ci,j \u2265 1 for all i \u2208 [1, d], j \u2208 [1, n], we store at most O(b) elements during the algorithm. In the for-loop, we compare the values at most d times. Then the computation cost per element in the algorithm is O(d).\nWe can obtain an approximation of the optimal value OPT by solving the d-MASK problem via Algorithm 2 . But in certain scenarios, requiring the knowledge of an approximation to the optimization problem and utilizing the approximation in Algorithm 2 lead to a chicken and egg dilemma. That is, we have to first estimate OPT and then use it to compute OPT. Fortunately, even in such scenarios, we still have the following lemma to estimate OPT if we know m , max1\u2264i\u2264d,1\u2264j\u2264n f({j})/ci,j , the maximum value per weight of any single element.\nLemma 3. Let Q = {\n[1 + (1 + 2d) ]l |l \u2208 Z, m 1 + (1 + 2d) \u2264 [1 + (1 + 2d) ]l \u2264 bm } for some with 0 < < 11+2d . Then there exists at least some v \u2208 Q such that [1\u2212 (1 + 2d) ]OPT \u2264 v \u2264 OPT.\nProof: First, choose i\u2032 \u2208 [1, d], j\u2032 \u2208 [1, n] such that f({j\u2032})/ci\u2032,j\u2032 = m. Since ci\u2032,j\u2032 \u2265 1, we have\nOPT \u2265 f({j\u2032}) = mci\u2032,j\u2032 \u2265 m.\nAlso, let {j1, j2, . . . , jt} be a subset of V such that f({j1, j2, . . . , jt}) = OPT. Then by the submodularity of f , OPT = f(\u2205) + t\u2211 i=1 [f({j1, j2, . . . , ji})\u2212 f({j1, j2, . . . , ji\u22121})]\n\u2264 f(\u2205) + t\u2211 i=1 [f({ji})\u2212 f(\u2205)]\n\u2264 t\u2211 i=1 f({ji}) \u2264 m t\u2211 i=1 c1,ji \u2264 bm.\nSetting v = [1 + (1 + 2d) ]blog1+(1+2d) OPTc, we then obtain m\n1 + (1 + 2d) \u2264 1 1 + (1 + 2d) OPT \u2264 v \u2264 OPT \u2264 bm,\nand\nv \u2265 1 1 + (1 + 2d) OPT \u2265 [1\u2212 (1 + 2d) ]OPT.\nBased on Lemma 3, we propose the following algorithm that gets around the chick and egg dilemma.\nAlgorithm 3 m-KNOWN-d-MASK 1: Input: m. 2: Q := {[1 + (1 + 2d) ]l|l \u2208 Z, 3: m1+(1+2d) \u2264 [1 + (1 + 2d) ]\nl \u2264 bm}. 4: for v \u2208 Q 5: Sv := \u2205. 6: end for 7: for j := 1 to n 8: if ci,j \u2265 b2 and f({j}) ci,j\n\u2265 2vb(1+2d) for some i \u2208 [1, d] then\n9: S := {j}. 10: return S. 11: end if 12: for v \u2208 Q 13: if \u2211 l\u2208S\u222a{j} ci,l \u2264 b and \u2206f (j|S) ci,j\n\u2265 2vb(1+2d) for all i \u2208 [1, d] then\n14: Sv := Sv \u222a {j}. 15: end if 16: end for 17: end for 18: S := argmax\nSv,v\u2208Q f(Sv).\n19: return S.\nThen we establish the following theorem to show that the above algorithm achieves a ( 1 1+2d \u2212 ) -approximation\nguarantee, and requires O ( b log b ) memory and O ( log b ) computation complexity per element.\nTheorem 3. With m known, Algorithm 3 has the following properties: \u2022 It outputs S that satisfies f(S) \u2265 ( 1 1+2d \u2212 )\nOPT; \u2022 It goes one pass over the dataset, stores at most O ( b log b d ) elements, and has O ( log b ) computation\ncomplexity per element.\nProof: By Lemma 3, we choose v \u2208 Q such that [1 \u2212 (1 + 2d) ]OPT \u2264 v \u2264 OPT. Then by Theorem 2, the output S satisfies\nf(S) \u2265 1\u2212 (1 + 2d) 1 + 2d\nOPT = ( 1 1 + 2d \u2212 ) OPT.\nNotice that there are at most \u2308 log1+(1+2d) b \u2309 + 1 (of order log b d ) elements in Q. At the end of the algorithm, Sv with\n6 the largest function value will be picked to be the output. Since S contains at most b elements, Algorithm 3 stores at most O ( b log b d ) elements and has O ( log b ) computation complexity per element. Introducing the maximum marginal value per weight m avoids the chicken and egg dilemma in Algorithm 2. With m known, Algorithm 3 needs only one pass over the dataset. However, we need an extra pass through the dataset to obtain the value of m. In the following, we will develop our final one-pass streaming algorithm with m unknown.\nAlgorithm 4 d-KNAPSACK-STREAMING 1: Q := {[1 + (1 + 2d)\u03b5]l|l \u2208 Z}. 2: for v \u2208 Q 3: Sv := \u2205. 4: end for 5: m := 0. 6: for j := 1 to n 7: if ci,j \u2265 b2 and f({j}) ci,j\n\u2265 2vb(1+2d) for some i \u2208 [1, d] then\n8: S := {j}. 9: return S.\n10: end if 11: for i := 1 to d 12: m := max{m, f({j})/ci,j}. 13: end for 14: Q := {[1 + (1 + 2d)\u03b5]l|l \u2208 Z, 15: m1+(1+2d) \u2264 [1 + (1 + 2d)\u03b5]\nl \u2264 2bm}. 16: for v \u2208 Q 17: if \u2211 l\u2208S\u222a{j} ci,l \u2264 b and \u2206f (j|S) ci,j\n\u2265 2vb(1+2d) for all i \u2208 [1, d] then\n18: Sv := Sv \u222a {j}. 19: end if 20: end for 21: end for 22: S := argmax\nSv,v\u2208Q f(Sv).\n23: return S.\nWe modify the estimation candidate set Q into {[1 + (1 + 2d) ]l|l \u2208 Z, m1+(1+2d) \u2264 [1 + (1 + 2d) ]\nl \u2264 2bm}, and maintain the variable m that holds the current maximum marginal value per weight of all single element. During the data streaming, if a big element a is observed, the algorithm simply outputs {a} and terminates. Otherwise, the algorithm will update m and the estimation candidate set Q. If the marginal value per weight for each knapsack constraint \u2206f (j|S)/ci,j is at least 2v/b(1 + 2d) for 1 \u2264 i \u2264 d, and the overall d-knapsack constraint is still satisfied, then an element j is added to the corresponding candidate set. Then we establish the following theorem, which shows the property of the output of Algorithm 4. Its proof follows the same lines as the proof of Theorem 3.\nTheorem 4. Algorithm 4 has the following properties: \u2022 It outputs S that satisfies that f(S) \u2265 (\n1 1+2d \u2212\n) OPT;\n\u2022 It goes one pass over the dataset, stores at most\nO ( b log b d ) elements, and has O ( log b ) computation complexity per element."}, {"heading": "C. Online Bound", "text": "To evaluate the performance of our proposed algorithms, we need to compare the function values obtained by our streaming algorithm against OPT, by calculating their relative difference. Since OPT is unknown, we could use an upper bound of OPT to evaluate the performance of the proposed algorithms.\nBy Theorem 4, we obtain\nOPT \u2264 1 + 2d 1\u2212 (1 + 2d) f(S). (5)\nThen 1+2d1\u2212(1+2d) f(S) is an upper bound of the optimal value to the d-MASK problem. In most of cases, this bound is not tight enough. In the following, we provide a much tighter bound derived by the submodularity of f .\nTheorem 5. Consider a subset S \u2286 V . For 1 \u2264 i \u2264 d, let ri,s = \u2206f (s|S)/ci,s, and si,1, . . . , si,|V \\S| be the sequence such that ri,si,1 \u2265 ri,si,2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 ri,si,|V \\S| . Let ki be the integer such that \u2211ki\u22121 j=1 ci,si,j \u2264 b and \u2211ki j=1 ci,si,j > b. And\nlet \u03bbi = ( b\u2212 \u2211ki\u22121 j=1 ci,si,j )/ ci,si,ki . Then we have\nOPT = max CxS\u2032\u2264b\nf(S\u2032) \u2264 f(S)\n+ min 1\u2264i\u2264d ki\u22121\u2211 j=1 \u2206f (si,j |S) + \u03bbi\u2206f (si,ki |S)  . (6) Proof: Here we use a similar proof as the proof of Theorem 8.3.3 in [17], where the author deals with the submodular maximization problem under one knapsack constraint. Let S\u2217 be the optimal solution to Problem (1). First we consider the 1-MASK problem, which has the same objective function as Problem (1) but only with the i-th knapsack constraint. Assume S\u2217i is its optimal solution. Since this 1- MASK problem has fewer constraints than Problem (1), we have f(S\u2217) \u2264 f(S\u2217i ). Hence,\nf(S\u2217) \u2264 min 1\u2264i\u2264d f(S\u2217i ). (7)\nSince f is monotone submodular, for 1 \u2264 i \u2264 d,\nf(S\u2217i ) \u2264 f(S \u222a S\u2217i ) \u2264 f(S) + \u2211 s\u2208S\u2217i \u2206f (s|S). (8)\nWe first assume that all weights ci,j and knapsack b are rational numbers. For the i-th 1-MASK problem, we can multiply all ci,j and b by the least common multiple of their denominators, making each weight and budget be an integer. We then replicate each element s in V into ci,s copies. Let s\u2032i denote any one copy of s, and let V \u2032 i and S \u2217 i \u2032 be the sets of the copies of all elements in V and Si\u2217, respectively. Also,\n7 define \u2206\u2032f (s \u2032 i|S) , \u2206f (s|S)/ci,s. Then\n\u2211 s\u2208S\u2217i \u2206f (s|S) = \u2211 s\u2032i\u2208S\u2217i \u2032 \u2206\u2032f (s \u2032 i|S)\n\u2264 max K\u2032\u2286V \u2032i ,|K\u2032|\u2264b \u2211 s\u2032i\u2208K\u2032 \u2206\u2032f (s \u2032 i|S). (9)\nTo find the value of the right-hand side of (9), we actually need to solve a unit-cost modular optimization problem as follows. We first sort all elements s\u2032 in V \u2032i such that the corresponding values \u2206\u2032f (s\n\u2032|S) form a non-increasing sequence. In this sequence, the first b elements are ci,si,j copies of si,j for 1 \u2264 j \u2264 ki \u2212 1, and ( b\u2212 \u2211ki\u22121 j=1 ci,si,j ) copies of si,ki . Therefore, we obtain\nmax K\u2032\u2286V \u2032i |K\u2032|\u2264b \u2211 s\u2032i\u2208K\u2032 \u2206\u2032f (s \u2032 i|S) = ki\u22121\u2211 j=1 \u2206f (si,j |S) + \u03bbi\u2206(si,ki |S).\n(10)\nCombining (7), (8), (9) and (10), we obtain (6). For irrational weights and knapsacks, let { ci,si,j ,t }\u221e t=1\nand {bt}\u221et=1 be two rational sequences with limits ci,si,j and b, respectively. And further let ki,t be the integer such that\u2211ki,t\u22121 j=1 ci,si,j ,t \u2264 bt and \u2211ki,t j=1 ci,si,j ,t > bt, and let\n\u03bbi,t = bt \u2212 ki,t\u22121\u2211 j=1 ci,si,j ,t / ci,si,ki,t ,t.\nThen {\u03bbi,t}\u221et=1 is a rational sequence with limit \u03bbi. According to the above argument, we obtain for each t,\nmax CxS\u2032\u2264b\nf(S\u2032) \u2264 f(S)\n+ min 1\u2264i\u2264d ki\u22121\u2211 j=1 \u2206f (si,j |S) + \u03bbi,t\u2206f (si,ki |S)  .\nBy letting t go to infinity, we then finish the proof.\nA bound is called to be offline [17] if it can be stated before we run the algorithm; otherwise, it is an online one [17]. Here, we obtain an offline bound (5) and an online bound (6), the latter of which can be calculated by the following algorithm.\nAlgorithm 5 Online Bound of the d-MASK Problem 1: Input: S. 2: for i := 1 to d 3: S\u2032i := \u2205. 4: for s in V 5: ri,s := \u2206f (s|S)/ci,s. 6: end for 7: while {s \u2208 V \\ (S \u222a S\u2032i)| \u2211 j\u2208S\u222aS\u2032i\u222a{s}\nci,j \u2264 b} 6= \u2205 8: s\u2032 := argmax\ns\u2208V \\(S\u222aS\u2032i), \u2211 j\u2208S\u222aS\u2032\ni \u222a{s} ci,j\u2264b\nri,s.\n9: S\u2032i := S \u2032 i \u222a {s\u2032}.\n10: end while 11: s\u2032 := argmax\ns\u2208V \\(S\u222aS\u2032i), \u2211 j\u2208S\u222aS\u2032\ni \u222a{s} ci,j\u2264b\nri,s.\n12: \u03bbi := (b\u2212 \u2211 s\u2208S\u2032i\nci,s)/ci,s\u2032 . 13: \u03b4i := \u2211 s\u2208S\u2032i\n\u2206f (s|S) + \u03bbi\u2206f (s\u2032|S). 14: end for 15: return f(S) + min1\u2264i\u2264d \u03b4i."}, {"heading": "D. Problems with Ground-Set Dependent Submodular Functions", "text": "In the previous sections, we have discussed the case when the submodular function f is independent of the ground set V . In the following, we will discuss the setting where f is additively decomposable [8], and the value of f(S) depends on not only the subset S but also the ground set V . Here a function f is called to be additively decomposable [8] over the ground set V , if there exists a family of functions {fi}|V |i=1 with fi : 2V \u2192 [0,\u221e) independent of the ground set V such that\nf(S) = 1 |V | \u2211 i\u2208V fi(S). (11)\nAlgorithm 4 is still useful for the case when f is dependent on the ground set but additively decomposable. To reduce the computational complexity, we randomly choose a small subset V\u0303 of V , and use\nfV\u0303 (S) , 1\n|V\u0303 | \u2211 i\u2208V\u0303 fi(S)\ninstead of f in Algorithm 4. It can be proved that with a high probability, we can still obtain a good approximation to the optimal solution, when fi\u2019s are bounded. The accuracy of the approximation is quantified by the following theorem.\nTheorem 6. Assume that for S \u2286 V and 1 \u2264 i \u2264 n, |fi(S)| \u2264 1. We uniformly choose a subset V\u0303 from V , with\n|V\u0303 | \u2265 2 \u22122b2 (b log |V |+ log(2/\u03b4)) ,\nand use fV\u0303 instead of f in Algorithm 4. Then with probability of at least 1\u2212 \u03b4, the output S of Algorithm 4 satisfies\nfV\u0303 (S) \u2265 ( 1 1 + 2d \u2212 ) (OPT\u2212 ).\n8 Its proof follows the similar argument as the proof of Theorem 6.2 in [5], where the authors deal with the submodular maximization problem under one cardinality constraint. Now we adopt a two-pass streaming algorithm for the dMASK problem with ground-set dependent submodular objective functions: in the first pass, we utilize reservoir sampling [18] to sample an evaluation set V\u0303 randomly; in the second pass, we run Algorithm 4 with the objective function fV\u0303 instead of f ."}, {"heading": "IV. APPLICATIONS", "text": "In this section, we discuss two real-world applications for Algorithm 4: news recommendation and scientific literature recommendation."}, {"heading": "A. News Recommendation", "text": "Nowadays, people are facing many news articles on the daily basis, which highly stresses their limited reading time. A news recommendation system helps people quickly fetch the information they need. Specifically, it provides the most relevant and diversified news to people by exploiting their behaviors, considering their reading preferences, and learning from their previous reading histories.\nHowever, the vast amount of news articles in the dataset are hard to be processed efficiently. In [19], the authors modeled the user behavior as a submodular maximization problem. Based on the learning result, a classical greedy algorithm [4] was implemented to provide a set of relevant articles to the users. However, the large amount of data in the dataset prevents the classical greedy algorithm from producing the solution in time due to its expensive computation cost. Besides, the reading behavior of the users was oversimplified in [19], where it is assumed that each user reads a fixed number of articles per day. Since the time spent on different news articles varies, it is more reasonable to use the number of words of the articles as the measure of the reading behaviour. Hence, we can formulate this question into a 1-MASK problem as follows:\nmaximize S\u2286V\nf(S) = wTF(S)\nsubject to \u2211 j\u2208S cj \u2264 b,\nwhere cj is the number of words in article j. Here F : 2V \u2192 [0,\u221e)m, where m is the number of features. We require the total number of words in the selected articles not to exceed a specified budget b, due to the limitation of the user reading time. In addition, we assume that the non-negative parameter vector w is learnt by a statistical learning algorithm, based on the historical user preference (three such learning algorithms can be found in [19], [20], and [21], respectively). Let (\u03c61(d), . . . , \u03c6m(d)) be the characteristic vector of article d, where for 1 \u2264 j \u2264 m, \u03c6j(d) = 1 if d has feature j, \u03c6j(d) = 0, otherwise. We then define F(S) = (F1(S), . . . , Fm(S)); here for 1 \u2264 j \u2264 m, Fj(S) is the aggregation function of S with respect to feature j and defined by\nFj(S) , log\n( 1 +\n\u2211 s\u2208S \u03c6j(s)\n) .\nFig. 1. Comparison of Utilities and Computation Costs between the Greedy Algorithm and Streaming Algorithm\nThis choice of function Fj guarantees both precision and coverage of the solution set. On one hand, the monotonicity of Fj(S) encourages feature j to be selected if its corresponding weighting parameter wj (the j-th coordinate of the vector w) is relatively large. On the other hand, the diminishing return property of Fj prevents too many items with feature j from being selected.\nNotice that function Fj is a monotone submodular function. To see this, let Gj(S) , \u2211 s\u2208S \u03c6j(s).\nObviously, Gj(S) is a non-decreasing modular function. With the fact that \u03b6(x) , log(1 + x) is an increasing concave function, we can conclude that Fj(S) = \u03b6(Gj(S)) is a monotone submodular function. Since both monotonicity and submodularity are closed under the non-negative linear combinations [22], f is a monotone submodular function as well. The solution based on Algorithm 4 to this 1-MASK problem provides the user a quick news recommendation.\nAs an illustration, we analyze the dataset collected in [23], which contains over 7, 000 feedback entries from 25 people with around 8, 000 news articles. We set m = 480 and b = 20, with each entry of C randomly chosen from a uniform distribution over {1, 2, 3, 4, 5}. The learning algorithm proposed in [19] is used to calculate w. We then compare Algorithm 4 with the greedy algorithm in [7].\nIn Fig. 1, we set the objective function value obtained by the classical greedy algorithm and its computation time both to be 1, after using them to normalize the function value and computation time corresponding to our streaming algorithm, respectively. It has been shown that our streaming algorithm achieves 94% utility of the greedy algorithm, but only requires a tiny fraction of the computation cost. Thus the proposed algorithm works well in the news recommendation system and is practically useful over large datasets."}, {"heading": "B. Scientific Literature Recommendation", "text": "Next, we introduce an application in scientific literature recommendation. Nowadays, the researchers have to face an enormous amount of articles and collect information that they\n9\nare interested in, where they have to filter the massive existing scientific literatures and pick the most useful ones. A common approach to locate the targeted literatures is based on the socalled citation networks [24]. The authors in [24] mapped a citation network onto a rating matrix to filter research papers. In [25], an algorithm utilizing the random-walker properties was proposed. It transforms a citation matrix into a probability transition matrix and outputs the entries with the highest biased PageRank scores.\nWe here propose a new scientific literature recommendation system based on the citation networks and the newly proposed streaming algorithm (Algorithm 4). Consider a directed acyclic graph G = (V,E) with V = {1, 2, . . . , n}, where each vertex in V represents a scientific article. Let Ri denote the number of references contained in article i. The arcs between papers represent their citation relationship. For two vertices i, j \u2208 V , arc (i, j) \u2208 E if and only if paper i cites paper j. The information spreads over the reverse directions of the arcs. As an example, Fig. 2 presents a citation network, which contains six vertices and seven arcs. Each of six papers cites a certain number of references. The information initiates from a set of vertices (source papers), and then spreads across the network. Let A be the collection of the source papers. Our target is to select a subset S out of V to quickly detect the information spreading of A. For example, A = {1, 3, 4} in\nwhere C = (ci,j) is a 3\u00d7 n matrix and b = (b1, b2, b3)T . Observe that the papers in A transfer their influence through the citation network, but this influence becomes less as it spreads through more hops. Let T (s, a) be the length of the shortest directed path from s to a. Then the shortest path length from any vertex in S to a is defined as\nT (S, a) , min s\u2208S T (s, a).\n1The reason why we set d = 3 will be explained later in this section; based on the different usages, the number of knapsack constraints and the corresponding budgets can be changed accordingly.\nLet W (a) be a pre-assigned weight to each vertex a \u2208 A such that \u2211 a\u2208AW (a) = 1. Then our goal is to minimize the expected penalty\n\u03c0(S) , \u2211 a\u2208A W (a) min{T (S, a), Tmax},\nor maximize the expected penalty reduction R(S) , Tmax \u2212 \u03c0(S) = \u2211 a\u2208A W (a)[Tmax \u2212 T (S, a)]+,\nwhere [x]+ , max{x, 0} and Tmax is a given maximum penalty. Note that R is a monotone submodular function. To see this, for two subsets B \u2286 C \u2286 V , we have T (B, a) \u2265 T (C, a) for any a \u2208 A, such that R(B) \u2264 R(C); Tmax \u2212 T (S, a) is a submodular function with respect to S since\nT (B, a)\u2212 T (B \u222a {v}, a) = [T (B, a)\u2212 T (v, a)]+\n\u2265 [T (C, a)\u2212 T (v, a)]+ = T (C, a)\u2212 T (C \u222a {v}, a),\nwith v \u2208 V \\ C. Then R(S) is also submodular, since it is a convex combination of Tmax \u2212 T (S, a) for a \u2208 A.\nWe construct three constraints in (12) from the aspects of recency, biased PageRank score, and reference number respectively. The first aspect is from the fact that readers prefer to read the recently published papers. Let c1,j be the time difference between the publishing date of paper j and the current date, and b1 be the corresponding limit.\nFor the second aspect, the classical PageRank algorithm [26] could be used to compute an important score for every vertex in the graph: a vertex will be assigned a higher score if it is connected to a more important vertex with a lower out-degree. The authors in [25] introduced a so-called biased PageRank score. It is a measure of the significance of each paper, not only involving the propagation and attenuation properties of the network, but also taking the set of source vertices into account. Let \u03c1(j) be the biased PageRank score of article j. We further choose a function \u03be(x) , 1 + 11+x to map the PageRank score onto (1, 2]. Then paper j with the smaller value c2,j , \u03be(\u03c1(j)) is more valuable for the researchers. Also we set b2 to be corresponding budget.\nThirdly, we assume that more references listed in the paper, more time the reader spends on picking the valuable information. Then we set c3,j to be the number Rj of references in paper j and b3 be the budget of the total number of references.\nTo evaluate the performance of Algorithm 4, for scientific literature recommendation, we utilize a dataset collected in [27]. This dataset includes more than 20,000 papers in the Association of Computational Linguistics (ACL). There are two methods to evaluate the performance of an algorithm for literature recommendation: online evaluation and offline evaluation. In the online evaluation, some volunteers are invited to test the performance of the recommendation system and express their opinions. Here we use the offline evaluation to compare the function values obtained by our proposed algorithm (Algorithm 4) and the PageRank algorithm proposed in [25].\n10\nWe perform the sensitive analysis over different knapsack constraints. With the other two constraints fixed, we change the value of the budget corresponding to the recency, biased PageRank score or reference number, respectively. Here we randomly select five nodes as the source papers. We set Tmax = 50 and W (a) = 0.2 for each source paper a. The results for the optimal objective values are shown in Fig. 3 (with fixed b2 = 10, b3 = 20), Fig. 4 (with fixed b1 = 20, b3 = 20) and Fig. 5 (with fixed b1 = 20, b2 = 10), respectively. It can be observed that the relative difference is around 10% between the function values obtained by our streaming algorithm (blue lines) and the corresponding online bounds (red lines).\nAlso, we find that our algorithm highly outperforms the\nbiased PageRank algorithm as shown in Figs. 3, 4 and 5, respectively. Although the biased PageRank algorithm suggests the papers with high biased PageRank scores, most of the suggested papers have very long distances from the set of source articles (even disconnected from the source papers in some cases), which leads to a very low objective function value."}, {"heading": "V. CONCLUSIONS", "text": "In this paper, we proposed a streaming algorithm to maximize a monotone submodular function under a d-knapsack constraint. It leads to a ( 1 1+2d \u2212 )\napproximation of the optimal value, and requires only a single pass through the dataset and a small memory size. It achieves a major fraction of the utility function value obtained by the greedy algorithm with a much lower computation cost, which makes it very practically implementable. Our algorithm provides a more efficient way to solve the related combinatorial optimization problems, which could find many good applications, such as in news and scientific literature recommendations as shown in the paper."}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank Dr. Nao Kakimura for comments that greatly improved the manuscript."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Submodular maximization problems belong to the family of combinatorial optimization problems and enjoy wide applications. In this paper, we focus on the problem of maximizing a monotone submodular function subject to a d-knapsack constraint, for which we propose a streaming algorithm that achieves a ( 1 1+2d \u2212 ) -approximation of the optimal value, while it only needs one single pass through the dataset without storing all the data in the memory. In our experiments, we extensively evaluate the effectiveness of our proposed algorithm via two applications: news recommendation and scientific literature recommendation. It is observed that the proposed streaming algorithm achieves both execution speedup and memory saving by several orders of magnitude, compared with existing approaches.", "creator": "LaTeX with hyperref package"}}}