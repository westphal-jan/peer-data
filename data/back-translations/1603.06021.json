{"id": "1603.06021", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "A Fast Unified Model for Parsing and Sentence Understanding", "abstract": "Tree-structured neural networks exploit valuable syntactical parser information in interpreting the meaning of sentences, but they suffer from two major technical problems that make them slow and cumbersome for large-scale NLP tasks: they can only work with analyzed sentences and do not support direct batch processing. We solve these problems by introducing the stack augmented parser interpreter Neural Network (SPINN), which combines parsing and interpreting within a single tree-sequence hybrid model by integrating tree-structured sentence interpretations into the linear sequence structure of a shift-reducing parser. Our model supports batch processing for up to 25x faster than other tree-structured models, and its built-in parser allows working with unparsed data without much loss of accuracy.", "histories": [["v1", "Sat, 19 Mar 2016 00:22:20 GMT  (203kb,D)", "http://arxiv.org/abs/1603.06021v1", "Manuscript under review"], ["v2", "Mon, 13 Jun 2016 23:19:08 GMT  (352kb,D)", "http://arxiv.org/abs/1603.06021v2", "To appear at ACL 2016"], ["v3", "Fri, 29 Jul 2016 18:36:15 GMT  (349kb,D)", "http://arxiv.org/abs/1603.06021v3", "To appear at ACL 2016"]], "COMMENTS": "Manuscript under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel r bowman", "jon gauthier", "abhinav rastogi", "raghav gupta", "christopher d manning", "christopher potts"], "accepted": true, "id": "1603.06021"}, "pdf": {"name": "1603.06021.pdf", "metadata": {"source": "CRF", "title": "A Fast Unified Model for Parsing and Sentence Understanding", "authors": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "emails": ["sbowman@stanford.edu", "jgauthie@stanford.edu", "arastogi@stanford.edu", "rgupta93@stanford.edu", "manning@stanford.edu", "cgpotts@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "A wide range of current models in NLP are built around a neural network component that produces vector representations of sentence meaning (e.g., Sutskever et al., 2014; Tai et al., 2015). This component, the sentence encoder, is generally formulated as a learned parametric function from a sequence of word vectors to a sentence vector, and this function can take a range of different forms. Common sentence encoders include sequence-based recurrent neural network\n\u2217The first two authors contributed equally.\nmodels (RNNs, see Figure 1a) with Long ShortTerm Memory (LSTM, Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Ku\u0308chler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree.\nOf these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with\nar X\niv :1\n60 3.\n06 02\n1v 1\n[ cs\n.C L\n] 1\n9 M\nar 2\n01 6\nbatched computation and their reliance on external parsers. Batched computation\u2014performing synchronized computation across many examples at once\u2014yields order-of-magnitude improvements in model run time, and is crucial in enabling neural networks to be trained efficiently on large datasets. Because TreeRNNs use a different model structure for each sentence, as in Figure 1, batching is impossible in standard implementations. In addition, standard TreeRNN models can only operate on sentences that have already been processed by a syntactic parser, which slows and complicates the use of these models at test time for most applications.\nThis paper introduces a new model to address both these issues: the Stack-augmented ParserInterpreter Neural Network, or SPINN, shown in Figure 2. SPINN executes the computations of a tree-structured model in a linearized sequence, and can incorporate a neural network parser that produces the required parse structure on the fly. This design improves upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences without incurring a substantial additional computational cost, removing the dependence on an external parser. In addition, it supports batched computation for both parsed and unparsed sentences, which yields dramatic speedups over standard TreeRNNs. Finally, it supports a novel tree-\nsequence hybrid mechanism for handling local context in sentence interpretation that yields substantial gains in accuracy over pure sequence- or tree-based models.\nWe evaluate SPINN on the Stanford Natural Language Inference entailment task (SNLI, Bowman et al., 2015a), and find that it significantly outperforms other sentence-encoding-based models, and that it yields speed increases of up to 25x over a standard TreeRNN implementation."}, {"heading": "2 Related work", "text": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing or generation.\nSocher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods re-\nquire an expensive search process at test time. Our model presents a fast alternative approach."}, {"heading": "3 Our model: SPINN", "text": ""}, {"heading": "3.1 Background: Shift-reduce parsing", "text": "SPINN is inspired by the shift-reduce parsing formalism (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003).\nA shift-reduce parser accepts a sequence of input tokens x = (x0, . . . , xN\u22121) and consumes transitions t = (t0, . . . , tT\u22121), where each tt \u2208 {SHIFT, REDUCE} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binarybranching tree structure over N words, this requires 2N \u2212 1 transitions.\nThe parser uses two auxiliary data structures: a stack S of partially completed subtrees and a buffer B of tokens yet to be parsed. The parser is initialized with the stack empty and the buffer containing the tokens x of the sentence in order. Let \u3008S,B\u3009 = \u3008\u2205,x\u3009 denote this starting state. It next proceeds through the transition sequence, where each transition tt selects one of the two following operations. Below, the | symbol denotes the cons (concatenation) operator. We arbitrarily choose to always cons on the left in the notation below.\nSHIFT: \u3008S, x | B\u3009 \u2192 \u3008x | S,B\u3009. This operation pops an element from the buffer and pushes it onto the top of the stack.\nREDUCE: \u3008x | y | S,B\u3009 \u2192 \u3008(x, y) | S,B\u3009. This operation pops the top two elements from the stack, merges them into a binary tree with children (x, y), and pushes the result back onto the stack."}, {"heading": "3.2 Composition and representation", "text": "SPINN is based on a shift-reduce parser, but it is designed to produce a vector representation of a sentence as its output, rather than a tree as in standard shift-reduce parsing. It modifies the shiftreduce formalism by using fixed length vectors to represent each entry in the stack and the buffer.\nCorrespondingly, its REDUCE operation combines two vector representations from the stack into another vector using a neural network function.\nThe composition function When a REDUCE operation is performed, the vector representations of two tree nodes are popped off of the stack and fed into a composition function, which is a neural network function that produces a representation for a new tree node that is the parent of the two popped nodes. This new node is pushed on to the stack.\nThe TreeLSTM composition function (Tai et al., 2015) generalizes the LSTM neural network layer to tree- rather than sequence-based inputs, and it shares with the LSTM the idea of representing intermediate states as a pair of a fast-changing state representation ~h and a slower-changing memory representation ~c. Our version is formulated as:\n~i ~fl ~fr ~o ~g\n =  \u03c3 \u03c3 \u03c3 \u03c3\ntanh\n Wcomp ~h1s~h2s ~e +~bcomp (1)\n~c = ~fl ~c 2s + ~fr ~c 1s +~i ~g(2) ~h = ~o ~c(3)\nwhere \u03c3 is the sigmoid activation function, is the elementwise product, the pairs \u3008~h1s,~c 1s \u3009 and \u3008~h2s,~c 2s \u3009 are the two input tree nodes popped off the stack, and ~e is an optional vector-valued input argument which is either empty or comes from from an external source like the tracking LSTM (see Section 3.3). The result of this function, the pair \u3008~h,~c\u3009, is placed back on the stack. Each vector-valued variable listed is of dimensionD except ~e, of the independent dimension Dtracking.\nThe stack and buffer The stack and the buffer are arrays of N elements each (for sentences of up to N words), with the two D-dimensional vectors ~h and ~c in each element.\nWord representations We use word representations based on the standard 300D vector package provided with GloVe (Pennington et al., 2014). We do not update these representations during training. Instead, we use a learned linear transformation to map each input word vector ~xGloVe into a vector pair \u3008~h,~c\u3009 that is stored in the buffer:\n(4) [ ~h ~c ] =Wwd~xGloVe +~bwd"}, {"heading": "3.3 The tracking LSTM", "text": "In addition to the stack, the buffer, and the composition function, our full model includes an additional component: the tracking LSTM. This is a simple low-dimensional sequence-based LSTM RNN that operates in tandem with the model, taking inputs from the buffer and stack at each step. It is meant to maintain a low-resolution summary of the portion of the sentence that has been processed so far, which is used for two purposes: it supplies feature representations to the transition classifier, which allows the model to stand alone as a parser, and it additionally supplies a secondary input ~e (see Equation 1) to the composition function, allowing context information to leak into the construction of sentence meaning, and forming what is effectively a tree-sequence hybrid model.\nThe tracking LSTM\u2019s inputs (yellow in Figure 2) are the top element of the buffer ~h1b (which would be moved in a SHIFT operation) and the top two elements of the stack ~h1s and ~h 2 s (which would be composed in a REDUCE operation).\nWhy a tree-sequence hybrid? Lexical ambiguity is ubiquitous in natural language. Most words have multiple senses or meanings, and it is generally necessary to use the context in which a word occurs to determine which of its senses or meanings is meant in a given sentence. Even though TreeRNNs are much more effective at composing meanings in principle, this ambiguity can give simpler sequence-based sentence-encoding models an advantage: when a sequence-based model first processes a word, it has direct access to a state vector that summarizes the left context of that word, which acts as a cue for disambiguation. In contrast, when a standard tree-structured model first processes a word, it only has access to the constituent that the word is merging with, which is often just a single additional word. Feeding a context representation from the tracking LSTM into the composition function is a simple and efficient way to mitigate this disadvantage of treestructured models.\nIt would be straightforward to augment SPINN to support the use of some amount of right-side context as well, but this would add complexity to the model that we think is largely unnecessary: humans are very effective at understanding the beginnings of sentences before having seen or heard the ends, suggesting that it is possible to get by without the unavailable right-side context."}, {"heading": "3.4 Parsing: Predicting transitions", "text": "For SPINN to operate on unparsed inputs, it needs to be able to produce its own transition sequence t rather than relying on an external parser to supply it as part of the input. To do this, the model predicts tt at each step using a simple two-way softmax classifier whose input is the state of the tracking LSTM:\n(5) ~pt = softmax(Wtrans~htracking +~btrans)\nAt test time, the model uses whichever transition (i.e., SHIFT or REDUCE) is assigned a higher probability. The prediction function is trained to mimic the decisions of an external parser, and these decisions are used as the inputs to the model during training. For SNLI, we use the binary Stanford PCFG Parser parses that are included with the corpus. We did not find scheduled sampling (Bengio et al., 2015)\u2014allowing the model to use its own transition decisions in some instances at training time\u2014to help."}, {"heading": "3.5 Implementation issues", "text": "Representing the stack efficiently A na\u0131\u0308ve implementation of SPINN would require representing a stack of size N for each timestep of each input sentence at training time to support backpropagation. This implies a per-example space requirement of N \u00d7 T \u00d7 D, which is prohibitively large for significant batch sizes or sentence lengths N . Such a na\u0131\u0308ve implementation would also require copying a largely unchanged stack at each timestep, since each SHIFT or REDUCE operation writes only one new representation to the stack.\nAlgorithm 1 The thin stack algorithm 1: function STEP(bufferTop, op, t, S, Q) 2: if op = SHIFT then 3: S[t] := bufferTop 4: else if op = REDUCE then 5: right := S[Q.pop()] 6: left := S[Q.pop()] 7: S[t] := COMPOSE(left, right) 8: Q.push(t)\nWe propose an alternative space-efficient stack representation inspired by the zipper technique (Huet, 1997), that we call thin stack. For each input sentence, we represent the stack with a single T \u00d7 D matrix S. Each row St represents the top of the actual stack at timestep t. We maintain a queue of backpointers onto S that indicates which elements would be involved in a REDUCE operation at any given step. Algorithm 1 describes the full mechanics of a stack feedforward in this compressed representation. It operates on the compressed T \u00d7D matrix S and a backpointer queue Q. Table 1 shows an example run.\nThis stack representation requires substantially less space. It stores each element involved in the feedforward computation exactly once, meaning that this representation can still support efficient backpropagation. Furthermore, all of the updates to S and Q can be performed batched and in-place on a GPU, yielding substantial speed gains. We describe speed results in Section 3.7.\nA simpler variant of this technique can be used to represent the buffer, since information is not written to the buffer during model operation. It can be stored as a single fixed matrix with a scalar pointer variable indicating which element is the head at each step.\nPreparing the data At training time, SPINN requires both a transition sequence t and a token sequence x as its inputs for each sentence. The token sequence is simply the words in the sentence in order. t can be obtained from any constituency parse for the sentence by first converting that parse into an unlabeled binary parse, then linearizing it (with the usual in-order traversal), then taking each word token as a SHIFT transition and each \u2018)\u2019 as a REDUCE transition, as here:\nUnlabeled binary parse: ( ( the cat ) ( sat down ) ) t: SHIFT, SHIFT, REDUCE, SHIFT, SHIFT, REDUCE, REDUCE\nx: the, cat, sat, down\nHandling variable sentence lengths For any sentence model to be trained with batched computation, it is necessary to pad or crop sentences to a fixed length. We fix this length at N = 25 words, longer than about 98% of sentences in SNLI. Transition sequences t are cropped at the left or padded at the left with SHIFTs. Token sequences x are then cropped or padded with empty tokens at the left to match the number of SHIFTs added or removed from t, and can then be padded with empty tokens at the right to meet the desired length N ."}, {"heading": "3.6 TreeRNN-equivalence", "text": "Without the addition of the tracking LSTM, SPINN (in particular the SPINN-PI-NT variant, for parsed input, no tracking) is precisely equivalent to a conventional tree-structured neural network model in the function that it computes, and therefore also has the same learning dynamics. In both, the representation of each sentence consists of the representations of the words combined recursively using a TreeRNN composition function (in our case, the TreeLSTM function). The SPINN, however, is dramatically faster, and supports both integrated parsing and a novel approach to context through the tracking LSTM."}, {"heading": "3.7 Inference speed", "text": "In this section, we compare the test time speed of our SPINN-PI-NT with an equivalent TreeRNN implemented in the conventional fashion and with a standard RNN sequence model. While the full models evaluated below are implemented and trained using Theano (Bergstra et al., 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models. To do this, we reimplement the feedforward1 of SPINN-PI-NT and an LSTM RNN baseline in C++/CUDA, and compare that implementation with a CPU-based C++/Eigen TreeRNN implementation from Irsoy and Cardie (2014), which we modified to perform exactly the same computations as SPINN-PI-NT.2 TreeRNNs like this can\n1We chose to reimplement and evaluate only the feedforward/inference pass, as inference speed is the relevant performance metric for most practical applications.\n2The original code for Irsoy & Cardie\u2019s model is available at https://github.com/oir/deep-recursive. We plan to release our modified code at publication time, alongside the optimized C++/CUDA models and the Theano source code for the full SPINN.\nonly operate on a single example at a time and are thus poorly suited for GPU computation.\nEach model is restricted to run on sentences of 30 tokens or fewer. We fix the model dimension D and the word embedding dimension at 300. We run the CPU performance test on a 2.20-GHz 16- core Intel Xeon E5-2660 processor with hyperthreading enabled. We test our thin-stack implementation and the RNN model on an NVIDIA Titan X GPU.\nFigure 3 compares the sentence encoding speed of the three models on random input data. We observe a substantial difference in runtime between the CPU and thin-stack implementations that increases with batch size. With a large but practical batch size of 512, the largest we on which we tested the TreeRNN, our model is about 25x faster than the standard CPU implementation, and about 4x slower than the RNN baseline.\nThough this experiment only covers SPINNPI-NT, the results should be similar for the full SPINN model: most of the computation involved in running SPINN is involved in populating the buffer, applying the composition function, and manipulating the buffer and the stack, with the low-dimensional tracking and parsing components adding only a small additional load."}, {"heading": "4 NLI Experiments", "text": "We evaluate SPINN on the task of natural language inference (NLI, a.k.a. recognizing textual entailment, or RTE; Dagan et al., 2006). NLI is a sentence pair classification task, in which a model reads two sentences (a premise and a hypothesis), and outputs a judgment of entailment, contradiction, or neutral, reflecting the relationship between the meanings of the two sentences, as in this example from the SNLI corpus, which we use:\nPremise: Girl in a red coat, blue head wrap and jeans is making a snow angel.\nHypothesis: A girl outside plays in the snow. Label: entailment\nAlthough NLI is framed as a simple three-way classification task, it is nonetheless an effective way of evaluating the ability of some model to extract broadly informative representations of sentence meaning. In order for a model to perform reliably well on NLI, it must be able to represent and reason with the core phenomena of natural language semantics, including quantification, coreference, scope, and several types of ambiguity.\nSNLI is a corpus of 570k human-labeled pairs of scene descriptions like the one above. We use the standard train\u2013test split and ignore unlabeled examples, which leaves about 549k examples for training, 9,842 for development, and 9,824 for testing. SNLI labels are roughly balanced, with the most frequent label, entailment, making up 34.2% of the test set."}, {"heading": "4.1 Applying SPINN to SNLI", "text": "Creating a sentence-pair classifier To classify an SNLI sentence pair, we run two copies of SPINN with shared parameters: one on the premise sentence and another on the hypothesis sentence. We then use their outputs (the ~h states at the top of each stack at time t = T ) to construct a feature vector ~xclassifier for the pair. This feature vector consists of the concatenation of these two sentence vectors, their difference, and their elementwise product (following Mou et al., 2015):\n(6) ~xclassifier =  ~hpremise ~hhypothesis\n~hpremise \u2212 ~hhypothesis ~hpremise ~hhypothesis  Following Bowman et al. (2015a), this feature vector is then passed to a series of 1024D ReLU neural network layers (i.e., an MLP; the number of layers is tuned as a hyperparameter), then passed into a linear transformation, and then finally passed to a softmax layer, which yields a distribution over the three labels.\nThe objective function Our objective combines a cross-entropy objective Ls for the SNLI classification task, a cross-entropy objective {Lpt ,Lht } for each parsing decision for each of the sentences at each step t, and an L2 regularization term on the\ntrained parameters. The terms are weighted using the tuned hyperparameters \u03b1 and \u03bb: (7) Lm =Ls + \u03b1 T\u22121\u2211 t=0 (Lpt + Lht ) + \u03bb|\u03b8|22\nInitialization, optimization, and tuning We initialize the model parameters using the nonparametric strategy of He et al. (2015), with the exception of the softmax classifier parameters, which we initialize using random uniform samples from [\u22120.005, 0.005].\nWe use minibatch SGD with the RMSProp optimizer (Tieleman and Hinton, 2012) and a tuned starting learning rate that decays by a factor of 0.75 every 10k steps. We apply both dropout (Srivastava et al., 2014) and batch normalization (Ioffe and Szegedy, 2015) to the output of the word embedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.\nWe train each model for 250k steps in each run, using a batch size of 32. We track each model\u2019s performance on the development set during training and save parameters when this performance reaches a new peak. We use early stopping, evaluating on the test set using the parameters that perform best on the development set.\nAn appendix discusses hyperparameter tuning."}, {"heading": "4.2 Models evaluated", "text": "We evaluate four models. The four all use the sentence-pair classifier architecture described in Section 4.1, and differ only in the function computing the sentence encodings. First, a singlelayer LSTM RNN (similar to that of Bowman\net al., 2015a) serves as a baseline encoder. Next, the minimal SPINN-PI-NT model (equivalent to a TreeLSTM) introduces the SPINN model design. SPINN-PI adds the tracking LSTM to that design. Finally, the full SPINN adds the integrated parser.\nWe compare our models against several baselines, including the strongest published non-neural network-based result from Bowman et al. (2015a) and previous neural network models built around several types of sentence encoders."}, {"heading": "4.3 Results", "text": "Table 2 shows our results on SNLI inference classification. For the full SPINN, we also report a measure of agreement between this model\u2019s parses and the parses included with SNLI, calculated as classification accuracy over transitions averaged across timesteps.\nWe find that the bare SPINN-PI-NT model performs little better than the RNN baseline, but that SPINN-PI with the added tracking LSTM performs well. The success of SPINN-PI, which is a hybrid tree-sequence model, suggests that the treeand sequence-based encoding methods are at least partially complementary. The full SPINN model with its relatively weak internal parser performs slightly less well, but nonetheless robustly exceeds the performance of the RNN baseline.\nBoth SPINN-PI and the full SPINN significantly outperform all previous sentence-encoding models. Most notably, these models outperform the tree-based CNN of Mou et al. (2015), which also uses tree-structured composition for local feature extraction, but uses simpler pooling techniques to build sentence features in the interest of efficiency. Our results show that a model that uses\ntree-structured composition fully (SPINN) outperforms one which uses it only partially (tree-based CNN), which in turn outperforms one which does not use it at all (RNN).\nThe full SPINN performed moderately well at reproducing the Stanford Parser\u2019s parses of the SNLI data at a transition-by-transition level, with 92.4% accuracy at test time. However, its transition prediction errors were fairly evenly distributed across sentences, and most sentences were assigned partially invalid transition sequences that either left a few words out of the final representation or incorporated a few padding tokens into the final representation."}, {"heading": "4.4 Discussion", "text": "The use of tree structure improves the performance of sentence-encoding models for SNLI. We suspect that this improvement is largely attributable to the more efficient learning of accurate generalizations overall, and not to any particular few phenomena. However, some patterns are identifiable in the results. In particular, it seems that tree-structured models are better able to focus on the key actors and events named in a sentence (possibly by learning an approximation of the linguistic notion of headedness), and that this allows them to better focus on the central actors and events described in a sentence. For example, this pair was classified successfully by all three SPINN variants, but not by the RNN baseline (key words emphasized):\nPremise: A woman in red blouse is standing with small blond child in front of a small folding chalkboard.\nHypothesis: a woman stands with her child Label: neutral\nThe tree-sequence hybrid models seem to be especially good at reasoning over pairs of sentences with very different structures, for which it is helpful to use strict compositionality with the tree-structured component to get the meanings of the sentence parts, but then to also accumulate a broad sentence summary that abstracts away from the precise structures of each sentences. For example, this pair is classified correctly by both hybrid models, but not by the RNN or the SPINN-PI-NT:\nPremise: Nurses in a medical setting conversing over a plastic cup.\nHypothesis: The nurses are discussing a patient. Label: neutral\nWe suspect that the hybrid nature of the full SPINN model is also responsible for its ability to\nperform better than an RNN baseline even when its internal parser is relatively ineffective at producing correct full-sentence parses. We suspect that it is acting somewhat like the tree-based CNN, only with access to larger trees: using tree structure to build up local phrase meanings, and then using the tracking LSTM, at least in part, to combine those meanings."}, {"heading": "5 Conclusions and future work", "text": "We introduce a model architecture (SPINN-PINT) that is equivalent to a TreeLSTM, but an order of magnitude faster at test time. We expand that architecture into a tree-sequence hybrid model (SPINN-PI), and show that this yields significant gains on the SNLI entailment task. Finally, we show that it is possible to exploit the strengths of this model without the need for an external parser by integrating a fast parser into the model (as in the full SPINN), and that the lack of external parse information yields little loss in accuracy.\nBecause this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockta\u0308schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.3 However, we expect that it should be possible to productively combine our model with soft attention to reach state-of-the-art performance.\nOur tracking LSTM uses only simple, quickto-compute features drawn from the head of the buffer and the head of the stack. It is plausible that giving the tracking LSTM access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding. One promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by Dyer et al. (2015).\nFor a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable PUSH and POP operations (as in Grefenstette et al., 2015; Joulin and Mikolov, 2015). This would make it possible for the model to learn to parse using guidance from the semantic representation objective, essentially allowing it\n3Attention based models like Rockta\u0308schel et al. (2015) and the unpublished Cheng et al. (2016) have shown accuracies as high as 89.0% on SNLI, but are more narrowly engineered to suit the task, and do not yield sentence encodings.\nto learn to produce parses that are, in aggregate, better suited to supporting semantic interpretation than those supplied in the training data."}, {"heading": "Acknowledgments", "text": "We acknowledge funding from a Google Faculty Research Award and the Stanford Data Science Initiative. In addition, this material is based upon work supported by the National Science Foundation under Grant No. BCS 1456077. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Some of the Tesla K40s used for this research were donated by the NVIDIA Corporation."}, {"heading": "A Hyperparameters", "text": "We use random search to tune the hyperparameters of the model, setting the ranges for search for each hyperparameter heuristically (and validating the reasonableness of the ranges on the development set), and then launching eight copies of each experiment each with newly sampled hyperparameters from those ranges. Table 3 (on the following page) shows the hyperparameters used in the best run of each model."}], "references": [{"title": "The theory of parsing, translation, and compiling", "author": ["Alfred V. Aho", "Jeffrey D. Ullman."], "venue": "Prentice-Hall, Inc.", "citeRegEx": "Aho and Ullman.,? 1972", "shortCiteRegEx": "Aho and Ullman.", "year": 1972}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "Deep Learning and Unsuper-", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "Proc. NIPS.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Theano: a CPU and GPU math expression", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015a", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Tree-structured composition in neural networks without treestructured architectures", "author": ["Samuel R. Bowman", "Christopher D. Manning", "Christopher Potts."], "venue": "Proc. 2015 NIPS", "citeRegEx": "Bowman et al\\.,? 2015b", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Generative incremental dependency parsing with neural networks", "author": ["Jan Buys", "Phil Blunsom."], "venue": "Proc. ACL.", "citeRegEx": "Buys and Blunsom.,? 2015", "shortCiteRegEx": "Buys and Blunsom.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "arXiv:1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tec-", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Compositionality as an empirical problem", "author": ["David Dowty."], "venue": "Direct Compositionality, Oxford Univ. Press.", "citeRegEx": "Dowty.,? 2007", "shortCiteRegEx": "Dowty.", "year": 2007}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "arXiv:1602.07776.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "A neural syntactic language model", "author": ["Ahmad Emami", "Frederick Jelinek."], "venue": "Machine learning 60(1-3).", "citeRegEx": "Emami and Jelinek.,? 2005", "shortCiteRegEx": "Emami and Jelinek.", "year": 2005}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas K\u00fcchler."], "venue": "Proc. IEEE International Conference on Neural Networks.", "citeRegEx": "Goller and K\u00fcchler.,? 1996", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "arXiv:1506.02516.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv:1502.01852.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson."], "venue": "Proc. ACL.", "citeRegEx": "Henderson.,? 2004", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "The zipper", "author": ["G\u00e9rard Huet."], "venue": "Journal of functional programming 7(05).", "citeRegEx": "Huet.,? 1997", "shortCiteRegEx": "Huet.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proc. NIPS.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Proc. NIPS.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proc. ACL.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Easy-first dependency parsing with hierarchical tree LSTMs", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv:1603.00375.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "When are tree structures necessary for deep learning of representations? In Proc", "author": ["Jiwei Minh-Thang Luong Li", "Dan Jurafsky", "Eudard Hovy."], "venue": "EMNLP.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recognizing entailment and contradiction by tree-based convolution", "author": ["Lili Mou", "Men Rui", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin."], "venue": "arXiv:1512.08422.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "Proc. IWPT .", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "arXiv:1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Sentence disambiguation by a shift-reduce parsing technique", "author": ["Stuart M. Shieber."], "venue": "Proc. ACL.", "citeRegEx": "Shieber.,? 1983", "shortCiteRegEx": "Shieber.", "year": 1983}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. ICML.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR 15.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proc. NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long shortterm memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lecture 6.5 \u2013 RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4:2", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Trends in Parsing Technology, Springer.", "citeRegEx": "Titov and Henderson.,? 2010", "shortCiteRegEx": "Titov and Henderson.", "year": 2010}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv:1511.06361.", "citeRegEx": "Vendrov et al\\.,? 2015", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "arXiv:1509.01626.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Top-down tree long short-term memory networks", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata."], "venue": "arXiv:1511.00060.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 34, "context": "A wide range of current models in NLP are built around a neural network component that produces vector representations of sentence meaning (e.g., Sutskever et al., 2014; Tai et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 23, "context": "models (RNNs, see Figure 1a) with Long ShortTerm Memory (LSTM, Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and K\u00fcchler, 1996; Socher et al.", "startOffset": 189, "endOffset": 236}, {"referenceID": 38, "context": "models (RNNs, see Figure 1a) with Long ShortTerm Memory (LSTM, Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and K\u00fcchler, 1996; Socher et al.", "startOffset": 189, "endOffset": 236}, {"referenceID": 34, "context": "TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with ar X iv :1 60 3.", "startOffset": 28, "endOffset": 85}, {"referenceID": 25, "context": "TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with ar X iv :1 60 3.", "startOffset": 28, "endOffset": 85}, {"referenceID": 6, "context": "TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with ar X iv :1 60 3.", "startOffset": 28, "endOffset": 85}, {"referenceID": 18, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 14, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 36, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 8, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 7, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 12, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 24, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 39, "context": "In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016).", "startOffset": 150, "endOffset": 189}, {"referenceID": 13, "context": "In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016).", "startOffset": 150, "endOffset": 189}, {"referenceID": 0, "context": "SPINN is inspired by the shift-reduce parsing formalism (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.", "startOffset": 56, "endOffset": 78}, {"referenceID": 27, "context": "The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003).", "startOffset": 57, "endOffset": 91}, {"referenceID": 34, "context": "The TreeLSTM composition function (Tai et al., 2015) generalizes the LSTM neural network layer to tree- rather than sequence-based inputs, and it shares with the LSTM the idea of representing intermediate states as a pair of a fast-changing state representation ~h and a slower-changing memory representation ~c.", "startOffset": 34, "endOffset": 52}, {"referenceID": 28, "context": "Word representations We use word representations based on the standard 300D vector package provided with GloVe (Pennington et al., 2014).", "startOffset": 111, "endOffset": 136}, {"referenceID": 3, "context": "We did not find scheduled sampling (Bengio et al., 2015)\u2014allowing the model to use its own transition decisions in some instances at training time\u2014to help.", "startOffset": 35, "endOffset": 56}, {"referenceID": 19, "context": "We propose an alternative space-efficient stack representation inspired by the zipper technique (Huet, 1997), that we call thin stack.", "startOffset": 96, "endOffset": 108}, {"referenceID": 4, "context": "While the full models evaluated below are implemented and trained using Theano (Bergstra et al., 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models.", "startOffset": 79, "endOffset": 124}, {"referenceID": 2, "context": "While the full models evaluated below are implemented and trained using Theano (Bergstra et al., 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models.", "startOffset": 79, "endOffset": 124}, {"referenceID": 2, "context": ", 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models. To do this, we reimplement the feedforward1 of SPINN-PI-NT and an LSTM RNN baseline in C++/CUDA, and compare that implementation with a CPU-based C++/Eigen TreeRNN implementation from Irsoy and Cardie (2014), which we modified to perform exactly the same computations as SPINN-PI-NT.", "startOffset": 8, "endOffset": 371}, {"referenceID": 21, "context": "Batch size Fe ed fo rw ar d tim e (s ec ) Thin-stack GPU CPU (Irsoy and Cardie, 2014) RNN", "startOffset": 61, "endOffset": 85}, {"referenceID": 10, "context": "We evaluate SPINN on the task of natural language inference (NLI, a.k.a. recognizing textual entailment, or RTE; Dagan et al., 2006).", "startOffset": 60, "endOffset": 132}, {"referenceID": 5, "context": "Following Bowman et al. (2015a), this feature vector is then passed to a series of 1024D ReLU neural network layers (i.", "startOffset": 10, "endOffset": 32}, {"referenceID": 5, "context": "Previous non-NN results Lexicalized classifier (Bowman et al., 2015a) \u2014 \u2014 99.", "startOffset": 47, "endOffset": 69}, {"referenceID": 5, "context": "Previous sentence encoder-based NN results 100D LSTM encoders (Bowman et al., 2015a) 221k \u2014 84.", "startOffset": 62, "endOffset": 84}, {"referenceID": 37, "context": "6 1024D pretrained GRU encoders (Vendrov et al., 2015) 15m \u2014 98.", "startOffset": 32, "endOffset": 54}, {"referenceID": 26, "context": "4 300D Tree-based CNN encoders (Mou et al., 2015) 3.", "startOffset": 31, "endOffset": 49}, {"referenceID": 17, "context": "Initialization, optimization, and tuning We initialize the model parameters using the nonparametric strategy of He et al. (2015), with the exception of the softmax classifier parameters, which we initialize using random uniform samples from [\u22120.", "startOffset": 112, "endOffset": 129}, {"referenceID": 35, "context": "We use minibatch SGD with the RMSProp optimizer (Tieleman and Hinton, 2012) and a tuned starting learning rate that decays by a factor of 0.", "startOffset": 48, "endOffset": 75}, {"referenceID": 32, "context": "We apply both dropout (Srivastava et al., 2014) and batch normalization (Ioffe and Szegedy, 2015) to the output of the word embedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.", "startOffset": 22, "endOffset": 47}, {"referenceID": 20, "context": ", 2014) and batch normalization (Ioffe and Szegedy, 2015) to the output of the word embedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.", "startOffset": 32, "endOffset": 57}, {"referenceID": 5, "context": "We compare our models against several baselines, including the strongest published non-neural network-based result from Bowman et al. (2015a) and previous neural network models built around several types of sentence encoders.", "startOffset": 120, "endOffset": 142}, {"referenceID": 26, "context": "Most notably, these models outperform the tree-based CNN of Mou et al. (2015), which also uses tree-structured composition for local feature extraction, but uses simpler pooling techniques to build sentence features in the interest of efficiency.", "startOffset": 60, "endOffset": 78}, {"referenceID": 1, "context": "Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.", "startOffset": 127, "endOffset": 176}, {"referenceID": 29, "context": "Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.", "startOffset": 127, "endOffset": 176}, {"referenceID": 22, "context": "For a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable PUSH and POP operations (as in Grefenstette et al., 2015; Joulin and Mikolov, 2015).", "startOffset": 182, "endOffset": 241}, {"referenceID": 1, "context": "Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.3 However, we expect that it should be possible to productively combine our model with soft attention to reach state-of-the-art performance. Our tracking LSTM uses only simple, quickto-compute features drawn from the head of the buffer and the head of the stack. It is plausible that giving the tracking LSTM access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding. One promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by Dyer et al. (2015). For a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable PUSH and POP operations (as in Grefenstette et al.", "startOffset": 128, "endOffset": 899}, {"referenceID": 28, "context": "Attention based models like Rockt\u00e4schel et al. (2015) and the unpublished Cheng et al.", "startOffset": 28, "endOffset": 54}, {"referenceID": 9, "context": "(2015) and the unpublished Cheng et al. (2016) have shown accuracies as high as 89.", "startOffset": 27, "endOffset": 47}], "year": 2016, "abstractText": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stackaugmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single treesequence hybrid model by integrating treestructured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.", "creator": "TeX"}}}