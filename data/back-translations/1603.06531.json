{"id": "1603.06531", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Deep video gesture recognition using illumination invariants", "abstract": "In this paper, we present architectures based on deep neural networks for gesture recognition in videos that are invariant to local scaling. We fuse autocoder and predictor architectures using an adaptive weighting scheme that can handle a scaled-down dataset while enriching our models from enormous unlabeled sets. We improve the robustness of light conditions by introducing a new adaptive filter based on local scale normalization. We provide better results than known methods, including newer approaches based on neural networks.", "histories": [["v1", "Mon, 21 Mar 2016 18:33:29 GMT  (2032kb,D)", "http://arxiv.org/abs/1603.06531v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["otkrist gupta", "dan raviv", "ramesh raskar"], "accepted": false, "id": "1603.06531"}, "pdf": {"name": "1603.06531.pdf", "metadata": {"source": "META", "title": "Deep video gesture recognition using illumination invariants", "authors": ["Otkrist Gupta", "Dan Raviv", "Ramesh Raskar"], "emails": [], "sections": [{"heading": null, "text": "In this paper we present architectures based on deep neural nets for gesture recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filer based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets.\nKeywords: deep learning, gesture recognition, video classification, neural nets, machine learning."}, {"heading": "1 Introduction", "text": "Human beings, as social animals, rely on a vast array of methods to communicate with each other in the society. Non-verbal communication, that includes body language and gestures, is an essential aspect of interpersonal communication. In fact, studies have shown that non-verbal communication accounts for more than half of all societal interactions [Frith 2009]. Studying facial gestures is therefore of vital importance in fields like sociology, psychology and automated recognition of gestures can be applied towards creating more user affable software and user agents in these fields.\nAutomatic gesture recognition has wide implications in the field of human computer interaction. As technology progresses, we spend large amounts of our time looking at screens, interacting with computers and mobile phones. In spite of their wide usage, majority of software interfaces are still non-verbal, impersonal, primitive and terse. Adding emotion recognition and tailoring responses towards users emotional state can help improve human computer interaction drastically [Cowie et al. 2001; Zhang et al. 2015] and help keep users engaged. Such technologies can then be applied towards improvement of workplace productivity, education and telemedicine [Ko\u0142akowska et al. 2014]. Last two decades have seen some innovation in this area [Klein and Picard 1999; Cerezo et al. 2007;\nAndre\u0301 et al. 2000] such as humanoid robots for example Pepper which can both understand and mimic human emotions.\nModeling and parameterizing human faces is one of the most fundamental problems in computer graphics [Liu et al. 2014a]. Understanding and classification of gestures from videos can have applications towards better modeling of human faces in computer graphics and human computer interaction. Accurate characterization of face geometry and muscle motion can be used for both expression identification and synthesis [Pighin et al. 2006; Wang et al. 2013] with applications towards computer animation [Cassell et al. 1994]. Such approaches combine very high dimensional facial features from facial topology and compress them to lower dimensions using a series of parameters or transformations [Waters 1987; Pyun et al. 2003]. This paper demonstrates how to use deep neural networks to reduce dimensionality of high information facial videos and recover the embedded temporal and spatial information by utilizing a series of stacked autoencoders.\nOver the past decade algorithms for training neural nets have dramatically evolved, allowing us to efficiently train deep neural nets [Hinton et al. 2006; Jung et al. 2015]. Such models have become a strong driving force in modern computer vision and excel at object classification [Krizhevsky et al. 2012], segmentation and facial recognition [Taigman et al. 2014]. In this paper we apply deep neural nets for recognizing and classifying facial gestures, while pushing forward several architectures. We obtain high level information in both space and time by implementing 4D convolutional layers and training an autoencoder on videos. Most of neural net applications use still images as input and rely on convolutional architectures for automatically learning semantic information in spatial domain. Second, we reface an old challenge in learning theory, where not all datasets are labeled. Known as semi-supervised learning, this problem, once again, attracts attention as deep nets require massive datasets to outperform other architectures. Finally, we provide details of a new normalization layer, which robustly handles temporal lighting changes within the network itself. This new architecture is adaptively fine tuned as part of the learning process, and outperforms all other reported techniques for the tested datasets. We summarize our contributions as follows: ar X\niv :1\n60 3.\n06 53\n1v 1\n[ cs\n.C V\n] 2"}, {"heading": "1.1 Contributions", "text": "1. We develop a scale invariant architecture for generating illumination invariant deep motion features.\n2. We report state of the art results for video gesture recognition using spatio-temporal convolutional neural networks.\n3. We introduce an improved topology and protocol for semisupervised learning, where the number of labeled data points is only a fraction of the entire dataset."}, {"heading": "2 Related Work", "text": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015]. Other intriguing methodologies include performing emotion recognition through speech [Nwe et al. 2003; Schuller et al. 2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al. 2014c; Senechal et al. 2015]. Majority of such systems involve a pipeline with multiple stages - face discovery, face alignment, feature extraction and landmark localization followed by classification of labels as the final step. Our approach combines all of these phases (after face detection) into the neural net which takes entire video clip as input.\nRecently, deep neural nets have triumphed over traditional vision algorithms, thereby dominating the world of computer vision. Deep neural networks have proven to be an effective tool to classify and segment high dimensional data such as images [Krizhevsky et al. 2012; Szegedy et al. 2015], audio and videos [Karpathy et al. 2014; Tran et al. 2014]. With advances in convolutional neural nets, we have seen neural nets applied for face detection [Taigman et al. 2014; Zhao et al. 2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these\nnetworks were not deep enough or used other feature extraction techniques like PCA or Fisherface. By contrast this paper proposes an end to end system which takes a sequence of frames as input and gives classification labels as output while using deep autoencoders to generate high dimensional spatio-temporal features.\nWhile deep neural nets are notorious for stellar results, training a neural net can be challenging because of huge data requirements. A way around this is to use autoencoders for feature extraction or weights initialization [Vincent et al. 2008], followed by fine tuning over a smaller labeled dataset. This issue can also be solved using embeddings in lower dimensional manifold [Weston et al. 2012; Kingma et al. 2014] or pre-train using pseudo labels [Lee 2013] thereby requiring fewer number of labeled samples. Approaches based on semi supervised learning have shown to work for smaller labeled datasets [Papandreou et al. 2015] and techniques using deep neural nets to combine labels and unlabeled data in the same architecture [Liu et al. 2014d; Kahou et al. 2013] have emerged victorious. In this paper we propose similar hybrid approaches incorporating deep autoencoders for unlabeled data and additive loss function for the classification tasks.\nIntroducing invariants in neural networks is an area of active research, some examples include illumination invariant face recognition techniques [Mathur et al. 2008; Li et al. 2004] and deep lambertian networks [Tang et al. 2012; Jung et al. 2015]. Our method tries to introduce similar invariants for video neural networks by introducing temporal invariants to illumination. While we test our techniques on facial gesture datasets, in principal they can be extended to any neural network taking videos as input. In [Anonymous Submission 2016], the author considered velocity changes in videos as well as a semi-supervised learning approach. Here we focus on a different neural network topology and parameter calibration, and report better results on similar databases using new invariant layers."}, {"heading": "3 Method", "text": "Our facial expression recognition pipeline comprises of Viola-Jones algorithm [Viola and Jones 2004] for face detection followed by a deep convolutional neural network for predicting expressions.\nThe deep convolutional network includes an autoencoder combined with a predictor which relies on the semi-supervised learning paradigm. The autoencoder neural network takes videos containing 9 frames of size 145 \u00d7 145 as input and produces 145 \u00d7 145 \u00d7 9 tensor as output. Predictor neural net sources innermost hidden layer of autoencoder and uses a cascade of fully connected layers accompanied by the softmax layer to classify expressions. Since videos can have different sizes and durations they need to be resized in temporal and spatial domain using standard interpolation techniques. The network topologies and implementation are describe henceforth."}, {"heading": "3.1 Autoencoder", "text": "Stacked autoencoders can be used to convert high dimensional data into lower dimensional space which can be useful for classification, visualization or retrieval [Hinton and Salakhutdinov 2006]. Since video data is extremely high dimensional we rely on a deep convolutional autoencoder to extract meaningful features from this data by embedding it into R4096. The autoencoder topology is inspired by ImageNet [Krizhevsky et al. 2012] and comprises of convolutional layers gradually reducing data dimensionality until we reach a fully connected layer. Central fully connected layers are followed by a cascade of deconvolutional layers which essentially invert the convolutional layers thereby reconstructing the input tensor (R145\u00d7145\u00d79). The complete autoencoder architecture can be described in following shorthandC(96, 11, 3)\u2212N\u2212C(256, 5, 2)\u2212 N\u2212C(384, 3, 2)\u2212N\u2212FC(4096)\u2212FC(4096)\u2212DC(96, 11, 3)\u2212 N \u2212DC(256, 5, 2)\u2212N \u2212DC(384, 3, 2). Here C(96, 11, 3) is a\nconvolutional layer containing 96 filters of size 11 \u00d7 11 in spatial domain and spanning 3 frames in temporal domain. N stands for local response normalization layers,DC stands for deconvolutional layers and FC(4096) stands for fully connected layers containing 4096 neurons.\nIn the same way that spatial convolutions consolidate nearby spatial characteristics of an image, we use the slow fusion model described in [Karpathy et al. 2014] to gradually combine temporal features across multiple frames. We implement slow fusion by extending spatial convolution to the temporal domain and adding representation of filter stride for both space and time domains. This allows us to control filter size and stride in both temporal and spatial domains leading to a generalized 3D convolution over spatiotemporal input tensor followed by 4D convolutions on intermediate layers. The first convolutional layer sets temporal size and stride as 3 and 2 respectively whereas the subsequent layer has both size and stride of 2 in temporal domain. Finally the third convolutional layer merges temporal information from all frames together, culminating in a lower dimensional vector of size 4096 at the innermost layer.\nSince weight initialization is critical for convergence in a deep autoencoder, we use pre-training for each convolutional layer as we add the layers on. Instead of initializing all weights at once and training from there, we train the first and last layer first, followed by the next convolutional layer and so on. We discuss this in detail in section 5.1."}, {"heading": "3.2 Semi-Supervised Learner", "text": "Our predictor neural net consists of a combination of several convolutional layers followed by multiple fully connected layers ending in a softmax logistic regression layer for prediction. Architecture can be described as C(96, 11, 3) \u2212 N \u2212 C(256, 5, 2) \u2212 N \u2212C(384, 3, 2)\u2212N \u2212FC(4096)\u2212FC(8192)\u2212FC(4096)\u2212 FC(1000) \u2212 FC(500) \u2212 FC(8) using shorthand notation described in section 3.1. Notice that our autoencoder architecture is overlaid on top of the predictor architecture by adding deconvolutional layers after the first fully connected layer to create a semisupervised topology which is capable of training both autoencoder and predictor together (see Figure 3). We use autoencoder to initialize weights for all convolutional layers, all deconvolutional layers and central fully connected layers and we initialize any remaining layers randomly. We use stochastic gradient descent to train weights by combining losses from both predictor and autoencoder while training, this combined loss function for the semi-supervised\nlearner is described in the equation 1.\nL = \u2212\u03b2 \u2211 j yj log ( eoj\u2211 k e ok ) + \u03b1||x\u0304\u2212 x\u0304o||2 (1)\nEquation 1 defines semi-supervised learner loss by combining the loss terms from predictor and autoencoder neural networks. Here yj refers to the input labels to represent each facial expression uniquely while ok are the outputs from the final layer of predictor neural net. Also x\u0304 is the input tensor (\u2208 R145\u00d7145\u00d79) and x\u0304o is the corresponding output from autoencoder. Autoencoder loss is the Euclidean loss between input and output tensors given by ||x\u0304\u2212x\u0304o||2 whereas\u2212 \u2211 j yj log ( e oj\u2211\nk e ok\n) is the softmax loss from\nthe predictor [Bengio et al. 2005]. Each step of stochastic gradient descent is performed over a batch of 22 inputs and loss is obtained by adding loss terms for the entire batch. At the commencement of training of the predictor layers, we select values of \u03b2 which make softmax loss term an order of magnitude higher than the Euclidean loss term (see equation 1). We continue training predictor layers by gradually decreasing loss coefficient \u03b1 alongside of softmax loss to prevent overfitting of autoencoder. Amalgamation of predictor and autoencoder architectures is depicted in Figure 2."}, {"heading": "3.3 Illumination Invariant Learner", "text": "We introduce scale invariance to pixel intensities by adding additional layers as an illumination invariant neural network in the beginning of semi-supervised learner. The illumination invariant layers include a convolutional layer, an absolute value layer, a reciprocal layer followed by a Hadamard product layer. Scale invariance is achieved by applying element wise multiplication between the output layers of proposed architecture and the original input layer. This normalization can be written as C(9, 1, 9) \u2212 Abs \u2212 Log(\u03b1, \u03b2) \u2212 Exp(\u2212\u03b3, \u03b4)\u2212Prod(x1, x2) (please refer to shorthand notation in section 3.1). Here C(9, 1, 9) refers to the first convolutional layer containing 9 filters with size 1 \u00d7 1 in spatial domain and a size of 9 in time domain. Abs is a fixed layer to compute absolute value, Log(\u03b1, \u03b2) layer computes the function ln(\u03b1\u2217x+\u03b2) andExp(\u03b3, \u03b4) layer gives us e\u03b3\u2217x+\u03b4 . In the end Prod(x1, x2) layer takes two in-\nputs (x1, x2) and multiplies the output of exponential layer (x2) with the original input tensor (x1). If F\u0304 (x\u0304) denotes function emulated by first convolution layer, we can write the transfer function of this sub-net as follows (equation 2).\nH(x\u0304) = x\u0304e\u2212\u03b3log(\u03b1|F\u0304 (x\u0304)|+\u03b2)+\u03b4 = e\u03b4x\u0304\n(\u03b1|F\u0304 (x\u0304)|+ \u03b2)\u03b3 (2)\nLog and Exp layers are used to generate a reciprocal layer by setting meta-parameters \u03b3 to 1 and \u03b4 to zero. We can also \u201dswitch off\u201d this sub-net by setting both of these parameters to zero. Transfer function meta parameters \u03b1 (scale) and \u03b2 (shift) can be tuned as well for optimal performance. We perform a grid search to find optimal values for these after re-characterizing the transfer function parameters as a global multiplicative factor \u03c4 and a proportion factor \u03b7 (see equation 3). Table 1 shows results for various choices of \u03b1 and \u03b2. We can reformulate equation 2 as given below:\nH(x\u0304) = e0x\u0304\n(\u03b1|F\u0304 (x\u0304)|+ \u03b2)1 =\n1 \u03b2 x\u0304\n1 + \u03b1 \u03b2 |F\u0304 (x\u0304)|\n= \u03c4 x\u0304\n1 + \u03b7|F\u0304 (x\u0304)| (3)\nThe output from scale invariant neural net is a 145 \u00d7 145 \u00d7 9 tensor which is used as input in the autoencoder and predictor neural networks. The convolution layer can be parametrized using a 9 \u00d7 1 \u00d7 1 \u00d7 9 tensor and changes during fine tuning while \u03b1 and \u03b2 are fixed constants greater than zero. In our experiments we initialized convolutional filter of scale invariant sub-net using several approaches, such as partial derivatives, mellin transform, moving average and laplacian kernel and found that it performed best when using neighborhood averaging. Algorithm 1 demonstrates initialization of convolutional layer at the beginning of illumination invariant neural net."}, {"heading": "4 Datasets and Implementation", "text": "Surprisingly, high quality facial expression datasets are hard to come across. In the same way that majority of facial expression algorithms focus on still images, majority of facial gesture datasets\nAlgorithm 1 Generate convolution layer for scale invariant sub-net Input: Total number of frames nFrames, window size wSize Output: Caffe Weight Matrix W\n1: function AUTOENCODERWEIGHTS(nFrames, wSize) 2: r \u2190 (wSize\u2212 1)/2 3: A\u2190 zeros(nFrames, nFrames) 4: for (i\u2190 0; i < nFrames; i+ +) do 5: n\u2190 min(i, r) 6: n\u2190 min(n, nFrames\u2212 i) 7: Ai,i\u2212n:i+n \u2190 1/(2n+ 1) 8: W\u2190 A 9: return W\nrely on images alone and don\u2019t emphasize on complete video clips. For accurate analysis we compare our method against external techniques using 3 different datasets. Each of these datasets have facial video clips varying from neutral face to its peak facial expression. Facial expressions can be naturally occurring (non-posed) or artificially enacted (posed), we attempt to classify both using our method and compare our results against published techniques. Here we present the two known datasets from literature along with two additional datasets collected by us. The first dataset was used for unsupervised learning and contains 160 million face images combined into 6.5 million short (25 frames) clips. The second dataset contains 2777 video clips which are labeled for seven basic emotions."}, {"heading": "4.1 Autoencoder dataset", "text": "Training the unsupervised component of our neural net required a large amount of data to ensure that the deep features were general enough to represent any face expression. We trained the deep convolutional autoencoder using a massive collection of unlabeled data points comprising of 6.5 million video clips with 25 image frames per clip. The clips were generated by running Viola-Jones face algorithm to detect and isolate face bounding boxes on public domain videos. We further enhanced the data quality by removing any clips which showed high variation in-between consecutive frames. This eliminated video clips containing accidental appearance of occlusions, rapid facial motions or sudden appearance of another face.\nAs an additional step we obtained the facial pose information by using active appearance models and generating facial landmarks\n[Asthana et al. 2014]. We fitted the facial landmarks to a 3D deformable model and restricted our dataset to clips containing less than 30 degrees of yaw, pitch or roll, thereby eliminating faces looking sideways. For data generation, we relied on daily feeds from news sources such as CNN, MSNBC, FOX and CSPAN. Collection of this dataset required development of an automated system to mine video clips, segment faces and filter meaningful data and it took us more than 6 months to collect the entire dataset. To our knowledge this is the largest dataset containing facial video clips and we plan to share it with scientific community by making it public."}, {"heading": "4.2 Cohn Kanade Dataset", "text": "The Cohn Kanade dataset [Lucey et al. 2010] is one of the oldest and well known dataset containing facial expression video clips. It contains a total of 593 video clip sequences from which 327 clips are labeled for seven basic emotions (most of these are posed). Clips contain the frontal view of face performing facial gesture varying from neutral expression to maximum intensity of emotion. While the dataset contains a lot of natural smile expressions it lacks diversity of induced samples for other facial expressions."}, {"heading": "4.3 MMI Dataset", "text": "MMI facial expression dataset [Pantic et al. 2005] involves an ongoing effort for representing both enacted and induced facial expres-\nsions. The dataset comprises of 2894 video samples out of which around 200 video clips are labeled for six basic emotions. The clips contain faces going from blank expression to the peak emotion and then back to neutral facial gesture. MMI which originally contained only posed facial expressions, was recently extended to include natural versions of happiness, disgust and surprise [Valstar and Pantic 2010]."}, {"heading": "4.4 Florentine dataset", "text": "We developed specialized video recording and annotation tools to collect and label facial gestures (first presented in [Anonymous Submission 2016]). The application was developed in Python programming language and we used well known libraries such as OpenCV for video capture and annotation. The database contains facial clips from 160 subjects (both male and female), where gestures were artificially generated according to a specific request, or genuinely given due to a shown stimulus. We captured 1032 clips for posed expressions and 1745 clips for induced facial expressions amounting to a total of 2777 video clips. Genuine facial expressions were induced in subjects using visual stimuli, i.e. videos selected randomly from a bank of Youtube videos to generate a specific emotion. Please refer to Table 2 to see the distribution of database, where posed clips refers to the artificially generated expressions and non-posed refers to the stimulus activation procedure."}, {"heading": "5 Experiments and results", "text": ""}, {"heading": "5.1 Video autoencoder", "text": "Since deep autoencoders can show slow convergence when trained from randomly initialized weights [Hinton and Salakhutdinov 2006], we used contrastive divergence minimization to train stacked autoencoder layers iteratively [Carreira-Perpinan and Hinton 2005]. Initially, we pre-trained the beginning and end convolutional layers by creating an intermediate neural network (C(96, 11, 3) \u2212 N \u2212 C(256, 5, 2) \u2212 N \u2212 DC(256, 5, 2) \u2212 N \u2212 DC(384, 3, 2)) and training it on facial video clips. Inner layers were trained successively by adding them to the intermediate neural network and keep-\ning pre-trained layers fixed until the convergence of weights. To yield best results, we also fine tuned the entire network at the end of each iteration. This process was repeated until the required number of layers had been added and final architecture was achieved. Training of the entire autoencoder typically required 3 days and a million data inputs.\nOur neural network was implemented using the Caffe framework [Jia et al. 2014] and trained using NVIDIA Tesla K40 GPUs. The trained weights used to initialize next phase were stored as Caffe model files and each intermediate neural network was implemented as a separate prototxt file. Weights were shared using shared parameter feature and transferred across neural networks using the resume functionality provided in Caffe. Our deep autoencoder took 145\u00d7145\u00d79 clips as input, the spatial resolution was achieved by down-sampling all clips to a fixed size using bi-cubic interpolation. 9 frames were obtained by extracting every third frame from video clips. All videos were converted into 1305\u00d7 145 image clips containing consecutive input frames placed horizontally and we used the Caffe \u201dimagedata\u201d, \u201dsplit\u201d and \u201dconcat\u201d layers to isolate individual frames for autoencoder input and output.\nPlease see Figure 5 to visualize results obtained from intermediate autoencoders using different number of layers."}, {"heading": "5.2 Semi-Supervised predictor", "text": "We created a semi-supervised predictor by adding a deep neural network after the innermost fully connected layer of our autoencoder. The architecture of predictor neural net can be written as FC(8192) \u2212 FC(4096) \u2212 FC(1000) \u2212 FC(500) \u2212 FC(8). The complete semi-supervised neural network contains an autoencoder and a predictor that share neural links and can be trained on the same input simultaneously. Weights from autoencoder training were used to initialize weights of semi-supervised predictor which were later fine tuned using labeled inputs from datasets described in section 3.2. The weights from this step are used for initialization of our scale-invariant predictor which we describe next."}, {"heading": "5.3 Illumination-Invariant Semi-Supervised predictor", "text": "Our scale-invariant neural network prefixes semi-supervised learner with an axillary neural net to induce scale invariance (see 3.3). We test our method on three datasets (MMI, CK and Florentine ) by randomly dividing each of them into non-intersecting train, test and validation subsets. Our training dataset contains 50% inputs while testing and validation datasets contain 30% and 20% of inputs. After the split we increase the size of training dataset by adding rotation, translation or flipping the image.\nFor quantitative analysis we compare our results against expression-lets base approaches [Liu et al. 2014b] and multiple kernel methods [Liu et al. 2014c]. We utilize sources downloaded from Visual data transforming and taking in Resources [Sources ] as a reference to contrast with our strategies. For reasonable comparison we use same partitioning techniques while comparing our techniques with external methods. While we cannot compare against methods such as [Liu et al. 2014a] because of absence of publicly available code our method still wins on MMI dataset.\nWe test our method with and without varying illumination on external datasets, results of our findings can be summarized in Table 4. Please see tables 3 for confusion matrices demonstrating results for each expression. We outperform all external methods on datasets in almost all cases. Our method also shows large margin of improvement over plain semi-supervised approaches. Both autoencoder and predictor network topologies are implemented as Caffe prototxt files [Jia et al. 2014] and they will be made available for public usage."}, {"heading": "6 Discussions and future work", "text": "In this paper we introduce a framework for facial gesture recognition which combines semi-supervised learning approaches with carefully designed neural network topologies. We demonstrate how to induce illumination invariance by including specialized layers and use spatio-temporal convolutions to extract features from multiple image frames. Currently, our system relies on utilization of Viola-Jones to distinguish and segment out the faces and is limited to analyzing only the front facing views. Emotion recognition in the wild still remains an elusive problem with low reported accuracies which we hope will be addresses in future work.\nIn this work we only considered video frames but other, richer, modalities could be taken into account. Sound, for example, has a direct influence on the emotional status and can improve our current system. Higher refresh rates, multi-resolution in space and time, or interactions between our subjects are just few of many possibilities\nwhich can to enrich our data and can lead to better classification or inference.\nDeep neural networks have proven to be extremely effective in solving computer vision problems even though training them at large scale continues to be both CPU and memory intensive. Our system tries to make best use of resources available and further improvements in hardware and software can help us build even larger and deeper neural networks while enabling us to train and test them on portable devices. Over here, we introduce a new layer which creates illumination invariance adaptively and can be fine tuned to get best results. In this work, we emphasize on scale invariance for illumination, in future we hope to explore induction of other invariants, which continues to be an area of rapid research in neural networks.\nAnother approach to induce scale invariance can involve using standardized Local Response Normalization (LRN) based layers in the neural network right after the first input layer. This approach is similar to pre-normalizing the data before testing. We compare our method to this approach as well and found that adaptive normalization performed better than plain LRN based learner. Our results are summarized in Table 5."}, {"heading": "6.1 Limitations", "text": "In this section we explore limitations of our system and discuss where our system may fail or be of less value. One of our greatest limitations is that the system was built and tested using only frontal perspectives thereby imposing a constraint on the input facial orientations. Further the pipeline takes a fixed number of video frames as input which imposes a restriction on minimum number of frames required for recognition. We restrict individual frames to a fixed size of 140\u00d7 140 and higher resolution frames need to be resized which may lead to information loss. Both spatio and temporal\nsize constraints can be improved by increasing neural network size at the cost of computing resources.\nLearning for deep neural networks can be extremely computationally intensive and can impose massive constraints on systemic space-time complexity. Our system is no different and requires specialized hardware (NVIDIA TeslaTMor K40TMGrid GPUs) with a minimum of 9 GB of VRAM on the graphics card for lowest of batch sizes. Deep autoencoders can be data intensive and require millions of unlabeled samples to train. Further the stacked autoencoder we train takes over 3 days to train requiring an additional day to fine tune predictor weights for larger labeled datasets. Even though the system supports 7 emotions and 1 neutral face state, it was not trained to detect neutral emotions - a constraint which can be fixed by adding more labeled data for neutral facial gestures. The pipeline only recognizes 7 facial emotions but recent research shows that there is a much wider range of emotions. Even though neural networks win in a lot of scenarios, a lot more research needs to be done to understand exactly how and why they work."}, {"heading": "7 Conclusions", "text": "This paper uses semi-supervised paradigms in convolutional neural nets for classification of facial gestures in video sequences. Our topologies are trained on millions of facial video clips and use spatio-temporal convolutions to extract transient features in videos. We developed a new scale-invariant sub-net which showed superior results for gesture recognition under variable lighting conditions. We demonstrate effectiveness of our approach on both publicly available datasets and samples collected by us."}, {"heading": "ANDRE\u0301, E., KLESEN, M., GEBHARD, P., ALLEN, S., AND RIST,", "text": "T. 2000. Exploiting models of personality and emotions to control the behavior of animated interactive agents. In Workshop on Achieving Human-Like Behavior in Interactive Animated Agents, 3\u20137."}, {"heading": "ASTHANA, A., ZAFEIRIOU, S., CHENG, S., AND PANTIC, M.", "text": "2014. Incremental face alignment in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1859\u2013 1866."}, {"heading": "BENGIO, Y., ROUX, N. L., VINCENT, P., DELALLEAU, O., AND", "text": "MARCOTTE, P. 2005. Convex neural networks. In Advances in Neural Information Processing Systems, 123\u2013130.\nCARREIRA-PERPINAN, M. A., AND HINTON, G. E. 2005. On contrastive divergence learning. In Proceedings of International Workshop on Artificial Intelligence and Statistics, 33\u201340."}, {"heading": "CASSELL, J., PELACHAUD, C., BADLER, N., STEEDMAN, M.,", "text": "ACHORN, B., BECKET, T., DOUVILLE, B., PREVOST, S., AND STONE, M. 1994. Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents. In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, ACM, 413\u2013420.\nCEREZO, E., BALDASSARRI, S., AND SERON, F. 2007. Interactive agents for multimodal emotional user interaction. Multi Conferences on Computer Science and Information Systems, 35\u2013 42.\nCHEN, H., LI, J., ZHANG, F., LI, Y., AND WANG, H. 2015. 3d model-based continuous emotion recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1836\u20131845."}, {"heading": "COWIE, R., DOUGLAS-COWIE, E., TSAPATSOULIS, N., VOTSIS, G., KOLLIAS, S., FELLENZ, W., AND TAYLOR, J. G.", "text": "2001. Emotion recognition in human-computer interaction. Signal Processing Magazine, IEEE 18, 1, 32\u201380."}, {"heading": "DHALL, A., ASTHANA, A., GOECKE, R., AND GEDEON, T.", "text": "2011. Emotion recognition using PHOG and LPQ features. In International Conference on Automatic Face & Gesture Recognition, IEEE, 878\u2013883.\nFRITH, C. 2009. Role of facial expressions in social interactions. Philosophical Transactions of the Royal Society B: Biological Sciences 364, 1535, 3453\u20133458.\nGARGESHA, M., AND KUCHI, P. 2002. Facial expression recognition using artificial neural networks. Artificial Neural Computer Systems, 1\u20136."}, {"heading": "HE, L., JIANG, D., YANG, L., PEI, E., WU, P., AND SAHLI,", "text": "H. 2015. Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks. In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, ACM, 73\u201380.\nHINTON, G. E., AND SALAKHUTDINOV, R. R. 2006. Reducing the dimensionality of data with neural networks. Science 313, 5786, 504\u2013507.\nHINTON, G. E., OSINDERO, S., AND TEH, Y.-W. 2006. A fast learning algorithm for deep belief nets. Neural Computation 18, 7, 1527\u20131554."}, {"heading": "JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG,", "text": "J., GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.\nJUNG, H., LEE, S., YIM, J., PARK, S., AND KIM, J. 2015. Joint fine-tuning in deep neural networks for facial expression recognition. In Proceedings of the IEEE International Conference on Computer Vision, 2983\u20132991."}, {"heading": "KAHOU, S. E., PAL, C., BOUTHILLIER, X., FROUMENTY, P.,", "text": "GU\u0308LC\u0327EHRE, C\u0327., MEMISEVIC, R., VINCENT, P., COURVILLE, A., BENGIO, Y., AND FERRARI, R. C. 2013. Combining modality specific deep neural networks for emotion recognition in video. In Proceedings of the 15th ACM on International conference on multimodal interaction, ACM, 543\u2013550."}, {"heading": "KAHOU, S. E., BOUTHILLIER, X., LAMBLIN, P., GULCEHRE,", "text": "C., MICHALSKI, V., KONDA, K., JEAN, S., FROUMENTY, P., DAUPHIN, Y., BOULANGER-LEWANDOWSKI, N., ET AL. 2015. Emonets: Multimodal deep learning approaches for emotion recognition in video. Journal on Multimodal User Interfaces, 1\u201313.\nKARPATHY, A., TODERICI, G., SHETTY, S., LEUNG, T., SUKTHANKAR, R., AND FEI-FEI, L. 2014. Large-scale video classification with convolutional neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1725\u2013 1732."}, {"heading": "KINGMA, D. P., MOHAMED, S., REZENDE, D. J., AND", "text": "WELLING, M. 2014. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, 3581\u20133589.\nKLEIN, T., AND PICARD, W. 1999. Computer response to user frustration. MIT Media Laboratory Vision and Modelling Group Technical Reports, TR 480."}, {"heading": "KO\u0141AKOWSKA, A., LANDOWSKA, A., SZWOCH, M., SZWOCH,", "text": "W., AND WRO\u0301BEL, M. 2014. Emotion recognition and its applications. In Human-Computer Systems Interaction: Backgrounds and Applications 3. Springer, 51\u201362.\nKOTSIA, I., AND PITAS, I. 2007. Facial expression recognition in image sequences using geometric deformation features and support vector machines. Transactions on Image Processing 16, 1, 172\u2013187."}, {"heading": "KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.", "text": "Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 1097\u2013 1105.\nLEE, D.-H. 2013. Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, vol. 3.\nLI, W.-J., WANG, C.-J., XU, D.-X., AND CHEN, S.-F. 2004. Illumination invariant face recognition based on neural network ensemble. In International Conference on Tools with Artificial Intelligence, IEEE, 486\u2013490.\nLIU, M., LI, S., SHAN, S., WANG, R., AND CHEN, X. 2014. Deeply learning deformable facial action parts model for dynamic expression analysis. In Computer Vision\u2013ACCV 2014. Springer, 143\u2013157.\nLIU, M., SHAN, S., WANG, R., AND CHEN, X. 2014. Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1749\u20131756."}, {"heading": "LIU, M., WANG, R., LI, S., SHAN, S., HUANG, Z., AND CHEN,", "text": "X. 2014. Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild. In Proceedings of the 16th International Conference on Multimodal Interaction, ACM, 494\u2013501.\nLIU, P., HAN, S., MENG, Z., AND TONG, Y. 2014. Facial expression recognition via a boosted deep belief network. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1805\u20131812."}, {"heading": "LUCEY, P., COHN, J. F., KANADE, T., SARAGIH, J., AMBADAR,", "text": "Z., AND MATTHEWS, I. 2010. The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression. In Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, 94\u2013101."}, {"heading": "MATHUR, S. N., AHLAWAT, A. K., AND VISHWAKARMA, V. P.", "text": "2008. Illumination invariant face recognition using supervised and unsupervised learning algorithms. In Proceedings of World Academy of Science, Engineering and Technology, vol. 33.\nMICHEL, P., AND EL KALIOUBY, R. 2003. Real time facial expression recognition in video using support vector machines. In Proceedings of the 5th international conference on Multimodal interfaces, ACM, 258\u2013264.\nNWE, T. L., FOO, S. W., AND DE SILVA, L. C. 2003. Speech emotion recognition using Hidden Markov Models. Speech communication 41, 4, 603\u2013623."}, {"heading": "PANTIC, M., VALSTAR, M., RADEMAKER, R., AND MAAT, L.", "text": "2005. Web-based database for facial expression analysis. In International Conference on Multimedia and Expo, IEEE, 5\u2013pp."}, {"heading": "PAPANDREOU, G., CHEN, L.-C., MURPHY, K., AND YUILLE,", "text": "A. L. 2015. Weakly-and semi-supervised learning of a dcnn for semantic image segmentation. arXiv:1502.02734."}, {"heading": "PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND", "text": "SALESIN, D. H. 2006. Synthesizing realistic facial expressions from photographs. In ACM SIGGRAPH 2006 Courses, ACM, 19.\nPRESTI, L., AND CASCIA, M. 2015. Using hankel matrices for dynamics-based facial emotion recognition and pain detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 26\u201333."}, {"heading": "ANONYMOUS SUBMISSION. 2016.", "text": ""}, {"heading": "PYUN, H., KIM, Y., CHAE, W., KANG, H. W., AND SHIN, S. Y.", "text": "2003. An example-based approach for facial expression cloning. In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, Eurographics Association, 167\u2013176.\nSCHULLER, B., RIGOLL, G., AND LANG, M. 2004. Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture. In International Conference on Acoustics, Speech, and Signal Processing, vol. 1, IEEE, I\u2013577.\nSENECHAL, T., MCDUFF, D., AND KALIOUBY, R. 2015. Facial action unit detection using active learning and an efficient non-linear kernel approximation. In Proceedings of the IEEE International Conference on Computer Vision Workshops, 10\u201318.\nSHAN, C., GONG, S., AND MCOWAN, P. W. 2005. Robust facial expression recognition using local binary patterns. In International Conference on Image Processing, vol. 2, IEEE, II\u2013370.\nSOURCES. Visual information processing and learning. [Online; accessed 10-July-2015]."}, {"heading": "SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S.,", "text": "ANGUELOV, D., ERHAN, D., VANHOUCKE, V., AND RABINOVICH, A. 2015. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1\u20139."}, {"heading": "TAIGMAN, Y., YANG, M., RANZATO, M., AND WOLF, L. 2014.", "text": "Deepface: Closing the gap to human-level performance in face verification. In Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1701\u20131708.\nTANG, Y., SALAKHUTDINOV, R., AND HINTON, G. 2012. Deep lambertian networks. arXiv:1206.6445."}, {"heading": "TRAN, D., BOURDEV, L., FERGUS, R., TORRESANI, L., AND", "text": "PALURI, M. 2014. Learning spatiotemporal features with 3d convolutional networks. arXiv preprint arXiv:1412.0767.\nVALSTAR, M., AND PANTIC, M. 2010. Induced disgust, happiness and surprise: an addition to the mmi facial expression database. In Workshop on EMOTION: Corpora for Research on Emotion and Affect, 65."}, {"heading": "VIERIU, R.-L., TULYAKOV, S., SEMENIUTA, S., SANGINETO,", "text": "E., AND SEBE, N. 2015. Facial expression recognition under a wide range of head poses. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1\u20137."}, {"heading": "VINCENT, P., LAROCHELLE, H., BENGIO, Y., AND MANZAGOL,", "text": "P.-A. 2008. Extracting and composing robust features with de-\nnoising autoencoders. In Proceedings of the 25th international conference on Machine learning, ACM, 1096\u20131103.\nVIOLA, P., AND JONES, M. J. 2004. Robust real-time face detection. International Journal of Computer Vision 57, 2, 137\u2013154."}, {"heading": "WALECKI, R., RUDOVIC, O., PAVLOVIC, V., AND PANTIC, M.", "text": "2015. Variable-state latent conditional random fields for facial expression recognition and action unit detection. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1\u20138.\nWANG, Z., WANG, S., AND JI, Q. 2013. Capturing complex spatio-temporal relations among facial muscles for facial expression recognition. In Computer Vision and Pattern Recognition (CVPR), IEEE, 3422\u20133429.\nWATERS, K. 1987. A muscle model for animation threedimensional facial expression. In Proceedings of the annual conference on Computer graphics and interactive techniques, vol. 21, ACM, 17\u201324."}, {"heading": "WESTON, J., RATLE, F., MOBAHI, H., AND COLLOBERT, R.", "text": "2012. Deep learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade. Springer, 639\u2013655.\nZHANG, Z., LUO, P., LOY, C.-C., AND TANG, X. 2015. Learning social relation traits from face images. In Proceedings of the IEEE International Conference on Computer Vision, 3631\u20133639."}, {"heading": "ZHAO, K., CHU, W.-S., DE LA TORRE, F., COHN, J. F., AND", "text": "ZHANG, H. 2015. Joint patch and multi-label learning for facial action unit detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2207\u20132216."}], "references": [{"title": "A neural network based facial expression recognition using fisherface", "author": ["Z. ABIDIN", "A. HARJOKO"], "venue": "International Journal of Computer Applications 59, 3, 30\u201334.", "citeRegEx": "ABIDIN and HARJOKO,? 2012", "shortCiteRegEx": "ABIDIN and HARJOKO", "year": 2012}, {"title": "Exploiting models of personality and emotions to control the behavior of animated interactive agents", "author": ["E. ANDR\u00c9", "M. KLESEN", "P. GEBHARD", "S. ALLEN", "T. RIST"], "venue": "Workshop on Achieving Human-Like Behavior in Interactive Animated Agents, 3\u20137.", "citeRegEx": "ANDR\u00c9 et al\\.,? 2000", "shortCiteRegEx": "ANDR\u00c9 et al\\.", "year": 2000}, {"title": "Incremental face alignment in the wild", "author": ["A. ASTHANA", "S. ZAFEIRIOU", "S. CHENG", "M. PANTIC"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1859\u2013 1866.", "citeRegEx": "ASTHANA et al\\.,? 2014", "shortCiteRegEx": "ASTHANA et al\\.", "year": 2014}, {"title": "Convex neural networks", "author": ["Y. BENGIO", "N.L. ROUX", "P. VINCENT", "O. DELALLEAU", "P. MARCOTTE"], "venue": "Advances in Neural Information Processing Systems, 123\u2013130.", "citeRegEx": "BENGIO et al\\.,? 2005", "shortCiteRegEx": "BENGIO et al\\.", "year": 2005}, {"title": "On contrastive divergence learning", "author": ["M.A. CARREIRA-PERPINAN", "G.E. HINTON"], "venue": "Proceedings of International Workshop on Artificial Intelligence and Statistics, 33\u201340.", "citeRegEx": "CARREIRA.PERPINAN and HINTON,? 2005", "shortCiteRegEx": "CARREIRA.PERPINAN and HINTON", "year": 2005}, {"title": "Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents", "author": ["J. CASSELL", "C. PELACHAUD", "N. BADLER", "M. STEEDMAN", "B. ACHORN", "T. BECKET", "B. DOUVILLE", "S. PREVOST", "M. STONE"], "venue": "Proceedings of the 21st annual con-", "citeRegEx": "CASSELL et al\\.,? 1994", "shortCiteRegEx": "CASSELL et al\\.", "year": 1994}, {"title": "Interactive agents for multimodal emotional user interaction", "author": ["E. CEREZO", "S. BALDASSARRI", "F. SERON"], "venue": "Multi Conferences on Computer Science and Information Systems, 35\u2013", "citeRegEx": "CEREZO et al\\.,? 2007", "shortCiteRegEx": "CEREZO et al\\.", "year": 2007}, {"title": "3d model-based continuous emotion recognition", "author": ["H. CHEN", "J. LI", "F. ZHANG", "Y. LI", "H. WANG"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1836\u20131845.", "citeRegEx": "CHEN et al\\.,? 2015", "shortCiteRegEx": "CHEN et al\\.", "year": 2015}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. COWIE", "E. DOUGLAS-COWIE", "N. TSAPATSOULIS", "G. VOTSIS", "S. KOLLIAS", "W. FELLENZ", "J.G. TAYLOR"], "venue": "Signal Processing Magazine, IEEE 18, 1, 32\u201380.", "citeRegEx": "COWIE et al\\.,? 2001", "shortCiteRegEx": "COWIE et al\\.", "year": 2001}, {"title": "Emotion recognition using PHOG and LPQ features", "author": ["A. DHALL", "A. ASTHANA", "R. GOECKE", "T. GEDEON"], "venue": "International Conference on Automatic Face & Gesture Recognition, IEEE, 878\u2013883.", "citeRegEx": "DHALL et al\\.,? 2011", "shortCiteRegEx": "DHALL et al\\.", "year": 2011}, {"title": "Role of facial expressions in social interactions", "author": ["C. FRITH"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences 364, 1535, 3453\u20133458.", "citeRegEx": "FRITH,? 2009", "shortCiteRegEx": "FRITH", "year": 2009}, {"title": "Facial expression recognition using artificial neural networks", "author": ["M. GARGESHA", "P. KUCHI"], "venue": "Artificial Neural Computer Systems, 1\u20136.", "citeRegEx": "GARGESHA and KUCHI,? 2002", "shortCiteRegEx": "GARGESHA and KUCHI", "year": 2002}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. HE", "D. JIANG", "L. YANG", "E. PEI", "P. WU", "H. SAHLI"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, ACM, 73\u201380.", "citeRegEx": "HE et al\\.,? 2015", "shortCiteRegEx": "HE et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. HINTON", "R.R. SALAKHUTDINOV"], "venue": "Science 313, 5786, 504\u2013507.", "citeRegEx": "HINTON and SALAKHUTDINOV,? 2006", "shortCiteRegEx": "HINTON and SALAKHUTDINOV", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. HINTON", "S. OSINDERO", "TEH", "Y.-W."], "venue": "Neural Computation 18, 7, 1527\u20131554.", "citeRegEx": "HINTON et al\\.,? 2006", "shortCiteRegEx": "HINTON et al\\.", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. JIA", "E. SHELHAMER", "J. DONAHUE", "S. KARAYEV", "J. LONG", "R. GIRSHICK", "S. GUADARRAMA", "T. DARRELL"], "venue": "arXiv preprint arXiv:1408.5093.", "citeRegEx": "JIA et al\\.,? 2014", "shortCiteRegEx": "JIA et al\\.", "year": 2014}, {"title": "Joint fine-tuning in deep neural networks for facial expression recognition", "author": ["JUNG H.", "LEE S.", "YIM J.", "PARK S.", "KIM", "J."], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2983\u20132991.", "citeRegEx": "H. et al\\.,? 2015", "shortCiteRegEx": "H. et al\\.", "year": 2015}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. KAHOU", "C. PAL", "X. BOUTHILLIER", "P. FROUMENTY", "\u00c7. G\u00dcL\u00c7EHRE", "R. MEMISEVIC", "P. VINCENT", "A. COURVILLE", "Y. BENGIO", "R.C. FERRARI"], "venue": "Proceedings of the 15th ACM on International con-", "citeRegEx": "KAHOU et al\\.,? 2013", "shortCiteRegEx": "KAHOU et al\\.", "year": 2013}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. KAHOU", "X. BOUTHILLIER", "P. LAMBLIN", "C. GULCEHRE", "V. MICHALSKI", "K. KONDA", "S. JEAN", "P. FROUMENTY", "Y. DAUPHIN", "N BOULANGER-LEWANDOWSKI"], "venue": "Journal on Multimodal User Inter-", "citeRegEx": "KAHOU et al\\.,? 2015", "shortCiteRegEx": "KAHOU et al\\.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. KARPATHY", "G. TODERICI", "S. SHETTY", "T. LEUNG", "R. SUKTHANKAR", "L. FEI-FEI"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1725\u2013 1732.", "citeRegEx": "KARPATHY et al\\.,? 2014", "shortCiteRegEx": "KARPATHY et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. KINGMA", "S. MOHAMED", "D.J. REZENDE", "M. WELLING"], "venue": "Advances in Neural Information Processing Systems, 3581\u20133589.", "citeRegEx": "KINGMA et al\\.,? 2014", "shortCiteRegEx": "KINGMA et al\\.", "year": 2014}, {"title": "Computer response to user frustration", "author": ["T. KLEIN", "W. PICARD"], "venue": "MIT Media Laboratory Vision and Modelling Group Technical Reports, TR 480.", "citeRegEx": "KLEIN and PICARD,? 1999", "shortCiteRegEx": "KLEIN and PICARD", "year": 1999}, {"title": "Emotion recognition and its applications", "author": ["A. KO\u0141AKOWSKA", "A. LANDOWSKA", "M. SZWOCH", "W. SZWOCH", "M. WR\u00d3BEL"], "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3. Springer, 51\u201362.", "citeRegEx": "KO\u0141AKOWSKA et al\\.,? 2014", "shortCiteRegEx": "KO\u0141AKOWSKA et al\\.", "year": 2014}, {"title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines", "author": ["I. KOTSIA", "I. PITAS"], "venue": "Transactions on Image Processing 16, 1, 172\u2013187.", "citeRegEx": "KOTSIA and PITAS,? 2007", "shortCiteRegEx": "KOTSIA and PITAS", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "Advances in Neural Information Processing Systems, 1097\u2013 1105.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks", "author": ["LEE", "D.-H."], "venue": "Workshop on Challenges in Representation Learning, ICML, vol. 3.", "citeRegEx": "LEE and D..H.,? 2013", "shortCiteRegEx": "LEE and D..H.", "year": 2013}, {"title": "Illumination invariant face recognition based on neural network ensemble", "author": ["LI", "W.-J.", "WANG", "XU C.-J.", "D.-X.", "CHEN", "S.-F."], "venue": "International Conference on Tools with Artificial Intelligence, IEEE, 486\u2013490.", "citeRegEx": "LI et al\\.,? 2004", "shortCiteRegEx": "LI et al\\.", "year": 2004}, {"title": "Deeply learning deformable facial action parts model for dynamic expression analysis", "author": ["M. LIU", "S. LI", "S. SHAN", "R. WANG", "X. CHEN"], "venue": "Computer Vision\u2013ACCV 2014. Springer, 143\u2013157.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["M. LIU", "S. SHAN", "R. WANG", "X. CHEN"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1749\u20131756.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild", "author": ["M. LIU", "R. WANG", "S. LI", "S. SHAN", "Z. HUANG", "X. CHEN"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, ACM, 494\u2013501.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["P. LIU", "S. HAN", "Z. MENG", "Y. TONG"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1805\u20131812.", "citeRegEx": "LIU et al\\.,? 2014", "shortCiteRegEx": "LIU et al\\.", "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression", "author": ["P. LUCEY", "J.F. COHN", "T. KANADE", "J. SARAGIH", "Z. AMBADAR", "I. MATTHEWS"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE, 94\u2013101.", "citeRegEx": "LUCEY et al\\.,? 2010", "shortCiteRegEx": "LUCEY et al\\.", "year": 2010}, {"title": "Illumination invariant face recognition using supervised and unsupervised learning algorithms", "author": ["S.N. MATHUR", "A.K. AHLAWAT", "V.P. VISHWAKARMA"], "venue": "Proceedings of World Academy of Science, Engineering and Technology, vol. 33.", "citeRegEx": "MATHUR et al\\.,? 2008", "shortCiteRegEx": "MATHUR et al\\.", "year": 2008}, {"title": "Real time facial expression recognition in video using support vector machines", "author": ["P. MICHEL", "R. EL KALIOUBY"], "venue": "Proceedings of the 5th international conference on Multimodal interfaces, ACM, 258\u2013264.", "citeRegEx": "MICHEL and KALIOUBY,? 2003", "shortCiteRegEx": "MICHEL and KALIOUBY", "year": 2003}, {"title": "Speech emotion recognition using Hidden Markov Models", "author": ["NWE T.L.", "FOO S.W.", "SILVA L.C. DE"], "venue": "Speech communication 41, 4, 603\u2013623.", "citeRegEx": "L. et al\\.,? 2003", "shortCiteRegEx": "L. et al\\.", "year": 2003}, {"title": "Web-based database for facial expression analysis", "author": ["M. PANTIC", "M. VALSTAR", "R. RADEMAKER", "L. MAAT"], "venue": "International Conference on Multimedia and Expo, IEEE, 5\u2013pp.", "citeRegEx": "PANTIC et al\\.,? 2005", "shortCiteRegEx": "PANTIC et al\\.", "year": 2005}, {"title": "Weakly-and semi-supervised learning of a dcnn for semantic image segmentation", "author": ["G. PAPANDREOU", "CHEN", "L.-C.", "K. MURPHY", "A.L. YUILLE"], "venue": "arXiv:1502.02734.", "citeRegEx": "PAPANDREOU et al\\.,? 2015", "shortCiteRegEx": "PAPANDREOU et al\\.", "year": 2015}, {"title": "Synthesizing realistic facial expressions from photographs", "author": ["F. PIGHIN", "J. HECKER", "D. LISCHINSKI", "R. SZELISKI", "D.H. SALESIN"], "venue": "ACM SIGGRAPH 2006 Courses, ACM,", "citeRegEx": "PIGHIN et al\\.,? 2006", "shortCiteRegEx": "PIGHIN et al\\.", "year": 2006}, {"title": "Using hankel matrices for dynamics-based facial emotion recognition and pain detection", "author": ["L. PRESTI", "M. CASCIA"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 26\u201333.", "citeRegEx": "PRESTI and CASCIA,? 2015", "shortCiteRegEx": "PRESTI and CASCIA", "year": 2015}, {"title": "An example-based approach for facial expression cloning", "author": ["H. PYUN", "Y. KIM", "W. CHAE", "H.W. KANG", "S.Y. SHIN"], "venue": "Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, Eurographics Association, 167\u2013176.", "citeRegEx": "PYUN et al\\.,? 2003", "shortCiteRegEx": "PYUN et al\\.", "year": 2003}, {"title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture", "author": ["B. SCHULLER", "G. RIGOLL", "M. LANG"], "venue": "International Conference on Acoustics, Speech, and Signal Processing, vol. 1, IEEE, I\u2013577.", "citeRegEx": "SCHULLER et al\\.,? 2004", "shortCiteRegEx": "SCHULLER et al\\.", "year": 2004}, {"title": "Facial action unit detection using active learning and an efficient non-linear kernel approximation", "author": ["T. SENECHAL", "D. MCDUFF", "R. KALIOUBY"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 10\u201318.", "citeRegEx": "SENECHAL et al\\.,? 2015", "shortCiteRegEx": "SENECHAL et al\\.", "year": 2015}, {"title": "Robust facial expression recognition using local binary patterns", "author": ["C. SHAN", "S. GONG", "P.W. MCOWAN"], "venue": "International Conference on Image Processing, vol. 2, IEEE, II\u2013370.", "citeRegEx": "SHAN et al\\.,? 2005", "shortCiteRegEx": "SHAN et al\\.", "year": 2005}, {"title": "Going deeper with convolutions", "author": ["C. SZEGEDY", "W. LIU", "Y. JIA", "P. SERMANET", "S. REED", "D. ANGUELOV", "D. ERHAN", "V. VANHOUCKE", "A. RABINOVICH"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1\u20139.", "citeRegEx": "SZEGEDY et al\\.,? 2015", "shortCiteRegEx": "SZEGEDY et al\\.", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. TAIGMAN", "M. YANG", "M. RANZATO", "L. WOLF"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 1701\u20131708.", "citeRegEx": "TAIGMAN et al\\.,? 2014", "shortCiteRegEx": "TAIGMAN et al\\.", "year": 2014}, {"title": "Deep lambertian networks", "author": ["Y. TANG", "R. SALAKHUTDINOV", "G. HINTON"], "venue": "arXiv:1206.6445.", "citeRegEx": "TANG et al\\.,? 2012", "shortCiteRegEx": "TANG et al\\.", "year": 2012}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. TRAN", "L. BOURDEV", "R. FERGUS", "L. TORRESANI", "M. PALURI"], "venue": "arXiv preprint arXiv:1412.0767.", "citeRegEx": "TRAN et al\\.,? 2014", "shortCiteRegEx": "TRAN et al\\.", "year": 2014}, {"title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database", "author": ["M. VALSTAR", "M. PANTIC"], "venue": "Workshop on EMOTION: Corpora for Research on Emotion and Affect, 65.", "citeRegEx": "VALSTAR and PANTIC,? 2010", "shortCiteRegEx": "VALSTAR and PANTIC", "year": 2010}, {"title": "Facial expression recognition under a wide range of head poses", "author": ["VIERIU", "R.-L.", "S. TULYAKOV", "S. SEMENIUTA", "E. SANGINETO", "N. SEBE"], "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1\u20137.", "citeRegEx": "VIERIU et al\\.,? 2015", "shortCiteRegEx": "VIERIU et al\\.", "year": 2015}, {"title": "Extracting and composing robust features", "author": ["P. VINCENT", "H. LAROCHELLE", "Y. BENGIO", "MANZAGOL", "P.-A"], "venue": null, "citeRegEx": "VINCENT et al\\.,? \\Q2008\\E", "shortCiteRegEx": "VINCENT et al\\.", "year": 2008}, {"title": "Robust real-time face detection", "author": ["P. VIOLA", "M.J. JONES"], "venue": "International Journal of Computer Vision 57, 2, 137\u2013154.", "citeRegEx": "VIOLA and JONES,? 2004", "shortCiteRegEx": "VIOLA and JONES", "year": 2004}, {"title": "Variable-state latent conditional random fields for facial expression recognition and action unit detection", "author": ["R. WALECKI", "O. RUDOVIC", "V. PAVLOVIC", "M. PANTIC"], "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on, vol. 1, IEEE, 1\u20138.", "citeRegEx": "WALECKI et al\\.,? 2015", "shortCiteRegEx": "WALECKI et al\\.", "year": 2015}, {"title": "Capturing complex spatio-temporal relations among facial muscles for facial expression recognition", "author": ["Z. WANG", "S. WANG", "JI", "Q."], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE, 3422\u20133429.", "citeRegEx": "WANG et al\\.,? 2013", "shortCiteRegEx": "WANG et al\\.", "year": 2013}, {"title": "A muscle model for animation threedimensional facial expression", "author": ["K. WATERS"], "venue": "Proceedings of the annual conference on Computer graphics and interactive techniques, vol. 21, ACM, 17\u201324.", "citeRegEx": "WATERS,? 1987", "shortCiteRegEx": "WATERS", "year": 1987}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. WESTON", "F. RATLE", "H. MOBAHI", "R. COLLOBERT"], "venue": "Neural Networks: Tricks of the Trade. Springer, 639\u2013655.", "citeRegEx": "WESTON et al\\.,? 2012", "shortCiteRegEx": "WESTON et al\\.", "year": 2012}, {"title": "Learning social relation traits from face images", "author": ["Z. ZHANG", "P. LUO", "LOY", "C.-C.", "X. TANG"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 3631\u20133639.", "citeRegEx": "ZHANG et al\\.,? 2015", "shortCiteRegEx": "ZHANG et al\\.", "year": 2015}, {"title": "Joint patch and multi-label learning for facial action unit detection", "author": ["K. ZHAO", "CHU", "W.-S.", "F. DE LA TORRE", "J.F. COHN", "H. ZHANG"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2207\u20132216.", "citeRegEx": "ZHAO et al\\.,? 2015", "shortCiteRegEx": "ZHAO et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "In fact, studies have shown that non-verbal communication accounts for more than half of all societal interactions [Frith 2009].", "startOffset": 115, "endOffset": 127}, {"referenceID": 8, "context": "Adding emotion recognition and tailoring responses towards users emotional state can help improve human computer interaction drastically [Cowie et al. 2001; Zhang et al. 2015] and help keep users engaged.", "startOffset": 137, "endOffset": 175}, {"referenceID": 55, "context": "Adding emotion recognition and tailoring responses towards users emotional state can help improve human computer interaction drastically [Cowie et al. 2001; Zhang et al. 2015] and help keep users engaged.", "startOffset": 137, "endOffset": 175}, {"referenceID": 21, "context": "Last two decades have seen some innovation in this area [Klein and Picard 1999; Cerezo et al. 2007; Andr\u00e9 et al. 2000] such as humanoid robots for example Pepper which can both understand and mimic human emotions.", "startOffset": 56, "endOffset": 118}, {"referenceID": 6, "context": "Last two decades have seen some innovation in this area [Klein and Picard 1999; Cerezo et al. 2007; Andr\u00e9 et al. 2000] such as humanoid robots for example Pepper which can both understand and mimic human emotions.", "startOffset": 56, "endOffset": 118}, {"referenceID": 37, "context": "Accurate characterization of face geometry and muscle motion can be used for both expression identification and synthesis [Pighin et al. 2006; Wang et al. 2013] with applications towards computer animation [Cassell et al.", "startOffset": 122, "endOffset": 160}, {"referenceID": 52, "context": "Accurate characterization of face geometry and muscle motion can be used for both expression identification and synthesis [Pighin et al. 2006; Wang et al. 2013] with applications towards computer animation [Cassell et al.", "startOffset": 122, "endOffset": 160}, {"referenceID": 5, "context": "2013] with applications towards computer animation [Cassell et al. 1994].", "startOffset": 51, "endOffset": 72}, {"referenceID": 53, "context": "Such approaches combine very high dimensional facial features from facial topology and compress them to lower dimensions using a series of parameters or transformations [Waters 1987; Pyun et al. 2003].", "startOffset": 169, "endOffset": 200}, {"referenceID": 39, "context": "Such approaches combine very high dimensional facial features from facial topology and compress them to lower dimensions using a series of parameters or transformations [Waters 1987; Pyun et al. 2003].", "startOffset": 169, "endOffset": 200}, {"referenceID": 14, "context": "Over the past decade algorithms for training neural nets have dramatically evolved, allowing us to efficiently train deep neural nets [Hinton et al. 2006; Jung et al. 2015].", "startOffset": 134, "endOffset": 172}, {"referenceID": 24, "context": "Such models have become a strong driving force in modern computer vision and excel at object classification [Krizhevsky et al. 2012], segmentation and facial recognition [Taigman et al.", "startOffset": 108, "endOffset": 132}, {"referenceID": 44, "context": "2012], segmentation and facial recognition [Taigman et al. 2014].", "startOffset": 43, "endOffset": 64}, {"referenceID": 23, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 42, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 9, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 51, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 38, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 48, "context": "Machine learning strategies such as random forests or SVMs combined with local binary features (or sometimes facial fiducial points) have been used for facial expression recognition in the past [Kotsia and Pitas 2007; Michel and El Kaliouby 2003; Shan et al. 2005; Dhall et al. 2011; Walecki et al. 2015; Presti and Cascia 2015; Vieriu et al. 2015].", "startOffset": 194, "endOffset": 348}, {"referenceID": 40, "context": "Other intriguing methodologies include performing emotion recognition through speech [Nwe et al. 2003; Schuller et al. 2004], using temporal features and manifold learning [Liu et al.", "startOffset": 85, "endOffset": 124}, {"referenceID": 52, "context": "2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al.", "startOffset": 53, "endOffset": 126}, {"referenceID": 18, "context": "2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al.", "startOffset": 53, "endOffset": 126}, {"referenceID": 7, "context": "2004], using temporal features and manifold learning [Liu et al. 2014b; Wang et al. 2013; Kahou et al. 2015; Chen et al. 2015] and combining multiple kernel based approaches [Liu et al.", "startOffset": 53, "endOffset": 126}, {"referenceID": 41, "context": "2015] and combining multiple kernel based approaches [Liu et al. 2014c; Senechal et al. 2015].", "startOffset": 53, "endOffset": 93}, {"referenceID": 24, "context": "Deep neural networks have proven to be an effective tool to classify and segment high dimensional data such as images [Krizhevsky et al. 2012; Szegedy et al. 2015], audio and videos [Karpathy et al.", "startOffset": 118, "endOffset": 163}, {"referenceID": 43, "context": "Deep neural networks have proven to be an effective tool to classify and segment high dimensional data such as images [Krizhevsky et al. 2012; Szegedy et al. 2015], audio and videos [Karpathy et al.", "startOffset": 118, "endOffset": 163}, {"referenceID": 19, "context": "2015], audio and videos [Karpathy et al. 2014; Tran et al. 2014].", "startOffset": 24, "endOffset": 64}, {"referenceID": 46, "context": "2015], audio and videos [Karpathy et al. 2014; Tran et al. 2014].", "startOffset": 24, "endOffset": 64}, {"referenceID": 44, "context": "With advances in convolutional neural nets, we have seen neural nets applied for face detection [Taigman et al. 2014; Zhao et al. 2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 56, "context": "With advances in convolutional neural nets, we have seen neural nets applied for face detection [Taigman et al. 2014; Zhao et al. 2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 0, "context": "2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 33, "endOffset": 99}, {"referenceID": 11, "context": "2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 33, "endOffset": 99}, {"referenceID": 12, "context": "2015] and expression recognition [Abidin and Harjoko 2012; Gargesha and Kuchi 2002; He et al. 2015] but these networks were not deep enough or used other feature extraction techniques like PCA or Fisherface.", "startOffset": 33, "endOffset": 99}, {"referenceID": 54, "context": "This issue can also be solved using embeddings in lower dimensional manifold [Weston et al. 2012; Kingma et al. 2014] or pre-train using pseudo labels [Lee 2013] thereby requiring fewer number of labeled samples.", "startOffset": 77, "endOffset": 117}, {"referenceID": 20, "context": "This issue can also be solved using embeddings in lower dimensional manifold [Weston et al. 2012; Kingma et al. 2014] or pre-train using pseudo labels [Lee 2013] thereby requiring fewer number of labeled samples.", "startOffset": 77, "endOffset": 117}, {"referenceID": 36, "context": "Approaches based on semi supervised learning have shown to work for smaller labeled datasets [Papandreou et al. 2015] and techniques using deep neural nets to combine labels and unlabeled data in the same architecture [Liu et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 17, "context": "2015] and techniques using deep neural nets to combine labels and unlabeled data in the same architecture [Liu et al. 2014d; Kahou et al. 2013] have emerged victorious.", "startOffset": 106, "endOffset": 143}, {"referenceID": 32, "context": "Introducing invariants in neural networks is an area of active research, some examples include illumination invariant face recognition techniques [Mathur et al. 2008; Li et al. 2004] and deep lambertian networks [Tang et al.", "startOffset": 146, "endOffset": 182}, {"referenceID": 26, "context": "Introducing invariants in neural networks is an area of active research, some examples include illumination invariant face recognition techniques [Mathur et al. 2008; Li et al. 2004] and deep lambertian networks [Tang et al.", "startOffset": 146, "endOffset": 182}, {"referenceID": 45, "context": "2004] and deep lambertian networks [Tang et al. 2012; Jung et al. 2015].", "startOffset": 35, "endOffset": 71}, {"referenceID": 50, "context": "Our facial expression recognition pipeline comprises of Viola-Jones algorithm [Viola and Jones 2004] for face detection followed by a deep convolutional neural network for predicting expressions.", "startOffset": 78, "endOffset": 100}, {"referenceID": 13, "context": "Stacked autoencoders can be used to convert high dimensional data into lower dimensional space which can be useful for classification, visualization or retrieval [Hinton and Salakhutdinov 2006].", "startOffset": 162, "endOffset": 193}, {"referenceID": 24, "context": "The autoencoder topology is inspired by ImageNet [Krizhevsky et al. 2012] and comprises of convolutional layers gradually reducing data dimensionality until we reach a fully connected layer.", "startOffset": 49, "endOffset": 73}, {"referenceID": 19, "context": "In the same way that spatial convolutions consolidate nearby spatial characteristics of an image, we use the slow fusion model described in [Karpathy et al. 2014] to gradually combine temporal features across multiple frames.", "startOffset": 140, "endOffset": 162}, {"referenceID": 3, "context": "the predictor [Bengio et al. 2005].", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "[Asthana et al. 2014].", "startOffset": 0, "endOffset": 21}, {"referenceID": 31, "context": "The Cohn Kanade dataset [Lucey et al. 2010] is one of the oldest and well known dataset containing facial expression video clips.", "startOffset": 24, "endOffset": 43}, {"referenceID": 35, "context": "MMI facial expression dataset [Pantic et al. 2005] involves an ongoing effort for representing both enacted and induced facial expres-", "startOffset": 30, "endOffset": 50}, {"referenceID": 47, "context": "MMI which originally contained only posed facial expressions, was recently extended to include natural versions of happiness, disgust and surprise [Valstar and Pantic 2010].", "startOffset": 147, "endOffset": 172}, {"referenceID": 13, "context": "Since deep autoencoders can show slow convergence when trained from randomly initialized weights [Hinton and Salakhutdinov 2006], we used contrastive divergence minimization to train stacked autoencoder layers iteratively [Carreira-Perpinan and Hinton 2005].", "startOffset": 97, "endOffset": 128}, {"referenceID": 4, "context": "Since deep autoencoders can show slow convergence when trained from randomly initialized weights [Hinton and Salakhutdinov 2006], we used contrastive divergence minimization to train stacked autoencoder layers iteratively [Carreira-Perpinan and Hinton 2005].", "startOffset": 222, "endOffset": 257}, {"referenceID": 15, "context": "Our neural network was implemented using the Caffe framework [Jia et al. 2014] and trained using NVIDIA Tesla K40 GPUs.", "startOffset": 61, "endOffset": 78}, {"referenceID": 15, "context": "Both autoencoder and predictor network topologies are implemented as Caffe prototxt files [Jia et al. 2014] and they will be made available for public usage.", "startOffset": 90, "endOffset": 107}], "year": 2016, "abstractText": "In this paper we present architectures based on deep neural nets for gesture recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filer based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets.", "creator": "LaTeX acmsiggraph.cls"}}}