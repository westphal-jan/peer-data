{"id": "1603.07185", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings", "abstract": "To form vectors, we apply a learning method to a token sequence derived from the database. We describe different techniques for extracting token sequences from a database. The techniques differ in their complexity, in the token sequences they output, and in the database information used (e.g. foreign keys). The vectors can be used to algebraically quantify semantic relationships between the token sequences, such as similarities and analogies. Vectors allow a dual view of the data: relational and (meaningful rather than purely syntactic) text sources.", "histories": [["v1", "Wed, 23 Mar 2016 13:57:33 GMT  (242kb,D)", "http://arxiv.org/abs/1603.07185v1", "Submitted to VLDB"]], "COMMENTS": "Submitted to VLDB", "reviews": [], "SUBJECTS": "cs.CL cs.DB", "authors": ["rajesh bordawekar", "oded shmueli"], "accepted": false, "id": "1603.07185"}, "pdf": {"name": "1603.07185.pdf", "metadata": {"source": "CRF", "title": "Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings", "authors": ["Rajesh Bordawekar", "Oded Shmueli"], "emails": ["bordaw@us.ibm.com", "oshmu@cs.technion.ac.il"], "sections": [{"heading": null, "text": "Vectors enable a dual view of the data: relational and (meaningful rather than purely syntactical) text. We introduce and explore a new class of queries called cognitive intelligence (CI) queries that extract information from the database based, in part, on the relationships encoded by vectors. We have implemented a prototype system on top of Spark [1] to exhibit the power of CI queries. Here, CI queries are realized via SQL UDFs. This power goes far beyond text extensions to relational systems due to the information encoded in vectors.\nWe also consider various extensions to the basic scheme, including using a collection of views derived from the database to focus on a domain of interest, utilizing vectors and/or text from external sources, maintaining vectors as the database evolves and exploring a database without utilizing its schema. For the latter, we consider minimal extensions to SQL to vastly improve query expressiveness."}, {"heading": "1. INTRODUCTION", "text": "Traditionally, relational databases have been used to analyze enterprise datasets that comprise mostly of well-qualified\n\u2217Work done while the author was visiting IBM Research.\ntyped entities (e.g., character(n), decimal, float, or timestamp). However, over the years, relational databases have been increasingly used to store and process free-formed unstructured text data, e.g., customer reviews, comments to posts, call center interactions, medical transcriptions, genomic datasets, system logs, etc. Databases with such unstructured text entities have a significant amount of latent semantic information; e.g., a word has a meaning (e.g., Deep), a group of words has a meaning (e.g., Deep Learning), and finally, even a group of words in a table row can be viewed to have a meaning (e.g., a person with ID 110 with Title of Professor has a job description, Deep Learning Research). At present, there is very limited support in the SQL infrastructure to develop semantic queries that can exploit semantic relationships between database tables\u2019 entities. Although, many databases support semantic queries either via text extenders that use dictioneries to identify word synonyms, e.g., DB2 Text Extender [5] or using RDF-based ontologies [13], these approaches lack capabilities for extracting and using latent semantic information from all the database entities.\nIn this paper, we introduce and explore a new class of queries, called Cognitive Intelligence (CI) queries, that extract information from a database based, in part, on the relationships among database entities encoded as vectors produced by a machine learning method. The vectors are designed such that vectors corresponding to closely related entities are also close in the semantic (vector) space using a cost metric. We use a distributed language embedding method from the Natural Language Processing (NLP) domain to assign a vector to each database-associated token (e.g., a token may be a word occurring in a table row, or the name of a column). Vectors may be produced by either learning on the database itself or using external text, or vector, sources. These vectors usually have low dimension (200-300) and capture the meaning of a token based on the contributions of other tokens in the contexts in which the token appears.\nOver the last few decades, a number of methods have been introduced for computing vector representations of words in a natural language [3]. The methods range from brute force learning by various types of neural networks [3], to log-linear classifiers [18] and to various matrix factorization techniques [12]. Lately, word2vec [17, 20, 19, 18] has gained prominence as the vectors it produces appear to capture syntactic as well semantic properties of words. There are alternative mechanisms for producing vectors of similar quality, for example GloVe [23] (we use word2vec as the method for\nar X\niv :1\n60 3.\n07 18\n5v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\n01 6\nconstructing vectors from database entities, although, we could use GloVe as well). These vectors can then be used to compute the semantic and/or grammatical closeness (e.g., singular/plural) of words as well as test for analogies such as a king to a man is like a queen to what? (answer: woman).\nIn the database context, a natural way of generating vectors is to apply a word embedding method (in our case, word2vec) to a token sequence generated from the database. Our scheme has three main phases: (1) Generating token sequences from the database tables (textification or tokenization), (2) Applying a vector construction method to the concatenation of these token sequences, and (3) Executing CI queries using the vectors produced by this method (once created, these vectors can be reused for answering any future queries). We usually use the terms training or learning to describe the operation of vector construction.\nThe execution flow in our scheme is depicted in Figure 1. At the top, we see a relation, empl, containing rows describing employees. Next, a textual representation of the data in the empl relation is extracted. Note that, which rows are textified can be controlled using standard relational operations (e.g., by defining a view). Next, we use a machine learning method (e.g., word2vec) to learn vectors for the words (tokens) in the extracted text. This phase can also use an external source, e.g., Wikipedia, as the source for text for method training. The result is a set of low-dimensional (say, of dimension 200) vectors, each representing one word (token). We use word as a synonym to token although some tokens may not be valid words in any natural language. In the last phase, these vectors may be used in querying, for example, via user defined functions (UDFs) in SQL. These UDFs compute distances between vectors in a semantic (vector) space using a distance metric (e.g., cosine distance, Jaccard distance) to determine contextual semantic similarities between corresponding database entities. The similarity results are then used to guide the relational query execution, thus enabling the relational engine to exploit latent semantic information for answering relational queries.\nIn this paper, we discuss various techniques for extracting token sequences from a database. The techniques differ in complexity, in the tokens they output and in the database information used (e.g., foreign keys).\nVectors can be used to quantify both syntactic and semantic relationships between the tokens such as similarities and analogies. By computing vector averages, database entities such as a column value in a row, a whole row, or even in some cases a whole column or relation, may be usefully encoded as vectors. This basic scheme can be extended in multiple ways, e.g., we can first form a collection of views derived from the database to focus on a domain of interest, and apply vector construction methods to the token sequence derived from these views. For example, suppose we have a database containing corporate data. We can restrict it (via a collection of views) to only information that is related to mid-size companies. Or in a medical database, restrict to only diabetes related information. This mechanism restricts the vocabulary (set of tokens) and focuses training on the domain of interest.\nVectors enable a dual view of the data: relational and (meaningful) text. The text view of a database can be queried directly, but more importantly, it can be used for enabling novel CI queries. Such queries can also navigate database tables without precise schema knowledge. For ex-\nample, one can ask: list database rows (of any relation) that have a field whose content designates an address that is physically nearby to the address of employee 55; the list is ordered by nearness to the employee 55 address. With this query, we are totally oblivious to the schema of the database. In fact, we can ask which relations contain such rows, and rank them according to the number of such rows. Specific examples of this feature are shown in Section 4.\nWe have implemented a prototype system (on top of Spark SQL [1]) to exhibit the power of CI queries. CI queries take relations as input and return relations as output. CI queries are realized via SQL and user defined functions, UDFs. CI queries power goes far beyond text extensions to relational systems due to the information encoded in vectors. For example, we can encode a field of a row as a vector and perform approximate semantic equality joins on such fields. CI queries augment the capabilities of the traditional relational OLAP queries and can be used in conjuction with the existing SQL operators (e.g., OLAP [8]).\nWe believe that this is the first work to explore applications of NLP-based machine learning techniques for enhancing and answering relational queries. The paper makes the following key contributions:\n1. Database tokenization and the association of a vector with each database entity represented by a token. This enables a dual database view: relational and meaningful text.\n2. Introducing a new class of queries, Cognitive Intelligence (CI) queries, that take advantage of this dual view. CI queries can be realized within standard SQL using UDFs or with minimal extensions to SQL. The\nvectors capture latent semantic relationships among database text entities.\n3. CI queries enrich SQL\u2019s expressiveness and enable sophisticated approximate similarity, semantic contextual matching and analogy queries.\n4. Using externally computed vectors, the CI queries can evaluate semantic relationships of entities that are not even present in a database with the database entities.\n5. CI queries can be used to navigate through the database using standard SQL or a minimal extension thereof, with very little knowledge of the database schema.\n6. We exhibit the feasibility of the approach by implementing a prototype system on top of Spark.\nThe paper is organized as follows: First, Section 2 discusses related work. Section 3 presents various methods for tokenization \u2013 producing the sequence of tokens for training, using external sources for text and vectors and operational issues. Section 4 presents usage examples. In Section 4.1, we present similarity queries. In Section 4.2, we explore query language extensions to support database navigation with minimal schema knowledge. In Section 4.3, we briefly introduce the power of analogy queries. Section 5 presents initial experiences of executing CI queries using a publically available dataset (DBLP), and outlines case-studies from other relevant domains. It also describes a prototype implementation on top of Spark SQL. An appendix reviews word2vec, the system we used to generate vectors, as well as mentions other similar systems. We conclude in Section 6."}, {"heading": "2. RELATED WORK", "text": "Language Embedding: Over the last 25 years or so, a number of methods have been introduced for obtaining a vector representation of words in a language [3], called language embedding. The methods range from \u201dbrute force\u201d learning by various types of neural networks (NNs) [3], to log-linear classifiers [18] and to various matrix formulations, such as matrix factorization techniques [12].\nLately, word2vec [17, 25] has gained prominence as the vectors it produces appear to capture syntactic as well semantic properties of words. word2vec is a popular machine learning method primarily used in the field of Natural Language Processing (NLP) [20, 19, 11]. Scanning a corpus (text), it generates a vector representation for each word in the text. A vector is usually of a low dimension (about 200-300) and represents the word. The vectors can be used to compute the semantic and/or grammatical closeness of words as well as test for analogies. The exact mechanism employed by word2vec and suggestions for alternatives are the subject of much research [21, 7, 11, 23, 2]. We note therefore that although word2vec has gained much prominence it is one of many possible methods for generating word representing vectors. Additional details concerning word2vec are provided in the appendix.\nVectors may be associated with larger bodies of text such as paragraphs and even documents. Applications to the paragraph and document embedding appear in [10, 6]. Recent work has also been exploring applying word embeddings to capture image semantics [27].\nRelational Databases: In the context of SQL, text capabilities, e.g., the use of synonyms, have been in practice for while [5]. In the literature, techniques for detecting similarity between records and fields have also been explored, some of the relevant references follow. Semantic similarity between database records is taken into account in [9]. Phrasebased ranking by applying an IR approach to relational data appears in [14]. Indexing and searching relational data by modeling tuples as virtual documents appear in [15]. Effective keyword-based selection of relational databases is explored in [31]. A system for detecting XML similarity, in content and structure, using a relational database is described in [30]. Related work on similarity Join appears in [4]. Semantic Queries are described in [22, 13]. Knowledge Databases [29, 28] are relevant to extracting semantics. Most recently, Shin et. al have described DeepDive [26] that uses machine learning techniques, e.g., Markov Logic based rules, to convert input unstructured documents into a structured knowledge base.\nWhat distinguishes this work from the relevant prior work is that we enable queries on relational data that exploit latent semantic information in the relational database. We use NLP techniques for associating each database text entity with a vector that captures its syntactical and semantic relationship to other database text entities. Further, these vectors are primarily based on the database itself (with external text or vectors as an option). This means that we assume no reliance on dictionaries, thesauri, word nets and the like. Once these vectors are produced (using machine learning methods) they may be used in vastly enriching the querying expressiveness of virtually any query language."}, {"heading": "3. TOKENIZATION", "text": ""}, {"heading": "3.1 Basic Tokenization", "text": "Distributed language embedding refers to assigning a vector to each word, term, concept, or more generally token, appearing in a sequence where the vectors indicate various aspects of the associated words, enabling computing semantic closeness. Distributed language embedding can expose hidden entity relationships in structured databases. The term text entity is used to refer to some discernible item appearing in a database (or some external source), such as a character, a delimited character sequence, a Boolean value, a number, or a meaningful short sequence such as Theory of Relativity. The text entities are determined by a tokenization process that takes the content of a row field and identifies a sequence of one or more text entities within it. The tokenization process then uniquely associates a token with each distinct entity. Technically, a token is a string with no intervening white space or punctuation and is also referred to as a word. Here are some possible example tokens: TRUE, USA, physical, Theory of Relativity.\nA word vector is a vector representation of a token (word) in a language whose vocabulary is the tokens. The methods for obtaining vector representations range from \u2019brute force\u2019 learning by various types of neural networks, to loglinear classifiers and to various matrix formulations, such as matrix factorization techniques. One example method is a tool called word2vec which produces vectors that capture syntactic as well as semantic properties of words. word2vec scans a corpus (text) to generate vector representations for each word in the text. A word vector is usually of a low\ndimension (200-300) and represents the word (token). All vectors trained on a sequence of tokens have the same dimension. Closeness of vectors is determined by using the cosine or Jacard distance measures between vectors (other measures may apply for specific applications). The vectors can be used to compute the semantic and/or grammatical closeness of words as well as test for analogies (such as a king to a man is like a queen to what? ). Sections 2 and the appendix briefly survey the language embedding field (i.e., vector representation of words).\nTokenizing a database: A database is a collection of tables. Each table is a collection of rows having the same number of columns. Each column is associated with a data type of the usual character, string, Boolean and numeric types, or BLOB (which we do not address in this paper). The value of a row in a particular column conforms with the data type of the column. Vector learning devices operate on (text) documents (for us the token sequence is the document). So, the database needs to be tokenized so as to form a token sequence (the document). The basic tokenization process operates as follows. A character value such as \u2019y\u2019 is represented by the string \u201cy\u201d. A Boolean is represented as \u201cTRUE\u201d or \u201cFALSE\u201d. Numbers are represented in their textual representation, for example, the integer 12 is represented as the token \u201c12\u201d, and the real number 123.001 by the token \u201c123.001\u201d. A Null is represented as \u201cNull\u201d.\nA string value is tokenized as a string containing a sequence of words separated by blanks; these words are extracted by the tokenization process from the input string value. There are a few tokenization options as to how this extraction is performed. For example, viewing a sequence of words such as \u201cDeep Learning\u201d as a single token \u201cDeep Learning\u201d. A row is tokenized as a token sequence made by concatenating the sequences of tokens tokenized from its fields (columns) in order. A table is tokenized as a token sequence made by concatenating the sequences of tokens tokenized from its rows, and the whole database is tokenized by concatenating the token sequences of its tables (again, there are a few options here).\nThe token sequences in Figure 2 (a), and (b) exhibit two fundamental ways in which token sequences may be derived from a database. For example, the tokens corresponding to John Smith\u2019s jobDesc (i.e., \u201cmanager multimedia entertainment\u201d in the sequence of tokens) may instead be derived as \u201cjobDesc manager jobDesc multimedia jobDesc entertainment\u201d, in which case the resulting trained vectors would likely show a stronger relationship between the vector of word \u201cjobDesc\u201d and the word vectors of \u201cmanager\u201d, \u201cmultimedia\u201d, and \u201centertainment\u201d."}, {"heading": "3.2 Extensions", "text": "Encoding Relation Name: There are some extensions to these basic tokenization techniques. In Figure 2 (a) each field tokens are preceded by the field\u2019s name. Similarly, one may precede each row of a relation with a token uniquely representing the associated relation. Yet a third possibility is to precede some token sub-sequences of a field with both relation name and field name.\nSupport for foreign keys: A foreign key in a relation A with respect to a relation B is a set of columns in A whose values uniquely determine a row in B. When a foreign key is present, during token generation for relation A, we can follow the foreign key to a row in relation B. We can then\ntokenize fields of interest in the row of relation B and insert the resulting sequences into the sequence generated for relation A. Figure 3 presents another example of a database table, address, and a resulting token sequence that utilizes a relationship between the empl relation and the address relation; namely the address table provides the addresses for the employees of database table empl. Technically, the resulting token sequence is based on foreign key empNum in emp which provides a value for key attribute id of the address relation. The straight forward way to tokenize with foreign keys is to insert the subsequence generated out of the B row immediately after the one generated for the A row as depicted in Figure 3; another possibility is to intermix the subsequence from the B row within the A row sequence following the tokenization of the foreign keys values of the A row (again, other options may apply).\nUsing foreign keys in generating the set of token sequences provides a user with the ability to make connections between entities of database tables that are not easily evident in the database. For example, if many news articles mention Mamaroneck and Larchmont together then the vectors of the corresponding tokens will be close. If a user constructs a query to locate employees that live close to each other and Alice Morgan lives at 9999 Main Street, Mamaroneck, and Janice Brown lives at 1000 Hutchinson Ave Larchmont, then proximityMAX(9999 Main Street Mamaroneck, 1000 Hutchinson Street Larchmont) is high because the cosine distance(V Mamaroneck, V Larchmont) is high. Given two sets of vectors, the UDF proximityMAX() returns the highest cosine distance between a vector from one set and a vector from the other set.\nSupport for numerical values: A third extension concerns numerical values. In this extension, each number is also preceded by a range designator. Designators are system parameters. They allow detecting closeness of numerical values whose textual representation may not yield close vectors despite their numerical closeness. For example, for positive numbers we may consider ranges such as \u201c1-4, 5-9, 10-49, 50-99, 100-499, 500-999, 1000-4999\u201d,... Similar ranges may be constructed for values between 0 and 1 and for negative numbers. Continuing the example, if a field contains the number 78.5 the corresponding output subsequence of tokens would be \u201c50-99 78.5\u201d. For lack of space, we shall not display designators in examples and figures.\nA fourth extension concerns using external token sequences (text) as well as externally produced vectors. Tokenizing the database concatenated (in various ways) with an external token sequence has the advantage that learning is performed on a larger document with possibly a larger vocabulary. A larger document usually means better learning; a larger vocabulary enhances user querying abilities and enables querying with text entities that do not even appear in the database. One can even contemplate using two (or more) sets of vectors where in each usage (e.g., proximityMAX()), the relevant vector set is identified. Yet another possibility is to train on the token sequence of the database with vectors for tokens that also appear in the external source fixed throughout the vector training process.\nThe above described tokenization methods may be extended to non-relational databases as well. They can be applied to JSON, XML, RDF (and the query language to be extended may be one of Xpath, Xquery, SPARQL, HQL,\n(a) Tokenization without attribute names\nempNum firstName lastName salary dept\n119 John Smith 95\nevaljobDescr\nMultimedia manager\nmultimedia entertainment\ngood people skills not punctual need improvement\n129 Judy White 67 Sports golf section\nemployee\nalways on time\ndiligent\n122 Joe Brown 77 Toy employee techincal viz\nVery Pleasant\n406 Morgan 60 Cosmetics employee\nfragrances\ntakes longer to\nmake a sale Alice\nBrownJanice023 50 Toy entry level\nelectronic games\nelectronic games\nneeds to improve\nfriendliness\nempl Relation\nempNum 119 firstName John lastName Smith\nmanager multimedia entertainment eval good people skills not punctual need improvement empNum 129 firstName Judy lastName White salary 67 dept Sports jobDescr employee golf section eval diligent always on time ............\nsalary 95 dept Multimedia jobDescr\n(b) Tokenization with attribute names\n119 John Smith 95 Multimedia manager multimedia entertainment good people skills not punctual need improvement 129 Judy White 67 Sports employee golf section diligent always on time\nFigure 2: Dual View of a Relational Table\n(Foreign Key)\nempl Relation\nempNum firstName lastName salary dept\n119 John Smith 95\nevaljobDescr\nMultimedia manager\nmultimedia entertainment\ngood people skills not punctual need improvement\naddress Relation\nid stNum street city state remarks\n119 100 10th Newark NJ alternate 19 Chatsworth Ave Larchmont NY\nzip\n07105\n119 John Smith 95 Multimedia manager multimedia entertainment good people skills not punctual need improvement 119 100 10th Newark NJ 07105 alternate 19 Chatsworth Ave Larchmont NY ..............................................................\n(Primary Key)\nFigure 3: Text view of two tables joined using primary and foreign keys\nSpark SQL). Generating token sequences for tree and graph structures is more complex. The basic idea is exploring relevant token sequences such as those implied by paths, by siblings, and combinations thereof. We will not elaborate further on this due to space limitations.\nMainainance of Vectors: As the database changes vectors may need adjustment. One option is periodical retraining that can be applied to a snapshot of the database. Another option is continuous adjustments and handling of brand new tokens. We have explored techniques for continuous adjustments and due to space limitations defer discussion to a future paper."}, {"heading": "4. COGNITIVE INTELLIGENCE QUERIES", "text": "BY EXAMPLES"}, {"heading": "4.1 Similarity Queries", "text": "In this section we present examples from two domains: a business Human Resources (HR) database and a scientific publications database. In both domains we use SQL queries that invoke user defined functions (UDFs). Both UDFs take two arguments and return a real value between -1.0 and 1.0. proximityMax() is a UDF that (a) generates two sets of tokens S1 and S2, S1 from its first and S2 from its second argument, (b) calculates the cosine distance of the vector of each token in S1 with the vector of each token in S2, and (c) returns the maximum such cosine distance (a real number between -1.0 and 1.0). Intuitively, proximityMax() assesses the closeness of two token sets by the maximum closeness of an element (token) from one set and an element (token)\nin the other set using the bag-of-words model [16]. In case either set is empty, -1.0 is returned. proximityAvg() is a UDF that (a) generates two sets of tokens, one from its first and one from its second argument, (b) represents a set of tokens via a single vector which is the average vector of the vectors associated with the tokens in the set, and (c) returns the cosine distance of the two vectors representing the two sets.\nWe note that both proximityMax() and proximityAvg() ignore highly frequent tokens such as : on, up, down, a, the, their, its, if, his, her, and, or, not, of, in, for, using as they provide little context information (but may still be useful in the vector learning process).\nConsider a publication relation with columns number, author, title, year where number is key and the columns have their obvious meaning. At this point we shall focus on querying and delay discussing the tokenization process till the next example. So assume the publication database was already tokenized and each database text entity is associated with a vector (of its token). Suppose we would like to list pairs of authors, author 1 and author 2, and publications that deal with similar topics. We quantify similar by using proximityMax() to determine if there is at least one token derived from a title of an author 1 article that is very close to a token derived from a title of an author 2 article. Here very close is quantified as cosine distance greater than 0.3. The following query operates over Spark SQL.\nThe example query above returns up to 10 results, where each result will list number1, author1, title1, number2, author2, title2, proximityMax() value such that title1\nand title2 are concerned with at least one very similar topic. The statement int(X.number) < int(Y.number) prevents a row from being compared to itself and from a pair of qualifying authors to appear twice in the result due to the same two articles.\nLet us examine how the statement proximityMax(X.title, Y.title) > 0.3 is processed. Suppose X.title= Linear Approximation of Image Similarity and Y.title = Examining Algebraic Identification Methods. Then, a standard tokenization will identify the tokens Linear, Approximation, of, Image, Similarity in X.title and Examining, Algebraic, Identification, Methods in Y.title. Next, two sets of vectors will be identified V1 = { VLinear, VApproximation, VImage, VSimilarity}, and V2= { VExamining, VAlgebraic, VIdentification}. There is no Vof as of is excluded due to its high frequency of occurrence. Next, the cosine distance is calculated between each member of V1 and each member of V2. If any of these calculations is higher than 0.3, tuples X and Y qualify (provided X.number < Y.number).\nOne may argue that proximityMax() is not appropriate as it does not take the whole title into account. As an alternative, one can use another UDF, proximityAvg():\nThe example query above (also in Spark SQL) returns up to 15 results, here each result lists number1, author1, title1, number2, author2, title2, proximityAvg() value such that title1 and title2 generally deal with similar topics. Note that a lower bound of 0.2 is used instead of 0.3 in the previous query as the average correlation will tend to be lower in this scenario. Note also that excluding commonly occurring terms will enhance accuracy. In general, the exact set of terms to be excluded could also be a parameter. Also, splitting a string on spaces (blanks) may be changed to splitting of other forms, e.g. on tabs, commas.\nOne can define additional UDFs, for example a variation on proximityMax(), proximityTop2Avg(), that returns the average of the top 2 cosine distances rather than the maximum, UDFs that use a different distance measure than cosine distance, UDFs that return minimum rather than maximum cosine distances and so forth.\nWe next continue with an examples from the HR domain. Figure 2 displays five records of employee information from table emp; emp has the following seven columns (fields): empNum, firstName, lastName, salary, dept, jobDesc, and eval (containing free text employee evaluation); the\ncolumn names are self-explanatory. Figure 2 (a) and (b) illustrate two simple examples of resulting tokenizations of these database tables. The idea is to convert each row to text and concatenate these texts. A row is converted one column at a time. The exhibited token sequences (a) and (b) each present a possible tokenization of the five records of employee information stored in the database emp table. The difference between (a) and (b) in the figure is that column (field) names also appear as text just ahead of their content in (b).\nIn Figure 1 the resulting sequence of tokens, (a) or (b), in this example, is presented to a machine learning device (such as word2vec) to obtain a vector for each token in the sequence. Optionally, other information may be fed into the machine learning device, for example a public corpus (text) such as Wikipedia. The machine learning device then outputs vectors representing the entities in the database table. The database entity John in the example shown in Figure 2 is assigned a vector of 200 dimensions, denoted by the vector V John(0, 199). However, in other situations a different dimension value may be used (same dimension for all vectors).\nTypically, if a user would like to gather information, say financial, regarding an employee, then the user would have to be familiar with the attribute names of the database tables. For example, if a user wanted to gather information about employees salaries, then the user need construct a SQL query which specifically states the attribute name salary. However, by incorporating vectors a user gains the ability to construct queries with terms that are not even in the database when the training is based on a rich external source (optionally with the database), such as Wikipedia. This will create the connection between terms mentioned in the database and ones that are not.\nFor example, suppose we produce a token sequence as in Figure 2 (b), i.e., including column names. The sequence of tokens would be empNum, 119, firstName, John, lastName, Smith, salary, 95, dept, Multimedia, jobDesc, manager, multimedia, entertainment, eval, good, people, skills, punctual, need, improvement, . . . . Further, suppose that other relations include columns such as bonus, fine, sales and the like that mention Smith and that are used in training. Then, the user may construct a query regarding Smith\u2019s financial records, in a history relation, by simply stating the query in Figure 6.\nAlthough the token money may not even be mentioned at all in the history relation, any tuple in the history relation that mentions such text entities as Smith, salary, sale, bonus, fine, IRA will likely be retrieved because of the associated closeness of the vectors of these text entities and the vector for money as will likely be established by the training process. This type of functionality is not currently available to relational database users.\nA typical use of Similarly queries is shown in the following\nexample in which one inquires for job evaluations of employees that are similar to that of Smith:\nTo understand the exploitation of foreign keys, consider the following example. Suppose we have relation sales that describes sales, and has, among other columns, the columns authorizedBy and followupBy. In both these columns there are employee numbers of the relevant personnel, moreover these employee numbers are foreign keys into the empl relation whose primary key is empNum. A tuple in sales that does not even mention Smith may well be retrieved by a query that inquires about Smith if this tuple is connected to Smith via a foreign key (or keys). This is due to the fact that when the token sequence is produced using foreign keys (as discussed previously in further detail with respect to Figure 3), the token Smith will be associated with tokens in tuples that are connected to the tuple containing Smith via one or more hops through relationships implied by following foreign keys. So, a user may query the sales relation for sales made by, or related to, Smith even though Smith may not explicitly be mentioned in the sales relation (Figure 8).\nThe UDF proximityAvg() can be biased due to long token sequences which lead to mixing many vectors and the UDF proximityMax() can be biased due to having one strong interaction and other very weak ones. Another UDF, subsetProximityAvg(size, sequence1, sequence2) can circumvent these problems. This UDF takes an additional integer parameter, size. Intuitively, it considers all sets of tokens out of sequence1 of cardinality size and compute an average vector for each such subset. Similarly, it does the same for subsets of tokens of cardinality size taken out of sequence2. It then computes the cosine distances between each average vector associated with sequence1 and each average vector associated with sequence2, and returns the maximum value. So, intuitively, it returns \u2019maximum of cosine distances between average vectors of subsets\u2019. This places the subsetProximityAvg() in between proximityMax() and proximityAvg(). subsetProximityAvg() is a generalization of the proximityTop2Avg()."}, {"heading": "4.2 Schema-less Navigation", "text": "We now consider extensions to SQL that take advantage of the existence of vectors associated with database entities. The first extension is an ability to focus on individual entities within a field of a row. We add entities as first class citizens by adding a phrase Entity e in the FROM part of a\nSQL query. We then extend SQL with the constructs (a) contains(column, entity), (b) contains(row, entity), (c) contains(database, entity),(d) cosineDistance( entity1, entity2)RELOP c, where entity, entity1,and entity2 are token denoting variables or token constants and c denotes a constant \u22121.0 \u2264 c \u2264 1.0, RELOP is one of {\u2264,\u2265, >,<,=}.\nThe declaration Token entity e declares e to be a variable that can be bound to tokens. The statement contains( row, entity) states that entity must be bound to a token generated by tokenizing row. If entity is specified, e.g., contains(r, \"Physics\") then the result is TRUE for row r whose tokenization includes the token Physics and FALSE otherwise. The argument column is specified by a relation name or alias followed by the column name as in contains(e.jobTitle, \"manager\") where e is an alias for the emp relation. To indicate a whole row we use the star notation as in contains(e.*, \"manager\"). In fact, the function contains() returns the number of occurrences where a number greater than zero is interpreted as TRUE. One can use the returned value in expressions such as contains(e.*, \"expert\") > 1.\nThe ability to specify entities provides for a finer query specification as compared to the UDFs we saw thus far. Relations may be navigated using entities in a similar way as database keys. For example, the following query (Figure 9) checks for the existence of two strongly related (cosine distance > 0.5) entities in the address field of an employee and a row (tuple) in the DEPT table (relation).\nTo enhance querying ability, we can proceed one step further and introduce relation variables whose names are not specified at query writing time; they are identified at runtime via entities. Introducing relation variables is done as shown in the following example.\nIn this example query (Figure 10), field (attribute) values are retrieved from EMP and the (unknown) relation S and column(s) X such that there are entities e1 and e2 that strongly link them. The related relation and column names and the relevant values may also be retrieved. Intuitively, we retrieve names and salaries from EMP and tuple columns X out of some other relation S such that (the tokenization of) EMP.Address contains a token e1, (the tokenization of) S\u2019s column X contains a token e2, and e1 and e2 are close (cosine distance > 0.5) and has more than 1 occurrence of\ntoken e2. In general, there may be more than one column in a qualifying relation that qualifies and there may be more than one qualifying relation, and as such, the construct S.X should indicate the relation and column names. For example, a result tuple may look like: (John Smith, 112000, Dept.Mgr:Judy Smith).\nThe closeness between entities may also be expressed qualitatively on a scale, for example very strong, strong, moderate, weak, very weak, e.g. strong(e1,e2). which enables defining the numeric values separately, e.g. very strong = 0.95. For example, understanding the relationship between the two text entities John and New York includes deriving the tokens for the entities, in this case simply John and New York, locating their associated vectors, say VJohn and VNew Y ork, and performing an algebraic operation on the two vectors, for example, calculating the cosine distance. If the cosine distance between these vectors is high (i.e., closer to 1) then the two text entities are closely related; if the cosines distance is low (i.e. closer to -1) the two entities are not closely related.\nWe note that the notation S.X is basically syntactic sugar. A software translation tool can substitute for S.X actual table name and column. Then, perform the query for each such substitution and return the union of the results."}, {"heading": "4.3 Analogy Queries", "text": "A unique feature of vectors is the fact that we can use them to deduce analogies; e.g., a king to man is like a queen to woman. The following query (Figure 11) identifies pairs of products that relate to each other like tokens peanut butter and jelly (both are product types). The key is that if product p1 relates to product p2 like peanut butter to jelly, their associated vectors V p1, V p2, V peanut butter and V jelly are likely to satisfy that the vector differences (V peanut butter - V jelly) and (V p1 - V p2) are cosine-similar. Potential answers to this query can include (chips, salsa) or (pancake, maple syrup). Here, we use a UDF vec() that takes a token and returns its associated vector.\nIn the analogy query we assume that column type of the relation Product contains a single word, in case it may contain more than one word we can obtain the tokens within them using an UDF contains(). We note that, assuming all vectors are of length 1, instead of the traditional cosineDistance(vec(PX.type), vec(P1.type) - vec(\"peanut butter\") + vec(\"jelly\"))), we can also use the 3COSMUL formulation of [11], namely: (cosineDistance(vec(PX.type),vec(P1.type) *\ncosineDistance(vec(PX.type),vec(\"jelly\"))) / (cosineDistance(vec(PX.type),vec(\"peanut butter\")) + 0.001)."}, {"heading": "5. CI QUERIES IN PRACTICE", "text": "In this section, we discuss how a Cognitive Intelligence (CI) database is used in practice, and then demonstrate its capabilities by describing execution of CI queries in a Sparkbased prototype.\nFigure 12 presents the three key phases in the execution flow of a CI database. The first, optional, training phase takes place when the database is used to train the model. Using the approaches described in Section 3, the database table data is first tokenized into a meaningful text format and then used to generate a set of vectors (phase 1). Alternatively, this phase can also use an external text repository (e.g., Wikipedia) to train the vectors. Following vector training, the resultant vectors are stored in a relational system table (phase 2). At runtime, the SQL query execution engine uses various UDFs that fetch the trained vectors from the system table as needed and answer CI queries (phase 3).\nThe training phase is optional, and can be executed in background as a batch process. If vectors are learned using the database data, as the database evolves, the vectors need be updated. This can be done by training anew every so often. Alternatively, or in conjunction, the vectors may be maintained over time. The critical issue is the introduction of new tokens (that do not have associated vectors). One way of handling this is to first associate these with an average vector whose entries are very small (positive and negative) values. Then, initiate training for a short duration (in the case of word2vec) where the older vectors are not allowed to change (or, their changes are multiplied by a very small positive fraction) and changes to the new vectors are amplified. The net effect is that the new vectors are assigned reasonable vector values. A detailed discussion of this and similar techniques is deferred to a subsequent publication."}, {"heading": "5.1 CI Queries over a DBLP-based Database", "text": "We have implemented an initial prototype of a CI database using the Apache Spark infrastructure. The prototype uses UDFs programmed in Python. The main UDFs are cosineDistance(), proximityMax() and proximityAvg(). All three UDFs rely on functions for computing the dot product and length of vectors. For our evaluation, we used the publicly available DBLP bibliography dataset. From the source XML file, we extracted bibliographic information on some of the papers that appeared in SIGMOD and VLDB conferences and generated a CSV file in which every row corresponds to a separate entry for every author (i.e., a paper\nwith multiple authors gets split into multiple, almost duplicate, rows). The CSV file has the following fields: PaperID (integer), Author (string), Title (char array), and Conference (string). To ease the training process, we converted author names to single words (e.g., Jim Gray to Jim Gray), and also appended the year to a conference name (e.g., SIGMOD 2002 to SIGMOD 2002). This CSV file was then used to populate a relational table in our Spark environment.\nFor training the vectors, the CSV file was first processed to remove the commas and a text file containing tokens was created. This text file was then used in a standard word2Vec application to generate 200 dimension vectors. The vectors were then loaded into an in-memory Python structure, containing the relevant words (tokens), and with each word its associated vector as an array. Once the vectors were loaded, the system was ready to perform SQL queries (we omit the details of starting a Spark SQL environment). For our experiments, we used a subset of the CSV data to populate the relational tables. Due to space limitations, in this paper we focus only on the similarity queries.\nBefore discussing CI queries, let us first get a taste of the vector-implied relationships among the tokens. Table 1 presents of a list of the top 10 tokens that the word2Vec produced vectors classify as being semantically closest to the input token Concurrency. The semantic similarity is measured using the cosine distance between the 200-dimension vectors of the input token, and the tokens from the input text data. The tokens listed in Table 1 are close as they co-occur with the input token (Concurrency) in the token sequence used for vector training (e.g., in a paper title authored by that person). Thus, these tokens contribute to the overall meaning of the token Concurrency.\nNext, we demonstrate the use of vector-based similarities for answering novel CI queries. The first query (Figure 13) aims to find authors based on an input topic (e.g., XML) and return the corresponding papers and conferences. The query uses the cosineDistance() UDF to compare the vectors of all author tokens from the database against the vector of\nthe XML token. Whereas we demonstrate the query with an explicit constant (XML), clearly it can be passed to the query through a program variable. The top 5 nearest authors are then selected and their papers, with corresponding conferences, are returned. Table 2(a) presents the results of the query. It is important to note that the token XML is compared against the author tokens. As none of the authors had XML in their names, the matching is purely semantic, based on the cosine distance between corresponding vectors.\nThe second query (Figure 14) performs a similar task: it finds out authors that are semantically close to an input author (e.g., Surajit Chaudhuri). In this case, the vector of the token Surajit Chaudhuri is compared against the vectors of the authors using the cosineDistance() UDF and the top 5 closest authors are returned, along with their titles and conferences. Table 2(b) presents the results of the query.\nThe final query (Figure 15) finds papers with related titles using the proximityMax(), proximityAvg() or proximityTop2Avg(). In this query, the title of the paper with number 471 is \u201cNative Xquery processing in oracle XMLDB\u201d. In this query, vectors for different tokens in a title are compared in three ways to identify similar titles: proximityAvg() selects the title whose average vectors are close, whereas proximityMax() uses maximum closeness between individual tokens to select related titles. Finally, proximityTop2Avg() uses the average of the top two cosine distances of its two token sequences arguments, thereby \u2019mixing\u2019 maximum and average. As Table 3 illustrates these three UDFs give different sets of answers: proximityMax() chooses two papers with Xquery in their titles as this is the closest match (1.0), proximityAvg() returns a broader set of papers (note that there is no XML or relational token in the query title.) proximityTop2Avg() uses two top tokens to determine similarity, thus produces a different ordering. For example, the the closest title chosen by proximityTop2Avg() containts both XQuery and XML tokens.\nThe choice of the proximity function (e.g., proximityAvg(), proximityMax(), or proximityTop2Avg()) as well as the\nlower bound on the cosine-distance (e.g., 0.3 or 0.2) is dependent on the application domain and needs to be fine-tuned based on the workload characteristics."}, {"heading": "5.2 Performance Issues", "text": "In an end-to-end CI system, there are three spots where performance can become a concern. First, in the training phase, if the vectors have to be trained from the raw database data, depending on the size of the text used, the vector training time can be very large. However, this training phase is not necessary as the system can use pre-trained vectors, and if the vectors need to be trained, the training (or retraining for new data), can be implemented as a batch process. Performance of training can be further improved by using GPUs. Second, the cost of acceessing the trained vectors from the system tables has to be as minimal as possible as any delay would impact the performance of runtime query execution (Figure 12). Access to the system tables can be improved by building a traditional B+-tree index, with the tokens as keys. Finally, the execution cost of a CI query is dependent on the performance of the distance function. In many cases, we may need to compute distances among a large number of vectors (e.g., for analogy queries). The distance calculations can be accelerated either using CPU\u2019s SIMD capabilities or using accelerators such as GPUs. This is a focus of our current work and we will provide details in subsequent publications."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "We describe the enhanced querying power resulting from having each database text entity associated with a vector that captures its syntactic as well as its semantic characteristics. Vectors are obtained by a learning device operating on the database-derived text. We use these vectors to enhance database querying capabilities. In essence, these vectors provide another way to look at the database, almost orthogonal to the structured relational regime, as vectors enable a dual view of the data: relational and meaningful text. We thereby introduce and explore a new class of queries called cognitive intelligence (CI) queries that extract information from the database based, in part, on the relationships encoded by these vectors.\nWe outline a tokenization process and extensions thereof. These extensions increase the learning precision. We show how SQL user defined functions (UDFs) can take advantage of these vectors in querying. In particular, we define some very basic UDFs such as proximityMAX(), proximityAVG(), and proximityTop2Avg(). SQL queries using these UDFs implement CI queries. However, we can make CI queries more expressive by extending standard SQL. These features enable query navigation with little knowledge of the database schema. In large installations, with hundreds of relations, this is a very useful extension.\nWe implemented a prototype system (on top of Apache Spark [1]) to exhibit the power of CI queries. This power goes far beyond text extensions to relational systems due to the information encoded in vectors. We also considered various important aspects beyond the basic scheme, including using a collection of views derived from the database to focus on a domain of interest, utilizing vectors and text from external sources, and maintaining vectors as the database evolves.\nWe are currently extending our infrastructure to support more complex query patterns. We are also working on accelerating vector training, vector distance calculations using GPUs, and developing new techniques for incremental vector training. Although we used an academic scenario (DBLP) to demonstrate our ideas, we believe CI queries are applicable to a broad class of application domains including healthcare, bio-informatics, document searching, retail analysis, and data integration. We are currently working on applying the CI capabilities to some of these domains. One important direction of research is establishing agreed-upon benchmarks for evaluating and ranking CI systems. Such benchmarks exist in NLP and in some database areas such as transaction processing. This is a non-trivial task as such benchmarks need be of a significant size on the one hand and may require manual construction on the other hand. One possible direction here is to use crowdsourcing for building the benchmarks."}, {"heading": "7. REFERENCES", "text": "[1] Apache Foundation. Apache spark: A fast and general\nengine for large-scale data processing. http://spark.apache.org.\n[2] S. Arora, Y. Li, Y. Liang, T. Ma, and A. Risteski. Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings. CoRR, abs/1502.03520, 2015.\n[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137\u20131155, 2003.\n[4] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive operator for similarity joins in data cleaning. In Proceedings of the 22nd International Conference on Data Engineering, Washington, DC, USA, 2006. IEEE Computer Society.\n[5] R. Cutlip and J. Medicke. Integrated Solutions with DB2. IBM Press, 2003.\n[6] A. M. Dai, C. Olah, and Q. V. Le. Document embedding with paragraph vectors. CoRR, abs/1507.07998, 2015.\n[7] Y. Goldberg and O. Levy. word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method. CoRR, abs/1402.3722, 2014.\n[8] J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Venkatrao, F. Pellow, and H. Pirahesh. Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals. Data Mining and Knowledge Discovery, 1(1):29\u201353, 1997.\n[9] V. Kashyap and A. P. Sheth. Semantic and schematic similarities between database objects: A context-based approach. VLDB J., 5(4):276\u2013304, 1996.\n[10] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. CoRR, abs/1405.4053, 2014.\n[11] O. Levy and Y. Goldberg. Linguistic regularities in sparse and explicit word representations. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, CoNLL 2014, pages 171\u2013180, 2014.\n[12] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Annual Conference on\nNeural Information Processing Systems 2014, pages 2177\u20132185, 2014.\n[13] L. Lim, H. Wang, and M. Wang. Semantic queries by example. In Proceedings of the 16th International Conference on Extending Database Technology (EDBT 2013), 2013.\n[14] F. Liu, C. T. Yu, W. Meng, and A. Chowdhury. Effective keyword search in relational databases. In Proceedings of the ACM SIGMOD International Conference on Management of Data, Chicago, Illinois, USA, June 27-29, 2006, pages 563\u2013574, 2006.\n[15] Y. Luo, X. Lin, W. Wang, and X. Zhou. Spark: top-k keyword query in relational databases. In Proceedings of the ACM SIGMOD International Conference on Management of Data, Beijing, China, June 12-14, 2007, pages 115\u2013126, 2007.\n[16] C. D. Manning, P. Raghavan, and H. Schutze. Introduction to Information Retrieval. Cambridge University Press, 2008.\n[17] T. Mikolov. word2vec: Tool for computing continuous distributed representations of words. https://code.google.com/p/word2vec.\n[18] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.\n[19] T. Mikolov, Q. V. Le, and I. Sutskever. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168, 2013.\n[20] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In 27th Annual Conference on Neural Information Processing Systems 2013., pages 3111\u20133119, 2013.\n[21] A. Mnih and K. Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In 27th Annual Conference on Neural Information Processing Systems 2013., pages 2265\u20132273, 2013.\n[22] Z. Pan and J. Heflin. DLDB: extending relational databases to support semantic web queries. In PSSS1 - Practical and Scalable Semantic Systems, 2003.\n[23] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543, 2014.\n[24] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: online learning of social representations. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, pages 701\u2013710, 2014.\n[25] X. Rong. word2vec parameter learning explained. CoRR, abs/1411.2738, 2014.\n[26] J. Shin, S. Wu, F. Wang, C. De Sa, C. Zhang, and C. Re\u0301. Incremental knowledge base construction using deepdive. Proc. VLDB Endow., 8(11), July 2015.\n[27] R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. D. Manning, and A. Y. Ng. Zero-shot learning through cross-modal transfer. CoRR, abs/1301.3666, 2013.\n[28] P. Verga, D. Belanger, E. Strubell, B. Roth, and A. McCallum. Multilingual relation extraction using compositional universal schema. CoRR,\nabs/1511.06396, 2015.\n[29] L. Vilnis and A. McCallum. Word representations via gaussian embedding. CoRR, abs/1412.6623, 2014.\n[30] W. Viyanon and S. K. Madria. A system for detecting xml similarity in content and structure using relational database. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM, pages 1197\u20131206, 2009.\n[31] B. Yu, G. Li, K. R. Sollins, and A. K. H. Tung. Effective keyword-based selection of relational databases. In Proceedings of the ACM SIGMOD International Conference on Management of Data, Beijing, China, June 12-14, 2007, pages 139\u2013150, 2007."}, {"heading": "8. APPENDIX", "text": "word2vec is a software tool for associating vocabulary words with vectors. The tool comes in two flavors, continuous bag-of-words (CBOW) and continuous skip-gram (SG). Intuitively, in CBOW, each word w is represented by two vectors Vw and V \u2032 w. The algorithm scans a window of text of a pre-determined size d, and attempts, by computing a gradient, to move the Vw vectors of the words w in the window (except the center word) towards the V \u2032c vector of the window\u2019s center word c. SG has a similar but a bit more costly mechanism. word2vec can employ a technique called negative sampling (NS) in which examples having (with high probability, by picking randomly) no connection to the text window words are presented to the learning device as negative examples. Intuitively, the window words vectors are to move away from the primed vector of a negative example. The inner workings of word2vec are explained in great detail in Xin Rong\u2019s paper [25]. It turns out that word vectors carry semantic information about various roles a word plays. For example, a king has roles as a human, male, ruler, monarch, dictator and others. There are a few explanations [7, 2] for this phenomenon od role capturing.\nThere are other vector construction methods, most notably GloVe [23] and the method of Arora et. al [23, 2]. Vector construction methods have a few characteristics in common:\n\u2022 The context is usually Natural Language Processing (NLP). One exception is [24] that uses word2vec to produce synthetic features for graph nodes to be later on used by classifiers. \u2022 In scanning methods, learning takes place by presenting examples from a very large corpus of Natural Language (NL) sentences. The examples may be presented to a neural network or a learning device such as word2vec. Examples are usually based on a sliding window through the text. \u2022 In matrix methods (e.g., [12] and GloVe), text characteristics are extracted into a matrix form and an optimization method is utilized to minimize a function expressing the word-oriented desired word vector representation. \u2022 One can manipulate these vectors in order to answer analogy questions, cluster similar words based on a similarity measure such as cosine distance by using a clustering algorithm, or use these vectors in other analytical models such as a classification/regression model for making various predictions."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We apply distributed language embedding methods from<lb>Natural Language Processing to assign a vector to each<lb>database entity associated token (for example, a token may<lb>be a word occurring in a table row, or the name of a col-<lb>umn). These vectors, of typical dimension 200, capture the<lb>meaning of tokens based on the contexts in which the to-<lb>kens appear together. To form vectors, we apply a learning<lb>method to a token sequence derived from the database. We<lb>describe various techniques for extracting token sequences<lb>from a database. The techniques differ in complexity, in<lb>the token sequences they output and in the database infor-<lb>mation used (e.g., foreign keys). The vectors can be used<lb>to algebraically quantify semantic relationships between the<lb>tokens such as similarities and analogies.<lb>Vectors enable a dual view of the data: relational and<lb>(meaningful rather than purely syntactical) text. We in-<lb>troduce and explore a new class of queries called cognitive<lb>intelligence (CI) queries that extract information from the<lb>database based, in part, on the relationships encoded by<lb>vectors. We have implemented a prototype system on top<lb>of Spark [1] to exhibit the power of CI queries. Here, CI<lb>queries are realized via SQL UDFs. This power goes far<lb>beyond text extensions to relational systems due to the in-<lb>formation encoded in vectors.<lb>We also consider various extensions to the basic scheme,<lb>including using a collection of views derived from the database<lb>to focus on a domain of interest, utilizing vectors and/or text<lb>from external sources, maintaining vectors as the database<lb>evolves and exploring a database without utilizing its schema.<lb>For the latter, we consider minimal extensions to SQL to<lb>vastly improve query expressiveness.", "creator": "LaTeX with hyperref package"}}}