{"id": "1603.08016", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Classifying Syntactic Regularities for Hundreds of Languages", "abstract": "This paper presents a comparison of classification methods for linguistic typology with the aim of extending an extensive but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and neighboring methods for classification across a set of 325 languages and six syntactic rules from WALS. To classify each rule, we consider the typological characteristics of the other five rules; linguistic characteristics extracted from a word-altered Bible in each language; and genealogical characteristics (genus and family) of each language. In general, we find that the distribution of the majority label among all languages of the same genre achieves the best accuracy in label prediction, resulting in a logistic regression model that combines typological and linguistic characteristics. Interestingly, this model actually outperforms the majority label among all languages of the same family.", "histories": [["v1", "Fri, 25 Mar 2016 20:09:29 GMT  (58kb,D)", "https://arxiv.org/abs/1603.08016v1", null], ["v2", "Wed, 27 Apr 2016 18:40:55 GMT  (58kb,D)", "http://arxiv.org/abs/1603.08016v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["reed coke", "ben king", "dragomir radev"], "accepted": false, "id": "1603.08016"}, "pdf": {"name": "1603.08016.pdf", "metadata": {"source": "CRF", "title": "Classifying Syntactic Regularities for Hundreds of Languages", "authors": ["Reed Coke", "Ben King", "Dragomir Radev"], "emails": ["reedcoke@umich.edu", "benking@ubiquiti.com", "radev@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "Linguistic typology is a subfield of linguistics concerned with understanding the various patterns that are present across the world\u2019s languages and how languages can be grouped via these patterns as well as other historic and geographic factors. Other fields of linguistics, such as historical linguistics and the study of endangered languages, depend heavily on knowledge drawn from typological comparisons. Linguistic typology is also useful in many areas of natural language processing.\nWALS is an expansive linguistic resource useful in a variety of different NLP applications. WALS has data for over 2,500 languages regarding almost 200 rules compiled by a group of 55 authors spanning phonology, syntax, and lexicology. Examples of such rules and their possible values are given in section 3.2. WALS has been used as a lens through which to view worldwide typological relations (Littauer et al., 2012) as well as a tool with which to discover which linguistic rules are rare across the world\u2019s languages (Cysouw, 2011). Unsurprisingly, it has also served as a baseline for typological similarity measurements (Berzak et al., 2014). In this work the authors also show that the accuracy of the baseline increases with the number of WALS rules considered, but note the sparseness of these rules. Indeed, the sparseness of WALS has been noted by several researchers (Georgi et al., 2010; Cysouw and Comrie, 2008; Teh et al., 2008). In fact, currently the average rule in WALS has data for only 400 languages. This represents a matrix that is only 16% populated. To this end, we examine methods for automatically identifying rule values in the WALS database for the purpose of reducing sparseness in WALS. Our methods use the WALS database as of March 2015 as well as an additional copy of the Bible in each target language downloaded from bible.com or bible.is and word-aligned to an English version.\nDespite its sparseness, WALS is still an excellent resource. Consider a similar resource, Syntactic Structures of the World\u2019s Languages (SSWL) (Collins and Kayne, 2011). SSWL is considerably less sparse, with 56% of all possible data for 112\nar X\niv :1\n60 3.\n08 01\n6v 2\n[ cs\n.C L\n] 2\n7 A\npr 2\nrules and 251 languages. In reality, many of these rules actually represent pieces of a single rule in WALS - SSWL rule 3 is Verb Object while SSWL rule 4 is Object Verb. In WALS, both of these rules are possible values for rule 83A: Order of Object and Verb. Thus, SSWL lacks the breadth that makes WALS such an appealing resource. WALS has an order of magnitude more languages and many more rules. In addition, WALS has genealogical information about each language that SSWL lacks.\nWhile many of the rules in WALS, such as rule 83A: Order of Object and Verb, may seem simple, properties of linguistic universals can be exploited to extrapolate further information. Joseph Greenberg proposed a set of linguistic universals (Greenberg, 1963) that refer to statistical tendencies across the world\u2019s languages. Further universals have been described since, particularly with respect to word order (Hawkins, 1983). While our system does not directly make use of these universals, the core mechanic of classifying rules based on the labels of other rules for the same language is central to our experimentation. This is the grounding theory behind the use of typological features.\nApplying effective, automated methods for expanding this database, or similar databases, will not only create a better resource, but will also lead to improvements in multilingual NLP via better parsing, a better understanding of linguistic typology, and many other basic NLP tasks such as machine translation (Mikolov et al., 2013) and part-of-speech tagging (Das and Petrov, 2011). In addition, having a more expansive resource will allow for NLP to be leveraged in many under-resourced languages not only in Europe, but around the world.\nThe rest of this paper is structured as follows: Section 2 discusses other work in the field of NLP that has made of use of features in WALS; Section 3 describes in detail how we generated our feature vectors and present the classification models we compared; Section 4 presents our results and provides an analysis; Section 5 concludes our work; and Section 6 suggests future work."}, {"heading": "2 Related Work", "text": "Typological similarity has previously been shown to correlate with genealogical similarity both in the\nfields of NLP (Rama and Kolachina, 2012) and historical linguistics (Dunn et al., 2005). Due to this, WALS has been used to study linguistic typology via computational methods. In order to determine language similarity via genealogical relatedness, it is important to know which rules are more telling of historical relatedness. To do this, (Wichmann and Kamholz, 2008) measured the variance of linguistic rules within language families, at the genus level, to determine their stability. Rules that change less often within language families, then, are more indicative of historical relationships when they are shared between languages. To do this, the authors extracted data for language families from WALS and calculated the probability of features being shared within and across language families. Feature variation was shown to be significantly different across genera. This indicates that language families are more similar internally than across genera, which in turn supports the concept of using typological similarity to predict language similarity.\nThese syntactic regularities are not only predictable within language genera, however. (Daume\u0301 and Campbell, 2007) used WALS as a database to discover implicative associations, similar to Greenberg\u2019s universals. Not only did they recover many of Greenberg\u2019s universals, but they also uncovered a host of other implicatures.\nIn similar work, (Cysouw and Comrie, 2008) sought to determine how consistent certain linguistic rules are in an effort to determine languages that are \u201cmore central for the structure of human language.\u201d The authors cite \u201cwidely varying frequencies of available data\u201d as a difficulty in their study, but conclude that many word order features seem to be central rules in language. Among these are rules 83A, 85A, 86A, 88A, and 107A, five of the six rules considered in this study.\nApart from typology, the data from WALS have also been used in natural language processing to advance several core NLP tasks. (Naseem et al., 2012) used language similarity information to improve multilingual parsing by defined a distance metric over WALS rules. The authors considered six WALS rules to create their similarity metric, three of which (85A, 86A, and 88A), are also used in this study.\n(Bender et al., 2013) attempted a similar task, us-\ning interlinear glossed text (IGT) to predict typological features such as major consituent order and case system. (Lewis and Xia, 2008) also predict typological features from IGT by first learning a context free grammar for a language and then examining its structure. Both studies use data from WALS to evaluate their performance. We address a similar task, but we utilize projected dependency parses from English text as a source of knowledge for each foreign language, as many do not have extensive resources of their own. This is similar to IGT, but does not contain morphological information to the same extent. This method of projecting dependencies has been used to bootstrap linguistic resources in the past (Xia, 2007), (Hwa et al., 2005)."}, {"heading": "3 Experiments", "text": "We first created a corpus of Bibles word-aligned to an English Bible. The 325 languages represented in the corpus are very diverse, ranging from French, German, and Modern Standard Arabic to Acholi, Basque, and Tamil. Note that translated Bibles exist for many other languages that were not included in this study. In total, we use over 2 million aligned sentences. The rules were selected based on the frequency of their use in previous NLP research, a measure of usefulness. More information about them is provided in Section 3.2. For each rule we ran experiments using a variety of classifiers and feature vector combinations. All results are given in Section 4."}, {"heading": "3.1 General Textual Feature Extraction", "text": "For each rule described in Section 3.2, we extract English dependencies according to certain criteria unique to that rule. We then project these dependencies onto the foreign language biblical sentences using word alignments determined by BerkeleyAligner (DeNero and Klein, 2007; Liang et al., 2006). Using these inferred dependencies, we calculate the feature vectors for the rule in question by the policy described in Section 3.2.\nTo give a simple example of projecting word alignments, consider the English sentence and its Ma\u2019di translation in Figure 1. is and resurrection are the verb and object of the English sentence, while i and onzika are the Ma\u2019di alignments. These alignments are shown using blue edges. As these words\nare aligned, we assume they serve the same grammatical function. We can see that the English sentence shows a verb-object ordering, while the Ma\u2019di sentence demonstrates an object-verb ordering. This would then add one to the count of object-verb orderings to the linguistic feature vector of Ma\u2019di for rule 83A.\nNext, we give a detailed example of how the features for WALS rule 83A Order of Object and Verb can be drawn from projected dependency parses of English text as in the German example in Figures 2 and 3.\nIn this example, German would receive one count for having the same verb-object ordering in any phrase due to the (Dothan, found) dependency and one count for different ordering due to the (brothers, after) dependency. This sentence is not a question or a dependent clause, so the corresponding same/different counts would not be changed for those features. This process is then repeated for all aligned sentences in all languages with a known label for the rule."}, {"heading": "3.2 Specific Feature Extraction", "text": "Each rule is described in detail below, along with definitions of its possible classes, rounded distributional information over the languages considered in this paper, and a brief motivation for why we chose to classify this rule. All parses referred to in this section were obtained using a Hash Kernel parser (Bohnet, 2010) and represented in the CONLL dependency parsing format. We restrict our system to instances where the English sentence aligns to only one foreign language sentence. The count of languages given for each rule is the size of the set of languages whose label for this rule is given in WALS and for whom we have a Bible."}, {"heading": "3.2.1 Rule 83A: Order of Object and Verb", "text": "Rule 83A defines the order in which the direct object and verb of a sentence occur. Its label is known for 299 languages. The three possible classes are Object Verb (The man saw the dog. 38%), Verb Object (The man the dog saw. 58%), and No Dominant Order (both valid in certain conditions. 4%). Besides being fundamental to any sort of parsing or chunking task, many other typological rules have been found to be statistically related to the order of subject, object, and verb (Greenberg, 1963). As it has also been found that subject generally precedes object (Greenberg, 1963), knowing the order of the object and verb allows not only accurate guesses about the order of subject, object, and verb, but also about many other correlated rules. To identify all occurrences, we examine all dependencies with the label dobj. The instance is kept as long as both of the English and foreign object words align only to each other and both of the English and foreign verb words align only to each other. The resulting feature vector has six columns: the count of all sentences in which the verb-object ordering is identical between English and the foreign language; the count of all sentence in which the ordering is reversed; the count\nof all questions in which the ordering is identical; the count of all questions in which it is reversed; and finally the count of dependent clauses where the order is identical and the count of dependent clauses in which the order is reversed."}, {"heading": "3.2.2 Rule 85A: Order of Adposition and Noun Phrase", "text": "Rule 85A defines the order in which adpositions occur relative to their governing noun phrase. Its label is known for 256 languages. The five possible classes are postpositions (The dog went the park to. 43%), prepositions (The dog went to the park. 49%), inpositions (The dog went the to park. 0%), more than one type with none dominant (multiple valid. 6%), and no adpositions (The dog went the park. 1%). This rule is critical for having any sort of successful semantic parse or narrative processing. We examine all dependencies with the label adpmod and have a parent word in the sentence. The instance is kept as long as the English and foreign adpositions align only to each other and the English and foreign governing nouns align only to each other. The feature vector has a dimensionality of six and consists of the same features as the feature vector for Rule 83A, simply computed for the order of the adposi-\ntion and noun phrase, rather than object and verb."}, {"heading": "3.2.3 Rule 86A: Order of Genitive and Noun", "text": "Rule 86A defines the order in which genitives appear with respect to their governing noun. 254 languages in our dataset have a known label for this rule. The three possible classes are Genitive-noun (My dog 47%), Noun-genitive (Dog my 45%), and Both occur, none dominant (Both valid 8%). This rule is absolutely necessary for coreference resolution. We examine all dependencies with the label poss, as long as the genitive and noun align only to their respective foreign language counterpart. Again, we use a length six feature vector with the same procedure over this set of dependencies."}, {"heading": "3.2.4 Rule 88A: Order of Demonstrative and Noun", "text": "Rule 88A defines the order in which demonstratives appear together with nouns. There are 171 languages in our set of Bibles with a known label for this rule. The six possible classes are Demonstrative noun (This dog is shaggy. 50%), Noun demonstrative (Dog this is shaggy. 42%), prefix on noun (This-dog is shaggy. 0%), suffix on noun (Dog-this is shaggy. 2%), Demonstrative noun demonstrative (This dog this is shaggy. 2%), and two or more of the previous options, none dominant (multiple valid. 4%). This rule is also necessary for coreference resolution. We select all dependencies that are of type det or pron, have a lexical value of this, that, these, or those, and whose parent is a noun. We compute the feature vector in the same fashion as the above rules."}, {"heading": "3.2.5 Rule 92A: Position of Polar Question Particles", "text": "Rule 92A defines the position that a polar question particle appears in a sentence. Its rule is known for 91 languages. Polar question particles signal grammatically that the sentence is a yes or no question. The six possible classes are beginning of sentence (23%), end of sentence (36%), second word in sentence (3%), anywhere else (2%), either of two positions (1%), no question particle (34%). In languages with polar questions, this rule would allow for much higher quality question answering and dialogue systems. Our selection of this rule also\ndemonstrates that the phenomenon does not even need to occur in the source language in order to be approached. We select all dependencies that contain questions. We then use the presence of an English wh- word to sort these questions into polar and information questions. Using these labels, we examine the foreign language questions for the word that appears most often in polar questions but not information questions using the ratio of relative frequencies. We then count the number of times this word appears in each of the possible positions. The feature vector has a length of four, and each entry corresponds directly to a label class: we count how often the inferred question word appears initially, second, last, or elsewhere."}, {"heading": "3.2.6 Rule 107A: Passive Constructions", "text": "107A describes the presence or absence of passive constructions. We have Bibles for 93 of the WALS languages with a known label for this rule. The two possible classes are passive construction (The dog was seen by the butler. 53%) and no passive construction (The butler saw the dog. 47%). This rule was chosen not only because it would allow for better grammar induction, a very relevant task for such under-resourced languages, but also to demonstrate that our system and approach can handle rules other than simple word order rules. We select all dependencies that are of type nsubjpass, signaling a passive subject, and nsubj, signaling an active subject. We then determine the number of times that the order of subject and verb differs between these two sentence types in the foreign language. The feature vector is built identically to all other rules except for 92A."}, {"heading": "3.3 Text Classifier Training", "text": "In order to train the text classifiers, we used wordaligned biblical texts in English and each of the languages considered along with the labels from WALS. For each rule discussed in Section 3.2, we select dependencies and create normalized feature vectors according to the discussed method. This forms the Text feature set in Section 4."}, {"heading": "3.4 Typological Classifier Training", "text": "We also consider that some labels can be predicted from the labels of other rules within a language. We\ncreate feature vectors for each language using five of the six rules discussed in Section 3.2 in order to classify the sixth rule. This comprises the Rules feature set in Section 4."}, {"heading": "3.5 Genealogical Classifier Training", "text": "Previous work has shown that propagating knowledge from genealogically similar languages has demonstrated markedly better results than propagating knowledge from a random language (Naseem et al., 2012). For our purposes we consider the genus and family of each language as given by WALS. To this end, we consider simply propagating the majority label from all languages of the same genus and, separately, all languages of the same family. We also create feature vectors such as those for the Text and Rules features, as described above."}, {"heading": "4 Results and Discussion", "text": "Majority is simply the majority class as shown in 3.2. Na\u0131\u0308ve Bayes is run using the default implement in Weka (Hall et al., 2009). For logistic regression, as we do not have a developmental data set, we simply experiment with five common regularization parameters - 1.0, 0.5, 0.1, 0.01, and 10\u22128 (the default value in Weka). As we are only evaluating over languages that already have known labels in WALS, we consider the accuracy of each classifier through leave-one-out cross-validation."}, {"heading": "4.1 Simple Linguistic Features", "text": "It is clear from Tables 1 - 6 that the combination of textual and typological features works best. It can also be seen that almost regardless of the regularization parameter, logistic regression models this data with higher accuracy than na\u0131\u0308ve Bayes. In the cases that this is not true, rule 83A and rule 88A, it is the linguistic features that perform best. This is also encouraging, as the main problem we are trying to address is the sparseness of the WALS data.\nThere are also two rules that perform quite poorly, rule 92A and rule 107A, depicted in Tables 5 and 6. These rules are significantly more subtle linguistically than the rest, which are relatively straightforward for our parser to detect accurately. The complexity of the pipeline for rule 92A and the numerous ways in which languages can mark passive con-\nstructions, regarding rule 107A, are simply difficult to perceive.\nWe also find that regularization does make a difference for the various logistic regression models. In some cases, such as Table 5, the difference can be as large as 10% accuracy. In others, such as Table 3, the difference is only 0.8%. In general, it appears that a regularization parameter between 1 and 0.5 yields the best, most consistent results for the combined feature vector case. This is likely due to the nature of the two feature vectors being combined, as one is inherently discrete while the other is continuous, but more experimentation is necessary to determine the relationship more precisely."}, {"heading": "4.2 Genealogy-based Classification", "text": "The genealogical experiments led to rather different results. We first tested a very simple approach, assigning the majority label for languages of the same genus or same family. Table 7 demonstrates these results.\nIt is very clear that this majority voting scheme works much better than the other approaches. This is particularly interesting, given the argument that typological features have been claimed to be an effective proxy for genealogical data (Rama and Kolachina, 2012).\nWe also attempted to incorporate the genealogical features as a third feature source, similar to our textual and typological features. Combining genealogical features with the either Text or Rule features did not outperform the systems already presented, nor did the combination of all three for the na\u0131\u0308ve Bayes or any logistic regression model. We also considered that perhaps there is something intrinsic to majority voting that is boosting the performance of this simple approach. To confirm, we ran experiments on the Text and Rule feature vectors using k-nearest neighbor classification. Again, these experiments did not rival the performance of our other classifiers, regardless of the choice of k.\nIt has been estimated that the accuracy of the current WALS database is possibly close to 96%, based on examination of the entries for Latvian (Cysouw, 2011). The results in Table 7 are just shy of this number for many of the rules. This means that for many rules, the simple genealogical majority measure could be used as is to expand WALS. Obviously\nthis system will not be as accurate as a trained linguist, and we are not suggesting that if we expanded WALS this way it would be fit to replace the current database, but rather that the system could be used to create a more expansive database at the cost of slightly lower accuracy."}, {"heading": "5 Conclusion", "text": "In this study, we examined the effectiveness of classifying a series of syntactic rules across a set of foreign languages. This is doable by considering the values of other syntactic rules in the database. However, it is also worth considering actual linguistic features for this linguistic task, as well as combin-\ning the two disparate sources. Finally, we also considered genealogical data available from the WALS database.\nWe have shown that with proper regularization, we can achieve better classification accuracy by augmenting the currently available WALS data with linguistic features. Though in some cases the individual linguistic features actually outperform the combined features, this is not generally the case and it only applies when the linguistic features are performing well on their own. In addition, it is not possible to say from our results which regularization parameter is best. We do, however, demonstrate that this combination of features can lead to greater success than either feature independently. This result is important despite performing worse than the genealogical approach as the goal of our study is ultimately to find accurate methods to classify features for languages that are not already contained in WALS as well as features that are missing for languages already present.\nConsidering only the genus of the language, we are within 2% of achieving human-like accuracy for half of our six rules. This is encouraging and further experimentation needs to be done over the entire set of rules in WALS. However, this approach is limited as the fundamental problem of sparsity applies to typological data, which is also used to determine the true testing labels in this case. For this reason, it is also important that we have demonstrated the usefulness of purely linguistic features that are separate from the WALS database, as well as their compatibility with the typological values in WALS."}, {"heading": "6 Future Work", "text": "Moving ahead, more complex WALS rules could be addressed with this method. While the rules we\nconsider are all useful in NLP research, there are other rules that are almost as frequently used that are more complex, such as those governing negation. In particular, modules concerning morphological features that are not easily detectable using word alignments are the next logical direction. It would also be worth gathering word aligned Bibles from other well-supported source languages besides English. As was demonstrated by Rule 92A: Position of Polar Question Particle, the system can work even when the feature does not appear in the source language, but it is certainly more difficult to account for. Having several source languages would minimize the number of rules for which this is the case.\nIn addition, experiments with combinations of genealogical, textual, and typological features should be performed. However, rather than simply concatenating these features, the features themselves should be combined into compound features, such as \u201dGermanic and 83A-No Dominant Order\u201d. We believe these types of features could improve upon the performance of the majority-vote features that are currently the best measure. The system might then achieve even higher accuracy, possibly on par with human performance for some rules, allowing the WALS database to be automatically expanded, perhaps as a parallel resource to the current database.\nIn addition, having shown that this is a reasonably accurate method for determining values of WALS rules, the method should be applied on as of yet unlabeled languages in the WALS database. We also have nearly 400 such Bibles in languages not contained in WALS, which have not been used in this experiment. Adding them to WALS would expand the number of languages in the database by more than ten percent. While we would not be able to apply the most accurate, genealogical method, we\ncould certainly make use of linguistic features from the Bibles.\nIt would also be possible to use semi-supervised methods such as semi-supervised k-Nearest Neighbors and graph regularization (Zhu et al., 2003) as an improvement to our classification method. Though we did experiment with k-Nearest Neighbors in this study, it is possible that a different approach could still yield good results. The idea of nearestneighbors classification stands to be very useful, as shown by the majority label propagation. Graph regularization has been shown to be effective in weaklysupervised situations where there is sparse training data (Hassan et al., 2014). Therefore, it would be a good idea to apply it to the WALS data. One could build a similarity network of all the languages using the known WALS rules that they share in common and regularize this graph in order to propagate labels to some extent whenever possible. Constraints such as the genus-majority relationship discussed in this paper as well as constraints borrowed from linguistic universals, representing conditional dependencies between rules, could also be used to help regularize the entire network."}], "references": [{"title": "Towards creating precision grammars from interlinear glossed text: Inferring large-scale typological properties", "author": ["Michael Wayne Goodman", "Joshua Crowgey", "Fei Xia"], "venue": "In Proceedings of the 7th Workshop on Lan-", "citeRegEx": "Bender et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bender et al\\.", "year": 2013}, {"title": "Reconstructing native language typology from foreign language usage. CoRR, abs/1404.6312", "author": ["Roi Reichart", "Boris Katz"], "venue": null, "citeRegEx": "Berzak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berzak et al\\.", "year": 2014}, {"title": "Very high accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Syntactic structures of the world\u2019s languages. http://sswl.railsplayground", "author": ["Collins", "Kayne2011] Chris Collins", "Richard Kayne"], "venue": null, "citeRegEx": "Collins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2011}, {"title": "How varied typologically are the lan", "author": ["Cysouw", "Comrie2008] Michael Cysouw", "Bernard Comrie"], "venue": null, "citeRegEx": "Cysouw et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cysouw et al\\.", "year": 2008}, {"title": "Quantitative explorations of the world-wide distribution of rare characteristics, or: the exceptionality of northwestern european languages", "author": ["Michael Cysouw"], "venue": "Expecting the Unexpected,", "citeRegEx": "Cysouw.,? \\Q2011\\E", "shortCiteRegEx": "Cysouw.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "A bayesian model for discovering typological implications", "author": ["Daum\u00e9", "Campbell2007] Hal III Daum\u00e9", "Lyle Campbell"], "venue": null, "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2007}, {"title": "Tailoring word alignments to syntactic machine translation", "author": ["DeNero", "Klein2007] John DeNero", "Dan Klein"], "venue": "In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "DeNero et al\\.,? \\Q2007\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2007}, {"title": "Structural phylogenetics and the reconstruction of ancient language history", "author": ["Dunn et al.2005] Michael Dunn", "Angela Terrill", "Ger Reesink", "Robert A Foley", "Stephen C Levinson"], "venue": null, "citeRegEx": "Dunn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dunn et al\\.", "year": 2005}, {"title": "Comparing language similarity across genetic and typologically-based groupings", "author": ["Georgi et al.2010] Ryan Georgi", "Fei Xia", "William Lewis"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Georgi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Georgi et al\\.", "year": 2010}, {"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["Joseph H. Greenberg"], "venue": "Universals of Human Language,", "citeRegEx": "Greenberg.,? \\Q1963\\E", "shortCiteRegEx": "Greenberg.", "year": 1963}, {"title": "The weka data mining software: An update", "author": ["Hall et al.2009] Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "A random walk based model for identifying semantic orientation", "author": ["Hassan et al.2014] Ahmed Hassan", "Amjad Abu-Jbara", "Wanchen Lu", "Dragomir R. Radev"], "venue": "Computational Linguistics,", "citeRegEx": "Hassan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2014}, {"title": "Word order universals: Quantitative analyses of linguistic structure", "author": ["John A Hawkins"], "venue": null, "citeRegEx": "Hawkins.,? \\Q1983\\E", "shortCiteRegEx": "Hawkins.", "year": 1983}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Hwa et al.2005] Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak"], "venue": "Natural Language Engineering,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Automatically identifying computationally relevant typological features", "author": ["Lewis", "Xia2008] William D Lewis", "Fei Xia"], "venue": "In IJCNLP,", "citeRegEx": "Lewis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2008}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Visualising typological relationships: Plotting wals with heat maps", "author": ["Rory Turnbull", "Alexis Palmer"], "venue": "In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH,", "citeRegEx": "Littauer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Littauer et al\\.", "year": 2012}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL", "author": ["Naseem et al.2012] Tahira Naseem", "Regina Barzilay", "Amir Globerson"], "venue": null, "citeRegEx": "Naseem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "How good are typological distances for determining genealogical relationships among languages", "author": ["Rama", "Kolachina2012] Taraka Rama", "Prasanth Kolachina"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics", "citeRegEx": "Rama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rama et al\\.", "year": 2012}, {"title": "Bayesian agglomerative clustering with coalescents", "author": ["Teh et al.2008] Yee Whye Teh", "Hal Daum Iii", "Daniel Roy"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Teh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2008}, {"title": "A stability metric for typological features. STUF-Language Typology and Universals Sprachtypologie und Universalienforschung, 61(3):251\u2013262", "author": ["Wichmann", "Kamholz2008] S\u00f8ren Wichmann", "David Kamholz"], "venue": null, "citeRegEx": "Wichmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wichmann et al\\.", "year": 2008}, {"title": "Multilingual structural projection across interlinear text", "author": ["Fei Xia"], "venue": "Proc. of the Conference on Human Language Technologies (HLT/NAACL", "citeRegEx": "Xia.,? \\Q2007\\E", "shortCiteRegEx": "Xia.", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": ", 2012) as well as a tool with which to discover which linguistic rules are rare across the world\u2019s languages (Cysouw, 2011).", "startOffset": 110, "endOffset": 124}, {"referenceID": 1, "context": "Unsurprisingly, it has also served as a baseline for typological similarity measurements (Berzak et al., 2014).", "startOffset": 89, "endOffset": 110}, {"referenceID": 10, "context": "Indeed, the sparseness of WALS has been noted by several researchers (Georgi et al., 2010; Cysouw and Comrie, 2008; Teh et al., 2008).", "startOffset": 69, "endOffset": 133}, {"referenceID": 22, "context": "Indeed, the sparseness of WALS has been noted by several researchers (Georgi et al., 2010; Cysouw and Comrie, 2008; Teh et al., 2008).", "startOffset": 69, "endOffset": 133}, {"referenceID": 11, "context": "proposed a set of linguistic universals (Greenberg, 1963) that refer to statistical tendencies across the world\u2019s languages.", "startOffset": 40, "endOffset": 57}, {"referenceID": 14, "context": "Further universals have been described since, particularly with respect to word order (Hawkins, 1983).", "startOffset": 86, "endOffset": 101}, {"referenceID": 19, "context": "and many other basic NLP tasks such as machine translation (Mikolov et al., 2013) and part-of-speech tagging (Das and Petrov, 2011).", "startOffset": 59, "endOffset": 81}, {"referenceID": 9, "context": "Typological similarity has previously been shown to correlate with genealogical similarity both in the fields of NLP (Rama and Kolachina, 2012) and historical linguistics (Dunn et al., 2005).", "startOffset": 171, "endOffset": 190}, {"referenceID": 20, "context": "(Naseem et al., 2012) used language similarity information to improve multilingual parsing by defined a distance", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Bender et al., 2013) attempted a similar task, us-", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "past (Xia, 2007), (Hwa et al.", "startOffset": 5, "endOffset": 16}, {"referenceID": 15, "context": "past (Xia, 2007), (Hwa et al., 2005).", "startOffset": 18, "endOffset": 36}, {"referenceID": 17, "context": "We then project these dependencies onto the foreign language biblical sentences using word alignments determined by BerkeleyAligner (DeNero and Klein, 2007; Liang et al., 2006).", "startOffset": 132, "endOffset": 176}, {"referenceID": 2, "context": "All parses referred to in this section were obtained using a Hash Kernel parser (Bohnet, 2010) and represented in the CONLL dependency parsing format.", "startOffset": 80, "endOffset": 94}, {"referenceID": 11, "context": "Besides being fundamental to any sort of parsing or chunking task, many other typological rules have been found to be statistically related to the order of subject, object, and verb (Greenberg, 1963).", "startOffset": 182, "endOffset": 199}, {"referenceID": 11, "context": "As it has also been found that subject generally precedes object (Greenberg, 1963), knowing the order of the object and verb allows not only accurate guesses about the order of subject, object, and verb, but also about many other correlated rules.", "startOffset": 65, "endOffset": 82}, {"referenceID": 20, "context": "edge from genealogically similar languages has demonstrated markedly better results than propagating knowledge from a random language (Naseem et al., 2012).", "startOffset": 134, "endOffset": 155}, {"referenceID": 12, "context": "Na\u0131\u0308ve Bayes is run using the default implement in Weka (Hall et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 5, "context": "It has been estimated that the accuracy of the current WALS database is possibly close to 96%, based on examination of the entries for Latvian (Cysouw, 2011).", "startOffset": 143, "endOffset": 157}, {"referenceID": 13, "context": "data (Hassan et al., 2014).", "startOffset": 5, "endOffset": 26}], "year": 2016, "abstractText": "This paper presents a comparison of classification methods for linguistic typology for the purpose of expanding an extensive, but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and nearest-neighbor methods for use in classification over a set of 325 languages and six syntactic rules drawn from WALS. To classify each rule, we consider the typological features of the other five rules; linguistic features extracted from a word-aligned Bible in each language; and genealogical features (genus and family) of each language. In general, we find that propagating the majority label among all languages of the same genus achieves the best accuracy in label prediction. Following this, a logistic regression model that combines typological and linguistic features offers the next best performance. Interestingly, this model actually outperforms the majority labels among all languages of the same family.", "creator": "LaTeX with hyperref package"}}}