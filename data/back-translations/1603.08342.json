{"id": "1603.08342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes", "abstract": "A hierarchical cluster algorithm based on the Gaussian mixing model is presented; the main difference from traditional hierarchical mixing models is the ability to store objects in both terminal and non-terminal nodes. Upper hierarchical levels contain sparsely distributed objects, while lower levels contain densely represented objects. Experiments have shown that this ability helps in noise detection (modeling). In addition, compared to traditional hierarchical mixing models, the method presented produces more compact, higher-quality dendrograms measured using an F-measure.", "histories": [["v1", "Mon, 28 Mar 2016 08:54:03 GMT  (130kb,D)", "http://arxiv.org/abs/1603.08342v1", "This article was presented on CORES2015 conferencethis http URL. The final publication is available at Springer viathis http URL"]], "COMMENTS": "This article was presented on CORES2015 conferencethis http URL. The final publication is available at Springer viathis http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["{\\l}ukasz p olech", "mariusz paradowski"], "accepted": false, "id": "1603.08342"}, "pdf": {"name": "1603.08342.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes", "authors": ["Lukasz P. Olech", "Mariusz Paradowski"], "emails": ["lukasz.olech@pwr.edu.pl"], "sections": [{"heading": null, "text": "Keywords: background model, outliers detection, noise modelling, hierarchical clustering, hierarchic Gaussian mixture model"}, {"heading": "1 Introduction", "text": "This paper addresses the topic of hierarchical data clustering which is a countertype to flat clustering. Flat clustering approaches generate groups without structural connections between them. Hierarchical clustering algorithms generate groups and arrange them in a tree structured manner. In such a tree structure (known as a dendrogram) all child clusters are attached to their parent cluster. Clusters without any further children are called terminal nodes or tree leaves. Clusters with attached child clusters are called non-terminal or internal nodes.\nHierarchical clustering algorithms can be divided into two categories depending on the objects attachment to the generated groups. The first category represents methods that attach objects only to terminal nodes, and non-terminal nodes of the hierarchy remain empty. This kind of methods are the majority of hierarchical data clustering methods. It is possible to fill an internal node with objects, by gathering all objects belonging to its child nodes. The second category represents methods attaching objects both to internal nodes and tree leaves. All tree leaves need to have at least one attached object. Internal nodes can have attached objects or remain empty. The key difference is that if an object is attached to the internal node, it is not attached to any of its child nodes. Methods belonging to this category are the minority. The presented research addresses this category. ar X\niv :1\n60 3.\n08 34\n2v 1\n[ cs\n.L G\n] 2\n8 M\nThis paper is organized as follows. In following subsections we give the necessary background of clustering problems. The proposed method is introduced in the second section. The third section presents the experimental results and comparison with regular hierarchical Gaussian mixture model. Finally, the fourth section summarises this paper."}, {"heading": "1.1 Hierarchical approaches to clustering", "text": "One of the earliest approaches of hierarchical clustering is hierarchical agglomerative clustering (HAC) [17]. HAC creates a dendrogram with all objects attached to its leaves. At each level of the hierarchy, two groups are merged. As a result, the created structure is an unbalanced binary tree. Various merging schemes are available, e.g., Ward criterion [22], single-link [19] or complete-link [5].\nBoth binary and non-binary hierarchies can be constructed using various extensions of the k-means algorithm [10,11]. Usage of hierarchical k-means leads to two major consequences comparing to flat k-means. First, the clustering process is much faster because the number of groups in a tree path is much lower. This is especially important if the number of clusters and the volume of data are high. Second, the overall quality of clustering tends to be worse, because cluster centers are not optimized simultaneously. One of the key problems of k-means clustering (both flat and hierarchical) is estimation of the number of clusters. There are many attempts to address this issue, e.g. x-means algorithm [18].\nHierarchical clustering using a probabilistic approach is also possible, e.g., [14]. The milestone in probabilistic clustering was the formulation of the expectation maximization (EM) algorithm [6]. Hierarchical setup of mixture models can be trained using modified EM [4]. One of the most common choices for mixture components is multivariate normal distribution."}, {"heading": "1.2 Clustering in the presence of noise", "text": "Yet another important issue is clustering of data in the presence of noise or outliers. There are two common solutions to this problem. The first solution consists of two stages, e.g., [1]. In the initial stage data is filtered in order to detect and remove outliers. Then in the second stage clusterisation is performed only on the accepted data. The second solution is to directly incorporate the noise model into the clustering process. Usually, the type or distribution of noise or outliers is not known. Various assumptions regarding these distributions have to be made. Exemplary, DBSCAN [7] and OPTICS [2] clustering algorithms assume a minimum density of the meaningful data. In probabilistic clustering, noise can be directly modeled by appropriate mixture components, e.g., [3,9]."}, {"heading": "1.3 Problem formulation, motivation and contribution", "text": "Probabilistic approach to clustering can be formulated using the parametric model. The key issue is the formulation of an appropriate probability density\nfunction (PDF). There are several forms of the probabilistic density function. Gaussian mixture model is one of the most prominent [9]. Let the Gaussian mixture model G with n mixture components be defined as:\nG(w, \u00b5,\u03a3) = n\u2211 i=1 wiN(\u00b5i, \u03a3i), wi \u2208 \u30080, 1\u3009, n\u2211 i=1 wi = 1, (1)\nand N(\u00b5,\u03a3) represents the multivariate normal distribution, w = [w1, ..., wn], \u00b5 = [\u00b51, ..., \u00b5n], \u03a3 = [\u03a31, ..., \u03a3n]. In such case clustering problem becomes a probability density function estimation problem, where PDF parameters maximize likelihood L:\n\u3008w\u2217, \u00b5\u2217, \u03a3\u2217\u3009 = arg max \u3008w,\u00b5,\u03a3\u3009 L(w, \u00b5,\u03a3|x1, ..., xm), (2)\nwhere: x1, ..., xm are the data vectors. This is typically solved by the EM algorithm, but other methods are also available, e.g., [8,21,23].\nGaussian mixture model fits to data distributed among several clusters, but does not model outliers [9]. Data not fitting to the assumed distribution can be interpreted in several ways, including: noise, measurement errors or sparser representation of meaningful objects. Statistical modelling of noisy data requires making assumptions on the noise distribution. The data distribution is usually combined with noise distribution, e.g., [16].\nIn the presented approach we follow the third interpretation of the not fitting data, i.e., sparser representation of meaningful objects. We do not want to reject the data, we want to model it on some level of the generated hierarchy. Data bound to the parent clusters should have lesser density comparing to the data bound to the child clusters. In the paper we show a simple approach to adapt hierarchical Gaussian mixture model to handle objects attached to any node in the tree. Similar to noise modelling [3,9] we add an additional mixture component to the mixture model. But unlike that approaches, we do not estimate it, but directly take it from the higher level of the hierarchy. As a consequence, parameters of the adapted mixture model are estimated in an identical manner as for the classic mixture model. They can be estimated both using EM or any other appropriate approach."}, {"heading": "2 Proposed approach", "text": "The proposed approach is an extension of a hierarchical setup of Gaussian mixture models. At each level of the hierarchy an additional mixture component, called background component, is introduced. This component is responsible for capturing outliers at a given level. Unlike all other mixture components, it is not estimated, but directly inherited from the higher level of the hierarchy. Root level also has this additional component. Its parameters are estimated (by definition) from all available data."}, {"heading": "2.1 Formal model of the hierarchy", "text": "Let us define the model of the hierarchy in a recursive way. Any parent node has all its child nodes. A tree node T generated from a data set X is defined as:\nT (X) : \u3008n,GB , B \u2286 X, [T1(X1), ..., Tn(Xn)]\u3009, (3)\nwhere:\nB \u222a n\u22c3 i=1 Xi = X, \u2200i\u2208[1,n]B \u2229Xi = \u2205, \u2200i,j\u2208[i,n]i 6= j \u21d2 Xi \u2229Xj = \u2205 (4)\nand: n is the maximum number of child nodes (and mixture components), GB is the Gaussian mixture model with background component N(\u00b5B , \u03a3B):\nGB(\u03b1,w, \u00b5B , \u00b5,\u03a3B , \u03a3) = \u03b1N(\u00b5B , \u03a3B) + (1\u2212 \u03b1)G(w, \u00b5,\u03a3) =\n= \u03b1N(\u00b5B , \u03a3B) + n\u2211 i=1 (1\u2212 \u03b1)wiN(\u00b5i, \u03a3i), (5) \u03b1 \u2208 \u30080, 1\u3009, \u00b5B = E[X], \u03a3B = V ar[X], (6)\nB \u2286 X is the data subset attached to the node T , related to background mixture component N(\u00b5B , \u03a3B), T1, ..., Tn are child nodes or void. Mixture component GB and set B are representing the data that remain in tree node T . They are the key difference when comparing to classic hierarchical clustering methods."}, {"heading": "2.2 Hierarchy generation", "text": "Cluster hierarchy generation is done in an recursive way. First, the top level is generated and its parameters are estimated. Later on, child levels are added sequentially in breadth-first manner. For each level the process terminates if a stop criterion is reached. This process is similar to the one used in hierarchical kmeans approach [20]. It allows a dynamic generation of the hierarchical structure.\nAs shown in the formal model, each level of the hierarchy contains only a subset of the data. The top level starts with all the data. Expectation maximization method is used to estimate the Gaussian mixture model. Because the proposed method is iterative, stochastic, and strongly depended on cluster initialization, several cluster reinitialisations should be performed. Thus the number of cluster reinitialization R and number of EM iterations N are the parameters.\nClusters initialization is based on choosing random n distinct points from the data and set them as initial centres \u00b5 of new clusters. Initial covariances \u03a3 of that clusters are the same as parent cluster covariance. Full covariance matrices are used. When covariance matrix is non-invertible, regularization is introduced. Mixing coefficients (see eq. 6) are initialized as equal values:\n\u03b1 = 1\nn+ 1 , (1\u2212 \u03b1)wi =\n1\nn+ 1 . (7)\nThe denominator takes into account n newly created clusters and a background cluster. The data is distributed to all mixture components, according to data probability assignments. A single data instance is assigned to the mixture component with highest probability of generating that instance. As a result, some mixture components, including the background component, may remain empty. After initialisation, the EM algorithm works through N iterations, changing initial values of \u00b5,\u03a3,w and \u03b1. After performing R reinitialisations, a solution with the largest likelihood is chosen (see eq. 2) as the final one.\nAll mixture components with assigned data instances generate child nodes. The above process repeats for every generated node. In case a mixture component does not receive any data, it also does not generate a child node. The child nodes generation process is terminated when a stop criterion is reached. There are two stop criteria and each of them terminates the method. The first stop criterion is connected with the content of current leaf nodes. The clustering process proceeds only on those leaf nodes that contain at least k different data samples. The algorithm terminates when there are no leaf nodes to split or all data is assigned to background mixture component B. The second criterion occurs when provided W overall number of nodes was created."}, {"heading": "3 Experimental verification", "text": "Experimental verification of the proposed approach consists of two parts. In the first part we give illustrative examples to demonstrate the idea behind the method. Manually prepared toy datasets are used for visualization purposes. In the second part we test the proposed approach on a set of benchmark datasets from UCI repository [13]. We choose well-known iris, wine, glass identification and image segmentation datasets varying in number of classes, attributes and instances, as shown in Table 1.\nSince the mentioned datasets do not contain any noise points we added them manually. Noise points are uniformly distributed among original points. In each dataset, the number of noise points is equal to the half of the number of original points. The proposed approach is compared to a standard hierarchical set-up of Gaussian mixture model.\nIn order to compare the obtained results on the benchmark datasets we use a metric based on F-measure [12]. It takes a class attribute into consideration and\nyield a grouping quality by considering the whole dendrogram, not only a chosen level. This makes the measure adequate for hierarchical methods. F-measure is calculated for each generated group B with respect to each class C:\nP (Xi, Cc) = Nic |Xi| , R(Xi, Cc) = Nic NCc , (8)\nFic = 2P (Xi, Cc)R(Xi, Cc)\nP (Xi, Cc) +R(Xi, Cc) , (9)\nwhere: Fic \u2013 F-measure for i-th group and c-th class, P (Xi, Cc) \u2013 precision and R(Xi, Cc) \u2013 recall, for i-th group with respect to c-th class, Nic is the number of objects from c-th class which are within i-th group, NCc is the number of object from c-th class in the entire tree and |Xi| is the number of objects that are within i-th cluster. Noise points are not regarded as an additional class, they are only counted in each |Xi|. Given the above definitions, F-measure for a chosen class Cc is defined as the maximum value of the measure over all nodes of the tree:\nF (Cc) = max i Fic. (10)\nFinally, it is averaged over all classes giving F-Measure for whole hierarchy:\nF = 1\nN |C|\u2211 c=1 NCcF (Cc), (11)\nwhere: |C| is number of classes used in dataset, N is the total number of objects (including noise points) and NCc is the number of data objects of class c. Proposed evaluation criterion has the ability to explore hierarchy structure, which is a key point in the proposed method. F maximum value is 1 and minimum is 0. Better hierarchies have higher F values."}, {"heading": "3.1 Manually generated data with noise \u2013 an illustration", "text": "All results presented in this section are two dimensional toy examples. Their sole purpose is to illustrate the behavior of the proposed method. The following examples are presented:\n1. three groups with a large central group and a small amount of noise (LC), 2. small circular data clusters with a small amount of noise (LN), 3. small circular data clusters with a large amount of noise (HN).\nBoth the data and clustering results for the toy datasets are shown in Fig. 1. The method has some ability to capture less dense data. This data is attached to the intermediate nodes of the hierarchy. The additional background model component captures these instances. In consequence they are automatically bound to the node related to the background component. At the same time, densely distributed data is moved to the bottom of the hierarchy. This can be observed (to some extent) at all presented test cases."}, {"heading": "3.2 UCI benchmark datasets", "text": "The second part of the experiments addresses the clustering of the UCI benchmark datasets. Instances of all processed datasets have both feature vectors and class assignment. Feature vectors without class information are used in the clustering process. Available class assignment is used in the evaluation process.\nTwo methods are compared: (1) the proposed Gaussian mixture model with outlier modelling and (2) classic Gaussian mixture model. The first method is denoted as B and the second as G. Both methods are trained using the same expectation-maximization routine. Hierarchies of both models are constructed in the same manner. Two quality estimates are shown: (1) log-likelihood values\nto address data fitting to the distribution, (2) f-measure values to check if the generated groups are meaningful.\nPerformed experiments consider mentioned quality estimators when W parameter vary between 2 and 10. In all conducted experiments we set n parameter as a constant equal to 2. First of all, we found the best parameters configuration (N and R) for each method per single dataset instance and W value. Then, because of stochastic nature of both methods, we have performed 100 trials for each of dataset and W parameter value, calculating mean value \u00b5 and sample standard deviation \u03c3. Moreover we conducted the Wilcoxon rank-sum test [15] on calculated F-measure in order to show the statistic significance of the obtained results. Statistic value U is calculated with alpha level (\u03b1) equal to 0.05. Null hypotheses H0 are equality of population distributions and alternative hypotheses HA may vary (depending on the corresponding F-measure \u00b5 values). When F-measure mean values \u00b5 were different, then we performed a one-tailed test whereas equal means results in two-tailed. Achieved results are shown in Tab. 2. In that table the winner column shows whether there is statistical evidence to reject the null hypothesis and assume an alternative one.\nExperiments results in Tab. 2 shows that the proposed background component improves the quality of generated dendrograms, when considering data class labels. This is especially visible when maximum number of nodes W is less than 5. Our method, though has the ability of creating compact dendrograms with better quality than the method without the background component. It is desired, because shorter trees have better generalisation abilities. Moreover, considering the iris dataset, the background component helps obtaining higher F-measure in all cases, comparing to regular hierarchical Gaussian mixture model. Proposed method reaches statistically higher average F-measure results in 16 cases whereas the regular method wins only 13 times. There have been 7 draws."}, {"heading": "4 Summary", "text": "A hierarchical grouping method is presented. It has the ability to attach objects both to terminal and non-terminal nodes. It is an extension of the classic Gaussian mixture model. The mixture is extended with an additional component responsible for outlier modelling. Parameters of this mixture component are not estimated, but directly inherited from higher levels of the hierarchy.\nConducted experiments show that the proposed modification allows to treat part of the data as sparser representation of meaningful objects. Though upper levels of hierarchy consist of sparsely distributes data. This can be used in noise or outliers modelling. Comparison between regular hierarchic GMM and hierarchic GMM with proposed modification shows that the background component helps to improve the quality of short hierarchies in real datasets with random noise."}], "references": [{"title": "OPTICS: ordering points to identify the clustering structure", "author": ["M. Ankerst", "M.M. Breunig", "H. Kriegel", "J. Sander"], "venue": "Proc. of ACM SIGMOD International Conference on Management of Data. pp. 49\u201360", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Linear flaw detection in woven textiles using model-based clustering", "author": ["J.G. Campbell", "C. Fraley", "F. Murtagh", "A.E. Raftery"], "venue": "Pattern Recognition Letters 18(14), 1539\u20131548", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Supervised learning of semantic classes for image annotation and retrieval", "author": ["G. Carneiro", "A.B. Chan", "P.J. Moreno", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 29(3), 394\u2013410", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient algorithm for a complete link method", "author": ["D. Defays"], "venue": "Comput. J. 20(4), 364\u2013366", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1977}, {"title": "Maximum Likelihood from Incomplete Data via the EM Algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 39(1), 1\u201338", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1977}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H. Kriegel", "J. Sander", "X. Xu"], "venue": "Proc. of the 2nd Internat. Conf. on Knowledge Discovery and Data Mining (KDD). pp. 226\u2013231", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Unsupervised learning of finite mixture models", "author": ["M.A.T. Figueiredo", "A.K. Jain"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 24(3), 381\u2013396", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Model-based clustering, discriminant analysis, and density estimation", "author": ["C. Fraley", "A.E. Raftery"], "venue": "Journal of the American Stat. Assoc. 97(458), pp. 611\u2013631", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Algorithm as 136: A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Journal of the Royal Statistical Society. Series C 28(1), pp. 100\u2013108", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1979}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters 31(8), 651\u2013666", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast and effective text mining using linear-time document clustering", "author": ["B. Larsen", "C. Aone"], "venue": "Proc. of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 16\u201322", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Hierarchical gaussian mixture model for speaker verification", "author": ["M. Liu", "E. Chang", "B. Dai"], "venue": "7th International Conference on Spoken Language Processing", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "On a test of whether one of two random variables is stochastically larger than the other", "author": ["H.B. Mann", "D.R. Whitney"], "venue": "The Annals of Mathematical Statistics 18(1), pp. 50\u201360", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1947}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["T.P. Minka"], "venue": "Proc. of the 17th Conference in Uncertainty in Artificial Intelligence. pp. 362\u2013369", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey of recent advances in hierarchical clustering algorithms", "author": ["F. Murtagh"], "venue": "Comput. J. 26(4), 354\u2013359", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1983}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters", "author": ["D. Pelleg", "A.W. Moore"], "venue": "Proc. of the 17th International Conference on Machine Learning. pp. 727\u2013734", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "SLINK: an optimally efficient algorithm for the single-link cluster method", "author": ["R. Sibson"], "venue": "Comput. J. 16(1), 30\u201334", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1973}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "Proc. of Workshop on Text Mining, 6th ACM SIGKDD International Conference on Data Mining (KDD\u201900) pp. 109\u2013110", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient greedy learning of gaussian mixture models", "author": ["J.J. Verbeek", "N.A. Vlassis", "B.J.A. Kr\u00f6se"], "venue": "Neural Computation 15(2), 469\u2013485", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["J.H. Ward"], "venue": "Journal of the American Statistical Association 58(301), 236\u2013244", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1963}, {"title": "Recursive unsupervised learning of finite mixture models", "author": ["Z. Zivkovic", "F. van der Heijden"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 26(5), 651\u2013656", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 14, "context": "One of the earliest approaches of hierarchical clustering is hierarchical agglomerative clustering (HAC) [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": ", Ward criterion [22], single-link [19] or complete-link [5].", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": ", Ward criterion [22], single-link [19] or complete-link [5].", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": ", Ward criterion [22], single-link [19] or complete-link [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Both binary and non-binary hierarchies can be constructed using various extensions of the k-means algorithm [10,11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 9, "context": "Both binary and non-binary hierarchies can be constructed using various extensions of the k-means algorithm [10,11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "x-means algorithm [18].", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": ", [14].", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "The milestone in probabilistic clustering was the formulation of the expectation maximization (EM) algorithm [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Hierarchical setup of mixture models can be trained using modified EM [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Exemplary, DBSCAN [7] and OPTICS [2] clustering algorithms assume a minimum density of the meaningful data.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Exemplary, DBSCAN [7] and OPTICS [2] clustering algorithms assume a minimum density of the meaningful data.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": ", [3,9].", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": ", [3,9].", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": "Gaussian mixture model is one of the most prominent [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": ", [8,21,23].", "startOffset": 2, "endOffset": 11}, {"referenceID": 18, "context": ", [8,21,23].", "startOffset": 2, "endOffset": 11}, {"referenceID": 20, "context": ", [8,21,23].", "startOffset": 2, "endOffset": 11}, {"referenceID": 7, "context": "Gaussian mixture model fits to data distributed among several clusters, but does not model outliers [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": ", [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Similar to noise modelling [3,9] we add an additional mixture component to the mixture model.", "startOffset": 27, "endOffset": 32}, {"referenceID": 7, "context": "Similar to noise modelling [3,9] we add an additional mixture component to the mixture model.", "startOffset": 27, "endOffset": 32}, {"referenceID": 17, "context": "This process is similar to the one used in hierarchical kmeans approach [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "In order to compare the obtained results on the benchmark datasets we use a metric based on F-measure [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Moreover we conducted the Wilcoxon rank-sum test [15] on calculated F-measure in order to show the statistic significance of the obtained results.", "startOffset": 49, "endOffset": 53}], "year": 2016, "abstractText": "A hierarchical clustering algorithm based on Gaussian mixture model is presented. The key difference to regular hierarchical mixture models is the ability to store objects in both terminal and nonterminal nodes. Upper levels of the hierarchy contain sparsely distributed objects, while lower levels contain densely represented ones. As it was shown by experiments, this ability helps in noise detection (modelling). Furthermore, compared to regular hierarchical mixture model, the presented method generates more compact dendrograms with higher quality measured by adopted F-measure.", "creator": "LaTeX with hyperref package"}}}