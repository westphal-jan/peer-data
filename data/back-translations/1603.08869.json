{"id": "1603.08869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Algorithms for Batch Hierarchical Reinforcement Learning", "abstract": "Hierarchical Reinforcement Learning (HRL) uses temporal abstraction to solve large Markov decision-making processes (MDP) and provide transferable subtask guidelines. In this paper, we present a non-political HRL algorithm: Hierarchical Q-value Iteration (HQI). We show that it is possible to learn recursive optimal guidelines for any valid hierarchical decomposition of the original MDP, since a fixed data set was collected from a flat stochastic behavior policy. We first formally prove the convergence of the algorithm for tabular MDP. Then, our taxi-related experiments show that HQI converges faster than a flat Q-value iteration and enjoys a simple government abstraction. We also show that our algorithm is able to learn optimal guidelines for different hierarchical structures from the same fixed data set, enabling model comparison without collecting data.", "histories": [["v1", "Tue, 29 Mar 2016 18:17:17 GMT  (807kb,D)", "http://arxiv.org/abs/1603.08869v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tiancheng zhao", "mohammad gowayyed"], "accepted": false, "id": "1603.08869"}, "pdf": {"name": "1603.08869.pdf", "metadata": {"source": "CRF", "title": "Algorithms for Batch Hierarchical Reinforcement Learning", "authors": ["Tiancheng Zhao", "Mohammad Gowayyed"], "emails": ["gowayyed}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Conventional tabular reinforcement learning is bottle-necked by the curse of dimensionality for practical applications. The number of parameters that needs to be trained grows exponentially with respect to the size of states and actions. In order to make reinforcement learning practically tractable, one line of research is hierarchical reinforcement learning (HRL), which develops principled ways of temporal and state abstraction to reduce the dimensionality for sequential decision making.\nThe basic idea of temporal abstraction is to develop macroactions that take several steps to terminate before returning. Usually good macro-actions aim to solve sub-goals, so that multiple macro-actions divide difficult tasks into several simpler ones. In addition, state abstraction tries to reduce the dimensionality by removing irrelevant state variables for decision making, reducing the cardinality of state space and helping in tackling over-fitting. These two techniques lead to natural hierarchical control architecture, which intuitively resembles how humans solve complex tasks.\nAnother area of research closely related to our work is batch reinforcement learning. Batch reinforcement learning aims to learn the best policy from a fixed set of prior-known\nsamples. Compared to on-policy algorithms, batch reinforcement learning enjoys stability and data-efficiency. More importantly, it allows to apply reinforcement learning in a practical problem that is expensive in collecting new samples, such as education, spoken dialog system and medical systems. Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.\nIn this paper, we combine batch learning and hierarchical reinforcement learning, in order to achieve faster learning speed, data efficiency and model comparison."}, {"heading": "2 Related Work", "text": "There are three major approaches developed relatively independently [1], aiming to formalize the idea of abstraction into reinforcement learning. The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].\nUnder the option framework, the developers augment the original action set by options, which are macro actions that have their own predefined policy, termination state and active state. Sutton et al have shown that such a system is a semi-Markov Decision Process (SMDP), which converges to a unique hierarchical optimal solution using a modified Qlearning algorithm. For HAM framework, rather than giving out the entire policy of these macro actions, developers just need to specify a partial program that specifies a part of the policy. Using HAMQ learning [10], HAM can also converge to a hierarchical optimal solution.\nAt last, MAXQ framework provides an elegant formulation that decomposes the original MDP into several subroutines in a hierarchy and the algorithm can learn policies recursively for all the subroutines. Therefore, in the MAXQ framework, there is no need to specify the policy for any macro-actions. However, Dietterich shows that it can only achieve recursive optimal solution, which in the extreme case, can be arbitrarily worse than the hierarchical optimal solution.\nAll of the above work assumes that the agent can interact with the world while learning. However, in real-world applications that needs HRL, it is usually very expensive to collect data and terrible failures are not allowed on operation. This forbids the usage of online learning algorithms that could potentially preform horribly in the early learning stage. To our\nar X\niv :1\n60 3.\n08 86\n9v 1\n[ cs\n.A I]\n2 9\nM ar\n2 01\n6\nbest knowledge, there is little prior work [2] in developing batch learning algorithms that allow a hierarchical SMDP to be trained from an existing data set collected from a stochastic behavior policy. We believe that such algorithms are valuable for applying HRL in complex practical domains."}, {"heading": "3 Batch Learning for HSMDP", "text": ""}, {"heading": "3.1 Definitions", "text": "Mostly, we follow the definitions in the MAXQ framework. However, for notation simplicity, we also borrow some notations from the option framework."}, {"heading": "3.2 Markov Decision Process", "text": "An MDP M is described by (S,A, P,R, P0)\n1. S is the state space of M\n2. A is a set of primitive actions that are available\n3. P (s\u2032|s, a) defines the transition probability of executing primitive action a in state, s\n4. R(s\u2032|s, a) is the reward function defined over S and A"}, {"heading": "3.3 Hierarchical Decomposition", "text": "An MDP, M , can be decomposed into a finite set of subtasks O = {O0, O1...On} with the convention that O0 is the root subtask, i.e. solving O0 solves the entire original MDP, M . Oi is then a Semi-Markov Decision Process (SMDP) that shares the same S, R, P with M , and has an extra tuple < \u03b2i, Ui >, where:\n1. \u03b2i(s) is the termination predicate of subtask Oi that partition S into a set of active states, Si and a set of terminal states Ti. If Oi enters a state in Ti, Oi and its subtasks exit immediately, i.e. \u03b2i(s) = 1 if s \u2208 Ti, otherwise \u03b2i(s) = 0.\n2. Ui is a nonempty set of actions that can be performed by Oi. The actions can be either primitive actions from A or other subtask, Oj , where i 6= j. We will refer to Ui as the children of subtask Oi.\nIt is evident that a valid hierarchical decomposition forms a direct acyclic graph (DAG) where each non-terminal node corresponds to a subtask, and each terminal node corresponds to a primitive action. For later discussion, we will use hierarchical decomposition and DAG interchangeably."}, {"heading": "3.4 Hierarchical Policy", "text": "A hierarchical policy, \u03c0, is a set of policies for each subtask, Oi, \u03c0 = {\u03c00, \u03c01...\u03c0n}. In the terminology of option framework, a subtask policy is a deterministic option, with \u03b2i(s) = 1 for s \u2208 Ti, and 0 otherwise."}, {"heading": "3.5 Recursive Optimality", "text": "A recursive optimal policy for MDP M with hierarchical decomposition is a hierarchical policy \u03c0 = {\u03c00...\u03c0n}, such that for each subtask, Oi, the corresponding policy \u03c0i is optimal for the SMDP defined by the set of states, Si, the set of actions Ui, the state transition probability P\u03c0(s\u2032, N |s, a), and the rewards function R(s\u2032|s, a)."}, {"heading": "4 Algorithm", "text": "The problem formulation is as following: given any finite set of samples, D = { (sm, am, rm, s \u2032 m)|m = 1, 2, ...,M }\nand any valid hierarchical decomposition O of the original MDP M , we wish to learn the recursive optimal hierarchical policy \u03c0\u2217.\nWe now propose Hierarchical Q-value Iteration, HQI, and we prove that it converges to the recursive optimal solution for any hierarchical decomposition given that the batch sample distribution has sufficient state action exploration. The basic idea is to train every subtask using Subtask Q-value Iteration (SQI) in a bottom up fashion. The training prerequisite of SQI for a specific subtask Oi is that all of its children Ui have converged to their greedy optimal policies. In order to fulfil this constraint, HQI first topologically sorts the DAG and running SQI from subtasks whose children have only primitive actions. After those subtasks converge to their optimal policy, the algorithm continues to other subtasks whose children are either converged or primitive actions. We will show that there always exist an ordering of training every subtask in a valid DAG that fulfills the prerequisite of SQI.\nOne challenge of training a subtask with subtask children is that we cannot use the optimal SMDP Bellman equation described in the MAXQ framework [5], Qi(s, u), which is the Q-value function for subtask, Oi, at state, s and action u:\nQi(s, u) = V (s, u) + \u2211 s\u2032,N P\u03c0i (s \u2032, N |s, u)\u03b3NQ\u03c0i (s\u2032, \u03c0i(s\u2032))\n(1)\nV (s, u) =\n{ maxu\u2032(Qu(s, u\n\u2032)) u is subtask\u2211 s\u2032 P (s \u2032|s, u)R(s\u2032|s, u) u is primitive (2)\nThe main problem of this equation(1) is that in order to estimate the Q-value for a subtask children, u, the parent Oi needs to have an estimate about the transition probability P\u03c0i (s\n\u2032, N |s, u), which is the distribution of u\u2019s exit state and number of primitive steps needed to reach its termination. Although the termination states of the child u are given by Tu, it is difficult to estimate the joint distribution of termination steps N and s\u2019 if u follows an policy that is different from the behavior policy without recollecting new samples. This is because since the behavior policy is usually random and poor in performance, the collected samples do not provide information about how many steps the subtask u would take to terminate if following a different (optimal) policy.\nTherefore, instead of using the above Bellman equation that updates the Q table of the parent when a child exits, we use the intra-option Bellman equation proposed in the option framework [13]:\nQi(s, u) = \u2211 a\u2208A \u03c0u(a|s)E[r(s, a) + \u03b3Vi(s\u2032, u)] (3)\n= \u2211 a\u2208A \u03c0u(a|s) [ r(s, a) + \u03b3 \u2211 s\u2032 P (s\u2032|s, a)Vi(s\u2032, u) ]\n(4)\nWhere\nVi(s, u) = (1\u2212 \u03b2i(s))Qi(s, u) + \u03b2i(s) max u\u2032\u2208Ui Qi(s, u \u2032) (5)\nEquation (3) also yeilds a contraction in the max norm and is able to learn the Q table after observing every new reward, which eliminates the need to estimate P\u03c0i (s\n\u2032, N |s, u). Another key benefit is that we can use flat samples to estimate the one step transition probability and rewards in equation (3), which makes the algorithm independent of the hierarchical decomposition and is able to learn optimal polices for different structures from the same dataset. Specifically, we can estimate the above two terms by \u2211 s\u2032 P (s\n\u2032|s, a)V (i, s\u2032, u) \u2248 1 c \u2211c m=1 V (i, s \u2032 m = s \u2032, u) and r(s, a) \u2248 1c \u2211c m=1 r(sm = s, am = a), where c is the number of experiences that has s\u2032 and (s, a), respectively. At last, since we assume converged subtasks follow deterministic greedy policy, \u03c0u(a|s) = 1 if a is the greedy primitive action that subtask u would take at state s, and \u03c0u(a|s) = 0 otherwise. This step is in fact crucial for HQI to learn the optimal policy because it allows a subtask to discard those samples that are not following the optimal behavior of its children.\nThe HQI algorithm is summarized in Algorithm 1 and SQI is summarized in Algorithm 2. Any dataset D can be used at every iteration of SQI. If the initial data is sufficient to cover important state-action space, the same dataset is able to train all subtasks of the DAG."}, {"heading": "4.1 Extension to Function Approximation and State Abstraction", "text": "We note that it is trivial to extend SQI to Fitted-SQI which uses a function approximator to model the Q-value function for a subtask at the end of each iteration. The direct advantage of using function approximation is that it can incorporate powerful supervised regression methods, such as Gaussian Processes or Neural Networks to scale up to large-scale and continuous MDPs. Although using function approximations usually compromises the theoretical convergence guarantee for tabular MDP, our experiments shows that Fitted-HQI is able to converge to the unique optimal solution. The FittedSQI is summarized in Algorithm 3.\nFurthermore, state abstraction here means finding a subset of state variables that are most informative for each subtask. A good hierarchical decomposition decomposes the original MDP into several simpler ones, such that the agent only needs to care about a small set of features in each task. Therefore, a good structure should create easy opportunity for state abstraction at each level of its hierarchy. Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions. These methods can be directly applied in fitting step of Fitted-SQI and each subtask can learn its own sparse state representation. Due to the space limit, we conduct simple manual state abstraction for each subtask in this paper, and leave the study of analyzing automatic feature selection techniques into future works."}, {"heading": "5 Proof of Convergence", "text": "In this section, we prove that our HQI (in the tabular case) converges to the recursive optimal policy. Assume that the policy at each subtask Mi is ordered, such that it break ties deterministically (e.g favor left to right), it defines a unique recursive optimal hierarchical policy, \u03c0\u2217r , and a corresponding\nAlgorithm 1 Hierarchical Q-value iteration (HQI) Require: O, D Train\u2190 Oi \u2208 O with only primitive children Done\u2190 {A} while Train 6= empty do\nfor Oi \u2208 Train do SQI(Oi, D) Done.add(Oi) end for Train\u2190 Oi \u2208 (O \u2212Done) AND Ui \u2208 Done\nend while\nAlgorithm 2 Subtask Q-value Iteration (SQI) Require: Oi, D\nwhile k < maxIter do for (s, a, r, s\u2032) \u2208 D do\nfor u \u2208 Ui do if s\u2032 \u2208 Ti then y \u2190 r Qki (s, u)\u2190 (1\u2212 \u03b1)Q k\u22121 i (s, u) + \u03b1y\nelse if GreedyPolicy(u, s) = a then y \u2190 r + \u03b3((1\u2212 \u03b2u(s\u2032))Qk\u22121i (s\u2032, u)\n+\u03b2u(s \u2032)maxu\u2032\u2208UiQ k\u22121 i (s \u2032, u\u2032))\nQki (s, u)\u2190 (1\u2212 \u03b1)Q k\u22121 i (s, u) + \u03b1y\nend if end if\nend for end for\nend while\nAlgorithm 3 Fitted Subtask Q-value Iteration (Fitted SQI) Require: Oi, D\nwhile k < maxIter do X \u2190 [], Y \u2190 [] for (s, a, r, s\u2032) \u2208 D do\nfor u \u2208 Ui do if s\u2032 \u2208 Ti then y \u2190 r\nelse if GreedyPolicy(u, s) = a then y \u2190 r + \u03b3((1\u2212 \u03b2u(s\u2032))Qk\u22121i (s\u2032, u)\n+\u03b2u(s \u2032)maxu\u2032\u2208UiQ k\u22121 i (s \u2032, u\u2032)) end if\nend if X.add((s, u)), Y.add(y)\nend for end for Qki \u2190 fit(X,Y )\nend while\nAlgorithm 4 GreedyPolicy Require: u, s\nif u \u2208 A then return u else u\u2217 = argmaxu\u2032\u2208Uu Qu(s, u\n\u2032) return GreedyPolicy(u\u2217, s)\nend if\nrecursive optimal Q function Q\u2217r . We then show that HQI converge to \u03c0\u2217r and Q \u2217 r . The r subscript refers to recursive optimality."}, {"heading": "5.1 Proof", "text": "We want to prove that for an MDP M = (S,A, P,R, P0, \u03b3) with hierarchical decomposition O = {O0, ..On}, HQI converges to recursive optimal policy for the hierarchical policy of M , \u03c0\u2217r .\n1. We first prove that: For a subtask Oi, with all of its children converged to their recursive optimal policies and infinity amount of batch data, algorithm SQI converge to the optimal Q-value function after infinity number of iterations, limk\u2212>\u221eQki = Q \u2217 i\n2. We then show that HQI provides an order of training all the subtasks in the DAG graph, such that when training a subtask, Oi, all of its children, Ui already converged to their optimal recursive policies."}, {"heading": "5.2 Proof Sketch", "text": "Step 1 We begin with the base case for subtask whose children are all primitive actions. We can notice that equation (3) falls back to traditional Bellman operator for flat MDP, because a primitive action always terminate after one step:\nQ(i, s, u) = r(s, a) + \u03b3 \u2211 s\u2032 P (s\u2032|s, a) max u\u2032\u2208Ui Q(i, s, u\u2032) (6)\nTherefore, for subtask with only primitive children, SQI is equivalent to flat Q-value iteration, which is guaranteed to converge to optimal policy given sufficient data.\nThen for subtasks with other subtask children, by definition, when we run SQI, the children of Oi (Ui) have converged to their unique deterministic optimal recursive policy. This means that every action u \u2208 Ui, is a deterministic deterministic Markov option as defined in the option framework [13]. [13] proved that \u201dfor any set of deterministic Markov options one step intra-option Q-learning converges w.p. 1 to the optimal Q-values, for every option regardless of what options are executed during learning provided every primitive action gets executed in every state infinitely often\u201d. Refer to the [13], for the detailed proof.\nStep 2 By definition, a hierarchical decomposition is a Directed Acyclic Graph (DAG) with edges from parents to their children. In this proof, we first reverse the edges so that they are from children to their parents. Also we know from Graph Theory that any Directed Acyclic Graph has at least one topological sort, such that every edge uv, u comes before v in the\nordering [3]. Therefore, we can to topologically sort the hierarchical decomposition with reversed edges such that SQI can always train the children before parents.\nAlso, the definition of topological sort ensures the initial condition that there is at least one subtask that only has primitive children. Therefore, we can then conclude that for any DAG, HQI can traverse the subtasks such that the conditions of SQI convergence are met. Then, HQI converges for all subtasks."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Experimental Setup", "text": "We applied our algorithm to the Taxi domain described in [13]. This is a simple grid world that contains a taxi, a passenger, and four specially-designated locations labeled R, G, B, and Y. In the starting state, the taxi is in a randomly-chosen cell of the grid, and the passenger is at one of the four special locations. The passenger has a desired destination that he/she wishes to reach, and the job of the taxi is to go to the passenger, pick him/her up, go to the passenger\u2019s destination, and drop the passenger. The taxi has six primitive actions available to it: move one step to one of the four directions (north, south, east and west), pick up the passenger and put down the passenger. To make the task more difficult, the move actions are not deterministic, so that it has 20% chance of moving in one of the other directions. Also, every move in the grid will cost \u22121 reward. Attempting to pick up or drop passenger at wrong location will cause \u221210 reward. At last, successfully finish the task has 20 reward. The grid is described in figure 1. Therefore, there are 4 possible state for the destination, 5 possible state for the passenger (4 location and 5 is on the car), 25 possible locations, which results into 500 \u2217 6 = 3000 parameters in the Q-table that needs to be learned. We denote the state variable as [dest, pass, x, y] for later discussion.\nThe dataset for each run were collected in advance by choosing actions uniformly at random with different sizes. We evaluate the performance of algorithms by running greedy execution for 100 times to obtain average discounted return at every 5000 new samples and up to 60, 000 samples. We repeat the experiments for 5 times to evaluate the influence of different sample distribution. The discounting factor is set to be 0.99."}, {"heading": "6.2 Results", "text": "We conducted three sets of experiments: 1) comparison of HQI with flat Q-value Iteration and the effect of state abstraction. 2) learning polices for different DAGs from the same dataset and 3) learning policy using Fitted-HQI with Random Forest as the function approximator.\nThe first experiment compares HQI against flat Q-value Iteration (FQI). Also, as pointed out in [4], state abstraction is essential for MAXQ to have fast learning speed compared to flat Q learning. As a result, we manually conduct state abstraction for each subtask in DAG 1. However, different from the aggressive state abstraction described in [4], where every subtask and child pair has a different set of state variables, we only conduct a simple state abstraction at subtask level, i.e. all children of a subtask has the same state abstraction. The final state abstraction is listed in Table 1. As described above, we run 5 independent runs with different random samples of different sizes, we report the mean average discounted return over five runs in Figure 3, as well as the best average discounted reward of the five runs in Figure 4.\nResults show that both HQI with and without state abstraction consistently outperforms the FQI when there is limited training data. When the dataset is large enough, they all converge to the same optimal performance, which is around 1.0. We also notice that, occasionally, HQI with state abstraction can learn the optimal performance state abstraction with very limited samples, i.e 5000 samples. This demonstrates that with proper hierarchy constraints and good behavioral policy, HQI can generalize much faster than FQI. Moreover, even the HQI without state abstraction consistently outperforms FQI in terms of sample efficiency. This is different from the behavior of the on-policy MAXQ-Q algorithm reported in [4], which needs state abstraction in order to learn faster than Qlearning. We argue that HQI without state abstraction is more sample efficient than FQI for the following reasons: 1) HQI uses all applicable primitive samples to update the Q-table for every subtask while MAXQ-Q only updates for the subtask that executes that particular action. 2) Upper level subtask in\nMAXQ-Q needs to wait for its children gradually converges to their greedy optimal policy before it can have have a good estimate of P (s\u2032, N |s, u) while HQI does not have this limitation.\nThe second experiment is running HQI on different variations of hierarchical decomposition of the original MDP. Figure 5 and Figure 6 show two different valid DAGs that could also solve the original MDP. Figure 7 demonstrates that with sufficient data all three DAG converge to their recursive optimal solution, which confirms that HQI is able converge for different hierarchies. In terms of sample efficiency, three structures demonstrate slight different behavior. We can notice that DAG 2 learns particularly slower than the other two. We argue that this is because of poor decomposition of the original MDP. Based on the problem settings, pick and drop are all risky actions (illegal execution lead to \u221210 reward), while in DAG 2 these two actions are mixed with low-cost move actions while the other two DAGs isolated them in a higher level of decision making. Therefore, designing good hierarchy is crucial to obtain performance gain versus flat RL approaches. This emphasizes the importance of the offpolicy nature of HQI, which allows developers to experiment with different DAG structures without collecting new samples. How to effectively evaluate the performance of particular hierarchical decomposition without using a simulator is a part of our future research.\nThe last experiment utilizes Random Forests as the func-\ntion approximator to model the Q-value function in DAG 1. The main purpose is to demonstrate the convergence of Fitted HQI. For each subtaskOi the Q-value functionQi(s, u) is modelled by a random forest with [dest, pass, x, y] as the input feature. Since dest and pass are categorical variables, we represent them as a one-hot vector, which transforms the state variable into a 11 dimension vector (4d for destination, 5d for passenger and 2d for the x, y coordinate). We report the mean average discounted rewards over 5 independent runs with different random samples of different sizes. Figure 8 shows that Fitted-HQI achieves similar performance compared to Tabular HQI."}, {"heading": "6.3 Comparison with MAXQ-Q and Intra-Option Learning", "text": "Compared to MAXQ-Q, HQI enjoys sample efficiency and the ability to be off-policy. The advantage of off-policy is that it does not require hyper-parameter tuning such as exploration rate. Since high level subtask in MAXQ-Q needs to wait for its children converge first, developers usually set a faster exploration decay rate for lower level subtasks, which is an extra hyperparameter that needs tuning. The limitation of HQI is that it maintains an independent Q table for each subtask, while MAXQ-Q allows a part of the parent value function recursively retrieved from its children, a technique known as value function decomposition [4]. This allows more compact memory usage and accelerates the learning of the parents. How to share value function in the off-policy setting is a future research topic.\nFor intra-option learning in option framework, the main advantage of HQI is that it does not require developers to fully define the policy of every options. Instead, one only needs to define a DAG with terminal predicate for each node in the graph. We argue that in general it is easier to define a task hierarchy than giving a full policy for macro-actions. Therefore, HQI combines the strength of intra-option off-policy learning with MAXQ. [2] provides a method of training options in an off-policy fashion. Compared to it, HQI has the advantage of learning all subtasks from flat batch dataset, so that our algorithm does not require a task DAG priory to collecting data and a manual definition of option policies."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we introduced an off-policy batch learning algorithm for hierarchical RL. We showed that it is possible to blindly collect data using a random flat policy. Then, we use this data to learn different structures that the data collection were not aware of. Our experiments on the Taxi domain show that it converges faster than FQI to the optimal policy. It also shows that different DAG structures are able to learn from this flat data, with different speeds. Every DAG structure has its own number of parameters, which suggests a possible line for research to try to minimize the number of parameters in the hierarchy. Other future work include comparing different feature selections techniques for Fitted-SQI and applying the algorithm to large-scale and complex domains."}], "references": [{"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Reducing commitment to tasks with off-policy hierarchical reinforcement learning", "author": ["Mitchell Keith Bloch"], "venue": "arXiv preprint arXiv:1104.5059,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Topological sort. Introduction to Algorithms (2nd ed.)", "author": ["Thomas H Cormen", "Charles E Leiserson", "Ronald L Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Thomas G Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "An overview of maxq hierarchical reinforcement learning. In Abstraction, Reformulation, and Approximation, pages 26\u201344", "author": ["Thomas G Dietterich"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Batch-ifdd for representation expansion in large mdps", "author": ["Alborz Geramifard", "Thomas J Walsh", "Nicholas Roy", "Jonathan How"], "venue": "arXiv preprint arXiv:1309.6831,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Least-squares policy iteration", "author": ["Michail G Lagoudakis", "Ronald Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Greedy algorithms for sparse reinforcement learning", "author": ["Christopher Painter-Wakefield", "Ronald Parr"], "venue": "arXiv preprint arXiv:1206.6485,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Sparse reinforcement learning via convex optimization", "author": ["Zhiwei Qin", "Weichang Li", "Firdaus Janoos"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}], "referenceMentions": [{"referenceID": 7, "context": "Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": "Well-known algorithms in batch reinforcement learning include Least Square Policy Iteration (LSPI) [8], Fitted Q iteration (FQI) [6], Neural Fitted Q Iteration (NFQ) [12] and etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "There are three major approaches developed relatively independently [1], aiming to formalize the idea of abstraction into reinforcement learning.", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "The three approaches are: 1) the option framework [13], 2) Hierarchies of Abstract Machines (HAMs) [10] and 3) MAXQ framework [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "Using HAMQ learning [10], HAM can also converge to a hierarchical optimal solution.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "best knowledge, there is little prior work [2] in developing batch learning algorithms that allow a hierarchical SMDP to be trained from an existing data set collected from a stochastic behavior policy.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "One challenge of training a subtask with subtask children is that we cannot use the optimal SMDP Bellman equation described in the MAXQ framework [5], Qi(s, u), which is the Q-value function for subtask, Oi, at state, s and action u:", "startOffset": 146, "endOffset": 149}, {"referenceID": 12, "context": "Therefore, instead of using the above Bellman equation that updates the Q table of the parent when a child exits, we use the intra-option Bellman equation proposed in the option framework [13]:", "startOffset": 188, "endOffset": 192}, {"referenceID": 8, "context": "Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "Many techniques have been explored [9][11][7] in non-hierarchical batch reinforcement learning to achieve state abstractions.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "This means that every action u \u2208 Ui, is a deterministic deterministic Markov option as defined in the option framework [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "[13] proved that \u201dfor any set of deterministic Markov options one step intra-option Q-learning converges w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Refer to the [13], for the detailed proof.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "ordering [3].", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "We applied our algorithm to the Taxi domain described in [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Also, as pointed out in [4], state abstraction is essential for MAXQ to have fast learning speed compared to flat Q learning.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "However, different from the aggressive state abstraction described in [4], where every subtask and child pair has a different set of state variables, we only conduct a simple state abstraction at subtask level, i.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "This is different from the behavior of the on-policy MAXQ-Q algorithm reported in [4], which needs state abstraction in order to learn faster than Qlearning.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "The limitation of HQI is that it maintains an independent Q table for each subtask, while MAXQ-Q allows a part of the parent value function recursively retrieved from its children, a technique known as value function decomposition [4].", "startOffset": 231, "endOffset": 234}, {"referenceID": 1, "context": "[2] provides a method of training options in an off-policy fashion.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to solve large Markov Decision Processes (MDP) and provide transferable subtask policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical Q-value Iteration (HQI). We show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original MDP, given a fixed dataset collected from a flat stochastic behavioral policy. We first formally prove the convergence of the algorithm for tabular MDP. Then our experiments on the Taxi domain show that HQI converges faster than a flat Q-value Iteration and enjoys easy state abstraction. Also, we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset, which enables model comparison without recollecting data.", "creator": "LaTeX with hyperref package"}}}