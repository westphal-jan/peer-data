{"id": "1603.09630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Differentiable Pooling for Unsupervised Acoustic Model Adaptation", "abstract": "We present an acoustic model for a deep neural network (DNN) including parameterized and differentiated pool operators. Unmonitored adjustments of acoustic models are considered a problem of updating the decision limits implemented by each pool operator. Specifically, we experiment with two types of pooling parameterization: learned $L _ p $normal pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker dependent. We conduct studies with three different large vocabulary speech recognition corpora: AMI meetings, TED conversations and telephone conversations. We show that differentiable pooling operators offer a robust and relatively low-dimensional way to adapt acoustic models, with error rates ranging from 5-20\\% for non-adapted systems that are themselves better than fully networked DNN-based acoustic models. We also examine how the proposed techniques work with proposed adaptation of the pool conditions, including the adaptation of the proposed space.", "histories": [["v1", "Thu, 31 Mar 2016 15:10:40 GMT  (712kb,D)", "https://arxiv.org/abs/1603.09630v1", "Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v2", "Wed, 13 Jul 2016 18:12:49 GMT  (2039kb,D)", "http://arxiv.org/abs/1603.09630v2", "11 pages, 7 Tables, 7 Figures in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, num. 11, 2016"]], "COMMENTS": "Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pawel swietojanski", "steve renals"], "accepted": false, "id": "1603.09630"}, "pdf": {"name": "1603.09630.pdf", "metadata": {"source": "CRF", "title": "Differentiable Pooling for Unsupervised Acoustic Model Adaptation", "authors": ["Pawel Swietojanski"], "emails": ["p.swietojanski@ed.ac.uk,", "s.renals@ed.ac.uk)"], "sections": [{"heading": null, "text": "I. INTRODUCTION AND SUMMARY\nDEEP neural network (DNN) acoustic models have signifi-cantly extended the state-of-the-art in speech recognition [1] and are known to be able to learn significant invariances through many layers of non-linear transformations [2]. If the training and deployment conditions of the acoustic model are mismatched then the runtime data distribution can differ from the training distribution, bringing a degradation in accuracy, which may be addressed through explicit adaptation to the test conditions [2]\u2013[9].\nIn this paper we explore the use of parametrised and differentiable pooling operators for acoustic adaptation. We introduce the approach of differentiable pooling using speakerdependent pooling operators, specifically Lp-norm pooling and weighted Gaussian pooling (Section III), showing how the pooling parameters may be optimised by minimising the negative log probability of the class given the input data (Section IV), and providing a justification for the use of pooling operators in adaptation (Section V). To evaluate this novel adaptation approach we performed experiments on three corpora \u2013 TED talks, Switchboard conversational telephone\nP Swietojanski and S Renals are with The Centre for Speech Technology Research, University of Edinburgh, Edinburgh EH89AB, U.K., (e-mail: p.swietojanski@ed.ac.uk, s.renals@ed.ac.uk)\nThe authors were supported by EPSRC Programme Grant grant EP/I031022/1 Natural Speech Technology (NST) and the European Union under H2020 project SUMMA, grant agreement 688139.\nspeech, and AMI meetings \u2013 presenting results on using differentiable pooling for speaker independent acoustic modelling, followed by unsupervised speaker adaptation experiments in which adaptation of the pooling operators is compared (and combined) with learning hidden unit contributions (LHUC) [10], [11] and constrained/feature-space maximum likelihood linear regression (fMLLR) [12]."}, {"heading": "II. DNN ACOUSTIC MODELLING AND ADAPTATION", "text": "DNN acoustic models typically estimate the posterior distribution over a set of context-dependent tied states s of a hidden Markov model (HMM) [13] given an acoustic observation o, P (s|o) = DNN(o;\u03b8) [1], [14], [15]. The DNN is implemented as a nested function comprising L processing layers (non-linear transformations):\nDNN(o;\u03b8) = fL ( fL\u22121 ( . . . f1 ( o; \u03b81 ) . . . ; \u03b8L\u22121 ) ; \u03b8L ) (1)\nThe model is thus parametrised by a set of weights \u03b8 = {\u03b8l}Ll=1 in which the lth layer consists of a weight matrix and bias vector, \u03b8l = {Wl,bl}, followed by a non-linear transformation \u03c6, acting on arbitrary input x:\nf l(x;\u03b8l) = \u03c6l ( Wl>x+ bl ) (2)\nTo form a probability distribution, the output layer employs a softmax transformation [16] \u03c6Li (x) = exp(xi)/ \u2211 j exp(xj), whereas the hidden layer activation functions are typically chosen to be either sigmoid \u03c6l(x) = 1/(1 + exp(\u2212x)) or rectified linear \u03c6l(x) = max(0, x) units (ReLU) [17].\nYu et al [2] experimentally demonstrated that the invariance of the internal representations with respect to variabilities in the input space increases with depth (the number of layers) and that the DNN can interpolate well around training samples but fails to extrapolate if the data mismatch increases. Therefore one often explicitly compensates for unseen variabilities in the acoustic space.\nFeature-space normalisation increases the invariance to unseen data by transforming the data such that it better matches the training data. In this approach the DNN learns an additional transform of the input features conditioned on the speaker or the environment. The transform, which is typically affine, is parametrised by an additional set of adaptation parameters. The most effective form of feature-space normalisation is constrained (feature-space) maximum-likelihood linear regression (MLLR), referred to as fMLLR [12], in which the linear transform parameters are estimated by maximising the likelihood of the adaptation data under a Gaussian Mixture\n10.1109/TASLP.2016.2584700 c\u00a9 2016 IEEE.\nar X\niv :1\n60 3.\n09 63\n0v 2\n[ cs\n.C L\n] 1\n3 Ju\nl 2 01\n6\nModel (GMM) / HMM acoustic model. To use fMLLR with a DNN acoustic model it is necessary to estimate a single input transform per speaker (using a trained GMM), using the resultant transformed data to train a DNN in a speaker adaptive training (SAT) manner. At runtime another set of fMLLR parameters is estimated for each speaker and the data transformed accordingly. This technique has consistently and significantly reduced the word error rate (WER) across several different benchmarks for both hybrid [1], [14] and tandem [18], [19] approaches. There are many successful examples of fMLLR adaptation of DNN acoustic models [1], [6], [8], [20]\u2013[25]. One can also estimate the linear transform as an input layer of the DNN, often referred to as a linear input network (LIN) [3], [4], [6], [26]. LIN-based approaches have been mostly used in test-only adaptation schemes, whereas fMLLR requires SAT, but usually results in lower WERs.\nAuxiliary feature approaches augment the acoustic feature vectors with additional speaker-dependent information computed for each speaker at both training and runtime stages \u2013 this is a form of SAT in which the model learns the distribution over tied states conditioned on some additional speaker-specific information. There has been considerable recent work exploring the use of i-vectors [27] for this purpose. I-vectors, which can be regarded as basis vectors spanning a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28], and were later successfully employed for DNN adaptation [29]\u2013[34]. Other examples of auxiliary features include the use of speakerspecific bottleneck features obtained from a speaker separation DNN [35], the use of out-of-domain tandem features [24], as well as speaker codes [36]\u2013[38] in which a specific set of units for each speaker is optimised. Kundu et al. [39] present an approach using auxiliary input features derived from the bottleneck layer of a DNN which is combined with i-vectors.\nModel-based approaches adapt the DNN parameters using data from the target speaker. Liao [40] investigated this approach in both supervised and unsupervised settings using a few minutes of adaptation data. On a large DNN, when all weights were adapted, up to 5% relative improvement was observed for unsupervised adaptation, using a speaker independent decoding to obtain DNN targets. Yu et al [9] have explored the use of regularisation for adapting the weights of a DNN, using the Kullback-Liebler (KL) divergence between the output distributions produced by speaker-independent and the speaker-adapted models. This approach was also recently used to adapt parameters of sequence-trained models [41]. LIN may also be regarded as a form of model-based adaptation, and related approaches include adaptation using a linear output network (LON) or linear hidden network (LHN) [4], [7], [42].\nDirectly adapting all the weights of a large DNN is computationally and data intensive, and results in large speakerdependent parameter sets. Smaller subsets of the DNN weights may be modified, including biases and slopes of hidden units [7], [34], [43], [44]. Another recently developed approach relies on learning hidden unit contributions (LHUC) for testonly adaptation [10], [11] as well as in a SAT framework [45]. One can also adapt the top layer using Bayesian methods resulting in a maximum a posteriori (MAP) approach [46],\nor address the sparsity of context-dependent tied-states when few adaptation data-points are available by modelling both monophones and context-dependent tied states using multitask adaptation [47], [48] or a hierarchical output layer [49]."}, {"heading": "III. DIFFERENTIABLE POOLING", "text": "Building on our initial work [50], we present an approach to adaptation by learning hidden layer pooling operators with parameters that can be learned and adapted in a similar way to the other model parameters. The idea of feature pooling originates from Hubel and Wiesel\u2019s pioneering study on visual cortex in cats [51], and was first used in computer vision to combine spatially local features [52]. Pooling in DNNs involves the combination of a set of hidden unit outputs into a summary statistic. Fixed poolings are typically used, such as average pooling (used in the original formulation of convolutional neural networks \u2013 CNNs) [53], [54] and max pooling (used in the context of feature hierarchies [55] and later applied to CNNs [56], [57]).\nReducing the dimensionality of hidden layers by pooling some subsets of hidden unit activations has become well investigated beyond computer vision, and the max operator has been interpreted as a way to learn piecewise linear activation functions \u2013 referred to as Maxout [58]. Maxout has been widely investigated for both fully-connected [59]\u2013 [61] and convolutional [62], [63] DNN-based acoustic models. Max pooling, although differentiable, performs a one-from-K selection, and hence does not allow hidden unit outputs to be interpolated, or their combination to be learned within a pool.\nThere have been a number of approaches to pooling with differentiable operators \u2013 differentiable pooling \u2013 a notion introduced by Zeiler and Fergus [64] in the context of constructing unsupervised feature extract for support vector machines in computer vision tasks. There has been some interest in the use of Lp-norm pooling with CNN models [57], [65] in which the sufficient statistic is the p-norm of the group of (spatially-related) hidden unit activations. Fixed order Lpnorm pooling was recently applied within the context of a convolutional neural network acoustic model [66], where it did not reduce the WER over max-pooling, and as an activation function in a fully-connected DNNs [67], where it was found to improve over maxout and ReLU.\nA. Lp-norm (Diff-Lp) pooling\nIn this approach we pool a set of activations using an Lpnorm. A hidden unit pool is formed by a set Rk of K affine projections which form the input to the kth pooling unit, which we write as an ordered set (vector) ak = {w>i x + bi}i\u2208Rk . The output of the kth pooling unit is produced as an Lp norm:\nfLp ( ak; pk ) = ||ak||pk =\n( 1\nK \u2211 i\u2208Rk |aki |pk ) 1 pk , (3)\nwhere pk is the learnable norm order for the kth unit, that can be jointly optimised with the other parameters in the model. To ensure that (3) satisfies a triangle inequality (pk \u2265 1; a\nnecessary property of the norm), during optimisation pk is reparametrised as pk = \u03b6(\u03c1k) = max(1, \u03c1k), where \u03c1k is the actual learned parameter. For the case when pk =\u221e we obtain the max-pooling operator [55]:\n||ak||\u221e = max ( {|aki |}i\u2208Rk ) . (4)\nSimilarly, if pk = 1 we obtain absolute average pooling (assuming the pool is normalised by K). We refer to this model as Diff-Lp, and it is parametrised by \u03b8Lp = {{Wl,bl, \u03c1l}L\u22121l=1 ,WL,bL}. Serement et al [65] investigated fixed-order Lp pooling for image classification, which was applied to speaker independent acoustic modelling [67]. Here we allow each Lp unit in the model to have a learnable order p [69], and we use the pooling parameters to perform modelbased test-only acoustic adaptation.\nB. Gaussian kernel (Diff-Gauss) pooling\nThe second pooling approach estimates the pooling coefficients using a Gaussian kernel. We generate the pooling inputs at each layer as:\nzk = { \u03b7k \u00b7 \u03c6(w>i x+ bi) } i\u2208Rk = { \u03b7k \u00b7 \u03c6(aki ) } i\u2208Rk , (5)\nwhere \u03c6 is a non-linearity (tanh in this work) and ak is a set of affine projections as before. A non-linearity is essential as otherwise (contrary to Lp pooling) we would produce a linear transformation through a linear combination of linear projections. \u03b7k is the pool amplitude; this parameter is tied and learned per-pool as this was found to give similar results to per-unit amplitudes (but with fewer parameters), and better results compared to setting to a fixed value \u03b7k = 1.0 [50].\nGiven the activation (5), the pooling operation is defined as a weighted average over a set Rk of hidden units, where the k-th pooling unit fG(\u00b7;\u03d1k) is expressed as:\nfG ( zk;\u03d1k ) = \u2211 i\u2208Rk ui(z k;\u03d1k)zki . (6)\nThe pooling contributions u(zk;\u03d1k) are normalised to sum to one within each pooling region Rk (7) and each weight ui(z k i ;\u03d1\nk) is coupled with the corresponding value of zki by a Gaussian kernel (8) (one per pooling unit) parameterised by the mean and precision, \u03d1k = {\u00b5k, \u03b2k}:\nui(z k;\u03d1k) = v(zki ;\u03d1 k)\u2211\ni\u2032\u2208Rk v(z k i\u2032 ;\u03d1\nk) , (7)\nv(zki ;\u03d1 k) = exp\n( \u2212\u03b2k\n2\n( zki \u2212 \u00b5k )2) . (8)\nSimilar to Lp-norm pooling, this formulation allows a generalised pooling to be learned \u2013 from average (\u03b2 \u2192 0) to max (\u03b2 \u2192\u221e) \u2013 separately for each pooling unit fG(zk;\u03d1k) within a model. The Diff-Gauss model is thus parametrised by \u03b8G = {{Wl,bl,\u00b5l,\u03b2 l, \u03b7 l}L\u22121l=1 ,WL,bL}."}, {"heading": "IV. LEARNING DIFFERENTIABLE POOLERS", "text": "We optimise the acoustic model parameters by minimising the negative log probability of the target HMM tied state given the acoustic observations using gradient descent and error back-propagation [70]; the pooling parameters may be updated in a speaker-dependent manner, to adapt the acoustic model to unseen data. In this section we give the necessary partial derivatives for Diff-Lp and Diff-Gauss pooling.\nA. Learning and adapting Diff-Lp pooling\nIn Diff-Lp pooling we learn pk which we express in terms of \u03c1, pk = \u03b6(\u03c1k). Error back-propagation requires the partial derivative of the pooling region fLp(a\nk; \u03c1k) with respect to \u03c1k, which is given as:\n\u2202fLp(a k; \u03c1k)\n\u2202\u03c1k = (\u2211 i\u2208Rk log(|aki |) \u00b7 |aki |pk pk \u2211 i\u2208Rk |aki |pk (9)\n\u2212 log \u2211\ni\u2208Rk |aki |pk p2k\n) \u2202\u03b6(\u03c1k)\n\u2202\u03c1k fLp(a\nk; \u03c1k),\nwhere \u2202\u03b6(\u03c1k)/\u2202\u03c1k = 1 when pk > 1 and 0 otherwise. The back-propagation through the norm itself is implemented as:\n\u2202fLp(a k; pk)\n\u2202ak = ak \u25e6 |ak|pk\u22122\u2211 i\u2208Rk |aki |pk \u25e6Gk, (10)\nwhere \u25e6 represents the element-wise Hadamard product, and Gk is a vector of fLp(a\nk; pk) activations repeated K times, so the resulting operation can be fully vectorised:\nGk = [ fLp(a k; pk) 1, . . . , fLp(a k; pk) K ]> . (11)\nNormalisation by K in (3) is optional (see also Section VII-A) and the partial derivatives in (9) and (10) hold for the unnormalised case also: the effect of this is taken into account in the forward activation fLp(a k; pk).\nSince (9) and (10) are not continuous everywhere, they need to be stabilised when \u2211 i\u2208Rk |aki |pk = 0. When computing logarithm in the numerator of (9) it is also necessary to ensure that each aki > 0. In practise, we threshold each element to have at least a value = 10\u22128 if aki < . Note, this numerical stabilisation of ak only applies to Lp units, not Diff-Gauss.\nB. Learning and adapting Diff-Gauss pooling regions\nTo learn the Diff-Gauss pooling parameters \u03d1k = {\u00b5k, \u03b2k}, we require the partial derivatives \u2202fG(zk)/\u2202\u00b5k and \u2202fG(zk)/\u2202\u03b2k to update pooling parameters, as well as \u2202fG(z\nk)/\u2202zk in order to back-propagate error signals to lower layers.\nOne can compute the partial derivative of (6) with respect to the input activations zk as:\n\u2202fG(z k) \u2202zk = [ (zk)> ( Ju(v(z k))Jv(z k) ) + u(zk)> ]> , (12)\nwhere Ju(v(zk)) is the Jacobian representing the partial derivative \u2202u(zk)/\u2202v(zk):\nJu(v(z k)) =\n\u2202u(zk) \u2202v(zk) =  \u2202u(zk1 ) \u2202v(zk1 ) \u00b7 \u00b7 \u00b7 \u2202u(z k 1 ) \u2202v(zkK) ... . . .\n... \u2202u(zkK)\n\u2202v(zk1 ) \u00b7 \u00b7 \u00b7 \u2202u(z\nk K) \u2202v(zkK)  , (13) whose elements can be computed as:\n\u2202u(zki ) \u2202v(zki ) = ( \u2211\nm\u2208Rk\nv(zm) )\u22121 ( 1\u2212 u(zki ) ) , (14)\n\u2202u(zki ) \u2202v(zki\u2032) = ( \u2211\nm\u2208Rk\nv(zm) )\u22121 ( \u2212u(zki ) ) . (15)\nLikewise, Jv(zk) represents the Jacobian of the kernel function v(zk) in (8) with respect to zk:\nJv(z k) =\n\u2202v(zk)\n\u2202zk =  \u2202v(zk1 ) \u2202zk1 \u00b7 \u00b7 \u00b7 0 ... . . . ...\n0 \u00b7 \u00b7 \u00b7 \u2202v(z k K)\n\u2202zkK  , (16) and the elements of Jv(zk) can be computed as:\n\u2202v(zki )\n\u2202zki = \u2212\u03b2k(zki \u2212 \u00b5k)v(zki ). (17)\nSimilarly, one can obtain the gradients with respect to the pooling parameters \u03d1k. In particular, for \u03b2k, the gradient is:\n\u2202fG(z k) \u2202\u03b2k = \u2211 i\u2208Rk [ (zk)> ( Ju(v(z k))Jv(\u03b2k) )] i , (18)\nwhere Jv(\u03b2k) = \u2202v(zk)/\u2202\u03b2k and \u2202v(zki )/\u2202\u03b2k is:\n\u2202v(zki )\n\u2202\u03b2k = \u22121 2\n( zki \u2212 \u00b5k )2 v(zi). (19)\nThe corresponding gradient for \u2202fG(zk)/\u2202\u00b5k is obtained below (20). Notice, that \u2202v(zki )/\u2202z k i (17) and \u2202v(z k i )/\u2202\u00b5k (21) are symmetric, hence Jv(\u00b5k) = \u2212Jv(zk), and to compute\n\u2202fG(z k)/\u2202\u00b5k one can reuse the (zk)>Ju(v(zk))Jv(zk) term in (12), as follows:\n\u2202fG(z k) \u2202\u00b5k = \u2211 i\u2208Rk [ (zk)> ( Ju(v(z k))Jv(\u00b5k) )] i\n= \u2212 \u2211 i\u2208Rk [ (zk)> ( Ju(v(z k))Jv(z k) )] i , (20)\n\u2202v(zki )\n\u2202\u00b5k = \u2212\u2202v(z\nk i )\n\u2202zki = \u03b2k(z\nk i \u2212 \u00b5k)v(zki ). (21)"}, {"heading": "V. REPRESENTATIONAL EFFICIENCY OF POOLING UNITS", "text": "The aim of model-based DNN adaptation is to alter the learned speaker independent representation in order to improve the classification accuracy for data from a possibly mismatched test distribution. Owing to the highly distributed representations that are characteristic of DNNs, it is rarely clear which parameters should be adapted in order generalise well to a new speaker or acoustic condition.\nPooling enables decision boundaries to be altered, through the selection of relevant hidden features, while keeping the parameters of the feature extractors (the hidden units) fixed: this is similar to LHUC adaptation [68]. The pooling operators allow for a geometrical interpretation of the decision boundaries and how they will be affected by a constrained adaptation \u2013 the units within the pool are jointly optimised given the pooling parametrisation, and share some underlying relationship within the pool.\nThis is visualised for Lp units in Fig. 2. Fig. 2 (a) illustrates the unit circles obtained by solving ||ak||p = d for different orders p, with d = 1.0 and a pool of K = 2 linear inputs ak. Such an Lp unit is capable of closed-region decision boundaries, illustrated in Fig. 2 (b). The distance threshold d is implicitly learned from data (through the ak parameters given p), resulting in an efficient representation [67], [69] compared with representing such boundaries using sigmoid units or ReLUs, which would require more parameters. Figs. 2 (c) and (d) show how those boundaries are affected when p = 1 (average pooling) and p = \u221e (max pooling), while keeping ak fixed. As shown in Section VII we found that updating p is an efficient and relatively low-dimensional way to adjust decision boundaries such that the the model\u2019s accuracy on the adaptation data distribution improves.\nIt is also possible to update the biases (Fig. 2 (e), red contours) and the LHUC amplitudes (Fig. 2 (f), red contours). We experimentally investigate how each approach impacts adaptation WER in Section VII-B. Although models implementing Diff-Gauss units are theoretically less efficient in terms of SI representations compared to Lp units, and comparable to standard fully-connected models, the pooling mechanism still allows for more efficient (in terms of number of SD parameters) speaker adaptation."}, {"heading": "VI. EXPERIMENTAL SETUPS", "text": "We have carried out experiments on three corpora: the TED talks corpus [71] following the IWSLT evaluation protocol (www.iwslt.org); the Switchboard corpus of conversational telephone speech [72] (ldc.upenn.edu) and the AMI meetings corpus [73], [74] (corpus.amiproject.org). Unless explicitly stated otherwise, our baseline models share similar structure across the tasks \u2013 DNNs with 6 hidden layers (2,048 units per layer) using a sigmoid non-linearity. The output softmax layer models the distribution of context-dependent clustered tied states [75]. The features are presented in 11 (\u00b15) frame long context windows. All the adaptation experiments, if not stated otherwise, were performed unsupervised using adaptation targets obtained from first-pass speaker-independent decoding of the corresponding SI system.\nTED: The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe [8]. However, compared to our previous work [8], [11], [50], our systems here make use of more accurate language models developed for our IWSLT\u20132014 systems [76]: in particular, the final reported results use a 4-gram language model estimated from 751 million words. The baseline TED acoustic models were trained on unadapted PLP features with\nfirst and second order time derivatives. We present results on four IWSLT test sets: dev2010, tst2010, tst2011 and tst2013 containing 8, 11, 8, and 28 talks respectively.\nAMI: We follow a Kaldi GMM recipe [77] and use the individual headset microphone (IHM) recordings. On this corpus, we train the acoustic models using 40 mel-filter-bank (FBANK) features. We decode with a pruned 3-gram language model estimated from 800k words of AMI training transcripts interpolated with an LM trained on Fisher conversational telephone speech transcripts (1M words) [78].\nSwitchboard (SWBD): We follow a Kaldi GMM recipe [79], [80]1, using Switchboard\u20131 Release 2 (LDC97S62). Our baseline unadapted acoustic models were trained on MFCC features, while the SAT trained fMLLR variants utilise the usual Kaldi feature preprocessing pipeline, which is MFCC+LDA/MLLT+fMLLR2. The results are reported on the full Hub5\u201900 set (LDC2002S09) \u2013 eval2000. eval2000 contains two types of data: Switchboard \u2013 which is better matched to the training data; and CallHome (CHE) English. Our reported results use 3-gram LMs estimated from the Switchboard and Fisher Corpus transcripts."}, {"heading": "VII. RESULTS", "text": ""}, {"heading": "A. Baseline speaker independent models", "text": "The structures of the differentiable pooling models were selected such that the number of parameters was comparable to the corresponding baseline DNN models, described in detail in [68]. For the Diff-Lp and Diff-L2 types, the resulting models utilised non-overlapping pooling regions of size K = 5, with 900 Lp-norm units per layer. The Diff-Gauss models had pool sizes set to K = 3 (this was found to work best in our previous work [50]) which (assuming nonoverlapping regions) results in 1175 pooling units per layer.\nTraining speaker independent Diff-L2 and Diff-Lp models: For both Diff-Lp and Diff-L2 we trained with an initial learning rate of .008 (for MFCC, PLP, FBANK features) and .006 (for fMLLR features). The learning rate was adjusted using the newbob learning scheme [81] based on the validation frame error rate. We found that applying explicit pool normalisation (dividing by K in (3)) gives consistently higher error rates (typically an absolute increase of 0.3% WER): hence we used un-normalised Lp units in all experiments. We did not apply post-layer normalisation [67]. Instead, we use max-norm approach \u2013 after each update we scaled the columns (i.e. each aki ) of the fully connected weight matrices such that their L2 norms were below a given threshold (set to 1.0 in this work) [82]. For Diff-Lp models we initialised p = 2.0. Those parameters were optimised on TED and directly applied without further tuning for the other two corpora. In this work we have focussed on adaptation; Zhang et\n1To stay compatible with our previous adaptation work on Switchboard [45], [68] we are using the older set of Kaldi recipe scripts called s5b, and our baseline results are comparable with the corresponding baseline numbers previously reported. A newer set of improved scripts exists under s5c which, in comparison to s5b, offer about 1.5% absolute lower WER.\n2MFCC-Mel-frequency Cepstral Coefficients, LDA - Linear Discriminant Analysis, MLLT - Maximum Likelihood Linear Transform\nal [67] have reported further speaker independent experiments for fixed order Lp units.\nTraining speaker independent Diff-Gauss models: The initial learning rate was set to 0.08 (regardless of the feature type), again adjusted using newbob. Initial pooling parameters were sampled randomly from normal distribution: \u00b5 \u223c N (0, 1) and \u03b2 \u223c N (1, 0.5). Otherwise, the hyperparameters were the same as for the baseline DNN models.\nBaseline speaker independent results: Table I gives speaker independent results for each of the considered model types. The Diff-Gauss and Diff-L2/Diff-Lp models have comparable WERs, with a small preference towards Diff-Lp in terms of the final WER on TED and AMI; all have lower average WER than the baseline DNN. The gap between the pooled models increases on AMI data where Diff-Lp has a substantially lower WER (3.2% relative) than the fixed order Diff-L2 which is in turn has a lower WER than the other two models (Diff-Gauss and baseline DNN) by 2.1% relative.\nFig. 3 gives more insight into the Diff-Lp models by showing how the final distributions of the learned order p differ across AMI, TED and SWBD corpora. p deviates more from its initialisation in the lower layers of the model; there is also a difference across corpora. This follows the intuition of how a multi-layer network builds its representation: lower layers are more dependent on acoustic variabilities, normalising for such effects, and hence feature extractors may differ across datasets \u2013 in contrast to the upper layers which rely on features abstracted away from the acoustic data. For these corpora, the order p rarely exceeded 3, sometimes dropping below 2 \u2013 especially for layer 1 with SWBD data. However, most Lp units, especially in higher layers, tend to have p \u223c 2. This corresponds to previous work [67] in which fixed Lp=2 units tended to obtain lower WER. A similar analysis of Diff-Gauss pooling does not show large data-dependent differences in the learned pooling parameters.\nTraining speed: Table II shows the average training speeds for each of the considered models. Training pooling units is significantly more expensive than training baseline DNN models. This is to be expected as the pooling operations cannot be easily and fully vectorised. In our implementation training the Diff-Gauss or Diff-Lp models is about 40% slower than training a baseline DNN. Not optimising p during training (9) decreases the gap to about 20% slower. This indicates that training using fixed L2 units, and then adapting the order p in a speaker adaptive manner could make a good compromise."}, {"heading": "B. Adaptation experiments", "text": "We initially used the TED talks corpus to investigate how WERs are affected by adapting different layers in the model. The results indicated that adapting only the bottom layer brings the largest drop in WER; however, adapting more layers further improves the accuracy for both Diff-Lp and Diff-Gauss models (Fig. 4 (a)). Since obtaining the gradients for the pooling parameters at each layer is inexpensive compared to the overall back-propagation, and adapting bottom layer gives largest gains, in the remainder of this work we adapt all pooling units. Similar trends hold when pooling adaptation is combined with LHUC adaptation, which on tst2010 improves the accuracies by 0.2-0.3% absolute.\nFig. 4 (b) shows WER vs. the number of adaptation iterations. The results indicate that one adaptation iteration is\nsufficient and, more importantly, the model does not overfit when more iterations are used. This suggests that it is not necessary to regularise the model carefully (by KullbackLeibler divergence [9], for instance) which is usually required when weights that directly transform the data are adapted. In the remainder, we adapt all models with a learning rate of 0.8 for three iterations (optimised on dev2010).\nTable III shows the effect of adapting different pooling parameters (including LHUC amplitudes) for Lp units. Updating only p, rather than any other stand-alone pooling parameter, gives a lower WER than LHUC adaptation with the same number of parameters (cf Fig. 2); however, updating both brings further reductions in WER. Adapting the bias is more data-dependent with a substantial increase in WER for SWBD; this also significantly increases the number of adapted parameters. Hence we adapted either p alone, or p with LHUC in the remaining experiments\nTable IV shows similar analysis but for Diff-Gauss model. For Diff-Gauss, it is beneficial to update both \u00b5 and \u03b2 (as in [50]), and LHUC was also found to be complementary. Notice, adapting with LHUC scalers is similar\nto altering \u03b7 in eq. (5) (assuming \u03b7 is tied per pool, as mentioned in Section III-B). As such, new parameters need not be introduced to adapt Diff-Gauss with LHUC as it is the case for Diff-Lp units. In fact, last two rows of Table IV show that jointly updating \u00b5, \u03b2 and \u03b7 gives lower WER than updating \u00b5, \u03b2 and applying LHUC after pooling (see Fig. 1).\nAnalysis of Diff-Lp: Fig. 5 shows how the distribution of p changes after the Diff-Lp model adapts to each of the 28 speakers of tst2013. We plot the speaker independent histograms as well as the contours of the mean bin frequencies for each layer. For the adapted models the distributions of p become less dispersed, especially in higher layers, which can be interpreted as shrinking the decision regions of particular Lp units (cf Fig. 2). This follows the intuition that speaker adaptation involves reducing the variability that needs to be modelled, in contrast to the speaker independent model.\nTaking into account the increased training time of Diff-Lp models, one can also consider training fixed order Diff-L2 [67], adapting p using (9). The results in Fig. 5, as well as later results, cover this scenario. The adapted\nDiff-L2 models display a similar trend in the distribution of p to the Diff-Lp models.\nAnalysis of Diff-Gauss: We performed a similar investigation on the learned Diff-Gauss pooling parameters (Fig. 6). In the bottom layers they are characterised by a large negative means and positive precisions which has the effect of turning off many units. After adaptation, some of them become more active, which can be seen based on shifted distributions of adapted pooling parameters in Fig. 6. The adaptation with Diff-Gauss has a similar effect as the adaptation of slopes and amplitudes [44], [83], but adapts K times fewer parameters.\nAmount of adaptation data and quality of targets: We investigated the effect of the amount of adaptation data by randomly selecting adaptation utterances from tst2010 to give totals of 10s, 30s, 60s, 120s, 300s and more speakerspecific adaptation data per talker (Fig. 7 (a)). The WERs are an average over three independent runs, each sampling a different set of adaptation utterances (we did more passes in our previous work [11], [50], however, both LHUC and differentiable pooling operators were not sensitive to this aspect, resulting in small error bars between different results\nobtained with different random utterances). The Diff-Lp models offer lower WER and more rapid adaptation, with 10s of adaptation data resulting in a decrease in WER by 0.6% absolute (3.6% relative) which further increases up to 2.1% absolute (14.4% relative) when using all the speaker\u2019s data in an unsupervised manner. Diff-Gauss is comparable in terms of WER to a DNN adapted with LHUC. In addition, both methods are complementary to LHUC adaptation, and to feature-space adaptation with fMLLR (Tables VI and VII).\nIn order to demonstrate the modelling capacities of the different model-based adaptation techniques, we carried out a supervised adaptation (oracle) experiment in which the adaptation targets were obtained by aligning the audio data with reference transcripts (Fig. 7 (b)). We do not refine what the model knows about speech, nor the way it classifies it (the feature receptors and output layer are fixed during adaptation and remain speaker independent), but show that the re-composition and interpolation of these basis functions to approximate the unseen distribution of adaptation data is able to decrease the WER by 26.7% relative for Diff-Lp + LHUC scenario.\nThe methods are also not very sensitive to the quality of adaptation targets, they show very similar trends as LHUC, for which exact results for different qualities of adaptation targets resulting from re-scoring adaptation hypotheses with different language models were reported in [68].\nSummary: Results for the proposed techniques are summarised in Tables V, VI, and VII for AMI, TED, and SWBD, respectively. The overall observed trends are as follows: (I) speaker independent pooling models return lower WERs than the baseline DNNs: Diff-Gauss < Diff-L2 \u2264 Diff-Lp (although the last two seem to be data-dependent); (II) the pooling models (Diff-Gauss, Diff-L2 and Diff-Lp) are complementary to both fMLLR and LHUC adaptation \u2013 as expected, the final gain depends on the degree of data mismatch; (III) one can effectively train speaker independent Diff-L2 models and later alter p in a speaker dependent manner; (IV) the average relative improvement across all tasks with respect to baseline unadapted DNN models were 6.8% for Diff-Gauss, 9.1% for Diff-L2 and 10.4% for Diff-Lp; and (V) when comparing LHUC adapted DNN to LHUC adapted differentiable pooling models, the relative reductions in WER for the pooling models were 2%, 3.4% and 4.8% for Diff-Gauss, Diff-L2 and Diff-Lp, respectively."}, {"heading": "VIII. DISCUSSION AND CONCLUSIONS", "text": "We have proposed the use of differentiable pooling operators with DNN acoustic models to perform unsupervised speaker adaptation. Differentiable pooling operators offer a relativelylow dimensional set of parameters which may be adapted in a speaker-dependent fashion.\nWe investigated the complementarity of differentiable pooling adaptation with two other approaches \u2013 model-based LHUC adaptation and feature-space fMLLR adaptation. We have not performed an explicit comparison with an i-vector approach to adaptation. However, some recent papers have compared ivector adaptation with either LHUC and/or fMLLR on similar\ndata which enables us some make indirect comparisons. For example, Samarakoon and Sim [34] showed that speakeradaptive training with i-vectors gives a comparable results to test-only LHUC using TED data, and Miao et. al [33] suggested that LHUC is better than a standard use of i-vectors (as in Saon\net al. [29]) on TED data, with a more sophisticated i-vector post-processing needed to equal LHUC. Since the proposed Diff-Lp and Diff-Gauss techniques resulted in WERs that were at least as good as LHUC (and were found to be complementary to fMLLR) we conclude that the proposed pooling-based adaptation techniques are competitive.\nIn the future, one could investigate extending the proposed techniques to speaker adaptive training (SAT) [84], [85], for example in a similar spirit as proposed in the context of SAT-LHUC [45]. In addition it would be interesting to investigate the suitability of adapting pooling regions in the framework of sequence discriminative training [79], [86], [87]. Our experience of LHUC in this framework [68], together with the observation that the pooling models are not prone to overfitting in the case of small amounts of adaptation data, suggests that adaptation based on differentiable pooling is a promising technique for sequence trained models."}, {"heading": "ACKNOWLEDGEMENT", "text": "The NST research data collection may be accessed at http://datashare.is.ed.ac.uk/handle/10283/786. This research utilised a K40 GPGPU board donated by NVIDA Corporation. The authors would like to thank the reviewers for insightful comments that helped to improve the manuscript."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G Hinton", "L Deng", "D Yu", "GE Dahl", "A Mohamed", "N Jaitly", "A Senior", "V Vanhoucke", "P Nguyen", "TN Sainath", "B Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature learning in deep neural networks - studies on speech recognition", "author": ["D Yu", "M Seltzer", "J Li", "J-T Huang", "F Seide"], "venue": "Proc. ICLR, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker adaptation for hybrid HMM\u2013ANN continuous speech recognition system", "author": ["J Neto", "L Almeida", "M Hochberg", "C Martins", "L Nunes", "S Renals", "T Robinson"], "venue": "Proc. Eurospeech, 1995, pp. 2171\u20132174.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B Li", "KC Sim"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "On speaker adaptive training of artificial neural networks", "author": ["J Trmal", "J Zelinka", "L M\u00fcller"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F Seide", "X Chen", "D Yu"], "venue": "Proc. IEEE ASRU, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K Yao", "D Yu", "F Seide", "H Su", "L Deng", "Y Gong"], "venue": "Proc. IEEE SLT, 2012, pp. 366\u2013369.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Revisiting hybrid and GMM- HMM system combination techniques", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Proc. IEEE ICASSP, 2013, pp. 6744\u20136748.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["D Yu", "K Yao", "H Su", "G Li", "F Seide"], "venue": "Proc. IEEE ICASSP, 2013, pp. 7893\u20137897.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Rapid and effective speaker adaptation of convolutional neural network based models for speech recognition", "author": ["O Abdel-Hamid", "H Jiang"], "venue": "Proc. ICSA Interspeech, 2013, pp. 1248\u20131252.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Maximum likelihood linear transformations for HMMbased speech recognition", "author": ["MJF Gales"], "venue": "Computer Speech and Language, vol. 12, pp. 75\u201398, April 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "State clustering in hidden Markov modelbased continuous speech recognition", "author": ["SJ Young", "PC Woodland"], "venue": "Computer Speech and Language, vol. 8, no. 4, pp. 369\u2013383, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H Bourlard", "N Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S Renals", "N Morgan", "H Bourlard", "M Cohen", "H Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 2, pp. 161\u2013174, 1994.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["JS Bridle"], "venue": "Neurocomputing, F Fogelman Souli\u00e9 and J H\u00e9rault, Eds., pp. 227\u2013236. Springer, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V Nair", "G Hinton"], "venue": "Proc. ICML, 2010, pp. 131\u2013136.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["H Hermansky", "DPW Ellis", "S Sharma"], "venue": "Proc. IEEE ICASSP, 2000, pp. 1635\u20131638.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic and bottleneck features for LVCSR of meetings", "author": ["F Grezl", "M Karafiat", "S Kontar", "J Cernocky"], "venue": "Proc. IEEE ICASSP, 2007, pp. IV\u2013757\u2013IV\u2013760.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A Mohamed", "TN Sainath", "G Dahl", "B Ramabhadran", "GE Hinton", "MA Picheny"], "venue": "Proc. IEEE ICASSP, 2011, pp. 5060\u20135063.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Transcribing meetings with the AMIDA systems", "author": ["T Hain", "L Burget", "J Dines", "PN Garner", "F Gr\u00e9zl", "A El Hannani", "M Karaf\u0131\u0301at", "M Lincoln", "V Wan"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 20, pp. 486\u2013498, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["TN Sainath", "B Kingsbury", "B Ramabhadran"], "venue": "Proc. IEEE ICASSP, 2012, pp. 4153\u20134156.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["TN Sainath", "A Mohamed", "B Kingsbury", "B Ramabhadran"], "venue": "Proc. IEEE ICASSP, 2013, pp. 8614\u20138618.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-level adaptive networks in tandem and hybrid ASR systems", "author": ["P Bell", "P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of unsupervised adaptation of dnn acoustic models with filter bank input", "author": ["T Yoshioka", "A Ragni", "MJF Gales"], "venue": "Proc. IEEE ICASSP, 2014, pp. 6344\u20136348.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectionist speaker normalization and adaptation", "author": ["V Abrash", "H Franco", "A Sankar", "M Cohen"], "venue": "Proc. Eurospeech, 1995, pp. 2183\u2013 2186.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Front end factor analysis for speaker verification", "author": ["N Dehak", "PJ Kenny", "R Dehak", "P Dumouchel", "P Ouellet"], "venue": "IEEE Trans Audio, Speech and Language Processing, vol. 19, pp. 788\u2013798, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "I-vectorbased discriminative adaptation for automatic speech recognition", "author": ["M Karafiat", "L Burget", "P Matejka", "O Glembek", "J Cernozky"], "venue": "Proc. IEEE ASRU, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G Saon", "H Soltau", "D Nahamoo", "M Picheny"], "venue": "Proc. IEEE ASRU, 2013, pp. 55\u201359.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving DNN speaker independence with i-vector inputs", "author": ["A Senior", "I Lopez-Moreno"], "venue": "Proc. IEEE ICASSP, 2014, pp. 225\u2013229.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "I-vector based speaker adaptation of deep neural networks for french broadcast audio transcription", "author": ["V Gupta", "P Kenny", "P Ouellet", "T Stafylakis"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptation of deep neural network acoustic models using factorised i-vectors", "author": ["P Karanasou", "Y Wang", "MJF Gales", "PC Woodland"], "venue": "Proc. ICSA Interspeech, 2014, pp. 2180\u20132184.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptive training of deep neural network acoustic models using i-vectors", "author": ["Y Miao", "H Zhang", "F Metze"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 11, pp. 1938\u20131949, Nov 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1938}, {"title": "On combining i-vectors and discriminative adaptation methods for unsupervised speaker normalization in dnn acoustic models", "author": ["L Samarakoon", "K C Sim"], "venue": "Proc. IEEE ICASSP, 2016, pp. 5275\u20135279.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y Liu", "P Zhang", "T Hain"], "venue": "Proc. IEEE ICASSP, 2014, pp. 5542\u20135546.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Recnorm: Simultaneous normalisation and classification applied to speech recognition", "author": ["JS Bridle", "S Cox"], "venue": "Advances in Neural Information and Processing Systems, 1990.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1990}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O Abdel-Hamid", "H Jiang"], "venue": "Proc. IEEE ICASSP, 2013, pp. 4277\u20134280.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast adaptation of deep neural network based on discriminant codes for speech recognition", "author": ["S Xue", "O Abdel-Hamid", "J Hui", "L Dai", "Q Liu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1713\u20131725, Dec 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint acoustic factor learning for robust deep neural network based automatic speech recognition", "author": ["S Kundu", "G Mantena", "Y Qian", "T Tan", "M Delcroix", "KC Sim"], "venue": "Proc. IEEE ICASSP, March 2016, pp. 5025\u2013 5029.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H Liao"], "venue": "Proc. IEEE ICASSP, 2013, pp. 7947\u20137951.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized sequence-level deep neural network model adaptation", "author": ["Y Huang", "Y Gong"], "venue": "Proc. ICSA Interspeech, 2015, pp. 1081\u20131085.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptive training using deep neural networks", "author": ["T Ochiai", "S Matsuda", "X Lu", "C Hori", "S Katagiri"], "venue": "Proc. IEEE ICASSP, 2014, pp. 6349\u20136353.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Hermitian polynomial for speaker adaptation of connectionist speech recognition systems", "author": ["SM Siniscalchi", "J Li", "CH Lee"], "venue": "IEEE Trans Audio, Speech, and Language Processing, vol. 21, pp. 2152\u20132161, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigating online low-footprint speaker adaptation using generalized linear regression and click-through data", "author": ["Y Zhao", "J Li", "J Xue", "Y Gong"], "venue": "Proc. IEEE ICASSP, 2015, pp. 4310\u20134314.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "SAT-LHUC: Speaker adaptive training for learning hidden unit contributions", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2016, pp. 5010\u20135014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Maximum a-posteriori adaptation of network parameters in deep models", "author": ["Z Huang", "S M Siniscalchi", "I-F Chen", "J Wu", "C-H Lee"], "venue": "arXiv preprint arXiv:1503.02108, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid adaptation for deep neural networks through multi-task learning", "author": ["Z Huang", "J Li", "SM Siniscalchi", "I-F Chen", "J Wu", "C-H Lee"], "venue": "Proc. ICSA Interspeech, 2015, pp. 3625\u20133629.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. 24, NUM. 11  11", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured output layer with auxiliary targets for context-dependent acoustic modelling", "author": ["P Swietojanski", "P Bell", "S Renals"], "venue": "Proc. ICSA Interspeech, 2015, pp. 3605\u20133609.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of deep neural networks using a hierarchy of output layers", "author": ["R Price", "K Iso", "K Shinoda"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Differentiable pooling for unsupervised speaker adaptation", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2015, pp. 4305\u20134309.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Receptive fields, binocular interaction, and functional architecture in the cat\u2019s visual cortex", "author": ["D Hubel", "T Wiesel"], "venue": "Journal of Physiology, vol. 160, pp. 106\u2013154, 1962.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1962}, {"title": "Neocognitron: A new algoriothm for pattern recognition tolerant of deformations", "author": ["K Fukushima", "S Miyake"], "venue": "Pattern Recognition, vol. 15, pp. 455\u2013469, 1982.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1982}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y LeCun", "B Boser", "JS Denker", "D Henderson", "RE Howard", "W Hubbard", "LD Jackel"], "venue": "Neural Computation, vol. 1, pp. 541\u2013551, 1989.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 1998.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1998}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M Riesenhuber", "T Poggio"], "venue": "Nature Neuroscience, vol. 2, pp. 1019\u20131025, 1999.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1999}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["MA Ranzato", "FJ Huang", "Y-L Boureau", "Y LeCun"], "venue": "Proc. IEEE CVPR, 2007.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y-L Boureau", "J Ponce", "Y LeCun"], "venue": "Proc. ICML, 2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Maxout networks", "author": ["IJ Goodfellow", "D Warde-Farley", "M Mirza", "A Courville", "Y Bengio"], "venue": "Proc. ICML, 2013, pp. 1319\u20131327.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout networks for lowresource speech recognition", "author": ["Y Miao", "F Metze", "S Rawat"], "venue": "Proc. IEEE ASRU, 2013.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout neural networks for speech recognition", "author": ["M Cai", "Y Shi", "J Liu"], "venue": "Proc. IEEE ASRU, 2013, pp. 291\u2013296.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of maxout networks for speech recognition", "author": ["P Swietojanski", "J Li", "J-T Huang"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural networks for distant speech recognition", "author": ["S Renals", "P Swietojanski"], "venue": "Proc. HSCMA, 2014.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional deep maxout networks for phone recognition", "author": ["L Toth"], "venue": "Proc. ICSA Interspeech, 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Differentiable pooling for hierarchical feature learning", "author": ["MD Zeiler", "R Fergus"], "venue": "CoRR, vol. abs/1207.0151, 2012.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P Sermanet", "S Chintala", "Y LeCun"], "venue": "CoRR, vol. abs/1204.3968, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Improvements to deep convolutional neural networks for LVCSR", "author": ["TN Sainath", "B Kingsbury", "A Mohamed", "GE Dahl", "G Saon", "H Soltau", "T Beran", "AY Aravkin", "B Ramabhadran"], "venue": "Proc. IEEE ASRU, 2013, pp. 315\u2013320.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving deep neural network acoustic models using generalized maxout networks", "author": ["X Zhang", "J Trmal", "D Povey", "S Khudanpur"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hidden unit contributions for unsupervised acoustic model adaptation", "author": ["P Swietojanski", "J Li", "S Renals"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1450\u2013 1463, 2016.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2016}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["C G\u00fcl\u00e7ehre", "K Cho", "R Pascanu", "Y Bengio"], "venue": "Proc. ECML and KDD. 2014, pp. 530\u2013546, Springer-Verlag.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning internal representations by error-propagation", "author": ["DE Rumelhart", "GE Hinton", "RJ Williams"], "venue": "Parallel Distributed Processing, vol. 1, pp. 318\u2013362. MIT Press, 1986.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 1986}, {"title": "Wit3: Web inventory of transcribed and translated talks", "author": ["M Cettolo", "C Girardi", "M Federico"], "venue": "Proc. EAMT, 2012, pp. 261\u2013268.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["JJ Godfrey", "EC Holliman", "J McDaniel"], "venue": "Proc. IEEE ICASSP. IEEE, 1992, pp. 517\u2013520.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1992}, {"title": "Unleashing the killer corpus: Experiences in creating the multi-everything AMI meeting corpus", "author": ["J Carletta"], "venue": "Language Resources and Evaluation, vol. 41, no. 2, pp. 181\u2013190, 2007.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognition and understanding of meetings: The AMI and AMIDA projects", "author": ["S Renals", "T Hain", "H Bourlard"], "venue": "Proc. IEEE ASRU, Kyoto, 12 2007, IDIAP-RR 07-46.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["GE Dahl", "D Yu", "L Deng", "A Acero"], "venue": "IEEE Transaction on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2012}, {"title": "The UEDIN system for the IWSLT 2014 evaluation", "author": ["P Bell", "P Swietojanski", "J Driesen", "M Sinclair", "F McInnes", "S Renals"], "venue": "Proc. IWSLT, 2014, pp. 26\u201333.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Proc. IEEE ASRU, 2013.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2013}, {"title": "The Fisher corpus: a resource for the next generations of speech-to-text", "author": ["C Cieri", "D Millerand K Walker"], "venue": "Proc LREC, 2004.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2004}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K Vesely", "A Ghoshal", "L Burget", "D Povey"], "venue": "Proc. ICSA Interspeech, 2013, pp. 2345\u20132349.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D Povey", "A Ghoshal", "G Boulianne", "L Burget", "O Glembek", "N Goel", "M Hannemann", "P Motl\u0131\u0301\u010dek", "Y Qian", "P Schwarz", "J Silovsk\u00fd", "G Stemmer", "K Vesel\u00fd"], "venue": "Proc. IEEE ASRU, December 2011.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2011}, {"title": "Connectionist probability estimation in the DECIPHER speech recognition system", "author": ["S Renals", "N Morgan", "M Cohen", "H Franco"], "venue": "Proc. IEEE ICASSP, 1992.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 1992}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N Srivastava", "G Hinton", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1929}, {"title": "Parameterised sigmoid and ReLU hidden activation functions for DNN acoustic modelling", "author": ["C Zhang", "PC Woodland"], "venue": "Proc. ICSA Interspeech, 2015, pp. 3224\u20133228.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2015}, {"title": "A compact model for speaker-adaptive training", "author": ["T Anastasakos", "J McDonough", "R Schwartz", "J Makhoul"], "venue": "Proc. ICSLP, 1996, pp. 1137\u2013 1140.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1996}, {"title": "Cluster adaptive training of hidden markov models", "author": ["MJF Gales"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 8, no. 4, pp. 417\u2013428, 2000.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2000}, {"title": "Discriminative training for large vocabulary speech recognition", "author": ["D Povey"], "venue": "Ph.D. thesis, University of Cambridge,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "DEEP neural network (DNN) acoustic models have significantly extended the state-of-the-art in speech recognition [1] and are known to be able to learn significant invariances through many layers of non-linear transformations [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "DEEP neural network (DNN) acoustic models have significantly extended the state-of-the-art in speech recognition [1] and are known to be able to learn significant invariances through many layers of non-linear transformations [2].", "startOffset": 225, "endOffset": 228}, {"referenceID": 1, "context": "If the training and deployment conditions of the acoustic model are mismatched then the runtime data distribution can differ from the training distribution, bringing a degradation in accuracy, which may be addressed through explicit adaptation to the test conditions [2]\u2013[9].", "startOffset": 267, "endOffset": 270}, {"referenceID": 8, "context": "If the training and deployment conditions of the acoustic model are mismatched then the runtime data distribution can differ from the training distribution, bringing a degradation in accuracy, which may be addressed through explicit adaptation to the test conditions [2]\u2013[9].", "startOffset": 271, "endOffset": 274}, {"referenceID": 9, "context": "speech, and AMI meetings \u2013 presenting results on using differentiable pooling for speaker independent acoustic modelling, followed by unsupervised speaker adaptation experiments in which adaptation of the pooling operators is compared (and combined) with learning hidden unit contributions (LHUC) [10], [11] and constrained/feature-space maximum likelihood linear regression (fMLLR) [12].", "startOffset": 297, "endOffset": 301}, {"referenceID": 10, "context": "speech, and AMI meetings \u2013 presenting results on using differentiable pooling for speaker independent acoustic modelling, followed by unsupervised speaker adaptation experiments in which adaptation of the pooling operators is compared (and combined) with learning hidden unit contributions (LHUC) [10], [11] and constrained/feature-space maximum likelihood linear regression (fMLLR) [12].", "startOffset": 303, "endOffset": 307}, {"referenceID": 11, "context": "speech, and AMI meetings \u2013 presenting results on using differentiable pooling for speaker independent acoustic modelling, followed by unsupervised speaker adaptation experiments in which adaptation of the pooling operators is compared (and combined) with learning hidden unit contributions (LHUC) [10], [11] and constrained/feature-space maximum likelihood linear regression (fMLLR) [12].", "startOffset": 383, "endOffset": 387}, {"referenceID": 12, "context": "DNN acoustic models typically estimate the posterior distribution over a set of context-dependent tied states s of a hidden Markov model (HMM) [13] given an acoustic observation o, P (s|o) = DNN(o;\u03b8) [1], [14], [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "DNN acoustic models typically estimate the posterior distribution over a set of context-dependent tied states s of a hidden Markov model (HMM) [13] given an acoustic observation o, P (s|o) = DNN(o;\u03b8) [1], [14], [15].", "startOffset": 200, "endOffset": 203}, {"referenceID": 13, "context": "DNN acoustic models typically estimate the posterior distribution over a set of context-dependent tied states s of a hidden Markov model (HMM) [13] given an acoustic observation o, P (s|o) = DNN(o;\u03b8) [1], [14], [15].", "startOffset": 205, "endOffset": 209}, {"referenceID": 14, "context": "DNN acoustic models typically estimate the posterior distribution over a set of context-dependent tied states s of a hidden Markov model (HMM) [13] given an acoustic observation o, P (s|o) = DNN(o;\u03b8) [1], [14], [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "To form a probability distribution, the output layer employs a softmax transformation [16] \u03c6i (x) = exp(xi)/ \u2211 j exp(xj), whereas the hidden layer activation functions are typically chosen to be either sigmoid \u03c6(x) = 1/(1 + exp(\u2212x)) or rectified linear \u03c6(x) = max(0, x) units (ReLU) [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "To form a probability distribution, the output layer employs a softmax transformation [16] \u03c6i (x) = exp(xi)/ \u2211 j exp(xj), whereas the hidden layer activation functions are typically chosen to be either sigmoid \u03c6(x) = 1/(1 + exp(\u2212x)) or rectified linear \u03c6(x) = max(0, x) units (ReLU) [17].", "startOffset": 283, "endOffset": 287}, {"referenceID": 1, "context": "Yu et al [2] experimentally demonstrated that the invariance of the internal representations with respect to variabilities in the input space increases with depth (the number of layers) and that the DNN can interpolate well around training samples but fails to extrapolate if the data mismatch increases.", "startOffset": 9, "endOffset": 12}, {"referenceID": 11, "context": "The most effective form of feature-space normalisation is constrained (feature-space) maximum-likelihood linear regression (MLLR), referred to as fMLLR [12], in which the linear transform parameters are estimated by maximising the likelihood of the adaptation data under a Gaussian Mixture", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "This technique has consistently and significantly reduced the word error rate (WER) across several different benchmarks for both hybrid [1], [14] and tandem [18], [19] approaches.", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "This technique has consistently and significantly reduced the word error rate (WER) across several different benchmarks for both hybrid [1], [14] and tandem [18], [19] approaches.", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "This technique has consistently and significantly reduced the word error rate (WER) across several different benchmarks for both hybrid [1], [14] and tandem [18], [19] approaches.", "startOffset": 157, "endOffset": 161}, {"referenceID": 18, "context": "This technique has consistently and significantly reduced the word error rate (WER) across several different benchmarks for both hybrid [1], [14] and tandem [18], [19] approaches.", "startOffset": 163, "endOffset": 167}, {"referenceID": 0, "context": "There are many successful examples of fMLLR adaptation of DNN acoustic models [1], [6], [8], [20]\u2013[25].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "There are many successful examples of fMLLR adaptation of DNN acoustic models [1], [6], [8], [20]\u2013[25].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "There are many successful examples of fMLLR adaptation of DNN acoustic models [1], [6], [8], [20]\u2013[25].", "startOffset": 88, "endOffset": 91}, {"referenceID": 19, "context": "There are many successful examples of fMLLR adaptation of DNN acoustic models [1], [6], [8], [20]\u2013[25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "There are many successful examples of fMLLR adaptation of DNN acoustic models [1], [6], [8], [20]\u2013[25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "One can also estimate the linear transform as an input layer of the DNN, often referred to as a linear input network (LIN) [3], [4], [6], [26].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "One can also estimate the linear transform as an input layer of the DNN, often referred to as a linear input network (LIN) [3], [4], [6], [26].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "One can also estimate the linear transform as an input layer of the DNN, often referred to as a linear input network (LIN) [3], [4], [6], [26].", "startOffset": 133, "endOffset": 136}, {"referenceID": 25, "context": "One can also estimate the linear transform as an input layer of the DNN, often referred to as a linear input network (LIN) [3], [4], [6], [26].", "startOffset": 138, "endOffset": 142}, {"referenceID": 26, "context": "There has been considerable recent work exploring the use of i-vectors [27] for this purpose.", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "I-vectors, which can be regarded as basis vectors spanning a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28], and were later successfully employed for DNN adaptation [29]\u2013[34].", "startOffset": 162, "endOffset": 166}, {"referenceID": 28, "context": "I-vectors, which can be regarded as basis vectors spanning a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28], and were later successfully employed for DNN adaptation [29]\u2013[34].", "startOffset": 224, "endOffset": 228}, {"referenceID": 33, "context": "I-vectors, which can be regarded as basis vectors spanning a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28], and were later successfully employed for DNN adaptation [29]\u2013[34].", "startOffset": 229, "endOffset": 233}, {"referenceID": 34, "context": "Other examples of auxiliary features include the use of speakerspecific bottleneck features obtained from a speaker separation DNN [35], the use of out-of-domain tandem features [24], as well as speaker codes [36]\u2013[38] in which a specific set of units for each speaker is optimised.", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "Other examples of auxiliary features include the use of speakerspecific bottleneck features obtained from a speaker separation DNN [35], the use of out-of-domain tandem features [24], as well as speaker codes [36]\u2013[38] in which a specific set of units for each speaker is optimised.", "startOffset": 178, "endOffset": 182}, {"referenceID": 35, "context": "Other examples of auxiliary features include the use of speakerspecific bottleneck features obtained from a speaker separation DNN [35], the use of out-of-domain tandem features [24], as well as speaker codes [36]\u2013[38] in which a specific set of units for each speaker is optimised.", "startOffset": 209, "endOffset": 213}, {"referenceID": 37, "context": "Other examples of auxiliary features include the use of speakerspecific bottleneck features obtained from a speaker separation DNN [35], the use of out-of-domain tandem features [24], as well as speaker codes [36]\u2013[38] in which a specific set of units for each speaker is optimised.", "startOffset": 214, "endOffset": 218}, {"referenceID": 38, "context": "[39] present an approach using auxiliary input features derived from the bottleneck layer of a DNN which is combined with i-vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Liao [40] investigated this approach in both supervised and unsupervised settings using a few minutes of adaptation data.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Yu et al [9] have explored the use of regularisation for adapting the weights of a DNN, using the Kullback-Liebler (KL) divergence between the output distributions produced by speaker-independent and the speaker-adapted models.", "startOffset": 9, "endOffset": 12}, {"referenceID": 40, "context": "This approach was also recently used to adapt parameters of sequence-trained models [41].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "LIN may also be regarded as a form of model-based adaptation, and related approaches include adaptation using a linear output network (LON) or linear hidden network (LHN) [4], [7], [42].", "startOffset": 171, "endOffset": 174}, {"referenceID": 6, "context": "LIN may also be regarded as a form of model-based adaptation, and related approaches include adaptation using a linear output network (LON) or linear hidden network (LHN) [4], [7], [42].", "startOffset": 176, "endOffset": 179}, {"referenceID": 41, "context": "LIN may also be regarded as a form of model-based adaptation, and related approaches include adaptation using a linear output network (LON) or linear hidden network (LHN) [4], [7], [42].", "startOffset": 181, "endOffset": 185}, {"referenceID": 6, "context": "Smaller subsets of the DNN weights may be modified, including biases and slopes of hidden units [7], [34], [43], [44].", "startOffset": 96, "endOffset": 99}, {"referenceID": 33, "context": "Smaller subsets of the DNN weights may be modified, including biases and slopes of hidden units [7], [34], [43], [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 42, "context": "Smaller subsets of the DNN weights may be modified, including biases and slopes of hidden units [7], [34], [43], [44].", "startOffset": 107, "endOffset": 111}, {"referenceID": 43, "context": "Smaller subsets of the DNN weights may be modified, including biases and slopes of hidden units [7], [34], [43], [44].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "Another recently developed approach relies on learning hidden unit contributions (LHUC) for testonly adaptation [10], [11] as well as in a SAT framework [45].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Another recently developed approach relies on learning hidden unit contributions (LHUC) for testonly adaptation [10], [11] as well as in a SAT framework [45].", "startOffset": 118, "endOffset": 122}, {"referenceID": 44, "context": "Another recently developed approach relies on learning hidden unit contributions (LHUC) for testonly adaptation [10], [11] as well as in a SAT framework [45].", "startOffset": 153, "endOffset": 157}, {"referenceID": 45, "context": "One can also adapt the top layer using Bayesian methods resulting in a maximum a posteriori (MAP) approach [46], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by modelling both monophones and context-dependent tied states using multitask adaptation [47], [48] or a hierarchical output layer [49].", "startOffset": 107, "endOffset": 111}, {"referenceID": 46, "context": "One can also adapt the top layer using Bayesian methods resulting in a maximum a posteriori (MAP) approach [46], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by modelling both monophones and context-dependent tied states using multitask adaptation [47], [48] or a hierarchical output layer [49].", "startOffset": 306, "endOffset": 310}, {"referenceID": 47, "context": "One can also adapt the top layer using Bayesian methods resulting in a maximum a posteriori (MAP) approach [46], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by modelling both monophones and context-dependent tied states using multitask adaptation [47], [48] or a hierarchical output layer [49].", "startOffset": 312, "endOffset": 316}, {"referenceID": 48, "context": "One can also adapt the top layer using Bayesian methods resulting in a maximum a posteriori (MAP) approach [46], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by modelling both monophones and context-dependent tied states using multitask adaptation [47], [48] or a hierarchical output layer [49].", "startOffset": 348, "endOffset": 352}, {"referenceID": 49, "context": "Building on our initial work [50], we present an approach to adaptation by learning hidden layer pooling operators with parameters that can be learned and adapted in a similar way to the other model parameters.", "startOffset": 29, "endOffset": 33}, {"referenceID": 50, "context": "The idea of feature pooling originates from Hubel and Wiesel\u2019s pioneering study on visual cortex in cats [51], and was first used in computer vision to combine spatially local features [52].", "startOffset": 105, "endOffset": 109}, {"referenceID": 51, "context": "The idea of feature pooling originates from Hubel and Wiesel\u2019s pioneering study on visual cortex in cats [51], and was first used in computer vision to combine spatially local features [52].", "startOffset": 185, "endOffset": 189}, {"referenceID": 52, "context": "Fixed poolings are typically used, such as average pooling (used in the original formulation of convolutional neural networks \u2013 CNNs) [53], [54] and max pooling (used in the context of feature hierarchies [55] and later applied to CNNs [56], [57]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 53, "context": "Fixed poolings are typically used, such as average pooling (used in the original formulation of convolutional neural networks \u2013 CNNs) [53], [54] and max pooling (used in the context of feature hierarchies [55] and later applied to CNNs [56], [57]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 54, "context": "Fixed poolings are typically used, such as average pooling (used in the original formulation of convolutional neural networks \u2013 CNNs) [53], [54] and max pooling (used in the context of feature hierarchies [55] and later applied to CNNs [56], [57]).", "startOffset": 205, "endOffset": 209}, {"referenceID": 55, "context": "Fixed poolings are typically used, such as average pooling (used in the original formulation of convolutional neural networks \u2013 CNNs) [53], [54] and max pooling (used in the context of feature hierarchies [55] and later applied to CNNs [56], [57]).", "startOffset": 236, "endOffset": 240}, {"referenceID": 56, "context": "Fixed poolings are typically used, such as average pooling (used in the original formulation of convolutional neural networks \u2013 CNNs) [53], [54] and max pooling (used in the context of feature hierarchies [55] and later applied to CNNs [56], [57]).", "startOffset": 242, "endOffset": 246}, {"referenceID": 57, "context": "Reducing the dimensionality of hidden layers by pooling some subsets of hidden unit activations has become well investigated beyond computer vision, and the max operator has been interpreted as a way to learn piecewise linear activation functions \u2013 referred to as Maxout [58].", "startOffset": 271, "endOffset": 275}, {"referenceID": 58, "context": "Maxout has been widely investigated for both fully-connected [59]\u2013 [61] and convolutional [62], [63] DNN-based acoustic models.", "startOffset": 61, "endOffset": 65}, {"referenceID": 60, "context": "Maxout has been widely investigated for both fully-connected [59]\u2013 [61] and convolutional [62], [63] DNN-based acoustic models.", "startOffset": 67, "endOffset": 71}, {"referenceID": 61, "context": "Maxout has been widely investigated for both fully-connected [59]\u2013 [61] and convolutional [62], [63] DNN-based acoustic models.", "startOffset": 90, "endOffset": 94}, {"referenceID": 62, "context": "Maxout has been widely investigated for both fully-connected [59]\u2013 [61] and convolutional [62], [63] DNN-based acoustic models.", "startOffset": 96, "endOffset": 100}, {"referenceID": 63, "context": "There have been a number of approaches to pooling with differentiable operators \u2013 differentiable pooling \u2013 a notion introduced by Zeiler and Fergus [64] in the context of constructing unsupervised feature extract for support vector machines in computer vision tasks.", "startOffset": 148, "endOffset": 152}, {"referenceID": 56, "context": "There has been some interest in the use of Lp-norm pooling with CNN models [57], [65] in which the sufficient statistic is the p-norm of the group of (spatially-related) hidden unit activations.", "startOffset": 75, "endOffset": 79}, {"referenceID": 64, "context": "There has been some interest in the use of Lp-norm pooling with CNN models [57], [65] in which the sufficient statistic is the p-norm of the group of (spatially-related) hidden unit activations.", "startOffset": 81, "endOffset": 85}, {"referenceID": 65, "context": "Fixed order Lpnorm pooling was recently applied within the context of a convolutional neural network acoustic model [66], where it did not reduce the WER over max-pooling, and as an activation function in a fully-connected DNNs [67], where it was found to improve over maxout and ReLU.", "startOffset": 116, "endOffset": 120}, {"referenceID": 66, "context": "Fixed order Lpnorm pooling was recently applied within the context of a convolutional neural network acoustic model [66], where it did not reduce the WER over max-pooling, and as an activation function in a fully-connected DNNs [67], where it was found to improve over maxout and ReLU.", "startOffset": 228, "endOffset": 232}, {"referenceID": 67, "context": "LHUC scaling follows [68] and is used only during adaptation.", "startOffset": 21, "endOffset": 25}, {"referenceID": 54, "context": "For the case when pk =\u221e we obtain the max-pooling operator [55]:", "startOffset": 59, "endOffset": 63}, {"referenceID": 64, "context": "Serement et al [65] investigated fixed-order Lp pooling for image classification, which was applied to speaker independent acoustic modelling [67].", "startOffset": 15, "endOffset": 19}, {"referenceID": 66, "context": "Serement et al [65] investigated fixed-order Lp pooling for image classification, which was applied to speaker independent acoustic modelling [67].", "startOffset": 142, "endOffset": 146}, {"referenceID": 68, "context": "Here we allow each Lp unit in the model to have a learnable order p [69], and we use the pooling parameters to perform modelbased test-only acoustic adaptation.", "startOffset": 68, "endOffset": 72}, {"referenceID": 49, "context": "0 [50].", "startOffset": 2, "endOffset": 6}, {"referenceID": 69, "context": "We optimise the acoustic model parameters by minimising the negative log probability of the target HMM tied state given the acoustic observations using gradient descent and error back-propagation [70]; the pooling parameters may be updated in a speaker-dependent manner, to adapt the acoustic model to unseen data.", "startOffset": 196, "endOffset": 200}, {"referenceID": 67, "context": "Pooling enables decision boundaries to be altered, through the selection of relevant hidden features, while keeping the parameters of the feature extractors (the hidden units) fixed: this is similar to LHUC adaptation [68].", "startOffset": 218, "endOffset": 222}, {"referenceID": 66, "context": "The distance threshold d is implicitly learned from data (through the a parameters given p), resulting in an efficient representation [67], [69] compared with representing such boundaries using sigmoid units or ReLUs, which would require more parameters.", "startOffset": 134, "endOffset": 138}, {"referenceID": 68, "context": "The distance threshold d is implicitly learned from data (through the a parameters given p), resulting in an efficient representation [67], [69] compared with representing such boundaries using sigmoid units or ReLUs, which would require more parameters.", "startOffset": 140, "endOffset": 144}, {"referenceID": 70, "context": "We have carried out experiments on three corpora: the TED talks corpus [71] following the IWSLT evaluation protocol (www.", "startOffset": 71, "endOffset": 75}, {"referenceID": 71, "context": "org); the Switchboard corpus of conversational telephone speech [72] (ldc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 72, "context": "edu) and the AMI meetings corpus [73], [74] (corpus.", "startOffset": 33, "endOffset": 37}, {"referenceID": 73, "context": "edu) and the AMI meetings corpus [73], [74] (corpus.", "startOffset": 39, "endOffset": 43}, {"referenceID": 74, "context": "The output softmax layer models the distribution of context-dependent clustered tied states [75].", "startOffset": 92, "endOffset": 96}, {"referenceID": 7, "context": "TED: The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "However, compared to our previous work [8], [11], [50], our systems here make use of more accurate language models developed for our IWSLT\u20132014 systems [76]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 39, "endOffset": 42}, {"referenceID": 10, "context": "However, compared to our previous work [8], [11], [50], our systems here make use of more accurate language models developed for our IWSLT\u20132014 systems [76]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 44, "endOffset": 48}, {"referenceID": 49, "context": "However, compared to our previous work [8], [11], [50], our systems here make use of more accurate language models developed for our IWSLT\u20132014 systems [76]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 50, "endOffset": 54}, {"referenceID": 75, "context": "However, compared to our previous work [8], [11], [50], our systems here make use of more accurate language models developed for our IWSLT\u20132014 systems [76]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 152, "endOffset": 156}, {"referenceID": 76, "context": "AMI: We follow a Kaldi GMM recipe [77] and use the individual headset microphone (IHM) recordings.", "startOffset": 34, "endOffset": 38}, {"referenceID": 77, "context": "We decode with a pruned 3-gram language model estimated from 800k words of AMI training transcripts interpolated with an LM trained on Fisher conversational telephone speech transcripts (1M words) [78].", "startOffset": 197, "endOffset": 201}, {"referenceID": 78, "context": "Switchboard (SWBD): We follow a Kaldi GMM recipe [79], [80]1, using Switchboard\u20131 Release 2 (LDC97S62).", "startOffset": 49, "endOffset": 53}, {"referenceID": 79, "context": "Switchboard (SWBD): We follow a Kaldi GMM recipe [79], [80]1, using Switchboard\u20131 Release 2 (LDC97S62).", "startOffset": 55, "endOffset": 59}, {"referenceID": 67, "context": "The structures of the differentiable pooling models were selected such that the number of parameters was comparable to the corresponding baseline DNN models, described in detail in [68].", "startOffset": 181, "endOffset": 185}, {"referenceID": 49, "context": "The Diff-Gauss models had pool sizes set to K = 3 (this was found to work best in our previous work [50]) which (assuming nonoverlapping regions) results in 1175 pooling units per layer.", "startOffset": 100, "endOffset": 104}, {"referenceID": 80, "context": "The learning rate was adjusted using the newbob learning scheme [81] based on the validation frame error rate.", "startOffset": 64, "endOffset": 68}, {"referenceID": 66, "context": "We did not apply post-layer normalisation [67].", "startOffset": 42, "endOffset": 46}, {"referenceID": 81, "context": "0 in this work) [82].", "startOffset": 16, "endOffset": 20}, {"referenceID": 44, "context": "1To stay compatible with our previous adaptation work on Switchboard [45], [68] we are using the older set of Kaldi recipe scripts called s5b, and our baseline results are comparable with the corresponding baseline numbers previously reported.", "startOffset": 69, "endOffset": 73}, {"referenceID": 67, "context": "1To stay compatible with our previous adaptation work on Switchboard [45], [68] we are using the older set of Kaldi recipe scripts called s5b, and our baseline results are comparable with the corresponding baseline numbers previously reported.", "startOffset": 75, "endOffset": 79}, {"referenceID": 66, "context": "al [67] have reported further speaker independent experiments for fixed order Lp units.", "startOffset": 3, "endOffset": 7}, {"referenceID": 66, "context": "This corresponds to previous work [67] in which fixed Lp=2 units tended to obtain lower WER.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "This suggests that it is not necessary to regularise the model carefully (by KullbackLeibler divergence [9], for instance) which is usually required when weights that directly transform the data are adapted.", "startOffset": 104, "endOffset": 107}, {"referenceID": 49, "context": "For Diff-Gauss, it is beneficial to update both \u03bc and \u03b2 (as in [50]), and LHUC was also found to be complementary.", "startOffset": 63, "endOffset": 67}, {"referenceID": 66, "context": "Taking into account the increased training time of Diff-Lp models, one can also consider training fixed order Diff-L2 [67], adapting p using (9).", "startOffset": 118, "endOffset": 122}, {"referenceID": 43, "context": "The adaptation with Diff-Gauss has a similar effect as the adaptation of slopes and amplitudes [44], [83], but adapts K times fewer parameters.", "startOffset": 95, "endOffset": 99}, {"referenceID": 82, "context": "The adaptation with Diff-Gauss has a similar effect as the adaptation of slopes and amplitudes [44], [83], but adapts K times fewer parameters.", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "The WERs are an average over three independent runs, each sampling a different set of adaptation utterances (we did more passes in our previous work [11], [50], however, both LHUC and differentiable pooling operators were not sensitive to this aspect, resulting in small error bars between different results obtained with different random utterances).", "startOffset": 149, "endOffset": 153}, {"referenceID": 49, "context": "The WERs are an average over three independent runs, each sampling a different set of adaptation utterances (we did more passes in our previous work [11], [50], however, both LHUC and differentiable pooling operators were not sensitive to this aspect, resulting in small error bars between different results obtained with different random utterances).", "startOffset": 155, "endOffset": 159}, {"referenceID": 67, "context": "The methods are also not very sensitive to the quality of adaptation targets, they show very similar trends as LHUC, for which exact results for different qualities of adaptation targets resulting from re-scoring adaptation hypotheses with different language models were reported in [68].", "startOffset": 283, "endOffset": 287}, {"referenceID": 33, "context": "For example, Samarakoon and Sim [34] showed that speakeradaptive training with i-vectors gives a comparable results to test-only LHUC using TED data, and Miao et.", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "al [33] suggested that LHUC is better than a standard use of i-vectors (as in Saon TABLE VII SUMMARY WER(%) RESULTS ON SWITCHBOARD EVAL2000", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "[29]) on TED data, with a more sophisticated i-vector post-processing needed to equal LHUC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 83, "context": "In the future, one could investigate extending the proposed techniques to speaker adaptive training (SAT) [84], [85], for example in a similar spirit as proposed in the context of SAT-LHUC [45].", "startOffset": 106, "endOffset": 110}, {"referenceID": 84, "context": "In the future, one could investigate extending the proposed techniques to speaker adaptive training (SAT) [84], [85], for example in a similar spirit as proposed in the context of SAT-LHUC [45].", "startOffset": 112, "endOffset": 116}, {"referenceID": 44, "context": "In the future, one could investigate extending the proposed techniques to speaker adaptive training (SAT) [84], [85], for example in a similar spirit as proposed in the context of SAT-LHUC [45].", "startOffset": 189, "endOffset": 193}, {"referenceID": 78, "context": "In addition it would be interesting to investigate the suitability of adapting pooling regions in the framework of sequence discriminative training [79], [86], [87].", "startOffset": 148, "endOffset": 152}, {"referenceID": 85, "context": "In addition it would be interesting to investigate the suitability of adapting pooling regions in the framework of sequence discriminative training [79], [86], [87].", "startOffset": 154, "endOffset": 158}, {"referenceID": 67, "context": "Our experience of LHUC in this framework [68], together with the observation that the pooling models are not prone to overfitting in the case of small amounts of adaptation data, suggests that adaptation based on differentiable pooling is a promising technique for sequence trained models.", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "We present a deep neural network (DNN) acoustic model that includes parametrised and differentiable pooling operators. Unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator. In particular, we experiment with two types of pooling parametrisations: learned Lp-norm pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker-dependent. We perform investigations using three different large vocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard conversational telephone speech. We demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models, with relative word error rates reductions ranging from 5\u201320% with respect to unadapted systems, which themselves are better than the baseline fully-connected DNNbased acoustic models. We also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other featureand model-space adaptation methods, as well as providing an analysis of the characteristics of each of the proposed approaches.", "creator": "LaTeX with hyperref package"}}}