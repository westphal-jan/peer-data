{"id": "1604.00400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "The most commonly used metric in the evaluation of summaries is the ROUGE family. ROUGE relies solely on lexical overlaps between terms and phrases in sentences, so ROUGE is less effective in cases of terminology variations and paraphrases. Scientific summaries is one such case that differs from general summaries in the domain (e.g. newswire data). We offer a comprehensive analysis of the effectiveness of ROUGE as a yardstick for evaluating scientific summaries; we show that scientific summaries are not particularly reliable, contrary to popular belief; we also show how different variants of the ROUGE summary result in very different correlations with the manual pyramid values. Finally, we propose an alternative metric for evaluating summaries that is based on a high degree of relevance between the written SGE article and written evaluation.", "histories": [["v1", "Fri, 1 Apr 2016 20:06:46 GMT  (33kb,D)", "http://arxiv.org/abs/1604.00400v1", "LREC 2016"]], "COMMENTS": "LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["arman cohan", "nazli goharian"], "accepted": false, "id": "1604.00400"}, "pdf": {"name": "1604.00400.pdf", "metadata": {"source": "CRF", "title": "Revisiting Summarization Evaluation for Scientific Articles", "authors": ["Arman Cohan", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu,", "nazli@ir.cs.georgetown.edu"], "sections": [{"heading": null, "text": "Keywords: Summarization, Evaluation, Scientific articles"}, {"heading": "1. Introduction", "text": "Automatic text summarization has been an active research area in natural language processing for several decades. To compare and evaluate the performance of different summarization systems, the most intuitive approach is assessing the quality of the summaries by human evaluators. However, manual evaluation is expensive and the obtained results are subjective and difficult to reproduce (Giannakopoulos and Karkaletsis, 2013). To address these problems, automatic evaluation measures for summarization have been proposed. ROUGE (Lin, 2004) is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries. It is inspired by the success of a similar metric BLEU (Papineni et al., 2002) which is being used in Machine Translation (MT) evaluation. The main success of ROUGE is due to its high correlation with human assessment scores on standard benchmarks (Lin, 2004). ROUGE has been used as one of the main evaluation metrics in later summarization benchmarks such as TAC1 (Owczarzak and Dang, 2011). Since the establishment of ROUGE, almost all research in text summarization have used this metric as the main means for evaluating the quality of the proposed approaches. The public availability of ROUGE as a toolkit for summarization evaluation has contributed to its wide usage. While ROUGE has originally shown good correlations with human assessments, the study of its effectiveness was only limited to a few benchmarks on news summarization data (DUC2 2001-2003 benchmarks). Since 2003, summarization\n1Text Analysis Conference (TAC) is a series of workshops for evaluating research in Natural Language Processing\nhas grown to much further domains and genres such as scientific documents, social media and question answering. While there is not enough compelling evidence about the effectiveness of ROUGE on these other summarization tasks, published research is almost always evaluated by ROUGE. In addition, ROUGE has a large number of possible variants and the published research often (arbitrarily) reports only a few of these variants. By definition, ROUGE solely relies on lexical overlaps (such as n-gram and sequence overlaps) between the system generated and human written gold-standard summaries. Higher lexical overlaps between the two show that the system generated summary is of higher quality. Therefore, in cases of terminology nuances and paraphrasing, ROUGE is not accurate in estimating the quality of the summary. We study the effectiveness of ROUGE for evaluating scientific summarization. Scientific summarization targets much more technical and focused domains in which the goal is providing summaries for scientific articles. Scientific articles are much different than news articles in elements such as length, complexity and structure. Thus, effective summarization approaches usually have much higher compression rate, terminology variations and paraphrasing (Teufel and Moens, 2002). Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al. (2013), and Cohan and Goharian (2015)). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is ROUGE, as\n2Document Understanding Conference (DUC) was one of NIST workshops that provided infrastructure for evaluation of text summarization methodologies (http://duc.nist.gov/).\nar X\niv :1\n60 4.\n00 40\n0v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n6\nan evaluation metric for scientific summarization? We answer this question by comparing ROUGE scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset1. Results reveal that, contrary to the common belief, correlations between ROUGE and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different ROUGE variants and the manual evaluations which further makes the reliability of ROUGE for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in ROUGE. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores. Our contributions are as follows:\n\u2013 Study the validity of ROUGE as the most widely-used summarization evaluation metric in the context of scientific summarization.\n\u2013 Compare and contrast the performance of all variants of ROUGE in scientific summarization.\n\u2013 Propose an alternative content relevance based evaluation metric for assessing the content quality of the summaries (SERA).\n\u2013 Provide human Pyramid annotations for summaries in TAC 2014 scientific summarization dataset.2"}, {"heading": "2. Summarization evaluation by ROUGE", "text": "ROUGE has been the most widely used family of metrics in summarization evaluation. In the following, we briefly describe the different variants of ROUGE:\n\u2013 ROUGE-N: ROUGE-N was originally a recall oriented metric that considered N-gram recall between a system generated summary and the corresponding gold human summaries. In later versions, in addition to the recall, precision was also considered in ROUGE-N, which is the precision of N-grams in the system generated summary with respect to the gold human summary. To combine both precision and recall, F1 scores are often reported. Common values of N range from 1 to 4.\n\u2013 ROUGE-L: This variant of ROUGE compares the system generated summary and the human generated summary based on the Longest Common Subsequences (LCS) between them. The premise is that, longer LCS between the system and human summaries shows more similarity and therefore higher quality of the system summary.\n1http://www.nist.gov/tac/2014/BiomedSumm/ 2The annotations can be accessed via the following\nrepository: https://github.com/acohan/ TAC-pyramid-Annotations/\n\u2013 ROUGE-W: One problem with ROUGE-L is that all LCS with same lengths are rewarded equally. The LCS can be either related to a consecutive set of words or a long sequence with many gaps. While ROUGE-L treats all sequence matches equally, it makes sense that sequences with many gaps receive lower scores in comparison with consecutive matches. ROUGE-W considers an additional weighting function that awards consecutive matches more than non-consecutive ones.\n\u2013 ROUGE-S: ROUGE-S computes the skip-bigram co-occurrence statistics between the two summaries. It is similar to ROUGE-2 except that it allows gaps between the bigrams by skipping middle tokens.\n\u2013 ROUGE-SU: ROUGE-S does not give any credit to a system generated sentence if the sentence does not have any word pair co-occurring in the reference sentence. To solve this potential problem, ROUGE-SU was proposed which is an extension of ROUGE-S that also considers unigram matches between the two summaries.\nROUGE-L, ROUGE-W, ROUGE-S and ROUGE-SU were later extended to consider both the recall and precision. In calculating ROUGE, stopword removal or stemming can also be considered, resulting in more variants. In the summarization literature, despite the large number of variants of ROUGE, only one or very few of these variants are often chosen (arbitrarily) for evaluation of the quality of the summarization approaches. When ROUGE was proposed, the original variants were only recall-oriented and hence the reported correlation results (Lin, 2004). The later extension of ROUGE family by precision were only reflected in the later versions of the ROUGE toolkit and additional evaluation of its effectiveness was not reported. Nevertheless, later published work in summarization adopted this toolkit for its ready implementation and relatively efficient performance. The original ROUGE metrics show high correlations with human judgments of the quality of summaries on the DUC 2001-2003 benchmarks. However, these benchmarks consist of newswire data and are intrinsically very different than other summarization tasks such as summarization of scientific papers. We argue that ROUGE is not the best metric for all summarization tasks and we propose an alternative metric for evaluation of scientific summarization. The proposed alternative metric shows much higher and more consistent correlations with manual judgments in comparison with the well-established ROUGE."}, {"heading": "3. Summarization Evaluation by Relevance Analysis (SERA)", "text": "ROUGE functions based on the assumption that in order for a summary to be of high quality, it has to share many words or phrases with a human gold summary. However, different terminology may be used to refer to the same concepts and thus relying only on lexical overlaps may underrate content quality scores. To overcome this problem, we propose an approach based on the premise that concepts take meanings\nfrom the context they are in, and that related concepts co-occur frequently. Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts (Turney et al., 2010). The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality. Based on the domain of interest, we first construct an index from a set of articles in the same domain. Since TAC 2014 was focused on summarization in the biomedical domain, our index also comprises of biomedical articles. Given a candidate summary C and a set of gold summaries Gi (i = 1, ...,M ; M is the total number of human summaries), we submit the candidate summary and gold summaries to the search engine as queries and compare their ranked results. Let I = \u3008d1, ..., dN \u3009 be the entire index which comprises of N total documents. Let RC = \u3008d`1 , ..., d`n\u3009 be the ranked list of retrieved documents for candidate summary C, and RGi = \u3008d\n` (i) 1 , ..., d ` (i) n \u3009 the ranked list of results for the gold summary Gi. These lists of results are based on a rank cut-off point n that is a parameter of the system. We provide evaluation results on different choices of cut-off point n in the Section 5. We consider the following two scores: (i) simple intersection and (ii) discounted intersection by rankings. The simple intersection just considers the overlaps of the results in the two ranked lists and ignores the rankings. The discounted ranked scores, on the other hand, penalizes ranking differences between the two result sets. As an example consider the following list of retrieved documents (denoted by dis) for a candidate and a gold summary as queries: Results for candidate summary: \u3008d1, d2, d3, d4\u3009 Results for gold summary: \u3008d3, d2, d1, d4\u3009\nThese two sets of results consist of identical documents but the ranking of the retrieved documents differ. Therefore, the simple intersection method assigns a score of 1.0 while in the discounted ranked score, the score will be less than 1.0 (due to ranking differences between the result lists). We now define the metrics more precisely. Using the above notations, without loss of generality, we assume that |RC | \u2265 |RGi |. SERA is defined as follows:\nSERA = 1\nM M\u2211 i=1 |RC \u2229RGi | |RC |\nTo also account for the ranked position differences, we modify this score to discount rewards based on rank differences. That is, in ideal score, we want search results from candidate summary (RC) to be the same as results for gold-standard summaries (RG) and the rankings of the results also be the same. If the rankings differ, we discount the reward by log of the differences of the ranks. More specifically, the discounted score (SERA-DIS) is defined as:\nSERA-DIS =\nM\u2211 i=1 ( |RC |\u2211 j=1 |RGi |\u2211 k=1\n{ ( 1 log(|j\u2212k|+2) ) if R (j) C = R (k) Gi\n0 otherwise ) M \u00d7Dmax\nwhere, as previously defined, M , RC and RGi are total number of human gold summaries, result list for the candidate summary and result list for the human gold summary, respectively. In addition, R(j)C shows the jth results in the ranked list RC and Dmax is the maximum attainable score used as the normalizing factor. We use elasticsearch1, an open-source search engine, for indexing and querying the articles. For retrieval model, we use the Language Modeling retrieval model with Dirichlet smoothing (Zhai and Lafferty, 2001). Since TAC 2014 benchmark is on summarization of biomedical articles, the appropriate index would be the one constructed from articles in the same domain. Therefore, we use the open access subset of Pubmed2 which consists of published articles in biomedical literature. We also experiment with different query (re)formulation approaches. Query reformulation is a method in Information Retrieval that aims to refine the query for better retrieval of results. Query reformulation methods often consist of removing ineffective terms and expressions from the query (query reduction) or adding terms to the query that help the retrieval (query expansion). Query reduction is specially important when queries are verbose. Since we use the summaries as queries, the queries are usually long and therefore we consider query reductions. In our experiments, the query reformulation is done by 3 different ways: (i) Plain: The entire summary without stopwords and numeric values; (ii) Noun Phrases (NP): We only keep the noun phrases as informative concepts in the summary and eliminate all other terms; and (iii) Keywords\n1https://github.com/elastic/elasticsearch 2PubMed is a comprehensive resource of articles and abstracts published in life sciences and biomedical literature http:// www.ncbi.nlm.nih.gov/pmc/\n(KW): We only keep the keywords and key phrases in the summary. For extracting the keywords and keyphrases (with length of up to 3 terms), we extract expressions whose idf 1 values is higher than a predefined threshold that is set as a parameter. We set this threshold to the average idf values of all terms except stopwords. idf values are calculated on the same index that is used for the retrieval. We hypothesize that using only informative concepts in the summary prevents query drift and leads to retrieval of more relevant documents. Noun phrases and keywords are two heuristics for identifying the informative concepts."}, {"heading": "4. Experimental setup", "text": ""}, {"heading": "4.1. Data", "text": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of ROUGE variants and our metric (SERA), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries."}, {"heading": "4.2. Annotations", "text": "In the TAC 2014 summarization track, ROUGE was suggested as the evaluation metric for summarization and no human assessment was provided for the topics. Therefore, to study the effectiveness of the evaluation metrics, we use the semi-manual Pyramid evaluation framework (Nenkova and Passonneau, 2004; Nenkova et al., 2007). In the pyramid scoring, the content units in the gold human written summaries are organized in a pyramid. In this pyramid, the content units are organized in tiers and higher tiers of the pyramid indicate higher importance. The content quality of a given candidate summary is evaluated with respect to this pyramid. To analyze the quality of the evaluation metrics, following the pyramid framework, we design an annotation scheme that is based on identification of important content units. Consider the following example: Endogeneous small RNAs (miRNA) were genetically screened and studied to find the miRNAs which are related to tumorigenesis. In the above example, the underlined expressions are the content units that convey the main meaning of the text. We call these small units, nuggets which are phrases or concepts that are the main contributors to the content quality of the summary. We asked two human annotators to review the gold summaries and extract content units in these summaries. The pyramid tiers represent the occurrences of nuggets across all the human written gold-standard summaries, and therefore the nuggets are weighted based on these tiers. The intuition is that, if a nugget occurs more frequently in the human summaries, it is a more important contributor (thus belongs to higher tier in the pyramid). Thus, if a candidate summary contains this nugget, it should be rewarded more. An example of the nuggets annotations in pyramid framework is shown in Table 1. In this\n1Inverted Document Frequency\nexample, the nugget \u201ccell mutation\u201d belongs to the 4th tier and it suggests that the \u201ccell mutation\u201d nugget is a very important representative of the content of the corresponding document. Let Ti define the tiers of the pyramid with T1 being the bottom tier and Tn the top tier. Let Ni be the number of the nuggets in the candidate summary that appear in the tier Ti. Then the pyramid score P of the candidate summary will be:\nP = 1\nPmax n\u2211 i=1 i\u00d7Ni\nwhere Pmax is the maximum attainable score used for normalizing the scores:\nPmax = n\u2211 i=j+1 i\u00d7 |Ti|+ j \u00d7 (X \u2212 n\u2211 i=j+1 |Ti|)\nwhereX is the total number of nuggets in the summary and\nj = max i n\u2211 t=i |Tt| \u2265 X . We release the pyramid annotations of the TAC 2014 dataset through a public repository2."}, {"heading": "4.3. Summarization approaches", "text": "We study the effectiveness of ROUGE and our proposed method (SERA) by analyzing the correlations with semi-manual human judgments. Very few teams participated in TAC 2014 summarization track and the official results and the review paper of TAC 2014 systems were never published. Therefore, to evaluate the effectiveness of ROUGE, we applied 9 well-known summarization approaches on the TAC 2014 scientific summarization dataset. Obtained ROUGE and SERA results of each of these approaches are then correlated with semi-manual human judgments. In the following, we briefly describe each of these summarization approaches.\n1. LexRank (Erkan and Radev, 2004): LexRank finds the most important (central) sentences in a document by using random walks in a graph constructed from the document sentences. In this graph, the sentences are nodes\n2https://github.com/acohan/ TAC-pyramid-Annotations\nand the similarity between the sentences determines the edges. Sentences are ranked according to their importance. Importance is measured in terms of centrality of the sentence \u2014 the total number of edges incident on the node (sentence) in the graph. The intuition behind LexRank is that a document can be summarized using the most central sentences in the document that capture its main aspects.\n2. Latent Semantic Analysis (LSA) based summarization (Steinberger and Jezek, 2004): In this summarization method, Singular Value Decomposition (SVD) (Deerwester et al., 1990) is used for deriving latent semantic structure of the document. The document is divided into sentences and a term-sentence matrix A is constructed. The matrix A is then decomposed into a number of linearly-independent singular vectors which represent the latent concepts in the document. This method, intuitively, decomposes the document into several latent topics and then selects the most representative sentences for each of these topics as the summary of the document.\n3. Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998): Maximal Marginal Relevance (MMR) is a greedy strategy for selecting sentences for the summary. Sentences are added iteratively to the summary based on their relatedness to the document as well as their novelty with respect to the current summary.\n4. Citation based summarization (Qazvinian et al., 2013): In this method, citations are used for summarizing an article. Using the LexRank algorithm on the citation network of the article, top sentences are selected for the final summary.\n5. Using frequency of the words (Luhn, 1958): In this method, which is one the earliest works in text summarization, raw word frequencies are used to estimate the saliency of sentences in the document. The most salient sentences are chosen for the final summary.\n6. SumBasic (Vanderwende et al., 2007): SumBasic is an approach that weights sentences based on the distribution of words that is derived from the document. Sentence selection is applied iteratively by selecting words with highest probability and then finding the highest scoring sentence that contains that word. The word weights are updated after each iteration to prevent selection of similar sentences.\n7. Summarization using citation-context and discourse structure (Cohan and Goharian, 2015): In this method, the set of citations to the article are used to find the article sentences that directly reflect those citations (citation-contexts). In addition, the scientific discourse of the article is utilized to capture different aspects of the article. The scientific discourse usually follows a structure in which the authors first describe their hypothesis, then the methods, experiment, results and implications. Sentence selection is based on finding the most important sentences in each of the discourse facets of the document using the MMR heuristic.\n8. KL Divergence (Haghighi and Vanderwende, 2009) In this method, the document unigram distribution P and the summary unigram distributation Q are considered; the goal is to find a summary whose distribution is very close to the document distribution. The difference of the distributions is captured by the Kullback-Lieber (KL) divergence, denoted by KL(P ||Q).\n9. Summarization based on Topic Models (Haghighi and Vanderwende, 2009): Instead of using unigram distributions for modeling the content distribution of the document and the summary, this method models the document content using an LDA based topic model (Blei et al., 2003). It then uses the KL divergence between the document and the summary content models for selecting sentences for the summary."}, {"heading": "5. Results and Discussion", "text": "We calculated all variants of ROUGE scores, our proposed metric, SERA, and the Pyramid score on the generated summaries from the summarizers described in Section 4.3.. We do not report the ROUGE, SERA or pyramid scores of individual systems as it is not the focus of this study.\nOur aim is to analyze the effectiveness of the evaluation metrics, not the summarization approaches. Therefore, we consider the correlations of the automatic evaluation metrics with the manual Pyramid scores to evaluate their effectiveness; the metrics that show higher correlations with manual judgments are more effective. Table 2 shows the Pearson, Spearman and Kendall correlation of ROUGE and SERA, with pyramid scores. Both ROUGE and SERA are calculated with stopwords removed and with stemming. Our experiments with inclusion of stopwords and without stemming showed similar results and thus, we do not include those to avoid redundancy."}, {"heading": "5.1. SERA", "text": "The results of our proposed method (SERA) are shown in the bottom part of Table 2. In general, SERA shows better correlation with pyramid scores in comparison with ROUGE. We observe that the Pearson correlation of SERA with cut-off point of 5 (shown by SERA-5) is 0.823 which is higher than most of the ROUGE variants. Similarly, the Spearman and Kendall correlations of the SERA evaluation score is 0.941 and 0.857 respectively, which are higher than all ROUGE correlation values. This shows the effectiveness of the simple variant of our proposed summarization evaluation metric. Table 2 also shows the results of other SERA variants including discounting and query reformulation methods. Some of these variants are the result of applying query reformulation in the process of document retrieval which are described in section 3. As illustrated, the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as SERA-NP-5) achieves the highest correlations among all the SERA variants (r = 0.859, \u03c1 = \u03c4 = 1.0). In the case of Keywords (KW) query reformulation, without using discounting, we can see that there is no positive gain in correlation. However, keywords when applied on the discounted variant of SERA, result in higher correlations. Discounting has more positive effect when applied on query reformulation-based SERA than on the simple variant of SERA. In the case of discounting and NP query reformulation (SERA-DIS-NP), we observe higher correlations in comparison with simple SERA. Similarly, in the case of Keywords (KW), positive correlation gain is obtained in most of correlation coefficients. NP without discounting and at cut-off point of 5 (SERA-NP-5) shows the highest non-parametric correlation. In addition, the discounted NP at cut-off point of 10 (SERA-NP-DIS-10) shows the highest parametric correlations. In general, using NP and KW as heuristics for finding the informative concepts in the summary effectively increases the correlations with the manual scores. Selecting informative terms from long queries results in more relevant documents and prevents query drift. Therefore, the overall similarity between the two summaries (candidate and the human written gold summary) is better captured."}, {"heading": "5.2. ROUGE", "text": "Another important observation is regarding the effectiveness of ROUGE scores (top part of Table 2).\nInterestingly, we observe that many variants of ROUGE scores do not have high correlations with human pyramid scores. The lowest F-score correlations are for ROUGE-1 and ROUGE-L (with r=0.454). Weak correlation of ROUGE-1 shows that matching unigrams between the candidate summary and gold summaries is not accurate in quantifying the quality of the summary. On higher order n-grams, however, we can see that ROUGE correlates better with pyramid. In fact, the highest overall r is obtained by ROUGE-3. ROUGE-L and its weighted version ROUGE-W, both have weak correlations with pyramid. Skip-bigrams (ROUGE-S) and its combination with unigrams (ROUGE-SU) also show sub-optimal correlations. Note that \u03c1 and \u03c4 correlations are more reliable in our setup due to the small sample size. These results confirm our initial hypothesis that ROUGE is not accurate estimator of the quality of the summary in scientific summarization. We attribute this to the differences of scientific summarization with general domain summaries. When humans summarize a relatively long research paper, they might use different terminology and paraphrasing. Therefore, ROUGE which only relies on term matching between a candidate and a gold summary, is not accurate in quantifying the quality of the candidate summary."}, {"heading": "5.3. Correlation of SERA with ROUGE", "text": "Table 3 shows correlations of our metric SERA with ROUGE-2 and ROUGE-3, which are the highest correlated ROUGE variants with pyramid. We can see that in general, the correlation is not strong. Keyword based reduction variants are the only variants for which the correlation with ROUGE is high. Looking at the correlations of KW variants of SERA with pyramid (Table 2, bottom part), we observe that these variants are also highly correlated with manual evaluation."}, {"heading": "5.4. Effect of the rank cut-off point", "text": "Finally, Figure 1 shows \u03c1 correlation of different variants of SERA with pyramid based on selection of different cut-off points (r and \u03c4 correlations result in very similar\ngraphs). When the cut-off point increases, more documents are retrieved for the candidate and the gold summaries, and therefore the final SERA score is more fine-grained. A general observation is that as the search cut-off point increases, the correlation with pyramid scores decreases. This is because when the retrieved result list becomes larger, the probability of including less related documents increases which negatively affects correct estimation of the similarity of the candidate and gold summaries. The most accurate estimations are for metrics with cut-off points of 5 and 10 which are included in the reported results of all variants in Table 2."}, {"heading": "6. Related work", "text": "ROUGE (Lin, 2004) assesses the content quality of a candidate summary with respect to a set of human gold summaries based on their lexical overlaps. ROUGE consists of several variants. Since its introduction, ROUGE has been one of the most widely reported metrics in the summarization literature, and its high adoption has been due to its high correlation with human assessment scores in DUC datasets (Lin, 2004). However, later research has casted doubts about the accuracy of ROUGE against manual evaluations. Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers.\nSince introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries. BEs were extracted based on syntactic structure of the sentence. The work by Conroy et al. (2011) was another attempt for improving ROUGE for update summarization which combined two different ROUGE variants and showed higher correlations with manual judgments for TAC 2008 update summaries. Apart from the content, other aspects of summarization such as linguistic quality have been also studied. Pitler et al. (2010) evaluated a set of models based on syntactic features, language models and entity coherences for assessing the linguistic quality of the summaries. Machine translation evaluation metrics such as BLUE have also been compared and contrasted against ROUGE (Graham, 2015). Despite these works, when gold-standard summaries are available, ROUGE is still the most common evaluation metric that is used in the summarization published research. Apart from ROUGE\u2019s initial good results on the newswire data, the availability of the software and its efficient performance have further contributed to its popularity."}, {"heading": "7. Conclusions", "text": "We provided an analysis of existing evaluation metrics for scientific summarization with evaluation of all variants of ROUGE. We showed that ROUGE may not be the best metric for summarization evaluation; especially in summaries with high terminology variations and paraphrasing (e.g. scientific summaries). Furthermore, we showed that different variants of ROUGE result in different correlation values with human judgments, indicating that not all ROUGE scores are equally effective. Among all variants of ROUGE, ROUGE-2 and ROUGE-3 are better correlated with manual judgments in the context of scientific summarization. We furthermore proposed an alternative and more effective approach for scientific summarization evaluation (Summarization Evaluation by Relevance Analysis - SERA). Results revealed that in general, the proposed evaluation metric achieves higher correlations with semi-manual pyramid evaluation scores in comparison with ROUGE. Our analysis on the effectiveness of evaluation measures for scientific summaries was performed using correlations with manual judgments. An alternative approach to follow would be to use statistical significance testing on the ability of the metrics to distinguish between the summarizers (similar to Rankel et al. (2011)). We studied the effectiveness of existing summarization evaluation metrics in the scientific text genre and proposed an alternative superior metric. Another extension of this work would be to evaluate automatic summarization evaluation in other genres of text (such as social media). Our proposed method only evaluates the content quality of the summary. Similar to most of existing summarization evaluation metrics, other qualities such as linguistic cohesion, coherence and readability are not captured by this method. Developing metrics that also incorporate these qualities is yet another future direction to follow."}, {"heading": "Acknowledgments", "text": "We would like to thank all three anonymous reviewers for their feedback and comments, and Maryam Iranmanesh for helping in annotation. This work was partially supported by National Science Foundation (NSF) through grant CNS-1204347."}, {"heading": "8. Bibliographical References", "text": "Abu-Jbara, A. and Radev, D. (2011). Coherent\ncitation-based summarization of scientific papers. In ACL \u201911, pages 500\u2013509. Association for Computational Linguistics. Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022. Carbonell, J. and Goldstein, J. (1998). The use of mmr, diversity-based reranking for reordering documents and producing summaries. In SIGIR, pages 335\u2013336. ACM. Cohan, A. and Goharian, N. (2015). Scientific article summarization using citation context and article\u2019s discourse structure. In EMNLP, pages 390\u2013400. Association for Computational Linguistics. Conroy, J. M. and Dang, H. T. (2008). Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 145\u2013152. Association for Computational Linguistics. Conroy, J. M., Schlesinger, J. D., and O\u2019Leary, D. P. (2011). Nouveau-rouge: A novelty metric for update summarization. Computational Linguistics, 37(1):1\u20138. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391. Erkan, G. and Radev, D. R. (2004). Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457\u2013479. Giannakopoulos, G. and Karkaletsis, V. (2013). Summary evaluation: Together we stand npower-ed. In Computational Linguistics and Intelligent Text Processing, pages 436\u2013450. Springer. Graham, Y. (2015). Re-evaluating automatic summarization with bleu and 192 shades of rouge. In EMNLP \u201915\u2019, pages 128\u2013137, Lisbon, Portugal, September. Association for Computational Linguistics. Haghighi, A. and Vanderwende, L. (2009). Exploring content models for multi-document summarization. In NAACL-HLT \u201909, pages 362\u2013370. Association for Computational Linguistics. Hovy, E., Lin, C.-Y., Zhou, L., and Fukumoto, J. (2006). Automated summarization evaluation with basic elements. In LREC \u201906, pages 604\u2013611. Citeseer. Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8.\nLuhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of research and development, 2(2):159\u2013165. Nenkova, A. and Passonneau, R. (2004). Evaluating content selection in summarization: The pyramid method. In Proceedings of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004. Nenkova, A., Passonneau, R., and McKeown, K. (2007). The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing (TSLP), 4(2):4. Owczarzak, K. and Dang, H. T. (2011). Overview of the tac 2011 summarization track: Guided task and aesop task. In TAC 2011. Owczarzak, K., Conroy, J. M., Dang, H. T., and Nenkova, A. (2012). An assessment of the accuracy of automatic evaluation in summarization. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1\u20139. Association for Computational Linguistics. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In ACL \u201902, pages 311\u2013318. Association for Computational Linguistics. Pitler, E., Louis, A., and Nenkova, A. (2010). Automatic evaluation of linguistic quality in multi-document summarization. In Proceedings of the ACL 2010, pages 544\u2013554. Association for Computational Linguistics. Qazvinian, V., Radev, D. R., Mohammad, S., Dorr, B. J., Zajic, D. M., Whidby, M., and Moon, T. (2013). Generating extractive summaries of scientific paradigms. J. Artif. Intell. Res.(JAIR), 46:165\u2013201. Rankel, P., Conroy, J. M., Slud, E. V., and O\u2019Leary, D. P. (2011). Ranking human and machine summarization systems. EMNLP \u201911, pages 467\u2013473, Stroudsburg, PA, USA. Association for Computational Linguistics. Steinberger, J. and Jezek, K. (2004). Using latent semantic analysis in text summarization and summary evaluation. In Proc. ISIM\u201904, pages 93\u2013100. Teufel, S. and Moens, M. (2002). Summarizing scientific articles: experiments with relevance and rhetorical status. Computational linguistics, 28(4):409\u2013445. Turney, P. D., Pantel, P., et al. (2010). From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141\u2013188. Vanderwende, L., Suzuki, H., Brockett, C., and Nenkova, A. (2007). Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion. Information Processing & Management, 43(6):1606\u20131618. Zhai, C. and Lafferty, J. (2001). A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334\u2013342. ACM."}], "references": [{"title": "Coherent citation-based summarization of scientific papers", "author": ["A. Abu-Jbara", "D. Radev"], "venue": "ACL \u201911, pages 500\u2013509. Association for Computational Linguistics.", "citeRegEx": "Abu.Jbara and Radev,? 2011", "shortCiteRegEx": "Abu.Jbara and Radev", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "SIGIR, pages 335\u2013336. ACM.", "citeRegEx": "Carbonell and Goldstein,? 1998", "shortCiteRegEx": "Carbonell and Goldstein", "year": 1998}, {"title": "Scientific article summarization using citation context and article\u2019s discourse structure", "author": ["A. Cohan", "N. Goharian"], "venue": "EMNLP, pages 390\u2013400. Association for Computational Linguistics.", "citeRegEx": "Cohan and Goharian,? 2015", "shortCiteRegEx": "Cohan and Goharian", "year": 2015}, {"title": "Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality", "author": ["J.M. Conroy", "H.T. Dang"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 145\u2013152. Association for", "citeRegEx": "Conroy and Dang,? 2008", "shortCiteRegEx": "Conroy and Dang", "year": 2008}, {"title": "Nouveau-rouge: A novelty metric for update summarization", "author": ["J.M. Conroy", "J.D. Schlesinger", "D.P. O\u2019Leary"], "venue": "Computational Linguistics,", "citeRegEx": "Conroy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Conroy et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res.(JAIR), 22(1):457\u2013479.", "citeRegEx": "Erkan and Radev,? 2004", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "Summary evaluation: Together we stand npower-ed", "author": ["G. Giannakopoulos", "V. Karkaletsis"], "venue": "Computational Linguistics and Intelligent Text Processing, pages 436\u2013450. Springer.", "citeRegEx": "Giannakopoulos and Karkaletsis,? 2013", "shortCiteRegEx": "Giannakopoulos and Karkaletsis", "year": 2013}, {"title": "Re-evaluating automatic summarization with bleu and 192 shades of rouge", "author": ["Y. Graham"], "venue": "EMNLP \u201915\u2019, pages 128\u2013137, Lisbon, Portugal, September. Association for Computational Linguistics.", "citeRegEx": "Graham,? 2015", "shortCiteRegEx": "Graham", "year": 2015}, {"title": "Exploring content models for multi-document summarization", "author": ["A. Haghighi", "L. Vanderwende"], "venue": "NAACL-HLT \u201909, pages 362\u2013370. Association for Computational Linguistics.", "citeRegEx": "Haghighi and Vanderwende,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende", "year": 2009}, {"title": "Automated summarization evaluation with basic elements", "author": ["E. Hovy", "Lin", "C.-Y.", "L. Zhou", "J. Fukumoto"], "venue": "LREC \u201906, pages 604\u2013611. Citeseer.", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8.", "citeRegEx": "Lin and C..Y.,? 2004", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "The automatic creation of literature abstracts", "author": ["H.P. Luhn"], "venue": "IBM Journal of research and development, 2(2):159\u2013165.", "citeRegEx": "Luhn,? 1958", "shortCiteRegEx": "Luhn", "year": 1958}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["A. Nenkova", "R. Passonneau"], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004.", "citeRegEx": "Nenkova and Passonneau,? 2004", "shortCiteRegEx": "Nenkova and Passonneau", "year": 2004}, {"title": "The pyramid method: Incorporating human content selection variation in summarization evaluation", "author": ["A. Nenkova", "R. Passonneau", "K. McKeown"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(2):4.", "citeRegEx": "Nenkova et al\\.,? 2007", "shortCiteRegEx": "Nenkova et al\\.", "year": 2007}, {"title": "Overview of the tac 2011 summarization track: Guided task and aesop task", "author": ["K. Owczarzak", "H.T. Dang"], "venue": "TAC 2011.", "citeRegEx": "Owczarzak and Dang,? 2011", "shortCiteRegEx": "Owczarzak and Dang", "year": 2011}, {"title": "An assessment of the accuracy of automatic evaluation in summarization", "author": ["K. Owczarzak", "J.M. Conroy", "H.T. Dang", "A. Nenkova"], "venue": "Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1\u20139.", "citeRegEx": "Owczarzak et al\\.,? 2012", "shortCiteRegEx": "Owczarzak et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "ACL \u201902, pages 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Automatic evaluation of linguistic quality in multi-document summarization", "author": ["E. Pitler", "A. Louis", "A. Nenkova"], "venue": "Proceedings of the ACL 2010, pages 544\u2013554. Association for Computational Linguistics.", "citeRegEx": "Pitler et al\\.,? 2010", "shortCiteRegEx": "Pitler et al\\.", "year": 2010}, {"title": "Generating extractive summaries of scientific paradigms", "author": ["V. Qazvinian", "D.R. Radev", "S. Mohammad", "B.J. Dorr", "D.M. Zajic", "M. Whidby", "T. Moon"], "venue": "J. Artif. Intell. Res.(JAIR), 46:165\u2013201.", "citeRegEx": "Qazvinian et al\\.,? 2013", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2013}, {"title": "Ranking human and machine summarization systems", "author": ["P. Rankel", "J.M. Conroy", "E.V. Slud", "D.P. O\u2019Leary"], "venue": "EMNLP \u201911,", "citeRegEx": "Rankel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rankel et al\\.", "year": 2011}, {"title": "Using latent semantic analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. ISIM\u201904, pages 93\u2013100.", "citeRegEx": "Steinberger and Jezek,? 2004", "shortCiteRegEx": "Steinberger and Jezek", "year": 2004}, {"title": "Summarizing scientific articles: experiments with relevance and rhetorical status", "author": ["S. Teufel", "M. Moens"], "venue": "Computational linguistics, 28(4):409\u2013445.", "citeRegEx": "Teufel and Moens,? 2002", "shortCiteRegEx": "Teufel and Moens", "year": 2002}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "Information Processing & Management, 43(6):1606\u20131618.", "citeRegEx": "Vanderwende et al\\.,? 2007", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2007}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334\u2013342.", "citeRegEx": "Zhai and Lafferty,? 2001", "shortCiteRegEx": "Zhai and Lafferty", "year": 2001}], "referenceMentions": [{"referenceID": 8, "context": "However, manual evaluation is expensive and the obtained results are subjective and difficult to reproduce (Giannakopoulos and Karkaletsis, 2013).", "startOffset": 107, "endOffset": 145}, {"referenceID": 18, "context": "It is inspired by the success of a similar metric BLEU (Papineni et al., 2002) which is being used in Machine Translation (MT) evaluation.", "startOffset": 55, "endOffset": 78}, {"referenceID": 16, "context": "ROUGE has been used as one of the main evaluation metrics in later summarization benchmarks such as TAC1 (Owczarzak and Dang, 2011).", "startOffset": 105, "endOffset": 131}, {"referenceID": 23, "context": "Thus, effective summarization approaches usually have much higher compression rate, terminology variations and paraphrasing (Teufel and Moens, 2002).", "startOffset": 124, "endOffset": 148}, {"referenceID": 0, "context": "Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al.", "startOffset": 90, "endOffset": 117}, {"referenceID": 0, "context": "Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al. (2013), and Cohan and Goharian (2015)).", "startOffset": 90, "endOffset": 142}, {"referenceID": 0, "context": "Scientific summarization has attracted more attention recently (examples include works by Abu-Jbara and Radev (2011), Qazvinian et al. (2013), and Cohan and Goharian (2015)).", "startOffset": 90, "endOffset": 173}, {"referenceID": 26, "context": "For retrieval model, we use the Language Modeling retrieval model with Dirichlet smoothing (Zhai and Lafferty, 2001).", "startOffset": 91, "endOffset": 116}, {"referenceID": 14, "context": "Therefore, to study the effectiveness of the evaluation metrics, we use the semi-manual Pyramid evaluation framework (Nenkova and Passonneau, 2004; Nenkova et al., 2007).", "startOffset": 117, "endOffset": 169}, {"referenceID": 15, "context": "Therefore, to study the effectiveness of the evaluation metrics, we use the semi-manual Pyramid evaluation framework (Nenkova and Passonneau, 2004; Nenkova et al., 2007).", "startOffset": 117, "endOffset": 169}, {"referenceID": 7, "context": "LexRank (Erkan and Radev, 2004): LexRank finds the most important (central) sentences in a document by using random walks in a graph constructed from the document sentences.", "startOffset": 8, "endOffset": 31}, {"referenceID": 22, "context": "Latent Semantic Analysis (LSA) based summarization (Steinberger and Jezek, 2004): In this summarization method, Singular Value Decomposition (SVD) (Deerwester et al.", "startOffset": 51, "endOffset": 80}, {"referenceID": 6, "context": "Latent Semantic Analysis (LSA) based summarization (Steinberger and Jezek, 2004): In this summarization method, Singular Value Decomposition (SVD) (Deerwester et al., 1990) is used for deriving latent semantic structure of the document.", "startOffset": 147, "endOffset": 172}, {"referenceID": 2, "context": "Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998): Maximal Marginal Relevance (MMR) is a greedy strategy for selecting sentences for the summary.", "startOffset": 33, "endOffset": 64}, {"referenceID": 20, "context": "Citation based summarization (Qazvinian et al., 2013): In this method, citations are used for summarizing an article.", "startOffset": 29, "endOffset": 53}, {"referenceID": 13, "context": "Using frequency of the words (Luhn, 1958): In this method, which is one the earliest works in text summarization, raw word frequencies are used to estimate the saliency of sentences in the document.", "startOffset": 29, "endOffset": 41}, {"referenceID": 25, "context": "SumBasic (Vanderwende et al., 2007): SumBasic is an approach that weights sentences based on the distribution of words that is derived from the document.", "startOffset": 9, "endOffset": 35}, {"referenceID": 3, "context": "Summarization using citation-context and discourse structure (Cohan and Goharian, 2015): In this method,", "startOffset": 61, "endOffset": 87}, {"referenceID": 10, "context": "KL Divergence (Haghighi and Vanderwende, 2009) In this method, the document unigram distribution P and the summary unigram distributation Q are considered; the goal is to find a summary whose distribution is very close to the document distribution.", "startOffset": 14, "endOffset": 46}, {"referenceID": 10, "context": "Summarization based on Topic Models (Haghighi and Vanderwende, 2009): Instead of using unigram distributions for modeling the content distribution of the document and the summary, this method models the document content using an LDA based topic model (Blei et al.", "startOffset": 36, "endOffset": 68}, {"referenceID": 1, "context": "Summarization based on Topic Models (Haghighi and Vanderwende, 2009): Instead of using unigram distributions for modeling the content distribution of the document and the summary, this method models the document content using an LDA based topic model (Blei et al., 2003).", "startOffset": 251, "endOffset": 270}, {"referenceID": 9, "context": "Machine translation evaluation metrics such as BLUE have also been compared and contrasted against ROUGE (Graham, 2015).", "startOffset": 105, "endOffset": 119}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries.", "startOffset": 0, "endOffset": 505}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers.", "startOffset": 0, "endOffset": 795}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers. Since introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries.", "startOffset": 0, "endOffset": 1041}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers. Since introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries. BEs were extracted based on syntactic structure of the sentence. The work by Conroy et al. (2011) was another attempt for improving ROUGE for update summarization which combined two different ROUGE variants and showed higher correlations with manual judgments for TAC 2008 update summaries.", "startOffset": 0, "endOffset": 1260}, {"referenceID": 4, "context": "Conroy and Dang (2008) analyzed DUC 2005 to 2007 data and showed that while some systems achieve high ROUGE scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high ROUGE scores. We studied the effectiveness of ROUGE through correlation analysis with manual scores. Besides correlation with human assessment scores, other approaches have been explored for analyzing the effectiveness of summarization evaluation. Rankel et al. (2011) studied the extent to which a metric can distinguish between the human and system generated summaries. They also proposed the use of paired two-sample t-tests and the Wilcoxon signed-rank test as an alternative to ROUGE in evaluating several summarizers. Similarly, Owczarzak et al. (2012) proposed the use of multiple binary significance tests between the system summaries for ranking the best summarizers. Since introduction of ROUGE, there have been other efforts for improving automatic summarization evaluation. Hovy et al. (2006) proposed an approach based on comparison of so called Basic Elements (BE) between the candidate and reference summaries. BEs were extracted based on syntactic structure of the sentence. The work by Conroy et al. (2011) was another attempt for improving ROUGE for update summarization which combined two different ROUGE variants and showed higher correlations with manual judgments for TAC 2008 update summaries. Apart from the content, other aspects of summarization such as linguistic quality have been also studied. Pitler et al. (2010) evaluated a set of models based on syntactic features, language models and entity coherences for assessing the linguistic quality of the summaries.", "startOffset": 0, "endOffset": 1580}, {"referenceID": 21, "context": "An alternative approach to follow would be to use statistical significance testing on the ability of the metrics to distinguish between the summarizers (similar to Rankel et al. (2011)).", "startOffset": 164, "endOffset": 185}], "year": 2016, "abstractText": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE\u2019s effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "creator": "LaTeX with hyperref package"}}}