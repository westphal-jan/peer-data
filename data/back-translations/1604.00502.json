{"id": "1604.00502", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Online Updating of Word Representations for Part-of-Speech Tagging", "abstract": "We propose an online adaptation of Unattended Domains (DA) that occurs incrementally as soon as the data arrives and is applicable when a batch DA is not possible. In an evaluation of Part-of-Speech (POS) tagging, we find that online unattended DA performs just as well as Batch DA.", "histories": [["v1", "Sat, 2 Apr 2016 13:52:23 GMT  (758kb,D)", "http://arxiv.org/abs/1604.00502v1", "EMNLP'2015. Released POS tagger \"FLORS\" for online domain adaptation"]], "COMMENTS": "EMNLP'2015. Released POS tagger \"FLORS\" for online domain adaptation", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin 0001", "tobias schnabel", "hinrich sch\u00fctze"], "accepted": true, "id": "1604.00502"}, "pdf": {"name": "1604.00502.pdf", "metadata": {"source": "CRF", "title": "Online Updating of Word Representations for Part-of-Speech Tagging", "authors": ["Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de", "tbs49@cornell.edu", "inquiries@cislmu.org"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised domain adaptation is a scenario that practitioners often face when having to build robust NLP systems. They have labeled data in the source domain, but wish to improve performance in the target domain by making use of unlabeled data alone. Most work on unsupervised domain adaptation in NLP uses batch learning: It assumes that a large corpus of unlabeled data of the target domain is available before testing. However, batch learning is not possible in many real-world scenarios where incoming data from a new target domain must be processed immediately. More importantly, in many real-world scenarios the data does not come with neat domain labels and it may not be immediately obvious that an input stream is suddenly delivering data from a new domain.\nConsider an NLP system that analyzes emails at an enterprise. There is a constant stream of incoming emails and it changes over time \u2013 without any clear indication that the models in use should be adapted to the new data distribution. Because the system needs to work in real-time, it is also desirable to do any adaptation of the system online, without the need of stopping the system, changing it and restarting it as is done in batch mode.\nIn this paper, we propose online unsupervised domain adaptation as an extension to traditional unsupervised DA. In online unsupervised DA, domain adaptation is performed incrementally as data comes in. Specifically, we adopt a form of\nrepresentation learning. In our experiments, the incremental updating will be performed for representations of words. Each time a word is encountered in the stream of data at test time, its representation is updated.\nTo the best of our knowledge, the work reported here is the first study of online unsupervised DA. More specifically, we evaluate online unsupervised DA for the task of POS tagging. We compare POS tagging results for three distinct approaches: static (the baseline), batch learning and online unsupervised DA. Our results show that online unsupervised DA is comparable in performance to batch learning while requiring no retraining or prior data in the target domain."}, {"heading": "2 Experimental setup", "text": "Tagger. We reimplemented the FLORS tagger (Schnabel and Schu\u0308tze, 2014), a fast and simple tagger that performs well in DA. It treats POS tagging as a window-based (as opposed to sequence classification), multilabel classification problem. FLORS is ideally suited for online unsupervised DA because its representation of words includes distributional vectors \u2013 these vectors can be easily updated in both batch learning and online unsupervised DA. More specifically, a word\u2019s representation in FLORS consists of four feature vectors: one each for its suffix, its shape and its left and right distributional neighbors. Suffix and shape features are standard features used in the literature; our use of them is exactly as described by Schnabel and Schu\u0308tze (2014).\nDistributional features. The ith entry xi of the left distributional vector of w is the weighted number of times the indicator word ci occurs immediately to the left of w:\nxi = tf (freq (bigram(ci, w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci, w)) is the number of occurrences of the bigram \u201cci w\u201d and we weight non-\nar X\niv :1\n60 4.\n00 50\n2v 1\n[ cs\n.C L\n] 2\nA pr\n2 01\n6\nzero frequencies logarithmically: tf(x) = 1 + log(x). The right distributional vector is defined analogously. We restrict the set of indicator words to the n = 500 most frequent words. To avoid zero vectors, we add an entry xn+1 to each vector that counts omitted contexts:\nx501 = tf( \u2211 j:j>n freq (bigram(cj , w)))\nLet f(w) be the concatentation of the two distributional and suffix and shape vectors of word w. Then FLORS represents token vi as follows:\nf(vi\u22122)\u2295f(vi\u22121)\u2295f(vi)\u2295f(vi+1)\u2295f(vi+2) where \u2295 is vector concatenation. FLORS then tags token vi based on this representation.\nFLORS assumes that the association between distributional features and labels does not change fundamentally when going from source to target. This is in contrast to other work, notably Blitzer et al. (2006), that carefully selects \u201cstable\u201d distributional features and discards \u201cunstable\u201d distributional features. The hypothesis underlying FLORS is that basic distributional POS properties are relatively stable across domains \u2013 in contrast to semantic and other more complex tasks. The high performance of FLORS (Schnabel and Schu\u0308tze, 2014) suggests this hypothesis is true.\nData. Test set. We evaluate on the development sets of six different TDs: five SANCL (Petrov and McDonald, 2012) domains \u2013 newsgroups, weblogs, reviews, answers, emails \u2013 and sections 22- 23 of WSJ for in-domain testing.\nWe use two training sets of different sizes. In condition l:big (big labeled data set), we train FLORS on sections 2-21 of Wall Street Journal (WSJ). Condition l:small uses 10% of l:big.\nData for word representations. We also vary the size of the datasets that are used to compute the word representations before the FLORS model is trained on the training set. In condition u:big, we compute distributional vectors on the joint corpus of all labeled and unlabeled text of source and target domains (except for the test sets). We also\ninclude 100,000 WSJ sentences from 1988 and 500,000 sentences from Gigaword (Parker, 2009). In condition u:0, only labeled training data is used.\nMethods. We implemented the following modification compared to the setup in (Schnabel and Schu\u0308tze, 2014): distributional vectors are kept in memory as count vectors. This allows us to increase the counts during online tagging.\nWe run experiments with three versions of FLORS: STATIC, BATCH and ONLINE. All three methods compute word representations on \u201cdata for word representations\u201d (described above) before the model is trained on one of the two \u201ctraining sets\u201d (described above).\nSTATIC. Word representations are not changed during testing.\nBATCH. Before testing, we update count vectors by freq (bigram(ci, w)) += freq\u2217 (bigram(ci, w)), where freq\u2217(\u00b7) denotes the number of occurrences of the bigram \u201cci w\u201d in the entire test set.\nONLINE. Before tagging a test sentence, both left and right distributional vectors are updated via freq (bigram(ci, w)) += 1 for each appearance of bigram \u201cci w\u201d in the sentence. Then the sentence is tagged using the updated word representations. As tagging progresses, the distributional representations become increasingly specific to the target domain (TD), converging to the representations that BATCH uses at the end of the tagging process.\nIn all three modes, suffix and shape features are always fully specified, for both known and unknown words."}, {"heading": "3 Experimental results", "text": "Table 1 compares performance on SANCL for a number of baselines and four versions of FLORS: S&S, Schnabel and Schu\u0308tze (2014)\u2019s version of FLORS, \u201cS&S (reimpl.)\u201d, our reimplementation of that version, and BATCH and ONLINE, the two versions of FLORS we use in this paper. Compar-\ning lines \u201cS&S\u201d and \u201cS&S (reimpl.)\u201d in the table, we see that our reimplementation of FLORS is comparable to S&S\u2019s. For the rest of this paper, our setup for BATCH and ONLINE differs from S&S\u2019s in three respects. (i) We use Gigaword as additional unlabeled data. (ii) When we train a FLORS model, then the corpora that the word representations are derived from do not include the test set. The set of corpora used by S&S for this purpose includes the test set. We make this change because application data may not be available at training time in DA. (iii) The word representations used when the FLORS model is trained are derived from all six SANCL domains. This simplifies the experimental setup as we only need to train a single model, not one per domain. Table 1 shows that our setup with these three changes (lines BATCH and ONLINE) has state-of-the-art performance on SANCL for domain adaptation (bold numbers).\nTable 2 investigates the effect of sizes of labeled and unlabeled data on performance of ONLINE and BATCH. We report accuracy for all (ALL) tokens, for tokens occurring in both l:big and l:small (KN), tokens occurring in neither l:big nor l:small (OOV) and tokens ocurring in l:big, but not in l:small (SHFT).1 Except for some minor variations in a few cases, both using more labeled data and using more unlabeled data improves tagging accuracy for both ONLINE and BATCH. ONLINE and BATCH are generally better or as good as STATIC (in bold), always on ALL and OOV, and with a few exceptions also on KN and SHFT.\nONLINE performance is comparable to BATCH performance: it is slightly worse than BATCH on u:0 (largest ALL difference is .29) and at most .02 different from BATCH for ALL on u:big. We ex-\n1We cannot give the standard, single OOV evaluation number here since OOVs are different in different conditions, hence the breakdown into three measures.\nplain below why ONLINE is sometimes (slightly) better than BATCH, e.g., for ALL and condition l:small/u:big."}, {"heading": "3.1 Time course of tagging accuracy", "text": "The ONLINE model introduced in this paper has a property that is unique compared to most other work in statistical NLP: its predictions change as it tags text because its representations change.\nTo study this time course of changes, we need a large application domain because subtle changes will be too variable in the small test sets of the SANCL TDs. The only labeled domain that is big enough is the WSJ corpus. We therefore reverse the standard setup and train the model on the dev sets of the five SANCL domains (l:big) or on the first 5000 labeled words of reviews (l:small). In this reversed setup, u:big uses the five unlabeled SANCL data sets and Gigaword as before. Since variance of performance is important, we run on 100 randomly selected 50% samples of WSJ and report average and standard deviation of tagging error over these 100 trials.\nThe results in Table 32 show that error rates are only slightly worse for ONLINE than for BATCH or the same. In fact, l:small/u:0 known error rate (.1186) is lower for ONLINE than for BATCH (similar to what we observed in Table 2). This will be discussed at the end of this section.\nTable 3 includes results for \u201cunseens\u201d as well as unknowns because Schnabel and Schu\u0308tze (2014) show that unseens cause at least as many errors as unknowns. We define unseens as words with a tag that did not occur in training; we compute unseen error rates on all occurrences of unseens, i.e., occurrences with both seen and unseen tags are included. As Table 3 shows, the error rate for unknowns is greater than for unseens which is in turn greater than the error rate on known words.\n2Significance test: test of equal proportion, p < .05\nExamining the single conditions, we can see that ONLINE fares better than STATIC in 10 out of 12 cases and only slightly worse for l:small/u:big (unseens, known words: .1086 vs .1084, .0802 vs .0801). In four conditions it is significantly better with improvements ranging from .005 (.1404 vs .1451: l:big/u:0, unknown words) to >.06 (.3094 vs .3670: l:small/u:0, unknown words).\nThe differences between ONLINE and STATIC in the other eight conditions are negligible. For the six u:big conditions, this is not surprising: the Gigaword corpus consists of news, so the large unlabeled data set is in reality the same domain as WSJ. Thus, if large unlabeled data sets are available that are similar to the TD, then one might as well use STATIC tagging since the extra work required for ONLINE/BATCH is unlikely to pay off.\nUsing more labeled data (comparing l:small and l:big) always considerably decreases error rates. We did not test for significance here because the differences are so large. By the same token, using more unlabeled data (comparing u:0 and u:big) also consistently decreases error rates. The differences are large and significant for ONLINE tagging in all six cases (indicated by \u2217 in the table).\nThere is no large difference in variability ONLINE vs. BATCH (see columns \u201cstd\u201d). Thus, given that it has equal variability and higher performance, ONLINE is preferable to BATCH since it assumes no dataset available prior to the start of tagging.\nFigure 1 shows the time course of tagging accuracy.3 BATCH and STATIC have constant error rates since they do not change representations during tagging. ONLINE error decreases for unknown words \u2013 approaching the error rate of BATCH \u2013 as\n3In response to a reviewer question, the initial (leftmost) errors of ONLINE and STATIC are not identical; e.g., ONLINE has a better chance of correctly tagging the very first occurrence of an unknown word because that very first occurrence has a meaningful (as opposed to random) distributed representation.\nmore and more is learned with each additional occurrence of an unknown word (top).\nInterestingly, the error of ONLINE increases for unseens and known words (middle&bottom panels) (even though it is always below the error rate of BATCH). The reason is that the BATCH update swamps the original training data for l:small/u:0 because the WSJ test set is bigger by a large fac-\ntor than the training set. ONLINE fares better here because in the beginning of tagging the updates of the distributional representations consist of small increments. We noticed this in Table 2 too: there, ONLINE outperformed BATCH in some cases on KN for l:small/u:big. In future work, we plan to investigate how to weight distributional counts from the target data relative to that from the (labeled und unlabeled) source data."}, {"heading": "4 Related work", "text": "Online learning usually refers to supervised learning algorithms that update the model each time after processing a few training examples. Many supervised learning algorithms are online or have online versions. Active learning (Lewis and Gale, 1994; Tong and Koller, 2001; Laws et al., 2011) is another supervised learning framework that processes training examples \u2013 usually obtained interactively \u2013 in small batches (Bordes et al., 2005).\nAll of this work on supervised online learning is not directly relevant to this paper since we address the problem of unsupervised DA. Unlike online supervised learners, we keep the statistical model unchanged during DA and adopt a representation learning approach: each unlabeled context of a word is used to update its representation.\nThere is much work on unsupervised DA for POS tagging, including work using constraintbased methods (Subramanya et al., 2010; Rush et al., 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al., 2009; Huang and Yates, 2010), and co-training (Ku\u0308bler and Baucom, 2011). All of this work uses batch learning. For space reasons, we do not discuss supervised DA (e.g., Daume\u0301 III and Marcu (2006))."}, {"heading": "5 Conclusion", "text": "We introduced online updating of word representations, a new domain adaptation method for cases where target domain data are read from a stream and BATCH processing is not possible. We showed that online unsupervised DA performs as well as batch learning. It also significantly lowers error rates compared to STATIC (i.e., no domain adaptation). Our implementation of FLORS is available at cistern.cis.lmu.de/flors\nAcknowledgments. This work was supported by a Baidu scholarship awarded to Wenpeng Yin and by Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/10-1 FADeBaC)."}], "references": [{"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Fast kernel classifiers with online and active learning", "author": ["Seyda Ertekin", "Jason Weston", "L\u00e9on Bottou"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2005}, {"title": "Fast and robust part-of-speech tagging using dynamic model selection", "author": ["Choi", "Palmer2012] Jinho D. Choi", "Martha Palmer"], "venue": "In ACL: Short Papers,", "citeRegEx": "Choi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2012}, {"title": "Domain adaptation for statistical classifiers", "author": ["III Daum\u00e9", "III Marcu2006] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2006}, {"title": "Exploring representation-learning approaches to domain adaptation", "author": ["Huang", "Yates2010] Fei Huang", "Alexander Yates"], "venue": "In DANLP,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Improving a simple bigram HMM part-of-speech tagger by latent annotation and self-training", "author": ["Vladimir Eidelman", "Mary Harper"], "venue": "In NAACL-HLT: Short Papers,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Fast domain adaptation for part of speech tagging for dialogues", "author": ["K\u00fcbler", "Baucom2011] Sandra K\u00fcbler", "Eric Baucom"], "venue": "In RANLP,", "citeRegEx": "K\u00fcbler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2011}, {"title": "Active learning with Amazon Mechanical Turk", "author": ["Laws et al.2011] Florian Laws", "Christian Scheible", "Hinrich Sch\u00fctze"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Laws et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laws et al\\.", "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["Lewis", "Gale1994] David D. Lewis", "William A. Gale"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Lewis et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1994}, {"title": "English gigaword fourth edition", "author": ["Robert Parker"], "venue": "Linguistic Data Consortium", "citeRegEx": "Parker.,? \\Q2009\\E", "shortCiteRegEx": "Parker.", "year": 2009}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Improved parsing and POS tagging using intersentence consistency constraints", "author": ["Roi Reichart", "Michael Collins", "Amir Globerson"], "venue": "In EMNLPCoNLL,", "citeRegEx": "Rush et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2012}, {"title": "FLORS: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["Slav Petrov", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "Subramanya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2010}, {"title": "Support vector machine active learning with applications to text classification", "author": ["Tong", "Koller2001] Simon Tong", "Daphne Koller"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Tong et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "This is in contrast to other work, notably Blitzer et al. (2006), that carefully selects \u201cstable\u201d distributional features and discards \u201cunstable\u201d dis-", "startOffset": 43, "endOffset": 65}, {"referenceID": 9, "context": "We also include 100,000 WSJ sentences from 1988 and 500,000 sentences from Gigaword (Parker, 2009).", "startOffset": 84, "endOffset": 98}, {"referenceID": 7, "context": "Active learning (Lewis and Gale, 1994; Tong and Koller, 2001; Laws et al., 2011) is another supervised learning framework that processes training examples \u2013 usually obtained interactively \u2013 in small batches (Bordes et al.", "startOffset": 16, "endOffset": 80}, {"referenceID": 1, "context": ", 2011) is another supervised learning framework that processes training examples \u2013 usually obtained interactively \u2013 in small batches (Bordes et al., 2005).", "startOffset": 134, "endOffset": 155}, {"referenceID": 13, "context": "POS tagging, including work using constraintbased methods (Subramanya et al., 2010; Rush et al., 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al.", "startOffset": 58, "endOffset": 102}, {"referenceID": 11, "context": "POS tagging, including work using constraintbased methods (Subramanya et al., 2010; Rush et al., 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al.", "startOffset": 58, "endOffset": 102}, {"referenceID": 5, "context": ", 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al., 2009; Huang and Yates, 2010), and co-training (K\u00fcbler and Baucom, 2011).", "startOffset": 67, "endOffset": 110}, {"referenceID": 4, "context": ", 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al., 2009; Huang and Yates, 2010), and co-training (K\u00fcbler and Baucom, 2011). All of this work uses batch learning. For space reasons, we do not discuss supervised DA (e.g., Daum\u00e9 III and Marcu (2006)).", "startOffset": 68, "endOffset": 278}], "year": 2016, "abstractText": "We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.", "creator": "LaTeX with hyperref package"}}}