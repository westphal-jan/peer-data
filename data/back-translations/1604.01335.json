{"id": "1604.01335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "Deep Cross Residual Learning for Multitask Visual Recognition", "abstract": "These problems often occur in multimedia and vision applications that recognize content on a large scale. We propose a novel extension of deep-network residual learning that enables intuitive learning across multiple related tasks using cross-residuals, which can be considered a form of in-network regularization and allow for greater generalization of the network. We show how cross-residual learning (CRL) can be integrated into multi-task networks to jointly train and recognize visual concepts across multiple tasks. We present a single multi-task cross-residual network with & gt; 40% fewer parameters that are capable of achieving competitive or even better recognition performance over a visual perception problem that normally requires multiple specialized single-task networks.", "histories": [["v1", "Tue, 5 Apr 2016 17:08:14 GMT  (865kb,D)", "https://arxiv.org/abs/1604.01335v1", "9 pages, 6 figures, ACM Multimedia"], ["v2", "Wed, 20 Jul 2016 01:55:12 GMT  (868kb,D)", "http://arxiv.org/abs/1604.01335v2", "10 pages, 6 figures, To appear in ACM Multimedia"]], "COMMENTS": "9 pages, 6 figures, ACM Multimedia", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["brendan jou", "shih-fu chang"], "accepted": false, "id": "1604.01335"}, "pdf": {"name": "1604.01335.pdf", "metadata": {"source": "CRF", "title": "Deep Cross Residual Learning for Multitask Visual Recognition", "authors": ["Brendan Jou", "Shih-Fu Chang"], "emails": ["bjou@ee.columbia.edu", "sfchang@ee.columbia.edu"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Computing methodologies\u2192Computer vision; Multitask learning; Neural networks; \u2022Information systems \u2192 Multimedia information systems;\nKeywords residual learning; deep networks; multitask learning; concept detection; generalization; regularization"}, {"heading": "1. INTRODUCTION", "text": "In concept detection, leveraging the complex relationships between learning tasks remains an open challenge in the con-\nMM \u201916, October 15-19, 2016, Amsterdam, Netherlands\nACM ISBN .\nDOI: http://dx.doi.org/10.1145/2964284.2964309\nstruction of many multimedia systems. While some recent approaches have begun to model these relationships in deep architectures [8, 42], still many multimedia solutions tend to have multiple parts that specialize rather than a more versatile, general solution that leverages cross-task dependencies. As an illustration, visual sentiment prediction is a rising topic of interest in multimedia and vision. In [2], a semantic construct called adjective-noun pairs (ANPs) was proposed wherein there are visual concept pairs like \u2018happy girl\u2019, \u2018misty woods\u2019 and \u2018good food\u2019. These semantic concepts serve as a bridge between vision-based tasks that are focused on object (or \u201cnoun\u201d) recognition and affective computing tasks that are focused on qualifying the affective capacity or strength of multimedia, e.g. through the \u201cadjective\u201d in the ANP. However, even though the tasks of object recognition, affect prediction and ANP detection all have some relation to each other, the construction of classifiers for each is treated independently. In this work, we propose ar X\niv :1\n60 4.\n01 33\n5v 2\n[ cs\n.C V\n] 2\n0 Ju\nl 2 01\n6\na novel method for jointly learning and generalizing across tasks which can be easily and very efficiently integrated into a deep residual network and as a proof-of-concept show how it can be used for visual sentiment concept detection.\nTo understand how \u201crelatedness\u201d is both important and applicable to visual concept detection, consider several example images and concepts from [2] in Figure 2. In the example, we observe that the ANP \u2018shiny cars\u2019 can be superclassed by both the \u2018shiny\u2019 adjective category and \u2018cars\u2019 noun category. Within the \u2018shiny\u2019 adjective category, there are other concepts like \u2018shiny shoes\u2019 that bear both semantic and visual similarities to the \u2018shiny cars\u2019 ANP, e.g. gloss or surface luster. This intra-relatedness also exists within the noun superclass which includes ANPs like \u2018amazing cars\u2019 and \u2018classic cars\u2019. In addition to relatedness within the same (super)class, we observe that there are visual similarities also present between classes of different superclasses, e.g. \u2018classic cars\u2019 and \u2018shiny shoes\u2019. This inter-relatedness between (super)classes illustrates how in settings like concept detection, classifiers can benefit from exploiting representational similarities across related tasks. Both of these senses of relatedness show that visual representations across related tasks can be shared to a degree. We develop a multitask learning problem for visual concept detection to illustrate a setting in which our proposed method can be applied. We design a deep neural network with a stack of shared low-level representations and then higher level representations that both specialize and mix information across related tasks during learning. We then show how such a multitask network architecture with cross-task exchanges can be used to simultaneously learn classifiers to detect adjective, noun and adjectivenoun pair visual concepts.\nIn [14], residual learning is proposed as an approach for enabling much deeper networks while addressing the degradation problem where very deep networks have a tendency to underfit compared to shallower counterpart networks [40].\nIn residual learning, an identity mapping through the use of shortcut connections [34] is proposed where an underlying mapping H(x) = F(x) + x is learned given that F(x) = H(x)\u2212 x represents another mapping fit by several stacked layers. One interpretation is that F(x) represents a noise term and the model is fitting the input plus some additive nonlinear noise. Thus, if we were performing reconstruction, a trivial solution to the residual learning problem is that an identity mapping is optimal, i.e. F(x) = 0. However, in [14], it is argued that optimization software may actually have difficulty with approximating identity mappings with a stack of nonlinear layers, and also that for prediction problems, it is unlikely that the strict identity is optimal. They also argue that fitting residual mappings can enable deeper networks given the information boost achieved via the shortcut connection and thus reduces the likelihood of model degradation. Our work extends residual learning [14] to also integrate information from other related tasks enabling cross-task representations. Specifically, we hypothesize and experimentally show that reference components from correlated tasks can be synergistically fused in a residual deep learning network for cross-residual learning.\nOur contributions include (1) the proposal of a novel extension of residual learning [14] using cross-connections for coupling multiple related tasks in a setting called crossresidual learning (CRL), (2) the development of a multitask network with a fan-out architecture using cross-residual layers, and (3) an evaluation of cross-residual networks on a multitask visual sentiment concept detection problem yielding a single network with very competitive or even better accuracy compared to individual networks on three classification tasks (noun, adjective, and adjective-noun pair detection) but uses >40% less model memory via branching, while also outperforming the predictive performance of a standard multitask configuration without cross-residuals by about 10.4%."}, {"heading": "2. RELATED WORK", "text": "Our work broadly intersects three major lines of research areas: transfer learning, deep neural architectures for vision, and affective computing. In traditional data mining and machine learning tasks, we often seek to statistically model a collection of labeled or unlabeled data and apply them to other collections. In general, the distributions of these sets of data collections are assumed to be the same. In transfer learning [32], the domain, tasks and distributions are allowed to be different in both training/source and testing/target. In this work, we specifically focus on a subset of transfer learning problems that assume some relatedness between these collections. Specifically, in multimedia and vision contexts, relatedness may refer to settings where groups of tasks have semantic correlation, e.g. classifying dog breeds and bird species, or visual similarity, e.g. jointly classifying and reconstructing objects, and is often referred to as multitask learning [3]. Likewise, relatedness may also refer to the same source task but applied in different domains, e.g. classifying clothing style across cultures, and is sometimes called cross-domain learning [20] or domain transfer/adaptation [11, 21]. Nonetheless, the hypothesis of explicitly learning from related tasks is that we can learn more generalized representations with minimal performance cost or in some cases, leading to gains from learning jointly.\nMultitask networks are recently becoming a popular ap-\nproach to multitask learning, riding on successes in deep neural networks. One early work in [5] showed how a single network could be trained to solve multiple natural language processing tasks simultaneously like part-of-speech tagging, named entity recognition, etc. Multitask networks have since proven effective for automated drug discovery [6, 35], query classification and retrieval [28], and semantic segmentation [7]. Recently, [10] proposed multitask auto-encoders for generalizing object detectors across domains; and in [29], multitask sequence-to-sequence learning is proposed for text translation. Also, architectures like [36, 44] can be categorized as multitask networks since they reconstruct and classify simultaneously. Unlike other multitask networks but similar to ladder networks [36], instead of a single branching point in our network that creates forked paths to only specialize to individual tasks, we continue mixing information even after branching via our cross-skip connections.\nWhereas multitask learning can generally be understood as a fan-out approach where a (usually, single) shared representation is learned to solve multiple tasks, an analogous complement is a fan-in approach where multiple either features or decision scores are fused together to solve a singletask. For example, graph diffusion can be used smooth decision scores for leveraging intra-relatedness between categories [21]. In [8], instead of an undirected graph, explicit directed edges were used to model class relationships like exclusion and subsumption. And with some semblance to our work, in [42], a multimodal neural network structure is developed where inter-class (but still intra-task) relationships are integrated as an explicit regularizer. Although inspired from multitask learning, the network design in [42] still operates in a single-task context as there is only a single output network head. Additionally, because the network integrates multiple input feature towers, the overall memory and training burden of the image-to-decision pipeline is much greater than that of a fan-out network alternative.\nSince we use visual sentiment concept detection to illustrate the efficacy of our proposed cross-residual learning approach, it is worth also briefly noting several advances in visual affect. In visual affective computing, a longstanding goal is to bridge the affective gap, a conceptual disconnect between low-level multimedia features and high-level affective states like emotions or sentiment. In [43], a codebook over local color histogram and Gabor features were proposed for image-based sentiment prediction; and in [30], psychology and art theory inspired features were proposed. Again, in [2], adjective-noun pairs were proposed as a mid-level semantic construct and an ontology was mined from a pop-\nular social multimedia platform using psychology-grounded seed queries [33]. Other problems related to affect detection include quality assessment [25], memorability [18], interestingness [12] and popularity [26]. In this work, we develop a single deep multitask cross-residual network able to simultaneously predict noun, adjective and adjective-noun visual concepts."}, {"heading": "3. CROSS RESIDUAL LEARNING", "text": "Given an input x and output y vector to a residual learning layer and the mapping function F(x, {Wi}) to fit, where for vision problems this might represent, for example, a stack of convolutional operations with batch normalization [17] and ReLU activation [31], we have the following in residual learning [14]:\ny = F(x, {Wi}) + Wsx, (1)\nwhere Ws is an optional linear projection, but required when matching dimensions, on the shortcut connection. For identity shortcut connections, Ws = I.\nHere, we propose a simple and efficient extension of [14] when fitting across multiple related learning tasks which we refer to as cross-residual learning (CRL). Given a task t and N \u2212 1 other related tasks, we define the task output of the cross-residual module as:\ny(t) = F(x, {W(t)i }) + N\u2211\nj=1\nW(j)s x, (2)\nwhere the superscript (\u00b7) indexes the target task and a normalization factor is omitted for simplicity and can be lumped with the shortcut weights W (j) s . As also illustrated in Figure 3, the other target tasks additively contribute to the\ncurrent target task t by \u2211\nj 6=t W (j) s x. The cross-residual\ncontributions can also more generally be stacks of operations C(x, {W(j)s,m}), but here, we only illustrate the simple weighted once case W (j) s x.\n\u201cEarly\u201d Regularization Interpretation. In optimization, when minimizing a loss L(f(x),y), we often add a regularization term R(f(x)) to constrain the \u201cbadness\u201d of the solution, factor in assumptions of our system, and reduce overfitting. For example, in solving deep networks, the squared 2-norm is a common choice to penalize large parameter values and smooth network mappings. Crossresidual units can be viewed as a way of regularizing the solution of a specific task by other related tasks, i.e. we do not want the learned mapping F(x, {W(t)i }) to be too far from a weighted combination of task-specialized transforma-\ntions of the input \u2211\nj W (j) s x. For example, when learning to\nvisually recognize species of birds, we may want to introduce regularization to ensure the mapping fit is not too far from the separate, but related task of recognizing types of mammals. While such a regularization usually takes place in the loss layer of a neural network, using cross-residual layers we can introduce this task conditioning \u201cearlier\u201d in the network and also stack them for additional information mixing. Unlike typical \u201cregularization,\u201d a cross-residual layer introduces regularization by biasing at the layer-level, i.e. with respect to a given task\u2019s residual rather than with respect to the penultimate loss. Cross-residual layers thus serve as a type of in-network regularization somewhat similar to dropout [39], though with less stochasticity.\nConnection to Highway Networks [40] & LSTM [16]. As also discussed in [14], residual networks can be seen as highway networks [40] that do not have transform or carry gates. In highway networks, an output highway layer is defined as\ny = H(x,WH)T (x,WT ) + x \u00b7 C(x,WC), (3)\nwhere T and C are the transform and carry gates, respectively. Clearly, when both gates are on, this is precisely the same as a residual layer. By extension, a cross-residual layer can be thought of as an ungated highway layer with multiple \u201chighways\u201d merging onto the same information path. Crossresidual weighting layers then are carry gates which govern the amount of cross-task pollination.\nSimilarly, it has been argued (though somewhat reductionist) that residual layers can also be viewed as a feedforward long short-term memory (LSTM) [16] units without gates. Specifically, consider the LSTM version from [9]:\nik = \u03c3(Wxixk + Whihk\u22121 + bi) fk = \u03c3(Wxfxk + Whfhk\u22121 + bf ) ck = fkck\u22121 + ik tanh(Wxcxk + Whchk\u22121 + bc) ok = \u03c3(Wxoxk + Whohk\u22121 + bo) hk = ok tanh(ck)  , (4) where k indexes the timestep, i, f and o are the input, forget and output gates, c and h are the cell and output states, all respectively, and peephole connections and some bias terms are omitted for simplicity. By ignoring recurrent connections k\u2212 1 for the feed-forward case and making the LSTM completely ungated, i.e. i = f = o = I, and initializing the cell state to the input ck\u22121 = x, we are left with a residual layer. Again by extension then, cross-residual layers can be thought of as feed-forward, ungated LSTMs whose cell states are additively coupled. LSTM forget gates then are analogous to cross-residual weight layers. And indeed, this is much like highway networks\u2019 carry gate, since highway layers can be viewed as feed-forward LSTMs with only forget gates [40]. A major difference to note though is that cross-residual layers couple the transformed input H with multiple and usually different prior cell states c\n(t) k\u22121 or infor-\nmation highways x(t).\nSimilarities to Ladder Networks [36]. Structurally, the building blocks of cross-residual learning bears some resemblance to the layout in ladder networks [36]. In ladder networks, two encoders and one decoder joined via lateral connections are used to jointly optimize a weighted sum over a cross-entropy and reconstruction loss and have thus proven successful in semi-supervised learning. As part of the reconstruction process, a Gaussian noise term is injected in one of the encoders and the decoder receives a combination of this noisy signal via a lateral connection and a vertical \u201cfeedback\u201d connection to reconstruct the original input into the noisy encoder. Since the mapping term F(x) in residual learning can be viewed as perturbation term, albeit learned unlike in ladder networks, both models essentially are trying to fit the input subject to some additive nonlinear perturbation. For cross-residual learning, although we use shortcut connections instead of lateral connections as in ladder networks, both designs operate on the principle that combining channels of information at the same structural level in the network can ultimately result in a model with higher learning capacity under less constraints, e.g. for ladder networks,\nless labeled data requirements since it is semi-supervised."}, {"heading": "4. MULTITASK CROSS RESIDUAL NETS", "text": "While there may be a number of settings that would benefit from cross-residual learning, we illustrate one natural setting here in multitask learning [3]. To implement a multitask network, a common approach [5, 10, 28, 35] is to introduce a branching point in the architecture that leads to one network head per task, e.g. see Figure 1. In Table 1 and Figure 4, we show 50-layer multitask residual networks with a branching point at the last input size reduction. The earlier in the network this branching point is introduced the larger the input feature map size is to the individual network heads, often resulting in multitask networks with a large memory footprint. On the other hand, if the branching point begins deeper in the network, the representational specialization available for each task is limited to a small space of high-level abstract features. In our design of a multitask cross-residual network (X-ResNet), we address this latter problem by allowing additional cross-task mixing via crossresidual weights which cheaply increases late-layer representational power without requiring large input feature spaces. While it is possible to completely forego a branching point in the network design and simply couple multiple network towers using cross-residual skip connections, this results in a composite network that is very memory intensive and only feasible in a multi-GPU environment (though this could be somewhat alleviated by freezing weights, e.g. in combination with greedy layerwise training).\nIn addition, to introduce some task specialization, at the branching point in our multitask network design and before the cross-residual layers, we move the last ReLU activation and batch normalization canonically present inside the residual building block outside, placing it after the elementwise addition such that there is one per task. This helps to produce a slightly different normalization for each task branch and in practice, slightly improves performance. As in most multitask networks with a branching point, the total network loss is taken to be a combination of each of the individual network head losses. While some tune the loss weight for each of these network heads, we simply use the unweighted sum over all the network head losses."}, {"heading": "5. MULTITASK VISUAL SENTIMENT", "text": "In order to illustrate the utility and effectiveness of crossresidual layers when used in multitask networks, we re-frame visual sentiment concept detection in a multitask context. In particular, we use the Visual Sentiment Ontology (VSO)1 [2] and cast affective mid-level concept detection as a multitask learning problem. We chose the VSO dataset for our preliminary experiments because it presents a visual affect challenge currently of rising interest in the multimedia community as VSO has since had multilingual extensions [24] and been applied in aesthetics understanding [1], emotion prediction [22, 23], popularity modeling [26], and more. While similar problems could have been created from other datasets, e.g. CIFAR-100 where we might choose to predict classes and superclasses simultaneously, the adjective-noun pair detection problem can be recast to naturally fit the multitask setting with a sufficiently large accompanying image corpus over three tasks, i.e. adjective, noun and adjective-noun\n1https://visual-sentiment-ontology.appspot.com\npair, while other image datasets are often smaller and/or only consist of two learning tasks which yield a small number of task interactions.\nGiven the diversity of adjective-noun pairs, including concepts like \u2018cute dress\u2019, \u2018gentle smile\u2019, \u2018scary skull\u2019, \u2018wild rose\u2019 and \u2018yummy cake\u2019, there is both a considerable amount of semantic variance in VSO as well as inter-class visual variance due to the image data being gathered from social media streams. As a result, to cope with this diversity and variance, we believe that exploiting cross-task correlations as part of the network design is important, especially when the tasks are tightly related as they are with noun, adjective, and adjective-noun pair concept detection.\nWe additionally note that even though VSO [2] argues that the noun component of the ANP serves to visually ground the mid-level concept, no experiments were actually ever run to determine the performance of detecting adjective (or even, noun) concepts separately2. Our evaluation thus also serves as the first evaluation on the VSO dataset to benchmark noun-only and adjective-only detection performance."}, {"heading": "5.1 Multitask-structured VSO", "text": "Briefly, the data in VSO [2] was originally collected from\n2From independent communication with the authors.\nthe social multimedia platform, Flickr3, using psychologygrounded seed queries from Plutchik\u2019s Wheel of Emotions [33] which consists of 24 basic emotions, such as joy, terror, and anticipation. The query results yielded images with user-entered image tags which were annotated using a partof-speech tagger for identifying adjective and noun components and parsed for sentiment strength. The identified adjective and noun components were combined, checked for semantic consistency and filtered based on sentiment strength then used to feed back as queries to Flickr to filter based on frequency of usage. A subsampling of adjective-noun pair combinations is then done to prevent many adjective variations on any one noun, resulting in the final visual sentiment ontology. The adjective-noun pairs were then used to query and pull down an image corpus from Flickr, limiting to at most 1,000 images per concept.\nThe image dataset in VSO [2] has a long tail distribution where some adjective-noun pair concepts are singletons and do not share any adjectives or nouns with other concept pairs. As a result, we use a subset of VSO and use it to perform adjective, noun, and ANP concept detection in social images, specifically, as a multitask learning problem. The original VSO dataset [2] consists of a refined set of 1,200 ANP concepts. Since there are far less adjectives that\n3https://www.flickr.com\nserve to compose these adjective-noun pairs, and also some nouns that are massively over-represented in the ontology, we filtered to keep concepts that matched the following criteria: (1) adjectives with \u22653 paired nouns, (2) nouns that are not overwhelmingly biasing, v.s. face or flowers, and nonabstract, unlike loss, adventure or history, and (3) ANPs with \u2265500 images. It is helpful to think of ANPs as a bipartite graph with nouns and adjectives on either side and valid ANPs as edges. From these conditions, we obtained a visual sentiment sub-ontology, suitable for multitask learning, that normalized the number of adjective and noun nodes while ensuring maximal ANP edge coverage. The final multitaskflavored VSO contains 167 nouns and 117 adjectives which form 553 adjective-noun pairs over 384,258 social images collected from Flickr."}, {"heading": "5.2 Experiments & Discussion", "text": "In our experiments, we use a 80/20 partition of the multitask VSO data stratified by adjective-noun pairs resulting in 307,185 images for training and 77,073 for test at 224\u00d7224. All our residual layers use\u201cB option\u201dshortcut connections as detailed in [14] where projections are only used when matching dimensions (stride 2) and other shortcuts are identity. Except for cross-residual weight layers, projections are performed with a 1 \u00d7 1 convolution with ReLU activation and batch normalization as in [14]. For our cross-residual weight layers W (j) s , we use the identity on self-shortcut connections W (t) s = I and a cheap channelwise scaling layer for crosstask connections a x, \u2200 j 6= t which adds no more than 2,048 parameters each, i.e. so in our case, after branching we have x \u2208 R7\u00d77\u00d72048 and so a \u2208 R1\u00d71\u00d72048 for scaling.\nFor training multitask networks, we initialized most layers using weights from a residual network (ResNet) model trained on ILSVRC-2015 [37], but done such that for layers after the branching point in our network we initialize\nthem to the same corresponding layer weights in the original ResNet model. For cross-residual weight layers, we follow [14] and initialize them as in [13], i.e. zero mean random\nGaussian with a \u221a\n2/nl standard deviation where we set nl to be the average of input and output units layerwise. No dropout [39] was used in residual or cross-residual networks. We use random flips of the input at training. We trained our cross-residual networks with stochastic gradient descent using a batch size of 24, momentum of 0.9 and weight decay of 0.0001. We used a starting fixed learning rate of 0.001 and decreased it by a factor of ten whenever the loss plateaued until convergence. All networks and experiments were run using a single NVIDIA GeForce GTX Titan X GPU and implemented with Caffe [19].\nWe baseline against four single-task architectures: a variant of AlexNet [27] swapping pooling and normalization layers called CaffeNet [19], the first iteration of the GoogLeNet architecture [41] denoted as Inception-v1 which uses a bottlenecked 5\u00d7 5 convolution in the sub-modules, the 16-layer version of VggNet [38] (VggNet-16), and the ResNet architecture [14] with 50-layers (ResNet-50). Each of these singletask architectures were fine-tuned from an ImageNet-trained model and represent competitive baselines that achieved top ranks in ILSVRC tasks in the past. In addition, we also evaluated against DeepSentiBank [4], also an AlexNet-styled model trained on the full, unrestricted VSO data [2] to detect 2,089 ANPs. We did not retrain [4] but rather reevaluated their model on the subset of 553 ANP concepts we focus on here; however, since we do not know the train and test image splits that they used, the result provided for DeepSentiBank [4] could still be an over-estimate. In Figure 4 (rightmost), we show the learning and inference paradigm represented by these single-task architectures with residual networks (ResNet) used as an example. Each of these baselines treat the adjective, noun and adjective-noun recognition tasks as independent targets.\nWe summarize network parameter costs and top-k accuracy on the multitask VSO tasks in Table 2. For network parameter costs, note that for Inception-v1 [41] we did not count the parameters from auxiliary heads although they are used during training. Top-k accuracy denotes the percentage of correct predictions within the top k ranked decision outputs.\n5.2.1 Adjective vs. Noun vs. ANP Detection In general, as originally posited by the VSO work [2], in\nterms of problem difficulty ordering, noun prediction is indeed \u201ceasier\u201d as visual recognition task than adjective prediction. However, though not in stark contrast to [2], and although there are indeed more ANP classes than nouns and adjectives, we still did expect to observe higher accuracy rates for ANP concept detection than we did, expecting that the rates would be much closer to that of noun detection and not lower than adjective detection since [2] argues that adjectives lack visual grounding. We suspect that this difference by almost a half at top-1 between noun and ANP detection may point to the difficulty of the ANP detection problem in a slightly different sense than difficulty for the adjective detection problem. For adjective detection, visual recognition difficulty is likely to arise from visual variance, e.g. there may be a wide range of visual features required to describe the concept \u2018pretty\u2019. However, for ANP detection, we believe that visual recognition difficulty is more likely\ndue to visual nuances than overall visual variance. Much like fine-grained classification, this may imply that in ANP concept detection, concepts like \u2018sad dog\u2019 and \u2018happy dog\u2019 may share many visual characteristics but differ on few but highly distinguishing traits. The hope then is that by using a scaling layer, which acts as a soft gating mechanism in cross-residual connections, these few but distinguishable characteristics are accentuated.\n5.2.2 Effects of Cross-residual Weighting In Table 2, we also show results for multitask cross-residual\nnetworks with different types of weighting: no cross-residual weighting (X0-ResNet-50), with all identity cross-residual weights (XI-ResNet-50) and with identity on the self-task connections and channelwise scaling on just cross-residuals as described earlier (Xs-ResNet-50). The multitask crossresidual networks without and with cross-residuals are illustrated in Figure 4 (leftmost and center, respectively), and all of these multitask networks use a residual network with 50-layers (ResNet-50) as the basis and branch as described in Section 4. As we might expect, when all cross-residual weights are identity (XI-ResNet-50), the accuracy of the multitask network across all tasks drastically reduces since the \u201camount\u201d of cross-task mixing is forced to be equally weighted. Even as related as tasks might be forcing crossresidual weights equal across all tasks makes it difficult during learning for any single task to specialize and determine discriminative patterns useful for that specific task. It may\nbe tempting to then assume that the other extreme of making the cross-residual weights zero where W (j) s = 0, \u2200 j 6= t, i.e. equivalent to a multitask network without cross-residuals (X0-ResNet-50), allows more specialization and would naturally achieve the best discriminative performance. However, we found that this actually consistently achieves lower accuracy across all tasks compared to its single-task equivalents (ResNet-50), e.g. \u223c9% worse relative on ANP detection. We hypothesize that without cross-residuals the performance case becomes upper-bounded by the shared representation learned before the branching the multitask network.\nOnce we allow for even some simple learned weighting on the cross-residuals, like a channelwise scaling (Xs-ResNet50), the predictive performance of the multitask network improves, outperforming both the case when no cross-residuals are used as well as equally weighted cross-residuals. In general, we observed that multitask networks achieved comparable performance to the three specialized single-task networks with just a single network while requiring less than 60% of the combined parameters of the three single-task networks (\u223c43.2M vs. \u223c72.4M). This confirms our original hypothesis that the low-level representations can be shared across these related tasks and can be generalized to perform well across all tasks. However, in order to ensure that we do not take a hit in accuracy by generalizing, weighted cross-residuals layers can be used which, at a very marginal parameter cost, enable the multitask network to match the performance of specialized single-task networks. Notably, as we had hoped, the highest gain from using cross-residuals was on the most difficult of the three tasks: ANP detection. We observe that adding scaling cross-residual weights improves the concept detection performance by as much as \u223c10.37% relative on the ANP detection task compared to without any weighting.\nThough we do not claim that our cross-residual multitask network (Xs-ResNet-50) definitively achieves a significantly higher accuracy over the single-task networks, we do note that we observed marginally better concept detection rates with our network across all tasks. Since we only used two cross-residual layers in our multitask network (c.f. Figure 4), it is possible that increasing the number of stacked crossresidual layers or beginning the branching in the network earlier could improve the overall cross-task performance; however, doing so would naturally come at increased parameter cost. Nonetheless, we believe that all of these observations show that jointly learning across related tasks with cross-task information mixing even at the late layers of a network can simultaneously improve the network\u2019s capacity to discriminate and generalize.\nTo further reinforce that the optimal weightings for crossresidual connections are unlikely to be zero or identity, in Figure 5, we show the unnormalized weight magnitudes of a learned multitask cross-residual network sorted by channel index for two cross-residual layers in a network structured as in Figure 4 (center). If an all zero or identity cross-residual connection were to be optimal, we would expect to see a plateau with many weights near zero or one. Instead, we observe that mostly non-negative cross-task weights were learned across all shortcut connections such that the overall network objective was optimized. Additionally, we note that though the weight magnitudes are indeed small, this also follows from intuition in the original residual network work [14] that these small, but non-zero weights are precisely what enable residual networks to be made very deep.\n5.2.3 Example Multitask Detection Results In Figure 6, we show example classification results from\nour multitask cross-residual network. Note the presence of both intra- and inter-relatedness between tasks in the top detected concepts. In many cases, the cross-residual network is able to surface concepts not visually present but intuitively related; for example, in the first image, \u2018spider\u2019 is a detected noun which may be a result of either the branches in the image or the visual co-occurrence of the \u2018spider\u2019 concept in the training set with other top ranked concepts like \u2018tiny\u2019 (adjective) and \u2018leaves\u2019 (noun). As a potential failure case, in the last image, the ANP \u2018sexy lips\u2019 was ranked highly possibly due to relatedness learned with the \u2018colorful\u2019 adjective concept. In these cases, just as with over regularization in other learning settings, the network may have indeed have learned a more general representation, but as a result is unable to decouple certain learned relationships. Such cases may be easily addressed in cross-residual networks by giving cross-task weighting layers more computational budget, e.g. convolutional projections, to model more complicated task relationships. Overall, we observe here that the multitask cross-residual network is able to successfully co-detect concepts across multiple related visual recognition tasks."}, {"heading": "6. CONCLUSIONS & FUTURE WORK", "text": "We presented an extension of residual learning enabling information mixing between related tasks called cross-residual learning (CRL) achieved by coupling the residual to other related tasks to ensure the learned mapping is not too far from other task representations. This enables more generalized representations to be learned in a deep network that are useful for multiple related tasks while preserving their discriminative power. We also showed how cross-residuals can be used for multitask learning by integrating cross-residual layers in a fan-out multitask network. We illustrated how such a multitask cross-residual network can achieve competitive, or even better, predictive performance on a visual sentiment concept detection problem as compared to specialized single-task networks but with >40% less parameters, while also outperforming a standard multitask residual network with no cross-residuals by about 10.4% relative on adjective-noun pair detection, the hardest of the three related target tasks. Without cross-residual connections, we\nobserved a \u223c9% drop in accuracy on ANP detection, indicating the importance of using cross-residuals. In addition, we showed the importance of cross-residual weighting over simply forcing identity cross-residual connections since equally weighting cross-task connections bottlenecks the information flow in the network.\nWe believe cross-residual networks are also applicable to other learning settings and domains, and can be extended in several ways. Cross-residual networks can be applied to other multitask learning settings where we are not only interested in classification but also other tasks like reconstruction, object segmentation, etc. Likewise, cross-residual networks are likely to be useful in domain transfer and adaptation problems where, for example, network tower weights are frozen but cross-residual weights are learned. Architecturally, while we only explored the canonical shortcut connections of [14] and used a channelwise scaling layer for the cross-residual, there is recent work exploring different types of transforms and gating on shortcuts [15] that can also be extended to the self- and cross-connections in cross-residual networks. We plan to explore these learning settings and network architectures in the future."}, {"heading": "Acknowledgements", "text": "We thank our reviewers for their helpful and constructive feedback. We also thank Rogerio Feris for insightful discussions on the network design and Tao Chen for support with the Visual Sentiment Ontology (VSO) dataset as well as discussions on prior VSO experiments."}, {"heading": "7. REFERENCES", "text": "[1] S. Bhattacharya, B. Nojavanasghari, T. Chen, D. Liu,\nS.-F. Chang, and M. Shah. Towards a comprehensive computational model for aesthetic assessment of videos. In ACM MM, 2013.\n[2] D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In ACM MM, 2013.\n[3] R. Caruana. Multitask learning. Machine Learning, 28(1), 1997.\n[4] T. Chen, D. Borth, T. Darrell, and S.-F. Chang. DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks. arXiv preprint arXiv:1410.8586, 2014.\n[5] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, 2008.\n[6] G. E. Dahl, N. Jaitly, and R. Salakhutdinov. Multi-task neural networks for QSAR predictions. arXiv preprint arXiv:1406.1231, 2014.\n[7] J. Dai, K. He, and J. Sun. Instance-aware semantic segmentation via multi-task network cascades. In CVPR, 2016.\n[8] J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio, Y. Li, H. Neven, and H. Adam. Large-scale object classification using label relation graphs. In ECCV, 2014.\n[9] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber. Learning precise timing with LSTM recurrent networks. JMLR, 3, 2002.\n[10] M. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015.\n[11] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, 2011.\n[12] M. Gygli, H. Grabner, H. Riemenschneider, F. Nater, and L. V. Gool. The interestingness of images. In ICCV, 2013.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In ICCV, 2015.\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. arXiv preprint arXiv:1603.05027, 2016.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997.\n[17] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.\n[18] P. Isola, J. Xiao, A. Torralba, and A. Oliva. What makes an image memorable? In CVPR, 2011.\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM MM, 2014.\n[20] W. Jiang, E. Zavesky, S.-F. Chang, and A. Loui. Cross-domain learning methods for high-level visual concept classification. In ICIP, 2008.\n[21] Y.-G. Jiang, J. Wang, S.-F. Chang, and C.-W. Ngo. Domain adaptive semantic diffusion for large scale context-based video annotation. In ICCV, 2009.\n[22] Y.-G. Jiang, B. Xu, and X. Xue. Predicting emotions in user-generated videos. In AAAI, 2014.\n[23] B. Jou, S. Bhattacharya, and S.-F. Chang. Predicting viewer perceived emotions in animated GIFs. In ACM MM, 2014.\n[24] B. Jou, T. Chen, N. Pappas, M. Redi, M. Topkara, and S.-F. Chang. Visual affect around the world: A large-scale multilingual visual sentiment ontology. In ACM MM, 2015.\n[25] Y. Ke, X. Tang, and F. Jing. The design of high-level features for photo quality assessment. In CVPR, 2006.\n[26] A. Khosla, A. Das Sarma, and R. Hamid. What makes an image popular? In WWW, 2014.\n[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.\n[28] X. Liu, J. Gao, X. He, L. Deng, K. Duh, and Y.-Y. Wang. Representation learning using multi-task deep neural networks for semantic classification and information retrieval. In NAACL, 2015.\n[29] M.-T. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser. Multi-task sequence to sequence learning. In ICLR, 2016.\n[30] J. Machajdik and A. Hanbury. Affective image classification using features inspired by psychology and art theory. In ACM MM, 2010.\n[31] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.\n[32] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE TKDE, 22(10), 2010.\n[33] R. Plutchik. Emotion: A Psychoevolutionary Synthesis. Harper & Row, 1980.\n[34] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012.\n[35] B. Ramsundar, S. Kearnes, P. Riley, D. Webster, D. Konerding, and V. Pande. Massively multitask networks for drug discovery. arXiv preprint arXiv:1502.02072, 2015.\n[36] A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko. Semi-supervised learning with ladder networks. arXiv preprint arXiv:1507.02672, 2015.\n[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3), 2015.\n[38] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n[39] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(1), 2014.\n[40] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. In ICMLW, 2015.\n[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n[42] Z. Wu, Y.-G. Jiang, J. Wang, J. Pu, and X. Xue. Exploring inter-feature and inter-class relationships with deep neural networks for video classification. In ACM MM, 2014.\n[43] V. Yanulevskaya, J. van Gemert, K. Roth, A. Herbold, N. Sebe, and J. M. Geusebroek. Emotional valence categorization using holistic image features. In ICIP, 2008.\n[44] J. Yim, H. Jung, B.-I. Yoo, C. Choi, D. Park, and J. Kim. Rotating your face using multi-task deep neural network. In CVPR, 2015."}], "references": [{"title": "Towards a comprehensive computational model for aesthetic assessment of videos", "author": ["S. Bhattacharya", "B. Nojavanasghari", "T. Chen", "D. Liu", "S.-F. Chang", "M. Shah"], "venue": "ACM MM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang"], "venue": "ACM MM,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, 28(1),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["T. Chen", "D. Borth", "T. Darrell", "S.-F. Chang"], "venue": "arXiv preprint arXiv:1410.8586,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-task neural networks for QSAR predictions", "author": ["G.E. Dahl", "N. Jaitly", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1406.1231,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale object classification using label relation graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "ECCV,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "JMLR, 3,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Domain generalization for object recognition with multi-task autoencoders", "author": ["M. Ghifary", "W.B. Kleijn", "M. Zhang", "D. Balduzzi"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "The interestingness of images", "author": ["M. Gygli", "H. Grabner", "H. Riemenschneider", "F. Nater", "L.V. Gool"], "venue": "ICCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch Normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "What makes an image memorable", "author": ["P. Isola", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACM MM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-domain learning methods for high-level visual concept classification", "author": ["W. Jiang", "E. Zavesky", "S.-F. Chang", "A. Loui"], "venue": "ICIP,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain adaptive semantic diffusion for large scale context-based video annotation", "author": ["Y.-G. Jiang", "J. Wang", "S.-F. Chang", "C.-W. Ngo"], "venue": "ICCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting emotions in user-generated videos", "author": ["Y.-G. Jiang", "B. Xu", "X. Xue"], "venue": "AAAI,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting viewer perceived emotions in animated GIFs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "ACM MM,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual affect around the world: A large-scale multilingual visual sentiment ontology", "author": ["B. Jou", "T. Chen", "N. Pappas", "M. Redi", "M. Topkara", "S.-F. Chang"], "venue": "ACM MM,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "The design of high-level features for photo quality assessment", "author": ["Y. Ke", "X. Tang", "F. Jing"], "venue": "CVPR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "What makes an image popular", "author": ["A. Khosla", "A. Das Sarma", "R. Hamid"], "venue": "In WWW,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang"], "venue": "NAACL,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["M.-T. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "ICLR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "ACM MM,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE TKDE, 22(10),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Emotion: A Psychoevolutionary Synthesis", "author": ["R. Plutchik"], "venue": "Harper & Row,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1980}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["T. Raiko", "H. Valpola", "Y. LeCun"], "venue": "AISTATS,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Massively multitask networks for drug discovery", "author": ["B. Ramsundar", "S. Kearnes", "P. Riley", "D. Webster", "D. Konerding", "V. Pande"], "venue": "arXiv preprint arXiv:1502.02072,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV, 115(3),", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 15(1),", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "ICMLW,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In CVPR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Exploring inter-feature and inter-class relationships with deep neural networks for video classification", "author": ["Z. Wu", "Y.-G. Jiang", "J. Wang", "J. Pu", "X. Xue"], "venue": "ACM MM,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Emotional valence categorization using holistic image features", "author": ["V. Yanulevskaya", "J. van Gemert", "K. Roth", "A. Herbold", "N. Sebe", "J.M. Geusebroek"], "venue": "In ICIP,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Rotating your face using multi-task deep neural network", "author": ["J. Yim", "H. Jung", "B.-I. Yoo", "C. Choi", "D. Park", "J. Kim"], "venue": "CVPR,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "While some recent approaches have begun to model these relationships in deep architectures [8, 42], still many multimedia solutions tend to have multiple parts that specialize rather than a more versatile, general solution that leverages cross-task dependencies.", "startOffset": 91, "endOffset": 98}, {"referenceID": 41, "context": "While some recent approaches have begun to model these relationships in deep architectures [8, 42], still many multimedia solutions tend to have multiple parts that specialize rather than a more versatile, general solution that leverages cross-task dependencies.", "startOffset": 91, "endOffset": 98}, {"referenceID": 1, "context": "In [2], a semantic construct called adjective-noun pairs (ANPs) was proposed wherein there are visual concept pairs like \u2018happy girl\u2019, \u2018misty woods\u2019 and \u2018good food\u2019.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "To understand how \u201crelatedness\u201d is both important and applicable to visual concept detection, consider several example images and concepts from [2] in Figure 2.", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "In [14], residual learning is proposed as an approach for enabling much deeper networks while addressing the degradation problem where very deep networks have a tendency to underfit compared to shallower counterpart networks [40].", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [14], residual learning is proposed as an approach for enabling much deeper networks while addressing the degradation problem where very deep networks have a tendency to underfit compared to shallower counterpart networks [40].", "startOffset": 225, "endOffset": 229}, {"referenceID": 33, "context": "In residual learning, an identity mapping through the use of shortcut connections [34] is proposed where an underlying mapping H(x) = F(x) + x is learned given that F(x) = H(x)\u2212 x represents another mapping fit by several stacked layers.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "However, in [14], it is argued that optimization software may actually have difficulty with approximating identity mappings with a stack of nonlinear layers, and also that for prediction problems, it is unlikely that the strict identity is optimal.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Our work extends residual learning [14] to also integrate information from other related tasks enabling cross-task representations.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "Our contributions include (1) the proposal of a novel extension of residual learning [14] using cross-connections for coupling multiple related tasks in a setting called crossresidual learning (CRL), (2) the development of a multitask network with a fan-out architecture using cross-residual layers, and (3) an evaluation of cross-residual networks on a multitask visual sentiment concept detection problem yielding a single network with very competitive or even better accuracy compared to individual networks on three classification tasks (noun, adjective, and adjective-noun pair detection) but uses >40% less model memory via branching, while also outperforming the predictive performance of a standard multitask configuration without cross-residuals by about 10.", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "In transfer learning [32], the domain, tasks and distributions are allowed to be different in both training/source and testing/target.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "jointly classifying and reconstructing objects, and is often referred to as multitask learning [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "classifying clothing style across cultures, and is sometimes called cross-domain learning [20] or domain transfer/adaptation [11, 21].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "classifying clothing style across cultures, and is sometimes called cross-domain learning [20] or domain transfer/adaptation [11, 21].", "startOffset": 125, "endOffset": 133}, {"referenceID": 20, "context": "classifying clothing style across cultures, and is sometimes called cross-domain learning [20] or domain transfer/adaptation [11, 21].", "startOffset": 125, "endOffset": 133}, {"referenceID": 4, "context": "One early work in [5] showed how a single network could be trained to solve multiple natural language processing tasks simultaneously like part-of-speech tagging, named entity recognition, etc.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "Multitask networks have since proven effective for automated drug discovery [6, 35], query classification and retrieval [28], and semantic segmentation [7].", "startOffset": 76, "endOffset": 83}, {"referenceID": 34, "context": "Multitask networks have since proven effective for automated drug discovery [6, 35], query classification and retrieval [28], and semantic segmentation [7].", "startOffset": 76, "endOffset": 83}, {"referenceID": 27, "context": "Multitask networks have since proven effective for automated drug discovery [6, 35], query classification and retrieval [28], and semantic segmentation [7].", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "Multitask networks have since proven effective for automated drug discovery [6, 35], query classification and retrieval [28], and semantic segmentation [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "Recently, [10] proposed multitask auto-encoders for generalizing object detectors across domains; and in [29], multitask sequence-to-sequence learning is proposed for text translation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "Recently, [10] proposed multitask auto-encoders for generalizing object detectors across domains; and in [29], multitask sequence-to-sequence learning is proposed for text translation.", "startOffset": 105, "endOffset": 109}, {"referenceID": 35, "context": "Also, architectures like [36, 44] can be categorized as multitask networks since they reconstruct and classify simultaneously.", "startOffset": 25, "endOffset": 33}, {"referenceID": 43, "context": "Also, architectures like [36, 44] can be categorized as multitask networks since they reconstruct and classify simultaneously.", "startOffset": 25, "endOffset": 33}, {"referenceID": 35, "context": "Unlike other multitask networks but similar to ladder networks [36], instead of a single branching point in our network that creates forked paths to only specialize to individual tasks, we continue mixing information even after branching via our cross-skip connections.", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "For example, graph diffusion can be used smooth decision scores for leveraging intra-relatedness between categories [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "In [8], instead of an undirected graph, explicit directed edges were used to model class relationships like exclusion and subsumption.", "startOffset": 3, "endOffset": 6}, {"referenceID": 41, "context": "And with some semblance to our work, in [42], a multimodal neural network structure is developed where inter-class (but still intra-task) relationships are integrated as an explicit regularizer.", "startOffset": 40, "endOffset": 44}, {"referenceID": 41, "context": "Although inspired from multitask learning, the network design in [42] still operates in a single-task context as there is only a single output network head.", "startOffset": 65, "endOffset": 69}, {"referenceID": 42, "context": "In [43], a codebook over local color histogram and Gabor features were proposed for image-based sentiment prediction; and in [30], psychology and art theory inspired features were proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [43], a codebook over local color histogram and Gabor features were proposed for image-based sentiment prediction; and in [30], psychology and art theory inspired features were proposed.", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "Again, in [2], adjective-noun pairs were proposed as a mid-level semantic construct and an ontology was mined from a popular social multimedia platform using psychology-grounded seed queries [33].", "startOffset": 10, "endOffset": 13}, {"referenceID": 32, "context": "Again, in [2], adjective-noun pairs were proposed as a mid-level semantic construct and an ontology was mined from a popular social multimedia platform using psychology-grounded seed queries [33].", "startOffset": 191, "endOffset": 195}, {"referenceID": 24, "context": "Other problems related to affect detection include quality assessment [25], memorability [18], interestingness [12] and popularity [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Other problems related to affect detection include quality assessment [25], memorability [18], interestingness [12] and popularity [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "Other problems related to affect detection include quality assessment [25], memorability [18], interestingness [12] and popularity [26].", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "Other problems related to affect detection include quality assessment [25], memorability [18], interestingness [12] and popularity [26].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Given an input x and output y vector to a residual learning layer and the mapping function F(x, {Wi}) to fit, where for vision problems this might represent, for example, a stack of convolutional operations with batch normalization [17] and ReLU activation [31], we have the following in residual learning [14]:", "startOffset": 232, "endOffset": 236}, {"referenceID": 30, "context": "Given an input x and output y vector to a residual learning layer and the mapping function F(x, {Wi}) to fit, where for vision problems this might represent, for example, a stack of convolutional operations with batch normalization [17] and ReLU activation [31], we have the following in residual learning [14]:", "startOffset": 257, "endOffset": 261}, {"referenceID": 13, "context": "Given an input x and output y vector to a residual learning layer and the mapping function F(x, {Wi}) to fit, where for vision problems this might represent, for example, a stack of convolutional operations with batch normalization [17] and ReLU activation [31], we have the following in residual learning [14]:", "startOffset": 306, "endOffset": 310}, {"referenceID": 13, "context": "Here, we propose a simple and efficient extension of [14] when fitting across multiple related learning tasks which we refer to as cross-residual learning (CRL).", "startOffset": 53, "endOffset": 57}, {"referenceID": 38, "context": "Cross-residual layers thus serve as a type of in-network regularization somewhat similar to dropout [39], though with less stochasticity.", "startOffset": 100, "endOffset": 104}, {"referenceID": 39, "context": "Connection to Highway Networks [40] & LSTM [16].", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "Connection to Highway Networks [40] & LSTM [16].", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "As also discussed in [14], residual networks can be seen as highway networks [40] that do not have transform or carry gates.", "startOffset": 21, "endOffset": 25}, {"referenceID": 39, "context": "As also discussed in [14], residual networks can be seen as highway networks [40] that do not have transform or carry gates.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Similarly, it has been argued (though somewhat reductionist) that residual layers can also be viewed as a feedforward long short-term memory (LSTM) [16] units without gates.", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "Specifically, consider the LSTM version from [9]:", "startOffset": 45, "endOffset": 48}, {"referenceID": 39, "context": "And indeed, this is much like highway networks\u2019 carry gate, since highway layers can be viewed as feed-forward LSTMs with only forget gates [40].", "startOffset": 140, "endOffset": 144}, {"referenceID": 35, "context": "Similarities to Ladder Networks [36].", "startOffset": 32, "endOffset": 36}, {"referenceID": 35, "context": "Structurally, the building blocks of cross-residual learning bears some resemblance to the layout in ladder networks [36].", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "While there may be a number of settings that would benefit from cross-residual learning, we illustrate one natural setting here in multitask learning [3].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "To implement a multitask network, a common approach [5, 10, 28, 35] is to introduce a branching point in the architecture that leads to one network head per task, e.", "startOffset": 52, "endOffset": 67}, {"referenceID": 9, "context": "To implement a multitask network, a common approach [5, 10, 28, 35] is to introduce a branching point in the architecture that leads to one network head per task, e.", "startOffset": 52, "endOffset": 67}, {"referenceID": 27, "context": "To implement a multitask network, a common approach [5, 10, 28, 35] is to introduce a branching point in the architecture that leads to one network head per task, e.", "startOffset": 52, "endOffset": 67}, {"referenceID": 34, "context": "To implement a multitask network, a common approach [5, 10, 28, 35] is to introduce a branching point in the architecture that leads to one network head per task, e.", "startOffset": 52, "endOffset": 67}, {"referenceID": 1, "context": "In particular, we use the Visual Sentiment Ontology (VSO) [2] and cast affective mid-level concept detection as a multitask learning problem.", "startOffset": 58, "endOffset": 61}, {"referenceID": 23, "context": "We chose the VSO dataset for our preliminary experiments because it presents a visual affect challenge currently of rising interest in the multimedia community as VSO has since had multilingual extensions [24] and been applied in aesthetics understanding [1], emotion prediction [22, 23], popularity modeling [26], and more.", "startOffset": 205, "endOffset": 209}, {"referenceID": 0, "context": "We chose the VSO dataset for our preliminary experiments because it presents a visual affect challenge currently of rising interest in the multimedia community as VSO has since had multilingual extensions [24] and been applied in aesthetics understanding [1], emotion prediction [22, 23], popularity modeling [26], and more.", "startOffset": 255, "endOffset": 258}, {"referenceID": 21, "context": "We chose the VSO dataset for our preliminary experiments because it presents a visual affect challenge currently of rising interest in the multimedia community as VSO has since had multilingual extensions [24] and been applied in aesthetics understanding [1], emotion prediction [22, 23], popularity modeling [26], and more.", "startOffset": 279, "endOffset": 287}, {"referenceID": 22, "context": "We chose the VSO dataset for our preliminary experiments because it presents a visual affect challenge currently of rising interest in the multimedia community as VSO has since had multilingual extensions [24] and been applied in aesthetics understanding [1], emotion prediction [22, 23], popularity modeling [26], and more.", "startOffset": 279, "endOffset": 287}, {"referenceID": 25, "context": "We chose the VSO dataset for our preliminary experiments because it presents a visual affect challenge currently of rising interest in the multimedia community as VSO has since had multilingual extensions [24] and been applied in aesthetics understanding [1], emotion prediction [22, 23], popularity modeling [26], and more.", "startOffset": 309, "endOffset": 313}, {"referenceID": 1, "context": "We additionally note that even though VSO [2] argues that the noun component of the ANP serves to visually ground the mid-level concept, no experiments were actually ever run to determine the performance of detecting adjective (or even, noun) concepts separately.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Briefly, the data in VSO [2] was originally collected from", "startOffset": 25, "endOffset": 28}, {"referenceID": 32, "context": "the social multimedia platform, Flickr, using psychologygrounded seed queries from Plutchik\u2019s Wheel of Emotions [33] which consists of 24 basic emotions, such as joy, terror, and anticipation.", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "The image dataset in VSO [2] has a long tail distribution where some adjective-noun pair concepts are singletons and do not share any adjectives or nouns with other concept pairs.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "The original VSO dataset [2] consists of a refined set of 1,200 ANP concepts.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "All our residual layers use\u201cB option\u201dshortcut connections as detailed in [14] where projections are only used when matching dimensions (stride 2) and other shortcuts are identity.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Except for cross-residual weight layers, projections are performed with a 1 \u00d7 1 convolution with ReLU activation and batch normalization as in [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 36, "context": "For training multitask networks, we initialized most layers using weights from a residual network (ResNet) model trained on ILSVRC-2015 [37], but done such that for layers after the branching point in our network we initialize them to the same corresponding layer weights in the original ResNet model.", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "For cross-residual weight layers, we follow [14] and initialize them as in [13], i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "For cross-residual weight layers, we follow [14] and initialize them as in [13], i.", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": "No dropout [39] was used in residual or cross-residual networks.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "All networks and experiments were run using a single NVIDIA GeForce GTX Titan X GPU and implemented with Caffe [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "We baseline against four single-task architectures: a variant of AlexNet [27] swapping pooling and normalization layers called CaffeNet [19], the first iteration of the GoogLeNet architecture [41] denoted as Inception-v1 which uses a bottlenecked 5\u00d7 5 convolution in the sub-modules, the 16-layer version of VggNet [38] (VggNet-16), and the ResNet architecture [14] with 50-layers (ResNet-50).", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "We baseline against four single-task architectures: a variant of AlexNet [27] swapping pooling and normalization layers called CaffeNet [19], the first iteration of the GoogLeNet architecture [41] denoted as Inception-v1 which uses a bottlenecked 5\u00d7 5 convolution in the sub-modules, the 16-layer version of VggNet [38] (VggNet-16), and the ResNet architecture [14] with 50-layers (ResNet-50).", "startOffset": 136, "endOffset": 140}, {"referenceID": 40, "context": "We baseline against four single-task architectures: a variant of AlexNet [27] swapping pooling and normalization layers called CaffeNet [19], the first iteration of the GoogLeNet architecture [41] denoted as Inception-v1 which uses a bottlenecked 5\u00d7 5 convolution in the sub-modules, the 16-layer version of VggNet [38] (VggNet-16), and the ResNet architecture [14] with 50-layers (ResNet-50).", "startOffset": 192, "endOffset": 196}, {"referenceID": 37, "context": "We baseline against four single-task architectures: a variant of AlexNet [27] swapping pooling and normalization layers called CaffeNet [19], the first iteration of the GoogLeNet architecture [41] denoted as Inception-v1 which uses a bottlenecked 5\u00d7 5 convolution in the sub-modules, the 16-layer version of VggNet [38] (VggNet-16), and the ResNet architecture [14] with 50-layers (ResNet-50).", "startOffset": 315, "endOffset": 319}, {"referenceID": 13, "context": "We baseline against four single-task architectures: a variant of AlexNet [27] swapping pooling and normalization layers called CaffeNet [19], the first iteration of the GoogLeNet architecture [41] denoted as Inception-v1 which uses a bottlenecked 5\u00d7 5 convolution in the sub-modules, the 16-layer version of VggNet [38] (VggNet-16), and the ResNet architecture [14] with 50-layers (ResNet-50).", "startOffset": 361, "endOffset": 365}, {"referenceID": 3, "context": "In addition, we also evaluated against DeepSentiBank [4], also an AlexNet-styled model trained on the full, unrestricted VSO data [2] to detect 2,089 ANPs.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "In addition, we also evaluated against DeepSentiBank [4], also an AlexNet-styled model trained on the full, unrestricted VSO data [2] to detect 2,089 ANPs.", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "We did not retrain [4] but rather reevaluated their model on the subset of 553 ANP concepts we focus on here; however, since we do not know the train and test image splits that they used, the result provided for DeepSentiBank [4] could still be an over-estimate.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "We did not retrain [4] but rather reevaluated their model on the subset of 553 ANP concepts we focus on here; however, since we do not know the train and test image splits that they used, the result provided for DeepSentiBank [4] could still be an over-estimate.", "startOffset": 226, "endOffset": 229}, {"referenceID": 40, "context": "For network parameter costs, note that for Inception-v1 [41] we did not count the parameters from auxiliary heads although they are used during training.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "In general, as originally posited by the VSO work [2], in terms of problem difficulty ordering, noun prediction is indeed \u201ceasier\u201d as visual recognition task than adjective prediction.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "However, though not in stark contrast to [2], and although there are indeed more ANP classes than nouns and adjectives, we still did expect to observe higher accuracy rates for ANP concept detection than we did, expecting that the rates would be much closer to that of noun detection and not lower than adjective detection since [2] argues that adjectives lack visual grounding.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "However, though not in stark contrast to [2], and although there are indeed more ANP classes than nouns and adjectives, we still did expect to observe higher accuracy rates for ANP concept detection than we did, expecting that the rates would be much closer to that of noun detection and not lower than adjective detection since [2] argues that adjectives lack visual grounding.", "startOffset": 329, "endOffset": 332}, {"referenceID": 3, "context": "DeepSentiBank [4] ANP 65.", "startOffset": 14, "endOffset": 17}, {"referenceID": 18, "context": "CaffeNet [19] Noun 57.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "Inception-v1 [41] Noun 10.", "startOffset": 13, "endOffset": 17}, {"referenceID": 37, "context": "VggNet-16 [38] Noun 134.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "ResNet-50 [14] Noun 23.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Additionally, we note that though the weight magnitudes are indeed small, this also follows from intuition in the original residual network work [14] that these small, but non-zero weights are precisely what enable residual networks to be made very deep.", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "Architecturally, while we only explored the canonical shortcut connections of [14] and used a channelwise scaling layer for the cross-residual, there is recent work exploring different types of transforms and gating on shortcuts [15] that can also be extended to the self- and cross-connections in cross-residual networks.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "Architecturally, while we only explored the canonical shortcut connections of [14] and used a channelwise scaling layer for the cross-residual, there is recent work exploring different types of transforms and gating on shortcuts [15] that can also be extended to the self- and cross-connections in cross-residual networks.", "startOffset": 229, "endOffset": 233}], "year": 2016, "abstractText": "Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of innetwork regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with >40% less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4% over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting.", "creator": "LaTeX with hyperref package"}}}