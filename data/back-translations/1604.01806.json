{"id": "1604.01806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Generalising the Discriminative Restricted Boltzmann Machine", "abstract": "We present a new theoretical result that generalizes the Discriminative Restricted Boltzmann Machine (DRBM). While the DRBM was originally defined under the assumption of the {0, 1} Bernoulli distribution in each of its hidden units, this result enables the derivation of cost functions for variants of the DRBM that use other distributions, including some that are common in the literature, illustrated by the binomical and {-1, + 1} Bernoulli distributions. We evaluate these two DRBM variants and compare them with the original using three benchmark datasets, namely the MNIST and USPS number classification datasets, and the 20 newsgroups document classification datasets. The results show that each of the three compared models exceeds the remaining two in one of the three datasets, suggesting that the proposed theoretical generalization of the DRBM may be valuable in practice.", "histories": [["v1", "Wed, 6 Apr 2016 21:01:35 GMT  (31kb)", "http://arxiv.org/abs/1604.01806v1", "Submitted to ECML 2016 conference track"]], "COMMENTS": "Submitted to ECML 2016 conference track", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["srikanth cherla", "son n tran", "tillman weyde", "artur d'avila garcez"], "accepted": false, "id": "1604.01806"}, "pdf": {"name": "1604.01806.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["srikanth.cherla.1@city.ac.uk", "son.tran.1@city.ac.uk", "t.e.weyde@city.ac.uk", "a.garcez@city.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n01 80\n6v 1\n[ cs\n.L G\n] 6\nA pr\nKeywords: restricted boltzmann machine, discriminative learning, hidden layer activation function"}, {"heading": "1 Introduction", "text": "The Restricted Boltzmann Machine (RBM) is a generative latent-variable model which models the joint distribution of a set of input variables. It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13]. One of its applications is as a standalone classifier, referred to as the Discriminative Restricted Boltzmann Machine (DRBM). As the name might suggest, the DRBM is a classifier obtained by carrying out discriminative learning in the RBM and it directly models the conditional distribution one is interested in for prediction. This bypasses one of the key problems faced in learning the parameters of the RBM generatively, which is the computation of the intractable partition function (discussed in Section 3). In the DRBM this partition function is cancelled out in the expression for the conditional distribution thus simplifying the learning process.\nIn the original work that introduced the DRBM its cost function was derived under the assumption that each of its hidden units models the {0, 1}- Bernoulli distribution. We observe that while effort has gone into enhancing the performance of a few other connectionist models by changing the nature of their hidden units, this has not been attempted with the DRBM. So in this paper, we first describe a novel theoretical result that makes it possible to generalise the model\u2019s cost function. The result is then used to derive two new cost functions corresponding to DRBMs containing hidden units with the Binomial and {\u22121,+1}-Bernoulli distributions respectively. These two variants are evaluated and compared with the original DRBM on the benchmark MNIST and USPS digit classification datasets, and the 20 Newsgroups document classification dataset. We find that each of the three compared models outperforms the remaining two in one of the three datasets, thus indicating that the proposed theoretical generalisation of the DRBM may be valuable in practice.\nThe next section describes the motivation for this paper and covers the related work that led to the ideas presented here. This is followed by an introduction to the Restricted Boltzmann Machine (RBM) in Section 3 and its discriminative counterpart, the Discriminative RBM (DRBM) in Section 4. The latter of these is the focus of this paper. Section 5 first presents the novel theoretical result that generalises this model, and then its two aforementioned variants that follow as a consequence. These are evaluated on three benchmark datasets, and the results discussed in Section 6. Section 7 presents a summary, together with potential extensions of this work."}, {"heading": "2 Related Work", "text": "The subject of activation functions in has received attention from time to time in the literature. Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3]. In the case of feedforward neural networks, while [4] concluded in favour of the Logistic Sigmoid activation function, recent work in [2] found a drawback in its use with deep feedforward networks and suggested the Hyperbolic Tangent function as a more suitable alternative to it. Even more recently, [3] highlighted the biological plausibility of the Rectified Linear activation function and its potential to outperform both the aforementioned activations without even the need to carry out unsupervised pre-training.\nThe RBM was originally defined as a model for binary-valued variables whose values are governed by the {0, 1}-Bernoulli distribution. An early variant of this model [1] known as the Influence Combination Machine, while still modelling binary data, employed units of values {\u22121,+1}. In [20], an extension of the standard RBM to model real-valued variables in its visible layers was proposed with the aid of the Gaussian activation function. With the same goal of modelling non-binary variables, the rate-coded RBM [18] was introduced in which both the visible and hidden layers could model integer-values by grouping the activations\nof sets of binary units all of which share the same values of weights. This idea was extended in [14] which introduced Rectified Linear activations for the hidden layer of the RBM. In the context of topic modelling, the replicated softmax RBM [6] was introduced which models the categorical distribution in its visible layer. All these variants of the RBM have been shown to be better suited for certain specific learning tasks.\nIt is often the case that a new type of activation function results in an improvement in the performance of an existing model or in a new insight into the behaviour of the model itself. In the least, it offers researchers with the choice of a new modelling alternative. This is the motivation for the present work. As outlined above, effort has gone into enhancing the performance of a few models in this way. We observe that it has not, however, been attempted with the DRBM. So here we address this by first introducing two new variants of the DRBM based on a novel theoretical result, and then comparing the performance of these and the original {0, 1}-Bernoulli DRBM on benchmark datasets."}, {"heading": "3 Restricted Boltzmann Machine", "text": "The Restricted Boltzmann Machine (RBM) [16] is an undirected bipartite graphical model. It contains a set of visible units v \u2208 Rnv and a set of hidden units h \u2208 Rnh which make up its visible and hidden layers respectively. The two layers are fully inter-connected but there exist no connections between any two hidden units, or any two visible units. Additionally, the units of each layer are connected to a bias unit whose value is always 1. The edge between the ith visible unit and the jth hidden unit is associated with a weight wij . All these weights are together represented as a weight matrix W \u2208 Rnv\u00d7nh . The weights of connections between visible units and the bias unit are contained in a visible bias vector b \u2208 Rnv . Likewise, for the hidden units there is a hidden bias vector c \u2208 Rnh . The RBM is fully characterized by the parameters W , b and c. Its bipartite structure is illustrated in Figure 1.\nThe RBM is a special case of the Boltzmann Machine \u2014 an energy-based model [11] \u2014 which gives the joint probability of every possible pair of visible and hidden vectors via an energy function E, according to the equation\nP (v,h) = 1\nZ e\u2212E(v,h) (1)\nwhere the \u201cpartition function\u201d, Z, is given by summing over all possible pairs of visible and hidden vectors\nZ = \u2211\nv,h\ne\u2212E(v,h) . (2)\nand ensures that P (v,h) is a probability. The joint probability assigned by the model to the elements of a visible vector v, is given by summing (marginalising) over all possible hidden vectors:\nP (v) = 1\nZ\n\u2211\nh\ne\u2212E(v,h) (3)\nIn the case of the RBM, the energy function E is given by\nE(v,h) = \u2212b\u22a4v \u2212 c\u22a4h\u2212 v\u22a4Wh . (4)\nIn its original form, the RBM models the Bernoulli distribution in its visible and hidden layers. The activation probabilities of the units in the hidden layer given the visible layer (and vice versa) are P (h = 1|v) = \u03c3(c + W\u22a4v) and P (v = 1|h) = \u03c3(b+Wh) respectively, where \u03c3(x) is the logistic sigmoid function \u03c3(x) = (1 + e\u2212x)\u22121 applied element-wise to the vector x.\nLearning in energy-based models can be carried generatively, by determining the weights and biases that minimize the overall energy of the system with respect to the training data. This amounts to maximizing the log-likelihood function L over the training data V (containing N examples), which is given by\nL = 1\nN\nN \u2211\nn=1\nlogP (vn) (5)\nwhere P (v) is the joint probability distribution given by\nP (v) = e\u2212Efree(v)\nZ , (6)\nwith Z = \u2211 v e \u2212Efree(v), and\nEfree(v) = \u2212 log \u2211\nh\ne\u2212E(v,h) . (7)\nThe probability that the RBM assigns to a vector vn belonging to the training data can be raised by adjusting the weights and biases to lower the energy associated with that vector and to raise the energy associated with others not in\nthe training data. Learning can be carried out using gradient-based optimisation, for which the gradient of the log-likelihood function with respect to the RBM\u2019s parameters \u03b8 needs to be calculated first. This is given by\n\u2202L \u2202\u03b8 = \u2212\n\u2329\n\u2202Efree\n\u2202\u03b8\n\u232a\n0\n+\n\u2329\n\u2202Efree\n\u2202\u03b8\n\u232a\n\u221e\n(8)\nwhere \u3008\u00b7\u30090 denotes the average with respect to the data distribution, and \u3008\u00b7\u3009\u221e that with respect to the model distribution. The former is readily computed using the training data V , but the latter involves the normalisation constant Z, which very often cannot be computed efficiently as it involves a sum over an exponential number of terms. To avoid the difficulty in computing the above gradient, an efficiently computable and widely adopted approximation of the gradient was proposed in the Contrastive Divergence method [19]."}, {"heading": "4 The Discriminative Restricted Boltzmann Machine", "text": "The generative RBM described above models the joint probability P (v) of the set of binary variables v. For prediction, one is interested in a conditional distribution of the form P (y|x). It has been demonstrated in [9] how discriminative learning can be carried out in the RBM, thus making it feasible to use it as a standalone classifier. This is done by assuming one subset of its visible units to be inputs x, and the remaining a set of categorical units y representing the classconditional probabilities P (y|x). This is illustrated in Figure 1. The weight matrix W can be interpreted as two matrices R \u2208 Rni\u00d7nh and U \u2208 Rnc\u00d7nh , where ni is the input dimensionality and nc is the number of classes and nv = ni +nc. Likewise, the visible bias vector b \u2208 Rnv is also split into a set of two bias vectors \u2014 a vector a \u2208 Rni and a second vector d \u2208 Rnc , as shown in Figure 1.\nThe posterior probability in this Discriminative RBM can be inferred as\nP (y|x) = exp (\u2212Efree(x,y)) \u2211\ny\u2217 exp (\u2212Efree(x,y \u2217))\n(9)\nwhere x is the input vector, and y is the one-hot encoding of the class-label. The denominator sums over all class-labels y\u2217 to make P (y|x) a probability distribution. In the original RBM, x and y together make up the visible layer v. The model is learned discriminatively by maximizing the log-likelihood function based on the following expression of the conditional distribution:\nP (y|x) = exp(dy)\n\u220f\nj(1 + exp(r \u22a4 j x+ uyj + cj))\n\u2211 y\u2217 exp(dy\u2217) \u220f j(1 + exp(r \u22a4 j x+ uy\u2217j + cj))\n. (10)\nThe gradient of this function, for a single input-label pair (xi, yi) with respect to its parameters \u03b8 can be computed analytically. It is given by\n\u2202 logP (yi|xi)\n\u2202\u03b8 =\n\u2211\nj\n\u03c3 (oyj (xi)) \u2202oyj (xi)\n\u2202\u03b8\n\u2212 \u2211\nj,y\u2217\n\u03c3 (oy\u2217j (xi)) p (y \u2217|xi)\n\u2202oy\u2217j (xi)\n\u2202\u03b8\n(11)\nwhere oyj(x) = cj + r \u22a4 j x + uyj. Note that in order to compute the conditional distribution in (10) the model does not have to be learned discriminatively, and one can also use the above generatively learned RBM as it learns the joint distribution p(y,x) from which p(y|x) can be inferred."}, {"heading": "5 Generalising the DRBM", "text": "This section describes a novel generalisation of the cost function of the DRBM [9]. This facilitates the formulation of similar cost functions for variants of the model with other distributions in their hidden units that are commonly encountered in the literature. This is illustrated here first with the {\u22121,+1}-Bernoulli distribution and then the Binomial distribution."}, {"heading": "5.1 Generalising the Conditional Probability", "text": "We begin with the expression for the conditional distribution P (y|x), as derived in [9]. This is given by\nP (y|x) =\n\u2211\nh P (x,y,h) \u2211\ny\u2217\n\u2211\nh P (x,y \u2217,h)\n=\n\u2211\nh exp (\u2212E (x,y,h)) \u2211\ny\u2217\n\u2211\nh exp (\u2212E (x,y \u2217,h))\n(12)\nwhere y is the one-hot encoding of a class label y, and \u2212 log \u2211\nh exp(\u2212E(x,y,h)) is the Free Energy Efree of the RBM. We consider the term containing the summation over h in (12):\nexp (Efree(x,y)) = \u2212 \u2211\nh\nexp (\u2212E (x,y,h))\n= \u2212 \u2211\nh\nexp\n\n\n\u2211\ni,j\nxiwijyj + \u2211\nj\nuyjhj + \u2211\ni\naixi + by + \u2211\nj\ncjhj\n\n\n= \u2212 exp\n(\n\u2211\ni\naixi + by\n)\n\u2211\nh\nexp\n\n\n\u2211\nj\nhj \u2211\ni\nxiwij + uyj + cj\n\n\n(13)\nNow consider only the second term of the product in (13). We simplify it by re-writing \u2211\ni xiwij + uyj + cj as \u03b1j . Thus, we have\n\u2211\nh\nexp\n\n\n\u2211\nj\nhj \u2211\ni\nxiwij + uyj + cj\n\n = \u2211\nh\nexp\n\n\n\u2211\nj\nhj\u03b1j\n\n\n= \u2211\nh\n\u220f\nj\nexp (hj\u03b1j)\n= \u220f\nj\n\u2211\nk\nexp (sk\u03b1j)\n(14)\nwhere sk is each of the k states that can be assumed by each hidden unit j of the model. The last step of (14) results from re-arranging the terms after expanding the summation and product over h and j in the previous step respectively. The summation \u2211\nh over all the possible hidden layer vectors h can be replaced by the summation \u2211\nk over the states of the units in the layer. The number and values of these states depend on the nature of the distribution in question (for instance {0, 1} in the original DRBM). The result in (14) can be applied to (13) and, in turn, to (12) to get the following general expression of the conditional probability P (y|x):\nP (y|x) = exp (by)\n\u220f\nj\n\u2211\nk exp (sk\u03b1j) \u2211\ny\u2217 exp (by\u2217) \u220f j \u2211 k exp ( sk\u03b1 \u2217 j )\n= exp (by)\n\u220f\nj\n\u2211 k exp (sk \u2211\ni xiwij + uyj + cj) \u2211\ny\u2217 exp (by\u2217) \u220f j \u2211 k exp (sk \u2211 i xiwij + uy\u2217j + cj)\n(15)\nThe result in (15) generalises the conditional probability of the DRBM first introduced in [9]. The term inside the summation over k can be viewed as a product between \u03b1j corresponding to each hidden unit j and each possible state sk of this hidden unit. Knowing this makes it possible to extend the original DRBM to be governed by other types of distributions in the hidden layer."}, {"heading": "5.2 Extensions to other Hidden Layer Distributions", "text": "We first use the result in (15) to derive the expression for the conditional probability P (y|x) in the original DRBM [9]. This will be followed by its extension, first to the {\u22121,+1}-Bernoulli distribution (referred to here as the Bipolar DRBM )and then the Binomial distribution (the Binomial DRBM ). Section 6 presents a comparison between the performance of the DRBM with these different activations.\nDRBM: The {0, 1}-Bernoulli DRBM corresponds to the model originally introduced in [9]. In this case, each hidden unit hj can either be a 0 or a 1, i.e.\nsk = {0, 1}. This reduces P (y|x) in (15) to\nPber (y|x) = exp (by)\n\u220f\nj\n\u2211\nsk\u2208{0,1} exp (sk\u03b1j)\n\u2211 y\u2217 exp (by\u2217) \u220f j \u2211 sk\u2208{0,1} exp\n(\nsk\u03b1 \u2217 j\n)\n= exp (by)\n\u220f\nj (1 + exp (\u03b1j)) \u2211\ny\u2217 exp (by\u2217) \u220f j\n( 1 + exp ( \u03b1\u2217j ))\n(16)\nwhich is identical to the result obtained in [9].\nBipolar DRBM: A straightforward adaptation to the DRBM involves replacing its hidden layer states by {\u22121,+1} as previously done in [1] in the case of the RBM. This is straightforward because in both cases the hidden states of the models are governed by the Bernoulli distribution, however, in the latter case each hidden unit hj can either be a \u22121 or a +1, i.e. sk = {\u22121,+1}. Applying this property to (15) results in the following expression for P (y|x):\nPbip (y|x) = exp (by)\n\u220f\nj\n\u2211\nsk\u2208{\u22121,+1} exp (sk\u03b1j)\n\u2211 y\u2217 exp (by\u2217) \u220f j \u2211 sk\u2208{\u22121,+1} exp\n(\nsk\u03b1 \u2217 j\n)\n= exp (by)\n\u220f\nj (exp (\u2212\u03b1j) + exp (\u03b1j)) \u2211\ny\u2217 exp (by\u2217) \u220f j\n( exp ( \u2212\u03b1\u2217j ) + exp ( \u03b1\u2217j )) .\n(17)\nBinomial DRBM: It was demonstrated in [18] how groups of N (where N is a positive integer greater than 1) stochastic units of the standard RBM can be combined in order to approximate discrete-valued functions in its visible layer and hidden layers to increase its representational power. This is done by replicating each unit of one layer N times and keeping the weights of all connections to each of these units from a given unit in the other layer identical. The key advantage for adopting this approach was that the learning algorithm remained unchanged. The number of these \u201creplicas\u201d of the same unit whose values are simultaneously 1 determines the effective integer value (in the range [0, N ]) of the composite unit, thus allowing it to assume multiple values. The resulting model was referred to there as the Rate-Coded RBM (RBMrate).\nThe intuition behind this idea can be extended to the DRBM by allowing the states sk of each hidden unit to assume integer values in the range [0, N ]. The summation in (15) would then be SN = \u2211N\nsk=0 exp (sk\u03b1j), which simplifies\nas below\nSN =\nN \u2211\nsk=0\nexp (sk\u03b1j)\n= 1 + exp (\u03b1j)\n(N\u22121) \u2211\nsk=0\nexp (sk\u03b1j)\n= 1\u2212 exp ((N + 1)\u03b1j)\n1\u2212 exp (\u03b1j)\n(18)\nin (15) to give\nPbin (y|x) = exp (by)\n\u220f\nj\n\u2211N\nsk=0 exp (sk\u03b1j)\n\u2211 y\u2217 exp (by\u2217) \u220f j \u2211N sk=0 exp\n(\nsk\u03b1 \u2217 j\n)\n= exp (by)\n\u220f\nj 1\u2212exp((N+1)\u03b1j) 1\u2212exp(\u03b1j)\n\u2211 y\u2217 exp (by\u2217) \u220f j 1\u2212exp((N+1)\u03b1\u2217j ) 1\u2212exp(\u03b1\u2217j )\n.\n(19)"}, {"heading": "6 Experiments", "text": "We evaluated the Bipolar and the Binomial DRBMs on three benchmark machine learning datasets. These are two handwritten digit racognition datasets \u2014 USPS [5] and MNIST [10], and one document classification dataset \u2014 20 Newsgroups [8]. Before going over the results of experiments carried out on each of these, we describe the evaluation methodology employed which was common to all three and the evaluation metric.\nMethodology A grid search was performed to determine the best set of model hyperparameters. The procedure involved first evaluating each of the trained models on a validation set and then selecting the best of these to be evaluated on the test set. When a dataset did not contain a pre-defined validation set, it was created using a subset of the training set. The initial learning rate \u03b7init for stochastic gradient descent was varied as {0.0001, 0.001, 0.01}. Early-stopping was used for regularisation. For this, the classification average loss of the model on the validation set was determined after every epoch. If the loss happened to be higher than the previous best one for ten consecutive epochs, the parameters were reverted back to their values in the previous best model, and training was resumed with a reduced learning rate. And if this happened five times, training was terminated. The learning rate reduction was according to a schedule where it is progressively scaled by the factors 12 , 1 3 , 1 4 , and so on at each reduction step. The number of hidden units nhid was varied as {50, 100, 500, 1000}. The maximum number of training epochs was set to 2000, but it was found that training always ended well before this limit. The negative log-likelihood error criterion was used to optimise the model parameters. The DRBM generated a probability distribution over the different classes. The class-label corresponding to the greatest probability value was chosen as the predicted class. As all three datasets contain only a single data split, i.e. only one set of training, validation and test sets, the results reported here are each an average over those obtained with 10 model parameter initialisations using different randomisation seeds.\nAn additional hyperparameter to be examined in the case of the Binomial DRBM is the number of bins nbins. It corresponds to the number of states that can be assumed by each hidden unit of the model. The value of nbins was varied as {2, 4, 8} in our experiments.\nEvaluation Measure In all of the prediction tasks, each model is expected to predict the one correct label corresponding to the image of a digit, or the category of a document. All the models in this task are evaluated using the average loss E(y,y\u2217), given by:\nE(y,y\u2217) = 1\nN\nN \u2211\ni=1\nI (yi 6= y \u2217 i ) (20)\nwhere y and y\u2217 are the predicted and the true labels respectively, N is the total number of test examples, and I is the 0\u2212 1 loss function."}, {"heading": "6.1 MNIST Handwritten Digit Recognition", "text": "The MNIST dataset [10] consists of optical characters of handwritten digits. Each digit is a 28 \u00d7 28 pixel gray-scale image (or a vector x \u2208 [0, 1]784). Each pixel of the image corresponds to a floating-point value lying in the range [0, 1] after normalisation from an integer value in the range [0, 255]. The dataset is divided into a single split of pre-determined training, validation and test folds containing 50, 000 images, 10, 000 images and 10, 000 images respectively.\nTable 1 lists the classification performance on this dataset of the three DRBM variants derived above using the result in (15). The first row of the table corresponds to the DRBM introduced in [9]. We did not perform a grid search in the case of this one model and only used the reported hyperparameter setting in that paper to reproduce their result1. It was stated there that a difference of 0.2% in the average loss is considered statistically significant on this dataset. Going by this threshold of difference, it can be said that the performance of all\nthree models is equivalent on this dataset although the average accuracy of the DRBM is the highest, followed by that of the Bipolar and the Binomial DRBMs. All three variants perform best with 500 hidden units. It was observed that the number of bins nbins didn\u2019t play as significant a role as first expected. There seemed to be a slight deterioration in accuracy with an increase in the number of bins, but the difference cannot be considered significant given the threshold for this dataset. These results are listed in Table 2. 1 We obtained a marginally lower average loss of 1.78% in our evaluation of this model than the 1.81% reported in [9]."}, {"heading": "6.2 USPS Handwritten Digit Recognition", "text": "The USPS dataset [5] contains optical characters of handwritten digits. Each digit is a 16 \u00d7 16 pixel gray-scale image (or a vector x \u2208 [0, 1]256). Each pixel of the image corresponds to a floating-point value lying in the range [0, 1] after normalisation from an integer value in the range [0, 255]. The dataset is divided into a single split of pre-determined training, validation and test folds containing 7, 291 images, 1, 458 images and 2, 007 images respectively.\nTable 3 lists the classification performance on this dataset of the three DRBM variants derived above using the result in (15). Here the Binomial DRBM (of nbins = 8) was found to have the best classification accuracy, followed by the Bipolar DRBM and then the DRBM. The number of hidden units used by each of these models varies inversely with respect to their average loss.\nTable 4 shows the change in classification accuracy with a change in the number of bins. In contrast to the observation in the case of MNIST, here an increase in nbins is accompanied by an improvement in accuracy."}, {"heading": "6.3 20 Newsgroups Document Classification", "text": "The 20 Newsgroups dataset [8] is a collection of approximately 20, 000 newsgroup documents, partitioned evenly across 20 different categories. A version of the dataset where the training and the test sets contain documents collected at different times is used here. The aim is to predict the correct category of a document published after a certain date given a model trained on those published\nbefore the date. We used the 5, 000 most frequent words for the binary input features to the models. This preprocessing follows the example of [9], as it was the second data used to evaluate the DRBM there. We made an effort to adhere as closely as possible to the evaluation methodology there to obtain results comparable to theirs despite the unavailability of the exact validation set. Hence a validation set of the same number of samples was created2.\nTable 5 lists the classification performance on this dataset of the three DRBM variants derived above using the result in (15). Here the Bipolar DRBM outperformed the remaining two variants, followed by the Binomial DRBM and the DRBM.\nTable 6 shows the change in classification accuracy with a change in the number of bins.\n2 Our evaluation resulted in a model with a classification accuracy of 28.52% in comparison with the 27.6% reported in [9]."}, {"heading": "7 Conclusions and Future Work", "text": "This paper introduced a novel theoretical result that makes it possible to generalise the hidden layer activations of the Discriminative RBM (DRBM). This result was first used to reproduce the derivation of the cost function of the DRBM, and additionally to also derive those of two new variants of it, namely the Bipolar DRBM and the Binomial DRBM. The three models thus derived were evaluated on three benchmark machine learning datasets \u2014 MNIST, USPS and 20 Newsgroups. It was found that each of the three variants of the DRBM outperformed the rest on one of the three datasets, thus confirming that generalisations of the DRBM may be useful in practice.\nIt was found in the experiments in Section 6, that the DRBM achieved the best classification accuracy on the MNIST dataset, the Bipolar DRBM on the 20 Newsgroups dataset and the Binomial DRBM on the USPS dataset. While this does indicate the practical utility of the two new variants of the DRBM introduced here, the question of whether each of these is better suited for any particular types of dataset than the rest is to be investigated further.\nGiven the application of the result in (15) to obtain the Binomial DRBM, it is straightforward to extend it to what we refer to here as the Rectified Linear DRBM. This idea is inspired by [14], where the Rate-coded RBM [18] (analogous to the Binomial DRBM here) is extended to derive an RBM with Rectified Linear units by increasing the number of replicas of a single binary unit to infinity. Adopting the same intuition here in the case of the DRBM, this would mean that we allow the states sk to assume integer values in the range [0,\u221e) and thus extend the summation SN in the case of the Binomial DRBM to an infinite sum S\u221e resulting in the following derivation:\nS\u221e =\n\u221e \u2211\nsk=0\nexp (sk\u03b1j)\n= 1 + exp (\u03b1j)\n\u221e \u2211\nsk=0\nexp (sk\u03b1j)\n= 1\n1\u2212 exp (\u03b1j)\n(21)\nwith the equation for the Rectified Linear DRBM posterior probability in (15) becoming\nPrelu (y|x) = exp (by)\n\u220f\nj \u2211\u221e sk=0\nexp (sk\u03b1j) \u2211\ny\u2217 exp (by\u2217) \u220f j \u2211\u221e sk=0 exp ( sk\u03b1 \u2217 j )\n= exp (by)\n\u220f\nj 1\n1\u2212exp(\u03b1j) \u2211\ny\u2217 exp (by\u2217) \u220f j 1\n1\u2212exp(\u03b1\u2217j )\n.\n(22)\nExperiments with this variant of the DRBM are due, and will be carried out in the future."}], "references": [{"title": "Unsupervised Learning of Distributions on Binary Vectors using Two Layer Networks", "author": ["Y. Freund", "D. Haussler"], "venue": "Advances in Neural Information Processing Systems. pp. 912\u2013919", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1992}, {"title": "Understanding the Difficulty of Training Deep Feedforward Neural Networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics. pp. 249\u2013256", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics. pp. 315\u2013323", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The Power of Approximation: A Comparison of Activation Functions", "author": ["B.D. Gupta", "G. Schnitger"], "venue": "Advances in Neural Information Processing Systems. pp. 615\u2013622", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Replicated Softmax: An Undirected Topic Model", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems. pp. 1607\u20131614", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks", "author": ["B. Karlik", "A.V. Olgac"], "venue": "International Journal of Artificial Intelligence and Expert Systems 1(4), 111\u2013122", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Newsweeder: Learning to Filter Netnews", "author": ["K. Lang"], "venue": "Proceedings of the 12th international conference on machine learning. pp. 331\u2013339", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Classification using discriminative restricted Boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "International Conference on Machine Learning. pp. 536\u2013543. ACM Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "A Tutorial on EnergyBased Learning", "author": ["Y. LeCun", "S. Chopra", "R. Hadsell", "M. Ranzato", "F. Huang"], "venue": "Predicting Structured Data", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "International Conference on Machine Learning. pp. 609\u2013616. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Acoustic Modeling using Deep Belief Networks", "author": ["A.R. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing 20(1), 14\u201322", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807\u2013814", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning. pp. 791\u2013798. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["P. Smolensky"], "venue": "1. chap. Information Processing in Dynamical Systems: Foundations of Harmony Theory, pp. 194\u2013281. MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning Multilevel Distributed Representations for High-Dimensional Sequences", "author": ["I. Sutskever", "G. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics. pp. 548\u2013555", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Rate-Coded Restricted Boltzmann Machines for Face Recognition", "author": ["Y.W. Teh", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems pp. 908\u2013914", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient", "author": ["T. Tieleman"], "venue": "International Conference on Machine Learning. pp. 1064\u20131071. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Exponential Family Harmoniums with an Application to Information Retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems. pp. 1481\u20131488", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "It has gained popularity over the past decade in many applications, including feature learning [12], collaborative filtering [15], high-dimensional sequence modelling [17], and pretraining deep neural network classifiers [13].", "startOffset": 221, "endOffset": 225}, {"referenceID": 16, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 12, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 2, "context": "Previous work, in general, has either introduced a new type of activation function [18][20][6][14], carried out a comparative evaluation [4][7], or re-evaluated the significance of an existing activation function in a new context [2][3].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "In the case of feedforward neural networks, while [4] concluded in favour of the Logistic Sigmoid activation function, recent work in [2] found a drawback in its use with deep feedforward networks and suggested the Hyperbolic Tangent function as a more suitable alternative to it.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "In the case of feedforward neural networks, while [4] concluded in favour of the Logistic Sigmoid activation function, recent work in [2] found a drawback in its use with deep feedforward networks and suggested the Hyperbolic Tangent function as a more suitable alternative to it.", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "Even more recently, [3] highlighted the biological plausibility of the Rectified Linear activation function and its potential to outperform both the aforementioned activations without even the need to carry out unsupervised pre-training.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "An early variant of this model [1] known as the Influence Combination Machine, while still modelling binary data, employed units of values {\u22121,+1}.", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "In [20], an extension of the standard RBM to model real-valued variables in its visible layers was proposed with the aid of the Gaussian activation function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "With the same goal of modelling non-binary variables, the rate-coded RBM [18] was introduced in which both the visible and hidden layers could model integer-values by grouping the activations", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "This idea was extended in [14] which introduced Rectified Linear activations for the hidden layer of the RBM.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "In the context of topic modelling, the replicated softmax RBM [6] was introduced which models the categorical distribution in its visible layer.", "startOffset": 62, "endOffset": 65}, {"referenceID": 14, "context": "3 Restricted Boltzmann Machine The Restricted Boltzmann Machine (RBM) [16] is an undirected bipartite graphical model.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "The RBM is a special case of the Boltzmann Machine \u2014 an energy-based model [11] \u2014 which gives the joint probability of every possible pair of visible and hidden vectors via an energy function E, according to the equation P (v,h) = 1 Z e (1) where the \u201cpartition function\u201d, Z, is given by summing over all possible pairs of visible and hidden vectors Z = \u2211", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "To avoid the difficulty in computing the above gradient, an efficiently computable and widely adopted approximation of the gradient was proposed in the Contrastive Divergence method [19].", "startOffset": 182, "endOffset": 186}, {"referenceID": 7, "context": "It has been demonstrated in [9] how discriminative learning can be carried out in the RBM, thus making it feasible to use it as a standalone classifier.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "5 Generalising the DRBM This section describes a novel generalisation of the cost function of the DRBM [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "1 Generalising the Conditional Probability We begin with the expression for the conditional distribution P (y|x), as derived in [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "The result in (15) generalises the conditional probability of the DRBM first introduced in [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "2 Extensions to other Hidden Layer Distributions We first use the result in (15) to derive the expression for the conditional probability P (y|x) in the original DRBM [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "DRBM: The {0, 1}-Bernoulli DRBM corresponds to the model originally introduced in [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "which is identical to the result obtained in [9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "Bipolar DRBM: A straightforward adaptation to the DRBM involves replacing its hidden layer states by {\u22121,+1} as previously done in [1] in the case of the RBM.", "startOffset": 131, "endOffset": 134}, {"referenceID": 16, "context": "Binomial DRBM: It was demonstrated in [18] how groups of N (where N is a positive integer greater than 1) stochastic units of the standard RBM can be combined in order to approximate discrete-valued functions in its visible layer and hidden layers to increase its representational power.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "These are two handwritten digit racognition datasets \u2014 USPS [5] and MNIST [10], and one document classification dataset \u2014 20 Newsgroups [8].", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "These are two handwritten digit racognition datasets \u2014 USPS [5] and MNIST [10], and one document classification dataset \u2014 20 Newsgroups [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": "1 MNIST Handwritten Digit Recognition The MNIST dataset [10] consists of optical characters of handwritten digits.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Each digit is a 28 \u00d7 28 pixel gray-scale image (or a vector x \u2208 [0, 1]).", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "Each pixel of the image corresponds to a floating-point value lying in the range [0, 1] after normalisation from an integer value in the range [0, 255].", "startOffset": 81, "endOffset": 87}, {"referenceID": 7, "context": "The first row of the table corresponds to the DRBM introduced in [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "81% reported in [9].", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Each digit is a 16 \u00d7 16 pixel gray-scale image (or a vector x \u2208 [0, 1]).", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "Each pixel of the image corresponds to a floating-point value lying in the range [0, 1] after normalisation from an integer value in the range [0, 255].", "startOffset": 81, "endOffset": 87}, {"referenceID": 6, "context": "3 20 Newsgroups Document Classification The 20 Newsgroups dataset [8] is a collection of approximately 20, 000 newsgroup documents, partitioned evenly across 20 different categories.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "This preprocessing follows the example of [9], as it was the second data used to evaluate the DRBM there.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "6% reported in [9].", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "This idea is inspired by [14], where the Rate-coded RBM [18] (analogous to the Binomial DRBM here) is extended to derive an RBM with Rectified Linear units by increasing the number of replicas of a single binary unit to infinity.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "This idea is inspired by [14], where the Rate-coded RBM [18] (analogous to the Binomial DRBM here) is extended to derive an RBM with Rectified Linear units by increasing the number of replicas of a single binary unit to infinity.", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "We present a novel theoretical result that generalises the Discriminative Restricted Boltzmann Machine (DRBM). While originally the DRBM was defined assuming the {0, 1}-Bernoulli distribution in each of its hidden units, this result makes it possible to derive cost functions for variants of the DRBM that utilise other distributions, including some that are often encountered in the literature. This is illustrated with the Binomial and {\u22121,+1}-Bernoulli distributions here. We evaluate these two DRBM variants and compare them with the original one on three benchmark datasets, namely the MNIST and USPS digit classification datasets, and the 20 Newsgroups document classification dataset. Results show that each of the three compared models outperforms the remaining two in one of the three datasets, thus indicating that the proposed theoretical generalisation of the DRBM may be valuable in practice.", "creator": "LaTeX with hyperref package"}}}