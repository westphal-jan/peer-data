{"id": "1604.02126", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "On Stochastic Belief Revision and Update and their Combination", "abstract": "I propose a framework for an agent to change his probable beliefs when observing a new statement information $\\ alpha $. Traditionally, faith change occurs either through a revision process or through an updating process, depending on whether the agent is informed in a static world with $\\ alpha $or whether $\\ alpha $is a \"signal\" from the environment due to an occurring event. Boutilier proposed a unified model of qualitative faith change that \"combines aspects of revision and updating, thus providing a more realistic characterization of faith change.\" In this paper, I propose a unified model of quantitative faith change in which an agent's beliefs are presented as a probability distribution across possible worlds. Like Boutilier, I take a dynamic system perspective.", "histories": [["v1", "Thu, 7 Apr 2016 19:28:00 GMT  (28kb,D)", "http://arxiv.org/abs/1604.02126v1", "Presented at the Sixteenth International Workshop on Non-Monotonic Reasoning, 22-24 April 2016, Cape Town, South Africa. 10 pages"]], "COMMENTS": "Presented at the Sixteenth International Workshop on Non-Monotonic Reasoning, 22-24 April 2016, Cape Town, South Africa. 10 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gavin rens"], "accepted": false, "id": "1604.02126"}, "pdf": {"name": "1604.02126.pdf", "metadata": {"source": "CRF", "title": "On Stochastic Belief Revision and Update and their Combination", "authors": ["Gavin Rens"], "emails": ["gavinrens@gmail.com"], "sections": [{"heading": null, "text": "Information acquired can be due to evolution of the world or revelation about the world. That is, one may notice via some \u2018signal\u2019 generated by the changing environment that the environment has changed, or, one may be informed by an independent agent in a static environment that some \u2018fact\u2019 holds.\nIn the present work, I deal with belief change of agents who handle uncertainty by maintaining a probability distribution over possible situations. The agents in this framework also have models for nondeterministic events, and noisy observations. Noisy observation models can model imperfect sensory equipment for receiving environmental signals, but they can also model untrustworthy informants in a static world.\nIn this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work. However, I do not intend simply to give a probabilistic version of his Generalized Update Semantics. Whereas Boutilier (1998) presents a model for unifying qualitative belief revision and update, I build on his work to present a unified model of belief revision and update in a stochastic (probabilistic) setting. I also take a dynamical systems perspective, like him. Due to my quantitative approach, an agent can maintain a probability distribution\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nover the worlds it believes possible, using an expectation semantics of change. This is in contrast to Boutilier\u2019s \u201cgeneralized update\u201d approach, which takes a most-plausible event and most-plausible world approach. Finally, my proposal requires a trade-off factor to mix the changes in probability distribution over possible worlds brought about due to the probabilistic belief revision process and, respectively, the probabilistic belief update process. Boutilier\u2019s model has revision and update more tightly coupled. For this reason, his approach is better called \u201cunified\u201d while mine is called \u201chybrid\u201d.\nThe belief change community does not study probabilistic belief update; it is studied almost exclusively in frameworks employing Bayesian conditioning \u2013 for modeling events and actions in dynamical domains (e.g., DBNs, MDPs, POMDPs) (Koller and Friedman, 2009; Poole and Mackworth, 2010, e.g.). The part of my approach responsible for updating stays within the Bayesian framework, but combines the essential elements of belief update with unobservable events and belief update as partially observable Markov decision process (POMDP) state estimation.\nOn the other hand, there is plenty of literature on probabilistic belief revision (Voorbraak, 1999; Grove and Halpern, 1998; Kern-Isberner, 2008; Yue and Liu, 2008, e.g.). The subject is both deep and broad. There is no one accepted approach and to argue which is the best is not the focus of this paper. I shall choose one reasonable method for probabilistic belief revision suitable to the task at hand.\nIn the first section, Boutilier\u2019s \u2018generalized update\u2019 is reviewed. Then, in the next section, I introduce stochastic update and stochastic revision, culminating in the \u2018hybrid stochastic belief change\u2019 (HSBC) approach. The final section presents an example inspired by Boutilier\u2019s article (1998) and analyses the results.\nSome proofs of propositions are omitted to save space; they are available on request."}, {"heading": "Boutilier\u2019s Generalized Update", "text": "I use Boutilier\u2019s notation and descriptions, except that I am more comfortable with \u03b1 and \u03b2 to represent sentences, instead of A and B. It is assumed that an agent has a deductively closed belief set K, a set of sentences drawn from some logical language reflecting the agent\u2019s beliefs about the current state of the world. For ease of presentation, I assume\nar X\niv :1\n60 4.\n02 12\n6v 1\n[ cs\n.A I]\n7 A\npr 2\n01 6\na logically finite, classical propositional language, denoted L (LCPL in Boutilier (1998)), and consequence operation Cn . The belief set K will often be generated by some finite knowledge base KB (i.e., K = Cn(KB)). The identically true and false propositions are denoted > and \u22a5, respectively. Given a set of possible worlds W (or valuations over L) and \u03b1 \u2208 L, the set of \u03b1-worlds, that is, the elements of W satisfying \u03b1, is denoted by ||\u03b1||. The worlds satisfying all sentences in a set K is denoted ||K||."}, {"heading": "Update", "text": "Given a belief set K, an agent will often observe a change in the world \u03b1, requiring the agent to change K. This is the update of K by \u03b1, denoted K \u03b1.\n\u201c||KB || represents the set of possibilities we are prepared to accept as the actual state of affairs. Since observation \u03b1 is the result of some change in the actual world, we ought to consider, for each possibilityw \u2208 ||KB ||, the most plausible way (or ways) in which w might have changed in order to make \u03b1 true. That is, we want to consider the most plausible evolution of world w into a world satisfying the observation \u03b1. To capture this intuition, Katsuno and Mendelzon (1991) propose a family of preorders {\u2264w| w \u2208 W}, where each \u2264w is a reflexive, transitive relation over W . We interpret each such relation as follows: if u \u2264w v then u is at least as plausible a change relative to w as is v; that is, situation w would more readily evolve into u than it would into v.\nFinally, a faithfulness condition is imposed: for every world w, the preorder \u2264w has w as a minimum element; that is, w <w v for all v 6= w. Naturally, the most plausible candidate changes in w that result in \u03b1 are those worlds v satisfying \u03b1 that are minimal in the relation \u2264w. The set of such minimal \u03b1-worlds for each relation \u2264w, and each w \u2208 ||KB ||, intuitively capture the situations we ought to accept as possible when updating KB with \u03b1,\u201d (Boutilier, 1998, p. 9). In other words,\n||KB \u03b1|| = \u22c3\nw\u2208||KB||\n{Min(\u03b1,\u2264w)},\nwhere Min(\u03b1,\u2264w) specifies the minimal \u03b1-worlds with respect to the preorder \u2264w. Then K \u03b1 = Cn(KB \u03b1), where K is the belief set associated with KB ."}, {"heading": "Revision", "text": "Given a belief set K, an agent will often obtain information \u03b1 in a static world, which must be incorporated into K. This is the revision of K by \u03b1, denoted K\u2217\u03b1.\nThe AGM theory of belief revision (Alchourro\u0301n, Ga\u0308rdenfors, and Makinson, 1985) provides a set of guidelines, in the form of the postulates, governing the process. \u201cUnfortunately, while the postulates constrain possible revisions, they do not dictate the precise beliefs that should be retracted when \u03b1 is observed. An alternative model of revision, based on the notion of epistemic entrenchment (Ga\u0308rdenfors, 1988), has a more constructive nature,\u201d (Boutilier, 1998, p. 6).\n\u201cSemantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988;\nBoutilier, 1994). However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a \u03ba-ranking. Such a ranking \u03ba : W \u2192 N assigns to each world a natural number reflecting its plausibility or degree of believability. If \u03ba(w) < \u03ba(v) then w is more plausible than v or more consistent with the agent\u2019s beliefs. We insist that \u03ba\u22121(0) 6= \u2205, so that maximally plausible worlds are assigned rank 0. These maximally plausible worlds are exactly those consistent with the agent\u2019s beliefs; that is, the epistemically possible worlds according to K are those deemed most plausible in \u03ba (see Spohn (1988) for further details). We sometimes assume \u03ba is a partial function, and loosely write \u03ba(w) = \u221e to mean \u03ba(w) is not defined (i.e., w is not in the domain of \u03ba, or w is impossible),\u201d (Boutilier, 1998, p. 6).\nA \u03ba-ranking captures the entrenchment of the agent\u2019s beliefs in its belief set K. This entrenchment determines how K will be revised when the agent receives new information / makes an observation \u03b1. \u03ba induces a belief set as follows.\nK = {\u03b1 \u2208 L | \u03ba\u22121(0) \u2286 ||\u03b1||}.\nDue to the ranking or entrenchment of knowledge provided by \u03ba, \u03ba is considered an epistemic state.\n\u201cIn other words, the set of most plausible worlds (those such that \u03ba(w) = 0) determine the agent\u2019s beliefs. The ranking \u03ba also induces a revision function: to revise by \u03b1 an agent adopts the most plausible \u03b1-worlds as epistemically possible,\u201d (Boutilier, 1998, p. 6).\nLet Wi = {w \u2208 W | \u03ba(w) = i}. And let Min(\u03b1, \u03ba) be the setWi with the least i such that for allwi \u2208Wi,wi |= \u03b1. Then K\u2217\u03b1 := {\u03b2 \u2208 L | Min(\u03b1, \u03ba) \u2286 ||\u03b2||}. In words, the belief set revised by \u03b1 contains all those sentences entailed by the set of worlds with the same rank, where that rank is the least such that they are all \u03b1-worlds."}, {"heading": "Generalized Update", "text": "As explained in the introduction, my intention with this paper is not to give a probabilistic version of the Generalized Update approach (Boutilier, 1998). For completeness, however, I sketch the approach here covering the approach in detail would take up unnecessary space without lending much insight into my Hybrid Stochastic Belief Change approach.\nBoutilier motivates the need for a generalized update method which includes revision, by claiming that KM update (Katsuno and Mendelzon, 1991) is insufficient. He provides the following example adopted from Moore (1990). Suppose you want to test whether the contents of a beaker are chemically acid or base. If it is acid, a piece of litmus paper will turn red, if base, the paper will turn blue. Suppose that the test has not yet been performed, but you believe that the contents in the beaker are acidic. When the litmus paper is dipped into and pulled out of the beaker, the paper turns blue, indicating a basic compound. \u201cUnfortunately, the KM theory does not allow this to take place. [...] One is forced to accept that, if the contents were acidic (in which case it should turn red), some extraordinary change occurred (the\ntest failed, the contents of the beaker were switched, etc.). [...] Of course, the right thing to do is simply admit that the beaker did not, in fact, contain an acid\u2014the agent should revise its beliefs about the contents of the beaker,\u201d (Boutilier, 1998, p. 13).\nBoutilier adopts an event-based approach where a set of events E is assumed. These events are allowed to be nondeterministic, and each possible outcome of an event is ranked according to its plausibility via a \u03ba-ranking. \u201cAs in the original event-based semantics, we will assume each world has an event ordering associated with it that describes the plausibility of various event occurrences at that world,\u201d (Boutilier, 1998, p. 14).\nA generalized update model is then defined as \u3008W,\u03ba,E, \u00b5\u3009, where W is a set of worlds (the agent\u2019s epistemic state), \u03ba is a ranking overW ,E is a set of events (mappings overW ), and \u00b5 is an event ordering (a set of mappings over E).\nAs with KM update, updates usually occur in response to some observation, with the assumption that something occurred to cause this observation. After observing \u03b1, an agent should adjust its beliefs by considering that only the most plausible transitions leading to \u03b1 actually occurred. The set of possible \u03b1-transitions are those transitions leading to \u03b1worlds. The most plausible \u03b1-transitions are those possible \u03b1-transitions with the minimal \u03ba-ranking. Given that \u03b1 has actually been observed, an agent should assume that one of these transitions describes the actual course of events. The worlds judged to be epistemically possible are those that result from the most plausible of these transitions.\nBoutilier (1998) has a proposition that states that generalized belief update as described above is equivalent to \u201cfirst determining the predicted updated ranking \u03ba followed by standard (AGM) revision by \u03b1 with respect to \u03ba ,\u201d (Boutilier, 1998, p. 16). \u03ba is determined by taking the worlds in the current possible worlds ||K|| (induced from \u03ba) and shifting them to all possible worlds given all possible transitions given all possible events (the actual event is unknown), taking into account the relevant plausibility rankings."}, {"heading": "Stochastic Belief Change", "text": "I now consider agents who deal with uncertainty by maintaining a probability distribution over possible situations (worlds) they could be in. Let a belief state b be defined as the set {(w, p) | w \u2208W,p \u2208 [0, 1]}, where \u2211 (w,p)\u2208b p = 1. The probability of being in w is denoted b(w). That is, b is a probability distribution over all the worlds in W . In the hybrid stochastic belief change (HSBC) framework, an agent maintains a belief state, which changes as new information is received or observed.\nAn agent is assumed to have a model of how the world works.\nDefinition 1. The stochastic belief change modelM has the form \u3008W, \u03b5, T,E,O, os\u3009, where \u2022 W is a set of possible worlds, \u2022 \u03b5 is a set of events,\n\u2022 T : (W \u00d7 \u03b5 \u00d7W ) \u2192 [0, 1] is a transition function such that for every e \u2208 \u03b5 and w \u2208 W , \u2211 w\u2032\u2208W T (w, e, w\n\u2032) = 1 (T (w, e, w\u2032) models the probability of a transition to world w\u2032, given the occurrence of event e in world w), \u2022 E is the event likelihood function (E(e, w) = P (e | w), the probability of the occurrence of event e in w), \u2022 O : (L\u00d7W )\u2192 [0, 1] is an observation function such that for every world w, \u2211 \u03b1\u2208\u2126O(\u03b1,w) = 1 (O(\u03b1,w) models\nthe probability of observing \u03b1 in w), where \u2126 \u2282 L is the set of possible observations, up to equivalence, and where if \u03b1 \u2261 \u03b2, then O(\u03b1,w) = O(\u03b2,w), for all worlds w.1 \u2022 os : (\u2126 \u00d7 W ) \u2192 [0, 1] (os(\u03b1,w) is the agent\u2019s ontic strength for \u03b1 perceived in w.)\nDefinition 2. b(\u03b1) := \u2211 w\u2208W,w|=\u03b1 b(w).\nLet b\u25e6\u03b1 := b \u25e6 \u03b1 so that we can write b\u25e6\u03b1(w), where \u25e6 is any update or revision operator.\nOften, in the exposition of this paper, a world will be referred to by its truth vector. For instance, if the vocabulary is {q, r} and w3 |= \u00acq \u2227 r, then w3 may be referred to as 01.\nFor parsimony, let b = \u3008p1, . . . , pn\u3009 be the probabilities that belief state b assigns to w1, . . . , wn where \u3008w1, w2, w3, w4\u3009 = \u300811, 10, 01, 00\u3009, and \u3008w1, w2, . . . , w8\u3009 = \u3008111, 110, . . . , 000\u3009."}, {"heading": "Update", "text": "Transitions associated with the observation of \u03b1 from a world w in the current belief state bcur to a world w\u2032 could be caused by different events. According to Boutilier (1998), update can be defined as\nbeventnew := {\n(w\u2032, p\u2032) | w\u2032 \u2208W,p\u2032 =\u2211 w\u2208W \u2211 e\u2208\u03b5 T (w, e, w\u2032)E(e, w)bcur (w) } .\nBecause the actual event is unobservable/hidden, p\u2032 is the expected probability of reaching w\u2032, given the event probabilities.\nIn partially observable Markov decision process (POMDP) theory (Astro\u0308m, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden. Then, given current belief state bcur , selected action a and observation o, the state estimation function is defined by\nbpomdpnew :=\n{ (w\u2032, p\u2032) | w\u2032 \u2208W,p\u2032 =\nO(o, a, w\u2032) \u2211 w\u2208W T (w, a,w \u2032)bcur (w)\nP (o | a, bcur )\n} ,\nwhere \u2126 is a set of observation objects and O : (\u2126 \u00d7 A \u00d7 W ) \u2192 [0, 1] is an observation function, such that for every a and w\u2032, \u2211 o\u2208\u2126O(o, a, w\n\u2032) = 1. O(o, a, w\u2032) models the probability of perceiving o in arrival world w\u2032, given the execution of some action a \u2208 A. Note that P (o | a, bcur ) is a normalizing constant.\n1\u2261 denotes logical equivalence.\nBut what is the probabilistic update, given new information/evidence \u03b1? I suggest that \u03b1 is the (overt) \u2018signal\u2019 generated by the (covert) event. An important question is, When is \u03b1 received \u2013 in the current/departure world (wc) or in the new/arrival world (wn)? Although it is not clear to me, in POMDP theory, observations are always assumed to be received in the arrival world \u2013 I shall assume the same.\nIn the present framework, actions are not selected by the agent, but by nature. In other words, actions are considered to be events occurring in the environment, uncontrollable by the agent. Further, at the present stage of research, I shall assume that the agent has a less detailed observation model, that is, an agent only knows O(\u03b1,wn), the probability of perceiving \u03b1 in arrival world wn (defined in Def. 1). Hence, I propose to weight beventnew (w\n\u2032) by O(\u03b1,wn) when receiving new information \u03b1 and one knows that one\u2019s belief state should be updated (due to an evolving world). Then we can define\nDefinition 3. b \u03b1 := {\n(w\u2032, p\u2032) | w\u2032 \u2208W,p\u2032 = 1 \u03b3 O(\u03b1,w\u2032) \u2211 w\u2208W \u2211 e\u2208\u03b5 T (w, e, w\u2032)E(e, w)b(w) } ,\nwhere \u03b3 is a normalizing factor.\nAs far as I know, no-one has proposed rationality postulates for probabilistic update. The reason is likely due to probabilistic update being defined in terms of standard probability theory. The axioms of probability theory have been argued to be rational for several decades (although it is not without its detractors).\nThe following basic postulates for my probabilistic belief update are proposed. (Unless stated otherwise, it is assumed that \u03b1 is logically satisfiable, i.e., ` \u00ac\u03b1 is false.)\n(P 1) b \u03b1 is a belief state iff not ` \u00ac\u03b1 (P 2) b \u03b1(\u03b1) = 1 (P 3) If \u03b1 \u2261 \u03b2, then b \u03b1 = b \u03b2\nProposition 1. If b \u03b1(\u03b1) > 0, it is not necessary that b \u03b1(\u03b1) = 1.\nProof. Let the vocabulary be {q, r}. Let b = \u30080.4, 0, 0.1, 0.5\u3009. Let there be only one event e. Let the transition function be specified as T (11, e, 11) = 0.5, T (11, e, 10) = 0.5, T (10, e, 01) = 1, T (01, e, 00) = 1, T (00, e, 11) = 1. Let E(e, w) = 1 for all w \u2208 W . Let the evidence be q. Let O(q, 11) = 0.2, O(q, 10) = 0, O(q, 01) = 0, O(q, 00) = 0.3. Then applying operation to b produces b q = \u30080.82, 0, 0, 0.18\u3009. Hence, b q(q) = 0.82 6= 1.\nAlthough the following proposition is mostly negative, the reader will soon see that constraining the stochastic belief change model to be \u2018rational\u2019, the negative postulates become positive.\nProposition 2. Postulate (P 3) holds, while (P 1) and (P 2) do not hold.\nDefinition 4. We say event e is event-rational when for all w \u2208 W : there exists a w\u2032 such that T (w, e, w\u2032) > 0 iff E(e, w) > 0. Definition 5. We say \u03b1 is an e-signal when for all w\u2032 \u2208W : there exists a w such that T (w, e, w\u2032) > 0 iff O(\u03b1,w\u2032) > 0. Definition 6. We say a model M is observation-rational iff for all \u03b1, whenever ` \u00ac\u03b1, O(\u03b1,w) = 0 for all w \u2208W .\nThe proposition below says that if one is rational w.r.t. observations and w.r.t. some event, and \u03b1 is a signal produced by that event, then updating on \u03b1 is defined. Proposition 3. If M is observation-rational, there exists an event e \u2208 \u03b5 which is event-rational and \u03b1 is an e-signal, then b \u03b1 is a belief state iff not ` \u00ac\u03b1 (i.e., then (P 1) holds).\n(P 2) does not hold under the antecedents of Proposition 3. Another definition is required as qualification: Definition 7. We say evidence \u03b1 is trustworthy iff for all w \u2208W , if w 6|= \u03b1, then O(\u03b1,w) = 0.\nThe proposition below says that if \u03b1 is trustworthy, one is rational w.r.t. some event, and \u03b1 is a signal produced by that event, then one should accept \u03b1 in the updated belief state. Proposition 4. If \u03b1 is trustworthy, there exists an event e \u2208 \u03b5 which is event-rational and \u03b1 is an e-signal, then b \u03b1(\u03b1) = 1 (i.e., then (P 2) holds).\nProof. Not ` \u00ac\u03b1 is assumed by default. Recall that b \u03b1(\u03b1) = \u2211 w\u2208W,w|=\u03b1 b \u03b1(w). Referring to the (\u21d0) part of\nthe proof of Proposition 3, b \u03b1(\u03b1) is a belief state and thus\u2211 w\u2208W b \u03b1(w) = 1. Hence, for b \u03b1(\u03b1) to be less than 1, there must exist a w\u2032 \u2208 W s.t. w\u2032 6|= \u03b1 and b \u03b1(w\u2032) > 0. But then O(\u03b1,w\u2032) > 0. Therefore, for (P 2) not to hold, an agent needs to believe thatO(\u03b1,w\u2032) > 0 for some world w\u2032 where w\u2032 6|= \u03b1. But then \u03b1 cannot be trustworthy (i.e., then (P 2) holds.\nDefinition 8 (Ga\u0308rdenfors, 1988). A probabilistic belief change operation \u25e6 is said to be preservative iff for all belief states P and for all propositions \u03b1 and \u03b2, if P (\u03b1) > 0 and P (\u03b2) = 1, then P \u25e6\u03b1(\u03b2) = 1. Proposition 5. Operation is not preservative. Definition 9. We say evidence \u03b1 is \u03b2-trustworthy if for all w \u2208W , if w 6|= \u03b2, then O(\u03b1,w) = 0. Proposition 6. If b \u03b1(\u03b2) is a belief state, b(\u03b2) = 1 and \u03b1 is \u03b2-trustworthy, then b \u03b1(\u03b2) = 1.\nProof. \u2211 w\u2208W b \u03b1(w) = 1. Hence, for b \u03b1(\u03b2) to be < 1, there must exist a w\u00d7 \u2208 W s.t. w\u00d7 6|= \u03b2 and b \u03b1(w\u00d7) > 0. And because b(\u03b2) = 1, b(w\u00d7) = 0. So some probability mass must have been shifted from some \u03b2-world to the non\u03b2-world w\u00d7.\nBy definition, b \u03b1(w \u00d7) = 1\u03b3 O(\u03b1,w \u00d7)\u2211 w\u2208W \u2211 e\u2208\u03b5 T (w, e, w \u00d7)E(e, w)b(w). So for b \u03b1(w \u00d7) to be > 0, O(\u03b1,w\u00d7) must be > 0. However, because \u03b1 is \u03b2-trustworthy, O(\u03b1,w\u00d7) = 0. Hence, O(\u03b1,w\u00d7) 6> 0 and b \u03b1(\u03b2) 6< 1.\nProposition 7. b \u03b1\u2227\u03b2 6= (b \u03b1) \u03b2 .\nProof. For instance, consider the example used in the proof of Proposition 1. Let \u03b1 be q and let \u03b2 be q \u2227 r. Note that \u03b1\u2227\u03b2 is then logically equivalent to q\u2227r. LetO(q\u2227r, 11) = O(q \u2227 r, 10) = 0.5 and O(q \u2227 r, 01) = O(q \u2227 r, 00) = 0.\nWe know that b q = \u30080.82, 0, 0, 0.18\u3009. Then (b q) q\u2227r = \u30081, 0, 0, 0\u3009. On the other hand, b q\u2227r = \u30080.875, 0, 0, 0.125\u3009."}, {"heading": "Revision", "text": "Using Bayes\u2019 Rule2 , P (wn | \u03b1) can be determined:\nP (w | \u03b1) := O(\u03b1,w)b(w)\u2211 w\u2032\u2208W O(\u03b1,w \u2032)b(w\u2032) .\nNote that if O(\u03b1,w) = 0, then P (w | \u03b1) = 0. It is not yet universally agreed what revision means in a probabilistic setting. In classical belief change, it is understood that if the new information \u03b1 is consistent with the agent\u2019s current beliefs KB , then revision is equivalent to belief expansion (denoted +), where expansion is the logical consequences of KB \u222a {\u03b1}. It is mostly agreed upon that Bayesian conditioning corresponds to classical belief expansion. This is evidenced by Bayesian conditioning (BC) being defined only when b(\u03b1) 6= 0. In other words, one could define revision to be\nb BC \u03b1 := {(w, p) | w \u2208W,p = P (w | \u03b1)},\nas long as P (\u03b1) 6= 0.3 To accommodate cases where b(\u03b1) 6= 0, that is, where \u03b1 contradicts the agent\u2019s current beliefs and its beliefs need to be revised in the stronger sense, we shall make use of imaging. Imaging was introduced by Lewis (1976) as a means of revising a probability function. It has also been discussed in the work of, for instance, Ga\u0308rdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). The following version of imaging must not be regarded as a fundamental part of the larger belief change framework presented here; it should be regarded as a placeholder or suggestion for the \u2018revision-module\u2019 of the framework. Informally, Lewis\u2019s original solution for accommodating contradicting evidence \u03b1 is to move the probability of each world to its closest, \u03b1-world. Lewis made the strong assumption that every world has a unique closest \u03b1-world. More general versions of imaging allow worlds to have several, equally proximate, closest worlds.\nGa\u0308rdenfors (1988) calls one of his generalizations of Lewis\u2019s imaging general imaging. Our method is also a generalization. We thus refer to his as Ga\u0308rdenfors\u2019s general imaging and to our method as generalized imaging to distinguish them. It should be noted that these imaging methods are general revision methods and can be used in place of Bayesian conditioning for expansion. \u201cThus imaging is a more general method of describing belief changes than conditionalization,\u201d (Ga\u0308rdenfors, 1988, p. 112).\n2Bayes\u2019 Rule states (in the notation of this paper) that P (w | \u03b1) = P (\u03b1 | w)P (w)/P (\u03b1) or P (w | \u03b1) = P (\u03b1 | w)P (w)/ \u2211 w\u2032\u2208W P (\u03b1 | w\n\u2032)P (w\u2032). 3Note that in my notation, b(\u03b1) is equivalent to P (\u03b1).\nLet Min(\u03b1,w, d) be the set of \u03b1-worlds closest to w measured with d. Formally,\nMin(\u03b1,w, d) :=\n{w\u2032 \u2208 ||\u03b1|| | \u2200w\u2032\u2032 \u2208 ||\u03b1||, d(w\u2032, w) \u2264 d(w\u2032\u2032, w)}, where d(\u00b7) is some acceptable measure of distance between worlds (e.g., Hamming or Dalal distance). It must also obey the faithfulness condition that for every world w, d(w,w) < d(v, w) for all v 6= w. Example 1. Let the vocabulary be {q, r, s}. Let \u03b1 be (q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s). Suppose d is Hamming distance. Then\nMin((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 111, d) = {111} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 110, d) = {110} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 101, d) = {101} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 100, d) = {110, 101} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 011, d) = {111} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 010, d) = {110} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 001, d) = {101} Min((q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s), 000, d) = {110, 101}\nThen generalized imaging (denoted GI) is defined as Definition 10.\nb GI \u03b1 := { (w, p) | w \u2208W,p = 0 if w 6\u2208 ||\u03b1||,\nelse p = \u2211 w\u2032\u2208W\nw\u2208Min(\u03b1,w\u2032,d)\nb(w\u2032)/|Min(\u03b1,w\u2032, d)| } .\nExample 2. Continuing on Example 1: Let b = \u30080, 0.1, 0, 0.2, 0, 0.3, 0, 0.4\u3009.\n(q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s) is abbreviated as \u03b1. bGI\u03b1 (111) = \u2211 w\u2032\u2208W\n111\u2208Min(\u03b1,w\u2032,d) b(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= b(111)/|Min(\u03b1, 111, d)| + b(011)/|Min(\u03b1, 011, d)| = 0/1 + 0/1 = 0. bGI\u03b1 (110) = \u2211 w\u2032\u2208W\n110\u2208Min(\u03b1,w\u2032,d) b(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= b(110)/|Min(\u03b1, 110, d)| + b(100)/|Min(\u03b1, 100, d)| + b(010)/|Min(\u03b1, 010, d)| + b(000)/|Min(\u03b1, 000, d)| = 0.1/1 + 0.2/2 + 0.3/1 + 0.4/2 = 0.7. bGI\u03b1 (101) = \u2211 w\u2032\u2208W\n101\u2208Min(\u03b1,w\u2032,d) b(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= b(101)/|Min(\u03b1, 101, d)| + b(100)/|Min(\u03b1, 100, d)| + b(001)/|Min(\u03b1, 001, d)| + b(000)/|Min(\u03b1, 000, d)| = 0/1 + 0.2/2 + 0/1 + 0.4/2 = 0.3.\nAnd bGI\u03b1 (100) = b GI \u03b1 (011) = b GI \u03b1 (010) = b GI \u03b1 (001) = bGI\u03b1 (000) = 0. Notice how the probability mass of non-\u03b1-worlds is shifted to their closest \u03b1-worlds. If a non-\u03b1-world w\u00d7 with probability p has n closest \u03b1-worlds (equally distant), then each of these closest \u03b1-worlds gets p/n mass from w\u00d7.\nRecall that in the proposed framework, agents have access to an observation model (formalized via an observation function O(\u00b7, \u00b7)). Given enough computational power and time, it would be irrational for an agent to ignore its observation model when revising its beliefs. Another proposed\ndefinition for a stochastic belief revision operation based on imaging (denoted OGI) is thus Definition 11.\nb OGI \u03b1 := { (w, p) | w \u2208W,p = O(\u03b1,w)b GI \u03b1 (w)\u2211\nw\u2032\u2208W O(\u03b1,w \u2032)bGI\u03b1 (w \u2032)\n} ,\nwhere the denominator is a normalizing factor. b OGI \u03b1 is not defined as{\n(w, p) | w \u2208W,p = 0 if w 6\u2208 ||\u03b1||, else p = \u2211 w\u2032\u2208W\nw\u2208Min(\u03b1,w\u2032,d)\nO(\u03b1,w\u2032)b(w\u2032)/|Min(\u03b1,w\u2032, d)| } ,\nbecause \u03b1 is assumed perceived in the new world w, not the old world w\u2032.\nNote that if P (\u00b7 | \u03b1) were used instead of O(\u03b1, \u00b7), then OGI would be undefined whenever b(\u03b1) = 0. But this is exactly the problem we want to avoid by using imaging. Another justification to rather use O(\u03b1,w) is that its value is positively correlated with P (w | \u03b1): If O(\u03b1,w) = 0, then P (w | \u03b1) = 0. If P (w | \u03b1) = 1, then O(\u03b1,w) is maximal in b in the following sense: for all w\u2032 \u2208 W , if w\u2032 6= w, then either b(w\u2032) = 0 or O(\u03b1,w\u2032) = 0, whereas b(w) > 0 and O(\u03b1,w) > 0.\nNote that the denominator my be zero, making OGI undefined in that case. I shall deal with this issue a little bit later. Example 3. Recall from Example 2 that bGI\u03b1 = \u30080, 0.7, 0.3, 0, 0, 0, 0, 0\u3009 and \u03b1 is (q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s). Let O(\u03b1,w) = 0.3, say, for all w \u2208 W . Then bOGI\u03b1 = bGI\u03b1 . Obviously, if the observation model carries no information with respect to \u03b1, then it has no influence on the agent\u2019s revised beliefs.\nNow let O(\u03b1,w) = 0.3 if w |= r, else O(\u03b1,w) = 0.2. Then bOGI\u03b1 = \u30080.3 \u00d7 0/0.23, 0.2 \u00d7 0.7/0.23, 0.3 \u00d7 0.3/0.23, 0.2 \u00d7 0/0.23, 0.3 \u00d7 0/0.23, 0.2 \u00d7 0/0.23, 0.3 \u00d7 0/0.23, 0.2 \u00d7 0/0.23\u3009 = \u30080, 0.61, 0.39, 0, 0, 0, 0, 0\u3009. If the agent has an observation model telling it that \u03b1 is more likely to be perceived in r-worlds than in \u00acr-worlds, then when it receives \u03b1, the agent should be biased to believing that it is actually in an r-world. However, the agent was certain that it was in a \u00acr-world when its belief state was b. GI thus pushes the agent to favour the \u03b1-worlds being \u00acrworlds. Hence, in this example there is tension between being in a \u00acr-world (due to previous beliefs) and being in an r-world (due to the observation model). Definition 12.\nb BCI \u03b1 := { b BC \u03b1 if b(\u03b1) > 0 b OGI \u03b1 if b(\u03b1) = 0\nI denote the expansion of belief state b on \u03b1 as b+\u03b1 (resp., probability function P on \u03b1 as P+\u03b1 ) and delay its definition till later. P\u22a5 is conventionally defined to be the absurd probability function which is defined to be P\u22a5(\u03b4) = 1 for all \u03b4 \u2208 L.\nGa\u0308rdenfors (1988) proposed six rationality postulates for probabilistic belief revision. (Unless stated otherwise, it is assumed that \u03b1 is logically satisfiable, i.e., ` \u00ac\u03b1 is false.)\n1. P \u2217\u03b1 is a probability function 2. P \u2217\u03b1(\u03b1) = 1 3. If \u03b1 \u2261 \u03b2, then P \u2217\u03b1 = P \u2217\u03b2 4. P \u2217\u03b1 6= P\u22a5 iff not ` \u00ac\u03b1 5. If P (\u03b1) > 0, then P \u2217\u03b1 = P + \u03b1 6. If P \u2217\u03b1(\u03b2) > 0, then P \u2217 \u03b1\u2227\u03b2 = (P \u2217 \u03b1) + \u03b2 .\nInstead of saying that the result of an operation is P\u22a5, I simply say that the result is undefined. And by noting that the result of an operation is not a belief state if it is undefined, one can merge postulates 1 and 4. The stochastic belief revision postulates in my notation are thus\n(P \u22171) b\u2217\u03b1 is a belief state iff not ` \u00ac\u03b1 (P \u22172) b\u2217\u03b1(\u03b1) = 1 (P \u22173) If \u03b1 \u2261 \u03b2, then b\u2217\u03b1 = b\u2217\u03b2 (P \u22174) If b(\u03b1) > 0, then b\u2217\u03b1 = b + \u03b1 (P \u22175) If b\u2217\u03b1(\u03b2) > 0, then b \u2217 \u03b1\u2227\u03b2 = (b \u2217 \u03b1) + \u03b2 .\nI now test OGI and BCI against each of the five postulates. Recall that if the denominator in the definition of OGI\nis zero, it is undefined. To guarantee that OGI is defined,\u2211 w\u2032\u2208W O(\u03b1,w \u2032)bGI\u03b1 (w \u2032) must be non-zero, that is, there must be at least one w\u2032 \u2208 W for which O(\u03b1,w\u2032)bGI\u03b1 (w\u2032) > 0. We know that when w\u2032 6\u2208 ||\u03b1||, O(\u03b1,w\u2032)bGI\u03b1 (w\u2032) = bGI\u03b1 (w\n\u2032) = 0. Definition 13. We say \u03b1 is weakly observable iff there exists a w \u2208 W such that w |= \u03b1 and O(\u03b1,w) > 0. We say \u03b1 is strongly observable iff for all w \u2208 W for which w |= \u03b1, O(\u03b1,w) > 0. Proposition 8. When \u2217 is OGI, postulate (P \u22171), in general, does not hold, but does hold if evidence \u03b1 is strongly observable. Proof. Firstly, observe that b(w\u2032) =\u2211 |Min(\u03b1,w\u2032,d)| b(w \u2032)/|Min(\u03b1,w\u2032, d)|. Therefore,\n1 = \u2211 w\u2032\u2208w b(w\u2032)\n= \u2211 w\u2032\u2208w \u2211 |Min(\u03b1,w\u2032,d)| b(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= \u2211\nw\u2032\u2208w,|Min(\u03b1,w\u2032,d)|\nb(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= \u2211\nw\u2032\u2208w,w\u2208W,w\u2208Min(\u03b1,w\u2032,d)\nb(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= \u2211 w\u2208W \u2211 w\u2032\u2208w,w\u2208Min(\u03b1,w\u2032,d) b(w\u2032)/|Min(\u03b1,w\u2032, d)|\n= \u2211 w\u2208W bGI\u03b1 (w).\nLet b = \u30080, 0.1, 0, 0.2, 0, 0.3, 0, 0.4\u3009 and \u03b1 be (q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s). Let O(\u03b1, 111) = 0.9 and O(\u03b1,w) = 0 for all w \u2208 W , w 6= 111. (Notice that \u03b1 is weakly observable.) From Example 2, we know that bGI\u03b1 (111) = 0, implying that bOGI\u03b1 (111) = 0, and one can deduce that b OGI \u03b1 (w) = 0 for all w \u2208W , due to the specification of the observation model.\nNow, let \u03b1 be strongly observable: let O(\u03b1, 111) = O(\u03b1, 110) = O(\u03b1, 101) = 0.1, else O(\u03b1, \u00b7) = 0. Then bOGI\u03b1 = \u30080, 0.7, 0.3, 0, 0, 0, 0, 0\u3009. In general, let O(\u03b1,w) > 0 for all w \u2208 W for which w |= \u03b1. By definition of GI, the probability mass of all non-\u03b1-worlds is shifted to their closest \u03b1-worlds; the total mass (of the \u03b1-worlds) thus remains 1. Hence, bGI\u03b1 (\u03b1) = 1 and there exists a w\n\u2032 |= \u03b1 s.t. bGI\u03b1 (w\n\u2032) > 0. Now, by definition of strong observability, O(\u03b1,w\u2032) > 0. Therefore, O(\u03b1,w\u2032)bGI\u03b1 (w\n\u2032) > 0. And due to the normalizing effect of the denominator in the definition of OGI, bOGI\u03b1 is a belief state.\nProposition 9. When \u2217 is OGI, postulate (P \u22172), in general, does not hold and does hold when \u03b1 is strongly observable.\nProof. This result follows directly from an understanding of the proof of Proposition 8.\nProposition 10. When \u2217 is OGI, postulate (P \u22173) holds. Proposition 11. Let \u2217 be OGI. If + is OGI, postulate (P \u22174) holds, otherwise it does not.\nAssuming (P \u22174) holds, I consider whether (P \u22175) holds only for two combinations of instantiations of \u2217 and +. Proposition 12. When \u2217 is OGI and + is OGI, postulate (P \u22175) does not hold.\nProof. An instance is provided where bOGI\u03b1 (\u03b2) > 0 and bOGI\u03b1\u2227\u03b2 6= (bOGI\u03b1 )OGI\u03b2 .\nContinuing with Example 3, where b = \u30080, 0.1, 0, 0.2, 0, 0.3, 0, 0.4\u3009, \u03b1 is (q \u2227 r) \u2228 (q \u2227 \u00acr \u2227 s) and bOGI\u03b1 = b GI \u03b1 = \u30080, 0.7, 0.3, 0, 0, 0, 0, 0\u3009. Let \u03b2 be q \u2227 r, then bOGI\u03b1 (\u03b2) = 0.7 > 0. But bOGI\u03b1\u2227\u03b2 = b OGI \u03b2 = \u30080, 1, 0, 0, 0, 0, 0, 0\u3009 and (bOGI\u03b1 )OGI\u03b2 = \u30080.3, 0.7, 0, 0, 0, 0, 0, 0\u3009.\nProposition 13. When \u2217 is BCI, postulate (P \u22171) holds.\nProof. It is known that Bayesian conditioning results in a belief state when the conditional is non-contradictory.\nProposition 14. When \u2217 is BCI, postulate (P \u22172) holds.\nProof. By definition of BC, all non-\u03b1-worlds get zero probability and the probabilities of the remaining \u03b1-worlds are magnified to sum to 1.\nProposition 15. When \u2217 is BCI, postulate (P \u22173) holds. Proposition 16. Let \u2217 be BCI. If + is BC or BCI, postulate (P \u22174) holds, otherwise it does not.\nFor the proof of the next proposition, a lemma is required.\nLemma 1. Let b(\u03b1) > 0. If bBC\u03b1 (\u03b2) > 0, then b(\u03b1\u2227\u03b2) > 0.\nProof. Assume bBC\u03b1 (\u03b2) > 0. Then there exists a w \u03b2 \u2208 W s.t. w\u03b2 |= \u03b2 and bBC\u03b1 (w\u03b2) > 0. By definition, bBC\u03b1 (w) = b(\u03b1,w) b(\u03b1) , implying b(\u03b1,w\u03b2) b(\u03b1) > 0. Hence, b(w\n\u03b2) > 0 and w\u03b2 |= \u03b1. But if w\u03b2 |= \u03b1, then w\u03b2 |= \u03b1\u2227 \u03b2, and due to b(w\u03b2) > 0, b(\u03b1 \u2227 \u03b2) > 0.\nProposition 17. When \u2217 is BCI and + is BC, postulate (P \u22175), in general, does not hold, but does hold when b(\u03b1) > 0.\nRecall that a probabilistic belief change operation \u25e6 is preservative iff for all belief states b and for all propositions \u03b1 and \u03b2, if b(\u03b1) > 0 and b(\u03b2) = 1, then b\u25e6\u03b1(\u03b2) = 1. Proposition 18. Operation OGI is not preservative, while BCI is preservative.\nProof. OGI: Let the vocabulary be {q, r} and b = \u30080, 0.5, 0.5, 0\u3009. Let \u03b1 be q and \u03b2 be q \u2194 \u00acr. Then b(\u03b1) > 0, b(\u03b2) = 1 and bGI\u03b1 = \u30080.5, 0.5, 0, 0\u3009. Let O(q, w) = 1 for all w \u2208W . Then bOGI\u03b1 = \u30080.5, 0.5, 0, 0\u3009 and bOGI\u03b1 (\u03b2) = 0.5.4\nBCI: bBCI\u03b1 (\u03b2) = b BC \u03b1 (\u03b2). By assuming that b(\u03b1) > 0 and b(\u03b2) = 1, one is implicitly assuming that if w |= \u03b1 s.t. b(\u03b1) > 0, then w |= \u03b2. This in turn implies that whenever bBC\u03b1 (w) > 0, that w |= \u03b2. The latter is due to conditionalization: {w \u2208 W | bBC\u03b1 (w) > 0} is a subset of {w \u2208 W | w |= \u03b1, b(w) > 0}. By (P \u22172), bBC\u03b1 (\u03b1) = 1. But due to the fact that for all w \u2208 W , if bBC\u03b1 (w) > 0, then w |= \u03b2, it must then be the case that bBC\u03b1 (\u03b2) = 1."}, {"heading": "Ontic and Epistemic Strength", "text": "Suppose there is a range of degrees for information being ontic (the effect of a physical action or occurrence) or epistemic (purely informative). I shall assume that the higher the information\u2019s degree of being ontic, the lower the epistemic status of that information. An agent has a certain sense of the degree to which a piece of received information is due to a physical action or event in the world. This sense may come about due to a combination of sensor readings and reasoning. If the agent performs an action and a change in the local environment matches the expected effect of the action, it can be quite certain that the effect is ontic information. If the agent receives the information from another agent (e.g., radio, through reading, a person speaking directly to the agent), then it should be clear to the agent that the information is epistemic and thus has a low degree of being ontic. If the agent\u2019s sensors show activity, but the agent knows that it did not presently perform an action with an effect matching its sensor readings, and if the readings do not reveal an epistemic source for the information, then the agent will have to infer from the present world conditions and the information received, or access learnt knowledge matching the present world conditions and the information received, the degree to which the information should be regarded as ontic. For instance, a person might stop talking just after you ask him/her to be quiet. Under particular conditions the person may stop talking due to your request and in other conditions he/she may have stopped talking anyway. Depending on the present world conditions, you might assign a higher (but not definitely certainty) or lower (but not definitely zero) degree of likelihood that the information (i.e., that the person stopped talking) is ontic. Or suppose you have been wearing dark glasses for one hour. You put them on due to the sky being clear and (too) bright. When you take your glasses off, it is not as bright as you thought it would be. So, has the ambient\n4Here, d is Hamming distance.\nbrightness decreased due to changes in the weather, or does it only seem darker when you remove your glasses, due to some unknown physiological process? In this case, it would be convenient to consider the brightness/darkness information as being equally likely ontic and epistemic.\nRecall from Definition 1 that os(\u03b1,w) indicates an agent\u2019s sense for the ontic strength of \u03b1 received in w. We say that os(\u03b1,w) = 1 when \u03b1 is certainly ontic in w. When \u03b1 is certainly epistemic in w, then os(\u03b1,w) = 0. In fact, let the epistemic strength of \u03b1 in w be defined as es(\u03b1,w) := 1\u2212 os(\u03b1,w)."}, {"heading": "Combining Update and Revision", "text": "I propose a way of trading off the probabilistic update and probabilistic revision defined earlier, using the notion of ontic strength.\nThe hybrid stochastic change of belief state b due to new information \u03b1 with ontic strength (denoted bC\u03b1) is defined as\nDefinition 14. bC \u03b1 := {\n(w, p) | w \u2208W,p = 1\n\u03b3\n[ (1\u2212 os(\u03b1,w))b\u2217\u03b1(w) + os(\u03b1,w)b \u03b1(w) ]} ,\nwhere \u03b3 is a normalizing factor so that \u2211 w\u2208W b C \u03b1 (w) = 1.\nDue to our assumption that \u03b1 is observed in the arrival world, not the departure world, os(\u00b7) is applied to the arrival world.\nConsidering the rationality postulates presented so far for belief update and revision, one can naturally suggest the following postulates for their combination.\n(PC1) bC\u03b1 is a belief state iff not ` \u00ac\u03b1 (PC2) bC\u03b1 (\u03b1) = 1 (PC3) If \u03b1 \u2261 \u03b2, then bC\u03b1 = bC\u03b2\nProposition 19. Postulate (PC1) does not hold. Proposition 20. Postulate (PC2) does not hold.\nProof. (PC2) does not hold because (P 2) does not hold.\nProposition 21. Postulate (PC3) holds.\nProof. (PC3) is holds because (P 3) and (P \u22173) hold.\nTheorem 1. If: the agent model M is observation-rational, \u03b1 is trustworthy and strongly observable, there exists an event e \u2208 \u03b5 which is event-rational and \u03b1 is an e-signal, then (i) bC\u03b1 is a belief state iff not ` \u00ac\u03b1 (i.e., then (PC1) is true) and (ii) bC\u03b1 (\u03b1) = 1 (i.e., then (P C2) is true).\nProof. Note that by Propositions 3 and 4, (P 1) and (P 2) hold. And recall that (P \u22171) and (P \u22172) are true when \u03b1 is strongly observable (see Props. 8, 9, 13 and 14).\n(i)(PC1) Given the antecedents of this proposition, we know by Proposition 4 that b \u03b1 is defined iff not ` \u00ac\u03b1. And by (P \u22174), b\u2217\u03b1 is defined iff not ` \u00ac\u03b1.\n(\u21d2) Assume bC\u03b1 is defined. So there exists a w \u2208 W s.t. bC\u03b1 (w) > 0, that is, 1 \u03b3 [ (1 \u2212 os(\u03b1,w))b\u2217\u03b1(w) +\nos(\u03b1,w)b \u03b1(w) ] > 0. Thus, either b\u2217\u03b1(w) > 0 (while 1 \u2212 os(\u03b1,w) > 0) or b \u03b1(w) > 0 (while os(\u03b1,w) > 0) (or both), which implies that b\u2217\u03b1 resp. b \u03b1 is defined. Therefore, not ` \u00ac\u03b1. (\u21d0) Assume not ` \u00ac\u03b1. Then b\u2217\u03b1 and b \u03b1 are defined. This implied that there exists a w \u2208 W s.t. either b\u2217\u03b1(w) > 0 or b \u03b1(w) > 0 (or both). Hence, b C \u03b1 (w) > 0 and due to normalization in the definition of C, bC\u03b1 is defined. (ii)(PC2) bC\u03b1 (\u03b1) = \u2211 w\u2208W,w|=\u03b1 b\nC \u03b1 (w) =\u2211\nw\u2208W,w|=\u03b1 1 \u03b3 [ (1 \u2212 os(\u03b1,w))b\u2217\u03b1(w) + os(\u03b1,w)b \u03b1(w) ] ,\nwhere \u03b3 = \u2211 w\u2208W [ (1 \u2212 os(\u03b1,w))b\u2217\u03b1(w) +\nos(\u03b1,w)b \u03b1(w) ] . But by (P \u22172) and (P 2), if w 6|= \u03b1, then b\u2217\u03b1(w) = 0 and b \u03b1(w) = 0. Hence, \u03b3 = \u2211 w\u2208W,w|=\u03b1 [ (1 \u2212 os(\u03b1,w))b\u2217\u03b1(w) + os(\u03b1,w)b \u03b1(w) ] . Therefore, bC\u03b1 (\u03b1) =\u2211\nw\u2208W,w|=\u03b1 (1\u2212os(\u03b1,w))b\u2217\u03b1(w)+os(\u03b1,w)b \u03b1(w)\u2211\nw\u2208W,w|=\u03b1 [ (1\u2212os(\u03b1,w))b\u2217\u03b1(w)+os(\u03b1,w)b \u03b1(w) ] = 1.\nAlthough one cannot expect C to be preservative, due to probabilistic update not being preservative (Prop. 5), one can expect C to have preservative-like behaviour under particular conditions: Recall that \u03b1 is defined to be \u03b2-trustworthy if for all w \u2208W , if w 6|= \u03b2, then O(\u03b1,w) = 0. Proposition 22. If b \u03b1(\u03b2) is a belief state, bC\u03b1 (\u03b2) is a belief state, b(\u03b2) = 1, \u03b1 is \u03b2-trustworthy and \u2217 is BCI, then bC\u03b1 (\u03b2) = 1.\nProof. By Proposition 6, b \u03b1(\u03b2) = 1, when \u03b1 is \u03b2trustworthy. By Proposition 18, \u2217 is preservative when defined as BCI. Then, for all w \u2208 W , if w 6|= \u03b2, then b \u03b1(\u03b2) = b \u2217 \u03b1(\u03b2) = 0. Hence, for all w \u2208 W , if w 6|= \u03b2, bC\u03b1 (w) = 0. Therefore, because b C \u03b1 (\u03b2) is a belief state,\nbC\u03b1 (\u03b2) = 1\u2212 bC\u03b1 (\u00ac\u03b2) = 1\u2212 \u2211 w\u2208w,w|=\u00ac\u03b2 bC\u03b1 (w)\n= 1\u2212 \u2211\nw\u2208w,w 6|=\u03b2\nbC\u03b1 (w)\n= 1\u2212 0 = 1."}, {"heading": "Examples and Analysis", "text": "HSBC is now analyzed via examples. The example domain is adapted from one of the domains in the article of Boutilier (1998) \u2013 here though, worlds are associated with probabilities, not plausibility ranks. There are eight possible worlds, depending on whether a book B is inside the house (if it is not in the house, then it is assumed to be on the patio, adjacent to the lawn), whether the book is dry and whether the lawn-grass G is dry. There are three events: rain \u2013 it rains, sprnk \u2013 the sprinkler is on, and null \u2013 neither of these, the\nnull event.5 In Boutilier\u2019s example, events are deterministic; however, events in this paper are modeled to be stochastic, to better illustrate the behaviour of the framework.\nTo simplify calculations and to aid the reader in understanding the results, in the following examples, the agent will associate equal epistemic/ontic strength to a particular piece of information for all worlds (per example case). I shall compute the agent\u2019s new belief state for each of os(\u03b1,w) \u2208 {0, 0.25, 0.5, 0.75, 1} (for all w \u2208 W ), for the two cases where \u03b1 is \u00acDry(G) and where \u03b1 is \u00acDry(G) \u2227 Dry(B).\nBoutilier models the agent\u2019s current (initial) epistemic state with the most plausible situation (rank 0) being (\u00acInside(B), Dry(B), Dry(G)) and the next plausible situation (rank 1) being (Inside(B), Dry(B), Dry(G)). I translate this as the agent having a belief state where b(\u00acInside(B), Dry(B), Dry(G)) = 0.67 and b(Inside(B), Dry(B), Dry(G)) = 0.33. Observe that in these examples, revision as OGI is equivalent to revision as BCI, because b(\u00acDry(G)) = b(\u00acDry(G) \u2227 Dry(B)) = 0.\nThe HSBC model M = \u3008W, \u03b5, T,E,O, os\u3009 is now specified.\nLet w1, . . . , w8 refer to worlds w1: (Inside(B), Dry(B), Dry(G)) w2: (Inside(B), Dry(B),\u00acDry(G)) w3: (Inside(B),\u00acDry(B), Dry(G)) w4: (Inside(B),\u00acDry(B),\u00acDry(G)) w5: (\u00acInside(B), Dry(B), Dry(G)) w6: (\u00acInside(B), Dry(B),\u00acDry(G)) w7: (\u00acInside(B),\u00acDry(B), Dry(G)) w8: (\u00acInside(B),\u00acDry(B),\u00acDry(G))\nThe events are \u03b5 = {rain, sprnk, null}. The following probabilities are debatable; they should not\nbe taken too seriously but serve to illustrate the framework.\nT (w1, null, w1) = 0.75 T (w5, null, w5) = 1 T (w1, null, w2) = 0.1 T (w5, null, w6) = 0 T (w1, null, w3) = 0.1 T (w5, null, w7) = 0 T (w1, null, w4) = 0.05 T (w5, null, w8) = 0\nT (w1, rain, w1) = 0 T (w5, rain, w5) = 0 T (w1, rain, w2) = 0.75 T (w5, rain, w6) = 0.05 T (w1, rain, w3) = 0 T (w5, rain, w7) = 0.05 T (w1, rain, w4) = 0.25 T (w5, rain, w8) = 0.9\nT (w1, sprnk, w1) = 0 T (w5, sprnk, w5) = 0 T (w1, sprnk, w2) = 0.8 T (w5, sprnk, w6) = 0.8 T (w1, sprnk, w3) = 0 T (w5, sprnk, w7) = 0.05 T (w1, sprnk, w4) = 0.2 T (w5, sprnk, w8) = 0.15\nE(null, w1) = 0.06 E(null, w5) = 0.15 E(rain, w1) = 0.31 E(rain, w5) = 0.7 E(sprnk, w1) = 0.63 E(sprnk, w5) = 0.15\n5I shall assume that the null event may include some unknown events (with unknown effects).\nO(\u00acDry(G), w1) = 0.05 O(\u00acDry(G) \u2227 Dry(B), w1) = 0.5 O(\u00acDry(G), w2) = 0.95 O(\u00acDry(G) \u2227 Dry(B), w2) = 0.8 O(\u00acDry(G), w3) = 0.05 O(\u00acDry(G) \u2227 Dry(B), w3) = 0.1 O(\u00acDry(G), w4) = 0.95 O(\u00acDry(G) \u2227 Dry(B), w4) = 0.05 O(\u00acDry(G), w5) = 0.05 O(\u00acDry(G) \u2227 Dry(B), w5) = 0.6 O(\u00acDry(G), w6) = 0.95 O(\u00acDry(G) \u2227 Dry(B), w6) = 0.98 O(\u00acDry(G), w7) = 0.05 O(\u00acDry(G) \u2227 Dry(B), w7) = 0.2 O(\u00acDry(G), w8) = 0.95 O(\u00acDry(G) \u2227 Dry(B), w8) = 0.15\nRecall that the current belief state is b = \u30080.33, 0, 0, 0, 0.67, 0, 0, 0\u3009. The following is a list of resulting belief states b\u2032 = b C \u00acDry(G) for the specified ontic strengths.\nos(\u00b7) bC \u00acDry(G)\n0.00 \u30080.00, 0.33, 0.00, 0.00, 0.00, 0.67, 0.00, 0.00\u3009 0.25 \u30080.00, 0.32, 0.00, 0.02, 0.00, 0.53, 0.00, 0.13\u3009 0.50 \u30080.00, 0.31, 0.00, 0.04, 0.00, 0.40, 0.00, 0.25\u3009 0.75 \u30080.00, 0.30, 0.00, 0.06, 0.00, 0.26, 0.00, 0.38\u3009 1.00 \u30080.00, 0.28, 0.00, 0.08, 0.01, 0.12, 0.00, 0.51\u3009\nSeveral behaviours can be noted: When the observation is completely epistemic, the probabilities of the two believed worlds are each shifted to their closest \u00acDry(G)-worlds. The more the agent considers the information to be ontic, the more its beliefs are spread out due to the nondeterminism of the events. Whether the observation is considered ontic or epistemic, the agent has a relatively strong belief (between 28% and 33%) that the book is inside and dry. However, in cases where the book is outside, there is a considerable shift in probability from the book being dry (w6) to it being wet (w8), as the agent moves towards an ontic mindset. One could perhaps argue that in an ontic mindset, the agent has access to event/transition information so as to reason about the causes of the book getting wet: it believes that there is a moderate to high likelihood that the book will get wet if it is on the patio, due to the sprinkler coming on or it starting to rain (explaining the wet-grass evidence).\nThe following is a list of resulting belief states b\u2032 = b C \u00acDry(G) \u2227 Dry(B) for the specified epistemic strengths.\nos(\u00b7) bC \u00acDry(G) \u2227 Dry(B)\n0.00 \u30080.00, 0.29, 0.00, 0.00, 0.00, 0.71, 0.00, 0.00\u3009 0.25 \u30080.00, 0.33, 0.00, 0.00, 0.03, 0.59, 0.00, 0.04\u3009 0.50 \u30080.01, 0.37, 0.00, 0.00, 0.07, 0.47, 0.01, 0.07\u3009 0.75 \u30080.01, 0.41, 0.00, 0.01, 0.10, 0.35, 0.01, 0.11\u3009 1.00 \u30080.02, 0.45, 0.00, 0.01, 0.14, 0.23, 0.01, 0.15\u3009\nWhen the agent considers the observation completely epistemically, its beliefs change very similarly to when it was only told that the grass is wet; the agent already believed that the book was dry. However, the extra information has a significant impact on how the agent\u2019s beliefs change when the observation is considered ontically. The agent now believes much less that the book is outside and wet and the grass is wet, and with 78% (as opposed to 40% with the first observation) that the book is dry and the grass is wet (independent of where the book is located). The reason is that when the received information includes a dry book, transitions are focused on going to dry-book worlds."}, {"heading": "Conclusion", "text": "In this paper I suggested a method to arrive at a new (probabilistic) belief state when the agent has mixed feelings about whether to revise or update its beliefs, given a new piece of information. Much attention was given to the design and analysis of the separate update and revision operations. The postulates and finally Theorem 1 add weight to my argument that the hybrid stochastic belief change (HSBC) operation is rational when the agent has a rational frame of mind.\nLooking at the examples above, the way in which probabilities shift among the possible worlds, given the different ontic/epistemic strengths, seems justifiable. However, more analysis is required here, especially when considering more complicated specification patterns of the ontic/epistemic strengths.\nDetermining os(\u03b1,w) for every foreseen \u03b1 in every possible world w will be challenging for a designer. Some deep questions are: Should the designer/agent provide the strengths (via stored values or programmed reasoning), or do these strengths come to the agent attached to the new information? What is the reasoning process we go through to determine whether information is epistemic or ontic, if at all? In general, how does an agent know when information is epistemic (requiring revision) or ontic (requiring update)?"}], "references": [{"title": "On the logic of theory change: Partial meet contraction and revision functions", "author": ["C.E. Alchourr\u00f3n", "P. G\u00e4rdenfors", "D. Makinson"], "venue": "Journal of Symbolic Logic 50(2):510\u2013530.", "citeRegEx": "Alchourr\u00f3n et al\\.,? 1985", "shortCiteRegEx": "Alchourr\u00f3n et al\\.", "year": 1985}, {"title": "Optimal control of Markov decision processes with incomplete state estimation", "author": ["K. Astr\u00f6m"], "venue": "Journal of Mathematical Analysis and Applications 10:174\u2013205.", "citeRegEx": "Astr\u00f6m,? 1965", "shortCiteRegEx": "Astr\u00f6m", "year": 1965}, {"title": "Unifying default reasoning and belief revision in a modal framework", "author": ["C. Boutilier"], "venue": "Artificial Intelligence 68:33\u201385.", "citeRegEx": "Boutilier,? 1994", "shortCiteRegEx": "Boutilier", "year": 1994}, {"title": "A unified model of qualitative belief change: a dynamical systems perspective", "author": ["C. Boutilier"], "venue": "Artificial Intelligence 98(1\u20132):281\u2013316.", "citeRegEx": "Boutilier,? 1998", "shortCiteRegEx": "Boutilier", "year": 1998}, {"title": "Proceedings of the thirteenth pacific rim international conference on artificial intelligence (pricai 2014)", "author": ["K. Chhogyal", "A. Nayak", "R. Schwitter", "A. Sattar"], "venue": "Pham, D., and Park, S., eds., Proc. of PRICAI 2014, volume 8862 of LNCS, 694\u2013707. Springer-Verlag.", "citeRegEx": "Chhogyal et al\\.,? 2014", "shortCiteRegEx": "Chhogyal et al\\.", "year": 2014}, {"title": "Belief revision and updates in numerical formalisms: An overview, with new results for the possibilistic framework", "author": ["D. Dubois", "H. Prade"], "venue": "Proceedings of the 13th International Joint Conference on Artifical Intelligence, volume 1 of IJCAI\u201993, 620\u2013625. San Francisco,", "citeRegEx": "Dubois and Prade,? 1993", "shortCiteRegEx": "Dubois and Prade", "year": 1993}, {"title": "Knowledge in Flux: Modeling the Dynamics of Epistemic States", "author": ["P. G\u00e4rdenfors"], "venue": "Massachusetts/England: MIT Press.", "citeRegEx": "G\u00e4rdenfors,? 1988", "shortCiteRegEx": "G\u00e4rdenfors", "year": 1988}, {"title": "Rank-based systems: A simple approach to belief revision, belief update", "author": ["M. Goldszmidt", "J. Pearl"], "venue": null, "citeRegEx": "Goldszmidt and Pearl,? \\Q1992\\E", "shortCiteRegEx": "Goldszmidt and Pearl", "year": 1992}, {"title": "Updating sets of probabilities", "author": ["A. Grove", "J. Halpern"], "venue": "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201998, 173\u2013182. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.", "citeRegEx": "Grove and Halpern,? 1998", "shortCiteRegEx": "Grove and Halpern", "year": 1998}, {"title": "Two modellings for theory change", "author": ["A. Grove"], "venue": "Journal of Philosophical Logic 17:157\u2013170.", "citeRegEx": "Grove,? 1988", "shortCiteRegEx": "Grove", "year": 1988}, {"title": "On the difference between updating a knowledge base and revising it", "author": ["H. Katsuno", "A. Mendelzon"], "venue": "Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning, 387\u2013394.", "citeRegEx": "Katsuno and Mendelzon,? 1991", "shortCiteRegEx": "Katsuno and Mendelzon", "year": 1991}, {"title": "Linking iterated belief change operations to nonmonotonic reasoning", "author": ["G. Kern-Isberner"], "venue": "Proceedings of the Eleventh International Conference on Principles of Knowledge Representation and Reasoning, 166\u2013176. Menlo Park, CA: AAAI Press.", "citeRegEx": "Kern.Isberner,? 2008", "shortCiteRegEx": "Kern.Isberner", "year": 2008}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "Cambridge, MA and London, England: The MIT Press.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Probabilities of conditionals and conditional probabilities", "author": ["D. Lewis"], "venue": "Philosophical Review 85(3):297\u2013315.", "citeRegEx": "Lewis,? 1976", "shortCiteRegEx": "Lewis", "year": 1976}, {"title": "A survey of algorithmic methods for partially observed Markov decision processes", "author": ["W. Lovejoy"], "venue": "Annals of Operations Research 28:47\u201366.", "citeRegEx": "Lovejoy,? 1991", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "A survey of partially observable Markov decision processes: Theory, models, and algorithms", "author": ["G. Monahan"], "venue": "Management Science 28(1):1\u201316.", "citeRegEx": "Monahan,? 1982", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "A formal theory of knowledge and action", "author": ["R. Moore"], "venue": "Allen, J.; Hendler, J.; and Tate, A., eds., Readings in Planning. San Mateo: Morgan-Kaufmann. 480\u2013519.", "citeRegEx": "Moore,? 1990", "shortCiteRegEx": "Moore", "year": 1990}, {"title": "Artificial Intelligence: Foundations of Computational Agents", "author": ["D. Poole", "A. Mackworth"], "venue": "New York, USA: Cambridge University Press.", "citeRegEx": "Poole and Mackworth,? 2010", "shortCiteRegEx": "Poole and Mackworth", "year": 2010}, {"title": "A new approach to probabilistic belief change", "author": ["G. Rens", "T. Meyer"], "venue": "Russell, I., and Eberle, W., eds., Proceedings of the International Florida AI Research Society Conference (FLAIRS), 582\u2013587. AAAI Press.", "citeRegEx": "Rens and Meyer,? 2015", "shortCiteRegEx": "Rens and Meyer", "year": 2015}, {"title": "Ordinal conditional functions: A dynamic theory of epistemic states", "author": ["W. Spohn"], "venue": "Harper, W., and Skyrms, B., eds., Causation in Decision, Belief Change, and Statistics, volume 42 of The University of Western Ontario Series in Philosophy of Science. Springer Netherlands. 105\u2013134.", "citeRegEx": "Spohn,? 1988", "shortCiteRegEx": "Spohn", "year": 1988}, {"title": "Partial Probability: Theory and Applications", "author": ["F. Voorbraak"], "venue": "Proceedings of the First International Symposium on Imprecise Probabilities and Their Applications, 360\u2013368. url: decsai.ugr.es/ smc/isipta99/proc/073.html.", "citeRegEx": "Voorbraak,? 1999", "shortCiteRegEx": "Voorbraak", "year": 1999}, {"title": "Revising imprecise probabilistic beliefs in the framework of probabilistic logic programming", "author": ["A. Yue", "W. Liu"], "venue": "Proceedings of the Twenty-third AAAI Conf. on Artificial Intelligence (AAAI-08), 590\u2013596.", "citeRegEx": "Yue and Liu,? 2008", "shortCiteRegEx": "Yue and Liu", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "In this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work.", "startOffset": 37, "endOffset": 54}, {"referenceID": 2, "context": "In this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work. However, I do not intend simply to give a probabilistic version of his Generalized Update Semantics. Whereas Boutilier (1998) presents a model for unifying qualitative belief revision and update, I build on his work to present a unified model of belief revision and update in a stochastic (probabilistic) setting.", "startOffset": 37, "endOffset": 274}, {"referenceID": 2, "context": "This is in contrast to Boutilier\u2019s \u201cgeneralized update\u201d approach, which takes a most-plausible event and most-plausible world approach. Finally, my proposal requires a trade-off factor to mix the changes in probability distribution over possible worlds brought about due to the probabilistic belief revision process and, respectively, the probabilistic belief update process. Boutilier\u2019s model has revision and update more tightly coupled. For this reason, his approach is better called \u201cunified\u201d while mine is called \u201chybrid\u201d. The belief change community does not study probabilistic belief update; it is studied almost exclusively in frameworks employing Bayesian conditioning \u2013 for modeling events and actions in dynamical domains (e.g., DBNs, MDPs, POMDPs) (Koller and Friedman, 2009; Poole and Mackworth, 2010, e.g.). The part of my approach responsible for updating stays within the Bayesian framework, but combines the essential elements of belief update with unobservable events and belief update as partially observable Markov decision process (POMDP) state estimation. On the other hand, there is plenty of literature on probabilistic belief revision (Voorbraak, 1999; Grove and Halpern, 1998; Kern-Isberner, 2008; Yue and Liu, 2008, e.g.). The subject is both deep and broad. There is no one accepted approach and to argue which is the best is not the focus of this paper. I shall choose one reasonable method for probabilistic belief revision suitable to the task at hand. In the first section, Boutilier\u2019s \u2018generalized update\u2019 is reviewed. Then, in the next section, I introduce stochastic update and stochastic revision, culminating in the \u2018hybrid stochastic belief change\u2019 (HSBC) approach. The final section presents an example inspired by Boutilier\u2019s article (1998) and analyses the results.", "startOffset": 23, "endOffset": 1782}, {"referenceID": 2, "context": "L (LCPL in Boutilier (1998)), and consequence operation Cn .", "startOffset": 11, "endOffset": 28}, {"referenceID": 8, "context": "To capture this intuition, Katsuno and Mendelzon (1991) propose a family of preorders {\u2264w| w \u2208 W}, where each \u2264w is a reflexive, transitive relation over W .", "startOffset": 27, "endOffset": 56}, {"referenceID": 6, "context": "An alternative model of revision, based on the notion of epistemic entrenchment (G\u00e4rdenfors, 1988), has a more constructive nature,\u201d (Boutilier, 1998, p.", "startOffset": 80, "endOffset": 98}, {"referenceID": 9, "context": "\u201cSemantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988; Boutilier, 1994).", "startOffset": 159, "endOffset": 189}, {"referenceID": 2, "context": "\u201cSemantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988; Boutilier, 1994).", "startOffset": 159, "endOffset": 189}, {"referenceID": 19, "context": "However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a \u03ba-ranking.", "startOffset": 86, "endOffset": 127}, {"referenceID": 7, "context": "However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a \u03ba-ranking.", "startOffset": 86, "endOffset": 127}, {"referenceID": 2, "context": "An alternative model of revision, based on the notion of epistemic entrenchment (G\u00e4rdenfors, 1988), has a more constructive nature,\u201d (Boutilier, 1998, p. 6). \u201cSemantically, an entrenchment relation (hence a revision function) can be modeled using an ordering on possible worlds reflecting their relative plausibility (Grove, 1988; Boutilier, 1994). However, rather than use a qualitative ranking relation, we adopt the presentation of (Spohn, 1988; Goldszmidt and Pearl, 1992) and rank all possible worlds using a \u03ba-ranking. Such a ranking \u03ba : W \u2192 N assigns to each world a natural number reflecting its plausibility or degree of believability. If \u03ba(w) < \u03ba(v) then w is more plausible than v or more consistent with the agent\u2019s beliefs. We insist that \u03ba\u22121(0) 6= \u2205, so that maximally plausible worlds are assigned rank 0. These maximally plausible worlds are exactly those consistent with the agent\u2019s beliefs; that is, the epistemically possible worlds according to K are those deemed most plausible in \u03ba (see Spohn (1988) for further details).", "startOffset": 134, "endOffset": 1022}, {"referenceID": 3, "context": "As explained in the introduction, my intention with this paper is not to give a probabilistic version of the Generalized Update approach (Boutilier, 1998).", "startOffset": 137, "endOffset": 154}, {"referenceID": 10, "context": "Boutilier motivates the need for a generalized update method which includes revision, by claiming that KM update (Katsuno and Mendelzon, 1991) is insufficient.", "startOffset": 113, "endOffset": 142}, {"referenceID": 2, "context": "As explained in the introduction, my intention with this paper is not to give a probabilistic version of the Generalized Update approach (Boutilier, 1998). For completeness, however, I sketch the approach here covering the approach in detail would take up unnecessary space without lending much insight into my Hybrid Stochastic Belief Change approach. Boutilier motivates the need for a generalized update method which includes revision, by claiming that KM update (Katsuno and Mendelzon, 1991) is insufficient. He provides the following example adopted from Moore (1990). Suppose you want to test whether the contents of a beaker are chemically acid or base.", "startOffset": 138, "endOffset": 573}, {"referenceID": 2, "context": "According to Boutilier (1998), update can be defined as", "startOffset": 13, "endOffset": 30}, {"referenceID": 1, "context": "In partially observable Markov decision process (POMDP) theory (Astr\u00f6m, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden.", "startOffset": 63, "endOffset": 107}, {"referenceID": 15, "context": "In partially observable Markov decision process (POMDP) theory (Astr\u00f6m, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden.", "startOffset": 63, "endOffset": 107}, {"referenceID": 14, "context": "In partially observable Markov decision process (POMDP) theory (Astr\u00f6m, 1965; Monahan, 1982; Lovejoy, 1991), events are actions chosen by the agent (and thus observable) and observations are hidden.", "startOffset": 63, "endOffset": 107}, {"referenceID": 6, "context": "Definition 8 (G\u00e4rdenfors, 1988).", "startOffset": 13, "endOffset": 31}, {"referenceID": 10, "context": "Imaging was introduced by Lewis (1976) as a means of revising a probability function.", "startOffset": 26, "endOffset": 39}, {"referenceID": 4, "context": "It has also been discussed in the work of, for instance, G\u00e4rdenfors (1988); Dubois and Prade (1993); Chhogyal et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 4, "context": "It has also been discussed in the work of, for instance, G\u00e4rdenfors (1988); Dubois and Prade (1993); Chhogyal et al.", "startOffset": 76, "endOffset": 100}, {"referenceID": 4, "context": "It has also been discussed in the work of, for instance, G\u00e4rdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015).", "startOffset": 101, "endOffset": 124}, {"referenceID": 4, "context": "It has also been discussed in the work of, for instance, G\u00e4rdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). The following version of imaging must not be regarded as a fundamental part of the larger belief change framework presented here; it should be regarded as a placeholder or suggestion for the \u2018revision-module\u2019 of the framework.", "startOffset": 101, "endOffset": 147}, {"referenceID": 4, "context": "It has also been discussed in the work of, for instance, G\u00e4rdenfors (1988); Dubois and Prade (1993); Chhogyal et al. (2014); Rens and Meyer (2015). The following version of imaging must not be regarded as a fundamental part of the larger belief change framework presented here; it should be regarded as a placeholder or suggestion for the \u2018revision-module\u2019 of the framework. Informally, Lewis\u2019s original solution for accommodating contradicting evidence \u03b1 is to move the probability of each world to its closest, \u03b1-world. Lewis made the strong assumption that every world has a unique closest \u03b1-world. More general versions of imaging allow worlds to have several, equally proximate, closest worlds. G\u00e4rdenfors (1988) calls one of his generalizations of Lewis\u2019s imaging general imaging.", "startOffset": 101, "endOffset": 718}, {"referenceID": 6, "context": "G\u00e4rdenfors (1988) proposed six rationality postulates for probabilistic belief revision.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "The example domain is adapted from one of the domains in the article of Boutilier (1998) \u2013 here though, worlds are associated with probabilities, not plausibility ranks.", "startOffset": 72, "endOffset": 89}], "year": 2016, "abstractText": "I propose a framework for an agent to change its probabilistic beliefs when a new piece of propositional information \u03b1 is observed. Traditionally, belief change occurs by either a revision process or by an update process, depending on whether the agent is informed with \u03b1 in a static world or, respectively, whether \u03b1 is a \u2018signal\u2019 from the environment due to an event occurring. Boutilier suggested a unified model of qualitative belief change, which \u201ccombines aspects of revision and update, providing a more realistic characterization of belief change.\u201d In this paper, I propose a unified model of quantitative belief change, where an agent\u2019s beliefs are represented as a probability distribution over possible worlds. As does Boutilier, I take a dynamical systems perspective. The proposed approach is evaluated against several rationality postulated, and some properties of the approach are worked out. Information acquired can be due to evolution of the world or revelation about the world. That is, one may notice via some \u2018signal\u2019 generated by the changing environment that the environment has changed, or, one may be informed by an independent agent in a static environment that some \u2018fact\u2019 holds. In the present work, I deal with belief change of agents who handle uncertainty by maintaining a probability distribution over possible situations. The agents in this framework also have models for nondeterministic events, and noisy observations. Noisy observation models can model imperfect sensory equipment for receiving environmental signals, but they can also model untrustworthy informants in a static world. In this paper, I provide the work of Boutilier (1998) as background, because it has several connections with and was the seed for the present work. However, I do not intend simply to give a probabilistic version of his Generalized Update Semantics. Whereas Boutilier (1998) presents a model for unifying qualitative belief revision and update, I build on his work to present a unified model of belief revision and update in a stochastic (probabilistic) setting. I also take a dynamical systems perspective, like him. Due to my quantitative approach, an agent can maintain a probability distribution Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. over the worlds it believes possible, using an expectation semantics of change. This is in contrast to Boutilier\u2019s \u201cgeneralized update\u201d approach, which takes a most-plausible event and most-plausible world approach. Finally, my proposal requires a trade-off factor to mix the changes in probability distribution over possible worlds brought about due to the probabilistic belief revision process and, respectively, the probabilistic belief update process. Boutilier\u2019s model has revision and update more tightly coupled. For this reason, his approach is better called \u201cunified\u201d while mine is called \u201chybrid\u201d. The belief change community does not study probabilistic belief update; it is studied almost exclusively in frameworks employing Bayesian conditioning \u2013 for modeling events and actions in dynamical domains (e.g., DBNs, MDPs, POMDPs) (Koller and Friedman, 2009; Poole and Mackworth, 2010, e.g.). The part of my approach responsible for updating stays within the Bayesian framework, but combines the essential elements of belief update with unobservable events and belief update as partially observable Markov decision process (POMDP) state estimation. On the other hand, there is plenty of literature on probabilistic belief revision (Voorbraak, 1999; Grove and Halpern, 1998; Kern-Isberner, 2008; Yue and Liu, 2008, e.g.). The subject is both deep and broad. There is no one accepted approach and to argue which is the best is not the focus of this paper. I shall choose one reasonable method for probabilistic belief revision suitable to the task at hand. In the first section, Boutilier\u2019s \u2018generalized update\u2019 is reviewed. Then, in the next section, I introduce stochastic update and stochastic revision, culminating in the \u2018hybrid stochastic belief change\u2019 (HSBC) approach. The final section presents an example inspired by Boutilier\u2019s article (1998) and analyses the results. Some proofs of propositions are omitted to save space; they are available on request. Boutilier\u2019s Generalized Update I use Boutilier\u2019s notation and descriptions, except that I am more comfortable with \u03b1 and \u03b2 to represent sentences, instead of A and B. It is assumed that an agent has a deductively closed belief set K, a set of sentences drawn from some logical language reflecting the agent\u2019s beliefs about the current state of the world. For ease of presentation, I assume ar X iv :1 60 4. 02 12 6v 1 [ cs .A I] 7 A pr 2 01 6 a logically finite, classical propositional language, denoted L (LCPL in Boutilier (1998)), and consequence operation Cn . The belief set K will often be generated by some finite knowledge base KB (i.e., K = Cn(KB)). The identically true and false propositions are denoted > and \u22a5, respectively. Given a set of possible worlds W (or valuations over L) and \u03b1 \u2208 L, the set of \u03b1-worlds, that is, the elements of W satisfying \u03b1, is denoted by ||\u03b1||. The worlds satisfying all sentences in a set K is denoted ||K||.", "creator": "LaTeX with hyperref package"}}}