{"id": "1605.02697", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "abstract": "We are engaged in a task designed as a Visual Turing Test to answer questions related to real-world images. By combining the latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, holistic formulation of this problem.", "histories": [["v1", "Mon, 9 May 2016 19:04:23 GMT  (4583kb,D)", "https://arxiv.org/abs/1605.02697v1", null], ["v2", "Thu, 24 Nov 2016 10:30:18 GMT  (3940kb,D)", "http://arxiv.org/abs/1605.02697v2", "Improved version, it also has a final table from the VQA challenge, and more baselines on DAQUAR"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["mateusz malinowski", "marcus rohrbach", "mario fritz"], "accepted": false, "id": "1605.02697"}, "pdf": {"name": "1605.02697.pdf", "metadata": {"source": "CRF", "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "authors": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "emails": ["mmalinow@mpi-inf.mpg.de", "rohrbach@berkeley.edu", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": null, "text": "Keywords Computer Vision \u00b7 Scene Understanding \u00b7 Deep Learning \u00b7 Natural Language Processing \u00b7 Visual Turing Test \u00b7 Visual Question Answering\nMateusz Malinowski Max Planck Institute for Informatics Saarbru\u0308cken, Germany E-mail: mmalinow@mpi-inf.mpg.de\nMarcus Rohrbach UC Berkeley EECS and ICSI Berkeley, CA, United States E-mail: rohrbach@berkeley.edu\nMario Fritz Max Planck Institute for Informatics Saarbru\u0308cken, Germany E-mail: mfritz@mpi-inf.mpg.de\nLSTM LSTM LSTM LSTM LSTM LSTM LSTM\nWhat is behind the table ?\nchairs window\nLSTM\n<END>\nCNN\nFig. 1 Our approach Ask Your Neurons to question answering with a Recurrent Neural Network using Long Short Term Memory (LSTM). To answer a question about an image, we feed in both, the image (CNN features) and the question (green boxes) into the LSTM. After the (variable length) question is encoded, we generate the answers (multiple words, orange boxes). During the answer generation phase the previously predicted answers are fed into the LSTM until the \u3008END\u3009 symbol is predicted. See Section 3.1 for more details."}, {"heading": "1 Introduction", "text": "With the advances of natural language processing and image understanding, more complex and demanding tasks have become within reach. Our aim is to take advantage of the most recent developments to push the state-of-the-art for answering natural language questions on real-world images. This task unites inference of a question intent and visual scene understanding with an answer generation task.\nRecently, architectures based on the idea of layered, endto-end trainable artificial neural networks have improved the state of the art across a wide range of diverse tasks. Most prominently Convolutional Neural Networks have raised the bar on image classification tasks [Krizhevsky et al., 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al., 2014].\nar X\niv :1\n60 5.\n02 69\n7v 2\n[ cs\n.C V\n] 2\n4 N\nov 2\nMost recently, these two trends of employing neural architectures have been combined fruitfully with methods that can generate image [Karpathy and Fei-Fei, 2015] and video descriptions [Venugopalan et al., 2015a]. Both are conditioned on the visual features that stem from deep learning architectures and employ recurrent neural network approaches to produce descriptions.\nTo further push the boundaries and explore the limits of deep learning architectures, we propose an architecture for answering questions about images. In contrast to prior work, this task needs conditioning on language as well visual input. Both modalities have to be interpreted and jointly represented as an answer depends on inferred meaning of the question and image content.\nWhile there is a rich body of work on natural language understanding that has addressed textual question answering tasks based on semantic parsing, symbolic representation and deduction systems, which also has seen applications to question answering about images [Malinowski and Fritz, 2014a], there is evidence that deep architectures can indeed achieve a similar goal [Weston et al., 2014]. This motivates our work to seek end-to-end architectures that learn to answer questions in a single, holistic model.\nWe propose Ask Your Neurons, an approach to question answering with a recurrent neural network at its core. An overview is given in Figure 1. The image information is encoded via a Convolutional Neural Network (CNN) and the question together with the visual representation is fed into a Long Short Term Memory (LSTM) network. The system is trained to produce the correct answer to the question about the image. CNN and LSTM are trained jointly and end-toend starting from words and pixels.\nOutline. In Section 3, we present our novel approach based on recurrent neural networks for the challenging task of answering questions about images, which we presented originally in Malinowski et al. [2015]. The approach combines a CNN with an LSTM into an end-to-end architecture that predicts answers conditioning on a question and an image. Section 4 shows that the proposed approach doubles performance compared to a prior symbolic approach on this task. We collect additional data to study human consensus on this task, propose two new metrics sensitive to these effects, and provide a new baseline, by asking humans to answer the questions without observing the image. We demonstrate a variant of our system that also answers question without accessing any visual information, which beats the human baseline. We also frame the multimodal approach to answer questions about images that combines LSTM with CNN [Malinowski et al., 2015] as a special instance of an encoder-decoder framework. This modular perspective, shown in Section 3.2, allows us to study different design choices on a large scale visual question answering dataset VQA.\nSection 6 shows our analysis that leads to an improved visual question answering architecture. A deeper visual encoding together with several important design choices lead to a model that achieves stronger performance on VQA and DAQUAR datasets."}, {"heading": "2 Related Work", "text": "Since we have proposed a challenge and first methods for answering questions about real-world images [Malinowski and Fritz, 2014a,b, 2015, Malinowski et al., 2015], frequently also referred to as \u201cVisual Question Answering\u201d, numerous follow up works have appeared. In the following we first discuss related tasks and subtasks, then early approaches to tackle a broader Visual Turing Test and datasets proposed for it. Finally, we discuss the relations to our work.\n2.1 Convolutional neural networks for visual recognition\nOne component to answer questions about images is to extract information from visual content. Since the proposal of AlexNet [Krizhevsky et al., 2012], Convolutional Neural Networks (CNNs) have become dominant and most successful approaches to extract relevant representation from the image. CNNs directly learn the representation from the raw image data and are trained on large image corpora, typically ImageNet [Russakovsky et al., 2014]. Interestingly, after these models are pre-trained on ImageNet, they can typically be adapted for other tasks. In this work, we evaluate how well the most dominant and successful CNN models can be adapted for the Visual Turing Test. Specifically, we evaluate AlexNet [Krizhevsky et al., 2012], VGG [Simonyan and Zisserman, 2014], GoogleNet [Szegedy et al., 2014], and ResNet [He et al., 2015]. These models, reportedly, achieve more and more accurate results on the ImageNet dataset, and hence, arguably, serve as increasingly stronger models of visual perception.\n2.2 Encodings for text sequence understanding\nThe other important component to answer a question about an image is to understand the natural language question, which means here building a representation of a variable length sequence of words (or characters, but we will focus only on the words in this work). The first approach is to encode all words of the question as a Bag-Of-Words [Manning and Schu\u0308tze, 1999], and hence ignoring an order in the sequence of words. Another option is to use, similar to the image encoding, a CNN with pooling to handle variable length input [Kim, 2014, Kalchbrenner et al., 2014]. Finally, Recurrent Neural Networks (RNNs) are methods developed to\ndirectly handle sequences, and have shown recent success on natural language tasks such as machine translation [Cho et al., 2014, Sutskever et al., 2014]. In this work we investigate a Bag-Of-Words (BOW), a CNN, and two RNN variants (LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al., 2014]) to encode the question.\n2.3 Combining RNNs and CNNs for description of visual content\nThe task of describing visual content like still images as well as videos has been successfully addressed with a combination of encoding the image with CNNs and decoding, i.e. predicting the sentence description with an RNN [Donahue et al., 2015, Karpathy and Fei-Fei, 2015, Venugopalan et al., 2015b, Vinyals et al., 2014, Zitnick et al., 2013]. This is achieved by using the RNN model that first gets to observe the visual content and is trained to afterwards predict a sequence of words that is a description of the visual content. Our work extends this idea to question answering, where we formulate a model trained to either generate or classify an answer based on visual as well as natural language input.\n2.4 Grounding of natural language and visual concepts\nDealing with natural language input does involve the association of words with meaning. This is often referred to as the grounding problem - in particular if the \u201cmeaning\u201d is associated with a sensory input. While such problems have been historically addressed by symbolic semantic parsing techniques [Krishnamurthy and Kollar, 2013, Matuszek et al., 2012], there is a recent trend of machine learning-based approaches [Kong et al., 2014, Karpathy et al., 2014, Karpathy and Fei-Fei, 2015, Akata et al., 2016, Hu et al., 2016b, Rohrbach et al., 2015a, Mao et al., 2016, Wang et al., 2016, Hu et al., 2016a, Plummer et al., 2016] to find the associations. These approaches have partially been enabled by recently proposed larger scale datasets [Kazemzadeh et al., 2014, Plummer et al., 2015, Yu et al., 2016], providing phrases or referential expressions which are associated with their corresponding image location. Answering questions about images can be interpreted as first grounding the question in the image and then predicting an answer. Our approach thus is similar to the latter approaches in that we do not enforce or evaluate any particular representation of \u201cmeaning\u201d on the language or image modality. We treat this as latent and leave it to the joint training approach to establish an appropriate hidden representation to link the visual and textual representations.\n2.5 Textual question answering\nAnswering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task. Weston et al. [2014] propose different kind of network - memory networks - that is used to answer questions about short stories. In their work, all the parts of the story are embedded into different \u201cmemory cells\u201d, and next a network is trained to attend to relevant cells based on the question and decode an answer from that. A similar idea has also been applied to question answering about images, for instance by Yang et al. [2015].\n2.6 Towards a Visual Turing Test\nRecently, a large number architectures have been proposed to approach the Visual Turing Test [Malinowski and Fritz, 2014b], in which they mainly tackle a particularly important subtask that tests machines regarding their abilities to answer to questions about real-world images. Such methods range from symbolic to neural based. There are also architectures that combine both symbolic and neural paradigms together. Some approaches use explicit visual representation in the form of bounding boxes surrounding objects of interest, while other use global full frame image representation, or soft attention mechanism. Yet others use an external knowledge base that helps in answering questions.\nSymbolic based approaches. In our first work towards a Visual Turing Test [Malinowski and Fritz, 2014a], we present a question answering system based on a semantic parser on a varied set of human question-answer pairs. Although it is the first attempt to handle question answering on DAQUAR, and despite its introspective benefits, it is a rule-based approach that requires a careful schema crafting, is not that scalable, and finally it strongly depends on the output of visual analysis methods as joint training in this model is not yet possible. Due to such limitations, the community has rather shifted towards either neural based or combined approaches.\nDeep Neural Approaches with full frame CNN. Several contemporary approaches use a global image representation, i.e. they encode the whole image with a CNN. Questions are then encoded with an RNN [Malinowski et al., 2015, Ren et al., 2015, Gao et al., 2015] or a CNN [Ma et al., 2016].\nIn contrast to symbolic based approaches, neural based architectures offer scalable and joint end-to-end training that liberates them from ontological commitment that would otherwise be introduced by a semantic parser. Moreover, such approaches are not \u2018hard\u2019 conditioned on the visual input and therefore can naturally take advantage of different language biases in question answer pairs, which can be interpret as learning common sense knowledge.\nAttention-based Approaches. Following Xu et al. [2015], who proposed to use spatial attention for image description, Yang et al. [2015], Xu and Saenko [2015], Zhu et al. [2016], Chen et al. [2015], Shih et al. [2016], Fukui et al. [2016] predict a latent weighting (attention) of spatially localized images features (typically a convolutional layer of the CNN) based on the question. The weighted image representation rather than the full frame feature representation is then used as a basis for answering the question. In contrast to the previous models using attention, Dynamic Memory Networks (DMN) [Kumar et al., 2015, Xiong et al., 2016] first pass all spatial image features through a bi-directional GRU that captures spatial information from the neighboring image patches, and next retrieve an answer from a recurrent attention based neural network that allows to focus only on a subset of the visual features extracted in the first pass. Another interesting direction has been taken by Ilievski et al. [2016] who run state-of-the-art object detector of the classes extracted from the key words in the question. In contrast to other attention mechanisms, this approach offers a focused, question dependent, \u201chard\u201d attention.\nAnswering with an external knowledge base. Wu et al. [2016] argue for an approach that first represents an image as an intermediate semantic attribute representation, and next query external knowledge sources based on the most prominent attributes and relate them to the question. With the help of such external knowledge bases, this approach captures richer semantic representation of the world, beyond what is directly contained in images.\nCompositional approaches. A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences. Related to the Visual Turing Test, Malinowski and Fritz [2014c] have also combined a neural based representation with the compositionality of the language for the text-to-image retrieval task.\nDynamic parameters. Noh et al. [2015] have an image recognition network and a Recurrent Neural Network (GRU) that dynamically change the parameters (weights) of visual representation based on the question. More precisely, the parameters of its second last layer are dynamically predicted from the question encoder network and in this way changing for each question. While question encoding and image encoding is pre-trained, the network learns parameter prediction only from image-question-answer triples.\n2.7 Datasets for visual question answering\nDatasets are a driving force for the recent progress in visual question answering. A large number of visual question answering datasets have recently been proposed. The first proposed datasets is DAQUAR [Malinowski and Fritz, 2014a], which contains about 12.5 thousands manually annotated question-answer pairs about 1449 indoor scenes [Silberman et al., 2012]. While the dataset originally contained a single answer (that can consist of multiple words) per question, in this work we extend the dataset by collecting additional answers for each questions. This captures uncertainties in evaluation. We evaluate our approach on this dataset and discuss several consensus evaluation metrics that take the extended annotations into account. In parallel to our work, Geman et al. [2015] developed another variant of the Visual Turing Test. Their work, however, focuses on sequences of yes/no type of questions, and provides detailed object-scene annotations.\nShortly after the introduction of DAQUAR, three other large-scale datasets have been proposed. All are based on MS-COCO [Lin et al., 2014]. Gao et al. [2015] have annotated about 158k images with 316k Chinese question answer pairs together with their corresponding English translations. Ren et al. [2015] have taken advantage of the existing annotations for the purpose of image description generation task and transform them into question answer pairs with the help of a set of hand-designed rules and a syntactic parser [Klein and Manning, 2003]. This procedure has approximately generated 118k question answer pairs. Finally and currently the most popular, large scale dataset on question answering about images is VQA [Antol et al., 2015]. It has approximately 614k questions about the visual content of about 205k real-world images (the whole VQA dataset also contains 150k questions about 50k abstract scenes that are not considered in this work). Similarly to our Consensus idea, VQA provides 10 answers per each image. For the purpose of the challenge the test answers are not publicly available. We perform one part of the experimental analysis in this paper on the VQA dataset, examining different variants of our proposed approach.\nAlthough simple, automatic performance evaluation metrics have been a part of building first visual question answer-\ning datasets [Malinowski and Fritz, 2014a,b, 2015], Yu et al. [2015] have simplified the evaluation even further by introducing Visual Madlibs - a multiple choice question answering by filling the blanks task. In this task, a question answering architecture has to choose one out of four provided answers for a given image and the prompt. Formulating question answering task in this way has wiped out ambiguities in answers, and just a simple accuracy metric can be used to evaluate different architectures on this task. Yet, the task requires holistic reasoning about the images, and despite of simple evaluation, it remains challenging for machines.\nThe Visual7W [Zhu et al., 2016] extends canonical question and answer pairs with additional groundings of all objects appearing in the questions and answers to the image by annotating the correspondences. It contains natural language answers, but also answers which require to locate the object, which is then similar to the task of explicit grounding discussed above. Visual7W builds question answer pairs based on the Visual Genome dataset [Krishna et al., 2016], and contains about 330k questions. In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers.\nRelated to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system. This work focuses on a user centric, dynamic scenario, where the provided answers are conditioned not only on questions but also on the geographical position of the questioner.\nMoving from asking questions about images to questions about video enhances typical questions with temporal structure. Zhu et al. [2015] propose a task which requires to fill in blanks the captions associated with videos. The task requires inferring the past, describing the present and predicting the future in a diverse set of video description data ranging from cooking videos [Regneri et al., 2013] over web videos [Trecvid, 2014] to movies [Rohrbach et al., 2015b]. Tapaswi et al. [2016] propose MovieQA, which requires to understand long term connections in the plot of the movie. Given the difficulty of the data, both works provide multiplechoice answers.\n2.8 Relations to our work\nThe original version of this work [Malinowski et al., 2015] belongs to the category of \u201cDeep Neural Approaches with full frame CNN\u201d, and is among the very first methods of\nthis kind (Section 3.1). We extend [Malinowski et al., 2015] by introducing a more general and modular encoder-decoder perspective (Section 3.2) that encapsulates a few different neural approaches. Next, we broaden our original analysis done on DAQUAR (Section 4) to the analysis of different neural based approaches on VQA showing the importance of getting a few details right together with benefits of a stronger visual encoder (Section 6). Finally, we transfer lessons learnt from VQA [Antol et al., 2015] to DAQUAR [Malinowski and Fritz, 2014a], showing a significant improvement on this challenging task (Section 6)."}, {"heading": "3 Ask Your Neurons", "text": "Answering questions about images can be formulated as the problem of predicting an answer a given an image x and a question q according to a parametric probability measure:\na\u0302 = argmax a\u2208A\np(a|x, q;\u03b8) (1)\nwhere \u03b8 represent a vector of all parameters to learn and A is a set of all answers. The question q is a sequence of words, i.e. q = [q1, . . . , qn], where each qt is the t-th word question with qn = \u201c?\u201d encoding the question mark - the end of the question. In the following we describe how we represent x, a, q, and p(\u00b7|x, q;\u03b8) in more details.\nIn a scenario of multiple word answers, we consequently decompose the problem to predicting a set of answer words aq,x = { a1,a2, ...,aN (q,x) } , where at are words from a finite vocabulary V \u2032, and N (q, x) is the number of answer words for the given question and image. In our approach, named Ask Your Neurons, we propose to tackle the problem as follows. To predict multiple words we formulate the problem as predicting a sequence of words from the vocabulary V := V \u2032 \u222a {$} where the extra token $ indicates the end of the answer sequence, and points out that the question has been fully answered. We thus formulate the prediction procedure recursively:\na\u0302t = argmax a\u2208V\np(a|x, q, A\u0302t\u22121;\u03b8) (2)\nwhere A\u0302t\u22121 = {a\u03021, . . . , a\u0302t\u22121} is the set of previous words, with A\u03020 = {} at the beginning, when our approach has not given any answer word so far. The approach is terminated when a\u0302t = $. We evaluate the method solely based on the predicted answer words ignoring the extra token $. To ensure uniqueness of the predicted answer words, as we want to predict the set of answer words, the prediction procedure can be be trivially changed by maximizing over V \\ A\u0302t\u22121. However, in practice, our algorithm learns to not predict any previously predicted words.\nIf we only have single word answers, or if we model each multi-word answer as a different answer (i.e. vocabulary entry), we use Equation 1 only once to pick the most likely answer.\nIn the following we first present a Ask Your Neurons that models multi-word answers with a single recurrent network for question and image encoding and answer prediction (Section 3.1) and then present a more general and modular framework with question and image encoders, as well as answer decoder as modules (Section 3.2).\n3.1 Method\nAs shown in Figure 1 and Figure 2, we feed our approach Ask Your Neurons with a question as a sequence of words. Since our problem is formulated as a variable-length inputoutput-sequence, we decide to model the parametric distribution p(\u00b7|x, q;\u03b8) of Ask Your Neurons with a recurrent neural network and a softmax prediction layer. More precisely, Ask Your Neurons is a deep network built of CNN [LeCun et al., 1998] and Long-Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997]. We decide on LSTM as it has been recently shown to be effective in learning a variable-length sequence-to-sequence mapping [Donahue et al., 2015, Sutskever et al., 2014].\nBoth question and answer words are represented with one-hot vector encoding (a binary vector with exactly one non-zero entry at the position indicating the index of the word in the vocabulary) and embedded in a lower dimensional space, using a jointly learnt latent linear embedding. In the training phase, we augment the question words sequence q with the corresponding ground truth answer words sequence a, i.e. q\u0302 := [q,a]. During the test time, in the prediction phase, at time step t, we augment q with previously predicted answer words a\u03021..t := [a\u03021, . . . , a\u0302t\u22121], i.e. q\u0302t := [q, a\u03021..t]. This means the question q and the previous answer words are encoded implicitly in the hidden states of the LSTM, while the latent hidden representation is learnt. We encode the image x using a CNN and provide it at every\n\u03c3\n\u03c3\u03c3\nvt ht-1 ct-1\nht = zt\nOutput Gate Input Gate\nForget Gate\nInput Modulation Gate\nLSTM Unit\n\u03d5 + ct \u03d5\nFig. 3 LSTM unit. See Section 3.1.1, Equations (3)-(8) for details.\ntime step as input to the LSTM. We set the input vt as a concatenation of [\u03a6(x), q\u0302t], where \u03a6(\u00b7) is the CNN encoding."}, {"heading": "3.1.1 Long-Short Term Memory (LSTM)", "text": "As visualized in detail in Figure 3, the LSTM unit takes an input vector vt at each time step t and predicts an output word zt which is equal to its latent hidden state ht. As discussed above, zt is a linear embedding of the corresponding answer word at. In contrast to a simple RNN unit, the LSTM unit additionally maintains a memory cell c. This allows to learn long-term dynamics more easily and significantly reduces the vanishing and exploding gradients problem [Hochreiter and Schmidhuber, 1997]. More precisely, we use the LSTM unit as described in Zaremba and Sutskever [2014]. With the sigmoid nonlinearity \u03c3 : R 7\u2192 [0, 1], \u03c3(v) = (1 + e\u2212v)\u22121 and the hyperbolic tangent nonlinearity \u03c6 : R 7\u2192 [\u22121, 1], \u03c6(v) = ev\u2212e\u2212vev+e\u2212v = 2\u03c3(2v) \u2212 1, the LSTM updates for time step t given inputs vt, ht\u22121, and the memory cell ct\u22121 as follows:\nit = \u03c3(Wvivt +Whiht\u22121 + bi) (3)\nf t = \u03c3(Wvfvt +Whfht\u22121 + bf ) (4)\not = \u03c3(Wvovt +Whoht\u22121 + bo) (5)\ngt = \u03c6(Wvgvt +Whght\u22121 + bg) (6)\nct = f t ct\u22121 + it gt (7) ht = ot \u03c6(ct) (8)\nwhere denotes element-wise multiplication. All the weights W and biases b of the network are learnt jointly with the cross-entropy loss. Conceptually, as shown in Figure 3, Equation 3 corresponds to the input gate, Equation 6 the input modulation gate, and Equation 4 the forget gate, which determines how much to keep from the previous memory ct\u22121 state. As Figures 1 and 2 suggest, all the output predictions that occur before the question mark are excluded from the loss computation, so that the model is penalized solely based on the predicted answer words.\nQuestion Encoder\nVisual Encoder C Answer Decoder\nFig. 4 Our Refined Ask Your Neurons architecture for answering questions about images that includes the following modules: visual and question encoders, and answer decoder. A multimodal embedding C combines both encodings into a joint space that the decoder decodes from. See Section 3.2 for details.\n3.2 Refined Ask Your Neurons\nIn the previous section, we have described how to achieve visual question answering with a single recurrent network for question and image encoding and answering. In this section, we abstract away from the particular design choices taken and describe a modular framework, where a question encoder has to be combined with a visual encoder in order to produce answers with an answer decoder (Figure 4). This modular representation allows us to systematically investigate and evaluate a range of design choices of different encoders, multimodal embeddings, and decoders."}, {"heading": "3.2.1 Question encoders", "text": "The main goal of a question encoder is to capture a meaning of the question, which we write here as \u03a8(q). Such an encoder can range from a very structured ones like Semantic Parser used in Malinowski and Fritz [2014a] and Liang et al. [2013] that explicitly model compositional nature of the question, to orderless Bag-Of-Word (BOW) approaches that merely compute a histogram over the question words (Figure 6). In this work, we investigate a few encoders within such spectrum that are compatible with the proposed Deep Learning approach: Two recurrent question encoders, LSTM [Hochreiter and Schmidhuber, 1997] (see Section 3.1.1) and GRU [Cho et al., 2014], that assume a temporal ordering in questions, as well as the aforementioned BOW.\nGated Recurrent Unit (GRU). GRU is a simplified variant of LSTM that also uses gates (a reset gate r and an update gate u) in order to keep long term dependencies. GRU is expressed by the following set of equations:\nrt = \u03c3(Wvrvt +Whrht\u22121 + br) (9)\nut = \u03c3(Wvuvt +Whuht\u22121 + bu) (10)\nct =Wvcvt +Whc(rt ht\u22121) + bc (11) ht = ut ht\u22121 + (1\u2212 ut) \u03c6(ct) (12) where \u03c3 is the sigmoid function, \u03c6 is the hyperbolic tangent, and vt, ht are input and hidden state at time t. The representation of the question q is the hidden vector at last time step, i.e. \u03a8RNN(q) := hT .\nWhat is behind the table?\npixels, and then take the features from the last pooling layer, which therefore have a dimension of 512\u21e514\u21e514, as shown in Fig. 2. 14\u21e5 14 is the number of regions in the image and 512 is the dimension of the feature vector for each region. Accordingly, each feature vector in fI corresponds to a 32\u21e5 32 pixel region of the input images. We denote by fi, i 2 [0, 195] the feature vector of each image region.\nThen for modeling convenience, we use a single layer perceptron to transform each feature vector to a new vector that has the same dimension as the question vector (described in Sec. 3.2):\nvI = tanh(WIfI + bI), (2)\nwhere vI is a matrix and its i-th column vi is the visual feature vector for the region indexed by i.\n3.2. Question Model\nAs [25, 22, 6] show that LSTMs and CNNs are powerful to capture the semantic meaning of texts, we explore both models for question representations in this study.\n3.2.1 LSTM based question model\nLSTM LSTM LSTM\u2026\nwhat are bicycle\nWe We We\nQuestion:\n\u2026\n\u2026\nFigure 3: LSTM based question model\nThe essential structure of a LSTM unit is a memory cell ct which reserves the state of a sequence. At each step, the LSTM unit takes one input vector (word vector in our case) xt and updates the memory cell ct, then output a hidden state ht. The update process uses the gate mechanism. A forget gate ft controls how much information from past state ct 1 is preserved. An input gate it controls how much the current input xt updates the memory cell. An output gate ot controls how much information of the memory is fed to the output as hidden state. The detailed update process is as follows:\nit = (Wxixt + Whiht 1 + bi), (3) ft = (Wxfxt + Whfht 1 + bf ), (4) ot = (Wxoxt + Whoht 1 + bo), (5) ct =ftct 1 + it tanh(Wxcxt + Whcht 1 + bc), (6) ht =ot tanh(ct), (7)\nwhere i, f, o, c are input gate, forget gate, output gate and memory cell, respectively. The weight matrix and bias are parameters of the LSTM and are learned on training data.\nunigram bigram\ntrigram max pooling over time\nconvolution\nw hat are sitting bicycle \u2026Question:\nembedding\nFigure 4: CNN based question model\nIn this study, we also explore to use a CNN similar to [11] for question representation. Similar to the LSTMbased question model, we first embed words to vectors xt = Weqt and get the question vector by concatenating the word vectors:\nx1:T = [x1, x2, ..., xT ]. (10)\nThen we apply convolution operation on the word embedding vectors. We use three convolution filters, which have the size of one (unigram), two (bigram) and three (trigram) respectively. The t-th convolution output using window size c is given by:\nhc,t = tanh(Wcxt:t+c 1 + bc). (11)\nThe filter is applied only to window t : t + c 1 of size c. Wc is the convolution weight and bc is the bias. The feature map of the filter with convolution size c is given by:\nhc = [hc,1, hc,2, ..., hc,T c+1]. (12)\nThen we apply max-pooling over the feature maps of the convolution size c and denote it as\nh\u0303c = max t [hc,1, hc,2, ..., hc,T c+1]. (13)\nBag-Of-Word (BOW). Conceptually the simplest encoder, the BOW approach (Figure 6) sums over the words embeddings:\n\u03a8BOW(q) := n\u2211 t We(qt). (13)\nwhereW e is a m trix and qt is one-hot binary vector of the word with exactly one 1 pointing to a place of the \u2019word\u2019 in the vocabulary (Figure 6). BOW does not encode the ordering of words in the question, so that especially questions with swapped arguments of spatial prepositions become indistinguishable, i.e. \u03a8BOW(red chair left of sofa) = \u03a8BOW(red sofa left of chair) in the BOW sentence representation.\nConvolutional Neural Network (CNN). Convolutional Neural Networks (CNN) have been proposed to encode language [Kim, 2014, Kalchbrenner et al., 2014, Ma et al., 2016, Yang et al., 2015] and since have shown to be fast to compute and result in good accuracy. Since they consider a larger context, they arguably maintain more structure than BOW, but do not model such long term dependencies as recurrent neural networks. Figure 5 depicts our CNN architecture, which is very similar to Ma et al. [2016] and Yang et al. [2015], that convolves word embeddings with three convolutional kernels of length 1, 2 and 3. For the sake of clarity, we only show two kernels in the figure. We either learn them jointly with the whole model or use GLOVE [Pennington et al., 2014] in our experiments. We call such architecture with 1, ..., n kernel lengths n views CNN. At the end, the kernel\u2019s outputs are temporarily aggregated for the final question\u2019s representation. We use either sum pooling or a recurrent neural network (CNN-RNN) to accomplish this step."}, {"heading": "3.2.2 Visual encoders", "text": "The second important component of the encoder-decoder architectures is the visual representation. Convolutional Neural Networks (CNNs) have become the state-of-the-art framework that provide features from images. The typical protocol of using the visual models is to first pre-train them on the ImageNet dataset [Russakovsky et al., 2014], a large scale recognition dataset, and next use them as an input for the rest of the architecture. Fine-tuning the weights of the encoder to the task at hand is also possible. In our experiments, we use chronologically the oldest CNN architecture fully trained on ImageNet \u2013 a Caffe implementation of AlexNet [Jia et al., 2014, Krizhevsky et al., 2012] \u2013 as well as the recently introduced deeper networks \u2013 Caffe implementations of GoogLeNet and VGG [Szegedy et al., 2014, Simonyan and Zisserman, 2014] \u2013 to the most recent extremely deep architectures \u2013 a Facebook implementation of 152 layered ResidualNet [He et al., 2015]. As can be seen from our experiments in Section 6, a strong visual encoder plays an important role in the overall performance of the architecture."}, {"heading": "3.2.3 Multimodal embedding", "text": "The presented neural question encoders transform linguistic question into a vector space. Similarly visual encoders encode images as vectors. A multimodal fusion module combines both vector spaces into another vector based on which the answer is decoded. Let \u03a8(q) be a question representation (BOW, CNN, LSTM, GRU), and\u03a6(x) be a representation of an image. Then C(\u03a8(q), \u03a6(x)) is a function which embeds both vectors. In this work, we investigate three multimodal embedding techniques: Concatenation, element-wise multiplication, and summation. Since the last two techniques require compatibility in the number of feature components, we\nuse additional visual embedding matrixWve \u2208 R|\u03a8(q)|\u00d7|\u03a6(x)|. Let W be weights of an answer decoder. Then we have WC(\u03a8(q), \u03a6(x)), which is\nWq\u03a8(q) +Wv\u03a6(x) (14)"}, {"heading": "W (\u03a8(q) Wve\u03a6(x)) (15)", "text": "W\u03a8(q) +WWve\u03a6(x) (16)\nin concatenation, element-wise multiplication, and summation fusion techniques respectively. In Equation 14, we decompose W into two matrices Wq and Wv , that is W = [Wq;Wv]. In Equation 15, is an element-wise multiplication. Similarity between Equation 14 and Equation 16 is interesting as the latter is the former with weight sharing and additional decomposition into WWve."}, {"heading": "3.2.4 Answer decoders", "text": "We consider two approach to decode the internal representation of our model into an answer.\nAnswer words generation. The last component of our architecture (Figure 4) is an answer decoder. Inspired by the work on the image description task [Donahue et al., 2015], we uses an LSTM as decoder that shares the parameters with the encoder.\nClassification. As alternative, we can cast the answering problem as a classification task, with answers as different classes. This approach has been widely explored, especially on VQA [Antol et al., 2015]."}, {"heading": "4 Analysis on DAQUAR", "text": "In this section, we benchmark our method on a task of answering questions about images. We compare different variants of our proposed model to prior work in Section 4.1. In addition, in Section 4.2, we analyze how well questions can be answered without using the image in order to gain an understanding of biases in form of prior knowledge and common sense. We provide a new human baseline for this task. In Section 4.3 we discuss ambiguities in the question answering tasks and analyze them further by introducing metrics that are sensitive to these phenomena. In particular, the WUPS score [Malinowski and Fritz, 2014a] is extended to a consensus metric that considers multiple human answers. All the material is available on our project webpage 1.\n1 http://mpii.de/visual_turing_test\nAccu- WUPS WUPS racy @0.9 @0.0\nMalinowski and Fritz [2014a] 7.86 11.86 38.79\nAsk Your Neurons (ours) - multiple words 17.49 23.28 57.76 - single word 19.43 25.28 62.00 Human answers 2014a 50.20 50.82 67.27\nQuestion-only (ours) - multiple words 17.06 22.30 56.53 - single word 17.15 22.80 58.42 Human answers, no images 7.34 13.17 35.56\nTable 1 Results on DAQUAR, all classes, single reference, in %.\nExperimental protocol. We evaluate our approach from Section 3 on the DAQUAR dataset [Malinowski and Fritz, 2014a], which provides 12, 468 human question answer pairs on images of indoor scenes [Silberman et al., 2012] and follow the same evaluation protocol by providing results on accuracy and the WUPS score at {0.9, 0.0}. We run experiments for the full dataset as well as their proposed reduced set that restricts the output space to only 37 object categories and uses 25 test images. In addition, we also evaluate the methods on different subsets of DAQUAR where only 1, 2, 3 or 4 word answers are present.\nWe use default hyper-parameters of LSTM [Donahue et al., 2015] and CNN [Jia et al., 2014]. All CNN models are first pre-trained on the ImageNet dataset [Russakovsky et al., 2014], and next we randomly initialize and train the last layer together with the LSTM network on the task. We find this step crucial to obtain good results. We have explored the use of a 2 layered LSTM model, but have consistently obtained worse performance. In a pilot study, we have found that GoogleNet architecture [Jia et al., 2014, Szegedy et al., 2014] consistently outperforms the AlexNet architecture [Jia et al., 2014, Krizhevsky et al., 2012] as a CNN model for our task and model.\nWUPS scores. We base our experiments and the consensus metrics on WUPS scores [Malinowski and Fritz, 2014a]. The metric is a generalization of the accuracy measure that accounts for word-level ambiguities in the answer words. For instance \u2018carton\u2019 and \u2018box\u2019 can be associated with a similar concept, and hence models should not be strongly penalized for this type of mistakes. Formally:\nWUPS(A, T ) = 1\nN N\u2211 i=1 min{ \u220f a\u2208Ai max t\u2208T i \u00b5(a, t),\n\u220f t\u2208T i max a\u2208Ai \u00b5(a, t)}\nTo embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based\nWu-Palmer similarity [Wu and Palmer, 1994] for \u00b5. Smaller thresholds yield more forgiving metrics. As in Malinowski and Fritz [2014a], we report WUPS at two extremes, 0.0 and 0.9.\n4.1 Evaluation of Ask Your Neurons\nWe start with the evaluation of our Ask Your Neurons on the full DAQUAR dataset in order to study different variants and training conditions. Afterwards we evaluate on the reduced DAQUAR for additional points of comparison to prior work.\nResults on full DAQUAR. Table 1 shows the results of our Ask Your Neurons method on the full set (\u201cmultiple words\u201d) with 653 images and 5673 question-answer pairs available at test time. In addition, we evaluate a variant that is trained to predict only a single word (\u201csingle word\u201d) as well as a variant that does not use visual features (\u201cQuestion-only\u201d). Note, however, that \u201csingle word\u201d refers to a training procedure. All the methods in Table 1 are evaluated on the full DAQUAR dataset at test time that also contains multi-word answers. In comparison to the prior work [Malinowski and Fritz, 2014a] (shown in the first row in Table 1), we observe strong improvements of over 9% points in accuracy and over 11% in the WUPS scores (second row in Table 1 that corresponds to \u201cmultiple words\u201d). Note that, we achieve this improvement despite the fact that the only published number available for the comparison on the full set uses ground truth object annotations [Malinowski and Fritz, 2014a] \u2013 which puts our method at a disadvantage. Further improvements are observed when we train only on a single word answer, which doubles the accuracy obtained in prior work. We attribute this to a joint training of the language and visual representations and the dataset bias, where about 90% of the answers contain only a single word.\nWe further analyze this effect in Figure 7, where we show performance of our approach (\u201cmultiple words\u201d) in dependence on the number of words in the answer (truncated at 4 words due to the diminishing performance). The performance of the \u201csingle word\u201d variants on the one-word subset are shown as horizontal lines. Although accuracy drops rapidly for longer answers, our model is capable of producing a significant number of correct two words answers. The \u201csingle word\u201d variants have an edge on the single answers and benefit from the dataset bias towards these type of answers. Quantitative results of the \u201csingle word\u201d model on the one-word answers subset of DAQUAR are shown in Table 2. While we have made substantial progress compared to prior work, there is still a 30% points margin to human accuracy and 25 in WUPS score (\u201cHuman answers\u201d in Table 1).\nLater on, in Section 7, we will show improved results on DAQUAR with a stronger visual model and a pre-trained word embedding, with ADAM [Kingma and Ba, 2014] as\nAccu- WUPS WUPS racy @0.9 @0.0\nAsk Your Neurons (ours) 21.67 27.99 65.11\nQuestion-only (ours) 19.13 25.16 61.51\nTable 2 Results of the single word model on the one-word answers subset of DAQUAR, all classes, single reference, in %.\n1 2 3 4\n0\n10\n20\n30\nWords number\nA cc\nur ac\ny\n1 2 3 4\n0\n10\n20\n30\nWords number\nW U\nPS 0.\n9\nFig. 7 Question-only (blue bar) and Ask Your Neurons (red bar) \u201cmulti word\u201d models evaluated on different subsets of DAQUAR. We consider 1, 2, 3, 4 word subsets. The blue and red horizontal lines represent \u201csingle word\u201d variants evaluated on the answers with exactly 1 word.\nthe chosen optimization technique. We also put the method in a broader context, and compare with other approaches.\nResults on reduced DAQUAR. In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time.\nTable 3 shows that Ask Your Neurons also improves on the reduced DAQUAR set, achieving 34.68% Accuracy and 40.76% WUPS at 0.9 substantially outperforming Malinowski and Fritz [2014a] by 21.95 percent points of Accuracy and 22.6 WUPS points. Similarly to previous experiments, we achieve the best performance using the \u201csingle word\u201d variant of our method. Note that Ren et al. [2015] reported 36.94%, 48.15%, and 82.68% Accuracy, WUPS at 0.9, and WUPS at 0.0 respectively. However, they use another variant of our reduced DAQUAR dataset, where all the multiple word answers are removed. This roughly accounts for 98% of the original reduced DAQUAR dataset.\n4.2 Answering questions without looking at images\nIn order to study how much information is already contained in questions, we train a version of our model that ignores the visual input. The results are shown in Table 1 and Table 3 under \u201cQuestion-only (ours)\u201d. The best \u201cQuestion-only\u201d models with 17.15% and 32.32% compare very well in terms of accuracy to the best models that include vision. The latter\nAccu- WUPS WUPS racy @0.9 @0.0\nMalinowski and Fritz [2014a] 12.73 18.10 51.47\nAsk Your Neurons (ours) - multiple words 29.27 36.50 79.47 - single word 34.68 40.76 79.54\nQuestion-only (ours) - multiple words 32.32 38.39 80.05 - single word 31.65 38.35 80.08\nTable 3 Results on reduced DAQUAR, single reference, with a reduced set of 37 object classes and 25 test images with 297 questionanswer pairs, in %\nachieve 19.43% and 34.68% on the full and reduced set respectively.\nIn order to further analyze this finding, we have collected a new human baseline \u201cHuman answer, no image\u201d, where we have asked participants to answer on the DAQUAR questions without looking at the images. It turns out that humans can guess the correct answer in 7.86% of the cases by exploiting prior knowledge and common sense. Interestingly, our best \u201cQuestion-only\u201d model outperforms the human baseline by over 9 percent points. A substantial number of answers are plausible and resemble a form of common sense knowledge employed by humans to infer answers without having seen the image.\n4.3 Human Consensus\nWe observe that in many cases there is an inter human agreement in the answers for a given image and question and this is also reflected by the human baseline performance on the question answering task of 50.20% (\u201cHuman answers\u201d in Table 1). We study and analyze this effect further by extending our dataset to multiple human reference answers in Section 4.3.1, and proposing a new measure \u2013 inspired by the work in psychology [Cohen et al., 1960, Fleiss and Cohen, 1973, Nakashole et al., 2013] \u2013 that handles agreement in Section 4.3.2, as well as conducting additional experiments in Section 4.3.3."}, {"heading": "4.3.1 DAQUAR-Consensus", "text": "In order to study the effects of consensus in the question answering task, we have asked multiple participants to answer the same question of the DAQUAR dataset given the respective image. We follow the same scheme as in the original data collection effort, where the answer is a set of words or numbers. We do not impose any further restrictions on the answers. This extends the original data [Malinowski and Fritz, 2014a] to an average of 5 test answers per image and\n0 50 100\n0\n50\n100\nHuman agreement\nFr ac\ntio n\nof da\nta\n0 50 100\n0\n50\n100\nHuman agreement\nFr ac\ntio n\nof da\nta\nFig. 8 Study of inter human agreement. At x-axis: no consensus (0%), at least half consensus (50%), full consensus (100%). Results in %. Left: consensus on the whole data, right: consensus on the test data.\nquestion collected from 5 in-house annotators. The annotators were first tested for their English proficiency so they would be able to accomplish the task. They were instructed verbally and were given the image and entered an answer for a given question in a text editor. Regular quality checks were performed with a random set of question-answer-image triplets. We refer to this dataset as DAQUAR-Consensus."}, {"heading": "4.3.2 Consensus Measures", "text": "While we have to acknowledge inherent ambiguities in our task, we seek a metric that prefers an answer that is commonly seen as preferred. We make two proposals:\nAverage Consensus. We use our new annotation set that contains multiple answers per question in order to compute an expected score in the evaluation:\n1\nNK N\u2211 i=1 K\u2211 k=1 min{ \u220f a\u2208Ai max t\u2208T ik \u00b5(a, t), \u220f t\u2208T ik max a\u2208Ai \u00b5(a, t)}\n(17)\nwhere for the i-th questionAi is the answer generated by the architecture and T ik is the k-th possible human answer corresponding to the k-th interpretation of the question. Both answersAi and T ik are sets of the words, and \u00b5 is a membership measure, for instance WUP [Wu and Palmer, 1994]. We call this metric \u201cAverage Consensus Metric (ACM)\u201d since, in the limits, as K approaches the total number of humans, we truly measure the inter human agreement of every question.\nMin Consensus. The Average Consensus Metric puts more weights on more \u201cmainstream\u201d answers due to the summation over possible answers given by humans. In order to measure if the result was at least with one human in agreement, we propose a \u201cMin Consensus Metric (MCM)\u201d by replacing the averaging in Equation 17 with a max operator. We call such metric Min Consensus and suggest using both\nAccu- WUPS WUPS racy @0.9 @0.0\nSubset: No agreement Question-only (ours) - multiple words 8.86 12.46 38.89 - single word 8.50 12.05 40.94\nAsk Your Neurons (ours) - multiple words 10.31 13.39 40.05 - single word 9.13 13.06 43.48 Subset: \u2265 50% agreement Question-only (ours) - multiple words 21.17 27.43 66.68 - single word 20.73 27.38 67.69\nAsk Your Neurons (ours) - multiple words 20.45 27.71 67.30 - single word 24.10 30.94 71.95 Subset: Full Agreement Question-only (ours) - multiple words 27.86 35.26 78.83 - single word 25.26 32.89 79.08\nAsk Your Neurons (ours) - multiple words 22.85 33.29 78.56 - single word 29.62 37.71 82.31\nTable 4 Results on DAQUAR, all classes, single reference in % (the subsets are chosen based on DAQUAR-Consensus).\nmetrics in the benchmarks. We will make the implementation of both metrics publicly available.\n1\nN N\u2211 i=1 K max k=1 min{\u220f a\u2208Ai max t\u2208T ik \u00b5(a, t), \u220f t\u2208T ik max a\u2208Ai \u00b5(a, t)}  (18)\nIntuitively, the max operator uses in evaluation a human answer that is the closest to the predicted one \u2013 which represents a minimal form of consensus."}, {"heading": "4.3.3 Consensus results", "text": "Using the multiple reference answers in DAQUAR-Consensus we can show a more detailed analysis of inter human agreement. Figure 8 shows the fraction of the data where the answers agree between all available questions (\u201c100\u201d), at least 50% of the available questions and do not agree at all (no agreement - \u201c0\u201d). We observe that for the majority of the data, there is a partial agreement, but even full disagreement is possible. We split the dataset into three parts according to the above criteria \u201cNo agreement\u201d, \u201c\u2265 50% agreement\u201d and \u201cFull agreement\u201d and evaluate our models on these splits (Table 4 summarizes the results). On subsets with stronger agreement, we achieve substantial gains of up to 10% and 20% points in accuracy over the full set (Table 1) and the Subset: No agreement (Table 4), respectively. These splits can be seen as curated versions of DAQUAR, which allows studies with factored out ambiguities.\nAccu- WUPS WUPS racy @0.9 @0.0\nAverage Consensus Metric Question-only (ours) - multiple words 11.60 18.24 52.68 - single word 11.57 18.97 54.39\nAsk Your Neurons (ours) - multiple words 11.31 18.62 53.21 - single word 13.51 21.36 58.03 Min Consensus Metric Question-only (ours) - multiple words 22.14 29.43 66.88 - single word 22.56 30.93 69.82\nAsk Your Neurons (ours) - multiple words 22.74 30.54 68.17 - single word 26.53 34.87 74.51\nThe aforementioned \u201cAverage Consensus Metric\u201d generalizes the notion of the agreement, and encourages predictions of the most agreeable answers. On the other hand \u201cMin Consensus Metric\u201d has a desired effect of providing a more optimistic evaluation. Table 5 shows the application of both measures to our data and models.\nMoreover, Table 6 shows that \u201cMCM\u201d applied to human answers at test time captures ambiguities in interpreting questions by improving the score of the human baseline from Malinowski and Fritz [2014a] (here, as opposed to Table 5, we exclude the original human answers from the measure). It cooperates well with WUPS at 0.9, which takes word ambiguities into account, gaining an 18% higher score.\n4.4 Qualitative results\nWe show predicted answers of different architecture variants in Tables 19, 20, and 21. We chose the examples to highlight differences between Ask Your Neurons and the \u201cQuestiononly\u201d. We use a \u201cmultiple words\u201d approach only in Table 20, otherwise the \u201csingle word\u201d model is shown. Despite some failure cases, \u201cQuestion-only\u201d makes \u201creasonable guesses\u201d like predicting that the largest object could be table or an object that could be found on the bed is a pillow or doll.\nFig. 9 Figure showing correlation between question and answer words of the \u201cQuestion-only\u201d model (at x-axis), and a similar correlation of the \u201cHuman-baseline\u201d [Malinowski and Fritz, 2014a] (at y-axis).\n4.5 Failure cases\nWhile our method answers correctly on a large part of the challenge (e.g.\u2248 35 WUPS at 0.9 on \u201cwhat color\u201d and \u201chow many\u201d question subsets), spatial relations (\u2248 21 WUPS at 0.9) which account for a substantial part of DAQUAR remain challenging. Other errors involve questions with small objects, negations, and shapes (below 12 WUPS at 0.9). Too few training data points for the aforementioned cases may contribute to these mistakes. Table 21 shows examples of failure cases that include (in order) strong occlusion, a possible answer not captured by our ground truth answers, and unusual instances (red toaster).\n4.6 Common Sense Knowledge\nAlthough \u201cQuestion-only\u201d ignores the image, it is still able to make \u201creasonable guesses\u201d by exploiting biases captured by the dataset. Some of such biases we interpret as a type of a common sense knowledge. For instance, \u201ctea kettle\u201d often sits on the oven, cabinets are usually \u201cbrown\u201d, \u201cchair\u201d is typically placed in front of a table, and we commonly keep a \u201cphoto\u201d on a cabinet (Table 22, 24, 25, 28). On the other hand, some other biases are hardly related to the common sense knowledge. For instance, \u201c11\u201d as the answer to the question \u201cHow many bottles are on the desk?\u201d or a \u201cclock\u201d as the answer to the question \u201cwhat is the black and white object on the top right of the brown board?\u201d. This effect is analysed in Figure 9. Each data point in the plot represents the correlation between a question and a predicted answer words for our \u201cQuestion-only\u201d model (x-axis) versus the correlation in the human answers (y-axis). Despite the reasonable guesses of the \u201cQuestion-only\u201d architecture, the \u201cAsk Your Neurons\u201d predicts in average better answers (shown in Table 1). For instance in Table 26 the \u201cQuestiononly\u201d model incorrectly answers \u201c6\u201d on the question \u201cHow\nmany burner knobs are there ?\u201d because it has seen only this answer during the training with exactly the same question but on different image."}, {"heading": "5 Extended Experiments on DAQUAR", "text": "In this section, we first extend our experiments with other baseline methods, and next, guided by our findings on VQA shown in Section 7, we show the results with the refined model in the context of a larger body of results on DAQUAR.\nBaseline methods. To gain a better understanding of the effectiveness of our neural-based approach, we relate the obtained results to other baseline techniques. Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR. The Constant technique uses the most frequent answer in the training set to answer to every question in the test set. In Constant, per question type, we first break all questions into a few categories and then use the most frequent answer per category to answer to every question in that category at the test time. Table 7 provides more details on the chosen categories. Look-up table builds a hash map from a textual question into the most frequent answer for that question at the training time. At the test time, the method just looks up the answer for the question in the hashmap. If the question exists then the most popular answer for that question is provided, otherwise an \u2018empty\u2019 answer is given. In addition, we also remove articles, such as \u2018the\u2019 and \u2018a\u2019, from all the questions. However, this brings only a minor improvement. Finally, we experiment with two nearestneighbor methods. Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014]. In the following, we call this the representation the semantic space. Nearest Neighbor, Question-only searches at test time for the most similar question in the semantic space from the training set. Then it takes the answer that corresponds to this question. Nearest Neighbor is inspired by a similar baseline introduced in Antol et al. [2015]. At the test time we first search for the 4 most similar questions in the semantic space available in the training set. Next, we form candidate images that correspond to the aforementioned 4 questions. At the last step, we choose an answer that is associated with the best match in the visual space. The latter is done by a cosine similarity between global CNN representations of the test image and every candidate image. We experiment with several CNN representations (VGG-19, GoogLeNet, ResNet152) but to our surprise there is little performance difference between them. We decide to use GoogLeNet as the results are slightly better.\nBaseline results. Constant shows how the dataset is biased w.r.t. the most frequent answer. This answer turns out to be the number \u201c2\u201d, which also explain a good performance of Ask Your Neurons on the \u201chow many\u201d question subset in DAQUAR. Constant, per question type shows that question types provide quite strong clues for answering questions. Kafle and Kanan [2016] take advantage of such clues in their Bayesian and Hybrid models (in Table 9, we only show a better performing Hybrid model). Our next baseline, Lookup table can be seen as an extreme case of the Constant, per question type model. It also gives surprisingly good results even though it cannot answer to novel questions. This result also confirms our intuitions that \u201cQuestion-only\u201d biases are important in the \u2018question answering about images\u2019 datasets. Finally, next nearest-neighbor baselines show that a visual representation still helps.\nState-of-the-Art. Based on our further analysis on VQA (for more details we refer to Section 6 and Section 7), we have also applied the improved model to DAQUAR, and we significantly outperform Malinowski et al. [2015] presented in Section 4. In the experiments, we first choose last 10% of training set as a validation set in order to determine a number of training epochs K, and next we train the model for K epochs. We evaluate model on two variants of DAQUAR: all data points (\u2018all\u2019 in Table 9), and a subset (\u2018single word\u2019 in Table 9) containing only single word answers, which consists of about 90% of the original dataset. As Table 9 shows, our model, Vision + Language with GLOVE and Residual Net that sums visual and question representations, outperforms the model of Malinowski et al. [2015] by 5.05, 4.50, 0.80 of Accuracy, WUPS at 0.9, and WUPS at 0.0 respectively. This shows how important a strong visual model is, as well as the aforementioned details used in training. Likewise to our conclusions on VQA, we are also observing an improvement with attention based models (comparison in Attention and Global sections in Table 9)."}, {"heading": "6 Analysis on VQA", "text": "While Section 4 analyses our original architecture [Malinowski et al., 2015] on the DAQUAR dataset, in this section, we analyze different variants and design choices for neural question answering on the large-scale Visual Question Answering (VQA) dataset [Antol et al., 2015]. It is currently one of the largest and most popular visual question answering dataset with human question answer pairs. In the following, after describing the experimental setup (Section 6.1), we first describe several experiments which examine the different variants of question encoding, only looking at language input to predict the answer (Section 6.1), and then, we examine the full model (Section 6.3).\n6.1 Experimental setup\nWe evaluate on the VQA dataset [Antol et al., 2015], which is built on top of the MS-COCO dataset [Lin et al., 2014]. Although VQA offers a different challenge tasks, we focus our efforts on the Real Open-Ended Visual Question Answering challenge. The challenge consists of 10 answers per question with about 248k training questions, about 122k validation questions, and about 244k test questions.\nAs VQA consist mostly of single word answers (over 89%), we treat the question answering problem as a classifi-\ncation problem of the most frequent answers in the training set. For the evaluation of the different model variants and design choices, we train on the training set and test on the validation set. Only the final evaluations (Table 17) are evaluated on the test set of the VQA challenge, we evaluate on both parts test-dev and test-standard, where the answers are not publicly available. As a performance measure we use a consensus variant of accuracy introduced in Antol et al. [2015], where the predicted answer gets score between 0 and 1, with 1 if it matches with at least three human answers. We use ADAM [Kingma and Ba, 2014] throughout our ex-\nkernel length single view multi view k = k \u2264 k\n1 47.43 47.43 2 48.11 48.06 3 48.26 48.09 4 48.27 47.86\nperiments as we found out it performs better than SGD with momentum. We keep default hyper-parameters for ADAM. Employed Recurrent Neural Networks maps input question into 500 dimensional vector representation. All the CNNs for text are using 500 feature maps in our experiments, but the output dimensionality also depends on the number of views. In preliminary experiments, we found that removing question mark \u2018?\u2019 in the questions slightly improves the results, and we report the numbers only with this setting. Since VQA has 10 answers associated with each question, we need to consider a suitable training strategy that takes this into account. We have examined the following strategies: (1) picking an answer randomly, (2) randomly but if possible annotated as confidently answered, (3) all answers, or (4) choosing the most frequent answer. In the following, we only report the results using the last strategy as we have found out little difference in accuracy between the strategies. To allow training and evaluating many different models with limited time and computational power, we do not fine-tune the visual representations in these experiments, although our model would allow us to do so. All the models, which are publicly available together with our tutorial [Malinowski and Fritz, 2016] 2, are implemented in Keras [Chollet, 2015] and Theano [Bastien et al., 2012].\n6.2 Question-only\nWe start our analysis from \u201cQuestion-only\u201d models that do not use images to answer on questions. Note that the \u201cQuestiononly\u201d baselines play an important role in the question answering about images tasks since how much performance is\n2 https://github.com/mateuszmalinowski/ visual_turing_test-tutorial\ntop frequent answers Encoder 1000 2000 3000\nBOW 47.91 48.13 47.94 CNN 48.53 48.67 48.57 LSTM 48.58 48.86 48.65\nTable 12 Results on VQA validation set, \u201cQuestion-only\u201d model: Analysis of the number of top frequent answer classes, with different question encoders. All using GLOVE; accuracy in %; see Section 6.2.4 for discussion.\nadded by vision. Hence, better overall performance of the model is not obscured by a better language model. To understand better different design choices, we have conducted our analysis along the different \u2018design\u2019 dimensions."}, {"heading": "6.2.1 CNN questions encoder", "text": "We first examine different hyper-parameters for CNNs to encode the question. We first consider the filter\u2019s length of the convolutional kernel. We run the model over different kernel lengths ranging from 1 to 4 (Table 10, left column). We notice that increasing the kernel lengths improves performance up to length 3 were the performance levels out, we thus use kernel length 3 in the following experiments for, such CNN can be interpreted as a trigram model. We also tried to run simultaneously a few kernels with different lengths. In Table 10 (right column) one view corresponds to a kernel length 1, two views correspond to two kernels with length 1 and 2, three views correspond to length 1, 2 and 3, etc. However, we find that the best performance still achieve with a single view and kernel length 3 or 4."}, {"heading": "6.2.2 BOW questions encoder", "text": "Alternatively to neural network encoders, we consider BagOf-Words (BOW) approach where one-hot representations of the question words are first mapped to a shared embedding space, and subsequently summed over (Equation 13), i.e. \u03a8(question) := \u2211 word We(word). Surprisingly, such a simple approach gives very competitive results (first row in Table 11) compared to the CNN encoding discussed in the previous section (second row).\nRecurrent questions encoder. We examine two recurrent questions encoders, LSTM [Hochreiter and Schmidhuber, 1997] and a simpler GRU [Cho et al., 2014]. The last two rows of Table 11 show a slight advantage of using LSTM."}, {"heading": "6.2.3 Pre-trained words embedding", "text": "In all the previous experiments, we jointly learn the embedding transformation W e together with the whole architecture only on the VQA dataset. This means we do not have\nno norm L2 norm\nConcatenation 47.21 52.39 Summation 40.67 53.27 Element-wise multiplication 49.50 52.70\nany means for dealing with unknown words in questions at test time apart from using a special token \u3008UNK\u3009 to indicate such class. To address such shortcoming, we investigate the pre-trained word embedding transformation GLOVE [Pennington et al., 2014] that encodes question words (technically it maps one-hot vector into a 300 dimensional real vector). This choice naturally extends the vocabulary of the question words to about 2 million words extracted a large corpus of web data \u2013 Common Crawl [Pennington et al., 2014] \u2013 that is used to train the GLOVE embedding. Since the BOW architecture in this scenario becomes shallow (only classification weights are learnt), we add an extra hidden layer between pooling and classification (without this embedding, accuracy drops by 5%). Table 11 (right column) summarizes our experiments with GLOVE. For all question encoders, the word embedding consistently improves performance which confirms that using a word embedding model learnt from a larger corpus helps. LSTM benefits most from GLOVE embedding, archiving the overall best performance with 48.58% accuracy."}, {"heading": "6.2.4 Top most frequent answers", "text": "Our experiments reported in Table 12 investigate predictions using different number of answer classes. We experiment with a truncation of 1000, 2000, or 4000 most frequent classes. For all question encoders (and always using GLOVE word embedding), we find that a truncation at 2000 words is best, being apparently a good compromise between answer frequency and missing recall."}, {"heading": "6.2.5 Summary Question-only", "text": "We achieve the best \u201cQuestion-only\u201d accuracy with GLOVE word embedding, LSTM sentence encoding, and using the\nMethod Accuracy\nAlexNet 53.69 GoogLeNet 54.52 VGG-19 54.29 ResNet-152 55.52\ntop 2000 most frequent answers. This achieves an performance of 48.86% accuracy. In the remaining experiments, we use these settings for language and answer encoding.\n6.3 Vision and Language\nAlthough Question-only models can answer on a substantial number of questions as they arguably capture common sense knowledge, in order to address the full problem we will now also observing the image the question is based on."}, {"heading": "6.3.1 Multimodal fusion", "text": "Table 13 investigates different techniques that combine visual and language representations. To speed up training, we combine the last unit of the question encoder with the visual encoder, as it is explicitly shown in Figure 4. In the experiments we use concatenation, summation, and element-wise multiplication on the BOW language encoder with GLOVE word embedding and features extracted from the VGG-19 net. In addition, we also investigate using L2 normalization of the visual features, which divides every feature vector by its L2 norm. The experiments show that the normalization is crucial in obtaining good performance, especially for Concatenation and Summation. In the remaining experiments, we use Summation."}, {"heading": "6.3.2 Questions encoders", "text": "Table 14 shows how well different questions encoders combine with the visual features. We can see that LSTM slightly\noutperforms two other encoders GRU and CNN, while BOW remains the worst, confirming our findings in our languageonly experiments with GLOVE and 2000 answers (Table 12, second column)."}, {"heading": "6.3.3 Visual encoders", "text": "Next we fix the question encoder to LSTM and vary different visual encoders: Caffe variant of AlexNet [Krizhevsky et al., 2012], GoogLeNet [Szegedy et al., 2014], VGG-19 [Simonyan and Zisserman, 2014], and recently introduced 152 layered ResNet (we use the Facebook implementation of He et al. [2015]). Table 15 confirms our hypothesis that stronger visual models perform better."}, {"heading": "6.3.4 Qualitative results", "text": "We show predicted answers using our best model on VQA test set in Tables 30, 31 ,32, 33. We show chosen examples with \u2018yes/no\u2019, \u2018counting\u2019, and \u2018what\u2019 questions, where our model, according to our opinion, makes valid predictions. Moreover, Table 33 shows predicted compound answers.\n6.4 Summary VQA results\nTable 16 summarises our findings on the validation set. We can see that on one hand methods that use contextual language information such as CNN and LSTM are performing better, on the other hand adding strong vision becomes crucial. Furthermore, we use the best found models to run experiments on the VQA test sets: test-dev2015 and teststandard. To prevent overfitting, the latter restricts the number of submissions to 1 per day and 5 submissions in total. Here, we also study the effect of larger datasets where first we train only on the training set, and next we train for 20 epochs on a joint, training and validation, set. When we train on the join set, we consider question answer pairs with answers among 2000 the most frequent answer classes from the training and validation sets. Training on the joint set have gained us about 0.9%. This implies that on one hand having more data indeed helps, but arguably we also need better models that exploit the current training datasets more effectively. Our findings are summarized in Table 17."}, {"heading": "7 State-of-the-art on VQA", "text": "In this section, we first put our findings on VQA in a broader context, where we compare our refined version of Ask Your Neurons with other, publicly available, approaches. Table 18 compares our Refined Ask Your Neurons model with other approaches. Some methods, likewise to our approach, use global image representation, other attention mechanism, yet other dynamically predict question dependent weights, external textual sources, or fuse compositional question\u2019s representation with neural networks. Table 18 shows a few trends. First of all, a better visual representation significantly helps (Table 15). Most of the leading approaches to VQA also uses variants of ResNet, which is among the strongest approaches to the image classification task. It is, however, important to normalize the visual features (Table 13). Additionally, all the best models use an explicit attention mechanism (e.g. DMN+, FDA, SAN, POSTECH, MCB, HieCoAtt)\nIn this work, however, we focus on the extension of the plain \u201cAsk Your Neurons\u201d model that uses a global, fullframe image representation. A similar representation is used in Refined Ask Your Neurons, iBOWIMG, VQA team, and LSTM Q+I. The best performing approaches also use different variants of the Recurrent Neural Networks (LSTM and GRU are the most popular). Such a question encoding outperforms Bag-of-Words representations (iBOWIMG in Table 18, and BOW in Table 14). As we hypothesize, a multimodal embedding plays an important role. This is not only shown in Table 13, but also emphasized in two leading approaches to VQA (MCB and SNUBI). Both methods use novel multimodal embedding techniques that build upon the element-wise multiplication. Finally, using external textual resources also seems to be beneficial (AMA)."}, {"heading": "8 Conclusions", "text": "We have presented a neural architecture for answering natural language questions about images that contrasts with prior efforts based on semantic parsing and outperforms a prior symbolic based approach by doubling performance on this challenging task. A variant of our model, that does not use the image to answer the question, already explains a substantial part of the overall performance and helps to understand the contribution of the visual features in this task. From a comparison with the human baseline where humans are not\nshown the image to answer the question, we conclude that this language-only model has learnt biases and patterns that can be seen as forms of common sense and prior knowledge that are also used by humans to accomplish this task. We have extended our existing DAQUAR dataset to the new DAQUAR-Consensus, which now provides multiple reference answers which allows to study inter-human agreement and consensus on the question answering task. We propose two new metrics: \u201cAverage Consensus\u201d, which takes into account human disagreement, and \u201cMin Consensus\u201d that captures disagreement in human question answering. Finally, we extend our analysis to the large-scale VQA dataset showing competitive performance, yet still using global visual model, and training the model solely on the provided question answer image triples. A broader analysis of the different Deep Learning components and design choices in our model has led to improved results and highlights the importance of a strong visual model.\nAcknowledgements Marcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the German Academic Exchange Service (DAAD). The project was in part supported by the Collaborative Research Center (CRC) 1223 from the German Research Foundation (DFG)."}], "references": [{"title": "Multi-cue zero-shot learning with strong supervision", "author": ["Zeynep Akata", "Mateusz Malinowski", "Mario Fritz", "Bernt Schiele"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Akata et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Akata et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Berant and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Xplore-m-ego: Contextual media retrieval using natural language queries", "author": ["Sreyasi Nag Chowdhury", "Mateusz Malinowski", "Andreas Bulling", "Mario Fritz"], "venue": "In ACM International Conference on Multimedia Retrieval (ICMR),", "citeRegEx": "Chowdhury et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chowdhury et al\\.", "year": 2016}, {"title": "A coefficient of agreement for nominal scales", "author": ["Jacob Cohen"], "venue": "Educational and psychological measurement,", "citeRegEx": "Cohen,? \\Q1960\\E", "shortCiteRegEx": "Cohen", "year": 1960}, {"title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability", "author": ["Joseph L Fleiss", "Jacob Cohen"], "venue": "Educational and psychological measurement,", "citeRegEx": "Fleiss and Cohen.,? \\Q1973\\E", "shortCiteRegEx": "Fleiss and Cohen.", "year": 1973}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "In Proceedings of the National Academy of Sciences. National Academy of Sciences,", "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Segmentation from natural language expressions", "author": ["Ronghang Hu", "Marcus Rohrbach", "Trevor Darrell"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Natural language object retrieval", "author": ["Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "venue": "arXiv:1604.01485, 2016", "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Compositional memory for visual question answering", "author": ["Aiwen Jiang", "Fang Wang", "Fatih Porikli", "Yi Li"], "venue": null, "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Referit game: Referring to objects in photographs of natural scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L. Berg"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kazemzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Hadamard product for lowrank bilinear pooling", "author": ["Jin-Hwa Kim", "Kyoung Woon On", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "arXiv preprint arXiv:1610.04325,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "What are you talking about? text-to-image coreference", "author": ["Chen Kong", "Dahua Lin", "Mohit Bansal", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Jayant Krishnamurthy", "Thomas Kollar"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Krishnamurthy and Kollar.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy and Kollar.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Malinowski and Fritz.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Learning Semantics (NIPS workshop),", "citeRegEx": "Malinowski and Fritz.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski and Fritz.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Hard to cheat: A turing test based on answering questions about images", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "AAAI Workshop: Beyond the Turing Test,", "citeRegEx": "Malinowski and Fritz.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2015}, {"title": "Tutorial on answering questions about images with deep learning", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1610.01076,", "citeRegEx": "Malinowski and Fritz.,? \\Q2016\\E", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Foundations of statistical natural language processing, volume 999", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze.,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Cynthia Matuszek", "Nicholas Fitzgerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Fine-grained semantic typing of emerging entities", "author": ["Ndapandula Nakashole", "Tomasz Tylenda", "Gerhard Weikum"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Nakashole et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2013}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": null, "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["Bryan Plummer", "Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2016}, {"title": "Highway networks for visual question answering", "author": ["Aaditya Prakash", "James Storer"], "venue": null, "citeRegEx": "Prakash and Storer.,? \\Q2016\\E", "shortCiteRegEx": "Prakash and Storer.", "year": 2016}, {"title": "Grounding Action Descriptions in Videos", "author": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "Regneri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Regneri et al\\.", "year": 2013}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "A dataset for movie description", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Berg and Fei.Fei.,? \\Q2014\\E", "shortCiteRegEx": "Berg and Fei.Fei.", "year": 2014}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Kuniaki Saito", "Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "venue": "arXiv preprint arXiv:1606.06108,", "citeRegEx": "Saito et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saito et al\\.", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Shih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shih et al\\.", "year": 2016}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["Nathan Silberman", "Derek Hoiem", "Pushmeet Kohli", "Rob Fergus"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Movieqa: Understanding stories in movies through questionanswering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Sequence to sequence \u2013 video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["Liwei Wang", "Yin Li", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Verbs semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wu and Palmer.,? \\Q1994\\E", "shortCiteRegEx": "Wu and Palmer.", "year": 1994}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": null, "citeRegEx": "Xu and Saenko.,? \\Q2015\\E", "shortCiteRegEx": "Xu and Saenko.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C Berg", "Tamara L Berg"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Modeling context in referring expressions", "author": ["Licheng Yu", "Patrick Poirson", "Shan Yang", "Alexander C Berg", "Tamara L Berg"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Uncovering temporal context for video question and answering", "author": ["Linchao Zhu", "Zhongwen Xu", "Yi Yang", "Alexander G Hauptmann"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "Learning the visual interpretation of sentences", "author": ["C Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 30, "context": "Most prominently Convolutional Neural Networks have raised the bar on image classification tasks [Krizhevsky et al., 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al.", "startOffset": 97, "endOffset": 122}, {"referenceID": 13, "context": ", 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al.", "startOffset": 44, "endOffset": 78}, {"referenceID": 59, "context": ", 2012] and Long Short Term Memory Networks [Hochreiter and Schmidhuber, 1997] are dominating performance on a range of sequence prediction tasks such as machine translation [Sutskever et al., 2014].", "startOffset": 174, "endOffset": 198}, {"referenceID": 21, "context": "Most recently, these two trends of employing neural architectures have been combined fruitfully with methods that can generate image [Karpathy and Fei-Fei, 2015] and video descriptions [Venugopalan et al.", "startOffset": 133, "endOffset": 161}, {"referenceID": 42, "context": "We also frame the multimodal approach to answer questions about images that combines LSTM with CNN [Malinowski et al., 2015] as a special instance of an encoder-decoder framework.", "startOffset": 99, "endOffset": 124}, {"referenceID": 42, "context": "In Section 3, we present our novel approach based on recurrent neural networks for the challenging task of answering questions about images, which we presented originally in Malinowski et al. [2015]. The approach combines a CNN with an LSTM into an end-to-end architecture that predicts answers conditioning on a question and an image.", "startOffset": 174, "endOffset": 199}, {"referenceID": 30, "context": "Since the proposal of AlexNet [Krizhevsky et al., 2012], Convolutional Neural Networks (CNNs) have become dominant and most successful approaches to extract relevant representation from the image.", "startOffset": 30, "endOffset": 55}, {"referenceID": 30, "context": "Specifically, we evaluate AlexNet [Krizhevsky et al., 2012], VGG [Simonyan and Zisserman, 2014], GoogleNet [Szegedy et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 58, "context": ", 2012], VGG [Simonyan and Zisserman, 2014], GoogleNet [Szegedy et al.", "startOffset": 13, "endOffset": 43}, {"referenceID": 12, "context": ", 2014], and ResNet [He et al., 2015].", "startOffset": 20, "endOffset": 37}, {"referenceID": 43, "context": "The first approach is to encode all words of the question as a Bag-Of-Words [Manning and Sch\u00fctze, 1999], and hence ignoring an order in the sequence of words.", "startOffset": 76, "endOffset": 103}, {"referenceID": 13, "context": "In this work we investigate a Bag-Of-Words (BOW), a CNN, and two RNN variants (LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al.", "startOffset": 84, "endOffset": 118}, {"referenceID": 5, "context": "Answering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task.", "startOffset": 77, "endOffset": 477}, {"referenceID": 5, "context": "Answering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task. Weston et al. [2014] propose different kind of network - memory networks - that is used to answer questions about short stories.", "startOffset": 77, "endOffset": 616}, {"referenceID": 5, "context": "Answering on purely textual questions has been studied in the NLP community [Berant and Liang, 2014, Liang et al., 2013] and state of the art techniques typically employ semantic parsing to arrive at a logical form capturing the intended meaning and infer relevant answers. Only recently, the success of the previously mentioned neural sequence models, namely RNNs, has carried over to this task [Iyyer et al., 2014, Weston et al., 2014]. More specifically Iyyer et al. [2014] use dependency-tree Recursive NN instead of LSTM, and reduce the question-answering problem to a classification task. Weston et al. [2014] propose different kind of network - memory networks - that is used to answer questions about short stories. In their work, all the parts of the story are embedded into different \u201cmemory cells\u201d, and next a network is trained to attend to relevant cells based on the question and decode an answer from that. A similar idea has also been applied to question answering about images, for instance by Yang et al. [2015].", "startOffset": 77, "endOffset": 1030}, {"referenceID": 36, "context": ", 2015] or a CNN [Ma et al., 2016].", "startOffset": 17, "endOffset": 34}, {"referenceID": 62, "context": "Following Xu et al. [2015], who proposed to use spatial attention for image description, Yang et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 62, "context": "Following Xu et al. [2015], who proposed to use spatial attention for image description, Yang et al. [2015], Xu and Saenko [2015], Zhu et al.", "startOffset": 10, "endOffset": 108}, {"referenceID": 62, "context": "[2015], Xu and Saenko [2015], Zhu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 62, "context": "[2015], Xu and Saenko [2015], Zhu et al. [2016], Chen et al.", "startOffset": 8, "endOffset": 48}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al. [2016], Fukui et al.", "startOffset": 8, "endOffset": 47}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al. [2016], Fukui et al. [2016] predict a latent weighting (attention) of spatially localized images features (typically a convolutional layer of the CNN) based on the question.", "startOffset": 8, "endOffset": 68}, {"referenceID": 6, "context": "[2016], Chen et al. [2015], Shih et al. [2016], Fukui et al. [2016] predict a latent weighting (attention) of spatially localized images features (typically a convolutional layer of the CNN) based on the question. The weighted image representation rather than the full frame feature representation is then used as a basis for answering the question. In contrast to the previous models using attention, Dynamic Memory Networks (DMN) [Kumar et al., 2015, Xiong et al., 2016] first pass all spatial image features through a bi-directional GRU that captures spatial information from the neighboring image patches, and next retrieve an answer from a recurrent attention based neural network that allows to focus only on a subset of the visual features extracted in the first pass. Another interesting direction has been taken by Ilievski et al. [2016] who run state-of-the-art object detector of the classes extracted from the key words in the question.", "startOffset": 8, "endOffset": 847}, {"referenceID": 64, "context": "Wu et al. [2016] argue for an approach that first represents an image as an intermediate semantic attribute representation, and next query external knowledge sources based on the most prominent attributes and relate them to the question.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences.", "startOffset": 34, "endOffset": 425}, {"referenceID": 1, "context": "A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences. Related to the Visual Turing Test, Malinowski and Fritz [2014c] have also combined a neural based representation with the compositionality of the language for the text-to-image retrieval task.", "startOffset": 34, "endOffset": 582}, {"referenceID": 1, "context": "A different direction is taken by Andreas et al. [2016b,a] who predict the most important components to answer the question with a natural language parser. The components are then mapped to neural modules, which are composed to a deep neural network based on the parse tree. While each question induces a different network, the modules are trained jointly across questions. This work compares to Malinowski and Fritz [2014a] by exploiting explicit assumptions about the compositionality of natural language sentences. Related to the Visual Turing Test, Malinowski and Fritz [2014c] have also combined a neural based representation with the compositionality of the language for the text-to-image retrieval task. Dynamic parameters. Noh et al. [2015] have an image recognition network and a Recurrent Neural Network (GRU) that dynamically change the parameters (weights) of visual representation based on the question.", "startOffset": 34, "endOffset": 749}, {"referenceID": 57, "context": "5 thousands manually annotated question-answer pairs about 1449 indoor scenes [Silberman et al., 2012].", "startOffset": 78, "endOffset": 102}, {"referenceID": 34, "context": "All are based on MS-COCO [Lin et al., 2014].", "startOffset": 25, "endOffset": 43}, {"referenceID": 27, "context": "[2015] have taken advantage of the existing annotations for the purpose of image description generation task and transform them into question answer pairs with the help of a set of hand-designed rules and a syntactic parser [Klein and Manning, 2003].", "startOffset": 224, "endOffset": 249}, {"referenceID": 3, "context": "Finally and currently the most popular, large scale dataset on question answering about images is VQA [Antol et al., 2015].", "startOffset": 102, "endOffset": 122}, {"referenceID": 9, "context": "In parallel to our work, Geman et al. [2015] developed another variant of the Visual Turing Test.", "startOffset": 25, "endOffset": 45}, {"referenceID": 9, "context": "Gao et al. [2015] have annotated about 158k images with 316k Chinese question answer pairs together with their corresponding English translations.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Gao et al. [2015] have annotated about 158k images with 316k Chinese question answer pairs together with their corresponding English translations. Ren et al. [2015] have taken advantage of the existing annotations for the purpose of image description generation task and transform them into question answer pairs with the help of a set of hand-designed rules and a syntactic parser [Klein and Manning, 2003].", "startOffset": 0, "endOffset": 165}, {"referenceID": 75, "context": "The Visual7W [Zhu et al., 2016] extends canonical question and answer pairs with additional groundings of all objects appearing in the questions and answers to the image by annotating the correspondences.", "startOffset": 13, "endOffset": 31}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer.", "startOffset": 34, "endOffset": 54}, {"referenceID": 70, "context": "Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers.", "startOffset": 28, "endOffset": 45}, {"referenceID": 50, "context": "The task requires inferring the past, describing the present and predicting the future in a diverse set of video description data ranging from cooking videos [Regneri et al., 2013] over web videos [Trecvid, 2014] to movies [Rohrbach et al.", "startOffset": 158, "endOffset": 180}, {"referenceID": 35, "context": "ing datasets [Malinowski and Fritz, 2014a,b, 2015], Yu et al. [2015] have simplified the evaluation even further by introducing Visual Madlibs - a multiple choice question answering by filling the blanks task.", "startOffset": 14, "endOffset": 69}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers. Related to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system.", "startOffset": 35, "endOffset": 527}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers. Related to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system. This work focuses on a user centric, dynamic scenario, where the provided answers are conditioned not only on questions but also on the geographical position of the questioner. Moving from asking questions about images to questions about video enhances typical questions with temporal structure. Zhu et al. [2015] propose a task which requires to fill in blanks the captions associated with videos.", "startOffset": 35, "endOffset": 975}, {"referenceID": 3, "context": "In contrast to others such as VQA [Antol et al., 2015] or DAQUAR [Malinowski and Fritz, 2014a] that has collected unconstrained question answer pairs, the Visual Genome focuses on the six, so called, Ws: what, where, when, who, why, and how, which can be answered with a natural language answer. An additional 7th question \u2013 which \u2013 requires a bounding box location as answer. Similarly to Visual Madlibs [Yu et al., 2015], Visual7W also contains multiple-choice answers. Related to Visual Turing Test, Chowdhury et al. [2016] have proposed collective memories and Xplore-M-Ego - a dataset of images with natural language queries, and a media retrieval system. This work focuses on a user centric, dynamic scenario, where the provided answers are conditioned not only on questions but also on the geographical position of the questioner. Moving from asking questions about images to questions about video enhances typical questions with temporal structure. Zhu et al. [2015] propose a task which requires to fill in blanks the captions associated with videos. The task requires inferring the past, describing the present and predicting the future in a diverse set of video description data ranging from cooking videos [Regneri et al., 2013] over web videos [Trecvid, 2014] to movies [Rohrbach et al., 2015b]. Tapaswi et al. [2016] propose MovieQA, which requires to understand long term connections in the plot of the movie.", "startOffset": 35, "endOffset": 1331}, {"referenceID": 42, "context": "The original version of this work [Malinowski et al., 2015] belongs to the category of \u201cDeep Neural Approaches with full frame CNN\u201d, and is among the very first methods of this kind (Section 3.", "startOffset": 34, "endOffset": 59}, {"referenceID": 42, "context": "We extend [Malinowski et al., 2015] by introducing a more general and modular encoder-decoder perspective (Section 3.", "startOffset": 10, "endOffset": 35}, {"referenceID": 3, "context": "Finally, we transfer lessons learnt from VQA [Antol et al., 2015] to DAQUAR [Malinowski and Fritz, 2014a], showing a significant improvement on this challenging task (Section 6).", "startOffset": 45, "endOffset": 65}, {"referenceID": 32, "context": "More precisely, Ask Your Neurons is a deep network built of CNN [LeCun et al., 1998] and Long-Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997].", "startOffset": 64, "endOffset": 84}, {"referenceID": 13, "context": ", 1998] and Long-Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997].", "startOffset": 42, "endOffset": 76}, {"referenceID": 13, "context": "This allows to learn long-term dynamics more easily and significantly reduces the vanishing and exploding gradients problem [Hochreiter and Schmidhuber, 1997].", "startOffset": 124, "endOffset": 158}, {"referenceID": 13, "context": "This allows to learn long-term dynamics more easily and significantly reduces the vanishing and exploding gradients problem [Hochreiter and Schmidhuber, 1997]. More precisely, we use the LSTM unit as described in Zaremba and Sutskever [2014]. With the sigmoid nonlinearity \u03c3 : R 7\u2192 [0, 1], \u03c3(v) = (1 + e\u2212v) and the hyperbolic tangent nonlinearity \u03c6 : R 7\u2192 [\u22121, 1], \u03c6(v) = ev\u2212e\u2212v ev+e\u2212v = 2\u03c3(2v) \u2212 1, the LSTM updates for time step t given inputs vt, ht\u22121, and the memory cell ct\u22121 as follows:", "startOffset": 125, "endOffset": 242}, {"referenceID": 13, "context": "In this work, we investigate a few encoders within such spectrum that are compatible with the proposed Deep Learning approach: Two recurrent question encoders, LSTM [Hochreiter and Schmidhuber, 1997] (see Section 3.", "startOffset": 165, "endOffset": 199}, {"referenceID": 35, "context": "Such an encoder can range from a very structured ones like Semantic Parser used in Malinowski and Fritz [2014a] and Liang et al.", "startOffset": 83, "endOffset": 112}, {"referenceID": 32, "context": "Such an encoder can range from a very structured ones like Semantic Parser used in Malinowski and Fritz [2014a] and Liang et al. [2013] that explicitly model compositional nature of the question, to orderless Bag-Of-Word (BOW) approaches that merely compute a histogram over the question words (Figure 6).", "startOffset": 116, "endOffset": 136}, {"referenceID": 69, "context": "1 and Yang et al. [2015] for details.", "startOffset": 6, "endOffset": 25}, {"referenceID": 47, "context": "We either learn them jointly with the whole model or use GLOVE [Pennington et al., 2014] in our experiments.", "startOffset": 63, "endOffset": 88}, {"referenceID": 20, "context": "Convolutional Neural Networks (CNN) have been proposed to encode language [Kim, 2014, Kalchbrenner et al., 2014, Ma et al., 2016, Yang et al., 2015] and since have shown to be fast to compute and result in good accuracy. Since they consider a larger context, they arguably maintain more structure than BOW, but do not model such long term dependencies as recurrent neural networks. Figure 5 depicts our CNN architecture, which is very similar to Ma et al. [2016] and Yang et al.", "startOffset": 86, "endOffset": 463}, {"referenceID": 20, "context": "Convolutional Neural Networks (CNN) have been proposed to encode language [Kim, 2014, Kalchbrenner et al., 2014, Ma et al., 2016, Yang et al., 2015] and since have shown to be fast to compute and result in good accuracy. Since they consider a larger context, they arguably maintain more structure than BOW, but do not model such long term dependencies as recurrent neural networks. Figure 5 depicts our CNN architecture, which is very similar to Ma et al. [2016] and Yang et al. [2015], that convolves word embeddings with three convolutional kernels of length 1, 2 and 3.", "startOffset": 86, "endOffset": 486}, {"referenceID": 12, "context": ", 2014, Simonyan and Zisserman, 2014] \u2013 to the most recent extremely deep architectures \u2013 a Facebook implementation of 152 layered ResidualNet [He et al., 2015].", "startOffset": 143, "endOffset": 160}, {"referenceID": 3, "context": "This approach has been widely explored, especially on VQA [Antol et al., 2015].", "startOffset": 58, "endOffset": 78}, {"referenceID": 57, "context": "We evaluate our approach from Section 3 on the DAQUAR dataset [Malinowski and Fritz, 2014a], which provides 12, 468 human question answer pairs on images of indoor scenes [Silberman et al., 2012] and follow the same evaluation protocol by providing results on accuracy and the WUPS score at {0.", "startOffset": 171, "endOffset": 195}, {"referenceID": 18, "context": ", 2015] and CNN [Jia et al., 2014].", "startOffset": 16, "endOffset": 34}, {"referenceID": 65, "context": "To embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based Wu-Palmer similarity [Wu and Palmer, 1994] for \u03bc.", "startOffset": 136, "endOffset": 157}, {"referenceID": 37, "context": "To embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based Wu-Palmer similarity [Wu and Palmer, 1994] for \u03bc.", "startOffset": 43, "endOffset": 72}, {"referenceID": 37, "context": "To embrace the aforementioned ambiguities, Malinowski and Fritz [2014a] suggest using a thresholded taxonomy-based Wu-Palmer similarity [Wu and Palmer, 1994] for \u03bc. Smaller thresholds yield more forgiving metrics. As in Malinowski and Fritz [2014a], we report WUPS at two extremes, 0.", "startOffset": 43, "endOffset": 249}, {"referenceID": 26, "context": "Later on, in Section 7, we will show improved results on DAQUAR with a stronger visual model and a pre-trained word embedding, with ADAM [Kingma and Ba, 2014] as", "startOffset": 137, "endOffset": 158}, {"referenceID": 37, "context": "In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time.", "startOffset": 99, "endOffset": 128}, {"referenceID": 37, "context": "In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time. Table 3 shows that Ask Your Neurons also improves on the reduced DAQUAR set, achieving 34.68% Accuracy and 40.76% WUPS at 0.9 substantially outperforming Malinowski and Fritz [2014a] by 21.", "startOffset": 99, "endOffset": 441}, {"referenceID": 37, "context": "In order to provide performance numbers that are comparable to the proposed MultiWorld approach in Malinowski and Fritz [2014a], we also run our method on the reduced set with 37 object classes and only 25 images with 297 question-answer pairs at test time. Table 3 shows that Ask Your Neurons also improves on the reduced DAQUAR set, achieving 34.68% Accuracy and 40.76% WUPS at 0.9 substantially outperforming Malinowski and Fritz [2014a] by 21.95 percent points of Accuracy and 22.6 WUPS points. Similarly to previous experiments, we achieve the best performance using the \u201csingle word\u201d variant of our method. Note that Ren et al. [2015] reported 36.", "startOffset": 99, "endOffset": 641}, {"referenceID": 65, "context": "Both answersA and T i k are sets of the words, and \u03bc is a membership measure, for instance WUP [Wu and Palmer, 1994].", "startOffset": 95, "endOffset": 116}, {"referenceID": 37, "context": "Moreover, Table 6 shows that \u201cMCM\u201d applied to human answers at test time captures ambiguities in interpreting questions by improving the score of the human baseline from Malinowski and Fritz [2014a] (here, as opposed to Table 5, we exclude the original human answers from the measure).", "startOffset": 170, "endOffset": 199}, {"referenceID": 47, "context": "Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014].", "startOffset": 111, "endOffset": 136}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al.", "startOffset": 42, "endOffset": 62}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR.", "startOffset": 42, "endOffset": 84}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR. The Constant technique uses the most frequent answer in the training set to answer to every question in the test set. In Constant, per question type, we first break all questions into a few categories and then use the most frequent answer per category to answer to every question in that category at the test time. Table 7 provides more details on the chosen categories. Look-up table builds a hash map from a textual question into the most frequent answer for that question at the training time. At the test time, the method just looks up the answer for the question in the hashmap. If the question exists then the most popular answer for that question is provided, otherwise an \u2018empty\u2019 answer is given. In addition, we also remove articles, such as \u2018the\u2019 and \u2018a\u2019, from all the questions. However, this brings only a minor improvement. Finally, we experiment with two nearestneighbor methods. Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014]. In the following, we call this the representation the semantic space. Nearest Neighbor, Question-only searches at test time for the most similar question in the semantic space from the training set. Then it takes the answer that corresponds to this question. Nearest Neighbor is inspired by a similar baseline introduced in Antol et al. [2015]. At the test time we first search for the 4 most similar questions in the semantic space available in the training set.", "startOffset": 42, "endOffset": 1539}, {"referenceID": 3, "context": "Similar baselines were also introduced in Antol et al. [2015] and Ren et al. [2015], although they target either a different dataset or another variant of DAQUAR. The Constant technique uses the most frequent answer in the training set to answer to every question in the test set. In Constant, per question type, we first break all questions into a few categories and then use the most frequent answer per category to answer to every question in that category at the test time. Table 7 provides more details on the chosen categories. Look-up table builds a hash map from a textual question into the most frequent answer for that question at the training time. At the test time, the method just looks up the answer for the question in the hashmap. If the question exists then the most popular answer for that question is provided, otherwise an \u2018empty\u2019 answer is given. In addition, we also remove articles, such as \u2018the\u2019 and \u2018a\u2019, from all the questions. However, this brings only a minor improvement. Finally, we experiment with two nearestneighbor methods. Both methods rely on a Bag-of-Words representation of questions, where every question word is encoded by GLOVE [Pennington et al., 2014]. In the following, we call this the representation the semantic space. Nearest Neighbor, Question-only searches at test time for the most similar question in the semantic space from the training set. Then it takes the answer that corresponds to this question. Nearest Neighbor is inspired by a similar baseline introduced in Antol et al. [2015]. At the test time we first search for the 4 most similar questions in the semantic space available in the training set. Next, we form candidate images that correspond to the aforementioned 4 questions. At the last step, we choose an answer that is associated with the best match in the visual space. The latter is done by a cosine similarity between global CNN representations of the test image and every candidate image. We experiment with several CNN representations (VGG-19, GoogLeNet, ResNet152) but to our surprise there is little performance difference between them. We decide to use GoogLeNet as the results are slightly better. Baseline results. Constant shows how the dataset is biased w.r.t. the most frequent answer. This answer turns out to be the number \u201c2\u201d, which also explain a good performance of Ask Your Neurons on the \u201chow many\u201d question subset in DAQUAR. Constant, per question type shows that question types provide quite strong clues for answering questions. Kafle and Kanan [2016] take advantage of such clues in their Bayesian and Hybrid models (in Table 9, we only show a better performing Hybrid model).", "startOffset": 42, "endOffset": 2543}, {"referenceID": 42, "context": "Based on our further analysis on VQA (for more details we refer to Section 6 and Section 7), we have also applied the improved model to DAQUAR, and we significantly outperform Malinowski et al. [2015] presented in Section 4.", "startOffset": 176, "endOffset": 201}, {"referenceID": 42, "context": "Based on our further analysis on VQA (for more details we refer to Section 6 and Section 7), we have also applied the improved model to DAQUAR, and we significantly outperform Malinowski et al. [2015] presented in Section 4. In the experiments, we first choose last 10% of training set as a validation set in order to determine a number of training epochs K, and next we train the model for K epochs. We evaluate model on two variants of DAQUAR: all data points (\u2018all\u2019 in Table 9), and a subset (\u2018single word\u2019 in Table 9) containing only single word answers, which consists of about 90% of the original dataset. As Table 9 shows, our model, Vision + Language with GLOVE and Residual Net that sums visual and question representations, outperforms the model of Malinowski et al. [2015] by 5.", "startOffset": 176, "endOffset": 784}, {"referenceID": 42, "context": "While Section 4 analyses our original architecture [Malinowski et al., 2015] on the DAQUAR dataset, in this section, we analyze different variants and design choices for neural question answering on the large-scale Visual Question Answering (VQA) dataset [Antol et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 3, "context": ", 2015] on the DAQUAR dataset, in this section, we analyze different variants and design choices for neural question answering on the large-scale Visual Question Answering (VQA) dataset [Antol et al., 2015].", "startOffset": 186, "endOffset": 206}, {"referenceID": 42, "context": "Ask Your Neurons [Malinowski et al., 2015] 19.", "startOffset": 17, "endOffset": 42}, {"referenceID": 42, "context": "Ask Your Neurons [Malinowski et al., 2015] 19.43 21.67 25.28 Question-only of Malinowski et al. [2015] 17.", "startOffset": 18, "endOffset": 103}, {"referenceID": 42, "context": "Ask Your Neurons [Malinowski et al., 2015] 19.", "startOffset": 17, "endOffset": 42}, {"referenceID": 36, "context": "79 IMG-CNN [Ma et al., 2016] 21.", "startOffset": 11, "endOffset": 28}, {"referenceID": 69, "context": "SAN (2, CNN) [Yang et al., 2015] - 29.", "startOffset": 13, "endOffset": 32}, {"referenceID": 66, "context": "60 DMN+ [Xiong et al., 2016] - 28.", "startOffset": 8, "endOffset": 28}, {"referenceID": 6, "context": "79 ABC-CNN [Chen et al., 2015] - 25.", "startOffset": 11, "endOffset": 30}, {"referenceID": 19, "context": "[Jiang et al., 2015] 24.", "startOffset": 0, "endOffset": 20}, {"referenceID": 42, "context": "Ask Your Neurons architecture: originally presented in Malinowski et al. [2015], results in %.", "startOffset": 55, "endOffset": 80}, {"referenceID": 3, "context": "We evaluate on the VQA dataset [Antol et al., 2015], which is built on top of the MS-COCO dataset [Lin et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 34, "context": ", 2015], which is built on top of the MS-COCO dataset [Lin et al., 2014].", "startOffset": 54, "endOffset": 72}, {"referenceID": 26, "context": "We use ADAM [Kingma and Ba, 2014] throughout our ex-", "startOffset": 12, "endOffset": 33}, {"referenceID": 3, "context": "As a performance measure we use a consensus variant of accuracy introduced in Antol et al. [2015], where the predicted answer gets score between 0 and 1, with 1 if it matches with at least three human answers.", "startOffset": 78, "endOffset": 98}, {"referenceID": 41, "context": "All the models, which are publicly available together with our tutorial [Malinowski and Fritz, 2016] 2, are implemented in Keras [Chollet, 2015] and Theano [Bastien et al.", "startOffset": 72, "endOffset": 100}, {"referenceID": 4, "context": "All the models, which are publicly available together with our tutorial [Malinowski and Fritz, 2016] 2, are implemented in Keras [Chollet, 2015] and Theano [Bastien et al., 2012].", "startOffset": 156, "endOffset": 178}, {"referenceID": 13, "context": "We examine two recurrent questions encoders, LSTM [Hochreiter and Schmidhuber, 1997] and a simpler GRU [Cho et al.", "startOffset": 50, "endOffset": 84}, {"referenceID": 47, "context": "To address such shortcoming, we investigate the pre-trained word embedding transformation GLOVE [Pennington et al., 2014] that encodes question words (technically it maps one-hot vector into a 300 dimensional real vector).", "startOffset": 96, "endOffset": 121}, {"referenceID": 47, "context": "This choice naturally extends the vocabulary of the question words to about 2 million words extracted a large corpus of web data \u2013 Common Crawl [Pennington et al., 2014] \u2013 that is used to train the GLOVE embedding.", "startOffset": 144, "endOffset": 169}, {"referenceID": 30, "context": "Next we fix the question encoder to LSTM and vary different visual encoders: Caffe variant of AlexNet [Krizhevsky et al., 2012], GoogLeNet [Szegedy et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 58, "context": ", 2014], VGG-19 [Simonyan and Zisserman, 2014], and recently introduced 152 layered ResNet (we use the Facebook implementation of He et al.", "startOffset": 16, "endOffset": 46}, {"referenceID": 12, "context": ", 2014], VGG-19 [Simonyan and Zisserman, 2014], and recently introduced 152 layered ResNet (we use the Facebook implementation of He et al. [2015]).", "startOffset": 130, "endOffset": 147}, {"referenceID": 24, "context": "SNUBI [Kim et al., 2016] - - 84.", "startOffset": 6, "endOffset": 24}, {"referenceID": 49, "context": "1 Brandeis [Prakash and Storer, 2016] 82.", "startOffset": 11, "endOffset": 37}, {"referenceID": 35, "context": "9 HieCoAtt [Lu et al., 2016] 79.", "startOffset": 11, "endOffset": 28}, {"referenceID": 55, "context": "1 DualNet [Saito et al., 2016] 82.", "startOffset": 10, "endOffset": 30}, {"referenceID": 66, "context": "8 DMN+ [Xiong et al., 2016] 80.", "startOffset": 7, "endOffset": 27}, {"referenceID": 16, "context": "3 FDA [Ilievski et al., 2016] 81.", "startOffset": 6, "endOffset": 29}, {"referenceID": 64, "context": "4 AMA [Wu et al., 2016] 81.", "startOffset": 6, "endOffset": 23}, {"referenceID": 69, "context": "4 SAN [Yang et al., 2015] 79.", "startOffset": 6, "endOffset": 25}, {"referenceID": 67, "context": "4 SMem [Xu and Saenko, 2015] 80.", "startOffset": 7, "endOffset": 28}, {"referenceID": 3, "context": "2 VQA team [Antol et al., 2015] 80.", "startOffset": 11, "endOffset": 31}, {"referenceID": 46, "context": "2 DPPnet [Noh et al., 2015] 80.", "startOffset": 9, "endOffset": 27}, {"referenceID": 73, "context": "4 iBOWIMG [Zhou et al., 2015] 76.", "startOffset": 10, "endOffset": 29}, {"referenceID": 3, "context": "9 LSTM Q+I [Antol et al., 2015] 78.", "startOffset": 11, "endOffset": 31}, {"referenceID": 19, "context": "[Jiang et al., 2015] 78.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Baselines from Antol et al. [2015] are not considered.", "startOffset": 15, "endOffset": 35}], "year": 2016, "abstractText": "We propose a Deep Learning approach to the visual question answering task, where machines answer to questions about real-world images. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, endto-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We evaluate our approaches on the DAQUAR as well as the VQA dataset where we also report various baselines, including an analysis how much information is contained in the language part only. To study human consensus, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus. Finally, we evaluate a rich set of design choices how to encode, combine and decode information in our proposed Deep Learning formulation.", "creator": "LaTeX with hyperref package"}}}