{"id": "1605.04638", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient", "abstract": "This work focuses on the dynamic regret of convex online optimization, which compares the performance of online learning with a clairvoyant who knows the sequence of loss functions in advance and therefore selects at each step the minimizer of the loss function. By assuming that the clairvoyant moves slowly (i.e. the minimizers change slowly), we present several improved variation-based upper limits of dynamic regret under the true and loud gradient feedback, which is \"optimal\" in the light of the presented lower limits. The key to our analysis is to explore a regularity measurement that measures the temporal changes in the clairvoyant minimizers, which we refer to as \"path variation.\" First, we present a general lower limit in terms of pastoral variation and then show that we are able to achieve optimal dynamic regret under complete information or gradient feedback. Second, we set a lower limit in terms of noise feedback, and then we show that a lower limit is reached with a small gradient feedback in two.", "histories": [["v1", "Mon, 16 May 2016 03:01:41 GMT  (36kb)", "http://arxiv.org/abs/1605.04638v1", "Accepted by the 33rd International Conference on Machine Learning (ICML 2016)"]], "COMMENTS": "Accepted by the 33rd International Conference on Machine Learning (ICML 2016)", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["tianbao yang", "lijun zhang 0005", "rong jin", "jinfeng yi"], "accepted": true, "id": "1605.04638"}, "pdf": {"name": "1605.04638.pdf", "metadata": {"source": "META", "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient", "authors": ["Tianbao Yang", "Lijun Zhang", "Rong Jin"], "emails": ["TIANBAO-YANG@UIOWA.EDU", "ZLJZJU@GMAIL.COM", "JINRONG.JR@ALIBABA-INC.COM", "JINFENGY@US.IBM.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n04 63\n8v 1\n[ cs\n.L G\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s)."}, {"heading": "1. Introduction", "text": "Online convex optimization (OCO) can be deemed as a repeated game between an online player and an adversary, in which an online player iteratively chooses a decision and then her decisions incur (possibly different) losses by the loss functions chosen by the adversary. These loss functions are unknown to the decision maker ahead of time, and can be adversarial or even depend on the action taken by the decision maker. To formulate the problem mathematically, let \u2126 \u2286 Rd denote a convex decision set (i.e., the feasible set of the decision vector), wt \u2208 \u2126 denote the decision vector and ft(\u00b7) : Rd \u2192 R denote the loss function at the t-th step, respectively. The goal of the online learner is to minimize her cumulative loss \u2211T t=1 ft(wt). The traditional performance metric - the regret of the decision maker, is defined as the difference between the total cost she has incurred and that of the best fixed decision in hindsight, i.e.,\nT\u2211\nt=1\nft(wt)\u2212 min w\u2208\u2126\nT\u2211\nt=1\nft(w). (1)\nRecently, there emerges a surge of interest (Besbes et al., 2013; Hall & Willett, 2013; Jadbabaie et al., 2015) in the dynamic regret that compares the performance of online learning to a sequence of optimal solutions. If we denote by w\u2217t \u2208 \u2126 an optimal solution of ft(w), the dynamic regret is defined as\nT\u2211\nt=1\n(ft(wt)\u2212 ft(w\u2217t )) = T\u2211\nt=1\n(ft(wt)\u2212 min w\u2208\u2126 ft(w)) (2)\ni.e., the performance of the online learner is compared to a clairvoyant who knows the sequence of loss functions in\nadvance, and hence selects the minimizer w\u2217t at each step. Compared to the traditional regret in (1) (termed as static regret), the dynamic regret is more aggressive since the performance of the clairyomant in the dynamic regret model is always better than that in the static regret model, i.e.,\u2211T\nt=1 ft(w \u2217 t ) \u2264 minw\u2208\u2126 \u2211T t=1 ft(w). It was pointed out that algorithms that achieve performance close to the best fixed decision may perform poorly in terms of dynamic regret (Besbes et al., 2013).\nUnfortunately, it is impossible to achieve a sublinear dynamic regret for any sequences of loss functions (c.f. Proposition 1). In order to achieve a sublinear dynamic regret, one has to impose some regularity constraints on the sequence of loss functions. In this work, we leverage a notion of variation that measures how fast the clairvoyant moves, i.e., how fast the minimizers of the sequence of loss functions change, to which we refer as path variation in order to differentiate with other variation definitions. Formally the path variation is defined as\nV pT , max{w\u2217t \u2208\u2126\u2217t }Tt=1\nT\u22121\u2211\nt=1\n\u2016w\u2217t \u2212w\u2217t+1\u20162 (3)\nwhere \u2126\u2217t denotes the set of all minimizers of ft(w) to account for the potential non-uniqueness. We aim to develop optimal dynamic regrets when the clairvoyant moves slowly given (noisy) gradient feedback (including bandit feedback) for non-strongly convex loss functions. The main results are summarized in Table 1 and the contributions of this paper are summarized below.\n\u2022 We present a general lower bound dependent solely on V pT and show that under true gradient feedback for smooth functions with vanishing gradients in the feasible domain, one can achieve the optimal dynamic regret of O(V pT ) comparable to that with full information feedback.\n\u2022 We present a lower bound under a noisy (sub)gradient feedback dependent on V pT and T , and then show that online gradient descent (OGD) with an appropriate step size can achieve the optimal dynamic regret of O( \u221a V pT T ) under both stochastic gradient feedback\nand two-point bandit feedback.\n\u2022 When the loss functions are smooth, we establish an improved dynamic regret under the two-point bandit feedback, which could match the bound achieved with the full information feedback in a certain condition.\nWe note that a regularity metric similar to the path variation (possibly measured in different norms) has been explored in shifting regret analysis (Herbster & Warmuth, 1998) and drifting regret analysis (Cesa-Bianchi et al., 2012; Buchbinder et al., 2012). The regret against the shifting experts was studied in tracking the best expert, where\nthe best sequence of minimizers are assumed to change for a constant number of times. In drifting regret analysis, the constraint is relaxed to that the path variation is small. In fact, a similar dynamic regret bound to \u221a V pT T has been established for online convex optimization over the simplex (Cesa-Bianchi et al., 2012), where the path variation is measured in \u21131 norm. The present work focuses on OCO in the Euclidean space and considers noisy gradient feedback. A more general variation is considered in (Hall & Willett, 2013), where a sequence of (or a family of) dynamic models \u03c61, . . . , \u03c6T are revealed by the environment for the learner to predict the decision in the next step. Their variation is defined as V \u03c6T = \u2211T\u22121 t=1 \u2016w\u2217t+1 \u2212 \u03c6t(w\u2217t )\u2016 for a sequence of comparators and their dynamic regret scales as V \u03c6T \u221a T , which is worse than our bounds when \u03c6t(w) = w.\nThere has been a different notion to measure the point-wise changes in the sequence of loss functions that measure the changes of two consecutive functions at any feasible points. For example, Besbes et al. (2013) considered the functional variation defined as\nV fT = T\u22121\u2211\nt=1\nmax w\u2208\u2126\n|ft(w)\u2212 ft+1(w)|. (4)\nBesbes et al. considered two feedback structures, i.e., the noisy gradient and the noisy cost, and established sublinear dynamic regret for both feedback structures. For Lipschitz continuous loss functions, their results are presented in Table 1 1. An annoying fact is that even the sequence of Lipschitz loss functions change slowly (namely the functional variation is small), Besbes\u2019 dynamic regret is worse than O( \u221a T ), the optimal rate for static regret. In comparison, our results match that for static regret when the clairvoyant moves slowly such that the path variation is a constant. Another variation that measures point-wise difference between loss functions is the gradient variation introduced in (Chiang et al., 2012), which is defined as\nV gT ,\nT\u2211\nt=1\nmax w\u2208\u2126\n\u2016\u2207ft(w)\u2212\u2207ft\u22121(w)\u201622. (5)\nThe gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014). Recently, Jadbabaie et al. (2015) used the three variations and developed possibly better dynamic regret than using a single variation measure for nonstrongly convex loss functions. They considered the full information feedback (i.e., the whole loss function is revealed to the learner) and a true gradient feedback for a sequence of bounded functions. Their results are also presented in Table 1. In comparison, our results could be potentially better when the clairvoyant moves slowly. Dif-\n1For strongly convex loss functions, better bounds were also established in (Besbes et al., 2013)\nferent from (Jadbabaie et al., 2015), we consider the noisy gradient feedback (including the bandit feedback) and develop both upper bounds and lower bounds."}, {"heading": "2. Optimal Dynamic Regret with Noiseless Information", "text": "In this section, we present an optimal dynamic regret bound dependent solely on V pT . We will first present a lower bound and then present optimal upper bounds in two settings: (i) the full information of the loss function is revealed at each step; (ii) only the true gradient at the decision vector is revealed for smooth loss functions that have vanishing gradients in the feasible domain."}, {"heading": "2.1. Preliminaries and a Lower Bound", "text": "Since it is impossible to achieve a sublinear dynamic regret for any sequence of loss functions. We consider the following family of functions that admit a path variation constraint:\nVp = {{f1, . . . , fT} : V pT \u2264 BT } (6) where BT is the budget. For a (randomized) policy \u03c0 that generates a sequence of solutions w1, . . . ,wT for a sequence of loss functions f1, . . . , fT under the feedback structure \u03c6, its dynamic regret is defined as\nR\u03c0\u03c6({f1, . . . , fT }) = E\u03c0 [ T\u2211\nt=1\nft(wt)\n] \u2212 T\u2211\nt=1\nft(w \u2217 t )\nThe worst dynamic regret of \u03c0 over f \u2208 Vp is R\u03c0\u03c6(Vp, T ) = sup\nf\u2208Vp R\u03c0\u03c6({f1, . . . , fT })\nNote that the dynamic regret remains the same for different sequences of optimal solutions w\u2217t , t = 1, . . . , T .\nBelow, we establish a general lower bound of the dynamic\nregret for the following family of policies:\nA = { \u03c0 : wt = { \u03c01(U), t = 1 \u03c0t({\u03c6\u03c4 (f\u03c4 )}t\u22121\u03c4=1, U), t \u2265 1 } (7)\nwhere \u03c6t(ft) \u2208 Rk denotes any feedback of ft, and U \u2208 U denotes a random variable, \u03c01 : U \u2192 \u2126, \u03c0t : R(t\u22121)k \u00d7 U \u2192 \u2126 are measurable functions. Proposition 1. Let C,C1, C2 be positive constants independent of T and V pT , and let \u03c0 be any policy in A.\n\u2022 If BT \u2265 C1T , then there exists a positive constant C2 such that\nR\u03c0\u03c6(Vp, T ) \u2265 C2T.\n\u2022 For any \u03b3 \u2208 (0, 1), there exists a sequence of loss functions f1, . . . , fT and a positive constant C such that V pT \u2264 o(T ) and\nR\u03c0\u03c6({f1, . . . , ft}) \u2265 C(V pT )\u03b3 .\nRemark: The first part indicates that it is impossible to achieve a sublinear dynamic regret if there is no constraint on the sequence of loss functions. Therefore, in the sequel, we only consider BT \u2264 o(T ). A similar result to the first part using V fT as the regularity measure has been established (Besbes et al., 2013). The second part is novel, which indicates that it is impossible to achieve a better dynamic regret bound of O((V pT )\n\u03b1) with \u03b1 < 1. If otherwise, it then contradicts to the lower bound in the second part of Proposition 1.\nProof. Fix T \u2265 1 and \u03b3 \u2208 (0, 1). To generate the sequence of loss functions, we create a sequence of random variables \u03b51, . . . , \u03b5T , where each \u03b5t is sampled independently from {\u03c3,\u2212\u03c3} with equal probabilities. It is obvious that E[\u03b5t] = 0 and E[\u03b52t ] = \u03c3\n2. For each \u03b5t, we define a loss function ft(w) = 1 2 (w \u2212 \u03b5t)2. Assume \u03c3 \u2208 (0, 1) whose value will be specified later. Let the feasible domain be \u2126 = [\u22121, 1].\nWe have\nE [ R\u03c0\u03c6({f1, . . . , ft}) ] = E\n[ T\u2211\nt=1\nft(wt)\u2212 ft(\u03b5t) ]\n=\nT\u2211\nt=1\nE [ w2t 2 + \u03b52t 2 \u2212 wt\u03b5t ] \u2265 \u03c3 2 2 T\nwhere E[\u00b7] denotes the expectation over the randomness in the sequence of loss functions f1, . . . , ft and the policy \u03c0 and the last inequality is due to that wt is independent of \u03b5t. We also have V p T = \u2211T\u22121 t=1 |\u03b5t \u2212 \u03b5t+1| \u2264 2\u03c3T . To prove the first part, we let \u03c3 be a constant C1/2, then any sequences of loss functions generated as above constitute a subset V \u2032p \u2282 Vp. Then R\u03c0\u03c6(Vp, T ) \u2265 R\u03c0\u03c6(V \u2032p, T ) \u2265 E [ R\u03c0\u03c6({f1, . . . , ft}) ] \u2265 C 2 1\n8 T\nTo prove the second part, we set \u03c3 = T\u2212\u00b5 with \u00b5 = (1\u2212\u03b3)/(2\u2212\u03b3) \u2208 (0, 1/2). Then there exits a positive constant C such that E [ R\u03c0\u03c6({f1, . . . , fT })\u2212 C(V p T ) \u03b3 ] \u2265 0, which implies that there exists a sequence of loss functions f1, . . . , fT such that R\u03c0\u03c6({f1, . . . , fT }) \u2265 C(V p T ) \u03b3 .\nWe note that if \u03b3 = 1, we have \u00b5 = 0 and therefore BT = \u2126(T ) which reduces to the lower bound in the first part. Therefore, we restrict \u03b3 \u2208 (0, 1).\nAn interesting question is that whether an O(V pT ) dynamic regret bound is achievable, if not what is the best we can achieve. In particular, we are interested in scenarios when the feedback \u03c6t(ft) = \u03c6t(wt, ft) only gives a (noisy) gradient of ft(w) at wt.\nBefore delving into the noisy gradient feedback, we first show that an O(V pT ) upper bound is achievable with full information of the loss functions or with full gradient feedback provided that the loss functions are smooth and have vanishing gradients. We make the following assumptions throughout the paper without explicitly mentioning it in the sequel.\nAssumption 1. For {f1, . . . , fT } \u2208 Vp, there exists a r > 0 such that sup\nw \u2217 t \u2208\u2126\u2217t \u2016w\u2212w \u2217 t \u20162 \u2264 r, for any w \u2208 \u2126 and\n1 \u2264 t \u2264 T ."}, {"heading": "2.2. Online Learning with Full Information", "text": "Assume that at each step the full information of the loss function ft(w) is revealed after the decision wt is submitted, and each loss function ft(w) is G-Lipschitz continuous. Then we can update wt+1 by\nwt+1 = min w\u2208\u2126\nft(w), t \u2265 1\nwith w1 be any point in \u2126. To analyze the dynamic regret, we denote by w\u22170 = w1.\nT\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft(w \u2217 t ) =\nT\u2211\nt=1\nft(w \u2217 t\u22121)\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n\u2264 T\u2211\nt=1\nG\u2016w\u2217t\u22121 \u2212w\u2217t \u20162 = G\u2016w1 \u2212w\u22171\u20162\n+G\nT\u22121\u2211\nt=1\n\u2016w\u2217t \u2212w\u2217t+1\u20162 \u2264 Gr +GV pT = O(max(V p T , 1)).\nIt is notable that a similar upper bound of O(max(V fT , 1)) with the full information can be achieved (Jadbabaie et al., 2015)."}, {"heading": "2.3. Online Learning with Gradient Feedback", "text": "Full information may not be available. In practice, only some partial information of the ft(w) regarding the decision vector wt is available. In this subsection, we assume that only the gradient information \u2207ft(wt) is available after the decision wt is submitted. Below, we will first present several examples showing that O(V pT ) is achievable and generalize the analysis to a broad family.\nWe consider two loss functions g1(w) = max(w, 0)2 and g2(w) = (w \u2212 \u03b1)2 defined in the domain \u2126 = [\u22121, 3] and divide all iterations 1, . . . , T into a number m of batches with each batch size of \u2206T . Assume the adversary selects g1(\u00b7) in odd batches and g2(\u00b7) in even batches, and at each step the full gradient feedback is available, i.e., \u03c6t(wt, ft) = f \u2032 t(wt). The example is similar to that presented in (Besbes et al., 2013) except that g1(w) is not strongly convex. Below, we consider two instances of the above example with different \u2206T and \u03b1. For the updates, we adopt the OGD, i.e.,\nwt+1 = \u03a0\u2126[wt \u2212 \u03b7f \u2032t(wt)], t = 1, . . . , T \u2212 1 where \u03a0\u2126[\u00b7] denotes the projection into the domain \u2126. Instance 1. \u2206T = \u2308T/2\u2309 and \u03b1 = 1. Then V pT = 1. Given the value of \u2206T , there are two batches. Let \u03931,\u03932 \u2286 T denote the iteration indices in the first and the second batch, respectively, and let \u0393j [1] denote the first iteration of the j-th batch. We adopt a constant step size \u03b7 = 1/2 with a starting point w0 = 0. Then wt = 0, t \u2208 \u03931. For the first iteration t \u2208 \u03932 we have wt = \u03a0\u2126[0 \u2212 \u03b7g\u20321(0)] = 0. And for all remaining iterations t \u2208 \u03932, we have wt = \u03a0\u2126[wt\u22121 \u2212 \u03b7g\u20322(wt\u22121)] = \u03b1. As a result wt = w\u2217t , t \u2208 [T ] except w\u03932[1] = 0 6= w\u2217\u03932[1], which indicates that the dynamic regret is f\u03932[1](w\u03932[1])\u2212f\u03932[1](w\u2217\u03932[1]) = g2(0)\u2212 g2(1) = 1. Instance 2. \u03b8 = C/ \u221a T , \u2206T = \u230a1+1/(2\u03b8)\u230b, \u03b1 = 1+(1\u2212 2\u03b8)\u2206T , and T > 4C2 (note that \u03b8 < 1/2 and 1 \u2264 \u03b1 \u2264 2).\nThen\nV pT =\nm\u2211\nj=1\n\u03b1 \u2264 2m = 2T \u2206T = 2T \u230a1 + \u221a T/(2C)\u230b\n\u2264 4C \u221a T\nThe example is similar to the above except that the loss functions change more frequently. We consider OGD with a constant step size \u03b7 = 1/2 and w1 = 1. Similar as before, we use j = 1, . . . ,m to denote the batch index, \u0393j \u2286 T to denote the indices in the j-th batch, \u0393j[1] and \u0393j [2 :] to denote the first iteration and remaining iterations in batch j, respectively. Note that w\u2217t = 0, t \u2208 \u03932j\u22121 and w\u2217t = \u03b1, t \u2208 \u03932j . For t \u2208 \u03931[2 :] or t = \u03932[1], by induction we can show that wt = \u03a0\u2126[wt\u22121 \u2212 \u03b7g\u20321(wt\u22121)] = 0. Therefore, wt = w\u2217t , t \u2208 \u03931[2:] and w\u03932[1] = 0. For t \u2208 \u03932[2:] or t = \u03933[1], following the OGD update wt = \u03a0\u2126[wt\u22121\u2212\u03b7g\u20322(wt\u22121)] = \u03a0\u2126[wt\u22121 \u2212 2\u03b7(wt\u22121 \u2212 \u03b1)] = \u03b1. Therefore, wt = \u03b1, t \u2208 \u03932[2:] and t = \u03933[1]. Following the same analysis, we have wt = w \u2217 t for t \u2208 \u0393j [2 :], w\u03932j\u22121 [1] = \u03b1 and w\u03932j [1] = 0. It means that the difference between the decision vector wt and the optimal solutions w\u2217t only happens at the first iterations of all batches. As a result, the dynamic regret is m\u2211\nj=1\nmax(g1(\u03b1)\u2212 g1(0), g2(0)\u2212 g2(\u03b1)) = m\u2211\nj=1\n\u03b12 \u2264 2V pT\nIt is notable the key ingredient to achieve an O(V pT ) dynamic regret is to use a constant step size. Next, we generalize this result to a broad family of loss functions. In particular, we assume the sequence of loss functions satisfy the following assumption.\nAssumption 2. Assume that every loss function ft(\u00b7) is defined over Rd and is convex and smooth, i.e., for any w,w\u2032 \u2208 Rd, we have\n\u2016\u2207ft(w) \u2212\u2207ft(w\u2032)\u20162 \u2264 L\u2016w\u2212w\u2032\u20162, where L > 0 is the smoothness constant. In addition, we assume that there exists w\u2217t \u2208 \u2126\u2217t such that \u2207ft(w\u2217t ) = 0.\nThe condition \u2207ft(w\u2217t ) = 0 is referred to as the vanishing gradient condition. The examples considered before indeed satisfy Assumption 2. Consider the policy of OGD:\n\u03c0 : wt= { w1 \u2208 \u2126 t = 1 \u03a0\u2126[wt\u22121 \u2212 \u03b7\u2207ft\u22121(wt\u22121)] t > 1 (8)\nThe following theorem states the dynamic regret bound of OGD with a constant step size.\nTheorem 3. (upper bound) Suppose Assumption 2 hold. By the policy \u03c0 in (8) with \u03b7 = 1/(2L), for any {f1, . . . , fT } \u2208 Vp we have\nT\u2211\nt=1\nft(wt)\u2212 ft(w\u2217t ) \u2264 2L ( r2 + 2rBT ) .\nTo prove the theorem, we first give the following lemma\nwhose proof is deferred to Appendix.\nLemma 4. Let wt = \u03a0\u2126[wt\u22121 \u2212 \u03b7gt\u22121], t > 1. Then\ng\u22a4t (wt \u2212w\u2217t ) \u2264 \u03b7\n2 \u2016gt\u201622 + r\u2016w\u2217t \u2212w\u2217t+1\u20162 \u03b7\n+ \u2016wt \u2212w\u2217t \u201622 \u2212 \u2016wt+1 \u2212w\u2217t \u201622 \u2212 \u2016w\u2217t \u2212w\u2217t+1\u201622\n2\u03b7\nProof of Theorem 3. Following Lemma 4 and the convexity of ft(w), we have\nft(wt)\u2212 ft(w\u2217t ) \u2264 \u03b7\n2 \u2016\u2207ft(wt)\u201622 + r\u2016w\u2217t \u2212w\u2217t+1\u20162 \u03b7\n(9)\n+ \u2016wt \u2212w\u2217t \u201622 \u2212 \u2016wt+1 \u2212w\u2217t+1\u201622 \u2212 \u2016w\u2217t+1 \u2212w\u2217t \u201622\n2\u03b7\nBy the smoothness of f(w), for any w \u2208 Rd\nft(w) \u2212 ft(wt) \u2264 \u3008\u2207ft(wt),w \u2212wt\u3009+ L\n2 \u2016w\u2212wt\u201622\nLet w = w\u2032t = wt \u2212 1L\u2207ft(wt) in the above inequality, we have ft(w\u2032t) \u2212 ft(wt) \u2264 \u2212 \u2016\u2207ft(wt)\u201622 2L . By convexity of ft(w),\nft(w \u2032 t) \u2265 ft(w\u2217t ) +\u2207ft(w\u2217t )\u22a4(w\u2032t \u2212w\u2217t ) = ft(w\u2217t )\nwhere the equality follows the vanishing gradient condition. Then\nft(w \u2217 t )\u2212 ft(wt) \u2264 ft(w\u2032t)\u2212 ft(wt) \u2264 \u2212 \u2016\u2207ft(wt)\u201622 2L\nCombing the inequality above with (9), we have\nft(wt)\u2212 ft(w\u2217t ) \u2264 \u03b7L(ft(wt)\u2212 ft(w\u2217t ))\n+ \u2016wt \u2212w\u2217t \u201622 \u2212 \u2016wt+1 \u2212w\u2217t+1\u201622\n2\u03b7 + r\u2016w\u2217t \u2212w\u2217t+1\u20162 \u03b7\nBy summing over t = 1, . . . , T , we have\nT\u2211\nt=1\n(ft(wt)\u2212 ft(w\u2217t )) \u2264 1\n1\u2212 \u03b7L\n( r2\n2\u03b7 +\nr \u03b7 V pT\n)\nWe complete the proof by choosing \u03b7 = 1/[2L].\nRemark: From Theorem 3, we can see that OGD can achieve an O(max(V pT , 1)) dynamic regret for a sequence of loss functions in Vp that satisfy Assumption 2 with only the gradient feedback, which is comparable to that achieved in the full information feedback. The instance 1 and 2 in Section 2 has V pT = O(1) and V p T \u2248 4C \u221a T , respectively. Therefore, using OGD with \u03b7 = C we can obtain an O(1) and O( \u221a T ) dynamic regret.\nFinally, it is worth mentioning that the OGD with restarting proposed in Besbes et al.\u2019 work achieves an O(T 1/3) dynamic regret for instance 1 and an O(T 5/6) dynamic regret for instance 2 due to that the functional variation for the\nfirst instance is bounded by a constant and for the second instance is bounded by O( \u221a T )."}, {"heading": "3. Optimal Dynamic Regret with Noisy Gradient", "text": "In this section, we focus on noisy gradient feedback, i.e., \u03c6t(wt, ft) is only a noisy (sub)gradient of ft(w) at wt."}, {"heading": "3.1. A Lower Bound with Noisy Gradient Feedback", "text": "Before presenting the upper bounds of the dynamic regret with noisy gradient feedback, we will first present a lower bound. For establishing the lower bound, we consider the following class of policies\n\u03c0 : wt = { \u03c01(U), t = 1 \u03c0t({\u03c6\u03c4 (w\u03c4 , f\u03c4 )}t\u22121\u03c4=1, U), t \u2265 1\n(10)\nwhere U, \u03c01, \u03c0t are defined similarly as before, and \u03c6t(wt, ft) is a noisy subgradient of ft at wt. In particular, we assume the noisy gradient is given by \u03c6t(wt, ft) \u2208 \u2202ft(wt) + \u01ebt with \u01ebt satisfying the following condition.\nAssumption 5. \u01ebt \u2208 Rd, t \u2265 1 are iid random vectors with zero mean and covariance matrix \u03a3 with bounded entries such that tr(\u03a3) \u2264 \u03bb2. Let P (\u00b7) denote the cumulative function of \u01ebt. There exists a constant C\u0303 such that for any a \u2208 Rd, \u222b log ( dP (y)\ndP (y+a)\n) dP (y) \u2264 C\u0303\u2016a\u201622.\nWhen the noise vectors are independent Gaussian random vectors with zero mean and covariance matrices with entries uniformly bounded by \u03c32, the above assumption is satisfied with C\u0303 = 1/(2\u03c32) and \u03bb2 = d\u03c32 (Besbes et al., 2013).\nTheorem 6. (lower bound) For any 1 \u2264 BT \u2264 T and \u03ba \u2208 (1/2, 1), there exist V \u2032p \u2282 Vp and C(\u03ba) > 0 independent of T and BT such that for any policy \u03c0 in (10) under the noisy gradient feedback that satisfies Assumption 5, we have\nR\u03c0\u03c6(V \u2032p, T ) \u2265 C(\u03ba)B\u03baTT 1\u2212\u03ba.\nRemark: Note that from the proof presented below when \u03ba \u2192 1/2, C(\u03ba) \u2192 0. However, the above lower bound can be used to argue that it is impossible to achieve a better dynamic regret than O(B1/2T T\n1/2) with the noisy gradient feedback for any sequence of loss functions. We prove this by contradiction. In particular, assume there exists an algorithm under the noisy gradient feedback achieves better bound than O(B1/2T T\n1/2) for any sequence of loss functions. We can consider two lower orders O(B\u03b1TT\n1\u2212\u03b1) with 1 > \u03b1 > 1/2 and O(B\u03b1TT\n\u03b2) with \u03b1 \u2264 1/2, \u03b2 \u2264 1/2 and \u03b1 + \u03b2 < 1. First, we assume O(B\u03b1TT\n1\u2212\u03b1) is achievable. By Theorem 6 we know that there exists \u03b1 < \u03ba < 1 (e.g., \u03ba = \u03b1+12 ) and V \u2032p such that R\u03c0\u03c6(V \u2032p, T ) \u2265 \u2126(B\u03baTT 1\u2212\u03ba) \u2265 \u2126(B\u03b1TT 1\u2212\u03b1), which yields a contradic-\ntion. To show that the second lower bound is unachievable, we can construct a BT such that B\u03baTT\n1\u2212\u03ba \u2265 \u2126(B\u03b1TT \u03b2), i.e., BT \u2265 \u2126(T \u03b2+\u03ba\u22121 \u03ba\u2212\u03b1 ), where \u03b2 + \u03ba\u2212 1 < \u03ba\u2212 \u03b1.\nProof. We construct two functions over the domain \u2126 = [\u22121/2,+1/2]. They are\nf(x) =    1 1+\u03b3 \u03b4 1+\u03b3 \u2212 \u03b4\u03b3x x \u2208 [\u22121/2, 0] 1\n1+\u03b3 |x\u2212 \u03b4|1+\u03b3 x \u2208 [0, 2\u03b4] \u2212 1+2\u03b31+\u03b3 \u03b41+\u03b3 + \u03b4\u03b3x x \u2208 [2\u03b4, 1/2] , (11)\ng(x) =    \u2212 1+2\u03b31+\u03b3 \u03b41+\u03b3 \u2212 \u03b4\u03b3x x \u2208 [\u22121/2,\u22122\u03b4] 1 1+\u03b3 |x+ \u03b4|1+\u03b3 x \u2208 [\u22122\u03b4, 0] 1\n1+\u03b3 \u03b4 1+\u03b3 + \u03b4\u03b3x x \u2208 [0, 1/2]\n(12)\nwhere 0 < \u03b4 < 1/2 and \u03b3 > 0 will be determined later. It is easy to verify that both f(\u00b7) and g(\u00b7) are convex but not strongly convex. It is also easy to see that the optimal solutions for f(\u00b7) and g(\u00b7) are x\u2217f = \u03b4 and x\u2217g = \u2212\u03b4, respectively. Hence |x\u2217f \u2212 x\u2217g| = 2\u03b4. For a given budget BT , we will construct a subset of Vp by only considering the sequence of these two loss functions. For some \u2206T \u2208 {1, . . . , T } that, we divide the entire sequence T into m = \u2308T/\u2206T \u2309 batches, denoted by T1, . . . , Tm, each with size of \u2206T (except perhaps Tm), i.e. Tj = {(j \u2212 1)\u2206T + 1, . . . ,min(j\u2206t, T )}, j = 1, . . . ,m. To generate the sequence of loss functions f1, . . . , fT , at the beginning of each batch Tj , we randomly choose between the two functions f(\u00b7) and g(\u00b7) and the same loss function will be used throughout the batch. We denote by V \u2032p = {{ft(\u00b7), t = 1, . . . , T }} the set of a sequence of randomly sampled loss functions, and by X1, . . . , XT the sequence of solutions generated by any policy in (10). Let \u03b4 = BT\u2206T /2T . For any f \u2208 V \u2032p, we have\nV pT =\nm\u2211\nj=2\n|x\u2217j \u2212 x\u2217j\u22121| \u2264 (\u2308T/\u2206T \u2309 \u2212 1) 2\u03b4 \u2264 T 2\u03b4\n\u2206T = BT\nTherefore, V \u2032p \u2282 Vp. We denote by P\u03c0f the probability measure under policy \u03c0 when f is the sequence of the loss functions, and by E\u03c0f the associated expectation operator. Set\n\u2206T = max { ( 4 \u03b3\n4C\u0303 )1/(2\u03b3+1)( TBT )\n2\u03b3/(2\u03b3+1), 1 } . Then\nC\u0303E\u03c0f\n  \u2211\nt\u2208Tj (\u2207f(Xt)\u2212\u2207g(Xt))2\n  \u2264 C\u0303 \u2211\nt\u2208Tj 4\u03b42\u03b3\n\u2264 4C\u0303\u2206T \u03b42\u03b3 \u2264 4C\u0303 B2\u03b3T \u2206 2\u03b3+1\n22\u03b3T 2\u03b3 \u2264 max(1, 4C\u0303B\n2\u03b3 T\nT 2\u03b34\u03b3 )\n\u2264 max(1, 4C\u0303 4\u03b3 ) \u2264 max(1, 4C\u0303) , \u03b2\nwhere we use the condition that BT \u2264 T . Using Lemma A-1 and A-2 from (Besbes et al., 2013), we have\nmax { P \u03c0 f {Xt > 0},P\u03c0g{Xt \u2264 0} } \u2265 1\n4e\u03b2 , \u2200t\nUsing the above result, we have\nR\u03c0\u03c6(V \u2032p, T ) \u2265 E\n  m\u2211\nj=1\n\u2211 t\u2208Tj ft(Xt)\u2212 ft(x\u2217j )\n \n= 1\n2\nm\u2211\nj=1\nE\u03c0f\n  \u2211\nt\u2208Tj f(Xt)\u2212 f(x\u2217f )\n \n+ 1\n2\nm\u2211\nj=1\nE\u03c0g\n  \u2211\nt\u2208Tj g(Xt)\u2212 g(x\u2217g)\n \n\u2265 1 2\nm\u2211\nj=1\n\u2211 t\u2208Tj P{Xt \u2264 0} ( f(0)\u2212 f(x\u2217f ) )\n+ 1\n2\nm\u2211\nj=1\n\u2211 t\u2208Tj P{Xt > 0} ( g(0)\u2212 g(x\u2217g) )\n= \u03b41+\u03b3\n2(1 + \u03b3)\nm\u2211\nj=1\n\u2211 t\u2208Tj (P{Xt \u2264 0}+ P{Xt > 0})\n\u2265 \u03b4 1+\u03b3T\n8(1 + \u03b3)e\u03b2 .\nIn the above derivations, the first expectation is taking over all randomness in \u03c0 and f1, . . . , fT , the second inequality holds because\nE[f(Xt)] = E[f(Xt)|Xt > 0] Pr(Xt > 0) + E[f(Xt)|Xt \u2264 0] Pr(Xt \u2264 0) \u2265 f(0) Pr(Xt \u2264 0) E[g(Xt)] = E[g(Xt)|Xt > 0] Pr(Xt > 0) + E[g(Xt)|Xt \u2264 0] Pr(Xt \u2264 0) \u2265 g(0) Pr(Xt > 0)\nwhere the inequalities are due to f(x) \u2265 0, g(x) \u2265 0 and f(x) \u2265 f(0) when x \u2264 0 and g(x) \u2265 g(0) when x \u2265 0. To proceed, we plug in the value of \u03b4 into the lower bound of R\u03c0\u03c6(V \u2032p, T )\nR\u03c0\u03c6(V \u2032p, T ) \u2265 T\n8e\u03b2(1 + \u03b3)\nB1+\u03b3T \u2206 1+\u03b3 T\n21+\u03b3T 1+\u03b3\n\u2265 4 \u03b3(1+\u03b3)/(1+2\u03b3) 8e\u03b2(1 + \u03b3)21+\u03b3(4C\u0303)(1+\u03b3)/(1+2\u03b3) B\n1+\u03b3\u2212(1+\u03b3) 2\u03b3 1+2\u03b3 T\nT \u03b3\u2212(1+\u03b3) 2\u03b3 1+2\u03b3\n= 4\u03b3(1+\u03b3)/(1+2\u03b3)\n8e\u03b2(1 + \u03b3)21+\u03b3(4C\u0303)(1+\u03b3)/(1+2\u03b3) B\n1+\u03b3 1+2\u03b3\nT T 1\u2212 1+\u03b3 1+2\u03b3\nLet \u03b3 = (1\u2212\u03ba)/(2\u03ba\u22121), i.e., \u03ba = (1+\u03b3)/(1+2\u03b3). Then\nR\u03c0\u03c6(V \u2032p, T ) \u2265 (4\u03b3)\u03ba (1 + \u03b3)2\u03b3 1 16e\u03b2(4C\u0303)\u03ba B\u03baTT 1\u2212\u03ba.\nIn the next two subsections, we consider two types of noisy gradient feedback, namely a stochastic subgradient feedback that is an unbiased estimation of the true subgradient and a bandit feedback that gives an unbiased estimation of\nthe subgradient of a smoothed function instead of the original function. We show that under the two noisy gradient feedback, we are able to achieve an optimal dynamic regret of O( \u221a V pT T ). Furthermore, for smooth loss functions under the two-point bandit feedback, we establish an even better upper bound by leveraging the gradient variation in the form of O(max( \u221a V pT V g T , V p T )), which when the gradient variation is small matches the lower bound presented in Proposition 1."}, {"heading": "3.2. Online Learning with Bounded Stochastic Gradient Feedback", "text": "We adopt the policy defined by OGD using the noisy gradient feedback, i.e.,\n\u03c0 : wt = { w1 \u2208 \u2126 t = 1 \u03a0\u2126[wt\u22121 \u2212 \u03b7\u03c6t\u22121(wt\u22121, ft\u22121)] t > 1\n(13) where \u03c6t(wt, ft) \u2208 \u2202ft(wt) + \u01ebt is a noisy subgradient with \u01ebt satisfying Assumption 5. The upper bound of the dynamic regret of OGD with an appropriate step size is presented below.\nTheorem 7. (upper bound) Suppose Assumption 5 hold. Assume \u2016\u2202ft(w)\u20162 \u2264 G, for any w \u2208 \u2126 and 1 \u2264 t \u2264 T . By the policy \u03c0 in (13) with \u03b7 = \u221a r2+2rBT T (G2+\u03bb2) , we have\nR\u03c0\u03c6(Vp, T ) \u2264 \u221a (r2 + 2rBT )(G2 + \u03bb2)T .\nProof. Let Et[\u00b7] denote the expectation over the randomness in \u03c6t given the randomness before t. We abuse the notation w\u2217T+1 = w \u2217 T . Note that\nEt [ \u2016\u03c6t(wt, ft)\u201622 ] \u2264 (G2 + \u03bb2).\nFollowing Lemma 4 and the convexity of ft(w), we have\nEt [ft(wt)\u2212 ft(w\u2217t )] \u2264 Et [\u3008\u03c6t(wt, ft),wt \u2212w\u2217t \u3009]\n\u2264 \u2016wt \u2212w \u2217 t \u201622 2\u03b7 \u2212 \u2016wt+1 \u2212w \u2217 t+1\u201622 2\u03b7 \u2212 \u2016w \u2217 t \u2212w\u2217t+1\u201622 2\u03b7 + \u03b7\n2 (G2 + \u03bb2) +\nr \u03b7 \u2016w\u2217t \u2212w\u2217t+1\u20162\nHence, by summing the above inequalities over t = 1, . . . , T we have\nE\n[ T\u2211\nt=1\nft(wt)\u2212 ft(w\u2217t ) ] \u2264 1\n2\u03b7\n( r2 + 2rV pT ) + \u03b7\n2 G2\u03bbT\nwhere G2\u03bb = G 2+\u03bb2. Since the above inequality holds for any w\u2217t \u2208 \u2126\u2217t , we thus conclude\nE\n[ T\u2211\nt=1\nft(wt)\u2212 ft(w\u2217t ) ] \u2264 1\n2\u03b7 (r2 + 2rBT ) +\n\u03b7 2 G2\u03bbT\nWe complete the proof by choosing \u03b7 = \u221a\nr2+2rBT T (G2+\u03bb2) .\nRemark: From Theorem 7, we can see that OGD can achieve an O( \u221a max(V pT , 1)T ) dynamic regret with a\nstep size \u03b7 = C \u221a\nmax(V pT , 1)/T . Compared to OGD with restarting proposed in (Besbes et al., 2013), our result could be better when \u221a V pT T \u2264 O((V f T )\n1/3T 2/3), i.e., V pT \u2264 O((V f T ) 2/3T 1/3)."}, {"heading": "3.3. Online Learning with Bandit Feedback", "text": "In this subsection, we analyze the dynamic regret with bandit feedback by building on previous work. Bandit feedback has been analyzed before for the static regret. In particular, using one-point bandit feedback Flaxman et al. (2005) showed an O(T 3/4) static regret bound, while Agarwal et al. (2010) established an optimal static regret bound of O( \u221a T ) using two-point bandit feedback. Recently, Chiang et al. (2013) derived a variational static regret bound in the two-point bandit setting that depends on\u221a V gT where V g T is the gradient variation defined in Section 1. In order to have optimal dynamic regret bounds, we also consider two-point bandit setting and show that the previous algorithms in (Agarwal et al., 2010; Chiang et al., 2013) by adjusting the step size can achieve an O( \u221a V pT T ) dynamic regret for general Lipschitz continuous loss functions and an O(max( \u221a V gT V p T , V p T )) dynamic regret for smooth loss functions. Below, we present more details. The omitted proof can be found in Appendix.\nSimilar to previous work, we assume that ft(w) is GLipschitz continuous and R1B \u2286 \u2126 \u2286 R2B where B = {w \u2208 Rd : \u2016w\u20162 \u2264 1} is the unit ball centered at 0. Let ut \u2208 Rd be a random unit vector, ei \u2208 Rd be the i-th canonical vector, w1 = 0. For Lipschitz continuous loss functions, the update is given by\nwt+1 = \u03a0(1\u2212\u03be)\u2126[wt \u2212 g\u0302t] (14) where \u03be \u2208 (0, 1) and g\u0302t is computed from two-point bandit feedback\ng\u0302t = d\n2\u03b4 [ft(wt + \u03b4ut)\u2212 ft(wt \u2212 \u03b4ut)]ut\nwith \u03b4 = \u03beR1. For any wt \u2208 (1\u2212 \u03be)\u2126 and any unit vector u, wt + \u03b4u \u2208 \u2126 (Flaxman et al., 2005). It can be shown that g\u0302t is an unbiased stochastic gradient of the function f\u0302t(w) = Eu[ft(w + \u03b4u)]. Importantly, \u2016g\u0302t\u20162 \u2264 Gd. The following theorem states the dynamic regret bound for the policy in (14).\nTheorem 8. Assume ft(w) is G-Lipschitz continuous. By the policy in (14) with \u03be = 1T , \u03b4 = \u03beR1, and \u03b7 =\u221a\nr2+2rBT TG2d2 , we have\nE\n[ T\u2211\nt=1\n1 2 (ft(w\u0302 1 t ) + f(w\u0302 2 t ))\n] \u2212 T\u2211\nt=1\nft(w \u2217 t )\n\u2264 \u221a (r2 + 2rBT )G2d2T +G(3R1 +R2).\nAlgorithm 1 META algorithm 1: Initialize solution w1 = w\u03021 = 0 and g\u03020 = 0 2: for t = 1, . . . , T do 3: Choose it uniformly from [d] 4: Submit w\u03021t = w\u0302t + \u03b4eit and w\u0302 2 t = w\u0302t \u2212 \u03b4eit\n5: Receive the feedback ft(w\u03021t ) and ft(w\u0302 2 t ) and let\nvt,it = 1 2\u03b4 (ft(w\u0302 1 t )\u2212 f(w\u03022t ))\n6: Compute\ngt = d(vt,it \u2212 g\u0302t\u22121,it)eit , and g\u0302t = d(vt,it \u2212 g\u0302t\u22121,it)eit + g\u0302t\u22121\n7: Update\nwt+1 = \u03a0(1\u2212\u03be)\u2126[wt \u2212 \u03b7gt] w\u0302t+1 = \u03a0(1\u2212\u03be)\u2126[wt+1 \u2212 \u03b7g\u0302t]\n8: end for\nwhere w\u03021t = wt + \u03b4ut, w\u0302 2 t = wt \u2212 \u03b4ut.\nRemark: The dynamic regret averaged over two decisions with two-point bandit feedback is in the same order of\u221a max(V pT , 1)T to that in Theorem 7 with stochastic gradient feedback.\nFinally, we present an upper bound for smooth loss functions by leveraging the gradient variation, which leads to an improved dynamic regret bound compared to Lipschitz continuous loss functions. The updates are based on the META algorithm proposed in (Chiang et al., 2013), which is presented in Algorithm 1. It was proved to achieve a better static regret of O( \u221a V gT ) than O( \u221a T ). Below, we show that the same policy but with a different step size can achieve an improved dynamic regret, i.e., O(max( \u221a V gT max(V p T , 1), V p T )) for a sequence of smooth loss functions from the following set\nVp,g = {{f1, . . . , fT} : V pT \u2264 BT , V g T \u2264 ST }\nThe theorem below states the result.\nTheorem 9. Assume {f1, . . . , fT } \u2208 Vp,g and ft(w) is Lsmooth for any t \u2265 1. By the policy in Algorithm 1 with \u03be = 1 T , \u03b4 = \u03beR1 and \u03b7 = min (\u221a (2rBT+r2) 8ST d4 , 1 4Ld3/2 \u221a lnT ) ,\nwe have\nE\n[ T\u2211\nt=1\n1 2 (ft(w\u0302 1 t ) + f(w\u0302 2 t ))\n] \u2212 T\u2211\nt=1\nft(w \u2217 t )\n\u2264 O ( max { d2 \u221a ST max(BT , 1), d 3/2 max(BT , 1) }) .\nwhere w\u03021t = wt + \u03b4ut, w\u0302 2 t = wt \u2212 \u03b4ut.\nRemark: When the gradient variation is small such that\nthe upper bound is dominated by O(V pT ), it matches the lower bound established in Proposition 1. Finally, we note that a similar upper bound can be achieved for linear loss functions by extending the static regret analysis in (Chiang et al., 2013) to the dynamic regret similarly to the proof of Theorem 9."}, {"heading": "4. Conclusions", "text": "In this paper, we have considered dynamic regret for online learning under true and noisy gradient feedback. We have developed several lower and upper bounds of the dynamic regret based on the path variation that measures the temporal changes in the optimal solutions. In light of the presented lower bounds, the achieved upper bounds are optimal for non-strongly convex loss functions when the clairvoyant moves slowly. An interesting question that remains open is that what is the optimal dynamic regret bound for strongly convex loss functions in terms of the path variation."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the anonymous reviewers for their helpful comments. T. Yang was supported in part by NSF (1463988, 1545995)."}, {"heading": "A. Proof of Lemma 4", "text": "Let w\u2032t = wt \u2212 \u03b7gt. Thus wt+1 = \u03a0\u2126[w\u2032t]. 1 2 \u2016wt+1 \u2212w\u2217t \u201622 \u2264 1 2 \u2016w\u2032t \u2212w\u2217t \u201622 = 1 2 \u2016wt \u2212 \u03b7gt \u2212w\u2217t \u201622\n= 1 2 \u2016wt \u2212w\u2217t \u201622 \u2212 \u03b7g\u22a4t (wt \u2212w\u2217t ) + 1 2 \u03b72\u2016gt\u201622\nThen\ng\u22a4t (wt \u2212w\u2217t ) \u2264 1 2 \u2016wt \u2212w\u2217t \u201622 \u2212 1 2 \u2016wt+1 \u2212w\u2217t \u201622\n+ 1\n2 \u03b72\u2016gt\u201622\n= 1\n2\u03b7 \u2016wt \u2212w\u2217t \u201622 \u2212\n1\n2\u03b7 \u2016wt+1 \u2212w\u2217t+1 +w\u2217t+1 \u2212w\u2217t \u201622\n+ 1\n2 \u03b7\u2016gt\u201622\n= 1 2\u03b7 \u2016wt \u2212w\u2217t \u201622 \u2212 1 2\u03b7 \u2016wt+1 \u2212w\u2217t+1\u201622\n\u2212 1 2\u03b7 \u2016w\u2217t+1 \u2212w\u2217t \u201622 + 1 \u03b7 (w\u2217t+1 \u2212wt+1)\u22a4(w\u2217t \u2212w\u2217t+1)\n+ 1\n2 \u03b7\u2016gt\u201622\n\u2264 1 2\u03b7 \u2016wt \u2212w\u2217t \u201622 \u2212 1 2\u03b7 \u2016wt+1 \u2212w\u2217t+1\u201622\n\u2212 1 2\u03b7 \u2016w\u2217t+1 \u2212w\u2217t \u201622 + 1 \u03b7 r\u2016w\u2217t \u2212w\u2217t+1\u20162 + 1 2 \u03b7\u2016gt\u201622"}, {"heading": "B. Proof of Theorem 8", "text": "Define f\u0302t(w) as\nf\u0302t(w) = Eu[ft(w + \u03b4u)]\nwhere u is a random unit vector. We first give the following lemma.\nLemma 10. Let w\u0302\u2217t = (1\u2212 \u03be)w\u2217t . T\u2211\nt=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 2 t ))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n\u2264 T\u2211\nt=1\nf\u0302t(wt)\u2212 T\u2211\nt=1\nf\u0302t(w\u0302 \u2217 t ) + 3TG\u03b4 + TGR2\u03be\nNote that the updating rule is OGD with a noisy gradient applied to a sequence of functions f\u0302t(w), t = 1, . . . , T . Following (Agarwal et al., 2010), g\u0302t is bounded by \u2016g\u0302t\u20162 \u2264 Gd. Then following the proof of Theorem 6, we have,\nT\u2211\nt=1\nf\u0302t(wt)\u2212 T\u2211\nt=1\nf\u0302t(w\u0302 \u2217 t ) \u2264 \u2016w1 \u2212 w\u0302\u22171\u201622 2\u03b7\n+\nT\u2211\nt=1\n1 \u03b7 \u2016wt+1 \u2212 w\u0302\u2217t+1\u20162\u2016w\u0302\u2217t \u2212 w\u0302\u2217t+1\u20162 + \u03b7 2 G2d2T\nSince wt, w\u0302\u2217t \u2208 (1\u2212 \u03be)\u2126, we have \u2016wt \u2212 w\u0302\u2217t \u20162 \u2264 r\ndue to Assumption 1. In addition,\n\u2016w\u0302\u2217t \u2212 w\u0302\u2217t+1\u20162 = \u2016(1\u2212 \u03be)w\u2217t \u2212 (1\u2212 \u03be)w\u2217t+1\u20162 \u2264 (1\u2212 \u03be)\u2016w\u2217t \u2212w\u2217t+1\u20162 \u2264 \u2016w\u2217t \u2212w\u2217t+1\u20162\nThen T\u2211\nt=1\nf\u0302t(wt)\u2212 T\u2211\nt=1\nf\u0302t(w\u0302 \u2217 t ) \u2264\n1\n2\u03b7 (r2 + 2rV pT ) +\n\u03b7 2 G2d2T\n\u2264 1 2\u03b7 (r2 + 2rBT ) + \u03b7 2 G2d2T\nBy plugging the value of \u03b7, we have\nT\u2211\nt=1\nf\u0302t(wt)\u2212 T\u2211\nt=1\nf\u0302t(w\u0302 \u2217 t ) \u2264 \u221a (r2 + 2rBT )G2d2T\nThen combining the above inequality with Lemma 13, we have\nT\u2211\nt=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 2 t ))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n\u2264 \u221a (r2 + 2rBT )G2d2T + 3TG\u03b4 + TGR2\u03be \u2264 \u221a (r2 + 2rBT )G2d2T +G(3R1 +R2).\nB.1. Proof of Lemma 10\nThe proof is almost identical to that of Lemma 2 in (Agarwal et al., 2010). By the Lipschitz property of ft(w), we have\nft(w\u0302 1 t ) = ft(wt + \u03b4ut) \u2264 ft(wt) +G\u03b4\u2016ut\u20162 ft(w\u0302 2 t ) = ft(wt \u2212 \u03b4ut) \u2264 ft(wt) +G\u03b4\u2016ut\u20162\nSince \u2016ut\u20162 = 1, thus 1\n2 (ft(w\u0302\n1 t ) + ft(w\u0302 2 t )) \u2264 ft(wt) +G\u03b4\nBy the Lipschitz property and \u2126 \u2282 R2B, we have for any w \u2208 \u2126 ft((1 \u2212 \u03be)w) \u2264 ft(w) +GR2\u03be Further for any w \u2208 (1\u2212 \u03be)\u2126,\n|ft(w) \u2212 f\u0302t(w)| = |ft(w)\u2212 Etft(w + \u03b4u)| \u2264 Et|ft(w)\u2212 ft(w + \u03b4u)| \u2264 G\u03b4\nThen\nft(wt) \u2264 f\u0302t(wt)+G\u03b4, and f\u0302t((1\u2212\u03be)w\u2217t ) \u2264 ft((1\u2212\u03be)w\u2217t )+G\u03b4 Combining the above inequalities, we get\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 2 t )) + f\u0302t((1 \u2212 \u03be)w\u2217t ) \u2264 ft(wt) +G\u03b4 + ft((1 \u2212 \u03be)w\u2217t ) +G\u03b4 \u2264 ft(wt) + ft(w\u2217t ) +GR2\u03be + 2G\u03b4 \u2264 f\u0302t(wt) + ft(w\u2217t ) +GR2\u03be + 3G\u03b4\nAs a result, T\u2211\nt=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 2 t ))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n\u2264 T\u2211\nt=1\nf\u0302t(wt)\u2212 T\u2211\nt=1\nf\u0302t((1\u2212 \u03be)w\u2217t ) + 3TG\u03b4 + TGR2\u03be"}, {"heading": "C. Proof of Theorem 9", "text": "The proof follows similarly to the analysis in (Chiang et al., 2013). We present a series of Lemmas with some of the Lemmas\u2019 proof omitted due to that they are identical to that in (Chiang et al., 2013). To simply the presentation, we denote by O(1) any constant independent of T .\nLemma 11. T\u2211\ni=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 2 t ))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n\u2264 T\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft((1 \u2212 \u03be)w\u2217t ) +O(1)\nThe proof of this lemma is presented later.\nLemma 12. Let w\u0302\u2217t = (1\u2212 \u03be)w\u2217t . T\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft(w\u0302 \u2217 t ) \u2264\nT\u2211\nt=1\n\u2207ft(wt)\u22a4(wt \u2212 w\u0302\u2217t )\nThe lemma above follows the convexity of ft(w). Lemma 13. Let w\u0302\u2217t = (1\u2212 \u03be)w\u2217t .\nE\n[ T\u2211\nt=1\n\u2207ft(wt)\u22a4(wt \u2212 w\u0302\u2217t ) ]\n\u2264 E [ T\u2211\nt=1\ng\u22a4t (wt \u2212 w\u0302\u2217t ) ] +O(1)\nThe proof of the above lemma follows the same to the proof of Lemma 5 in (Chiang et al., 2013).\nLemma 14. Define\nSt = \u03b7t\u2016gt \u2212 gt\u22121\u201622 At = 1\n2\u03b7 \u2016wt \u2212 w\u0302\u2217t \u201622 \u2212\n1\n2\u03b7 \u2016wt+1 \u2212 w\u0302\u2217t \u201622\nCt = 1\n2 \u2016wt+1 \u2212 w\u0302t\u201622 +\n1\n2\u03b7 \u2016wt \u2212 w\u0302t\u201622\nThen T\u2211\nt=1\ng\u22a4t (wt \u2212 w\u0302\u2217t ) \u2264 T\u2211\nt=1\nSt +\nT\u2211\nt=1\nAt \u2212 T\u2211\nt=1\nCt\nThe above lemma is a result of the Lemma 4 in (Chiang et al., 2013). Combining the above lemmas, we\nhave\nTheorem 15.\nE\n[ T\u2211\ni=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 2 t ))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n]\n\u2264 E [ T\u2211\nt=1\nSt +\nT\u2211\nt=1\nAt \u2212 T\u2211\nt=1\nCt\n] +O(1)\nTo proceed we bound the three summation terms in the R.H.S..\nLemma 16. T\u2211\nt=1\nCt \u2265 1\n4\u03b7 E\n[ T\u2211\nt=1\n\u2016w\u0302t \u2212 w\u0302t+1\u201622\n] \u2212O(1)\nThis is the same to the Lemma 11 in (Chiang et al., 2013).\nLemma 17. T\u2211\nt=1\nAt \u2264 \u2016w1 \u2212 w\u0302\u22171\u201622\n2\u03b7\n+ 1\n\u03b7\nT\u2211\nt=1\n\u2016wt+1 \u2212 w\u0302\u2217t+1\u20162\u2016w\u0302\u2217t \u2212 w\u0302\u2217t+1\u20162\n\u2264 1 2\u03b7 (r2 + 2rV pT )\nThe proof follows similarly to that of Lemma 1.\nLemma 18. T\u2211\nt=1\nSt \u2264 4\u03b7d4V gT +4\u03b7d3L2 lnTE [ T\u2211\nt=1\n\u2016w\u0302t \u2212 w\u0302t+1\u201622\n] +O(1)\nwhere L is the smoothness parameter.\nThe lemma follows the Lemma 12 in (Chiang et al., 2013). Combining Lemma 16, 17 and Lemma 18, we have\nE\n[ T\u2211\nt=1\nSt +\nT\u2211\nt=1\nAt \u2212 T\u2211\nt=1\nCt ] \u2264 4\u03b7d4V gT + 1\n2\u03b7 (r2 + 2rV pT )\n+ 4\u03b7d3L2 lnTE\n[ T\u2211\nt=1\n\u2016w\u0302t \u2212 w\u0302t+1\u201622\n]\n\u2212 1 4\u03b7 E\n[ T\u2211\nt=1\n\u2016w\u0302t \u2212 w\u0302t+1\u201622\n] +O(1)\nSince \u03b7 \u2264 1 4d3/2L \u221a lnT , then\nE\n[ T\u2211\nt=1\nSt +\nT\u2211\nt=1\nAt \u2212 T\u2211\nt=1\nCt\n]\n\u2264 4\u03b7d4V gT + 1\n2\u03b7 (r2 + 2rV pT ) +O(1)\n\u2264 4\u03b7d4ST + 1\n2\u03b7 (r2 + 2rBT ) +O(1)\nThen by the value of \u03b7, we have\nE\n[ T\u2211\nt=1\nSt +\nT\u2211\nt=1\nAt \u2212 T\u2211\nt=1\nCt\n]\n\u2264 4\u03b7d4ST + 1\n2\u03b7 (r2 + 2rBT ) +O(1)\n\u2264 max { 2 \u221a 2 \u221a d4ST (r2 + 2rBT ), 4d 3/2L \u221a lnT (r2 + 2rBT ) }\n+O(1)\nTherefore,\nE\n[ T\u2211\ni=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 t 1))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n]\n\u2264 O ( max(d2 \u221a ST max(1, BT ), d 3/2 max(1, BT )) )\nC.1. Proof of Lemma 11\nFrom the Lipschitz property,\nft(w\u0302 1 t )\u2212 ft(wt) \u2264 G\u2016w\u03021t \u2212wt\u20162 \u2264 G\u03b4 ft(w\u0302 2 t )\u2212 ft(wt) \u2264 G\u2016w\u03022t \u2212wt\u20162 \u2264 G\u03b4\nThen T\u2211\ni=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 t 1))\u2212\nT\u2211\nt=1\nft(wt) \u2264 G\u03b4T\nTo proceed,\nft((1 \u2212 \u03be)w\u2217t ) \u2264 \u03beft(0) + (1\u2212 \u03be)ft(w\u2217t ) = ft(w \u2217 t ) + \u03be(ft(0)\u2212 ft(w\u2217t )) \u2264 ft(w\u2217t ) + \u03beG\u2016w\u2217t \u20162\n= ft(w \u2217 t ) + \u03beGR2\nThen T\u2211\nt=1\nft((1 \u2212 \u03be)w\u2217t )\u2212 T\u2211\nt=1\nft(w \u2217 t ) \u2264 \u03beGR2T\nThus T\u2211\ni=1\n1 2 (ft(w\u0302 1 t ) + ft(w\u0302 t 1))\u2212\nT\u2211\nt=1\nft(w \u2217 t )\n\u2264 T\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft((1\u2212 \u03be)w\u2217t ) +G\u03b4T + \u03beGR2T\n\u2264 T\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft((1\u2212 \u03be)w\u2217t ) +O(1)"}], "references": [{"title": "Optimal algorithms for online convex optimization with multi-point bandit feedback", "author": ["Agarwal", "Alekh", "Dekel", "Ofer", "Xiao", "Lin"], "venue": "In Proceedings of the 23rd Conference on Learning Theory (COLT),", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "Unified algorithms for online learning and competitive analysis", "author": ["Buchbinder", "Niv", "Chen", "Shahar", "Naor", "Joseph", "Shamir", "Ohad"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Buchbinder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buchbinder et al\\.", "year": 2012}, {"title": "A new look at shifting", "author": ["Cesa-Bianchi", "Nicol", "Gaillard", "Pierre", "Lugosi", "Gbor", "Stoltz", "Gilles"], "venue": "regret. CoRR,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Online optimization with gradual variations", "author": ["Chiang", "Chao-Kai", "Yang", "Tianbao", "Lee", "Chia-Jung", "Mahdavi", "Mehrdad", "Lu", "Chi-Jen", "Jin", "Rong", "Zhu", "Shenghuo"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Chiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2012}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Flaxman", "Abraham", "Kalai", "Adam Tauman", "McMahan", "H. Brendan"], "venue": "In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Online optimization in dynamic environments", "author": ["Hall", "Eric C", "Willett", "Rebecca M"], "venue": "CoRR, abs/1307.5944,", "citeRegEx": "Hall et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2013}, {"title": "Tracking the best expert", "author": ["Herbster", "Mark", "Warmuth", "Manfred K"], "venue": "Machine Learning,", "citeRegEx": "Herbster et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Herbster et al\\.", "year": 1998}, {"title": "Online optimization : Competing with dynamic comparators", "author": ["Jadbabaie", "Ali", "Rakhlin", "Alexander", "Shahrampour", "Shahin", "Sridharan", "Karthik"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Jadbabaie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jadbabaie et al\\.", "year": 2015}, {"title": "Regret bounded by gradual variation for online convex optimization", "author": ["Yang", "Tianbao", "Mahdavi", "Mehrdad", "Jin", "Rong", "Zhu", "Shenghuo"], "venue": "Machine Learning,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Recently, there emerges a surge of interest (Besbes et al., 2013; Hall & Willett, 2013; Jadbabaie et al., 2015) in the dynamic regret that compares the performance of online learning to a sequence of optimal solutions.", "startOffset": 44, "endOffset": 111}, {"referenceID": 2, "context": "We note that a regularity metric similar to the path variation (possibly measured in different norms) has been explored in shifting regret analysis (Herbster & Warmuth, 1998) and drifting regret analysis (Cesa-Bianchi et al., 2012; Buchbinder et al., 2012).", "startOffset": 204, "endOffset": 256}, {"referenceID": 1, "context": "We note that a regularity metric similar to the path variation (possibly measured in different norms) has been explored in shifting regret analysis (Herbster & Warmuth, 1998) and drifting regret analysis (Cesa-Bianchi et al., 2012; Buchbinder et al., 2012).", "startOffset": 204, "endOffset": 256}, {"referenceID": 2, "context": "In fact, a similar dynamic regret bound to \u221a V p T T has been established for online convex optimization over the simplex (Cesa-Bianchi et al., 2012), where the path variation is measured in l1 norm.", "startOffset": 122, "endOffset": 149}, {"referenceID": 1, "context": ", 2012; Buchbinder et al., 2012). The regret against the shifting experts was studied in tracking the best expert, where the best sequence of minimizers are assumed to change for a constant number of times. In drifting regret analysis, the constraint is relaxed to that the path variation is small. In fact, a similar dynamic regret bound to \u221a V p T T has been established for online convex optimization over the simplex (Cesa-Bianchi et al., 2012), where the path variation is measured in l1 norm. The present work focuses on OCO in the Euclidean space and considers noisy gradient feedback. A more general variation is considered in (Hall & Willett, 2013), where a sequence of (or a family of) dynamic models \u03c61, . . . , \u03c6T are revealed by the environment for the learner to predict the decision in the next step. Their variation is defined as V \u03c6 T = \u2211T\u22121 t=1 \u2016w\u2217 t+1 \u2212 \u03c6t(w\u2217 t )\u2016 for a sequence of comparators and their dynamic regret scales as V \u03c6 T \u221a T , which is worse than our bounds when \u03c6t(w) = w. There has been a different notion to measure the point-wise changes in the sequence of loss functions that measure the changes of two consecutive functions at any feasible points. For example, Besbes et al. (2013) considered the functional variation defined as", "startOffset": 8, "endOffset": 1222}, {"referenceID": 3, "context": "Another variation that measures point-wise difference between loss functions is the gradient variation introduced in (Chiang et al., 2012), which is defined as", "startOffset": 117, "endOffset": 138}, {"referenceID": 3, "context": "(5) The gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014).", "startOffset": 76, "endOffset": 143}, {"referenceID": 8, "context": "(5) The gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014).", "startOffset": 76, "endOffset": 143}, {"referenceID": 3, "context": "(5) The gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014). Recently, Jadbabaie et al. (2015) used the three variations and developed possibly better dynamic regret than using a single variation measure for nonstrongly convex loss functions.", "startOffset": 77, "endOffset": 179}, {"referenceID": 7, "context": ", 2013) (Jadbabaie et al., 2015) Loss function Feedback path variation functional variation three variations Lipschitz Full Information O(V p T ) O(V f T ) O(min( \u221a V p T V g T , (V g T ) T (V f T ) )) Lipschitz True Gradient N.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "ferent from (Jadbabaie et al., 2015), we consider the noisy gradient feedback (including the bandit feedback) and develop both upper bounds and lower bounds.", "startOffset": 12, "endOffset": 36}, {"referenceID": 7, "context": "It is notable that a similar upper bound of O(max(V f T , 1)) with the full information can be achieved (Jadbabaie et al., 2015).", "startOffset": 104, "endOffset": 128}, {"referenceID": 0, "context": "In order to have optimal dynamic regret bounds, we also consider two-point bandit setting and show that the previous algorithms in (Agarwal et al., 2010; Chiang et al., 2013) by adjusting the step size can achieve an O( \u221a V p T T ) dynamic regret for general Lipschitz continuous loss functions and an O(max( \u221a V g T V p T , V p T )) dynamic regret for smooth loss functions.", "startOffset": 131, "endOffset": 174}, {"referenceID": 4, "context": "For any wt \u2208 (1\u2212 \u03be)\u03a9 and any unit vector u, wt + \u03b4u \u2208 \u03a9 (Flaxman et al., 2005).", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "In particular, using one-point bandit feedback Flaxman et al. (2005) showed an O(T ) static regret bound, while Agarwal et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 0, "context": "(2005) showed an O(T ) static regret bound, while Agarwal et al. (2010) established an optimal static regret bound of O( \u221a T ) using two-point bandit feedback.", "startOffset": 50, "endOffset": 72}, {"referenceID": 0, "context": "(2005) showed an O(T ) static regret bound, while Agarwal et al. (2010) established an optimal static regret bound of O( \u221a T ) using two-point bandit feedback. Recently, Chiang et al. (2013) derived a variational static regret bound in the two-point bandit setting that depends on \u221a V g T where V g T is the gradient variation defined in Section 1.", "startOffset": 50, "endOffset": 191}], "year": 2016, "abstractText": "This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variationbased upper bounds of the dynamic regret under the true and noisy gradient feedback, which are optimal in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant\u2019s minimizers, to which we refer as path variation. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches what is achieved with full information. Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).", "creator": "LaTeX with hyperref package"}}}