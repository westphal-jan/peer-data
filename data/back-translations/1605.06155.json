{"id": "1605.06155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Inter-Battery Topic Representation Learning", "abstract": "In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach expands traditional topic models by learning factorized latent variable representation. Structured representation leads to a model that combines benefits traditionally associated with a discriminatory approach, such as feature selection, with those of a generative model, such as principled regularization and the ability to deal with missing data. Factorization occurs by presenting data in the form of aligned observation pairs as distinct views. This provides means of selecting a representation that separates issues that exist in both views from issues that are unique to a single view. This structured consolidation enables efficient and robust conclusions and provides a compact and efficient representation. Learning is done in a Bajesian way, maximizing a strict attachment to the likelihood of protocol. First, we illustrate the benefits of the model based on a synthetic network representation - both the classification and the extreme with the help of two separate data tasks.", "histories": [["v1", "Thu, 19 May 2016 21:44:12 GMT  (26105kb,D)", "https://arxiv.org/abs/1605.06155v1", null], ["v2", "Thu, 28 Jul 2016 10:08:40 GMT  (13982kb,D)", "http://arxiv.org/abs/1605.06155v2", "ECCV 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["cheng zhang", "hedvig kjellstrom", "carl henrik ek"], "accepted": false, "id": "1605.06155"}, "pdf": {"name": "1605.06155.pdf", "metadata": {"source": "CRF", "title": "Inter-Battery Topic Representation Learning", "authors": ["Cheng Zhang", "Hedvig Kjellstr\u00f6m", "Carl Henrik Ek"], "emails": ["chengz@kth.se", "hedvig@kth.se", "carlhenrik.ek@bristol.ac.uk"], "sections": [{"heading": null, "text": "Keywords: Factorized Representation, Topic Model, Multi-View Model, CNN Feature, Image Classification"}, {"heading": "1 Introduction", "text": "The representation of an image has a large impact on the ease and efficiency with which prediction can be performed. This has generated a huge interest in directly learning representation from data [1]. Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4]. Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7]. A topic model learns a set of topics, which are distributions over words and represents each document as a distribution over topics. In computer vision applications, a topic is a distribution over visual words, while a document is usually an image or a video. Due to its generative nature, the learned representation will\nThis research has been supported by the Swedish Research Council (VR) and Stiftelsen Promobilia.\nar X\niv :1\n60 5.\n06 15\n5v 2\n[ cs\n.L G\nprovide rich information about the structure of the data with high interpretability. It offers a highly compact representation and can handle incomplete data, to a high degree, in comparison to other types of representation methodologies. Topic models have been demonstrated with successful performance in many applications. Similar to other latent space probabilistic models, the topic distributions can easily be adapted with different distributions with respect to the types of the input data. In this paper, we will use a LDA model as our basic framework and apply an effective factorized representation learning scheme.\nModeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12]. For example, for object classification, separating the key features of the object from the intra-class variations and background information is key to the performance. The idea of factorized representation can be traced back to the early work of Tucker, \u2019An Inter-Battery Method of Factory Analysis\u2019 [8], hence, we name the model presented in this paper Inter-Battery Topic Model (IBTM).\nImagine a scenario in which we want to visually represent \u201da cup of coffee\u201d, illustrated in Figure 1 (a). Apart from a cup of coffee, such images commonly contain additional information that is not correlated to this labeling, e.g., the rose and the table in the upper image and the coffee beans in the lower image. One can think of the information that is common among all images of this class and thus correlated with the label, as the shared information. Images with a cup of coffee will share a set of \u201dcup of coffee\u201d topics between them. In addition, each image does also contain information that can be found only in a small share of the other images. This information can be thought of as private. Since the shared, but not the private, information should be employed in the estimation task (e.g., classification), it is highly beneficial to use a factorized model which represents the information needed for the tasks (shared topics) separately from the information that is not task related (private topics).\nA similar idea can be applied in the case when two different modalities of the data are available. A common case is images as one modality and the captions of the images as another, as shown in Figure 1 (b). In this scenario, commonly not all of the content in the image has its corresponding caption words; and not every word in the caption has its corresponding image patches. However, the important aspects of the scene or object depicted in the image are also described in the caption, and vice versa, the central aspects of the caption are those that correlate with what is seen in the image. Based on this idea, an ideal multi-modal representation should factorize out information that is present in both modalities (words describing central concepts, and image patches from the corresponding image areas) and represent it separately from information that is only present in one of the modalities (words not correlated with the image, and image patches in the background). Other modality examples include video and audio data captured at the same event, or optical flow and depth measurements extracted from a video stream.\nTo summarize, there is a strong need of modeling information in a factorized manner such that shared information and private information are represented separately. In our model, the shared part of the representation will capture the aspects of the data that are essential for the prediction (e.g., classification) task, leading to better performance. Additionally, inspecting the factorized latent representation gives a better understanding of the structure of the data, which is helpful in the design of domain-specific modeling and data collection.\nThe main contribution of this paper is a generative model, IBTM, for factorized representation learning, which efficiently factorizes essential information for an estimation task from information that is not task related (Section 3). This results in a very effective latent representation that can be used for predication tasks, such as classifications. IBTM is a general framework, which is applicable to both single- and multi-modal data, and can easily be adapted to data with different noise levels. To infer the latent variables of the model, we derive an efficient variational inference algorithm for IBTMs.\nWe evaluate our model in different experimental scenarios (Section 4). Firstly, we test IBTM with a synthetic dataset to illustrate how the learning is performed. Then we apply IBTM to state-of-the-art datasets in different scenarios to illustrate how different computer vision tasks benefit from IBTM. In a multi-modal setting, modality-specific information is factorized from cross-modality information (Section 4.2.1.2 and 4.2.2.2). In a uni-modal setting, instance-specific information is factorized from class-specific information (Section 4.2.1.1 and 4.2.2.1)."}, {"heading": "2 Related Work", "text": "With respect to the scope of this paper, we will summarize the related work mainly from two aspects: Topic Modeling and Factorized Models.\nTopic Modeling. Latent Dirichlet Allocation (LDA) [3] is the corner stone of topic modeling. In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words. In correspondence with other works in representation learning, the\nthemes can be interpreted as factors, components or dictionaries. The topic distribution for each document can be interpreted as factor weights or as a sparse and lowdimensional representation of the visual document. This has achieved promising results in different tasks and provided an intuitive understanding of the data structure. For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12]. Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6]. Being a generative model, it represents all information found in the data. However, for a specific task, only a portion of this information might be relevant. Extracting this information is essential for a good representation of the data. Hence a model that describes key information for the current task is beneficial.\nFactorized Models. The benefit of modeling the between-view variance separately from the within-view variance was first pointed out by Tucker [8]. It was rediscovered in machine learning in recent years by Ek et.al. [21]. Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities. For uni-modal scenarios, a special words topic model with a background distribution (SWB) [22] is one of the first studies on factorized representation using topic model for information retrieval tasks. In addition to topics, SWB uses a words distribution for each document to represent document specific information and a global word distribution for background information. As shown in the experiments, this text-specific factorization model is less suitable for computer vision tasks than IBTM. Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6]. The multi-modal factorized topic model which is based on correlated topic models [23] only provides an implicit link between different modalities with hierarchical Dirichlet priors since the factorization is enforced on the logistic normal prior, while VTT is only designed for the specific application.\nIn this paper, we present a general framework IBTM which models the topic structure in a factorized manner and can be applied to both uni- and multi-modal scenarios."}, {"heading": "3 Model", "text": "In this section, firstly, we will shortly review LDA [3] which IBTM is based on and then present the modeling details and inference of IBTM. Finally, we will describe how the latent representation can be used for classification tasks with which we evaluate our approach."}, {"heading": "3.1 Latent Dirichlet Allocation", "text": "LDA is a classical generative model which is able to model the latent structure of discrete data, for example, a bag of words representation of documents. Figure 2 (a) shows the graphic representation of LDA [3]. In LDA, the words (visual words) w are assumed to be generated by sampling from a per document topic distribution \u03b8 \u223c Dir(\u03b1) and a per topic words distribution \u03b2 \u223c Dir(\u03c3). The Dirichlet distribution is a natural choice as it is conjugate to multinomial distribution."}, {"heading": "3.2 Inter-Battery Topic Model", "text": "We propose the IBTM which models latent variables in a factorized manner for multi-view scenarios. Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20]. In the following, we present the more generalized IBTM, which can encode any number of views.\nTwo View IBTM. The two view version of IBTM, shown in Figure 2 (c), is an LDAbased model, in which each document contains two views and the words w and a from the two views are observed respectively. The two views can represent different types of data, such as two modalities, for example, image and caption as in Figure 1 (b); or two different descriptors for the same data, for example, SIFT and SURF features of the same image. They can also be two instances of the same class, for example, the two cups of coffee as in Figure 1 (a).\nThe key of IBTM is that we assume that topics are factorized. We do not force topics from two views to be matched completely since commonly each view has its view-specific information. Hence, in our model, a shared topic distribution between two\nviews for each document is separated from a private topic distribution for each view. As in Figure 2 (c), \u03b8 \u223cDir(\u03b1s) is the shared per topic distribution for each document, and correspondingly \u03b2 \u223c Dir(\u03c3s1) and \u03b7 \u223c Dir(\u03c3s2) are the per shared topic words distributions for each view. \u03ba \u223c Dir(\u03b1p1) and \u03bd \u223c Dir(\u03b1p2) are the private per document topic distributions for each view respectively, and correspondingly \u03b6 \u223c Dir(\u03c3p1) and \u03c4 \u223c Dir(\u03c3p2) are the private per topic word distributions for each view. To determine how much information is shared and how much information is private, partition parameters \u03c1 \u223c Beta(\u03b91) and \u00b5 \u223c Beta(\u03b92) are used for each view. In this case, to generate topic assignments for each word in each view, z and y are sampled as\nz\u223cMult([\u03c1 \u2217\u03b8 ;(1\u2212\u03c1)\u2217\u03ba])1, y\u223cMult([\u00b5 \u2217\u03b8 ;(1\u2212\u00b5)\u2217\u03bd ]). (1)\nIn the extreme cases, if \u03c1 = 0 and \u00b5 = 0, no information is shared between the two views and IBTM becomes two separated LDA. Otherwise, if \u03c1 = 1 and \u00b5 = 1, IBTM becomes a regular multi-modal topic model [7,20].\nThe whole IBTM is represented as:\np(\u03ba,\u03b8 ,\u03bd ,\u03c1,z,w,\u00b5,y,a,\u03b6 ,\u03b2 ,\u03b7 ,\u03c4|\u0398)\n= ( T \u220f t=1 p(\u03b6t |\u03c3p1) )( K \u220f k=1 p(\u03b2k |\u03c3s1) )( K \u220f k=1 p(\u03b7k |\u03c3s2) )( S \u220f s=1 p(\u03c4s|\u03c3p2) ) M \u220f m=1 ( p(\u03bam|\u03b1p1 )p(\u03b8m|\u03b1s)\np(\u03bdm|\u03b1p2 )p(\u03c1m|\u03b9)p(\u00b5m|\u03b92) ( N\n\u220f n=1\np(zmn|\u03bam,\u03b8m,\u03c1m)p(wmn|zmn,\u03b2 ,\u03b6 ) )( L\n\u220f l=1\np(yml |\u03bdm,\u03b8m,\u00b5m)p(aml |yml ,\u03b7 ,\u03c4) ))\nwhere \u0398 = {\u03b1p1 ,\u03b1s,\u03b1p2 ,\u03c3p1 ,\u03c3p2 ,\u03c3s1 ,\u03c3s2 , \u03b91, \u03b92}, and as in the graphic representation of IBTM in Figure 2 (b), the total number of documents is M; the number of words for each document is N and L for the first view and the second view respectively; the number of shared topics for both views is K; the number of private topics is T and S and the vocabulary size is V and W for the first view and the second view respectively.\nMean Field Variational Inference. Exact inference on this model is intractable due to the coupling between latent variables. Variational inference and sampling based methods are the two main groups of methods to perform approximate inference. Variational inference is known for its fast convergence and theoretical attractiveness. It can also be easily adapted to online requirements when facing big data or streaming data. Hence, in this paper, we use mean field variational inference for IBTM. The fully factorized variational distribution is assumed following the mean field manner:\nq(\u03ba,\u03b8 ,\u03bd ,\u03c1,z,\u00b5,y,\u03b6 ,\u03b2 ,\u03b7 ,\u03c4) = q(\u03ba)q(\u03b8)q(\u03bd)q(\u03c1)q(z)q(\u00b5)q(y)q(\u03b6 )q(\u03b2 )q(\u03b7)q(\u03c4) .\nFor each term above, the per document topic distributions are: q(\u03ba)=\u220fMm=1 q(\u03bam|\u03b4m) where \u03b4m \u2208 RT ; q(\u03b8) = \u220fMm=1 q(\u03b8m|\u03b3m) where \u03b3m \u2208 RK ; q(\u03bd) = \u220fMm=1 q(\u03bdm|\u03b5m) where \u03b5m \u2208 RS. The per word topic assignments are: q(z) = \u220fMm=1 \u220fNn=1 q(zmn|\u03c6mn) where \u03c6mn \u2208 RK+T such that the first K topics correspond to the shared topics and the last T topics correspond to the private topics; q(y) = \u220fMm=1 \u220f L l=1 q(ymn|\u03c7mn) where \u03c7mn \u2208 RK+S such that the first K topics correspond to the shared topics and the last S topics correspond to the private topics. The per document beta parameters are: q(\u03c1) =\n1 We use [A;B] to indicate matrix and vector concatenation\n\u220fMm=1 q(\u03c1m|rm) and q(\u00b5) = \u220fMm=1 q(\u00b5m|um). Finally, the per topic words distributions are: q(\u03b6 ) = \u220fTt=1 q(\u03b6t |\u03bet), q(\u03b2 ) = \u220fKk=1 q(\u03b2k|\u03bbk), q(\u03b7) = \u220fKk=1 q(\u03b7k|\u03c5k), q(\u03c4) = \u220fSs=1 q(\u03c4s|os). All the variational distributions follow the same family of distributions under the model assumption.\nApplying Jensen\u2019s inequality on the log likelihood of the model, we get the evidence lower bound (ELBO) L :\nlog p(w,a,Z|\u0398) = log \u222b p(w,a,Z|\u0398)q(Z)\nq(Z) dZ\u2265 E q [log p(w,a,Z|\u0398)]\u2212E q [logq(Z)] = L\nwhere Z= {\u03ba,\u03b8 ,\u03bd ,\u03c1,z,\u00b5,y,\u03b6 ,\u03b2 ,\u03b7 ,\u03c4}. By maximizing the ELBO, we get the update equations for the variational parameters. Only the ones that differ from LDA are presented here and derivation details are presented in the supplementary material. The update equations for the per document topic variational distribution are:\n\u03b4mt = \u03b1p1 + N\n\u2211 n=1\n\u03c6mn(K+t), \u03b3mk = \u03b1s + N\n\u2211 n=1\n\u03c6mnk + L\n\u2211 l=1\n\u03c7mlk , \u03b5ms = \u03b1p2 + L\n\u2211 l=1 \u03c7ml(K+s).\nThe update equation for the topic assignment in the first view is, when i\u2264 K:\n\u03c6mni = exp (( \u03a8(\u03b3mk)\u2212\u03a8( K\n\u2211 i=1\n\u03b3mi) ) + ( \u03a8(rm1)\u2212\u03a8(rm1 + rm2) ) + V\n\u2211 v=1 [wmn = v]\n( \u03a8(\u03bbiv)\u2212\u03a8( V\n\u2211 p=1 \u03bbip))\n) \u22121 ) ; 2\nand when i > K (as i = K + t):\n\u03c6mni = exp ((( \u03a8(\u03b4m(i\u2212K))\u2212\u03a8( T\n\u2211 p=1\n\u03b4mp) ) + ( \u03a8(rm2)\u2212\u03a8(rm1 + rm2) )) + V\n\u2211 v=1 [wmn = v]\n( \u03a8(\u03beiv)\u2212\u03a8( V\n\u2211 p=1 \u03beip))\n) \u22121 ) .\nThe update equations for the partition parameters are:\nrm1 = \u03b911 + N\n\u2211 n=1\nK\n\u2211 i=1\n\u03c6mni, rm2 = \u03b912 + N\n\u2211 n=1\nK+T \u2211 i=K \u03c6mni\nThe update for the second view follows equivalently. In the implementation, all global latent variables are initialized randomly except\nfor the shared per topic word distribution for the second modality, which is initialized uniformly. Due to the exchangeability of Dirichlet distribution which leads to rotational symmetry in the inference, initializing only one of the shared per topic word distribution randomly will increase the robustness of the model performance.\nGeneralized IBTM. It is straight-forward to generalize the two view IBTM to more views. The graphical representation of the generalized IBTM is shown in Figure 2 (b), where D is the total number of views. When D = 2, the models in Figure 2 (b) and 2 (c) are identical. The inference procedure can be adapted easily, since the updates of both topic assignments and partition parameters for each view follow the same form. The only difference is the per document shared topic variational distribution \u03b3mk = \u03b1s +\u2211Dd=1 \u2211 N(d) n=1 \u03c6 (d) mnk, where \u03c6 (d) mnk is the variational distribution of the topic assignment for the d-th view. 2 \u03a8(x) is the digamma function."}, {"heading": "3.3 Classification", "text": "Topic models provide a compact representation of the data. Both LDA and IBTM are unsupervised models and can be used for representation learning. The topic representation can be applied to different tasks, for example, image classification and image retrieval. Commonly, the whole topic representation will be employed for these tasks using LDA. Using IBTM, we will only rely on the shared topic space which represents the information essence. For image classification, we can simply apply a Support Vector Machine or softmax regression, taking the shared topic representation as the input. In our experimental evaluation, softmax regression is used. Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar. The minor improvement on the performance commonly comes with significant improvement of computation cost. Hence, we keep IBTM as a general framework for representation learning in an unsupervised manner."}, {"heading": "4 Experiments", "text": "In the experiments, firstly, we will evaluate the inference scheme and demonstrate the model behavior in a controlled manner in Section 4.1. Then we will use two benchmark datasets to evaluate the model behavior in real world scenarios in Section 4.2. For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization."}, {"heading": "4.1 Inference Evaluation using Synthetic Data", "text": "To test the inference performance, we generate a set of synthetic data using the model given different topic distributions \u03b6 ,\u03b2 ,\u03b7 ,\u03c4 and hyper-parameters for \u00b5,\u03c1,\u03ba,\u03b8 ,\u03bd . We generate 500 documents and each document has 100 words for each view. Given the\ngenerated data, a correct inference algorithm will be able to recover all the latent parameters. Figure 3 (a) shows the ground truth that we used for the per topic words distribution and the estimation of these latent variables using variational inference as described in Section 3.2. All the topics are correctly recovered. Due to the exchangeability of Dirichlet distribution, the estimation gives different order of the topics which is shown as row-wise exchanges in Figure 3(b). Figure 4 shows the parameter recovery for the partition parameters \u03c1 and \u00b5 which are generated from beta distribution. In the example, we use \u03b91 = (4,2) and \u03b92 = (1,1) as hyper-parameters for the beta distributions. In this setting, the first view is comparably clean; the second view is more noisy with big variations on the noise level among the data. As Figure 4 shows, almost all the partition parameters are correctly recovered."}, {"heading": "4.2 Performance Evaluation using Real-World Data", "text": "In this section, model performance is evaluated on real-world data. We present two experimental groups. The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification. We focus on the model performance where we investigate the distribution of topics and partition parameters. This will provide us with insight into the data structure and model behavior. Thereafter, we will present the classification performance. In these experiments, the classification results are obtained by applying softmax regression on the topic representation. In all experimental settings, the hyper-parameters for the per document topic distributions are set to \u03b1\u2217 = 0.8, the hyper-parameters for the per topic word distributions are set to \u03c3\u2217 = 0.6 and the hyper-parameters for the partition variables are set to \u03b9\u2217 = (5,5)3. We also perform experiments with different features, including offthe-shelf CNN-features from different layers and traditional SIFT features. Here, we only present the results using off-the-shelf CNN conv5 1 features as an example. We use the pre-trained Oxford VGG 16-layer CNN [27] for feature extraction. We create sliding windows in 3 scales with a 32 pixels step size to extract features, in the same manner as [28], and use K-means clustering to create a codebook and represent each image using a bag-of-visual-words. The vocabulary size is 1024. In general, the performance is robust when higher layers are used and when the vocabulary size is sufficient. More results using different features and different parameter settings are enclosed in the supplementary material.\n4.2.1 LabelMe Dataset. We use the LabelMe Dataset as in [18,25] for this group of the experiments. The LabelMe dataset contains 8 classes of 256\u00d7256 images: highway, inside city, coast, forest, tall buildings, street, open country and mountain. For each class, 200 images are randomly selected, half of which are used for training, and half of which are used for testing. This results in 800 training and 800 testing images. We perform the experiment in two different scenarios: Image and Image, where only images are available; and Image and Annotation, where different modalities are available.\n3 \u03b1\u2217 includes \u03b1p1 ,\u03b1s and \u03b1p2 . \u03c3\u2217 includes \u03c3p1 , \u03c3p2 , \u03c3s1 and \u03c3s2 . \u03b9\u2217 includes \u03b91 and \u03b92.\n4.2.1.1 Image and Image. In this experiment, we explore the scenario in which only one modality is available. We want to model essential information that captures the within class variations and explains away the instance specific variations. Both views are bag-of-CNN Conv5 1 feature representations of the image data. For each document, two training images from the same class are randomly paired. This represents the scenario as shown in the introductory Figure 1 (a). For the experimental results presented below, the numbers of topics are set to K = 15, T = 15, S = 15.4\nFigure 6 shows the histograms of the partition parameters in this case. Figure 6 (a) and (b) appear to be similar. This is according to intuition; since both views are images and they are randomly paired within the same classes, the statistical features are expected to be the same for both views. Most partition parameters are larger than 0.8, which means that large parts of information can be shared between images from the same class and that the CNN Conv5 1 features provide a good raw representation of the images. For image pairs with more variation that does not correlate with the image class, the partition parameters will be smaller. The essential information ratio varies among images which causes the partition parameters to vary among different images.\nFigure 5 visualizes the document distribution in different topic representation spaces. Figure 5 (a) shows that documents from different classes are well separated in the space defined by the shared topic representation. Figure 5 (b) and (c) show that documents from different classes are more mixed in the private topic spaces. Thus, the private information is used to explain instance specific features of a data point, but not class-specific features \u2013 these have been pushed into the shared space, according to the intention of the model. The variations in the private spaces are small due to the low noise ratio in the dataset. For the classification performance where only images are available, using IBTM with classification using only the shared representation leads to a classification rate of 89.75%. The classification results are summarized in Table 1. A standard LDA obtains better performance than PCA with the same number of dimensions. IBTM outperforms LDA with the same number of topics and can even obtain better results than using the full dimension (1024) of bag-of-Conv5 1 features together with linear SVM. While using SWB [22] 5, the performance is unsatisfactory for such computer vision tasks due to the noisy properties of images. The results show that IBTM is able to learn a factorized latent representation, which separates task-relevant variation in the data from variation that is less relevant for the task at hand, here classification.\n4.2.1.2 Image and Annotation. In this experiment, we explore the scenario when two different modalities are available for different views. We use the bag-of-Conv5 1 representation of images as the first view and the image annotations as the second view. The word counts for the annotations are scaled with the annotated region. For each document, 79 Conv5 1 features are extracted from the image view, and the sum over the word histogram for each view is normalized to 100. The number of topics is set to K = 15, T = 15, S = 15 in the experimental results presented here. Figure 7 shows the\n4 The performance is robust with a sufficient amount of topics, 15 or higher. More results with different numbers of topics are presented in the supplement. 5 We implemented SWB using Gibbs Sampling following the description in the paper [22]. The parameter settings are the same as in [22]. Linear SVM is used for classification using the topic representation from SWB. More analysis using SWB is presented in the supplementary material of this paper.\nhistograms of the partition parameters \u03c1 and \u00b5 for the two views respectively. Figure 7 (b) shows that the partition parameters are more concentrated around large values compared to Figure 7 (a), which indicate that most annotation information is more essential. This is consistent with the intuition of the relative noise levels in image vs annotation data.\nFigure 8 shows the distribution of documents using different topic representations. As in the previous experiment, documents from different classes are well separated in the shared topic representation and are more mixed in the private topic representations. Table 2 summarizes the classification performance.6 IBTM is able to outperform other methods with a performance of 89.38% even when only images are available for testing. When both modalities are available, the performance goes up to 95%, while ideal classification by humans for this dataset is reported to be 90% in [24].\n4.2.2 Leeds Butterfly Dataset. In this section, the Leeds butterfly dataset [26] is used to evaluate the IBTM model on a fine-grained classification task. This dataset contains 10 classes of butterfly images collected from Google image search, both the original images with cluttered background and segmentation masks for the butterflies are provided in the dataset. For each class, 55 to 100 images have been collected and there are 832 images in total. In this experiment, 30 images are randomly selected from each class for training and the remaining 532 images are used for testing. Similarly to above, we perform the experiment in two different scenarios: Image and Image, where only the natural images with cluttered backgrounds are available; and Image and Segmentation, where one modality is the natural image and the other modality is the segmented image.\n4.2.2.1 Image and Image. In this experiment, we use only the natural images to evaluate the model performance in the uni-modal scenario. The experimental setting is similar to Section 4.2.1.1, where two images from the same class are paired randomly. K = 15, T = 3 and S = 3 are used for the results presented here. The histograms in Figure 10 are to the previous dataset, however, with smaller values. As natural images of butterflies have more background information that is not related to the class of the butterfly, while for the LabelMe dataset, almost the whole image has information contributing to the natural scene class.\nFigure 9 visualizes the image distribution in the different topic representations, where the shared topic representation separates images from different classes better\n6 The 0.65% difference of Full SVM performance in Table 1 and Table 2 were due to different random data partitions.\nthan the private ones. Table 3 summarizes the classification performance for this dataset. There \u201dII IBTM 15\u201d shows the result of IBTM using only natural images, which obtains the highest performance 95.86% in this uni-modality setting with only 15 topics.\n4.2.2.2 Image and Segmented Image. In this experimental setting, natural images and segmented images are used as two different views for training to demonstrate the multi-\n7 Learning Models for Object Recognition from Natural Language Descriptions (NLD) trained a classification model based on text descriptors. All images are tested to use visual information to extract attributes to fit the text template for testing. The experiment setting is different from our experiments. However, we include the result from the original paper for completeness.\n\u22120.5\n0\n0.5\n1\n\u22121\n\u22120.5\n0\n0.5\n1 \u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\nTheta\n(a) \u03b8 , Shared\n\u22125 0\n5 10\n15\nx 10\u22123\n\u22121\n\u22120.5\n0\n0.5\n1\nx 10\u221218\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\nx 10\u221235\n1nd Private\n(b) \u03ba , Private\n\u22120.2 0\n0.2 0.4\n0.6 0.8\n\u22120.5\n0\n0.5\n1 \u22124\n\u22122\n0\n2\n4\n6\n8\n10\nx 10\u22123\n2nd Private\n(c) \u03bd , Private\nFig. 12. Visualization of the shared topic representation (\u03b8 ) and private topic representations (\u03ba and \u03bd) for experiments on the Leeds Butterfly dataset using images paired with their segmentation masks. The documents of different classes are colored differently and the plots show the first three principal components after applying PCA on the per document topic distributions for all the training data.\nmodality scenario. The segmented images are used as the first view and the natural images are used as the second view. Since the model is symmetric, the order of the views has no impact on the model. Figure 11 shows the histogram of the partition parameter. It is apparent that the partition parameters of the segmented images are more concentrated around the large values. Thus, the model has learned that the segmented images contain more relevant information. This is consistent with human intuition. Figure 12 shows the topic distribution using shared and private latent representations where the shared topic representations for different classes are naturally separated. Classification performance is summarized in Table 3. SWB performs better with this dataset than with the LabelMe dataset. The reason for this is probably that the visual words here are less noisy than in LabelMe. \u201dIS IBTM15\u201d denotes the performance of testing with only natural images and \u201dIS IBTM15\u201d shows the performance of testing with both natural images and their segmentation. We can see that IBTM performs better than other methods even if only natural images are available for testing. With the segmentation, the performance is almost ideal."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a different variant of the topic model IBTM with a factored latent representation. It is able to model shared information and private information using different views which has been proven to be beneficial for different computer vision tasks. Experimental results show that IBTM can effectively encode the taskrelevant information. Using this representation, the state-of-the-art results are achieved in different experimental scenarios.\nIn this paper, the focus lay on exploring the concept of factorized representations and the experiments were centered around two view scenarios. In future work, we plan to evaluate the performance of IBTM by using any number of views and in different scenarios such as cue-integration. In the end, efficient inference algorithms are the key for probabilistic graphic models in general. In this paper, we used variational inference in a batch manner. In the future, more efficient and robust inference algorithms [29,30] can be explored."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "PAMI 35(8)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic Principal Component Analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society 61(3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR 3", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Gaussian Process Latent Variable Models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "NIPS.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR. Volume 2., IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning tags from unsegemented videos of multiple human actions", "author": ["T.M. Hospedales", "S.G. Gong", "T. Xiang"], "venue": "ICDM.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextual Modeling with Labeled Multi-LDA", "author": ["C. Zhang", "D. Song", "H. Kjellstrom"], "venue": "IROS, Tokyo, Japan", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "An Inter-Battery Method of Factory Analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika 23", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1958}, {"title": "Manifold Relevance Determination", "author": ["A. Damianou", "H.C. Ek", "M. Titsias", "N.D. Lawrence"], "venue": "ICML.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Factorized Topic Models", "author": ["C. Zhang", "C.H. Ek", "A. Damianou", "H. Kjellstrom"], "venue": "ICLR.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Factorized multi-modal topic model", "author": ["S. Virtanen", "Y. Jia", "A. Klami", "T. Darrell"], "venue": "arXiv preprint arXiv:1210.4920", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "How to Supervise Topic Models", "author": ["C. Zhang", "H. Kjellstrom"], "venue": "ECCV workshop on Graphical Models in Computer Vision.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised Topic Models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "arXiv preprint arXiv:1003.0783", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised hierarchical Dirichlet processes with variational inference", "author": ["C. Zhang", "C.H. Ek", "X. Gratal", "F.T. Pokorny", "H. Kjellstr\u00f6m"], "venue": "ICCV workshop on Inference for probabilistic graphical models.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "DiscLDA: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "NIPS.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "ICML.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Gibbs max-margin supervised topic models with fast sampling algorithms", "author": ["J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": "ICML.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D. Blei", "L. Fei-Fei"], "venue": "CVPR, IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Max-margin Latent Dirichlet Allocation for Image Classification and Annotations", "author": ["Y. Wang", "G. Mori"], "venue": "BMVC.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling annotated data", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "International Conference on Research and Development in Information Retrieval, ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Ambiguity modeling in latent spaces", "author": ["C.H. Ek", "J. Rihan", "P. Torr", "G. Rogez", "N. Lawrence"], "venue": "Machine learning for multimodal interaction. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling general and specific aspects of documents with a probabilistic topic model", "author": ["C. Chemudugunta", "P. Smyth", "M. Steyvers"], "venue": "NIPS. Volume 19.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Correlated topic models", "author": ["D. Blei", "J. Lafferty"], "venue": "NIPS. Volume 18.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Objects as attributes for scene classification", "author": ["L.J. Li", "H. Su", "Y. Lim", "L. Fei-Fei"], "venue": "Trends and Topics in Computer Vision. Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Topic Modeling of Multimodal Data: An Autoregressive Approach", "author": ["Y. Zheng", "Y.J. Zhang", "H. Larochelle"], "venue": "CVPR.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning models for object recognition from natural language descriptions", "author": ["J. Wang", "K. Markert", "M. Everingham"], "venue": "BMVC.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "ECCV, Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Divergence measures and message passing", "author": ["T.P. Minka"], "venue": "Microsoft Research Technical Report.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Structured stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei"], "venue": "AISTATS.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This has generated a huge interest in directly learning representation from data [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 3, "context": "Generative models for representation learning treat the desired representation as an unobserved latent variable [2,3,4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 5, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 6, "context": "Topic models, which are generally a group of generative models based on Latent Dirichlet Allocation (LDA) [3], have successfully been applied for learning representations that are suitable for computer vision tasks [5,6,7].", "startOffset": 215, "endOffset": 222}, {"referenceID": 5, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 7, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 8, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 9, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 10, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 11, "context": "Modeling the essence of the information among all sources of information for a particular task has been shown to offer high interpretability and better performance [6,8,9,10,11,12].", "startOffset": 164, "endOffset": 180}, {"referenceID": 7, "context": "The idea of factorized representation can be traced back to the early work of Tucker, \u2019An Inter-Battery Method of Factory Analysis\u2019 [8], hence, we name the model presented in this paper Inter-Battery Topic Model (IBTM).", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Latent Dirichlet Allocation (LDA) [3] is the corner stone of topic modeling.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words.", "startOffset": 25, "endOffset": 32}, {"referenceID": 5, "context": "In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words.", "startOffset": 25, "endOffset": 32}, {"referenceID": 6, "context": "In computer vision tasks [5,6,7], topic modeling assumes that each visual document is generated by selecting different themes while the themes are distributions over visual words.", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 13, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 14, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 15, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 16, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 113, "endOffset": 129}, {"referenceID": 11, "context": "For computer vision tasks, topic modeling has been used for classification, either with supervision in the model [13,14,15,16,17] or by learning the topic representation in an unsupervised manner and applying standard classifiers such as softmax regression on the latent topic representation [12].", "startOffset": 292, "endOffset": 296}, {"referenceID": 10, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 17, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 18, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 19, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 170, "endOffset": 183}, {"referenceID": 6, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 222, "endOffset": 225}, {"referenceID": 5, "context": "Another interesting direction using topic modeling in computer vision is the multi-modal extension of topic models; it has been applied to tasks such as image annotation [11,18,19,20], contextual action/object recognition [7] and video tagging [6].", "startOffset": 244, "endOffset": 247}, {"referenceID": 7, "context": "The benefit of modeling the between-view variance separately from the within-view variance was first pointed out by Tucker [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 152, "endOffset": 162}, {"referenceID": 11, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 152, "endOffset": 162}, {"referenceID": 21, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 152, "endOffset": 162}, {"referenceID": 5, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 234, "endOffset": 242}, {"referenceID": 8, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 234, "endOffset": 242}, {"referenceID": 20, "context": "Recent research in latent structure models has also shown that modeling information in a factorized manner is advantageous for both uni-modal scenarios [10,12,22], in which only one type of data is available and multi-modal scenarios [6,9,21], in which different views correspond to different modalities.", "startOffset": 234, "endOffset": 242}, {"referenceID": 21, "context": "For uni-modal scenarios, a special words topic model with a background distribution (SWB) [22] is one of the first studies on factorized representation using topic model for information retrieval tasks.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 72, "endOffset": 78}, {"referenceID": 10, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 72, "endOffset": 78}, {"referenceID": 10, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 126, "endOffset": 130}, {"referenceID": 5, "context": "Works that apply such a factorized scheme on multi-modal topic modeling [6,11] include the multi-modal factorized topic model [11] and Video Tags and Topics Model (VTT) [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 22, "context": "The multi-modal factorized topic model which is based on correlated topic models [23] only provides an implicit link between different modalities with hierarchical Dirichlet priors since the factorization is enforced on the logistic normal prior, while VTT is only designed for the specific application.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "In this section, firstly, we will shortly review LDA [3] which IBTM is based on and then present the modeling details and inference of IBTM.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Figure 2 (a) shows the graphic representation of LDA [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 7, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 17, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 19, "context": "Firstly, we will explain how to apply IBTM to a two view scenario such that it easily can be compared to other models [7,8,18,20].", "startOffset": 118, "endOffset": 129}, {"referenceID": 6, "context": "Otherwise, if \u03c1 = 1 and \u03bc = 1, IBTM becomes a regular multi-modal topic model [7,20].", "startOffset": 78, "endOffset": 84}, {"referenceID": 19, "context": "Otherwise, if \u03c1 = 1 and \u03bc = 1, IBTM becomes a regular multi-modal topic model [7,20].", "startOffset": 78, "endOffset": 84}, {"referenceID": 12, "context": "Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar.", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar.", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "Although there are different types of supervised topic models [13,16] where class label is encoded as part of the model, the work in [12] shows that the performance on computer vision classification tasks using supervised model and unsupervised model with an additional classifier is similar.", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 89, "endOffset": 99}, {"referenceID": 23, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 89, "endOffset": 99}, {"referenceID": 24, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 89, "endOffset": 99}, {"referenceID": 25, "context": "For this purpose, we use the LabelMe natural scene data for natural scene classification [18,24,25] and the Leeds butterfly dataset [26] for fine-grained categorization.", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 23, "context": "The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 25, "context": "The first one is using the LabelMe natural scene dataset [18,24] and the second one is using the Leeds butterfly dataset [26] for fine-grained classification.", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "We use the pre-trained Oxford VGG 16-layer CNN [27] for feature extraction.", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "We create sliding windows in 3 scales with a 32 pixels step size to extract features, in the same manner as [28], and use K-means clustering to create a codebook and represent each image using a bag-of-visual-words.", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "We use the LabelMe Dataset as in [18,25] for this group of the experiments.", "startOffset": 33, "endOffset": 40}, {"referenceID": 24, "context": "We use the LabelMe Dataset as in [18,25] for this group of the experiments.", "startOffset": 33, "endOffset": 40}, {"referenceID": 21, "context": "While using SWB [22] 5, the performance is unsatisfactory for such computer vision tasks due to the noisy properties of images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "5 We implemented SWB using Gibbs Sampling following the description in the paper [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "The parameter settings are the same as in [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "DocNADE [25] SupDocNADE[25] Full SVM PCA15 SVM LDA15 SWB15 [22] IBTM15 81.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "DocNADE [25] SupDocNADE[25] Full SVM PCA15 SVM LDA15 SWB15 [22] IBTM15 81.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "DocNADE [25] SupDocNADE[25] Full SVM PCA15 SVM LDA15 SWB15 [22] IBTM15 81.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "Full SVM PCA 15 LDA15 SWB 2V [22] IBTM15 1V IBTM15 2V 87.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "When both modalities are available, the performance goes up to 95%, while ideal classification by humans for this dataset is reported to be 90% in [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "In this section, the Leeds butterfly dataset [26] is used to evaluate the IBTM model on a fine-grained classification task.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "NLD[26] 7 Full SVM PCA 15 II SWB15 [22] IS SWB15 [22] LDA15 II IBTM15 IS IBTM 1V IS IBTM 2V 56.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "NLD[26] 7 Full SVM PCA 15 II SWB15 [22] IS SWB15 [22] LDA15 II IBTM15 IS IBTM 1V IS IBTM 2V 56.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "NLD[26] 7 Full SVM PCA 15 II SWB15 [22] IS SWB15 [22] LDA15 II IBTM15 IS IBTM 1V IS IBTM 2V 56.", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "In the future, more efficient and robust inference algorithms [29,30] can be explored.", "startOffset": 62, "endOffset": 69}, {"referenceID": 29, "context": "In the future, more efficient and robust inference algorithms [29,30] can be explored.", "startOffset": 62, "endOffset": 69}], "year": 2016, "abstractText": "In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uniand multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations.", "creator": "LaTeX with hyperref package"}}}