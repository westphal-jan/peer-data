{"id": "1605.06523", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "TensorLog: A Differentiable Deductive Database", "abstract": "Large Knowledge Base (KBs) are useful for many tasks, but it is unclear how this type of knowledge can be integrated into \"deep\" gradient-based learning systems. To solve this problem, we describe a probabilistic deductive database called TensorLog, in which the reasoning is done through a differentiated process. In TensorLog, each clause in a logical theory is first converted into a certain type of factor diagram. Subsequently, for each type of query to the factor graph, the steps required to perform faith propagation (BP) are \"unrolled\" into a function that is differentiable. We show that these functions can be put together recursively to perform conclusions in non-trivial logical theories that contain several related clauses and predicates. Both the compilation and the inference in TensorLog are efficient: the compilation is linear in theory size and evidence, and the sequence is linear in the number of data base steps.", "histories": [["v1", "Fri, 20 May 2016 20:10:46 GMT  (69kb,D)", "https://arxiv.org/abs/1605.06523v1", null], ["v2", "Tue, 19 Jul 2016 21:03:55 GMT  (152kb,D)", "http://arxiv.org/abs/1605.06523v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB cs.LG", "authors": ["william w cohen"], "accepted": false, "id": "1605.06523"}, "pdf": {"name": "1605.06523.pdf", "metadata": {"source": "CRF", "title": "TensorLog: A Differentiable Deductive Database", "authors": ["William W. Cohen"], "emails": ["wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Large knowledge bases (KBs) have proven useful in many tasks, but it is unclear how to integrate this sort of knowledge into \u201cdeep\u201d gradient-based learning systems. Motivated by this, we describe a probabilistic deductive database (PrDDB) system in which reasoning is performed by a differentiable process. In addition to enabling novel gradient-based learning algorithms for PrDDBs, this approach could potentially enable tight integration of logical reasoning into deep learners (or conversely, of deep learning into reasoning systems.\nIn a traditional deductive database (DDB), a database DB with a theory T together define a set of facts f1, . . . , fn which can be derived by accessing the database and reasoning using T . As an example, Figure 1 contains a small theory and an associated database. End users can test to see if a fact f is derivable, or retrieve all derivable facts that match some query: e.g., one could test if f = uncle(joe,bob) is derivable in the sample database, or find all values of Y such that uncle(joe,Y) holds. A probabilistic DDB is a \u201csoft\u201d extension of a DDB, where derived facts f have a numeric confidence, typically based on augmenting DB with a set of parameters \u0398. In many existing PrDDB models computation of confidences is computationally expensive, and often not be conducive to learning the parametersb \u0398.\nHere we describe a probabilistic deductive database called TensorLog in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph, in which each logical variable appearing in the clause is associated with a random variable in the factor graph, and each literal is associated with a factor (as shown in Figure 2). Then, for each type of query to the factor graph, the message-passing steps required to perform BP are \u201cunrolled\u201d into a function, which is differentiable. Each function will answer queries for a particular combination of evidence variables and query variables in the factor graph, which in turn corresponds to logical queries in a particular syntactic form. We also show how these functions can be composed\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n06 52\n3v 2\n[ cs\n.A I]\n1 9\nJu l 2\n01 6\nrecursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates.\nIn TensorLog, compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. Most importantly, inference is also differentiable, enabling gradient-based parameter learning. Formally, we can show that TensorLog subsumes some prior probabilistic logic programming models, including several variants of stochastic logic programs (SLPs) [3, 17], and approximates others [9, 5].\nBelow, we first present background material, then introduce our main results for differentiable inference, We then discuss related work, in particular the relationship between TensorLog and existing probabilistic logics, present experimental results, and conclude."}, {"heading": "2 Background: Deductive and Probabilistic DBs", "text": "To begin, we review the definition for an ordinary DDB, an example of which is in Figure 1. A database, DB, is a set {f1, . . . , fN} of ground facts. We focus here on DB relations which are unary or binary (e.g., from a \u201cknowledge graph\u201d), hence, facts will be written as p(a, b) or q(c) where p and q are predicate symbols, and a, b, c are constants from a fixed domain C. A theory, T , is a set of function-free Horn clauses. Clauses are written A:-B1, . . . , Bk, where A is called the head of the clause, B1, . . . , Bk is the body, and A and the Bi\u2019s are called literals. Literals must be of the form q(X), p(X,Y ), p(c, Y ), or p(X, c), where X and Y are logical variables, and c is a database constant.\nClauses can be understood as logical implications. Let \u03c3 be a substitution, i.e., a mapping from logical variables to constants in C, and let \u03c3(L) be the result of replacing all logical variables X in the literal L with \u03c3(X). A set of tuples S is deductively closed with respect to the clause A \u2190 B1, . . . , Bk iff for all substitutions \u03c3, either \u03c3(A) \u2208 S or \u2203Bi : \u03c3(Bi) 6\u2208 S. For example, if S contains the facts of Figure 1, S is not deductively closed with respect to the clause 1 unless it also contains uncle(chip,liam) and uncle(chip,dave). The least model for a pair DB, T , written Model(DB, T ), is the smallest superset of DB that is deductively closed with respect to every clause in T . In the usual DDB semantics, a ground fact f is considered \u201ctrue\u201d iff f \u2208 Model(DB, T ). To introduce \u201csoft\u201d computation into this model, we add a parameter vector \u0398 which associates each fact f \u2208 DB with a positive scalar \u03b8f (as shown in the example). The semantics of this parameter vary in different PrDDB models, but \u0398 will always define a distribution Pr(f |T ,DB,\u0398) over the facts in Model(T ,DB)."}, {"heading": "3 Differentiable soft reasoning", "text": "Numeric encoding of PrDDB\u2019s and queries. We will implement reasoning in PrDDB\u2019s by defining a series of numeric functions, each of finds answers to a particular family of queries. It will be convenient to encode the database numerically. We will assume all constants have been mapped to integers. For a constant c \u2208 C, we define uc to be a one-hot row-vector representation for c, i.e., a row vector of dimension |C| where u[c] = 1 and u[c\u2032] = 0 for c\u2032 6= C. We can also represent a binary predicate p by a sparse matrix Mp, where Mp[a, b] = \u03b8p(a,b) if p(a, b) \u2208 DB, and a unary predicate q as an analogous row vector vq . Note that Mp encodes information not only about the database facts in predicate p, but also about their parameter values.\nPrDDB\u2019s are commonly used test to see if a fact f is derivable, or retrieve all derivable facts that match some query: e.g., one could test if f = uncle(joe,bob) is derivable in the sample database, or\nfind all values of Y such that uncle(joe,Y) holds. We focus here on the latter type of query, which we call an argument-retrieval query. An argument-retrieval query Q is of the form p(c, Y ) or p(Y, c): we say that p(c, Y ) has an input-output mode of in,out and p(Y, c) has an input-output mode of out,in. For the sake of brevity, below we will assume below the mode in,out when possible, and abbreviate the two modes as io and io.\nThe response to a query p(c, Y ) is a distribution over possible substitutions for Y , encoded as a vector vY such that for all constants d \u2208 C, vY [d] = Pr(p(c, d)|T ,DB,\u0398). Alternatively (since often we care only about the relative scores of the possible answers), the system might instead return a conditional probability vector vY |c: if Up(c,Y ) is the set of facts f that \u201cmatch\u201d p(c, Y ), then vY |c[d] = Pr(f = p(c, d)|f \u2208 Up(c,Y ), T ,DB,\u0398). Since the ultimate goal of our reasoning system is to correctly answer queries using functions, we also introduce a notation for functions that answer particular types of queries: in particular, for a predicate symbol fpio denotes a query response function for all queries with predicate p and mode io, i.e., queries of the form p(c, Y ), when given a one-hot encoding of c, fpio returns the appropriate conditional probability vector:\nfpio(uc) \u2261 vY |X where \u2200d \u2208 C : vY |c[d] = Pr(f = p(c, d)|f \u2208 Up(c,Y ), T ,DB,\u0398) (1)\nand similarly for fpoi.\nSyntactic restrictions. Algorithmically it will be convenient to constrain the use of constants in clauses. We introduce a special DB predicate assign, which will be used only in literals of the form assign(W,c), which in turn will be treated as literals for a special unary predicate assign_c. Without loss of generality, we can now assume that constants only appear in assign literals. For instance, the clause 3 of Figure 1 would be rewritten as\nstatus(X,T):-assign_tired(T),child(X,W),infant(W). (2)\nWe will also introduce another special DB predicate any, where any(a, b) is conceptually true for any pair of constants a, b; however, as we show below, the matrix Many need not be explicitly stored. We also constrain clause heads to contain distinct variables which all appear also in the body.\nA factor graph for a one-clause program. We will start by considering a highly restricted class of theories T , namely programs containing only one non-recursive clause r that obeys the restrictions above. We build a factor graph Gr for r as follows: for each logical variable W in the body, there is a random variable W ; and for every literal q(Wi,Wj) in the body of the clause, there is a factor with potentials Mq linking variables Wi and Wj . Finally, if the factor graph is disconnected, we add any factors between the components until it is connected. Figure 2 gives examples. The variables appearing in the clause\u2019s head are starred.\nWe now argue that Gr imposes a valid distribution Pr(f |T ,DB,\u0398) over facts in Model(T ,DB). In Gr the variables are multinomials over C, the factors represent predicates and the graph Gr represents a distribution of possible bindings to the logical variables in the clause f , i.e., to possible substitutions \u03c3. Let W1, . . . ,Wm be the variables of Gr, and for each factor/edge e let pe(Wie ,Wje) be the literal\nassociated with it. In the distribution defined by Gr\nPr Gr\n(W1 = c1, . . . ,Wm = cm) = 1\nZ \u220f (ci,cj)\u2208edges e \u03c6e(ci, cj) = \u220f (ci,cj)\u2208edges e \u03b8pe(cie ,cje )\nRecall \u2200f, \u03b8f > 0, so if Pr(W1 = c1, . . . ,Wm = cm) > 0 then for each edge pe(cie , cje) \u2208 DB, and hence the substitution \u03c3 = {W1 = c1, . . . ,Wm = cm} makes the body of clause r true. The converse is also clearly true: so Gr defines a distribution over exactly those substitutions \u03c3 that make the the body of r true.\nBP over Gr can now be used to compute the conditional vectors f p io(uc) and f p oi(uc). For example to compute fpio(uc) for clause 1, we would set the message for the evidence variable X to uc, run BP, and read out as the value of f the marginal distribution for Y .\nHowever, we would like to do more: we would like to compute an explicit, differentiable, query response function, which computes fpio(uc). To do this we \u201cunroll\u201d the message-passing steps into a series of operations, following [6].\nFor completeness, we include in Figure 3 a sketch of the algorithm used in the current implementation of TensorLog, which makes the (strong) assumption that Gr is a tree. In the code, we found it convenient to extend the notion of input-output modes for a query, a variable X appearing in a literal L = p(X,Y ) in a clause body is an nominal input if it appears in the input position of the head, or any literal to the left of L in the body, and is an nomimal output otherwise. In Prolog a convention is that nominal inputs appear as the first argument of a predicate, and in TensorLog, if the user respects this convention, then \u201cforward\u201d message-passing steps use Mp rather than MTp w (reducing the cost of transposing large DB-derived matrices, since our message-passing schedule tries to maximize forward messages.) The code contains two mutually recursive routines, and is invoked by requesting a message from the output variable to a fictional output literal. The result will be to emit a series of operations, and return the name of a register that contains the unnormalized conditional probability vector for the output variable: e.g., for the sample clauses the functions returned are:\nr1 gr1io(~uc) = { v1,W = ucMparent; vW = v1,W ; v2,Y = vW Mbrother; vY = v2,Y ; return vY } r2 gr2io(~uc) = { v1,W = ucMaunt; vW = v1,W ; v2,Y = vW Mhusband; vY = v2,Y ; return vY } r3 gr3io(~uc) = { v2,W = ucMparent; v3,W = vinfant; W = v2,W \u25e6 v3,W ;\nv1,T = vassign_tired; v4,T = vW Many; T = v1,T \u25e6 v4,T ; return vT }\nHere we use grio(~uc) for the unnormalized version of the query response function build from Gr, i.e.,\nfpio(~uc) \u2261 g r io(~uc)/||g r io(~uc)||1\nwhere r is the one-clause theory defining p.\nSets of factor graphs for multi-clause programs. We now extend this idea to theories with many clauses. We first note that if there are several clauses with the same predicate symbol in the head, we simply sum the unnormalized query response functions: e.g., for the predicate cduncle, defined by\nrules r1 and r2, we can define guncleio = g r1 io + g r2 io\nand then re-normalize. This is equivalent to building a new factor graph G, which would be approximately \u222aiGri, together global input and output variables, and a factor that constrains the input variables of the Gri\u2019s to be equal, and a factor that constrains the output variable of G to be the sum of the outputs of the Gri\u2019s.\nA more complex situation is when the clauses for one predicate, p, use a second theory predicate q, in their body: for example, this would be the case if aunt was also defined in the theory, rather than the database. For a theory with no recursion, we can replace the message-passing operations vY = vXMq with the function call vY = gqio(vX), and likewise the operation vY = vXM T q with the function call vY = gqoi(vX). It can be shown that this is equivalent to taking the factor graph for q and \u201csplicing\u201d it into the graph for p.\nIt is also possible to allow function calls to recurse to a fixed maximum depth: we must simply add some sort of extra argument that tracks depth to the recursively-invoked gq functions, and make sure that gp returns an all-zeros vector (indicating no more proofs can be found) when the depth bound is exceeded. Currently this is implemented by marking learned functions g with the predicate q, a mode, and a depth argument d, and ensuring that function calls inside gpio,d to q always call the next-deeper version of the function for q, e.g., gqio,d+1.\nUncertain inference rules. Notice that \u0398 associates confidences with facts in the databases, not with clauses in the theory. To attach a probability to a clause, a standard trick is to introduce a special clause-specific fact, and add it to the clause body [9]. For example, a soft version of clause 3 could be re-written as\nstatus(X,tired):-assign(RuleId,c3),weighted(RuleId),child(W,X),infant(W)\nwhere the (parameterized) fact weighted(c3) appears inDB, and the constant c3 appears nowhere else in T . TensorLog supports some special syntax to make it easy to build rules with associated weights: for instance, status(X,tired) :- assign(C3,c3), weighted(C3), child(W,X), infant(W) can be written simply as status(X,tired) :- child(W,X), infant(W) {c3}.\nDiscussion. Collectively, the computation performed by TensorLog\u2019s functions are equivalent to computing a set of marginals over a particular factor graph G: specifically G would be formed by using the construction for multiple clauses with the same head (described above), and then splicing in the factor graphs of subpredicates. The unnormalized messages over this graph, and their functional equivalent, can be viewed implementing a first-order version of weighted model counting, a well-studied problem in satisfiability.\nComputationally, the algorithm we describe is quite efficient. Assuming the matrices Mp exist, the additional memory needed for the factor-graph Gr is linear in the size of the clause r, and hence the compilation to response functions is linear in the theory size and the number of steps of BP. For theories where every Gr is a tree, the number of message-passing steps is also linear. Message size is (by design) limited to |C|, and is often smaller due to sparsity. The current implementation of TensorLog includes many restrictions that could be relaxed: e.g., predicates must be unary or binary, only queries of the types discussed here are allowed, and every factor graphGr must be a tree. Matrix operations are implemented in the scipy sparse-matrix package, and the \u201cunrolling\u201d code performs a number of optimizations to the sequence in-line: one important one is to use the fact that vX \u25e6 (vY Many) = vX ||vY ||1 to avoid explicitly building Many."}, {"heading": "4 Related Work", "text": "Hybrid logical/neural systems. There is a long tradition of embedding logical expressions in neural networks for the purpose of learning, but generally this is done indirectly, by conversion of the logic to a boolean formula, rather than developing a differentiable theorem-proving mechanism, as considered here. Embedding logic may lead to a useful architecture [15] or regularizer [12].\nRecently [11] have proposed a differentiable theorem prover, in which a proof for an example is unrolled into a network. Their system includes representation-learning as a component, as well as a template-instantiation approach (similar to [18]), allowing structure learning as well. However,\npublished experiments with the system been limited to very small datasets. Another recent paper [1] describes a system in which non-logical but compositionally defined expressions are converted to neural components for question-answering tasks.\nExplicitly grounded probabilistic first-order languages. Many first-order probabilistic models are implemented by \u201cgrounding\u201d, i.e., conversion to a more traditional representation.1 For example, Markov logic networks (MLNs) are a widely-used probabilistic first-order model [10] in which a Bernoulli random variable is associated with each potential ground database fact (e.g., in the binary-predicate case, there would be a random variable for each possible p(a, b) where a and b are any facts in the database and p is any binary predicate) and each ground instance of a clause is a factor. The Markov field built by an MLN is hence of size O(|C|2) for binary predicates, which is much larger than the factor graphs used by TensorLog, which are of size linear in the size of the theory. In our experiments we compare to ProPPR, which has been elsewhere compared extensively to MLNs.\nInference on the Markov field can also be expensive, which motivated the development of probabilistic similarity logic (PSL), [2] a MLN variant which uses a more tractible hinge loss, as well as lifted relational neural networks [13], a recent model which grounds first-order theories to a neural network. However, any grounded model for a first-order theory can be very large, limiting the scalability of such techniques.\nProbabilistic deductive databases and tuple independence.\nTensorLog is superficially similar to the tuple independence model for PrDDB\u2019s [14], which use \u0398 to define a distribution, Pr(I|DB,\u0398), over \u201chard\u201d databases (aka interpretations) I . In particular, to generate I , each fact f \u2208 DB sampled by independent coin tosses, i.e., PrTupInd(I|DB,\u0398) \u2261\u220f\nt\u2208I \u03b8t \u00b7 \u220f\nt\u2208DB\u2212I(1\u2212 \u03b8t). The probability of a derived fact f is defined as follows, where |[\u00b7]| is a zero-one indicator function:\nPr TupInd (f |T ,DB,\u0398) \u2261 \u2211 I |[f \u2208 Model(I, T )]| \u00b7 Pr(I|DB,\u0398) (3)\nThere is a large literature (for surveys, see [14, 4]) on approaches to tractibly estimating Eq 3, which naively requires marginalizing over all 2|DB| interpretations. One approach, taken by the ProbLog system [5], relies on the notion of an explanation. An explanation E for f is a minimal interpretation that supports f : i.e., f \u2208 Model(T , E) but f 6\u2208 Model(T , E\u2032) for all E\u2032 \u2282 E. It is easy to show that if E\u2032\u2032 \u2283 E then f \u2208 Model(T , E\u2032\u2032); hence, the set Ex(f) of all explanations for f is a more concise representation of the interpretations that support f .\nUnder the tuple independence model, the marginal probability of drawing some interpretation I \u2287 E is simply \u2211\nI\u2287E \u220f f \u2032\u2208I \u03b8f \u2032 \u220f f \u2032\u2208DB\u2212I (1\u2212 \u03b8f \u2032) = \u220f f \u2032\u2208E \u03b8f \u2032\nwhile in TensorLog,\nPr TenLog\n(f) = 1\nZ gTenLog(f), where gTenLog(f) = \u2211 E\u2208Ex(f) \u220f f \u2032\u2208E \u03b8f \u2032\nSo TensorLog\u2019s score for a single-explanation fact is the same as under PrTupInd, but more generally only approximates Eq 3, since\nPr TupInd (f) = \u2211 I |[f \u2208 Model(I, T )]| \u00b7 Pr(I) = \u2211\nI:f\u2208Model(I,T )\n\u220f f \u2032\u2208I \u03b8f \u2032 \u220f f \u2032\u2208DB\u2212I (1\u2212 \u03b8f \u2032)\n= \u2211\nE\u2208Ex(I) \u2211 I\u2287E \u220f f \u2032\u2208I \u03b8f \u2032 6= \u2211 E\u2208Ex(f) \u220f f \u2032\u2208E \u03b8f \u2032 = gTenLog(f)\nthe inequality occurring because TensorLog overcounts interpretations I that are supersets of more than one explanation.\nThis approximation step is important to TensorLog\u2019s efficiency, however. Exact computation of probabilities in the tuple independence model are #P hard to compute [5] in the size of the set of\n1For a survey of such models see [7].\nexplanations, which as noted, can itself be exponentially large. A number of methods have been developed for approximating this computation, or performing it as efficiently as can be done\u2014for example, by grounding to a boolean formula and converting to a decision-tree like format that facilitates counting [14]. Below we experimentally compare inference times to ProbLog2, one system which adopts these semantics.\nStochastic logic programs and ProPPR. TensorLog is more closely related to stochastic logic programs (SLPs) [3]. In an SLP, a probabilistic process is associated with a top-down theorem-prover: i.e., each clause r used in a derivation has an assocated probability \u03b8r. Let N(r, E) be the number of times r was used in deriving the explanation E: then in SLPs, PrSLP(f) = 1Z \u2211 E\u2208Ex(f) \u220f r \u03b8 N(r,E) r . The same probability distribution can be generated by TensorLog if (1) for each rule r, the body of r is prefixed with the literals assign(RuleId,r),weighted(RuleId), where r is a unique identifier for the rule and (2) \u0398 is constructed so that \u03b8f = 1 for ordinary database facts f , and \u03b8weighted(r) = \u03b8 \u2032 r, where \u0398\u2032 is the parameters for a SLP.\nSLPs can be normalized or unnormalized; in normalized SLPs, \u0398 is defined so for each set of clauses Sp of clauses with the same predicate symbol p in the head, \u2211 r\u2208Sp \u03b8r = 1. TensorLog can represent both normalized and unnormalized SLPs (although clearly learning must be appropriately constrained to learn parameters for normalized SLPs.) Normalized SLPs generalize probabilistic context-free grammars, and unnormalized SLPs can express Bayesian networks or Markov random fields [3].\nProPPR [17] is a variant of SLPs in which (1) the stochastic proof-generation process is augmented with a reset, and (2) the transitional probabilities are based on a normalized soft-thresholded linear weighting of features. The first extension to SLPs can be easily modeled in TensorLog, but the second cannot: the equivalent of ProPPR\u2019s clause-specific features can be incorporated, but they are globally normalized, not locally normalized as in ProPPR.\nProPPR also includes an approximate grounding procedure which generates networks of size linear in m, \u03b1\u22121, \u22121, and where m is the number of training examples, \u03b1 is the reset parameter, deg itmax is the maximum degree of the proof graph, and is the pointwise error of the approximation. Asymptotic analysis suggests that ProPPR should be faster for very large database and small numbers of training examples (assuming moderate values of and \u03b1 are feasible to use), but that TensorLog should be faster with large numbers of training examples and moderate-sized databases."}, {"heading": "5 Experiments", "text": "We compared TensorLog\u2019s inference time with ProbLog2, a mature probabilistic logic programming system which implements the tuple independence semantics, on two inference problems described in [5]. One is a version of the \u201cfriends and smokers\u201d problem, a toy model of social influence. In [5] small graphs were artificially generated using a preferential attachment model, the details of which were not described; instead we used a small existing network dataset2 which displays preferential-attachment statistics. The inference times we report are for the same inference tasks, for a subset of 120 randomly-selected entities. In spite of querying six times as many entities, TensorLog is many times faster.\nWe also compare on a path-finding task, also described in [5], which is intended to test performance on deeply recursive tasks. The goal here is to compute fixed-depth transitive closure on a grid: in [5] a 16-by-16 grid was used, with a maximum path length of 10. Again TensorLog shows much faster performance, and better scalability3, as shown by run times on a larger 64-by-64 grid.\nThese results demonstrate that TensorLog\u2019s approximation to ProbLog2\u2019s semantics is efficient, but not that it is useful. To demonstrate that TensorLog can efficiently and usefully approximate deeply recursive concepts, we posed a learning task on the 16-by-16 grid, and trained TensorLog to approximate the distribution for this task. The dataset consists of 256 grid cells connected by 2116 edges, so there are 256 example queries of the form path(a,X) where a is a particular grid cell. We picked 1/3 of these queries as test, and the remainder as train, and trained so that that the single positive answer to the query path(a,X) is the extreme corner closest to a\u2014i.e., one of the corners\n2The Citeseer dataset from [8]. 3We set TensorLog\u2019s maximum depth to 10 for the 16-by-16 grid, and to 99 for the larger grid.\n(1,1), (1,16), (16,1) or (16,16). Training for 20 epochs brings the accuracy from to 0% to 96.5% (for test), and learning takes approximately 3 sec/epoch. After learning query times are still quite fast.\nWe note, however, that ProbLog2, in addition to implementing the full tuple-independence semantics, implements a much more expressive logic than considered here, including a large portion of full Prolog. In contrast TensorLog includes only a subset of Datalog.\nThe table also includes a visualization of the learned weights for a small 6x6 grid. For every pair of adjacent grid cells u, v, there are two weights to learn, one for the edge from u to v and one for its converse. For each weight pair, we show a single directed edge (the heavy blue squares are the arrows) colored by the magnitude of the difference.\nWe also compared experimentally with ProPPR on several tasks. One was a citation-matching task (from [17]), in which ProPPR was favorable compared to MLNs4. Motivated by recent comparisons between ProPPR and embedding-based approaches to knowledge-base completion [16], we also compared to ProPPR on six relation-prediction tasks5 involving two databases, Wordnet and FreeBase15k, a 15,000-entity subset of FreeBase, using rules from the (non-recursive) theory used in [16].\nIn all of these tasks parameters are learned on a separate training set. For TensorLog\u2019s learner, we optimized unregularized cross-entropy loss, using a fixed-rate gradient descent learner. We set the learning rate to 0.1, used no regularization, and used a fixed number of epochs (30), which approximately matched ProPPR\u2019s learning time.6 The parameters \u03b8f are simply \u201cclipped\u201d to prevent them becoming negative (as in a rectified linear unit) and we use softmax to convert the output of the gp functions to distributions. We used the default parameters for ProPPR\u2019s learning.\n4We replicated the experiments with the most recent version of ProPPR, obtaining a result slightly higher than the 2013 version\u2019s published AUC of 80.0\n5We chose this protocol since the current TensorLog implementation can only learn parameters for one target relation at a time.\n6Since the current TensorLog implementation is single-threaded we used only one thread for ProPPR as well.\nEncouragingly, the accuracy of the two systems after learning is comparable, even with TensorLog\u2019s rather simplistic learning scheme. ProPPR, of course, is not well suited to tight integration with deep learners."}, {"heading": "6 Concluding Remarks", "text": "Large knowledge bases (KBs) are useful in many tasks, but integrating this knowledge into deep learners is a challenge. To address this problem, we described a probabilistic deductive database, called TensorLog, in which reasoning is performed with a differentiable process. The current TensorLog prototype is limited in many respects: for instance, it is not multithreaded, and only the simplest learning algorithms have been tested. In spite of this, it appears to be comparable to more mature first-order probabilistic learners in learning performance and inference time\u2014while holding the promise of allowing large KBs to be tightly integrated with deep learning."}, {"heading": "Acknowledgements", "text": "Thanks to William Wang for providing some of the datasets used here; and to William Wang, Katie Mazaitis, and many other colleagues contributed with technical discussions and advice. The author is greatful to Google for financial support, and also to NSF for their support of his work via grants CCF-1414030 and IIS-1250956."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Probabilistic similarity logic", "author": ["Matthias Brocheler", "Lilyana Mihalkova", "Lise Getoor"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Parameter estimation in stochastic logic programs", "author": ["James Cussens"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Probabilistic inductive logic programming", "author": ["Luc De Raedt", "Kristian Kersting"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Inference and learning in probabilistic logic programs using weighted boolean formulas", "author": ["Daan Fierens", "Guy Van Den Broeck", "Joris Renkens", "Dimitar Shterionov", "Bernd Gutmann", "Ingo Thon", "Gerda Janssens", "Luc De Raedt"], "venue": "To appear in Theory and Practice of Logic Programming,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Approximation-aware dependency parsing by belief propagation", "author": ["Matthew R. Gormley", "Mark Dredze", "Jason Eisner"], "venue": "Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Lifted graphical models: a survey", "author": ["Angelika Kimmig", "Lilyana Mihalkova", "Lise Getoor"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Semi-supervised classification of network data using very few labels", "author": ["Frank Lin", "William W. Cohen"], "venue": "IEEE Computer Society,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The independent choice logic for modelling multiple agents under uncertainty", "author": ["David Poole"], "venue": "Artificial intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Learning knowledge base inference with neural theorem provers", "author": ["Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "In NAACL Workshop on Automated Knowledge Base Construction (AKBC),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Tim Rockt\u00e4schel", "Sameer Singh", "Sebastian Riedel"], "venue": "In Proc. of ACL/HLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Probabilistic databases", "author": ["Dan Suciu", "Dan Olteanu", "Christopher R\u00e9", "Christoph Koch"], "venue": "Synthesis Lectures on Data Management,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Refinement of approximate domain theories by knowledge-based artificial neural networks", "author": ["Geoffrey Towell", "Jude Shavlik", "Michiel Noordewier"], "venue": "In Proceedings of the Eighth National Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Learning first-order logic embeddings via matrix factorization", "author": ["William Yang Wang", "William W. Cohen"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Programming with personalized PageRank: a locally groundable first-order probabilistic logic", "author": ["William Yang Wang", "Kathryn Mazaitis", "William W Cohen"], "venue": "In Proceedings of the 22nd ACM International Conference on Conference on Information & Knowledge Management,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Structure learning via parameter learning", "author": ["William Yang Wang", "Kathryn Mazaitis", "William W Cohen"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Formally, we can show that TensorLog subsumes some prior probabilistic logic programming models, including several variants of stochastic logic programs (SLPs) [3, 17], and approximates others [9, 5].", "startOffset": 160, "endOffset": 167}, {"referenceID": 14, "context": "Formally, we can show that TensorLog subsumes some prior probabilistic logic programming models, including several variants of stochastic logic programs (SLPs) [3, 17], and approximates others [9, 5].", "startOffset": 160, "endOffset": 167}, {"referenceID": 8, "context": "Formally, we can show that TensorLog subsumes some prior probabilistic logic programming models, including several variants of stochastic logic programs (SLPs) [3, 17], and approximates others [9, 5].", "startOffset": 193, "endOffset": 199}, {"referenceID": 4, "context": "Formally, we can show that TensorLog subsumes some prior probabilistic logic programming models, including several variants of stochastic logic programs (SLPs) [3, 17], and approximates others [9, 5].", "startOffset": 193, "endOffset": 199}, {"referenceID": 5, "context": "To do this we \u201cunroll\u201d the message-passing steps into a series of operations, following [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "To attach a probability to a clause, a standard trick is to introduce a special clause-specific fact, and add it to the clause body [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "Embedding logic may lead to a useful architecture [15] or regularizer [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "Embedding logic may lead to a useful architecture [15] or regularizer [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "Recently [11] have proposed a differentiable theorem prover, in which a proof for an example is unrolled into a network.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "Their system includes representation-learning as a component, as well as a template-instantiation approach (similar to [18]), allowing structure learning as well.", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "Another recent paper [1] describes a system in which non-logical but compositionally defined expressions are converted to neural components for question-answering tasks.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Inference on the Markov field can also be expensive, which motivated the development of probabilistic similarity logic (PSL), [2] a MLN variant which uses a more tractible hinge loss, as well as lifted relational neural networks [13], a recent model which grounds first-order theories to a neural network.", "startOffset": 126, "endOffset": 129}, {"referenceID": 11, "context": "TensorLog is superficially similar to the tuple independence model for PrDDB\u2019s [14], which use \u0398 to define a distribution, Pr(I|DB,\u0398), over \u201chard\u201d databases (aka interpretations) I .", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "There is a large literature (for surveys, see [14, 4]) on approaches to tractibly estimating Eq 3, which naively requires marginalizing over all 2|DB| interpretations.", "startOffset": 46, "endOffset": 53}, {"referenceID": 3, "context": "There is a large literature (for surveys, see [14, 4]) on approaches to tractibly estimating Eq 3, which naively requires marginalizing over all 2|DB| interpretations.", "startOffset": 46, "endOffset": 53}, {"referenceID": 4, "context": "One approach, taken by the ProbLog system [5], relies on the notion of an explanation.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Exact computation of probabilities in the tuple independence model are #P hard to compute [5] in the size of the set of", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "For a survey of such models see [7].", "startOffset": 32, "endOffset": 35}, {"referenceID": 11, "context": "A number of methods have been developed for approximating this computation, or performing it as efficiently as can be done\u2014for example, by grounding to a boolean formula and converting to a decision-tree like format that facilitates counting [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 2, "context": "TensorLog is more closely related to stochastic logic programs (SLPs) [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": ") Normalized SLPs generalize probabilistic context-free grammars, and unnormalized SLPs can express Bayesian networks or Markov random fields [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "ProPPR [17] is a variant of SLPs in which (1) the stochastic proof-generation process is augmented with a reset, and (2) the transitional probabilities are based on a normalized soft-thresholded linear weighting of features.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "We compared TensorLog\u2019s inference time with ProbLog2, a mature probabilistic logic programming system which implements the tuple independence semantics, on two inference problems described in [5].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "In [5] small graphs were artificially generated using a preferential attachment model, the details of which were not described; instead we used a small existing network dataset2 which displays preferential-attachment statistics.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We also compare on a path-finding task, also described in [5], which is intended to test performance on deeply recursive tasks.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "The goal here is to compute fixed-depth transitive closure on a grid: in [5] a 16-by-16 grid was used, with a maximum path length of 10.", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "The Citeseer dataset from [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 14, "context": "One was a citation-matching task (from [17]), in which ProPPR was favorable compared to MLNs4.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Motivated by recent comparisons between ProPPR and embedding-based approaches to knowledge-base completion [16], we also compared to ProPPR on six relation-prediction tasks5 involving two databases, Wordnet and FreeBase15k, a 15,000-entity subset of FreeBase, using rules from the (non-recursive) theory used in [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Motivated by recent comparisons between ProPPR and embedding-based approaches to knowledge-base completion [16], we also compared to ProPPR on six relation-prediction tasks5 involving two databases, Wordnet and FreeBase15k, a 15,000-entity subset of FreeBase, using rules from the (non-recursive) theory used in [16].", "startOffset": 312, "endOffset": 316}], "year": 2016, "abstractText": "Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into \u201cdeep\u201d gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are \u201cunrolled\u201d into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.", "creator": "LaTeX with hyperref package"}}}