{"id": "1605.06715", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2016", "title": "Factored Temporal Sigmoid Belief Networks for Sequence Learning", "abstract": "The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). Transition matrices are further taken into account to reduce the number of parameters and improve generalization. If page information is not available, a general framework for semi-supervised learning is created based on the proposed model, enabling robust sequence classification. Experimental results show that the proposed approach achieves state-of-the-art prediction and classification performance based on sequential data and has the ability to synthesize sequences, with transitions and crossfades occurring in a controlled style.", "histories": [["v1", "Sun, 22 May 2016 00:17:31 GMT  (1511kb,D)", "http://arxiv.org/abs/1605.06715v1", "to appear in ICML 2016"]], "COMMENTS": "to appear in ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jiaming song", "zhe gan", "lawrence carin"], "accepted": true, "id": "1605.06715"}, "pdf": {"name": "1605.06715.pdf", "metadata": {"source": "META", "title": "Factored Temporal Sigmoid Belief Networks for Sequence Learning", "authors": ["Jiaming Song", "Zhe Gan", "Lawrence Carin"], "emails": ["JIAMING.TSONG@GMAIL.COM", "ZHE.GAN@DUKE.EDU", "LCARIN@DUKE.EDU"], "sections": [{"heading": "1. Introduction", "text": "The Restricted Boltzmann Machine (RBM) is a wellknown undirected generative model with state-of-the-art performance on various problems. It serves as a building block for many deep generative models, such as the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov & Hinton, 2009). Other variants of the RBM have been used for modeling discrete time series data, such as human motion (Taylor et al., 2006), videos (Sutskever et al., 2009), and weather prediction (Mittelman et al., 2014). Among these variants is the Conditional Restricted Boltzmann Machine\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\n(CRBM) (Taylor et al., 2006), where the hidden and visible states at the current time step are dependent on directed connections from observations at the last few time steps. Most RBM-based models use Contrastive Divergence (CD) (Hinton, 2002) for efficient learning.\nRecently, there has been a surging interest in deep directed generative models, with applications in both static and dynamic data. In particular, advances in variational methods (Mnih & Gregor, 2014; Kingma & Welling, 2014; Rezende et al., 2014) have yielded scalable and efficient learning and inference for such models, avoiding poor inference speed caused by the \u201cexplaining away\u201d effect (Hinton et al., 2006). One directed graphical model that is related to the RBM is the Sigmoid Belief Network (SBN) (Neal, 1992).\nThe Temporal Sigmoid Belief Network (TSBN)(Gan et al., 2015c) is a fully directed generative model for discretely sampled time-series data, defined by a sequential stack of SBNs. The hidden state for each SBN is inherited from the states of the previous SBNs in the sequence. The TSBN can be regarded as a generalization of Hidden Markov Models (HMM), with compact hidden state representations, or as a generalization of Linear Dynamical Systems (LDS), characterized by non-linear dynamics. The model can be utilized to analyze many kinds of data, e.g., binary, realvalued, and counts, and has demonstrated state-of-the-art performance in many tasks.\nHowever, the TSBN exhibits certain limitations, in that it does not discriminate different types of sequences when training, nor does it utilize (often available) side information during generation. For example, in the case of modeling human motion, the TSBN does not regard \u201cwalking\u201d and \u201crunning\u201d as different motions, and the style of generated motions is not controlled. To allow for conditional generation, we first propose a straightforward modification of the TSBN, introducing three-way tensors for weight transitions, from which weight matrices for the TSBN are extracted according to the side information provided. We then adopt ideas from Taylor & Hinton (2009), where the\nar X\niv :1\n60 5.\n06 71\n5v 1\n[ st\nat .M\nL ]\n2 2\nM ay\n2 01\nthree-way weight tensor is factored into the multiplication of matrices, effectively reducing the model parameters. In our case, factoring is not directly imposed on the tensor parameters but over the style-dependent transition matrices, which provides a more-compact representation of the transitions, where sequences with different attributes are modeled by a shared set of parameters. The model is able to capture the subtle similarities across these sequences, while still preserving discriminative attributes. Experiments show that the factored model yields better prediction performance than its non-factored counterpart.\nFor cases where side information is not available for the entire training set, we propose a framework for semisupervised learning, inspired by Kingma et al. (2014), to infer that information from observations (e.g., by using a classifier to infer the motion style), while simultaneously training a generative model.\nThe principal contributions of this paper are as follows: (i) A fully directed deep generative model for sequential data is developed, that permits fast conditional generation of synthetic data, and controlled transitioning and blending of different styles. (ii) A new factoring method is proposed, that reduces the number of parameters and improves performance across multiple sequences. (iii) A general framework for semi-supervised learning is constituted, allowing robust sequence classification. (iv) A new recognition model with factored parameters is utilized to perform scalable learning and inference."}, {"heading": "2. Model Formulation", "text": ""}, {"heading": "2.1. Temporal Sigmoid Belief Network", "text": "The Temporal Sigmoid Belief Network (TSBN) (Gan et al., 2015c) models uniformly sampled time series data of length T through a sequence of SBNs, such that at any given time step SBN biases depend on the states of SBNs in the previous time steps. Assume that each observation is real-valued, and the t-th time step is denoted vt \u2208 RM . The TSBN defines the joint probability of visible data V and hidden states H as\np\u03b8(V,H) = T\u220f t=1 p(ht|ht\u22121,vt\u22121) \u00b7 p(vt|ht,vt\u22121) (1)\nwhere V = [v1, . . . ,vT ], H = [h1, . . . ,hT ], and ht \u2208 {0, 1}J is the hidden state corresponding to time step t. Each conditional distribution in (1) is expressed as\np(hjt = 1|ht\u22121,vt\u22121) = \u03c3(h\u0303jt) (2) p(vt|ht,vt\u22121) = N (\u00b5t, diag(\u03c32t )) (3)\nh\u0303t = W1ht\u22121 + W3vt\u22121 + b (4)\n\u00b5t = W2ht + W4vt\u22121 + c (5)\nlog\u03c32t = W \u2032 2ht + W \u2032 4vt\u22121 + c \u2032 (6)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)), and diag(v) is the diagonal matrix whose diagonal entries are v. h0 and v0 are defined as zero vectors. The model parameters, \u03b8, are specified as W1 \u2208 RJ\u00d7J , W3 \u2208 RJ\u00d7M , {W2,W\u20322} \u2208 RM\u00d7J , {W4,W\u20324} \u2208 RM\u00d7M , b \u2208 RJ and {c, c\u2032} \u2208 RM .\nThe TSBN with W3 = 0 and {W4,W\u20324} = 0 is an HMM with an exponentially large state space. Specifically, in the HMM, each hidden state is represented as a one-hotencoded vector of length J , while the hidden states of the TSBN are encoded by length-J binary vectors, which can express 2J different states. Compared with the Temporal Restricted Boltzmann Machine (TRBM) (Sutskever & Hinton, 2007), the TSBN is fully directed, allowing fast sequence generation from the inferred model."}, {"heading": "2.2. Conditional Temporal SBN", "text": "We consider modeling multiple styles of time series data by exploiting additional side information, which can appear in the form of \u201cone-hot\u201d encoded vectors in the case of style labels, or real-valued vectors in other situations (e.g., the longitude and latitude information in the weather prediction task considered in Section 5.5). Let yt \u2208 RS denote the side information at time step t. The joint probability is now described as\np\u03b8(V,H|Y) = T\u220f t=1 p(ht|ht\u22121,vt\u22121,yt) \u00b7 p(vt|ht,vt\u22121,yt) (7)\nwhere Y = [y1, . . . ,yT ]. A straightforward method to incorporate side information is by modifying (4)-(6) to allow the weight parameters to be dependent on yt. Specifically,\nh\u0303t = W (y) 1 ht\u22121 + W (y) 3 vt\u22121 + b (y) (8)\n\u00b5t = W (y) 2 ht + W (y) 4 vt\u22121 + c (y) (9)\nlog\u03c32t = W \u2032(y) 2 ht + W \u2032(y) 4 vt\u22121 + c \u2032(y) (10)\nwhere b(y) = Byt, c(y) = Cyt; for i \u2208 {1, 2, 3, 4}, W\n(y) ijk = \u2211S s=1 W\u0302ijksyst, and W\u0302i is a three-way tensor, shown in Figure 1 (left).1 The model parameters are specified as W\u03021 \u2208 RJ\u00d7J\u00d7S , W\u03022 \u2208 RM\u00d7J\u00d7S , W\u03023 \u2208 RJ\u00d7M\u00d7S , W\u03024 \u2208 RM\u00d7M\u00d7S , B \u2208 RJ\u00d7S , and C \u2208 RM\u00d7S . When the side information are one-hot encoded vectors, this is equivalent to training one TSBN for each style of sequences. We name this model the Conditional Temporal Sigmoid Belief Network (CTSBN).\n1W \u2032(y) 2 and W \u2032(y) 4 are also similarly defined, and we omit them when discussing W(y)2 and W (y) 4 in the following sections. b(y), c(y) and W(y)ijk are dependent on t, but we choose to omit t for simplicity."}, {"heading": "2.3. Factoring Weight Parameters", "text": "While the CTSBN enables conditional generation, it has several disadvantages: (i) the number of parameters is proportional to S (recall that this is the number of different styles for the time-series data), which is prohibitive for large S; (ii) parameters for different attributes/styles are not shared (no sharing of \u201cstatistical strength\u201d), therefore the model fails to capture the underlying regularities between different data, resulting in poor generalization.\nTo remedy these drawbacks, we factor the weight matrices W\n(y) i defined in (8)-(10), for i \u2208 {1, 2, 3, 4}, as\nW(y) = Wa \u00b7 diag(Wbyt) \u00b7Wc (11)\nSuppose W(y) \u2208 RJ\u00d7M , then Wa \u2208 RJ\u00d7F , Wb \u2208 RF\u00d7S and Wc \u2208 RF\u00d7M , where F is the number of factors. Wa and Wc are shared among different styles, which capture the input-to-factor and factor-to-output relationships, respectively; the diagonal term, diag(Wbyt), models the unique factor-to-factor relationship for each style. If the number of factors is comparable to that of other terms (i.e., J and M ), this reduces the number of parameters needed for all the W(y) from J \u00b7M \u00b7 S to (J + M + S) \u00b7 F , or equivalently, O(N3) to O(N2). In practice, we factor all parameters except for W2 to improve generation by allowing a more flexible transition from ht to vt across multiple styles. 2 We call this model the Factored Conditional Temporal Sigmoid Belief Network (FCTSBN)."}, {"heading": "2.4. Deep Architecture with FCTSBN", "text": "The shallow model described above may be restrictive in terms of representational power. Therefore, we propose a\n2W2 is independent to the order n introduced in Section 5; when n is large, the number of parameters in W2 is relatively small, thus factoring W2 does not provide much advantage for reducing parameters.\ndeep architecture by adding stochastic hidden layers. We consider a deep FCTSBN with hidden layers h(`)t for t = 1 . . . T and ` = 1 . . . L, where we denote h(0)t = vt and h (L+1) t = 0 for conciseness. Each of the hidden layers h (`) t contains stochastic binary hidden variables, which is generated by p(h(`)t ) = \u220fJ(`) j=1 p(h (`) jt |h (`+1) t ,h (`) t\u22121,h (`\u22121) t\u22121 ,yt), where each conditional distribution is parameterized by a sigmoid function, as in (2). Learning and inference for the model with stochastic hidden layers is provided in Section 3.3. We choose not to consider deterministic layers here, as was considered in Gan et al. (2015c), since such complicates the gradient computation in the FCTSBN, while having similar empirical results compared with the usage of stochastic layers (we tried both in practice)."}, {"heading": "2.5. Semi-supervised Learning with FCTSBN", "text": "As expressed thus far, training the FCTSBN requires side information for each time step. In many applications (Le & Mikolov, 2014; Srivastava et al., 2015), however, unlabeled sequential data might be abundant, while obtaining the side information for all the data is expensive or sometimes impossible. We propose a framework for semisupervised learning based on FCTSBN, with the capacity to simutaneously train a generative model and a classifier from labeled and unlabeled data.\nSpecifically, assume that the side information is generated from the prior distribution p\u03b8(Y;\u03c0) parametrized by \u03c0 (which can be a multinomial distribution if yt are labels), then the generative model can be described as\np\u03b8(V,H,Y) = p\u03b8(Y;\u03c0) \u00b7 p\u03b8(V,H|Y) (12)\nwhere p\u03b8(V,H|Y) is the conditional generative model described in (7). Details on training p\u03b8(V,H,Y) are discussed in Section 3.4."}, {"heading": "3. Scalable Learning and Inference", "text": "The exact posterior over the hidden variables in (7) is intractable, and methods like Gibbs sampling and mean-field\nvariational Bayes (VB) inference, can be inefficient (particularly for inference at test time). We follow Gan et al. (2015c), and apply the Neural Variational Inference and Learning (NVIL) algorithm described in Mnih & Gregor (2014), allowing tractable and scalable parameter learning and inference by introducing a new recognition model."}, {"heading": "3.1. Lower Bound Objective", "text": "We are interested in training the CTSBN and FCTSBN models, p\u03b8(V,H|Y), both of which come in the form of (7). Given an observation V, we introduce a fixed-form distribution q\u03c6(H|V,Y) with parameters \u03c6, to approximate the true posterior p(H|V,Y). According to the variational principle (Jordan et al., 1999), we construct a lower bound on the marginal log-likelihood with the following form\nL(V|Y,\u03b8,\u03c6) = J (q\u03c6(H|V,Y), p\u03b8(V,H|Y)) (13)\nwhereJ (q, p) = Eq[log p\u2212log q] is used to save space, and q\u03c6(H|V,Y) is the recognition model. For both models, the recognition model is expressed as\nq\u03c6(H|V,Y) = T\u220f t=1 q(ht|ht\u22121,vt,vt\u22121,yt) (14)\nand each conditional distribution is specified as\nq(hjt = 1|ht\u22121,vt,vt\u22121,yt) = \u03c3(h\u0302jt) (15)\nh\u0302t = U (y) 1 ht\u22121 + U (y) 2 vt + U (y) 3 vt\u22121 + d (y) (16)\nFor CTSBN, U(y)ijk = \u2211L l=1 U\u0302ijklylt, where U\u0302i is a threeway tensor similar to W\u0302i; for FCTSBN, the weight matrix U\n(y) i is factored as in (11). The recognition model intro-\nduced in (14), allows fast and scalable inference."}, {"heading": "3.2. Parameter Learning", "text": "To optimize the lower bound described in (13), we apply Monte Carlo integration to approximate the expectations over q\u03c6, and stochastic gradient descent (SGD) for parameter optimization. The gradients of L with respect to \u03b8 and \u03c6 are expressed as\n\u2207\u03b8L = Eq\u03c6(H|V,Y)[\u2207\u03b8 log p\u03b8(V,H|Y)] (17) \u2207\u03c6L = Eq\u03c6(H|V,Y)[l\u03c6(V,H,Y) \u00b7 \u2207q\u03c6(H|V,Y)] (18)\nwhere l\u03c6 = log p\u03b8(V,H|Y) \u2212 log q\u03c6(H|V,Y), termed the learning signal for the recognition parameters \u03c6.\nThe expectation of l\u03c6 is exactly the lower bound (13). The recognition model gradient estimator, however, can be very noisy, since the estimated learning signal is potentially large. According to Mnih & Gregor (2014), we apply two variance-reduction techniques. The first technique\nis applied by centering the learning signal by substracting the data-dependent baseline and data-independent baseline learned during training. The second technique is variance normalization, which is normalizing the learning signal by a running estimate of its standard deviation. All learning details and evaluation of gradients are provided in Supplementary Sections B.1-B.3."}, {"heading": "3.3. Extension for deep models", "text": "The recognition model with respect to the deep FCTSBN is shown in Figure 3 (right). Since the middle layers are also stochastic, the calculation of the lower bound includes more terms, in the form of\nL = L\u2211 `=0 J (q\u03c6(H(`+1)|H(`),Y), p\u03b8(H(`+1),H(`)|Y))\nEach layer has a unique set of parameters whose gradient is zero for the lower bound of the other layers. Therefore, we can calculate the gradients separately for each layer of parameters, in a similar fashion to the single-layer FCTSBN. All details are provided in Supplementary Section B.4."}, {"heading": "3.4. Extension for Semi-supervised Learning", "text": "The recognition model in Section 3.1 provides a fast bottom-up probabilistic mapping from observations to hidden variables. Following Kingma et al. (2014), for the semi-supervised model (12), we introduce a recognition model for both H and Y, with the factorized form q\u03c6(H,Y|V) = q\u03c6(H|V,Y) \u00b7 q\u03c6(Y|V). The first term, q\u03c6(H|V,Y), is the same as in (14). When yt is a onehot encoded vector, we assume a discriminative classifier q\u03c6(yt|vt) = Cat(vt;\u03c0\u03c6(vt)), where Cat(x;\u03c0) denotes the categorical distribution.\nWhen the label corresponding to a sequence is missing, it is treated as a latent variable over which we perform posterior inference. The variational lower bound for the unlabeled data is\nLu = J (q\u03c6(H,Y|V), p\u03b8(H,V,Y)) (19)\nWhen the label is observed, the variational bound is a simple extension of (13), expressed as\nL = J (q\u03c6(H|V,Y), p\u03b8(V,H|Y)) + log p\u03b8(Y;\u03c1) (20) where p\u03b8(Y;\u03c1) is the prior distribution for Y, a constant term that can be omitted. To exploit the discriminative power of the labeled data, we further add a classification loss to (20). The resulting objective function is\nLl = L+ \u03b1 \u00b7 Ep\u0303l(V,Y)[log q\u03b8(Y|V)] (21)\nwhere L is the lower bound for the generative model, defined in (20), and p\u0303l(V,Y) denotes the empirical distribution. Parameter \u03b1 controls the relative weight between\nthe generative and discriminative components within the semi-supervised learning framework3, where the extended objective function Ls = Ll + Lu takes both labeled and unlabeled data into account. Details for optimizing Ls are included in Supplementary Section B.5."}, {"heading": "4. Related Work", "text": "Probabilistic models for sequential data in the deep learning literature can be roughly divided into two categories. The first category includes generative models without latent variables, which rely on use of Recurrent Neural Networks (RNNs) (Sutskever et al., 2011; Graves, 2013; Chung et al., 2015a). The second category contains generative models with latent variables, which can be further divided into two subcategories: (i) undirected latent variable models, utilizing the RBM as the building block (Taylor et al., 2006; Sutskever & Hinton, 2007; Sutskever et al., 2009; Boulanger-Lewandowski et al., 2012; Mittelman et al., 2014); (ii) directed latent variable models, e.g., extending the variational auto-encoders (Bayer & Osendorfer, 2014; Fabius et al., 2014; Chung et al., 2015b), or utilizing the SBN as the building block (Gan et al., 2015a;b;c).\nAmong this work, the Factored Conditional Restricted Boltzmann Machine (FCRBM) (Taylor & Hinton, 2009) and Temporal Sigmoid Belief Network (TSBN) (Gan et al., 2015c) are most related to the work reported here. However, there exist several key differences. Compared with FCRBM, our proposed model is fully directed, allowing fast generation through ancestral sampling; while in the FCRBM, alternating Gibbs sampling is required to obtain a sample at each time-step. Compared with TSBN, where generation of different styles was purely based on initialization, our model utilizes side information to gate the connections of a TSBN, which makes the model context-sensitive, and permits controlled transitioning and blending (detailed when presenting experiments in Section 5).\nAnother novelty of our model lies in the utilization of a factored three-way tensor to model the multiplicative interactions between side information and sequences. Similar ideas have also been exploited in (Memisevic & Hinton, 2007; Taylor & Hinton, 2009; Sutskever et al., 2011; Kiros et al., 2014a;b). We note that our factoring method is different from the one used in FCRBM. Specifically, in FCRBM, the energy function was factored, while our model factors over the transition parameters, by which we capture the underlying similarities within different sequences, boosting performance.\nMost deep generative models focus on exploring the generative ability (Kingma & Welling, 2014), and little work\n3We use \u03b1 = 2 \u00b7 T throughout the experiments, but obtain similar performance for other values of \u03b1 within [0.1 \u00b7 T, 4 \u00b7 T ].\nhas been done on examining the discriminative ability of deep generative models, except Kingma et al. (2014); Li et al. (2015). However, both works are restricted to the application of static data. Our paper is the first to develop a semi-supervised sequence classification method with deep generative models.\nIn terms of inference, the wake-sleep algorithm (Hinton et al., 1995), Stochastic Gradient Variational Bayes (SGVB) (Kingma & Welling, 2014) and Neural Variational Inference and Learning (NVIL) (Mnih & Gregor, 2014) are widely studied for training recognition models. We utilize NVIL for scalable inference, and our method is novel in designing a new factored multiplicative recognition model."}, {"heading": "5. Experiments", "text": "Sections 5.1-5.4 report the results of training several models with data from the CMU Motion Capture Database. Specifically, we consider two datasets: (i) motion sequences performed by subject 35 (Taylor et al., 2006)(mocap2), which contains two types of motions, i.e., walking and running; (ii) motion sequences performed by subject 137 (Taylor & Hinton, 2009)(mocap10), which contains 10 different styles of walking. Both datasets are preprocessed using downsampling and scaling to have zero mean and unit variance. In Section 5.5 and 5.6, we present additional experiments on weather prediction and conditional text generation to further demonstrate the versatility of the proposed model. The weather prediction dataset (Liu et al., 2010) contains monthly observations of time series data of 18 climate agents (data types) over different places in North America.\nThe FCTSBN model with W(y)3 \u2261 0 and W (y) 4 \u2261 0 is denoted Hidden Markov FCTSBN. The deep extension to FCTSBN is abbreviated as dFCTSBN. We use this abbreviation to denote the conditional generative model instead of the semi-supervised model, unless explicitly stated. Furthermore, we allow each observation to be dependent on the hidden and visible states of the previous n time steps, instead of n = 1. We refer to n as the order of the model. The choice of n can be different according to the specific scenario. For mocap2 prediction and mocap10 generation, n is selected to align with previous methods on these tasks; for other prediction tasks, n is chosen to balance between performance and model complexity; for semi-supervised tasks, n = 6 is considered to allow consecutive frames of data to be utilized effectively by the classifier.\nFor CTSBN, the model parameters for weights are initialized by sampling fromN (0, 0.0012I), whereas the bias parameters are initialized as zero. For FCTSBN, the parameters are initialized differently, since the actual initialization value of the weight parameters depends on the product\nof factors. To ensure faster convergence, the initial values Wa, Wb and Wc are sampled fromN (0, 0.012I). We use RMSprop (Tieleman & Hinton, 2012) throughout all the experiments. This is a stochastic gradient descent (SGD) method that allows the gradients to be adaptively rescaled by a running average of their recent magnitude. The datadependent baseline is implemented with a single-hiddenlayer neural network with 100 tanh units. We update the estimated learning signal with a momentum of 0.9.\nThe prediction of vt given v1:t\u22121 requires first sampling h1:t\u22121 from q\u03c6(h1:t\u22121|v1:t\u22121,y1:t\u22121), then calculating the posterior p\u03b8(ht|h1:t\u22121,v1:t\u22121,yt) for ht. Given ht, a prediction can be made for vt by sampling from p\u03b8(vt|h1:t,v1:t\u22121,yt), or by using the expectation of sampled variables. Generating samples, on the other hand, is a similar but relatively easier task, where sequences can be generated through ancestral sampling. A special advantage of the conditional generative model appears when the side information yt changes over time, so that we can generate sequences with style transitions that do not appear in the training data. More details on generating such sequences are discussed in Section 5.4."}, {"heading": "5.1. Mocap2 Prediction", "text": "For the first experiment, we perform the prediction task using the mocap2 dataset. We used 33 running and walking sequences, partitioned them into 31 training sequences and 2 test sequences, as in Gan et al. (2015c). Our models have 100 hidden units (and 50 factors for factored models) in each layer and the order of n = 1, according to the settings in Gan et al. (2015c). Deep models, including deep FCTSBN and deep TSBN, have two hidden layers of 100 units each. The side information, yt, is a one-hot encoded vector of length 2, which indicates \u201crunning\u201d or \u201cwalking\u201d.\nFrom Table 1, it is observed that the one-layered conditional models have a significant improvement over TSBN, ss(spike-slab)-SRTRBM (Mittelman et al., 2014) and g(Gaussian)-RTRBM (Sutskever et al., 2009), whereas FCTSBN has better performance than CTSBN, due to the factoring mechanism. Results of deep FCTSBN have comparable performances to that of TSBN with stochastic hidden layers; this may be because only two styles of timeseries data are considered."}, {"heading": "5.2. Mocap10 Generation and Prediction", "text": "In order to demonstrate the conditional generative capacity of our model, we choose the mocap10 dataset, which contains 10 different walking styles, namely cat, chicken, dinosaur, drunk, gangly, graceful, normal, old-man, sexy and strong. A single-layer Hidden Markov FCTSBN with 100 hidden units and order of n = 12 is trained on the dataset for 400 epochs, whereas the parameters are updated\n10 times for each epoch. The side information yt is provided as a one-hot encoded vector of length 10. We set a fixed learning rate of 3 \u00d7 10\u22123, and a decay rate of 0.9. Motions of a particular style are generated by initializing with 12 frames of the training data for that style and fixing the side information during generation.\nOur one-layer model can generate walking sequences of 9 styles out of 10, which are included in the supplementary videos. The only exception is the old-man style, where our model captures the general posture but fails to synthesize the subtle frame-to-frame changes in the footsteps.\nWe also present prediction results. For each style, we select 90% of the sequences as training data, and use the rest as testing data. We consider FCTSBN and a two-layer deep FCTSBN with order n = 8, 100 hidden variables and 50 factors on each layer. We also consider FCRBM with 600 hidden variables, 200 factors, 200 features and order of n = 12, as in Taylor & Hinton (2009). The average prediction error over 10 styles for FCRBM, FCTSBN and dFCTSBN are 0.4355, 0.4832, and 0.3599, respectively, but the performances for different styles vary significantly. Detailed prediction error results for each style is provided in Supplementary Section C."}, {"heading": "5.3. Semi-supervised learning with mocap2 and mocap10", "text": "Our semi-supervised learning framework can train a classifier and a generative model simultaneously. In this section we present semi-supervised classification results for mocap10, and generated sequences for mocap2. 4\nFor mocap10, we perform 10-class classification over the motion style with windows of 7 consecutive frames from\n4We cannot perform both tasks on a single dataset due to the limitations of the mocap datasets: for mocap2, the 2-class classification task is so simple that the test accuracy is almost 1; for mocap10, the number of frames is too small to train a generative model with semi-supervised learning.\nthe sequences. We use FCTSBN and dFCTSBN of order n = 6 with 100 hidden variables on each layer, set q\u03c6(Y|V) as a one-layer softmax classifier, and use \u03b1 = 2 \u00b7 T throughout our experiments. Our baseline models include K-nearest neighbors (K-NN), the one-layer softmax classifier with regularization parameter \u03bb (Softmax-\u03bb), and transductive support vector machines (TSVM) (Gammerman et al., 1998). Sequences of certain styles are truncated such that each style has 500 frames of actual motions. We hold out 100 frames for testing, and the rest as training data, a consecutive proportion of which is provided with yt.\nFigure 4 shows the average test accuracy of different methods under various percentage of labeled training data, where our models consistently have better performance than other methods, even with a simple, one-layer classifier q\u03c6. To ensure fair comparison, the other parametric methods have roughly the same number of parameters. This would demonstrate that the Lu term in (19) serves as an effective regularizer that helps prevent overfitting. Our deep FCTSBN model has better performance than the shallow model, which may be attributed to a more flexible regularizer due to more parameters in the generative model objective.\nFor mocap2 generation, the training data are processed such that one sequence for each style is provided with yt, whereas the remaining 31 sequences are unlabeled. Then the data are used to train a Hidden Markov FCTSBN of order n = 6 with 100 hidden variables and 50 factors, and q\u03c6(Y|V) as a one-layer softmax classifier. Although the proportion of labeled data is very small, we are able to synthesize smooth, high-quality motion sequences for both styles. Related videos are presented in the Supplementary Material."}, {"heading": "5.4. Generating sequences with style transitions and blending", "text": "One particular advantage of modeling multiple time-series data with one FCTSBN over training several TSBNs for each sequence is that the FCTSBN has a shared hidden representation for all styles, so yt are no longer restricted to a constant value during generation. This flexibility allows generation of sequences that do not appear in the training set, such as transitions between styles, or combinations of multiple styles.\nVideos displaying smooth and efficient style transitions and blending are provided in the supplementary material. Two examples are provided in Figure 6. We also include transitions between walking and running with the generative model trained in Section 5.3. To enable such transition, we transform the side information from one to another using a sigmoid-shaped form as in Figure 5. The entire transition process happens over around 60 frames, which is much faster than the 200-frame transition proposed for FCRBM (Taylor & Hinton, 2009) 5. For style blending, the side information is provided by a convex combination of several one-hot-encoded side information vectors."}, {"heading": "5.5. Weather prediction", "text": "We demonstrate weather prediction in multiple places, and discuss how different representations of side information can affect performance. We select 25 places in the United States, which are located in a 5 \u00d7 5 grid spanning across\n5In practice, a sudden change of labels will result in \u201djerky\u201d transitions (Taylor & Hinton, 2009), but we aim to use as few frames as possible during transition.\nmultiple climate regions. The weather data are processed so that for every month in 1990-2002, each place has 17 climate indicator values (e.g., temperature). We use only longitude and latitude as side information.\nOne way of providing yt is by using the geographic information directly, such that yt would be a real-valued vector of length 2 (we call this setting 2d for simplicity). Such representation is oversimplified, since the transition parameters W of a certain location cannot be described as a linear combination of longitude and latitude. Therefore, we consider two other representations of side information. One uses a vector with 10 binary values (10d), which is the concatenation of two one-hot encoded vector of length 5 for longitude and latitude, respectively; the other assumes a one-hot encoded vector of 25, one for each location (25d).\nWe use the monthly climate data from 1990 to 2001 as training set, and the data of 2002 as test set. We consider CTSBN, FCTSBN, and two-layer deep FCTSBN of order n = 6, with 100 hidden variables and 50 factors on each hidden layer. During test time, we predict the weather of every month in 2002 given the information of previous 6 months.\nAverage prediction error of 5 trials with random initializations are provided in Table 2. We observe that the factored model has significantly higher performance than the non-factored CTSBN, since the factored model has shared input-to-factor and factor-to-output parameters, and the factor-to-factor parameters have the potential to capture the correlations among side information at a higher level. Moreover, the performance improves as we increase the dimension of side information, which might be due to the increase of the number of parameters, along with the one-hot embeddings of side information. The 10d model has a significant performance boost compared with the 2d model, while using the 25d model further improves the performance slightly. We also compare our method with the one layer FCRBM model, which has 100 hidden variables and 50 variables for each of the three style factors and features, and achieved better results on the one-layer setting."}, {"heading": "5.6. Conditional Text Generation", "text": "Finally, we use a simple experiment to demonstrate the versatility of our model over the task of conditional text gen-\neration. We select 4 books (i.e., Napoleon the Little, The Common Law, Mysticism and Logic and The Bible) in the Gutenberg corpus, and use word2vec (Mikolov et al., 2013) to extract word embeddings, where words with few occurrences are omitted. This preprocessing step provides us with real-valued vectors, which are treated as observations and fit into our current model.\nThe real-valued word vector sequences are trained on a one-layer CTSBN of order 15 and 100 hidden variables. During generation, we clamp each generated value to the nearest possible word vector using cosine similarity. We focus on modeling real-valued sequences, hence the clamping approach is employed, instead of adopting a softmax layer to predict the words directly.\nTable 3 displays the generated samples from the model when provided with the side information of The Bible (B), The Common Law (C) and an even mixture of these two books (BC). As can be seen, our model learns to capture the styles associated with different books. By conditioning on the average of two styles, the model can generate reasonable samples that represent a hybrid of both styles (distinctive words such as<#> and god from B, and application and violence from C), even though such style combinations were not observed during training."}, {"heading": "6. Conclusions and Future Work", "text": "We have presented the Factored Conditional Temporal Sigmoid Belief Network, which can simultaneously model temporal sequences of multiple subjects. A general framework for semi-supervised sequence classification is also provided, allowing one to train a conditional generative model along with a classifier. Experimental results on several datasets show that the proposed approaches obtain superior predictive performance, boost classification accuracy, and synthesize a large family of sequences.\nWhile we assume side information as a simple vector, it would be interesting to incorporate more structured side information into the model, e.g., utilizing separate style and content components. We are also interested in models that can extract structured properties from side information."}, {"heading": "Acknowledgements", "text": "This research was supported in part by ARO, DARPA, DOE, NGA and ONR."}, {"heading": "C. Extended Experiment Results", "text": "C.1. Generated Videos\nAlong with this supplementary article including more details for our model, we present a number of videos to demonstrate the generative capacities of our models. The videos are available at https://goo.gl/9R59d7.\nmocap2 We present synthesized sequences by the semisupervised Hidden Markov FCTSBN trained with labeled and unlabeled data, namely walk.mp4, run.mp4, walkrun.mp4 and run-walk.mp4.\nThe videos denotes sequences of (i) walking; (ii) running; (iii) transition from walking to running; and (iv) transition from running to walking.\nmocap10 Sequences produced by the Hidden Markov FCTSBN over the mocap10 dataset are presented, including 9 styles and some style transitions and combinations.\nC.2. Detailed Results on mocap10 Prediction\nAverage prediction error for each style can be found in Table 4. For the FCTSBN and deep FCTSBN, each hidden layer has 100 hidden variables and 50 factors, whereas the FCRBM contains 600 hidden variables, 200 features and 200 factors.\nC.3. Detailed Results on mocap10 Classification\nWe include error bar results for the mocap10 classification task in Table 5."}], "references": [{"title": "Learning stochastic recurrent networks", "author": ["J. Bayer", "C. Osendorfer"], "venue": "In arXiv:1411.7610,", "citeRegEx": "Bayer and Osendorfer,? \\Q2014\\E", "shortCiteRegEx": "Bayer and Osendorfer", "year": 2014}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "In ICML,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Variational recurrent auto-encoders", "author": ["O. Fabius", "J.R. van Amersfoort", "D.P. Kingma"], "venue": "In arXiv:1412.6581,", "citeRegEx": "Fabius et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fabius et al\\.", "year": 2014}, {"title": "Learning by transduction", "author": ["A. Gammerman", "V. Vovk", "V. Vapnik"], "venue": "In UAI,", "citeRegEx": "Gammerman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gammerman et al\\.", "year": 1998}, {"title": "Scalable deep poisson factor analysis for topic modeling", "author": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Learning deep sigmoid belief networks with data augmentation", "author": ["Z. Gan", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Deep temporal sigmoid belief networks for sequence modeling", "author": ["Z. Gan", "C. Li", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In NIPS,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "In arXiv:1308.0850,", "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "In Neural computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": "In Science,", "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "In Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "In Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["R. Kiros", "R. Zemel", "R.R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Max-margin deep generative models", "author": ["C. Li", "J. Zhu", "T. Shi", "Zhang", "Bo"], "venue": "In NIPS,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Learning temporal causal graphs for relational timeseries analysis", "author": ["Y. Liu", "A. Niculescu-Mizil", "A.C. Lozano", "Y. Lu"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Unsupervised learning of image transformations", "author": ["R. Memisevic", "G. Hinton"], "venue": "In CVPR,", "citeRegEx": "Memisevic and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Memisevic and Hinton", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Structured recurrent temporal restricted boltzmann machines", "author": ["R. Mittelman", "B. Kuipers", "S. Savarese", "H. Lee"], "venue": "In ICML,", "citeRegEx": "Mittelman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mittelman et al\\.", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "In Artificial intelligence,", "citeRegEx": "Neal,? \\Q1992\\E", "shortCiteRegEx": "Neal", "year": 1992}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Learning multilevel distributed representations for high-dimensional sequences", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Sutskever and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Sutskever and Hinton", "year": 2007}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G. Hinton", "G. Taylor"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Modeling human motion using binary latent variables", "author": ["G. Taylor", "G. Hinton", "S. Roweis"], "venue": "In NIPS,", "citeRegEx": "Taylor et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2006}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["G.W. Taylor", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Taylor and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Hinton", "year": 2009}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G.E. Hinton"], "venue": "In COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "It serves as a building block for many deep generative models, such as the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov & Hinton, 2009).", "startOffset": 101, "endOffset": 122}, {"referenceID": 32, "context": "Other variants of the RBM have been used for modeling discrete time series data, such as human motion (Taylor et al., 2006), videos (Sutskever et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 30, "context": ", 2006), videos (Sutskever et al., 2009), and weather prediction (Mittelman et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 23, "context": ", 2009), and weather prediction (Mittelman et al., 2014).", "startOffset": 32, "endOffset": 56}, {"referenceID": 32, "context": "(CRBM) (Taylor et al., 2006), where the hidden and visible states at the current time step are dependent on directed connections from observations at the last few time steps.", "startOffset": 7, "endOffset": 28}, {"referenceID": 10, "context": "Most RBM-based models use Contrastive Divergence (CD) (Hinton, 2002) for efficient learning.", "startOffset": 54, "endOffset": 68}, {"referenceID": 26, "context": "In particular, advances in variational methods (Mnih & Gregor, 2014; Kingma & Welling, 2014; Rezende et al., 2014) have yielded scalable and efficient learning and inference for such models, avoiding poor inference speed caused by the \u201cexplaining away\u201d effect (Hinton et al.", "startOffset": 47, "endOffset": 114}, {"referenceID": 12, "context": ", 2014) have yielded scalable and efficient learning and inference for such models, avoiding poor inference speed caused by the \u201cexplaining away\u201d effect (Hinton et al., 2006).", "startOffset": 153, "endOffset": 174}, {"referenceID": 25, "context": "One directed graphical model that is related to the RBM is the Sigmoid Belief Network (SBN) (Neal, 1992).", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "We then adopt ideas from Taylor & Hinton (2009), where the ar X iv :1 60 5.", "startOffset": 34, "endOffset": 48}, {"referenceID": 15, "context": "For cases where side information is not available for the entire training set, we propose a framework for semisupervised learning, inspired by Kingma et al. (2014), to infer that information from observations (e.", "startOffset": 143, "endOffset": 164}, {"referenceID": 6, "context": "We choose not to consider deterministic layers here, as was considered in Gan et al. (2015c), since such complicates the gradient computation in the FCTSBN, while having similar empirical results compared with the usage of stochastic layers (we tried both in practice).", "startOffset": 74, "endOffset": 93}, {"referenceID": 28, "context": "In many applications (Le & Mikolov, 2014; Srivastava et al., 2015), however, unlabeled sequential data might be abundant, while obtaining the side information for all the data is expensive or sometimes impossible.", "startOffset": 21, "endOffset": 66}, {"referenceID": 6, "context": "We follow Gan et al. (2015c), and apply the Neural Variational Inference and Learning (NVIL) algorithm described in Mnih & Gregor (2014), allowing tractable and scalable parameter learning and inference by introducing a new recognition model.", "startOffset": 10, "endOffset": 29}, {"referenceID": 6, "context": "We follow Gan et al. (2015c), and apply the Neural Variational Inference and Learning (NVIL) algorithm described in Mnih & Gregor (2014), allowing tractable and scalable parameter learning and inference by introducing a new recognition model.", "startOffset": 10, "endOffset": 137}, {"referenceID": 13, "context": "According to the variational principle (Jordan et al., 1999), we construct a lower bound on the marginal log-likelihood with the following form", "startOffset": 39, "endOffset": 60}, {"referenceID": 15, "context": "Following Kingma et al. (2014), for the semi-supervised model (12), we introduce a recognition model for both H and Y, with the factorized form q\u03c6(H,Y|V) = q\u03c6(H|V,Y) \u00b7 q\u03c6(Y|V).", "startOffset": 10, "endOffset": 31}, {"referenceID": 31, "context": "The first category includes generative models without latent variables, which rely on use of Recurrent Neural Networks (RNNs) (Sutskever et al., 2011; Graves, 2013; Chung et al., 2015a).", "startOffset": 126, "endOffset": 185}, {"referenceID": 9, "context": "The first category includes generative models without latent variables, which rely on use of Recurrent Neural Networks (RNNs) (Sutskever et al., 2011; Graves, 2013; Chung et al., 2015a).", "startOffset": 126, "endOffset": 185}, {"referenceID": 32, "context": "The second category contains generative models with latent variables, which can be further divided into two subcategories: (i) undirected latent variable models, utilizing the RBM as the building block (Taylor et al., 2006; Sutskever & Hinton, 2007; Sutskever et al., 2009; Boulanger-Lewandowski et al., 2012; Mittelman et al., 2014); (ii) directed latent variable models, e.", "startOffset": 202, "endOffset": 333}, {"referenceID": 30, "context": "The second category contains generative models with latent variables, which can be further divided into two subcategories: (i) undirected latent variable models, utilizing the RBM as the building block (Taylor et al., 2006; Sutskever & Hinton, 2007; Sutskever et al., 2009; Boulanger-Lewandowski et al., 2012; Mittelman et al., 2014); (ii) directed latent variable models, e.", "startOffset": 202, "endOffset": 333}, {"referenceID": 1, "context": "The second category contains generative models with latent variables, which can be further divided into two subcategories: (i) undirected latent variable models, utilizing the RBM as the building block (Taylor et al., 2006; Sutskever & Hinton, 2007; Sutskever et al., 2009; Boulanger-Lewandowski et al., 2012; Mittelman et al., 2014); (ii) directed latent variable models, e.", "startOffset": 202, "endOffset": 333}, {"referenceID": 23, "context": "The second category contains generative models with latent variables, which can be further divided into two subcategories: (i) undirected latent variable models, utilizing the RBM as the building block (Taylor et al., 2006; Sutskever & Hinton, 2007; Sutskever et al., 2009; Boulanger-Lewandowski et al., 2012; Mittelman et al., 2014); (ii) directed latent variable models, e.", "startOffset": 202, "endOffset": 333}, {"referenceID": 4, "context": ", extending the variational auto-encoders (Bayer & Osendorfer, 2014; Fabius et al., 2014; Chung et al., 2015b), or utilizing the SBN as the building block (Gan et al.", "startOffset": 42, "endOffset": 110}, {"referenceID": 15, "context": "has been done on examining the discriminative ability of deep generative models, except Kingma et al. (2014); Li et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 15, "context": "has been done on examining the discriminative ability of deep generative models, except Kingma et al. (2014); Li et al. (2015). However, both works are restricted to the application of static data.", "startOffset": 88, "endOffset": 127}, {"referenceID": 11, "context": "In terms of inference, the wake-sleep algorithm (Hinton et al., 1995), Stochastic Gradient Variational Bayes (SGVB) (Kingma & Welling, 2014) and Neural Variational Inference and Learning (NVIL) (Mnih & Gregor, 2014) are widely studied for training recognition models.", "startOffset": 48, "endOffset": 69}, {"referenceID": 32, "context": "Specifically, we consider two datasets: (i) motion sequences performed by subject 35 (Taylor et al., 2006)(mocap2), which contains two types of motions, i.", "startOffset": 85, "endOffset": 106}, {"referenceID": 20, "context": "The weather prediction dataset (Liu et al., 2010) contains monthly observations of time series data of 18 climate agents (data types) over different places in North America.", "startOffset": 31, "endOffset": 49}, {"referenceID": 6, "context": "We used 33 running and walking sequences, partitioned them into 31 training sequences and 2 test sequences, as in Gan et al. (2015c). Our models have 100 hidden units (and 50 factors for factored models) in each layer and the order of n = 1, according to the settings in Gan et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 6, "context": "We used 33 running and walking sequences, partitioned them into 31 training sequences and 2 test sequences, as in Gan et al. (2015c). Our models have 100 hidden units (and 50 factors for factored models) in each layer and the order of n = 1, according to the settings in Gan et al. (2015c). Deep models, including deep FCTSBN and deep TSBN, have two hidden layers of 100 units each.", "startOffset": 114, "endOffset": 290}, {"referenceID": 23, "context": "From Table 1, it is observed that the one-layered conditional models have a significant improvement over TSBN, ss(spike-slab)-SRTRBM (Mittelman et al., 2014) and g(Gaussian)-RTRBM (Sutskever et al.", "startOffset": 133, "endOffset": 157}, {"referenceID": 30, "context": ", 2014) and g(Gaussian)-RTRBM (Sutskever et al., 2009), whereas FCTSBN has better performance than CTSBN, due to the factoring mechanism.", "startOffset": 30, "endOffset": 54}, {"referenceID": 6, "context": "(\u25e6) taken from Gan et al. (2015c); ( ) taken from Mittelman et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "(\u25e6) taken from Gan et al. (2015c); ( ) taken from Mittelman et al. (2014). Bold indicate the best results for shallow deep models, respectively.", "startOffset": 15, "endOffset": 74}, {"referenceID": 10, "context": "We also consider FCRBM with 600 hidden variables, 200 factors, 200 features and order of n = 12, as in Taylor & Hinton (2009). The average prediction error over 10 styles for FCRBM, FCTSBN and dFCTSBN are 0.", "startOffset": 112, "endOffset": 126}, {"referenceID": 5, "context": "Our baseline models include K-nearest neighbors (K-NN), the one-layer softmax classifier with regularization parameter \u03bb (Softmax-\u03bb), and transductive support vector machines (TSVM) (Gammerman et al., 1998).", "startOffset": 182, "endOffset": 206}, {"referenceID": 22, "context": ", Napoleon the Little, The Common Law, Mysticism and Logic and The Bible) in the Gutenberg corpus, and use word2vec (Mikolov et al., 2013) to extract word embeddings, where words with few occurrences are omitted.", "startOffset": 116, "endOffset": 138}], "year": 2016, "abstractText": "Deep conditional generative models are developed to simultaneously learn the temporal dependencies of multiple sequences. The model is designed by introducing a three-way weight tensor to capture the multiplicative interactions between side information and sequences. The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). The transition matrices are further factored to reduce the number of parameters and improve generalization. When side information is not available, a general framework for semi-supervised learning based on the proposed model is constituted, allowing robust sequence classification. Experimental results show that the proposed approach achieves state-of-theart predictive and classification performance on sequential data, and has the capacity to synthesize sequences, with controlled style transitioning and blending.", "creator": "LaTeX with hyperref package"}}}