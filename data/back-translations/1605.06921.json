{"id": "1605.06921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Generative Choreography using Deep Learning", "abstract": "Recent advances in deep learning have enabled the extraction of high-level features from sensor data, which has opened up new opportunities in many different areas, including computer-generated choreography. In this article, we present a system for generating novel choreographic material in the nuanced choreographic language and style of a single choreographer. It also shows promising results in generating higher compositional cohesion rather than just motion sequences. chor-rnn is a deep, recurring neural network trained on raw movement data that can generate new dance sequences for a solo dancer. Chorus-rnn can be used for collaborative human-machine choreography or as a creative catalyst that serves as inspiration for a choreographer.", "histories": [["v1", "Mon, 23 May 2016 07:36:49 GMT  (269kb)", "http://arxiv.org/abs/1605.06921v1", "This article will be presented at the 7th International Conference on Computational Creativity, ICCC2016"]], "COMMENTS": "This article will be presented at the 7th International Conference on Computational Creativity, ICCC2016", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MM cs.NE", "authors": ["luka crnkovic-friis", "louise crnkovic-friis"], "accepted": false, "id": "1605.06921"}, "pdf": {"name": "1605.06921.pdf", "metadata": {"source": "CRF", "title": "Generative Choreography using Deep Learning", "authors": ["Luka Crnkovic-Friis", "Louise Crnkovic-Friis"], "emails": ["luka@peltarion.com", "louise@theluluartgroup.com"], "sections": [{"heading": null, "text": "Generative Choreography using Deep Learning\nLuka Crnkovic-Friis Peltarion\nluka@peltarion.com\nLouise Crnkovic-Friis The Lulu Art Group\nlouise@theluluartgroup.com"}, {"heading": "Abstract", "text": "Recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields, including computer generated choreography. In this paper we present a system chorrnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. It also shows promising results in producing a higher level compositional cohesion, rather than just generating sequences of movement. At the core of chor-rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer. Chor-rnn can be used for collaborative human-machine choreography or as a creative catalyst, serving as inspiration for a choreographer."}, {"heading": "Introduction", "text": "Can a computer create meaningful choreographies? With its potential to expand and facilitate artistic expression, this question has been explored since the start of the computer age. To answer it, a good starting point is to identify the different levels that go into a choreographic work. A choreography can be said to contain three basic levels of abstraction, style (the dynamic execution and expression of movement by the dancer), syntax (the choreographic language of the work and choreographer) and semantics (the overall meaning or theme that binds the work into a coherent unit) (Blacking & Kealiinohomoko, 1979). All three levels present unique practical and theoretical challenges to computer generated choreography. As syntax is the easiest to formalize in the form of a notation system, it has been the logical starting point for creating generative choreography (Calvert, Wilke, Ryman, & Fox, 2005). However, unlike music or literature dance lacks a universally accepted notation system. Although systems, such as Benesh movement notation and Labanotation have been proposed they have not been universally adopted mostly because of their steep learning curve (Guest, 1998). They also cannot capture style - the nuanced dynamics of movement that emerges as a collaboration between choreographer and dancer (Blom & Chaplin, 1982). The alternative of building computational models from raw data (video, motion capture) is alluring as it contains much more information. It has however until recently not been feasible both because of combination of lack of computing power, algorithms and available data (LeCun, Bengio, & Hinton, 2015). With the advent of GPU powered deep learning that has changed and we can now start building entirely data driven end-to-end generative models that are capable of capturing both style and syntax. Furthermore, as deep neural networks\nare capable of extracting multiple layers of abstraction, they can begin to model the semantic level as well. In this paper we describe such a system, chor-rnn that we have developed and discuss related work, show how the raw data is collected and present a deep recurrent neural network model. Finally, we also detail the training and discuss results, possible use and future work."}, {"heading": "Related work", "text": "Earlier work in this field has included various programmatic approaches with parametrized skeleton systems (Noll, 2013) as well as using simplified movement models combined with genetic algorithms to explore the parameter space (Lapointe, 2005). Several systems have been developed as a combination of a visualization system with a choice of predefined movement material that could be sequenced into longer compositions by the choreographer. Fully autonomous sequence generation has mostly been limited to sequencing a combination of snippets of movement material. Several proposed systems have been interactive, requiring a choreographer to make a number of selections during the generation phase (Carlson, Schiphorst, & Pasquier, 2011). Artificial neural networks have been used in generative systems in the past (McCormick, 2015). They have however not involved deep learning and the neural network presented in this paper is using tens of millions of model parameters rather than thousands.\nRecording movement A choreography is the purposeful arrangement of sequences of motion. The basic building block is the change of position in a 3D space (Maletic, 1987). Techniques for recording the movement of human body in space are called \u201cmotion capture\u201d and while here are various technical solutions at the time of writing, the most simple to use and cost effective was the Microsoft Kinect v2 sensor (Berger et al., 2011).\nIt consists of a 3D camera augmented by an infrared camera and software that can automatically perform efficient and accurate joint tracking. The sensor records the movements of 25 joints (see Figure 1) at up to 30 frames per second. Each joint position is represented by a 3D coordinate for each frame. The sensor can in theory track up to 6 bodies simultaneously but multiple bodies can occlude each other relative to the field of view of the sensor. It has no way of tracking occluded joints (F\u00fcrntratt & Neuschmied, 2014). Multiple sensors can be used to overcome that limitation, but it requires more complex software to combine the results (Kwon et al., 2015). Our work was done with one sensor and one body."}, {"heading": "Generative model", "text": "Recurrent neural networks (RNNs) have been used to get state-of-the-art results for complex time series modeling tasks such as speech recognition and translation (Greff, Srivastava, Koutn\u00edk, Steunebrink, & Schmidhuber, 2015). Since the motion capture data is a multidimensional time series we use a deep RNN model."}, {"heading": "Long Short-Term Memory", "text": "Standard RNNs are difficult to train in a stable way (due to the vanishing/exploding gradient problem) so we use a Long Short-Term Memory (LSTM) type of RNN. LSTMs are stable over long training runs and can be stacked to form deeper networks without loss of stability (Schmidhuber, 2015). Contrary to a regular RNN which uses simple recurrent neurons, the central unit in an LSTM is a memory cell that holds a state over time, regulated by gates that control the signal flow in and out of the cell. As the signal flow is tightly controlled, the risk is minimized of overloading the cell through positive feedback or extinguishing it through negative feedback. The following equations show the relations in an LSTM cell (see Figure 2):\n\ud835\udc8a\ud835\udc8a\ud835\udc95\ud835\udc95 = \ud835\udf39\ud835\udf39(\ud835\udc7e\ud835\udc7e\ud835\udc8a\ud835\udc8a\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 + \ud835\udc79\ud835\udc79\ud835\udc8a\ud835\udc8a\ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc91\ud835\udc91\ud835\udc8a\ud835\udc8a \u2299 \ud835\udc84\ud835\udc84\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc83\ud835\udc83\ud835\udc8a\ud835\udc8a) (1)\n\ud835\udc87\ud835\udc87\ud835\udc95\ud835\udc95 = \ud835\udf39\ud835\udf39\ufffd\ud835\udc7e\ud835\udc7e\ud835\udc87\ud835\udc87\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 + \ud835\udc79\ud835\udc79\ud835\udc87\ud835\udc87\ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc91\ud835\udc91\ud835\udc87\ud835\udc87 \u2299 \ud835\udc84\ud835\udc84\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc83\ud835\udc83\ud835\udc87\ud835\udc87\ufffd (2)\n\ud835\udc84\ud835\udc84\ud835\udc95\ud835\udc95 = \ud835\udc87\ud835\udc87\ud835\udc95\ud835\udc95 \u2299 \ud835\udc84\ud835\udc84\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf \u2299 \ud835\udc88\ud835\udc88(\ud835\udc7e\ud835\udc7e\ud835\udc84\ud835\udc84\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 + \ud835\udc79\ud835\udc79\ud835\udc84\ud835\udc84\ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc83\ud835\udc83\ud835\udc84\ud835\udc84) (3)\n\ud835\udc90\ud835\udc90\ud835\udc95\ud835\udc95 = \ud835\udf39\ud835\udf39(\ud835\udc7e\ud835\udc7e\ud835\udc90\ud835\udc90\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 + \ud835\udc79\ud835\udc79\ud835\udc90\ud835\udc90\ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc91\ud835\udc91\ud835\udc8a\ud835\udc8a \u2299 \ud835\udc84\ud835\udc84\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc83\ud835\udc83\ud835\udc8a\ud835\udc8a) (4)\n\ud835\udc8a\ud835\udc8a\ud835\udc95\ud835\udc95 = \ud835\udf39\ud835\udf39(\ud835\udc7e\ud835\udc7e\ud835\udc8a\ud835\udc8a\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 + \ud835\udc79\ud835\udc79\ud835\udc8a\ud835\udc8a\ud835\udc89\ud835\udc89\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc91\ud835\udc91\ud835\udc90\ud835\udc90 \u2299 \ud835\udc84\ud835\udc84\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + \ud835\udc83\ud835\udc83\ud835\udc90\ud835\udc90) (5)\nHere it, ft, ct, ot and ht are the input gate, forget gate, memory cell and output gate at time step t; xt is the input while \ud835\udf39\ud835\udf39() and g() are the sigmoid and tangent activation functions; W and R are the weight matrices applied to input and recurrent units; p and b are the peep-hole connection and biases while \u2299 denotes dot product. Typically, when training a generative system, target output data would be the same as the input data but shifted with one sample:\n\ud835\udc95\ud835\udc95\ud835\udc95\ud835\udc95 = \ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95+\ud835\udfcf\ud835\udfcf (6) This works well when the input and targets are discrete and the last layer is a softmax function (such as in the case of words or characters). For continuous functions as in this case there is a fundamental problem. When sampling dance movements 30 times/second xt is trivial to predict if xt-1 and xt-2 are known. It is just a continuation of the previous vector. A very simple model will produce very low errors during training, validation and testing:\n\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 = \ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf + (\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf \u2212 \ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95\u2212\ud835\udfd0\ud835\udfd0) (7) However, when using it in a generative fashion where the output of the LSTM is used as the next input\n\ud835\udc32\ud835\udc32\ud835\udc2d\ud835\udc2d = \ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc74\ud835\udc95\ud835\udc95(\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95\u2212\ud835\udfcf\ud835\udfcf) (8)\nit will fail completely. In cases where the data is discrete, a softmax introduces a controlled random element that can force the trajectory of the network into a new but controlled direction. In the case of continuous data, it is not possible as we do not have a controlled statistical distribution of the output so adding random noise will not help. In general, it can be shown that when using a mean square error metric, the output will stagnate and converge to an average output (Bishop, 1994)."}, {"heading": "Mixture Density LSTMs", "text": "To counteract the issue of stagnating output we attach a mixture density network (MDN) to the output of the LSTM. This technique has been used successfully among other things for robotic arm control (Bishop, 1994) as well as handwriting generation (Graves, 2013). Instead of just outputting a single position tensor, we output a probability density function for each dimension in the tensor. The output of the LSTM consists of a layer of linear Figure 2 LSTM neuron\noutput units that provide parameters for a mixture model defined as the probability of a target t given an input x:\n\ud835\udc5d\ud835\udc5d(\ud835\udc95\ud835\udc95|\ud835\udc99\ud835\udc99) = \ufffd \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56(\ud835\udc99\ud835\udc99)\ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56(\ud835\udc95\ud835\udc95|\ud835\udc99\ud835\udc99) \ud835\udc5a\ud835\udc5a\n\ud835\udc56\ud835\udc56=1 (9)\nwhere m is the number of components in the mixture with \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56 being the mixing coefficients as a function of the inputs (x). The function \ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56 is the conditional density of the target tensor t for the i:th kernel. We use a Gaussian kernel, defined as:\n\ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56(\ud835\udc95\ud835\udc95|\ud835\udc99\ud835\udc99) = 1\n(2\ud835\udf0b\ud835\udf0b) \ud835\udc50\ud835\udc50 2\ud835\udf0e\ud835\udf0e\ud835\udc56\ud835\udc56(\ud835\udc99\ud835\udc99)\ud835\udc50\ud835\udc50\n\ud835\udc52\ud835\udc52 \u2212 \ufffd\ud835\udc95\ud835\udc95\u2212\ud835\udf41\ud835\udf41\ud835\udc8a\ud835\udc8a(\ud835\udc99\ud835\udc99)\u20162 2\ud835\udf0e\ud835\udf0e\ud835\udc56\ud835\udc56(\ud835\udc99\ud835\udc99)2 (10)\nThe neural network part that feeds into the mixture density model hence provides a set of values for the mixture coefficients, a set of means and a set of variances. The total number of output variables will be \ud835\udc5a\ud835\udc5a(\ud835\udc50\ud835\udc50 + 2) where m is the number of mixture components and c the number of output variables (a regular LSTM would have c outputs). The outputs from the neural network will consist of a tensor\n\ud835\udc9b\ud835\udc9b = [\ud835\udc67\ud835\udc671\ud835\udefc\ud835\udefc , . . , \ud835\udc67\ud835\udc67\ud835\udc5a\ud835\udc5a\ud835\udefc\ud835\udefc , \ud835\udc67\ud835\udc67\ud835\udc5a\ud835\udc5a+1 \ud835\udf07\ud835\udf07 , . . , \ud835\udc67\ud835\udc67\ud835\udc5a\ud835\udc5a\ud835\udc50\ud835\udc50+\ud835\udc5a\ud835\udc5a+1 \ud835\udf07\ud835\udf07 , \ud835\udc67\ud835\udc67\ud835\udc5a\ud835\udc5a\ud835\udc50\ud835\udc50+\ud835\udc5a\ud835\udc5a+2\ud835\udf0e\ud835\udf0e , . . , \ud835\udc67\ud835\udc67\ud835\udc5a\ud835\udc5a(\ud835\udc50\ud835\udc50+2)\ud835\udf0e\ud835\udf0e ] (11)\ncontaining all the necessary parameters to construct a mixture model. The number of mixture components, m, is arbitrary and can be interpreted as the number of different choices the network can pick at each time step. With the parametrized output, the whole MDN part can be encoded as a simple error metric where the error function becomes a negative log likelihood function (for the q:th sample):\n\ud835\udc38\ud835\udc38\ud835\udc5e\ud835\udc5e = \u2212log \ufffd \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56(\ud835\udc99\ud835\udc99\ud835\udc92\ud835\udc92)\ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56(\ud835\udc95\ud835\udc95\ud835\udc92\ud835\udc92|\ud835\udc99\ud835\udc99\ud835\udc92\ud835\udc92) \ud835\udc5a\ud835\udc5a\n\ud835\udc56\ud835\udc56=1\n\ufffd (12)\nWhere\n\ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56 = \ud835\udc52\ud835\udc52\ud835\udc67\ud835\udc67\ud835\udc56\ud835\udc56\n\ud835\udefc\ud835\udefc\n\u2211 \ud835\udc52\ud835\udc52 \ud835\udc67\ud835\udc67\ud835\udc57\ud835\udc57 \ud835\udefc\ud835\udefc\n\ud835\udc40\ud835\udc40 \ud835\udc57\ud835\udc57=1\n, \ud835\udf0e\ud835\udf0e\ud835\udc56\ud835\udc56 = \ud835\udc52\ud835\udc52\ud835\udc67\ud835\udc67\ud835\udc56\ud835\udc56 \ud835\udf0e\ud835\udf0e , \ud835\udf07\ud835\udf07\ud835\udc56\ud835\udc56 = \ud835\udc67\ud835\udc67\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56 \ud835\udf07\ud835\udf07 (13)\nand the derivatives needed for the training can be expressed as:\n\ud835\udf15\ud835\udf15\ud835\udc38\ud835\udc38\ud835\udc5e\ud835\udc5e \ud835\udf15\ud835\udf15\ud835\udc67\ud835\udc67\ud835\udc56\ud835\udc56\ud835\udefc\ud835\udefc = \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56 \u2212 \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56\ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56 \u2211 \ud835\udefc\ud835\udefc\ud835\udc57\ud835\udc57\ud835\udf11\ud835\udf11\ud835\udc57\ud835\udc57\ud835\udc5a\ud835\udc5a\ud835\udc57\ud835\udc57=1\n(14)\n\ud835\udf15\ud835\udf15\ud835\udc38\ud835\udc38\ud835\udc5e\ud835\udc5e \ud835\udf15\ud835\udf15\ud835\udc67\ud835\udc67\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56 \ud835\udf07\ud835\udf07 = \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56\ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56 \u2211 \ud835\udefc\ud835\udefc\ud835\udc57\ud835\udc57\ud835\udf11\ud835\udf11\ud835\udc57\ud835\udc57\ud835\udc5a\ud835\udc5a\ud835\udc57\ud835\udc57=1 \ud835\udf07\ud835\udf07\ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56 \u2212 \ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc56 \ud835\udf0e\ud835\udf0e\ud835\udc56\ud835\udc562\n(15)\n\ud835\udf15\ud835\udf15\ud835\udc38\ud835\udc38\ud835\udc5e\ud835\udc5e \ud835\udf15\ud835\udf15\ud835\udc67\ud835\udc67\ud835\udc56\ud835\udc56\ud835\udf0e\ud835\udf0e = \u2212 \ud835\udefc\ud835\udefc\ud835\udc56\ud835\udc56\ud835\udf11\ud835\udf11\ud835\udc56\ud835\udc56 \u2211 \ud835\udefc\ud835\udefc\ud835\udc57\ud835\udc57\ud835\udf11\ud835\udf11\ud835\udc57\ud835\udc57\ud835\udc5a\ud835\udc5a\ud835\udc57\ud835\udc57=1 \ufffd \u2016\ud835\udc61\ud835\udc61 \u2212 \ud835\udf07\ud835\udf07\ud835\udc56\ud835\udc56\u20162 \ud835\udf0e\ud835\udf0e\ud835\udc56\ud835\udc562 \u2212 \ud835\udc50\ud835\udc50\ufffd (16)\nThe derivatives of the error function can be used with any standard gradient based optimization algorithm together with backpropagation."}, {"heading": "Training", "text": "The data collected consisted of five hours of contemporary dance motion capture material created and performed by a choreographer. The resulting data set consisted of 13.5 million spatiotemporal joint positions. We used multiple deep configurations but the final neural network topology consists of 3 hidden layers with 1024 neurons in each (a total of ~21M weights). The input data was a 75-dimensional tensor (25 joints x 3 dimensions). The model was trained for ~48h on a GPU computation server with 4 x Nvida Titan X GPUs (a total of 27 teraflops capacity). A batch size of 512 sequence parts (128/GPU) was used with a sequence length of 1024 samples. The sequence length corresponds to how many steps the system is unrolled in time and in effect the number of layers becomes 1024*3 = 3072 during training.\nThe number of layers, and their effect in a recurrent neural network requires a far more complex interpretation than standard feed forward/convolutional neural networks as a signal can take an indeterminate number of spatiotemporal paths through the network. (Greff et al., 2015) The neural network was trained with RMS Prop using Back Propagation Through-Time. The software was implemented in lua/Torch7 using the Peltarion Cortex platform. A comparison of a network trained with MDN and without can be seen in Figure 3. In generation mode the MDN distributions were sampled at each time step to get a new set of coordinates for the joints. For this experiment we used unbiased sampling."}, {"heading": "Results", "text": "Our chor-rnn system can produce novel choreographies in the general style represented in the training data. Over the training interval it passes through several stages: basic joint relations (understanding the anatomy of human joints), basic movement style and syntax and at last the composition of several movements into a meaningful composition (semantics). See Table 1 for examples of results and Figure 4 for a visualization of example generated trajectories.\nDiscussion The generated material, presented as an animated \u201cstick figure\u201d, was evaluated qualitatively by the choreographer. As choreography is an art largely based on physical expression and embodied knowledge (Blom & Chaplin, 1982), the choreographer also learned and executed the generated material. The conclusion was that the chor-rnn system produces novel, anatomically feasible movements in the specific choreographic language of the choreographer whose work it was trained on. If you generated an hours\u2019 worth of new choreography, it would have significantly less semantic meaning than the work it was trained on. Generally speaking, the semantic level is the most difficult to quantify, especially when it comes to avant-garde art as it does not follow an established form (Foster, 1986). It is also the most complex one from the point of view of the neural network. As with text or image generation (Hinton, 2014), the semantic level is the last one to emerge from the training. There will of course be significant limitations when it comes to generating novel semantic levels as an artificial neural network can\u2019t draw on the human choreographer\u2019s life experience.\nUse as an artist\u2019s tool While there are interesting philosophical questions regarding machine creativity especially in a longer perspective, it is also interesting to see how current results can be used as a practical tool for a working choreographer. The chor-rnn system in its current state can be used to facilitate a human choreographer\u2019s creative process in several ways. Two examples are collaborative choreography and mutually interpreted choreography.\nIn the first case the artist and chor-rnn can collaborate in creating a sequence by alternating between them as shown in Figure 5.\n1. The artist choreographs a sequence A1 2. Chor-rnn takes sequence A1 as input and produces\na new sequence B1 as a continuation of A1 3. The artist looks at sequence B1 and choreographs a\nnew sequence A2 as a continuation of B1 4. Steps 2-3 are repeated\nThe resulting sequence will be A1B1A2B2..ANBN \u2013 an alternation between human and computer choreography. In the second case, the artist can start a sequence, let the chor-rnn generate a new sequence. The human can then reinterpret the output of the chor-rnn and input the interpretation into the system.\n1. The artist choreographs a sequence A1 2. Chor-rnn takes sequence A1 as input and produces\na new sequence (or set of sequences) B1 as a continuation of A1 3. The artist looks at B1 and choreographs a reinterpretation of B1 as a new sequence, A2\n4. Steps 2-3 are repeated The resulting sequence will be A1A2A3..AN \u2013 a computer inspired human made choreography. Due to the symmetry of the process, a secondary sequence is created as well, B1B2B3..BN \u2013 a human inspired computer made choreography. When a choreographer works with a dancer to develop a choreography, the latter will inevitably influence the end result. While this may be desirable, it also dilutes the distinctive style (and possibly syntax) that is unique to the choreographer. With chor-rnn, the choreographer works with a virtual companion using the same choreographic language. At the same time as it is capable of producing novel work, it can provide creative inspiration. As the level of machine involvement is variable and can be chosen by the choreographer, the results can be an interesting starting point of philosophical discussions on authenticity and computer generated art."}, {"heading": "Future work", "text": "Collect a larger corpus of data The five hours of motion capture data was enough to build a proof of concept system but ideally the corpus should be larger \u2013 especially if multiple choreographers are involved. For comparison state of the art speech recognition models use 100+ hours of data (and\nit is considered to be a major bottleneck in that field of research) (Graves & Jaitly, 2014). Derive a choreographic symbolic language One of the most intriguing features of deep neural networks is that they internally build up multiple levels of abstraction (Hinton, 2014). Using a recurrent variational autoencoder would allow us to compress meaningful higher order information into a fixed size tensor (encoding) (Sutskever, Vinyals, & Le, 2014). This in turn would allow a derivation of a symbolic language and by mapping it to feature detectors that operate on that encoding. A general symbolic encoding could provide an alternative to the existing notation systems and simplify the creation of computer created choreography. It could also provide a convenient method of recording a choreographic work in a compact, human readable format. As multiple mobile phone makers are now integrating 3D cameras (comparable to the Kinect) into their devices, an easy way of transforming recorded material to a symbolic encoding may be of significant practical use for documentation/archiving purposes (Kadambi, Bhandari, & Raskar, 2014). Multiple bodies The Kinect sensor cannot directly handle occluded body parts. This is problematic even with one dancer and makes it nearly impossible to capture interactions between multiple dancers. The solution is to use multiple Kinect sensors and combine their data (Kwon et al., 2015). This would allow us to record choreographies with up to 6 dancers and allow the system to learn about interactions between dancers. Multi-modal input The input data could be extended to beyond motion capture data also include sound (and even images and video). One could for instance build a system that in the generated choreography relates to a musical composition."}, {"heading": "Conclusions", "text": "This paper details a system, chor-rnn that is trained using a corpus of motion captured contemporary dance. The system can produce novel choreographic sequences in the choreographic style represented in the corpus. Using a deep recurrent neural network, it is capable of understanding and generating choreography style, syntax and to some extent semantics. Although it is currently limited to generating choreographies for a solo dancer there are a number of interesting paths to explore for future work. This includes the possibility of tracking multiple dancers and experimenting with variational autoencoders that would allow the automatic construction of a symbolic language for movement that goes beyond simple syntax. Apart from fully autonomous operation, chor-rnn can be used by a choreographer as a creativity catalyst or choreographic partner. We asked if a computer could create meaningful choreographies and with tools like chor-rnn we think we can get one step closer to answering that question or at least to discover new relevant questions."}], "references": [{"title": "Markerless Motion Capture using multiple Color-Depth Sensors. Paper presented at the VMV", "author": ["K. Berger", "K. Ruhl", "Y. Schroeder", "C. Bruemmer", "A. Scholz", "M.A. Magnor"], "venue": null, "citeRegEx": "Berger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berger et al\\.", "year": 2011}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q1994\\E", "shortCiteRegEx": "Bishop", "year": 1994}, {"title": "The Performing arts: music and dance: Walter de Gruyter", "author": ["J. Blacking", "J.W. Kealiinohomoko"], "venue": null, "citeRegEx": "Blacking and Kealiinohomoko,? \\Q1979\\E", "shortCiteRegEx": "Blacking and Kealiinohomoko", "year": 1979}, {"title": "The intimate act of choreography: University of Pittsburgh Pre", "author": ["L.A. Blom", "L.T. Chaplin"], "venue": null, "citeRegEx": "Blom and Chaplin,? \\Q1982\\E", "shortCiteRegEx": "Blom and Chaplin", "year": 1982}, {"title": "Applications of computers to dance", "author": ["T. Calvert", "L. Wilke", "R. Ryman", "I. Fox"], "venue": "Computer Graphics and Applications,", "citeRegEx": "Calvert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Calvert et al\\.", "year": 2005}, {"title": "Scuddle: Generating movement catalysts for computeraided choreography", "author": ["K. Carlson", "T. Schiphorst", "P. Pasquier"], "venue": "Paper presented at the Proceedings of the Second International Conference on Computational Creativity", "citeRegEx": "Carlson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2011}, {"title": "Reading dancing: Bodies and subjects in contemporary American dance", "author": ["S.L. Foster"], "venue": null, "citeRegEx": "Foster,? \\Q1986\\E", "shortCiteRegEx": "Foster", "year": 1986}, {"title": "Evaluating pointing accuracy on Kinect V2 sensor. Paper presented at the International Conference on Multimedia and Human-Computer Interaction (MHCI)", "author": ["H. F\u00fcrntratt", "H. Neuschmied"], "venue": null, "citeRegEx": "F\u00fcrntratt and Neuschmied,? \\Q2014\\E", "shortCiteRegEx": "F\u00fcrntratt and Neuschmied", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Paper presented at the Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u00edk", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Choreo-graphics: a comparison of dance notation systems from the fifteenth century to the present", "author": ["A.H. Guest"], "venue": null, "citeRegEx": "Guest,? \\Q1998\\E", "shortCiteRegEx": "Guest", "year": 1998}, {"title": "Where do features come from", "author": ["G. Hinton"], "venue": "Cognitive science,", "citeRegEx": "Hinton,? \\Q2014\\E", "shortCiteRegEx": "Hinton", "year": 2014}, {"title": "3d depth cameras in vision: Benefits and limitations of the hardware Computer Vision and Machine Learning with RGB-D Sensors", "author": ["A. Kadambi", "A. Bhandari", "R. Raskar"], "venue": null, "citeRegEx": "Kadambi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kadambi et al\\.", "year": 2014}, {"title": "Implementation of Human Action Recognition System Using Multiple Kinect Sensors Advances in Multimedia Information Processing--PCM", "author": ["B. Kwon", "D. Kim", "J. Kim", "I. Lee", "H. Oh", "S. . . Lee"], "venue": null, "citeRegEx": "Kwon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kwon et al\\.", "year": 2015}, {"title": "Choreogenetics: The generation of choreographic variants through genetic mutations and selection", "author": ["Lapointe", "F.-J."], "venue": "Paper presented at the Proceedings of the 7th annual workshop on Genetic and evolutionary computation.", "citeRegEx": "Lapointe and F..J.,? 2005", "shortCiteRegEx": "Lapointe and F..J.", "year": 2005}, {"title": "Body-space-expression: The development of Rudolf Laban's movement and dance concepts (Vol", "author": ["V. Maletic"], "venue": "75): Walter de Gruyter.", "citeRegEx": "Maletic,? 1987", "shortCiteRegEx": "Maletic", "year": 1987}, {"title": "Emergent Behaviour: Learning From An Artificially Intelligent Performing", "author": ["J.H.S. McCormick", "Vincs", "Kim", "Vincent", "Jordan Beth"], "venue": null, "citeRegEx": "McCormick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McCormick et al\\.", "year": 2015}, {"title": "EARLY DIGITAL COMPUTER ART & ANIMATION AT BELL TELEPHONE LABORATORIES, INC", "author": ["A.M. Noll"], "venue": null, "citeRegEx": "Noll,? \\Q2013\\E", "shortCiteRegEx": "Noll", "year": 2013}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61, 85-117.", "citeRegEx": "Schmidhuber,? 2015", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. Paper presented at the Advances in neural information processing systems", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Although systems, such as Benesh movement notation and Labanotation have been proposed they have not been universally adopted mostly because of their steep learning curve (Guest, 1998).", "startOffset": 171, "endOffset": 184}, {"referenceID": 18, "context": "Earlier work in this field has included various programmatic approaches with parametrized skeleton systems (Noll, 2013) as well as using simplified movement models combined with genetic algorithms to explore the parameter space (Lapointe, 2005).", "startOffset": 107, "endOffset": 119}, {"referenceID": 16, "context": "The basic building block is the change of position in a 3D space (Maletic, 1987).", "startOffset": 65, "endOffset": 80}, {"referenceID": 0, "context": "Techniques for recording the movement of human body in space are called \u201cmotion capture\u201d and while here are various technical solutions at the time of writing, the most simple to use and cost effective was the Microsoft Kinect v2 sensor (Berger et al., 2011).", "startOffset": 237, "endOffset": 258}, {"referenceID": 14, "context": "Multiple sensors can be used to overcome that limitation, but it requires more complex software to combine the results (Kwon et al., 2015).", "startOffset": 119, "endOffset": 138}, {"referenceID": 19, "context": "LSTMs are stable over long training runs and can be stacked to form deeper networks without loss of stability (Schmidhuber, 2015).", "startOffset": 110, "endOffset": 129}, {"referenceID": 1, "context": "In general, it can be shown that when using a mean square error metric, the output will stagnate and converge to an average output (Bishop, 1994).", "startOffset": 131, "endOffset": 145}, {"referenceID": 1, "context": "This technique has been used successfully among other things for robotic arm control (Bishop, 1994) as well as handwriting generation (Graves, 2013).", "startOffset": 85, "endOffset": 99}, {"referenceID": 8, "context": "This technique has been used successfully among other things for robotic arm control (Bishop, 1994) as well as handwriting generation (Graves, 2013).", "startOffset": 134, "endOffset": 148}, {"referenceID": 10, "context": "(Greff et al., 2015) The neural network was trained with RMS Prop using Back Propagation Through-Time.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Generally speaking, the semantic level is the most difficult to quantify, especially when it comes to avant-garde art as it does not follow an established form (Foster, 1986).", "startOffset": 160, "endOffset": 174}, {"referenceID": 12, "context": "As with text or image generation (Hinton, 2014), the semantic level is the last one to emerge from the training.", "startOffset": 33, "endOffset": 47}, {"referenceID": 12, "context": "Derive a choreographic symbolic language One of the most intriguing features of deep neural networks is that they internally build up multiple levels of abstraction (Hinton, 2014).", "startOffset": 165, "endOffset": 179}, {"referenceID": 14, "context": "The solution is to use multiple Kinect sensors and combine their data (Kwon et al., 2015).", "startOffset": 70, "endOffset": 89}], "year": 2016, "abstractText": "Recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields, including computer generated choreography. In this paper we present a system chorrnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. It also shows promising results in producing a higher level compositional cohesion, rather than just generating sequences of movement. At the core of chor-rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer. Chor-rnn can be used for collaborative human-machine choreography or as a creative catalyst, serving as inspiration for a choreographer.", "creator": "Acrobat PDFMaker 15 for Word"}}}