{"id": "1605.07110", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Deep Learning without Poor Local Minima", "abstract": "In this paper, we substantiate a conjecture published in 1989 and in part also address an open problem that was announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the assumption of independence adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs between flat networks (with three layers) and deeper networks (with more than three layers). Furthermore, we prove that the same four statements apply to deep linear neural networks with arbitrary depth, arbitrary width, and without unrealistic assumptions. As a result, we represent an instance for which we can answer the following question: How difficult is it to learn a deep model directly in theory? It is still more difficult than the gap between conventional learning machine (because of the non-conventional learning theory) and the non-conventional learning machine (because).", "histories": [["v1", "Mon, 23 May 2016 17:34:20 GMT  (33kb,D)", "http://arxiv.org/abs/1605.07110v1", null], ["v2", "Mon, 22 Aug 2016 14:26:22 GMT  (39kb,D)", "http://arxiv.org/abs/1605.07110v2", "In NIPS 2016. Selected for NIPS oral presentation (top 2% submissions). ---- This accepted version's contents remain the same as v1 (presentation was improved by following the reviewers' suggestions)"], ["v3", "Tue, 27 Dec 2016 22:47:50 GMT  (39kb)", "http://arxiv.org/abs/1605.07110v3", "In NIPS 2016. Selected for NIPS oral presentation (top 2% submissions). ---- The final NIPS 2016 version: the results remain the same"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["kenji kawaguchi"], "accepted": true, "id": "1605.07110"}, "pdf": {"name": "1605.07110.pdf", "metadata": {"source": "CRF", "title": "Deep Learning without Poor Local Minima", "authors": ["Kenji Kawaguchi"], "emails": ["kawaguch@mit.edu"], "sections": [{"heading": null, "text": "In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice."}, {"heading": "1 Introduction", "text": "Deep learning has been a great practical success in many fields, including the fields of computer vision, machine learning, and artificial intelligence. In addition to its practical success, theoretical results have shown that deep learning is attractive in terms of its generalization properties (Livni et al., 2014; Mhaskar et al., 2016). That is, deep learning introduces good function classes that may have a low capacity in the VC sense while being able to represent target functions of interest well. However, deep learning requires us to deal with seemingly intractable optimization problems. Typically, training of a deep model is conducted via non-convex optimization. Because finding a global minimum of a general non-convex function is an NP-complete problem (Murty & Kabadi, 1987), a hope is that a function induced by a deep model has some structure that makes the non-convex optimization tractable. Unfortunately, it was shown in 1992 that training a very simple neural network is indeed NP-hard (Blum & Rivest, 1992). In the past, such theoretical concerns in optimization played a major role in shrinking the field of deep learning. That is, many researchers instead favored classical machining learning models (with or without a kernel approach) that require only convex optimization. While the recent great practical successes have revived the field, we do not yet know what makes optimization in deep learning tractable in theory.\nIn this paper, as a step toward establishing the optimization theory for deep learning, we prove a conjecture noted in (Goodfellow et al., 2016) for deep linear networks, and also address an open problem announced in (Choromanska et al., 2015b) for deep nonlinear networks. Moreover, for both the conjecture and the open problem, we prove more general and tighter statements than those previously given.\nar X\niv :1\n60 5.\n07 11\n0v 1\n[ st\nat .M\nL ]\n2 3\nM ay\n2 01"}, {"heading": "2 Deep linear neural networks", "text": "Given the absence of a theoretical understanding of deep nonlinear neural networks, Goodfellow et al. (2016) noted that it is beneficial to theoretically analyze the loss functions of simpler models, i.e., linear neural networks. The function class of a linear neural network only contains functions that are linear with respect to inputs. However, their loss functions are non-convex in the weight parameters and thus nontrivial. Saxe et al. (2014) empirically showed that the optimization of deep linear models exhibits similar properties to those of the optimization of deep nonlinear models. Ultimately, for theoretical development, it is natural to start with linear models before working with nonlinear models (Baldi & Lu, 2012), and yet even for linear models, the understanding is scarce when the models become deep."}, {"heading": "2.1 Model and notation", "text": "We begin by defining the notation. Let H be the number of hidden layers, and let (X,Y ) be the training data set, with Y \u2208 Rdy\u00d7m and X \u2208 Rdx\u00d7m, where m is the number of data points. Here, dy \u2265 1 and dx \u2265 1 are the number of components (or dimensions) of the outputs and inputs, respectively. We denote the model (weight) parameters by W , which consists of parameter matrices corresponding to each layer: WH+1 \u2208 Rdy\u00d7dH , . . . ,Wk \u2208 Rdk\u00d7dk\u22121 , . . . ,W1 \u2208 Rd1\u00d7dx . Here, dk represents the width of the k-th layer, where the 0-th layer is the input layer and the (H + 1)-th layer is the output layer (i.e., d0 = dx and dH+1 = dy). Let Idk be the dk \u00d7 dk identity matrix. Let p = min(dH , . . . , d1) be the smallest width of a hidden layer. We denote the (j, i)-th entry of a matrix M by Mj,i. We also denote the j-th row vector of M by Mj,\u00b7 and the i-th column vector of M by M\u00b7,i.\nWe can then write the output of a feedforward deep linear model, Y (W,X) \u2208 Rdy\u00d7m, as Y (W,X) = WH+1WHWH\u22121 \u00b7 \u00b7 \u00b7W2W1X.\nWe consider one of the most widely used loss functions, squared error loss:\nL\u0304(W ) = 1 2 m\u2211 i=1 \u2016Y (W,X)\u00b7,i \u2212 Y\u00b7,i\u201622 = 1 2 \u2016Y (W,X)\u2212 Y \u20162F ,\nwhere \u2016\u00b7\u2016F is the Frobenius norm. Note that 2m L\u0304(W ) is the usual mean squared error, for which all of our theorems hold as well, since multiplying L\u0304(W ) by a constant in W results in an equivalent optimization problem."}, {"heading": "2.2 Background", "text": "Recently, Goodfellow et al. (2016) remarked that when Baldi & Hornik (1989) stated and proved Proposition 2.1 for shallow linear networks, they also stated Conjecture 2.2 for deep linear networks.\nProposition 2.1 (Baldi & Hornik, 1989: shallow linear network) Assume that H = 1 (i.e., Y (W,X) = W2W1X), assume that XXT and XY T are invertible, and assume that p < dx, p < dy and dy = dx (e.g., an autoencoder). Then, the loss function L\u0304(W ) has the following properties:\n(i) It is convex in each matrix W1 (or W2) when the other W2 (or W1) is fixed.\n(ii) Every local minimum is a global minimum.\nConjecture 2.2 (Baldi & Hornik, 1989: deep linear network) Assume the same set of conditions as in Proposition 2.1 except for H = 1. Then, the loss function L\u0304(W ) has the following properties:\n(i) For any k \u2208 {1, . . . ,H + 1}, it is convex in each matrix Wk when for all k\u2032 6= k, Wk\u2032 is fixed.\n(ii) Every local minimum is a global minimum.\nBaldi & Lu (2012) recently provided a proof for Conjecture 2.2 (i), leaving the proof of Conjecture 2.2 (ii) for future work. They also noted that the case of p \u2265 dx = dx is of interest, but requires further analysis, even for a shallow network with H = 1. An informal discussion of Conjecture 2.2 can be found in (Baldi, 1989). In Appendix D in the supplementary material, we provide a more detailed discussion of this subject."}, {"heading": "2.3 Results", "text": "We now state our main theoretical results for deep linear networks, which imply Conjecture 2.2 (ii) as well as obtain further information regarding the critical points with more generality.\nTheorem 2.3 (Loss surface of deep linear networks with more generality) Assume that XXT and XY T are full rank. Further, assume that dy \u2264 dx. Then, for any depth H \u2265 1 and for any layer widths and any input-output dimensions dy, dH , dH\u22121, . . . , d1, dx (the widths can arbitrarily differ from each other and from dy and dx), the loss function L\u0304(W ) has the following properties:\n(i) It is non-convex and non-concave.\n(ii) Every local minimum is a global minimum.\n(iii) Every critical point that is not a global minimum is a saddle point.\n(iv) If rank(WH \u00b7 \u00b7 \u00b7W2) = p, then the Hessian at any saddle point has at least one (strictly) negative eigenvalue.1\nCorollary 2.4 (Effect of deepness on the loss surface) Assume the same set of conditions as in Theorem 2.3 and consider the loss function L\u0304(W ). For three-layer networks (i.e., H = 1), the Hessian at any saddle point has at least one (strictly) negative eigenvalue. In contrast, for networks deeper than three layers (i.e., H \u2265 2), there exist saddle points at which the Hessian does not have any negative eigenvalue.\nThe full rank assumptions on XXT and XY T in Theorem 2.3 are realistic and practically easy to satisfy, as discussed in previous work (e.g., Baldi & Hornik, 1989). In contrast to related previous work (Baldi & Hornik, 1989; Baldi & Lu, 2012), we do not assume the invertibility of XY T , p < dx, p < dy nor dy = dx. In Theorem 2.3, p \u2265 dx is allowed, as well as many other relationships among the widths of the layers. Therefore, Theorem 2.3 (ii) implies Conjecture 2.2 (ii) and is more general than Conjecture 2.2 (ii). Moreover, Theorem 2.3 (iv) and Corollary 2.4 provide additional information regarding the important properties of saddle points.\nTheorem 2.3 presents an instance of a deep model that is not too difficult to train with direct greedy optimization, such as gradient-based methods. If there are \u201cbad\u201d local minima with large loss values everywhere, we would have to search the entire space,2 the volume of which increases exponentially with the number of variables. This is a major cause of NP-hardness for non-convex optimization. In contrast, if there are no poor local minima as Theorem 2.3 (ii) states, then saddle points are the remaining concern in terms of tractability.3 Because the Hessian of L\u0304(W ) is Lipschitz continuous, if the Hessian at a saddle point has a negative eigenvalue, it starts appearing as we approach the saddle point. Thus, Theorem 2.3 and Corollary 2.4 suggest that for 1-hidden layer networks, training can be done in polynomial time with a second order method or even with a modified stochastic gradient decent method, as discussed in (Ge et al., 2015). For deeper networks, Corollary 2.4 states that there exist \u201cbad\u201d saddle points in the sense that the Hessian at the point has no negative eigenvalue. However, from Theorem 2.3 (iv), we know exactly when this can happen, and from the proof of Theorem 2.3, we see that some perturbation is sufficient to escape such bad saddle points."}, {"heading": "3 Deep nonlinear neural networks", "text": "Given this understanding of the loss surface of deep linear models, we discuss deep nonlinear models."}, {"heading": "3.1 Model", "text": "We use the same notation as for the deep linear models, defined in the beginning of Section 2.1. The output of deep nonlinear neural network, Y\u0302 (W,X) \u2208 Rdy\u00d7m, is defined as\nY\u0302(W,X) = q\u03c3H+1(WH+1\u03c3H(WH\u03c3H\u22121(WH\u22121 \u00b7 \u00b7 \u00b7 \u03c32(W2\u03c31(W1X)) \u00b7 \u00b7\u00b7))),\n1If H = 1, to be succinct, we define WH \u00b7 \u00b7 \u00b7W2 = W1 \u00b7 \u00b7 \u00b7W2 , Id1 , with a slight abuse of notation. 2Typically, we do this by assuming smoothness in the values of the loss function. 3Other problems such as the ill-conditioning can make it difficult to obtain a fast convergence rate.\nwhere q \u2208 R is simply a normalization factor, the value of which is specified later. Here, \u03c3k : Rdk\u00d7m \u2192 Rdk\u00d7m is the element-wise rectified linear function:\n\u03c3k   b11 . . . b1m... . . . ... bdk1 \u00b7 \u00b7 \u00b7 bdkm   =  \u03c3\u0304(b11) . . . \u03c3\u0304(b1m)... . . . ... \u03c3\u0304(bdk1) \u00b7 \u00b7 \u00b7 \u03c3\u0304(bdkm)  , where \u03c3\u0304(bij) = max(0, bij). In practice, we usually set \u03c3H+1 to be an identity map in the last layer, in which case all our theoretical results still hold true."}, {"heading": "3.2 Background", "text": "Following the work by Dauphin et al. (2014), Choromanska et al. (2015a) investigated the connection between the loss functions of deep nonlinear networks and a function well-studied via random matrix theory (i.e., the Hamiltonian of the spherical spin-glass model). They explained that their theoretical results relied on several unrealistic assumptions. Later, Choromanska et al. (2015b) suggested at the Conference on Learning Theory (COLT) 2015 that discarding these assumptions is an important open problem. The assumptions were labeled A1p, A2p, A3p, A4p, A5u, A6u, and A7p.\nHere, we discuss the most relevant assumptions: A1p, A5u, and A6u. We refer to the part of assumption A1p (resp. A5u) that corresponds only to the model assumption as A1p-m (resp. A5u-m). Note that assumptions A1p-m and A5u-m are explicitly used in the previous work (Choromanska et al., 2015a) and included in A1p and A5u (i.e., we are not making new assumptions here). As the model Y\u0302 (W,X) \u2208 Rdy\u00d7m represents a directed acyclic graph, we can express an output from one of the units in the output layer as\nY\u0302 (W,X)j,i = q \u03a8j\u2211 p=1 [Xi](j,p)[Zi](j,p) H+1\u220f k=1 w (k) (j,p),\nwhere \u03a8j is the total number of paths from the inputs to the j-th output in the directed acyclic graph. In addition, [Xi](j,p) \u2208 R represents the entry of the i-th sample input datum that is used in the p-th path of the j-th output. For each layer k, w(k)(j,p) \u2208 R is the entry of Wk that is used in the p-th path of the j-th output. Finally, [Zi](j,p) \u2208 {0, 1} represents whether the p-th path of the j-th output is active ([Zi](j,p) = 1) or not ([Zi](j,p) = 0) for each sample i because of the rectified linear activation.\nAssumption A1p-m assumes that the Z\u2019s are Bernoulli random variables with the same probability of success, Pr([Zi](j,p) = 1) = \u03c1 for all i and (j, p). Assumption A5u-m assumes that the Z\u2019s are independent from the input X\u2019s, parameters w\u2019s, and each other (the independence was required, for example, in the first equation of the proof of Theorem 3.3 in (Choromanska et al., 2015a)). With assumptions A1p-m and A5u-m, we can write EZ [Y\u0302 (W,X)j,i] = q \u2211\u03a8j p=1[Xi](j,p)\u03c1 \u220fH k=1 w (k) (j,p).\nThe previous work also assumes the use of \u201cindependent random\u201d loss functions. Consider the hinge loss, Lhinge(W )j,i = max(0, 1 \u2212 Yj,iY\u0302 (W,X)j,i). By modeling the max operator as a Bernoulli random variable \u03be, we can then write Lhinge(W )j,i = \u03be\u2212q \u2211\u03a8j p=1 Yj,i[Xi](j,p)\u03be[Zi](j,p) \u220fH+1 k=1 w (k) (j,p). A1p then assumes that for all i and (j, p), the \u03be[Zi](j,p) are Bernoulli random variables with equal probabilities of success. Furthermore, A5u assumes that the independence of \u03be[Zi](j,p), Yj,i[Xi](j,p), and w(j,p). Finally, A6u assumes that Yj,i[Xi](j,p) for all (j, p) and i are independent.\nProposition 3.1 (High-level description of a main result in Choromanska et al., 2015a) Assume A1p (including A1p-m), A2p, A3p, A4p, A5u (including A5u-m), A6u, and A7p (Choromanska et al., 2015b). Furthermore, assume that dy = 1. Then, the expected loss of each sample datum, E\u03be,Z [Lhinge(W )i,1], has the following property: above a certain loss value, the number of local minima diminishes exponentially as the loss value increases.\nChoromanska et al. (2015b) noted that A6u is unrealistic because it implies that the inputs are not shared among the paths. In addition, A5u is unrealistic because it implies that the activation of any path is independent of the input data."}, {"heading": "3.3 Results", "text": "We now state our main theoretical results for deep nonlinear networks, which partially address the aforementioned open problem and lead to more general and tighter results. Unlike the previous work,\nwe do not assume that we can take the expectation over random variable \u03be. Moreover, we consider loss functions for all the data points and all possible output dimensionalities (i.e., vectored-valued output). More concretely, we consider the expected squared error loss, EZ [L(W )] = EZ [ 12\u2016Y\u0302 (W,X)\u2212Y \u2016 2 F ]. We also consider the squared error loss of the expected model, LEZ [Y\u0302 ](W ) = 1 2\u2016E[Y\u0302 (W,X)]\u2212Y \u2016 2 F .\nTheorem 3.2 (Loss surface of deep nonlinear networks) Assume A1p-m and A5u-m. Further assume that dy \u2264 dx and that XXT and XY T are full rank. Let q = \u03c1\u22121. Then, for any depth H \u2265 1 and for any layer widths and any input-output dimensions dy, dH , dH\u22121, . . . , d1, dx (the widths can arbitrarily differ from each other and from dy and dx), both the expected loss function EZ [L(W )] and the loss function of the expected model LEZ [Y\u0302 ](W ) have the following properties:\n(i) They are non-convex and non-concave.\n(ii) Every local minimum is a global minimum.\n(iii) Every critical point that is not a global minimum is a saddle point.\n(iv) If rank(WH \u00b7 \u00b7 \u00b7W2) = p, then the Hessian at any saddle point has at least one (strictly) negative eigenvalue.4\nCorollary 3.3 (Effect of deepness on the loss surface) Assume the same set of conditions as in Theorem 3.2. Consider the loss function EZ [L(W )] or LEZ [Y\u0302 ](W ) . Then, for three-layer networks (i.e., H = 1), the Hessian at any saddle point has some (strictly) negative eigenvalue. In contrast, for networks deeper than three layers (i.e., H \u2265 2), there exist saddle points at which the Hessian does not have a negative eigenvalue.\nComparing Theorem 3.2 and Proposition 3.1, we can see that we successfully discarded assumptions A2p, A3p, A4p, A6u, and A7p while obtaining a tighter statement in general. Again, note that the full rank assumptions on XXT and XY T in Theorem 3.2 are realistic and practically easy to satisfy, as discussed in previous work (e.g., Baldi & Hornik, 1989). Furthermore, our model Y\u0302 is strictly more general than the model analyzed in (Choromanska et al., 2015a,b) (i.e., this paper\u2019s model class contains the previous work\u2019s model class but not vice versa)."}, {"heading": "4 Important lemmas", "text": "In this section, we provide additional theoretical results as lemmas that lead to further insights. The proofs of the lemmas are in the appendix in the supplementary material.\nLet M \u2297M \u2032 be the Kronecker product of M and M \u2032. Let Dvec(WTk )f(\u00b7) = \u2202f(\u00b7)\n\u2202 vec(WT\nk )\nbe the partial\nderivative of f with respect to vec(WTk ) in the numerator layout. That is, if f : Rdin \u2192 Rdout , we have Dvec(WTk )f(\u00b7) \u2208 R\ndout\u00d7(dkdk\u22121). Let R(M) be the range (or the column space) of a matrix M . Let M\u2212 be any generalized inverse of M . When we write a generalized inverse in a condition or statement, we mean it for any generalized inverse (i.e., we omit the universal quantifier over generalized inverses, as this is clear). Let r = (Y (W,X)\u2212 Y )T \u2208 Rm\u00d7dy be an error matrix. Let C = WH+1 \u00b7 \u00b7 \u00b7W2 \u2208 Rdy\u00d7d1 . When we write Wk \u00b7 \u00b7 \u00b7W \u2032k, we generally intend that k > k\u2032 and the expression denotes a product over Wj for integer k \u2265 j \u2265 k\u2032. For notational compactness, two additional cases can arise: when k = k\u2032, the expression denotes simply Wk, and when k < k\u2032, it denotes Idk . For example, in the statement of Lemma 4.1, if we set k := H + 1, we have that WH+1WH \u00b7 \u00b7 \u00b7WH+2 , Idy . In Lemma 4.6 and the proofs of Theorems 2.3 and 3.2, we use the following additional notation. Let \u03a3 = Y XT (XXT )\u22121XY T and its eigendecomposition be U\u039bUT = \u03a3, where the entries of the eigenvalues are ordered as \u039b1,1 \u2265 . . . \u2265 \u039bdy,dy with corresponding orthogonal eigenvector matrix U = [u1, . . . , udy ]. For each k \u2208 {1, . . . dy}, uk \u2208 Rdy\u00d71 is a column eigenvector. As \u03a3 is real symmetric, we can always make U orthogonal. Let p\u0304 = rank(C) \u2208 {1, . . . ,min(dy, p)}. We define a matrix containing the subset of the p\u0304 largest eigenvectors as Up\u0304 = [u1, . . . , up\u0304]. Given any ordered set Ip\u0304 = {i1, . . . , ip\u0304 | 1 \u2264 i1 < \u00b7 \u00b7 \u00b7 < ip\u0304 \u2264 min(dy, p)}, we define a matrix containing the subset of the corresponding eigenvectors as UIp\u0304 = [ui1 , . . . , uip\u0304 ]. Note the difference between Up\u0304 and UIp\u0304 .\n4If H = 1, to be succinct, we define WH \u00b7 \u00b7 \u00b7W2 = W1 \u00b7 \u00b7 \u00b7W2 , Id1 , with a slight abuse of notation.\nLemma 4.1 (Critical point necessary and sufficient condition) W is a critical point of L\u0304(W ) if and only if for all k \u2208 {1, ...,H + 1},(\nDvec(WTk )L\u0304(W ) )T = ( WH+1WH \u00b7 \u00b7 \u00b7Wk+1 \u2297 (Wk\u22121 \u00b7 \u00b7 \u00b7W2W1X)T )T vec(r) = 0.\nLemma 4.2 (Representation at critical point) If W is a critical point of L\u0304(W ), then\nWH+1WH \u00b7 \u00b7 \u00b7W2W1 = C(CTC)\u2212CTY XT (XXT )\u22121.\nLemma 4.3 (Block Hessian with Kronecker product) Write the entries of \u22072L\u0304(W ) in a block form as\n\u22072L\u0304(W ) =  Dvec(WTH+1) ( Dvec(WTH+1)L\u0304(W ) )T \u00b7 \u00b7 \u00b7 Dvec(WT1 ) ( Dvec(WTH+1)L\u0304(W ) )T ... . . . ...\nDvec(WTH+1) ( Dvec(WT1 )L\u0304(W ) )T \u00b7 \u00b7 \u00b7 Dvec(WT1 ) ( Dvec(WT1 )L\u0304(W )\n)T  .\nThen, for any k \u2208 {1, ...,H + 1}, Dvec(WTk ) ( Dvec(WTk )L\u0304(W ) )T = ( (WH+1 \u00b7 \u00b7 \u00b7Wk+1)T (WH+1 \u00b7 \u00b7 \u00b7Wk+1)\u2297 (Wk\u22121 \u00b7 \u00b7 \u00b7W1X)(Wk\u22121 \u00b7 \u00b7 \u00b7W1X)T ) ,\nand, for any k \u2208 {2, ...,H + 1}, Dvec(WTk ) ( Dvec(WT1 )L\u0304(W ) )T = ( CT (WH+1 \u00b7 \u00b7 \u00b7Wk+1)\u2297X(Wk\u22121 \u00b7 \u00b7 \u00b7W1X)T ) +\n[(Wk\u22121 \u00b7 \u00b7 \u00b7W2)T \u2297X] [Idk\u22121 \u2297 (rWH+1 \u00b7 \u00b7 \u00b7Wk+1)\u00b7,1 . . . Idk\u22121 \u2297 (rWH+1 \u00b7 \u00b7 \u00b7Wk+1)\u00b7,dk ] .\nLemma 4.4 (Hessian semidefinite necessary condition) If \u22072L\u0304(W ) is positive semidefinite or negative semidefinite at a critical point, then for any k \u2208 {2, ...,H + 1},\nR((Wk\u22121 \u00b7 \u00b7 \u00b7W3W2)T ) \u2286 R(CTC) or XrWH+1WH \u00b7 \u00b7 \u00b7Wk+1 = 0.\nCorollary 4.5 If\u22072L\u0304(W ) is positive semidefinite or negative semidefinite at a critical point, then for any k \u2208 {2, ...,H + 1},\nrank(WH+1WH \u00b7 \u00b7 \u00b7Wk) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W3W2) or XrWH+1WH \u00b7 \u00b7 \u00b7Wk+1 = 0.\nLemma 4.6 (Hessian positive semidefinite necessary condition) If\u22072L\u0304(W ) is positive semidefinite at a critical point, then\nC(CTC)\u2212CT = Up\u0304U T p\u0304 or Xr = 0."}, {"heading": "5 Proof sketches of theorems", "text": "We now provide overviews of the proofs of Theorems 2.3 and 3.2. We complete the proofs of the theorems in the appendix in the supplementary material.\nOur proof approach largely differs from those in previous work (Baldi & Hornik, 1989; Baldi & Lu, 2012; Choromanska et al., 2015a,b). In contrast to (Baldi & Hornik, 1989; Baldi & Lu, 2012), we need a different approach to deal with the \u201cbad\u201d saddle points that start appearing when the model becomes deeper (see Section 2.3), as well as to obtain more comprehensive properties of the critical points with more generality. While the previous proofs heavily rely on the first-order information, the main parts of our proofs take advantage of the second order information. In contrast, Choromanska et al. (2015a,b) used the seven assumptions to relate the loss functions of deep models to a function previously analyzed with a tool of random matrix theory (i.e., Gaussian orthogonal ensemble). With no reshaping assumptions (A3p, A4p, and A6u), we cannot relate our loss function to such a function. Moreover, with no distributional assumptions (A2p and A6u) (except the activation), our Hessian\nis deterministic, and therefore, even random matrix theory itself is insufficient for our purpose. Furthermore, with no spherical constraint assumption (A7p), the number of local minima in our loss function can be uncountable.\nOne natural strategy to proceed toward Theorems 2.3 and 3.2 would be to use the first order and the second order necessary conditions of local minima (e.g., the gradient is zero and the Hessian is positive semidefinite).5 However, are the first-order and second-order conditions sufficient to prove Theorems 2.3 and 3.2? Corollaries 2.4 and 3.3 show that the answer is negative for deep models with H \u2265 2, while it is affirmative for shallow models with H = 1. Thus, for deep models, a simple use of the first-order and second-order information is insufficient to characterize the properties of each critical point. In addition to the complexity of the Hessian of the deep models, this suggests that we must strategically extract the second order information. Accordingly, we obtained an organized representation of the Hessian in Lemma 4.3 and strategically extracted the information in Lemmas 4.4 and 4.6, with which we are ready to prove Theorems 2.3 and 3.2.\n5.1 Proof sketch of Theorem 2.3 (ii)\nBy case analysis, we show that any point that satisfies the necessary conditions and the definition of a local minimum is a global minimum.\nCase I: rank(WH \u00b7 \u00b7 \u00b7W2) = p and dy \u2264 p: Assume that rank(WH \u00b7 \u00b7 \u00b7W2) = p. If dy < p, Corollary 4.5 with k = H + 1 implies the necessary condition that Xr = 0. If dy = p, Lemma 4.6 with k = H + 1 and k = 2, combined with the fact thatR(C) \u2286 R(Y XT ), implies the necessary condition that Xr = 0. Therefore, we have the necessary condition, Xr = 0 . Interpreting condition Xr = 0, we conclude that W achieving Xr = 0 is indeed a global minimum.\nCase II: rank(WH \u00b7 \u00b7 \u00b7W2) = p and dy > p: From Lemma 4.6, we have the necessary condition that C(CTC)\u2212CT = Up\u0304U T p\u0304 or Xr = 0. If Xr = 0, using the exact same proof as in Case I, it is a global minimum. Suppose then thatC(CTC)\u2212CT = Up\u0304Up\u0304. From Lemma 4.4 with k = H+1, we conclude that p\u0304 , rank(C) = p. Then, from Lemma 4.2, we write WH+1 \u00b7 \u00b7 \u00b7W1 = UpUpY XT (XXT )\u22121, which is the orthogonal projection onto the subspace spanned by the p eigenvectors corresponding to the p largest eigenvalues following the ordinary least square regression matrix. This is indeed the expression of a global minimum.\nCase III: rank(WH \u00b7 \u00b7 \u00b7W2) < p: Suppose that rank(WH \u00b7 \u00b7 \u00b7W2) < p. From Lemma 4.4, we have the following necessary condition for the Hessian to be (positive or negative) semidefinite at a critical point: for any k \u2208 {2, . . . ,H + 1},\nR((Wk\u22121 \u00b7 \u00b7 \u00b7W2)T ) \u2286 R(CTC) or XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0,\nwhere the first condition is shown to imply rank(WH+1 \u00b7 \u00b7 \u00b7Wk) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) in Corollary 4.5. We repeatedly apply these conditions for k = 2, . . . ,H+1 to claim that with arbitrarily small > 0, we can perturb each parameter (i.e., each entry of WH , . . . ,W2) such that rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) without changing the value of L\u0304(W ). We prove this by induction on k, using Lemmas 4.2, 4.4, and 4.6.\nWe consider the base case, k = 2. From the condition with k = 2 of Lemma 4.4, we have that rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 d1 \u2265 p or XrWH+1 \u00b7 \u00b7 \u00b7W3 = 0 (note that d1 \u2265 p \u2265 p\u0304 by their definitions). The former condition is false since rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2264 rank(WH \u00b7 \u00b7 \u00b7W2) < p. From the latter condition, for an arbitrary L2, with A2 = WH+1 \u00b7 \u00b7 \u00b7W3,\n0 = XrWH+1 \u00b7 \u00b7 \u00b7W3 \u21d4W2W1 = ( AT2 A2 )\u2212 AT2 Y X\nT (XXT )\u22121 + (I \u2212 (AT2 A2)\u2212AT2 A2)L2 \u21d4WH+1 \u00b7 \u00b7 \u00b7W1 = A2 ( AT2 A2 )\u2212 AT2 Y X T (XXT )\u22121\n= C(CTC)\u2212CTY XT (XXT )\u22121 = Up\u0304U T p\u0304 Y X T (XXT )\u22121,\n5For a non-convex and non-differentiable function, we can still have a first-order and second-order necessary condition (e.g., Rockafellar & Wets, 2009, theorem 13.24, p. 606).\nwhere the last two equalities follow Lemmas 4.2 and 4.6 (since if Xr = 0, we immediately obtain the desired result). Since XY T is full rank with dy \u2264 dx (i.e., rank(XY T ) = dy),\nA2 ( AT2 A2 )\u2212 A2 = Up\u0304U T p\u0304 = Up\u0304(U T p\u0304 Up\u0304) \u22121UTp\u0304 .\nFrom this, with extra steps, we can deduce that we can have rank(W2) \u2265 min(p, dx) with arbitrarily small perturbation of each entry of W2 while retaining the loss value.\nThus, we conclude the proof for the base case with k = 2. For the inductive step with k \u2208 {3, . . . ,H + 1}, we essentially use the same proof procedure but with the inductive hypothesis that we can have rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) with arbitrarily small perturbation of each entry of Wk\u22121, . . . ,W2 without changing the loss value. We need the inductive hypothesis to conclude that the first condition in (R((Wk\u22121 \u00b7 \u00b7 \u00b7W2)T ) \u2286 R(CTC) or XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0) is false, and thus the second condition must be satisfied at a candidate point of a local minima.\nWe then conclude the induction, proving that we can have rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) with arbitrarily small perturbation of each parameter without changing the value of L\u0304(W ). If p \u2264 dx, this means that upon such a perturbation, we have the case of rank(WH \u00b7 \u00b7 \u00b7W2) = p. Thus, such a critical point is not a local minimum unless it is a global minimum. If p > dx, upon such a perturbation, we have rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 dx. Thus, WH+1 \u00b7 \u00b7 \u00b7W1 = Up\u0304UTp\u0304 Y XT (XXT )\u22121 = UUTY XT (XXT )\u22121, which is a global minimum. Summarizing the above, any point that satisfies the definition (and necessary conditions) of a local minimum is indeed a global minimum. Therefore, we conclude the proof sketch of Theorem 2.3 (ii).\n5.2 Proof sketch of Theorem 2.3 (i), (iii) and (iv)\nWe can prove the non-convexity and non-concavity of this function simply from its Hessian (Theorem 2.3 (i)). That is, we can show that in the domain of the function, there exist points at which the Hessian becomes indefinite. Indeed, The domain contains uncountably many points at which the Hessian is indefinite.\nWe now consider Theorem 2.3 (iii): every critical point that is not a global minimum is a saddle point. Combined with Theorem 2.3 (ii), which is proven independently, this is equivalent to the statement that there are no local maxima. We first show that if WH \u00b7 \u00b7 \u00b7W1 6= 0, the loss function is strictly convex in one of the coordinates. This means that there is always an increasing direction and hence no local maximum. If WH \u00b7 \u00b7 \u00b7W1 = 0, we show that at a critical point, if the Hessian is negative semidefinite, we can have WH \u00b7 \u00b7 \u00b7W1 6= 0 with arbitrarily small perturbation without changing the loss value. We can prove this by induction on k = 1, . . . ,H , similar to the induction in the proof of Theorem 2.3 (ii).\nTheorem 2.3 (iv) follows Theorem 2.3 (ii)-(iii) and the fact that when rank(WH \u00b7 \u00b7 \u00b7W2) = p, if \u22072L\u0304(W ) 0 at a critical point, W is a global minimum (this is the statement obtained in the proof of Theorem 2.3 (ii) for the case, rank(WH \u00b7 \u00b7 \u00b7W2) = p)."}, {"heading": "5.3 Proof sketch of Theorem 3.2", "text": "Similarly to the previous work (Choromanska et al., 2015a,b), we relate our loss function to another function under the adopted assumptions. More concretely, we show that all the theoretical results developed so far for the loss function of the deep linear models, L\u0304(W ), hold true for the loss functions of the deep nonlinear models, EZ [L(W )] and LEZ [Y\u0302 ](W )."}, {"heading": "6 Conclusion", "text": "In this paper, we addressed some open problems, pushing forward the theoretical foundations of deep learning and non-convex optimization. For deep linear neural networks, we proved the aforementioned conjecture and more detailed statements with more generality. For deep nonlinear neural networks with rectified linear activation, when compared with the previous work, we proved a tighter statement with more generality (dy can vary) and with strictly weaker model assumptions (only two assumptions out of seven). However, our theory does not yet directly apply to the practical situation. To fill the gap between theory and practice, future work would further discard the remaining two out of the seven assumptions made in previous work."}, {"heading": "Acknowledgments", "text": "The author would like to thank Prof. Leslie Kaelbling for her thoughtful comments on the paper. We gratefully acknowledge support from NSF grant 1420927, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors."}, {"heading": "A Proofs of lemmas and corollary in Section 4", "text": "We complete the proofs of the lemmas and corollary in Section 4.\nA.1 Proof of Lemma 4.1\nProof Since L\u0304(W ) = 12\u2016Y (W,X)\u2212 Y \u2016 2 F= 1 2 vec(r) T vec(r), Dvec(WTk )L\u0304(W ) = ( Dvec(r)L\u0304(W ) ) ( Dvec(WTk ) vec(r) ) = vec(r)T ( Dvec(WTk ) vec(X T IdxW T 1 \u00b7 \u00b7 \u00b7WTH+1Idy )\u2212Dvec(WTk ) vec(Y T ) )\n= vec(r)T ( Dvec(WTk )(WH+1 \u00b7 \u00b7 \u00b7Wk+1 \u2297 (Wk\u22121 \u00b7 \u00b7 \u00b7W1X) T ) vec(WTk ) )\n= vec(r)T ( WH+1 \u00b7 \u00b7 \u00b7Wk+1 \u2297 (Wk\u22121 \u00b7 \u00b7 \u00b7W1X)T ) .\nBy setting ( Dvec(WTk )L\u0304(W ) )T = 0 for all k \u2208 {1, ...,H + 1}, we obtain the statement of Lemma 4.1. For the boundary conditions (i.e., k = H + 1 or k = 1), it can be seen from the second to the third lines that we obtain the desired results with the definition, Wk \u00b7 \u00b7 \u00b7Wk+1 , Idk (i.e., WH+1 \u00b7 \u00b7 \u00b7WH+2 , Idy and W0 \u00b7 \u00b7 \u00b7W1 , Idx ).\nA.2 Proof of Lemma 4.2\nProof From the critical point condition with respect to W1 (Lemma 4.1), 0 = ( Dvec(WTk )L\u0304(W ) )T = ( WH+1 \u00b7 \u00b7 \u00b7W2 \u2297XT )T vec(r) = vec(XrWH+1 \u00b7 \u00b7 \u00b7W2),\nwhich is true if and only if XrWH+1 \u00b7 \u00b7 \u00b7W2 = 0. By expanding r, 0 = XXTWT1 CTC \u2212XY TC. By solving for W1,\nW1 = (C TC)\u2212CTY XT (XXT )\u22121 + (I \u2212 (CTC)\u2212CTC)L,\nfor an arbitrary matrix L. Due to the property of any generalized inverse (Zhang, 2006, p. 41), we have that C(CTC)\u2212CTC = C. Thus,\nCW1 = C(C TC)\u2212CTY XT (XXT )\u22121 + (C \u2212 C(CTC)\u2212CTC)L = C(CTC)\u2212CTY XT (XXT )\u22121.\nA.3 Proof of Lemma 4.3\nProof For the diagonal blocks: the entries of diagonal blocks are obtained simply using the result of Lemma 4.1 as\nDvec(WTk ) ( Dvec(WTk )L\u0304(W ) )T = ( WH+1 \u00b7 \u00b7 \u00b7Wk+1 \u2297 (Wk\u22121 \u00b7 \u00b7 \u00b7W1X)T )T Dvec(WTk ) vec(r). Using the formula of Dvec(WTk ) vec(r) computed in the proof of of Lemma 4.1 yields the desired result.\nFor the off-diagonal blocks with k = 2, ...,H:\nDvec(WTk )[Dvec(WT1 )L\u0304(W )] T\n= ( WH+1 \u00b7 \u00b7 \u00b7W2 \u2297X)T )T Dvec(WTk ) vec(r) + (Dvec(WTk )WH+1 \u00b7 \u00b7 \u00b7Wk+1 \u2297XT)T vec(r)\nThe first term above is reduced to the first term of the statement in the same way as the diagonal blocks. For the second term,(\nDvec(WTk )WH+1 \u00b7 \u00b7 \u00b7W2 \u2297X T )T vec(r)\n= m\u2211 i=1 dy\u2211 j=1 (( Dvec(WTk )WH+1,jWH \u00b7 \u00b7 \u00b7W2 ) \u2297XTi )T ri,j\n= m\u2211 i=1 dy\u2211 j=1 ( (Ak)j,\u00b7 \u2297BTk \u2297XTi )T ri,j\n= m\u2211 i=1 dy\u2211 j=1 [ (Ak)j,1 ( BTk \u2297Xi ) . . . (Ak)j,dk ( BTk \u2297Xi )] ri,j\n= [( BTk \u2297 \u2211m i=1 \u2211dy j=1 ri,j(Ak)j,1Xi ) . . . ( BTk \u2297 \u2211m i=1 \u2211dy j=1 ri,j(Ak)j,dkXi )] .\nwhere Ak = WH+1 \u00b7 \u00b7 \u00b7Wk+1 and Bk = Wk\u22121 \u00b7 \u00b7 \u00b7W2. The third line follows the fact that (WH+1,jWH \u00b7 \u00b7 \u00b7W2)T = vec(WT2 \u00b7 \u00b7 \u00b7WTHWTH+1,j) = (WH+1,j \u00b7 \u00b7 \u00b7Wk+1 \u2297 WT2 \u00b7 \u00b7 \u00b7WTk\u22121) vec(WTk ). In the last line, we have the desired result by rewriting\u2211m i=1 \u2211dy j=1 ri,j(Ak)j,tXi = X(rWH+1 \u00b7 \u00b7 \u00b7Wk+1)\u00b7,t.\nFor the off-diagonal blocks with k = H + 1: The first term in the statement is obtained in the same way as above (for the off-diagonal blocks with k = 2, ...,H). For the second term, notice that vec(WTH+1) = [ (WH+1) T 1,\u00b7 . . . (WH+1) T dy,\u00b7 ]T\nwhere (WH+1)j,\u00b7 is the j-th row vector of WH+1 or the vector corresponding to the j-th output component. That is, it is conveniently organized as the blocks, each of which corresponds to each output component (or rather we chose vec(WTk ) instead of vec(Wk) for this reason, among others). Also,( Dvec(WTH+1)WH+1 \u00b7 \u00b7 \u00b7W2 \u2297X T )T vec(r) =\n= [\u2211m i=1 (( D(WH+1)T1,\u00b7C1,\u00b7 ) \u2297XTi )T ri,1 . . . \u2211m i=1 (( D(WH+1)Tdy,\u00b7Cdy,\u00b7 ) \u2297XTi )T ri,dy ] ,\nwhere we also used the fact that m\u2211 i=1 dy\u2211 j=1 (( Dvec((WH+1)Tt,\u00b7)Cj,\u00b7 ) \u2297XTi )T ri,j = m\u2211 i=1 (( Dvec((WH+1)Tt,\u00b7)Ct,\u00b7 ) \u2297XTi )T ri,t.\nFor each block entry t = 1, . . . , dy in the above, similarly to the case of k = 2, ...,H , m\u2211 i=1 (( Dvec((WH+1)Tt,\u00b7)Cj,\u00b7 ) \u2297XTi )T ri,t = ( BTH+1 \u2297 m\u2211 i=1 ri,t(AH+1)j,tXi ) .\nHere, we have the desired result by rewriting \u2211m i=1 ri,t(AH+1)j,1Xi = X(rIdy )\u00b7,t = Xr\u00b7,t.\nA.4 Proof of Lemma 4.4\nProof Note that a similarity transformation preserves the eigenvalues of a matrix. For each k \u2208 {2, . . . ,H+1}, we take a similarity transform of\u22072L\u0304(W ) (whose entries are organized as in Lemma 4.3) as\nP\u22121k \u2207 2L\u0304(W )Pk =  Dvec(WT1 ) ( Dvec(WT1 )L\u0304(W ) )T Dvec(WTk ) ( Dvec(WT1 )L\u0304(W ) )T \u00b7 \u00b7 \u00b7 Dvec(WT1 ) ( Dvec(WTk )L\u0304(W ) )T Dvec(WTk ) ( Dvec(WTk )L\u0304(W ) )T \u00b7 \u00b7 \u00b7\n... ...\n. . .  Here, Pk = [ eH+1 ek P\u0303k ] is the permutation matrix where ei is the i-th element of the standard\nbasis (i.e., a column vector with 1 in the i-th entry and 0 in every other entries), and P\u0303k is any\narbitrarily matrix that makes Pk to be a permutation matrix. Let Mk be the principal submatrix of P\u22121k \u22072L\u0304(W )Pk that consists of the first four blocks appearing in the above equation. Then,\n\u22072L\u0304(W ) 0 \u21d2 \u2200k \u2208 {2, . . . ,H + 1},Mk 0 \u21d2 \u2200k \u2208 {2, . . . ,H + 1},R(Dvec(WTk )(Dvec(WT1 )L\u0304(W )) T ) \u2286 R(Dvec(WT1 )(Dvec(WT1 )L\u0304(W )) T ),\nHere, the first implication follows the necessary condition with any principal submatrix and the second implication follows the necessary condition with the Schur complement (Zhang, 2006, theorem 1.20, p. 44).\nNote that R(M \u2032) \u2286 R(M) \u21d4 (I \u2212MM\u2212)M \u2032 = 0 (Zhang, 2006, p. 41). Thus, by plugging in the formulas of Dvec(WTk )(Dvec(WT1 )L\u0304(W )) T and Dvec(WT1 )(Dvec(WT1 )L\u0304(W )) T that are derived in Lemma 4.3, \u22072L\u0304(W ) 0\u21d2 \u2200k \u2208 {2, . . . ,H + 1}, 0 = ( I \u2212 (CTC \u2297 (XXT ))(CTC \u2297 (XXT ))\u2212 ) (CTAk \u2297BkW1X)\n+ ( I \u2212 (CTC \u2297 (XXT ))(CTC \u2297 (XXT ))\u2212 ) [BTk \u2297X] [ Idk\u22121 \u2297 (rAk)\u00b7,1 . . . Idk\u22121 \u2297 (rAk)\u00b7,dk ] where Ak = WH+1 \u00b7 \u00b7 \u00b7Wk+1 and Bk = Wk\u22121 \u00b7 \u00b7 \u00b7W2. Here, we can replace (CTC\u2297 (XXT ))\u2212 by ((CTC)\u2212 \u2297 (XXT )\u22121) (see Appendix A.7). Thus, I \u2212 (CTC \u2297 (XXT ))(CTC \u2297 (XXT ))\u2212can be replaced by (Id1 \u2297 Idy )\u2212 (CTC(CTC)\u2212 \u2297 Idy ) = (Id1 \u2212 CTC(CTC)\u2212)\u2297 Idy . Accordingly, the first term is reduced to zero as(\n(Id1 \u2212 C TC(CTC)\u2212)\u2297 Idy )( CTAk \u2297BkW1X ) = ((Id1 \u2212 C TC(CTC)\u2212)CTAk)\u2297BkW1X = 0,\nsince CTC(CTC)\u2212CT = CT (Zhang, 2006, p. 41). Thus, with the second term remained, the condition is reduced to\n\u2200k \u2208 {2, . . . ,H + 1},\u2200t \u2208 {1, . . . , dy}, (BTk \u2212 CTC(CTC)\u2212BTk )\u2297X(rAk)\u00b7,t = 0. This implies \u2200k \u2208 {2, . . . ,H + 1}, (R(BTk ) \u2286 R(CTC) or XrAk = 0), which concludes the proof for the positive semidefinite case. For the necessary condition of the negative semidefinite, we obtain the same condition since\n\u22072L\u0304(W ) 0 \u21d2 \u2200k \u2208 {2, . . . , H + 1},Mk 0\n\u21d2 \u2200k \u2208 {2, . . . , H + 1},R(\u2212Dvec(WT k )(Dvec(WT1 )L\u0304(W )) T ) \u2286 R(\u2212Dvec(WT1 )(Dvec(WT1 )L\u0304(W )) T )\n\u21d2 \u2200k \u2208 {2, . . . , H + 1},R(Dvec(WT k )(Dvec(WT1 )L\u0304(W )) T ) \u2286 R(Dvec(WT1 )(Dvec(WT1 )L\u0304(W )) T ).\nA.5 Proof of Corollary 4.5\nProof From the first condition in the statement of Lemma 4.4, R(WT2 \u00b7 \u00b7 \u00b7WTk\u22121) \u2286 R(WT2 \u00b7 \u00b7 \u00b7WTH+1WH+1 \u00b7 \u00b7 \u00b7W2) \u21d2 rank(WTk \u00b7 \u00b7 \u00b7WTH+1) \u2265 rank(WT2 \u00b7 \u00b7 \u00b7WTk\u22121)\u21d2 rank(WH+1 \u00b7 \u00b7 \u00b7Wk) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2).\nThe first implication follows the fact that the rank of a product of matrices is at most the minimum of the ranks of the matrices, and the fact that the column space of WT2 \u00b7 \u00b7 \u00b7WTH+1 is subspace of the column space of WT2 \u00b7 \u00b7 \u00b7WTk\u22121.\nA.6 Proof of Lemma 4.6\nProof For the (Xr = 0) condition: Let MH+1 be the principal submatrix as defined in the proof of Lemma 4.4 (the principal submatrix of P\u22121H+1\u22072L\u0304(W )PH+1 that consists of the first four blocks of it). Let Bk = Wk\u22121 \u00b7 \u00b7 \u00b7W2. Let F = BH+1W1XXTWT1 BTH+1. Using Lemma 4.3 for the blocks corresponding to W1 and WH+1,\nMH+1 =\n[ CTC \u2297XXT (CT \u2297XXT (BH+1W1)T ) + E\n(C \u2297BH+1W1XXT ) + ET Idy \u2297 F\n]\nwhere E = [ BTH+1 \u2297Xr\u00b7,1 . . . BTH+1 \u2297Xr\u00b7,dy ] . Then, by the necessary condition with the Schur complement (Zhang, 2006, theorem 1.20, p. 44), MH+1 0 implies\n0 = ((Idy \u2297 IdH )\u2212 (Idy \u2297 F )(Idy \u2297 F )\u2212)((C \u2297BH+1W1XXT ) + ET ) \u21d2 0 = (Idy \u2297 IdH \u2212 FF\u2212)(C \u2297BH+1W1XXT ) + (Idy \u2297 IdH \u2212 FF\u2212)ET\n= (Idy \u2297 IdH \u2212 FF\u2212)ET\n= IdH \u2212 FF \u2212 \u2297 I1 0\n. . . 0 IdH \u2212 FF\u2212 \u2297 I1\n BH+1 \u2297 (Xr\u00b7,1) T\n... BH+1 \u2297 (Xr\u00b7,dy )T  =  (IdH \u2212 FF \u2212)BH+1 \u2297 (Xr\u00b7,1)T\n... (IdH \u2212 FF\u2212)BH+1 \u2297 (Xr\u00b7,dy )T  where the second line follows the fact that (Idy \u2297F )\u2212 can be replaced by (Idy \u2297F\u2212) (see Appendix A.7). The third line follows the fact that (I \u2212 FF\u2212)BH+1W1X = 0 because R(BH+1W1X) = R(BH+1W1XXTWT1 BTH+1) = R(F ). In the fourth line, we expanded E and used the definition of the Kronecker product. It implies\nFF\u2212BH+1 = BH+1 or Xr = 0.\nHere, if Xr = 0, we obtained the statement of the lemma. Thus, from now on, we focus on the case where FF\u2212BH+1 = BH+1 and Xr 6= 0 to obtain the other condition, C(CTC)\u2212CT = Up\u0304Up\u0304.\nFor the (C(CTC)\u2212CT = Up\u0304Up\u0304) condition: By using another necessary condition of a matrix being positive semidefinite with the Schur complement (Zhang, 2006, theorem 1.20, p. 44), MH+1 0 implies that\n(Idy \u2297 F )\u2212 ( C \u2297BH+1W1XXT + ET ) (CTC \u2297XXT )\u2212 ( CT \u2297XXT (BH+1W1)T + E ) 0 (1)\nSince we can replace (CTC \u2297XXT )\u2212 by (CTC)\u2212 \u2297 (XXT )\u22121 (see Appendix A.7), the second term in the left hand side is simplified as(\nC \u2297BH+1W1XXT + ET ) (CTC \u2297XXT )\u2212 ( CT \u2297XXT (BH+1W1)T + E ) = (( C(CTC)\u2212 \u2297BH+1W1 ) + ET ( (CTC)\u2212 \u2297 (XXT )\u22121 ))(( CT \u2297XXT (BH+1W1)T ) + E\n) = ( C(CTC)\u2212CT \u2297 F ) + ET ( (CTC)\u2212 \u2297 (XXT )\u22121 ) E\n= ( C(CTC)\u2212CT \u2297 F ) + ( rTXT (XXT )\u22121Xr \u2297BH+1(CTC)\u2212BH+1 ) (2)\nIn the third line, the crossed terms \u2013 ( C(CTC)\u2212 \u2297BH+1W1 ) E and its transpose \u2013 are vanished\nto 0 because of the following. From Lemma 4.1, ( Idy \u2297 (WH \u00b7 \u00b7 \u00b7W1X)T )T vec(r) = 0 \u21d4\nWH \u00b7 \u00b7 \u00b7W1Xr = BH+1W1Xr = 0 at any critical point. Thus, ( C(CTC)\u2212 \u2297BH+1W1 ) E =[\nC(CTC)\u2212BTH+1 \u2297BH+1W1Xr\u00b7,1 . . . C(CTC)\u2212BTH+1 \u2297BH+1W1Xr\u00b7,dy ]\n= 0.The forth line follows\nE T ( (C T C) \u2212 \u2297 (XXT )\u22121 ) E = BH+1(C TC)\u2212BTH+1 \u2297 (r\u00b7,1) TXT (XXT )\u22121Xr\u00b7,1 \u00b7 \u00b7 \u00b7 BH+1(CTC)\u2212BTH+1 \u2297 (r\u00b7,1) TXT (XXT )\u22121Xr\u00b7,dy ... . . .\n... BH+1(C TC)\u2212BTH+1 \u2297 (r\u00b7,dy ) TXT (XXT )\u22121Xr\u00b7,1 \u00b7 \u00b7 \u00b7BH+1(CTC)\u2212BTH+1 \u2297 (r\u00b7,dy ) TXT (XXT )\u22121Xr\u00b7,dy  = r T X T (XX T ) \u22121 Xr \u2297 BH+1(CTC)\u2212BH+1,\nwhere the last line is due to the fact that \u2200t, (r\u00b7,t)TXT (XXT )\u22121Xr\u00b7,t is a scaler and the fact that\nfor any matrix L, rTLr =  (r\u00b7,1) TLr\u00b7,1 \u00b7 \u00b7 \u00b7 (r\u00b7,1)TLr\u00b7,dy . . . . . . . . .\n(r\u00b7,dy ) TLr\u00b7,1\u00b7 \u00b7 \u00b7(r\u00b7,dy ) TLr\u00b7,dy\n.\nFrom equations 1 and 2, MH+1 0\u21d2\n((Idy \u2212 C(CTC)\u2212CT )\u2297 F )\u2212 ( rTXT (XXT )\u22121Xr \u2297BH+1(CTC)\u2212BH+1 ) 0. (3)\nIn the following, we simplify equation 3 by first showing thatR(C) \u2286 R(\u03a3) and then simplifying C(CTC)\u2212CT , rTXT (XXT )\u22121Xr, F and BH+1(CTC)\u2212BH+1.\nShowing thatR(C) \u2286 R(\u03a3): Again, using Lemma 4.1 with k = H + 1,\n0 = BH+1W1Xr \u21d4 FWTH+1 = BH+1W1XY T \u21d4WTH+1 = F\u2212BH+1W1XY T +(I\u2212F\u2212F )L,\nfor any arbitrary matrix L. Then,\nC = WH+1BH+1\n= Y XTWT1 B T H+1F \u2212BH+1 + L T (I \u2212 FF\u2212)BH+1\n= Y XTWT1 B T H+1F \u2212BH+1,\nwhere the second equality follows the fact that we are conducting the case analysis with the case of FF\u2212BH+1 = BH+1 here. Using Lemma 4.1 with k = 1,\n0 = XrWH+1 \u00b7 \u00b7 \u00b7W2 \u21d4W1 = (CTC)\u2212CTY XT (XXT )\u22121 + (I \u2212 (CTC)\u2212CTC)L,\nfor any arbitrary matrix L. Pugging this formula of W1 into the above,\nC = Y XT ((CTC)\u2212CTY XT (XXT )\u22121 + (I \u2212 (CTC)\u2212CTC)L)TBTH+1F\u2212BH+1 = \u03a3C(CTC)\u2212BTH+1F \u2212BH+1\nwhere the second line follows Lemma 4.4 with k = H + 1 (i.e., CTC(CTC)\u2212BTH+1 = B T H+1). Thus, we have the desired result,R(C) \u2286 R(\u03a3). SimplifyingC(CTC)\u2212CT : Remember that p\u0304 is the rank ofC. To simplify the notation, we rearrange the entries of D and U such that the eigenvalues and eigenvectors selected by the index set Ip\u0304 comes\nfirst. That is, U = [UIp\u0304 U\u2212Ip\u0304 ] and \u039b = [ \u039bIp\u0304 0\n0 \u039b\u2212Ip\u0304\n] where U\u2212Ip\u0304 consists of all the eigenvectors\nthat are not contained in UIp\u0304 , and accordingly \u039bIp\u0304 (resp. \u039b\u2212Ip\u0304) consists of all the eigenvalues that correspond (resp. do not correspond) to the index set Ip\u0304. SinceR(C) \u2286 R(\u03a3), we can write C in the following form: for some index set Ip\u0304, C = [UIp\u0304 ,0]G1, where 0 \u2208 Rdy\u00d7(d1\u2212p\u0304) and G1 \u2208 GLd1(R) (a d1 \u00d7 d1 invertible matrix) (notice that d1 \u2265 p \u2265 p\u0304 by their definitions). Then,\n(CTC)\u2212 = (GT1 [UIp\u0304 ,0] T [UIp\u0304 ,0]G1) \u2212 = ( GT1 [ Ip\u0304 0 0 0 ] G1 )\u2212 .\nNote that the set of all generalized inverse of CTC = GT1 [ Ip\u0304 0 0 0 ] G1 is as follows (Zhang, 2006, p. 41): { G\u221211 [ Ip\u0304 L1 L2 L3 ] G\u2212T1 | L1, L2, L3 arbitrary } .\nThus, for any arbitrary L1, L2 and L3,\nC(CTC)\u2212CT = CG\u221211 [ Ip\u0304 L1 L2 L3 ] G\u2212T1 C T = [UIp\u0304 0] [ Ip\u0304 L1 L2 L3 ] [ UTIp\u0304 0 ] = UIp\u0304U T Ip\u0304 .\nSimplifying rTXT (XXT )\u22121Xr:\nrTXT (XXT )\u22121Xr = (CW1X \u2212 Y )XT (XXT )\u22121X(XT (CW1)T \u2212 Y T ) = CW1XX T (CW1) T \u2212 CW1XY T \u2212 Y XT (CW1)T + \u03a3\n= PC\u03a3PC \u2212 PC\u03a3\u2212 \u03a3PC + \u03a3 = \u03a3\u2212 Up\u0304\u039bIp\u0304UTp\u0304\nwhere PC = C(CTC)\u2212CT = UIp\u0304U T Ip\u0304 and the last line follows the facts:\nPC\u03a3PC = UIp\u0304U T Ip\u0304U\u039bU TUIp\u0304U T Ip\u0304 = UIp\u0304 [Ip\u0304 0]\n[ \u039bIp\u0304 0\n0 \u039b\u2212Ip\u0304\n] [ Ip\u0304 0 ] UTIp\u0304 = UIp\u0304\u039bIp\u0304U T Ip\u0304 ,\nPC\u03a3 = UIp\u0304U T Ip\u0304U\u039bU T = UIp\u0304 [Ip\u0304 0]\n[ \u039bIp\u0304 0\n0 \u039b\u2212Ip\u0304 ] [ UTIp\u0304 U\u2212Ip\u0304 ] = UTIp\u0304\u039bIp\u0304UIp\u0304 ,\nand similarly, \u03a3PC = UTIp\u0304\u039bIp\u0304UIp\u0304 .\nSimplifying F : In the proof of Lemma 4.2, by using Lemma 4.1 with k = 1, we obtained that W1 = (C\nTC)\u2212CTY XT (XXT )\u22121 + (I \u2212 (CTC)\u2212CTC)L. Also, from Lemma 4.4, we have that Xr = 0 or BH+1(CTC)\u2212CTC = (CTC(CTC)\u2212BTH+1)\nT = BH+1. If Xr = 0, we got the statement of the lemma, and so we consider the case of BH+1(CTC)\u2212CTC = BH+1. Therefore,\nBH+1W1 = BH+1(C TC)\u2212CTY XT (XXT )\u22121.\nSince F = BH+1W1XXTWT1 B T H+1,\nF = BH+1(C TC)\u2212CT\u03a3C(CTC)\u2212BTH+1.\nFrom Lemma 4.4 with k = H + 1, R(BTH+1) \u2286 R(CTC) = R(BTH+1WTH+1WH+1BH+1) \u2286 R(BTH+1), which implies that R(BTH+1) = R(CTC). Therefore, R(C(CTC)\u2212BTH+1) = R(C(CTC)\u2212) = R(C) \u2286 R(\u03a3). Accordingly, we can write it in the form, C(CTC)\u2212BTH+1 = [UIp\u0304 ,0]G2, where 0 \u2208 Rdy\u00d7(d1\u2212p\u0304) and G2 \u2208 GLd1(R) (we can write it in the form of [UIp\u0304\u2032 ,0]G2 for some Ip\u0304\u2032 because of the inclusion \u2286 R(\u03a3) and Ip\u0304\u2032 = Ip\u0304 because of the equality = R(C)). Thus,\nF = GT2 [ UTIp\u0304 0 ] U\u039bUT [UIp\u0304 ,0]G2 = G T 2 [ Ip\u0304 0 0 0 ] \u039b [ Ip\u0304 0 0 0 ] G2 = G T 2 [ \u039bIp\u0304 0 0 0 ] G2.\nSimplifying BH+1(CTC)\u2212BH+1: From Lemma 4.4, CTC(CTC)\u2212BH+1 = BH+1 (again since we are done if Xr = 0). Thus, BH+1(CTC)\u2212BH+1 = BH+1(CTC)\u2212CTC(CTC)\u2212BTH+1. As discussed above, we write C(CTC)\u2212BTH+1 = [UIp\u0304 ,0]G2. Thus,\nBH+1(C TC)\u2212BH+1 = G T 2 [ UTIp\u0304 0 ] [UIp\u0304 ,0]G2 = G T 2 [ Ip\u0304 0 0 0 ] G2.\nPutting results together: We use the simplified formulas of C(CTC)\u2212CT , rTXT (XXT )\u22121Xr, F and BH+1(CTC)\u2212BH+1 in equation 3, obtaining\n((Idy \u2212 UIp\u0304UTIp\u0304)\u2297G T 2 [ \u039bIp\u0304 0 0 0 ] G2)\u2212 ( (\u03a3\u2212 Up\u0304\u039bIp\u0304UTp\u0304 )\u2297GT2 [ Ip\u0304 0 0 0 ] G2 ) 0.\nDue to the Sylvester\u2019s law of inertia (Zhang, 2006, theorem 1.5, p. 27), with a nonsingular matrix U \u2297G\u221212 (it is nonsingular because each of U and G \u22121 2 is nonsingular), the necessary condition is reduced to( U \u2297G\u221212 )T (( (Idy \u2212 UIp\u0304U T Ip\u0304 ) \u2297G T 2 [ \u039bIp\u0304 0\n0 0\n] G2 ) \u2212 ( (\u03a3 \u2212 Up\u0304\u039bIp\u0304U T p\u0304 ) \u2297GT2 [ Ip\u03040\n0 0\n] G2 ))( U \u2297G\u221212 ) = (( Idy \u2212 [ Ip\u0304 0\n0 0\n]) \u2297 [ \u039bIp\u0304 0\n0 0\n]) \u2212 (( \u039b \u2212 [ \u039bI\u0304\u2018p 0\n0 0\n]) \u2297 [ Ip\u0304 0\n0 0\n])\n=\n([ 0 0\n0 I(dy\u2212p\u0304)\n] \u2297 [ \u039bIp\u0304 0\n0 0\n]) \u2212 ([ 0 0\n0 \u039b\u2212Ip\u0304\n] \u2297 [ Ip\u0304 0\n0 0\n])\n=  0 0 0 \u039bIp\u0304 \u2212 (\u039b\u2212Ip\u0304 )1,1Ip\u0304 0\n. . . 0 \u039bIp\u0304 \u2212 (\u039b\u2212Ip\u0304 )(dy\u2212p\u0304),(dy\u2212p\u0304)Ip\u0304  0, which implies that for all (i, j) \u2208 {(i, j) | i \u2208 {1, . . . , p\u0304}, j \u2208 {1, . . . , (dy \u2212 p\u0304)}}, (\u039bIp\u0304)i,i \u2265 (\u039b\u2212Ip\u0304)j,j . In other words, the index set Ip\u0304 must select the largest p\u0304 eigenvalues whatever p\u0304 is. Since C(CTC)\u2212CT = UIp\u0304U T Ip\u0304 (which is obtained above), we have that C(C\nTC)\u2212CT = Up\u0304Up\u0304 in this case.\nSummarizing the above case analysis, if\u22072L\u0304(W ) 0 at a critical point, C(CTC)\u2212CT = Up\u0304Up\u0304 or Xr = 0.\nA.7 Generalized inverse of Kronecker product\n(A\u2212 \u2297B\u2212) is a generalized inverse of A\u2297B.\nProof For a matrix M , the definition of a generalized inverse, M\u2212, is MM\u2212M = M . Setting M := A \u2297 B, we check if (A\u2212 \u2297 B\u2212) satisfies the definition: (A \u2297 B)(A\u2212 \u2297 B\u2212)(A \u2297 B) = (AA\u2212A\u2297BB\u2212B) = (A\u2297B) as desired.\nWe avoid discussing the other direction as it is unnecessary in this paper (i.e., we avoid discussing if (A\u2212 \u2297 B\u2212) is the only generalized inverse of A \u2297 B). Notice that the necessary condition that we have in our proof (where we need a generalized inverse of A \u2297 B) is for any generalized inverse of A\u2297B. Thus, replacing it by one of any generalized inverse suffices to obtain a necessary condition. Indeed, choosing Moore\u2212Penrose pseudoinverse suffices here, with which we know (A\u2297B)\u2020 = (A\u2020 \u2297B\u2020). But, to give a simpler argument later, we keep more generality by choosing (A\u2212 \u2297B\u2212) as a generalized inverse of A\u2297B."}, {"heading": "B Proof of Theorems 2.3 and 3.2", "text": "We complete the proofs of Theorems 2.3 and 3.2.\nB.1 Proof of Theorem 2.3 (ii)\nProof By case analysis, we show that any point that satisfies the necessary conditions and the definition of a local minimum is a global minimum. When we write a statement in the proof, we often mean that a necessary condition of local minima implies the statement as it should be clear (i.e., we are not claiming that the statement must hold true unless the point is the candidate of local minima.).\nThe case where rank(WH \u00b7 \u00b7 \u00b7W2) = p and dy \u2264 p: Assume that rank(WH \u00b7 \u00b7 \u00b7W2) = p. We first obtain a necessary condition of the Hessian being positive semidefinite at a critical point,Xr = 0, and then interpret the condition. If dy < p, Corollary 4.5 with k = H + 1 implies the necessary condition that Xr = 0. This is because the other condition p > rank(WH+1) \u2265 rank(WH \u00b7 \u00b7 \u00b7W2) = p is false.\nIf dy = p, Lemma 4.6 with k = H + 1 implies the necessary condition that Xr = 0 or R(WH \u00b7 \u00b7 \u00b7W2) \u2286 R(CTC). Suppose that R(WH \u00b7 \u00b7 \u00b7W2) \u2286 R(CTC). Then, we have that p = rank(WH \u00b7 \u00b7 \u00b7W2) \u2264 rank(CTC) = rank(C). That is, rank(C) \u2265 p. From Corollary 4.5 with k = 2 implies the necessary condition that\nrank(C) \u2265 rank(Id1) or XrWH+1 \u00b7 \u00b7 \u00b7W3 = 0.\nSuppose the latter: XrWH+1 \u00b7 \u00b7 \u00b7W3 = 0. Since rank(WH+1 \u00b7 \u00b7 \u00b7W3) \u2265 rank(C) \u2265 p and dH+1 = dy = p, the left null space of WH+1 \u00b7 \u00b7 \u00b7W3 contains only zero. Thus,\nXrWH+1 \u00b7 \u00b7 \u00b7W3 = 0\u21d2 Xr = 0.\nSuppose the former: rank(C) \u2265 rank(Id1). Because dy = p \u2264 d1, rank(C) \u2265 p, and R(C) \u2286 R(Y XT ) as shown in the proof of Lemma 4.6, we have thatR(C) = R(Y XT ).\nrank(C) \u2265 rank(Id1)\u21d2 CTC is full rank \u21d2 Xr = XY TC(CTC)\u22121CT \u2212XY T = 0,\nwhere the last equality follows the fact that (Xr)T = C(CTC)\u22121CTY XT \u2212 Y XT = 0 since R(C) = R(Y XT ) and thereby the projection of Y XT onto the range of C is Y XT . Therefore, we have the condition, Xr = 0 when dy \u2264 p. To interpret the condition Xr = 0, consider a loss function with a linear model without any hidden layer, f(W \u2032) = \u2016W \u2032X \u2212 Y \u20162F where W \u2032 \u2208 Rdy\u00d7dx . Then, any point satisfying Xr\u2032 = 0 is a global minimum of f , where r\u2032 = (W \u2032X \u2212 Y )T is an error matrix.6 For any values of WH+1 \u00b7 \u00b7 \u00b7W1, there exists W \u2032 such that W \u2032 = WH+1 \u00b7 \u00b7 \u00b7W1 (the opposite is also true when dy \u2264 p although\n6Proof: Any point satisfying Xr\u2032 = 0 is a critical point of f , which directly follows the proof of Lemma 4.1. Also, f is convex since its Hessian is positive semidefinite for all input WH+1, and thus any critical point of f is a global minimum. Combining the pervious two statements results in the desired claim.\nwe don\u2019t need it in our proof). That is, R(L\u0304) \u2286 R(f) and R(r) \u2286 R(r\u2032) (as functions of W and W \u2032 respectively) (the equality is also true when dy \u2264 p although we don\u2019t need it in our proof). Summarizing the above, whenever Xr = 0, there exists W \u2032 = WH+1 \u00b7 \u00b7 \u00b7W1 such that Xr = Xr\u2032 = 0, which achieves the global minimum value of f , f\u2217 and f\u2217 \u2264 L\u0304\u2217 (i.e., the global minimum value of f is at most the global minimum value of L\u0304 since R(L\u0304) \u2286 R(f)). In other words, WH+1 \u00b7 \u00b7 \u00b7W1 achieving Xr = 0 attains a global minimum value of f that is at most the global minimum value of L\u0304. This means that WH+1 \u00b7 \u00b7 \u00b7W1 achieving Xr = 0 is a global minimum. Thus, we have proved that when rank(WH \u00b7 \u00b7 \u00b7W2) = p and dy \u2264 p, if \u22072L\u0304(W ) 0 at a critical point, it is a global minimum.\nThe case where rank(WH \u00b7 \u00b7 \u00b7W2) = p and dy > p: We first obtain a necessary condition of the Hessian being positive semidefinite at a critical point and then interpret the condition. From Lemma 4.6, we have that C(CTC)\u2212CT = Up\u0304UTp\u0304 or Xr = 0. If Xr = 0, with the exact same proof as in the case of dy \u2264 p, it is a global minimum. Suppose that C(CTC)\u2212CT = Up\u0304Up\u0304. Combined with Lemma 4.2, we have a necessary condition:\nWH+1 \u00b7 \u00b7 \u00b7W1 = C(CTC)\u2212CTY XT (XXT )\u22121 = Up\u0304Up\u0304Y XT (XXT )\u22121.\nFrom Lemma 4.4 with k = H + 1, R(WT2 \u00b7 \u00b7 \u00b7WTH) \u2286 R(CTC) = R(CT ), which implies that p\u0304 , rank(C) = p (since rank(WH \u00b7 \u00b7 \u00b7W2) = p). Thus, we can rewrite the above equation as WH+1 \u00b7 \u00b7 \u00b7W1 = UpUpY XT (XXT )\u22121, which is the orthogonal projection on to subspace spanned by the p eigenvectors corresponding to the p largest eigenvalues following the ordinary least square regression matrix. This is indeed the expression of a global minimum (Baldi & Hornik, 1989; Baldi & Lu, 2012).\nThus, we have proved that when rank(WH \u00b7 \u00b7 \u00b7W2) = p, if \u22072L\u0304(W ) 0 at a critical point, it is a global minimum.\nThe case where rank(WH \u00b7 \u00b7 \u00b7W2) < p: Suppose that rank(WH \u00b7 \u00b7 \u00b7W2) < p. From Lemma 4.4, we have a following necessary condition for the Hessian to be (positive or negative) semidefinite at a critical point: for any k \u2208 {2, . . . ,H + 1},\nR((Wk\u22121 \u00b7 \u00b7 \u00b7W2)T ) \u2286 R(CTC) or XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0,\nwhere the first condition is shown to imply rank(WH+1 \u00b7 \u00b7 \u00b7Wk) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) in Corollary 4.5. We repeatedly apply these conditions for k = 2, . . . ,H+1 to claim that with arbitrarily small > 0, we can perturb each parameter (i.e., each entry of WH , . . . ,W2) such that rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) without changing the value of L\u0304(W ). Let Ak = WH+1 \u00b7 \u00b7 \u00b7Wk+1. From Corollary 4.5 with k = 2, we have that rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 d1 \u2265 p or XrWH+1 \u00b7 \u00b7 \u00b7W3 = 0 (note that d1 \u2265 p \u2265 p\u0304 by their definitions). The former condition is false since rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2264 rank(WH \u00b7 \u00b7 \u00b7W2) < p. From the latter condition, for an arbitrary L2,\n0 = XrWH+1 \u00b7 \u00b7 \u00b7W3 \u21d4W2W1 = ( AT2 A2 )\u2212 AT2 Y X T (XXT )\u22121 + (I \u2212 (AT2 A2)\u2212AT2 A2)L2 (4)\n\u21d4WH+1 \u00b7 \u00b7 \u00b7W1 = A2 ( AT2 A2 )\u2212 AT2 Y X T (XXT )\u22121\n= C(CTC)\u2212CTY XT (XXT )\u22121 = Up\u0304U T p\u0304 Y X T (XXT )\u22121,\nwhere the last two equalities follow Lemmas 4.2 and 4.6 (since if Xr = 0, we immediately obtain the desired result as discussed above). Taking transpose,\n(XXT )\u22121XY TA2 ( AT2 A2 )\u2212 AT2 = (XX T )\u22121XY TUp\u0304U T p\u0304 ,\nwhich implies that XY TA2 ( AT2 A2 )\u2212 A2 = XY TUp\u0304Up\u0304.\nSince XY T is full rank with dy \u2264 dx (i.e., rank(XY T ) = dy), there exists a left inverse and the solution of the above linear system is unique as ((XY T )TXY T )\u22121(XY T )TXY T = I , yielding,\nA2 ( AT2 A2 )\u2212 A2 = Up\u0304U T p\u0304 (= Up\u0304(U T p\u0304 Up\u0304) \u22121UTp\u0304 ).\nIn other words,R(A2) = R(C) = R(Up\u0304).\nSuppose that (AT2 A2) \u2208 Rd2\u00d7d2 is nonsingular. Then, since R(A2) = R(C), we have that rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) = rank(A2) = d2 \u2265 p, which is false in the case being analyzed (the case of rank(WH \u00b7 \u00b7 \u00b7W2) < p). Thus, AT2 A2 is singular. If AT2 A2 is singular, from equation 4, it is inferred that we can perturb W2 to have rank(W2W1) \u2265 min(p, dx). To see this in a concrete algebraic way, first note that since R(A2) = R(Up\u0304), we can write A2 = [Up\u0304 0]G2 for some G2 \u2208 GLd2(R) where 0 \u2208 Rdy\u00d7(d2\u2212p\u0304). Thus,\nAT2 A2 = G T 2 [ Ip\u0304 0 0 0 ] G2.\nAgain, note that the set of all generalized inverse of GT2 [ Ip\u0304 0 0 0 ] G2 is as follows (Zhang, 2006, p. 41): { G\u221212 [ Ip\u0304 L \u2032 1\nL\u20322 L \u2032 3\n] G\u2212T2 | L\u20321, L\u20322, L\u20323 arbitrary } .\nSince equation 4 must hold for any generalized inverse, we choose a generalized inverse with L\u20321 = L \u2032 2 = L \u2032 3 = 0 for simplicity. That is,\n(AT2 A2) \u2212 := G\u221212 [ Ip\u0304 0 0 0 ] G\u2212T2 .\nThen, plugging this into equation 4, for an arbitrary L2,\nW2W1 = G \u22121 2 [ UTp\u0304 0 ] Y XT (XXT )\u22121 + (Id2 \u2212G\u221212 [ Ip\u0304 0 0 0 ] G2)L2\n= G\u221212\n[ UTp\u0304 Y X T (XXT )\u22121\n0\n] +G\u221212 [ 0 0 0 I(d2\u2212p\u0304) ] G2L2\n= G\u221212\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]G2L2\n] .\nHere, [0 I(d2\u2212p\u0304)]G2L2 \u2208 R(d2\u2212p\u0304)\u00d7dx is the last (d2 \u2212 p\u0304) rows of G2L2. Since rank(Y XT (XXT )\u22121) = dy (because the multiplication with the invertible matrix preserves the rank), the first p\u0304 rows in the above have rank p\u0304. Thus, W2W1 has rank at least p\u0304, and the possible rank deficiency comes from the last (d2\u2212 p\u0304) rows, [0 I(d2\u2212p\u0304)]G2L2. Since WH+1 \u00b7 \u00b7 \u00b7W1 = A2W2W1 = [Up\u0304 0]G2W2W1,\nWH+1 \u00b7 \u00b7 \u00b7W1 = [Up\u0304 0] [ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]G2L2\n] = Up\u0304U T p\u0304 Y X T (XXT )\u22121.\nThis means that changing the values of the last (d2 \u2212 p\u0304) rows of G2L2 (i.e., [0 I(d2\u2212p\u0304)]G2L2) does not change the value of L\u0304(W ). Therefore, the original necessary condition implies a necessary condition that without changing the loss value, we can make W2W1 to have full rank with arbitrarily small perturbation of the last (d2\u2212 p\u0304) rows as [0 I(d2\u2212p\u0304)]G2L2 + Mptb where Mptb is a perturbation matrix with arbitrarily small > 0.7\n7We have only proved that the submatrix of the first p\u0304 rows has rank p\u0304 and that changing the value of the last d2 \u2212 p\u0304 rows does not change the loss value. That is, we have not proven the exitance of Mptb that makes W2W1 full rank. Although this is trivial since the set of full matrices is dense, we show a proof in\nthe following to be complete. Let p\u0304\u2032 \u2265 p\u0304 be the rank of W2W1. That is, in [ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]G2L2\n] , there\nexist p\u0304\u2032 linearly independent row vectors including the first p\u0304 row vectors, denoted by b1, . . . , bp\u0304\u2032 \u2208 R1\u00d7dx . Then, we denote the rest of row vectors by v1, v2, . . . , vd2\u2212p\u0304\u2032 \u2208 R\n1\u00d7dx . Let c = min(d2 \u2212 p\u0304\u2032, dx \u2212 p\u0304\u2032). There exist linearly independent vectors v\u03041, v\u03042, . . . , v\u0304c such that the set, {b1, . . . , bp\u0304\u2032 , v\u03041, v\u03042, . . . , v\u0304c}, is linearly independent. Setting vi := vi + v\u0304i for all i \u2208 {1, . . . , c} makes W2W1 full rank since v\u0304i cannot be expressed as a linear combination of other vectors. Thus, a desired perturbation matrix Mptb can be obtained by setting Mptb to consists of v\u03041, v\u03042, . . . , v\u0304c row vectors for the corresponding rows and 0 row vectors for other rows.\nNow, we show that such a perturbation can be done via a perturbation of the entries of W2. From the above equation for W2W1, all the possible solutions of W2 can be written as: for an arbitrary L0 and L2,\nW2 = G \u22121 2\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]G2L2\n] W \u20201 + L T 0 (I \u2212W1W \u2020 1 ),\nwhere M\u2020 is the the Moore\u2014Penrose pseudoinverse of M . Thus, we perturb W2 as\nW2 := W2 + G \u22121 2\n[ 0\nMptb\n] W \u20201 = G \u22121 [ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]G2L2 + Mptb\n] W \u20201 + L T 0 (I \u2212W1W \u2020 1 ).\nNote that upon such a perturbation, equation 4 may not hold anymore; i.e.,\nG\u221212\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]GL2 + Mptb\n] W \u20201W1 6= G \u22121 2 [ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]GL2 + Mptb\n] .\nThis means that the original necessary condition that implies equation 4 no longer holds. In this case, we immediately conclude that the Hessian is no longer positive semidefinite and thus the point is a saddle point. We thereby consider the remaining case: equation 4 still holds. Then, with the perturbation on the entries of W1,\nW2W1 = G \u22121 2\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(d2\u2212p\u0304)]G2L2 + Mptb\n] ,\nas desired.\nThus, we showed that we can have rank(W2) \u2265 rank(W2W1) \u2265 min(p, dx), with arbitrarily small perturbation of each entry of W2 with the loss value being remained. To prove the corresponding results for Wk \u00b7 \u00b7 \u00b7W2 for any k = 2, ...,H + 1, we conduct induction on k = 2, . . . ,H + 1 with the same proof procedure. The proposition P (k) to be proven is as follows: the necessary conditions with j \u2264 k imply that we can have rank(Wk \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) with arbitrarily small perturbation of each entry of Wk, . . .W2 without changing the loss value. For the base case k = 2, we have already proved the proposition in the above.\nFor the inductive step with k \u2208 {3, . . . ,H + 1}, we have the inductive hypothesis that we can have rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) with arbitrarily small perturbation of each entry of Wk\u22121, . . .W2 without changing the loss value. Accordingly, suppose that rank(Wk\u22121 \u00b7 \u00b7 \u00b7W1) \u2265 min(p, dx). Again, from Lemma 4.4, for any k \u2208 {3, . . . ,H + 1},\nR((Wk\u22121 \u00b7 \u00b7 \u00b7W2)T ) \u2286 R(CTC) or XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0. If the former is true, rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx), which is the desired statement (it immediately implies the proposition P (k) for any k). If the latter is true, for an arbitrary Lk,\n0 = XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 \u21d4Wk \u00b7 \u00b7 \u00b7W1 = ( ATkAk )\u2212 ATk Y X T (XXT )\u22121 + (I \u2212 (ATkAk)\u2212ATkAk)Lk (5)\n\u21d4WH+1 \u00b7 \u00b7 \u00b7W1 = Ak ( ATkAk )\u2212 ATk Y X T (XXT )\u22121\n= C(CTC)\u2212CTY XT (XXT )\u22121 = Up\u0304U T p\u0304 Y X T (XXT )\u22121,\nwhere the last two equalities follow Lemmas 4.2 and 4.6. Taking transpose, (XXT )\u22121XY TAk ( ATkAk )\u2212 ATk = (XX T )\u22121XY TUp\u0304U T p\u0304 ,\nwhich implies that XY TAk ( ATkAk )\u2212 Ak = XY\nTUp\u0304Up\u0304. Since XY T is full rank with dy \u2264 dx (i.e., rank(XY T ) = dy), there exists a left inverse and the solution of the above linear system is unique as ((XY T )TXY T )\u22121(XY T )TXY T = I , yielding,\nAk ( ATkAk )\u2212 Ak = Up\u0304U T p\u0304 (= Up\u0304(U T p\u0304 Up\u0304) \u22121UTp\u0304 ).\nIn other words,R(Ak) = R(C) = R(Up\u0304).\nSuppose that (ATkAk) \u2208 Rdk\u00d7dk is nonsingular. Then, sinceR(Ak) = R(C), rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) = rank(Ak) = dk \u2265 p, which is false in the case being analyzed (the case of\nrank(WH \u00b7 \u00b7 \u00b7W2) < p). Thus, ATkAk is singular. Notice that for the boundary case with k = H + 1, ATkAk = Idy , which is always nonsingular and thus the proof ends here (i.e., For the case with k = H + 1, since the latter condition, XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0, implies a false statement, the former condition, rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) \u2265 min(p, dx), which is the desired statement, must be true).\nIf ATkAk is singular, from equation 5, it is inferred that we can perturb Wk to have rank(Wk \u00b7 \u00b7 \u00b7W1) \u2265 min(p, dx). To see this in a concrete algebraic way, first note that since R(Ak) = R(Up\u0304), we can write Ak = [Up\u0304 0]Gk for some Gk \u2208 GLdk(R) where 0 \u2208 Rdy\u00d7(dk\u2212p\u0304). Then, similarly to the base case with k = 2, plugging this into the condition in equation 5: for an arbitrary Lk,\nWk \u00b7 \u00b7 \u00b7W1 = G\u22121k\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GkLk\n] .\nSince rank(Y XT (XXT )\u22121) = dy, the first p\u0304 rows in the above have rank p\u0304. Thus, Wk \u00b7 \u00b7 \u00b7W1 has rank at least p\u0304. On the other hand, since WH+1 \u00b7 \u00b7 \u00b7W1 = AkWk \u00b7 \u00b7 \u00b7W1 = [Up\u0304 0]GWk \u00b7 \u00b7 \u00b7W1,\nWH+1 \u00b7 \u00b7 \u00b7W1 = [Up\u0304 0] [ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GkLk\n] = Up\u0304U T p\u0304 Y X T (XXT )\u22121,\nwhich means that changing the values of the last (dk \u2212 p\u0304) rows of Wk \u00b7 \u00b7 \u00b7W1 does not change the value of L\u0304(W ). Therefore, the original necessary condition implies a necessary condition that without changing the loss value, we can make Wk \u00b7 \u00b7 \u00b7W1 to have full rank with arbitrarily small perturbation on the last (dk \u2212 p\u0304) rows as [0 I(dk\u2212p\u0304)]GkLk + Mptb where Mptb is a perturbation matrix with arbitrarily small > 0 (a proof of the existence of a corresponding perturbation matrix is exactly the same as the proof in the base case with k = 2, which is in footnote 7).\nSimilarly to the base case with k = 2, we can conclude that this perturbation can be down via a perturbation on each entry of Wk. From the above equation for Wk \u00b7 \u00b7 \u00b7W1, all the possible solutions of Wk can be written as: for an arbitrary L0 and Lk,\nWk = G \u22121 k\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GkLk\n] (Wk\u22121 \u00b7 \u00b7 \u00b7W1)\u2020 + LT0 (I \u2212 (Wk\u22121 \u00b7 \u00b7 \u00b7W1)(Wk\u22121 \u00b7 \u00b7 \u00b7W1)\u2020).\nThus, we perturb Wk as\nWk := Wk + G \u22121 k\n[ 0\nMptb\n] (Wk\u22121 \u00b7 \u00b7 \u00b7W1)\u2020\n= G\u22121k\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GkLk + Mptb\n] (Wk\u22121 \u00b7 \u00b7 \u00b7W1)\u2020 + LT0 (I \u2212 (Wk\u22121 \u00b7 \u00b7 \u00b7W1)(Wk\u22121 \u00b7 \u00b7 \u00b7W1)\u2020).\nNote that upon such a perturbation, equation 5 may not hold anymore; i.e.,\nG\u22121k\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GkLk + Mptb\n] (Wk\u22121 \u00b7 \u00b7 \u00b7W1)\u2020(Wk\u22121 \u00b7 \u00b7 \u00b7W1) 6= G\u22121 [ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GL2 + Mptb\n] .\nThis means that the original necessary condition that implies equation 5 no longer holds. In this case, we immediately conclude that the Hessian is no longer positive semidefinite and thus the point is a saddle point. We thereby consider the remaining case: equation 5 still holds. Then, with the perturbation on the entries of Wk,\nWH+1 \u00b7 \u00b7 \u00b7W1 = G\u22121k\n[ UTp\u0304 Y X T (XXT )\u22121\n[0 I(dk\u2212p\u0304)]GkLk + Mptb\n] ,\nas desired. Therefore, we have that rank(Wk \u00b7 \u00b7 \u00b7W2) \u2265 rank(Wk \u00b7 \u00b7 \u00b7W1) \u2265 min(p, dx) upon such a perturbation.\nThus, we conclude the induction, proving that we can have rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 min(p, dx) with arbitrarily small perturbation of each parameter without changing the value of L\u0304(W ). If p \u2264 dx, this means that upon such a perturbation, we have the case of rank(WH \u00b7 \u00b7 \u00b7W2) = p (since we have that p \u2265 rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 p where the first inequality follows the definition of p), with which we have already proved the existence of some negative eigenvalue of the Hessian unless it is a global minimum. Thus, such\na critical point is not a local minimum unless it is a global minimum. On the other hand, if p > dx, upon such a perturbation, we have p\u0304 , rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 dx \u2265 dy. Thus, WH+1 \u00b7 \u00b7 \u00b7W1 = Up\u0304UTp\u0304 Y XT (XXT )\u22121 = UUTY XT (XXT )\u22121, which is a global minimum. We can see this in various ways. For example, Xr = XY TUUT \u2212XY T = 0, which means that it is a global minimum as discussed above.\nSummarizing the above, any point that satisfies the definition (and necessary conditions) of a local minimum is a global minimum, concluding the proof of Theorem 2.3 (ii).\nB.2 Proof of Theorem 2.3 (i)\nProof We can prove the non-convexity and non-concavity from its Hessian (Theorem 2.3 (i)). First, consider L\u0304(W ). For example, from Corollary 4.5 with k = H + 1, it is necessary for the Hessian to be positive or negative semidefinite at a critical point that rank(WH+1) \u2265 rank(WH \u00b7 \u00b7 \u00b7W2) or Xr = 0. The instances of W unsatisfying this condition at critical points form some uncountable set. For example, consider a uncountable set that consists of the points with WH+1 = W1 = 0 and with any WH , . . . ,W2. Then, every point in the set defines a critical point from Lemma 4.1. Also, Xr = XY T 6= 0 as rank(XY T ) \u2265 1. So, it does not satisfies the first semidefinite condition. On the other hand, with any instance of WH \u00b7 \u00b7 \u00b7W2 such that rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 1, we have that 0 = rank(WH+1) rank(WH \u00b7 \u00b7 \u00b7W2). So, it does not satisfy the second semidefinite condition as well. Thus, we have proved that in the domain of the loss function, there exist points, at which the Hessian becomes indefinite. This implies Theorem 2.3 (i): the functions are non-convex and non-concave.\nB.3 Proof of Theorem 2.3 (iii)\nProof We now prove Theorem 2.3 (iii): every critical point that is not a global minimum is a saddle point. Here, we want to show that if the Hessian is negative semidefinite at a critical point, then there is a increasing direction so that there is no local maximum. Since L\u0304(W ) = 1 2 \u2211m i=1 \u2211dy j=1((WH+1)j,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i \u2212 Yj,i)2,\nD(WH+1)1,t L\u0304(W ) = 1\n2 m\u2211 i=1 D(WH+1)1,t((WH+1)1,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i \u2212 Y1,i) 2\n= m\u2211 i=1 ((WH+1)1,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i \u2212 Y1,i) ( D(WH+1)1,t dH\u2211 l=1 (WH+1)1,l(WH)l,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i )\n= m\u2211 i=1 ((WH+1)1,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i \u2212 Y1,i) ((WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i) .\nSimilarly,\nD(WH+1)1,tD(WH+1)1,tL\u0304(W ) = m\u2211 i=1 ((WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i)2 \u2208 R.\nTherefore, with other variables being fixed, L\u0304 is strictly convex in (WH+1)t,1 \u2208 R coordinate for some t unless (WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i = 0 for all i = 1, . . . ,m and for all t = 1, . . . dH . Since rank(X) = dx, in order to have (WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1X\u00b7,i = 0 for all i = 1, . . . ,m, the dimension of the null space of (WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1 must be at least dx for each t. Since (WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1 \u2208 R1\u00d7dx for each each t, this means that (WH)t,\u00b7 \u00b7 \u00b7 \u00b7W1 = 0 for all t. Therefore, with other variables being fixed, L\u0304 is strictly convex in (WH+1)1,t \u2208 R coordinate for some t if WH \u00b7 \u00b7 \u00b7W1 6= 0. If WH \u00b7 \u00b7 \u00b7W1 = 0, we claim that at a critical point, if the Hessian is negative semidefinite, we can make WH \u00b7 \u00b7 \u00b7W1 6= 0 with arbitrarily small perturbation of each parameter without changing the loss value. We can prove this by using the similar proof procedure to that used for Theorem 2.3 (ii) in the case of rank(WH \u00b7 \u00b7 \u00b7W2) < p. Suppose that WH \u00b7 \u00b7 \u00b7W1 = 0 and thus rank(WH \u00b7 \u00b7 \u00b7W1) = 0. From Lemma 4.4, we have a following necessary condition for the Hessian to be (positive or negative) semidefinite at a critical point: for any k \u2208 {2, . . . ,H + 1},\nR((Wk\u22121 \u00b7 \u00b7 \u00b7W2)T ) \u2286 R(CTC) or XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0,\nwhere the first condition is shown to imply rank(WH+1 \u00b7 \u00b7 \u00b7Wk) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) in Corollary 4.5.\nLet Ak = WH+1 \u00b7 \u00b7 \u00b7Wk+1. From the condition with k = 2, we have that rank(WH+1 \u00b7 \u00b7 \u00b7W2) \u2265 d1 \u2265 1 or XrWH+1 \u00b7 \u00b7 \u00b7W3 = 0. The former condition is false since rank(WH \u00b7 \u00b7 \u00b7W2) < 1. From the latter condition, for an arbitrary L2,\n0 = XrWH+1 \u00b7 \u00b7 \u00b7W3 \u21d4W2W1 = ( AT2 A2 )\u2212 AT2 Y X T (XXT )\u22121 + (I \u2212 (AT2 A2)\u2212AT2 A2)L2 (6)\n\u21d4WH+1 \u00b7 \u00b7 \u00b7W1 = A2 ( AT2 A2 )\u2212 AT2 Y X T (XXT )\u22121\n= C(CTC)\u2212CTY XT (XXT )\u22121\nwhere the last follow the critical point condition (Lemma 4.2). Then, similarly to the proof of Theorem 2.3 (ii),\nA2 ( AT2 A2 )\u2212 A2 = C(C TC)\u2212CT .\nIn other words,R(A2) = R(C). Suppose that rank(AT2 A2) \u2265 1. Then, since R(A2) = R(C), we have that rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) \u2265 1, which is false (or else the desired statement). Thus, rank(AT2 A2) = 0, which implies that A2 = 0. Then, since WH+1 \u00b7 \u00b7 \u00b7W1 = A2W2W1 with A2 = 0, we can have W2W1 6= 0 without changing the loss value with arbitrarily small perturbation of W2 and W1.\nThus, we showed that we can have W2W1 6= 0, with arbitrarily small perturbation of each parameter with the loss value being unchanged. To prove the corresponding results for Wk \u00b7 \u00b7 \u00b7W2 for any k = 2, ...,H , we conduct induction on k = 2, . . . ,H with the same proof procedure. The proposition P (k) to be proven is as follows: the necessary conditions with j \u2264 k implies that we can have Wk \u00b7 \u00b7 \u00b7W2 6= 0 with arbitrarily small perturbation of each parameter without changing the loss value. For the base case k = 2, we have already proved the proposition in the above.\nFor the inductive step with k \u2265 3, we have the inductive hypothesis that we can have Wk\u22121 \u00b7 \u00b7 \u00b7W2 6= 0 with arbitrarily small perturbation of each parameter without changing the loss value. Accordingly, suppose that Wk\u22121 \u00b7 \u00b7 \u00b7W1 6= 0. Again, from Lemma 4.4, for any k \u2208 {2, . . . ,H + 1},\nR((Wk\u22121 \u00b7 \u00b7 \u00b7W2)T ) \u2286 R(CTC) or XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 = 0. If the former is true, rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2) \u2265 rank(Wk\u22121 \u00b7 \u00b7 \u00b7W2W1) \u2265 1, which is false (or the desired statement). If the latter is true, for an arbitrary L1,\n0 = XrWH+1 \u00b7 \u00b7 \u00b7Wk+1 \u21d4Wk \u00b7 \u00b7 \u00b7W1 = ( ATkAk )\u2212 ATk Y X\nT (XXT )\u22121 + (I \u2212 (ATkAk)\u2212ATkAk)L1 \u21d4WH+1 \u00b7 \u00b7 \u00b7W1 = Ak ( ATkAk )\u2212 ATk Y X T (XXT )\u22121\n= C(CTC)\u2212CTY XT (XXT )\u22121 = Up\u0304U T p\u0304 Y X T (XXT )\u22121,\nwhere the last follow the critical point condition (Lemma 4.2). Then, similarly to the above, Ak ( ATkAk )\u2212 Ak = C(C TC)\u2212CT .\nIn other words,R(Ak) = R(C). Suppose that rank(ATkAk) \u2265 1. Then, since R(Ak) = R(C), we have that rank(WH \u00b7 \u00b7 \u00b7W2) \u2265 rank(C) = rank(Ak) \u2265 1, which is false (or the desired statement). Thus, rank(ATkAk) = 0, which implies that Ak = 0. Then, since WH+1 \u00b7 \u00b7 \u00b7W1 = AkWk \u00b7 \u00b7 \u00b7W1 with Ak = 0, we can have Wk \u00b7 \u00b7 \u00b7W1 6= 0 without changing the loss value with arbitrarily small perturbation of each parameter. Thus, we conclude the induction, proving that if WH \u00b7 \u00b7 \u00b7W1 = 0, with arbitrarily small perturbation of each parameter without changing the value of L\u0304(W ), we can have WH \u00b7 \u00b7 \u00b7W2 6= 0. Thus, upon such a perturbation at any critical point with the negative semidefinite Hessian, the loss function is strictly convex in (WH+1)1,t \u2208 R coordinate for some t. That is, at any candidate point for a local maximum, there exists a strictly increasing direction in an arbitrarily small neighbourhood. This means that there is no local maximum. Thus, we obtained the statement of Theorem 2.3 (i).\nB.4 Proof of Theorem 2.3 (iv)\nProof In the proof of Theorem 2.3 (ii), the case analysis with the case, rank(WH \u00b7 \u00b7 \u00b7W2) = p, revealed that when rank(WH \u00b7 \u00b7 \u00b7W2) = p, if \u22072L\u0304(W ) 0 at a critical point, W is a global minimum. Thus, when rank(WH \u00b7 \u00b7 \u00b7W2) = p, if W is not a global minimum at a critical point, its Hessian is not positive semidefinite, containing some negative eigenvalue. From Theorem 2.3 (ii), if it is not a global minimum, it is not a local minimum. From Theorem 2.3 (iii), it is a saddle point. Thus, if rank(WH \u00b7 \u00b7 \u00b7W2) = p, the Hessian at any saddle point has some negative eigenvalue, which is the statement of Theorem 2.3 (iv).\nB.5 Proof of Theorem 3.2 and discussion of the assumptions\nProof\nEZ [L(W )] = EZ 1 2 m\u2211 i=1 dy\u2211 j=1 (Y\u0302 (W,X)j,i \u2212 Yj,i)2 \n= 1\n2 m\u2211 i=1 dy\u2211 j=1 EZ [Y\u0302 (W,X)2j,i]\u2212 2Yj,iEZ [Y\u0302 (W,X)j,i] + Y 2j,i\n= 1\n2 m\u2211 i=1 dy\u2211 j=1 \u03c12q2  \u03a8j\u2211 p=1 [Xi](j,p) H\u220f k=1 w(j,p) 2 \u2212 2\u03c1qYj,i  \u03a8j\u2211 p=1 [Xi](j,p) H\u220f k=1 w(j,p) + Y 2j,i The first line follows the definition of the Frobenius norm. In the second line, we used the linearity of the expectation. The third line follows the independence assumption (A1p-m and A5u-m in (Choromanska et al., 2015b,a)). That is, we have that EZ [Y\u0302 (W,X)j,i] = \u03c1q \u2211\u03a8j p=1[Xi](j,p) \u220fH k=1 w(j,p).\nAlso, since ( \u2211k p=1 ap) 2 = \u2211k p=1 a 2 p + 2 \u2211 p<p\u2032 apap\u2032 for any a and k, by denoting ai,j,p =\n[Xi](j,p) \u220fH k=1 w(j,p),\nEZ [Y\u0302 (W,X)2j,i] = EZ   \u03a8j\u2211 p=1 ai,j,p[Zi](j,p) 2 \n= \u03a8j\u2211 p=1 a2i,j,pEZ [[Zi]2(j,p)] + 2 \u2211 p<p\u2032 ai,j,pai,j,p\u2032EZ [[Zi](j,p)[Zi](j,p\u2032)] = \u03c12 \u03a8j\u2211 p=1 a2i,j,p + 2\u03c1 2 \u2211 p<p\u2032 ai,j,pai,j,p\u2032\n= \u03c12  \u03a8j\u2211 p=1 [Xi](j,p) H\u220f k=1 w(j,p) 2\nAll the assumptions used above are subset of assumptions that were used, for example, in the first equation of the proof of theorem 3.3 in (Choromanska et al., 2015a). Finally, since q = \u03c1\u22121 and\u2211\u03a8j p=1[Xi](j,p) \u220fH k=1 w(j,p) = (WH+1WHWH\u22121 \u00b7 \u00b7 \u00b7W2W1X)j,i = Y j,i, the last line of the above equation for EZ [L(W )] is equal to 12\u2016Y \u2212 Y \u2016 2 F= L\u0304(W ). Also, LEZ [Y\u0302 ](W ) = 1 2\u2016E[Y\u0302 (W,X)] \u2212 Y \u20162F= 12\u2016E[Y\u0302 (W,X)]\u2212 Y \u2016 2 F= 1 2\u2016Y \u2212 Y \u2016 2 F= L\u0304(W ).\nTherefore, what we have proved to be true for L\u0304(W ) is also true for EZ [L(W )] and LEZ [Y\u0302 ](W ). We conclude the proof of Theorem 3.2.\nNote that we could reduce the loss functions EZ [L(W )] and LEZ [Y\u0302 ](W ) to L\u0304(W ) only with a strict subset of the assumptions used in the previous work. Accordingly, a question might arise as to how much we can reshape the loss function with all the assumptions used in the previous work. To answer this question, we note that Choromanska et al. (2015b,a) reduced their loss functions of nonlinear neural networks to:\nE\u03be,Z [Lhinge(W )1,1] = 1\n\u039b(H\u22121)/2 \u039b\u2211 i1,i2,...,iH=1 Xi1,i2,...,iH w\u0303i1w\u0303i2 . . . w\u0303iH s.t. 1 \u039b \u039b\u2211 i=1 w\u03032i = 1,\nwhere \u039b \u2208 R is some constant related to the size of the network (i.e., not the matrix containing the eigenvalues). While we refer to (Choromanska et al., 2015b,a) for the detailed definitions of the symbols, X and w are defined in the same way as ours are, and w\u0303 is a modified version due to other assumptions that we did not adopt. Here, we observe that not only the model but also the loss function is linear in the inputs (the nonlinear activation function has disappeared\u2014The inputs are simply multiplied by some coefficients and then summed). Moreover, the target function Y has disappeared (i.e., the loss value does not depend on the target function). That is, whatever the data points of Y are, their loss values are the same. Thus, we see that the loss functions can be reduced to much different functions with all the assumptions used in the previous work (i.e, A1p, A2p, A3p, A4p, A5u, A6u, and A7p). We adopted a strict subset of the assumptions, with which we reduced our loss function to a more realistic loss function of a deep neural network."}, {"heading": "C Proofs of Corollaries 2.4 and 3.3", "text": "We complete the proofs of Corollaries 2.4 and 3.3.\nProof If H = 1, the condition in Theorem 2.3 (iv) reads \"if rank(W1 \u00b7 \u00b7 \u00b7W2) = rank(Id1) = d1 = p\", which is always true. This is because p is the smallest width of hidden layers and there is only one hidden layer, the width of which is d1. Thus, Theorem 2.3 (iv) immediately implies the statement of Corollary 2.4. For the statement of Corollary 2.4 with H \u2265 2, it is suffice to show the existence of a simple set containing saddle points with the Hessian having no negative eigenvalue. Suppose that WH = WH\u22121 = \u00b7 \u00b7 \u00b7 = W2 = W1 = 0. Then, from Lemma 4.1, it defines a uncountable set of critical points, in which WH+1 can vary in Rdy\u00d7dH . Since r = Y T 6= 0 due to rank(Y ) \u2265 1, it is not a global minimum. To see this, we write\nL\u0304(W ) = 1 2 \u2016Y (W,X)\u2212 Y \u20162F= 1 2 tr(rT r)\n= 1 2 tr(Y Y T )\u2212 1 2 tr(WH+1 \u00b7 \u00b7 \u00b7W1XY T )\u2212 1 2 tr((WH+1 \u00b7 \u00b7 \u00b7W1XY T )T )\n+ 1\n2 tr(WH+1 \u00b7 \u00b7 \u00b7W1XXT (WH+1 \u00b7 \u00b7 \u00b7W1)T ).\nFor example, with WH+1 \u00b7 \u00b7 \u00b7W1 = \u00b1 UpUTp Y XT (XX)\u22121,\nL\u0304(W ) = 1 2\n( tr(Y Y T )\u2212 tr(UpUTp \u03a3)\u2212 tr(\u03a3UpUTp ) + tr(UpUTp \u03a3UpUTp ) ) = 1\n2\n( tr(Y Y T )\u2212 tr(Up\u039b1:pUTp ) ) = 1\n2\n( tr(Y Y T )\u00b1\np\u2211 k=1 \u039bk,k\n) ,\nwhere we can see that there exists a strictly lower value of L\u0304(W ) than the loss value with r = Y T , which is 12 tr(Y Y\nT ) (since X 6= 0 and rank(\u03a3) 6= 0). Thus, these are not global minima, and thereby these are saddle points by Theorem 2.3 (ii) and (iii). On the other hand, from the proof of Lemma 4.3, every diagonal and off-diagonal element of the Hessian is zero if WH = WH\u22121 = \u00b7 \u00b7 \u00b7 = W2 = W1 = 0. Thus, the Hessian is simply a zero matrix, which has no negative eigenvalue. Using the argument in the proof of Theorem 3.2, we can deduce that the same results hold for EZ [L(W )] and LEZ [Y\u0302 ](W )."}, {"heading": "D Discussion of the 1989 conjecture", "text": "The 1989 conjecture is based on the result for a 1-hidden layer network with p < dy = dx (e.g., an autoencoder). That is, the previous work considered Y = W2W1 with the same loss function as ours with the additional assumption p < dy = dx. The previous work denotes A ,W2 and B ,W1.\nThe conjecture was expressed by Baldi & Hornik (1989) as\nOur results, and in particular the main features of the landscape of E, hold true in the case of linear networks with several hidden layers.\nHere, the \u201cmain features of the landscape of E\u201d refers to the following features, among other minor technical facts: 1) the function is convex in each matrix A (or B) when fixing other B (or A), and 2) every local minimum is a global minimum. No proof was provided in this work for this conjecture.\nIn 2012, the proof for the conjecture corresponding to the first feature (convexity in each matrix A (or B) when fixing other B (or A)) was provided in (Baldi & Lu, 2012) for both real-valued and complex-valued cases, while the proof for the conjecture for the second feature (every local minimum being a global minimum) was left for future work.\nIn (Baldi, 1989), there is an informal discussion regarding the conjecture. Let i \u2208 {1, \u00b7 \u00b7 \u00b7 , H} be an index of a layer with the smallest width p. That is, di = p. We write\nA := WH+1 \u00b7 \u00b7 \u00b7Wi+1\nB := Wi \u00b7 \u00b7 \u00b7W1. Then, what A and B can represent is the same as what the original A := W2 and B := W1, respectively, can represent in the 1-hidden layer case, assuming that p < dy = dx (i.e., any element in Rdy\u00d7p and any element in Rp\u00d7dx). Thus, we would conclude that all the local minima in the deeper models always correspond to the local minima of the collapsed 1-hidden layer version with A := WH+1 \u00b7 \u00b7 \u00b7Wi+1 and B := Wi \u00b7 \u00b7 \u00b7W1. However, the above reasoning turns out to be incomplete. Let us prove the incompleteness of the reasoning by contradiction in a way in which we can clearly see what goes wrong. Suppose that the reasoning is complete (i.e., the following statement is true: if we can collapse the model with the same expressiveness with the same rank restriction, then the local minima of the model correspond to the local minima of the collapsed model). Consider f(w) = W3W2W1 = 2w2 + w3, where W1 = [w w w], W2 = [1 1 w]T and W3 = w. Then, let us collapse the model as a := W3W2W1 and g(a) = a. As a result, what f(w) can represent is the same as what g(a) can represent (i.e., any element in R) with the same rank restriction (with a rank of at most one). Thus, with the same reasoning, we can conclude that every local minimum of f(w) corresponds to a local minimum of g(a). However, this is clearly false, as f(w) is a non-convex function with a local minimum at w = 0 that is not a global minimum, while g(a) is linear (convex and concave) without any local minima. The convexity for g(a) is preserved after the composition with any norm. Thus, we have a contradiction, proving the incompleteness of the reasoning. What is missed in the reasoning is that even if what a model can represent is the same, the different parameterization creates different local structure in the loss surface, and thus different properties of the critical points (global minima, local minima, saddle points, and local maxima).\nNow that we have proved the incompleteness of this reasoning, we discuss where the reasoning actually breaks down in a more concrete example. From Lemmas 4.1 and 4.2, if H = 1, we have the following representation at critical points:\nAB = A(ATA)\u2212ATY XT (XXT )\u22121.\nwhere A := W2 and B := W1. In contrast, from Lemmas 4.1 and 4.2, if H is arbitrary,\nAB = C(CTC)\u2212CTY XT (XXT )\u22121.\nwhere A := WH+1 \u00b7 \u00b7 \u00b7Wi+1 and B := Wi \u00b7 \u00b7 \u00b7W1 as discussed above, and C = WH+1 \u00b7 \u00b7 \u00b7W2. Note that by using other critical point conditions from Lemmas 4.1, we cannot obtain an expression such that C = A in the above expression unless i = 1. Therefore, even though what A and B can represent is the same, the critical condition becomes different (and similarly, the conditions from\nthe Hessian). Because the proof in the previous work with H = 1 heavily relies on the fact that AB = A(ATA)\u2212ATY XT (XXT )\u22121, the same proof does not apply for deeper models (we may continue providing more evidence as to why the same proof does not work for deeper models, but one such example suffices for the purpose here).\nIn this respect, we have completed the proof of the conjecture and also provided a complete analytical proof for more general and detailed statements; that is, we did not assume that p < dy = dx, and we also proved saddle point properties with negative eigenvalue information."}], "references": [{"title": "Linear learning: Landscapes and algorithms", "author": ["Baldi", "Pierre."], "venue": "Advances in neural information processing systems. pp. 65\u201372.", "citeRegEx": "Baldi and Pierre.,? 1989", "shortCiteRegEx": "Baldi and Pierre.", "year": 1989}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["Baldi", "Pierre", "Hornik", "Kurt"], "venue": "Neural networks,", "citeRegEx": "Baldi et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 1989}, {"title": "Training a 3-node neural network is NP-complete", "author": ["Blum", "Avrim L", "Rivest", "Ronald L"], "venue": "Neural Networks,", "citeRegEx": "Blum et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1992}, {"title": "The Loss Surfaces of Multilayer Networks", "author": ["Choromanska", "Anna", "Henaff", "MIkael", "Mathieu", "Michael", "Ben Arous", "Gerard", "LeCun", "Yann"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open Problem: The landscape of the loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "LeCun", "Yann", "Arous", "G\u00e9rard Ben"], "venue": "In Proceedings of The 28th Conference on Learning Theory", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann N", "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Escaping From Saddle Points\u2014Online Stochastic Gradient for Tensor Decomposition", "author": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": "In Proceedings of The 28th Conference on Learning Theory", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Deep Learning. Book in preparation for MIT Press. http://www.deeplearningbook.org", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Learning Real and Boolean Functions: When Is Deep Better Than Shallow", "author": ["Mhaskar", "Hrushikesh", "Liao", "Qianli", "Poggio", "Tomaso"], "venue": "Massachusetts Institute of Technology CBMM Memo No", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "Some NP-complete problems in quadratic and nonlinear programming", "author": ["Murty", "Katta G", "Kabadi", "Santosh N"], "venue": "Mathematical programming,", "citeRegEx": "Murty et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Murty et al\\.", "year": 1987}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "The Schur complement and its applications", "author": ["Zhang", "Fuzhen."], "venue": "Vol. 4. Springer Science & Business Media.", "citeRegEx": "Zhang and Fuzhen.,? 2006", "shortCiteRegEx": "Zhang and Fuzhen.", "year": 2006}, {"title": "By solving for W1, W1 = (C TC)\u2212CTY X (XX )\u22121 + (I \u2212 (CTC)\u2212CTC)L, for an arbitrary matrix L. Due to the property of any generalized inverse (Zhang", "author": ["C. CC \u2212XY"], "venue": null, "citeRegEx": "\u2212XY,? \\Q2006\\E", "shortCiteRegEx": "\u2212XY", "year": 2006}, {"title": "Here, the first implication follows the necessary condition with any principal submatrix and the second implication follows the necessary condition with the Schur complement (Zhang, 2006, theorem 1.20", "author": [], "venue": "(I \u2212MM\u2212)M \u2032 = 0 (Zhang,", "citeRegEx": "T,? \\Q2006\\E", "shortCiteRegEx": "T", "year": 2006}, {"title": "R(CC) = R(C", "author": [], "venue": null, "citeRegEx": "\u2286,? \\Q1989\\E", "shortCiteRegEx": "\u2286", "year": 1989}], "referenceMentions": [{"referenceID": 8, "context": "In addition to its practical success, theoretical results have shown that deep learning is attractive in terms of its generalization properties (Livni et al., 2014; Mhaskar et al., 2016).", "startOffset": 144, "endOffset": 186}, {"referenceID": 9, "context": "In addition to its practical success, theoretical results have shown that deep learning is attractive in terms of its generalization properties (Livni et al., 2014; Mhaskar et al., 2016).", "startOffset": 144, "endOffset": 186}, {"referenceID": 7, "context": "In this paper, as a step toward establishing the optimization theory for deep learning, we prove a conjecture noted in (Goodfellow et al., 2016) for deep linear networks, and also address an open problem announced in (Choromanska et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 7, "context": "Given the absence of a theoretical understanding of deep nonlinear neural networks, Goodfellow et al. (2016) noted that it is beneficial to theoretically analyze the loss functions of simpler models, i.", "startOffset": 84, "endOffset": 109}, {"referenceID": 7, "context": "Given the absence of a theoretical understanding of deep nonlinear neural networks, Goodfellow et al. (2016) noted that it is beneficial to theoretically analyze the loss functions of simpler models, i.e., linear neural networks. The function class of a linear neural network only contains functions that are linear with respect to inputs. However, their loss functions are non-convex in the weight parameters and thus nontrivial. Saxe et al. (2014) empirically showed that the optimization of deep linear models exhibits similar properties to those of the optimization of deep nonlinear models.", "startOffset": 84, "endOffset": 450}, {"referenceID": 7, "context": "Recently, Goodfellow et al. (2016) remarked that when Baldi & Hornik (1989) stated and proved Proposition 2.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "Recently, Goodfellow et al. (2016) remarked that when Baldi & Hornik (1989) stated and proved Proposition 2.", "startOffset": 10, "endOffset": 76}, {"referenceID": 6, "context": "4 suggest that for 1-hidden layer networks, training can be done in polynomial time with a second order method or even with a modified stochastic gradient decent method, as discussed in (Ge et al., 2015).", "startOffset": 186, "endOffset": 203}, {"referenceID": 3, "context": "Following the work by Dauphin et al. (2014), Choromanska et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "(2014), Choromanska et al. (2015a) investigated the connection between the loss functions of deep nonlinear networks and a function well-studied via random matrix theory (i.", "startOffset": 8, "endOffset": 35}, {"referenceID": 3, "context": "(2014), Choromanska et al. (2015a) investigated the connection between the loss functions of deep nonlinear networks and a function well-studied via random matrix theory (i.e., the Hamiltonian of the spherical spin-glass model). They explained that their theoretical results relied on several unrealistic assumptions. Later, Choromanska et al. (2015b) suggested at the Conference on Learning Theory (COLT) 2015 that discarding these assumptions is an important open problem.", "startOffset": 8, "endOffset": 352}], "year": 2016, "abstractText": "In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.", "creator": "LaTeX with hyperref package"}}}