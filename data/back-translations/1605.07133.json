{"id": "1605.07133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Towards Multi-Agent Communication-Based Language Learning", "abstract": "We propose an interactive multimodal framework for language learning. Rather than being passively exposed to large amounts of natural text, our learners (implemented as feedforward neural networks) engage in cooperative reference games based on a tabula rasa setup, developing their own language out of the need to communicate in order to succeed in the game. Pre-experiments provide promising results, but also suggest that it is important to ensure that trained agents in this way do not develop an ad hoc communication code that is only effective for the game they are playing.", "histories": [["v1", "Mon, 23 May 2016 18:46:46 GMT  (2026kb,D)", "http://arxiv.org/abs/1605.07133v1", "9 pages, manuscript under submission"]], "COMMENTS": "9 pages, manuscript under submission", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["angeliki lazaridou", "nghia the pham", "marco baroni"], "accepted": false, "id": "1605.07133"}, "pdf": {"name": "1605.07133.pdf", "metadata": {"source": "CRF", "title": "Towards Multi-Agent Communication-Based Language Learning", "authors": ["Angeliki Lazaridou", "Marco Baroni"], "emails": ["angeliki.lazaridou@unitn.it", "thenghia.pham@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "One of the most ambitious goals of AI is to develop intelligent conversational agents able to communicate with humans and assist them in their tasks. Thus, communication and interaction should be at the core of the learning process of these agents; failure to integrate communication as their main building block raises concerns regarding their usability. However, traditional machine-learning approaches to language are based on static, passive, and mainly supervised regimes (e.g., as in applications to parsing, machine translation, natural language generation). Computational \u201cpassive learners\u201d receive a lot of annotated data and, by observing regularities in them, discover patterns they can apply to new data. While this is a great way to learn general statistical associations, it is very far from interactive communication, which proceeds by an active and incremental updating of the speakers\u2019 knowledge states.\nIn the language community, after the seminal\nwork on the \u201cblocks-world\u201d environment of Winograd (1971), we are now experiencing a revival of interest in language learning frameworks that are centered around communication and interaction (e.g., the Roadmap of Mikolov et al. (2015) or the more recent dialogue-based learning proposal of Weston (2016)). Similar trends are also taking place in other fields of Artificial Intelligence, witness the revival of interest in game playing with the recent ground-breaking results of DQN on Atari (Mnih et al., 2015) and AlphaGo (Silver et al., 2016), following precursors such as DeepBlue (Campbell et al., 2002) and TDGammon (Tesauro, 1995).\nFocusing on language, current approaches to communication-based language learning simulate interactive environments in diverse ways, e.g., by having agents interacting directly with humans or other scripted agents. Both approaches exhibit potentially important limitations. The human-inthe-loop approach (e.g., the SHRLDU program of Terry Winograd, robots learning via interacting with humans as in Tellex et al. (2014)) faces serious scalability issues, as active human intervention is obviously required at each step of training. Scripted Wizard-of-Oz environments (Mikolov et al., 2015) shift the burden of heavy manual engineering from the learning agent to designing the right behaviour for the programmed teaching agents.\nIn this work, we are proposing a radically different research program, namely multi-agent communication-based language learning within a multimodal environment. The essence of this proposal is to let computational agents co-exist, so that their co-existence constitutes the interactive environment. In this multi-agent environment, agents need to collaborate to perform a task, and we hypothesize that (with the right priors and constraints) developing language production and\nar X\niv :1\n60 5.\n07 13\n3v 1\n[ cs\n.C L\n] 2\n3 M\nay 2\n01 6\nhank you! Thank you!\nAgent A1 Agent A2\nblue this!\nFigure 1: Schematic representation of the referential game. Agent A1 has to describe the dashed object with an attribute. Agent A2, has to guess which object A1 was referring to. In this example, the agents agree on the referent, so the communicative act was successful.\nunderstanding will be prerequisites to successful communication. Note that we are not suggesting that the \u201cpassive\u201d setup should be abandoned, as, even in the interactive paradigm, large-scale statistical learning is expected to be important, e.g., to let agents discover how to produce grammatical sentences, how to recognize object categories in general or even how to provide generic descriptions of what is present in a scene. Still, interaction is required for many other tasks, that only make sense within a communicative setup, e.g., how to refer to specific things, or how to ask a good question or respond to it.\nSeveral points make this proposal attractive. First of all, this framework requires minimum human intervention for designing agents, the environment and its physics, e.g, rewards, although humans do still need to specify the nature of the tasks that agents need to perform. Computational agents will co-exist (co-operate or antagonize) and selforganize freely, interacting with each other and being encouraged to learn in order to achieve communication. For example, imagine the simple case in which an agent needs to have some object that some other agent possesses, and she starts asking for it in various ways. Only when she manages to make herself understood she will be able to get hold of that object. The sort of learning taking place in such setup will have to be based on active request for information, and it will probably foster incremental agreement by interaction.\nWe start by considering the most basic act of communication, namely referring to things. We design multimodal riddles in the form of referen-\ntial games (see Figure 1) (Galantucci et al., 2012). The speaker in this game is asked to refer to one of the visible objects by uttering an expression. The listener, who sees the same objects but has no knowledge regarding which object the speaker was asked to describe, needs to identify it based on the speaker\u2019s expression.\nImportantly, the agents start in a tabula rasa state. They do not possess any form of language or understanding. They have no prior notion of the semantics of words. Meanings are assigned to words (that is, the arbitrary symbols used in our initial simulations) by playing the game and are reinforced by communication success. Thus, agents can agree to any sort of conceptualization and assign to any word any kind of interpretation that help them effectively solve the tasks. This essentially aligns with the view of Wittgenstein (1953) that language meaning is derived from usage.\nWe will report next a set of pilot experiments showing that, while it is feasible, within this multiagent environment, to learn efficient communication protocols succeeding in the referential game, such protocols might not necessarily be aligned with the sort of semantics that exists in natural language. Interestingly, in the seminal language evolution experiment \u201cTalking Heads\u201d of Steels (2015), when two robots were left free to interact with each other, they developed an artificial language bearing little resemblance to natural language. Thus, we anticipate that, if we want to move forward with this research programme, grounding the agents\u2019 communication into natural language will be crucial, since our ultimate goal is to be able to develop agents capable of communication with humans. We will return to this point in Section 4."}, {"heading": "2 A two-agent referential game simulation", "text": ""}, {"heading": "2.1 The game", "text": "We propose a simple referential game with 2 agents, A1 and A2. The game is defined as follows:\n\u2022 A1 is shown a visual scene with two objects and is told to describe the referent with an attribute that constitutes the referring expression (RE).\n\u2022 A2 is shown the same visual scene without information on which is the referent, and\ngiven the RE has to \u201cpoint\u201d to the intended object\n\u2022 if A2 points to the correct object, then both agents receive a game point.\nNote that this game resembles the ReferItGame (Kazemzadeh et al., 2014) played by humans and designed to collect RE annotations of real scenes.\nWe observe that this is a co-operative game, i.e., our agents must work together to achieve game points. A1 should learn how to provide accurate phrases that discriminate the object from all others, and A2 should be good at interpreting A1\u2019s REs in the presence of the objects, in order to point to the correct one. Thus, with this game A1 and A2 learn to perform referring expression generation and reference resolution respectively."}, {"heading": "2.2 Visual Scenes", "text": "For reducing the complexity of our first simulations, unlike the ReferItGame, we do not work with real-world visual scenes. Instead, we construct visual scenes consisting of objects depicted in images. Specifically, our current setup consists of visual scenes with only two objects, the referent and the context. This allows us to have control over the complexity of the image processing required and the game itself, e.g., by having a referent that is visually dissimilar from the context and can be thus easily discriminated. Towards this end, we created 3 games from 3 different datasets by controlling the type of objects involved in the visual scene. Each dataset focuses on some particular aspect of the referential game.\nWhile our framework does not require obtaining gold annotations for the RE of the referent,\nwe apply a number of heuristics to annotate each \u3008referent, context\u3009 pair with gold attributes acting as REs. This will allow us to conduct various analyses regarding the nature of the semantics that the agents assign to the induced attributes. Table 1 exemplifies an instance of the game for the different scenarios, and reports descriptive statistics for the 3 datasets.1\nReferIt The first game scenario uses data derived from the ReferItGame (Kazemzadeh et al., 2014). In its general format, ReferItGame contains annotations of bounding boxes in real images with referring expressions produced by humans when playing the game. In order to create plausible visual situations consisting of two objects only, we synthesize scenes by pairing each referent (as denoted by a bounding box in the image) with a distractor context that comes from the same image (i.e., some other bounding box in the image). Each bounding box in the initial RefetItGame is associated with a RE, which we pre-process to eliminate stop words, punctuation and spatial information, deriving single words attributes. We then follow a heuristic to obtain the gold attributes acting as the referring expression for a given \u3008referent, context\u3009 pair, by selecting words that were produced to describe the intended referent but not the context. The rationale for this decision is that a necessary condition for achieving successful reference is that REs accurately distinguish the intended referent from any other object in the context (Dale and Haddock, 1991). For maximizing the quality of the generated gold attributes, (i) we disregarded any distractor context whose bounding box overlapped significantly with the refer-\n1Our datasets will be made available.\nent\u2019s bounding box and (ii) we disregarded distractor contexts that had full attribute overlap with the referent, thus resulting in a null referring expression for the \u3008referent, context\u3009 pair.\nObjects To control for the complexity of the visual scenes and attributes while maintaining real images, we created a simpler dataset in which referent and context are always different objects. For a list of 100 concrete objects ranging across different categories (e.g., animal, furniture etc), we synthesized \u3008referent, context\u3009 pairs by taking all possible combinations of objects and, for each object, sampling an image from the respective ImageNet (Deng et al., 2009) object label entry. The RE/gold attributes for a given pair is then straightforwardly obtained by using the object name of the referent.\nShapes Finally, we introduce a third dataset which controls for the complexity that real images have, while allowing referent and context to differ in a diverse number of attributes, exactly like in real scenes. Following Andreas et al. (2016), we created a geometric shapes dataset consisting of images that contain a single object. We generate such single-object images by varying the values of 6 types of attributes and follow a similar approach as in the ReferIt dataset to annotate \u3008referent, context\u3009 pairs with gold attributes (i.e., by taking the difference of the attributes in the referent and context). 2"}, {"heading": "2.3 Agent Players", "text": "Agent A1 (Referring Expression Generation) Agent A1 is performing a task analogous to referring expression generation. Unlike traditional REG research (Dale and Reiter, 1995; Mitchell et al., 2010; Kazemzadeh et al., 2014) that produces phrases or words as RE, in our current framework, agent A1 learns to predict a single attribute that discriminates the referent from the context. Note however that attribute meaning is not pre-defined. Instead, it emerges \u201con-demand\u201d via their usage in the referential game. In particular, ideally agent A1 will learn to associate the attributes to systematic configurations of lower-level perceptual features present in the images. In the current exper-\n2The 18 attributes grouped by type: shape=\u3008triangle, square, circle\u3009, border color=\u3008fuchsia, indigo, crimson, cyan, black, limegreen, brown, gray\u3009, horizontal position=\u3008up, down\u3009, vertical position=\u3008right, left\u3009, shape size=\u3008small, medium, big\u3009\niments, the words are simply represented by numerical indices, but it would of course be trivial to associate such indices with phonetic strings.\nFigure 2 illustrates the network architecture of A1. The model is presented with the two images that constitute a visual scene. We assume that agents are already equipped with a pre-trained visual system that converts the raw pixel input of the referent and the context to higher-level visual vectors v (i.e., ConvNet fc7 layers). For games using the ReferIt and Objects dataset we used the pretrained VGG-network (Simonyan and Zisserman, 2014). For the Shapes, given that the nature of the images are different from the usual ImageNet data used train these networks, we trained our own model. Specifically, we trained a smaller network, i.e., AlexNet (Krizhevsky et al., 2012) on predicting bundles of two attributes. For both models, we use the second-to-last fully connected layer to represent images with 4096-D vectors. These visual vectors are mapped into attribute vectors of dimensionality |V | (cardinality of all available attributes), with weights Ma \u2208 R4096\u00d7|V | shared across the two objects. Intuitively, this layer learns which attributes are active for specific objects.\nPairwise interactions between attribute vectors of referent and context are captured in the discriminative layer. This layer, processes, for each attribute, the two units, one for each object, and by applying a linear transformation with weights Md \u2208 R2\u00d7h, followed by a sigmoid activation function, finally derives a single value by another\nlinear transformation with weights MD \u2208 Rh\u00d71, producing dv which encodes the degree of discriminativeness of attribute v for the specific referent. The same process with the same shared weights Md and MD, across attributes is applied to all attributes v \u2208 V , to derive the estimated discriminativeness vector d. Finally, the discriminativeness vector is converted to a probability distribution, from which the player samples one attribute a acting as the referring expression. This attribute is encoded in the reference vector with one-hot like representation, and is passed over to A2. The learnable parameters of A1 are \u03b8A1 = \u3008Ma,Md,MD\u3009.\nA1 does not receive supervised data regarding the attributes that are active in pictures, nor regarding which attributes should be used to refer to the referent. The only supervision regarding the \u201cgoodness\u201d of attributes for the given \u3008referent, context\u3009 pair comes from the success of the interaction between the agents while playing the referential game.\nAgent A2 (Reference Resolution) For the purposes of the game, A2 needs to perform a task similar to reference resolution. Given the same visual input as A1 (we assume that the agents share the same visual system) and the produced attribute a, A2 has to choose which of the 2 objects in the scene is the intended referent.\nattribute layer\npre-trained visual model\nvisual vectors\no1\nreference\nvector\ndot product\no2\nsoft-max and sampling\nMa' Ma'\nFigure 3: Network of A2. Given the two objects and the reference vector encoding the predicted attribute produced by A1, she correctly predicts that the referent is the second object.\nFollowing this reasoning, we design a simple implementation of A2 depicted in Figure 3. A2 is presented with the two objects o1 and o2, without knowing which is the referent, and embeds them into an attribute space using weights shared across\nthe two objects Ma \u2032 \u2208 R4096\u00d7|V |.3 Note, that as is in the case of A1, A2 will receive no direct imageattribute supervision. The resulting attribute vectors encode how active the attributes are across the two objects. Following that, A2 computes the dot product similarities between the reference vector (i.e., the one-hot representation of the selected attribute a) and the attribute vectors of the objects. Intuitively, the reference vector encodes which attribute characterizes the referent and as a result, the dot similarity will be high if the attribute a is very active in the attribute vector. These two dot similarities are converted to a probability distribution p(o|o1, o2, a) over the two image indices, and one index is sampled indicating which of the two objects is the chosen referent. The learnable parameters of A2 are just \u03b8A2 = \u3008Ma \u2032\u3009"}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 General Training Details", "text": "The parameters of the 2 agents, \u03b8 = \u3008\u03b8A1, \u03b8A2\u3009, are learned jointly while playing the game. The only supervision used is communication success, i.e., whether the agents agreed on the referent. This setup can be naturally modeled with Reinforcement Learning (Sutton and Barto, 1998). Under this framework, the parameters of the agents implement a policy. By executing this policy, the agents perform actions, i.e., A1 picks an attribute and A2 picks an image. The loss function that the two agents are minimizing is \u2212IEo\u0303\u223cp(o|o1,o2,a)[R(o\u0303)], where o1 and o2 are the 2 objects in the visual scene, p(o|o1, o2, a) is the conditional probability over the 2 objects as computed by A2 given the objects and the attribute produced by A1, and R is the reward function which returns 1 iff o\u0303 = referent. The parameter updates are done following the Reinforce update rule (Williams, 1992). We do mini-batch updates, with a batch-size of 32 and train in all datasets for 3.5k iterations.4\nThe agents are trained and tested separately within each dataset. At test time, visual scenes (i.e., combination of referent and contexts images) are novel but individual images might be familiar. For test and tuning we use 1k visual scenes, and\n3While we could tie Ma \u2032\nand Ma, here we do not enforce any such constraint, essentially allowing the agents to develop their own \u201cvisual\u201d understanding.\n4The model-specific hidden size of discriminative layer hyperparameter is set to 20.\nleave the rest for training (see Table 1 for exact numbers)."}, {"heading": "3.2 Results", "text": "Our pilot experiments aim to ascertain whether our proposal can result in agents that learn to play the game correctly. Moreover, given that the agents start from a clean state (i.e., they possess no prior semantics other than the relatively lowlevel features passed to them through the visual vectors), it is worth looking into the nature of the induced semantics of the attributes. Simply put, is communication-based learning enough to allow agents to use the attributes in such a way that reflects high-level understanding of the images?\nCan agents learn to develop a communication protocol within the referential game? Figure 4 shows the communication success performance, i.e., how often the intended referent was guessed correctly by A2 (chance guessing would lead to 0.5 performance). Overall, agents are able to come up with a communication protocol allowing them to solve the task, but it takes them approximately 500 iterations (i.e., 16k training examples) before they start communicating effectively. This is to be expected, as the agents have no knowledge of the game rules nor of how to refer to things, and must induce both by observing the rewards they receive. Moreover, fewer attributes in the vocabulary translates to faster, but not necessarily better learning. As a sanity check, we restricted the vocabulary to 2 attributes only. In this case, the agents also came close to solving the task (and that was done faster than in the other cases), although without approaching 100% performance, a tendency observed across the datasets. At first glance this might seem suspicious; even if we, as humans, use language flexibly through polysemy,\nstill it will not be possible to come up with 2 words being able to reliably distinguish all possible combinations of objects!\nHowever, when we closely inspected the way A1 used the 2 attributes (e.g., by looking into the induced weights Ma), it became clear that the agent was in a sense \u201ccheating\u201d. Instead of communicating about high-level semantics, the agents agreed to exploit the attributes to communicate about low-level embedding properties of \u3008referent, context \u3009 pairs (e.g., pick attribute 1 if the value in dimension 3 is greater in the referent than the context etc). While this might seem odd, such strategies are in fact the best in order to communicate efficiently with only 2 attributes. This resembles the so-called conceptual pacts that humans form to make conversation more efficient, i.e., mutually agreeing in using \u201cunconventional\u201d semantics to refer to things (Brennan and Clark, 1996). Still, the \u201cwords\u201d discovered in this way have a very ad-hoc meaning that will not generalize to any useful task beyond the specifics of our game, and we would not want the agents to learn them. We thus turn now to an analysis of the semantic nature of the induced attributes when the agents have a larger vocabulary available than just 2 attributes, to see if they learn more general meanings, corresponding to (clusters of) high-level visual properties such as \u201cred\u201d or \u201ccat\u201d.\nWhat is the nature of the induced attributes? Revealing the semantics assigned to the induced attributes a is not trivial. We tested whether the semantics of the induced attributes align with the semantics of the referring expressions as expressed by the gold attributes. We focused on the Objects and Shapes datasets that have a relatively small number of attributes (100 and 18 respectively) and trained the agents using as attribute vo-\ncabulary size |V | 100 and 18 respectively. After the training, we assign to each induced attribute a the gold attribute that appears most often in the annotations of the \u3008referent, context\u3009 pairs for which a was predicted by A1, enforcing a 1-1 mapping (an induced attribute can by paired only with one gold attribute). A1 had a tendency not to make use of all the available attributes a in the vocabulary, thus either using attributes in a polysemous way, or assigning them some semantics different than the one that the gold attributes encode. As an example, we plot in Figure 5 the inferred alignment between induced and gold attributes in the Shapes dataset.\nIrrespective of the exact interpretation of attributes, meanings induced within this referential framework should be consistent across \u3008referent, context\u3009 pairs. For example, if an agent used the word red to refer to the object X in the context of Y, then red cannot be used again to refer to the object Z in the context of X, since red is also a property of the latter. However, the \u201ccheating\u201d approach we reported above for the 2-attribute case, in which the same attribute is used communicate about whether a low-level feature is higher in the referent or the context, would not respect this consistency constraint, as X might have a higher value of the relevant feature when compared to Y but lower with respect to Z.\nTo capture this, we devised a measure that we termed \u201creferential inconsistency\u201d (RI). Specifically, for each image i we compute a set R(i) containing all the induced attributes that were activated when i was in the referent slot and C(i) in the context slot. Then, the referential inconsistency RI of an attribute a is computed as\u2211\ni [a\u2208R(i)\u2229C(i)]\u2211\ni [a\u2208R(i)\u222aC(i)]\n, which counts the number of im-\nages for which a was both in the referent and context sets, normalized by the times it appears in either. Ideally, this value should be 0, as this happens when it was never the case that an induced attribute was activated both when an image was in the referent and in the context slot.\nTable 2 reports the proportion of induced active attributes that have RI > 0 across datasets (smaller values reflect consistent use of attributes). As expected, when communicating with 2 attributes only, agents do not seem to be using them in a semantically meaningful way, since referential consistency is violated. However, we ought to mention that it is also possible is that the agents in this case have learned to use the attributes in a relative way (Parikh and Grauman, 2011). Imagine that we pair an image of Mona Lisa once with an image of a frowning face as the context, and once with an image of a broadly smiling face. It would be acceptable then to use \u201cthe smiling one\u201d to denote Mona Lisa in the first pair, but it would also be possible to refer in this way to the more overtly smiling face in the second case, as Mona Lisa is more smiley than the frowning face, but definitely less so than a fully smiling face! In any case, for all datasets, with 100 attributes referential consistency is largely respected.\nFinally, we consider a third way to assess the degree to which the induced attributes reflect the intuitive semantic properties of the images. Our hypothesis is that, if the induced attributes are used coherently across visually similar referents, then they should reflect properties that are typical of the class shared by the referents (e.g., \u201cfurry\u201d for mammals). For this experiment, we focus on the Objects dataset that is annotated with 100 gold attributes denoting the objects depicted in the pictures (e.g., cat, dog etc). For each gold attribute g, we construct a vector that records how often the induced attribute a was used for a \u3008referent, context\u3009 pair that was annotated with g, essentially representing gold attributes in a vector space with\ninduced attributes as dimensions. We then compute the pairwise cosine similarities of the gold attributes in this vector space, plotted in Figure 6 up. As is, there is no structure in the similarity matrix. However, if we organize the rows and columns in the similarity matrix, as in Figure 6 down, so that objects of the same category cluster together (e.g., the first 2 rows and columns correspond to appliances, the next 4 to fruits), then a pattern along the diagonal starts to emerge, suggesting that the induced attributes reflect, at least to same degree, the similarity that exists between objects of the same category."}, {"heading": "4 Discussion", "text": "We have presented here a proposal for developing intelligent agents with language capabilities, that breaks away from current passive supervised regimes. Agents co-exist and are able to interact with each other. In our proposed framework we do not restrict the number of agents, nor their role in the games, i.e., we envision a community of agents that all interact with each other having to perform different tasks and taking turns in them, requiring them to either co-operate or antagonize in minmax sort of zero-sum games, in which agents aim\nat minimizing the opponents gain (e.g., as in the case of the famous tic-tac-toe game).\nIn our test case, we considered the most basic act of communication, i.e., learning to refer to things, and we designed a \u201cgrounded\u201d cooperative task that takes the form of referential games played by two agents. The first experiments, while encouraging, have revealed that it is essential to ensure that agents will not \u201cdrift\u201d into their own language, but instead they will evolve one that is aligned to our natural languages. Thus, inspired by the success of AlphaGo (Silver et al., 2016), that combines both passive learning (experiencing past human games and using a CNN to learn valid moves) and interactive learning (learning by playing what is the best move given a particular situation/state), we believe that it is crucial to combine dynamic interactive learning with static statistical learning of patterns from association, something that should ensure the grounding of communication into natural language.\nWe plan to move along a similar direction, introducing our agents to multi-tasking. As an example, we could expose A1 to large collections of texts and train her on language modeling, a task requiring no manual annotation, from which basic word associations patterns can be learned. Similarly, A2 could be trained on an image retrieval task, from which basic concept recognition and naming capabilities can be acquired, i.e., associating the phonetic string \u201ccat\u201d to instances of cats. Still, the agents would be trained via playing the game for producing good referring expressions, which is a task that depends predominantly on the success of communication (i.e., did our listener understood what we were referring to?)"}], "references": [{"title": "Trevor Darrell", "author": ["Jacob Andreas", "Marcus Rohrbach"], "venue": "and Dan Klein.", "citeRegEx": "Andreas et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Conceptual pacts and lexical choice in conversation", "author": ["Brennan", "Clark1996] Susan E Brennan", "Herbert H Clark"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "Brennan et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Brennan et al\\.", "year": 1996}, {"title": "A Joseph Hoane", "author": ["Murray Campbell"], "venue": "and Feng-hsiung Hsu.", "citeRegEx": "Campbell et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Computational interpretations of the gricean maxims in the generation of referring expressions", "author": ["Dale", "Reiter1995] Robert Dale", "Ehud Reiter"], "venue": "Cognitive science,", "citeRegEx": "Dale et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dale et al\\.", "year": 1995}, {"title": "Lia-Ji Li", "author": ["Jia Deng", "Wei Dong", "Richard Socher"], "venue": "and Li Fei-Fei.", "citeRegEx": "Deng et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Simon Garrod", "author": ["Bruno Galantucci"], "venue": "and Gareth Roberts.", "citeRegEx": "Galantucci et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mark Matten", "author": ["Sahar Kazemzadeh", "Vicente Ordonez"], "venue": "and Tamara L Berg.", "citeRegEx": "Kazemzadeh et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ilya Sutskever", "author": ["Alex Krizhevsky"], "venue": "and Geoffrey Hinton.", "citeRegEx": "Krizhevsky et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Armand Joulin", "author": ["Tomas Mikolov"], "venue": "and Marco Baroni.", "citeRegEx": "Mikolov et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kees van Deemter", "author": ["Margaret Mitchell"], "venue": "and Ehud Reiter.", "citeRegEx": "Mitchell et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Human-level control through deep reinforcement learning", "author": ["Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "Antonoglou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antonoglou et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": null, "citeRegEx": "Nham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nham et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "The Talking Heads experiment: Origins of words and meanings, volume 1", "author": ["Luc Steels"], "venue": null, "citeRegEx": "Steels.,? \\Q2015\\E", "shortCiteRegEx": "Steels.", "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Barto1998] Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Daniela Rus", "author": ["Stefanie Tellex", "Ross Knepper", "Adrian Li"], "venue": "and Nicholas Roy.", "citeRegEx": "Tellex et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal difference learning and td-gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Dialog-based language learning. arXiv preprint arXiv:1604.06045", "author": ["Jason Weston"], "venue": null, "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Procedures as a representation for data in a computer program for understanding natural language", "author": ["Terry Winograd"], "venue": "Technical Report AI 235,", "citeRegEx": "Winograd.,? \\Q1971\\E", "shortCiteRegEx": "Winograd.", "year": 1971}], "referenceMentions": [], "year": 2016, "abstractText": "We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing.", "creator": "LaTeX with hyperref package"}}}