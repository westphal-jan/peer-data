{"id": "1605.07252", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models", "abstract": "We consider the problem of learning the underlying graph of an unknown issuing model on p-spins from a collection of i.i.d. samples generated from the model. We propose a new estimator that is computationally efficient and requires a number of samples that is approximately optimal with respect to previously established information theoretical boundaries. Our statistical estimator has a physical interpretation in terms of \"interaction screening.\" The estimator is consistent and is efficiently implemented by means of convex optimization. We prove that, with appropriate regulation, the estimator restores the underlying graph from a number of samples that are logarithmic in system size p and exponential in maximum coupling intensity and node degree.", "histories": [["v1", "Tue, 24 May 2016 01:36:48 GMT  (81kb,D)", "http://arxiv.org/abs/1605.07252v1", null], ["v2", "Mon, 14 Nov 2016 03:00:29 GMT  (81kb,D)", "http://arxiv.org/abs/1605.07252v2", "To be published in Advances in Neural Information Processing Systems 30"], ["v3", "Mon, 19 Dec 2016 13:32:25 GMT  (81kb,D)", "http://arxiv.org/abs/1605.07252v3", "To be published in Advances in Neural Information Processing Systems 30"]], "reviews": [], "SUBJECTS": "cs.LG cond-mat.stat-mech cs.IT math.IT math.ST stat.ML stat.TH", "authors": ["marc vuffray", "sidhant misra", "andrey y lokhov", "michael chertkov"], "accepted": true, "id": "1605.07252"}, "pdf": {"name": "1605.07252.pdf", "metadata": {"source": "CRF", "title": "Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models", "authors": ["Marc Vuffray", "Sidhant Misra", "Andrey Lokhov", "Michael Chertkov"], "emails": ["chertkov}@lanl.gov"], "sections": [{"heading": "1 Introduction", "text": "A Graphical Model (GM) describes a probability distribution over a set of random variables which factorizes over the edges of a graph. It is of interest to recover the structure of GMs from random samples. The graphical structure contains valuable information on the dependencies between the random variables. In fact, the neighborhood of a random variable is the minimal set that provides us maximum information about this variable. Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].\nThe origin of the GM reconstruction problem is traced back to the seminal 1968 paper by Chow and Liu [8], where the problem was posed and resolved for the special case of tree-structured GMs. In this special tree case the maximum likelihood estimator is tractable and is tantamount to finding a maximum weighted spanning-tree. However, it is also known that in the case of general graphs with cycles, maximum likelihood estimators are intractable as they require computation of the partition function of the underlying GM, with notable exceptions of the Gaussian GM, see for instance [9], and some other special cases, like planar Ising models without magnetic field [10].\nA lot of efforts in this field has focused on learning Ising models, which are the most general GMs over binary variables with pairwise interaction/factorization. Early attempts to learn the\nar X\niv :1\n60 5.\n07 25\n2v 1\n[ cs\n.L G\n] 2\nIsing model structure efficiently were heuristic, based on various mean-field approximations, e.g. utilizing empirical correlation matrices [11, 12, 13, 14]. These methods were satisfactory in cases when correlations decrease with graph distance. However it was also noticed that the mean-field methods perform poorly for the Ising models with long-range correlations. This observation is not surprising in light of recent results stating that learning the structure of Ising models using only their correlation matrix is, in general, computationally intractable [15, 16].\nAmong methods that do not rely solely on correlation matrices but take advantage of higherorder correlations that can be estimated from samples, we mention the so-called regularized pseudolikelihood estimator [17]. This estimator, like the one we propose in this paper, is from the class of M-estimators i.e. estimators that are the minimum of a sum of functions over the sampled data. The regularized pseudo-likelihood estimator can be interpreted as a surrogate for the intractable likelihood estimator with an additive `1-norm penalty to encourage sparsity of the reconstructed graph. The estimator offers guarantees for the structure reconstruction, but the result only applies to GMs that satisfy a certain condition that is rather restrictive and hard to verify. It was also proven that this estimator fails to reconstruct the structure of graphs with long-range correlations, even for simple test cases [18].\nPrincipal tractability of structure reconstruction of an arbitrary Ising model from samples was proven only very recently. Bresler, Mossel and Sly in [19] suggested an algorithm which reconstructs the graph without errors in polynomial time. They showed that the algorithm requires number of samples that is logarithmic in the number of variables. Although this algorithm is of a polynomial complexity, it relies on an exhaustive neighborhood search, and the degree of the polynomial is equal to the maximal node degree.\nPrior to the work reported in this manuscript the best known procedure for perfect reconstruction of an Ising model was through a greedy algorithm proposed by Bresler in [20]. Bresler\u2019s algorithm is based on the observation that the mutual information between neighboring nodes in an Ising model is lower bounded. This observation allows to reconstruct the Ising graph perfectly with only a logarithmic number of samples and in time quasi-quadratic in the number of variables. On the other hand, Bresler\u2019s algorithm suffers from two major practical limitations. First, the number of samples, hence the running time as well, scales double exponentially with respect to the largest node degree and with respect to the largest coupling intensity between pairs of variables. This scaling is rather far from the information-theoretic lower-bound reported in [21] predicting instead a single exponential dependency on the two aforementioned quantities. Second, Bresler\u2019s algorithm requires prior information on the maximum and minimum coupling intensities as well as on the maximum node degree, guarantees which, in reality, are not necessarily available.\nIn this paper we propose a novel estimator for the graph structure of an arbitrary Ising model which achieves perfect reconstruction in quasi-quartic time (although we believe it can be reduced to quasi-quadratic time) and with a number of samples logarithmic in the system size. The algorithm is near-optimal in the sense that the number of samples required to achieve perfect reconstruction, and the run time, scale exponentially with respect to the maximum node-degree and the maximum coupling intensity, thus matching parametrically the information-theoretic lower bound of [21]. Our statistical estimator has the structure of a consistent M-estimator [22] and is implemented via convex optimization. Moreover it allows intuitive interpretation in terms of what we coin the \u201cinteraction screening\u201d. We show that with a proper `1-regularization our estimator reconstructs couplings of an Ising model from a number of samples that is near-optimal (in the sense of comparison with the information-theoretic bound). In addition, our estimator does not rely on prior information on the model characteristics, such as maximum coupling intensity and maximum degree.\nThe rest of the paper is organized as follows. In Section 2 we give a precise definition of the structure estimation problem for the Ising models and we describe in detail our method for structure reconstruction within the family of Ising models. The main results related to the reconstruction guarantees are provided by Theorem 1 and Theorem 2. In Section 3 we explain the strategy and the sequence of steps that we use to prove our main theorems. Proofs of Theorem 1 and Theorem 2 are summarized at the end of this Section. Section 4 illustrates performance of our reconstruction algorithm via simulations. Here we show on a number of test cases that the sample complexity of the suggested method scales logarithmically with the number of variables and exponentially with the maximum coupling intensity. In Section 5 we discuss possible generalizations of the algorithm and future work."}, {"heading": "2 Main Results", "text": "Consider a graph G = (V,E) with p vertexes where V = {1, . . . , p} is the vertex set and E \u2282 V \u00d7V is the undirected edge set. Vertexes i \u2208 V are associated with binary random variables \u03c3i \u2208 {\u22121,+1} that are called spins. Edges (i, j) \u2208 E are associated with non-zero real parameters \u03b8\u2217ij 6= 0 that are called couplings. An Ising model is a probability distribution \u00b5 over spin configurations \u03c3 = {\u03c31, . . . , \u03c3p} that reads as follows:\n\u00b5 (\u03c3) = 1\nZ exp  \u2211 (i,j)\u2208E \u03b8\u2217ij\u03c3i\u03c3j  , (1) where Z is a normalization factor called the partition function.\nZ = \u2211 \u03c3 exp  \u2211 (i,j)\u2208E \u03b8\u2217ij\u03c3i\u03c3j  . (2) Notice that even though the main innovation of this paper \u2013 the efficient \u201cinteraction screening\" estimator \u2013 can be constructed for the most general Ising models, we restrict our attention in this paper to the special case of the Ising models with zero magnetic field. This simplification is not necessary and is done solely to simplify (generally rather bulky) algebra. Later in the text we will thus refer to the zero magnetic field model (2) simply as the Ising model."}, {"heading": "2.1 Structure-Learning of Ising Models", "text": "Suppose that n sequences/samples of p spins { \u03c3(k) } k=1,...,n are observed. Let us assume that each\nobserved spin configuration \u03c3(k) = { \u03c3 (k) 1 , . . . , \u03c3 (k) p } is i.i.d. from (1). Based on these measure-\nments/samples we aim to construct an estimator E\u0302 of the edge set that reconstructs the structure exactly with high probability, i.e. P [ E\u0302 = E ] = 1\u2212 , (3)\nwhere \u2208 ( 0, 12 ) is a prescribed reconstruction error.\nWe are interested to learn structures of Ising models in the high-dimensional regime where the number of observations/samples is of the order n = O (ln p). A necessary condition on the number\nof samples is given in [21, Thm. 1]. This condition depends explicitly on the smallest and largest coupling intensity\n\u03b1 := min (i,j)\u2208E\n|\u03b8 \u2217 |, \u03b2 := max\n(i,j)\u2208E |\u03b8\n\u2217 |, (4)\nand on the maximal node degree d := max\ni\u2208V |\u2202i| , (5)\nwhere the set of neighbors of a node i \u2208 V is denoted by \u2202i := {j | (i, j) \u2208 E}. According to [21], in order to reconstruct the structure of the Ising model with minimum coupling intensity \u03b1, maximum coupling intensity \u03b2, and maximum degree d, the required number of samples should be at least\nn \u2265 max\ne\u03b2d ln ( pd 4 \u2212 1 ) 4d\u03b1e\u03b1 , ln p 2\u03b1 tanh\u03b1  . (6) We see from Eq. (6) that the exponential dependence on the degree and the maximum coupling intensity are both unavoidable. Moreover, when the minimal coupling is small, the number of samples should scale at least as \u03b1\u22122.\nIt remains unknown if the inequality (6) is achievable. It is shown in [21, Thm. 3] that there exists a reconstruction algorithm with error probability \u2208 ( 0, 12 ) if the number of samples is greater than\nn \u2265\n( \u03b2d ( 3e2\u03b2d + 1 ) sinh2 (\u03b1/4) )2 (16 log p+ 4 ln (2/ )) . (7)\nUnfortunately, the existence proof presented in [21] is non-constructive and thus it does not guarantee actual existence of an algorithm with low computational complexity. Notice also that the number of samples in (7) scales as exp (4\u03b2d) when d and \u03b2 are asymptotically large and as \u03b1\u22124 when \u03b1 is asymptotically small."}, {"heading": "2.2 Regulrized Interaction Screening Estimator", "text": "The main contribution of this paper consists in presenting explicitly a structure-learning algorithm that is of low complexity and which is near-optimal with respect to bounds (6) and (7). Our algorithm reconstructs the structure of the Ising model exactly, as stated in Eq. (3), with an error probability \u2208 ( 0, 12 ) , and with a number of samples which is proportional to exp (6\u03b2d) and \u03b1\u22122. (See Theorem 1 and Theorem 2 below for mathematically accurate statements.) Our algorithm consists of two steps. First, we estimate couplings in the vicinity of every node. Then, on the second step, we threshold the estimated couplings that are sufficiently small to zero. Resulting zero coupling means that the corresponding edge is not present.\nDenote the set of couplings around node u \u2208 V by the vector \u03b8\u2217u \u2208 Rp\u22121. In this, slightly abusive notation, we use the convention that if a coupling is equal to zero it reads as absence of the edge, i.e. \u03b8\u2217ui = 0 if and only if (u, i) /\u2208 E. Note that if the node degree is bounded by d, it implies that the vector of couplings \u03b8\u2217u is non-zero in at most d entries.\nOur estimator for couplings around node u \u2208 V is based on the following loss function coined the Interaction Screening Objective (ISO):\nSn (\u03b8u) = 1\nn n\u2211 k=1 exp \u2212 \u2211 i\u2208V \\u \u03b8ui\u03c3 (k) u \u03c3 (k) i  . (8)\nThe objective of the ISO is inversely proportional to the corresponding factor in the distribution (1). As a result, the effect of the objective is to reweight the distribution in a way that the spin \u03c3u becomes independent of the rest; i.e., the dependencies of \u03c3u on the rest of the spins are completely \u201cscreened\", which explains our choice for the name of the loss function. This remarkable \u201cscreening\" feature of the ISO suggests the following choice of the Regularized Interaction Screening Estimator (RISE) for the interaction vector around node u:\n\u03b8\u0302u (\u03bb) = argmin \u03b8u\u2208Rp\u22121 Sn (\u03b8u) + \u03bb \u2016\u03b8u\u20161 , (9)\nwhere \u03bb > 0 is a tunable parameter promoting sparsity through the additive `1-penalty. Notice that the ISO is the empirical average of an exponential function of \u03b8u which implies it is convex. Moreover, addition of the `1-penalty preserves the convexity of the minimization objective in Eq. (9).\nAs expected, the performance of RISE does depend on the choice of the penalty parameter \u03bb. If \u03bb is too small \u03b8\u0302u (\u03bb) is too sensitive to statistical fluctuations. On the other hand, if \u03bb is too large \u03b8\u0302u (\u03bb) has too much of a bias towards zero. In general, the optimal value of \u03bb is hard to guess. Luckily, the following theorem provides strong guarantees on the square error for the case when \u03bb is chosen to be sufficiently large.\nTheorem 1 (Square Error of RISE). Let { \u03c3(k) } k=1,...,n\nbe n realizations of p spins drawn i.i.d. from an Ising model with maximum degree d and maximum coupling intensity \u03b2. Then for any node u \u2208 V and for any 1 > 0, the square error of the Regularized Interaction Screening Estimator (9) with penalty parameter \u03bb \u2265 4 \u221a ln(3p/ 1)\nn is bounded with probability at least 1\u2212 1 by\n\u2225\u2225\u2225\u03b8\u0302u (\u03bb)\u2212 \u03b8\u2217u\u2225\u2225\u2225 2 \u2264 48d (1 + d) e3\u03b2d \u221a ln 3p 1 n , (10)\nwhenever n \u2265 (9d)5 e6\u03b2d ln 3p 1 .\nOur structure estimator (for the second step of the algorithm), Structure-RISE, takes RISE output and thresholds couplings whose absolute value is less than \u03b1/2 to zero:\nE\u0302 (\u03bb, \u03b1) = { (i, j) \u2208 V \u00d7 V | \u03b8\u0302ij (\u03bb) + \u03b8\u0302ji (\u03bb) \u2265 \u03b1 } . (11)\nPerformance of the Structure-RISE is fully quantified by the following Theorem. Theorem 2 (Structure Learning of Ising Models). Let { \u03c3(k) } k=1,...,n\nbe n realizations of p spins drawn i.i.d. from an Ising model with maximum degree d, maximum coupling intensity \u03b2 and minimal coupling intensity \u03b1. Then for any 2 > 0, Structure-RISE with penalty parameter \u03bb \u2265 4 \u221a\nln(3p2/ 2) n reconstructs the edge-set perfectly with probability\nP ( E\u0302 (\u03bb, \u03b1) = E ) \u2265 1\u2212 2, (12)\nwhenever n \u2265 max ( 1, \u03b1\u22122 ) (9d) 5 e6\u03b2d ln 3p 2\n2 .\nProofs of Theorem 1 and Theorem 2 are given in Subsection 3.3. Theorem 1 states that RISE recovers not only the structure but also the correct value of the couplings up to an error based on the available samples. It is possible to improve the square-error bound (10) even further by first, running the Structure-RISE to recover edges, and then re-running RISE with \u03bb = 0 for the remaining non-zero couplings.\nThe computational complexity of RISE is tantamount to minimizing the convex objective function and, as such, it scales at most as O ( np3 ) . Therefore, computational complexity of the\nStructure-RISE scales at most as O ( np4 ) simply because one has to call RISE at every node. We believe that this running time estimate can actually be reduced to a quasi-quadratic one by using first-order minimization techniques, in the spirit of [23].\nNotice that in order to implement RISE there is no need for prior knowledge on the graph parameters. This is a considerable advantage in practical applications where the maximum degree or bounds on couplings is often unknown."}, {"heading": "3 Analysis", "text": "The Regularized Interaction Screening Estimator (9) is from the class of the so-called regularized M-estimators. Naghaband et al. proposed in [22] a framework to analyze the square error of such estimators. As per [22], enforcing only two conditions on the loss function is sufficient to get a handle on the square error of an `1-regularized M-estimator.\nThe first condition links the choice of the penalty parameter to the gradient of the objective function.\nCondition 1. The `1-penalty parameter strongly enforces regularization if it is greater than any partial derivatives of the objective function at \u03b8u = \u03b8\u2217u, i.e.\n\u03bb \u2265 2 \u2016\u2207Sn (\u03b8\u2217u)\u2016\u221e . (13)\nCondition 1 guarantees that if the vector of couplings \u03b8\u2217u has at most d non-zero entries, then the estimation difference \u03b8\u0302u (\u03bb)\u2212 \u03b8 \u2217 u lies within the set\nK := { \u2206 \u2208 Rp\u22121 | \u2016\u2206\u20161 \u2264 4 \u221a d \u2016\u2206\u20162 } . (14)\nThe second condition ensure that the objective function is strongly convex in a restricted subset of Rp\u22121. Denote the reminder of the first-order Taylor expansion of the objective function by\n\u03b4Sn (\u2206u, \u03b8\u2217u) := Sn (\u03b8\u2217u + \u2206u)\u2212 Sn (\u03b8\u2217u)\u2212 \u3008\u2207Sn (\u03b8\u2217u) ,\u2206u\u3009 , (15)\nwhere \u2206u \u2208 Rp\u22121 is an arbitrary vector. Then the second condition reads as follows.\nCondition 2. The objective function is strongly convex at \u03b8u = \u03b8\u2217u restricted to K on a neighborhood of radius R, if for all \u2206u \u2208 K such that \u2016\u2206u\u20162 \u2264 R, there exists a constant \u03ba > 0 such that\n\u03b4Sn (\u2206u, \u03b8\u2217u) \u2265 \u03ba \u2016\u2206u\u2016 2 2 . (16)\nStrong regularization and restricted strong convexity enables us to control that the minimizer \u03b8\u0302u of the full objective (9) lies in the vicinity of the sparse vector of parameters \u03b8\u2217u. The precise formulation is given in the proposition following from [22, Thm. 1].\nProposition 1. If the `1-regularized M-estimator of the form (9) satisfies Condition 1 and Condition 2 with R > 3d\u03bb\u03ba then the square-error is bounded by\u2225\u2225\u2225\u03b8\u0302u \u2212 \u03b8\u2217u\u2225\u2225\u2225\n2 \u2264 3d\u03bb \u03ba . (17)"}, {"heading": "3.1 Gradient Concentration", "text": "Like the ISO (8), its gradient in any component l \u2208 V \\ u is an empirical average\n\u2202\n\u2202\u03b8ul Sn (\u03b8u) =\n1\nn n\u2211 k=1 X (k) ul (\u03b8u) , (18)\nwhere the random variables X(k)ul (\u03b8u) are i.i.d and they are related to the spin configurations according to\nXul (\u03b8u) = \u2212\u03c3u\u03c3l exp \u2212 \u2211 i\u2208V \\u \u03b8ui\u03c3u\u03c3i  . (19) In order to prove that the ISO gradient concentrates we have to state few properties of the support, the mean and the variance of the random variables (19), expressed in the following three Lemmas.\nThe first of the Lemmas states that at \u03b8u = \u03b8 \u2217 u, the random variable Xul (\u03b8 \u2217 u) has zero mean.\nLemma 1. For any Ising model with p spins and for all l 6= u \u2208 V\nE [Xul (\u03b8\u2217u)] = 0. (20)\nProof. By direct computation, we find that\nE [Xul (\u03b8\u2217u)] = E [ \u2212\u03c3u\u03c3l exp ( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i )]\n= \u22121 Z \u2211 \u03c3 \u03c3u\u03c3l exp  \u2211 (i,j)\u2208E \u03b8\u2217ij\u03c3i\u03c3j \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i  = 0, (21) where in the last line we use the fact that the exponential terms involving \u03c3u cancel, implying that the sum over \u03c3u \u2208 {\u22121,+1} is zero.\nAs a direct corollary of the Lemma 1, \u03b8u = \u03b8 \u2217 u is always a minimum of the averaged ISO (8). The second Lemma proves that at \u03b8u = \u03b8 \u2217 u, the random variable Xul (\u03b8 \u2217 u) has a variance equal\nto one.\nLemma 2. For any Ising model with p spins and for all l 6= u \u2208 V\nE [ Xul (\u03b8 \u2217 u) 2 ] = 1. (22)\nProof. As a result of direct evaluation one derives\nE [ Xul (\u03b8 \u2217 u) 2 ] = E [ exp ( \u22122 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i )]\n= 1\nZ \u2211 \u03c3 exp  \u2211 (i,j)\u2208E,i,j 6=u \u03b8\u2217ij\u03c3i\u03c3j \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i  = 1\nZ \u2211 \u03c3 exp  \u2211 (i,j)\u2208E,i,j 6=u \u03b8\u2217ij\u03c3i\u03c3j + \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i  = 1. (23)\nNotice that in the second line the first sum over edges (under the exponential) does not depend on \u03c3u. Furthermore, the first sum is invariant under the change of variables, \u03c3u \u2192 \u2212\u03c3u, while the second sum changes sign. This transformation results in appearance of the partition function in the numerator.\nThe next lemma states that at \u03b8u = \u03b8 \u2217 u, the random variable Xul (\u03b8 \u2217 u) has a bounded support.\nLemma 3. For any Ising model with p spins, with maximum degree d and maximum coupling intensity \u03b2, we guarantee that for all l 6= u \u2208 V\n|Xul (\u03b8\u2217u)| \u2264 exp (\u03b2d) . (24)\nProof. Observe that components of \u03b8\u2217u are smaller than \u03b2 and at most d of them are non-zero. Recall that spins are binary, {\u22121,+1}, which results in the following estimate\n|Xul (\u03b8\u2217u)| = \u2223\u2223\u2223\u2223\u2223\u2212\u03c3u\u03c3i exp ( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i )\u2223\u2223\u2223\u2223\u2223 \u2264 exp\n( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i ) \u2264 exp (\u03b2d) . (25)\nWith Lemma 1, 2 and 3, and using Berstein\u2019s inequality we are now in position to prove that every partial derivative of the ISO concentrates uniformly around zero as the number of samples grows.\nLemma 4. For any Ising model with p spins, with maximum degree d and maximum coupling intensity \u03b2. For any 3 > 0, if the number of observation satisfies n \u2265 exp (2\u03b2d) ln 2p 3 , then the following bound holds with probability at least 1\u2212 3:\n\u2016\u2207Sn (\u03b8\u2217u)\u2016\u221e \u2264 2 \u221a ln 2p 3 n . (26)\nProof. Let us first show that every term is individually bounded by the RHS of (26) with highprobability. We further use the union bound to prove that all components are uniformly bounded with high-probability. Utilizing Lemma 1, Lemma 2 and Lemma 3 we apply the Bernstein\u2019s Inequality\nP [\u2223\u2223\u2223\u2223 \u2202\u2202\u03b8ulSn (\u03b8\u2217u) \u2223\u2223\u2223\u2223 > t] \u2264 2 exp(\u2212 12 t2n1 + 13 exp (\u03b2d) t ) . (27)\nInverting the following relation\ns = 1 2 t 2n\n1 + 13 exp (\u03b2d) t , (28)\nand substituting the result in the Eq. (27) one derives\nP [\u2223\u2223\u2223\u2223 \u2202\u2202\u03b8ulSn (\u03b8\u2217u) \u2223\u2223\u2223\u2223 > 13 ( u+ \u221a 18 exp (\u03b2d) u+ u2 )] \u2264 2 exp (\u2212s) , (29)\nwhere u = sn exp (\u03b2d) . When n \u2265 s exp (2\u03b2d) Eq. (29) can be simplified to become independent of \u03b2 and d\nP [\u2223\u2223\u2223\u2223 \u2202\u2202\u03b8ulSn (\u03b8\u2217u) \u2223\u2223\u2223\u2223 > 2\u221a sn ] \u2264 2 exp (\u2212s) . (30)\nUsing s = ln 2p 3 and the union bound on every component of the gradient leads to the desired result."}, {"heading": "3.2 Restricted Strong-Convexity", "text": "The remainder of the first-order Taylor-expansion of the ISO, defined in Eq. (15) reads\n\u03b4Sn (\u2206u, \u03b8\u2217) = 1\nn n\u2211 k=1 exp ( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3 (k) u \u03c3 (k) i )( exp ( \u2212Y (k)u (\u2206u) ) \u2212 1 + Y (k)u (\u2206u) ) , (31)\nwhere the random variables Y (k)u (\u2206u) are i.i.d and are related to the spin configurations according to\nYu (\u2206u) = \u2211 i\u2208V \\u \u2206ui\u03c3u\u03c3i. (32)\nIn order to prove that the reminder of Eq. (31) is lower bounded by a quadratic form we show a few properties of the relevant random variables in (32). The following Lemma provides a lower bound on the second moment of (32).\nLemma 5. Consider an Ising model with p spins, with maximum degree d and maximum coupling intensity \u03b2. For all \u2206u \u2208 Rp\u22121 the following bound holds\nE [ Yu (\u2206u) 2 ] \u2265 e \u22122\u03b2d\nd+ 1 \u2016\u2206u\u201622 . (33)\nProof. Our proof strategy here follows [16, Cor. 3.1]. Notice that the probability measure of the Ising model is symmetric with respect to the sign flip, i.e. \u00b5 (\u03c31, . . . , \u03c3p) = \u00b5 (\u2212\u03c31, . . . ,\u2212\u03c3p). Thus any spin has zero mean, which implies that for every \u2206u \u2208 Rp\u22121\nE  \u2211 i\u2208V \\u \u2206ui\u03c3i  = 0. (34) This allows to reinterpret (33) as a variance, using that \u03c32u = 1,\nE [ Yu (\u2206u) 2 ] = E   \u2211 i\u2208V \\u \u2206ui\u03c3i 2 \n= Var  \u2211 i\u2208V \\u \u2206ui\u03c3i  . (35) Construct a subset A \u2282 V recursively as follows: (i) let i0 = argmaxj\u2208V \\u \u22062uj and define A0 = {i0}, (ii) given At = {i0, . . . , it}, let it+1 = argmaxj\u2208Act\u2229V \\u \u2206 2 uj and set At+1 = At \u222a {it+1} and, (iii) terminate when V \\ u \u222aAct = \u2205 and declare A = At. The set A possesses the following two main properties. First, every node i \u2208 A does not have any neighbors in A and, second, \u2211 i\u2208A \u22062ui \u2265 (d+ 1) \u2211 i\u2208V \\u \u22062ui. (36)\nWe apply the law of total variance to (35) by conditioning on the set of spins \u03c3Ac whose indexes are from the complementary set Ac.\nVar  \u2211 i\u2208V \\u \u2206ui\u03c3i  \u2265 E Var  \u2211 i\u2208V \\u \u2206ui\u03c3i | \u03c3Ac  =\n\u2211 i\u2208A \u22062uiE [Var [\u03c3i | \u03c3Ac ]] , (37)\nwhere in the last line one uses that the spins in A are conditionally independent given their neighbors \u03c3Ac . One concludes the proof by using relation (36) and the fact that the conditional variance of a spin given its neighbors are bounded from below:\nVar [\u03c3i | \u03c3Ac ] \u2265 1\u2212 tanh 2 \u2211 j\u2208\u2202i \u03b8\u2217ij\u03c3j  \u2265 exp (\u22122\u03b2d) . (38)\nThe following lemma proves a concentration property for the remainder of the Taylor expansion.\nLemma 6. Consider an Ising model with p spins, with maximum degree d and maximum coupling intensity \u03b2. For all \u2206u \u2208 Rp\u22121 and for all 4 > 0, the remainder of the Taylor expansion (31) satisfies with probability at least 1\u2212 4, the following inequality\n\u03b4Sn (\u2206u, \u03b8\u2217u) \u2265 e\u22123\u03b2d\n2 (d+ 1) \u2016\u2206u\u201622 \u2212 \u221a ln 1 4 n ( 1 + 1 3 \u2016\u2206u\u20161 ) \u2016\u2206u\u201621 , (39)\nwhenever n \u2265 exp (2\u03b2d) ln 1 4 .\nProof. This concentration property is based on the Bernstein\u2019s inequality. First of all, observe that for all z \u2208 R, the following bound holds\ne\u2212z + 1 + z \u2265 1 2 z2 \u2212 1 6 z3. (40)\nThis implies that the remainder of the Taylor expansion (31) is lower-bounded\n\u03b4Sn (\u2206u, \u03b8\u2217u) \u2265 1\nn n\u2211 k=1 exp ( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i )( 1 2 Y (k)u (\u2206u) 2 \u2212 1 6 Y (k)u (\u2206u) 3 ) . (41)\nUsing similar arguments than in Lemma 1 it is easy to show that for all \u2206 \u2208 Rp\u22121\nE [ exp ( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i ) Yu (\u2206u) 3 ] = 0. (42)\nThen extending the technique of the type used in Lemma 2, one shows that for every integer l \u2208 N\nE [ exp ( \u2212 \u2211 i\u2208\u2202u \u03b8\u2217ui\u03c3u\u03c3i ) Yu (\u2206u) l ] \u2264 |Yu (\u2206u)|l . (43)\nNotice also that the support of Yu (\u2206u) is trivially upper bounded for all \u2206u \u2208 Rp\u22121\n|Yu (\u2206u)| \u2264 \u2016\u2206u\u20161 . (44)\nTo finish the proof we apply the Bernstein\u2019s inequality to the right-hand side of (41), thus combining relations (42), (43), (44) and Lemma 5. Moreover when n \u2265 e2\u03b2d ln 1 4 further simplifications can be made in the way similar the one used in Lemma 4.\nWe are now in the position to prove that the ISO satisfies the the restricted strong convexity condition.\nLemma 7. Consider an Ising model with p spins, with maximum degree d and maximum coupling intensity \u03b2. For all 4 > 0, when n \u2265 (9d)5 e6\u03b2d ln 1 4 the ISO (8) satisfies, with probability at least 1\u2212 4, the restricted strong convexity condition\n\u03b4Sn (\u2206u, \u03b8\u2217u) \u2265 e\u22123\u03b2d\n4 (d+ 1) \u2016\u2206u\u201622 , (45)\nfor all \u2206u \u2208 Rp\u22121 such that \u2016\u2206u\u20161 \u2264 4 \u221a d \u2016\u2206u\u20162 and \u2016\u2206u\u20162 \u2264 1. Proof. We prove it applying Lemma 6 directly to \u2016\u2206u\u20161 \u2264 4 \u221a d \u2016\u2206u\u20162 and \u2016\u2206u\u20162 \u2264 1 when n \u2265 (9d)5 e6\u03b2d ln 1 4 ."}, {"heading": "3.3 Proof of the main Theorems", "text": "Proof of Theorem 1 (Square Error of RISE). We seek to apply Proposition 1 to the Regularized\nInteraction Screening Estimator (9). Using 3 = 2 13 in Lemma 4 and letting \u03bb \u2265 4 \u221a ln 3p/ 1 n , it follows that Condition 1 is satisfied with probability greater than 1 \u2212 2 1/3, whenever n \u2265 (9d)\n5 e6\u03b2d ln 3p 1 . Using 4 = 1/3 in Lemma 7, and observing that 3d\u03bb ( e\u22123\u03b2d\n4(d+1)\n)\u22121 \u2264 1,\nfor n \u2265 (9d)5 e6\u03b2d ln p 1 , we conclude that condition 2 is satisfied with probability greater than 1\u2212 13 . Theorem 1 then follows by using a union bound and then applying Proposition 1.\nThe proof of Theorem 2 becomes an immediate application of Theorem 1.\nProof of Theorem 2 (Structure Learning of Ising Models). According to Theorem 1, one observes that, with probability 1 \u2212 1, the minimal amount of samples required to achieve an error of \u03b1/2 on every couplings around a single node is\nn \u2265 max ( 1, \u03b1\u22122 ) (9d) 5 e6\u03b2d ln 3p\n1 .\nLet us choose 2 = 1/p and use the union-bound to ensure that the couplings at every node (thresholded by \u03b1/2) is simultaneously recovered with probability greater than 1\u2212 2."}, {"heading": "4 Numerical Results", "text": "We test performance of the Struct-RISE, with the strength of the l1-regularization parametrized\nby \u03bb = 4 \u221a\nln(3p2/ ) n , on Ising models over two-dimensional grid with periodic boundary conditions\n(thus degree of every node in the graph is 4). We are interested to find the minimal number of samples, nmin, such that the graph is perfectly reconstructed with probability 1 \u2212 \u2265 0.95. In our numerical experiments, we recover the value of nmin as the minimal n for which Struct-RISE outputs the perfect structure 45 times from 45 different trials with n samples, thus guaranteeing that the probability of perfect reconstruction is greater than 0.95 with a statistical confidence of \u2248 90%.\nWe first verify the logarithmic scaling of nmin with respect to the number of spins p. The couplings are chosen uniform and positive \u03b8\u2217ij = 0.7. This choice ensures that samples generated by Glauber dynamics are i.i.d. according to (1). Values of nmin for p \u2208 {9, 16, 25, 36, 49, 64} are shown on the left in Figure 1. Empirical scaling is, \u2248 1.1 \u00d7 105 ln p, which is orders of magnitude better than the rather conservative prediction of the theory, 4.9\u00d7 1015 ln p.\nWe also test the exponential scaling of nmin with respect to the maximum coupling intensity \u03b2. The test is conducted over two different settings both with p = 16 spins: the ferromagnetic case where all couplings are uniform and positive, and the spin glass case where the sign of couplings is assigned uniformly at random. In both cases the absolute value of the couplings,\n\u2223\u2223\u03b8\u2217ij\u2223\u2223, is uniform and equal to \u03b2. To ensure that the samples are i.i.d, we sample directly from the exhaustive weighted list of the 216 possible spin configurations.\nExperimental values of nmin for different values of the maximum coupling intensity, \u03b2, are shown on the right in Fig. 1. Empirically observed exponential dependence on \u03b2 is matched best by,\nexp (12.8\u03b2), in the ferromagnetic case and by, exp (5.6\u03b2), in the case of the spin glass. Theoretical bound for d = 4 predicts exp (24\u03b2). We observe that the difference in sample complexity depends significantly on the type of interaction. An interesting observation one can make based on these experiments is that the case which is harder from the sample-generating perspective is easier for learning and vice verse."}, {"heading": "5 Conclusions and Path Forward", "text": "In this paper we construct and analyze the Regularized Interaction Screening Estimator (9). We show that the estimator is computationally efficient and needs an optimal number of samples for learning Ising models. The RISE estimator does not require any prior knowledge about the model parameters for implementation and it is based on the minimization of the loss function (8), that we call the Interaction Screening Objective. The ISO is an empirical average (over samples) of an objective designed to screen an individual spin/variable from its factor-graph neighbors.\nEven though we focus in this paper solely on learning pair-wise binary models, the \u201cinteraction screening\u201d approach we introduce here is generic. The approach extends to learning other Graphical Models, including those over higher (discrete, continuous or mixed) alphabets and involving highorder (beyond pair-wise) interactions. These generalizations are built around the same basic idea pioneered in this paper \u2013 the interaction screening objective is (a) minimized over candidate GM parameters at the actual values of the parameters we aim to learn; and (b) it is an empirical average over samples. In the future, we plan to explore further theoretical and experimental power, characteristics and performance of the generalized screening estimator."}, {"heading": "Acknowledgment", "text": "We are thankful to Guy Bresler and Andrea Montanari for valuable discussions, comments and insights. This work was carried out under the auspices of the National Nuclear Security Administration of the U.S. Department of Energy at Los Alamos National Laboratory under Contract No. DE-AC52-06NA25396, and was partially supported by DTRA Basic Research Project #10027- 13399 and by the Advanced Grid Modeling Program in the U.S. Department of Energy Office of Electricity."}], "references": [{"title": "Wisdom of crowds for robust gene network inference", "author": ["D. Marbach", "J.C. Costello", "R. Kuffner", "N.M. Vega", "R.J. Prill", "D.M. Camacho", "K.R. Allison", "M. Kellis", "J.J. Collins", "G. Stolovitzky"], "venue": "Nat Meth, vol. 9, pp. 796\u2013804, Aug 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Direct-coupling analysis of residue coevolution captures native contacts across many protein families", "author": ["F. Morcos", "A. Pagnani", "B. Lunt", "A. Bertolino", "D.S. Marks", "C. Sander", "R. Zecchina", "J.N. Onuchic", "T. Hwa", "M. Weigt"], "venue": "Proceedings of the National Academy of Sciences, vol. 108, no. 49, pp. E1293\u2013E1301, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Weak pairwise correlations imply strongly correlated network states in a neural population", "author": ["E. Schneidman", "M.J. Berry", "R. Segev", "W. Bialek"], "venue": "Nature, vol. 440, pp. 1007\u20131012, Apr 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Fields of experts: a framework for learning image priors", "author": ["S. Roth", "M.J. Black"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2, pp. 860\u2013867 vol. 2, June 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Inferring friendship network structure by using mobile phone data", "author": ["N. Eagle", "A.S. Pentland", "D. Lazer"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 36, pp. 15274\u2013 15278, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A dependency graph approach for fault detection and localization towards secure smart grid", "author": ["M. He", "J. Zhang"], "venue": "IEEE Transactions on Smart Grid, vol. 2, pp. 342\u2013351, June 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure learning and statistical estimation in distribution networks", "author": ["D. Deka", "S. Backhaus", "M. Chertkov"], "venue": "submitted to IEEE Control of Networks; arXiv:1501.04131; arXiv:1502.07820, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Transactions on Information Theory, vol. 14, pp. 462\u2013467, May 1968.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1968}, {"title": "First-order methods for sparse covariance selection", "author": ["A. d\u2019Aspremont", "O. Banerjee", "L.E. Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 1, pp. 56\u201366, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning planar ising models", "author": ["J. Johnson", "D. Oyen", "P. Netrapalli", "M. Chertkov"], "venue": "Journal of Machine Learning, in press; arXiv:1502.00916, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Mean-field theory of Boltzmann machine learning", "author": ["T. Tanaka"], "venue": "Phys. Rev. E, vol. 58, pp. 2302\u2013 2310, Aug 1998. 14", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient learning in Boltzmann machines using linear response theory", "author": ["H.J. Kappen", "F. d. B. Rodr\u00edguez"], "venue": "Neural Computation, vol. 10, no. 5, pp. 1137\u20131156, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Ising model for neural data: Model quality and approximate methods for extracting functional connectivity", "author": ["Y. Roudi", "J. Tyrcha", "J. Hertz"], "venue": "Phys. Rev. E, vol. 79, p. 051915, May 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1915}, {"title": "The Bethe approximation for solving the inverse Ising problem: a comparison with other inference methods", "author": ["F. Ricci-Tersenghi"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2012, no. 08, p. P08015, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Hardness of parameter estimation in graphical models", "author": ["G. Bresler", "D. Gamarnik", "D. Shah"], "venue": "Advances in Neural Information Processing Systems 27 (Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds.), pp. 1062\u20131070, Curran Associates, Inc., 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational implications of reducing data to sufficient statistics", "author": ["A. Montanari"], "venue": "Electron. J. Statist., vol. 9, no. 2, pp. 2370\u20132390, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "High-dimensional Ising model selection using `1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Ann. Statist., vol. 38, pp. 1287\u20131319, 06 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Which graphical models are difficult to learn", "author": ["A. Montanari", "J.A. Pereira"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, eds.), pp. 1303\u20131311, Curran Associates, Inc., 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Reconstruction of Markov random fields from samples: Some observations and algorithms", "author": ["G. Bresler", "E. Mossel", "A. Sly"], "venue": "SIAM Journal on Computing, vol. 42, no. 2, pp. 563\u2013578, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficiently learning Ising models on arbitrary graphs", "author": ["G. Bresler"], "venue": "Proceedings of the Forty- Seventh Annual ACM on Symposium on Theory of Computing, pp. 771\u2013782, ACM, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Information-theoretic limits of selecting binary graphical models in high dimensions", "author": ["N.P. Santhanam", "M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory, vol. 58, pp. 4117\u2013 4134, July 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statist. Sci., vol. 27, pp. 538\u2013557, 11 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["A. Agarwal", "S. Negahban", "M.J. Wainwright"], "venue": "Ann. Statist., vol. 40, pp. 2452\u20132482, 10 2012. 15", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 220, "endOffset": 226}, {"referenceID": 6, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 220, "endOffset": 226}, {"referenceID": 7, "context": "The origin of the GM reconstruction problem is traced back to the seminal 1968 paper by Chow and Liu [8], where the problem was posed and resolved for the special case of tree-structured GMs.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "However, it is also known that in the case of general graphs with cycles, maximum likelihood estimators are intractable as they require computation of the partition function of the underlying GM, with notable exceptions of the Gaussian GM, see for instance [9], and some other special cases, like planar Ising models without magnetic field [10].", "startOffset": 257, "endOffset": 260}, {"referenceID": 9, "context": "However, it is also known that in the case of general graphs with cycles, maximum likelihood estimators are intractable as they require computation of the partition function of the underlying GM, with notable exceptions of the Gaussian GM, see for instance [9], and some other special cases, like planar Ising models without magnetic field [10].", "startOffset": 340, "endOffset": 344}, {"referenceID": 10, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 11, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 12, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 13, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 14, "context": "This observation is not surprising in light of recent results stating that learning the structure of Ising models using only their correlation matrix is, in general, computationally intractable [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 15, "context": "This observation is not surprising in light of recent results stating that learning the structure of Ising models using only their correlation matrix is, in general, computationally intractable [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 16, "context": "Among methods that do not rely solely on correlation matrices but take advantage of higherorder correlations that can be estimated from samples, we mention the so-called regularized pseudolikelihood estimator [17].", "startOffset": 209, "endOffset": 213}, {"referenceID": 17, "context": "It was also proven that this estimator fails to reconstruct the structure of graphs with long-range correlations, even for simple test cases [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "Bresler, Mossel and Sly in [19] suggested an algorithm which reconstructs the graph without errors in polynomial time.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Prior to the work reported in this manuscript the best known procedure for perfect reconstruction of an Ising model was through a greedy algorithm proposed by Bresler in [20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "This scaling is rather far from the information-theoretic lower-bound reported in [21] predicting instead a single exponential dependency on the two aforementioned quantities.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "The algorithm is near-optimal in the sense that the number of samples required to achieve perfect reconstruction, and the run time, scale exponentially with respect to the maximum node-degree and the maximum coupling intensity, thus matching parametrically the information-theoretic lower bound of [21].", "startOffset": 298, "endOffset": 302}, {"referenceID": 21, "context": "Our statistical estimator has the structure of a consistent M-estimator [22] and is implemented via convex optimization.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "According to [21], in order to reconstruct the structure of the Ising model with minimum coupling intensity \u03b1, maximum coupling intensity \u03b2, and maximum degree d, the required number of samples should be at least n \u2265 max \uf8eb\uf8ede\u03b2d ln ( pd 4 \u2212 1 )", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Unfortunately, the existence proof presented in [21] is non-constructive and thus it does not guarantee actual existence of an algorithm with low computational complexity.", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "We believe that this running time estimate can actually be reduced to a quasi-quadratic one by using first-order minimization techniques, in the spirit of [23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "proposed in [22] a framework to analyze the square error of such estimators.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As per [22], enforcing only two conditions on the loss function is sufficient to get a handle on the square error of an `1-regularized M-estimator.", "startOffset": 7, "endOffset": 11}], "year": 2017, "abstractText": "We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information-theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of \u201cinteraction screening\u201d. The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.", "creator": "LaTeX with hyperref package"}}}