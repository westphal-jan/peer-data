{"id": "1605.07334", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Near-optimal Bayesian Active Learning with Correlated and Noisy Tests", "abstract": "We look at the Bayesian problem of active learning and experimental design, where the goal is to learn the value of an unknown target variable through a sequence of informative, noisy tests. In contrast to previous work, we focus on the challenging but practically relevant environment in which test results may be conditionally dependent on the hidden target variable. Under such assumptions, common heuristics, such as greedily performed tests that maximize the reduction of uncertainty of the target, often produce poor results. In this paper, we propose ECED, a novel, computationally efficient active learning algorithm, and demonstrate strong theoretical guarantees that hold true for correlated, noisy tests. Instead of directly optimizing the predictive error at each step, ECED selects the test that maximizes the gain of a substitute object that takes into account the dependencies between the tests. Our analysis relies on an ulsive informational function to directly optimize the predictive error at each step, to differentiate between the subset effectiveness of the problem and the subadaptation of the subset.", "histories": [["v1", "Tue, 24 May 2016 08:25:27 GMT  (3161kb,D)", "http://arxiv.org/abs/1605.07334v1", null], ["v2", "Mon, 11 Jul 2016 06:47:19 GMT  (3162kb,D)", "http://arxiv.org/abs/1605.07334v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["yuxin chen", "s hamed hassani", "reas krause"], "accepted": false, "id": "1605.07334"}, "pdf": {"name": "1605.07334.pdf", "metadata": {"source": "CRF", "title": "Near-optimal Bayesian Active Learning with Correlated and Noisy Tests", "authors": ["Yuxin Chen", "S. Hamed Hassani"], "emails": ["yuxin.chen@inf.ethz.ch", "hamed@inf.ethz.ch", "krausea@ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "Optimal information gathering, i.e., selectively acquiring the most useful data, is one of the central challenges in machine learning. The problem of optimal information gathering has been studied in the context of active learning (Dasgupta, 2004a; Settles, 2012), Bayesian experimental design (Chaloner & Verdinelli, 1995), policy making (Runge et al., 2011), optimal control (Smallwood & Sondik, 1973), and numerous other domains. In a typical set-up for these problems, there is some unknown target variable Y of interest, and a set of tests which correspond to observable variables defined through a probabilistic model. The goal is to determine the value of the target variable with a sequential policy \u2013 which adaptively selects the next test based on previous observations \u2013 such that the cost of performing these tests is minimized.\nDeriving the optimal testing policy is NP-hard in general (Chakaravarthy et al., 2007); however, under certain conditions, some approximation results are known. In particular, if test outcomes are deterministic functions of the target variable (i.e., in the noise-free setting), a simple greedy algorithm, namely Generalized Binary Search (GBS), is guaranteed to provide a near-optimal approximation of the optimal policy (Kosaraju et al., 1999). On the other hand, if test outcomes are noisy, but the outcomes of different tests are conditionally independent given Y (i.e., under the Na\u00efve Bayes assumption), then using the most informative selection policy, which greedily selects the test that maximizes the expected reduction in uncertainty of the target variable (quantified in terms of Shannon entropy), is guaranteed to perform near-optimally (Chen et al., 2015a).\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n07 33\n4v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\nHowever, in many practical problems, due to the effect of noise or complex structural assumptions in the probabilistic model (beyond Na\u00efve Bayes), we only have access to tests that are indirectly informative about the target variable Y (i.e., test outcomes depend on Y through another hidden random variable. See Fig. 1.) \u2013 as a consequence, the test outcomes become conditionally dependent given Y . Consider a medical diagnosis example, where a doctor wants to predict the best treatment for a patient, by carrying out a series of medical tests, each of which reveals some information about the patient\u2019s physical condition. Here, outcomes of medical tests are conditionally independent given the patient\u2019s condition, but are not independent given the treatment, which is made based on the patient\u2019s condition. It is known that in such cases, both GBS and the most informative selection policy (which myopically maximizes the information gain w.r.t. the distribution over Y ) can perform arbitrarily poorly. Golovin et al. (2010) then formalize this problem as an equivalence class determination problem (See \u00a72.1), and show that if the tests\u2019 outcomes are noise-free, then one can obtain near-optimal expected cost, by running a greedy policy based on a surrogate objective function. Their results rely on the fact that the surrogate objective function exhibits adaptive submodularity (Golovin & Krause, 2011), a natural diminishing returns property that generalizes the classical notion of submodularity to adaptive policies. Unfortunately, in the more general setting where tests are noisy, no efficient policies are known to be provably competitive with the optimal policy.\nOur Contribution. In this paper, we introduce Equivalence Class Edge Discounting (ECED), a novel algorithm for practical Bayesian active learning and experimental design problems, and prove strong theoretical guarantees with correlated, noisy tests. In particular, we focus on the setting where the tests\u2019 outcomes indirectly depend on the target variable (and hence conditionally dependent given Y ), and we assume that the outcome of each test can be corrupted by some random, persistent noise (\u00a72). We prove that when the test outcomes are binary, and the noise on test outcomes are mutually independent, then ECED is guaranteed to obtain near-optimal cost, compared with an optimal policy that achieves a lower prediction error (\u00a73). We develop a theoretical framework for analyzing such sequential policies, where we leverage an information-theoretic auxiliary function to reason about the effect of noise, and combine it with the theory of adaptive submodularity to attain the near-optimal bound (\u00a74). The key insight is to show that ECED is effectively making progress in the long run as it picks more tests, even if the myopic choices of tests do not have immediate gain in terms of reducing the uncertainty of the target variable. We demonstrate the compelling performance of ECED on two real-world problem instances, a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons (\u00a75). To facilitate better understanding, we provide the detailed proofs, illustrative examples and a third application on pool-based active learning in the supplemental material."}, {"heading": "2 Preliminaries and Problem Statement", "text": "The Basic Model Let Y be the target random variable whose value we want to learn. The value of Y , which ranges among set Y = {y1, . . . , yt}, depends deterministically on another random variable \u0398 \u2208 supp(\u0398) = {\u03b81, . . . , \u03b8n} with some known distribution P [\u0398]. Concretely, there is a deterministic mapping r : supp(\u0398) \u2192 Y that gives Y = r(\u0398). Let X = {X1, . . . , Xm} be a collection of discrete observable variables that are statistically dependent on \u0398 (see Fig. 1).\nWe use e \u2208 V , {1, . . . ,m} as the indexing variable of a test. Performing each testXe produces an outcome xe \u2208 O (here,O encodes the set of possible outcomes of a test), and incurs a unit cost. We can think of \u0398 as representing the underlying \u201croot-cause\u201d among a set of n possible root-causes of the joint event {X1, . . . , Xm}, and Y as representing the optimal \u201ctarget action\u201d to be taken for root-cause \u0398. Also, each of the Xe\u2019s is a \u201ctest\u201d that we can perform, whose\nobservation reveals some information about \u0398. In our medical diagnosis example (see Fig. 2(a)), Xe\u2019s encode tests\u2019 outcomes, Y encodes the treatment, and \u0398 encodes the patient\u2019s physical condition.\nCrucially, we assume that Xe\u2019s are conditionally independent given \u0398, i.e., P [\u0398, X1, . . . , Xm] = P [\u0398] \u220fm i=1 P [Xi | \u0398] with known parameters. Note that noise is implicitly encoded in our model, as we can equivalently assume that Xe\u2019s are first generated from a deterministic mapping of \u0398, and then perturbed by some random noise. As an example, if test outcomes are binary, then we can think of Xe as resulting from flipping the deterministic outcome of test e given \u0398 with some probability, and the flipping events of the tests are mutually independent.\nProblem Statement We consider sequential, adaptive policies for picking the tests. Denote a policy by \u03c0. In words, a policy specifies which test to pick next, as well as when to stop picking tests, based on the tests picked so far and their corresponding outcomes. After each pick, our observations so far can be represented as a partial realization \u03a8 \u2208 2V\u00d7O (e.g., \u03a8 encodes what tests have been performed and what their outcomes are). Formally, a policy \u03c0 : 2V\u00d7O 7\u2192 V is defined to be a partial mapping from partial realizations \u03a8 to tests. Suppose that running \u03c0 till termination returns a sequence of test-observation pairs of length k, denoted by \u03c8\u03c0, i.e., \u03c8\u03c0 , {(e\u03c0,1, xe\u03c0,1), (e\u03c0,2, xe\u03c0,2), \u00b7 \u00b7 \u00b7 , (e\u03c0,k, xe\u03c0,k)}. This can be interpreted as a random path1 taken by policy \u03c0. Once \u03c8\u03c0 is observed, we obtain a new posterior on \u0398 (and consequently on Y ). After observing \u03c8\u03c0, the MAP estimator of Y has error probability pMAPERR (\u03c8\u03c0) , 1\u2212maxy\u2208Y p(y | \u03c8\u03c0). The expected error probability after running policy \u03c0 is then defined as pERR(\u03c0) , E\u03c8\u03c0 [ pMAPERR (\u03c8\u03c0) ] . In words, pERR(\u03c0) is the expected error probability w.r.t. the posterior, given the final outcome of \u03c0. Let the (worst-case) cost of a policy \u03c0 be cost(\u03c0) , max\u03c8\u03c0 |\u03c8\u03c0|, i.e., the maximum number of tests performed by \u03c0 over all possible paths it takes. Given some small tolerance \u03b4 \u2208 [0, 1], we seek a policy with the minimal cost, such that upon termination, it will achieve expected error probability less than \u03b4. Denote such policy by OPT(\u03b4). Formally, we seek\nOPT(\u03b4) \u2208 arg min \u03c0 cost(\u03c0), s.t. pERR(\u03c0) < \u03b4. (2.1)"}, {"heading": "2.1 Special Case: The Equivalence Class Determination Problem", "text": "Note that computing the optimal policy for Problem (2.1) is intractable in general. When \u03b4 = 0, this problem reduces to the equivalence class determination problem (Golovin et al., 2010; Bellala et al., 2010). Here, the target variables are referred to as equivalence classes, since each y \u2208 Y corresponds to a subset of root-causes in supp(\u0398) that (equivalently) share the same \u201caction\u201d.\nIf tests are noise-free, i.e., \u2200e, P [Xe | \u0398] \u2208 {0, 1}, this problem can be solved near-optimally by the equivalence class edge cutting (EC2) algorithm (Golovin et al., 2010). As is illustrated in Fig. 2, EC2 employs an edge-cutting strategy based on a weighted graph G = (supp(\u0398), E), where vertices represent root-causes, and edges link root-causes that we want to distinguish between. Formally, E , {{\u03b8, \u03b8\u2032} : r(\u03b8) 6= r(\u03b8\u2032)} consists of all (unordered) pairs of root-causes corresponding to different target values (see Fig. 2(b)). We define a weight function w : E \u2192 R\u22650 by w({\u03b8, \u03b8\u2032}) , P [\u03b8] \u00b7 P [\u03b8\u2032], i.e., as the product of the probabilities of its incident root-causes. We extend the weight function on sets of edges E\u2032 \u2286 E, as the sum of weight of all edges {\u03b8, \u03b8\u2032} \u2208 E\u2032, i.e., w(E\u2032) , \u2211 {\u03b8,\u03b8\u2032}\u2208E\u2032 w({\u03b8, \u03b8\u2032}).\nPerforming test e \u2208 V with outcome xe is said to \u201ccut\u201d an edge, if at least one of its incident root-causes is inconsistent with xe (See Fig. 2(c)). Denote E(xe) , {{\u03b8, \u03b8\u2032} \u2208 E : P [xe | \u03b8] = 0 \u2228 P [xe | \u03b8\u2032] = 0} as the set of edges cut by observing xe. The EC2 objective (which is greedily maximized per iteration of EC2), is then defined as the total weight of edges cut by the current partial observation \u03c8\u03c0: fEC2(\u03c8\u03c0) , w (\u22c3 (e,xe)\u2208\u03c8\u03c0 E(xe) ) .\n1What \u03c0 returns in the end is random, dependent on the outcomes of selected tests.\nThe EC2 objective function is adaptive submodular, and strongly adaptive monotone (Golovin et al., 2010). Formally, let \u03c81, \u03c82 \u2208 2V\u00d7O be two partial realizations of tests\u2019 outcomes. We call \u03c81 a subrealization of \u03c82, denoted as \u03c81 \u03c82, if every test seen by \u03c81 is also seen by \u03c82, and P [\u03c82 | \u03c81] > 0. A function f : 2V\u00d7O \u2192 R is called adaptive submodular w.r.t. a distribution P, if for any \u03c81 \u03c82 and any Xe it holds that \u2206(Xe | \u03c81) \u2265 \u2206(Xe | \u03c82), where \u2206(Xe | \u03c8) := Exe [f(\u03c8 \u222a {(e, xe)})\u2212 f(\u03c8) | \u03c8] (i.e., \u201cadding information earlier helps more\u201d). Further, function f is called strongly adaptively monotone w.r.t. P, if for all \u03c8, test e not seen by \u03c8, and xe \u2208 O, it holds that f(\u03c8) \u2264 f(\u03c8 \u222a {(e, xe)}) (i.e., \u201cadding new information never hurts\u201d). For sequential decision problems satisfying adaptive submodularity and strongly adaptive monotonicity, the policy that greedily, upon having observed \u03c8, selects the test e\u2217 \u2208 arg maxe \u2206(Xe | \u03c8), is guaranteed to attain near-minimal cost (Golovin & Krause, 2011).\nIn the noisy setting, however, we can no longer attain 0 error probability (or equivalently, cut all the edges constructed for EC2), even if we exhaust all tests. A natural approach to solving Problem (2.1) for \u03b4 > 0 would be to pick tests greedily maximizing the expected reduction in the error probability pERR. However, this objective is not adaptive submodular; in fact, as we show in the supplemental material, such policy can perform arbitrarily badly if there are complementaries among tests, i.e., the gain of a set of tests can be far better than sum of the individual gains of the tests in the set. Therefore, motivated by the EC2 objective in the noise-free setting, we would like to optimize a surrogate objective function which captures the effect of noise, while being amenable to greedy optimization.\n3 The ECED Algorithm\nWe now introduce ECED for Bayesian active learning under correlated noisy tests, which strictly generalizes EC2 to the noisy setting, while preserving the near-optimal guarantee.\nEC2 with Bayesian Updates on Edge Weights In the noisy setting, the test outcomes are not necessarily deterministic given a root-cause, i.e., \u2200\u03b8, P [Xe | \u03b8] \u2208 [0, 1]. Therefore, one can no longer \u201ccut away\u201d a root-cause \u03b8 by observing xe, as long as P [Xe = xe | \u03b8] > 0. In such cases, a natural extension of the edge-cutting strategy will be \u2013 instead of cutting off edges \u2013 to discount the edge weights through Bayesian updates: After observing xe, we can discount the weight of an edge {\u03b8, \u03b8\u2032}, by multiplying the probabilities of its incident root-causes with the likelihoods of the observation2: w({\u03b8, \u03b8\u2032} | xe) := P [\u03b8]P [\u03b8\u2032] \u00b7 P [xe | \u03b8]P [xe | \u03b8\u2032] = P [\u03b8, xe] \u00b7 P [\u03b8\u2032, xe]. This gives us a greedy policy that, at every iteration, picks the test that has the maximal expected reduction in total edge weight. We call such policy EC2-Bayes. Unfortunately, as we demonstrate later in \u00a75, this seemingly promising update scheme is not ideal for solving our problem: it tends to pick tests that are very noisy, which do not help facilitate differentiation among different target values. Consider a simple example with three root-causes distributed as P [\u03b81] = 0.2,P [\u03b82] = P [\u03b83] = 0.4, and two target values r(\u03b81) = r(\u03b82) = y1, r(\u03b83) = y2. We want to evaluate two tests: (1) a purely noisy test X1, i.e., \u2200\u03b8, P [X1 = 1 | \u03b8] = 0.5, and (2) a noiseless test X2 with P [X2 = 1 | \u03b81] = 1 and P [X2 = 1 | \u03b82] = P [X2 = 1 | \u03b83] = 0. One can easily verify that by running EC2-Bayes, one actually prefers X1 (with expected reduction in edge weight 0.18, as opposed to 0.112 for X2).\nThe ECED Algorithm The example above hints us on an important principle of designing proper objective functions for this task: as the noise rate increases, one must take reasonable precautions when evaluating the informativeness of a test, such that the undesired contribution by noise is accounted for. Suppose we have performed test e and observed xe. We call a root-cause \u03b8 to be \u201cconsistent\u201d with observation xe, if xe is the most likely outcome of Xe given \u03b8 (i.e., xe \u2208 arg maxx P [Xe = x | \u03b8]). Otherwise, we say \u03b8 is inconsistent. Now, instead of discounting the weight of all root-causes by the likelihoods P [Xe = xe | \u03b8] (as EC2-Bayes does), we choose to discount the root-causes by the likelihood ratio: \u03bb\u03b8,xe ,\nP[Xe=xe|\u03b8] maxx\u2032e P[Xe=x\u2032e|\u03b8] . Intuitively, this is\nbecause we want to \u201cpenalize\u201d a root-cause (and hence the weight of its incident edges), only if it is inconsistent with the observation (See Fig. 2(d)). When xe is consistent with root-cause \u03b8, then \u03bb\u03b8,xe = 1 and we do not discount \u03b8; otherwise, if xe is inconsistent with \u03b8, we have \u03bb\u03b8,xe < 1. When a test is not informative for root-cause \u03b8, i.e. P [Xe | \u03b8] is uniform, then \u03bb\u03b8,e = 1, so that it neutralizes the effect of such test in terms of edge weight reduction. Formally, given\n2Here we choose not to normalize the probabilities of \u03b8, \u03b8\u2032 to their posterior probabilities. Otherwise, we can end up having 0 gain in terms of edge weight reduction, even if we perform a very informative test.\nAlgorithm 1: The Equivalence Class Edge Discounting (ECED) Algorithm 1 Input: [\u03bb\u03b8,x]n\u00d7m (or Conditional Probabilities P [X | \u0398]), Prior P [\u0398], Mapping r : supp(\u0398)\u2192 Y;\nbegin 2 \u03c8\u03c0 \u2190 \u2205; foreach (\u03b8, \u03b8\u2032) \u2208 E do 3 w\u03b8,\u03b8\u2032 \u2190 P [\u03b8]P [\u03b8\u2032]; while pERR(\u03c8\u03c0) > \u03b4 do 4 e\u2217 \u2190 arg maxe Exe [\u2211 {\u03b8,\u03b8\u2032}\u2208E w\u03b8,\u03b8\u2032 ( weight discounted\ufe37 \ufe38\ufe38 \ufe37 1\u2212 \u03bb\u03b8,xe\u03bb\u03b8\u2032,xe \u2212 offset term\ufe37 \ufe38\ufe38 \ufe37 (1\u2212max\u03b8 \u03bb2\u03b8,xe) )] ; 5 Observe xe\u2217 ; w\u03b8,\u03b8\u2032 \u2190 w\u03b8,\u03b8\u2032 \u00b7 P [xe\u2217 | \u03b8]P [xe\u2217 | \u03b8\u2032]; 6 \u03c8\u03c0 \u2190 \u03c8\u03c0 \u222a {(e\u2217, xe\u2217)}; 7 Output: y\u2217 = arg maxy P [y | \u03c8\u03c0].\nobservations \u03c8\u03c0 , we define the value of observing xe as the total amount of edge weight discounted: \u03b4BS(xe | \u03c8\u03c0) , \u2211 {\u03b8,\u03b8\u2032}\u2208E P [\u03b8, \u03c8\u03c0]P [\u03b8\u2032, \u03c8\u03c0] \u00b7 (1\u2212 \u03bb\u03b8,xe\u03bb\u03b8\u2032,xe).\nFurther, we call test e to be non-informative, if its outcome does not affect the distribution of \u0398, i.e., \u2200 \u03b8, \u03b8\u2032 \u2208 supp(\u0398) and xe \u2208 O, P [Xe = xe | \u03b8] = P [Xe = xe | \u03b8\u2032]. Obviously, performing a non-informative test does not reveal any useful information of \u0398 (and hence Y ). Therefore, we should augment our basic value function \u03b4BS, such that the value of a non-informative test is 0. Following this principle, we define \u03b4OFFSET(xe | \u03c8\u03c0) , \u2211 {\u03b8,\u03b8\u2032}\u2208E P [\u03b8, \u03c8\u03c0]P [\u03b8\u2032, \u03c8\u03c0]\u00b7(1\u2212max\u03b8 \u03bb2\u03b8,xe), as the offset value for observing outcome xe. It is easy to check that if test e is non-informative, then it holds that \u03b4BS(xe | \u03c8\u03c0)\u2212 \u03b4OFFSET(xe | \u03c8\u03c0) = 0 for all xe \u2208 O; otherwise \u03b4BS(xe | \u03c8\u03c0)\u2212 \u03b4OFFSET(xe | \u03c8\u03c0) \u2265 0. This motivates us to use the following objective function:\n\u2206ECED(Xe | \u03c8\u03c0) , Exe [\u03b4BS(xe | \u03c8\u03c0)\u2212 \u03b4OFFSET(xe | \u03c8\u03c0)] , (3.1) as the expected amount of edge weight that is effectively reduced by performing test e. We call the algorithm that greedily maximizes \u2206ECED the Equivalence Class Edge Discounting (ECED) algorithm, and present the pseudocode in Algorithm 1.\nSimilar with EC2, the efficiency (in terms of computation complexity as well as the query complexity) of ECED depends on the number of root-causes. Let \u03b8,e , 1\u2212maxx P [Xe = x | \u03b8] be the noise rate for test e. As our main theoretical result, we show that under the basic setting where test outcomes are binary, and the test noise is independent of the underlying root-causes (i.e., \u2200\u03b8 \u2208 supp(\u0398), \u03b8,e = e), ECED is competitive with the optimal policy that achieves a lower error probability for Problem (2.1): Theorem 1. Fix \u03b4 \u2208 (0, 1). To achieve expected error probability less than \u03b4, it suffices to run ECED for O ( k c\u03b5 ( log kn\u03b4 log n \u03b4 )2) steps where n , | supp(\u0398)| denotes the number of root-causes, c\u03b5 , mine\u2208V(1\u2212 2 e)2 characterizes the severity of noise, and k , cost (OPT(\u03b4opt)) is the worstcase cost of the optimal policy that achieves expected error probability \u03b4opt , O ( \u03b4 (logn\u00b7log(1/\u03b4))2 ) .\nNote that a pessimistic upper bound for k is the total number of tests m, and hence the cost of ECED is at mostO ( (log(mn/\u03b4) log(n/\u03b4)) 2 /c\u03b5 ) times the worst-case cost of the optimal algorithm, which achieves a lower error probability O ( \u03b4/(log n \u00b7 log(1/\u03b4))2 ) . Further, as one can observe, the upper bound on the cost of ECED degrades as we increase the maximal noise rate of the tests. When c\u03b5 = 1, we have e = 0 for all test e, and ECED reduces to the EC2 algorithm. Theorem 1 implies that running EC2 for O ( k ( log kn\u03b4 log n \u03b4 )2) in the noise-free setting is sufficient to achieve pERR \u2264 \u03b4. Finally, notice that by construction ECED never selects any non-informative test. Therefore, we can always remove purely noisy tests (i.e., {e : \u2200\u03b8, P [Xe = 1 | \u03b8] = P [Xe = 0 | \u03b8] = 1/2}), so that c\u03b5 > 0, and the upper bound in Theorem 1 becomes non-trivial."}, {"heading": "4 Theoretical Analysis", "text": "Information-theoretic Auxiliary Function We now present the main idea behind the proof of Theorem 1. In general, an effective way to relate the performance (measured in terms of the gain in the\ntarget objective function) of the greedy policy to the optimal policy is by showing that, the one-step gain of the greedy policy always makes effective progress towards approaching the cumulative gain of OPT over k steps. One powerful tool facilitating this is the adaptive submodularity theory, which imposes a lower bound on the one-step greedy gain against the optimal policy, given that the objective function in consideration exhibits a natural diminishing returns condition. Unfortunately, in our context, the target function to optimize, i.e., the expected error probability of a policy, does not satisfy adaptive submodularity. Furthermore, it is nontrivial to understand how one can directly relate the two objectives: the ECED objective of (3.1), which we utilize for selecting informative tests, and the gain in the reduction of error probability, which we use for evaluating a policy.\nWe circumvent such problems by introducing surrogate functions, as a proxy to connect the ECED objective \u2206ECED with the expected reduction in error probability pERR. Ideally, we aim to find some auxiliary objective fAUX, such that the tests with the maximal \u2206ECED also have a high gain in fAUX; meanwhile, fAUX should also be comparable with the error probability pERR, such that minimizing fAUX itself is sufficient for achieving low error probability.\nWe consider the function fAUX : 2V\u00d7O \u2192 R\u22650, defined as\nfAUX(\u03c8) = \u2211\n(\u03b8,\u03b8\u2032)\u2208E P [\u03b8 | \u03c8]P [\u03b8\u2032 | \u03c8] \u00b7 log 1\nP [\u03b8 | \u03c8]P [\u03b8\u2032 | \u03c8] + c \u2211\ny\u2208Y H2 (P [y | \u03c8]) . (4.1)\nHere H2 (x) := \u2212x log x\u2212 (1\u2212 x) log(1\u2212 x), and c is a constant that will be made concrete shortly (in Lemma 3). Interestingly, we show that function fAUX is intrinsically linked to the error probability:\nLemma 2. We consider the auxiliary function defined in Equation (4.1). Let n , | supp(\u0398)| be the number of root-causes, and pMAPERR (\u03c8) be the error probability given partial realization \u03c8. Then\n2c \u00b7 pMAPERR (\u03c8) \u2264 fAUX(\u03c8) \u2264 (3c+ 4) \u00b7 ( H2 ( pMAPERR (\u03c8) ) + pMAPERR (\u03c8) log n ) .\nTherefore, if we can show that by running ECED, we can effectively reduce fAUX, then by Lemma 2, we can conclude that ECED also makes significant progress in reducing the error probability pMAPERR .\nBounding the Gain w.r.t. the Auxiliary Function It remains to understand how ECED interacts with fAUX. For any test e, we define \u2206AUX(Xe | \u03c8) , Exe [fAUX(\u03c8 \u222a {e, xe})\u2212 fAUX(\u03c8) | \u03c8] to be the expected gain of test e in fAUX. Let \u2206EC2,\u03c8(Xe) denote the gain of test e in the EC\n2 objective, assuming that the edge weights are configured according to the posterior distribution P [\u0398 | \u03c8]. Similarly, let \u2206ECED,\u03c8(Xe) denote the ECED gain, if the edge weights are configured according to P [\u0398 | \u03c8]. We prove the following result: Lemma 3. Let n = | supp(\u0398)|, t = |Y|, and be the noise rate associated with test e \u2208 V . Fix \u03b7 \u2208 (0, 1). We consider fAUX as defined in Equation (4.1), with c = 8 ( log(2n2/\u03b7) )2 . It holds that\n\u2206AUX(Xe | \u03c8) + c\u03b7, \u2265 \u2206ECED,\u03c8(Xe) \u00b7 (1\u2212 )2/16 = c \u2206EC2,\u03c8(Xe) ,\nwhere c\u03b7, = 2t(1\u2212 2 )2\u03b7, and c , (1\u2212 2 )2/16.\nLemma 3 indicates that the test being selected by ECED can effectively reduce fAUX.\nLifting the Adaptive Submodularity Framework Recall that our general strategy is to bound the one step gain in fAUX against the gain of an optimal policy. In order to do so, we need to show that our surrogate exhibits, to some extent, the diminishing returns property. By Lemma 3 we can relate \u2206AUX(Xe | \u03c8\u03c0), i.e., the gain in fAUX under the noisy setting, to \u2206EC2,\u03c8(Xe), i.e., the expected weight of edges cut by the EC2 algorithm. Since fEC2 is adaptive submodular, this allows us to lift the adaptive submodularity framework into the analysis. As a result, we can now relate the 1-step gain w.r.t. fAUX of a test selected by ECED, to the cumulative gain w.r.t. fEC2 of an optimal policy in the noise-free setting. Further, observe that the EC2 objective at \u03c8 satisfies:\nfEC2,\u03c8 := \u2211\ny\nP [y | \u03c8] (1\u2212 P [y | \u03c8]) (a)\n\u2265 1\u2212max y P [y | \u03c8] = pMAPERR (\u03c8). (4.2)\nHereby, step (a) is due to the fact that the error probability of a MAP estimator always lower bounds that of a stochastic estimator (which is drawn randomly according to the posterior distribution of Y ).\nSuppose we want to compare ECED against an optimal policy OPT. By adaptive submodularity, we can relate the 1-step gain of ECED in fEC2,\u03c8 to the cummulative gain of OPT. Combining Equation (4.2) with Lemma 2 and Lemma 3, we can bound the 1-step gain in fAUX of ECED against the k-step gain of OPT, and consequently bound the cost of ECED against OPT for Problem 2.1. We defer a more detailed proof outline and the full proof to the supplemental material."}, {"heading": "5 Experimental Results", "text": "We now demonstrate the performance of ECED on two real-world problem instances: a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons. Due to space limitations, we defer a third case study on pool-based active learning to the supplemental material.\nBaselines. The first baseline we consider is EC2-Bayes, which uses the Bayes\u2019 rule to update the edge weights when computing the gain of a test (as described in \u00a73). Note that after observing the outcome of a test, both ECED and EC2-Bayes update the posteriors on \u0398 and Y according to the Bayes\u2019 rule; the only difference is that they use different strategies when selecting a test. We also compare with two commonly used sequential information gathering policies: Information Gain (IG), and Uncertainty Sampling (US), which consider picking tests that greedily maximizing the reduction of entropy over the target variable Y , and root-causes \u0398 respectively. Last, we consider myopic optimization of the decision-theoretic value of information (VOI) (Howard, 1966). In our problems, the VOI policy greedily picks the test maximizing the expected reduction in prediction error in Y ."}, {"heading": "5.1 Preference Elicitation in Behavioral Economics", "text": "We first conduct experiments on a Bayesian experimental design task, which intends to distinguish among economic theories of how people make risky decisions. Several theories have been proposed in behavioral economics to explain how people make decisions under risk and uncertainty. We test ECED on six theories of subjective valuation of risky choices (Wakker, 2010; Tversky & Kahneman, 1992; Sharpe, 1964), namely (1) expected utility with constant relative risk aversion, (2) expected value, (3) prospect theory, (4) cumulative prospect theory, (5) weighted moments, and (6) weighted standardized moments. Choices are between risky lotteries, i.e., known distribution over payoffs (e.g., the monetary value gained or lost). A test e , (L1, L2) is a pair of lotteries, and root-causes \u0398 correspond to parametrized theories that predict, for a given test, which lottery is preferable. The goal, is to adaptively select a sequence of tests to present to a human subject in order to distinguish which of the six theories best explains the subject\u2019s responses. We employ the same set of parameters used in Ray et al. (2012) to generate tests and root-causes. In particular, we have generated\u223c16K tests. Given root-cause \u03b8 and test e = (L1, L2), one can compute the values of L1 and L2, denoted by v1 and v2. Then, the probability that root-cause \u03b8 favors L1 is modeled as P [Xe = 1 | \u03b8] = 11+exp(\u2212\u03bb\u00b7(v1\u2212v2)) .\nResults Fig. 3(a) demonstrates the performance of ECED on this data set. The average error probability has been computed across 1000 random trials for all methods. We observe that ECED and EC2-Bayes have similar behavior on this data set; however, the performance of the US algorithm is much worse. This can be explained by the nature of the data set: it has more concentrated distribution over \u0398, but not Y . Therefore, since tests only provide indirect information about Y through \u0398, what the uncertainty sampling scheme tries to optimize is actually \u0398, hence it performs quite poorly."}, {"heading": "5.2 Preference Learning via Pairwise Comparisons", "text": "The second application considers a comparison-based movie recommendation system, which learns a user\u2019s movie preference (e.g., the favorable genre) by sequentially showing her pairs of candidate movies, and letting her choose which one she prefers. We use the MovieLens 100k dataset (Herlocker et al., 1999), which consists of a matrix of 1 to 5 ratings of 1682 movies from 943 users, and adopt the experimental setup proposed in Chen et al. (2015b). In particular, we extract movie features by computing a low-rank approximation of the user/rating matrix of the MovieLens 100k dataset through singular value decomposition (SVD). We then simulate the target \u201ccategories\u201d Y that a user may be interested by partitioning the set of movies into t (non-overlapping) clusters in the Euclidean space. A root-cause \u0398 corresponds to user\u2019s favorite movie, and tests e\u2019s are given in the form of movie pairs, i.e., e , (ma,mb), where a and b are embeddings of movie ma and mb in Euclidean space. Suppose user\u2019s movie is represented by \u03b8, then test e is realized as 1 if a is closer to y than b, and 0 otherwise. We simulate the effect of noise by P [Xe = 1 | \u03b8] = 11+exp(\u2212\u03bb\u00b7(d(ma,\u03b8)\u2212d(mb,\u03b8))) . where d(\u00b7, \u00b7) is the distance function, and \u03bb control the level of noise in the system.\nResults Fig. 3(b) shows the performance of ECED compared other baseline methods, when we fix the size of Y to be 20 and \u03bb to be 10. We compute the average error probability across 1000 random trials for all methods. We can see that ECED consistently outperforms all other baselines. Interestingly, EC2-Bayes performs poorly on this data set. This may be due to the fact that the noise level is still high, misguiding the two heuristics to select noisy, uninformative tests. Fig. 3(c) shows the performance of ECED as we vary \u03bb. When \u03bb = 100, the tests become close to deterministic given a root-cause, and ECED is able to achieve 0 error with \u223c 12 tests. As we increase the noise rate (i.e., decrease \u03bb), it takes ECED many more queries for the prediction error to converge. This is because with high noise rate, ECED discounts the root-causes more uniformly, hence they are hardly informative in Y . This comes at the cost of performing more tests, and hence low convergence rate."}, {"heading": "6 Related Work", "text": "Active learning in statistical learning theory. In most of the theoretical active learning literature (e.g., Dasgupta (2004b); Hanneke (2007, 2014); Balcan & Urner (2015)), sample complexity bounds have been characterized in terms of the structure of the hypothesis class, as well as additional distribution-dependent complexity measures (e.g., splitting index (Dasgupta, 2004b), disagreement coefficient (Hanneke, 2007), etc); In comparison, in this paper we seek computationally-efficient approaches that are provably competitive with the optimal policy. Therefore, we do not seek to bound how the optimal policy behaves, and hence we make no assumptions on the hypothesis class.\nPersistent noise vs non-persistent noise. If tests can be repeated with i.i.d. outcomes, the noisy problem can then be effectively reduced to the noise-free setting (K\u00e4\u00e4ri\u00e4inen, 2006; Karp & Kleinberg, 2007; Nowak, 2009). While the modeling of non-persistent noise may be appropriate in some settings (e.g., if the noise is due to measurement error), it is often important to consider the setting of persistent noise in many other applications. In many applications, repeating tests are impossible, or repeating a test produces identical outcomes. For example, it could be unrealistic to replicate a medical test for practical clinical treatment. Despite of some recent development in dealing with persistent noise in simple graphical models (Chen et al., 2015a) and strict noise assumptions (Golovin et al., 2010), more general settings, which we focus on in this paper, are much less understood."}, {"heading": "7 Conclusion", "text": "We have introduced ECED, which strictly generalizes the EC2 algorithm, for solving practical Bayesian active learning and experimental design problems with correlated and noisy tests. We have proved that ECED enjoys strong theoretical guarantees, by introducing an analysis framework that draws upon adaptive submodularity and information theory. We have demonstrated the compelling performance of ECED on two (noisy) problem instances, including an active preference learning task via pairwise comparisons, and a Bayesian experimental design task for preference elicitation in behavioral economics. We believe that our work makes an important step towards understanding the theoretical aspects of complex, sequential information gathering problems, and provides useful insight on how to develop practical algorithms to address noise."}, {"heading": "A Table of Notations Defined in the Main Paper", "text": "We summarize the notations used in the main paper in Table 1."}, {"heading": "B The Analysis Framework", "text": "In this section, we provide the proofs of our theoretical results in full detail. Recall that for the theoretical analysis, we study the basic setting where test outcomes are binary, and the test noise is independent of the underlying root-causes (i.e., given a test e, the noise rate on the outcome of test e is only a function of e, but not a function of \u03b8)."}, {"heading": "B.1 The Auxiliary Function and the Proof Outline", "text": "The general idea behind our analysis, is to show that by running ECED, the one-step gain in learning the value of the target variable is significant, compared with the cumulative gain of an optimal policy over k steps (see Fig. 4).\nIn Appendix \u00a7C, we show that if tests are greedily selected to optimize the (reduction in) expected prediction error, we may end up failing to pick some tests, which have negligible immediate gain in terms of error reduction, but are very informative in the long run. ECED bypasses such an issue by selecting tests that maximally distinguish root-causes with different target values. In order to analyze ECED, we need to find an auxiliary function that properly tracks the \u201cprogress\u201d of the ECED algorithm; meanwhile, this auxiliary function should allow us to connect the heuristic by which we select tests (i.e., \u2206ECED), with the target objective of interest (i.e., the expected prediction error pERR).\nWe consider the auxiliary function defined in Equation (4.1). For brevity, we suppress the dependence of \u03c8 where it is unambiguous. Further, we use p\u03b8, p\u03b8\u2032 , and py as shorthand notations for P [\u03b8 | \u03c8], P [\u03b8\u2032 | \u03c8] and P [y | \u03c8]. Equation (4.1) can be simplified as\nfAUX = \u2211\n(\u03b8,\u03b8\u2032)\u2208E p\u03b8p\u03b8\u2032 log\n1\np\u03b8p\u03b8\u2032 + c\n\u2211 y\u2208Y H2 (py) (B.1)\nWe illustrate the outline of our proofs in Fig. 5. Our goal is to bound the cost of ECED against the cost of OPT (Theorem 1; proof provided in Appendix \u00a7B.6). As we have explained earlier, our strategy is to relate the one-step gain of ECED 1-step: AUX(e`+1 | `) with the gain of OPT in k-steps OPT: AUX (Appendix \u00a7B.5, Lemma 8). To achieve that, we divide our proof into three parts:\n1. We show that the auxiliary function fAUX is closely related with the target objective function pERR. More specifically, we provide both an upper bound Ub pMAPerr and a lower bound\nLb pMAPerr of fAUX in Lemma 2, and give the detailed proofs in Appendix \u00a7B.2.\n2. To analyze the one-step gain of ECED, we introduce another intermediate auxiliary function: For a test e`+1 chosen by ECED, we relate its one-step gain in the auxiliary function 1-step: AUX(Xe`+1 | `) , to its one-step gain in the EC2 objective 1-step: EC2, (\nLemma 3, detailed proof provided in Appendix \u00a7B.3). The reason why we introduce this step is that the EC2 objective is adaptive submodular , by which we can relate the 1-step gain of a greedy policy 1-step: EC2, to an optimal policy OPT: EC2, .\n3. To close the loop, it remains to connect the gain of an optimal policy OPT in the EC2 objective function OPT: EC2, , with the gain of OPT in the auxiliary function OPT: AUX . We show how to achieve this connection ( ) in Appendix \u00a7B.4, by\nrelating OPT: EC2, to the expected reduction in prediction error, and further in \u00a7B.5, by applying the upper bound Ub pMAPerr provided in \u00a7B.2.\nTo make the proof more accessible, we insert the annotated color blocks from Fig. 5 (i.e., Ub pMAPerr ,\nLb pMAPerr , 1-step: AUX(Xe`+1 | `) , 1-step: EC2, , OPT: EC2, , OPT: AUX , etc), into the subsequent subsections in Appendix \u00a7B, so that readers can easily relate different parts of this section to the proof outline. Note that we only use these annotated color blocks for positioning the proofs, and hence readers can ignore the notations, as it may slightly differ from the ones used in the proof.\nB.2 Proof of Lemma 2: Relating fAUX to pERR\nDefine pE(\u03c8) , \u2211 y\u2208Y P [y | \u03c8] (1\u2212 P [y | \u03c8]) as the prediction error of a stochastic estimator upon observing \u03c8, i.e., the probability of mispredicting y if we make a random draw from P [Y | \u03c8]. We show in Lemma 4 that pMAPERR (\u03c8) is within a constant factor of pE(\u03c8):\nLemma 4. Fix \u03c8, it holds that pMAPERR (\u03c8) \u2264 pE(\u03c8) \u2264 2pMAPERR (\u03c8).\nProof of Lemma 4. We can always lower bound pE by pMAPERR , since by definition, p MAP ERR (\u03c8) = 1 \u2212\nmaxy P [y | \u03c8] = \u2211 y\u2208Y P [y | \u03c8] \u00b7 (1\u2212maxy P [y | \u03c8]) \u2264 \u2211 y\u2208Y P [y | \u03c8] (1\u2212 P [y | \u03c8]) = pE(\u03c8).\nTo prove the second part, we write pyi = P [Y = yi | \u03c8] for all yi \u2208 Y . W.l.o.g., we assume py1 \u2265 py2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 pyt . Then pMAPERR = 1\u2212 py1 . We further have\n2pMAPERR = 2(1\u2212 py1) = 2( t\u2211\ni=2\npyi) = 2(\nt\u2211\ni=1\npyi)(\nt\u2211\ni=2\npyi) = 2(py1 +\nt\u2211\ni=2\npyi)(\nt\u2211\ni=2\npyi)\n\u2265 2py1( t\u2211\ni=2\npyi) + (\nt\u2211\ni=2\npyi) 2\n\u2265 t\u2211\ni 6=j pyipyj =\n\u2211\ni\npyi(1\u2212 pyi) = pE\nNow, we provide lower and upper bounds of the second term in the RHS of Equation (B.1): Lemma 5. 2pMAPERR \u2264 \u2211 y\u2208Y H2 (py) \u2264 3(H2 ( pMAPERR ) + pMAPERR log n).\nProof of Lemma 5. We first prove the inequality on the left. Expanding the middle term involving the binary entropy of py , we get\n\u2211 y\u2208Y H2 (py) = \u2211 y\u2208Y ( py log 1 py + (1\u2212 py) log 1 1\u2212 py )\n(a) \u2265 2\nln 2\n\u2211 y\u2208Y py(1\u2212 py)\n\u2265 2pE Lemma 4 \u2265 2pMAPERR\nHere, step (a) is by inequality lnx \u2265 1\u2212 1/x for x \u2265 0. To prove the second part, we first show in the following that\n\u2211 y(1\u2212 py) log 11\u2212py \u2264 2 \u2211 y py log 1 py .\nW.l.o.g., we assume that the probabilities py\u2019s are in decreasing order, i.e., py1 \u2265 py2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 pyt . Observe that if py \u2208 [0, 1/2], then (1\u2212 py) log 11\u2212py \u2264 py log 1 py . Consider the following two cases:\n1. py1 \u2264 1/2. In this case, we have \u2211 y(1\u2212 py) log 11\u2212py \u2264 \u2211 y py log 1 py . 2. py1 > 1/2. Since \u2211 i>1 pyi = 1\u2212 py1 , we have\n\u2211\ni\n(1\u2212 pyi) log 1\n1\u2212 pyi = (1\u2212 py1) log\n1 1\u2212 py1 + \u2211\ni>1\n(1\u2212 pyi) log 1\n1\u2212 pyi\n= \u2211\ni>1\npyi log 1\u2211 i>1 pyi + \u2211\ni>1\n(1\u2212 pyi) log 1\n1\u2212 pyi\n\u2264 \u2211\ni>1\npyi log 1 pyi + \u2211\ni>1\n(1\u2212 pyi) log 1\n1\u2212 pyi\n\u2264 \u2211\ni>1\npyi log 1 pyi + \u2211\ni>1\npyi log 1\npyi\n\u2264 2 \u2211\ni>0\npyi log 1\npyi\nTherefore, \u2211\ny\u2208Y H2 (py) \u2264 3\n\u2211\ni>0\npyi log 1\npyi = 3H (Y ) . (B.2)\nFurthermore, by Fano\u2019s inequality (in the absence of conditioning), we know that H (Y ) \u2264 H2 ( pMAPERR ) + pMAPERR log(|Y| \u2212 1). Combining with Equation (B.2) we get\n\u2211\ny\nH2 (py) \u2264 3H (Y ) \u2264 3 ( H2 ( pMAPERR ) + log(|Y| \u2212 1) ) (b) \u2264 3 ( H2 ( pMAPERR ) + log(n) )\nwhere in (b) we use the fact that t = |Y| \u2264 | supp(\u0398)| = n, since Y = r(\u0398) is a function of \u0398. Hence it completes the proof.\nNext, we bound the first term on the RHS of Equation (B.1), i.e., \u2211 {\u03b8,\u03b8\u2032}\u2208E p\u03b8p\u03b8\u2032 log 1 p\u03b8p\u03b8\u2032\n, against pMAPERR : Lemma 6. \u2211 {\u03b8,\u03b8\u2032}\u2208E p\u03b8p\u03b8\u2032 log 1 p\u03b8p\u03b8\u2032 \u2264 2(H2 (pE) + pE log n).\nProof of Lemma 6. We can expand the LHS as\nLHS = \u2212 \u2211\n\u03b8\u2032\np\u03b8\u2032 \u2211\n\u03b8:r(\u03b8)6=r(\u03b8\u2032) p\u03b8(log p\u03b8 + log p\u03b8\u2032)\n= \u22122 \u2211\n\u03b8\u2032\np\u03b8\u2032 \u2211\n\u03b8:r(\u03b8)6=r(\u03b8\u2032) p\u03b8 log p\u03b8\n= \u22122 \u2211\ny\u2208Y\n\u2211\n\u03b8\u2032:r(\u03b8\u2032)=y\np\u03b8\u2032 \u2211\n\u03b8:r(\u03b8) 6=y p\u03b8 log p\u03b8\n= 2 \u2211\ny\u2208Y py(1\u2212 py)\n\u2211\n\u03b8:r(\u03b8)6=y\np\u03b8 1\u2212 py\n( log\np\u03b8 1\u2212 py\n+ log (1\u2212 py) )\n= \u22122 \u2211\ny\u2208Y py(1\u2212 py) log(1\u2212 py) + 2\n\u2211 y\u2208Y py(1\u2212 py)H\n({ p\u03b8\n(1\u2212 py)\n}\n\u03b8:r(\u03b8)6=y\n) (B.3)\n\u2264 2 \u2211\ny\u2208Y pyH2 (1\u2212 py) + 2\n\u2211 y\u2208Y py(1\u2212 py)H\n({ p\u03b8\n(1\u2212 py)\n}\n\u03b8:r(\u03b8)6=y\n)\nSince H ({\np\u03b8 (1\u2212py) } \u03b8:r(\u03b8)6=y ) \u2264 log t \u2264 log n, we have\nLHS \u2264 2 \u2211\ny\u2208Y pyH2 (1\u2212 py) + 2\n\u2211\ny py(1\u2212 py) log n\ufe38 \ufe37\ufe37 \ufe38 pE logn\nJensen \u2264 2H2\n \u2211\ny\u2208Y py(1\u2212 py)\n + 2pE log n\n= 2 (H2 (pE) + pE log n) . which completes the proof.\nNow, we are ready to state the upper bound Ub pMAPerr and lower bound Lb pMAPerr of fAUX.\nProof of Lemma 2. Clearly, \u2211 {\u03b8,\u03b8\u2032}\u2208E p\u03b8p\u03b8\u2032 log 1 p\u03b8p\u03b8\u2032 \u2265 0. By Lemma 5 we get the lower bound:\nfAUX(\u03c8) \u2265 2c \u00b7 pMAPERR (\u03c8). Now assume pMAPERR \u2264 1/4. By Lemma 4 we know pE \u2264 2pMAPERR , and H2 (pE) \u2264 H2 ( 2pMAPERR ) \u2264\n2H2 ( pMAPERR ) . Combining with Lemma 5 and Lemma 6, we get\nfAUX(\u03c8) \u2264 3c \u00b7 ( H2 ( pMAPERR ) + pMAPERR log n ) + 4 (H2 (pE) + pE log n)\n\u2264 (3c+ 4) \u00b7 ( H2 ( pMAPERR ) + pMAPERR log n ) ,\nwhich completes the proof."}, {"heading": "B.3 Proof of Lemma 3: Bounding \u2206AUX against \u2206EC2 , \u2206ECED", "text": "In this section, we analyze the 1-step gain in the auxiliary function 1-step: AUX(Xe`+1 | `) , of any test e \u2208 V . By the end of this section, we will show that it is lowered bounded by the one-step gain in the EC2 objective 1-step: EC2, .\nRecall that we assume test outcomes are binary for our analysis, and in the following of this section, we assume the outcome xe of test e is in {+,\u2212} instead of {0, 1}, for clarity purposes.\nB.3.1 Notations and the Intermediate Goal\nh : Pr[{ }] q : Pr[{ } | Xe = ]p : Pr[{ } | Xe = +]\nFor brevity, we first define a few short-hand notations to simplify our derivation. Let p, q be two distributions on \u0398, and h = h+p+ h\u2212q be the convex combination of the two, where h+, h\u2212 \u2265 0 and h+ + h\u2212 = 1.\nIn fact, we are using p and q to refer to the posterior distribution over \u0398 after we observe the (noisy) outcome of some binary test e, and use h to refer to the distribution over \u0398 before we perform the test, i.e., p\u03b8 , P [\u03b8 | Xe = +], q\u03b8 , P [\u03b8 | Xe = \u2212], and h\u03b8 , P [\u03b8] = h+p\u03b8 + h\u2212q\u03b8, where h+ = P [Xe = +] and h\u2212 = P [Xe = \u2212]. For yi \u2208 Y , we use pi , \u2211 \u03b8:r(\u03b8)=yi\np\u03b8 to denote the probability of yi under distribution p, and use qi , \u2211 \u03b8:r(\u03b8)=yi\nq\u03b8 to denote the probability of yi under distribution q.\nFurther, given a test e, we define \u0398+i , \u0398 \u2212 i to be the set of root-causes associated with target yi, whose favorable outcome of test e is + (for \u0398+i ) and \u2212 (for \u0398\u2212i ). Formally, \u0398+i , {\u03b8 : r(\u03b8) = yi \u2227 P [Xe = + | \u03b8] \u2265 1/2} \u0398\u2212i , {\u03b8 : r(\u03b8) = yi \u2227 P [Xe = + | \u03b8] < 1/2}\nWe then define \u0398+ , \u22c3 i\u2208{1,...,t}\u0398 + i , and \u0398 \u2212 , \u22c3 i\u2208{1,...,t}\u0398 \u2212 i , to be the set of \u201cpositive\u201d and \u201cnegative\u201d root-causes for test e, respectively.\nLet \u03b1i, \u03b2i be the probability mass of the root-causes in \u0398+i and \u0398 \u2212 i , i.e., \u03b1i , \u2211 y\u2208\u0398+i\nP [\u03b8], and \u03b2i , \u2211 y\u2208\u0398\u2212i P [\u03b8] . We further define \u03b1 , \u2211 yi\u2208Y \u03b1i = \u2211 \u03b8\u2208\u0398+ P [\u03b8], and \u03b2 , \u2211 yi\u2208Y \u03b2y =\u2211\n\u03b8\u2208\u0398\u2212 P [\u03b8], then clearly we have \u03b1+ \u03b2 = 1. See Fig. 6 for illustration.\nNow, we assume that test e has error rate . That is, \u2200\u03b8, min{P [Xe = + | \u03b8] ,P [Xe = \u2212 | \u03b8]} = . Then, by definition of h+, h\u2212, pi, qi, p\u03b8, q\u03b8, it is easy to verify that\nh+ = \u03b1\u0304+ \u03b2 , h\u2212 = \u03b1 + \u03b2\u0304\npi = \u03b1i\u0304+ \u03b2i\nh+ , qi =\n\u03b1i + \u03b2i\u0304\nh\u2212\np\u03b8 = h\u03b8 \u0304\nh+ , q\u03b8 =\nh\u03b8 h\u2212 , if \u03b8 \u2208 \u0398+i\np\u03b8 = h\u03b8\nh+ , q\u03b8 =\nh\u03b8 \u0304\nh\u2212 , if \u03b8 \u2208 \u0398\u2212i (B.4)\nFor the convenience of readers, we summarize the notations provided above in Table 2.\nGiven root-causes \u03b8 and \u03b8\u2032, we use \u03b8 \u03b8\u2032 to denote that the values of the target variable Y associated with root-causes \u03b8 and \u03b8\u2032 are different, i.e., r(\u03b8) 6= r(\u03b8\u2032). We can rewrite the auxiliary function (as defined in Equation (4.1)) as follows:\nfAUX = \u2211\n\u03b8 \u03b8\u2032 h\u03b8h\u03b8\u2032 log\n1\nh\u03b8h\u03b8\u2032 + c\n\u2211\nyi\u2208Y H2 (hi) .\nIf by performing test e we observe Xe = +, we have\nfAUX((e,+)) = \u2211\n\u03b8 \u03b8\u2032 p\u03b8p\u03b8\u2032 log\n1\np\u03b8p\u03b8\u2032 + c\n\u2211\nyi\u2208Y H2 (pi)\notherwise, if we observe Xe = \u2212,\nfAUX((e,\u2212)) = \u2211\n\u03b8 \u03b8\u2032 q\u03b8q\u03b8\u2032 log\n1\nq\u03b8q\u03b8\u2032 + c\n\u2211\nyi\u2208Y H2 (qi)\nTherefore, the expected gain (i.e., 1-step: AUX(Xe`+1 | `) ) of performing test e is,\n\u2206AUX =\n1\ufe37 \ufe38\ufe38 \ufe37 \u2211 \u03b8 \u03b8\u2032 h\u03b8h\u03b8\u2032 log 1 h\u03b8h\u03b8\u2032 \u2212 ( h+ \u2211 \u03b8 \u03b8\u2032 p\u03b8p\u03b8\u2032 log 1 p\u03b8p\u03b8\u2032 + h\u2212 \u2211 \u03b8 \u03b8\u2032 q\u03b8q\u03b8\u2032 log 1 q\u03b8q\u03b8\u2032 )\n+ c\n \u2211\nyi\u2208Y H2 (hi)\u2212\n h+ \u2211\nyi\u2208Y H2 (pi) + h\u2212\n\u2211\nyi\u2208Y H2 (qi)\n   \n\ufe38 \ufe37\ufe37 \ufe38 2\n(B.5)\nIn the following, we derive lower bounds for the above two terms respectively."}, {"heading": "B.3.2 A Lower Bound on Term 1", "text": "Let g\u03b8,\u03b8\u2032 , h+p\u03b8p\u03b8\u2032 + h\u2212q\u03b8q\u03b8\u2032 . Then, we can rewrite Term 1 as,\nTerm 1 = \u2211\n\u03b8 \u03b8\u2032 h\u03b8h\u03b8\u2032 log\n1 h\u03b8h\u03b8\u2032 \u2212 \u2211\n\u03b8 \u03b8\u2032 g\u03b8,\u03b8\u2032 log\n1\ng\u03b8,\u03b8\u2032 \ufe38 \ufe37\ufe37 \ufe38\nPart 1\n+ \u2211\n\u03b8 \u03b8\u2032 g\u03b8,\u03b8\u2032 log\n1 g\u03b8,\u03b8\u2032 \u2212 ( h+ \u2211\n\u03b8 \u03b8\u2032 p\u03b8p\u03b8\u2032 log\n1\np\u03b8p\u03b8\u2032 + h\u2212\n\u2211 \u03b8 \u03b8\u2032 q\u03b8q\u03b8\u2032 log 1 q\u03b8q\u03b8\u2032\n)\n\ufe38 \ufe37\ufe37 \ufe38 Part 2\n(B.6)\nPart 1. We first provide a lower bound for part 1 of Equation (B.6).\nNotice that for concave function f(x) = x log 1x and \u03b4 < x, it holds that f(x)\u2212f(x\u2212\u03b4) \u2265 \u03b4 \u2202f \u2202x \u2223\u2223 x = \u03b4(log 1x \u2212 1), then we get\n\u2211 \u03b8 \u03b8\u2032 h\u03b8h\u03b8\u2032 log 1 h\u03b8h\u03b8\u2032 \u2212 \u2211 \u03b8 \u03b8\u2032 g\u03b8,\u03b8\u2032 log 1 g\u03b8,\u03b8\u2032 \u2265 \u2211 \u03b8 \u03b8\u2032 (h\u03b8h\u03b8\u2032 \u2212 g\u03b8,\u03b8\u2032)\n( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\nFurther, observe\nh\u03b8h\u03b8\u2032 \u2212 g\u03b8,\u03b8\u2032 = (h+p\u03b8 + h\u2212q\u03b8)(h+p\u03b8\u2032 + h\u2212q\u03b8\u2032)\u2212 (h+p\u03b8p\u03b8\u2032 + h\u2212q\u03b8q\u03b8\u2032) = (h+p\u03b8 + h\u2212q\u03b8)(p\u03b8\u2032 + q\u03b8\u2032 \u2212 h\u2212p\u03b8\u2032 \u2212 h+q\u03b8\u2032)\u2212 (h+p\u03b8p\u03b8\u2032 + h\u2212q\u03b8q\u03b8\u2032) = h+h\u2212p\u03b8\u2032q\u03b8 \u2212 h+h\u2212p\u03b8\u2032p\u03b8 + h+h\u2212p\u03b8q\u03b8\u2032 \u2212 h\u2212h+q\u03b8\u2032q\u03b8 = \u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032)\nCombining the above two equations gives us\nPart 1 \u2265 \u2211\n\u03b8 \u03b8\u2032 \u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032)\n( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\nFor any root-cause pair {\u03b8, \u03b8\u2032} with \u03b8 \u03b8\u2032, and binary test e, there are only 4 possible combinations in terms of the root-causes\u2019 favorable outcomes. Namely,\n1. Both \u03b8 and \u03b8\u2032 maps x to +, i.e., \u03b8 \u2208 \u0398+ \u2227 \u03b8\u2032 \u2208 \u0398+.\nWe define such set of root-cause pairs with positive favorable outcomes as U(+,+) , {{\u03b8, \u03b8\u2032} : \u03b8 \u2208 \u0398+ \u2227 \u03b8\u2032 \u2208 \u0398+} (For other cases, we define U(\u2212,\u2212), U(+,\u2212), U(\u2212,+) in a similar way).\nIn this case, we have\n\u2211\n{\u03b8,\u03b8\u2032}\u2208U(+,+)\n\u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032) ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\nEq (B.4) =\n\u2211\n{\u03b8,\u03b8\u2032}\u2208U(+,+)\n\u2212h+h\u2212 ( h\u03b8 \u0304\nh+ \u2212 h\u03b8 h\u2212\n)( h\u03b8\u2032 \u0304\nh+ \u2212 h\u03b8\u2032 h\u2212\n)( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\n=h+h\u2212 ( h\u2212\u0304\u2212 h+\u0304 h+h\u2212 )2 \u2211\n{\u03b8,\u03b8\u2032}\u2208U(+,+)\n\u2212h\u03b8h\u03b8\u2032 ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\n= \u03b22 (1\u2212 2 )2 h+h\u2212\n\u2211\n{\u03b8,\u03b8\u2032}\u2208U(+,+)\n\u2212h\u03b8h\u03b8\u2032 ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\n= \u03b22 (1\u2212 2 )2 h+h\u2212\n\u2211\n{\u03b8,\u03b8\u2032}\u2208U(+,+)\n( \u22122h\u03b8h\u03b8\u2032 log 1\nh\u03b8 + h\u03b8h\u03b8\u2032\n)\n= \u03b22 (1\u2212 2 )2 h+h\u2212\n \u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\n\u2211\n\u03b8\u2208\u0398+i\n\u22122h\u03b8 log 1 h\u03b8 + \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i)\n \n= (1\u2212 2 )2 h+h\u2212\n \u22122\u03b22 \u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\n\u2211\n\u03b8\u2208\u0398+i\nh\u03b8 log 1\nh\u03b8 + \u03b22\n\u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i)\n \n2. Both \u03b8 and \u03b8\u2032 maps x to \u2212. Similarly, we get\n\u2211\n{\u03b8,\u03b8\u2032}\u2208U(\u2212,\u2212)\n\u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032) ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\n= (1\u2212 2 )2 h+h\u2212\n \u22122\u03b12 \u2211\nyi\u2208Y (\u03b2 \u2212 \u03b2i)\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 log 1\nh\u03b8 + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\n \n3. \u03b8 maps x to +, \u03b8\u2032 maps x to \u2212. We have\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,\u2212)\n\u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032) ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\n= (1\u2212 2 )2 h+h\u2212\n \u03b1\u03b2 \u2211\nyi\u2208Y (\u03b2 \u2212 \u03b2i)\n\u2211\n\u03b8\u2208\u0398+i\nh\u03b8 log 1\nh\u03b8 + \u03b1\u03b2\n\u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 log 1\nh\u03b8 \u2212 \u03b1\u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n4. \u03b8 maps x to \u2212, \u03b8\u2032 maps x to +. By symmetry we have\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(\u2212,+)\n\u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032) ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\n= \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,\u2212)\n\u2212h+h\u2212(p\u03b8 \u2212 q\u03b8)(p\u03b8\u2032 \u2212 q\u03b8\u2032) ( log 1 h\u03b8h\u03b8\u2032 \u2212 1 )\nCombining the above four equations, we obtain a lower bound on Part 1:\nPart 1 \u2265 (1\u2212 2 ) 2\nh+h\u2212\n \u22122\u03b22 \u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\n\u2211\n\u03b8\u2208\u0398+i\nh\u03b8 log 1\nh\u03b8 + \u03b22\n\u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i)\n\u22122\u03b12 \u2211\nyi\u2208Y (\u03b2 \u2212 \u03b2i)\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 log 1\nh\u03b8 + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\n+2\u03b1\u03b2 \u2211\nyi\u2208Y (\u03b2 \u2212 \u03b2i)\n\u2211\n\u03b8\u2208\u0398+i\nh\u03b8 log 1\nh\u03b8 + 2\u03b1\u03b2\n\u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 log 1\nh\u03b8 \u2212 2\u03b1\u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n= (1\u2212 2 )2 h+h\u2212\n   2\u03b1\u03b2 \u2211\nyi\u2208Y (\u03b2 \u2212 \u03b2i)\u2212 2\u03b22\n\u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\n  \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 log 1\nh\u03b8\n+  2\u03b1\u03b2 \u2211\nyi\u2208Y (\u03b1\u2212 \u03b1i)\u2212 2\u03b12\n\u2211\nyi\u2208Y (\u03b2 \u2212 \u03b2i)\n  \u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 log 1\nh\u03b8\n+\u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\u2212 2\u03b1\u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n= (1\u2212 2 )2 h+h\u2212\n\u00b7  2 \u2211\nyi\u2208Y \u03b2(\u03b2\u03b1i \u2212 \u03b1\u03b2i)\n\u2211\n\u03b8\u2208\u0398+i\nh\u03b8 log 1\nh\u03b8 + 2\n\u2211\nyi\u2208Y \u03b1(\u03b1\u03b2i \u2212 \u03b2\u03b1i)\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 log 1 h\u03b8 \u2212 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n \n= (1\u2212 2 )2 h+h\u2212\n\u00b7  2 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)\n \u03b2\u03b1i \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 \u03b1i log 1 h\u03b8 \u2212 \u03b1\u03b2i\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log 1 h\u03b8\n \u2212 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n \n(B.7)\nPart 2. Next, we will provide a lower bound on Part 2 of Equation (B.6).\nBy definition, we have\nPart 2 = \u2211\n\u03b8 \u03b8\u2032 (h+p\u03b8p\u03b8\u2032 + h\u2212q\u03b8q\u03b8\u2032) log\n1\nh+p\u03b8p\u03b8\u2032 + h\u2212q\u03b8q\u03b8\u2032\n\u2212 ( h+ \u2211\n\u03b8 \u03b8\u2032 p\u03b8p\u03b8\u2032 log\n1\np\u03b8p\u03b8\u2032 + h\u2212\n\u2211 \u03b8 \u03b8\u2032 q\u03b8q\u03b8\u2032 log 1 q\u03b8q\u03b8\u2032\n)\n(a) \u2265 h+h\u2212\n2\n\u2211\n\u03b8 \u03b8\u2032\n(p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032\nHereby, step (a) is due to the strong concavity3 of f(x) = x log 1x . Similarly with the analysis of Part 1, we consider the four sets of {\u03b8, \u03b8\u2032} pairs:\n1. {\u03b8, \u03b8\u2032} \u2208 U(+,+): both \u03b8 and \u03b8\u2032 maps x to +.\n3If f is strongly concave, then for t \u2208 [0, 1], it holds that f(tx + (1 \u2212 t)y) \u2212 tf(x) \u2212 (1 \u2212 t)f(y) \u2265 t(1\u2212t)\n2 m(x\u2212 y)2, where m = min (|f \u2032\u2032(x)|, |f \u2032\u2032(y)|) .\nIn this case, we have\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032 \u2265 \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2 ( \u221a p\u03b8p\u03b8\u2032 \u2212 \u221a q\u03b8q\u03b8\u2032) 2\nEq (B.4) =\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2\n(\u221a h\u03b8 \u0304\nh+\nh\u03b8\u2032 \u0304 h+ \u2212 \u221a h\u03b8 h\u2212 h\u03b8\u2032 h\u2212 )2\n= \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2 h\u03b8h\u03b8\u2032\n( \u0304\nh+ \u2212 h\u2212\n)2\n= \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2\nh\u03b8h\u03b8\u2032 \u03b22 (1\u2212 2 )2\n(h+h\u2212)2\n= (1\u2212 2 )2 2h+h\u2212 \u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i)\n2. (\u03b8, \u03b8\u2032) \u2208 U(\u2212,\u2212). Similarly, we get\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(\u2212,\u2212)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032 \u2265 (1\u2212 2 ) 2 2h+h\u2212 \u03b12 \u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\n3. (\u03b8, \u03b8\u2032) \u2208 U(+,\u2212): \u03b8 maps x to +, \u03b8\u2032 maps x to \u2212. We have\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,\u2212)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032 \u2265 \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2\n(\u221a h\u03b8 \u0304\nh+\nh\u03b8\u2032 h+ \u2212 \u221a h\u03b8 h\u2212 h\u03b8\u2032 \u0304 h\u2212 )2\n= \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2 h\u03b8h\u03b8\u2032 \u0304\n( 1\nh+ \u2212 1 h\u2212\n)2\n= (1\u2212 2 )2 2h+h\u2212 \u0304(\u03b1\u2212 \u03b2)2 \u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n4. (\u03b8, \u03b8\u2032) \u2208 U(\u2212,+): \u03b8 maps x to \u2212, \u03b8\u2032 maps x to +. By symmetry we have\n\u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,\u2212)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032 \u2265 (1\u2212 2 ) 2 2h+h\u2212 \u0304(\u03b1\u2212 \u03b2)2\n\u2211\nyi\u2208Y \u03b2i(\u03b1\u2212 \u03b1i)\nCombining the above four equations, we obtain a lower bound on Part 2:\nPart 2 \u2265 \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,+)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032 + \u2211\n(\u03b8,\u03b8\u2032)\u2208U(\u2212,\u2212)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032\n+ \u2211\n(\u03b8,\u03b8\u2032)\u2208U(+,\u2212)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032 + \u2211\n(\u03b8,\u03b8\u2032)\u2208U(\u2212,+)\nh+h\u2212 2 (p\u03b8p\u03b8\u2032 \u2212 q\u03b8q\u03b8\u2032)2 p\u03b8p\u03b8\u2032 + q\u03b8q\u03b8\u2032\n= (1\u2212 2 )2 2h+h\u2212\n \u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2 \u0304(\u03b1\u2212 \u03b2)2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n(B.8)"}, {"heading": "B.3.3 A Lower Bound on Term 2", "text": "Now we move on to analyze Term 2 of Equation (B.6). By strong concavity of f(x) = x log 1x + (1\u2212 x) log 11\u2212x , we obtain\nTerm 2 = c \u2211\nyi\u2208Y\n( hi log 1\nhi + (1\u2212 hi) log\n1\n1\u2212 hi \u2212 h+\n( pi log 1\npi + (1\u2212 pi) log\n1\n1\u2212 pi\n)\n\u2212 h\u2212 ( qi log 1\nqi + (1\u2212 qi) log\n1\n1\u2212 qi\n))\nfootnote 3 \u2265 c \u00b7 h+h\u2212\n2\n\u2211\nyi\u2208Y\n(pi \u2212 qi)2 max{pi(1\u2212 pi), qi(1\u2212 qi)}\nPlugging in the definition of pi, qi from Equation (B.4), we get\nTerm 2 = c \u00b7 h+h\u2212\n2\n\u2211\nyi\u2208Y\n( \u03b1i\u0304+ \u03b2i\nh+ \u2212 \u03b1i + \u03b2i\u0304 h\u2212\n)2 1\nmax{pi(1\u2212 pi), qi(1\u2212 qi)}\n= c\n2h+h\u2212\n\u2211\nyi\u2208Y\n((\u03b1 + \u03b2\u0304)(\u03b1i\u0304+ \u03b2i )\u2212 (\u03b1\u0304+ \u03b2 )(\u03b1i + \u03b2i\u0304))2 max{pi(1\u2212 pi), qi(1\u2212 qi)}\n= c\n2h+h\u2212\n\u2211\nyi\u2208Y\n( \u03b1\u03b2i 2 + \u03b2\u03b1i\u0304 2 \u2212 \u03b1\u03b2i\u03042 \u2212 \u03b2\u03b1i 2 )2\nmax{pi(1\u2212 pi), qi(1\u2212 qi)}\n= c(1\u2212 2 )2\n2h+h\u2212\n\u2211\nyi\u2208Y\n(\u03b2\u03b1i \u2212 \u03b1\u03b2i)2 max{pi(1\u2212 pi), qi(1\u2212 qi)}\n(B.9)"}, {"heading": "B.3.4 A Combined Lower Bound for \u2206AUX", "text": "Now, combining Equation (B.7), (B.8), and (B.9), we can get a lower bound for \u2206AUX:\n\u2206AUX \u2265 (1\u2212 2 )2 h+h\u2212 \u00b7\n 2 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)\n \u03b2\u03b1i \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 \u03b1i log 1 h\u03b8 \u2212 \u03b1\u03b2i\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log 1 h\u03b8\n \n\u2212 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n \n+ (1\u2212 2 )2 2h+h\u2212\n \u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2 \u0304(\u03b1\u2212 \u03b2)2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n+ c(1\u2212 2 )2\n2h+h\u2212\n\u2211\nyi\u2208Y\n(\u03b2\u03b1i \u2212 \u03b1\u03b2i)2 max{pi(1\u2212 pi), qi(1\u2212 qi)}\n(B.10)\nWe can rewrite Equation (B.10) as\n\u2206AUX \u2265\n(1\u2212 2 )2 4h+h\u2212\n \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2 + \u03b22\n\u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2 \u0304(\u03b1\u2212 \u03b2)2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n\ufe38 \ufe37\ufe37 \ufe38 LB1\n+ (1\u2212 2 )2 4h+h\u2212\n \u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2 \u0304(\u03b1\u2212 \u03b2)2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n+ 2c \u2211\nyi\u2208Y\n(\u03b2\u03b1i \u2212 \u03b1\u03b2i)2 max{pi(1\u2212 pi), qi(1\u2212 qi)} \u2212 5 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n+ 8 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)\n \u03b2\u03b1i \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 \u03b1i log 1 h\u03b8 \u2212 \u03b1\u03b2i\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log 1 h\u03b8\n   \n\ufe38 \ufe37\ufe37 \ufe38 LB2\n(B.11)"}, {"heading": "B.3.5 Connecting \u2206AUX with \u2206EC2", "text": "Next, we will show that term LB1 is lower-bounded by a factor of \u2206EC2 (i.e., 1-step: EC2, ), while LB2 cannot be too much less than 0. Concretely, we will show\n\u2022 LB1 \u2265 116 (1\u2212 2 ) 2 \u2206EC2 , and\n\u2022 LB2 \u2265 \u22122t (1\u2212 2 )2 \u03b7, for \u03b7 \u2208 (0, 1).\nAt the end of this subsection, we will combine the above results to connect 1-step: AUX(Xe`+1 | `) with 1-step: EC2, (See Equation (B.18)).\nLB1 VS. \u2206EC2 . We expand the EC2 gain 1-step: EC2, as\n\u2206EC2 = \u2211\nyi\u2208Y (\u03b1i + \u03b2i)(1\u2212 \u03b1i \u2212 \u03b2i)\u2212 \u03b1\n\u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i)\u2212 \u03b2\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\n= \u03b2 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b1\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) (B.12)\nDefine\n   * , 16h+h\u2212(1\u22122 )2 \u00b7 LB1 = 4 (\u2211 yi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i) 2 + \u03b22 \u2211 yi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12 \u2211 yi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) +2 \u0304(\u03b1\u2212 \u03b2)2\u2211yi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) ) # , h+h\u2212\u2206EC2 = ( \u0304(\u03b1\u2212 \u03b2)2 + \u03b1\u03b2 ) ( \u03b2 \u2211 yi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b1 \u2211 yi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2 \u2211 yi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) )\nTo bound LB1 against 116 (1\u2212 2 ) 2 \u2206EC2 , it suffices to show * \u2265 # . To prove the above inequality, we consider the following two cases:\n1. \u0304(\u03b1\u2212 \u03b2)2 \u2264 \u03b1\u03b2. In this case, we have \u0304(\u03b1\u2212 \u03b2)2 + \u03b1\u03b2 \u2264 2\u03b1\u03b2. Then,\n* \u2212 # 2 \u2265 * 2 \u2212 \u03b1\u03b2\n \u03b2 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b1\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n\u2265 \u03b22(1 + \u03b2) \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12(1 + \u03b1)\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\u2212 2\u03b1\u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n+ \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n\u2265 \u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\u2212 2\u03b1\u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) +\n\u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n= 0\n2. \u0304(\u03b1\u2212 \u03b2)2 > \u03b1\u03b2. W.l.o.g., we assume \u03b2 \u2264 \u03b1 \u2264 1. By \u03b1+ \u03b2 = 1 we get 2\u03b1 \u2265 1. Observe the fact that \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2 = \u2212\u03b22\n\u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i)\u2212 \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2\u03b1\u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) \u2265 0\nRearranging the terms in the above inequality, we get\n\u03b2 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) \u2264 2\u03b1\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) \u2264 2(\u03b1\u03b2 \u2212\n\u2211\nyi\u2208Y \u03b1i\u03b2i) = 2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) (B.13)\nHence,\n# \u2264 2 \u0304(\u03b1\u2212 \u03b2)2  \u03b2 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b1\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n(B.13) \u2264 2 \u0304(\u03b1\u2212 \u03b2)2  \u03b1 \u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 4\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n2\u03b1\u22651 \u2264 2 \u0304(\u03b1\u2212 \u03b2)2  2\u03b12 \u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + 4\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n\u0304(\u03b1\u2212\u03b2)2\u22641 \u2264 4  2 \u0304(\u03b1\u2212 \u03b2)2 \u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\n \n\u2264 * Therefore, we get\nLB1 \u2265 1 16 (1\u2212 2 )2 \u2206EC2 (B.14)\nA lower bound on LB2. In the following, we will analyze LB2.\nLB2 \u2265 (1\u2212 2 ) 2\n4h+h\u2212\n \u03b22 \u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + \u03b12\n\u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i)\u2212 5\n\u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)2\n+ 2c2 \u2211\nyi\u2208Y\n(\u03b2\u03b1i \u2212 \u03b1\u03b2i)2 max{pi(1\u2212 pi), qi(1\u2212 qi)}\n+ 8 \u2211\nyi\u2208Y (\u03b2\u03b1i \u2212 \u03b1\u03b2i)\n \u03b2\u03b1i \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 \u03b1i log \u03b1i h\u03b8 + \u03b2\u03b1i log 1 \u03b1i \u2212 \u03b1\u03b2i\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log \u03b2i h\u03b8 \u2212 \u03b1\u03b2i log 1 \u03b2i\n \nFor brevity, define \u00b5i , \u03b1i/\u03b1, and \u03bdi , \u03b2i/\u03b2. We can simplify the above equation as\nLB2 \u2265 \u03b1 2\u03b22 (1\u2212 2 )2\n4h+h\u2212\n\u2211\nyi\u2208Y\n( \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi)\u2212 5(\u00b5i \u2212 \u03bdi)2 +\n2c2 (\u00b5i \u2212 \u03bdi)2 max{pi(1\u2212 pi), qi(1\u2212 qi)}\n+ 8(\u00b5i \u2212 \u03bdi)  \u00b5i \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 \u03b1i log \u03b1i h\u03b8 + \u00b5i log 1 \u00b5i\u03b1 \u2212 \u03bdi\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log \u03b2i h\u03b8 \u2212 \u03bdi log 1 \u03bdi\u03b2\n   \n(B.15)\nDenote the summand on the RHS of the above equation as LB2i. If for any yi \u2208 Y we can lower bound LB2i, we can then bound the whole sum. Fix i. W.l.o.g., we assume \u00b5i \u2265 \u03bdi. Then\nLB2i , \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi)\u2212 5(\u00b5i \u2212 \u03bdi)2 + 2c (\u00b5i \u2212 \u03bdi)2\nmax{pi(1\u2212 pi), qi(1\u2212 qi)}\n+ 8(\u00b5i \u2212 \u03bdi)  \n:\u2265 0\n\u00b5i \u2211\n\u03b8\u2208\u0398+i\nh\u03b8 \u03b1i log \u03b1i h\u03b8 + \u00b5i log 1 \u00b5i\u03b1 \u2212 \u03bdi\n\u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log \u03b2i h\u03b8 \u2212 \u03bdi log 1 \u03bdi\u03b2\n \n\u2265 \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi)\u2212 5(\u00b5i \u2212 \u03bdi)2 + 2c (\u00b5i \u2212 \u03bdi)2\nmax{pi(1\u2212 pi), qi(1\u2212 qi)}\n\u2212 8(\u00b5i \u2212 \u03bdi)  \u03bdi\n* \u2264 log n \u2211\n\u03b8\u2208\u0398\u2212i\nh\u03b8 \u03b2i log \u03b2i h\u03b8 + \u03bdi log 1 \u03bdi + \u03bdi log 1 \u03b2\n \n\u2265 \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi)\u2212 5(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n)\n+ 2c (\u00b5i \u2212 \u03bdi)2\nmax{pi(1\u2212 pi), qi(1\u2212 qi)}\nIn order to put a lower bound on the above terms, we first need to lower bound the term involving (\u00b5i\u2212\u03bdi)2\nmax{pi(1\u2212pi),qi(1\u2212qi)} . Notice that pi = \u03b1i+\u03b2i /\u0304 \u03b1+\u03b2 /\u0304 , and pi = \u03b1i /\u0304+\u03b2i \u03b1 /\u0304+\u03b2 . Therefore, min {\u00b5i, \u03bdi} \u2264 pi, qi \u2264 max {\u00b5i, \u03bdi}. We check three different cases:\n\u2022 \u00b5i \u2265 \u03bdi \u2265 1/2, or \u03bdi \u2264 \u00b5i \u2264 1/2. In this case, max{pi(1\u2212 pi), qi(1\u2212 qi)} \u2264 max{\u00b5i(1\u2212 \u00b5i), \u03bdi(1\u2212 \u03bdi)}. Therefore,\nLB2i \u2265 \u22125(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n)\n+ 2c (\u00b5i \u2212 \u03bdi)2\nmax{\u00b5i(1\u2212 \u00b5i), \u03bdi(1\u2212 \u03bdi)} + \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi)\n\u2265 \u22125(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n)\n+ 2c (\u00b5i \u2212 \u03bdi)2\nmax{\u00b5i(1\u2212 \u00b5i), \u03bdi(1\u2212 \u03bdi)} + max{\u00b5i(1\u2212 \u00b5i), \u03bdi(1\u2212 \u03bdi)}\n\u2265 \u22125(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n) + 2 \u221a 2c(\u00b5i \u2212 \u03bdi)\n\u00b5i\u2212\u03bdi\u22641/2 \u2265 (\u00b5i \u2212 \u03bdi) ( 2 \u221a 2c\u2212 5/2\u2212 8 ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n))\n(a) \u2265 (\u00b5i \u2212 \u03bdi) ( 2 \u221a\n2c\u2212 5/2\u2212 8 log n \u03b2\n)\nHere, step (a) is due to the fact that f(x) = x log n\u03b2x is monotone increasing for n \u2265 3. When n < 3, we have \u00b5i = 1 and \u03bdi = 0 (otherwise, there is no uncertainty left in Y ) and hence the problem becomes trivial. \u2022 1/n \u2264 \u03bdi \u2264 1/2 \u2264 \u00b5i.\nIn this case, we cannot replace pi, qi with \u00b5i or \u03bdi. However, notice that max{\u00b5i(1\u2212 \u00b5i), \u03bdi(1\u2212 \u03bdi)} \u2264 1/4, we have LB2i \u2265 \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi)\u2212 5(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n) + 8c (\u00b5i \u2212 \u03bdi)2\n= \u00b5i(1\u2212 \u00b5i) + \u03bdi(1\u2212 \u03bdi) + (\u00b5i \u2212 \u03bdi)2 + (8c\u2212 6)(\u00b5i \u2212 \u03bdi)2\n\u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n)\n= \u00b5i(1\u2212 \u03bdi) + \u03bdi(1\u2212 \u00b5i) + (8c\u2212 6)(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n)\n\u2265 \u00b5i(1\u2212 \u03bdi) + (8c\u2212 6)(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n) (B.16)\n\u03bdi\u22651/n \u2265 \u00b5i(1\u2212 \u03bdi) + (8c\u2212 6)(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi)\u03bdi log n2\n\u03b2\nTo further simplify notation, we denote \u03b31 , 8c\u2212 6, and \u03b32 , 8 log n 2\n\u03b2 . Then the above equation can be rewritten as\nLB2i \u2265 \u00b5i(1\u2212 \u03bdi) + \u03b31(\u00b5i \u2212 \u03bdi)2 \u2212 \u03b32(\u00b5i \u2212 \u03bdi)\u03bdi\nIf \u00b5i \u2212 \u03bdi \u2264 12\u03b32 , then\nLB2i \u2265 \u00b5i(1\u2212 \u03bdi) + \u03b31(\u00b5i \u2212 \u03bdi)2 \u2212 1\n2\u03b32 \u03b32\u03bdi = \u00b5i(1\u2212 \u03bdi)\u2212 \u03bdi 2 \u2265 0\nOtherwise, if \u00b5i \u2212 \u03bdi > 12\u03b32 , we have\nLB2i \u2265 \u00b5i(1\u2212 \u03bdi) + (\u00b5i \u2212 \u03bdi) (\u03b31(\u00b5i \u2212 \u03bdi)\u2212 \u03b32\u03bdi)\n> \u00b5i(1\u2212 \u03bdi) + (\u00b5i \u2212 \u03bdi) ( \u03b31 1\n2\u03b32 \u2212 \u03b32\u03bdi\n)\n> \u00b5i \u2212 \u03bdi\n2\n( \u03b31 \u03b32 \u2212 \u03b32 )\n\u2022 \u03bdi \u2264 1/n < 1/2 \u2264 \u00b5i. In this case, we have\nLB2i Eq (B.16) \u2265 \u00b5i(1\u2212 \u03bdi) + \u03b31(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( \u03bdi log n\n\u03b2 + \u03bdi log\n1\n\u03bdi\n)\n\u2265 \u00b5i(1\u2212 \u03bdi) + \u03b31(\u00b5i \u2212 \u03bdi)2 \u2212 8(\u00b5i \u2212 \u03bdi) ( 1\nn log\nn \u03b2 + log n n\n)\n= \u00b5i(1\u2212 \u03bdi) + \u03b31(\u00b5i \u2212 \u03bdi)2 \u2212 \u03b32 n (\u00b5i \u2212 \u03bdi) > \u00b5i(1\u2212 \u03bdi) + (\u00b5i \u2212 \u03bdi) ( \u03b31 n\u2212 2\n2n \u2212 \u03b32 n\n)\n(a) \u2265 \u00b5i \u2212 \u03bdi\n3\n(\u03b31 2 \u2212 \u03b32 )\n\u2265 \u00b5i \u2212 \u03bdi 3 ( \u03b31 \u03b32 \u2212 \u03b32 )\nStep (a) is due to the fact that 1/n < 1/2 and therefore n \u2265 3.\nPutting the above cases together, we obtain the following equations:\nLB2i \u2265    (\u00b5i \u2212 \u03bdi) ( 2 \u221a 2c\u2212 5/2\u2212 8 log n\u03b2 ) if \u00b5i \u2265 \u03bdi \u2265 1/2, or \u03bdi \u2264 \u00b5i \u2264 1/2 0 if 1/n \u2264 \u03bdi \u2264 1/2 \u2264 \u00b5i, and \u00b5i \u2212 \u03bdi \u2264 12\u03b32 \u00b5i\u2212\u03bdi 2 ( \u03b31 \u03b32 \u2212 \u03b32 ) if 1/n \u2264 \u03bdi \u2264 1/2 \u2264 \u00b5i, and \u00b5i \u2212 \u03bdi > 12\u03b32\n\u00b5i\u2212\u03bdi 3 ( \u03b31 \u03b32 \u2212 \u03b32 ) if \u03bdi \u2264 1/n < 1/2 \u2264 \u00b5i\nFix \u03b7 \u2265 0. Let c = 8 ( log 2n 2\n\u03b7\n)2 , we have \u03b31 > ( 8 log n 2\n\u03b7\n)2 , and \u03b32 = 8 log n 2\n\u03b2 , so\n\u03b31 \u03b32 \u2212 \u03b32 =\n( \u221a \u03b31 \u2212 \u03b32)(\u221a\u03b31 + \u03b32)\n\u03b32 > 8\n\u221a \u03b31 + \u03b32\n\u03b32 log\n\u03b2\n\u03b7\nand thus we get\nLB2i \u2265    8(\u00b5i \u2212 \u03bdi) log \u03b2\u03b7 if \u00b5i \u2265 \u03bdi \u2265 1/2, or \u03bdi \u2264 \u00b5i \u2264 1/2 0 if 1/n \u2264 \u03bdi \u2264 1/2 \u2264 \u00b5i, and \u00b5i \u2212 \u03bdi \u2264 12\u03b32 4(\u00b5i\u2212\u03bdi)(\u221a\u03b31+\u03b32)\n\u03b32 log \u03b2\u03b7 if \u03bdi \u2264 1/2 \u2264 \u00b5i, and \u00b5i \u2212 \u03bdi > 12\u03b32\nThat is, if \u03b2 \u2265 \u03b7, we have LB2i \u2265 0 for all i \u2208 {1, . . . , t}.\nOn the other hand, if \u03b2 < \u03b7, we get 4( \u221a \u03b31+\u03b32)\n\u03b32 =\n4(log n 2 \u03b7 +log n2 \u03b2 )\nlog n 2\n\u03b2 \u2264 8, and therefore LB2i \u2265 8(\u00b5i \u2212 \u03bdi) log \u03b2\u03b7 . Summing over all i \u2208 {1, . . . , t}, we get that for \u03b2 < \u03b7, it holds LB2 \u2265 \u2211yi\u2208Y |\u00b5i \u2212 \u03bdi| \u00b7 2\u03b12\u03b22(1\u22122 )2\nh+h\u2212 log \u03b2\u03b7 . We hence get\nLB2 \u2265 { \u22122t (1\u2212 2 )2 \u03b1\u03b2 log \u03b7\u03b1\u03b2 if \u03b1\u03b2 < \u03b7 0 if \u03b1\u03b2 \u2265 \u03b7\nFurther relaxing the above condition by \u03b1\u03b2 log \u03b7\u03b1\u03b2 \u2264 \u03b7 \u2212 \u03b1\u03b2 \u2264 \u03b7, we obtain:\nLB2 \u2265 \u22122t (1\u2212 2 )2 \u03b7 (B.17)\nCombining Equation (B.11), (B.14), and (B.17), we get\n\u2206AUX \u2265 1\n16 (1\u2212 2 )2 \u2206EC2 \u2212 2t (1\u2212 2 )2 \u03b7. (B.18)\nHence, we have related 1-step: AUX(Xe`+1 | `) to 1-step: EC2, , as stated in Lemma 3."}, {"heading": "B.3.6 Bounding \u2206AUX against \u2206ECED", "text": "To finish the proof for Lemma 3, it remains to bound \u2206AUX against \u2206ECED. In this subsection, we complete the proof of Lemma 3, by showing that \u2206AUX(Xe | \u03c8) + 2t (1\u2212 2 )2 \u03b7 \u2265 \u2206ECED,\u03c8(Xe) /64. Recall that is the noise rate of test e. Let \u03c1 = 1\u2212 be the discount factor for inconsistent root-causes. By the definition of \u2206ECED in Equation (3.1), we first expand the expected offset value of performing test e:\nExe [\u03b4OFFSET(xe)] = \u2211\nyi\u2208Y (\u03b1i + \u03b2i)(1\u2212 \u03b1i \u2212 \u03b2i)\n( 1\u2212 \u03c12 ) .\nDenote \u03b3 = ( 1\u2212 \u03c12 ) . Then, we can expand \u2206ECED as\n\u2206ECED\n= \u2211\nyi\u2208Y\n  (initial total edge weight)\u2212(offset value)\ufe37 \ufe38\ufe38 \ufe37 (\u03b1i + \u03b2i)(1\u2212 \u03b1i \u2212 \u03b2i) (1\u2212 \u03b3)\n\u2212 expected remaining weight after discounting\ufe37 \ufe38\ufe38 \ufe37 (h+(\u03b1i + \u03c1\u03b2i)(\u03b1+ \u03c1\u03b2 \u2212 \u03b1i \u2212 \u03c1\u03b2i) + h\u2212(\u03b2i + \u03c1\u03b1i)(\u03b2 + \u03c1\u03b1\u2212 \u03b2i \u2212 \u03c1\u03b1i))  \n= h+ \u2211\nyi\u2208Y\n( \u2212\u03b3\u03b1i(\u03b1\u2212 \u03b1i) + \u03b1i(\u03b2 \u2212 \u03b2i)(1\u2212 \u03b3 \u2212 \u03c1) + \u03b2i(\u03b1\u2212 \u03b1i)(1\u2212 \u03b3 \u2212 \u03c1) + \u03b2i(\u03b2 \u2212 \u03b2i)(1\u2212 \u03b3 \u2212 \u03c12) )\n+ h\u2212 \u2211\nyi\u2208Y\n( \u2212\u03b3\u03b2i(\u03b2 \u2212 \u03b2i) + \u03b2i(\u03b1\u2212 \u03b1i)(1\u2212 \u03b3 \u2212 \u03c1) + \u03b1i(\u03b2 \u2212 \u03b2i)(1\u2212 \u03b3 \u2212 \u03c1) + \u03b1i(\u03b1\u2212 \u03b1i)(1\u2212 \u03b3 \u2212 \u03c12) )\n= \u2211\nyi\u2208Y\n( 2(1\u2212 \u03b3 \u2212 \u03c1)\u03b1i(\u03b2 \u2212 \u03b2i) + ( h+(1\u2212 \u03b3 \u2212 \u03c12)\u2212 h\u2212\u03b3 ) \u03b2i(\u03b2 \u2212 \u03b2i)\n+ ( h\u2212(1\u2212 \u03b3 \u2212 \u03c12)\u2212 h+\u03b3 ) \u03b1i(\u03b1\u2212 \u03b1i) )\nSince \u03b3 = (1\u22122 )(1\u2212 )2 , 1\u2212 \u03b3 \u2212 \u03c12 = 1\u22122 1\u2212 , and 1\u2212 \u03b3 \u2212 \u03c1 = ( 1\u22122 1\u2212 )2 , we have,\nh+(1\u2212 \u03b3 \u2212 \u03c12)\u2212 h\u2212\u03b3 = (\u03b1(1\u2212 ) + \u03b2 ) 1\u2212 2 1\u2212 \u2212 (\u03b1 + \u03b2(1\u2212 )) (1\u2212 2 ) (1\u2212 )2 = ( 1\u2212 2 1\u2212 )2 \u03b1\nTherefore\n\u2206ECED = ( 1\u2212 2 1\u2212\n)2  \u03b1 \u2211\nyi\u2208Y \u03b2i(\u03b2 \u2212 \u03b2i) + \u03b2\n\u2211\nyi\u2208Y \u03b1i(\u03b1\u2212 \u03b1i) + 2\n\u2211\nyi\u2208Y \u03b1i(\u03b2 \u2212 \u03b2i)\n \n= ( 1\u2212 2 1\u2212 )2 \u2206EC2 (B.19)\nCombining Equation (B.19) with Equation (B.18) we obtain\n\u2206AUX + 2t (1\u2212 2 )2 \u03b7 \u2265 (1\u2212 )2\n16 \u2206ECED\n= 1\n16 (1\u2212 2 )2 \u2206EC2\nWith the results from Appendix \u00a7B.3.5 and \u00a7B.3.6, we therefore complete the proof of Lemma 3.\nB.4 Bounding the error probability: Noiseless vs. Noisy setting\nNow that we have seen how ECED interacts with our auxiliary function in terms of the one-step gain, it remains to understand how one can relate the one-step gain to the gain of an optimal policy OPT: AUX , over k steps. In this subsection, we make an important step towards this goal.\nSpecifically, we provide Lemma 7. Consider a policy \u03c0 of length k, and assume that we are using a stochastic estimator (SE). Let p>E be the error probability of SE before running policy \u03c0, p \u22a5 E,noisy be the average error probability of SE after running \u03c0 in the noisy setting, and p\u22a5E,noiseless be the average error probability of SE after running \u03c0 in the noiseless setting. Then\np\u22a5E,noiseless \u2264 p\u22a5E,noisy\nProof of Lemma 7. Recall that a stochastic estimator predicts the value of a random variable, by randomly drawing from its distribution. Let \u03c0 be a policy. We denote by pE(\u03c0\u03c6) the expected error\nprobability of an stochastic estimator after observing \u03c0\u03c6 :\np\u22a5E,noisy = E\u03c6[pE(\u03c0\u03c6)] = \u2211\n\u03c6\np(\u03c0\u03c6) \u2211\ny\u2208Y p(y | \u03c0\u03c6)(1\u2212 p(y | \u03c0\u03c6))\nwhere \u03c6 \u2208 V \u00d7O denotes a set of test-outcome pairs, and \u03c0\u03c6 denotes a path taken by \u03c0, given that it observes \u03c6.\nNow, let us see what happens in the noiseless setting: we run \u03c0 exactly as it is, but in the end compute the error probability of the noiseless setting (i.e., as if we know which test outcomes are corrupted by noise). Denote the noise put on the tests by \u039e, and the realized noise by \u03be. We can imagine the noiseless setting through the following equivalent way: we ran the same policy \u03c0 exactly as in the noisy setting. But upon completion of \u03c0 we reveal what \u039e was. We thus have\np(y | \u03c0\u03c6) = \u2211\n\u039e=\u03be\np(y | \u03c0\u03c6, \u03be)p(\u03be | \u03c0)\nThe error probability upon observing \u03c0\u03c6 and \u039e = \u03be is\npE(\u03c0\u03c6, \u03be) = \u2211\ny\u2208Y p(y | \u03c0\u03c6, \u03be)(1\u2212 p(y | \u03c0\u03c6, \u03be)).\nThe expected error probability in the noiseless setting after running \u03c0 is\np\u22a5E,noiseless = E\u03c6,n[pE(\u03c0\u03c6, \u03be)] = \u2211\n\u03c6,n\np(\u03c0\u03c6, \u03be) \u2211\ny\u2208Y p(y | \u03c0\u03c6, \u03be)(1\u2212 p(y | \u03c0\u03c6, \u03be)) (B.20)\nNow, we can relate p\u22a5E,noisy to p \u22a5 E,noiseless.\np\u22a5E,noisy = \u2211\n\u03c6\np(\u03c0\u03c6) \u2211\ny\u2208Y p(y | \u03c0\u03c6)(1\u2212 p(y | \u03c0\u03c6))\n= \u2211\n\u03c6\np(\u03c0\u03c6) \u2211\ny\u2208Y\n\u2211\n\u03be\np(\u03be | \u03c0\u03c6)p(y | \u03c0\u03c6, \u03be)(1\u2212 \u2211\nn\np(\u03be | \u03c0\u03c6)p(y | \u03c0\u03c6, \u03be))\n(a) \u2265 \u2211\n\u03c6\np(\u03c0\u03c6) \u2211\ny\u2208Y\n\u2211\n\u03be\np(\u03be | \u03c0\u03c6)p(y | \u03c0\u03c6, \u03be)(1\u2212 p(y | \u03c0\u03c6, \u03be))\n= \u2211\n\u03c6,\u03be\np(\u03c0\u03c6, \u03be) \u2211\ny\u2208Y p(y | \u03c0\u03c6, \u03be)(1\u2212 p(y | \u03c0\u03c6, \u03be))\nwhere (a) is by Jensen\u2019s inequality and the fact that f(x) = x(1\u2212 x) is concave. Combining with Equation (B.20) we complete the proof.\nEssentially, Lemma 7 implies that, in terms of the reduction in the expected prediction error of SE, running a policy in the noise-free setting has higher gain than running the exact same policy in the noisy setting. This result is important to us, since analyzing a policy in the noise-free setting is often easier. We are going to use Lemma 7 in the next section, to relate the gain of an optimal policy OPT: EC2, in the EC2 objective (which assumes tests to be noise-free), with the gain OPT: AUX in the auxiliary function (which considers noisy test outcomes).\nB.5 The Key Lemma: One-step Gain of ECED VS. k-step Gain of OPT\nNow we are ready to state our key lemma, which connects 1-step: AUX(Xe`+1 | `) to OPT: AUX . Lemma 8 (Key Lemma). Fix \u03b7, \u03c4 \u2208 (0, 1). Let n = | supp(\u0398)| be the number of root-causes, t = |Y| be the number of target values, OPT(\u03b4opt) be the optimal policy that achieves pERR(OPT(\u03b4opt)) \u2264 \u03b4opt, and \u03c8` be the partial realization observed by running ECED with cost `. We denote by f avgAUX(`) := E\u03c8` [fAUX(\u03c8`)] the expected value of fAUX(\u03c8`) over all the paths \u03c8` at cost `. Assume that f avgAUX(`) \u2264 \u03b4g. We then have\nf avgAUX(`)\u2212 f avgAUX(`+ 1) \u2265 f avgAUX(`)\u2212 \u03b4opt\nk \u00b7 c c\u03b4 + c\u03b7, .\nwhere k = cost(OPT(\u03b4opt))), c\u03b7, , 2t(1\u2212 2 )2\u03b7, c\u03b4 , (6c+ 8) log(n/\u03b4g), c , 8 ( log(2n2/\u03b7) )2 , and c , (1\u2212 2 )2/16.\nProof of Lemma 8. Let \u03c8` be a path ending up at level ` of the greedy algorithm. Recall that \u2206EC2(Xe | \u03c8`) denotes the gain in fEC2 if we perform test e and assuming it to be noiseless (i.e., we perform edge cutting as if the outcome of test e is noiseless), conditioning on partial observation \u03c8`. Further, recall that \u2206AUX(Xe | \u03c8`) denotes the gain in fAUX if we perform noisy test e after observing \u03c8` and perform Bayesian update on the root-causes.\nLet e = arg maxe\u2032 \u2206ECED(Xe\u2032 | \u03c8`) be the test chosen by ECED, and e\u0302 = arg maxe\u2032 \u2206EC2(Xe\u2032 | \u03c8`) be the test that maximizes \u2206EC2 , then by Lemma 3 we know\n\u2206AUX(Xe | \u03c8`) + c\u03b7, \u2265 (1\u2212 )2\n16 (\u2206ECED,\u03c8`(Xe))\n\u2265 (1\u2212 ) 2\n16 (\u2206ECED,\u03c8`(Xe\u0302))\n= 1\n16 (1\u2212 2 )2 \u2206EC2,\u03c8(Xe\u0302) (B.21)\nNote that \u2206EC2,\u03c8`(Xe) is the EC2 gain of test e over the normalized edge weights at step `+ 1 in the noiseless setting. That is, upon observing \u03c8`, we create a new EC2 problem instance (by considering the posterior probability over root-causes at \u03c8`), and run (noiseless) greedy algorithm w.r.t. the EC2 objective on such problem instance. Recall that c , (1\u2212 2 )/16. By adaptive submodularity of fEC2 (in the noiseless setting, see Golovin et al. (2010)), we obtain\nmax e \u2206EC2,\u03c8(Xe)\nadaptive submodularity \u2265 f>EC2,\u03c8` \u2212 E[f \u22a5 EC2,\u03c8` ]\nk\nwhere by f>EC2,\u03c8` we mean the initial EC2 objective value given partial realization \u03c8`, and by E[f\u22a5EC2,\u03c8` ] we mean the expected gain in fEC2 when we run OPT (\u03b4opt). Note that OPT (\u03b4opt) has worst-case length k.\nNow, imagine that we run the policy OPT (\u03b4opt), and upon completion of the policy we can observe the noise. We consider the gain of such policy in fEC2 :\nf>EC2 \u2212 E[f\u22a5EC2 ] (a) = p>E \u2212 E[f\u22a5EC2 ] (b) \u2265 p>E \u2212 p\u22a5E,noiseless.\nThe reason for step (a) is that the error probability of the stochastic estimator upon observing \u03c8`, i.e., p>E , is equivalent to the total amount of edge weight at \u03c8`, i.e., f > EC2,\u03c8`\n. The reason for step (b) is that under the noiseless setting (i.e., assuming we have access to the noise), the EC2 objective is always a lower-bound on the error probability of the stochastic estimator (due to normalization). Thus, E[f\u22a5EC2 ] \u2264 p\u22a5E,noiseless. Hence we get\n\u2206AUX(Xe | \u03c8) + c\u03b7, \u2265 c p>E,\u03c8` \u2212 p\u22a5E,noiseless,\u03c8`\nk .\nHere p>E,\u03c8` denotes the error probability under P [Y | \u03c8`], and p\u22a5E,noisy,\u03c8` denotes the expected error probability of running OPT (\u03b4opt) after \u03c8` in the noise-free setting. By Lemma 7 we get\n\u2206AUX(Xe | \u03c8) + c\u03b7, \u2265 c p>E,\u03c8` \u2212 p\u22a5E,noisy,\u03c8`\nk ,\nwhere p\u22a5E,noisy,\u03c8` denotes the expected error probability of running OPT (\u03b4opt) after \u03c8` in the noisy setting. By (the lower bound in) Lemma 4, we know that p>E,\u03c8` = pE(\u03c8`) \u2265 pMAPERR (\u03c8`), and hence\n\u2206AUX(Xe | \u03c8) + c\u03b7, \u2265 c pMAPERR (\u03c8`)\u2212 \u03b4opt\nk ,\nTaking expectation with respect to \u03c8`, we get\nE\u03c8` [\u2206AUX(Xe | \u03c8) + c\u03b7, ] \u2265 c E\u03c8`\n[ pMAPERR (\u03c8`) ] \u2212 \u03b4opt\nk . (B.22)\nUsing (the upper bound in) Lemma 2, we obtain\nf avgAUX(`) = E\u03c8` [fAUX(\u03c8`)] \u2264 (3c+ 4) ( E\u03c8` [ H2 ( pMAPERR (\u03c8`) )] + E\u03c8` [ pMAPERR (\u03c8`) ] log n )\n(a) \u2264 (3c+ 4) ( H2 ( E\u03c8` [ pMAPERR (\u03c8`) ]) + E\u03c8` [ pMAPERR (\u03c8`) ] log n ) (B.23)\nwhere (a) is by Jensen\u2019s inequality.\nSuppose we run ECED, and achieve expected error probability \u03b4g, then clearly before ECED terminates we have E\u03c8` [ pMAPERR (\u03c8`) ] \u2265 \u03b4g. Assuming E\u03c8` [ pMAPERR (\u03c8`) ] \u2264 1/2, we have\nf avgAUX(`) \u2264 (3c+ 4)E\u03c8` [ pMAPERR (\u03c8`) ]( 2 log\n1\nE\u03c8` [pMAPERR (\u03c8`)] + log n\n)\n\u2264 (3c+ 4)E\u03c8` [ pMAPERR (\u03c8`) ]( 2 log 1\n\u03b4g + log n\n)\n\u2264 E\u03c8` [ pMAPERR (\u03c8`) ] \u00b7 (6c+ 8) log n\n\u03b4g (B.24)\nwhich gives us\nE\u03c8` [ pMAPERR (\u03c8`) ] \u2265 f avg AUX(`)\n(6c+ 8) log n\u03b4g\nc\u03b4,(6c+8) log n\u03b4g = f avgAUX(`)\nc\u03b4 . (B.25)\nCombining Equation (B.25) with Equation (B.22), we get\nf avgAUX(`)\u2212 f avgAUX(`+ 1) = E\u03c8` [\u2206AUX(e | \u03c8)]\n\u2265 c f avgAUX(`) c\u03b4 \u2212 \u03b4opt k \u2212 c\u03b7, = f avgAUX(`)\u2212 \u03b4optc\u03b4\nk \u00b7 c c\u03b4 \u2212 c\u03b7,\nwhich completes the proof.\nB.6 Proof of Theorem 1: Near-optimality of ECED\nWe are going to put together the pieces from previous subsection, to give a proof of our main theoretical result (Theorem 1).\nProof of Theorem 1. In the following, we use both OPT[k] and OPT(\u03b4opt) to represent the optimal policy that achieves prediction error \u03b4opt, with worst-cast cost (i.e., length) k. Define S(\u03c0, \u03c6) to be the (partial) realization seen by policy \u03c0 under realization \u03c6. With slight abuse of notation, we use f avgAUX ( OPT[k] ) := E\u03c6 [ fAUX(S(OPT[k], \u03c6)) ] to denote the expected value achieved by running OPT[k].\nAfter running OPT[k], we know by Lemma 2 that the expected value of fAUX is lower bounded by 2c \u00b7 \u03b4opt. That is, \u03b4opt \u00b7 c\u03b4 \u2264 f avgAUX ( OPT[k] ) \u00b7 c\u03b42c \u2264 f avg AUX ( OPT[k] ) \u00b7 4 log(n/\u03b4g), where the last inequality is due to c\u03b4 , (6c+ 8) log n\u03b4g < 8c log n \u03b4g . We then have\nf avgAUX(`)\u2212 f avgAUX(`+ 1) Lemma 8 \u2265 ( f avgAUX(`)\u2212 \u03b4opt \u00b7 c\u03b4 ) \u00b7 c\u03b5 kc\u03b4 \u2212 c\u03b7,\n\u2265 ( f avgAUX(`)\u2212 f avgAUX ( OPT[k] ) \u00b7 4 log n\n\u03b4g ) \u00b7 c\u03b5 kc\u03b4 \u2212 c\u03b7, (B.26)\nLet \u2206` , f avg AUX(`) \u2212 f avgAUX ( OPT[k] ) \u00b7 4 log n\u03b4g , so that Inequality (B.26) implies \u2206` \u2212 \u2206`+1 \u2265\n\u2206` \u00b7 c kc\u03b4 \u2212 c\u03b7, . From here we get \u2206`+1 \u2264 ( 1\u2212 c kc\u03b4 ) \u2206` + c\u03b7, , and hence\n\u2206k\u2032 \u2264 (\n1\u2212 c kc\u03b4\n)k\u2032 \u22060 + k\u2032\u2211\ni=0\n( 1\u2212 c\nkc\u03b4\n)i \u00b7 c\u03b7,\n(a) \u2264 exp ( \u2212k\u2032 c\nkc\u03b4\n) \u22060 +\n1\u2212 ( 1\u2212 c kc\u03b4 )k\u2032\nc kc\u03b4\n\u00b7 c\u03b7,\n(b) \u2264 exp ( \u2212k\u2032 c\nkc\u03b4\n) \u22060 +\nkc\u03b4 c \u00b7 c\u03b7,\nwhere step (a) is due to the fact that (1 \u2212 x)k\u2032 \u2264 exp(\u2212k\u2032x) for any x < 1, and step (b) is due to( 1\u2212 c kc\u03b4 )k\u2032 > 0. It follows that\nf avgAUX(k \u2032)\u2212 f avgAUX ( OPT[k] ) \u00b7 4 log n\n\u03b4g \u2264 exp\n( \u2212k\u2032 c\nkc\u03b4\n) \u22060 +\nkc\u03b4 c \u00b7 c\u03b7,\n\u2264 exp ( \u2212k\u2032 c\nkc\u03b4\n)( f avgAUX(\u2205)\u2212 f avgAUX ( OPT[k] ) \u00b7 4 log n\n\u03b4g\n) + kc\u03b4 c \u00b7 c\u03b7,\nThis gives us\nf avgAUX(k \u2032) \u2264 f avgAUX(\u2205) \u00b7 exp ( \u2212k\u2032 c\nkc\u03b4\n)\n\ufe38 \ufe37\ufe37 \ufe38 UB1\n+ f avgAUX ( OPT[k] ) \u00b7 4 log n\n\u03b4g\n( 1\u2212 exp ( \u2212k\u2032 c\nkc\u03b4\n))\n\ufe38 \ufe37\ufe37 \ufe38 UB2\n+ kc\u03b4 c \u00b7 c\u03b7,\n\ufe38 \ufe37\ufe37 \ufe38 UB3\n(B.27)\nDenote the three terms on the RHS. of Equation (B.27) as UB1, UB2 and UB3, respectively. We get\n   UB1 Eq (B.23) \u2264 (3c+ 4) (1 + log n) \u00b7 exp ( \u2212k\u2032 c kc\u03b4 ) UB2 Eq (B.24) < (6c+ 8) \u00b7 \u03b4opt log n\u03b4opt \u00b7 4 log n \u03b4g\nUB3 = k \u00b7 (6c+ 8) log n\u03b4g \u00b7 2t(1\u22122 )2\u03b7 1 16 (1\u22122 )2 = (6c+ 8) \u00b7 32 \u00b7 k \u00b7 log n\u03b4g \u00b7 t\u03b7\nNow we set { k\u2032 , kc\u03b4c\u03b5 \u00b7 ln 8 logn \u03b4g\n\u03b4opt , \u03b4g\n64\u00b736\u00b7logn\u00b7log 1\u03b4g \u00b7log n \u03b4g\n(B.28)\nand obtain exp ( \u2212k\u2032 c kc\u03b4 ) = \u03b4g 8 logn . It is easy to verify that UB1 \u2264 2c \u00b7 \u03b4g 4 , and UB2 \u2264 2c \u00b7 \u03b4g 2 .\nWe further set\n\u03b7 , \u03b4g16\u00b732\u00b7kt\u00b7log n\u03b4g , (B.29)\nand obtain UB3 = 2c \u00b7 \u03b4g4 . Combining the upper bound derived above for UB1, UB2, UB3, and by Equation (B.27), we get f avgAUX(k\u2032) \u2264 2c \u00b7 \u03b4g. By Lemma 2 we know that the error probability is upper bounded by pERR = E\u03c8k\u2032 [ pMAPERR (\u03c8k\u2032) ] \u2264 f avg AUX(k \u2032) 2c \u2264 \u03b4g. That is, with the cost k\u2032 specified in Equation (B.28), ECED is guaranteed to achieve pERR \u2264 \u03b4g. It remains to compute the (exact) value of k\u2032. Combining the definition of c , 8 ( log(2n2/\u03b7) )2 and\nc\u03b4 , (6c+ 8) log(n/\u03b4g) with Equation (B.29) it is easy to verify that\nc\u03b4 \u2264 c1 \u00b7 ( log nk\n\u03b4g\n)2 \u00b7 log n\n\u03b4g ,\nholds for some constant c1. Therefore by Equation (B.28),\nk\u2032 \u2264 k \u00b7 c1 ( log nk\n\u03b4g\n)2 log n\n\u03b4g \u00b7 1 c\u03b5 ln 8 log n \u03b4g = O\n( k\nc\u03b5\n( log nk\n\u03b4g\n)2( log n\n\u03b4g\n)2) .\nTo put it in words, it suffices to run ECED for O ( k c\u03b5 ( log nk\u03b4g )2 ( log n\u03b4g )2) steps to have expected\nerror below \u03b4g, where k denotes the worst-case cost the optimal policy that achieves expected error\nprobability \u03b4opt , O (\n\u03b4g (logn\u00b7log(1/\u03b4g))2\n) ; hence the completion of the proof.\nC Examples When GBS and the Most Informative Policy Fail\nIn this section, we provide problem instances where GBS and/or the Most Informative Policy may fail, while ECED performs well. Since in the noise-free setting ECED is equivalent to EC2, it suffices to demonstrate the limitations of GBS and the most informative policy, even if we provide just examples that apply to the noise-free setting.\nC.1 A Bad Example for GBS: Imbalanced Equivalence Classes\nWe use the same example as provided in Golovin et al. (2010). Consider an instance with a uniform prior over n root-causes, \u03b81, . . . , \u03b8n, and two target values y1 = r(\u03b81) = . . . r(\u03b8n\u22121), and y2 = r(\u03b8n). There are tests V = {1, . . . , n} such that P [Xe = 1 | \u03b8i] = 1 {i = e} (all of unit cost). Here, 1 {\u00b7} is the indicator function. See Fig. 7 for illustration.\nNow, suppose we want to solve Problem (2.1) for \u03b4 = 1/n. Note that in the noise-free setting, the problem is equivalent to find a minimal cost policy \u03c0 that achieves 0 prediction error, because once the error probability drops below 1/n we will know precisely which target value is realized.\nIn this case, the optimal policy only needs to select test n, however GBS may select tests {1, . . . , n} in order until running test e, where \u0398 = \u03b8e is the true root-cause. Given our uniform prior, it takes n/2 tests in expectation until this happens, so that GBS pays, in expectation, n/2 times the optimal expected cost in this instance. Note that in this example, ECED (equivalently, EC2) also selects test n, which is optimal."}, {"heading": "C.2 A Bad Example for the Most Informative Policy: Treasure Hunt", "text": "In this section, we provide a treasure-hunt example, in which the most informative policy pays \u2126 (n/ log(n)) times the optimal cost. This example is adapted from Golovin et al. (2010), where they show that the most informative policy (referred to as the Informative Gain policy), as well as the myopic policy that greedily maximizes the reduction in the expected prediction error (referred as the Value of Information policy), both perform badly, compared with EC2.\nConsider the problem instance in Fig. 8(a). Fix s > 0 to be some integer, and let t = |Y| = 2s. For each target value yi \u2208 Y , there exists two root-causes, i.e., \u03b8i,1, \u03b8i,0, such that r(\u03b8i,1) = r(\u03b8i,0) = yi. Denote a root-causes as \u03b8i,o, if it belongs to target i and is indexed by o. We assume a uniform prior over the root-causes: {\u03b8i,o}i\u2208{1,...,t},o\u22080,1.\nSuppose we want to solve Problem (2.1) for \u03b4 = 1/3. Similarly with \u00a7C.1, the problem is equivalent to find a minimal cost policy \u03c0 that achieves 0 prediction error, because once the error probability drops below 1/3, we will know precisely which target value is realized.\nThere are three set of tests, and all of them have binary outcomes and unit cost. The first set V1 := {e0} contains one test e0, which tells us the value of o of the underlying root-cause \u03b8i,o. Hence for all i, \u0398 = \u03b8i,o \u21d2 Xe0 = o (see Fig. 8(b)). The second set of tests are designed to help us quickly discover the index of the target value via binary search if we have already run e0, but to offer no information whatsoever (in terms of expected reduction in the prediction error, or expected reduction in entropy of Y ) if e0 has not yet been run. There are a total number of s tests in the second set V2 := {e1, e2, . . . , es}. For z \u2208 {1, . . . , t}, let bk(z) be the kth least-significant bit of the binary encoding of z, so that z = \u2211s k=1 2\nk\u22121bk(z). Then, if \u0398 = \u03b8i,o, then the outcome of test ek \u2208 V2 is Xek = 1 {\u03c6k(i) = o} (see Fig. 8(c)). The third set of tests are designed to allow us to do a (comparatively slow) sequential search on the index of the the target values. Specifically, we have V3 := {eseq1 , . . . , eseqt }, such that \u0398 = \u03b8i,o \u21d2 Xeseqk = 1 {i = k} (Fig. 8(d)). Now consider running the maximal informative policy \u03c0 (the same analysis also applies to the value of information policy, which we omits from the paper). Note that in the beginning, no single test from V1 \u222a V2 results in any change in the distribution over Y , as it remains uniform no matter with test is performed. Hence, the maximal informative policy only picks tests from V3, which have non-zero (positive) expected reduction in the posterior entropy of Y . In the likely event that the test chosen is not the index of Y , we are left with a residual problem in which tests in V1 \u222a V2 still have no effect on the posterior. The only difference is that there is one less class, but the prior remains uniform. Hence our previous argument still applies, and \u03c0 will repeatedly select tests in V3, until a test has an outcome of 1. In expectation, the cost of \u03c0 is least cost(\u03c0) \u2265 1t \u2211t z=1 z = t+1 2 . On the other hand, a smarter policy \u03c0\u2217 will select test e0 \u2208 V1 first, and then performs a binary search by running test e1, . . . , es \u2208 V2 to determine bk(i) for all 1 \u2264 k \u2264 s (and hence to determine the index i of Y ). Since the tests have unit cost, the cost of \u03c0\u2217 is cost(\u03c0\u2217) = s+ 1.\nSince t = 2s, and n = 2t = 2s+1, we conclude that\ncost(\u03c0) = t+ 1 2 > t 2 = n 4 s+ 1 log n =\nn\n4 log(n) cost(\u03c0\u2217)."}, {"heading": "D Case Study: Pool-based Active Learning for Classification", "text": "Experimental setup. To demonstrate the empirical performance of ECED, we further conduct experiments on two pool-based binary active classification tasks. In the active learning application, we can sequentially query from a pool of data points, and the goal is to learn a binary classifier, which achieves some small prediction error on the unseen data points from the pool, with the smallest number of queries as possible.\nActive Learning: Targets and Root-causes To discretize the hypotheses space, we use a noisy version of hit-and-run sampler as suggested in Chen & Krause (2013). Each hypothesis can be represented by a binary vector indicating the outcomes of all data points in the training set. Then, we construct an epsilon-net on the set of hypotheses (based on the Hamming distance between hypotheses). We obtain the equivalence classes for ECED, by assigning each hypothesis to its closest center of epsilon-ball, measured by their Hamming distances. Note that the Hamming distance between two hypotheses reflects the difference of prediction error. Consider epsilon-net of fixed radius \u03b5. By construction, hypotheses that lie in the some equivalence classes are at most 2\u03b5 away from each other; therefore the hypotheses which are within the epsilon-ball of the optimal hypotheses are considered to be near-optimal. Using the terminology in this paper, hypotheses correspond to root-causes, and the groups of hypothesis correspond to the target variable of interest. Running ECED, ideally, will help us locate a near-optimal epsilon-ball as quickly as possible.\nBaselines. We compare ECED with the popular uncertainty sampling heuristic (UNC-SVM), which sequentially queries the data points which are the closest to the decision boundary of a SVM classifier. We also compare with the GBS algorithm, which sequentially queries the data points that maximally reduces the volume of the version space.\nIn Fig. 9(a), we demonstrate the different behaviors between GBS and EC2 on a 2-d plane. In this simple example, there are 4 color-coded equivalence classes: we first sample hypotheses uniformly within the unit circle, and then generate equivalence classes, by constructing an epsilon-net over the sampled hypotheses as previously described. Fig. 9(a) illustrates two tests (i.e., the gray lines intersecting the circles) selected by ECED and GBS, respectively. ECED primarily selects tests that best disambiguate the clusters, while GBS focuses on disambiguate individual hypotheses.\nResults. We evaluate ECED and the baseline algorithms on the UCI WDBC dataset (569 instances, 32-d) and Fourclass dataset (862 instances, 2-d). For ECED and GBS, we sample a fixed number of 1000 hypotheses in each random trial. For both instances we assume a constant error rate = 0.02 for all tests. Fig. 9(b) and Fig. 9(c) demonstrate that ECED is competitive with the baselines. Such results suggests that grouping of hypotheses could be beneficial when learning under noisy data."}], "references": [{"title": "Active learning\u2013modern learning theory", "author": ["Balcan", "Maria-Florina", "Urner", "Ruth"], "venue": "Encyclopedia of Algorithms,", "citeRegEx": "Balcan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2015}, {"title": "Extensions of generalized binary search to group identification and exponential costs", "author": ["G. Bellala", "S. Bhavnani", "C. Scott"], "venue": "In NIPS,", "citeRegEx": "Bellala et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bellala et al\\.", "year": 2010}, {"title": "Decision trees for entity identification: Approximation algorithms and hardness results", "author": ["V.T. Chakaravarthy", "V. Pandit", "S. Roy", "P. Awasthi", "M. Mohania"], "venue": "SIGMOD/PODS,", "citeRegEx": "Chakaravarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chakaravarthy et al\\.", "year": 2007}, {"title": "Bayesian experimental design: A review", "author": ["K. Chaloner", "I. Verdinelli"], "venue": "Statistical Science,", "citeRegEx": "Chaloner and Verdinelli,? \\Q1995\\E", "shortCiteRegEx": "Chaloner and Verdinelli", "year": 1995}, {"title": "Near-optimal batch mode active learning and adaptive submodular optimization", "author": ["Chen", "Yuxin", "Krause", "Andreas"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Sequential information maximization: When is greedy near-optimal", "author": ["Chen", "Yuxin", "Hassani", "S. Hamed", "Karbasi", "Amin", "Krause", "Andreas"], "venue": "In COLT,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Submodular surrogates for value of information", "author": ["Chen", "Yuxin", "Javdani", "Shervin", "Karbasi", "Amin", "Bagnell", "James Andrew", "Srinivasa", "Siddhartha", "Krause", "Andreas"], "venue": "In AAAI,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "Dasgupta,? \\Q2004\\E", "shortCiteRegEx": "Dasgupta", "year": 2004}, {"title": "Analysis of a greedy active learning strategy", "author": ["Dasgupta", "Sanjoy"], "venue": "In NIPS,", "citeRegEx": "Dasgupta and Sanjoy.,? \\Q2004\\E", "shortCiteRegEx": "Dasgupta and Sanjoy.", "year": 2004}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["Golovin", "Daniel", "Krause", "Andreas"], "venue": "JAIR,", "citeRegEx": "Golovin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2011}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["Golovin", "Daniel", "Krause", "Andreas", "Ray", "Debajyoti"], "venue": "In NIPS,", "citeRegEx": "Golovin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Hanneke", "Steve"], "venue": "In ICML,", "citeRegEx": "Hanneke and Steve.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke and Steve.", "year": 2007}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["Herlocker", "Jonathan L", "Konstan", "Joseph A", "Borchers", "Al", "Riedl", "John"], "venue": "In SIGIR,", "citeRegEx": "Herlocker et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Herlocker et al\\.", "year": 1999}, {"title": "Information value theory", "author": ["R.A. Howard"], "venue": "Systems Science and Cybernetics, IEEE Trans. on,", "citeRegEx": "Howard,? \\Q1966\\E", "shortCiteRegEx": "Howard", "year": 1966}, {"title": "Active learning in the non-realizable case", "author": ["K\u00e4\u00e4ri\u00e4inen", "Matti"], "venue": "In Algorithmic Learning Theory, pp", "citeRegEx": "K\u00e4\u00e4ri\u00e4inen and Matti.,? \\Q2006\\E", "shortCiteRegEx": "K\u00e4\u00e4ri\u00e4inen and Matti.", "year": 2006}, {"title": "Noisy binary search and its applications", "author": ["Karp", "Richard M", "Kleinberg", "Robert"], "venue": "In SODA,", "citeRegEx": "Karp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Karp et al\\.", "year": 2007}, {"title": "On an optimal split tree problem", "author": ["Kosaraju", "S Rao", "Przytycka", "Teresa M", "Borgstrom", "Ryan"], "venue": "In Algorithms and Data Structures,", "citeRegEx": "Kosaraju et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kosaraju et al\\.", "year": 1999}, {"title": "Noisy generalized binary search", "author": ["Nowak", "Robert"], "venue": "In NIPS,", "citeRegEx": "Nowak and Robert.,? \\Q2009\\E", "shortCiteRegEx": "Nowak and Robert.", "year": 2009}, {"title": "Bayesian rapid optimal adaptive design (broad): Method and application distinguishing models of risky choice", "author": ["Ray", "Debajyoti", "Golovin", "Daniel", "Krause", "Andreas", "Camerer", "Colin"], "venue": "Tech. Report,", "citeRegEx": "Ray et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2012}, {"title": "Which uncertainty? using expert elicitation and expected value of information to design an adaptive program", "author": ["M.C. Runge", "S.J. Converse", "J.E. Lyons"], "venue": "Biological Conservation,", "citeRegEx": "Runge et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Runge et al\\.", "year": 2011}, {"title": "Active Learning", "author": ["B. Settles"], "venue": null, "citeRegEx": "Settles,? \\Q2012\\E", "shortCiteRegEx": "Settles", "year": 2012}, {"title": "Capital Asset Prices: A Theory of Market Equilibrium under Conditions of Risk", "author": ["Sharpe", "William F"], "venue": "The Journal of Finance,", "citeRegEx": "Sharpe and F.,? \\Q1964\\E", "shortCiteRegEx": "Sharpe and F.", "year": 1964}, {"title": "The optimal control of partially observable markov processes over a finite horizon", "author": ["Smallwood", "Richard D", "Sondik", "Edward J"], "venue": "Operations Research,", "citeRegEx": "Smallwood et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Smallwood et al\\.", "year": 1973}, {"title": "Advances in prospect theory: Cumulative representation of uncertainty", "author": ["Tversky", "Amos", "Kahneman", "Daniel"], "venue": "Journal of Risk and Uncertainty,", "citeRegEx": "Tversky et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Tversky et al\\.", "year": 1992}, {"title": "Prospect Theory: For Risk and Ambiguity", "author": ["P.P. Wakker"], "venue": null, "citeRegEx": "Wakker,? \\Q2010\\E", "shortCiteRegEx": "Wakker", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "The problem of optimal information gathering has been studied in the context of active learning (Dasgupta, 2004a; Settles, 2012), Bayesian experimental design (Chaloner & Verdinelli, 1995), policy making (Runge et al.", "startOffset": 96, "endOffset": 128}, {"referenceID": 19, "context": "The problem of optimal information gathering has been studied in the context of active learning (Dasgupta, 2004a; Settles, 2012), Bayesian experimental design (Chaloner & Verdinelli, 1995), policy making (Runge et al., 2011), optimal control (Smallwood & Sondik, 1973), and numerous other domains.", "startOffset": 204, "endOffset": 224}, {"referenceID": 2, "context": "Deriving the optimal testing policy is NP-hard in general (Chakaravarthy et al., 2007); however, under certain conditions, some approximation results are known.", "startOffset": 58, "endOffset": 86}, {"referenceID": 16, "context": ", in the noise-free setting), a simple greedy algorithm, namely Generalized Binary Search (GBS), is guaranteed to provide a near-optimal approximation of the optimal policy (Kosaraju et al., 1999).", "startOffset": 173, "endOffset": 196}, {"referenceID": 9, "context": "Golovin et al. (2010) then formalize this problem as an equivalence class determination problem (See \u00a72.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "When \u03b4 = 0, this problem reduces to the equivalence class determination problem (Golovin et al., 2010; Bellala et al., 2010).", "startOffset": 80, "endOffset": 124}, {"referenceID": 1, "context": "When \u03b4 = 0, this problem reduces to the equivalence class determination problem (Golovin et al., 2010; Bellala et al., 2010).", "startOffset": 80, "endOffset": 124}, {"referenceID": 10, "context": ", \u2200e, P [Xe | \u0398] \u2208 {0, 1}, this problem can be solved near-optimally by the equivalence class edge cutting (EC) algorithm (Golovin et al., 2010).", "startOffset": 122, "endOffset": 144}, {"referenceID": 10, "context": "The EC objective function is adaptive submodular, and strongly adaptive monotone (Golovin et al., 2010).", "startOffset": 81, "endOffset": 103}, {"referenceID": 13, "context": "Last, we consider myopic optimization of the decision-theoretic value of information (VOI) (Howard, 1966).", "startOffset": 91, "endOffset": 105}, {"referenceID": 24, "context": "We test ECED on six theories of subjective valuation of risky choices (Wakker, 2010; Tversky & Kahneman, 1992; Sharpe, 1964), namely (1) expected utility with constant relative risk aversion, (2) expected value, (3) prospect theory, (4) cumulative prospect theory, (5) weighted moments, and (6) weighted standardized moments.", "startOffset": 70, "endOffset": 124}, {"referenceID": 18, "context": "We employ the same set of parameters used in Ray et al. (2012) to generate tests and root-causes.", "startOffset": 45, "endOffset": 63}, {"referenceID": 12, "context": "We use the MovieLens 100k dataset (Herlocker et al., 1999), which consists of a matrix of 1 to 5 ratings of 1682 movies from 943 users, and adopt the experimental setup proposed in Chen et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 4, "context": ", 1999), which consists of a matrix of 1 to 5 ratings of 1682 movies from 943 users, and adopt the experimental setup proposed in Chen et al. (2015b). In particular, we extract movie features by computing a low-rank approximation of the user/rating matrix of the MovieLens 100k dataset through singular value decomposition (SVD).", "startOffset": 130, "endOffset": 150}, {"referenceID": 7, "context": ", Dasgupta (2004b); Hanneke (2007, 2014); Balcan & Urner (2015)), sample complexity bounds have been characterized in terms of the structure of the hypothesis class, as well as additional distribution-dependent complexity measures (e.", "startOffset": 2, "endOffset": 19}, {"referenceID": 7, "context": ", Dasgupta (2004b); Hanneke (2007, 2014); Balcan & Urner (2015)), sample complexity bounds have been characterized in terms of the structure of the hypothesis class, as well as additional distribution-dependent complexity measures (e.", "startOffset": 2, "endOffset": 64}, {"referenceID": 10, "context": ", 2015a) and strict noise assumptions (Golovin et al., 2010), more general settings, which we focus on in this paper, are much less understood.", "startOffset": 38, "endOffset": 60}, {"referenceID": 9, "context": "By adaptive submodularity of fEC2 (in the noiseless setting, see Golovin et al. (2010)), we obtain", "startOffset": 65, "endOffset": 87}, {"referenceID": 9, "context": "1 A Bad Example for GBS: Imbalanced Equivalence Classes We use the same example as provided in Golovin et al. (2010). Consider an instance with a uniform prior over n root-causes, \u03b81, .", "startOffset": 95, "endOffset": 117}, {"referenceID": 9, "context": "This example is adapted from Golovin et al. (2010), where they show that the most informative policy (referred to as the Informative Gain policy), as well as the myopic policy that greedily maximizes the reduction in the expected prediction error (referred as the Value of Information policy), both perform badly, compared with EC.", "startOffset": 29, "endOffset": 51}], "year": 2016, "abstractText": "We consider the Bayesian active learning and experimental design problem, where the goal is to learn the value of some unknown target variable through a sequence of informative, noisy tests. In contrast to prior work, we focus on the challenging, yet practically relevant setting where test outcomes can be conditionally dependent given the hidden target variable. Under such assumptions, common heuristics, such as greedily performing tests that maximize the reduction in uncertainty of the target, often perform poorly. In this paper, we propose ECED, a novel, computationally efficient active learning algorithm, and prove strong theoretical guarantees that hold with correlated, noisy tests. Rather than directly optimizing the prediction error, at each step, ECED picks the test that maximizes the gain in a surrogate objective, which takes into account the dependencies between tests. Our analysis relies on an information-theoretic auxiliary function to track the progress of ECED, and utilizes adaptive submodularity to attain the near-optimal bound. We demonstrate strong empirical performance of ECED on two problem instances, including a Bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions, and an active preference learning task via pairwise comparisons.", "creator": "LaTeX with hyperref package"}}}