{"id": "1605.07785", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Geometry-aware stationary subspace analysis", "abstract": "In many real-world applications, data exhibit instationality, i.e. their distribution changes over time. An approach to dealing with instationality is to remove or minimize it before attempting to analyze the data. In connection with the analysis of brain computer interface (BCI) data, this can be done by means of stationary subspace analysis (SSA). The classic SSA method finds a matrix that projects the data onto a stationary subspace by optimizing a cost function based on matrix divergence. In this paper, we present an alternative method for SSA based on a symmetrized version of this matrix divergence. We show that this frames the problem in terms of distances between symmetrical positively defined (SPD) matrices, suggesting a geometrical interpretation of the problem. Starting from this geometrical point of view, we introduce a method and analyze it that uses the matrix geometry of SPD-matrix to manifold invention properties.", "histories": [["v1", "Wed, 25 May 2016 09:11:23 GMT  (97kb,D)", "http://arxiv.org/abs/1605.07785v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["inbal horev", "florian yger", "masashi sugiyama"], "accepted": false, "id": "1605.07785"}, "pdf": {"name": "1605.07785.pdf", "metadata": {"source": "CRF", "title": "Geometry-aware Stationary Subspace Analysis", "authors": ["Inbal Horev", "Florian Yger", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A common assumption in statistical modeling is that the distribution of observed data does not change over time, i.e., that it is stationary. In most cases it is this assumption of stationarity which allows results to be effectively generalized from the sample to the population. When the stationarity assumption is violated, as is often the case in real-world applications such as speech enhancement [11] or neurological data analysis [25], specialized machine learning methods must be developed in order to maintain adequate prediction capabilities.\nA relatively well-studied non-stationary setting is covariate-shift [26], in which the input distribution changes but the conditional distribution of the outputs does not. The problem of covariate-shift has received growing attention in recent years, and many theoretical and practical aspects have been addressed (see [28] for an in-depth exploration of this topic).\nThese works typically do not aim to remove or reduce non-stationarity in the data, but rather they try to cope with its existence. A different approach\nar X\niv :1\n60 5.\n07 78\n5v 1\n[ cs\n.L G\n] 2\n5 M\nis to remove, or minimize, any existing non-stationarities before attempting to analyze the collected data. In the context of brain computer interface (BCI) data analysis, two such note-worthy methods are stationary subspace analysis (SSA) [30] and stationary common spacial patterns (sCSP) [32].\nSimilar in spirit to independent component analysis (ICA) [16], SSA statistically models the data as a mixture of stationary and non-stationary signals. Unlike ICA, however, the signals are not assumed to be independent of each other. The data is first split into (possibly overlapping) time frames called epochs. Then a projection matrix is found by optimizing a cost function based on the divergence between distributions in various epochs.\nThe other method, by now quite a standard step for classification tasks in BCI systems, is the (supervised) sCSP method. Its goal is to project the data onto a subspace in which the various data classes are more separable. The sCSP method directs this subspace towards a stationary subspace by means of regularization.\nAlthough SSA is essentially an unsupervised method, variations of it exist which are useful for supervised tasks such as classification [25]. These methods attempt to remove non-stationarity while keeping the discriminative inter-class variations intact. In this work we present an alternative method for stationary subspace extraction, focusing for the moment on the unsupervised setting.\nUnlike SSA, the inputs to our method are not the raw signals themselves, but rather covariance matrices, computed from the signals (or from features based on the signals) using one of the many existing covariance estimators (e.g., [20]). Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6]. Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13]. This in turn motivates a covariance-based approach which utilizes the geometric properties of the symmetric positive definite (SPD) matrix manifold for SSA."}, {"heading": "2 Geometry-aware stationary subspace analysis", "text": "As discussed in the introduction, the task of extracting the stationary part from an observed mixture of stationary and non-stationary signals is essential in various applications. In this section we present our approach to this problem. We name it geometry-aware SSA (gaSSA) since we utilize the geometric properties of covariance matrices. To find a stationary subspace, SSA uses a cost function which is based on a matrix divergence. As a first step, we suggest a formulation that uses a symmetrized version of the same matrix divergence. We then show that this symmetrized matrix divergence is a distance between SPD matrices and offer a geometric interpretation to the problem of SSA.\nWe begin with a formal statement of the problem. To this end we provide\na review of the original SSA model and framework [30]. Next, we present the symmetrized matrix divergence and discuss its relation to the Riemannian geometry of SPD matrices. Finally, we end this section by introducing and analyzing a general formulation for our geometry-aware SSA."}, {"heading": "2.1 Stationary subspace analysis", "text": "Let x(t) \u2208 RD be a vector of D input signals, composed of m stationary sources ss(t) = [s1(t), . . . , sm(t)]\n> (s-sources) and D\u2212m non-stationary sources sn(t) = [sm+1(t), . . . , sD(t)] > (n-sources), mixed by a linear mixing transformation,\nx(t) = As(t) = [ AsAn ] [ss(t) sn(t) ] , (1)\nwhere A \u2208 GLD(R), the general linear group of size D over R, i.e., the set of all D \u00d7D invertible matrices with entries in R. The spaces spanned by the column vectors As and An are referred to as the s-space and n-space, respectively.\nThe SSA model makes relatively few assumptions on the s- and n-sources. First, the s-sources are stationary only in the weak (or wide) sense [17]. That is, their first and second moments are required to be constant in time. For the n-sources, their first two moments may vary between epochs of length T denoted by \u03c4i = [t0(i), . . . , t0(i) + T ], where t0(i) is the start time of the i-th epoch. The sources do not necessarily follow a Gaussian distribution, but non-stationarities are assumed to be visible in the first two moments. Furthermore, this model does not assume that the sources are independent, namely, their covariance matrix is given by\n\u03a3(\u03c4i) = E [ s(\u03c4i)s(\u03c4i) >] = ( \u03a3s \u03a3sn(\u03c4i) \u03a3sn(\u03c4i) > \u03a3n(\u03c4i) ) , (2)\nwhere \u03a3s \u2208 Rm\u00d7m, \u03a3n \u2208 R(D\u2212m)\u00d7(D\u2212m) and \u03a3sn \u2208 Rm\u00d7(D\u2212m). Note that since \u03a3s is time independent we have dropped the notation (\u03c4i).\nThe goal of SSA is to find a de-mixing transformation A\u0302\u22121 that separates the s-sources from the n-sources. This matrix A\u0302\u22121 is not unique, but rather undetermined up to scaling, sign and linear transformations within the s- and n-spaces. So, without loss of generality, the data may be centered and whitened such that the s-sources have a zero mean and a diagonal covariance matrix with unit variance.1 Put differently, the de-mixing matrix is written as A\u0302\u22121 = B\u0302Z where Z = Cov(x)\u22121/2 is a whitening matrix created by a covariance estimator Cov(\u00b7) (in this case it is the empirical estimator) and B\u0302 \u2208 OD ={ V \u2208 RD\u00d7D : V >V = I } , the set of all D \u00d7D orthogonal matrices.\nTo find the matrix B\u0302, the signals are split into N epochs \u03c41, . . . , \u03c4N of length T . Each epoch is characterized by its empirical mean \u00b5\u0302i and covariance \u03a3\u0302i. Then for each epoch, the mean and covariance of the s-sources may be written as\n\u00b5\u0302si = ImDB\u0302Z\u00b5\u0302i, \u03a3\u0302si = ImDB\u0302Z\u03a3\u0302i ( ImDB\u0302Z )> , (3)\n1This is also common practice in ICA [16]\nwhere ImD is the D \u00d7D identity matrix, truncated to the first m columns. Since the true \u00b5s and \u03a3s are by definition stationary, the matrix B\u0302 is that for which \u00b5\u0302si and \u03a3\u0302si vary the least across all epochs. Owing to the maximum entropy principle, SSA uses the Kullback-Leibler (KL) divergence between Gaussian distributions to compare the epoch distributions up to their second moment. The matrix B\u0302 is thus found by minimizing the following cost function:\nL ( B\u0302 ) = N\u2211 i=1 DKL [ N ( \u00b5\u0302si , \u03a3\u0302 s i ) || N (0, I) ] = \u2212 N\u2211 i=1 ( log det \u03a3\u0302si + \u00b5\u0302 s i >\u00b5\u0302si ) , (4)\nwhere DKL is the KL divergence and N (\u00b5,\u03a3) denotes a multivariate Gaussian distribution with mean \u00b5 and covariance \u03a3."}, {"heading": "2.2 Symmetrized matrix divergence", "text": "SSA and its variants use in their cost function the KL divergence between Gaussian distributions. In what follows we assume that these distributions have a zero mean.Under this assumption the KL divergence is a Bregman matrix divergence [4] between covariance matrices. The family of Bregman matrix divergences is generally defined as\nD\u03a6 (X,Y ) = \u03a6(X)\u2212 \u03a6(Y )\u2212 tr ( (\u2207\u03a6 (Y ))> (X \u2212 Y ) ) . (5)\nDKL is obtained for \u03a6(X) = \u2212 log detX, so it is often called the log-determinant divergence [18].\nBregman matrix divergences are useful in machine learning and have a number of useful properties [18], such as linearity and convexity in the first argument (and, in the case of the KL divergence, also in the second). However, as can be seen from their definition, they are asymmetric and do not satisfy the triangle equality. In particular, in our case we have that DKL (X || Y ) 6= DKL (Y || X) for two arbitrary matrices X 6= Y . Subsequently, symmetrized versions of the Bregman matrix divergence, in particular Jensen-Bregman divergences, have been studied in recent years [23]. In the case of the KL divergence (for zero mean distributions), this gives\nDJBLD (X,Y ) = 1\n2\n[ DKL ( X || 1\n2 (X + Y )\n) +DKL ( Y || 1\n2 (X + Y ) )] = log det ( 1\n2 (X + Y )\n) \u2212 1\n2 log det (XY ) (6)\nand is called the Jensen-Bregman log-determinant (JBLD) divergence [10]. The JBLD has many favorable properties (see [27]), primarily that its square root comprises a metric on the SPD matrix manifold. Moreover, in Sra [27] it has been shown that it is a close approximation to the affine invariant Riemannian metric (AIRM) [7] and shares many of its mathematical properties. The practical properties of both metrics, in particular their invariance properties, will be\ndiscussed in Section 2.5. In the context of SPD matrices, the JBLD is referred to as the symmetric Stein divergence or the (square of the) log-determinant metric [27]. In the sequel we adopt the notation \u03b4s (X,Y ).\nMotivated by the above, we formulate SSA using a new cost function based on \u03b4s (cf. Eqs. (3) and (4)):\nL(B\u0302) = N\u2211 i=1 \u03b42s (\u03a3\u0302 s i , I) = N\u2211 i=1 \u03b42s (Q >\u03a3\u0303iQ, I) = N\u2211 i=1 [ log det ( 1 2 (\u03a3\u0302si + I) ) \u2212 1 2 log det(\u03a3\u0302si ) ] ,\n(7)\nwhereQ = ( ImDB\u0302 )> and \u03a3\u0303i = Z\u03a3iZ> are the matrices whitened with Z = \u03a3\u0304\u22121/2,\n\u03a3\u0304 = argmin \u03a3\u2208S+D\n\u03b42s (\u03a3i,\u03a3), the mean of \u03a3i w.r.t. \u03b4s. S+D denotes the set of all D\u00d7D\nSPD matrices."}, {"heading": "2.3 A geometric interpretation", "text": "By replacing the cost function with one based on the symmetrized divergence we gain not only the beneficial properties of the symmetric divergence, but also new insight into the problem of SSA. First, note that in Eq. (7) the problem is ultimately framed in terms of distances between SPD matrices. This suggests adopting a geometric perspective, whereby the notion of stationarity is captured by the dispersion of the matrices \u03a3i. In this view, the assumption that the covariance matrices of stationary signals do not vary much between epochs translates to them having small distances between them.\nAn illustration of this idea is presented in Fig. 1. In this figure, the matrices \u03a3i are seen as points on the SPD matrix manifold S+D . The goal of our method is to find transformations Q and W that map the matrices onto two separate manifolds of lower dimension - the stationary and non-stationary space, respectively. The matrices in the stationary space will exhibit small variation,\nwhile the non-stationarities will be captured in the non-stationary space where the variation of the matrices will be greater. The transformations Q and W may be chosen to be orthogonal to each other, producing well separated s- and n-spaces. That is, W \u2208 Q\u22a5 for Q = spanQ (likewise Q \u2208 W\u22a5), where \u22a5 denotes the orthogonal complement.\nPut formally, our objective is to find a rank-m transformation matrix Q \u2208 RD\u00d7m which maps \u03a3i \u2208 S+D to \u03a3\u0302si \u2208 S+m for m < D such that the logdeterminant distance between the compressed centered matrices \u03a3\u0302si = Q>\u03a3\u0303iQ and their mean, which for the centered matrices is I, is minimized. Note that the space spanned by the columns of Q is of importance, and not the specific columns themselves. So, we may optimize Q over the Grassmann manifold [12], G = { span(Q) : Q \u2208 RD\u00d7m, Q>Q = I } , the set of all m-dimensional linear subspaces of RD\u00d7D. In practice, for optimization we employ a Riemannian trust-regions method described in Absil et al. [2] and implemented efficiently in Boumal et al. [8]."}, {"heading": "2.4 A generic geometric formulation", "text": "Given the strong relation between the log-determinant metric and the AIRM [7], a natural progression is to incorporate the AIRM into the cost function. To understand why it would be beneficial to use the AIRM it is necessary first to briefly discuss the geometry of the SPD matrix manifold.\nWhen equipped with the Frobenius inner product \u3008A,B\u3009F = tr(A>B), the set S+n of SPD matrices of size n\u00d7 n, belongs to a Euclidean space. In this case, similarity between SPD matrices can be measured simply by using the Euclidean distance derived from the Euclidean norm. This is readily seen in the following example for 2\u00d7 2 SPD matrices. A matrix A \u2208 S+2 can be written as A = [ a c c b ] with ab\u2212 c2 > 0, a > 0 and b > 0. Then matrices in S+2 can be represented as points in R3 and the constraints can be plotted as a convex cone whose interior is populated by the SPD matrices (see Fig. 2). In this representation, the Euclidean geometry of symmetric matrices then implies that distances are computed along straight lines.\nDespite its simplicity, the Euclidean geometry has several drawbacks and is not always well suited for SPD matrices [13, 3]. For example, due to an\nartifact referred to as the swelling effect [3], for a task as simple as averaging two matrices, it may occur that the determinant of the average is larger than any of the two matrices. Another drawback, illustrated in Fig. 2 and documented by Fletcher et al. [13], is the fact that this geometry forms a non-complete space. Hence, in this Euclidean space interpolation between SPD matrices is possible, but extrapolation may produce indefinite matrices, leading to uninterpretable solutions.\nAn efficient alternative which addresses these issues is to consider the space of SPD matrices as a curved space, namely a Riemannian manifold. Of the possible Riemannian distances, the AIRM, due to its favorable mathematical properties, is widely used in many applications (see, for example [13, 24]). It is defined for any X,Y \u2208 S+D as:\n\u03b42r (X,Y ) = \u2225\u2225\u2225log (X\u22121/2Y X\u22121/2)\u2225\u2225\u22252\nF , (8)\nwhere log(\u00b7) is the matrix logarithm and \u2016X\u20162F = tr ( X>X ) is the Frobenius norm. In the curved space, the geodesics between matrices obtained by the AIRM are computed on curved lines as illustrated in Fig. 2 for the space S+2 . Symmetric matrices with null and infinite eigenvalues (i.e., those which lie on the boundary of the convex cone, but not in it) are both at an infinite distance from any SPD matrix on the manifold (within the cone).\nSo, let us also consider a cost function of the same form defined w.r.t. the AIRM.2 A general expression for our geometry-aware SSA (gaSSA) is then:\nDefinition 2.1 (gaSSA) gaSSA w.r.t. a metric \u03b4 is defined as\nQ\u0302 = argmin Q\u2208G(D,m) \u2211 i \u03b42 ( Q>\u03a3\u0303iQ, I ) , (9)\nwhere \u03a3\u0303i = Z\u03a3iZ> are the matrices whitened with Z = \u03a3\u0304\u22121/2 and \u03a3\u0304 = argmin \u03a3\u2208S+D \u03b42 (\u03a3i,\u03a3) is the matrix mean w.r.t. \u03b4. We note that this cost function is similar in spirit to the one in Harandi et al. [14] and can be considered an unsupervised version of it. In the next section we will show that the need for matrix whitening can be alleviated.\nThe Euclidean gradient w.r.t. Q of the cost function, used for the optimization, can be found in [15]."}, {"heading": "2.5 Symmetries and invariance properties", "text": "We now discuss the symmetries of our optimization problem and the invariance properties of our chosen metrics. These properties will enable us to significantly\n2Other metrics such as the Euclidean metric or the log-Euclidean metric [3] may also be used.\nsimplify our problem. For brevity we will state the results in terms of the AIRM, but the same holds true for the log-determinant metric.\nOur key observation stems from the fact that \u03b4r and \u03b4s are invariant to congruent transformations of the form X 7\u2192 PHXP for P \u2208 GLD(C) and PH = P > is the conjugate transpose [7]. However, since our discussion is limited to real matrices, we have\n\u03b42r (X,Y ) = \u03b4 2 r\n( P>XP,P>Y P ) . (10)\nfor X,Y \u2208 S+D and a real-valued invertible matrix P . This is a crucial point since the whitening matrix Z and, more importantly, the mixing matrix A, act on the covariance matrices in this way.\nProposition 2.2 Let \u039b = {\u039bi}ni=1 for \u039bi \u2208 S+n be a set of SPD matrices of size n\u00d7n and let \u03a3i = A\u039biA> for some real-valued invertible matrix A. Denoting the Riemannian mean of \u039b by \u039b\u0304, the Riemannian mean \u03a3\u0304 of the set \u03a3 = {\u03a3i}ni=1 is given by \u03a3\u0304 = A\u039b\u0304A>.\nProof. The Riemannian mean of the set \u039b is defined as \u039b\u0304 = argmin \u039b\u2208S+n\n\u2211 i \u03b4 2 r (\u039bi,\u039b).\nUsing the congruence invariance (Eq. (10)) we have \u03b42r (\u039bi,\u039b) = \u03b42r ( \u03a3i, A\u039bA >) and the result follows.\nUsing the above we obtain several useful equivalence relations.\nCorollary 2.3 The following expressions are equivalent:\n\u03b42r ( \u03a3\u0303i, I ) = \u03b42r ( \u03a3i, \u03a3\u0304 ) = \u03b42r ( \u0393i, \u0393\u0304 ) , (11)\nwhere \u0393i is the covariance matrix of the unmixed sources in the i-th epoch.\nWe have essentially shown that both the whitening operation and the mixing matrix A do not affect the distance between the covariance matrices of the original unmixed signals. We can then re-write our optimization problem as"}, {"heading": "Q\u0302 = argmin", "text": "Q\u2208G(D,m) \u2211 i \u03b42 ( Q>\u03a3\u0303iQ, I ) = argmin Q\u2208G(D,m) \u2211 i \u03b42 ( Q>A\u0393iA >Q,Q>A\u0393\u0304A>Q )\n= argmin Q\u2032\u2208G(D,m) \u2211 i \u03b42 ( Q\u2032>\u0393iQ \u2032, Q\u2032>\u0393\u0304Q\u2032 ) . (12)\nOne may remark that A>Q no longer has orthonormal columns and so does not belong to the Grassmann manifold. Indeed this is true. The final transition is due to the observation that the solution to our optimization problem is not unique. Rather, since we are interested in recovering the stationary subspace and not the exact sources themselves, the solution is invariant to any transformation (e.g., subspace scaling and rotation) acting within each of the s- and n-spaces separately. Furthermore, we have chosen W \u2208 Q\u22a5, and so the s-space is orthogonal to the n-space. Now, choosing orthogonal bases within\neach of the subspaces we may restrict ourselves to orthogonal mixing matrices A and find a transformation Q which lies in the Grassman manifold.\nThe final result is quite remarkable. First, it shows that our problem is essentially agnostic to the mixing matrix.Secondly, it eliminates the need to pre-whiten the matrices. This is useful in situations where it is not appropriate to whiten the data in advance, for example, when working with nearly illconditioned matrices. Furthermore, this eliminates any error that may be caused by inaccurate estimation of the matrix mean.\nIn conclusion, we have two variations of gaSSA given in the first and last terms of Eq. (12). The difference between the two is whether or not the input covariance matrices are whitened. Our analysis shows that whitening does not improve performance, and may in fact lead to a degradation of the results in certain cases. So, we claim that it is in general preferable not to whiten the matrices. In terms of the chosen metric, we do not expect a significant difference when using \u03b4r vs. \u03b4s. In the following section we will present experimental evidence to support these claims."}, {"heading": "3 Experimental results", "text": "In this section we present experimental results on synthetic and data taken from a real BCI experiment. We compare the performance of gaSSA to the existing SSA and investigate the effects of matrix whitening and the choice of metric."}, {"heading": "3.1 Toy data", "text": "For our first experiment we generated data following the SSA model as a mixture of stationary and non-stationary sources. To generate non-stationarity in the data we used a slightly modified version of the scheme provided in the SSA toolbox [22] and detailed in its user manual. Here we bring only a brief description:\nThe elements of the mixing matrix A are chosen uniformly from the range [\u22120.5, 0.5] and its columns are normalized to 1. The distribution of the s-sources is constant over all epochs, namely ss(t) \u223c N (0,\u039bs). In the SSA toolbox, \u039bs is taken to be the identity matrix, however, we choose \u039bs to be a random matrix of the form \u039bs = B\u0393B> for an orthogonal matrix B and diagonal matrix \u0393.\nThe n-sources are correlated with the s-sources, and for the i-th epoch \u03c4i = [t0(i), . . . , t0(i) + T ] they are given by sn(t) = Ciss(t) + Y n(t) for t \u2208 \u03c4i, where Ci \u2208 R(D\u2212m)\u00d7m and Y n(t) \u223c N (\u00b5i,\u039bni ). The covariance matrices \u039bni are generated for each epoch in the same way as \u039bs.\nSo, the covariance matrix of the (unmixed) sources in the i-th epoch may be written as \u039bi = cov ([ ss(t) sn(t) ]) = [ \u039bs (Ci\u039b s) >\nCi\u039b s Ci\u039b sC>i + \u039b n i\n] . (13)\nUsing the data generated by the scheme above we compared the performance of our method to that of SSA. We used the AIRM and log-determinant metric both with and without matrix whitening.\nAs a performance measure we used the distance between the estimated nspace A\u0302n and the true n-space An. This is owing to the fact that, as discussed in von B\u00fcnau et al. [31], the n-space and s-sources are identifiable, while the s-space and n-sources are not. To illustrate this, note that to be stationary, the s-sources must consist strictly of stationary sources, while the n-sources will remain non-stationary even if they include a mixture of stationary signals. The distance between sub-spaces is computed using \u03b4G , the metric on the Grassmann manifold [1]. Shortly, this metric is based on the principal angles between the two spaces.\nWe generated 50 epochs of length T = 250 for several values of D and m. For each pair (D,m) we conducted the experiment 25 times. At each iteration the optimization procedure was restarted 5 times with different initial guesses and the transformation matrix which obtained the lowest cost was selected.\nDue to length restrictions we describe only the results for D = 19 and m = 12. Other choices of parameters exhibited the same behavior. The n-space errors were identical for \u03b4r and \u03b4s (0.0067 \u00b1 0.0001). They were consistently lower than SSA (0.0115\u00b1 0.0004). For both metrics the scheme without matrix whitening performed slightly better than those including whitening (0.0063\u00b1 0.0001 vs. 0.0067 \u00b1 0.0001). These results are consistent with the analysis of Section 2.5."}, {"heading": "3.2 Brain-computer interface", "text": "Next, we applied our method to data taken from the BCI competition IV dataset II. This dataset contains motor imagery (MI) EEG signals affected by eye movement artifacts. It was collected in a multi-class setting, with the subjects performing more than 2 different MI tasks. However, as in Lotte and Guan [21], we evaluate our algorithms on two-class problems by selecting only signals of left- and right-hand MI trials.\nWe applied the same pre-processing as described in Lotte and Guan [21]. EEG signals were band-pass filtered in 8\u2212 30 Hz, using a 5th order Butterworth filter. For each trial, we extracted features from the time segment located from 0.5s to 2.5s after the cue instructing the subject to perform MI.\nThe data was initially divided into two parts: a training data set and a test data set. Similarly to von B\u00fcnau et al. [31] the first 20% of the test trials were set aside for adaptation. The aim of the adaptation part is to mitigate any non-stationarities between the test and the training session. We then learned the s-space in an unsupervised manner over the training and adaptation part. As before, our method was reinitialized 5 times and the transformation attaining the lowest cost was chosen.\nThe performance was measured by means of the classification rate on the test set. We used the following naive classifier, referred to as minimum distance to the mean (MDM) in Barachant et al. [5]: Using the labels of the training set, we compute the mean (in the s-space) for each of the two classes. Then, we classify the compressed covariance matrices in the test set according to their distance to the class means; each matrix is assigned the class to which it is closer.\nThe original data is comprised of 22 signals. Since the true number of stationary signals is unknown, we repeated the experiment for several values in the range m \u2208 [10, 18]. Due to lack of space, we bring the results only for the values m = 10 and m = 14, the former being an example of challenging task and the latter an example of an intermediate one. The results for the nine subjects in the dataset are summarized in Table 1. The full results can be found in the supplementary material.\nThe results show that our method outperforms SSA for most subjects. As predicted, the methods without pre-whitening generally performed better than those which included whitening of the covariance matrices. In this more complex setting, more accurate estimation of the mixing matrix does not guarantee better classification. In terms of the metric, we see that \u03b4r and \u03b4s perform roughly the same."}, {"heading": "4 Conclusion", "text": "We presented a covariance-based method for unsupervised stationary subspace analysis. The problem was phrased in terms of the distance between matrices and not, as in SSA, using the divergence between probability distributions. Owing to the symmetries of the problem and the invariance properties of the geometries, we derived useful equivalence relations. Experiments on both synthetic and BCI data supported our theoretical analysis and showed that our method outperforms SSA."}], "references": [{"title": "Riemannian geometry of Grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Applicandae Mathematica,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Clustering with Bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Multiclass brain\u2013computer interface classification by riemannian geometry", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Classification of covariance matrices using a riemannian-based kernel for bci applications", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Positive Definite Matrices", "author": ["R. Bhatia"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Manopt, a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Riemannian sparse coding for positive definite matrices", "author": ["A. Cherian", "S. Sra"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Efficient similarity search for covariance matrices via the Jensen-Bregman log-det divergence", "author": ["A. Cherian", "S. Sra", "A. Banerjee", "N. Papanikolopoulos"], "venue": "IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Speech enhancement for non-stationary noise environments", "author": ["I. Cohen", "B. Berdugo"], "venue": "Signal processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Principal geodesic analysis for the study of nonlinear statistics of shape", "author": ["T.P. Fletcher", "C. Lu", "S.M. Pizer", "S. Joshi"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "From manifold to manifold: geometryaware dimensionality reduction for spd matrices", "author": ["M. Harandi", "M. Salzmann", "R. Hartley"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Intrinsic PCA for SPD matrices", "author": ["I. Horev", "F. Yger", "M. Sugiyama"], "venue": "In Asian Conference on Machine Learning (ACML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Independent Component Analysis, volume 46", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Nonlinear Time Series Analysis, volume 7", "author": ["H. Kantz", "T. Schreiber"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Low-rank kernel learning with Bregman matrix divergences", "author": ["B. Kulis", "M.A. Sustik", "I.S. Dhillon"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Stochastic covariance compression", "author": ["M.J. Kusner", "N.I. Kolkin", "S. Tyree", "K.Q. Weinberger"], "venue": "arXiv preprint arXiv:1412.1740,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A well-conditioned estimator for large-dimensional covariance matrices", "author": ["O. Ledoit", "M. Wolf"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Regularizing common spatial patterns to improve bci designs: unified theory and new algorithms", "author": ["F. Lotte", "C. Guan"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "The stationary subspace analysis toolbox", "author": ["J.S. M\u00fcller", "P. v. B\u00fcnau", "F.C. Meinecke", "F.J. Kir\u00e1ly", "K.-R. M\u00fcller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Sided and symmetrized Bregman centroids", "author": ["F. Nielsen", "R. Nock"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A Riemannian framework for tensor computing", "author": ["X. Pennec", "P. Fillard", "N. Ayache"], "venue": "International Journal of Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Brain-computer interfacing in discriminative and stationary subspaces", "author": ["W. Samek", "K.-R. M\u00fcller", "M. Kawanabe", "C. Vidaurre"], "venue": "In 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Positive definite matrices and the s-divergence", "author": ["S. Sra"], "venue": "arXiv preprint arXiv:1110.1773,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Machine Learning in Non-Stationary Environments", "author": ["M. Sugiyama", "M. Kawanabe"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Region covariance: A fast descriptor for detection and classification", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Finding stationary subspaces in multivariate time series", "author": ["P. von B\u00fcnau", "F.C. Meinecke", "F.C. Kir\u00e1ly", "K.-R. M\u00fcller"], "venue": "Physical Review Letters,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Stationary subspace analysis", "author": ["P. von B\u00fcnau", "F.C. Meinecke", "K.-R. M\u00fcller"], "venue": "In Independent Component Analysis and Signal Separation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Stationary common spatial patterns: towards robust classification of non-stationary eeg signals", "author": ["W. Wojcikiewicz", "C. Vidaurre", "M. Kawanabe"], "venue": "In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "When the stationarity assumption is violated, as is often the case in real-world applications such as speech enhancement [11] or neurological data analysis [25], specialized machine learning methods must be developed in order to maintain adequate prediction capabilities.", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "When the stationarity assumption is violated, as is often the case in real-world applications such as speech enhancement [11] or neurological data analysis [25], specialized machine learning methods must be developed in order to maintain adequate prediction capabilities.", "startOffset": 156, "endOffset": 160}, {"referenceID": 25, "context": "A relatively well-studied non-stationary setting is covariate-shift [26], in which the input distribution changes but the conditional distribution of the outputs does not.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "The problem of covariate-shift has received growing attention in recent years, and many theoretical and practical aspects have been addressed (see [28] for an in-depth exploration of this topic).", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "In the context of brain computer interface (BCI) data analysis, two such note-worthy methods are stationary subspace analysis (SSA) [30] and stationary common spacial patterns (sCSP) [32].", "startOffset": 132, "endOffset": 136}, {"referenceID": 31, "context": "In the context of brain computer interface (BCI) data analysis, two such note-worthy methods are stationary subspace analysis (SSA) [30] and stationary common spacial patterns (sCSP) [32].", "startOffset": 183, "endOffset": 187}, {"referenceID": 15, "context": "Similar in spirit to independent component analysis (ICA) [16], SSA statistically models the data as a mixture of stationary and non-stationary signals.", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": "Although SSA is essentially an unsupervised method, variations of it exist which are useful for supervised tasks such as classification [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": "Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6].", "startOffset": 193, "endOffset": 197}, {"referenceID": 23, "context": "Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6].", "startOffset": 213, "endOffset": 217}, {"referenceID": 5, "context": "Covariance matrices have gained increasing attention in recent years, and are now commonly used in many machine learning and signal processing applications such as computer vision applications [29], brain imaging [24] and BCI data analysis [6].", "startOffset": 240, "endOffset": 243}, {"referenceID": 6, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 256, "endOffset": 259}, {"referenceID": 18, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 277, "endOffset": 281}, {"referenceID": 12, "context": "Their rich mathematical structure has been extensively studied [7], and advances in optimization methods on matrix manifolds in recent years have motivated the development of geometric methods for various machine learning tasks such as dictionary learning [9], metric learning [19] and dimensionality reduction [13].", "startOffset": 311, "endOffset": 315}, {"referenceID": 29, "context": "a review of the original SSA model and framework [30].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "First, the s-sources are stationary only in the weak (or wide) sense [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "1This is also common practice in ICA [16]", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "Under this assumption the KL divergence is a Bregman matrix divergence [4] between covariance matrices.", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "DKL is obtained for \u03a6(X) = \u2212 log detX, so it is often called the log-determinant divergence [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "Bregman matrix divergences are useful in machine learning and have a number of useful properties [18], such as linearity and convexity in the first argument (and, in the case of the KL divergence, also in the second).", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "Subsequently, symmetrized versions of the Bregman matrix divergence, in particular Jensen-Bregman divergences, have been studied in recent years [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "and is called the Jensen-Bregman log-determinant (JBLD) divergence [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "The JBLD has many favorable properties (see [27]), primarily that its square root comprises a metric on the SPD matrix manifold.", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "Moreover, in Sra [27] it has been shown that it is a close approximation to the affine invariant Riemannian metric (AIRM) [7] and shares many of its mathematical properties.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "Moreover, in Sra [27] it has been shown that it is a close approximation to the affine invariant Riemannian metric (AIRM) [7] and shares many of its mathematical properties.", "startOffset": 122, "endOffset": 125}, {"referenceID": 26, "context": "In the context of SPD matrices, the JBLD is referred to as the symmetric Stein divergence or the (square of the) log-determinant metric [27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "So, we may optimize Q over the Grassmann manifold [12], G = { span(Q) : Q \u2208 RD\u00d7m, Q>Q = I } , the set of all m-dimensional linear subspaces of RD\u00d7D.", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "[2] and implemented efficiently in Boumal et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Given the strong relation between the log-determinant metric and the AIRM [7], a natural progression is to incorporate the AIRM into the cost function.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "Despite its simplicity, the Euclidean geometry has several drawbacks and is not always well suited for SPD matrices [13, 3].", "startOffset": 116, "endOffset": 123}, {"referenceID": 2, "context": "Despite its simplicity, the Euclidean geometry has several drawbacks and is not always well suited for SPD matrices [13, 3].", "startOffset": 116, "endOffset": 123}, {"referenceID": 2, "context": "artifact referred to as the swelling effect [3], for a task as simple as averaging two matrices, it may occur that the determinant of the average is larger than any of the two matrices.", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "[13], is the fact that this geometry forms a non-complete space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Of the possible Riemannian distances, the AIRM, due to its favorable mathematical properties, is widely used in many applications (see, for example [13, 24]).", "startOffset": 148, "endOffset": 156}, {"referenceID": 23, "context": "Of the possible Riemannian distances, the AIRM, due to its favorable mathematical properties, is widely used in many applications (see, for example [13, 24]).", "startOffset": 148, "endOffset": 156}, {"referenceID": 13, "context": "[14] and can be considered an unsupervised version of it.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Q of the cost function, used for the optimization, can be found in [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "2Other metrics such as the Euclidean metric or the log-Euclidean metric [3] may also be used.", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Our key observation stems from the fact that \u03b4r and \u03b4s are invariant to congruent transformations of the form X 7\u2192 PXP for P \u2208 GLD(C) and P = P > is the conjugate transpose [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 21, "context": "To generate non-stationarity in the data we used a slightly modified version of the scheme provided in the SSA toolbox [22] and detailed in its user manual.", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "[31], the n-space and s-sources are identifiable, while the s-space and n-sources are not.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The distance between sub-spaces is computed using \u03b4G , the metric on the Grassmann manifold [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "However, as in Lotte and Guan [21], we evaluate our algorithms on two-class problems by selecting only signals of left- and right-hand MI trials.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "We applied the same pre-processing as described in Lotte and Guan [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "[31] the first 20% of the test trials were set aside for adaptation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5]: Using the labels of the training set, we compute the mean (in the s-space) for each of the two classes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Since the true number of stationary signals is unknown, we repeated the experiment for several values in the range m \u2208 [10, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Since the true number of stationary signals is unknown, we repeated the experiment for several values in the range m \u2208 [10, 18].", "startOffset": 119, "endOffset": 127}], "year": 2016, "abstractText": "In many real-world applications observed data exhibits non-stationarity, i.e., its distribution changes over time. One approach to handling nonstationarity is to remove or minimize it before attempting to analyze the data. In the context of brain computer interface (BCI) data analysis this may be done by means of stationary subspace analysis (SSA). The SSA method finds a matrix that projects the data onto a stationary subspace by optimizing a cost function based on a matrix divergence. In this work we present an alternative method for SSA based on a symmetrized version of this matrix divergence. We show that doing so frames the problem in terms of distances between symmetric positive definite (SPD) matrices, suggesting a geometric interpretation of the problem. Stemming from this geometric viewpoint, we introduce and analyze a method which utilizes the geometry of the SPD matrix manifold and the invariance properties of its metrics. We demonstrate the usefulness of our method in experiments on both synthesized and real-world data.", "creator": "LaTeX with hyperref package"}}}