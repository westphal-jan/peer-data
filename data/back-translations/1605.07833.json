{"id": "1605.07833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Effective Blind Source Separation Based on the Adam Algorithm", "abstract": "The proposed approach is based on a novel stochastic optimization approach, the Adaptive Moment Estimation (Adam) algorithm. The proposed BSS solution can benefit from the excellent properties of the Adam approach. To derive the new learning rule, the Adam algorithm is introduced into the derivation of cost function maximization in the standard InfoMax algorithm. Natural gradient adjustment is also considered. Finally, some experimental results show the effectiveness of the proposed approach.", "histories": [["v1", "Wed, 25 May 2016 11:40:46 GMT  (681kb,D)", "https://arxiv.org/abs/1605.07833v1", "This paper has been presented at the 26-th Italian Workshop on Neural Networks (WIRN2016) May 18-20, Vietri sul Mare, Salerno, Italy. It will be published as a chapter in a book of the the Springer Smart Innovation, Systems and Technologies series"], ["v2", "Mon, 26 Sep 2016 14:17:27 GMT  (1769kb,D)", "http://arxiv.org/abs/1605.07833v2", "Revised version after review process. This paper has been presented at the 26-th Italian Workshop on Neural Networks (WIRN2016) May 18-20, Vietri sul Mare, Salerno, Italy. It will be published soon as a chapter in a book of the the Springer Smart Innovation, Systems and Technologies series"]], "COMMENTS": "This paper has been presented at the 26-th Italian Workshop on Neural Networks (WIRN2016) May 18-20, Vietri sul Mare, Salerno, Italy. It will be published as a chapter in a book of the the Springer Smart Innovation, Systems and Technologies series", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michele scarpiniti", "simone scardapane", "danilo comminiello", "raffaele parisi", "aurelio uncini"], "accepted": false, "id": "1605.07833"}, "pdf": {"name": "1605.07833.pdf", "metadata": {"source": "CRF", "title": "Effective Blind Source Separation Based on the Adam Algorithm", "authors": ["Michele Scarpiniti", "Simone Scardapane", "Danilo Comminiello", "Raffaele Parisi", "Aurelio Uncini"], "emails": ["aurelio.uncini}@uniroma1.it"], "sections": [{"heading": null, "text": "Keywords: Blind Source Separation, Stochastic Optimization, Adam algorithm, InfoMax algorithm, Natural gradient."}, {"heading": "1 Introduction", "text": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20]. The problem is to recover original and unknown sources from a set of mixtures recorded in an unknown environment. The term blind refers to the fact that both the sources and the mixing environment are unknown.\nSeveral well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17]. Different approaches were proposed to solve BSS in linear and instantaneous environment. Some of these approaches perform separation by using high order statistics (HOS) while others exploit information theoretic (IT) measures [6]. One of the well-know algorithms in this latter class is the InfoMax one proposed by Bell and Sejnowski in [3]. The InfoMax algorithm is based on the maximization of the joint entropy of the output of a single layer neural network and it is very efficient and easy to implement since the gradient of the joint entropy can be evaluated simply in a closed form. Moreover, in order to avoid numerical instability, a natural gradient modification to InfoMax algorithm has also been proposed [1,6]. ar X iv :1\n60 5.\n07 83\n3v 2\n[ cs\n.L G\n] 2\n6 Se\np 20\nUnfortunately, all these solutions perform slowly when the number of the original sources to be separated is high and/or bad scaled. The separation becomes impossible if the number of sources is equal or greater than ten. In addition, the convergence speed problem worsen in the case of additive sensor noise to mixtures or when the mixing matrix is close to be ill-conditioned. However, specially when working with speech and audio signals, fast convergence speed is an important task to be performed. Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].\nRecently, a novel algorithm for gradient based optimization of stochastic cost functions has been proposed by Kingma and Ba in [12]. This algorithm is based on the adaptive estimates of the first and second order moments of the gradient, and for this reason has been called the Adaptive Moment Estimation (Adam) algorithm. The authors have demonstrated in [12] that Adam is easy to implement, computationally efficient, invariant to diagonal rescaling of the gradients and well suited for problems with large data and parameters.\nThe Adam algorithm combines the advantages of other state-of-the-art optimization algorithms, like AdaGrad [9] and RMSProp [19], outperforming the limitations of these algorithms. In addition, Adam can be related to the natural gradient (NG) adaptation [1], employing a preconditioning that adapts to the geometry of data.\nIn this paper we propose a modified InfoMax algorithm based on the Adam algorithm [12] for the solution of BSS problems. We derive the proposed modified algorithm by using the Adam algorithm instead of the standard stochastic gradient ascent rule. It is shown that the novel algorithm has a faster convergence speed with respect to the standard InfoMax algorithm and usually also reaches a better separation. Some experimental results, evaluated in terms of the Amari Performance index (PI) [6], show the effectiveness of the proposed idea.\nThe rest of the paper is organized as follows. In Section 2 we briefly introduce the problem of BSS. Then, we give some details on the Adam algorithm in Section 3. The main novelty of this paper, the extension of InfoMax algorithm with Adam is provided in Section 4. Finally, we validate our approach in Section 5. We conclude with some final remarks in Section 6."}, {"heading": "2 The Blind Source Separation Problem", "text": "Let us consider a set of N unknown and statistically independent sources denoted as s[n] = [s1[n], . . . ,sN [n]]T , such that the components si[n] are zero-mean and mutually independent. Signals received by an array of M sensors are denoted by x[n] = [x1[n], . . . ,xM[n]]\nT and are called mixtures. For simplicity, here we consider the case of N = M.\nIn the case of a linear and instantaneous mixing environment, the mixture can be described in a matrix form as\nx[n] = As[n]+v[n], (1)\nwhere the matrix A = (ai j) collects the mixing coefficients ai j, i, j = 1,2, . . . ,N and v[n] = [v1[n], . . . ,vN [n]]T is a noise vector, with correlation matrix Rv = \u03c32v I and noise variance \u03c32v .\nThe separated signals u[n] are obtained by a separating matrix W = (wi j) as described by the following equation\nu[n] = Wx[n]. (2)\nThe transformation in (2) is such that u[n] = [u1[n], . . . ,uN [n]]T has components uk[n], k = 1,2, . . . ,N, that are as independent as possible.\nMoreover, due to the well-known permutation and scaling ambiguity of the BSS problem, the output signals u[n] can be expressed as\nu[n] = PDs[n], (3)\nwhere P is an N\u00d7N permutation matrix and D is an N\u00d7N diagonal scaling matrix.\nThe weights wi j can be adapted by maximizing or minimizing some suitable cost function [6,5]. A particularly good approach is to maximize the joint entropy of a single layer neural network [3], as shown in Figure 1, leading to the Bell and Sejnowski InfoMax algorithm. In this network each output yi[n] is a nonlinear transformation of each signal ui[n]:\nyi[n] = hi (ui[n]) . (4)\nEach function hi(\u00b7) is known as activation function (AF). With reference to Figure 1, using the equation relating the probability density function (pdf) of a random variable px (x) and nonlinear transformation of it py (y) [14], the joint entropy H (y) of the network output y[n] can be evaluated as\nH (y)\u2261\u2212E {ln py (y)}= H (x)+ lndetW+ N\n\u2211 i=1\nln \u2223\u2223h\u2032i\u2223\u2223 , (5)\nwhere h\u2032i is the first derivative of the i-th AF with respect its input ui[n]. Evaluating the gradient of (5) with respect to the separating parameters W, after some not too complicated manipulations, leads to\n\u2207WH (y) = W\u2212T +\u03a8xT [n], (6)\nwhere (\u00b7)\u2212T denotes the transpose of the inverse and \u03a8 = [\u03a81, . . . ,\u03a8N ]T is a vector collecting the terms \u03a8k = h\u2032\u2032k/h \u2032 k, defined as the ratio of the second and the first derivatives of the AFs. In order to avoid the numerical problems of the matrix inversion in (6) and the possibility to remain blocked in a local minimum, Amari has introduced the Natural Gradient (NG) adaptation [1] that overcomes such problems. The NG adaptation rule, can be obtained simply by right multiplying the stochastic gradient for the term WT W. Hence, after multiplying (6) for this term, the NG InfoMax is simply\n\u2207WH (y) = ( I+\u03a8uT [n] ) W. (7)\nRegarding the selection of the AFs shape, there are several alternatives. However, especially in the case of audio and speech signals, a good nonlinearity is represented by the tanh(\u00b7) function. With this choice, the vector \u03a8 in (6) and (7) is simply evaluated as \u22122y[n].\nIn summary, using the tanh(\u00b7) AF, the InfoMax and natural gradient InfoMax algorithms are described by the following learning rules:\nWt = Wt\u22121 +\u00b5 ( W\u2212Tt\u22121\u2212y[n]x T [n] ) , (8)\nWt = Wt\u22121 +\u00b5 ( I\u2212y[n]uT [n] ) Wt\u22121, (9)\nwhere \u00b5 is the learning rate or step-size."}, {"heading": "3 The Adam Algorithm", "text": "Le us denote with f (\u03b8) a noisy cost function to be minimized (or maximized) with respect to the parameters \u03b8 . The problem is considered stochastic for the random nature of data samples or for inherent function noise. In the following, the noisy gradient vector at time t of the cost function f (\u03b8) with respect to the parameters \u03b8 , will be denoted with gt = \u2207\u03b8 ft (\u03b8).\nThe Adam algorithm performs the gradient descent (or ascent) optimization by evaluating the moving averages of the noisy gradient mt and the square gradient vt [12]. These moment vectors are updated by using two scalar coefficients \u03b21 and \u03b22 that control the exponential decay rates:\nmt = \u03b21mt\u22121 +(1\u2212\u03b21)gt , (10) vt = \u03b22vt\u22121 +(1\u2212\u03b22)gt gt , (11)\nwhere \u03b21,\u03b22 \u2208 [0,1) and denotes the element-wise multiplication, while m0 and v0 are initialized as zero vectors. These vectors represent the mean and the uncentered\nvariance of the gradient vector gt . Since the estimates of mt and vt are biased towards zero, due to their initialization, a bias correction is computed on these moments\nm\u0302t = mt\n1\u2212\u03b2 t1 , (12)\nv\u0302t = vt\n1\u2212\u03b2 t2 . (13)\nThe vector v\u0302t represents an approximation of the diagonal of the Fisher information matrix [15]. Hence Adam can be related to the natural gradient algorithm [1].\nFinally, the parameter vector \u03b8 t at time t, is updated by the following rule\n\u03b8 t = \u03b8 t\u22121\u2212\u03b7 m\u0302t\u221a v\u0302t + \u03b5 , (14)\nwhere \u03b7 is the step size and \u03b5 is a small positive constant used to avoid the division for zero. In the gradient ascent, the minus sign in (14) is substituted with the plus sign."}, {"heading": "4 Modified InfoMax algorithm", "text": "In this section, we introduce the modified Bell and Sejnowski InfoMax algorithm, based on the Adam optimization method. Since Adam algorithm uses a vector of parameters, we perform a vectorization of the gradient (6) or (7)\nw = vec(\u2207WH (y)) \u2208 RN 2\u00d71, (15)\nwhere vec(A) is the vectorization operator, that forms a vector by stacking the columns of the matrix A below one another. The gradient vector is evaluated on a number of blocks NB extracted from the signals.\nAt this point, the mean and variance vectors are evaluated from the knowledge of the gradient wt at time t by using equations (10)\u2013(13). Then, using (14), the gradient vector (15) is updated for the maximization of the joint entropy by\nwt = wt\u22121 +\u03b7 m\u0302t\u221a v\u0302t + \u03b5 . (16)\nFinally, the vector wt is reshaped in matrix form, by\nWt = mat(wt) \u2208 RN\u00d7N , (17)\nwhere mat(a) reconstructs the N\u00d7N matrix by unstacking the columns from the vector a. The whole algorithm is in case repeated for a certain number of epochs Nep.\nThe pseudo-code of the modified InfoMax algorithm with Adam, called here Adam InfoMax, is described in Algorithm 1.\nAlgorithm 1: Pseudo-code for the Adam InfoMax algorithm. Data: Mixture signals x[n], \u03b7 , \u03b21, \u03b22, \u03b5 , NB, Nep. 1 Initialization: W0 = I, m0 = 0, v0 = 0, w0 = 0 2 P = NBNep 3 for t=1:P do 4 Extract the t-th block xt from x[n] 5 ut = Wt\u22121xt 6 yt = tanh(ut) 7 Update gradient \u2207WH (yt) in (6) or (7) 8 wt = vec(\u2207WH (yt)) 9 mt = \u03b21mt\u22121 +(1\u2212\u03b21)wt\n10 vt = \u03b22vt\u22121 +(1\u2212\u03b22)wt wt 11 m\u0302t = mt1\u2212\u03b2 t1 12 v\u0302t = vt1\u2212\u03b2 t2 13 wt = wt\u22121 +\u03b7 m\u0302t\u221av\u0302t+\u03b5 14 Wt = mat(wt) 15 end\nResult: Separated signals: u[n] = WPx[n]"}, {"heading": "5 Experimental Results", "text": "In this section, we propose some experimental results to demonstrate the effectiveness of the proposed idea. We perform separation of mixtures of both synthetic and realworld data. The results are evaluated in terms of the Amari Performance Index (PI) [6], defined as\nPI = 1\nN (N\u22121)\nN\n\u2211 i=1  N\u2211 k=1 |qik| max\nj\n\u2223\u2223qi j\u2223\u2223 \u22121 +  N\u2211 k=1 |qki| max\nj\n\u2223\u2223q ji\u2223\u2223 \u22121  , (18)\nwhere qi j are the elements of the matrix Q = WA. This index is close to zero if the matrix Q is close to the product of a permutation matrix and a diagonal scaling matrix.\nThe performances of the proposed algorithm were also compared with the standard InfoMax algorithm [3] and the Momentum InfoMax described in [13]. In this last algorithm, the \u03b1 parameter is set to 0.5 in all experiments.\nIn a first experiment, we perform the separation of five mixtures obtained as linear combination of the following bad-scaled independent sources\ns1[n] = 10\u22126 \u00b7 sin(350n)sin(60n) , s2[n] = 10\u22125 \u00b7 tri(70n) , s3[n] = 10\u22124 \u00b7 sin(800n)sin(80n) , s4[n] = 10\u22125 \u00b7 cos(400n+4cos(60n)) , s5[n] = \u03be [n],\nwhere tri(\u00b7) denotes a triangular waveform and \u03be [n] is a uniform noise in the range [\u22121,1]. Each signal is composed of L = 30,000 samples. The mixing matrix is a 5\u00d75 Hilbert matrix, which is extremely ill-conditioned. All simulations have been performed by MATLAB 2015a, on an Intel Core i7 3.10 GHz processor at 64 bit with 8 GB of RAM. Parameters of the algorithms have been found heuristically.\nWe perform separation by the Adam modification of the standard InfoMax algorithm, with gradient in (6). We use a block length of B = 30 samples (hence NB = bL/Bc= 1,000), while the other parameters are set as: Nep = 200, \u03b21 = 0.5, \u03b22 = 0.75, \u03b5 = 10\u22128, \u03b7 = 0.001 and the learning rate of the standard InfoMax and the Momentum InfoMax to \u00b5 = 5 \u00b710\u22125. Performance in terms of the PI in (18) is reported in Figure 2, that clearly shows the effectiveness of the proposed idea.\nA second and third experiments are performed on speech audio signals sampled at 8 kHz. Each signal is composed of L = 30,000 samples. In the second experiment a male and a female speech are mixed with a 2\u00d72 random matrix with entries uniformly distributed in the interval [\u22121,1] while in the third one, two male and two female speeches are mixed with a 4\u00d7 4 ill-conditioned Hilbert matrix. In addition, an additive white noise with 30 dB of SNR is added to the mixtures in both cases.\nWe perform separation by the Adam modification of the NG InfoMax algorithm, with gradient in (7). We use a block length of B = 30 samples, while the other parameters are set as: Nep = 100, \u03b21 = 0.9, \u03b22 = 0.999, \u03b5 = 10\u22128, \u03b7 = 0.001 and the learning rate of the standard InfoMax and the Momentum InfoMax to \u00b5 = 0.001. Performances in terms of the PI, for the second and third experiments, are reported in Figures 3a and 3b, respectively, that clearly show also in these cases the effectiveness of the proposed idea. In particular, Figure 3b confirms that the separation obtained by using the Adam InfoMax algorithm in the third experiment is quite satisfactory, while the standard and the Momentum InfoMax give worse solutions.\nFinally, a last experiment is performed on real data. We used an EEG signal recorded according the 10-20 system, consisting of 19 signals with artifacts. ICA is a common approach to deal with the problem of artifact removal from EEG [11]. We use a block length of B = 30 samples, while the other parameters are set as: Nep = 270, \u03b21 = 0.9,\n\u03b22 = 0.999, \u03b5 = 10\u22128, \u03b7 = 0.01 and the learning rate of the standard InfoMax and the Momentum InfoMax to \u00b5 = 10\u22126. Since we used real data and the mixing matrix A is not available, the PI cannot be evaluated. Hence, we decided to evaluate the performance by the norm of the gradient of the cost function. As it can be seen from Figure 4, also in this case the Adam InfoMax algorithm achieves better results in a smaller number of iterations with respect to the compared algorithms."}, {"heading": "6 Conclusions", "text": "In this paper a modified InfoMax algorithm for the blind separation of independent sources, in a linear and instantaneous environment, has been introduced. The proposed approach is based on a novel and advanced stochastic optimization method known as Adam and it can benefit from the excellent properties of the Adam approach. In particular, it is easy to implement, computationally efficient, and it is well suited when the number of sources is high and bad-scaled, the mixing matrix is close to be ill-conditioned and some additive noise is considered. Some experimental results, evaluated in terms\nof the Amari Performance Index and compared with other state-of-the-art approaches, have shown the effectiveness of the proposed approach."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation 10(3), 251\u2013 276", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "The fundamental limitation of frequency domain blind source separation for convolutive mixtures of speech", "author": ["S. Araki", "R. Mukai", "S. Makino", "T. Nishikawa", "H. Saruwatari"], "venue": "IEEE Transactions on Speech and Audio Processing 11(2), 109\u2013116", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "An information-maximisation approach to blind separation and blind deconvolution", "author": ["A.J. Bell", "T.J. Sejnowski"], "venue": "Neural Computation 7(6), 1129\u20131159", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "On convolutive blind source separation in a noisy context and a total variation regularization", "author": ["T.Z. Boulmezaoud", "M. El Rhabi", "H. Fenniri", "E. Moreau"], "venue": "Proc. of IEEE Eleventh International Workshop on Signal Processing Advances in Wireless Communications (SPAWC2010). pp. 1\u20135. Marrakech", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Blind source separation and independent component analysis: a review", "author": ["S. Choi", "A. Cichocki", "H.M. Park", "S.Y. Lee"], "venue": "Neural Information Processing - Letters and Reviews 6(1), 1\u201357", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive Blind Signal and Image Processing", "author": ["A. Cichocki", "S. Amari"], "venue": "John Wiley", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Handbook of Blind Source Separation", "author": ["P. Comon", "Jutten", "C. (eds."], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaled natural gradient algorithm for instantaneous and convolutive blind source separation", "author": ["S.C. Douglas", "M. Gupta"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP2007). vol. 2, pp. 637\u2013640", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "H.E.", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12(7), 2121\u20132159", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Adaptive Filtering, vol", "author": ["Haykin", "S. (ed."], "venue": "2: Blind Source Separation. Wiley", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Wavelet-ICA methodology for efficient artifact removal from electroencephalographic recordings", "author": ["G. Inuso", "F. La Foresta", "N. Mammone", "F.C. Morabito"], "venue": "Proc. of International Joint Conference on Neural Networks", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive improved natural gradient algorithm for blind source separation", "author": ["J.Q. Liu", "D.Z. Feng", "W.W. Zhang"], "venue": "Neural Computation 21(3), 872\u2013889", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Probability, Random Variables and Stochastic Processes", "author": ["A. Papoulis"], "venue": "McGraw-Hill", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "International Conference on Learning Representations", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized splitting functions for blind separation of complex signals", "author": ["M. Scarpiniti", "D. Vigliano", "R. Parisi", "A. Uncini"], "venue": "Neurocomputing 71(10-12), 2245\u20132270", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Blind separation of convolved mixtures in the frequency domain", "author": ["P. Smaragdis"], "venue": "Neurocomputing 22(21\u201334)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Step-size control in blind source separation", "author": ["P. Thomas", "G. Allen", "N. August"], "venue": "International Workshop on Independent Component Analysis and Blind Source Separation. pp. 509\u2013514", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Flexible nonlinear blind signal separation in the complex domain", "author": ["D. Vigliano", "M. Scarpiniti", "R. Parisi", "A. Uncini"], "venue": "International Journal of Neural Systems 18(2), 105\u2013122", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 6, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 9, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 4, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 15, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 18, "context": "Blind Source Separation (BSS) is a well-known and well-studied field in the adaptive signal processing and machine learning [6,7,10,5,16,20].", "startOffset": 124, "endOffset": 140}, {"referenceID": 6, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 86, "endOffset": 91}, {"referenceID": 2, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 86, "endOffset": 91}, {"referenceID": 1, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 153, "endOffset": 161}, {"referenceID": 3, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 153, "endOffset": 161}, {"referenceID": 16, "context": "Several well-performing approaches exist when the mixing environment is instantaneous [7,3], while some problems still arise in convolutive environments [2,4,17].", "startOffset": 153, "endOffset": 161}, {"referenceID": 5, "context": "Some of these approaches perform separation by using high order statistics (HOS) while others exploit information theoretic (IT) measures [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "One of the well-know algorithms in this latter class is the InfoMax one proposed by Bell and Sejnowski in [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Moreover, in order to avoid numerical instability, a natural gradient modification to InfoMax algorithm has also been proposed [1,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 5, "context": "Moreover, in order to avoid numerical instability, a natural gradient modification to InfoMax algorithm has also been proposed [1,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 12, "context": "Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "Many authors have tried to overcome this problem: some solutions consist in incorporating a momentum term in the learning rule [13], in a self-adjusting variable step-size [18] or in a scaled natural gradient algorithm [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 11, "context": "Recently, a novel algorithm for gradient based optimization of stochastic cost functions has been proposed by Kingma and Ba in [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "The authors have demonstrated in [12] that Adam is easy to implement, computationally efficient, invariant to diagonal rescaling of the gradients and well suited for problems with large data and parameters.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "The Adam algorithm combines the advantages of other state-of-the-art optimization algorithms, like AdaGrad [9] and RMSProp [19], outperforming the limitations of these algorithms.", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "In addition, Adam can be related to the natural gradient (NG) adaptation [1], employing a preconditioning that adapts to the geometry of data.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "In this paper we propose a modified InfoMax algorithm based on the Adam algorithm [12] for the solution of BSS problems.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "Some experimental results, evaluated in terms of the Amari Performance index (PI) [6], show the effectiveness of the proposed idea.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "The weights wi j can be adapted by maximizing or minimizing some suitable cost function [6,5].", "startOffset": 88, "endOffset": 93}, {"referenceID": 4, "context": "The weights wi j can be adapted by maximizing or minimizing some suitable cost function [6,5].", "startOffset": 88, "endOffset": 93}, {"referenceID": 2, "context": "A particularly good approach is to maximize the joint entropy of a single layer neural network [3], as shown in Figure 1, leading to the Bell and Sejnowski InfoMax algorithm.", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "With reference to Figure 1, using the equation relating the probability density function (pdf) of a random variable px (x) and nonlinear transformation of it py (y) [14], the joint entropy H (y) of the network output y[n] can be evaluated as", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "In order to avoid the numerical problems of the matrix inversion in (6) and the possibility to remain blocked in a local minimum, Amari has introduced the Natural Gradient (NG) adaptation [1] that overcomes such problems.", "startOffset": 188, "endOffset": 191}, {"referenceID": 11, "context": "The Adam algorithm performs the gradient descent (or ascent) optimization by evaluating the moving averages of the noisy gradient mt and the square gradient vt [12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "The vector v\u0302t represents an approximation of the diagonal of the Fisher information matrix [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Hence Adam can be related to the natural gradient algorithm [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "The results are evaluated in terms of the Amari Performance Index (PI) [6], defined as", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "The performances of the proposed algorithm were also compared with the standard InfoMax algorithm [3] and the Momentum InfoMax described in [13].", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "The performances of the proposed algorithm were also compared with the standard InfoMax algorithm [3] and the Momentum InfoMax described in [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "ICA is a common approach to deal with the problem of artifact removal from EEG [11].", "startOffset": 79, "endOffset": 83}], "year": 2016, "abstractText": "In this paper, we derive a modified InfoMax algorithm for the solution of Blind Signal Separation (BSS) problems by using advanced stochastic methods. The proposed approach is based on a novel stochastic optimization approach known as the Adaptive Moment Estimation (Adam) algorithm. The proposed BSS solution can benefit from the excellent properties of the Adam approach. In order to derive the new learning rule, the Adam algorithm is introduced in the derivation of the cost function maximization in the standard InfoMax algorithm. The natural gradient adaptation is also considered. Finally, some experimental results show the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}