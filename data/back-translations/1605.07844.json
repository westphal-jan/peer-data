{"id": "1605.07844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation", "abstract": "In this paper, we propose a new method of query translation based on the dimensional projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, we first learn the low-dimensional representation of words in the pseudo-relevant collections separately and then aim to find a query-dependent transformation matrix between the vectors of the translation pairs. In the next step, the representation of each query term is projected onto the target language and then, after using a Softmax function, a query-dependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German and Persian show that the proposed method exceeds all competitive foundations in language modeling, especially when combined with a collection.", "histories": [["v1", "Wed, 25 May 2016 12:04:43 GMT  (864kb,D)", "https://arxiv.org/abs/1605.07844v1", null], ["v2", "Sat, 8 Oct 2016 11:19:10 GMT  (1166kb,D)", "http://arxiv.org/abs/1605.07844v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["javid dadashkarimi", "mahsa s shahshahani", "amirhossein tebbifakhr", "heshaam faili", "azadeh shakery"], "accepted": false, "id": "1605.07844"}, "pdf": {"name": "1605.07844.pdf", "metadata": {"source": "CRF", "title": "Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation", "authors": ["Javid Dadashkarimi", "Mahsa S. Shahshahani", "Amirhossein Tebbifakhr", "Heshaam Faili", "Azadeh Shakery"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3]. In cross-language environments where there are a couple of document sets in different languages, building such models requires bridging the gap of the languages. To this end, cross-lingual topical relevance models (CLTRLM) aims to find a way to transform knowledge of the sets to the query model using bilingual topic modeling and a bilingual dictionary [3,1]. The performance of CLTRLM is heavily depends on the number of alignments in a comparable corpora and their qualities as well. Recently, bilingual word embedding is tailored effectively to this end where low-dimensional vectors are built after shuffling all the alignments [10]. However the effectiveness of this method has not been investigated at cross-lingaul PRF yet.\nIn this paper we propose a new method for building translation models on pseudorelevant collections using a neural network-based language model. The proposed crosslingual word embedding translation model (CLWETM) takes advantage of a query-dependent transformation matrix between low-dimensional vectors of the languages. Indeed, we aim to find a transformation matrix to bring the vector of each query term, built on the source collection, to dimensionality of the target language and then compute the translation probabilities based on a softmax function. To this aim, first we learn word representations of the pseudo-relevant collections separately and then focus on finding a transformation matrix minimizing a distance function between all translation pairs appeared at the collections. This method captures semantics of both the collections with a rotation and a scaling\nar X\niv :1\n60 5.\n07 84\n4v 2\n[ cs\n.I R\n] 8\nO ct\n2 01\n6\nembedded in the matrix. Finally, a softmax function is used to build a query-dependent translation model based on similarity of transformed vector of each query term with the vectors of its translation candidates in the target language.\nUnlike CLTRLM and the mixed word embedding translation model (MIXWETM) based on shuffling alignments in a comparable corpora ([10]), CLWETM considers quite sentence-level contexts of the words and therefore captures deeper levels of n-grams in both the languages. Furthermore, the obtained model can be incorporated within a language modeling framework, the state-of-the-art retrieval framework in the literature, and therefore the proposed method does not suffer from disadvantages of low-dimensional query-document similarity in ad-hoc retrieval [10].\nExperimental results on four CLEF collections in French, German, Spanish, and Italian demonstrate that the proposed method outperforms all competitive baselines of dictionary-based cross-language information retrieval (CLIR) in language modeling when it is combined with a global translation model. The proposed method reaches up to 83% performance of the monolingual run and 87% performance of machine translation in short queries. CLWETM has more better results in verbose queries and even improvements compared to MT in the Italian collection."}, {"heading": "2 Related Works", "text": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12]. Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3]. Unlike CLRLM that depends on parallel corpora and bilingual lexicons, CLTRLM aims at finding a number of bilingual topical variables from a comparable corpus in order to transfer relevance score of a term from one language to another. Ganguly et al. proposed to use these variables for query translation and demonstrated that CLTRLM is an effective method particularly for resource-lean languages [1].\nIn CLTRLM top-ranked documents F s = {ds1, ds2, .., ds|F s|} retrieved in response to the source query (qs) and top-ranked documents F t = {dt1, dt2, .., dt|F t|} retrieved in response to a translation of the query (qt) are assumed to be relevant documents and then we expect that each word wt in target language is generated either from a target event or a source event as follows: p(wt|qs) = p(wt|zt)p(zt|qs)+ p(wt|ws)p(ws|qs) in which zt is a topical variable on F t and ws is a translation of wt in the dictionary (see whole the details in [1]).\nIt is noteworthy that topic modeling considers co-occurrences of the terms within documents without considering their sentence-level contexts. Language modeling based on neural networks is a popular technique for capturing co-occurrences of the terms within a constant window c and thus, this model embeds semantic of the language as well as deeper levels of n-grams [8].\nBut when it comes to cross-lingual environments a constraint is required for integrating dimensions in the languages. There are a couple of methods to this end; on-line methods and off-line methods. On-line methods aim at finding a unified space for the languages during the learning process as follows [2]: L(\u03b8) = L(\u03b8s)+L(\u03b8t)+\u03bbf(\u03b8s, \u03b8t) where the last term is a regularization term. Recently, Vulic et al., introduced the shuffling-based word embedding method over comparable corpora in which the alignments play as a number of constraints on the vectors [10]. Ultimately, this approach considers neither the absolute/relative positions of the terms nor their sentence level contexts for estimation.\nOff-line methods learn the vectors separately and then find a transformation matrix minimizing a constraint function f as follows [7]: f(W) = \u2211 x,z ||W T x \u2212 z||2 in which\nx \u2208 Rn\u00d71 and z \u2208 Rn\u00d71 are the vectors of translation pairs from a dictionary or a parallel corpus."}, {"heading": "3 Linear Projection between Languages based on Pseudo-relevant Documents", "text": "In this section we introduce the proposed method in more details. We employ an offline approach for learning bilingual representations of the words by exploiting pseudorelevant documents in both source and target languages. To this end, first we learn word representations of the pseudo-relevant collections separately and then focus on finding a transformation matrix minimizing a distance function between all translation pairs appeared on the collections. As shown in Equation 1 our goal is to minimize f with respect to a transformation matrix W \u2208 Rn\u00d7n; f is defined as follow:\nf(W) = \u2211\n(ws,wt)\n1 2 ||WT uws \u2212 vwt ||2 (1)\nwhere, wt is a translation pair of ws from F s that is appeared in F t; uws \u2208 Rn\u00d71 and uwt \u2208 Rn\u00d71 are the corresponding vectors respectively. To solve this problem we choose the stochastic gradient descent algorithm (i.e., \u2202f\u2202W = 0):\nWt+1 = Wt \u2212 \u03b7(WT uws \u2212 vwt)uTws (2)\nwhere \u03b7 is a constant learning rate. W is to be initialized randomly and then be updated incrementally."}, {"heading": "3.1 Bilingual Representations and Translation Models", "text": "In this section we introduce a method based on bilingual word representations for building a translation model for query and then incorporate it within language modeling, the stateof-the-art retrieval framework.\nu\u0302ws = WT uws (3)\nwhere WT uws transforms the source vector built on the source collection to the target low-dimensional space. The new translation model is built as follows:\np(wt|ws) = e u\u0302ws .vwt ||u\u0302ws || ||vwt ||\u2211\nw\u0304t\u2208T{ws} e\nu\u0302ws .vw\u0304t ||u\u0302ws || ||vw\u0304t ||\n(4)\nwhere T{ws} is the list of translations of ws. Instead of topical information propagation taking place on CLTRLM and joint cross-lingual topical relevance model (JCLTRLM), CLWETM tailors semantic projection and scaling both embedded in W. In other words, CLWETM embeds quite sentence-level contexts of the words and thus captures deeper levels of n-grams as well.\nCombining Translation Models Since the obtained model is a probabilistic translation model we can interpolate it with other models using a constant controlling parameter as follow:\np(wt|ws) = \u03b1p1(wt|ws) + (1\u2212 \u03b1)p2(wt|ws) (5)"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experimental Setup", "text": "Overview of the used collections is provided in Table 1. The source collection is a pool of Associated Press 1988-89, Los Angeles Times 1994, and Glasgow Herald 1995 collections that are used in previous TREC and CLEF evaluation campaigns for ad-hoc retrieval.\nIn all experiments, we use the language modeling framework with the KL-divergence retrieval model and Dirichlet smoothing method to estimate the document language models, where we set the dirichlet prior smoothing parameter \u00b5 to the typical value of 1000. To improve the retrieval performance, we use the mixture model for pseudo-relevance feedback with the feedback coefficient of 0.5. The number of feedback documents and feedback terms are set to the typical values of 10 and 50, respectively.\nAll European dictionaries, documents, and queries are normalized and stemmed using the Porter stemmer. Stopword removal is also performed.1 The Lemur toolkit2 is employed as the retrieval engine in our experiments.\nWe use the Google dictionaries in our experiments3. In the European languages, we do not transliterate out of vocabulary (OOV) terms of the source languages. The OOVs of the target language are used as their original forms in the source documents, since they are cognate languages. Note that we use the uniform distribution approach as the initial translation model for retrieving top documents. It is worth mentioning that p(wt|ws) in CLTRLM (see Sec. 2) is estimated by a bi-gram coherence translation model (BiCTM) introduced in [9]. Weights of the edges of the graph are estimated by p(wj |wi) computed\n1 We use the stopword lists and the normalizing techniques available at http://members. unine.ch/jacques.savoy/clef/. 2 http://www.lemurproject.org/ 3 http://translate.google.com\nby SRILM toolkit 4. BiCTM is also used as p2 where \u03b1 is set by 2-fold cross-validation (see Equation 5).\nAs discussed in Section 3, we used stochastic gradient descent for learning W which is initialized with random values in [\u22121, 1]; \u03b7 is set to a small value which also decreases after each iteration. uws and vwt are computed based on negative sampling skip-gram introduced in [8]; the size of the window, the number of negative samples, and the size of the vectors are set to typical values of 10, 45, and 50 respectively.\nAs shown in [1] JCLTRLM outperforms CLTRLM and therefore we opted JCLTRLM as a baseline. The parameters of LDA are set to the typical values \u03b1d = 0.5 and \u03b2d = 0.01. Number of topics in JCLTRLM is obtained by 2-fold cross-validation."}, {"heading": "4.2 Performance Comparison and Discussion", "text": "In this section we want to compare effectiveness of a number of competitive methods in CLIR. We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10]. As bases of comparisons we also provided results of the monolingual runs in each collection (MONO) and Google machine translator (MT). However, our main focus is to investigate superiority of the proposed method compared to the dictionarybased CLIR baselines which are available for almost all pairs of languages.\nAll the results (on short queries) are summarized in Table 2 and Fig. 1 shows sensitivity of CLWETM to \u03b1 and the number of feedback documents. As shown in the table, both MIXWETM and CLWETM outperform other methods in terms of MAP, P@5, and P@10 in all the collections. MIXWETM and CLWETM consistently achieved better results compared to others, but CLWETM is clearly more effective than MIXWETM in almost all the datasets. Although CLWETM lost the competition to MIXWETM in DE, but the differences are not statistically significant. One reason for this outcome is the lower performance of BiCTM compared to other collections (see Eq. 5, Table 2, and Fig. 1). Another reason can be the lower sensitivity of the method to n in this collection. As shown in Fig. 1 top-ranked documents in DE are not as helpful as FR, ES, and IT and thus neither CLWETM nor MIXWETM has significant improvements compared to BiCTM.\nAlthough the focus of this research is on dictionary-based CLIR, but it is clear to a reader that in the European collections with short queries the results of MT are higher than all the baselines in dictionary-based CLIR. In the rest of the experiments, we shed light on effectiveness of the methods on verbose queries obtained by concatenating title\n4 http://www.speech.sri.com/projects/srilm/\nand description parts of the topics. Table 3 shows the results; the results also confirm the effectiveness of CLWETM over the dictionary-based methods. The most interesting point is decrements of the gaps between CLWETM and MT in quite all the collections. CLWETM reached 93.2%, 76.3%, 95.7%, and 172.9% of the performance of MT in terms of MAP in ES, DE, ES, and IT respectively. In IT we see noticeable decrement of the performance by MT on the verbose queries where the dictionary-based techniques are quite stable."}, {"heading": "4.3 Parameter Sensitivity", "text": "We investigate the sensitivity of the proposed method to two parameters \u03b1 and n in Figure 1. We first fix one parameter to its optimal value and then try to get optimal value of the other one . It demonstrates that both parameters work stably across FR, ES and IT collections. The optimal \u03b1 value empirically is 0.6 and the optimal value of n is 10 in almost all the collections."}, {"heading": "5 Conclusion and Future Works", "text": "In this paper we presented a translation model for cross-lingual information retrieval, that uses feedback documents in source and target languages for creating word vectors in each one, and then learns a projection matrix to project word vectors in the source language to their translations in the target language. Then we introduced a method for building a translation model that can be easily interpolated with other models using a constant controlling parameter. We investigated the performance of the proposed method on four European collections of CLEF. Our method showed more improvements in FR, SP, and IT since top-ranked documents in DE are not as comparable as the previous ones; therefor applying the proposed method on comparable corpora is considered as an interesting future work. The proposed method reaches up to 87% performance of machine translation (MT) in short queries and considerable improvements in verbose queries."}], "references": [{"title": "Cross-Lingual Topical Relevance Models", "author": ["D. Ganguly", "J. Leveling", "G. Jones"], "venue": "COLING \u201912", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["S. Gouws", "Y. Bengio", "G. Corrado"], "venue": "arXiv preprint:1410.2455", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-lingual relevance models", "author": ["V. Lavrenko", "M. Choquette", "W.B. Croft"], "venue": "SIGIR \u201902", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Relevance based language models", "author": ["V. Lavrenko", "W.B. Croft"], "venue": "SIGIR \u201901. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Positional relevance model for pseudo-relevance feedback", "author": ["Y. Lv", "C. Zhai"], "venue": "SIGIR \u201910", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Revisiting the divergence minimization feedback model", "author": ["Y. Lv", "C. Zhai"], "venue": "CIKM \u201914", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint:1309.4168", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Adv. in Neur. Inf. Proc. Sys., pp. 3111\u20133119", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative Translation Disambiguation for Cross-language Information Retrieval", "author": ["C. Monz", "B.J. Dorr"], "venue": "SIGIR \u201905", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vulic", "M. Moens"], "venue": "SIGIR \u201915", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications", "author": ["I. Vulic", "W.D. Smet", "J. Tang", "M. Moens"], "venue": "IP&M 51(1)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Pseudo-relevance feedback based on matrix factorization", "author": ["H. Zamani", "J. Dadashkarimi", "A. Shakery", "W.B. Croft"], "venue": "CIKM \u201916", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 11, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 3, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 2, "context": "Pseudo-relevance feedback (PRF) has long been shown to be an effective approach for updating query language models in information retrieval (IR) [6,12,4,3].", "startOffset": 145, "endOffset": 155}, {"referenceID": 2, "context": "To this end, cross-lingual topical relevance models (CLTRLM) aims to find a way to transform knowledge of the sets to the query model using bilingual topic modeling and a bilingual dictionary [3,1].", "startOffset": 192, "endOffset": 197}, {"referenceID": 0, "context": "To this end, cross-lingual topical relevance models (CLTRLM) aims to find a way to transform knowledge of the sets to the query model using bilingual topic modeling and a bilingual dictionary [3,1].", "startOffset": 192, "endOffset": 197}, {"referenceID": 9, "context": "Recently, bilingual word embedding is tailored effectively to this end where low-dimensional vectors are built after shuffling all the alignments [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 9, "context": "Unlike CLTRLM and the mixed word embedding translation model (MIXWETM) based on shuffling alignments in a comparable corpora ([10]), CLWETM considers quite sentence-level contexts of the words and therefore captures deeper levels of n-grams in both the languages.", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "Furthermore, the obtained model can be incorporated within a language modeling framework, the state-of-the-art retrieval framework in the literature, and therefore the proposed method does not suffer from disadvantages of low-dimensional query-document similarity in ad-hoc retrieval [10].", "startOffset": 284, "endOffset": 288}, {"referenceID": 5, "context": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12].", "startOffset": 136, "endOffset": 144}, {"referenceID": 4, "context": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12].", "startOffset": 136, "endOffset": 144}, {"referenceID": 11, "context": "Pseudo-relevance feedback has long been employed as a powerful method for estimating query language models in a large number of studies [6,5,12].", "startOffset": 136, "endOffset": 144}, {"referenceID": 0, "context": "Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3].", "startOffset": 108, "endOffset": 116}, {"referenceID": 10, "context": "Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3].", "startOffset": 108, "endOffset": 116}, {"referenceID": 2, "context": "Cross-lingual relevance model (CLRLM) and CLTRLM are state-of-the-art methods in cross-lingual environments [1,11,3].", "startOffset": 108, "endOffset": 116}, {"referenceID": 0, "context": "proposed to use these variables for query translation and demonstrated that CLTRLM is an effective method particularly for resource-lean languages [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": ", d|F t|} retrieved in response to a translation of the query (q) are assumed to be relevant documents and then we expect that each word w in target language is generated either from a target event or a source event as follows: p(w|q) = p(w|z)p(z|q)+ p(w|w)p(w|q) in which z is a topical variable on F t and w is a translation of w in the dictionary (see whole the details in [1]).", "startOffset": 376, "endOffset": 379}, {"referenceID": 7, "context": "Language modeling based on neural networks is a popular technique for capturing co-occurrences of the terms within a constant window c and thus, this model embeds semantic of the language as well as deeper levels of n-grams [8].", "startOffset": 224, "endOffset": 227}, {"referenceID": 1, "context": "On-line methods aim at finding a unified space for the languages during the learning process as follows [2]: L(\u03b8) = L(\u03b8)+L(\u03b8)+\u03bbf(\u03b8, \u03b8) where the last term is a regularization term.", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": ", introduced the shuffling-based word embedding method over comparable corpora in which the alignments play as a number of constraints on the vectors [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 6, "context": "Off-line methods learn the vectors separately and then find a transformation matrix minimizing a constraint function f as follows [7]: f(W) = \u2211 x,z ||W T x \u2212 z|| in which", "startOffset": 130, "endOffset": 133}, {"referenceID": 8, "context": "2) is estimated by a bi-gram coherence translation model (BiCTM) introduced in [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "uws and vwt are computed based on negative sampling skip-gram introduced in [8]; the size of the window, the number of negative samples, and the size of the vectors are set to typical values of 10, 45, and 50 respectively.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "As shown in [1] JCLTRLM outperforms CLTRLM and therefore we opted JCLTRLM as a baseline.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10].", "startOffset": 268, "endOffset": 271}, {"referenceID": 0, "context": "We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10].", "startOffset": 309, "endOffset": 312}, {"referenceID": 9, "context": "We consider the following dictionary-based CLIR methods to evaluate the proposed method: (1) the top-1 translation of each term in the bilingual dictionaries (TOP-1), (2) all the possible translations of each term with equal weights (UNIFORM), (3) (BiCTM) proposed in [9] , (4) the JCLTRLM method proposed in [1], and (5) MIXWETM [10].", "startOffset": 330, "endOffset": 334}], "year": 2016, "abstractText": "Using top-ranked documents in response to a query has been shown to be an effective approach to improve the quality of query translation in dictionarybased cross-language information retrieval. In this paper, we propose a new method for dictionary-based query translation based on dimension projection of embedded vectors from the pseudo-relevant documents in the source language to their equivalents in the target language. To this end, first we learn low-dimensional vectors of the words in the pseudo-relevant collections separately and then aim to find a query-dependent transformation matrix between the vectors of translation pairs appeared in the collections. At the next step, representation of each query term is projected to the target language and then, after using a softmax function, a querydependent translation model is built. Finally, the model is used for query translation. Our experiments on four CLEF collections in French, Spanish, German, and Italian demonstrate that the proposed method outperforms a word embedding baseline based on bilingual shuffling and a further number of competitive baselines. The proposed method reaches up to 87% performance of machine translation (MT) in short queries and considerable improvements in verbose queries.", "creator": "LaTeX with hyperref package"}}}