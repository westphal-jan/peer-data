{"id": "1605.08187", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "The Symbolic Interior Point Method", "abstract": "A current trend in probabilistic reasoning emphasizes the codification of models in a formal syntax with appropriate high-level features such as individuals, relationships, and connectives that allow descriptive clarity, conciseness, and circumvention of the need for the modeler to develop a custom solver. Unfortunately, it has proved surprisingly difficult to incorporate these linguistic and pragmatic advantages into numerical optimization. In this paper, we address these challenges: We introduce a rich modeling language for which an interior-dot method generically calculates approximate solutions. While logical features slightly complicate the underlying model and often create complicated dependencies, we use and store local structures using algebraic decision diagrams (ADDs). In fact, the standard matrix vector algebra for ADDs is efficiently realizable, but we argue and show that known optimization methods are not ideal for demonstrating ADDs based on the transmitter-free machine.", "histories": [["v1", "Thu, 26 May 2016 08:26:34 GMT  (613kb,D)", "https://arxiv.org/abs/1605.08187v1", null], ["v2", "Sat, 28 May 2016 17:11:30 GMT  (745kb,D)", "http://arxiv.org/abs/1605.08187v2", null], ["v3", "Tue, 14 Jun 2016 18:29:14 GMT  (746kb,D)", "http://arxiv.org/abs/1605.08187v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LO cs.SC", "authors": ["martin mladenov", "vaishak belle", "kristian kersting"], "accepted": true, "id": "1605.08187"}, "pdf": {"name": "1605.08187.pdf", "metadata": {"source": "CRF", "title": "The Symbolic Interior Point Method", "authors": ["Martin Mladenov", "Vaishak Belle"], "emails": ["martin.mladenov@cs.tu-dortmund.de", "vaishak@cs.kuleuven.be", "kristian.kersting@cs.tu-dortmund.de"], "sections": [{"heading": "1 Introduction", "text": "Numerical optimization is arguably the most prominent computational framework in machine learning and AI. It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3]. What makes optimization particularly ubiquitous is that, similar to probabilistic inference, it provides a formal apparatus to reason (i.e. express preferences, costs, utilities) about possible worlds. However, it is also widely acknowledged that many AI models are often best described using a combination of natural and mathematical language and, in turn, require algorithms to be individually engineered.\nAn emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures. They are driven by the following desiderata:\n(D1) The syntax should be expressive, allowing high-level (logical) features such as individuals, relations, functions, connectives, in service of descriptive clarity and succinct characterizations, including context-specific dependencies (e.g. pixels probabilistically depend only on neighboring pixels, the flow of quantities at a node is limited to connected nodes).\n(D2) The inference engine should be generic, circumventing the need for the modeler to develop a custom solver.\nUnfortunately, bringing these linguistic and pragmatic benefits to optimization has proven surprisingly challenging. Mathematical programs are specified using linear arithmetic, matrix and tensor algebra, multivariate functions and their compositions. Classes of programs (e.g. linear, geometric) have\nar X\niv :1\n60 5.\n08 18\n7v 3\n[ cs\n.A I]\n1 4\nJu n\ndistinct standard forms, and solvers require models to be painstakingly reduced to the standard form; otherwise, a custom solver has to engineered. The approach taken in recent influential proposals, such as disciplined programming [7], is to carefully constrain the specification language so as to provide a structured interface between the model and the solver, by means of which geometric properties such as the curvature of the objective can be inferred. Basically, when contrasted with the above desiderata, solver genericity is addressed only partially, as it requires the modeler to be well-versed in matrix-vector algebra, and little can be said about high-level features, e.g. for codifying dependencies.\nIn this paper, we are addressing the above desiderata. Doing so has the potential to greatly simplify the specification and prototyping of AI and ML models [8, 1].\nLet us elaborate on the desiderata in our context. The introduction of logical features, while attractive from a modeling viewpoint, assuredly complicates the underlying model, yielding, for example, intricate dependencies. Local structure, on the other hand, can be exploited [9]. In probabilistic inference, local structure has enabled tractability in high-treewidth models, culminating in the promising direction of using circuits and decision-diagrams for the underlying graphical model [10]. Such data structures are a more powerful representation, in admitting compositionality and refinement. Here, we would like to consider the idea of using such a data structure for optimization, for which we turn to early pioneering research on algebraic decision diagrams (ADDs) that supports efficient matrix manipulations (compositionality) and caching of submatrices (repeated local structure) [11, 12].\nNow, suppose a linear program encoded in a high-level language has been to reduced to an efficiently manipulable data structure such as an ADD. Unfortunately, it is far from obvious how a generic solver can be engineered for it. Matrix operations with ADDs, for example, are efficient only under certain conditions, such as: a) they have to be done recursively, in a specific descent order; b) they have to involve the entire matrix (batch mode), i.e. access to arbitrary submatrices is not efficient. This places rather specific constraints on the kind of method that could benefit from an ADD representation, ruling out approaches like random coordinate descent. In this paper, we aim to engineer and construct a solver for such matrices in circuit representation. We employ ideas from the matrix-free interior point method [13], which appeals to an iterative linear equation solver together with the log-barrier method, to achieve a regime where the constraint matrix is only accessed through matrix-vector multiplications. Specifically, we show a ADD-based realization of the approach leverages the desirable properties of these representations (e.g. caching of submatrices), leading to a robust and fast engine. We demonstrate this claim empirically."}, {"heading": "2 Lineage", "text": "Expressiveness in modeling languages for numerical optimization is a focus of many proposals, e.g. [14, 15]. However, they blur the border between declarative and imperative paradigms, using sets of objects to index LP variables and do not embody logical reasoning. Disciplined programming [7] enables an object-oriented approach to constructing optimization problems, but falls short of our desiderata as argued before. Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]). This allows the specification of correlations, groups and properties in a natural manner, as also observed elsewhere [19, 20].\nThe efficiency of ADDs for matrix-vector algebra was established in [12]. In particular, the use of ADDs for compactly specifying (and solving) Markov decision processes (i.e. representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings. We differ fundamentally from these strands of work in that we are advocating the realization of iterative methods using ADDs, which (surprisingly) has never been studied in great detail to the best of our knowledge. Therefore, we call our approach symbolic numerical optimization."}, {"heading": "3 Primer on Logic and Decision Diagrams", "text": "We cover some of the logical preliminaries in this section. To prepare for the syntax of our highlevel mathematical programming language, we recap basic notions from mathematical logic [24]. A propositional language L consists of finitely many propositionsP = {p, . . . , q}, from which formulas are built using connectives { \u00b7 ,\u2228,\u2227} . A L-model M is a {0, 1}-assignment to the symbols in P ,\nwhich is extended to complex formulas inductively. For example, if P = {p, q} and M = \u3008p = 1, q = 0\u3009, we have M |= p, M |= p \u2228 q, M |= q but M 6|= p \u2227 q. The logical language of finite-domain function-free first-order logic consists of finitely many predicate symbols {P (x), . . . , Q(x, y), . . . , R(x, y, z), . . .} and a domain D of constants. Atoms are obtained by substituting variables in predicates with constants from D, e.g. P (a), Q(a, b), R(a, b, c) and so on wrt D = {a, b, . . . , c} . Formulas are built as usual using connectives { \u00b7 ,\u2228,\u2227,\u2200,\u2203}. Models are {0, 1}-assignments to atoms, but can also interpret quantified formulas, e.g. \u2200x[P (x)]. For example, if D = {0, 1} and M = \u3008P (0) = 1, P (1) = 1\u3009, then M |= P (0),M |= P (0) \u2227 P (1) and M |= \u2200xP (x). Finally, although finite-domain function-free first-order logic is essentially propositional, it can nonetheless serve as a convinient template for specifying correlations, groups and properties, cf. statistical relational learning [16].\nA BDD [25] is a compact and efficiently manipulable data structure for a Boolean function f : {0, 1}n \u2192 {0, 1} . Its roots are obtained by the Shannon expansion of the cofactors of the function: if fx and fx denote the partial evaluation of f(x, . . .) by setting the variable x to 1 and 0 respectively, then\nf = x \u00b7 fx + x \u00b7 fx.\nWhen the Shannon expansion is carried out recursively, we obtain a full binary tree whose nonterminal nodes, labeled by variables {. . . , xi, . . .}, represent a function: its left child is f \u2019s cofactor w.r.t. xi for some i and its right child is f \u2019s cofactor w.r.t. xi. The terminal node, then, is labeled 0 or 1 and corresponds to a total evaluation of f. By further ordering the variables, a graph, which we call the (ordered) BDD of f, can be constructed such that at the kth level of the tree, the cofactors wrt the kth variable are taken. Given an ordering, BDD representations are canonical: for any two functions f, g : {0, 1}n \u2192 {0, 1}, f \u2261 g \u21d4 fx \u2261 gx and fx \u2261 gx; and compact for Binary operators \u25e6 \u2208 {+,\u00d7, . . .}: |f \u25e6 g| \u2264 |f ||g|. ADDs generalize BDDs in representing functions of the form {0, 1}n \u2192 R, and so inherit the same structural properties as BDDs except, of course, that terminal nodes are labeled with real numbers [11, 12]. Consider any real-valued vector of length m: the vector is indexed by lgm bits, and so a function of the form {0, 1}lgm \u2192 R maps the vector\u2019s indices to its range. Thus, an ADD can represent the vector. By extension, any real-valued 2m \u00d7 2n matrix A with row index bits {x1, . . . , xm} and column index bits {y1, . . . , yn} can be represented as a function f(x1, y1, x2, . . .) such that its cofactors are the entries of the matrix. The intuition is to treat x1 as the most significant bit, and xm as the least. Then A represented by a function f as an ADD is: (fx1y1 fx1y1) as the first row and (fx1y1 fx1y1) as the second, where each submatrix is similarly defined wrt the next significant bit. Analogously, when multiplying two m-length vectors represented as Boolean functions f, g : {0, 1}lgm \u2192 R, we can write [fx1 fx1 ][gx1 gx1 ]T , taking, as usual, x1 as the most significant bit.\nx\n0 10\nx\n1 y\n5 3\nMIN =\nx\n0 y\n5 3\nxn\nyn\nWn 1 Wn 1\ntrue false\nLegend\nordering of the ADD) will best exploit the superiority of the ADD representation."}, {"heading": "4 First-Order Logical Quadratic Programming", "text": "A convex quadratic program (QP) is an optimization problem over the space Rn, that is, we want to to find a real-valued vector x \u2208 Rn from the solution set of a system of linear inequalities {x \u2265 0 : Ax = b} such that a convex quadratic objective function f(x) = xTQx + cTx is minimized. For each QP, there exists a complementary (dual) QP-D such that each solution of QP-D provides an upper bound on the minimum of QP, for the maximizer of QP-D, this bound is tight. In this paper we assume that QP and QP-D can be reduced to the following standard form\nPRIMAL-QP :\nminimize cTx+ 1/2xTQx\nsubject to Ax = b, x \u2265 0.\nDUAL-QP :\nmaximize bT y \u2212 1/2xTQx subject to AT y + s\u2212Qx = c,\ns \u2265 0.\nHere, Q is positive semi-definite. Whenever Q = 0, we speak of linear programs (LPs).\nIn this paper, we would like to provide an expressive modeling language with high-level (logical) features such as individuals, relations, functions, and connectives. While (some) high-level features are prominent in many optimization packages such as AMPL [26], they are reduced (naively) to canonical forms transparently to the user, and do not embody logical reasoning. Logic programming is supported in other proposals [18], but their restricted use of negation makes it difficult to understand the implications when modeling a domain. We take our cue from statistical relational learning [16], as considered in [19], to support any (finite) fragment of first-order logic with classical interpretations for operators. Given a logical language L, the syntax for first-order logical quadratic programs is: min. v \u2211 {x,x\u2032:\u03d5(x,x\u2032)} q(x, x\u2032)v(x)v(x\u2032)+ \u2211 {x:\u03c8(x)} c(x)v(x) s.t. {y : \u03c8(y)} : \u2211 {x:\u03c6(x,y)} a(x, y)v(x) \u2265 b(y)\nwhere, we write \u03b4(z) to mean that theL-formula \u03b4 mentions the logical symbols z, and read {z : \u03b4(z)} as the set of all assignments to z satisfying \u03b4(z). The constraints are to be read procedurally as follows: for every assignment to y satisfying \u03c8(y), consider the constraint \u2211 C a(x, y)v(x) \u2265 b(y), where the set C are those assignments to x satisfying \u03c6(x, y). For example, consider:\nminimizev \u2211\n{x:TRUE} v(x) subject to {y : TRUE} : \u2211 {x : x\u2228y} v(x) \u2265 1, {y : TRUE} : v(y) \u2265 0 .\nThat is, we are to minimize v(x) + v(x) subject to v(x) \u2265 1 and v(x) \u2265 1 (for the cases where y), v(x) + v(x) \u2265 1 and v(x) \u2265 0 (for the cases where y), yielding the following canonical form:\nA = 0 (xy) 1 (xy)1 (xy) 1 (xy)1 (xy) 0 (xy) 0 (xy) 1 (xy)  , b = 110 0  , c = [11 ] .\nBy extension, an example for a relational constraint is one of the form: \u2211 x:\u2203z Friends(z,x) v(x). Here, wrt D = {a, b, c} and a database {Friends(b, a),Friends(b, c)}, we would instantiate the constraint as v(b) + v(c). For simplicity, however, we limit discussions to propositions in the sequel.\nOne of the key contributions in our engine is that we are able to transform the constraints in such a high-level mathematical program directly to an ADD representation. (Quantifiers are eliminated as usual: existentials as disjunctions and universals as conjunctions [24].) The rough idea is this. Order the variables y in \u03c8(y). Any propositional formula can be seen as a Boolean function mapping to {0, 1} . That is, assignments to y determine the value of the corresponding Boolean function, e.g. \u3008y1 = 1, y2 = 0\u3009 maps the function y1 \u2227 y2 to 0. We build the ADD for \u03c8(y), and for each complete evaluation of this function, we build the ADD for \u03c6(x, y) and let its leaf nodes be mapped to b(y). We omit a full description of this procedure, but go over the main result:\nTheorem 1: There is an algorithm for building an ADD for terms from a first-order logical mathematical program without converting the program to the canonical (ground) form. The ADD obtained from the algorithm is identical to the one obtained from the ground form.\nProof: We simply need to argue that a term of a first-order logical mathematical program corresponds to a Boolean function {0, 1}n \u2192 R, where n is the number of propositions. The procedure described above builds its ADD. By the canonicity of ADDs, the claim follows."}, {"heading": "5 Solution Strategies for First-Order Logical QPs", "text": "Given the representation language, we now turn towards solving logical QPs. To prepare for discussions, let us establish an elementary notion of algorithmic correctness for ADD implementations. We assume that given a first-order logical mathematical program, we have in hand the ADDs for A, b and c. Then, it can be shown:\nTheorem 2: Suppose A, b and c are as above, and e is any arithmetic expression over them involving standard matrix binary operators. Then there is a sequence of operations over their ADDs, yielding a function h, such that h = e.\nThe kinds of expressions we have in mind are e = Ax\u2212 b (which corresponds to the residual in the corresponding system of linear equations). The proof is as follows:\nProof: For any f, g : {0, 1}n \u2192 R, and (standard) binary matrix operators (multiplication, summation, subtraction), observe that h = f \u25e6 g iff hx = fx \u25e6 gx and hx = fx \u25e6 gx. By canonicity, the ADD for h is precisely the same as the one for f and g composed over \u25e6. In other words, for any arithmetic expression e over {f, g, \u25e6}, the ADD realization for h is precisely the same function.\nTo guide the construction of the engine, we will briefly go over the operations previously established as efficient with ADDs [11, 12], and some implications thereof for a solver strategy.\nTheorem 3: [11, 12] Suppose A,A\u2032 are real-valued matrices. Then the following can be efficiently implemented in ADDs using recursive-decent procedures: 1) accessing and setting a submatrix A\u2217 of A; 2) termwise operations, i.e. (A \u25e6A\u2032)ij = Aij \u25e6A\u2032ij for any termwise operator \u25e6; and 3) vector and matrix multiplications.\nThe proof for these claims and the ones in the corollary below always proceed by leveraging the Shannon expansion for the corresponding Boolean functions, as shown in Section 3. We refer interested readers to [11, 12] for the complexity-theoretic properties of these operations. It is worth noting, for example, that multiplication procedures that perform both (naive) block computations and ones based on Strassen products can be recursively defined. For our purposes, we get:\nCorollary 4: Suppose d = [d1 \u00b7 \u00b7 \u00b7 dm] and e are m-length real-valued vectors, and k is a scalar quantity. Then the following can be efficiently implemented in ADDs using recursive-descent procedures: 1) scalar multiplication, e.g. k \u00b7 d; 2) vector arithmetic, e.g. d+ e; 3) element sum, e.g.\u2211i di; 4) element function application, e.g. if w : R \u2192 R, then computing w(d) = [w(d1) \u00b7 \u00b7 \u00b7 w(dm)]; and 5) norms, e.g. \u2016d\u2016 and \u2016d\u20162."}, {"heading": "5.1 A Naive Ground-and-Solve Method", "text": "The most straightforward approach for solving a first-order logical QP (FOQP) is to reduce the problem to the normal form and use a standard solver for QPs, such as an interior point method, an augmented Lagrangian method, or some form of active set method, e.g. generalized simplex. The correctness of this solution strategy is guaranteed by the semantics of the first-order constraints: in general, every FOQP can be brought to the normal form. While this method would work, it exhibits a significant drawback in that the optimization engine cannot leverage knowledge about the symbolic structure of the problem. That is, even if the problem compiles to a very small ADD, the running time of the optimizer will depend (linearly at best) on the number of nonzeros in the ground matrix. Clearly, large dense problems will be completely intractable. However, as we will see later, some dense problems can still be attacked with the help of structure.\nInput: (x0, y0, s0) with (x0, s0) \u2265 0 k \u2190 0; while stopping criterion not fulfilled do\nSolve (2) with (x, y, s) = (xk, yk, sk) to obtain a direction (\u2206xk,\u2206yk,\u2206sk); Choose step length \u03b1k \u2208 (0, 1]; Update (xk+1, yk+1, sk+1)\u2190 \u03b1k(\u2206xk,\u2206yk,\u2206sk) k \u2190 k + 1\nreturn xk ;\n(a) Primal-Dual Barrier Method\nInput: A \u2208 Rn\u00d7n , b \u2208 Rn k \u2190 0, r0 \u2190 b\u2212 Ax0 ; while stopping criterion not fulfilled do\nk \u2190 k + 1; if k = 1 then\np0 \u2190 r0 ; else\n\u03c4k\u22121 = (r T k\u22121rk\u22121)/(r T k\u22122r T k\u22122); pk = rk\u22121 + \u03c4k\u22121pk\u22121;\n\u00b5k = (r T k\u22121rk\u22121)/(p T k Apk); xk = xk\u22121 + \u00b5kpk ;\nrk = rk\u22121 \u2212 \u00b5kApk ; return xk ;\n(b) Conjugate Gradient Method"}, {"heading": "5.2 The Symbolic Interior Point Method", "text": "While the ground-and-solve method is indeed correct, one can do significantly better, as we will now show. In this section, we will construct a solver that automatically exploits the symbolic structure of the FOQP, in essence, by appealing to the strengths of the ADD representation. The reader should note that much of this discussion is predicated on problems having considerable logical structure, as is often the case in real-word problems involving relations and properties.\nRecall from the previous discussion that in the presence of cache, (compact) ADDs translate to very fast matrix-vector multiplications. Moreover, from Corollary 4, vector operations are efficiently implementable, which implies that given an ADD for A, x and b, the computation of the residual y = Ax\u2212b is more efficient than its matrix counterpart [11]. Analogously, a descent along a direction (xk = xk\u22121 + \u03b1\u2206x) given ADDs for xk\u22121 and \u2206x has the same run-time complexity as when performed on dense vectors.\nWe remark that for ADD-based procedures to be efficient, we need to respect the variable ordering implicit in the ADD. Therefore, the solver strategy rests on the following constraints: i) the engine must manipulate the matrix only through recursive-descent arithmetical operations, such as matrixvector multiplications (matvecs, for short); and ii) operations must manipulate either the entire matrix or those submatrices corresponding to cofactors (i.e. arbitrary submatrices are non-trivial to access).\nWe will now investigate a method that satisfies these requirements. We proceed in two steps. In step 1, we will demonstrate that solving a QP can be reduced to solving a sequence of linear equations over A. Next, in step 2, we will make use of an iterative solver that computes numerical solutions by a sequence of residuals and vector algebraic operations. As a result, we will obtain a method that fully utilizes the strengths of ADDs. Due to space constraints, we will not be able to discuss the construction in full detail, and so we sketch the main ideas that convey how ADDs are exploited.\nStep 1: From linear programs to linear equations. A prominent solver for QPs in standard form is the primal-dual barrier method, see e.g. [27], sketched in Alg. 2a. This method solves a perturbed version of the first-order necessary conditions (KKT conditions) for QP:\nAx = b, \u2212Qx+AT y + s = c, XSe = \u00b5e, (x, s) \u2265 0, where X = diag(x1, . . . , xn), S = diag(s1, . . . , sn), and \u00b5 \u2265 0. The underlying idea is as follows [27, 13]: by applying a perturbed Newton method to the equalities in the above system, the algorithm progresses the current solution along a direction obtained by solving the following linear system: A 0 0\u2212Q AT I\nS 0 X \u2206x\u2206y \u2206s  =  b\u2212Axc\u2212AT y \u2212 s \u00b5e\u2212Xs  , (1) where e is a vector of ones. Observe that besides A and Q, which we already have decision diagrams for, the only new information that needs to be computed is in the form of residuals in the right-hand side of the equation. By performing two pivots, \u2206x and \u2206s can be eliminated from the system, reducing it to the so-called normal equation:\nA(Q+ \u0398\u22121)\u22121AT\u2206y = f, (2)\nwhere \u0398 is the diagonal matrix \u0398ii = xisi [27, 13]. Once \u2206y is determined, \u2206x and \u2206s are recovered from \u2206y as \u2206x = (Q+ \u0398)\u22121(AT\u2206y \u2212 g) and \u2206s = h\u2212AT\u2206y, where h and g are obtained from the residuals via vector arithmetic. The reader will note that constructing the left-hand side involves the matrix inverse (Q+ \u0398\u22121)\u22121. The efficiency of computing this inverse cannot be guaranteed with ADDs, unfortunately; therefore, we assume that either the problem is separable (Q is diagonal), in which case computing this inverse reduces to computing the reciprocals of the diagonal elements (an efficient operation with ADDs), or that we have only box constraints, meaning that A is a diagonal matrix, in which case solving the equation reduces to solving (Q+ \u0398)\u2206y\u2032 = A\u22121f and re-scaling. (The general case of going beyond these assumptions is omitted here for space reasons [13].)\nThe benefit of reformulating (1) into (2) can be appreciated from the observation that 2 becomes positive-semidefinite, which is crucial for solving this system by residuals. To reiterate: the primaldual barrier method solves a quadratic program iteratively by solving one linear system (2) in each iteration. Constructing the right-hand side of this linear system only requires the calculation of residuals and vector arithmetic, which can be done efficiently with ADDs. Moreover, this system does not require taking arbitrary submatrices of A or Q. Hence, the primal-dual barrier method meets our requirements. Now, let us investigate how 2 can be solved via a sequence of residuals.\nStep 2: From linear equations to residuals. To solve (2), we employ the conjugate gradient method [28], sketched in Fig. 2b. Here, the algorithm uses three algebraic operations: 1) matrixvector products; 2) norm computation (rT r) and 3) scalar updates. From Corollary 4, all of these operations can be implemented efficiently in ADDs. There is, however, one challenge that remains to be addressed. As the barrier method approaches the solution of the QP, the iterates sk and xk approach complementary slackness (xisi = 0). This means that the diagonal entries of the matrix \u0398 in (2) tend to either 0 or +\u221e, making the condition number of (2) unbounded. This is a severe problem for any iterative solver, as the number of iterations required to reach a specified tolerance becomes unbounded. To remedy this situation, Gondzio [13] proposes the following approach: first, the system can be regularized to achieve a condition number bounded by the largest singular value of A. Second, due to the IPM\u2019s remarkable robustness to inexact search directions, it is not necessary to solve the system completely. In practice, decreasing the residual by a factor of 0.01 to 0.0001 has been found sufficient. Finally, a partial pivoted Cholesky factorization can be used to speed-up the convergence. That is, perform a small number k (say 50) Cholesky pivots, and use the resulting trapezoidal matrix as a preconditioner. More details on this can be found in [13]. As demonstrated in [13] this approach does lead to a practical algorithm. Unfortunately, in our setting, constructing this preconditioner requires that we query k rows of N and perform pivots with them. This forces us to partially back down on our requirement (ii), since we need random access to k rows. However, by keeping k small, we can guarantee the ADD in this unfavorable regime will be kept to a minimum.\nThus, we have a method for solving quadratic programs, implemented completely with ADD operations, and much of this work takes full advantage of the ADD representation (thereby, inheriting its superiority). Intuitively, one can expect significant speed-ups over matrix-based methods when the ADDs are compact, e.g. arising from structured (in a logical sense) problems."}, {"heading": "6 Empirical Illustration", "text": "Here, we aim to investigate the empirical performance of our ADD-based interior point solver. There are three main questions we wish to investigate, namely: (Q1) in the presence of symbolic structure, does our ADD-based solver perform better than its matrix-based counterpart? (Q2) On structured sparse problems, does solving with ADDs have advantages over solving with sparse matrices? And, (Q3), can the ADD-based method handle dense problems as easily as sparse problems?\nTo evaluate the performance of the approach, we implemented the entire pipeline described here, that is, a symbolic environment to specify QPs, a compiler to ADDs, based on the popular CUDD package, and the symbolic interior-point method described in the previous section.\nTo address Q1 and Q2, we applied the symbolic IPM on the problem of computing the value function of a family of Markov decision processes used in [21]. These MDPs concern a factory agent whose task is to paint two objects and connect them. A number of operations (actions) need to be performed on these objects before painting, each of which requires the use special of tools, which are possibly available. Painting and connecting can be done in different ways, yielding results of various quality, and each requiring different tools. The final product is rewarded according whether the required level\nof quality is achieved. Since these MDPs admit compact symbolic representations, we consider them good candidates to illustrate the potential advantages of symbolic optimization. The computation of an MDP value function corresponds to the following LP: min. \u2211 s:state(s) v(s) , s.t. {s : state(s), a :\nact(a)} : v(s) \u2265 rew(s)+\u03b3\u2211s\u2032:state(s\u2032) tprob(s, a, s\u2032)v(s\u2032), where s, s\u2032, a are vectors of Boolean variables, state is a Boolean formula whose models are the possible states of the MDP, act is a formula that models the possible actions, and rew and tprob are pseudo-Boolean functions that model the reward and the transition probability from s to s\u2032 under the action a. We compared our approach to a matrix implementation of the primal-dual barrier method, both algorithms terminate at the same relative residual, 10\u22125. The results are summarized in the following table.\nProblem Statistics Symbolic IPM Ground IPM name #vars #constr nnz(A) |ADD| time[s] time[s] factory 131.072 688.128 4.000.000 1819 6899 516 factory0 524.288 2.752.510 15.510.000 1895 6544 7920 factory1 2.097.150 11.000.000 59.549.700 2406 34749 159730 factory2 4.194.300 22.020.100 119.099.000 2504 36248 \u2265 48hrs.\nThe symbolic IPM outperforms the matrix-based IPM on the larger instances. However, the most striking observation is that the running time depends mostly on the size of the ADD, which essentially translates to scaling sublinear in the number of nonzeroes in the constraints matrix. To the best of our knowledge, no generic method, sparse or dense, can achieve this scaling behavior. This provides an affirmative answer to Q1 and Q2.\nOur second illustration concerns the problem of compressed sensing (CS). That is, we are interested in recovering the sparsest solution to the problem ||Ax\u2212 b||2, where A \u2208 Rm\u00d7n and n >> m where n is the signal length and m the number of measurments. While this is a hard combinatorial problem, if the matrix A admits the so-called Restricted Isometry Property, the following convex problem (Basis Pursuit Denoising - BPDN): min.x\u2208Rn \u03c4 ||x||1 + ||Ax\u2212 b||22 recovers the exact solution. The matrices typically used in CS tend to be dense, yet highly structured, often admitting an O(n log n) Fast Fourier Transform-style recur-and-cache matrix-vector multiplication scheme, making BPDN solvers efficient. A remarkable insight is that these fast recursive transforms are very similar to the ADD matrix-vector product. In fact, if we take A to be the Walsh matrix Wlog(n) which admits an ADD of size O(log n), the ADD matrix-vector product resembles the Walsh-Hadamard transform. Moreover, this compact ADD is extracted automatically from the symbolic specification of the Walsh matrix. We illustrate this in the following experiment. We apply the Symbolic IPM to the BPDN reformulation of [29], with the Walsh matrix specified symbolically, recovering random sparse vectors. We compare to the reference MATLAB code provided by Gondzio et al, which we have modified to use MATLAB\u2019s implementation on the fast Walsh-Hadamard transform. The task is to recover a sparse random vector with k = 50 normally distributed nonzero entries. Results below.\nProblem Statistics Symbolic IPM FWHT IPM [29] n m time[s] time[s] 212 210 8.3 18 213 211 20.2 28.2 214 212 46.5 43.9 215 213 99.2 65.7\nWhile the method does not scale as well as the hand-tailored solution, we demonstrate that the symbolic approach can handle dense matrices reasonably well, supporting the positive answer of Q3."}, {"heading": "7 Conclusions", "text": "A long-standing goal in machine learning and AI, which is also reflected in the philosophy on the democratization of data, is to make the specification and solving of real-world problems simple and natural, possibly even for non-experts. To this aim, we considered first-order logical mathematical programs that support individuals, relations and connectives, and and developed a new line of research of symbolically solving these programs in a generic way. In our case, a matrix-free interior point method was argued for. Our empirical results demonstrate the flexibility of this research direction.\nThe most interesting avenue for future work is to explore richer modeling languages paired with more powerful circuit representations.\nAcknowledgements The authors would like to thank the anonymous reviewers for their feedback. The work was partly supported by the DFG Collaborative Research Center SFB 876, project A6."}], "references": [{"title": "The interplay of optimization and machine learning research", "author": ["K.P. Bennett", "E. Parrado-Hern\u00e1ndez"], "venue": "JMLR, vol. 7, pp. 1265\u20131281, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, 1st ed", "author": ["L. M"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Machine learning: a probabilistic perspective", "author": ["K. Murphy"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S.J. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Proc. IJCAI, 2005, pp. 1352\u20131359.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Random-world semantics and syntactic independence for expressive languages", "author": ["D. McAllester", "B. Milch", "N.D. Goodman"], "venue": "MIT, Tech. Rep. MIT-CSAIL-TR-2008-025, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["M. Grant", "S. Boyd"], "venue": "Recent Advances in Learning and Control, ser. Lecture Notes in Control and Information Sciences, 2008, pp. 95\u2013110.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A few useful things to know about machine learning", "author": ["P.M. Domingos"], "venue": "Commun. ACM, vol. 55, no. 10, pp. 78\u201387, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Recursive decomposition for nonconvex optimization", "author": ["A. Friesen", "P. Domingos"], "venue": "IJCAI, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artif. Intell., vol. 172, no. 6-7, pp. 772\u2013799, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-terminal binary decision diagrams: An efficient data structure for matrix representation", "author": ["M. Fujita", "P. McGeer", "J.-Y. Yang"], "venue": "Formal Methods in System Design, vol. 10, no. 2, pp. 149\u2013169, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Matrix-free interior point method", "author": ["J. Gondzio"], "venue": "Comp. Opt. and Appl., vol. 51, no. 2, pp. 457\u2013480, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "AMPL: A Mathematical Programming Language", "author": ["R. Fourer", "D.M. Gay", "B.W. Kernighan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Unifying logical and statistical AI", "author": ["P. Domingos", "S. Kok", "H. Poon", "M. Richardson", "P. Singla"], "venue": "Proc. AAAI, 2006, pp. 2\u20137.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["W. Yih", "D. Roth"], "venue": "An Introduction to Statistical Relational Learning. MIT Press, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Algebraic modeling in a deductive database language", "author": ["D. Klabjan", "R. Fourer", "J. Ma"], "venue": "11th INFORMS Computing Society Conference, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Relational linear programming", "author": ["K. Kersting", "M. Mladenov", "P. Tokmakov"], "venue": "Artificial Intelligence Journal (AIJ), vol. OnlineFirst, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "First-order mixed integer linear programming", "author": ["G. Gordon", "S. Hong", "M. Dud\u00edk"], "venue": "UAI, 2009, pp. 213\u2013222.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Spudd: Stochastic planning using decision diagrams", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": "UAI, 1999, pp. 279\u2013288.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Robust optimization for hybrid mdps with state-dependent noise", "author": ["Z. Zamani", "S. Sanner", "K.V. Delgado", "L.N. de Barros"], "venue": "IJCAI, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Online symbolic gradient-based optimization for factored action mdps", "author": ["H. Cui", "R. Khardon"], "venue": "IJCAI, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A mathematical introduction to logic", "author": ["H. Enderton"], "venue": "Academic press New York,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1972}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "Computers, IEEE Transactions on, vol. 100, no. 8, pp. 677\u2013691, 1986.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}, {"title": "AMPL : a modeling language for mathematical programming", "author": ["R. Fourer", "D.M. Gay", "B.W. Kernighan"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1993}, {"title": "Interior-point methods", "author": ["F. Potra", "S.J. Wright"], "venue": "Journal of Computational and Applied Mathematics, vol. 124, pp. 281\u2013302, 2000.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Matrix Computations (3rd Ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "Matrix-free interior point method for compressed sensing problems", "author": ["K. Fountoulakis", "J. Gondzio", "P. Zhlobich"], "venue": "Mathematical Programming Computation, vol. 6, no. 1, pp. 1\u201331, 2014. 10", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3].", "startOffset": 237, "endOffset": 246}, {"referenceID": 1, "context": "It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3].", "startOffset": 237, "endOffset": 246}, {"referenceID": 2, "context": "It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3].", "startOffset": 237, "endOffset": 246}, {"referenceID": 3, "context": "An emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures.", "startOffset": 116, "endOffset": 125}, {"referenceID": 4, "context": "An emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures.", "startOffset": 116, "endOffset": 125}, {"referenceID": 5, "context": "An emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures.", "startOffset": 116, "endOffset": 125}, {"referenceID": 6, "context": "The approach taken in recent influential proposals, such as disciplined programming [7], is to carefully constrain the specification language so as to provide a structured interface between the model and the solver, by means of which geometric properties such as the curvature of the objective can be inferred.", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "Doing so has the potential to greatly simplify the specification and prototyping of AI and ML models [8, 1].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "Doing so has the potential to greatly simplify the specification and prototyping of AI and ML models [8, 1].", "startOffset": 101, "endOffset": 107}, {"referenceID": 8, "context": "Local structure, on the other hand, can be exploited [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "In probabilistic inference, local structure has enabled tractability in high-treewidth models, culminating in the promising direction of using circuits and decision-diagrams for the underlying graphical model [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "Here, we would like to consider the idea of using such a data structure for optimization, for which we turn to early pioneering research on algebraic decision diagrams (ADDs) that supports efficient matrix manipulations (compositionality) and caching of submatrices (repeated local structure) [11, 12].", "startOffset": 293, "endOffset": 301}, {"referenceID": 11, "context": "We employ ideas from the matrix-free interior point method [13], which appeals to an iterative linear equation solver together with the log-barrier method, to achieve a regime where the constraint matrix is only accessed through matrix-vector multiplications.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "[14, 15].", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "Disciplined programming [7] enables an object-oriented approach to constructing optimization problems, but falls short of our desiderata as argued before.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]).", "startOffset": 52, "endOffset": 60}, {"referenceID": 14, "context": "Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]).", "startOffset": 52, "endOffset": 60}, {"referenceID": 15, "context": "Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]).", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "This allows the specification of correlations, groups and properties in a natural manner, as also observed elsewhere [19, 20].", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "This allows the specification of correlations, groups and properties in a natural manner, as also observed elsewhere [19, 20].", "startOffset": 117, "endOffset": 125}, {"referenceID": 18, "context": "representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings.", "startOffset": 88, "endOffset": 96}, {"referenceID": 20, "context": "representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings.", "startOffset": 88, "endOffset": 96}, {"referenceID": 21, "context": "To prepare for the syntax of our highlevel mathematical programming language, we recap basic notions from mathematical logic [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "statistical relational learning [16].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "A BDD [25] is a compact and efficiently manipulable data structure for a Boolean function f : {0, 1} \u2192 {0, 1} .", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "ADDs generalize BDDs in representing functions of the form {0, 1} \u2192 R, and so inherit the same structural properties as BDDs except, of course, that terminal nodes are labeled with real numbers [11, 12].", "startOffset": 194, "endOffset": 202}, {"referenceID": 23, "context": "While (some) high-level features are prominent in many optimization packages such as AMPL [26], they are reduced (naively) to canonical forms transparently to the user, and do not embody logical reasoning.", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "Logic programming is supported in other proposals [18], but their restricted use of negation makes it difficult to understand the implications when modeling a domain.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "We take our cue from statistical relational learning [16], as considered in [19], to support any (finite) fragment of first-order logic with classical interpretations for operators.", "startOffset": 53, "endOffset": 57}, {"referenceID": 16, "context": "We take our cue from statistical relational learning [16], as considered in [19], to support any (finite) fragment of first-order logic with classical interpretations for operators.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "(Quantifiers are eliminated as usual: existentials as disjunctions and universals as conjunctions [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "To guide the construction of the engine, we will briefly go over the operations previously established as efficient with ADDs [11, 12], and some implications thereof for a solver strategy.", "startOffset": 126, "endOffset": 134}, {"referenceID": 10, "context": "Theorem 3: [11, 12] Suppose A,A\u2032 are real-valued matrices.", "startOffset": 11, "endOffset": 19}, {"referenceID": 10, "context": "We refer interested readers to [11, 12] for the complexity-theoretic properties of these operations.", "startOffset": 31, "endOffset": 39}, {"referenceID": 10, "context": "Moreover, from Corollary 4, vector operations are efficiently implementable, which implies that given an ADD for A, x and b, the computation of the residual y = Ax\u2212b is more efficient than its matrix counterpart [11].", "startOffset": 212, "endOffset": 216}, {"referenceID": 24, "context": "[27], sketched in Alg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The underlying idea is as follows [27, 13]: by applying a perturbed Newton method to the equalities in the above system, the algorithm progresses the current solution along a direction obtained by solving the following linear system: \uf8ee\uf8f0 A 0 0 \u2212Q A I", "startOffset": 34, "endOffset": 42}, {"referenceID": 11, "context": "The underlying idea is as follows [27, 13]: by applying a perturbed Newton method to the equalities in the above system, the algorithm progresses the current solution along a direction obtained by solving the following linear system: \uf8ee\uf8f0 A 0 0 \u2212Q A I", "startOffset": 34, "endOffset": 42}, {"referenceID": 24, "context": "where \u0398 is the diagonal matrix \u0398ii = xi si [27, 13].", "startOffset": 43, "endOffset": 51}, {"referenceID": 11, "context": "where \u0398 is the diagonal matrix \u0398ii = xi si [27, 13].", "startOffset": 43, "endOffset": 51}, {"referenceID": 11, "context": "(The general case of going beyond these assumptions is omitted here for space reasons [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "To solve (2), we employ the conjugate gradient method [28], sketched in Fig.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "To remedy this situation, Gondzio [13] proposes the following approach: first, the system can be regularized to achieve a condition number bounded by the largest singular value of A.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "More details on this can be found in [13].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "As demonstrated in [13] this approach does lead to a practical algorithm.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "To address Q1 and Q2, we applied the symbolic IPM on the problem of computing the value function of a family of Markov decision processes used in [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "We apply the Symbolic IPM to the BPDN reformulation of [29], with the Walsh matrix specified symbolically, recovering random sparse vectors.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "Problem Statistics Symbolic IPM FWHT IPM [29] n m time[s] time[s] 2 2 8.", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "A recent trend in probabilistic inference emphasizes the codification of models in a formal syntax, with suitable high-level features such as individuals, relations, and connectives, enabling descriptive clarity, succinctness and circumventing the need for the modeler to engineer a custom solver. Unfortunately, bringing these linguistic and pragmatic benefits to numerical optimization has proven surprisingly challenging. In this paper, we turn to these challenges: we introduce a rich modeling language, for which an interior-point method computes approximate solutions in a generic way. While logical features easily complicates the underlying model, often yielding intricate dependencies, we exploit and cache local structure using algebraic decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently realizable in ADDs, but we argue and show that well-known second-order methods are not ideal for ADDs. Our engine, therefore, invokes a sophisticated matrix-free approach. We demonstrate the flexibility of the resulting symbolic-numeric optimizer on decision making and compressed sensing tasks with millions of non-zero entries.", "creator": "LaTeX with hyperref package"}}}