{"id": "1605.08882", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Optimal Learning for Multi-pass Stochastic Gradient Methods", "abstract": "We analyze the learning characteristics of the stochastic gradient method when multiple runs over the data are allowed and mini-batches are permitted. In particular, we look at the square loss and show that in a universal choice of step size, the number of passes acts as regularization parameters and optimal finite sample limits can be achieved by early stopping. Furthermore, we show that larger step sizes are permissible when considering mini-batches. Our analysis is based on a uniform approach that includes both batch and stochastic gradient methods as special cases.", "histories": [["v1", "Sat, 28 May 2016 12:11:22 GMT  (76kb,D)", "http://arxiv.org/abs/1605.08882v1", "31 pages, 6 figures"], ["v2", "Sat, 21 Oct 2017 22:55:48 GMT  (91kb,D)", "http://arxiv.org/abs/1605.08882v2", "Extended versions of the previous one. Fixed some typos, JMLR, 2017"]], "COMMENTS": "31 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["junhong lin", "lorenzo rosasco"], "accepted": true, "id": "1605.08882"}, "pdf": {"name": "1605.08882.pdf", "metadata": {"source": "CRF", "title": "Optimal Learning for Multi-pass Stochastic Gradient Methods", "authors": ["Junhong Lin", "Lorenzo Rosasco"], "emails": ["jhlin5@hotmail.com", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [1]. This has motivated a recent interest in stochastic gradient methods (SGM), since on the one hand they enjoy good practical performances, especially in large scale scenarios, and on the other hand they are amenable to theoretical studies. In particular, unlike other learning approaches, such as empirical risk minimization or Tikhonov regularization, theoretical results on SGM naturally integrate statistical and computational aspects.\nMost generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]). In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6]. These latter works show that balancing these contributions, it is possible to derive a step-size choice leading to optimal learning bounds. Such a choice typically depends on some unknown properties of the data generating distributions and in practice can be chosen by cross-validation.\nWhile processing each data point only once is natural in streaming/online scenarios, in practice SGM is often used as a tool for processing large data-sets and multiple passes over the data are typically considered. In this case, the number of passes over the data, as well as the step-size, need then to be determined. While the role of multiple passes is well understood if the goal is empirical risk minimization [9], its effect with respect to generalization is less clear and a few recent works have recently started to tackle this question. In particular, results in this direction have been derived in [10] and [11]. The former work considers a general stochastic optimization\nar X\niv :1\n60 5.\n08 88\n2v 1\n[ cs\n.L G\n] 2\n8 M\nay 2\nsetting and studies stability properties of SGM allowing to derive convergence results as well as finite sample bounds. The latter work, restricted to supervised learning, further develops these results to compare the respective roles of step-size and number of passes, and show how different parameter settings can lead to optimal error bounds. In particular, it shows that there are two extreme cases: one between the step-size or the number of passes is fixed a priori, while the other one acts as a regularization parameter and needs to be chosen adaptively. The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization. Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17]. This latter strategy is often considered especially for parallel implementation of SGM.\nThe study in this paper, fills in these gaps in the case where the loss function is the least squares loss. We consider a variant of SGM for least squares, where gradients are sampled uniformly at random and mini-batches are allowed. The number of passes, the step-size and the mini-batch size are then parameters to be determined. Our main results highlight the respective roles of these parameters and show how can they be chosen so that the corresponding solutions achieve optimal learning errors. In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13]. Further, our analysis shows how the minibatch size and the step-size choice are tightly related. Indeed, larger mini-batch sizes allow to consider larger step-sizes while keeping the optimal learning bounds. This result could give an insight on how to exploit mini-batches for parallel computations while preserving optimal statistical accuracy. Finally we note that a recent work [19] is tightly related to the analysis in the paper. The generalization properties of a multi-pass incremental gradient are analyzed in [19], for a cyclic, rather than a stochastic, choice of the gradients and with no mini-batches. The analysis in this latter case appears to be harder and results in [19] give good learning bounds only in restricted setting and considering iterates rather than the excess risk. Compared to [19] our results show how stochasticity can be exploited to get faster capacity dependent rates and analyze the role of mini-batches.\nThe rest of this paper is organized as follows. Section 2 introduces the learning setting and the SGM algorithm. Main results with discussions and proof sketches are presented in Section 3. Finally, simple numerical simulations are given in Section 4 to complement our theoretical results.\nNotation For any a, b \u2208 R, a \u2228 b denotes the maximum of a and b. N is the set of all positive integers. For any T \u2208 N, [T ] denotes the set {1, \u00b7 \u00b7 \u00b7 , T}. For any two positive sequences {at}t\u2208[T ] and {bt}t\u2208[T ], the notation at . bt for all t \u2208 [T ] means that there exists a positive constant C \u2265 0 such that C is independent of t and that at \u2264 Cbt for all t \u2208 [T ]."}, {"heading": "2 Learning with SGM", "text": "We begin by introducing the learning setting we consider, and then describe the SGM learning algorithm. Following [19], the formulation we consider is close to the setting of functional regression, and covers the reproducing kernel Hilbert space (RKHS) setting as special cases. In\nparticular, it reduces to standard linear regression for finite dimensions."}, {"heading": "2.1 Learning Problems", "text": "Let H be a separable Hilbert space, with inner product and induced norm denoted by \u3008\u00b7, \u00b7\u3009H and \u2016 \u00b7 \u2016H , respectively. Let the input space X \u2286 H and the output space Y \u2286 R. Let \u03c1 be an unknown probability measure on Z = X \u00d7 Y, \u03c1X(\u00b7) the induced marginal measure on X, and \u03c1(\u00b7|x) the conditional probability measure on Y with respect to x \u2208 X and \u03c1.\nConsidering the square loss function, the problem under study is the minimization of the\nrisk,\ninf \u03c9\u2208H E(\u03c9), E(\u03c9) = \u222b X\u00d7Y (\u3008\u03c9, x\u3009H \u2212 y)2d\u03c1(x, y), (1)\nwhen the measure \u03c1 is known only through a sample z = {zi = (xi, yi)}mi=1 of size m \u2208 N, independently and identically distributed (i.i.d.) according to \u03c1. In the following, we measure the quality of an approximate solution \u03c9\u0302 \u2208 H (an estimator) considering the excess risk, i.e.,\nE(\u03c9\u0302)\u2212 inf \u03c9\u2208H E(\u03c9). (2)\nThroughout this paper, we assume that there exists a constant \u03ba \u2208 [1,\u221e[, such that\n\u3008x, x\u2032\u3009H \u2264 \u03ba2, \u2200x, x\u2032 \u2208 X. (3)"}, {"heading": "2.2 Stochastic Gradient Method", "text": "We study the following SGM (with mini-batches).\nAlgorithm 1. Let b \u2208 [m]. Given any sample z, the b-minibatch stochastic gradient method is defined by \u03c91 = 0 and\n\u03c9t+1 = \u03c9t \u2212 \u03b7t 1\nb bt\u2211 i=b(t\u22121)+1 (\u3008\u03c9t, xji\u3009H \u2212 yji)xji , t = 1, . . . , T, (4)\nwhere {\u03b7t > 0} is a step-size sequence. Here, j1, j2, \u00b7 \u00b7 \u00b7 , jbT are independent and identically distributed (i.i.d.) random variables from the uniform distribution on [m] 1.\nDifferent choices for the (mini-)batch size b can lead to different algorithms. In particular, for b = 1, the above algorithm corresponds to a simple SGM, while for b = m, it is a stochastic version of the batch gradient descent. The aim of this paper is to derive excess risk bounds for the above algorithm under appropriate assumptions. Throughout this paper, we assume that {\u03b7t}t is non-increasing, and T \u2208 N with T \u2265 3. We denote by Jt the set {jl : l = b(t\u2212 1) + 1, \u00b7 \u00b7 \u00b7 , bt} and by J the set {jl : l = 1, \u00b7 \u00b7 \u00b7 , bT}."}, {"heading": "3 Main Results with Discussions", "text": "In this section, we first state some basic assumptions. Then, we present and discuss our main results.\n1Note that, the random variables j1, \u00b7 \u00b7 \u00b7 , jbT are conditionally independent given the sample z."}, {"heading": "3.1 Assumptions", "text": "We first make the following assumption.\nAssumption 1. There exists constants M \u2208]0,\u221e[ and v \u2208]1,\u221e[ such that\u222b Y y2ld\u03c1(y|x) \u2264 l!M lv, \u2200l \u2208 N, (5) \u03c1X-almost surely.\nAssumption (5) is related to a moment hypothesis on |y|2. It is weaker than the often considered bounded output assumption, and trivially verified in binary classification problems where Y = {\u22121, 1}. To present our next assumption, we introduce the operator L : L2(H, \u03c1X) \u2192 L2(H, \u03c1X),\ndefined by L(f) = \u222b X \u3008x, \u00b7\u3009Hf(x)\u03c1X(x). Under Assumption (3), L can be proved to be positive trace class operators, and hence L\u03b6 with \u03b6 \u2208 R can be defined by using the spectrum theory [20]. The Hilbert space of square integral functions from H to R with respect to \u03c1X , with induced norm given by \u2016f\u2016\u03c1 = (\u222b X |f(x)|2d\u03c1X(x) )1/2 , is denoted by (L2(H, \u03c1X), \u2016 \u00b7 \u2016\u03c1). It is well known\nthat the function minimizing \u222b Z\n(f(x)\u2212 y)2d\u03c1(z) over all measurable functions f : H \u2192 R is the regression function, which is given by\nf\u03c1(x) = \u222b Y yd\u03c1(y|x), x \u2208 X. (6)\nDefine another Hilbert space H\u03c1 = {f : X \u2192 R|\u2203\u03c9 \u2208 H with f(x) = \u3008\u03c9, x\u3009H , \u03c1X -almost surely}. Under Assumption 3, it is easy to see that H\u03c1 is a subspace of L 2(H, \u03c1X). Let fH be the projection of the regression function f\u03c1 onto the closure of H\u03c1 in L 2(H, \u03c1X). It is easy to see that the search for a solution of Problem (1) is equivalent to the search of a linear function from H\u03c1 to approximate fH. From this point of view, bounds on the excess risk of a learning algorithm naturally depend on the following assumption, which quantifies how well, the target function fH can be approximated by H\u03c1. Assumption 2. There exist \u03b6 > 0 and R > 0, such that \u2016L\u2212\u03b6fH\u2016\u03c1 \u2264 R.\nThe above assumption is fairly standard [20, 19] in non-parametric regression. The bigger \u03b6 is, the more stringent the assumption is, since L\u03b61(L2(H, \u03c1X)) \u2286 L\u03b62(L2(H, \u03c1X)) when \u03b61 \u2265 \u03b62. In particular, for \u03b6 = 0, we are assuming \u2016fH\u2016\u03c1 < \u221e, while for \u03b6 = 1/2, we are requiring fH \u2208 H\u03c1, since [21, 19]\nH\u03c1 = L1/2(L2(H, \u03c1X)).\nFinally, the last assumption relates to the capacity of the hypothesis space.\nAssumption 3. For some \u03b3 \u2208]0, 1] and c\u03b3 > 0, L satisfies\ntr(L(L+ \u03bbI)\u22121) \u2264 c\u03b3\u03bb\u2212\u03b3 , for all \u03bb > 0. (7)\nThe left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13]. It can be related to covering/entropy number conditions, see [21] for further details. Assumption 3 is always true for \u03b3 = 1 and c\u03b3 = \u03ba 2, since L is a trace class operator which implies the\neigenvalues of L, denoted as \u03c3i, satisfy tr(L) = \u2211 i \u03c3i \u2264 \u03ba2. This is referred as the capacity independent setting. Assumption 3 with \u03b3 \u2208]0, 1] allows to derive better error rates. It is satisfied, for example, if the eigenvalues of L satisfy a polynomial decaying condition \u03c3i \u223c i\u2212\u03b3 , or with \u03b3 = 0 if L is finite rank."}, {"heading": "3.2 Main Results", "text": "We start with the following corollary, which is a simplified version of our main results stated next.\nCorollary 3.1. Under Assumptions 2 and 3, let \u03b6 \u2265 1/2 and |y| \u2264 M \u03c1X-almost surely for some M > 0. Consider the SGM with 1) p\u2217 = dm 1\n2\u03b6+\u03b3 e, b = 1, \u03b7t ' 1m for all t \u2208 [(p\u2217m)], and \u03c9\u0303p\u2217 = \u03c9p\u2217m+1. If m is large enough, with high probability2, there holds\nEJ[E(\u03c9\u0303p\u2217)]\u2212 inf \u03c9\u2208H E . m\u2212 2\u03b6 2\u03b6+\u03b3 .\nFurthermore, the above also holds for the SGM with3 2) or p\u2217 = dm 1 2\u03b6+\u03b3 e, b = \u221a m, \u03b7t ' 1\u221am for all t \u2208 [(p\u2217 \u221a m)], and \u03c9\u0303p\u2217 = \u03c9p\u2217 \u221a m+1.\nIn the above, p\u2217 is the number of \u2018passes\u2019 over the data, which is defined as d btme at t iterations. The above result asserts that, at p\u2217 passes over the data, the simple SGM with fixed step-size achieves optimal learning error bounds, matching those of ridge regression [13]. Furthermore, using mini-batch allows to use a larger step-size while achieving the same optimal error bounds.\nOur main theorem of this paper is stated next, and provides error bounds for the studied algorithm. For the sake of readability, we only consider the case \u03b6 \u2265 1/2 in a fixed step-size setting. General results in a more general setting (\u03b7t = \u03b71t\n\u2212\u03b8 with 0 \u2264 \u03b8 < 1, and/or the case \u03b6 \u2208]0, 1/2]) can be found in the appendix.\nTheorem 3.2. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2, \u03b4 \u2208]0, 1[, \u03b7t = \u03b7\u03ba\u22122 for all t \u2208 [T ], with \u03b7 \u2264 18(log T+1) . If m \u2265 m\u03b4, then the following holds with probability at least 1 \u2212 \u03b4: for all t \u2208 [T ],\nEJ[E(\u03c9t+1)]\u2212 inf \u03c9\u2208H E \u2264 q1(\u03b7t)\u22122\u03b6 + q2m\u2212 2\u03b6 2\u03b6+\u03b3 (1 +m\u2212 1 2\u03b6+\u03b3 \u03b7t)2 log2 T log2\n1 \u03b4\n+q3\u03b7b \u22121(1 \u2228m\u2212 1 2\u03b6+\u03b3 \u03b7t) log T.\n(8)\nHere, m\u03b4, q1, q2 and q3 are positive constants depending on \u03ba 2, \u2016T \u2016,M, v, \u03b6, R, c\u03b3 , \u03b3, and m\u03b4 also on \u03b4 (which will be given explicitly in the proof).\nThere are three terms in the upper bounds of (8). The first term depends on the regularity of the target function and it arises from bounding the bias, while the last two terms result from estimating the sample variance and the computational variance (due to the random choices of the points), respectively. To derive optimal rates, it is necessary to balance these three terms. Solving this trade-off problem leads to different choices on \u03b7, T , and b, corresponding to different regularization strategies, as shown in subsequent corollaries.\nThe first corollary gives generalization error bounds for SGM, with a universal step-size\ndepending on the number of sample points.\nCorollary 3.3. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2 , b = 1 and \u03b7t ' 1m for all t \u2208 [T ], where T \u2264 m2. If m \u2265 m0, then with probability at least 1\u2212 1/m, there holds\nEJ[E(\u03c9t+1)]\u2212 inf \u03c9\u2208H E . {(m t )2\u03b6 +m\u2212 2\u03b6+2 2\u03b6+\u03b3 ( t m )2} \u00b7 log4m, \u2200t \u2208 [T ], (9)\n2Here, \u2018high probability\u2019 refers to the sample z. 3Here, we assume that \u221a m is an integer.\nand in particular,\nEJ[E(\u03c9T\u2217+1)]\u2212 inf \u03c9\u2208H E . m\u2212 2\u03b6 2\u03b6+\u03b3 log4m, (10)\nwhere T \u2217 = dm 2\u03b6+\u03b3+1 2\u03b6+\u03b3 e. Here, m0 is a positive integer depending only on \u03ba, \u2016T \u2016, \u03b6 and \u03b3, and will be given explicitly in the proof.\nRemark 3.4. Ignoring the logarithmic term and letting t = pm, Eq. (9) becomes\nEJ[E(\u03c9pm+1)]\u2212 inf \u03c9\u2208H E . p\u22122\u03b6 +m\u2212 2\u03b6+2 2\u03b6+\u03b3 p2."}, {"heading": "A smaller p may lead to a larger bias, while a larger p may lead to a larger sample error. From", "text": "this point of view, p has a regularization effect.\nThe second corollary provides error bounds for SGM with a fixed mini-batch size and a fixed\nstep-size (which depend on the number of sample points). Corollary 3.5. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2, b = d \u221a me and \u03b7t ' 1\u221am for all t \u2208 [T ], where T \u2264 m2. If m \u2265 m0, then with probability at least 1\u2212 1/m, there holds\nEJ[E(\u03c9t+1)]\u2212 inf \u03c9\u2208H E . {(\u221am t )2\u03b6 +m\u2212 2\u03b6+2 2\u03b6+\u03b3 ( t\u221a m )2} log4m, \u2200t \u2208 [T ], (11)\nand particularly,\nEJ[E(\u03c9T\u2217+1)]\u2212 inf \u03c9\u2208H E . m\u2212 2\u03b6 2\u03b6+\u03b3 log4m, (12)\nwhere T \u2217 = dm 1 2\u03b6+\u03b3+ 1 2 e.\nThe above two corollaries follow from Theorem 3.2 with the simple observation that the dominating terms in (8) are the terms related to the bias and the sample variance, when a small step-size is chosen. The only free parameter in (9) and (11) is the number of iterations/passes. The ideal stopping rule is achieved by balancing the two terms related to the bias and the sample variance, showing the regularization effect of the number of passes. Since the ideal stopping rule depends on the unknown parameters \u03b6 and \u03b3, a hold-out cross-validation procedure is often used to tune the stopping rule in practice. Using an argument similar to that in Chapter 6 from [21], it is possible to show that this procedure can achieve the same convergence rate.\nWe give some further remarks. First, the upper bound in (10) is optimal up to a logarithmic factor, in the sense that it matches the minimax lower rate in [13]. Second, according to Corollaries 3.3 and 3.5, bT \u2217\nm ' m 1 2\u03b6+\u03b3 passes over the data are needed to obtain optimal rates in both\ncases. Finally, in comparing the simple SGM and the mini-batch SGM, Corollaries 3.3 and 3.5 show that a larger step-size is allowed to use for the latter.\nIn the next result, both the step-size and the stopping rule are tuned to obtain optimal rates for simple SGM with multiple passes. In this case, the step-size and the number of iterations are the regularization parameters.\nCorollary 3.6. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2, b = 1 and \u03b7t ' m\u2212 2\u03b6 2\u03b6+\u03b3 for all t \u2208 [T ], where T \u2264 m2. If m \u2265 m0, and T \u2217 = dm 2\u03b6+1 2\u03b6+\u03b3 e, then (10) holds with probability at least 1\u2212 1/m.\nRemark 3.7. If we make no assumption on the capacity, i.e., \u03b3 = 1, Corollary 3.6 recovers the result in [4] for one pass SGM.\nThe next corollary shows that for some suitable mini-batch sizes, optimal rates can be achieved with a constant step-size (which is nearly independent of the number of sample points) by early stopping.\nCorollary 3.8. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2, b = dm 2\u03b6\n2\u03b6+\u03b3 e and \u03b7t ' 1logm for all t \u2208 [T ], where T \u2264 m2. If m \u2265 m0, and T \u2217 = dm 1 2\u03b6+\u03b3 e, then (10) holds with probability at least 1\u2212 1/m.\nAccording to Corollaries 3.6 and 3.8, around m 1\u2212\u03b3 2\u03b6+\u03b3 passes over the data are needed to achieve the best performance in the above two strategies. In comparisons with Corollaries 3.3 and 3.5 where around m \u03b6+1 2\u03b6+\u03b3 passes are required, the latter seems to require fewer passes over the data. However, in this case, one might have to run the algorithms multiple times to tune the step-size, or the mini-batch size.\nFinally, the last result gives generalization error bounds for \u2018batch\u2019 SGM with a constant\nstep-size (nearly independent of the number of sample points).\nCorollary 3.9. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2, b = m and \u03b7t ' 1logm for all t \u2208 [T ], where T \u2264 m2. If m \u2265 m0, and T \u2217 = dm 1 2\u03b6+\u03b3 e, then (10) holds with probability at least 1\u2212 1/m.\nAs will be seen in the proof from the appendix, the above result also holds when replacing the sequence {\u03c9t} by the sequence {\u03bdt}t generated from real batch GM in (14). In this sense, we study the gradient-based learning algorithms simultaneously."}, {"heading": "3.3 Discussions", "text": "We compare our results with previous works. For non-parametric regression with the square loss, one pass SGM has been studied in, e.g., [4, 22, 5, 6]. In particular, [4] proved capacity independent rate of order O(m\u2212 2\u03b6 2\u03b6+1 logm) with a fixed step-size \u03b7 ' m\u2212 2\u03b6 2\u03b6+1 , and [6] derived capacity dependent error bounds of order O(m\u2212 2min(\u03b6,1) 2min(\u03b6,1)+\u03b3 ) (when 2\u03b6 + \u03b3 > 1) for the average. Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6\n2\u03b6+1 ) assuming that \u03b6 \u2208 [ 12 , 1]. In comparison with these existing convergence rates, our rates from (10) are comparable, either involving the capacity condition, or allowing a broader regularity parameter \u03b6 (which thus improves the rates).\nMore recently, [19] studied multiple passes SGM with a fixed ordering at each pass, also called incremental gradient method. Making no assumption on the capacity, rates of order O(m\u2212 \u03b6 \u03b6+1 ) (in L2(H, \u03c1X)-norm) with a universal step-size \u03b7 ' 1m are derived. In comparisons, Corollary 3.3 achieves better rates, while considering the capacity assumption. Note also that [19] proved sharp rate in H-norm for \u03b6 \u2265 1/2 in the capacity independent case. In fact, we can extend our analysis to the H-norm for Algorithm 4. We postpone this extension to a longer version of this paper.\nThe idea of using mini-batches (and parallel implements) to speed up SGM in a general stochastic optimization setting can be found, e.g., in [14, 15, 16, 17]. Our theoretical findings, especially the interplay between the mini-batch size and the step-size, can give further insights on parallelization learning. Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size \u03b7 ' b/ \u221a m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence\nrate is of order O( \u221a 1/m + b/m) by considering an averaging scheme. When adapting to the learning setting we consider, this reads as that if fH \u2208 H\u03c1, i.e., \u03b6 = 1/2, the convergence rate for the average is O( \u221a 1/m+ b/m). Note that, fH does not necessarily belongs to H\u03c1 in general. Also, our derived convergence rate from Corollary 3.5 is better, when the regularity parameter \u03b6 is greater than 1/2, or \u03b3 is smaller than 1."}, {"heading": "3.4 Error Decomposition", "text": "The key to our proof is a novel error decomposition, which may be also used in analysing other learning algorithms. We first introduce two sequences. The population iteration is defined by \u00b51 = 0 and\n\u00b5t+1 = \u00b5t \u2212 \u03b7t \u222b X (\u3008\u00b5t, x\u3009H \u2212 f\u03c1(x))xd\u03c1X(x), t = 1, . . . , T. (13)\nThe above iterated procedure is ideal and can not be implemented in practice, since the distribution \u03c1X is unknown in general. Replacing \u03c1X by the empirical measure and f\u03c1(xi) by yi, we derive the sample iteration (associated with the sample z), i.e., \u03bd1 = 0 and\n\u03bdt+1 = \u03bdt \u2212 \u03b7t 1\nm m\u2211 i=1 (\u3008\u03bdt, xi\u3009H \u2212 yi)xi, t = 1, . . . , T. (14)\nClearly, \u00b5t is deterministic and \u03bdt is a H-valued random variable depending on z. Given the sample z, the sequence {\u03bdt}t has a natural relationship with the learning sequence {\u03c9t}t, since\nEJ[\u03c9t] = \u03bdt. (15)\nIndeed, taking the expectation with respect to Jt on both sides of (4), and noting that \u03c9t depends only on J1, \u00b7 \u00b7 \u00b7 ,Jt\u22121 (given any z), one has\nEJt [\u03c9t+1] = \u03c9t \u2212 \u03b7t 1\nm m\u2211 i=1 (\u3008\u03c9t, xi\u3009H \u2212 yi)xi,\nand thus,\nEJ[\u03c9t+1] = EJ[\u03c9t]\u2212 \u03b7t 1\nm m\u2211 i=1 (\u3008EJ[\u03c9t], xi\u3009H \u2212 yi)xi, t = 1, . . . , T,\nwhich satisfies the iterative relationship given in (14). By an induction argument, (15) can then be proved.\nLet S\u03c1 : H \u2192 L2(H, \u03c1X) be the linear map defined by (S\u03c1\u03c9)(x) = \u3008\u03c9, x\u3009H ,\u2200\u03c9, x \u2208 H. We have the following error decomposition.\nProposition 3.10. We have\nEJ[E(\u03c9t)]\u2212 inf f\u2208H E(f) \u2264 2\u2016S\u03c1\u00b5t \u2212 fH\u20162\u03c1 + 2\u2016S\u03c1\u03bdt \u2212 S\u03c1\u00b5t\u20162\u03c1 + EJ[\u2016S\u03c1\u03c9t \u2212 S\u03c1\u03bdt\u20162]. (16)\nProof. For any \u03c9 \u2208 H, we have [21, 19]\nE(\u03c9)\u2212 inf f\u2208H E(f) = \u2016S\u03c1\u03c9 \u2212 fH\u20162\u03c1. (17)\nThus, E(\u03c9t)\u2212 inff\u2208H E(f) = \u2016S\u03c1\u03c9t \u2212 fH\u20162\u03c1, and\nEJ[\u2016S\u03c1\u03c9t \u2212 fH\u20162\u03c1] = EJ[\u2016S\u03c1\u03c9t \u2212 S\u03c1\u03bdt + S\u03c1\u03bdt \u2212 fH\u20162\u03c1] = EJ[\u2016S\u03c1\u03c9t \u2212 S\u03c1\u03bdt\u20162\u03c1 + \u2016S\u03c1\u03bdt \u2212 fH\u20162\u03c1] + 2EJ\u3008S\u03c1\u03c9t \u2212 S\u03c1\u03bdt,S\u03c1\u03bdt \u2212 fH\u3009\u03c1.\nUsing (15) to the above, we get EJ[\u2016S\u03c1\u03c9t \u2212 fH\u20162\u03c1] = EJ[\u2016S\u03c1\u03c9t \u2212 S\u03c1\u03bdt\u20162\u03c1 + \u2016S\u03c1\u03bdt \u2212 fH\u20162\u03c1]. Now the proof can be finished by considering\n\u2016S\u03c1\u03bdt \u2212 fH\u20162\u03c1 = \u2016S\u03c1\u03bdt \u2212 S\u03c1\u00b5t + S\u03c1\u00b5t \u2212 fH\u20162\u03c1 \u2264 2\u2016S\u03c1\u03bdt \u2212 S\u03c1\u00b5t\u20162\u03c1 + 2\u2016S\u03c1\u00b5t \u2212 S\u03c1fH\u20162\u03c1.\nThere are three terms in the upper bound of the error decomposition (16). We refer to the deterministic term \u2016S\u03c1\u00b5t \u2212 fH\u20162\u03c1 as the bias, the term \u2016S\u03c1\u03bdt \u2212 S\u03c1\u00b5t\u20162\u03c1 depending on z as the sample variance, and EJ[\u2016S\u03c1\u03c9t \u2212S\u03c1\u03bdt\u20162\u03c1] as the computational variance. These three terms will be estimated in the appendix, see Lemma B.2, Theorem C.6 and Theorem D.9. The bound in Theorem 3.2 thus follows plugging these estimations in the error decomposition."}, {"heading": "4 Numerical Simulations", "text": "In order to illustrate our theoretical results and the error decomposition, we first performed some simulations on a simple problem. We constructed m = 100 i.i.d. training examples of the form y = f\u03c1(xi)+\u03c9i. Here, the regression function is f\u03c1(x) = |x\u22121/2|\u22121/2, the input point xi is uniformly distributed in [0, 1], and \u03c9i is a Gaussian noise with zero mean and standard deviation 1, for each i \u2208 [m]. We perform three experiments with the same H, a RKHS associated with a Gaussian kernel K(x, x\u2032) = exp(\u2212(x \u2212 x\u2032)2/(2\u03c32)) where \u03c3 = 0.2. In the first experiment, we run mini-batch SGM, where the mini-batch size b = \u221a m, and the step-size \u03b7t = 1/(8 \u221a m). In the second experiment, we run simple SGM where the step-size is fixed as \u03b7t = 1/(8m), while in the third experiment, we run batch GM using the fixed step-size \u03b7t = 1/8. For each experiment,\nwe run the algorithm 50 times. For mini-batch SGM and SGM, the total error \u2016S\u03c1\u03c9t \u2212 f\u03c1\u20162L2\u03c1\u0302 , the bias \u2016S\u03c1\u00b5\u0302t \u2212 f\u03c1\u20162L2\u03c1\u0302 , the sample variance \u2016S\u03c1\u03bdt \u2212 S\u03c1\u00b5\u0302t\u2016 2 L2\u03c1\u0302 and the computational variance \u2016S\u03c1\u03c9t \u2212 S\u03c1\u03bdt\u20162L2\u03c1\u0302 , averaged over 50 trials, are depicted in Figures 1a and 1b, respectively. For batch GM, the total error \u2016S\u03c1\u03bdt \u2212 f\u03c1\u20162L2\u03c1\u0302 , the bias \u2016S\u03c1\u00b5\u0302t \u2212 f\u03c1\u2016 2 L2\u03c1\u0302 and the sample variance \u2016S\u03c1\u03bdt \u2212 \u00b5\u0302t\u20162L2\u03c1\u0302 , averaged over 50 trials are depicted in Figure 1c. Here, we replace the unknown marginal distribution \u03c1X by an empirical measure \u03c1\u0302 = 1\n2000 \u22112000 i=1 \u03b4x\u0302i , where each x\u0302i is uniformly\ndistributed in [0, 1]. From Figure 1a or 1b, we see that as the number of passes increases4, the bias decreases, while the sample error increases. Furthermore, we see that in comparisons with the bias and the sample error, the computational error is negligible. In all these experiments, the minimal total error is achieved when the bias and the sample error are balanced. These empirical results show the effects of the three terms from the error decomposition, and complement the derived bound (8), as well as the regularization effect of the number of passes over the data. Finally, we tested the simple SGM, mini-batch SGM, and batch GM, using similar step-sizes as those in the first simulation, on the BreastCancer data-set [24]. The classification errors on the training set and the testing set of these three algorithms are depicted in Figure 2. We see that all of these algorithms perform similarly, which complement the bounds in Corollaries 3.3, 3.5 and 3.9."}, {"heading": "Acknowledgments", "text": "This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. L. R. acknowledges the financial support of the Italian Ministry of Education, University and Research FIRB project RBFR12M3AC."}, {"heading": "A Preliminary", "text": ""}, {"heading": "A.1 Notation", "text": "We first introduce some notations. For t \u2208 N, \u03a0Tt+1(L) = \u220fT k=t+1(I \u2212 \u03b7kL) for t \u2208 [T \u2212 1] and \u03a0TT+1(L) = I, for any operator L : H \u2192 H, where H is a Hilbert space and I denotes the identity operator on H. E[\u03be] denotes the expectation of a random variable \u03be. For a given bounded operator L : L2(H, \u03c1X) \u2192 H, \u2016L\u2016 denotes the operator norm of L, i.e., \u2016L\u2016 = supf\u2208L2(H,\u03c1X),\u2016f\u2016\u03c1=1 \u2016Lf\u2016H . We will use the conventional notations on summation and production: \u220ft i=t+1 = 1 and \u2211t i=t+1 = 0.\nWe next introduce some auxiliary operators. Let S\u03c1 : H \u2192 L2(H, \u03c1X) be the linear map \u03c9 \u2192 \u3008\u03c9, \u00b7\u3009H , which is bounded by \u03ba under Assumption (3). Furthermore, we consider the adjoint operator S\u2217\u03c1 : L2(H, \u03c1X)\u2192 H, the covariance operator T : H \u2192 H given by T = S\u2217\u03c1S\u03c1, and the operator L : L2(H, \u03c1X) \u2192 L2(H, \u03c1X) given by S\u03c1S\u2217\u03c1 . It can be easily proved that S\u2217\u03c1g = \u222b X xg(x)d\u03c1X(x) and T = \u222b X \u3008\u00b7, x\u3009Hxd\u03c1X(x). The operators T and L can be proved to be positive trace class operators (and hence compact). For any \u03c9 \u2208 H, it is easy to prove the following isometry property [21]\n\u2016S\u03c1\u03c9\u2016\u03c1 = \u2016 \u221a T \u03c9\u2016H . (18)\nWe define the sampling operator Sx : H \u2192 Rm by (Sx\u03c9)i = \u3008\u03c9, xi\u3009H , i \u2208 [m], where the norm \u2016 \u00b7 \u2016Rm in Rm is the Euclidean norm times 1/m. Its adjoint operator S\u2217x : Rm \u2192 H, defined by \u3008S\u2217xy, \u03c9\u3009H = \u3008y,Sx\u03c9\u3009Rm for y \u2208 Rm is thus given by S\u2217xy = 1m \u2211m i=1 yixi. Moreover, we can define the empirical covariance operator Tx : H \u2192 H such that Tx = S\u2217xSx. Obviously,\nTx = 1\nm m\u2211 i=1 \u3008\u00b7, xi\u3009Hxi.\nWith these notations, (13) and (14) can be rewritten as\n\u00b5t+1 = \u00b5t \u2212 \u03b7t(T \u00b5t \u2212 S\u2217\u03c1f\u03c1), t = 1, . . . , T, (19)\nand\n\u03bdt+1 = \u03bdt \u2212 \u03b7t(Tx\u03bdt \u2212 S\u2217xy), t = 1, . . . , T, (20)\nrespectively.\nUsing the projection theorem, one can prove that\nS\u2217\u03c1f\u03c1 = S\u2217\u03c1fH. (21)\nIndeed, since fH is the projection of the regression function f\u03c1 onto the closer of H\u03c1 in L 2(H, \u03c1X), according to the projection theorem, one has\n\u3008fH \u2212 f\u03c1,S\u03c1\u03c9\u3009\u03c1 = 0, \u2200\u03c9 \u2208 H,\nwhich can be written as\n\u3008S\u2217\u03c1fH \u2212 S\u2217\u03c1f\u03c1, \u03c9\u3009H = 0, \u2200\u03c9 \u2208 H,\nand thus leads to (21)."}, {"heading": "A.2 Concentration Inequality", "text": "We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].\nLemma A.1. Let w1, \u00b7 \u00b7 \u00b7 , wm be i.i.d random variables in a Hilbert space with norm \u2016 \u00b7 \u2016. Suppose that there are two positive constants B and \u03c32 such that\nE[\u2016w1 \u2212 E[w1]\u2016l] \u2264 1\n2 l!Bl\u22122\u03c32, \u2200l \u2265 2. (22)\nThen for any 0 < \u03b4 < 1, the following holds with probability at least 1\u2212 \u03b4,\u2225\u2225\u2225\u2225\u2225 1m m\u2211 k=1 wm \u2212 E[w1] \u2225\u2225\u2225\u2225\u2225 \u2264 2 ( B m + \u03c3\u221a m ) log 2 \u03b4 ."}, {"heading": "In particular, (22) holds if", "text": "\u2016w1\u2016l \u2264 B/2 a.s., and E[\u2016w1\u20162] \u2264 \u03c32. (23)"}, {"heading": "A.3 Basic Estimates", "text": "Lemma A.2. Let \u03b8 \u2208 [0, 1[, and t \u2208 N. Then\nt1\u2212\u03b8\n2 \u2264 t\u2211 k=1 k\u2212\u03b8 \u2264 t 1\u2212\u03b8 1\u2212 \u03b8 .\nProof. Note that\nt\u2211 k=1 k\u2212\u03b8 \u2264 1 + t\u2211 k=2 \u222b k k\u22121 u\u2212\u03b8du = 1 + \u222b t 1 u\u2212\u03b8du = t1\u2212\u03b8 \u2212 \u03b8 1\u2212 \u03b8 ,\nwhich leads to the first part of the desired result. Similarly,\nt\u2211 k=1 k\u2212\u03b8 \u2265 t\u2211 k=1 \u222b k+1 k u\u2212\u03b8du = \u222b t+1 1 u\u2212\u03b8du = (t+ 1)1\u2212\u03b8 \u2212 1 1\u2212 \u03b8 ,\nand by mean value theorem, (t+ 1)1\u2212\u03b8 \u2212 1 \u2265 (1\u2212 \u03b8)t(t+ 1)\u2212\u03b8 \u2265 (1\u2212 \u03b8)t1\u2212\u03b8/2. This proves the second part of the desired result. The proof is complete.\nLemma A.3. Let \u03b8 \u2208 R and t \u2208 N. Then t\u2211\nk=1\nk\u2212\u03b8 \u2264 tmax(1\u2212\u03b8,0)(1 + log t).\nProof. Note that\nt\u2211 k=1 k\u2212\u03b8 = t\u2211 k=1 k\u22121k1\u2212\u03b8 \u2264 tmax(1\u2212\u03b8,0) t\u2211 k=1 k\u22121,\nand\nt\u2211 k=1 k\u22121 \u2264 1 + t\u2211 k=2 \u222b k k\u22121 u\u22121du = 1 + log t.\nLemma A.4. Let q \u2208 R and t \u2208 N with t \u2265 3. Then\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q \u2264 2t\u2212min(q,1)(1 + log t).\nProof. Note that\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q = t\u22121\u2211 k=1 k1\u2212q (t\u2212 k)k \u2264 tmax(1\u2212q,0) t\u22121\u2211 k=1\n1\n(t\u2212 k)k ,\nand that by Lemma A.3,\nt\u22121\u2211 k=1\n1\n(t\u2212 k)k =\n1 t t\u22121\u2211 k=1 ( 1 t\u2212 k + 1 k ) = 2 t t\u22121\u2211 k=1 1 k \u2264 2 t (1 + log t)."}, {"heading": "B Bias", "text": "In this section, we develop upper bounds for the bias, i.e., \u2016S\u03c1\u00b5t \u2212 fH\u20162\u03c1. Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].\nLemma B.1. Let L be a compact self-adjoint operator on a separable Hilbert space H. Assume that \u03b71\u2016L\u2016 \u2264 1. Then for t \u2208 N and any non-negative integer k \u2264 t\u2212 1,\n\u2016\u03a0tk+1(L)L\u03b6\u2016 \u2264\n( \u03b6\ne \u2211t j=k+1 \u03b7j\n)\u03b6 . (24)\nProof. Let {\u03c3i} be the sequence of eigenvalues of L. We have\n\u2016\u03a0tk+1(L)L\u03b6\u2016 = sup i t\u220f l=k+1 (1\u2212 \u03b7l\u03c3i)\u03c3\u03b6i .\nUsing the basic inequality\n1 + x \u2264 ex for all x \u2265 \u22121, (25)\nwith \u03b7l\u2016L\u2016 \u2264 1, we get\n\u2016\u03a0tk+1(L)L\u03b6\u2016 \u2264 sup i exp\n{ \u2212\u03c3i\nt\u2211 l=k+1 \u03b7l } \u03c3\u03b6i\n\u2264 sup x\u22650 exp\n{ \u2212x\nt\u2211 l=k+1 \u03b7l\n} x\u03b6 .\nThe maximum of the function g(x) = e\u2212cxx\u03b6( with c > 0) over R+ is achieved at xmax = \u03b6/c, and thus\nsup x\u22650\ne\u2212cxx\u03b6 =\n( \u03b6\nec\n)\u03b6 . (26)\nUsing this inequality, one can get the desired result (24).\nWith the above lemma and Lemma A.2 from the appendix, we can derive the following result\nfor the bias.\nProposition B.2. Under Assumption 2, let \u03b71\u03ba 2 \u2264 1. Then, for any t \u2208 N,\n\u2016S\u03c1\u00b5t+1 \u2212 fH\u2016\u03c1 \u2264 R\n( \u03b6\n2 \u2211t j=1 \u03b7j\n)\u03b6 . (27)"}, {"heading": "In particular, if \u03b7t = \u03b7t", "text": "\u2212\u03b8 for all t \u2208 N, with \u03b7 \u2208]0, \u03ba\u22122] and \u03b8 \u2208 [0, 1[, then\n\u2016S\u03c1\u00b5t+1 \u2212 fH\u2016\u03c1 \u2264 R\u03b6\u03b6\u03b7\u2212\u03b6t(\u03b8\u22121)\u03b6 . (28)\nProof. The result is essentially proved in [26], see also [19]. For the sake of completeness, we provide a proof here. Since \u00b5t+1 is given by (19), introducing with (21),\n\u00b5t+1 = \u00b5t \u2212 \u03b7t(T \u00b5t \u2212 S\u2217\u03c1fH). (29)\nThus,\nS\u03c1\u00b5t+1 = S\u03c1\u00b5t \u2212 \u03b7tS\u03c1(T \u00b5t \u2212 S\u2217\u03c1fH) = S\u03c1\u00b5t \u2212 \u03b7tL(S\u03c1\u00b5t \u2212 fH). (30)\nSubtracting both sides by fH,\nS\u03c1\u00b5t+1 \u2212 fH = (I \u2212 \u03b7tL)(S\u03c1\u00b5t \u2212 fH).\nUsing this equality iteratively, with \u00b51 = 0,\nS\u03c1\u00b5t+1 \u2212 fH = \u2212\u03a0t1(L)fH.\nTaking the L2(H, \u03c1X)-norm, by Assumption 2,\n\u2016S\u03c1\u00b5t+1 \u2212 fH\u2016\u03c1 = \u2016\u03a0t1(L)fH\u2016\u03c1 \u2264 \u2016\u03a0t1(L)L\u03b6\u2016R.\nBy applying Lemma B.1, we get (27). Combining (27) with Lemma A.2, we get (28). The proof is complete.\nThe following lemma gives upper bounds for the sequence {\u00b5t}t\u2208N in H-norm. It will be used for the estimation on the sample variance in the next section.\nLemma B.3. Under Assumption 2, the following holds for all t \u2208 N: 1) If \u03b6 \u2265 1/2,\n\u2016\u00b5t\u2016H \u2264 R\u03ba2\u03b6\u22121. (31)\n2) If \u03b6 \u2208]0, 1/2],\n\u2016\u00b5t\u2016H \u2264 \u03ba2\u03b6\u22121 \u2228\n( t\u2211\nk=1\n\u03b7k\n) 1 2\u2212\u03b6\n. (32)\nProof. The proof for the fixed step-size can be found in [19]. Following from (29), we have\n\u00b5t+1 = (I \u2212 \u03b7tT )\u00b5t + \u03b7tS\u2217\u03c1fH.\nApplying this relationship iteratively, and introducing with \u00b51 = 0, we get\n\u00b5t+1 = t\u2211 k=1 \u03b7k\u03a0 t k+1(T )S\u2217\u03c1fH = t\u2211 k=1 \u03b7kS\u2217\u03c1\u03a0tk+1(L)fH.\nTherefore, using Assumption 2 and the spectrum theory,\n\u2016\u00b5t+1\u2016H \u2264 \u2225\u2225\u2225\u2225\u2225 t\u2211\nk=1\n\u03b7kS\u2217\u03c1\u03a0tk+1(L)L\u03b6 \u2225\u2225\u2225\u2225\u2225R \u2264 R max\u03c3\u2208]0,\u03ba2]\u03c31/2+\u03b6 t\u2211 k=1 \u03b7k\u03a0 t k+1(\u03c3).\nIf \u03b6 \u2265 1/2, for any \u03c3 \u2208]0, \u03ba2],\n\u03c31/2+\u03b6 t\u2211\nk=1\n\u03b7k\u03a0 t k+1(\u03c3) \u2264 \u03ba2\u03b6\u22121\u03c3 t\u2211 k=1 \u03b7k\u03a0 t k+1(\u03c3) \u2264 \u03ba2\u03b6\u22121,\nwhere for the last inequality, we used\nt\u2211 k=1 \u03b7k\u03c3\u03a0 t k+1(\u03c3) = t\u2211 k=1 (1\u2212 (1\u2212 \u03b7k\u03c3))\u03a0tk+1(\u03c3) = t\u2211 k=1 \u03a0tk+1(\u03c3)\u2212 t\u2211 k=1 \u03a0tk(\u03c3) = 1\u2212\u03a0t1(\u03c3).\nThus,\n\u2016\u00b5t+1\u2016H \u2264 R\u03ba2\u03b6\u22121.\nThe case for \u03b6 \u2265 1/2 is similar to that in [19]. We omit it. The proof is complete."}, {"heading": "C Sample Variance", "text": "In this section, we aim to estimate the sample variance, i.e., E[\u2016S\u03c1\u00b5t \u2212 S\u03c1\u03bdt\u20162\u03c1]. Towards this end, we need some preliminary analysis. We first introduce the following key inequality, which provides the hinge idea on estimating E[\u2016S\u03c1\u00b5t \u2212 S\u03c1\u03bdt\u20162\u03c1].\nLemma C.1. For all t \u2208 [T ], we have\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u2016\u03c1 \u2264 t\u2211\nk=1\n\u03b7k \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)Nk\u2225\u2225\u2225 H , (33)\nwhere\nNk = (T \u00b5k \u2212 S\u2217\u03c1f\u03c1)\u2212 (Tx\u00b5k \u2212 S\u2217xy), \u2200k \u2208 [T ]. (34)\nProof. Since \u03bdt+1 and \u00b5t+1 are given by (20) and (19), respectively,\n\u03bdt+1 \u2212 \u00b5t+1 = \u03bdt \u2212 \u00b5t + \u03b7t { (T \u00b5t \u2212 S\u2217\u03c1f\u03c1)\u2212 (Tx\u03bdt \u2212 S\u2217xy) }\n= (I \u2212 \u03b7tTx)(\u03bdt \u2212 \u00b5t) + \u03b7t { (T \u00b5t \u2212 S\u2217\u03c1f\u03c1)\u2212 (Tx\u00b5t \u2212 S\u2217xy) } ,\nwhich is exactly\n\u03bdt+1 \u2212 \u00b5t+1 = (I \u2212 \u03b7tTx)(\u03bdt \u2212 \u00b5t) + \u03b7tNt.\nApplying this relationship iteratively, with \u03bd1 = \u00b51 = 0,\n\u03bdt+1 \u2212 \u00b5t+1 = \u03a0t1(Tx)(\u03bd1 \u2212 \u00b51) + t\u2211\nk=1\n\u03b7k\u03a0 t k+1(Tx)Nk = t\u2211 k=1 \u03b7k\u03a0 t k+1(Tx)Nk.\nBy (18), we have\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u2016\u03c1 = \u2225\u2225\u2225\u2225\u2225 t\u2211\nk=1\n\u03b7kT 1 2 \u03a0tk+1(Tx)Nk \u2225\u2225\u2225\u2225\u2225 H ,\nwhich leads to the desired result (33). The proof is complete.\nThe above lemma demonstrates that in order to upper bound E[\u2016S\u03c1\u00b5t \u2212 S\u03c1\u03bdt\u20162\u03c1], one may only need to bound \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)Nk\u2225\u2225\u2225 H . A detailed look at this latter term indicates that one may analysis the terms T 12 \u03a0tk+1(Tx) and Nk separately, since Ez[Nk] = 0 and the properties of the deterministic sequence {\u00b5k}k are well developed in Section B.\nLemma C.2. Under Assumptions 2 and 3 , let \u03b6 \u2265 1/2. Then for any fixed \u03bb > 0, with probability at least 1\u2212 \u03b41, the following holds for all k \u2208 N : 1) If \u03b6 \u2265 1/2,\n\u2016(T + \u03bb)\u2212 12Nk\u2016H \u2264 4(R\u03ba2\u03b6 + \u221a M)\n( \u03ba\nm \u221a \u03bb +\n\u221a 2 \u221a vc\u03b3\u221a\nm\u03bb\u03b3\n) log 4\n\u03b41 . (35)\n2) If \u03b6 \u2208]0, 1/2],\n\u2016(T + \u03bb)\u2212 12Nk\u2016H \u2264 4 \u03ba \u03ba2\u03b6\u22121 \u2228( k\u2211\ni=1\n\u03b7i\n) 1 2\u2212\u03b6 +\u221aM ( \u03ba m \u221a \u03bb + \u221a 2 \u221a vc\u03b3\u221a m\u03bb\u03b3 ) log 4 \u03b41 . (36)\nProof. We will apply Berstein inequality from Lemma A.1 to prove the result. Bounding \u2225\u2225\u2225(T + \u03bb)\u2212 12 (S\u2217\u03c1f\u03c1 \u2212 S\u2217xy)\u2225\u2225\u2225\nH\nFor all i \u2208 [m], let wi = yi(T + \u03bbI)\u2212 1 2xi. Obviously, from the definitions of f\u03c1 (see (6)) and S\u03c1,\nE[w1] = Ex1 [f\u03c1(x1)(T + \u03bbI)\u2212 1 2x1] = (T + \u03bbI)\u2212 1 2S\u2217\u03c1f\u03c1.\nThus,\n(T + \u03bb)\u2212 12 ( S\u2217\u03c1f\u03c1 \u2212 S\u2217xy ) = 1\nm \u2211 i=1 (E[wi]\u2212 wi).\nWe next estimate the constants B and \u03c32(w1) in (22). Note that for any l \u2265 2,\nE[\u2016w1 \u2212 E[w1]\u2016lH ] \u2264 E[(\u2016w1\u2016H + E[\u2016w1\u2016H ])l].\nBy using Ho\u0308lder\u2019s inequality twice,\nE[\u2016w1 \u2212 E[w1]\u2016lH ] \u2264 2l\u22121E[\u2016w1\u2016lH + (E[\u2016w1\u2016H ])l] \u2264 2l\u22121E[\u2016w1\u2016lH + E[\u2016w1\u2016lH ]].\nThe right-hand side is exactly 2lE[\u2016w1\u2016lH ]. Therefore, by recalling the definition of w1 and expanding the integration,\nE[\u2016w1 \u2212 E[w1]\u2016lH ] \u2264 2l \u222b Y yld\u03c1(y|x) \u222b X \u2016(T + \u03bbI)\u2212 12x\u2016lHd\u03c1X(x). (37)\nNote that by using Ho\u0308lder\u2019s inequality,\u222b Y yld\u03c1(y|x) \u222b X \u2264 (\u222b Y |y|2ld\u03c1(y|x) ) 1 2 .\nUsing Assumption 1 to the above,\u222b Y yld\u03c1(y|x) \u222b X \u2264 \u221a l!M lv \u2264 l!( \u221a M)l \u221a v.\nPlugging the above into (37), we reach\nE[\u2016w1 \u2212 E[w1]\u2016lH ] \u2264 l!(2 \u221a M)l \u221a v \u222b X \u2016(T + \u03bbI)\u2212 12x\u2016lHd\u03c1X(x).\nUsing Assumption (3) which imples\n\u2016(T + \u03bbI)\u2212 12x\u2016H \u2264 \u2016x\u2016H\u221a \u03bb \u2264 \u03ba\u221a \u03bb ,\nwe get that\nE[\u2016w1 \u2212 E[w1]\u2016lK ] \u2264 l!(2 \u221a M)l \u221a v ( \u03ba\u221a \u03bb )l\u22122 \u222b X \u2016(T + \u03bbI)\u2212 12x\u20162Hd\u03c1X(x).\nUsing the fact that E[\u2016\u03be\u20162H ] = E[tr(\u03be \u2297 \u03be)] = tr(E[\u03be \u2297 \u03be]) and E[x\u2297 x] = T , we know that\u222b X \u2016(T + \u03bbI)\u2212 12x\u20162Hd\u03c1X(x) = tr((T + \u03bbI)\u2212 1 2 T (T + \u03bbI)\u2212 12 ) = tr((T + \u03bbI)\u22121T ),\nand as a result of the above and Assumption 3,\u222b X \u2016(T + \u03bbI)\u2212 12x\u20162Hd\u03c1X(x) \u2264 c\u03b3\u03bb\u2212\u03b3 .\nTherefore,\nE[\u2016w1 \u2212 E[w1]\u2016lH ] \u2264 l!(2 \u221a M)l \u221a v ( \u03ba\u221a \u03bb )l\u22122 c\u03b3\u03bb \u2212\u03b3 = 1 2 l! ( 2\u03ba \u221a M\u221a \u03bb )l\u22122 8M \u221a vc\u03b3\u03bb \u2212\u03b3 .\nApplying Berstein inequality with B = 2\u03ba \u221a M\u221a \u03bb\nand \u03c3 = \u221a 8M \u221a vc\u03b3\u03bb\u2212\u03b3 , we get that with proba-\nbility at least 1\u2212 \u03b412 , there holds\u2225\u2225\u2225(T + \u03bb)\u2212 12 (S\u2217\u03c1f\u03c1 \u2212 S\u2217xy)\u2225\u2225\u2225 H = \u2225\u2225\u2225\u2225\u2225 1m\u2211 i=1 (E[wi]\u2212 wi) \u2225\u2225\u2225\u2225\u2225 H \u2264 4 \u221a M ( \u03ba m \u221a \u03bb + \u221a 2 \u221a vc\u03b3\u221a m\u03bb\u03b3 ) log 4 \u03b41 .\n(38)\nBounding \u2016(T + \u03bb)\u2212 12 (T \u2212 Tx)\u2016 Let \u03bei = (T + \u03bb)\u2212 1 2xi \u2297 xi, for all i \u2208 [m]. It is easy to see that E[\u03bei] = (T + \u03bb)\u2212 1 2 T , and\nthat (T + \u03bb)\u2212 12 (T \u2212 Tx) = 1m \u2211m i=1(E[\u03bei]\u2212 \u03bei). Denote the Hilbert-Schmidt norm of a bounded operator from H to H by \u2016 \u00b7 \u2016HS . Note that\n\u2016\u03be1\u20162HS = \u2016x1\u20162HTrace((T + \u03bb)\u22121/2x1 \u2297 x1(T + \u03bb)\u22121/2) = \u2016x1\u20162HTrace((T + \u03bb)\u22121x1 \u2297 x1).\nBy Assumption (3),\n\u2016\u03be1\u2016HS \u2264 \u221a \u03ba2Trace((T + \u03bb)\u22121x1 \u2297 x1) \u2264 \u221a \u03ba2Trace(x1 \u2297 x1)/\u03bb \u2264 \u03ba2/ \u221a \u03bb,\nand furthermore, by Assumption 3,\nE[\u2016\u03be1\u20162HS ] \u2264 \u03ba2ETrace((T + \u03bb)\u22121x1 \u2297 x1) = \u03ba2Trace((T + \u03bb)\u22121T ) \u2264 \u03ba2c\u03b3\u03bb\u2212\u03b3 .\nAccording to Lemma A.1, we get that with probability at least 1\u2212 \u03b412 , there holds\n\u2016(T + \u03bb)\u2212 12 (T \u2212 Tx)\u2016HS \u2264 2\u03ba ( 2\u03ba\nm \u221a \u03bb + \u221a c\u03b3\u221a m\u03bb\u03b3\n) log 4\n\u03b41 . (39)\nFinally, using the triangle inequality, we have,\n\u2016(T + \u03bb)\u2212 12Nk\u2016H \u2264 \u2016(T + \u03bb)\u2212 1 2 (T \u2212 Tx)\u2016\u2016\u00b5k\u2016H + \u2225\u2225\u2225(T + \u03bb)\u2212 12 (S\u2217\u03c1f\u03c1 \u2212 S\u2217xy)\u2225\u2225\u2225 H .\nApplying Lemma B.3 to the above, introducing with (38) and (39), and then noting that \u03ba \u2265 1 and v \u2265 1, one can prove the desired results.\nThe next lemma is borrowed from [27], derived by applying a recent Bernstein inequality\nfrom [28, 29] for a sum of random operators.\nLemma C.3. Let \u03b42 \u2208 (0, 1) and 9\u03ba 2 m log m \u03b42 \u2264 \u03bb \u2264 \u2016T \u2016. Then the following holds with probability at least 1\u2212 \u03b42, \u2016(Tx + \u03bbI)\u2212 1 2 T 12 \u2016 \u2264 \u2016(Tx + \u03bb)\u2212 1 2 (T + \u03bb) 12 \u2016 \u2264 2. (40)\nNow we are in a position to estimate the sample variance.\nProposition C.4. Let \u03b71\u03ba 2 \u2264 1 and (35) for all k \u2208 [T ]. Assume that (40) holds. Then the following holds for all t \u2208 [T ] : 1) If \u03b6 \u2265 1/2,\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u2016\u03c1\n\u22644(R\u03ba2\u03b6 + \u221a M)\n( \u03ba\nm \u221a \u03bb +\n\u221a 2 \u221a vc\u03b3\u221a\nm\u03bb\u03b3 )( t\u22121\u2211 k=1 \u03b7k/2\u2211t i=k+1 \u03b7i + \u03bb t\u22121\u2211 k=1 \u03b7k + \u221a 2\u03ba2\u03b7t ) log 4 \u03b41 . (41)\n2) If \u03b6 \u2264 1/2,\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u2016\u03c1 \u2264 4 \u03ba \u03ba2\u03b6\u22121 \u2228( k\u2211\ni=1\n\u03b7i\n) 1 2\u2212\u03b6 +\u221aM  \u00d7\n( t\u22121\u2211 k=1 \u03b7k/2\u2211t i=k+1 \u03b7i + \u03bb t\u22121\u2211 k=1 \u03b7k + \u221a 2\u03ba2\u03b7t )( \u03ba m \u221a \u03bb + \u221a 2 \u221a vc\u03b3\u221a m\u03bb\u03b3 ) log 4 \u03b41 . (42)\nProof. For notational simplicity, we let T\u03bb = T + \u03bbI and Tx,\u03bb = Tx + \u03bbI. Note that by Lemma C.1, we have (33). When k \u2208 [t\u2212 1], by rewriting T 12 \u03a0tk+1(Tx)Nk as\nT 12 T \u2212 1 2 x,\u03bb T 1 2 x,\u03bb\u03a0 t k+1(Tx)T 1 2 x,\u03bbT \u2212 12 x,\u03bb T 1 2 \u03bb T \u2212 12 \u03bb Nk,\nwe can upper bound \u2016T 12 \u03a0tk+1(Tx)Nk\u2016H as\n\u2016T 12 \u03a0tk+1(Tx)Nk\u2016H \u2264 \u2016T 1 2 T \u2212 1 2 x,\u03bb \u2016\u2016T 1 2 x,\u03bb\u03a0 t k+1(Tx)T 1 2 x,\u03bb\u2016\u2016T \u2212 12 x,\u03bb T 1 2 \u03bb \u2016\u2016T \u2212 12 \u03bb Nk\u2016H .\nApplying (40), the above can be relaxed as\n\u2016T 12 \u03a0tk+1(Tx)Nk\u2016H \u2264 4\u2016T 1 2 x,\u03bb\u03a0 t k+1(Tx)T 1 2 x,\u03bb\u2016\u2016T \u2212 12 \u03bb Nk\u2016H ,\nwhich is equivalent to\n\u2016T 1 2\n\u03bb \u03a0 t k+1(Tx)Nk\u2016H \u2264 4\u2016Tx,\u03bb\u03a0tk+1(Tx)\u2016\u2016T \u2212 12 \u03bb Nk\u2016H .\nThus, following from \u03b7k\u03ba 2 \u2264 1 which implies \u03b7k\u2016Tx\u2016 \u2264 1,\n\u2016Tx,\u03bb\u03a0tk+1(Tx)\u2016 \u2264 \u2016Tx\u03a0tk+1(Tx)\u2016+ \u2016\u03bb\u03a0tk+1(Tx)\u2016\n\u2264 \u2016Tx\u03a0tk+1(Tx)\u2016+ \u03bb.\nApplying Lemma B.1 with \u03b6 = 1 to bound \u2016Tx\u03a0tk+1(Tx)\u2016, we get\n\u2016Tx,\u03bb\u03a0tk+1(Tx)\u2016 \u2264 1 e \u2211t j=k+1 \u03b7j + \u03bb.\nWhen k = t,\n\u2016T 12 \u03a0tk+1(Tx)Nk\u2016H = \u2016T 1 2Nt\u2016H \u2264 \u2016T 1 2 \u2016\u2016T\n1 2 \u03bb \u2016\u2016T \u2212 12 \u03bb Nt\u2016H\n\u2264 \u2016T \u2016 12 (\u2016T \u2016+ \u03bb) 12 \u2016T \u2212 1 2\n\u03bb Nt\u2016H .\nSince \u03bb \u2264 \u2016T \u2016 \u2264 tr(T ) \u2264 \u03ba2, we derive\n\u2016T 12 \u03a0tk+1(Tx)Nt\u2016H \u2264 \u221a 2\u03ba2\u2016T \u2212 1 2 \u03bb Nt\u2016H .\nFrom the above analysis, we conclude that \u2211t k=1 \u03b7k \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)Nk\u2225\u2225\u2225 H can be upper bounded by\n\u2264 sup k\u2208[t] \u2016T \u2212 1 2 \u03bb Nk\u2016H ( t\u22121\u2211 k=1 \u03b7k/2\u2211t i=k+1 \u03b7i + \u03bb t\u22121\u2211 k=1 \u03b7k + \u221a 2\u03ba2\u03b7t ) .\nPlugging (35) (or (36)) into the above, and then combining with (33), we get the desired bound (41) (or (42)). The proof is complete.\nSetting \u03b7t = \u03b71t \u2212\u03b8 in the above proposition, with some basic estimates from Appendix A,\nwe get the following explicit bounds for the sample variance.\nProposition C.5. Let \u03b7t = \u03b71t \u2212\u03b8 and (35) for all t \u2208 [T ], with \u03b71 \u2208]0, \u03ba\u22122] and \u03b8 \u2208 [0, 1[. Assume that (40) holds. Then the following holds for all t \u2208 [T ]: 1) If \u03b6 \u2265 1/2,\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u2016\u03c1\n\u22644(R\u03ba2\u03b6 + \u221a M)\n( 2\u03bb\u03b71t 1\u2212\u03b8\n1\u2212 \u03b8 + log t+ 1 +\n\u221a 2\u03b71\u03ba 2\n)( \u03ba\nm \u221a \u03bb +\n\u221a 2 \u221a vc\u03b3\u221a\nm\u03bb\u03b3\n) log 4\n\u03b41 .\n(43)\n2) If \u03b6 \u2264 1/2,\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u2016\u03c1 \u2264 4 ( \u03ba ( \u03ba2\u03b6\u22121 \u2228 ( 2\u03b71t 1\u2212\u03b8\n1\u2212 \u03b8\n) 1 2\u2212\u03b6 )\n+ \u221a M\n)\n\u00d7 ( 2\u03bb\u03b71t 1\u2212\u03b8\n1\u2212 \u03b8 + log t+ 1 +\n\u221a 2\u03b71\u03ba 2\n)( \u03ba\nm \u221a \u03bb +\n\u221a 2 \u221a vc\u03b3\u221a\nm\u03bb\u03b3\n) log 4\n\u03b41 . (44)\nProof. By Proposition C.4, we have (41). Note that\nt\u22121\u2211 k=1 \u03b7k\u2211t i=k+1 \u03b7i = t\u22121\u2211 k=1 k\u2212\u03b8\u2211t i=k+1 i \u2212\u03b8 \u2264 t\u22121\u2211 k=1\nk\u2212\u03b8\n(t\u2212 k)t\u2212\u03b8 .\nApplying Lemma A.4, we get\nt\u22121\u2211 k=1 \u03b7k\u2211t i=k+1 \u03b7i \u2264 2 + 2 log t,\nand by Lemma A.2,\nt\u22121\u2211 k=1 \u03b7k = \u03b71 t\u22121\u2211 k=1 k\u2212\u03b8 \u2264 2\u03b71t 1\u2212\u03b8 1\u2212 \u03b8 .\nIntroducing the last two estimates into (41) and (43), one can get the desired results. The proof is complete.\nIn conclusion, we get the following result for the sample variance.\nTheorem C.6. Under Assumptions 1, 2 and 3, let \u03b41, \u03b42 \u2208]0, 1[ and 9\u03ba 2 m log m \u03b42 \u2264 \u03bb \u2264 \u2016T \u2016. Let \u03b7t = \u03b71t \u2212\u03b8 for all t \u2208 [T ], with \u03b71 \u2208]0, \u03ba\u22122] and \u03b8 \u2208 [0, 1[. Then with probability at least 1\u2212 \u03b41 \u2212 \u03b42, the following holds for all t \u2208 [T ] : 1) if \u03b6 \u2265 1/2, we have (43). 2) if \u03b6 < 1/2, we have (44)."}, {"heading": "D Computational Variance", "text": "In this section, we estimate the computational variance, E[\u2016S\u03c1\u03c9t \u2212 S\u03c1\u03bdt\u20162\u03c1]. For this, a series of lemmas is necessarily introduced."}, {"heading": "D.1 Bounding the Empirical Risk", "text": "This subsection is devoted to upper bounding EJ[Ez(\u03c9l)]. The process relies on some tools from convex analysis and a decomposition related to the weighted averages and the last iterates from [22, 30]. We begin by introducing the following lemma, a fact based on the square loss\u2019 special properties.\nLemma D.1. Given any sample z, and l \u2208 N, let \u03c9 \u2208 H be independent from Jl, then\n\u03b7l (Ez(\u03c9l)\u2212 Ez(\u03c9)) \u2264 \u2016\u03c9l \u2212 \u03c9\u20162H \u2212 EJl\u2016\u03c9l+1 \u2212 \u03c9\u20162H + \u03b72l \u03ba2Ez(\u03c9l). (45)\nProof. Since \u03c9t+1 is given be (4), subtracting both sides of (4) by \u03c9, taking the square H-norm, and expanding the inner product,\n\u2016\u03c9l+1 \u2212 \u03c9\u20162H = \u2016\u03c9l \u2212 \u03c9\u20162H + \u03b72l b2 \u2225\u2225\u2225\u2225\u2225\u2225 bl\u2211\ni=b(l\u22121)+1\n(\u3008\u03c9l, xji\u3009H \u2212 yji)xji \u2225\u2225\u2225\u2225\u2225\u2225 2\nH\n+ 2\u03b7l b bl\u2211 i=b(l\u22121)+1 (\u3008\u03c9l, xji\u3009H \u2212 yji)\u3008\u03c9 \u2212 \u03c9l, xji\u3009H .\nBy Assumption (3), \u2016xji\u2016H \u2264 \u03ba, and thus\u2225\u2225\u2225\u2225\u2225\u2225 bl\u2211\ni=b(l\u22121)+1\n(\u3008\u03c9l, xji\u3009H \u2212 yji)xji \u2225\u2225\u2225\u2225\u2225\u2225 2\nH\n\u2264  bl\u2211 i=b(l\u22121)+1 |\u3008\u03c9l, xji\u3009H \u2212 yji |\u03ba 2 \u2264 \u03ba2b bl\u2211\ni=b(l\u22121)+1\n(\u3008\u03c9l, xji\u3009H \u2212 yji)2,\nwhere for the last inequality, we used Cauchy-Schwarz inequality. Thus,\n\u2016\u03c9l+1 \u2212 \u03c9\u20162H \u2264 \u2016\u03c9l \u2212 \u03c9\u20162H + \u03b72l \u03ba 2\nb bl\u2211 i=b(l\u22121)+1 (\u3008\u03c9l, xji\u3009H \u2212 yji)2\n+ 2\u03b7l b bl\u2211 i=b(l\u22121)+1 (\u3008\u03c9l, xji\u3009H \u2212 yji)(\u3008\u03c9, xji\u3009H \u2212 \u3008\u03c9l, xji\u3009H).\nUsing the basic inequality a(b\u2212 a) \u2264 (b2 \u2212 a2)/2,\u2200a, b \u2208 R,\n\u2016\u03c9l+1 \u2212 \u03c9\u20162H \u2264 \u2016\u03c9l \u2212 \u03c9\u20162H + \u03b72l \u03ba 2\nb bl\u2211 i=b(l\u22121)+1 (\u3008\u03c9l, xji\u3009H \u2212 yji)2\n+ \u03b7l b bl\u2211 i=b(l\u22121)+1 ( (\u3008\u03c9, xji\u3009H \u2212 yji)2 \u2212 (\u3008\u03c9l, xji\u3009H \u2212 yji)2 ) .\nNoting that \u03c9l and \u03c9 are independent from Jl, and taking the expectation on both sides with respect to Jl,\nEJl\u2016\u03c9l+1 \u2212 \u03c9\u20162H \u2264 \u2016\u03c9l \u2212 \u03c9\u20162H + \u03b72l \u03ba2Ez(\u03c9l) + \u03b7l (Ez(\u03c9)\u2212 Ez(\u03c9l)) ,\nwhich leads to the desired result by rearranging terms. The proof is complete.\nUsing the above lemma and a decomposition related to the weighted averages and the last\niterates from [22, 30], we can prove the following relationship.\nLemma D.2. Let \u03b71\u03ba 2 \u2264 1/2 for all t \u2208 N. Then\n\u03b7tEJ[Ez(\u03c9t)] \u2264 4Ez(0) 1\nt t\u2211 l=1 \u03b7l + 2\u03ba 2 t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i EJ[Ez(\u03c9i)]. (46)\nProof. For k = 1, \u00b7 \u00b7 \u00b7 , t\u2212 1,\n1 k t\u2211 i=t\u2212k+1 \u03b7iEJ[Ez(\u03c9i)]\u2212 1 k + 1 t\u2211 i=t\u2212k \u03b7iEJ[Ez(\u03c9i)]\n= 1\nk(k + 1)\n{ (k + 1)\nt\u2211 i=t\u2212k+1 \u03b7iEJ[Ez(\u03c9i)]\u2212 k t\u2211 i=t\u2212k \u03b7iEJ[Ez(\u03c9i)]\n}\n= 1\nk(k + 1) t\u2211 i=t\u2212k+1 (\u03b7iEJ[Ez(\u03c9i)]\u2212 \u03b7t\u2212kEJ[Ez(\u03c9t\u2212k)]).\nSumming over k = 1, \u00b7 \u00b7 \u00b7 , t\u2212 1, and rearranging terms, we get [30]\n\u03b7tEJ[Ez(\u03c9t)] = 1\nt t\u2211 i=1 \u03b7iEJ[Ez(\u03c9i)] + t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 (\u03b7iEJ[Ez(\u03c9i)]\u2212 \u03b7t\u2212kEJ[Ez(\u03c9t\u2212k)]).\nSince {\u03b7t}t is decreasing and EJ[Ez(\u03c9t\u2212k)] is non-negative, the above can be relaxed as\n\u03b7tEJ[Ez(\u03c9t)] \u2264 1\nt t\u2211 i=1 \u03b7iEJ[Ez(\u03c9i)] + t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 \u03b7iEJ[Ez(\u03c9i)\u2212 Ez(\u03c9t\u2212k)]. (47)\nIn the rest of the proof, we will upper bound the last two terms of the above.\nTo bound the first term of the right side of (47), we apply Lemma D.1 with \u03c9 = 0 to get\n\u03b7lEJ (Ez(\u03c9l)\u2212 Ez(0)) \u2264 EJ[\u2016\u03c9l\u20162H \u2212 \u2016\u03c9l+1\u20162H ] + \u03b72l \u03ba2EJ[Ez(\u03c9l)].\nRearranging terms,\n\u03b7l(1\u2212 \u03b7l\u03ba2)EJ[Ez(\u03c9l)] \u2264 EJ[\u2016\u03c9l\u20162H \u2212 \u2016\u03c9l+1\u20162H ] + \u03b7lEz(0).\nIt thus follows from the above and \u03b7l\u03ba 2 \u2264 1/2 that\n\u03b7lEJ[Ez(\u03c9l)]/2 \u2264 EJ[\u2016\u03c9l\u20162H \u2212 \u2016\u03c9l+1\u20162H ] + \u03b7lEz(0).\nSumming up over l = 1, \u00b7 \u00b7 \u00b7 , t, t\u2211 l=1 \u03b7lEJ[Ez(\u03c9l)]/2 \u2264 EJ[\u2016w1\u20162H \u2212 \u2016\u03c9t+1\u20162H ] + Ez(0) t\u2211 l=1 \u03b7l. Introducing with \u03c91 = 0, \u2016\u03c9t+1\u20162H \u2265 0, and then multiplying both sides by 2/t, we get\n1 t t\u2211 l=1 \u03b7lEJ[Ez(\u03c9l)] \u2264 2Ez(0) 1 t t\u2211 l=1 \u03b7l. (48)\nIt remains to bound the last term of (47). Let k \u2208 [t \u2212 1] and i \u2208 {t \u2212 k, \u00b7 \u00b7 \u00b7 , t}. Note that given the sample z, \u03c9i is depending only on J1, \u00b7 \u00b7 \u00b7 ,Ji\u22121 when i > 1 and \u03c91 = 0. Thus, we can apply Lemma D.1 with \u03c9 = \u03c9t\u2212k to derive\n\u03b7i (Ez(\u03c9i)\u2212 Ez(\u03c9t\u2212k)) \u2264 \u2016\u03c9i \u2212 \u03c9t\u2212k\u20162H \u2212 EJi\u2016\u03c9i+1 \u2212 \u03c9t\u2212k\u20162H + \u03b72i \u03ba2Ez(\u03c9i).\nTherefore,\n\u03b7iEJ [Ez(\u03c9i)\u2212 Ez(\u03c9t\u2212k)] \u2264 EJ[\u2016\u03c9i \u2212 \u03c9t\u2212k\u20162H \u2212 \u2016\u03c9i+1 \u2212 \u03c9t\u2212k\u20162H ] + \u03b72i \u03ba2EJ[Ez(\u03c9i)].\nSumming up over i = t\u2212 k, \u00b7 \u00b7 \u00b7 , t,\nt\u2211 i=t\u2212k \u03b7iEJ [Ez(\u03c9i)\u2212 Ez(\u03c9t\u2212k)] \u2264 \u03ba2 t\u2211 i=t\u2212k \u03b72i EJ[Ez(\u03c9i)].\nNote that the left hand side is exactly \u2211t i=t\u2212k+1 \u03b7iEJ [Ez(\u03c9i)\u2212 Ez(\u03c9t\u2212k)]. We thus know that the last term of (47) can be upper bounded by\n\u03ba2 t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k \u03b72i EJ[Ez(\u03c9i)]\n= \u03ba2 t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i EJ[Ez(\u03c9i)] + \u03ba2\u03b72tEJ[Ez(\u03c9t)] t\u22121\u2211 k=1\n1\nk(k + 1) .\nUsing the fact that\nt\u22121\u2211 k=1\n1\nk(k + 1) = t\u22121\u2211 k=1 ( 1 k \u2212 1 k + 1 ) = 1\u2212 1 t \u2264 1,\nand \u03ba2\u03b7t \u2264 1/2, we get that the last term of (47) can be bounded as\nt\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 \u03b7i(EJ[Ez(\u03c9i)]\u2212 EJ[Ez(\u03c9t\u2212k)])\n\u2264 \u03ba2 t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i EJ[Ez(\u03c9i)] + \u03b7tEJ[Ez(\u03c9t)]/2.\nPlugging the above and (48) into the decomposition (47), and rearranging terms\n\u03b7tEJ[Ez(\u03c9t)]/2 \u2264 2M2 1\nt t\u2211 l=1 \u03b7l + \u03ba 2 t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i EJ[Ez(\u03c9i)],\nwhich leads to the desired result by multiplying both sides by 2. The proof is complete.\nWe also need to the following lemma, whose proof can be done by using an induction argu-\nment.\nLemma D.3. Let {ut}Tt=1, {At}Tt=1 and {Bt}Tt=1 be three sequences of non-negative numbers such that u1 \u2264 A1 and\nut \u2264 At +Bt sup i\u2208[t\u22121] ui, \u2200t \u2208 {2, 3, \u00b7 \u00b7 \u00b7 , T}. (49)\nLet supt\u2208[T ]Bt \u2264 B < 1. Then for all t \u2208 [T ],\nsup k\u2208[t]\nut \u2264 1\n1\u2212B sup k\u2208[t] Ak. (50)\nProof. When t = 1, (50) holds trivially since u1 \u2264 A1 and B < 1. Now assume for some t \u2208 N with 2 \u2264 t \u2264 T,\nsup i\u2208[t\u22121]\nui \u2264 1\n1\u2212B sup i\u2208[t\u22121] Ai.\nThen, by (49), the above hypothesis, and Bt \u2264 B, we have\nut \u2264 At +Bt sup i\u2208[t\u22121]\nui \u2264 At + Bt\n1\u2212B sup i\u2208[t\u22121] Ai \u2264 sup i\u2208[t] Ai\n( 1 +\nBt 1\u2212B ) \u2264 sup i\u2208[t] Ai 1 1\u2212B .\nConsequently,\nsup k\u2208[t]\nut \u2264 1\n1\u2212B sup k\u2208[t] Ak,\nthereby showing that indeed (50) holds for t. By mathematical induction, (50) holds for every t \u2208 [T ]. The proof is complete.\nNow we can bound EJ[Ez(fk)] as follows.\nLemma D.4. Let \u03b71\u03ba 2 \u2264 1/2 and for all t \u2208 [T ] with t \u2265 2,\n1 \u03b7t t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i \u2264 1 4\u03ba2 . (51)\nThen for all t \u2208 [T ],\nsup k\u2208[t] EJ[Ez(fk)] \u2264 8Ez(0) sup k\u2208[t]\n{ 1\n\u03b7kk k\u2211 l=1 \u03b7l\n} . (52)\nProof. By Lemma D.2, we have (46). Dividing both sides by \u03b7t, we can relax the inequality as\nEJ[Ez(\u03c9t)] \u2264 4Ez(0) 1\n\u03b7tt t\u2211 l=1 \u03b7l + 2\u03ba 2 1 \u03b7t t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i sup i\u2208[t\u22121] EJ[Ez(\u03c9i)].\nIn Lemma D.3, we let ut = EJ[Ez(\u03c9t)], At = 4Ez(0) 1\u03b7tt \u2211t l=1 \u03b7l and\nBt = 2\u03ba 2 1\n\u03b7t t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i .\nCondition (51) guarantees that supt\u2208[T ]Bt \u2264 1/2. Thus, (50) holds, and the desired result follows by plugging with B = 1/2. The proof is complete.\nFinally, we need the following lemma to bound Ez(0), whose proof follows from applying the Bernstein Inequality from Lemma A.1.\nLemma D.5. Under Assumption 1, with probability at least 1\u2212 \u03b43 (\u03b43 \u2208]0, 1[), there holds\nEz(0) \u2264Mv + 2M\n( 1\nm + \u221a 2v\u221a m\n) log 2\n\u03b43 .\nIn particular, if m \u2265 32 log2 2\u03b43 , then\nEz(0) \u2264 2Mv. (53)\nProof. Following from (5),\u222b Z y2ld\u03c1 \u2264 1 2 l!M l\u22122 \u00b7 (2M2v), \u2200l \u2208 N. Applying Lemma A.1, with \u03c9i = y 2 i for all i \u2208 [m], B = M and \u03c3 = M \u221a 2v, we know that with probability at least 1\u2212 \u03b43, there holds\n1 m m\u2211 i=1 y2i \u2212 \u222b Z y2d\u03c1 \u2264 2M ( 1 m + \u221a 2v\u221a m ) log 2 \u03b43 .\nBy setting l = 1 in (5), \u222b Z y2d\u03c1 \u2264Mv. It thus follows that\n1 m m\u2211 i=1 y2i \u2264 \u222b Z y2d\u03c1+ 2M ( 1 m + \u221a 2v\u221a m ) log 2 \u03b43 \u2264Mv + 2M ( 1 m + \u221a 2v\u221a m ) log 2 \u03b43 ,\nwhich leads to the desired results by noting that the left-hand side is exactly Ez(0) and \u03bd \u2265 1. The proof is complete.\nD.2 Bounding \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)\u2225\u2225\u2225\nLemma D.6. Assume (40) holds for some \u03bb > 0 and \u03b71\u03ba 2 \u2264 1. Then\n\u2016T 12 \u03a0tk+1(Tx)\u20162 \u2264 1\u2211t\ni=k+1 \u03b7i + 4\u03bb.\nProof. Note that we have\n\u2016T 12 \u03a0tk+1(Tx)\u2016 \u2264 \u2016T 1 2 (Tx + \u03bbI)\u2212 1 2 \u2016\u2016(Tx + \u03bbI) 1 2 \u03a0tk+1(Tx)\u2016.\nUsing (40), we can relax the above as\n\u2016T 12 \u03a0tk+1(Tx)\u2016 \u2264 2\u2016(Tx + \u03bbI) 1 2 \u03a0tk+1(Tx)\u2016,\nwhich leads to\n\u2016T 12 \u03a0tk+1(Tx)\u20162 \u2264 4\u2016(Tx + \u03bbI) 1 2 \u03a0tk+1(Tx)\u20162.\nSince\n\u2016(Tx + \u03bbI) 1 2 \u03a0tk+1(Tx)\u20162 = \u2016(Tx + \u03bbI)\u03a0tk+1(Tx)\u03a0tk+1(Tx)\u2016\n\u2264 \u2016Tx\u03a0tk+1(Tx)\u03a0tk+1(Tx)\u2016+ \u03bb = \u2016T 1 2\nx \u03a0 t k+1(Tx)\u20162 + \u03bb,\nand with \u03b7t\u03ba 2 \u2264 1, \u2016Tx\u2016 \u2264 tr(Tx) \u2264 \u03ba2, by Lemma B.1,\n\u2016T 1 2\nx \u03a0 t k+1(Tx)\u20162 \u2264\n1 2e \u2211t i=k+1 \u03b7i \u2264 1 4 \u2211t i=k+1 \u03b7i ,\nwe thus derive the desired result. The proof is complete."}, {"heading": "D.3 Deriving Error Bounds", "text": "With Lemmas D.4 and D.6, we are ready to estimate the computational variance , EJ\u2016ft\u2212 gt\u20162\u03c1, as follows.\nProposition D.7. Assume (40) holds for some \u03bb > 0, \u03b71\u03ba 2 \u2264 1/2, (51) and (53). Then, we have for all t \u2208 [T ],\nEJ\u2016S\u03c1\u03c9t+1 \u2212 S\u03c1\u03bdt+1\u20162\u03c1 \u2264 16Mv\u03ba2\nb sup k\u2208[t]\n{ 1\n\u03b7kk k\u2211 l=1 \u03b7l }( t\u22121\u2211 k=1 \u03b72k\u2211t i=k+1 \u03b7i + 4\u03bb t\u22121\u2211 k=1 \u03b72k + \u03b7 2 t \u03ba 2 ) .\n(54)\nProof. Since \u03c9t+1 and \u03bdt+1 are given by (4) and (20), respectively,\n\u03c9t+1 \u2212 \u03bdt+1 = (\u03c9t \u2212 \u03bdt) + \u03b7t (Tx\u03bdt \u2212 S\u2217xy)\u2212 1b bt\u2211\ni=b(t\u22121)+1\n(\u3008\u03c9t, xji\u3009H \u2212 yji)xji  = (I \u2212 \u03b7tTx)(\u03c9t \u2212 \u03bdt) +\n\u03b7t b bt\u2211 i=b(t\u22121)+1 {(Tx\u03c9t \u2212 S\u2217xy)\u2212 (\u3008\u03c9t, xji\u3009H \u2212 yji)xji} .\nApplying this relationship iteratively,\n\u03c9t+1 \u2212 \u03bdt+1 = \u03a0t1(Tx)(\u03c91 \u2212 \u03bd1) + 1\nb t\u2211 k=1 bk\u2211 i=b(k\u22121)+1 \u03b7k\u03a0 t k+1(Tx)Mk,i,\nwhere we denote\nMk,i = (Tx\u03c9k \u2212 S\u2217xy)\u2212 (\u3008\u03c9k, xji\u3009H \u2212 yji)xji . (55)\nIntroducing with \u03c91 = \u03bd1 = 0,\n\u03c9t+1 \u2212 \u03bdt+1 = 1\nb t\u2211 k=1 bk\u2211 i=b(k\u22121)+1 \u03b7k\u03a0 t k+1(Tx)Mk,i.\nTherefore,\nEJ\u2016S\u03c1\u03c9t+1 \u2212 S\u03c1\u03bdt+1\u20162\u03c1 = 1\nb2 EJ \u2225\u2225\u2225\u2225\u2225\u2225 t\u2211\nk=1 bk\u2211 i=b(k\u22121)+1 \u03b7k\u03a0 t k+1(Tx)Mk,i \u2225\u2225\u2225\u2225\u2225\u2225 2\n\u03c1\n= 1\nb2 t\u2211 k=1 bk\u2211 i=b(k\u22121)+1 \u03b72kEJ \u2225\u2225\u03a0tk+1(Tx)Mk,i\u2225\u22252\u03c1 , (56)\nwhere for the last equality, we use the fact that if k 6= k\u2032, or k = k\u2032 but i 6= i\u20325, then\nEJ\u3008\u03a0tk+1(Tx)Mk,i,\u03a0tk\u2032+1(Tx)Mk\u2032,i\u2032\u3009\u03c1 = 0.\nIndeed, if k 6= k\u2032, without loss of generality, we consider the case k < k\u2032. Recalling that Mk,i is given by (55) and that given any z, fk is depending only on J1, \u00b7 \u00b7 \u00b7 ,Jk\u22121, we thus have\nEJ\u3008\u03a0tk+1(Tx)Mk,i,\u03a0tk\u2032+1(Tx)Mk\u2032,i\u2032\u3009\u03c1 = EJ1,\u00b7\u00b7\u00b7 ,Jk\u2032\u22121\u3008\u03a0 t k+1(Tx)Mk,i,\u03a0tl+1(Tx)EJk\u2032 [Mk\u2032,i\u2032 ]\u3009\u03c1 = 0.\nIf k = k\u2032 but i 6= i\u2032, without loss of generality, we assume i < i\u2032. By noting that \u03c9k is depending only on J1, \u00b7 \u00b7 \u00b7 ,Jk\u22121 and Mk,i is depending only on \u03c9k and zji (given any sample z),\nEJ\u3008\u03a0tk+1(Tx)Mk,i,\u03a0tk+1(Tx)Mk,i\u2032\u3009\u03c1 = EJ1,\u00b7\u00b7\u00b7 ,Jk\u22121\u3008\u03a0tk+1(Tx)Eji [Mk,i],\u03a0tl+1(Tx)Eji\u2032 [Mk,i\u2032 ]\u3009\u03c1 = 0.\nUsing the isometry property (18) to (56),\nEJ \u2225\u2225\u03a0tk+1(Tx)Mk,i\u2225\u22252\u03c1 = EJ \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)Mk,i\u2225\u2225\u22252H \u2264 \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)\u2225\u2225\u22252 EJ \u2016Mk,i\u20162H ,\nand by applying the inequality E[\u2016\u03be \u2212 E[\u03be]\u20162H ] \u2264 E[\u2016\u03be\u20162H ],\nEJ \u2016Mk,i\u20162H \u2264 EJ \u2016(\u3008\u03c9k, xji\u3009H \u2212 yji)xji\u2016 2 H \u2264 \u03ba 2EJ[(\u3008\u03c9k, xji\u3009H \u2212 yji)2] = \u03ba2EJ[Ez(\u03c9k)], 5This is possible only when b \u2265 2.\nwhere for the last inequality we use (3). Therefore,\nEJ\u2016S\u03c1\u03c9t+1 \u2212 S\u03c1\u03bdt+1\u20162\u03c1 \u2264 \u03ba2\nb t\u2211 k=1 \u03b72k \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)\u2225\u2225\u22252 EJ[Ez(\u03c9k)]. According to Lemma D.4, we have (52). It thus follows that\nEJ\u2016S\u03c1\u03c9t+1 \u2212 S\u03c1\u03bdt+1\u20162\u03c1 \u2264 8Ez(0)\u03ba2\nb sup k\u2208[t]\n{ 1\n\u03b7kk k\u2211 l=1 \u03b7l\n} t\u2211\nk=1\n\u03b72k \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)\u2225\u2225\u22252 . Now the proof can be finished by applying Lemma D.6 which tells us that\nt\u2211 k=1 \u03b72k \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)\u2225\u2225\u22252 = t\u22121\u2211 k=1 \u03b72k \u2225\u2225\u2225T 12 \u03a0tk+1(Tx)\u2225\u2225\u22252 + \u03b72t \u2225\u2225\u2225T 12 \u2225\u2225\u22252 \u2264\nt\u22121\u2211 k=1 \u03b72k\u2211t i=k+1 \u03b7i + 4\u03bb t\u22121\u2211 k=1 \u03b72k + \u03b7 2 t \u03ba 2,\nand (53) to the above. The proof is complete.\nSetting \u03b7t = \u03b71t \u2212\u03b8 for some appropriate \u03b71 and \u03b8 in the above proposition, we get the\nfollowing explicitly upper bounds for EJ\u2016S\u03c1\u03c9t \u2212 S\u03c1\u03c9t\u20162\u03c1.\nProposition D.8. Assume (40) holds for some \u03bb > 0 and (53). Let \u03b7t = \u03b71t \u2212\u03b8 for all t \u2208 [T ], with \u03b8 \u2208 [0, 1[ and\n0 < \u03b71 \u2264 tmin(\u03b8,1\u2212\u03b8)\n8\u03ba2(log t+ 1) , \u2200t \u2208 [T ]. (57)"}, {"heading": "Then, for all t \u2208 [T ],", "text": "EJ\u2016\u03c9t+1 \u2212 \u03bdt+1\u20162\u03c1 \u2264 16Mv\u03ba2\nb(1\u2212 \u03b8)\n( 5\u03b71t \u2212min(\u03b8,1\u2212\u03b8) + 8\u03bb\u03b721t (1\u22122\u03b8)+ ) (1 \u2228 log t). (58)\nProof. We will use Proposition D.7 to prove the result. Thus, we need to verify the condition (51). Note that\nt\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i = t\u22121\u2211 i=1 \u03b72i t\u22121\u2211 k=t\u2212i\n1\nk(k + 1) = t\u22121\u2211 i=1 \u03b72i ( 1 t\u2212 i \u2212 1 t ) \u2264 t\u22121\u2211 i=1 \u03b72i t\u2212 i .\nSubstituting with \u03b7i = \u03b7i \u2212\u03b8, and by Lemma A.4,\nt\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i \u2264 \u03b721 t\u22121\u2211 i=1 i\u22122\u03b8 t\u2212 i \u2264 2\u03b721t\u2212min(2\u03b8,1)(log t+ 1).\nDividing both sides by \u03b7t (= \u03b7t \u2212\u03b8), and then using (57),\n1 \u03b7t t\u22121\u2211 k=1\n1\nk(k + 1) t\u22121\u2211 i=t\u2212k \u03b72i \u2264 2\u03b71t\u2212min(\u03b8,1\u2212\u03b8)(log t+ 1) \u2264 1 4\u03ba2 .\nThis verifies (51). Note also that by taking t = 1 in (57), for all t \u2208 [T ] ,\n\u03b7t\u03ba 2 \u2264 \u03b71\u03ba2 \u2264\n1 8\u03ba2 \u2264 1 2 .\nWe thus can apply Proposition D.7 to derive (54). What remains is to control the right hand side of (54). Since\nt\u22121\u2211 k=1 \u03b72k\u2211t i=k+1 \u03b7i = \u03b71 t\u22121\u2211 k=1 k\u22122\u03b8\u2211t i=k+1 i \u2212\u03b8 \u2264 \u03b71 t\u22121\u2211 k=1 k\u22122\u03b8 (t\u2212 k)t\u2212\u03b8 ,\ncombining with Lemma A.4,\nt\u22121\u2211 k=1 \u03b72k\u2211t i=k+1 \u03b7i \u2264 2\u03b71t\u2212min(\u03b8,1\u2212\u03b8)(log t+ 1).\nAlso, by Lemma A.2,\n1\n\u03b7kk k\u2211 l=1 \u03b7l = 1 k1\u2212\u03b8 k\u2211 l=1 l\u2212\u03b8 \u2264 1 1\u2212 \u03b8 ,\nand by Lemma A.3,\nt\u22121\u2211 k=1 \u03b72k = \u03b7 2 1 t\u22121\u2211 k=1 k\u22122\u03b8 \u2264 \u03b721tmax(1\u22122\u03b8,0)(log t+ 1).\nIntroducing the last three estimates into (54) and using that \u03b72t \u03ba 2 \u2264 \u03b71t\u2212\u03b8 by (57), we get the desired result. The proof is complete.\nCollect some of the above analysis, we get the following result for the computational variance.\nTheorem D.9. Under Assumptions 1 and 3, let \u03b42 \u2208]0, 1[, 9\u03ba 2 m log m \u03b42 \u2264 \u03bb \u2264 \u2016T \u2016, \u03b43 \u2208]0, 1[, m \u2265 32 log2 2\u03b43 , and \u03b7t = \u03b7t \u2212\u03b8 for all t \u2208 [T ], with \u03b8 \u2208 [0, 1[ and \u03b7 such that (57). Then, with probability at least 1\u2212 \u03b42 \u2212 \u03b43, (58) holds for all t \u2208 [T ]."}, {"heading": "E Deriving Total Error Bounds", "text": "The purpose of this section is to derive total error bounds."}, {"heading": "E.1 Attainable Case", "text": "We have the following general theorem for \u03b6 \u2265 1/2, with which we prove our main results stated in Section 3.\nTheorem E.1. Under Assumptions 1, 2 and 3, let \u03b6 \u2265 1/2, T \u2208 N with T \u2265 3, \u03b4 \u2208]0, 1[, \u03b7t = \u03b7\u03ba \u22122t\u2212\u03b8 for all t \u2208 [T ], with \u03b8 \u2208 [0, 1[ and \u03b7 such that\n0 < \u03b7 \u2264 t min(\u03b8,1\u2212\u03b8)\n8(log t+ 1) , \u2200t \u2208 [T ]. (59)\nIf for some \u2208]0, 1],\nm \u2265 ( 18\u03ba2\n\u2016T \u2016 log\n( 27\u03ba2\n\u2016T \u2016\u03b4\n))1/ , (60)\nthen the following holds with probability at least 1\u2212 \u03b4: for all t \u2208 [T ],\nEJ[E(\u03c9t+1)]\u2212 inf \u03c9\u2208H E(\u03c9) \u2264 q1(\u03b7t1\u2212\u03b8)\u22122\u03b6 + q2m\u03b3(1\u2212 )\u22121(1 \u2228 \u03b72m2 \u22122t2\u22122\u03b8)(log T )2 log2\n12\n\u03b4\n+q3\u03b7b \u22121(t\u2212min(\u03b8,1\u2212\u03b8) \u2228m \u22121\u03b7t(1\u22122\u03b8)+) log T.\n(61)"}, {"heading": "Here, q1 = 2R", "text": "2\u03b62\u03b6 , q2 =\n800(R\u03ba2\u03b6+ \u221a M)2(\u03ba/ \u221a \u2016T \u2016+ \u221a 2 \u221a vc\u03b3/\u2016T \u2016\u03b3)2\n(1\u2212\u03b8)2 , and q3 = 208Mv 1\u2212\u03b8 .\nProof. Let \u03bb = \u2016T \u2016m \u22121. Clearly, \u03bb \u2264 \u2016T \u2016. For any A \u2265 0 and B \u2265 1, by applying (26) with \u03b6 = 1, x = (Bm) and c = 2AB ,\nA log(Bm) = A log((Bm) ) \u2264 A log\n( 2AB\ne\n) + 1\n2 m \u2264 A log\n( AB ) + 1\n2 m . (62)\nUsing the above inequality with A = 9\u03ba 2\n\u2016T \u2016 and B = 1 \u03b42 , one can prove that the condition (60)\nensures that 9\u03ba 2\nm log m \u03b42 \u2264 \u03bb is satisfied with \u03b42 = \u03b43 , Therefore, by Lemma C.3, (40) holds with\nprobability at least 1 \u2212 \u03b42. Similarly the condition (60) implies that m \u2265 32 log2 2\u03b43 is satisfied with \u03b43 = \u03b4 3 , and thus by Lemma D.5, (53) holds with probability at least 1\u2212\u03b43. Combining with Lemma C.2, by taking the union bound, we know that with probability at least 1\u2212 \u03b41\u2212 \u03b42\u2212 \u03b43, (40), (53) and (35) hold for all k \u2208 [T ]. Now, we can apply Propositions C.5 and D.8 to get (43) and (58). Noting that by (57), \u221a 2\u03b7 \u2264 1, and by a simple calculation, we derive from (43) that\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u20162\u03c1\n\u2264 400(R\u03ba2\u03b6 +\n\u221a M)2(\u03ba/ \u221a \u2016T \u2016+ \u221a 2 \u221a vc\u03b3/\u2016T \u2016\u03b3)2\n(1\u2212 \u03b8)2 m\u03b3(1\u2212 )\u22121(1 \u2228 \u03bb2\u03b72\u03ba\u22124t2\u22122\u03b8 \u2228 log2 t) log2 4\n\u03b41\n\u2264 400(R\u03ba2\u03b6 +\n\u221a M)2(\u03ba/ \u221a \u2016T \u2016+ \u221a 2 \u221a vc\u03b3/\u2016T \u2016\u03b3)2\n(1\u2212 \u03b8)2 m\u03b3(1\u2212 )\u22121(1 \u2228 \u03b72m2 \u22122t2\u22122\u03b8)(log T )2 log2 4 \u03b41 ,\nwhere for the last inequality, we used \u2016T \u2016 \u2264 \u03ba2. Similarly, by a simple calculation, we get from (58) that\nEJ\u2016S\u03c1\u03c9t+1 \u2212 S\u03c1\u03bdt+1\u20162\u03c1 \u2264 208Mv\nb(1\u2212 \u03b8) (\u03b7t\u2212min(\u03b8,1\u2212\u03b8) \u2228 \u03bb\u03b72\u03ba\u22122t(1\u22122\u03b8)+)(1 \u2228 log t)\n\u2264 208Mv b(1\u2212 \u03b8) (\u03b7t\u2212min(\u03b8,1\u2212\u03b8) \u2228m \u22121\u03b72t(1\u22122\u03b8)+) log T.\nLetting \u03b41 = \u03b4 3 , and introducing the above estimates and (28) into (16), we get (61). The proof is complete.\nProof of Theorem 3.2. By choosing = 1\u2212 12\u03b6+\u03b3 and \u03b8 = 0 in Theorem E.1, then the condition (60) reduces to m \u2265 m\u03b4, where\nm\u03b4 =\n( 18\u03ba2p\n\u2016T \u2016 log\n( 27\u03ba2p\n\u2016T \u2016\u03b4\n))p , p = 2\u03b6 + \u03b3\n2\u03b6 + \u03b3 \u2212 1 . (63)\nThe desired result thus follows by applying Theorem E.1.\nProof of Corollary 3.3. We use Theorem 3.2 to prove the result. We first prove that the conditions of Theorem 3.2 are true. When \u03b4 = 1/m, the condition m \u2265 m\u03b4 with m\u03b4 given by (63), is equivalent to\nm1/p \u2265 18\u03ba 2p\n\u2016T \u2016 log\n( 27\u03ba2p\n\u2016T \u2016 m\n) .\nUsing (62) wit = 1/p, A = 18\u03ba 2p \u2016T \u2016 and B = 27\u03ba2p \u2016T \u2016 , 6 we know that\n18\u03ba2p\n\u2016T \u2016 log\n( 27\u03ba2p\n\u2016T \u2016 m\n) \u2264 18\u03ba 2p2\n\u2016T \u2016 log\n( 486\u03ba4p3\n\u2016T \u20162\n) + 1\n2 m1/p,\nand consequently we know that the condition m \u2265 m\u03b4 is met if m \u2265 m0, where\nm0 =\n( 36\u03ba2p2\n\u2016T \u2016 log\n( 486\u03ba4p3\n\u2016T \u20162\n))1/p .\nLetting \u03b7t = \u03b7\u03ba \u22122 with \u03b7 = 116m (which is \u03b7t ' 1 m ), we know that \u03b7 \u2264 1 8(log T+1) , since 3 \u2264 T \u2264 m2 and logm \u2264 m\u2212 1. We thus verified the conditions of Theorem 3.2. We thus have (8). By introducing with \u03b4 = 1/m, \u03b7 = 116m and T \u2264 m 2,\nEJ\u2016S\u03c1\u03c9t+1 \u2212 fH\u20162\u03c1 . ( m\u22121t )\u22122\u03b6 +m\u2212 2\u03b6 2\u03b6+\u03b3 (1 \u2228m\u2212 1 2\u03b6+\u03b3\u22121t)2 log6m+m\u22121(1 \u2228m\u2212 1 2\u03b6+\u03b3\u22121t) logm. Usingm\u22121 \u2264 m\u2212 2\u03b6 2\u03b6+\u03b3 \u2264 ( m\u22121t )\u22122\u03b6 +m\u2212 2\u03b6 2\u03b6+\u03b3 (m\u2212 1 2\u03b6+\u03b3\u22121t)2, one can prove the desired result.\n6B is always greater than 1 since \u03ba2 \u2265 \u2016T \u2016 and p \u2265 1.\nThe proofs for the other corollaries parallelize to the above. We thus omit."}, {"heading": "E.2 Non Attainable Case", "text": "Theorem E.2. Under Assumptions 1, 2 and 3, let \u03b6 \u2264 1/2, T \u2208 N with T \u2265 3, \u03b4 \u2208]0, 1[, \u03b7t = \u03b7\u03ba\n\u22122t\u2212\u03b8 for all t \u2208 [T ], with \u03b8 \u2208 [0, 1[ and \u03b7 such that (59) and for some \u2208]0, 1], (60) holds. Then the following holds with probability at least 1\u2212 \u03b4: for all t \u2208 [T ],\nEJ[E(\u03c9t+1)]\u2212 inf \u03c9\u2208H E(\u03c9) . (\u03b7t1\u2212\u03b8)\u22122\u03b6 +m\u03b3(1\u2212 )\u22121(1 \u2228 \u03b72m2 \u22122t2\u22122\u03b8)\n( 1 \u2228 \u03b7t1\u2212\u03b8 )1\u22122\u03b6 log2 t log2 4\n\u03b41\n+\u03b7b\u22121(t\u2212min(\u03b8,1\u2212\u03b8) \u2228m \u22121\u03b7t(1\u22122\u03b8)+) log T. (64)\nProof. The proof is similar to that for Theorem E.1. We include the sketch only. Similar to the proof of Theorem E.1, one can prove that with probability at least 1 \u2212 \u03b41 \u2212 \u03b42 \u2212 \u03b43, (40), (53) and (36) hold for all k \u2208 [T ]. Now, we can apply Propositions C.5 and D.8 to get (44) and (58). Noting that by (57), \u221a 2\u03b7 \u2264 1, and by a simple calculation, we derive from (44) that\n\u2016S\u03c1\u03bdt+1 \u2212 S\u03c1\u00b5t+1\u20162\u03c1 \u2264 400\n( \u03ba2\u03b6 ( 1 \u2228 2\u03b7t 1\u2212\u03b8\n1\u2212\u03b8\n) 1 2\u2212\u03b6\n+ \u221a M )2 (\u03ba/ \u221a \u2016T \u2016+ \u221a 2 \u221a vc\u03b3/\u2016T \u2016\u03b3)2\n(1\u2212 \u03b8)2\n\u00d7m\u03b3(1\u2212 )\u22121(1 \u2228 \u03bb2\u03b72\u03ba\u22124t2\u22122\u03b8 \u2228 log2 t) log2 4 \u03b41 .\nThe rest of the proof parallelizes to that for Theorem E.1.\nRemark E.3. Letting \u03b8 = 0 in the above theorem, and ignoring the logarithmic terms, the bound (64) reads as\nEJ[E(\u03c9t+1)]\u2212 inf \u03c9\u2208H E(\u03c9) . (\u03b7t)\u22122\u03b6 +m\u03b3(1\u2212 )\u22121(1 \u2228m \u22121\u03b7t)2 (1 \u2228 \u03b7t)1\u22122\u03b6 + \u03b7b\u22121(1 \u2228m \u22121\u03b7t).\nRemark E.4. Better bounds for the case \u03b6 \u2264 1/2 will be proved in the longer version of this paper."}], "references": [{"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Online gradient descent learning algorithms", "author": ["Yiming Ying", "Massimiliano Pontil"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence", "author": ["Pierre Tarres", "Yuan Yao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Non-parametric stochastic approximation with large step sizes", "author": ["Aymeric Dieuleveut", "Francis Bach"], "venue": "arXiv preprint arXiv:1408.0361,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Introduction to Optimization", "author": ["Boris T Poljak"], "venue": "Optimization Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Stochastic subgradient methods", "author": ["Stephen Boyd", "Almir Mutapcic"], "venue": "Notes for EE364b,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Generalization properties and implicit regularization of multiple passes SGM", "author": ["Junhong Lin", "Raffaello Camoriano", "Lorenzo Rosasco"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Learning bounds for kernel regression using effective data dimensionality", "author": ["Tong Zhang"], "venue": "Neural Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Optimal rates for the regularized least-squares algorithm", "author": ["Andrea Caponnetto", "Ernesto De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Optimization for Machine Learning", "author": ["Suvrit Sra", "Sebastian Nowozin", "Stephen J Wright"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Machine learning. Coursera", "author": ["Andrew Ng"], "venue": "Standford University,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Steve Smale", "Ding-Xuan Zhou"], "venue": "Constructive Approximation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Learning with incremental iterative regularization", "author": ["Lorenzo Rosasco", "Silvia Villa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning Theory: an Approximation", "author": ["Felipe Cucker", "Ding-Xuan Zhou"], "venue": "Theory Viewpoint,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": "Springer Science Business Media,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Remarks on inequalities for large deviation probabilities", "author": ["IF Pinelis", "AI Sakhanenko"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1986}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["Alessandro Rudi", "Raffaello Camoriano", "Lorenzo Rosasco"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["Joel A Tropp"], "venue": "Technical report, DTIC Document,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "On some extensions of bernstein\u2019s inequality for self-adjoint operators", "author": ["Stanislav Minsker"], "venue": "arXiv preprint arXiv:1112.5448,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 2, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 3, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 4, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 5, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 6, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 7, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].", "startOffset": 190, "endOffset": 199}, {"referenceID": 4, "context": "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].", "startOffset": 190, "endOffset": 199}, {"referenceID": 5, "context": "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].", "startOffset": 190, "endOffset": 199}, {"referenceID": 8, "context": "While the role of multiple passes is well understood if the goal is empirical risk minimization [9], its effect with respect to generalization is less clear and a few recent works have recently started to tackle this question.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "In particular, results in this direction have been derived in [10] and [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "In particular, results in this direction have been derived in [10] and [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization.", "startOffset": 164, "endOffset": 172}, {"referenceID": 12, "context": "The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization.", "startOffset": 164, "endOffset": 172}, {"referenceID": 13, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 14, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 15, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 16, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 17, "context": "In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 12, "context": "In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 18, "context": "Finally we note that a recent work [19] is tightly related to the analysis in the paper.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "The generalization properties of a multi-pass incremental gradient are analyzed in [19], for a cyclic, rather than a stochastic, choice of the gradients and with no mini-batches.", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "The analysis in this latter case appears to be harder and results in [19] give good learning bounds only in restricted setting and considering iterates rather than the excess risk.", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "Compared to [19] our results show how stochasticity can be exploited to get faster capacity dependent rates and analyze the role of mini-batches.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Following [19], the formulation we consider is close to the setting of functional regression, and covers the reproducing kernel Hilbert space (RKHS) setting as special cases.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "Under Assumption (3), L can be proved to be positive trace class operators, and hence L with \u03b6 \u2208 R can be defined by using the spectrum theory [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "The above assumption is fairly standard [20, 19] in non-parametric regression.", "startOffset": 40, "endOffset": 48}, {"referenceID": 18, "context": "The above assumption is fairly standard [20, 19] in non-parametric regression.", "startOffset": 40, "endOffset": 48}, {"referenceID": 20, "context": "In particular, for \u03b6 = 0, we are assuming \u2016fH\u2016\u03c1 < \u221e, while for \u03b6 = 1/2, we are requiring fH \u2208 H\u03c1, since [21, 19] H\u03c1 = L(L(H, \u03c1X)).", "startOffset": 104, "endOffset": 112}, {"referenceID": 18, "context": "In particular, for \u03b6 = 0, we are assuming \u2016fH\u2016\u03c1 < \u221e, while for \u03b6 = 1/2, we are requiring fH \u2208 H\u03c1, since [21, 19] H\u03c1 = L(L(H, \u03c1X)).", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "(7) The left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13].", "startOffset": 94, "endOffset": 102}, {"referenceID": 12, "context": "(7) The left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13].", "startOffset": 94, "endOffset": 102}, {"referenceID": 20, "context": "It can be related to covering/entropy number conditions, see [21] for further details.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "The above result asserts that, at p\u2217 passes over the data, the simple SGM with fixed step-size achieves optimal learning error bounds, matching those of ridge regression [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "Using an argument similar to that in Chapter 6 from [21], it is possible to show that this procedure can achieve the same convergence rate.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "First, the upper bound in (10) is optimal up to a logarithmic factor, in the sense that it matches the minimax lower rate in [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "6 recovers the result in [4] for one pass SGM.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 21, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 4, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 5, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 3, "context": "In particular, [4] proved capacity independent rate of order O(m\u2212 2\u03b6 2\u03b6+1 logm) with a fixed step-size \u03b7 ' m\u2212 2\u03b6 2\u03b6+1 , and [6] derived capacity dependent error bounds of order O(m 2min(\u03b6,1) 2min(\u03b6,1)+\u03b3 ) (when 2\u03b6 + \u03b3 > 1) for the average.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "In particular, [4] proved capacity independent rate of order O(m\u2212 2\u03b6 2\u03b6+1 logm) with a fixed step-size \u03b7 ' m\u2212 2\u03b6 2\u03b6+1 , and [6] derived capacity dependent error bounds of order O(m 2min(\u03b6,1) 2min(\u03b6,1)+\u03b3 ) (when 2\u03b6 + \u03b3 > 1) for the average.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 155, "endOffset": 165}, {"referenceID": 1, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 155, "endOffset": 165}, {"referenceID": 0, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 155, "endOffset": 165}, {"referenceID": 18, "context": "More recently, [19] studied multiple passes SGM with a fixed ordering at each pass, also called incremental gradient method.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "Note also that [19] proved sharp rate in H-norm for \u03b6 \u2265 1/2 in the capacity independent case.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 14, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 15, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 16, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 22, "context": "Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size \u03b7 ' b/ \u221a m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence", "startOffset": 30, "endOffset": 38}, {"referenceID": 14, "context": "Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size \u03b7 ' b/ \u221a m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence", "startOffset": 30, "endOffset": 38}, {"referenceID": 20, "context": "For any \u03c9 \u2208 H, we have [21, 19] E(\u03c9)\u2212 inf f\u2208H E(f) = \u2016S\u03c1\u03c9 \u2212 fH\u2016\u03c1.", "startOffset": 23, "endOffset": 31}, {"referenceID": 18, "context": "For any \u03c9 \u2208 H, we have [21, 19] E(\u03c9)\u2212 inf f\u2208H E(f) = \u2016S\u03c1\u03c9 \u2212 fH\u2016\u03c1.", "startOffset": 23, "endOffset": 31}, {"referenceID": 0, "context": "Here, the regression function is f\u03c1(x) = |x\u22121/2|\u22121/2, the input point xi is uniformly distributed in [0, 1], and \u03c9i is a Gaussian noise with zero mean and standard deviation 1, for each i \u2208 [m].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "Here, we replace the unknown marginal distribution \u03c1X by an empirical measure \u03c1\u0302 = 1 2000 \u22112000 i=1 \u03b4x\u0302i , where each x\u0302i is uniformly distributed in [0, 1].", "startOffset": 150, "endOffset": 156}, {"referenceID": 0, "context": "[1] Olivier Bousquet and L\u00e9on Bottou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Yiming Ying and Massimiliano Pontil.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Pierre Tarres and Yuan Yao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Aymeric Dieuleveut and Francis Bach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Francesco Orabona.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Boris T Poljak.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Stephen Boyd and Almir Mutapcic.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Moritz Hardt, Benjamin Recht, and Yoram Singer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Andrea Caponnetto and Ernesto De Vito.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Suvrit Sra, Sebastian Nowozin, and Stephen J Wright.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Andrew Ng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Steve Smale and Ding-Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Lorenzo Rosasco and Silvia Villa.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Felipe Cucker and Ding-Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Ingo Steinwart and Andreas Christmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Ohad Shamir and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] IF Pinelis and AI Sakhanenko.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Stanislav Minsker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "For any \u03c9 \u2208 H, it is easy to prove the following isometry property [21] \u2016S\u03c1\u03c9\u2016\u03c1 = \u2016 \u221a T \u03c9\u2016H .", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "2 Concentration Inequality We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].", "startOffset": 142, "endOffset": 146}, {"referenceID": 23, "context": "2 Concentration Inequality We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].", "startOffset": 198, "endOffset": 202}, {"referenceID": 3, "context": "Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].", "startOffset": 82, "endOffset": 88}, {"referenceID": 4, "context": "Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].", "startOffset": 82, "endOffset": 88}, {"referenceID": 24, "context": "The result is essentially proved in [26], see also [19].", "startOffset": 36, "endOffset": 40}, {"referenceID": 18, "context": "The result is essentially proved in [26], see also [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "The proof for the fixed step-size can be found in [19].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "The case for \u03b6 \u2265 1/2 is similar to that in [19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.", "startOffset": 93, "endOffset": 101}, {"referenceID": 27, "context": "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.", "startOffset": 93, "endOffset": 101}, {"referenceID": 21, "context": "The process relies on some tools from convex analysis and a decomposition related to the weighted averages and the last iterates from [22, 30].", "startOffset": 134, "endOffset": 142}, {"referenceID": 21, "context": "Using the above lemma and a decomposition related to the weighted averages and the last iterates from [22, 30], we can prove the following relationship.", "startOffset": 102, "endOffset": 110}], "year": 2016, "abstractText": "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.", "creator": "LaTeX with hyperref package"}}}