{"id": "1605.09082", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "One-Pass Learning with Incremental and Decremental Features", "abstract": "In many real-world tasks, the features evolve, with some features disappearing and other features added. For example, in environmental monitoring, some sensors expired while some new ones were used; in the mobile game recommendation, some games were dropped, while some new ones were added. Learning with such incremental and degressive features is critical, but rarely explored, especially when the data comes like a stream and it is therefore impracticable to keep all the data for optimization. In this paper, we examine this difficult problem and present the OPID approach. Our approach seeks to compress important information from vanished features into functions of surviving features and then extend it to the advanced features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store all the data in order to accommodate the evolving streaming data nature.The effectiveness of our approach is theoretically and empirically validated.", "histories": [["v1", "Mon, 30 May 2016 01:18:47 GMT  (287kb,D)", "http://arxiv.org/abs/1605.09082v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chenping hou", "zhi-hua zhou"], "accepted": false, "id": "1605.09082"}, "pdf": {"name": "1605.09082.pdf", "metadata": {"source": "CRF", "title": "One-Pass Learning with Incremental and Decremental Features", "authors": ["Chenping Hou", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically. Key words: One-pass learning, Incremental and Decremental Features, classification, robust learning\nIn many real applications, the features are evolving, with some features being vanished and some other features augmented. For example, in the research of environment monitoring, in order to detect the environment in full aspects, different kinds of sensors, such as trace metal, radioisotope, volatile organic compound and biological sensors [HRMD05], are deployed in a dynamic way. Due to the differences in working ways and working conditions, such as optical, electrochemical or gravimetric [SPY03], some sensors expired whereas some new sensors deployed. If we regard the output of each sensor as a feature, the data features are both incremental and decremental. This\n\u2217Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review May 31, 2016\nar X\niv :1\n60 5.\n09 08\n2v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 6\nsetting also occurs in the mobile game recommendation system in Android market [SMMP12]. There are a lot of people making ratings for many games. Some games dropped whereas some new ones added with times elapsing. It is also a feature evolution system.\nCompared with traditional problems, there are at least two challenges in analyzing this kinds of data. (1) In these applications, the instances are coming like a stream. It is different from traditional learning paradigm, since the features and instances are evolving simultaneously. (2) Due to the streaming nature, it is infeasible to keep the whole data. It requires us to access the data in one-pass way.\nLearning with such incremental and decremental features and evolving instances is crucial but rarely studied. Several related works have been proposed to solve part of this problem. To manipulate instance evolving problem, online learning, which can be traced back to Perceptron algorithm [Ros58], is a standard learning paradigm and there are plenty of researches recently [CP00, CBL06, HAK07, ZYJZ15]. One-pass learning, as a special case of online learning, has also attracted many research interests in recent years [GJZZ13, ZGZ15]. To solve feature incremental and decremental problem, there are some researches concerning missing and corrupted features, such as [GR06, DS08, TGRS07, HLM15].\nAlthough these researches have achieved prominent performances in their learning settings, they cannot fulfill the requirements arisen from the above-mentioned real applications, since they only focus on one aspect of instance and feature evolution problem, either instance evolution or feature evolution. Direct combination of two types of previous methods cannot deal with the problem well.\nIn this paper, we try to manipulate this problem by proposing One-Pass Incremental and Decremental learning approach (OPID). For clarity, we tackle it in two stages, i.e., Compressing stage (C-stage) and Expanding stage (E-stage). In C-stage, we propose a new one-pass learning method by compressing important information of vanished features into functions of survived features. It only accesses the instance once and extract useful information to assist the following learning task. In E-Stage, accompanied by the learning model trained in C-stage, we present a new learning method which can include augmented features and inherit the advantages from C-stage. As far as we know, this is the first research about the problem with simultaneous instance and feature evolution. Several theoretical and experimental results are provided for illustration."}, {"heading": "1. Preliminaries", "text": "In our work, we mainly focus on the learning problem in two stages, i.e., C-stage and E-stage with feature and instance evolution simultaneously. In C-stage, data are collected in mini-batch style. Assume that there are totally T1 batches. In each batch, the features of each instance can be divided into two parts. The first part contains the features which will vanish in E-stage and the other part consists of the features which survive for both stages. They are named as vanished feature and survived feature. In E-stage, we assume that there are two batches of data. One batch is employed for training and the other one is used for testing. The features of each instance in this stage can also be divided into two parts. The first part contains the features that are survived in both stages. The second part is augmented features, which is referred as augmented feature in the following.\nFormally, as pictured in Fig. 1, in the i-batch of C-stage, data points can be represented by two matrices, i.e., X (v) i \u2208 Rni\u00d7d (v) and X (s) i \u2208 Rni\u00d7d (s) , where ni is the number of points in this batch, d(v) and d(s) are the numbers of vanished features and survived features respectively. Here, the superscripts \u201d(v)\u201d and \u201d(s)\u201d correspond to vanished features and survived features. The j-th row of X (v) i is an instance with only vanished features. Correspondingly, the j-th row of X (s) i is an instance with only survived features. The label matrix of instances in the i-batch is denoted by Yi \u2208 Rni\u00d7c with c as the number of class. Its (k, l)-element Yi(k, l) = 1 if and only if the k-th instance in the i-batch belongs to the l-th category and Yi(k, l) = 0 otherwise.\nIn E-stage, it is often that we want to make prediction when we only get one batch training data.\nThus, there are only two batches in this stage. The first batch contains training data, represented by X (s) T1+1 \u2208 RnT1+1\u00d7d(s) and X(a)T1+1 \u2208 R nT1+1\u00d7d (a) , where nT1+1 is the number of training points in this stage, d(a) is the numbers of augmented features and the superscript \u201d(a)\u201d corresponds to augmented features. Similarly, the j-th row of X (s) T1+1 consists of survived features and the j-th row of X (a) T1+1 contains augmented features. The label matrix is denoted by YT1+1. Similarly, the second batch contains testing points with the same structure as training.\nAccording to above notations, our main task is to classify data points represented by X\u0304T1+2 , [X (s) T1+2 ,X (a) T1+2 ], based on {X\u0304T1+1 , [X (s) T1+1 ,X (a) T1+1 ], YT1+1}. Besides, we can also use models learned on {X\u0303i , [X(v)i ,X (s) i ], Yi} T1 i=1, without saving all the data. In the T -th step, we can only access the data {X\u0303T , YT }, without saving the past data {X\u0303i, Yi}T\u22121i=1 . Here, a tilde above the symbol represents variable in C-stage and a bar represents variable in E-stage.\nIt is noteworthy to mention that we can also use {X\u0304T1+1,YT1+1} merely to train a classifier in E-stage. Nevertheless, in many real applications, since nT1+1 is often comparable to that of ni for i = 1, 2, \u00b7 \u00b7 \u00b7 , nT1 and they are often small due to the limitation of storage, training with only the instance in E-stage trends to be over-fitting. It is better to learn a classifier with the assistance of model trained in C-stage."}, {"heading": "2. The OPID Approach", "text": "We investigate a real application problem with complicated settings, and it is difficult to use traditional approaches to solve this problem directly. There are two stages and both the instances and features are changed. In our paper, we tackle this problem in following way.\n1. In C-stage, we learn a classifier based on {X\u0303i,Yi}T1i=1 in one-pass way. That is, we only access training examples once. Besides, the learned classifier should also provide useful classification information to help the training in E-stage.\n2. In E-stage, we learn a classifier based on {X\u0304T1+1,YT1+1}, under the supervision of classifier\nlearned in C-stage.\nC-stage: In this stage, there are two different kinds of features, i.e., vanished feature and survived feature. If we combine them to learn a unified classifier, it will be difficult to use it in E-stage,\nsince the features are different in two stages. Notice that, there are features surviving in both stages. It is better to compress important information of vanished features into functions of survival features. In other words, we want to use the model trained in survived features to represent important information of both vanished features and survived features.\nLet H\u0303 and H(s) be the function space for all features (vanished features and survived features) and the survived features respectively. In C-stage, we try to learn two classifiers h\u0303 \u2208 H\u0303 and h\u0303(s) \u2208 H(s) with some consistency constraints between them. Denote the loss function on all features and survived features as \u02dc\u0300 and \u02dc\u0300(s), the expected classifiers in C-stage can be learned by optimizing\nmin h\u0303\u2208H,h\u0303(s)\u2208H(s) T1\u2211 i=1 \u02dc\u0300 ( h\u0303(X\u0303i),Yi ) + \u02dc\u0300(s) ( h\u0303(s)(X (s) i ),Yi ) s.t. D ( h\u0303(X\u0303i), h\u0303 (s)(X (s) i ) ) \u2264 . for i \u2208 [T1] .\n(1)\nHere X\u0303i = [X (v) i ,X (s) i ]. D is employed to measure the consistency between two classifiers on\nevery batch. We denote [T1] = {1, 2, \u00b7 \u00b7 \u00b7 , T1} for the convenience of presentation.\nFor example, we assume h\u0303 and h\u0303(s) are two linear classifiers with square loss. Besides, the Frobenius norm (denoted by \u2016 \u00b7 \u2016) is employed as the measurement of consistency. The optimization problem in Eq. (1) becomes\nmin W\u0303,W(s) T1\u2211 i=1 \u2016\u3008W\u0303, X\u0303i\u3009 \u2212Yi\u20162 + T1\u2211 i=1 \u2016\u3008W(s),X(s)i \u3009 \u2212Yi\u2016 2\n+ \u03bb T1\u2211 i=1 \u2016\u3008W\u0303, X\u0303i\u3009 \u2212 \u3008W(s),X(s)i \u3009\u2016 2 + \u03c1(\u2016W\u0303\u20162 + \u2016W(s)\u20162),\n(2)\nwhere W\u0303 and W(s) are classifier coefficients defined on all features and survived features in C-stage respectively, \u03bb > 0 is the parameter to tune the importance of consistency constraint and \u03c1 > 0 is the parameter for regularization. It is an extension of traditional regularized least square classifier by adding the consistency constraint. We will solve it by two types of one-pass learning methods.\nE-stage: We expand the classifier trained on survived features, i.e., h\u0303(s), in C-stage to accommodate augmented features. It can take X (s) T1+1 as the input directly. Denote h\u0303 (s) \u2217 as the optimal classifier and W (s) \u2217 as its coefficient in C-stage, then Z (s) T1+1 , h\u0303(s)\u2217 (X (s) T1+1 ) = X (s) T1+1 W (s) \u2217 is the\nprediction by employing the classifier training in C-stage. We take this prediction as new representations of X (s) T1+1 as in stacking [Bre96, Zho12]. After that, we train a classifier h\u0304(s) on Z (s) T1+1 and simultaneously, another classifier h\u0304 is trained on Z\u0304T1+1 , [Z (s) T1+1 ,X (a) T1+1 ] for accommodation. It can be regarded as expansion of the optimal classifier in C-stage to include augmented features.\nTo inherit advantages of h\u0303 (s) \u2217 trained in C-stage, we combine two classifiers like ensemble methods [Zho12]. At first, we employ this strategy to unify two classifiers by optimizing the following problem.\nmin h\u0304(s),h\u0304\nw1 \u00af\u0300 (s) ( h\u0304(s)(Z\n(s) T1+1 ),YT1+1\n) + w2 \u00af\u0300 ( h\u0304(Z\u0304T1+1),YT1+1 ) , (3)\nwhere w1 \u2265 0, w2 \u2265 0, w1 +w2 = 1, are the weights to balance two classifiers. \u00af\u0300(s) and h\u0304 are two\nsurrogate loss function defined on Z (s) T1+1 and Z\u0304T1+1 respectively.\nFor simplicity, we use the L2-regularized Logistic Regression model for each classifier. Take the binary classification problem as an example, the objective functions are\n\u00af\u0300(s) ( h\u0304(s)(Z\n(s) T1+1 ),yT1+1\n) = 1\n2 v(s)(v(s))> + \u03b11 nT1+1\u2211 j=1 log(1 + exp(\u2212yT1+1,jz (s) T1+1,j v(s)))\n\u00af\u0300 ( h\u0304(Z\u0304T1+1),yT1+1 ) = 1\n2 v\u0304v\u0304> + \u03b12 nT1+1\u2211 j=1 log(1 + exp(\u2212yT1+1,j z\u0304T1+1,jv\u0304))\n(4)\nwhere v(s) and v\u0304 are coefficients. z (s) T1+1,j and z\u0304T1+1,j are the j-th row (the j-th instances) of\nZ (s) T1+1 and Z\u0304T1+1. \u03b11 and \u03b12 are balance parameters. yT1+1,j is 1 or -1 for binary classification.\nOne point should be mentioned here. We take Z (s) T1+1 as the new representation, although it can be regarded as the classification results directly. The reasons are: (1) The prediction Z (s) T1+1 is computed by the classifier trained on C-stage, we can use training data in E-stage to improve h\u0303 (s) \u2217 . This composite works since h\u0303 (s) \u2217 and h\u0304 (s) are trained on different data sets. (2) As \u2211T1 i=1 ni is often much larger than nT1+1, by contrast with the classifier trained on X (s) T1+1 merely, h\u0303 (s) \u2217 is a better classifier and it could extract more discriminative information. In other words, compared with X (s) T1+1 , Z (s) T1+1 is a more compact and accurate representation. (3) If h\u0303 (s) \u2217 is good enough, the composite of h\u0304(s) will not degrade the performance by our following strategy."}, {"heading": "3. Optimization and Extension", "text": ""}, {"heading": "3.1. Optimization", "text": "C-stage: The optimization problem in Eq.(1) can be divided into T1 subproblems, thus, it is direct to use the online learning method, such as online ADMM [WB12], to solve it by scanning the data only once. Nevertheless, we aim to train h\u0303(s) to assist the classification in E-stage, and the most direct way is to employ a linear classifier. In this case, we will provide more effective one-pass learning methods than using ADMM.\nProposition 1 The optimal solution to Eq. (2) can be obtained by solving\nA[T1]  W\u0303 W(s)  = B[T1], with B[T ] , T\u2211 i=1  X\u0303>i (X (s) i ) > Yi , (5)\nA[T ] ,  (1 + \u03bb)\u2211Ti=1 X\u0303>i X\u0303i + \u03c1I \u2212\u03bb\u2211Ti=1 X\u0303>i X(s)i \u2212\u03bb \u2211T\ni=1(X (s) i ) >X\u0303i (1 + \u03bb) \u2211T i=1(X (s) i ) >X (s) i + \u03c1I  , (6) and I is an identity matrix.\nProof: Take the derivative of the objective function in Eq. (2) with respect to W\u0303 and W(s) and set them to zeros, we have the following equations.\nT1\u2211 i=1 X\u0303>i (X\u0303iW\u0303 \u2212Yi) + \u03bb T1\u2211 i=1 X\u0303>i (X\u0303iW\u0303 \u2212X (s) i W (s)) + \u03c1W\u0303 = 0,\nT1\u2211 i=1 (X (s) i ) >(X (s) i W (s) \u2212Yi) + \u03bb T1\u2211 i=1 (X (s) i ) >(X (s) i W (s) \u2212 X\u0303iW\u0303) + \u03c1W(s) = 0.\n(7)\nDenote A[T ] and B[T ] as shown in Eq. (6) and Eq. (5). The optimization problem problem in\nEq. (7) becomes\nA[T1]  W\u0303 W(s)  = B[T1]. (8) It is just the results shown in Proposition 1.\nBased on the above deduction, we turn to solve the problem in Eq. (2) in one-pass way quickly. In the T -th time, we only access the instance {X\u0303i, Yi}Ti=1. The counterpart optimization problem is the same as Eq. (2), except that the sum of subscript i is from 1 to T .\nNotice that the solution to problem in Eq. (2) is determined by A[T ] and B[T ] defined in Eq. (6) and Eq. (5). This evokes us to get the following updating rule.\nA[T+1] =A[T ] +  (1 + \u03bb)X\u0303>T+1X\u0303T+1 \u2212\u03bbX\u0303>T+1X(s)T+1 \u2212\u03bb(X(s)T+1)>X\u0303T+1 (1 + \u03bb)(X (s) T+1) >X (s) T+1  , B[T+1] =B[T ] +  X\u0303>T+1 (X\n(s) T+1) >\nYT+1, (9)\nA1 =  (1 + \u03bb)X\u0303>1 X\u03031 + \u03c1I \u2212\u03bbX\u0303>1 X(s)1 \u2212\u03bb(X(s)1 )>X\u03031 (1 + \u03bb)(X (s) 1 ) >X (s) 1 + \u03c1I  ,B1 =  X\u0303>1 (X (s) 1 ) > Y1. (10)\nAccording to this result, in time T + 1, we only need to update A[T+1] and B[T+1] by adding the matrices calculated based on the data in batch T + 1. In other words, we just need to store the matrices A[T ] and B[T ] and update them based on Eq.(9) in each iteration.\nThis approach has the following advantages: (1) It just needs to store two matrices with size (d(v) + 2d(s)) \u00d7 (d(v) + 2d(s)) and (d(v) + 2d(s)) \u00d7 c. When d(v) + 2d(s) < \u2211T1\ni=1 ni, it needs less\nspace than storing the whole data. Through this way, we can get the optimal solution to Eq. (2) by scanning the total data only once. (2) We only make matrix multiplication in updating A[T ] and B[T ], the computational cost is small. In solving Eq. (5), the most time-consuming step is computing the inverse of a matrix with size (d(v) + 2d(s)) \u00d7 (d(v) + 2d(s)). Thus, this method is very efficient with large data number and small data dimensionality.\nCompared with the data size, when the number of features, i.e., d(v) + d(s), is rather large, it is unwise to compute the inverse of A[T1] directly. In this case, we propose another one-pass learning approach in solving the optimization problem in Eq. (5).\nDefine A[0] = \u03c1I and B[0] = 0, the updating rule shown in Eq. (9) can be initialized from T = 0. Note that A\u22121[0] = (1/\u03c1)I. If we can replace the updating rule of A[T+1] in Eq. (9) by the\nupdating rule of A\u22121[T+1] with less computational cost in each iteration, the computational burden in calculating A\u22121[T1] will release. Notice that, the added matrix in updating A[T+1] is not a full rank matrix if nT+1 is smaller than min{d(v), d(s)}. We will use this property to compute the inverse with low cost.\nProposition 2 The updating rule of A\u22121[T+1] is\nA\u22121[T+1] = A \u22121 [T ] \u2212A \u22121 [T ]UT+1 ( I + U>T+1A \u22121 [T ]UT+1 )\u22121 U>T+1A \u22121 [T ]\n(11)\nwhere UT+1 = [UT+1,1,UT+1,2,UT+1,3] and\nUT+1,1 =  X\u0303>T+1 0  ,UT+1,2 =  0\n(X (s) T+1) >\n ,UT+1,3 =  \u221a\u03bbX\u0303>T+1 \u2212 \u221a \u03bb(X\n(s) T+1) >  . Proof: Note that, the updating rule of A[T+1] is shown in Eq. (9). We now decompose the adding part as (1 + \u03bb)X\u0303>T+1X\u0303T+1 \u2212\u03bbX\u0303>T+1X(s)T+1 \u2212\u03bb(X(s)T+1)>X\u0303T+1 (1 + \u03bb)(X (s) T+1) >X (s) T+1  =  X\u0303>T+1X\u0303T+1 0 0 0\n +\n 0 0 0 (X\n(s) T+1) >X (s) T+1 +  \u03bbX\u0303>T+1X\u0303T+1 \u2212\u03bbX\u0303>T+1X(s)T+1 \u2212\u03bb(X(s)T+1)>X\u0303T+1 \u03bb(X (s) T+1) >X (s) T+1  (12)\nDenote UT+1,1, UT+1,2 and UT+1,3 as shown in Proposition 2, we have (1 + \u03bb)X\u0303>T+1X\u0303T+1 \u2212\u03bbX\u0303>T+1X(s)T+1 \u2212\u03bb(X(s)T+1)>X\u0303T+1 (1 + \u03bb)(X (s) T+1) >X (s) T+1  =UT+1,1U > T+1,1 + UT+1,2U > T+1,2 + UT+1,3U > T+1,3\n=UT+1U > T+1,\n(13)\nwith UT+1 = [UT+1,1,UT+1,2,UT+1,3].\nUsing the Woodbury equation [Hig02], we have the results as follows.\nA\u22121[T+1] = (A[T ] + UT+1U > T+1) \u22121 = A\u22121[T ] \u2212A \u22121 [T ]UT+1 ( I + U>T+1A \u22121 [T ]UT+1 )\u22121 U>T+1A \u22121 [T ] (14)\nIt is the results shown in Proposition 2.\nSimilarly, it is also the one-pass way in updating A\u22121[T+1] and we need to access the whole data only once. In each iteration shown in Eq. (11), the most computational step is calculating the inverse of a matrix with size 3nT+1 \u00d7 3nT+1. If the batch size nT+1 is small, its computational cost is limited and we can compute A\u22121[T1] in a quick way. Especially, if ni = 1 for i \u2208 [T1] as in traditional online learning, we only need to compute the inverse of a 3\u00d7 3 matrix.\nBesides, a byproduct of this kind of iteration is that we can get the optimal solution at any time T , since we have derived A\u22121[T ] directly. If we use the iteration method shown in Eq. (9), we need to calculate A\u22121[T ] at each time T . When this requirement is frequent, the computational cost will increase since we need to compute the matrix inverse for each requirement.\nE-stage: The optimization problem in Eq. (3) with concrete forms defined in Eq. (4) has been widely investigated in previous works and the details are omitted. We use the implementation of LibLinear [FCH+08] to solve them and the parameters w1 and w2 are turned by cross validation."}, {"heading": "3.2. Extension", "text": "Note that in Eq. (3), the interaction between two classifiers is a balance of classification results by turning the weights. We think the more direct way is combining all the features as in stacking [Bre96] and training a unified classifier on the stacked representations. This evokes the following formulation.\nmin h\u0304(s),h\u0304,w1,w2\n` (\u221a w1h\u0304 (s)(Z\n(s) T1+1 ) + \u221a w2h\u0304(Z\u0304T1+1),YT1+1 ) , (15)\nwhere ` is a general loss on the joint representations. Here, we use the square root of balance parameters to guarantee their convexity and avoid the trivial solution. They can be learned automatically without extra hyper-parameters.\nTaking regression with kernels as an example, we have the following concrete formulation.\nmin V(s),V\u0304,w1,w2 \u2225\u2225\u2225\u221aw1\u3008V(s),\u03a6(Z(s)T1+1)\u3009+\u221aw2\u3008V\u0304,\u03a6(Z\u0304T1+1)\u3009 \u2212YT1+1\u2225\u2225\u22252 +\u03b3( 1\nc \u2016V(s)\u20162 + 1 c+ d(a) \u2016V\u0304\u20162), with w1 + w2 = 1, w1 \u2265 0, w2 \u2265 0 ,\n(16)\nwhere \u03a6(\u00b7) is a mapping function and \u03b3 is the parameter for regularization. The feature numbers\nof Z (s) T1+1 and Z\u0304T1+1 are c, c + d (a), and c is often much smaller than c + d(a). To alleviate the influence caused by the unbalance, each regularizer is divided by the corresponding feature number.\nIt is not easy to solve the problem in Eq. (16) directly. After some deductions, it is equal to\nmin V(s),V\u0304,w1,w2 \u2225\u2225\u2225\u3008V(s),\u03a6(Z(s)T1+1)\u3009+ \u3008V\u0304,\u03a6(Z\u0304T1+1)\u3009 \u2212YT1+1\u2225\u2225\u22252 +\u03b3( 1\nc\u00d7 w1 \u2016V(s)\u20162 + 1 (d(a) + c)\u00d7 w2 \u2016V\u0304\u20162), with w1 + w2 = 1, w1 \u2265 0, w2 \u2265 0.\n(17)\nThe optimization problem in Eq. (17) seems very similar to traditional regularized square loss regression. Nevertheless, they are different since the regularization parameter is fixed in traditional method, while it is also an optimization parameter in our setting.\nThere are two groups of optimization variables, i.e., the coefficients for classification and the balance parameters. It is difficult to optimize them together and we optimize them alternatively.\nWhen V(s) and V\u0304 are fixed, the optimal balance parameters w1 and w2 can be computed by the following proposition directly.\nProposition 3 When w1 + w2 = 1, w1 \u2265 0, w2 \u2265 0,\nmin w1,w2\n( 1 w1 \u00d7 c \u2016V(s)\u20162 + 1 w2 \u00d7 (d(a) + c) \u2016V\u0304\u20162) = ( 1\u221a c \u2016V(s)\u2016+ 1\u221a d(a) + c \u2016V\u0304\u2016)2. (18)\nThe optimal solution is\nw\u22171 = \u2016V(s)\u2016/\n\u221a c\n\u2016V(s)\u2016/ \u221a c+ \u2016V\u0304\u2016/ \u221a d(a) + c , w\u22172 = \u2016V\u0304\u2016/\n\u221a d(a) + c\n\u2016V(s)\u2016/ \u221a c+ \u2016V\u0304\u2016/ \u221a d(a) + c . (19)\nProof: The optimization problem is\nmin w1,w2\n( 1 w1 \u00d7 c \u2016V(s)\u20162 + 1 w2 \u00d7 (d(a) + c) \u2016V\u0304\u20162) ,\ns.t. w1 + w2 = 1, w1 \u2265 0, w2 \u2265 0. (20)\nReplace w2 with w2 = 1\u2212 w1 in Eq. (20), take derivative of this objective function with respect to w1 and set it to zero, we have\n\u2212 1 w21 \u00d7 c \u2016V(s)\u20162 + 1 (1\u2212 w1)2 \u00d7 (d(a) + c) \u2016V\u0304\u20162) = 0. (21)\nBy solving this problem and using w2 = 1\u2212 w1, we have\nw1 = \u2016V(s)\u2016/\n\u221a c\n\u2016V(s)\u2016/ \u221a c+ \u2016V\u0304\u2016/ \u221a d(a) + c , w2 = \u2016V\u0304\u2016/\n\u221a d(a) + c\n\u2016V(s)\u2016/ \u221a c+ \u2016V\u0304\u2016/ \u221a d(a) + c . (22)\nNote that the solution in Eq. (22) satisfies the constraint w1 \u2265 0, w2 \u2265 0 automatically. Thus, it is the optimal solution and the results in Proposition 3 hold.\nWhen w1 and w2 are fixed, the optimal solution to the problem in Eq. (17) can be obtained in a close form as shown in the following proposition.\nProposition 4 When w1 and w2 are fixed, the optimal solution to the problem in Eq. (17) can be derived by solving (\u03a6(Z(s)T1+1))>\u03a6(Z(s)T1+1) + \u03b3c\u00d7w1 I (\u03a6(Z(s)T1+1))>\u03a6(Z\u0304T1+1) (\u03a6(Z\u0304T1+1)) >\u03a6(Z (s) T1+1 ) (\u03a6(Z\u0304T1+1)) >\u03a6(Z\u0304T1+1) + \u03b3 (d(a)+c)\u00d7w2 I V = D, (23)\nV =  V(s) V\u0304  , D =  (\u03a6(Z(s)T1+1))>\n(\u03a6(Z\u0304T1+1)) > YT1+1. (24) Proof: When w1 and w2 are fixed, the optimization problem is shown in Eq. (17). Take the derivative of it with respect respect to V(s) and V\u0304, set them to zeros, we have( (\u03a6(Z\n(s) T1+1 ))>\u03a6(Z (s) T1+1\n) + \u03b3\nc\u00d7 w1 I\n) V(s) + (\u03a6(Z\n(s) T1+1 ))>\u03a6(Z\u0304T1+1)V\u0304 = (\u03a6(Z (s) T1+1 ))>YT1+1,( (\u03a6(Z\u0304T1+1)) >\u03a6(Z\u0304T1+1) + \u03b3\n(d(a) + c)\u00d7 w2 I\n) V\u0304 + (\u03a6(Z\u0304T1+1)) >\u03a6(Z (s) T1+1 )V(s) = (\u03a6(Z\u0304T1+1)) >YT1+1.\n(25)\nAfter making some notations shown in Proposition 4, we can get the results in Proposition 4.\nThere is a mapping function \u03a6 in this formulation. If we do not know its concrete form, the kernel trick [SBS99] can be employed to make prediction for testing data.\nAlgorithm 1 OPID\nInput: Training data and label {X\u0303i,Yi}T1i=1, {X\u0304T1+1,YT1+1}, testing data X\u0304T1+2, parameters \u03bb, \u03c1, \u03b3. Output: The optimal w1, w2, V (s) and V\u0304. Training: 1: Update A[T ] and B[T ] using Eq. (9) in the one-pass way. 2: Compute the optimal W (s) \u2217 by solving Eq. (5). 3: Compute the new representation of X (s) T1+1 by Z (s) T1+1 = X (s) T1+1 W (s) \u2217 . 4: Initialize w1=w2=1/2 Repeat 5: Update V(s) and V\u0304 by solving the problem in Eq. (23). 6: Update w1 and w2 by Eq. (19). Until converges Testing 7: Compute the new representation of X (s) T1+2 by X (s) T1+2 W (s) \u2217 . 8: Compute the predicted label matrix of X (s) T1+2 by using the optimal w1, w2, V (s), V\u0304 and kernel trick.\nWhen we obtain the final classifier, it can be used on the testing data X\u0304T1+2 = [X (s) T1+2 ,X (a) T1+2 ] by first computing the new representations of X (s) T1+2 as (h\u0303 (s) \u2217 (X (s) T1+2 )) and then employing this classifier. If we take the updating rule in Eq. (9) and train a classifier in E-stage by optimizing Eq. (16), the procedure of OPID is listed in Algorithm 1."}, {"heading": "4. Experimental Results", "text": "There are two implementations (Eq. (3) and Eq. (16)) of our algorithm. We name the OPID method with ensemble (Eq. (3)) as OPIDe and the implementation in Eq. (16) is still named as OPID. For simplicity, in Eq. (16), we take \u03a6(x) = x and thus the classifiers are linear. For fairness, the classifiers in Eq. (3) are also linear. We implement it using the LibLinear toolbox [FCH+08] with L2-regularized logistic regression and the parameters are tuned by five-fold cross validation. The multi-class problem is tackled by one-vs-rest strategy.\nWe will compare OPID with other related methods. To show whether the classifier trained in Cstage is helpful, we also train a linear classifier with L2-regularized logistic regression on the whole training data in E-stage, i.e., {X\u0304T1+1 , [X (s) T1+1 ,X (a) T1+1 ], YT1+1}. For simplicity, this method is notated by SVM. Besides, to show the effectiveness of expanding in E-stage, we also train the same kind of linear classifiers on data with survived feature {X(s)T1+1,YT1+1} and augmented feature {X(a)T1+1,YT1+1} respectively. They are named as SVM (s) and SVM(a).\nWe have evaluated our approach on eight different kinds of data sets. They are three digit data sets: Mnist1, Gisette2 and USPS3, three DNA data sets: DNA4, Splice5 and Protein6, the Vehicle data: SensIT Vehicle7, and the image data set: Satimage8. The digit data sets are employed as toy examples and the rest data sets are all collected in a sequential way. For example, the SensIT Vehicle data are collected from a sensor networks. Due to the life variance of different kinds of sensors, it is a typical feature and instance evolution system.\nFor simplicity, we assume that (1) n1 = n2 =, \u00b7 \u00b7 \u00b7 , nT1 = nT1+1 = nT1+2. In C-stage, the number of training points in each category is equal, whereas in E-stage, we randomly split nT1+1 +nT1+2 examples into two equal parts and assign them as training and testing samples respectively. (2) In C-stage, we fix the total number of examples and vary the points number in each batch. Thus, companied with this, the number of training and testing examples also changes in E-stage. (3)\nWe select the first ( \u2211T1\ni=1 ni)/c samples in each category as the training data in C-stage. (4) We\nassign the first d(v) features as vanished features, the next d(s) features as survived features and the rest as augmented features. Without specification, in our experiments, the first quarter and the last quarter are vanished features and augmented features, except for DNA and Splice, since their original dimensionality is low. Experimental setting details are shown in Table 1.\nThere are totally two groups of experiments. In the first group, we would like to report the classification accuracy comparison on the testing data in E-stage. In the second group, we will\n1http://yann.lecun.com/exdb/mnist/ 2http://clopinet.com/isabelle/Projects/NIPS2003/#challenge 3http://yann.lecun.com/exdb/mnist/ 4http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/ 5http://www.cs.toronto.edu/ delve/data/splice/desc.html 6http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/ 7 http://www.ecs.umass.edu/\u223cmduarte/Software.html 8http://www.cs.toronto.edu/ delve/data/splice/desc.html\nTable 1: The details about experimental setting.\nData c\n\u2211T1\ni=1 ni ni d (v) d(s) d(a) nT1+1 = nT1+2\nMnist0vs5 2 3200 40, 80, 160, 320 114 228 113 40, 80, 160, 320\nMnist0vs3vs5 3 4800 60, 120, 240, 480 123 245 121 60, 120, 240, 480\nDNA 3 1200 60, 120, 240, 300 50 80 50 60, 120, 240, 300 Splice 2 2240 40, 80, 160, 320 10 40 10 40, 80, 160, 320\nSensIT Vehicle 3 48000 60, 120, 240, 480 25 50 25 60, 120, 240, 480\nGisette 2 6000 40, 100, 200, 300 1239 2478 1238 40, 100, 200, 300\nUSPS0vs5 2 960 20, 40, 60, 80 64 128 64 20, 40, 60, 80\nUSPS0vs3vs5 3 1440 30, 60, 90, 120 64 128 64 30, 60, 90, 120\nProtein 3 4500 60, 150, 300, 450 70 200 86 60, 150, 300, 450\nSatimage 3 1080 30, 60, 90, 120 10 18 8 30, 60, 90, 120\nfocus on the performance variation caused by the number of survived features."}, {"heading": "4.1. Classification Accuracy Comparison", "text": "To show the results both intuitively and qualitatively, we report results on the first six data in the form of table and the rest with figures. The results of different methods on different data sets are presented in Table 2 and Fig. 2 respectively.\nThere are several observations from these results.\n(1) When we use the classifier trained in C-stage to assist learning in E-stage, the testing performance will increase significantly, especially when the training points in E-stage are rare. This is consistent with intuition since the assistance from C-stage will be weaker with the increase of training points.\n(2) Compared with the accuracy of SVM(a), our results have a remarkable improvement. This validates that our methods could, to some extent, inherit the metric from C-stage.\n(3) It seems that the improvement of our method with respect to other approaches is much larger in multi-class scenario. The reason may be that the binary classification accuracy is high enough and it is hard to make further improvement.\n(4) Compared OPID with OPIDe, it seems that OPID performs slightly better than OPIDe in most data sets. The t-test results show that their performances tie in most cases. Nevertheless, their performances are also data dependent.\n(5) It seems that our method achieves more significant improvement on biological data sets (DNA,\nSplice, Protein) than image data sets (Mnist, Gisette and Satimage). It may be caused by the fact that the biological data is more time dependent than the image data and it is more consistent with our settings."}, {"heading": "4.2. The Influence of the Number of Survived Features", "text": "Different from traditional problems, there are three kinds of features in our settings. To illustrate the effectiveness of our methods, we vary the percentages of survived features and compare our methods with other related works. As in previous subsection, we also conduct experiments on\nSensIT Vehicle and Gisette. Similar to the way of assigning three kinds of features, we select different percentages of features in the middle as the survived feature. The rest are assigned as vanished feature and augmented feature with equal feature number. Comparison results are shown in Fig. (3).\nThere are at least two observations from Fig. (3).\n(1) Our proposed methods outperform traditional methods, no matter what the percentage of survived features is. It validates the effectiveness of our method in dealing the problem in this setting.\n(2) With the increase number of survived features, OPID and OPIDe achieve higher accuracies on SensIT Vehicle data. Nevertheless, their performances have a little turbulence on Gisette data. The reason may be that compared with SensIT Vehicle data whose total feature number is 100, the feature number of Gisette is 4955. It trends to be suffered from the curse of dimensionality [Don00]."}, {"heading": "5. Conclusion", "text": "In this paper, we study the problem of learning with incremental and decremental features, and propose the one-pass learning approach that does not need to keep the whole data for\noptimization. Our approach is particularly useful when both the data and features are evolving, while robust learning performance are needed, and is well scalable because it only needs to scan each instance once. In this paper we focus on one-shot feature change where the survived and augmented features do not vanish. It will be interesting to extend to multi-shot feature change where the survived and augmented features can vanish later."}], "references": [{"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Incremental and decremental support vector machine learning", "author": ["Gert Cauwenberghs", "Tomaso A. Poggio"], "venue": "NIPS", "citeRegEx": "Cauwenberghs and Poggio.,? \\Q2000\\E", "shortCiteRegEx": "Cauwenberghs and Poggio.", "year": 2000}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Demsar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Demsar.,? \\Q2006\\E", "shortCiteRegEx": "Demsar.", "year": 2006}, {"title": "High-dimensional data analysis: The curses and blessings of dimensionality", "author": ["David L. Donoho"], "venue": "In AMS Conference on Math Challenges of the 21st Century,", "citeRegEx": "Donoho.,? \\Q2000\\E", "shortCiteRegEx": "Donoho.", "year": 2000}, {"title": "Learning to classify with missing and corrupted features", "author": ["Ofer Dekel", "Ohad Shamir"], "venue": "ICML", "citeRegEx": "Dekel and Shamir.,? \\Q2008\\E", "shortCiteRegEx": "Dekel and Shamir.", "year": 2008}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "One-pass AUC optimization", "author": ["Wei Gao", "Rong Jin", "Shenghuo Zhu", "Zhi-Hua Zhou"], "venue": "ICML", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Nightmare at test time: Robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "ICML", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Accuracy and Stability of Numerical Algorithms", "author": ["Nicholas J. Higham"], "venue": "SIAM, Philadelphia, PA, USA,", "citeRegEx": "Higham.,? \\Q2002\\E", "shortCiteRegEx": "Higham.", "year": 2002}, {"title": "Classification with low rank and missing data", "author": ["Elad Hazan", "Roi Livni", "Yishay Mansour"], "venue": "ICML", "citeRegEx": "Hazan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2015}, {"title": "Overview of sensors and needs for environmental monitoring", "author": ["Clifford K. Ho", "Alex Robinson", "David R. Miller", "Mary J. Davis"], "venue": null, "citeRegEx": "Ho et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2005}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Advances in Kernel Methods: Support Vector Learning", "author": ["Bernhard Sch\u00f6lkopf", "Christopher J.C. Burges", "Alexander J. Smola", "editors"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "The mars\u2013a multiagent recommendation system for games on mobile phones", "author": ["Pavle Skocir", "Luka Marusic", "Marinko Marusic", "Ana Petric"], "venue": "In 6th KES International Conference,", "citeRegEx": "Skocir et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Skocir et al\\.", "year": 2012}, {"title": "Sensors, chemical sensors, electrochemical sensors, and ecs", "author": ["Joseph R. Stetter", "William R. Penrose", "Sheng Yao"], "venue": "Journal of The Electrochemical Society,", "citeRegEx": "Stetter et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Stetter et al\\.", "year": 2003}, {"title": "Convex learning with invariances", "author": ["Choon Hui Teo", "Amir Globerson", "Sam T. Roweis", "Alexander J. Smola"], "venue": "NIPS", "citeRegEx": "Teo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2007}, {"title": "Online alternating direction method", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": "ICML", "citeRegEx": "Wang and Banerjee.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Banerjee.", "year": 2012}, {"title": "One-pass multi-view learning", "author": ["Yue Zhu", "Wei Gao", "Zhi-Hua Zhou"], "venue": "ACML", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Zhi-Hua Zhou"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Zhou.", "year": 2012}, {"title": "Online bandit learning for a special class of non-convex losses", "author": ["Lijun Zhang", "Tianbao Yang", "Rong Jin", "Zhi-Hua Zhou"], "venue": "AAAI", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically.", "creator": "LaTeX with hyperref package"}}}