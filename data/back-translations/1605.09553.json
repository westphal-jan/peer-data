{"id": "1605.09553", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Attention Correctness in Neural Image Captioning", "abstract": "In this paper, we focus on assessing and improving the accuracy of attention in neural captioning models. Specifically, we propose a quantitative measure of how well attention maps match human judgment, using recently published datasets with alignment between regions in images and units in captions. We then propose novel models with different levels of explicit monitoring for learning attention maps during training. Monitoring can be strong when alignment between regions and captions is available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing monitoring of attention maps during training solidly improves both attention correctness and caption quality.", "histories": [["v1", "Tue, 31 May 2016 10:04:20 GMT  (9583kb,D)", "http://arxiv.org/abs/1605.09553v1", null], ["v2", "Wed, 23 Nov 2016 07:29:46 GMT  (2067kb,D)", "http://arxiv.org/abs/1605.09553v2", "To appear in AAAI-17. Seethis http URLfor supplementary material"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["chenxi liu", "junhua mao", "fei sha", "alan l yuille"], "accepted": true, "id": "1605.09553"}, "pdf": {"name": "1605.09553.pdf", "metadata": {"source": "CRF", "title": "Attention Correctness in Neural Image Captioning", "authors": ["Chenxi Liu", "Junhua Mao", "Fei Sha", "Alan Yuille"], "emails": ["{cxliu@,", "mjhustc@,", "feisha@cs.}ucla.edu", "alan.l.yuille@gmail.com"], "sections": [{"heading": null, "text": "Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the \u201ccorrectness\u201d of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for how well the attention maps align with human judgment, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality."}, {"heading": "1 Introduction", "text": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29]. In these tasks, the input consists of a number of vectors with the same dimension. Deep models with attention address these tasks by learning a dynamic combination of these vectors.\nIn this work we focus on attention models for image captioning. The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description. These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].\nAlthough impressive visualization results of the attention maps for image captioning are shown in [29], the authors do not provide quantitative evaluations of the attention maps generated by their models. This is a common issue for attention models, because defining and evaluating the attention maps can be hard for attention models for most tasks. However, an accurate quantitative metric is important and can provide further insight in understanding and improving attention models.\nIn this work, we propose a novel quantitative metric to evaluate the \u201ccorrectness\u201d of attention maps. We define \u201ccorrectness\u201d as the consistency between the attention maps generated by the model and those annotated by humans (i.e. the ground truth maps). We use the alignment annotation between image regions and noun phrase caption entities provided in the recently released Flickr30k Entities dataset [22] as our ground truth maps. Using this metric, we show that the attention model of [29] performs better than the uniform attention baseline, but still has big room for improvement in terms of attention consistency with humans.\nar X\niv :1\n60 5.\n09 55\n3v 1\n[ cs\n.C V\n] 3\n1 M\nay 2\nBased on this observation, we propose a simple but effective model with explicit supervision of the attention maps. The model can be used not only in situations where detailed ground truth attention maps are given (e.g. the Flickr30k Entities dataset [22]) but also when only the object categories of image regions (which is a much cheaper type of annotations compared to [22]) are available (e.g. MS COCO dataset [16]). Our experiments show that our models perform consistently and significantly better than the implicit attention counterpart in terms of both attention maps accuracy and the quality of the final generated captions in both scenarios. To the best of our knowledge, this is the first work that quantitatively measures the quality of attention in deep models and shows significant improvement by adding supervision to the attention module."}, {"heading": "2 Related Work", "text": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5]. However, it is not clear whether the captioning models truly understand and recognize the objects in the image while generating the captions. [29] proposed an attention model and qualitatively showed that the model can attend to specific regions of the image by visualizing the attention maps of a few images. We build on their work and take a step further by quantitatively measuring the quality of the attention maps, which offers insight into understanding and improving current image captioning models.\nDeep Attention Models Attention mechanism is an important property of human visual systems [23, 7]. Since deep neural networks are inspired by the structure of neurons in human brains, exploring the use of attention in these artificial models seems natural and promising.\nIn machine translation, [2] introduced an extra softmax layer in the RNN/LSTM structure that generates weights of the individual words of the sentence to be translated. This allowed the individual representations of the words to be preserved. In image captioning, [29] replaced the individual words in machine translation model by convolutional image features, allowing the model to attend to different areas of the image when generating words one by one. This model is discussed in details in section 3.1. [30] proposed to target attention on a set of concepts extracted from the image to generate image captions. In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer. But none of these models quantitatively evaluates the quality of the attention maps or imposes supervision on the attention.\nImage Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets. Each image in these datasets has 5 accompanying captions. The original annotations of these datasets do not have alignment between the image regions and the entities (e.g. noun phrases) in the captions. Plummer et al. [22] developed the original caption annotations in Flickr30k by providing the region to phrase correspondences. Specifically, they align the noun phrases in the captions to image regions using human annotators. In this work, we\nuse this dataset for constructing ground truth attention maps to evaluate the quality of the generated attention maps, as well as to train our strongly supervised attention model."}, {"heading": "3 Deep Attention Models for Image Captioning", "text": "In this section, we first introduce Xu et al. [29]\u2019s attention model that learns the attention weights implicitly and then introduce our explicit supervised attention model."}, {"heading": "3.1 Implicit Attention Model", "text": "Xu et al. [29] was the first attempt to introduce attention models to image captioning. The model consists of three parts: the encoder which encodes the visual information (i.e. a visual feature extractor), the decoder which decodes the information into words, and the attention module which performs spatial attention.\nThe visual feature extractor produces L vectors that correspond to different spatial locations of the image: a = {a1, . . . ,aL}, ai \u2208 RD. Given the visual features, the goal of the decoder is to generate a caption y of length C: y = {y1, . . . , yC}. We use yt \u2208 RK to represent the one-hot encoding of yt, where K is the dictionary size.\nIn [29], an LSTM network [11] is used as the decoder:\nit = \u03c3(WiEyt\u22121 + Uiht\u22121 + Zizt + bi) (1) ft = \u03c3(WfEyt\u22121 + Ufht\u22121 + Zfzt + bf ) (2) ct = ftct\u22121 + ittanh(WcEyt\u22121 + Ucht\u22121 + Zczt + bc) (3) ot = \u03c3(WoEyt\u22121 + Uoht\u22121 + Zozt + bo) (4) ht = ottanh(ct) (5)\nwhere it, ft, ct,ot,ht are input gate, forget gate, memory, output gate, and hidden state of the LSTM respectively. W,U,Z,b are weight matrices and biases. E \u2208 Rm\u00d7K is an embedding matrix, and \u03c3 is the sigmoid function. The context vector zt \u2208 RD is a dynamic vector that represents the relevant part of image feature at time step t. In Xu et al. [29]\u2019s deterministic \u201csoft\u201d attention model,\nzt = L\u2211 i=1 \u03b1tiai (6)\nwhere \u03b1ti is a scalar weighting of visual vector ai at time step t, defined as follows:\neti = fattn(ai,ht\u22121) \u03b1ti = exp(eti)\u2211L\nk=1 exp(etk) (7)\nwhere fattn(ai,ht\u22121) is a function that determines the amount of attention allocated to image feature ai, conditioned on the LSTM hidden state ht\u22121. In [29], this function is implemented as a multilayer perceptron. Note that by construction \u2211L i=1 \u03b1ti = 1.\nThe output word probability is determined by the image zt, the previous word yt\u22121, and the hidden state ht: p(yt|a, yt\u22121) \u221d exp(Go(Eyt\u22121 +Ghht +Gzzt)) (8) where G are learned parameters. The loss function, ignoring the regularization terms, is the negative log probability of the ground truth words w = {w1, . . . , wC}:\nLt,cap = \u2212 log p(wt|a, yt\u22121) (9)"}, {"heading": "3.2 Supervised Attention Model", "text": "Deep network attention can be viewed as a form of alignment from language space to image space. However, it is not clear how good this alignment is. Moreover, even if the ground truth of this alignment is provided in a dataset, the model in [29] will not be able to take advantage of this information to learn better attention function fattn(ai,ht\u22121). In this work, we seek to enforce attention correctness by introducing explicit supervision.\nConcretely, we first consider the case when the ground truth attention map \u03b2 t = {\u03b2ti}i=1,...,L is provided for ground truth word wt, with \u2211L i=1 \u03b2ti = 1. Since \u2211L i=1 \u03b2ti = \u2211L i=1 \u03b1ti = 1, they can be considered as two probability distributions of attention and it is natural to introduce the cross entropy loss on the attention map. For the words that do not have an alignment with an image region (e.g. \u201ca\u201d, \u201cis\u201d), we simply set Lt,attn as 0:\nLt,attn =\n{ \u2212 \u2211L\ni=1 \u03b2ti log\u03b1ti if wt aligns with a ground truth region 0 otherwise\n(10)\nThe total loss is the weighted sum of the two loss terms:\nL = C\u2211 t=1 Lt,cap + \u03bb C\u2211 t=1 Lt,attn (11)\nWe then discuss two ways of constructing the ground truth attention map \u03b2 t, depending on the types of annotations available."}, {"heading": "3.2.1 Strong Supervision with Alignment Annotation", "text": "In the simplest case, we have direct annotation that links the ground truth word wt to a region Rt (in the form of bounding boxes or segment masks) in the image. We encourage the model to \u201cattend to\u201d Rt by first constructing \u03b2\u0302 t = {\u03b2\u0302t\u0302i}i\u0302=1,..,L\u0302 by:\n\u03b2\u0302t\u0302i = { 1 i\u0302 \u2208 Rt 0 otherwise\n(12)\nNote that the resolution of the region R (e.g. 224\u00d7 224) and the attention map \u03b1,\u03b2 (e.g. 14\u00d7 14) might be different, so L\u0302 could be different from L. Therefore we need to resize \u03b2\u0302 t to the same resolution as \u03b1t and normalize it to get \u03b2 t."}, {"heading": "3.2.2 Weak Supervision with Object Category Annotation", "text": "Ground truth alignment is expensive to collect and annotate. A much more general and cheaper annotation is to use bounding boxes or segments with object category. In this case, we are provided with a set of regions Rj in the image with associated object classes cj , j = 1, . . . ,M where M is the number of object bounding boxes or segments in the image. Although not ideal, these annotations contain important information to guide the attention of the model. For instance, for a caption of \u201ca boy is playing with a dog\u201d, the model should attend to the region of a person when generating the word \u201cboy\u201d, and attend to the region of a dog when generating the word \u201cdog\u201d. We can use this information to automatically find semantically related words in the sentences and regions with the object category labels in the image.\nFollowing this intuition, we set the likelihood that a word wt and a region Rj are aligned by the similarity of wt and cj in the word embedding space:\n\u03b2\u0302t\u0302i = { sim(E\u0303(wt), E\u0303(cj)) i\u0302 \u2208 Rj 0 otherwise\n(13)\nwhere E\u0303(wt) and E\u0303(cj) denotes the embeddings of the word wt and cj respectively. E\u0303 can be the embedding E learned by the model or any off-the-shelf word embedding. We then resize and normalize \u03b2\u0302 t in the same way as the strong supervision scenario."}, {"heading": "4 Attention Correctness: Evaluation Metric", "text": "At each time step in the implicit attention model, the LSTM not only predicts the next word yt but also generates an attention map \u03b1t \u2208 RL across all locations. However, the attention module is merely an intermediate step, while the error is only backpropagated from the word-likelihood loss in Equation 9. This opens the question whether this implicitly-learned attention module is indeed effective.\nTherefore in this section we introduce the concept of attention correctness, an evaluation metric that quantitatively analyzes the quality of the attention maps generated by the attention-based model.\n4.1 Definition\nFor a word yt with generated attention map \u03b1t, let Rt be the ground truth attention region, then we define the word attention correctness by\nAC(yt) = \u2211 i\u0302\u2208Rt \u03b1\u0302t\u0302i (14)\nwhich is a score between 0 and 1. Intuitively, this value captures the sum of the attention score that falls within human annotation (see Figure 2 for illustration). \u03b1\u0302t = {\u03b1\u0302t\u0302i}i\u0302=1,...,L\u0302 is the resized and normalized \u03b1t in order to ensure size consistency.\nIn some cases a phrase {yt, . . . , yt+l} refers to the same entity, therefore the individual words share the same attention region Rt. We define the phrase attention correctness as the maximum of the individual scores1.\nAC({yt, . . . , yt+l}) = max(AC(yt), . . . , AC(yt+l)) (15) The intuition is that the phrase may contain some words whose attention map is ambiguous, and the attention maps of these words can be ignored\nby the max operation. For example, when evaluating the phrase \u201ca group of people\u201d, we are more interested in the attention correctness for \u201cpeople\u201d rather than \u201cof\u201d.\nWe discuss next how to find ground truth attention regions during testing, in order to apply this evaluation metric."}, {"heading": "4.2 Ground Truth Attention Region During Testing", "text": "In order to compute attention correctness, we need the correspondence between regions in the image and phrases in the caption. However, in the testing stage, the generated caption is often different from ground truth captions. This makes evaluation difficult, because we only have Rt for the phrases in the ground truth caption, but not any possible phrase. To this end, we propose two strategies.\nGround Truth Caption One option is to enforce the trained model to output the ground truth sentence by resetting yt at each time step. This procedure to some extent allows us to \u201cdecorrelate\u201d the attention module from the captioning component, and diagnose if the learned attention module is meaningful. Since the generated caption exactly matches the ground truth, we can compute attention correctness for each noun phrase in the test set.\nGenerated Caption Another option is to align the entities in the generated caption to those in the ground truth caption. For each image, we first extract the noun phrases of the generated caption using POS tagger (e.g. Stanford Parser [17]), and see if there exists a word-by-word match in the set of noun phrases in the ground truth captions. For example, if the generated caption is \u201cA dog jumping over a hurdle\u201d and one of the ground truth captions is \u201cA cat jumping over a hurdle\u201d, we match the noun phrase \u201ca hurdle\u201d appearing in both sentences."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Implementation Details", "text": "Implicit/Supervised Attention Models All implementation details strictly follow [29]. We resize the image such that the shorter side has 256 pixels, and then center crop the 224\u00d7 224 image. We then extract the conv5_4 feature of the 19 layer version of VGG net [24] pretrained on ImageNet [8]. The model is trained using stochastic gradient descent with the Adam algorithm [14]. Dropout [25] is used as regularization. We use the hyperparameters provided in the publicly available code2. Specifically, we set the number of LSTM units to 1300 for Flickr30k and 1800 for COCO.\nGround Truth Attention for Strong Supervision Model We experiment with our model in section 3.2.1 on Flickr30k dataset [31]. We use the Flickr30k Entities dataset [22] for generating ground\n1In the experiments, we found that changing the definition from maximum to average does not affect our main conclusion.\n2https://github.com/kelvinxu/arctic-captions\ntruth attention maps. For each entity (noun phrase) in the caption, the Flickr30k Entities dataset provides the corresponding bounding box of the entity in the image. Therefore ideally, the model should \u201cattend to\u201d the marked region when predicting the associated words. We evaluate on noun phrases only, because for other types of words (e.g. determiner, preposition) the attention might be ambiguous and meaningless.\nGround Truth Attention for Weak Supervision Model The MS COCO dataset [16] contains instance segmentation masks of 80 classes in addition to the captions, which makes it suitable for our model with weak supervision in section 3.2.2. We only construct \u03b2 t for the nouns in the captions, which we extract using the Stanford Parser [17]. The similarity function is chosen to be the cosine distance between word vectors [19] pretrained on GoogleNews3, and we set an empirical threshold of 1/3.\nThe \u03b2 t generated in this way still contains obvious errors, primarily because word2vec cannot distinguish well between objects and scenes. For example, the similarity between the word \u201ckitchen\u201d and the object class \u201cspoon\u201d is above threshold. But when generating a scene word like \u201ckitchen\u201d, the model should be attending to the whole image instead of focusing on a small object like \u201cspoon\u201d.\nTo address this problem, we refer to the supplement of [16], which provides a scene category list containing key words of scenes used when collecting the dataset. Whenever some word in this scene category list appears in the caption, we set \u03b2 t to be uniform, i.e. equal attention across image. This greatly improves the quality of \u03b2 t (see illustration in Figure 3)."}, {"heading": "5.2 Evaluation of Attention Correctness", "text": "In this subsection, we quantitatively evaluate the attention correctness of both the implicit and the supervised attention model. All experiments are conducted on the 1000 test images of Flickr30k. We compare the result with uniform baseline, which attends equally across the whole image. Therefore the baseline score is simply the percentage of the bounding box size over the size of the whole image. The results are summarized in Table 1.\nGround Truth Caption Result In this setting, both implicit and supervised models are forced to produce exactly the same captions, resulting in 14566 noun phrase matches. We discard those with no attention region or full image attention (as the match score will be 1 regardless of the attention map). For each of the remaining matches, we resize the original attention map from 14\u00d7 14 to 224\u00d7 224 and perform normalization before we compute the attention correctness for this noun phrase.\n3https://code.google.com/archive/p/word2vec/\nBoth models are evaluated in Figure 4a. The horizontal axis is the improvement over baseline, therefore a better attention module should result in a distribution further to the right. On average, both models perform better than the baseline. Specifically, the average gain over uniform attention baseline is 6.22% for the implicit attention model [29], and 11.14% for the supervised version. Visually, the distribution of the supervised model is further to the right towards the oracle (where attention correctness is 1 for every match). This indicates that although the implicit model has captured some aspects of attention, the model learned with strong supervision has a better attention module.\nIn Figure 5 we show some examples where the supervised model successfully recovered the spatial location of the underlined entity, while the implicit model attends to the wrong region.\nGenerated Caption Result In this experiment, our algorithm is able to align 883 noun phrases for the implicit model and 901 for the supervised version. Since the word-by-word match strategy is rather conservative, these alignments are correct and reliable, as verified by manual check. Similarly, we discard those with no attention region or full image attention, and perform resize and normalization before we compute the correctness score.\nThe results are shown in Figure 4b. In general the conclusion is the same: the supervised attention model produces attention maps that are more consistent with human judgment. In terms of numbers, the average improvement over the uniform baseline is 12.07% for the implicit model and 18.19% for the supervised model, which is a 50% relative gain.\nIn Figure 6 we provide some qualitative results. These examples show that for the same entity, the supervised model produces more human-like attention than the implicit model."}, {"heading": "5.3 Evaluation of Captioning Performance", "text": "In the previous subsection we showed that supervised attention models achieve higher attention correctness than implicit attention models. Although this is meaningful in tasks such as region grounding, in many tasks attention only serves as an intermediate step. Our intuition is that a\nmeaningful dynamic weighting of the input vectors will allow later components to decode information more easily. In this subsection we give experimental support by showing that the supervised attention model also provides better captioning performance.\nWe report BLEU [21] and METEOR [3] scores to allow comparison with [29]. In Table 2 we show both the scores reported in [29] and our implementation. Note that our implementation of [29] gives slightly improved result over what they reported. We observe that BLEU and METEOR scores consistently increase after we introduce supervised attention for both Flickr30k and COCO. Specifically in terms of BLEU-4, we observe a significant increase of 0.9 and 0.7 percent respectively."}, {"heading": "6 Discussion", "text": "In this work we make a first attempt to give a quantitative answer to the question: to what extent are attention maps consistent with human perceptions? We first define attention correctness at both the word level and phrase level. In the context of image captioning, we evaluated the state-of-the-art models with implicitly trained attention modules. The quantitative evaluation results suggest that although the implicit models outperform the baseline, they still have big room for improvement.\nWe then show that by introducing supervision of attention map, we can improve both the image captioning performance and attention map quality. Even when attention ground truth is unavailable, we are still able to utilize the segmentation masks with object category as a weak supervision to the attention maps, and significantly boost captioning performance.\nWe believe closing the gap between machine attention and human perception is necessary, and expect to see similar efforts in other tasks, such as visual question answering."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, volume 29, pages 65\u201372", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.-C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Control of goal-directed and stimulus-driven attention in the brain", "author": ["M. Corbetta", "G.L. Shulman"], "venue": "Nature reviews neuroscience, 3(3):201\u2013215", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2625\u20132634", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "L. Bourdev", "R. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "C.L. Zitnick", "P. Doll\u00e1r"], "venue": "arXiv preprint arXiv:1405.0312", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations), pages 55\u201360", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "Recurrent models of visual attention. In Advances in Neural Information Processing Systems, pages 2204\u20132212", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2641\u20132649", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The dynamic representation of scenes", "author": ["R.A. Rensink"], "venue": "Visual cognition, 7(1-3):17\u201342", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask", "author": ["H. Xu", "K. Saenko"], "venue": "attend and answer: Exploring question-guided spatial attention for visual question answering. arXiv preprint arXiv:1511.05234", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "arXiv preprint arXiv:1603.03925", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.03416", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 104, "endOffset": 107}, {"referenceID": 19, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 126, "endOffset": 133}, {"referenceID": 0, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 126, "endOffset": 133}, {"referenceID": 27, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 161, "endOffset": 168}, {"referenceID": 3, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 161, "endOffset": 168}, {"referenceID": 28, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 191, "endOffset": 195}, {"referenceID": 14, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 17, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 12, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 8, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 26, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 25, "context": "These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].", "startOffset": 86, "endOffset": 89}, {"referenceID": 28, "context": "These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "Although impressive visualization results of the attention maps for image captioning are shown in [29], the authors do not provide quantitative evaluations of the attention maps generated by their models.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "We use the alignment annotation between image regions and noun phrase caption entities provided in the recently released Flickr30k Entities dataset [22] as our ground truth maps.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Using this metric, we show that the attention model of [29] performs better than the uniform attention baseline, but still has big room for improvement in terms of attention consistency with humans.", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "We propose a quantitative evaluation metric for the quality of attention maps and find there is room for improvement of the implicitly learned attention maps of [29].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "the Flickr30k Entities dataset [22]) but also when only the object categories of image regions (which is a much cheaper type of annotations compared to [22]) are available (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "the Flickr30k Entities dataset [22]) but also when only the object categories of image regions (which is a much cheaper type of annotations compared to [22]) are available (e.", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "MS COCO dataset [16]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 28, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 17, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 26, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 8, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 9, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 12, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 4, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 28, "context": "[29] proposed an attention model and qualitatively showed that the model can attend to specific regions of the image by visualizing the attention maps of a few images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Deep Attention Models Attention mechanism is an important property of human visual systems [23, 7].", "startOffset": 91, "endOffset": 98}, {"referenceID": 6, "context": "Deep Attention Models Attention mechanism is an important property of human visual systems [23, 7].", "startOffset": 91, "endOffset": 98}, {"referenceID": 1, "context": "In machine translation, [2] introduced an extra softmax layer in the RNN/LSTM structure that generates weights of the individual words of the sentence to be translated.", "startOffset": 24, "endOffset": 27}, {"referenceID": 28, "context": "In image captioning, [29] replaced the individual words in machine translation model by convolutional image features, allowing the model to attend to different areas of the image when generating words one by one.", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "[30] proposed to target attention on a set of concepts extracted from the image to generate image captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer.", "startOffset": 30, "endOffset": 41}, {"referenceID": 27, "context": "In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer.", "startOffset": 30, "endOffset": 41}, {"referenceID": 31, "context": "In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer.", "startOffset": 30, "endOffset": 41}, {"referenceID": 11, "context": "Image Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets.", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "Image Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Image Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "[22] developed the original caption annotations in Flickr30k by providing the region to phrase correspondences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29]\u2019s attention model that learns the attention weights implicitly and then introduce our explicit supervised attention model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] was the first attempt to introduce attention models to image captioning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], an LSTM network [11] is used as the decoder:", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [29], an LSTM network [11] is used as the decoder:", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "[29]\u2019s deterministic \u201csoft\u201d attention model,", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], this function is implemented as a multilayer perceptron.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Moreover, even if the ground truth of this alignment is provided in a dataset, the model in [29] will not be able to take advantage of this information to learn better attention function fattn(ai,ht\u22121).", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Stanford Parser [17]), and see if there exists a word-by-word match in the set of noun phrases in the ground truth captions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Implicit/Supervised Attention Models All implementation details strictly follow [29].", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "We then extract the conv5_4 feature of the 19 layer version of VGG net [24] pretrained on ImageNet [8].", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "We then extract the conv5_4 feature of the 19 layer version of VGG net [24] pretrained on ImageNet [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 13, "context": "The model is trained using stochastic gradient descent with the Adam algorithm [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Dropout [25] is used as regularization.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "1 on Flickr30k dataset [31].", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "We use the Flickr30k Entities dataset [22] for generating ground", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Ground Truth Attention for Weak Supervision Model The MS COCO dataset [16] contains instance segmentation masks of 80 classes in addition to the captions, which makes it suitable for our model with weak supervision in section 3.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "We only construct \u03b2 t for the nouns in the captions, which we extract using the Stanford Parser [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "The similarity function is chosen to be the cosine distance between word vectors [19] pretrained on GoogleNews3, and we set an empirical threshold of 1/3.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "To address this problem, we refer to the supplement of [16], which provides a scene category list containing key words of scenes used when collecting the dataset.", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "22% for the implicit attention model [29], and 11.", "startOffset": 37, "endOffset": 41}, {"referenceID": 28, "context": "Flickr30k Implicit [29] 28.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "49 Implicit [29]* 29.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "COCO Implicit [29] 34.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "90 Implicit [29]* 36.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "We report BLEU [21] and METEOR [3] scores to allow comparison with [29].", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "We report BLEU [21] and METEOR [3] scores to allow comparison with [29].", "startOffset": 31, "endOffset": 34}, {"referenceID": 28, "context": "We report BLEU [21] and METEOR [3] scores to allow comparison with [29].", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "In Table 2 we show both the scores reported in [29] and our implementation.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Note that our implementation of [29] gives slightly improved result over what they reported.", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the \u201ccorrectness\u201d of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for how well the attention maps align with human judgment, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality.", "creator": "LaTeX with hyperref package"}}}