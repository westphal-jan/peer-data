{"id": "1606.00061", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "abstract": "In this paper, we argue that in addition to modelling \"where to look\" or visual attention, it is equally important \"which words to hear\" or to question attention. We present a novel co-attention model for VQA that reflects together on image and attention. Furthermore, our model establishes the question, and hence the image, about the co-attention mechanism in a hierarchical way via a novel 1-dimensional Convolution Neural Networks (CNN) model. Our final model surpasses all reported methods and improves the state of the art in the VQA dataset from 60.4% to 62.1% and from 61.6% to 65.4% in the COCO QA dataset.", "histories": [["v1", "Tue, 31 May 2016 22:02:01 GMT  (4284kb,D)", "http://arxiv.org/abs/1606.00061v1", "11 pages, 7 figures, 3 tables"], ["v2", "Thu, 2 Jun 2016 01:51:13 GMT  (3549kb,D)", "http://arxiv.org/abs/1606.00061v2", "11 pages, 7 figures, 3 tables"], ["v3", "Wed, 26 Oct 2016 02:15:57 GMT  (3669kb,D)", "http://arxiv.org/abs/1606.00061v3", "11 pages, 7 figures, 3 tables"], ["v4", "Fri, 13 Jan 2017 16:18:03 GMT  (3669kb,D)", "http://arxiv.org/abs/1606.00061v4", "11 pages, 7 figures, 3 tables in 2016 Conference on Neural Information Processing Systems (NIPS)"], ["v5", "Thu, 19 Jan 2017 05:03:33 GMT  (3669kb,D)", "http://arxiv.org/abs/1606.00061v5", "11 pages, 7 figures, 3 tables in 2016 Conference on Neural Information Processing Systems (NIPS)"]], "COMMENTS": "11 pages, 7 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["jiasen lu", "jianwei yang", "dhruv batra", "devi parikh"], "accepted": true, "id": "1606.00061"}, "pdf": {"name": "1606.00061.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Co-Attention for Visual Question Answering", "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "emails": ["parikh}@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry. To correctly answer visual questions about an image, the machine needs to understand both the image and question. Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.\nSo far, all attention models for VQA in literature have focused on the problem of identifying \u201cwhere to look\u201d or visual attention. In this paper, we argue that the problem of identifying \u201cwhich words to listen to\u201d or question attention is equally important. Consider the questions \u201chow many horses are in this image?\u201d and \u201chow many horses can you see in this image?\". They have the same meaning, essentially captured by the first three words. A machine that attends to the first three words would arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question. Motivated by this observation, in addition to reasoning about visual attention, we also address the problem of question attention. Specifically, we present a novel multi-modal attention model for VQA with the following two unique features:\nCo-Attention: We propose a novel mechanism that jointly reasons about visual attention and question attention, which we refer to as co-attention. Unlike previous works, which only focus on visual attention, our model has a natural symmetry between the image and question, in the sense that the image representation is used to guide the question attention and the question representation(s) are used to guide image attention.\nQuestion Hierarchy: We build a hierarchical architecture that co-attends to the image and question at three levels: (a) word level, (b) phrase level and (c) question level. At the word level, we embed the words to a vector space through an embedding matrix. At the phrase level, 1-dimensional convolution neural networks (CNN) are used to capture the information contained in unigrams, bigrams and trigrams. Specifically, we convolve word representations with temporal filters of varying support, and then combine the various n-gram responses by pooling them into a single phrase level representation.\n1The source code can be downloaded from https://github.com/jiasenlu/HieCoAttenVQA\nar X\niv :1\n60 6.\n00 06\n1v 1\n[ cs\n.C V\n] 3\n1 M\nay 2\n01 6\nAt the question level, we use recurrent neural networks (RNN) to encode the entire question. For each level of the question representation in this hierarchy, we construct joint question and image co-attention maps, which are then combined recursively to ultimately predict a distribution over the answers.\nOverall, the main contributions of our work are:\n\u2022 We propose a novel co-attention mechanism for VQA that jointly performs question-guided visual attention and image-guided question attention. We explore this mechanism with two strategies, parallel and alternating co-attention, which are described in Sec. 3.3.1 and Sec. 3.3.2;\n\u2022 We propose a hierarchical architecture to represent the question, and consequently construct image-question co-attention maps at 3 different levels: word level, phrase level and question level. These co-attended features are then recursively combined from word level to question level for the final answer prediction;\n\u2022 At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation;\n\u2022 Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [14]. Our model sets a new state of art on both VQA and COCO-QA outperforming the previous best method on this challenging problem by 2% and 4% respectively. We also perform ablation studies to quantify the roles of different components in our proposed model."}, {"heading": "2 Related Work", "text": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA. We compare and relate our proposed co-attention mechanism to other vision and language attention mechanisms in literature.\nImage attention. Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA. Zhu et al. [25] add spatial attention to the standard LSTM model for pointing and grounded QA. Andreas et al. [1] propose a compositional scheme that consists of a language parser and a number of neural modules networks. The language parser predicts which neural module network should be instantiated to answer the question. One of these neural modules networks involves attending to specific regions in an image. Some other works perform image attention multiple times in a stacked manner. In [23], the authors propose a stacked attention network, which runs multiple iterations or hops to infer the answer progressively. To capture fine-grained\ninformation from the question, Xu et al. [22] propose a multi-hop image attention scheme. It aligns words to image patches in the first hop, and then refers to the entire question for obtaining image attention maps in the second hop. In [16], the authors generate image regions with object proposals and then select the regions relevant to the question and answer choice. Xiong et al. [21] augments dynamic memory network by introducing a new input fusion module. This module captures the spatial information from the neighboring image patches, and retrieves an answer from an attention GRU. Note that all of these approaches model visual attention alone, and do not model question attention.\nLanguage Attention. Though no prior work has explored question attention in VQA, there are some related works in natural language processing (NLP) in general that have modeled language attention. In order to overcome difficulty in translation of long sentences, Hermann et al. [3] propose RNNSearch to learn an alignment over the input sentences. In [7], the authors propose an attention model to circumvent the bottleneck caused by fixed width hidden vector in text reading and comprehension. The model first encodes the document and the query using separate bidirectional single layer LSTMs, and then use the outputs as cues for attention. A more fine-grained attention mechanism is proposed in [15]. The authors employ a word-by-word neural attention mechanism to reason about the entailment in two sentences. Also focused on modeling sentence pairs, the authors in [24] propose an attention-based bigram CNN for jointly performing attention between two CNN hierarchies. In their work, three attention schemes are proposed and evaluated."}, {"heading": "3 Method", "text": "We begin by introducing the notation used in this paper. To ease understanding, our full model is described in parts. First, our hierarchical question representation is described in Sec. 3.2 and the proposed co-attention mechanism is then described in Sec. 3.3. Finally, Sec. 3.4 shows how to recursively combine the attended question and image features to output answers."}, {"heading": "3.1 Notation", "text": "Given a question with T words, its representation is denoted by Q = {q1, . . . qT }, where qt is the feature vector for the t-th word. We denote qwt , q p t and q s t as word embedding, phrase embedding and question embedding at position t, respectively. The image feature is denoted by V = {v1, ...,vN}, where vn is the feature vector at the spatial location n. The co-attention features of image and question at each level in the hierarchy are denoted as v\u0302r and q\u0302r where r \u2208 {w, p, s}. The weights in different modules/layers are denoted with W , with appropriate sub/super-scripts as necessary. In the exposition that follows, we omit the bias term b to avoid notational clutter, but they are included in the model."}, {"heading": "3.2 Question Hierarchy", "text": "Given the 1-hot encoding of the question words Q = {q1, . . . , qT }, we first embed the words to a vector space (learnt end-to-end) to get Qw = {qw1 , . . . , qwT }. To compute the phrase features, we apply 1-D convolution on the word embedding vectors. Concretely, at each word location, we compute the inner product of the word vectors with filters of three window sizes: unigram, bigram and trigram. For the t-th word, the convolution output with window size s is given by\nq\u0302ps,t = tanh(W s c q w t:t+s\u22121), s \u2208 {1, 2, 3} (1)\nwhere W sc is the weight parameters. The word-level features Q w are appropriately 0-padded before feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution. Given the convolution result, we then apply max-pooling across different n-grams at each word location to obtain phrase-level features\nqpt = max(q\u0302 p 1,t, q\u0302 p 2,t, q\u0302 p 3,t), t \u2208 {1, 2, . . . , T} (2)\nOur pooling method differs from those used in previous works [8] in that it adaptively selects different gram features at each time step, while preserving the original sequence length and order. We use a LSTM to encode the sequence qpt after max-pooling. The corresponding question-level feature q s t is the LSTM hidden vector at time t.\nOur hierarchical representation of the question is depicted in Fig. 3(a)."}, {"heading": "3.3 Co-Attention", "text": "We propose two co-attention mechanisms that differ in the order in which image and question attention maps are generated. The first mechanism, which we call parallel co-attention, generates image and question attention simultaneously. The second mechanism, which we call alternating co-attention, sequentially alternates between generating image and question attentions. See Fig. 2. These co-attention mechanisms are executed at all three levels of the question hierarchy."}, {"heading": "3.3.1 Parallel Co-Attention", "text": "Parallel co-attention attends to the image and question simultaneously. Similar to [22], we connect the image and question by calculating the similarity between image and question features at all pairs of image-locations and question-locations. Specifically, given an image feature map V \u2208 Rd\u00d7N , and the question representation Q \u2208 Rd\u00d7T , the affinity matrix C \u2208 RT\u00d7N is calculated by\nC = tanh(QTWbV ) (3)\nwhere Wb \u2208 Rd\u00d7d contains the weights. After computing this affinity matrix, one possible way of computing the image (or question) attention is to simply maximize out the affinity over the locations of other modality, i.e. av[n] = maxi(Ci,n) and aq[t] = maxj(Ct,j). Instead of choosing the max activation, we find that performance is improved if we consider this affinity matrix as a feature and learn to predict image and question attention maps via the following\nHv = tanh(WvV +C(WqQ)), H q = tanh(WqQ+C T (WvV ))\nav = softmax(wThvH v), aq = softmax(wThqH\nq) (4)\nwhere Wv,Wq \u2208 Rk\u00d7d, whv,whq \u2208 Rk are the weight parameters. av \u2208 RN and aq \u2208 RT are the attention probabilities of each image region vn and word qt respectively. Based on the above attention weights, the image and question attention vectors are calculated as the weighted sum of the image features and question features, i.e.,\nv\u0302 = N\u2211 n=1 avnvn, q\u0302 = T\u2211 t=1 aqtqt (5)\nThe parallel co-attention is done at each level in the hierarchy, leading to v\u0302r and q\u0302r where r \u2208 {w, p, s}."}, {"heading": "3.3.2 Alternating Co-Attention", "text": "In this attention mechanism, we sequentially alternate between generating image and question attention. Briefly, this consists of three steps (marked in Fig. 2b): 1) summarize the question into a single vector q; 2) attend to the image based on the question summary q; 3) attend to the question based on the attended image feature.\nConcretely, we define an attention operation x\u0302 = A(X; g), which takes the image (or question) features X and attention guidance g derived from question (or image) as inputs, and outputs the attended image (or question) vector. The operation can be expressed in the following steps\nH = tanh(WxX + (Wgg)1 T )\nax = softmax(wThxH) x\u0302 = \u2211\naxi xi\n(6)\nwhere 1 \u2208 Rk is a vector with all elements 1. Wx,Wg \u2208 Rk\u00d7d and whx \u2208 Rk are parameters. ax is the attention weight of feature X .\nThe alternating co-attention process is illustrated in Fig. 2 (b). At the first step of alternating coattention, X = Q, and g is 0; At the second step, X = V where V is the image features, and the guidance g is intermediate attended question feature s\u0302 from the first step; Finally, we use the attended image feature v\u0302 as the guidance to attend the question again, i.e., X = Q and g = v\u0302. Similar to the parallel co-attention, the alternating co-attention is also done at each level of the hierarchy."}, {"heading": "3.4 Encoding for Predicting Answers", "text": "Following [2], we take the top-1000 frequent answers and treat VQA as 1000-way classification problem. We predict the answer based on the co-attended image and question features from all three levels. We use a multi-layer perceptron (MLP) to recursively encode the attention features as shown in Fig. 3(b).\nhw = tanh(Ww(q\u0302 w + v\u0302w)) hp = tanh(Wp[(q\u0302 p + v\u0302p),hw])\nhs = tanh(Ws[(q\u0302 s + v\u0302s),hp])\np = softmax(Whhs)\n(7)\nwhere Ww,Wp,Ws and Wh are the weight parameters. [\u00b7] is the concatenation operation on two vectors. p is the probability of the final answer."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Datasets", "text": "We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset [14].\nVQA dataset is the largest dataset for this problem, containing human annotated questions and answers on Microsoft COCO dataset [11]. The dataset contains 248,349 training questions, 121,512 validation questions and 244,302 testing questions. There are three sub-categories according to answer-types including yes/no, number, and other. Each question has 10 free-response answers. We\nuse the top 1000 most frequent answers as the possible outputs similar to [2]. This set of answers covers 86.54% of the train+val answers. For testing, we train our model on VQA train+val and report the test-dev and test-standard results from the VQA evaluation server. We use the evaluation protocol of [2] in the experiment.\nCOCO-QA dataset is automatically generated from captions in the Microsoft COCO dataset [11]. There are 78,736 train questions and 38,948 test questions in the dataset. These questions are based on 8,000 and 4,000 images respectively. There are four types of questions including object, number, color, and location. Each type takes 70%, 7%, 17%, and 6% of the whole dataset, respectively. All answers in this data set are single word. We report the results both on classification accuracy and Wu-Palmer similarity (WUPS) measure [20] in Table 2."}, {"heading": "4.2 Setup", "text": "We use torch [4] to develop our model. We use the Rmsprop optimizer [19] with a base learning rate of 4e-4, momentum 0.99 and weight-decay 1e-8. We set batch size to be 300 and train for up to 256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. For COCO-QA, the size of hidden layer Ws is set to 512 and 1024 for VQA since it is a much larger dataset. All the other word embedding and hidden layers were vectors of size d = 512. We apply dropout with probability 0.5 on each layer. Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature. Our model is end-to-end trainable, but we do not finetune the CNN. The code is available at https://github.com/jiasenlu/HieCoAttenVQA."}, {"heading": "4.3 Results and analysis", "text": "There are two test scenarios on VQA: open-ended and multiple-choice. The best performing method deeper LSTM Q + norm I from [2] is used as our baseline. For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21]. For multiple choice, we compare with Region Sel. [16] and FDN [9]. We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA. We use Oursp to refer to our parallel co-attention and Oursa for alternating co-attention.\nTable 1 shows results on the VQA test sets for both open-ended and multiple-choice settings. We can see that our approach outperforms all previous results, improving the state of art from 60.4% (DMN+ [21]) to 62.1% (Oursa+Residual) on open-ended and from 64.2% (FDN [9]) to 66.1% (Oursa+Residual) on multiple-choice. Notably, for the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions, and 4.0% and 1.1% on multiple-choice questions. As we can see, deep residual features outperform or match VGG features in all cases. Note that our improvements are not solely due to the use of a better CNN. Specifically, FDN [9] also uses ResidualNet [6], but Oursa+Residual outperforms it by 1.8% on test-dev. SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.2% on test-dev (DMN+ [21]).\nTable 2 shows results on the COCO-QA test set. Similar to the result on VQA, our model improves the state-of-the-art from 61.6% (SAN(2,CNN) [23]) to 65.4% (Oursa+Residual). We observe that parallel co-attention performs better than alternating co-attention in this setup. Due to significant computational requirements, we could not train Oursp+Residual on VQA and COCO-QA. This is left as future work."}, {"heading": "4.4 Ablation study", "text": "In this section, we perform ablation studies to quantify the role of each component in our model. Specifically, we re-train our approach by ablating certain components:\n\u2022 Image Attention alone, where in a manner similar to previous works [23], we do not use any question attention. The goal of this comparison is to verify that our improvements are not the result of orthogonal contributions (say better optimization or better CNN features).\n\u2022 W/O Conv, where no convolution and pooling is performed to represent phrases. Instead, we stack another word embedding layer on the top of word level outputs. The goal of this\nTable 3 shows the comparison of our full approach w.r.t these ablations on the VQA validation set (test sets are not recommended to be used for such experiments). The deeper LSTM Q + norm I baseline in [2] is also reported for comparison. We can see that image-attention-alone does improve performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with findings of previous attention models for VQA [23, 21].\nComparing the full model w.r.t. ablated versions without word, phrase, question level attentions reveals a clear interesting trend \u2013 the attention mechanisms closest to the \u2018top\u2019 of the hierarchy (i.e. question) matter most, with a drop of 1.7% in accuracy if not modeled; followed by the intermediate level (i.e. phrase), with a drop of 0.3%; finally followed by the \u2018bottom\u2019 of the hierarchy (i.e. word), with a drop of 0.2% in accuracy. We hypothesize that this is because the question level is the \u2018closest\u2019 to the answer prediction layers in our model. Note that all levels are important, and our final model significantly outperforms not using any linguistic attention (1.1% difference between Full Model and Image Atten).\nTo further understand our proposed co-attention mechanism, we break down the performance of our full model as a function of question lengths. Our hypothesis is that our proposed question attention (and corresponding co-attention) mechanism should play a more important role for longer questions, because shorter pithier questions may contain all equally important words. Fig. 4 shows these results using alternating co-attention (Oursa+VGG). We can see that while our model does provide larger improvements for longer questions (thus confirming our hypothesis), it works consistently well for\nall question lengths except for the boundary case of length 3, which has only two meaningful words (third token is the question mark) and occurs only 4 times in the VQA validation set."}, {"heading": "4.5 Qualitative results", "text": "In this section, we visualize some co-attention maps of our generated by our method in Fig. 5. From top to bottom, the rows are original images and questions, word level co-attention maps, phrase level co-attention maps and question level co-attention maps. At the word level, our model attends mostly to the object regions in an image, e.g., heads, bird. On the question side, the attentions depend on the specific question being asked. In the examples, we see that some have relatively uniform attention while some have peaky attention. At the phrase level, the image attention has different patterns across images. For the first two images and the forth image, the attention transfers from objects to background regions. For the other two images, the attention becomes more focused on the objects. We suspect that this is caused by the different question types. For the former three images, the questions ask more about global attributes of the images, while the later two images have more object-specific questions. Comparing the question attention at the phrase level and word level, we find that the emphasis changes. Because we convolve and pool multiple n-grams from words to phrases, our model is capable of localizing the key important phrases in the question, thus essentially discovering the question types in the dataset. For example, our model pays attention to the phrases \u201cwhat color\u201d, \u201chow many snowboarders\u201d and \u201cwhat sport playing\u201d. At the question level, our model summarizes the whole question and then performs the co-attention. It successfully attends to the regions in images and phrases in the questions appropriate for answering the question, e.g., \u201ccolor of the bird\u201d and bird region, \"what color ball\" and the ball region. Because our model performs co-attention at three levels, it often captures complementary information from each level, and then combines them to predict the answer. More success and failure cases can be seen in Fig. 6 and 7 respectively."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a hierarchical co-attention model for visual question answering. Coattention allows our model to attend to different regions of the image as well as different fragments of the question. We model the question hierarchically at three levels to capture information from different granularities. The experimental results showed that our model outperforms the state-of-theart on both the VQA and COCO-QA datasets. The ablation studies further demonstrate the roles of co-attention and question hierarchy in our final performance. Through visualizations, we can see that our model co-attends to interpretable regions of images and questions for predicting the answer. Though our model was evaluated on visual question answering, it can be potentially applied to other tasks involving vision and language."}, {"heading": "6 Acknowledgements", "text": "This work was supported in part by the Paul G. Allen Family Foundation, Google, and Institute for Critical Technology and Applied Science (ICTAS) at Virginia Tech through awards to D. P.; and by a\nNational Science Foundation CAREER award, an Army Research Office YIP award, an Office of Naval Research grant, an AWS in Education Research Grant, and GPU support by NVIDIA to D. B. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Jiashi Feng Ilija Ilievski", "Shuicheng Yan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J Shih", "Saurabh Singh", "Derek Hoiem"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "In Technical report,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Verb semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "In ACL,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": "In ACL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 4, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 12, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 13, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 15, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 20, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 21, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 22, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 1, "context": "These co-attended features are then recursively combined from word level to question level for the final answer prediction; \u2022 At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation; \u2022 Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [14].", "startOffset": 373, "endOffset": 376}, {"referenceID": 13, "context": "These co-attended features are then recursively combined from word level to question level for the final answer prediction; \u2022 At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation; \u2022 Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [14].", "startOffset": 389, "endOffset": 393}, {"referenceID": 1, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 4, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 9, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 12, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 13, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 1, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 11, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 12, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 13, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 0, "context": "[1] propose a compositional scheme that consists of a language parser and a number of neural modules networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "In [23], the authors propose a stacked attention network, which runs multiple iterations or hops to infer the answer progressively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "[22] propose a multi-hop image attention scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In [16], the authors generate image regions with object proposals and then select the regions relevant to the question and answer choice.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21] augments dynamic memory network by introducing a new input fusion module.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] propose RNNSearch to learn an alignment over the input sentences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In [7], the authors propose an attention model to circumvent the bottleneck caused by fixed width hidden vector in text reading and comprehension.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "A more fine-grained attention mechanism is proposed in [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Also focused on modeling sentence pairs, the authors in [24] propose an attention-based bigram CNN for jointly performing attention between two CNN hierarchies.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Our pooling method differs from those used in previous works [8] in that it adaptively selects different gram features at each time step, while preserving the original sequence length and order.", "startOffset": 61, "endOffset": 64}, {"referenceID": 21, "context": "Similar to [22], we connect the image and question by calculating the similarity between image and question features at all pairs of image-locations and question-locations.", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "Following [2], we take the top-1000 frequent answers and treat VQA as 1000-way classification problem.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset [14].", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "VQA dataset is the largest dataset for this problem, containing human annotated questions and answers on Microsoft COCO dataset [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "use the top 1000 most frequent answers as the possible outputs similar to [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "We use the evaluation protocol of [2] in the experiment.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "COCO-QA dataset is automatically generated from captions in the Microsoft COCO dataset [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "We report the results both on classification accuracy and Wu-Palmer similarity (WUPS) measure [20] in Table 2.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "We use torch [4] to develop our model.", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "We use the Rmsprop optimizer [19] with a base learning rate of 4e-4, momentum 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature.", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature.", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "The best performing method deeper LSTM Q + norm I from [2] is used as our baseline.", "startOffset": 55, "endOffset": 58}, {"referenceID": 21, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 102, "endOffset": 105}, {"referenceID": 20, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "[16] and FDN [9].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[16] and FDN [9].", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA.", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "4% (DMN+ [21]) to 62.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "2% (FDN [9]) to 66.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Specifically, FDN [9] also uses ResidualNet [6], but Ours+Residual outperforms it by 1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "Specifically, FDN [9] also uses ResidualNet [6], but Ours+Residual outperforms it by 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 21, "context": "SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "2% on test-dev (DMN+ [21]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "6% (SAN(2,CNN) [23]) to 65.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "\u2022 Image Attention alone, where in a manner similar to previous works [23], we do not use any question attention.", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "Open-Ended Multiple-Choice test-dev test-std test-dev test-std Method Y/N Num Other All All Y/N Num Other All All LSTM Q+I [2] 80.", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "[16] - - - - - 77.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "4 SMem [22] 80.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "2 - - - - SAN [23] 79.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "9 - - - - FDN [9] 81.", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "2 DMN+ [21] 80.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "0 2-VIS+BLSTM [14] 58.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "6 IMG-CNN [12] 58.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "7 SAN(2, CNN) [23] 64.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "The deeper LSTM Q + norm I baseline in [2] is also reported for comparison.", "startOffset": 39, "endOffset": 42}, {"referenceID": 22, "context": "We can see that image-attention-alone does improve performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with findings of previous attention models for VQA [23, 21].", "startOffset": 192, "endOffset": 200}, {"referenceID": 20, "context": "We can see that image-attention-alone does improve performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with findings of previous attention models for VQA [23, 21].", "startOffset": 192, "endOffset": 200}, {"referenceID": 16, "context": "Table 3: Ablation study on the VQA dataset using VGGNet [17].", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \u201cwhere to look\u201d or visual attention, it is equally important to model \u201cwhat words to listen to\u201d or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN) model. Our final model outperforms all reported methods, improving the state-of-the-art on the VQA dataset from 60.4% to 62.1%, and from 61.6% to 65.4% on the COCO-QA dataset1.", "creator": "LaTeX with hyperref package"}}}