{"id": "1606.00075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Applications of Probabilistic Programming (Master's thesis, 2015)", "abstract": "This paper describes the work on two applications of probabilistic programming: learning probabilistic program code based on predefined specifications, especially the program code of one-dimensional samplers; and facilitating sequential Monte Carlo conclusions using data-driven suggestions. The latter is presented with experimental results on a linear Gaussian model and a non-parametrically dependent Dirichlet process mix of object models for object recognition and tracing.", "histories": [["v1", "Tue, 31 May 2016 23:48:55 GMT  (2695kb)", "http://arxiv.org/abs/1606.00075v1", "Supervisor: Frank Wood. The thesis was prepared in the Department of Engineering Science at the University of Oxford"]], "COMMENTS": "Supervisor: Frank Wood. The thesis was prepared in the Department of Engineering Science at the University of Oxford", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yura n perov"], "accepted": false, "id": "1606.00075"}, "pdf": {"name": "1606.00075.pdf", "metadata": {"source": "CRF", "title": "Applications of Probabilistic Programming", "authors": ["Yura Perov"], "emails": [], "sections": [{"heading": null, "text": "Applications of\nProbabilistic Programming\nYura Perov Wolfson College\nDepartment of Engineering Science\nUniversity of Oxford\nA thesis submitted for the degree of\nMaster of Science by Research\nTrinity 2015\nii\nAbstract\nThis thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent Dirichlet process mixture of objects model for object recognition and tracking.\nWe begin this work by providing a brief introduction to probabilistic programming. In the second Chapter we present an approach to automatic discovery of samplers in the form of probabilistic programs. Specifically, we learn the procedure code of samplers for one-dimensional distributions. We formulate a Bayesian approach to this problem by specifying a grammar-based prior over probabilistic program code. We use an approximate Bayesian computation method to learn the programs, whose executions generate samples that statistically match observed data or analytical characteristics of distributions of interest. In our experiments we leverage different probabilistic programming systems, including Anglican and Probabilistic C, to perform Markov chain Monte Carlo sampling over the space of programs. Experimental results have demonstrated that, using the proposed methodology, we can learn approximate and even some exact samplers. Finally, we show that our results are competitive with regard to genetic programming methods.\niii\nIn Chapter 3, we describe a way to facilitate sequential Monte Carlo inference in probabilistic programming using data-driven proposals. In particular, we develop a distance-based proposal for the non-parametric dependent Dirichlet process mixture of objects model. We implement this approach in the probabilistic programming system Anglican, and show that for that model data-driven proposals provide significant performance improvements. We also explore the possibility of using neural networks to improve data-driven proposals.\niv\nHeartfelt gratitude\nThere are so many beings and organisations who made this work possible.\nFirst of all, I overwhelmingly thank my supervisor, Prof Frank Wood. His support as a supervisor is extraordinary. He helped me grow my initial idea seeds into fairly mature research projects, and helped me improve my research skills. He also has significantly supported and contributed to my overall personal development. I thank him extremely much.\nI am grateful to my Oxford colleagues and mentors: Tuan Anh Le, Brooks Paige, Neil Dhir, Tom Rainforth, Dr David Tolpin, Dr Jan Willem van der Meent, Tom Jin, Niclas Palmius, David Janz, as well as to senior colleagues, including Prof Hongseok Yang, Prof Michael Osborne, Prof Steven Roberts, Prof Nando de Freitas, Prof Yee Whye Teh, Prof Franc\u0327ois Caron, and many others. I am appreciative of feedback from and discussions with members of the probabilistic programming reading group, the machine learning lunches and the brainstorming artificial intelligence forum sessions.\nI thank Willie Neiswanger, whose help with the DDPMO model was very useful for our work with Tuan Anh Le and who also has provided us with some handy code that was important for the work.\nI am appreciative of the support of the University, of my college, Wolfson, and also\nthat of Somerville college, St Antony\u2019s college and Exeter college.\nThis thesis is the natural continuation of my work on probabilistic programming at\nv\nthe Massachusetts Institute of Technology. I thank my colleagues at the Massachusetts Institute of Technology, where the engagement with probabilistic programming and machine learning magic in general happened to me first time. I thank Dr Vikash Mansinghka and Prof Joshua Tenenbaum in particular, who made my visit to MIT, first of all, possible (thank you both very much for believing in me!), secondly, made it very fruitful in terms of improving my skills, and, last but not least, they made me feel it very cordially welcomed. I am extremely grateful to Vikash for all enormous amount of time and effort he committed to supervising and working with me. May I also thank my colleagues at MIT, in particular Tejas Kulkarni, Ardavan Saeedi, Daniel Selsam, Andreas Stuhlmu\u0308ller, Jonathan Huggins, Jonathan Malmaud, Dan Lovell, Jay Baxter, Zack Drach, Vlad Firoiu, Max Siegel, Max Kleiman-Weiner, Dr Cameron Freer, and many others, very much.\nI would also like to thank my colleagues and mentors at the Siberian Federal University, and in Russia in general, who educated me in Mathematics, Computer Science and Economics, and who also made my studies, as a visiting student, at MIT and Oxford, possible. In particular, I thank Prof Alexander Gorban (who is currently professor at the University of Leicester), Prof Alexander Kitmanov, Dr Tatiana Krupkina, Prof Eugene Semenkin, Vladimir Kontorin, Tatiana Mihailova, and literally hundreds of others. I am also appreciative of advice and support I received from the members of Russian Association for Artificial Intelligence, including Prof Vadim Stefanuk, Prof Gennady Osipov, Prof Oleg Kuznezov, Prof Igor Fominih, Prof Vladimir Khoroshevskij, Prof Vadim Vagin, and others.\nEverything I do, in terms of good and productive things, is made possible because of my beloved relatives and friends. I am extremely grateful to my parents, Olga and Nikolay, to my sister Olga and to her husband Igor, to my grandmas Galia and Toma,\nvi\nto my aunt Natalja, and, of course, to Maria. I thank Ardavan, Tejas, Daniil, Michelle, Neil and Roman for their huge friendly support.\nThis work, in particular that on learning probabilistic programs, and my skills have also significantly benefited from meetings and discussions with other research groups and researchers, including Microsoft Research at Cambridge (in particular Dr Andy Gordon, Dr Ali Eslami, Dr John Winn, Dr Tom Minka; and others), Microsoft Research at Bangalore (Dr Aditya Nori), Stanford (Prof Percy Liang, Prof Noah Goodman, and their research groups), Cambridge (Prof Zoubin Ghahramani, and his group), Berkley (Prof Stuart Russell, Prof Rastislav Bodik, and their groups), Harvard (Prof Ryan Adams and his group), E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne (Prof Patrick Thiran, Prof Matthias Grossglauser, Prof Auke Ijspeert and his group), the Dagstuhl seminar on \u201cApproaches and Applications of Inductive Programming\u201d (December 2013), and the meetings at the Heidelberg Laureate Forum (August 2015).\nMay I also thank the Supreme Being, and also all beings on this planet. I believe that I might be thankful to almost every person on this planet who have been helping me with my research work and my studies in this or that way though the huge interconnection of the world economy today.\nThese few pages are definitely not enough to list all people, whose help, mentoring, support and discussions have led me to be at Oxford and allowed me to work on these projects. Thank you, all of you, very much!\nI am really grateful to all sponsors who made this work possible, including the Russian President\u2019s fellowship, Xerox, Google, DARPA, Somerville College, my relatives, incomes and savings from my previous business projects (thus I thank clients, partners, investors and other contributors of those projects), and help from my friends.\nvii"}, {"heading": "Acknowledgement", "text": "The initial description of the approach and initial experimental results on learning probabilistic programs were described in an arXiv submission (Perov and Wood, 2014), and in my Bachelor\u2019s thesis (Perov, 2014).\nThis is also to acknowledge that the part of the work on data-driven proposals was made in collaboration with Tuan Anh Le under the supervision of Prof Frank Wood. In particular, we brainstormed and developed the ideas together, wrote probabilistic program code for linear Gaussian model and DDPMO model experiments, and performed and analysed sets of experiments together.\nThe proposal on the experiment for scientific rediscovery of classical genetics laws\nwas prepared with advice of Prof Joshua Tenenbaum.\nFor initial experiments, a framework for Stochastic Simulation in Java (L\u2019Ecuyer\net al., 2002) was usefully employed.\nviii\nContents"}, {"heading": "1 Introduction to probabilistic programming 1", "text": "1.1 An example of a probabilistic program, and its execution trace . . . . . 2 1.2 Another example of a probabilistic program, and the related execution\ntrace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Existing probabilistic programming platforms and statistical inference\nin them . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.4 Ways to improve general-purpose statistical inference in probabilistic\nprogramming platforms . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "2 Learning probabilistic programs 10", "text": "2.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.1.1 Automatic programming . . . . . . . . . . . . . . . . . . . . . 12 2.1.2 Generalising from data and automated modelling . . . . . . . . 13\n2.2 Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.1 Basics of approximate Bayesian computation (ABC) . . . . . . 14 2.2.2 Matching distributions using ABC . . . . . . . . . . . . . . . . 17\n2.3 Noisy ABC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.3.1 Moments matching as an example of statistic \u03b71 . . . . . . . . . 20 2.3.2 Use of hypothesis test statistics for \u03b71 . . . . . . . . . . . . . . 22\nix\n2.4 Prior over program code . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.5.1 Evaluation of our approach versus evolutionary algorithms . . . 31 2.5.2 Engines comparison . . . . . . . . . . . . . . . . . . . . . . . 32 2.5.3 Learning sampler code . . . . . . . . . . . . . . . . . . . . . . 34 2.5.4 Example of learning a standard Normal sampler . . . . . . . . . 36\n2.6 Outline of a motivating example for the scientific rediscovery in classi-\ncal genetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.6.1 Aims of the proposing experiment . . . . . . . . . . . . . . . . 39 2.6.2 Brief introduction to Mendel\u2019s work . . . . . . . . . . . . . . . 39 2.6.3 Abstractions to induce . . . . . . . . . . . . . . . . . . . . . . 42 2.6.4 Inducing a probabilistic procedure . . . . . . . . . . . . . . . . 42 2.6.5 An abstraction of an individual plant . . . . . . . . . . . . . . . 43 2.6.6 An abstraction of an observable plant feature . . . . . . . . . . 44 2.6.7 An abstraction of the plant hybridisation process . . . . . . . . 45 2.6.8 Procedures that are expected to be learnt . . . . . . . . . . . . . 46\n2.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47"}, {"heading": "3 Data-driven proposals in probabilistic programming 50", "text": "3.0.1 Proposals in sequential Monte Carlo inference . . . . . . . . . 50 3.0.2 Using a discriminative model for data-driven proposals . . . . . 51 3.0.3 Experiments with the linear Gaussian model . . . . . . . . . . . 54\n3.0.3.1 Functions to generate training and test episodes {y1:T} 55 3.0.3.2 Comparing sequential Monte Carlo runs without and\nwith data-driven proposals . . . . . . . . . . . . . . . 58\nx\n3.0.4 Applying the approach to the DDPMO model . . . . . . . . . . 60\n3.0.4.1 DDPMO model in Anglican . . . . . . . . . . . . . . 60 3.0.4.2 Conjugate priors . . . . . . . . . . . . . . . . . . . . 61 3.0.4.3 Data-driven proposal for SMC inference in DDPMO . 61\n3.0.5 Experiments with the DDPMO model . . . . . . . . . . . . . . 63\n3.1 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nConclusion 77"}, {"heading": "A Corpus of sampler code 78", "text": ""}, {"heading": "B The DDPMO and GPU code in Anglican 83", "text": "B.1 The DDPMO code . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 B.2 The GPU code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 B.3 Clojure code for the data-driven proposal . . . . . . . . . . . . . . . . 89 B.4 Anglican code (within the DDPMO model) for the data-driven proposal 90 B.5 Code for the GPU, to get data for the proposal for train datasets . . . . . 91 B.6 Code for the GPU, to use the proposal for test datasets . . . . . . . . . 92\nBibliography 94\nxi\nChapter 1\nIntroduction to probabilistic programming\nProbabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data. A probabilistic model in the form of probabilistic program code describes the data-generating process from unknown latent variables\u2019 values to observed data. The latent variables values are subject to assumptions that are given in the form of probability distributions. The types of models that may be written as probabilistic programs include not only basic Bayesian networks and graphical models, but also ones more expressive and flexible, such as nonparametric models and graphical models with dynamic structure.\n1"}, {"heading": "1.1 An example of a probabilistic program, and its exe-", "text": "cution trace\nA basic probabilistic model in the form of a probabilistic program is shown below: (query (let\n[unknown-mean-t1 (sample (normal 2 1)) unknown-mean-t2 (sample (normal unknown-mean-t1 1)) noise 0.1] (observe (normal unknown-mean-t1 noise) 3) (observe (normal unknown-mean-t2 noise) 3.1) (predict unknown-mean-t1) (predict unknown-mean-t2)))\nEquation 1.1 contains two latent variables, x1 and x2, and two observed data points, y1 and y2. Each run of a probabilistic program yields a single execution trace. An execution trace is a map from random choices to their specific values. An execution trace fully defines the execution of the probabilistic program. Given the execution trace, a probabilistic program becomes deterministic. An example of the execution trace for the program in Figure 1.1 is (x1 = 3.0, x2 = 2.5, y1 = 2.0, y2 = 2.1). For that particular program, each list of four random variables constitutes a valid execution trace: (x1 \u2208 R, x2 \u2208 R, y1 \u2208 R, y2 \u2208 R).\n2\nThe probability of an execution trace can be defined, in a similar but more restrictive way to (Wood et al., 2014), as p(y,x) \u2261\u220fNn=1 p(yn|\u03b6tn ,xn)p(xn|xn\u22121), where yn is the n-th output data point (i.e. an observation), p(yn|\u03b6tn ,xn) is its normalised likelihood, \u03b6tn(xn) is its argument, tn(xn) is a random procedure type (e.g. Normal), xn is the ordered set of all random choices that have to be computed before the likelihood of yn can be evaluated, p(xn|xn\u22121) is its normalised prior probability, and x and y are the sets of all latent and observing random procedure applications correspondingly."}, {"heading": "1.2 Another example of a probabilistic program, and", "text": "the related execution trace\nAnother simple example of a probabilistic program is a procedure that samples from the geometric distribution:\nFor simplicity, this second example of a probabilistic program does not have any observations, and thus it is unconditioned. On other words, its prior is the same as the posterior.\nExamples of valid execution traces of this program include:\n3\n1. true;\n2. false, true;\n3. false, false, false, false, true.\nA valid execution trace for that program is any, possibly empty as well, sequence of false draws, terminated by a true draw. The program is a good example, because it demonstrates that while the number of drawn random choices in any execution trace is expected to be always finite for programs that terminate with probability 1, that number is not necessarily bounded by any constant."}, {"heading": "1.3 Existing probabilistic programming platforms and", "text": "statistical inference in them\nMany probabilistic programming languages have been designed. Usually, for each probabilistic programming language, a new related engine is developed to conduct some type of statistical inference. Statistical inference methods are typically chosen and implemented in such a way as to allow automatic inference for any probabilistic programs that may be expressed in a probabilistic programming language. The expressiveness of the language thus varies, and depends on utilised inference methods.\nParticular languages and implementations include functional probabilistic programming languages such as Church (Goodman et al., 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al., 2014); logic probabilistic programming languages (De Raedt and Kimmig, 2013) such as ProbLog (Kimmig et al., 2011); and domain-specific PPLs, such as (Kiselyov and Shan, 2009). Other languages and implementations also endorse declarative definitions of probabilistic models and include IBAL (Pfeffer, 2001),\n4\nStan (Stan Development Team, 2014), BLOG (Milch et al., 2007), BUGS (Lunn et al., 2009), FACTORIE (McCallum et al., 2009), Markov Logic networks (Richardson and Domingos, 2006), and Infer.NET (Minka et al., 2012). More detailed overviews are given in (Roy, 2016; De Raedt and Kimmig, 2013; Gordon et al., 2014; Mansinghka et al., 2014).\nThese implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al., 2014), Hamiltonian Monte Carlo (Stan Development Team, 2014), variational inference (Mansinghka et al., 2014), belief propagation (Hershey et al., 2012), expectation propagation (Minka et al., 2012), and variational message passing (Minka et al., 2012).\nThe choice of the employed inference method is related to the trade-off between expressiveness and inference performance. For example, Infer.NET is one of the most high-performance probabilistic programming engines and may process huge datasets. This high performance is, however, achieved by internal compilation of an Infer.NET probabilistic program into a finite graphical model and an application of expectation propagation inference method (Minka, 2001), which seriously restricts the range of models that may be written in it. In particular, it is not possible to perform inference for non-parametric Bayesian models in Infer.NET. On the other hand, while languages like Church, Anglican and Venture are some of the most flexible and expressive (Ranca, 2014), their statistical inference performance is slower by at least the factor of 10x in comparison to languages like Infer.NET and at least by the factor of 50x in comparison to hand-written samplers. Here, by hand-written samplers we mean the implementations of inference with manually derived updated, written for specific models in fast languages such as C or C++.\n5"}, {"heading": "1.4 Ways to improve general-purpose statistical infer-", "text": "ence in probabilistic programming platforms\nSuch slow performance means that one of the current critical drawbacks of probabilistic programming is the lack of efficient general-purpose inference algorithms. This problem prevents probabilistic programming from being utilised on a large scale by users in the machine learning field, including researchers, scientists, data analysts and graduate students. Several possible approaches are being explored to address this issue. One is to employ new general-purpose inference methods. For example, two new probabilistic programming languages have recently been introduced, employing sequential Monte Carlo methods (Smith et al., 2013): Anglican (Wood et al., 2014) and Biips (Todeschini et al., 2014). In 2014 a general-purpose implementation of the particle Gibbs with ancestor sampling method (Lindsten et al., 2014) was introduced in (van de Meent et al., 2015) as an alternative engine for Anglican. Variational inference has been employed in probabilistic programming since 2013 in Stochastic MATLAB (Wingate and Weber, 2013), Venture (Mansinghka et al., 2014) and Stan (Kucukelbir et al., 2014). Finally, slice sampling for probabilistic programming has been proposed in (Ranca and Ghahramani, 2015).\nIn addition to statistical inference methods, optimisation methods have been applied for probabilistic programming. In particular, an approximation search algorithm for maximum a posteriori probability estimation has been presented in (Tolpin and Wood, 2015).\nAnother approach to facilitate inference for probabilistic programming is to increase the performance of already employed inference methods. Foremost, this is achieved by technical enhancements in implementations of probabilistic programming engines: by\n6\nthe choice of a faster implementation language (e.g. the implementation of Venture in C++ (Mansinghka et al., 2014) works faster than its very early prototype implementation in Clojure (Perov and Mansinghka, 2012)), by an intermediate compilation instead of a continuous interpretation (e.g. the recent Anglican implementation (Tolpin et al., 2015b), with a program compilation into a Clojure function, works faster than the previous Anglican interpreter (Wood et al., 2014)), and by an utilisation of just-in-time compilation (examples include engines, described in (Perov and Mansinghka, 2012; Tolpin et al., 2015b), which are implemented in Clojure). Another related work worth mentioning is \u201cProbabilistic C\u201d (Paige and Wood, 2014), where authors present a C library that allows sequential Monte Carlo and Particle Gibbs inference in any C and C++ program with just two added C functions, OBSERVE and PREDICT, to condition executions and get particle smoothing predictions correspondingly. This not only provides a very fast probabilistic programming engine implementation, but also serves as a compilation target and allows the transformation of almost any existing deterministic programming language into one that is probabilistic. Mostly, all these mentioned methods give constant performance improvements in time and memory. Furthermore, ways have been proposed to enhance the design of existing inference algorithms, e.g. by exploring conditional dependencies and making incremental updates only on a part of the execution trace, as in Venture (Perov and Mansinghka, 2012; Mansinghka et al., 2014) and Shred (Yang et al., 2014), where the latter is a tracing interpreter for Church language. For many models, these methods give asymptotic performance improvements in execution time. For example, while in old implementations of Church (Goodman et al., 2008) N sweeps1 of Metropolis-Hastings inference in a hidden Markov model with T\n1A sweep, in the context of doing Metropolis-Hastings inference on the probabilistic program with T random choices, consists of T local MH proposals on those random choices. One local MH proposal on a random choice is when we propose a new value just for one random choice\n7\ndata points had time complexity of O(T 2N), in Venture2 this complexity is O(TN).\nSo far, we discussed two approaches to improve probabilistic programming inference, exploitation of new general-purpose inference algorithms and their variations, and the increase of performance of already employed algorithms, without changing their statistical convergence properties. Another approach is to improve the statistical properties of inference algorithms such that they converge faster. An example of this approach is the improvement of message passing in expectation propagation (EP) by learning message passing operators. Methods to learn EP operators for inference in Infer.NET have included neural networks (Heess et al., 2013), just-in-time random forests (Eslami et al., 2014) and just-in-time kernel-based regression (Jitkrittum et al., 2015). In addition, to improve general-purpose message passing in Infer.NET for graphical models with many layers, which are often used in computer vision, a consensus message passing method has been proposed in (Jampani et al., 2015a). Another example of enhancing general-purpose inference is a recent paper (Tolpin et al., 2015a) introducing adaptive Metropolis-Hastings inference for probabilistic programming. That work proposes to learn an MCMC scheduler: they learn non-uniform probabilities of proposing random variables, on which the engine makes proposals. Their results show that their MCMC scheduler provides a consistent improvement in convergence. The way on using data-driven proposals and discriminative models described in Chapter 3 of this work is related to this approach.\nTwo following chapters illustrate examples of what problems might be addressed (given the fixed values of all other random choices in the execution trace), and accept or reject its new value.\n2Venture has such efficient asymptotics not only for MH, but for versions of Gibbs inference algorithm as well. It is also capable of handling efficient inference in probabilistic programs with varying number of random choices.\n8\nwith the help of the probabilistic programming framework. In particular, Chapter 2 describes an approach to learning probabilistic programs automatically, using existing highly expressive probabilistic programming platforms that support higher-order functions. The ultimate goal of work described in Chapter 2 is to automatically induce and employ generative models of the world for general artificial intelligence.\nChapter 3 demonstrates how to facilitate statistical inference in probabilistic programming by using discriminative models in order to improve Monte Carlo proposals. The approach is illustrated by experiments on the existing Bayesian generative nonparametric model, \u201cthe Dependent Dirichlet Process Mixture of Objects\u201d (Neiswanger et al., 2014). While this model has already existed in the field of machine learning, this is the first time it has been implemented in a probabilistic programming framework.\n9\nChapter 2\nLearning probabilistic programs\nThe aim of many machine learning algorithms is to process existing data and provide predictions. To do so, model parameters and/or model structure need to be learnt.\nIn this Chapter we present an approach to automatic discovery of generative models in the framework of probabilistic programming. Here, probabilistic programing is a suitable approach, since a probabilistic program is, in essence, a procedural representation of a generative model. The ultimate goal for the future will be to automatically induce generative models for the general artificial intelligence.\nOur intermediate task is far more modest. In this thesis, we aim to induce program code that, when executed repeatedly, returns values the distribution of which matches that of observed data. As a starting point, we consider the induction of programs that sample from parametrised one-dimensional distributions. In other words, the problem is to automatically learn simple versions of generative models (samplers), that statistically match observed data. Such samplers are to be learnt in the form of potentially interpretable probabilistic program code.\nProbabilistic programming is relevant to this problem because programs in Turingcomplete languages can represent a wide range of generative probabilistic models, and samples from these models can be generated efficiently by simply executing the program\n10\ncode. Representing generative models as code has the additional advantage that learned programs may potentially be analysed by humans. Finally, in higher-order languages like Anglican, where procedures may act on other procedures, it is possible to write a generative model for program code that is itself a probabilistic program. This enables us to perform inference by specifying an adaptor-based grammar prior over program code and to use general-purpose Markov chain Monte Carlo algorithms implemented by the inference engine of Anglican (Tolpin et al., 2015b) to sample over the space of programs.\nTo assess whether the distribution of samples generated by a program candidate matches the given distribution of interest, we use approximate Bayesian computation methods (Marin et al., 2012). We specify an approximate likelihood in terms of the similarity between a summary statistic of the generated samples and that of the observed distribution of interest. While this approach is inherently approximate, it still can be used to find exact sampler code. This argument is supported by the fact that we were able to successfully learn an exact sampler for the Bernoulli distribution family (Perov and Wood, 2014; Perov, 2014), given only an adaptor grammar-based prior learnt from a corpus of sampler code that did not include Bernoulli sampler code. We also found approximate samplers for other common one-dimensional distributions and for realworld data. Finally, our approach holds its own in comparison to state-of-the-art genetic programming methods (Koza, 1992; Poli et al., 2008).\nThe probabilistic programming language and system we use, Anglican, is Turingcomplete and higher-order, and this allows us to specify the grammar prior as a higherlevel probabilistic program that samples probabilistic program candidates of our interest.\n11"}, {"heading": "2.1 Related work", "text": "Our work on learning probabilistic programs is related both to automatic programming and generalising from data. The former, automatic programming, addresses the synthesis of program code from specifications. Those specifications are often incomplete and include input/output examples. The latter, generalising from data, is one of the main aims of machine learning in as a whole."}, {"heading": "2.1.1 Automatic programming", "text": "One recent overview of automatic programming is presented in (Gulwani et al., 2014) and its references. Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998). From a very abstract standpoint, automatic programming concerns the search in the complex space of program code for programs that satisfy a certain specification. Automatic programming approaches differ from each other in the way that the specifications are formulated, in the expressiveness of the search space, and as to which search algorithm is used. Automatic programming is often paired with the field of programming languages and verification, in order to find programs which satisfy formal specifications.\nOur approach is similar to the work on learning programs using a hierarchical Bayesian\nprior (Liang et al., 2010). In that paper authors also use a statistical inference framework, define a prior over program text and perform statistical inference. While they search for deterministic programs that satisfy several training input/output pairs, we look for probabilistic programs that are statistically similar to the distributions of in-\n12\nterest. To the best of our knowledge, our work has been the first attempt to perform inference over probabilistic models in the form of probabilistic programs in such expressive probabilistic programming languages as Church, Venture or Anglican."}, {"heading": "2.1.2 Generalising from data and automated modelling", "text": "The approach we describe in this chapter is related to density estimation (Silverman, 1986), which concerns the estimation of an unobservable probability density function given some observed data. There is, however, a major difference between density estimation and our approach. While most density estimation methods produce just a set of parameters (e.g. weights), we learn the representation of observed data in a structural and potentially interpretable form of generative model program code.\nOur approach is also related to probabilistic model learning, for example to learning probabilistic relational models (Friedman et al., 1999) and Bayesian network structure (Mansinghka et al., 2012). Also worthwhile of mentioning are recent works in search over generative probabilistic model structures (Grosse et al., 2012) and kernel compositions (Duvenaud et al., 2013). Similarly to our approach, they explore a huge complex space of models, in which enumeration is intractable. While they use a greedy search algorithms to find an optimum model, we employ a fully Bayesian approach and define a non-parametric prior distribution over program text itself. This allows us to search over a more expressive class of probabilistic models, and allows us to penalise long or atypical program text."}, {"heading": "2.2 Approach", "text": "We are interested in finding probabilistic programs, which when iteratively interpreted produce samples statistically similar to the distribution of interest F\u03bb with parameter\n13\nvector \u03bb. For now, we assume that parameter vector \u03bb is fixed, and omit it to simplify notation. Each probabilistic program, which we consider as a potential match, is represented as its program text T . We define the grammar prior over program text p(T ), details of which will be described in Section 2.4. The distribution of interest F may be given in different forms; for example, as a set of samples X = {xi \u223c F}, or as characteristics of that distribution F (e.g. its moments).\nFor every particular program candidate T , we want to evaluate how well it matches the distribution of interest F . There are no general ways to check this analytically by looking at its program text. We, therefore, employ the methods of approximate Bayesian computation. Specifically, we draw N samples X\u0302 = (x\u03021, . . . , x\u0302J) from program T by evaluating it. (That is, X\u0302 is drawn from the distribution p(X\u0302 |T ), since every probabilistic program text defines a distribution.) Before describing further details of our approach, we provide a brief outline of approximate Bayesian computation that is based on (Marin et al., 2012) and uses algorithms and equations there contained1."}, {"heading": "2.2.1 Basics of approximate Bayesian computation (ABC)", "text": "Let us start by considering the standard setup for Bayesian inference. There is some parameter of interest \u03b8, an intermediate hidden variable \u03be, and an observation y. We are able to model the prior distribution p(\u03b8), the condition distribution p(\u03be|\u03b8) for the intermediate hidden variable, and to sample observations from p(\u00b7|\u03be). We are interested in the posterior\np(\u03b8|y) = \u222b p(\u03b8, \u03be|y)d\u03be \u221d \u222b p(\u03b8, \u03be)p(y|\u03b8, \u03be)d\u03be = \u222b p(\u03b8)p(\u03be|\u03b8)p(y|\u03be)d\u03be.\nFor simplicity, let us agree to consider the intermediate variable \u03be to be the part of \u03b8, since we are able to marginalise over the part of variable \u03b8 := [\u03be, \u03b8]. This means that we 1In addition, notes (Huggins, 2013) from Jonathan Huggins were quite helpful.\n14\nhave the prior distribution p(\u03b8), the distribution p(\u00b7|\u03b8), and we are able to sample from both of them.\nApproximate Bayesian computation (ABC) framework (Marin et al., 2012) is different from the standard setup of Bayesian inference due to the impossibility (or intractability) to precisely calculate the likelihood p(y|\u03b8), even if we can sample from p(\u00b7|\u03b8). The ABC approach proposes different algorithms to address this issue.\nIn the case when the distribution p(\u00b7|\u03b8) is finite or countable, one solution to obtain N samples {\u03b8i}Ni=1 from p(\u03b8|y) is to use the rejection sampling algorithm. This algorithm (Rubin et al., 1984) may be considered to be the first ABC algorithm, and it is presented.\nAlgorithm 1 Likelihood-free rejection sampler (Marin et al., 2012) for i = 1 to N do\nrepeat Generate \u03b8 from the prior distribution p(\u03b8) Generate z from the distribution p(\u00b7|\u03b8) until z = y set \u03b8i = \u03b8\u2032,\nend for\nThis algorithm cannot deal with cases where the sample space D of the distribution p(\u00b7|\u03b8) is neither finite nor countable. Algorithm 1 may be extended to the case of continuous sample spaces. While we cannot precisely compare z and y, we may measure how similar they are. To do this, we need to introduce a statistic \u03b7 on the space of observations D, introduce a metric \u03c1 on \u03b7(D), and accept a new proposed z if the distance between \u03b7(y) and \u03b7(z) is lower than the introduced fixed threshold . This new algorithm is similar to the previous, and is presented below.\n15\nAlgorithm 2 Likelihood-free approximate rejection sampler (Marin et al., 2012) for i = 1 to N do\nrepeat Generate \u03b8 from the prior distribution p(\u03b8) Generate z from the distribution p(\u00b7|\u03b8) until \u03c1{\u03b7(z), \u03b7(y)} \u2264 set \u03b8i = \u03b8\u2032, zi = z\u2032,\nend for\nThe above algorithm samples from the joint distribution (Marin et al., 2012)\np (\u03b8, z|y) = p(\u03b8)p(z|\u03b8)IA ,y(z)\u222b\nA ,y\u00d7\u03b8 p(\u03b8)p(z|\u03b8) dz d\u03b8 , (2.1)\nwhere IA(\u00b7) is the indicator function of the set A, and\nA ,y = {z \u2208 D | \u03c1{\u03b7(z), \u03b7(y)} \u2264 } .\nBy marginalising this joint distribution 2.1 over z, we are able to get the sought after\nposterior distribution\np (\u03b8|y) = \u222b p (\u03b8, z|y) dz \u2248 p(\u03b8 | y) .\nFinally, let us emphasise that there are three levels of approximation in such ABC\nalgorithm:\n1. the choice of statistics \u03b7(D), for observations y \u2208 D, that often is chosen to be\ninsufficient,\n2. the distance function \u03c1 on \u03b7(D),\n3. and the value of tolerance level .\n16"}, {"heading": "2.2.2 Matching distributions using ABC", "text": "The previous subsection described the basics of ABC. Before describing a few more aspects of ABC, which are relevant for our work, let us begin to frame our problem in terms of ABC. Recall that in our set-up we have a distribution of interest F , which is given in a form of its characteristics, and we need to find the distribution T in the form of a probabilistic program, which matches F . We approach this problem in Bayesian way by providing a prior over samplers p(T ), and condition it to find such samplers T1, T2, . . . that match F .\nIdeally, we would like to have a general way to compare two distributions, one of which is given as probabilistic program code and another is given by its characteristics. One would imagine doing this by drawing infinite number of samples from T or by some sort of code analysis, but this is, to the best of our knowledge, not feasible in the general case. As mentioned before, we do not know any better general way to check if program code T matches the distribution of interest F rather than draw some number of samples from T and compare them to F . Therein lies a departure from the standard approximate Bayesian computation framework, which is that the type of ABC observation y is distribution in comparison to the type of one-dimensional or multidimensional continuous variables in common ABC settings. To represent and be able to compare this distribution to F , we need to sample from T some finite number of times M to get X\u0302 = {x\u03021, . . . , x\u0302M} such that x\u0302i \u223c p(\u00b7|T ). Finally, we would like to marginalise over X\u0302 to approximate p(T |F ).\nOne way to interpret this is to say that the whole statistic function \u03b7(\u00b7,M) is a composition of several helper statistic functions such that \u03b7(\u00b7,M) = \u03b71(\u03b72(\u00b7,M)). The fact that we draw only M samples X\u0302 from p(\u00b7|T ) is related to the partial contribution of\n17\nstatistic function \u03b72(\u00b7,M). In our case, the statistic \u03b72(\u00b7,M) will always be insufficient, since we cannot draw infinite number of samples. Thus, the whole statistic function \u03b7 will be definitely insufficient as well.\nOnce we have drawn M samples from the distribution candidate p(\u00b7|T ) of a probabilistic program candidate T , we can use any statistical method to compare how similar it is to the distribution of interest F . This step is related to the contribution of another statistic function \u03b71(\u00b7). One example of such statistic is some finite number of raw and central moments. Here we would compare moments of p(\u00b7|T ) and F given the threshold , so we accept only such T that \u03c1(\u03b71(X\u0302 ), \u03b71(F )), where X\u0302 = {x\u0302i \u223c p(\u00b7|T )}Mi=1. Details and examples are provided in further sections.\nLet us summarise how our problem relates to ABC approach: the ABC prior distribution p(\u03b8) is our prior over samplers p(T ), the ABC observation z is the distribution p(\u00b7|T ) that is defined by probabilistic program code T , the \u201cintermediate\u201d statistic \u03b72(z,M) of the distribution z is the finite number of samples {xi}Mi=1 from p(\u00b7|T ), the \u201cfull\u201d statistic \u03b7(z) = \u03b7(z,M) = \u03b71(\u03b72(z,M)) is the statistic of the distribution z, the observation y is the distribution of interest F , and the statistic \u03b7(y) of the observation y is the statistic \u03b7(F ) of the distribution of interest F . We consider that y \u2248 z if the distance between two statistics \u03c1(\u03b7(z), \u03b7(y)) < . Finally, our inference is intended to target an approximation of p(T |F ), namely\np (T |F ) = \u222b p (T , X\u0302 |F )dX\u0302 \u221d \u222b p(T )p(X\u0302 |T )IA ,F (X\u0302 )dX\u0302 . (2.2)\nIn the next section we describe how we can avoid using a fixed threshold , and instead employ continuous distance functions that measure the similarity between distributions.\n18"}, {"heading": "2.3 Noisy ABC", "text": "Another concept in ABC, that is essential for our work, is the proposal of \u201cnoisy ABC\u201d, made by Wilkinson in (Wilkinson, 2013). He suggests to replace the fixed threshold by a general noise kernel function K (y, z), which should be a valid probability density function given a particular observation y. The kernel might be interpreted as measurement or model error. It is expected to have high values when y \u2248 z, and low values otherwise. The joint ABC target distribution becomes as follows (Marin et al., 2012):\npK, (\u03b8, z | y) = p(\u03b8)p(z | \u03b8)K (y, z)\u222b\np(\u03b8)p(z | \u03b8)K (y, z) dz d\u03b8 , (2.3)\npK, (\u03b8 | y) \u221d \u222b p(\u03b8)p(z | \u03b8)K (y, z) dz . (2.4)\nThe important point, which was made by Wilkinson himself in (Wilkinson, 2013), is that if the model already includes the error as part of its generative model (i.e. K (y, z) is the part of the model in the same way as p(\u03b8) and p(z | \u03b8) are the parts of it), then the ABC will target the exact posterior for that model. If the kernel is approximate or added on the top of the model, then the ABC method will produce an approximate posterior estimate in general. In both cases, we are able to incorporate the kernel into probabilistic program code of the generative model and employ automatic general-purpose inference methods available in probabilistic programming engines. In particular, we used particle Markov chain Monte Carlo (PMCMC) inference in probabilistic program system Anglican (Tolpin et al., 2015b).\nIn the context of our work, the noisy ABC target p(T |F ) is intended to be propor-\ntional to the marginal in X\u0302 of the joint distribution\np(T )p(X\u0302 |T )\u03c0(F |X\u0302 ) , (2.5)\n19\nwhere \u03c0(F |X\u0302 ) is a kernel function that in our case measures a distance between two distributions. In other words, this is a penalty function that judges how distinct the samples X\u0302 from the program candidate T statistically are, as the whole, from the distribution of interest F ."}, {"heading": "2.3.1 Moments matching as an example of statistic \u03b71", "text": "Previously, we did not specify an example of statistic \u03b71 and kept it abstract. One example of such statistic is the set of moments of a distribution. The n-th moment, about a fixed value c, of the one-dimensional probability distribution, with existing probability density function f , is known to be:\n\u00b5n =\n\u222b \u221e\n\u2212\u221e (x\u2212 c)n f(x) dx , (2.6)\nwhere the constant c might be equal, for example, to zero (e.g. for the mean, which is the first raw moment) or to the mean (e.g. for the variance, which is the second central moment).\nMoments are widely examined in statistics and have been used for a long time to estimate distribution parameters. Ideally, we would like to match infinitely many moments so as to precisely match at least some bounded distributions2. Unfortunately, in practice, we are limited, and can check only finite number of moments. This means that the statistic \u03b71 will also be insufficient. Up to some extent, we can control the precision of our ABC inference by increasing number of moments that we consider.\n(defquery lpp-normal (let\n[noise-level 0.001 N 100 prog-candidate-tuple (grammar \u2018() \u2018real) prog-candidate (extract-compound prog-candidate-tuple) samples (apply-n-times prog-candidate N \u2019())] (observe (normal (mean samples) noise-level) 0.0) (observe (normal (std samples) noise-level) 1.0) (observe (normal (skew samples) noise-level) 0.0) (observe (normal (kurt samples) noise-level) 0.0) (predict (extract-text prog-candidate-tuple))))\naim to find probabilistic programs T -s that define distributions x\u0302 \u223c p(\u00b7|T ) with particular moments. Namely, the distribution x\u0302 \u223c p(\u00b7|T ) of sought T should have a mean of zero, a standard deviation of one, a skewness of zero, and an excess kurtosis of zero as well. As previously discussed, we approximate the distribution p(\u00b7|T ) by drawing a finite number of samples (i.e. X\u0302 ) from p(\u00b7|T ).\nBy seeking probabilistic programs that produce samples from distributions with those moments, we attempt to find such programs that draw samples from a distribution that is statistically similar to the standard Normal distribution. As discussed earlier, there are several levels of approximation, as we draw only finite number of samples from the program candidate T , and as we also constrain only a finite number of moments. The way we constrain moments with the Gaussian noise kernel is also approximate."}, {"heading": "2.3.2 Use of hypothesis test statistics for \u03b71", "text": "Another possible choice of statistic \u03b71 is a hypothesis test statistic. Figure 2.2 shows pseudocode that aims to find a sampler for the Bernoulli distribution family parametrised by \u03bb with the help of G-test statistic (McDonald, 2009):\nGn,\u03bb = 2 \u2211\ni\u22080,1\n#[X\u0302n = i]ln (\n#[X\u0302n = i] \u03bbi(1\u2212 \u03bb)(1\u2212i) \u00b7 |X\u0302n|\n) ,\nwhere #[X\u0302n = i] is the number of samples in X\u0302n that take value i. We calculate G-test\u2019s p-value3 of falsely rejecting a null hypothesis H0 : X\u0302 \u223c Bernoulli(\u03bbn). This p-value is incorporated to the kernel \u03c0(F |X\u0302 ) by observing a coin that is flipped and happens to face up with the probability equal to the p-value of the test.\nAs with the previous pseudocode for learning a standard Normal distribution sampler, in this example, we sample a program candidate from the grammar and draw N\n3Which is not necessarily the probability of incorrectly rejecting the null hypothesis.\n22\n(defquery lpp-bernoulli (let\n23\nsamples X\u0302 from the program candidate. Here, our task is more complex since we aim to find a sampler for the whole family of the Bernoulli distribution that is parametrised by one-dimensional parameter \u03bb \u2208 (0, 1). This brings us to another important generalisation of our approach: we want to learn probabilistic programs, which are parametrised by inputs \u03bb and which define conditional distribution samplers. We may incorporate this into our ABC target, such that it becomes as follows:\np(\u03bb)p(T |\u03bb)p(X\u0302 |T , \u03bb)\u03c0(F |X\u0302 , \u03bb), (2.7)\nwhere p(\u03bb) is some prior over parameters. We are free to define the latter prior as we wish. We might construct that prior such that it is concentrated on regions of the parameter \u03bb that are of particular interest to us."}, {"heading": "2.4 Prior over program code", "text": "We used grammar prior that is similar to one that was introduced in our preceding work (Perov and Wood, 2014; Perov, 2014). It complements the adaptor grammar (Johnson et al., 2007) prior that is used in (Liang et al., 2010) by the use of local environments4 and type signatures.\nThe basic element in functional languages is an expression. To generate a probabilistic program we recursively apply the production rules listed below starting with exprtype, where type is the desired output signature of the inducing program. Production rules are applied stochastically with some probabilities \u2211 pi = 1 (covered in more detail later in this Section). The set of types used for our experiments is {real, 4An environment is a mapping of typed symbols to values (including such values as primitive and compound procedures), but these values are not evaluated/applied until the actual run of a probabilistic program. A compound procedure consists of formal arguments (just names) and its body, which is an expression to be evaluated. For example, compound procedure (fn [x y] (+ x y)) has two formal arguments x and y, as well as body (+ x y).\n24\nbool}. We also employ type int, which in our experiments were the derivative type of real with values rounded.\nTo avoid numerical errors while interpreting generated programs we replace functions like log(a) with safe-log(a), which returns 0 if a < 0, and uniformcontinuous with safe-uc(a, b) which swaps arguments if a > b and returns a if a = b. The general set of procedures in the global environment include +, \u2212, *, safe-div, safe-uc, cos, safe-sqrt, safe-log, exp, inc, dec.\nAn example of the production rules code, written in Anglican, is provided in Fig-\nure 2.3. Schematically our prior is defined below:\n1. exprtype | env p1\u2212\u2192 v,\nwhere variable v is a randomly chosen variable from the environment env such that it has type type. An example of a sampled program using this rule:\n(fn [my-var another-var] my-var).\n2. exprtype | env p2\u2212\u2192 c,\nwhere c is a random constant c with the type type. Constants were drawn from the predefined constants set (including 0.0, \u03c0, etc.) and from normal and uniform continuous distributions. Example:\n(fn [my-var] 0.3).\n3. exprtype | env p3\u2212\u2192 (proceduretype exprarg 1 type ... exprarg N type),\nwhere procedure is a primitive or compound, and deterministic or stochastic procedure, which is chosen randomly from the global environment with output type signature type. Examples:\n(fn [my-var] (safe-uc ... ...)).\n25\n(fn [my-var] (+ ... ...)).\n4. exprtype | env p5\u2212\u2192 (let [new-symbol exprany] exprtype | env \u222a new-symbol)),\nwhere env \u222a new-symbol is an extended environment with a new variable named new-symbol (each time a symbol name is unique, e.g. generated by Lisp\u2019s gensym). The value of the new variable is defined by an expression, which is generated according to the same production rules. The variable has fixed but arbitrary type (i.e. type \u201cany\u201d), which is chosen randomly from the set of employed types (i.e. real, bool or int). Examples:\n(fn [my-var] (let [x (safe-uc -1 1)] (+ x x))).\n(fn [my-var] (let [x (cos (safe-uc -1 1))] ...)).\n(fn [my-var] (let [x (safe-uc -1 1)] (+ x my-var))).\n5. exprtype | env p4\u2212\u2192\n(let [new-symbol (fn formal-args exprany | env \u222a formal-args}] exprtype | env \u222a new-symbol), where formal-args is the list of unique, previously not used, symbol names. The body of the compound procedure is generated using the same production rules, given an environment that incorporates variables formal-args. After the compound procedure is defined in the environment, it might be used in the body of the let. Possible general examples:\n(fn [my-var] (let\n[cp1 (fn [x] (* x 2.3))] (cp1 (cos my-var))))\n26\n(fn [my-var] (let\n[cp1 (fn [x] (* x 2.3))] (cp1 (cp1 my-var))))\n6. exprtype | env p6\u2212\u2192 (if (exprbool) exprtype exprtype).\n7. exprtype | env p7\u2212\u2192 (recur exprarg 1 type ... exprarg M type),\ni.e. recursive call to the current compound procedure if we are inside it, or to the main inducing procedure, otherwise. Possible example:\n(fn [val] (if (= val 1)\n1 (* val (recur (- val 1)))))\n27\n28\nPrior probabilities {pi}were automatically extracted from a small corpus of sampler source code written in Anglican language. The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998). The corpus is provided in Appendix A. The prior was employed in our experiments in a manner similar to cross-validation. For example, when we were learning a sampler for the standard Normal distribution, we held out the source code for the standard Normal distribution and the general Normal distribution from the corpus. In addition, our prior was smoothed by Dirichlet priors.\nTo ensure that program candidates terminate, we allow only 10 nested self-recursive\ncalls. If that limit is reached, a procedure deterministically returns 0.0.\nFigure 2.4 illustrates how probable some of the sought probabilistic programs are given our production rules. Figure 2.5 illustrates the flexibility of our prior over code for one-dimensional samplers. It shows samples from some random probabilistic programs, which were sampled from the grammar prior."}, {"heading": "2.5 Experiments", "text": "The initial experiments had been described in our prior work (Perov, 2014; Perov and Wood, 2014). In the current work we show in Section 2.5.1 that our approach is comparable to evolutionary algorithms, a common method for program synthesis. Then we re-implement our method in new probabilistic programming systems, namely Anglican and Probabilistic Scheme. This provided us with a ten-fold improvement in speed, as reported in Section 2.5.2. Finally, in Section 2.5.3, we report new experimental results, similar to (Perov, 2014; Perov and Wood, 2014), but with thinner binning.\n29\n30"}, {"heading": "2.5.1 Evaluation of our approach versus evolutionary algorithms", "text": "Our approach was evaluated against genetic programming (Koza, 1992), one of stateof-the-art methods to search in the space of programs. Genetic programming is an evolutionary based metaheuristic optimisation algorithm that is used to generate a program given the specification. For a recent introduction into the field of genetic programming, see (Poli et al., 2008). The very similar grammar, which we described in Section 2.4, was reproduced in the evolutionary computation framework DEAP (Fortin et al., 2012) written in Python. The fitness function was selected as the log probability presented in the Equation 2.5 with the p ( X\u0302 | T ) term omitted, in accordance with the assumption that sought probabilistic programs will repeatedly appear in the results of search over programs. An alternative would be to marginalise over X\u0302 . However, that requires more program runs and is, therefore, more computationally expensive.\nWe used the DEAP framework to generate individual (program) code. Then, \u201cindividuals\u201d (i.e. program code candidates in genetic programming vocabulary) were evaluated in Anglican. We decided to use Anglican for this, and not Python, because functional-style let construction is not naturally supported in Python. Finally, evaluation results (log probabilities) were reported back to DEAP to generate and select individuals for the next generation. We had 100 individuals per generation and used DEAP set-up for the strong typed genetic programming optimisation.\nFigure 2.6 shows that PMCMC inference performance is similar to genetic programming. In contrast to genetic programming, PMCMC is a statistically valid estimator of the target distribution. In addition, the probabilistic programming system allows reasoning about the model over models and the inference of models within the same framework, while genetic programming is an external machinery which considers the\n31\n103 104 105\nProgram runs\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n\u03c0 ( X |X\u0302 )\n\u00d71011\n103 104 105\nProgram runs\n\u22128 \u22127 \u22126 \u22125 \u22124 \u22123 \u22122 \u22121\n0\n\u03c0 ( X |X\u0302 )\n\u00d71016\n103 104 105\nProgram runs\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n\u03c0 ( X |X\u0302 )\n\u00d71011\nFigure 2.6: Convergence of unnormalised penalty function \u03c0 ( X | X\u0302 ) for Bernoulli(p), Normal(\u00b5, \u03c3), and Geometric(p) correspondingly. X\u0302 is a samples set from a probabilistic program T as described in Section 2.2. Navy lines show the true sampler\u2019s penalty function value (averaged by 30 trials), red lines correspondent to genetic programming, and green lines \u2013 to PMCMC. Transparent filled intervals represent standard deviations within trials. We ran a smaller number of genetic programming runs because their evaluation took more time in our set-up.\noptimisation of probabilistic programs in a black box way."}, {"heading": "2.5.2 Engines comparison", "text": "Figure 2.7 shows a speed comparison between different probabilistic programming engines: Anglican (Tolpin et al., 2015b)5, Interpreted Anglican (Wood et al., 2014)6 and Probabilistic Scheme (Paige and Wood, 2014). Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al., 2014).\nInterpreted Anglican engine is written in Clojure and interprets Anglican code. Anglican engine is written in and integrated with Clojure. It treats control structures and translates them to Clojure code. Thus, Clojure is a compilation target for Anglican. Probabilistic Scheme engine is based on Scheme compiler \u201cStalin\u201d 7, which is written in C, with included Probabilistic C (Paige and Wood, 2014) library. Probabilistic program Scheme code is thus a compilation target for Probabilistic Scheme compiler that\n5https://bitbucket.org/probprog/anglican 6https://bitbucket.org/probprog/interpreted-anglican 7https://en.wikipedia.org/wiki/Stalin_(Scheme_implementation)\n32\nis enhanced by Probabilistic C.\nInterpreted Anglican is the only amongst those to support eval. There is thus an additional graph \u201cInt-Ang-Eval (smc)\u201d that shows the time complexity for not generating nested compound procedures, but instead generating full Anglican program text and then evaluating this text to a compound procedure. We prefer the former approach of generating nested compound procedures, since it is supported in more probabilistic programming systems and it provides an opportunity for inference optimisations in the future. For example, one might think of doing local proposals on the internal sub-body of the probabilistic program candidate body. An illustration of such potential proposal is that the following program\n(fn [...] ... (+ a (sin b)) ...)\nmight be changed, with the probability of the proposal, according to the inference kernel (e.g., MH or Gibbs), according to the generative model and given the observed distribution of interest, to\n(fn [...] ... (- b a) ...).\n33\nNew probabilistic programming systems, Anglican and Probabilistic Scheme, made the process of learning probabilistic programs at least 10 times faster. This is a significant constant-factor improvement, especially taking into the consideration the fact that the space over probabilistic program code is complex. In addition, the approach runs faster in Probabilistic Scheme, most probably because it is compiled to C++. For our latest experiments we decided to employ Anglican, as it has been easier for us to develop and especially debug our experiments in Clojure."}, {"heading": "2.5.3 Learning sampler code", "text": "Given the improvement in speed performance, we were able to reproduce the initial experiments much faster and report results with better accuracy. In particular, in Figure 2.8 we show results of learning sampler program code for six common one-dimensional distributions Bernoulli(p), Poisson(\u03bb), Gamma(a, 1.0), Beta(a, 1), Normal(0, 1), Normal(\u00b5, \u03c3). As before, we marginalised over the parameter space with a small randomly composed set of \u03bb1, . . . , \u03bbS . Figure 2.9 shows repeated experiments for learning independent onedimensional samplers that aim to match arbitrary one-dimensional real world empirical data from a credit approval dataset8 (Quinlan, 1987; Bache and Lichman, 2013).\n8We used continuous fields A2, A3, and A8. There were a bit more than 650 data points for each dimension. See https://archive.ics.uci.edu/ml/ machine-learning-databases/credit-screening/crx.names for details.\n34\n35"}, {"heading": "2.5.4 Example of learning a standard Normal sampler", "text": "We illustrate the process of learning a sampler for the standard Normal distribution by providing a few examples of probabilistic program text from the Markov chain that targets the posterior for such sampler. In Figure 2.10 we provide samples that are generated by evaluating those probabilistic programs. This Section provides program text for each corresponding subplot in order: left to right, top to bottom. One of the first inferred program, from the posterior over program text, is a program that always deterministically returns 0.0. This very short program text has a very high probability given the production rules, and it precisely matches the first moment, i.e. the mean:\n1 (lambda (stack_level) 0.0)\nNext sensible approximations, from the Markov chain, are probabilistic programs\nthat sample from an uniform continuous distribution with fixed bounds:\n1 (lambda (stack_level) (safe-uc -1.0 1.0))\n1 (lambda (stack_level) (safe-uc -2.0 (+ 3.14159 -1.0)))\n1 (lambda (stack_level) 2 (begin 3 (define G__3352 -1.0) 4 (safe-uc G__3352 (+ 1.0 (safe-div 0.0 0.0)))))\nFinally, the chain converges to more complex and more precise approximations to a\nstandard Normal distribution sampler:\n1 (lambda (stack_level) 2 (+ (safe-uc (+ -2.0 3.14159) (inc -1.0)) (safe-uc 1.0 -2.0)))\n36\n1 (lambda (stack_level) 2 (* (begin (define G__56510 3 (safe-div (safe-uc 1.0 (begin (define G__56511 4 (safe-uc 1.0 (safe-log 0.0))) G__56511)) 1.0)) 5 (safe-uc -2.0 (exp G__56510))) 6 (safe-sqrt (safe-uc 0.0 1.0))))\n1 (lambda (stack_level) 2 (safe-uc (safe-uc 0.0 (exp 1.0)) (* -1.0 1.0)))\n1 (lambda (stack_level) 2 (* (begin (define G__56510 (safe-div 3 (safe-uc 1.0 (begin (define G__56511 4 (safe-uc 1.0 (safe-log 0.0))) G__56511)) 1.0)) 5 (safe-uc -2.0 (exp G__56510))) 6 (safe-sqrt (safe-uc 0.0 1.0))))\n37\n38"}, {"heading": "2.6 Outline of a motivating example for the scientific re-", "text": "discovery in classical genetics\nOur approach allows us to automatically infer generative models. One of motivations for such inference is to induce laws of nature. As an example, we outline a set of experiments that aim to scientific rediscovery of laws of classical genetics. These laws might be induced in the form of probabilistic programs. The proposed experiments are designed to follow Mendel\u2019s original experiments (Mendel, 1985). In this thesis we only outline the potential set of experiments, thus leaving the experiments themselves as potential future work by ourselves or by others."}, {"heading": "2.6.1 Aims of the proposing experiment", "text": "We aim to induce three laws of classical genetics in the form of probabilistic programs (Wikipedia, 2015):\n1. Law of Segregation: (a) \u201can individual contains a pair of alleles for each particular\ntrait which segregate or separate during cell division for this trait\u201d, (b) and \u201ceach parent passes a randomly selected copy (allele) to its offspring\u201d.\n2. Law of Dominance: \u201cthere is a notion of dominant and recessive alleles, and a\nrecessive allele is always masked by a dominant allele\u201d.\n3. Law of Independent Assortment: \u201cseparate genes for separate traits are passed\nindependently of one another from parents to offspring\u201d."}, {"heading": "2.6.2 Brief introduction to Mendel\u2019s work", "text": "Mendel designed and conducted his experiments in plant hybridisation, aiming to find \u201ca generally applicable law governing the formation and development of hybrids\u201d. He\n39\npresented his results in his classic paper \u201cExperiments in Plant Hybridisation\u201d (1865) (Mendel, 1985). His well-known paper contains most of his experimental results in aggregated form that we can use as observations.\nThe parts of his work, which might be relevant for the possible scientific rediscovery\nof classical genetics laws, can be briefly described as follows9:\n1. He carefully considered requirements on what plants to use. In Section 2 he\ndescribes that he chose the species Leguminosae, in particular the genus Pisum.\n2. He selected 22 varieties of this genus. A variety is \u201ca group of organisms that\nare members of the same species, and have certain characteristics in common that are not shared by all members of the species\u201d. Plants from different varieties are differentiable by several characteristics, but they can still be hybridised with each other since they relate to the same genus.\n3. He first cultivated these varieties independently in order to verify that within the\nvariety, plants remain constant without exceptions. This is quite important, since this is also the part of experiments that would need to be reproduced. He does not provide any data for this part of his experiments, and thus it is necessary to guess as to how many plants and for how many generations he cultivated varieties independently.\n4. In Section 3 he defines 7 pairs of strongly differentiating characteristics between\nvariates. He decides to consider these differentiating characteristics as binary (e.g. the difference in the form of the ripe pods is strongly distinguished to be \u201ceither simply inflated, not contracted in places; or they are deeply constricted between\n9To the best of my understanding.\n40\nthe seeds and more or less wrinkled\u201d). He supports this decision by the evidence that there is no transition forms both in original varieties and in hybrids.\n5. For each pair, he fertilises varieties with different observable characteristics, with\neach other. \u201cEach of the two varieties which in one set of fertilizations served as seed-bearer in the other set was used as the pollen plant.\u201d In this way he produces hybrids.\n6. In Section 4 he elaborates on the forms of hybrids which he obtained. He defines\nthe terms \u201cdominant\u201d and \u201crecessive\u201d characteristics, by noting that the recessive characteristics always \u201cwithdraw or entirely disappear in the hybrids, but nevertheless reappear unchanged in their progeny\u201d. For each pair of characteristics he identifies what characteristic is the dominant of the two. He does not provide much data for this part of his experiments, and thus it is again necessary to guess how many hybrids he examined for each of trait.\n7. In Section 5 he reports on a first generation derived from hybrids (so it is the\nsecond generation from original varieties). He reports the average proportion of 3:1 for the dominant and recessive characteristics respectively. He provides aggregate data for each pair of characteristics.\n8. In Section 6 he reports that \u201cthose forms which in the first generation exhibit the\nrecessive characteristic [i.e. one third] do not further vary in the second generation as regards this characteristic; they remain constant in their offspring\u201d. \u201cOf these [other] two-thirds yield offspring which display the dominant and recessive characteristics in the proportion of 3:1, . . . \u201d.\n9. In Section 7 he reports that the subsequent generations from the hybrids follow\n41\nthe same rule as that discovered in Sections 5 and 6. In Section 8 he conducts experiments on 2 or 3 different characteristics together. He reports that \u201cthe relation of each pair of different characteristics in hybrid union is independent of other differences in the two original parental stocks\u201d. In Section 9 he examines the reproductive cells of the hybrids."}, {"heading": "2.6.3 Abstractions to induce", "text": "The aim of the experiment is to induce three probabilistic procedures that represent (a) an abstraction of a plant, (b) an abstraction of an observable feature of a plant, and (c) an abstraction of plants hybridisation. Below we elaborate on them, and briefly describe how these abstractions together can be interpreted as the three laws of Mendelian genetics."}, {"heading": "2.6.4 Inducing a probabilistic procedure", "text": "For the purpose of inducing probabilistic program code we refer to an abstract procedure sample-probabilistic-procedure that randomly generates a probabilistic program by sampling from some grammar over program text, which is similar to the one described in Section 2.4. This procedure takes two arguments:\n1. The list of input signatures of the inducing probabilistic procedure.\n2. The output signature of the inducing probabilistic procedure.\nFor example, if we want to induce program code of a probabilistic procedure which takes three real values and which returns the list of two lists both containing the pair of integers, we need to call the procedure sample-probabilistic-procedure as follows:\n42\n(define new-program (sample-probabilistic-procedure\n(list \u2018real \u2018real \u2018real) (list (list \u2018int \u2018int) (list \u2018int \u2018int))))\nBelow are provided two possible outcomes for new-program sampled from sample-\nprobabilistic-procedure:\n(lambda (a b c) (list\n(list a (+ a b)) (list b (safe/ a c))))\n(lambda (a b c) (list\n(list (normal b c) (+ a a)) (list (safe-uniform-continuous a b) 3)))\nThis is similar to the procedure grammar in Section 2.3.1."}, {"heading": "2.6.5 An abstraction of an individual plant", "text": "Firstly, we assume that an individual plant is just some data. In other words, the assumption is that to describe a real natural object it is enough to represent this object in the form of finite data. We need to induce a produce produce-random-individual that produces a random individual plant (like a factory). This procedure takes no arguments and returns an unknown, but some specific data structure. It is specific since we assume that all our plants and their offspring belong to a single species, even though they may belong to different varieties of this species:\n43\n(define produce-random-individual (induce-probabilistic-procedure\n\u2019() ; No inputs. \u2019any ; Any, but fixed output signature.\n))\nBy calling the procedure produce-random-individual we sample individ-\nual plants:\n(define plant-1 (produce-random-individual))\n(define plant-2 (produce-random-individual))\nBy an abstraction of a plant, we mean its entire life cycle, including time spent as a\nseed before it actually grows and becomes a real plant."}, {"heading": "2.6.6 An abstraction of an observable plant feature", "text": "Secondly, we need to have a probabilistic procedure get-feature that, based on an individual plant\u2019s data, samples an observable feature. This procedure should take an individual, and return a feature. Following Mendel\u2019s work, we work with strongly distinguishable binary features (like the colour is only purple or white). This means that we can assume that the output signature of the inducing procedure to be boolean:\n44\n(define individual-data-signature (get-output-signature produce-random-individual)) (define get-feature (induce-probabilistic-procedure\n(list individual-data-signature) ; One input: ; the individual data. \u2019bool ; The output is boolean. ; We rely on the fact that we work with ; a highly differentiable characteristic which ; does not have any gradient: ; it is just one or another.\n))\nThe procedure get-output-signature is just a language helper function that\nreturns the output signature of any procedure."}, {"heading": "2.6.7 An abstraction of the plant hybridisation process", "text": "Thirdly, we need to have a probabilistic procedure which hybridises plants, such that a new plant is obtained. We aim to induce two fully independent probabilistic procedures: the procedure produce-hybrid-sexually and the procedure produce-hybridasexually:\nProcedure produce-hybrid-sexually takes as an input two individual plants,\nand produces a new plant:\n(define produce-hybrid-sexually (induce-probabilistic-procedure\n(list (output-signature produce-random-individual) (output-signature produce-random-individual)) (output-signature produce-random-individual)))\nProcedure produce-hybrid-sexually takes as input an individual plant, and\nproduces a new one:\n45\n\\begin{lstlisting} (define produce-hybrid-asexually\n(induce-probabilistic-procedure (list (output-signature produce-random-individual)) (output-signature produce-random-individual)))"}, {"heading": "2.6.8 Procedures that are expected to be learnt", "text": "Using our current understanding of classic genetics, in Figures 2.11, 2.12, 2.13 and 2.14 we provide below the source code for probabilistic programs which could be expected to induce the basic laws of classical genetics. That is, we provide source code for four probabilistic programs: produce-random-individual, get-feature, produce-hybrid-sexually, produce-hybrid-asexually. For simplicity, we deal with only one trait (genetically determined characteristic).\n(define produce-random-individual (lambda ()\n(define get-feature (lambda (me)\n46\n(define produce-hybrid-sexually (lambda (pollen-parent egg-parent)\n(define produce-hybrid-asexually (lambda (parent)\n(produce-hybrid-sexually parent parent)))\nFigure 2.14: Interpretation: \u201casexual hybridisation is similar to a sexual one\u201d.\nTo prove the Law of Independent Assortment (i.e. that \u201cseparate genes for separate traits are passed independently of one another from parents to offspring\u201d) we need to consider several traits at once in the same way as Mendel did in his experiments as described in Section 8 of his paper.\nIn the proposed experiments, the same order might be followed as in Mendel\u2019s work.\nThis means that experiments are started for different traits independently.\nThe experiments that are proposed in this Section may constitute the future work in\nthe direction of learning probabilistic programs."}, {"heading": "2.7 Conclusion", "text": "In this Chapter we described an approach to learning samplers in the form of program code within the framework of probabilistic programming. The initial results have been\n47\nencouraging for the future work on automatically learning problem-specific generative models given data and just a few examples.\nAlthough we were able to reproduce the inference of the precise sampler code for the Bernoulli family distribution, we were only able to find approximate samplers for other one-dimesional families like the Normal or Gamma distributions. Better inference techniques and hierarchical/cumulative learning methods (Henderson, 2010; Dechter et al., 2013) are essential to finding more complex probabilistic programs, including human-interpretable programs as in Appendix A that are theoretically in the prior of our grammar but were not identified during our experiments. Better inference is especially important for the automatic induction of probabilistic programs that are internally conditioned on observations (i.e. to automatically induce problem-specific generative models that are similar to, for example, hidden Markov or latent Dirichlet allocation models; or even more sophisticated probabilistic problems that aim to describe our world and agents in it, for example as in (Stuhlmu\u0308ller, 2015) and in (ForestDB, 2016)). Ultimately, it should be fruitful to add higher-order types into the grammar (like function < int >, function < real >, function < \u2217 >, where \u2217 is a wild-card, in addition to int, real, bool, etc.), such that the grammar procedure is capable of producing itself (in other word, the grammar procedure code has a non-zero probability under the same grammar). By doing so, we will be able to receive an agent, in the form of a probabilistic program, that can both solve tasks and produce more advanced agents.\nWe appreciate as well that our results on finding one dimension representations of unknown distributions are not capable of outperforming the state-of-the-art methods for density estimation and related problems. The goal of the research described in this Chapter was to make an initial step towards learning probabilistic programs to describe potentially interpretable and composable generative models of the world.\n48\nWe also outlined a set of experiments concerning classical genetics that may constitute future work on applying the approach of learning probabilistic programs for scientific rediscovery and, eventually, discovery.\nIn addition, while we briefly showed that within the set of basic experiments we have received similar results to the optimisation-based method of genetic programming, the real applications of the program induction and synthesis would most probably benefit from the combination of statistical methods, such as MCMC, and optimisation methods, such as evoluationary algorithms or stochastic search methods. Finally, inference over program code should also benefit from better proposals using discriminative models (e.g. as in (Karpathy et al., 2015), and potentially in combination with (Reed and de Freitas, 2016)), similar to ones that are described in the following Chapter.\n49\nChapter 3\nData-driven proposals in probabilistic programming\nAs discussed in Chapter 1, to make probabilistic programming more efficient and widely used by the machine learning community, we need to enhance the quality and speed of statistical inference. This is also crucial for both existing and new applications, including our work on learning probabilistic programs. This section describes a way to facilitate sequential Monte Carlo inference with data-driven proposals using discriminative models.\n3.0.1 Proposals in sequential Monte Carlo inference\nIn applications of sequential Monte Carlo inference, including probabilistic programming systems, the initial proposal distribution q(xt|xs1:t\u22121, y1:t) for the particle s is often equal to the prior distributions p(xt|xs1:t\u22121, y1:t\u22121) of the generative model. This is known as a \u201cgenerate and test\u201d approach (Murphy, 2012) since we just sample values xt from the generative model and only then evaluate how good they fit a data point yt. Another name for this approach is \u201cbootstrap particle filter\u201d (Gordon et al., 1993). This approach does not require the specification of any additional parameters, and, therefore, it is convenient. The posterior is usually, however, different from the generative model\n50\nprior, and thus the convergence of this simple \u201cgenerate and test\u201d approach can be quite slow.\nIt is important to note that sample spaces for xt, as well as corresponding probability measures for both conditional distributions p(xt|xs1:t\u22121, y1:t\u22121) and q(xt|xs1:t\u22121, y1:t), are not trivial because number of generated random variables in xt is not fixed, and may even be unbounded. For example, consider a probabilistic procedure\n(lambda (w) (if (sample (flip w)) 0 (+ 1 (recur))))\nthat samples from a geometric distribution with parameter w. In addition, even the type and structure of random variables in xt might be not fixed. Because of that, in this work we consider only such proposal distributions q that have the same structure of random choices as p does.\nTo accelerate inference, we would like to take into the account the data yt so that\nq(xt|xs1:t\u22121, y1:t) = p(xt|xs1:t\u22121, y1:t) = p(yt|x1:t, y1:t\u22121)p(xt|xs1:t\u22121, y1:t\u22121)\np(yt|xs1:t\u22121, y1:t\u22121) .\nThis proposal is called \u201ca fully-adapted proposal\u201d. When we use this proposal, the\nnew weight\nwst \u221d wst\u22121p(yt|xs1:t\u22121, y1:t\u22121) = wst\u22121 \u222b p(yt|x\u20321:t, y1:t\u22121)p(x\u2032t|xst\u22121, y1:t\u22121)dx\u2032t. (3.1)\nThis is the optimal proposal because for any given xst\u22121 the new weight w s t will have the same value independently of the value of xst (Murphy, 2012). This means that, conditional on the old values x\u00b7t\u22121, the variance of weights w \u00b7 t is equal to zero.\n3.0.2 Using a discriminative model for data-driven proposals\nIn general, it is intractable to calculate the integral in Equation 3.1 and therefore sample from p(xt|\u03c1t) directly, where \u03c1t is an environment (xs1:t\u22121, y1:t). However, we can\n51\napproximate this distribution with another distribution q(xt|\u03b7), which is parametrised by unknown parameters \u03b7(\u03c1t). Such proposals are referred to as \u201cdata-driven\u201d. We use a discriminative model N to map features of the environment \u03c6(\u03c1t) to parameters \u03b7. The aim of training the discriminative model is to bring q(xt|\u03b7 = N(\u03c6(\u03c1t))) as close as possible to p(xt|\u03c1t). For now, let us also assume that t is fixed and we are learning a proposal for some specific xt.\nTo train the discriminative model, we need M pairs of training inputs {\u03c6(\u03c1t)}j and related outputs {xt}j such that each xt is drawn from the desired distribution p(xt|\u03c1t) using some \u201capproximate\u201d Monte Carlo sampling method, e.g. SMC or PMCMC. We call Monte Carlo methods \u201capproximate\u201d since we are never capable of having infinite number of particles in SMC or infinite number of MCMC iterations in PMCMC.\nThere are at least two approaches to obtaining these training pairs. The first is the unconditional offline simulation from the generative model, by sampling and capturing both latent variables xt and observations yt. The second approach is to run a thorough offline sequential Monte Carlo inference on some number of training episodes, and then to capture the values xt from the estimated filtering distribution p\u0303(xt|\u03c1t) = \u2211\nsws\u03b4xst (x s t).\nRecall that our aim is to provide a good approximation to p(xt|\u03c1t) in the form of q(xt|\u03b7), which is parametrised by the discriminative model output \u03b7. The choice of distribution type for q(xt|\u03b7) is flexible. As mentioned, it should have the same structure of random choices as the prior p(xt|\u03c1t). In addition to being able to sample from it, we have to be able to calculate sample\u2019s normalised likelihood. As for the choice of a discriminative model, one option is to use feedforward neural networks:\n\u03b7 = N\u03b8(\u03c6(\u03c1t)),\n52\nwhere \u03b8 are parameters of the neural network to learn. Here the values of \u03c6(\u03c1t) constitute the input layer, and the parametrisation \u03b7 of q(xt|\u03b7) constitutes the output layer of the neural network. Lastly, \u03b8 are parameters of the neural network we need to learn.\nWe look for such \u03b8 that q(xt|N\u03b8(\u03c6(\u03c1t))) is close to p(xt|\u03c1t). Hence, the loss function Lt(\u03b8) should depend on a measure of the difference between these two probability distributions. Specifically, we use the Kullback-Leibler divergence\nDKL(p(xt|\u03c1t) || q(xt|N\u03b8(\u03c6(\u03c1t)))) = Ep(xt|\u03c1t) [log p(xt|\u03c1t)\u2212 log q(xt|N\u03b8(\u03c6(\u03c1t))))] .\nLet us highlight that by we need to maximise the similarity of the whole the family of distributions q(xt|N\u03b8(\u03c6(\u03c1t))) to another family p(xt|\u03c1t), not just the similarity of one distribution to another. This means we have to take another expectation over parameters \u03c1t. Therefore, the loss function Lt(\u03b8) approximates two nested expected values:\nLt(\u03b8) \u2248 Ep(\u03c1t) [DKL(p(xt|\u03c1t) || q(xt|N\u03b8(\u03c6(\u03c1t))))]\n= Ep(\u03c1t) [ Ep(xt|\u03c1t) [log p(xt|\u03c1t)\u2212 log q(xt|N\u03b8(\u03c6(\u03c1t))))] ] = Ep(\u03c1t,xt) [log p(xt|\u03c1t)\u2212 log q(xt|N\u03b8(\u03c6(\u03c1t))))] .\nIgnoring the first term since it is a constant w.r.t. \u03b8:\nLt(\u03b8) \u2248 \u2212Ep(\u03c1t,xt) [log q(xt|N\u03b8(\u03c6(\u03c1t))))] .\nIn practice, we do not integrate over all possible observations, but instead consider\na specific set of training observation values, such that\nLt(\u03b8) \u2248 \u2212Ep(\u03c1t,xt|y1:t) [log q(xt|N\u03b8(\u03c6(\u03c1t))))] .\nOnce we replace the expectations by their approximation, the loss function for the\ndiscriminative model becomes as follows:\nLt(\u03b8) = \u2212 M\u2211\nj=1\nws(j) log q(xt = x s(j) t | \u03b7 = N\u03b8(\u03c6(xs(j)1:t\u22121, y1:t))), (3.2)\n53\nwhere values x\u2217\u2217 and weights w\u2217 are obtained by exhaustive inference runs (e.g., SMC).\nIn many models xt and yt are homogeneous. Therefore in such models we can learn\none q(\u00b7|\u03b7) for all t. This means that L(\u03b8) =\u2211t Lt(\u03b8).\nSince our proposal q(xt|\u03b7) will only be an approximation to p(xt|z1:T , y1:T ), we do\nnot set the weight wst \u221d wst\u22121p(yt|xst\u22121). Instead, we calculate the full equation:\nwst \u221d p(yt|xs1:t\u22121, y1:t\u22121)p(xst |xs1:t\u22121, y1:t\u22121)\nq(xt|\u03b7) .\nThis should not be a problem since we assume that we know the likelihood function of q(xt|\u03b7).\nFinally, in practice, we may use other distributions to approximate the filtering distribution p(xt|\u03c1t). For example, we may use the approximation of the smoothing distribution p(xt|z1:T , y1:T ). This is as most of statistical inference in existing probabilistic programming systems is directed towards the approximation of the smoothing distribution.\n3.0.3 Experiments with the linear Gaussian model\nAs a basic proof of concept, we test the approach on a linear Gaussian model with the following parameters:\nx0 \u223c Normal(0, 2), xi \u223c Normal(xi\u22121, 0.12),\nyi \u223c Normal(xi, 0.12).\nFor this experiment, identity features \u03c6(\u03c1t), provided as input to the neural network, are the values of last 10 latent states xt\u221210, . . . , xt\u22121 and last 10 observed values of yt\u22129, . . . , yt. The neural network has therefore 20 input nodes, with a 25 node hidden\n54\nlayer. The proposal distribution q(xt | . . .) \u223c Normal(xt | (\u00b5, \u03c32) = N\u03b8(\u03c6(\u03c1t))) is the Normal distribution with two unknown parameters \u00b5 and \u03c3, and hence there are 2 nodes on the output layer, one for each of those.\nWe use the sigmoid function to connect the input layer to the hidden layer, and use the identity and exponential functions to connect the hidden layer with the output nodes \u00b5 and \u03c3.\nTo train the neural network, we follow the second approach described above, thus producing several synthetic training and test episodes. By an episode we just mean here a dataset {y1, . . . , yT}. While we run SMC inference with 100 particles on training episodes, we capture the values xt from the estimated smoothing distribution. Initial training episodes\u2019 observations and estimated latent variables will exactly constitute all the necessary training data for our neural network. Details on how we generate data for episodes are provided in the next subsection.\n3.0.3.1 Functions to generate training and test episodes {y1:T}\nTo generate data for our training and test episodes we use following periodic functions:\n1. Step training functions: square(t), square(t - pi/6), square(t +\npi/6), square(t - pi/4), square(t + pi/4), square(t + pi/3), square(t - pi/3), square(t + pi/2), square(t - pi/2). Function square(t) is a MATLAB square wave function1 which is similar to sin(t), but instead of a sine waves it creates peaks of \u00b11.\n2. Smooth training functions: sin(t - pi/6), sin(t + pi/6), sin(t -\npi/4), sin(t + pi/4), sin(t - pi/3), sin(t + pi/3), sin(t - 1See details by the link: http://uk.mathworks.com/help/signal/ref/ square.html.\n55\npi/2), sin(t + pi/2).\nTest functions are similar to train functions, but have different offsets:\n1. Step test functions: square(t - 1), square(t + 1), square(t - 2),\nsquare(t + 2).\n2. Smooth test functions: sin(t - 1), sin(t + 1), sin(t - 2), sin(t\n+ 2).\nEach episode is a list of outputs of these functions with argument t uniformly dis-\ntributed in the range [1, 100] with the step size of \u03b4 = 0.5. The Gaussian noise Normal(0.0, 0.12) is added to the values. A subset of the training and test episodes is given in Figure 3.1 for illustration purposes.\n56\nTi m e 0 10 20 30 40\n50 60\n70 80\n90 10\n0\nx - 1.\n5-1 -0 .500. 51 1. 5\nTr ai\nn ep\nis od\nes\nsq ua\nre (t)\nsq ua\nre (t\n- p i/6 ) si n( t) si n( t - p i/6 )\nTi m e 0 10 20 30 40\n50 60\n70 80\n90 10\n0\nx - 1.\n5-1 -0 .500. 51 1. 5\nTe st\ne pi\nso de\ns\nsq ua\nre (t\n- 1 )\nsq ua\nre (t\n+ 1)\nsi n(\nt - 1 ) si n( t +\n1 )\nFi gu\nre 3.\n1: Su\nbs et\nof tr\nai ni\nng an\nd te\nst fu\nnc tio\nns .\n57\n3.0.3.2 Comparing sequential Monte Carlo runs without and with data-driven proposals\nSubfigures 3.2a and 3.2b show, on the left, average errors per latent state running SMC continuously with 100 particles on training episodes\u2019 data. By continuously, we mean that once SMC has been run on the episode, we additionally train the neural network on the data from the estimated smoothing distribution. To take account of all previous episodes, we incrementally add training data for the neural network after each episode. In addition, once SMC for the episode has been run and new training data has been added, we start neural network learning algorithm with parameters learnt after the previous episode. On their right, Subfigures 3.2a and 3.2b show average errors per latent state if we run SMC on test episodes with 10 particles with and without neural networks proposals. We run SMC for each episode independently. When we use neural network proposals, we use them with probability p = 0.7; that is, with probability p = 0.3 we still sample from the model prior. Experiments were repeated five times. Bars show standard deviations of average errors per latent state.\nThese basic experiments show that data-driven proposals with a disriminative model may significantly improve sequential Monte Carlo, especially when the prior distribution of the model is misspecified (the variance of xi \u223c Normal(xi\u22121, 0.12) was purposefully quite low for the provided training and test data).\n58\nEp is\nod e\n0 2\n4 6\n8 10\nAverage error per latent state\n0 0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 8 0. 9\nTr ai\nn ep\nis od\nes\nN ot\nd at\nadr\niv en\nD at\nadr\niv en\n(a lp\nha =\n0 .5\n)\n0 2\n4 6\n8 10\nAverage error per latent state\n0 0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 8 0. 9\nTe st\ne pi\nso de\ns\nN ot\nd at\nadr\niv en\nD at\nadr\niv en\n(a lp\nha =\n0 .7\n)\n(a )T\nra in\nin g\nth e\ndi sc\nri m\nin at\niv e\nm od\nel on\nly on\nst ep\ntr ai\nn fu\nnc -\ntio ns\n(l ef\nt) ,a\nnd te\nst in\ng it\non al\nlt es\ntf un\nct io\nns (r\nig ht\n).\nEp is\nod e\n0 2\n4 6\n8 10\nAverage error per latent state\n0 0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 8 0. 9\nTr ai\nn ep\nis od\nes\nN ot\nda ta\n-d riv en D at adr iv en (a\nlp ha\n= 0\n.5 )\n0 2\n4 6\n8 10\nAverage error per latent state\n0 0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7 0. 8 0. 9\nTe st\ne pi\nso de\ns\nN ot\nd at\nadr\niv en\nD at\nadr\niv en\n(a lp\nha =\n0 .7\n)\n(b )\nTr ai\nni ng\nth e\ndi sc\nri m\nin at\niv e\nm od\nel on\nly on\nsm oo\nth tr\nai n\nfu nc\ntio ns\n(l ef\nt) ,a\nnd te\nst in\ng it\non al\nlt es\ntf un\nct io\nns .(\nri gh\nt)\nFi gu\nre 3.\n2: A\nve ra\nge er\nro rs\npe rl\nat en\nts ta\nte .\n59\n3.0.4 Applying the approach to the DDPMO model\nBasic experiments with the linear Gaussian model have shown promising results. We would like to verify the approach on a more complex model. It would be also helpful to generalise our approach so that it may be easily applied to other models, which are written in one of probabilistic programming languages. For our further experiments we chose a dependent Dirichlet Process mixture of objects (DDPMO) model (Neiswanger et al., 2014). This is a recent Bayesian non-parametric model for detection-free tracking and object modeling. The model is based on a generalised Po\u0301lya urn (GPU) for timevarying Dirichlet process mixtures (Caron et al., 2007).\nThe DDPMO models the position and colour yt,n of a foreground pixel n in a video frame t as an observed variable. This observed variable yt,n depends on the latent variables of the model, such as cluster assignments ct,1:Nt and object parameters \u03b8kt for each cluster k. The DDPMO is a native Bayesian non-parametric model, since the number of clusters and the related object parameters is unbounded and dependent on the observed data. The generative process of the DDPMO is described in (Neiswanger et al., 2014). The comparison of the object recognition and tracking performance of Bayesian statistical inference in the DDPMO model against the performance of some others state-of-the-art models and methods (not necessarily Bayesian) is also provided in (Neiswanger et al., 2014).\n3.0.4.1 DDPMO model in Anglican\nWe have expressed the DDPMO model as a 190 line Anglican program, and the GPU as another 75 line program. The GPU code may be reused in the future. See Appendix B for source code.\n60\n3.0.4.2 Conjugate priors\nThe DDPMO model, as presented in (Neiswanger et al., 2014), uses conjugate priors. In particular, it uses a Multinomial distribution with a Dirichlet prior and a Multivariate normal distribution with a Normal-inverse-Wishart prior. Conjugate priors may be implemented in languages like Church (Goodman et al., 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al., 2014) in the form of exchangeable random procedures (XRPs). Conjugate priors give a closed-form expression for the likelihood with marginalised-out prior distribution parameters, in this case Dirichlet and Normalinverse-Wishart priors. While conjugate priors are not necessary, inference often becomes tractable only with the conjugate priors. That is because with conjugate priors we do not need to spend excessive computational resources integrating over hyperparameters in the process of Monte Carlo inference.\nTo perform inference in DDPMO, a library of additional XRPs for Anglican has been implemented, including procedure mvn-conjugate-fast for a Multinomial distribution with a Dirichlet prior, and dirichlet-multinomial for a Multivariate normal distribution with a Normal-inverse-Wishart prior. This library is also made such that it can be reused in the future.\n3.0.4.3 Data-driven proposal for SMC inference in DDPMO\nWe would like to obtain a better proposal for a cluster assignment of any new data point (pixel) in the DDPMO. Recall that the model is non-parametric, so that the number of clusters is not fixed. Below we provide a specification of the input and output of a discriminative model used as the proposal. It defines what data is needed from the current model state in order to perform a proposal on the cluster assignment for a new pixel.\n61\nInputs. The features \u03c6(\u03c1t) of the environment \u03c1k = (xt\u22121, y1:t), which are the inputs\nto the neural network, consist of the following:\n\u2022 Distances to the three already existing nearest clusters, of K, in the ascending\norder, di \u2208 R, i = 1, . . . , 3.\n\u2022 Colour histograms of a 7\u00d7 7 patch surrounding these three clusters in the discre-\ntised HSV space, normalised to sum to one, such that hi \u2208 R10, \u221110 j=1 hij = 1.\n\u2022 Colour histogram of a 7 \u00d7 7 patch surrounding the new data point (i.e. pixel) in\nthe discretised HSV space, normalised to sum to one, c \u2208 R10,\u221110i=1 ci = 1.\nOutput. We use the just described features \u03c6(\u03c1t), which reduce an undefined number of clusters to the three2 closest ones and all others. This allows us to aim to approximate the posterior of a discrete random variable. We use a categorical distribution with five bins to approximate the proposal q (xn|\u03b7). The five outputs of the neural network are the probabilities p1:3 of drawing one of the three nearest clusters, the probability p4 of the remaining K \u2212 3 existing clusters (so that each one has probability p4/(K \u2212 3)), and the probability p5 of sampling a new cluster. If K < 3, the prior proposal is used. If K = 3, the p4 is set to zero (all other probabilities are re-normalised).\nWe set q (xn|\u03b7) to the softmax output of the neural network. The aim for the cost function is to maximise the likelihood of the family of discrete distributions given training samples from a discrete distribution. We set the cost function to be the negative log probability given in (3.2). This well relates to neural networks with the negative log of the softmax output. Hence, we can use neural network packages out-of-the-box.\n2The number of three closest clusters is arbitrary chosen by us. It would be interesting, in the possible future research, to vary this number and see how it influences the improvements in convergence.\n62\n3.0.5 Experiments with the DDPMO model\nFor our experiments, we chose a soccer video dataset (D\u2019Orazio et al., 2009), for which there already exists a human-authored ground truth. It contains many active objects of different colours that move quickly, both with and without occlusions.\nWe selected two subsequences of frames to form a training dataset (40 frames) and a test dataset (30 frames). Both datasets consist mostly of intensive play with many players on the field. We use a foreground detector in MATLAB (from package vision) to process raw frames and extract positions and colour histograms of foreground pixels3. To measure the performance, we use and report commonly used performance metrics: the sequence frame detection accuracy (SFDA) for object detection and the average tracking accuracy (ATA) for tracking (Kasturi et al., 2009).\nAt first, we run several iterations of sequential Monte Carlo inference in Anglican for this model given the input frames from the training dataset, with 5000 particles. This allows us to extract inputs and outputs for the neural network, as described in the previous section. Then we train the neural network4 using this extracted data.\nOnce the neural network is trained, we run inference again both on train and test frame sequences. We measure inference performance with three different types of proposals:\n1. the DDPMO prior proposal (i.e. just following the generative model).\n2. the data-driven proposal with a trained neural network that outputs probabilities\n3We trained the foreground detector on frames that did not contribute to the test dataset frame sequence.\n4We used a feedforward neural network: one hidden layer with 100 nodes, tansig transfer function from the input to the hidden layer, softmax transfer function to the output, and crossentropy error.\n63\np1, . . . , p5 given the observation and the current state of the model during inference.\n3. and the data-driven proposal with fixed, hand-tuned probabilities p1, . . . , p5. They\napproximate the distribution over the outputs \u201c1\u201d, \u201c2\u201d, \u201c3\u201d, \u201cany other cluster\u201d and \u201cnew\u201d with some smoothing.\nWe vary number of particles in order to understand how object recognition and track-\ning performance varies with different proposal types and different number of particles.\nFigures 3.3 and 3.4 illustrate the experimental results. For inference with few particles, we get significant improvement in performance using the data-driven proposal. With respect to the particle log-weight, the SMC inference with 10 particles with the data-driven proposal produces results similar to the results from running SMC with thousands of particles under the prior proposal. Thus, using the data-driven proposal, the inference explores the high-probability regions in the posterior space much faster than otherwise.\nWith respect to performance metrics, for few particles, the performance of SMC with the data-driven proposal is significantly better in comparison to the SMC with the prior proposal with the same number of particles. However, the improvement is less significant, especially in respect to the SFDA metric. In addition, with many particles, SMC with the prior proposal outperforms SMC with the data-driven proposal.\nAlso, in general, data-driven proposals with the neural network show the same performance as the data-driven proposal with a hand-tuned discriminative model that always returns fixed p1:5. However, for the case of SFDA metric performance on the test dataset, the hand-tuned proposal outformed the data-driven proposal with the neural network.\n64\nAs mentioned earlier, in cases where there were yet no more than 2 clusters, the prior proposal was used. In all experiments using the data-driven proposals, they were used with probability p\u2217 = 0.8; thus, with probability 1\u2212p\u2217 = 0.2, the original prior proposal was used. This mixture proposal probability p\u2217 was incorporated into the particle logweights, to ensure that that mixture proposal is a valid SMC proposal.\nIn addition, it is worth noting that even when we attempted to decrease p\u2217 (thus increasing the probability of using the prior proposal), the SFDA metric values for SMC, with the data-driven proposals with 100 particles and more, did not become better for the test dataset and remained very similar to what we see in Figure 3.4. This might mean that, even though the data-driven proposal allows inference to find high-probability posterior regions much faster and with much less computation effort (as shown in \u201cLogweight\u201d subfigure in Figure 3.4), it is not necessarily the case that all performance metrics of interest will be high for samples from those high-probability posterior regions. On the other hand, the last statement is apparent since the generative model is always only a simplification of the real process. Future experiments might be helpful to provide more experimental details on this.\nScatter plots in Figures 3.5, 3.6, 3.7, 3.8, 3.9, and 3.10 compare the inference time\nversus particle log-weights, SFDA and ATA metric values.\nExamples of frames with detected and tracked objects are provided in Figure 3.11.\n65\n10 0\n10 1\n10 2\n10 3\nNu m\nbe r o\nf p ar\ntic le\ns\n35 00\n00 0\n30 00\n00 0\n25 00\n00 0\n20 00\n00 0\n15 00\n00 0\n10 00\n00 0\n50 00\n000 Lo\ngw\nei gh\nt, tr\nai n\nda ta\ns et\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\n10 0\n10 1\n10 2\n10 3\nNu m\nbe r o\nf p ar\ntic le\ns\n0. 0 0. 1 0. 2 0. 3 0. 4 0. 5\nSF DA\n, t ra\nin d\nat a\nse t\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\n10 0\n10 1\n10 2\n10 3\nNu m\nbe r o\nf p ar\ntic le\ns\n0. 2 0. 1 0. 0 0. 1 0. 2 0. 3 0. 4 0. 5\nAT A,\ntr ai\nn da\nta s\net\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n3: Tr\nai n\nda ta\nse t.\nPa rt\nic le\nlo g-\nw ei\ngh t\nan d\npe rf\nor m\nan ce\nm et\nri cs\nva lu\nes ,n\nam el\ny th\ne se\nqu en\nce fr\nam e\nde te\nct io\nn ac - cu ra cy (S FD A ) fo r ob je ct de te ct io n an d th e av er ag e tr ac ki ng ac cu ra cy (A TA ), fo r in fe re nc e re su lts w ith di ff er en tp ro po sa l ty pe s an d di ff er en tn um be ro fp ar tic le s. Fo rt he lo gw ei gh ta nd bo th m et ri cs ,t he hi gh er va lu e is ge ne ra lly be tte r.\n66\n10 0\n10 1\n10 2\n10 3\nNu m\nbe r o\nf p ar\ntic le\ns\n35 00\n00 0\n30 00\n00 0\n25 00\n00 0\n20 00\n00 0\n15 00\n00 0\n10 00\n00 0\n50 00\n000\n50 00\n00 Lo\ngw\nei gh\nt, te\nst d\nat a\nse t\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\n10 0\n10 1\n10 2\n10 3\nNu m\nbe r o\nf p ar\ntic le\ns\n0. 0 0. 1 0. 2 0. 3 0. 4 0. 5\nSF DA\n, t es\nt d at\na se\nt\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\n10 0\n10 1\n10 2\n10 3\nNu m\nbe r o\nf p ar\ntic le\ns\n0. 1 0. 0 0. 1 0. 2 0. 3 0. 4\nAT A,\nte st\nd at\na se\nt\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n4: Te\nst da\nta se\nt. Pa\nrt ic\nle lo\ngw\nei gh\nta nd\npe rf\nor m\nan ce\nm et\nri cs\nva lu\nes ,n\nam el\ny SF\nD A\nan d\nA TA\n,f or\nin fe\nre nc\ne re\nsu lts\nw ith\ndi ff\ner en\nt pr\nop os\nal ty\npe s\nan d\ndi ff\ner en\nt nu\nm be\nr of\npa rt\nic le\ns. Fo\nr th\ne lo\ngw\nei gh\nt an\nd bo\nth m\net ri\ncs ,t\nhe hi\ngh er\nva lu\ne is\nge ne\nra lly\nbe tte\nr.\n67\n50 00\n0 50\n00 10\n00 0\n15 00\n0 20\n00 0\n25 00\n0 30\n00 0\nTi m\ne, s\n50 00\n00\n45 00\n00\n40 00\n00\n35 00\n00\n30 00\n00\nLog-weight\nLo g-\nw ei\ngh t v\ns Ti\nm e,\ntr ai\nn da\nta s\net\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n5: Tr\nai n\nda ta\nse t.\nT he\nsc at\nte rp\nlo tt\nha ts\nho w\ns ho\nw th\ne pa\nrt ic\nle lo\ngw\nei gh\nti s\naf fe\nct ed\nby th\ne tim\ne th\nat in\nfe re\nnc e\nto ok\n(w e\nva ri\ned nu\nm be\nr of\npa rt\nic le\ns, th\nus th\ne tim\ne he\nre is\na de\nriv at\niv e\nof th\ne nu\nm be\nr of\npa rt\nic le\ns us\ned fo\nr th\ne SM\nC in\nfe re\nnc e) . T he hi gh er va lu e is ge ne ra lly be tte r.\n68\n20 00\n0 20\n00 40\n00 60\n00 80\n00 10\n00 0\n12 00 0 Ti m e, s\n50 00\n00\n45 00\n00\n40 00\n00\n35 00\n00\n30 00\n00\nLog-weight\nLo g-\nw ei\ngh t v\ns Ti\nm e,\nte st\nd at\na se\nt\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n6: Te\nst da\nta se\nt. T\nhe sc\nat te\nr pl\not th\nat sh\now s\nho w\nth e\npa rt\nic le\nlo g-\nw ei\ngh ti\ns af\nfe ct\ned by\nth e\ntim e\nth at\nin fe\nre nc\ne to ok (w e va ri ed nu m be r of pa rt ic le s, th us th e tim e he re is a de riv at iv e of th e nu m be r of pa rt ic le s us ed fo r th e SM C in fe re nc e) . T he hi gh er va lu e is ge ne ra lly be tte r.\n69\n50 00\n0 50\n00 10\n00 0\n15 00\n0 20\n00 0\n25 00\n0 30\n00 0\nTi m\ne, s\n0. 1 0. 0 0. 1 0. 2 0. 3 0. 4 0. 5\nSFDA\nSF DA\nv s\nTi m\ne, tr\nai n\nda ta\ns et\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n7: Tr\nai n\nda ta\nse t.\nT he\nsc at\nte rp\nlo tt\nha ts\nho w\nsh ow\nth e\nSF D\nA m\net ri\nc pe\nrf or\nm an\nce is\naf fe\nct ed\nby th\ne tim\ne th\nat in\nfe re\nnc e\nto ok\n(w e\nva ri\ned nu\nm be\nro fp\nar tic\nle s,\nth us\nth e\ntim e\nhe re\nis a\nde riv\nat iv\ne of\nth e\nnu m\nbe ro\nfp ar\ntic le\nsu se\nd fo\nrt he\nSM C\nin fe\nre nc\ne) .\nT he\nhi gh\ner va\nlu e\nis ge\nne ra\nlly be\ntte r.\n70\n20 00\n0 20\n00 40\n00 60\n00 80\n00 10\n00 0\n12 00 0 Ti m e, s\n0. 1 0. 0 0. 1 0. 2 0. 3 0. 4 0. 5\nSFDA\nSF DA\nv s\nTi m\ne, te\nst d\nat a\nse t\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n8: Te\nst da\nta se\nt. T\nhe sc\nat te\nrp lo\ntt ha\nts ho\nw s\nho w\nth e\nSF D\nA m\net ri\nc pe\nrf or\nm an\nce is\naf fe\nct ed\nby th\ne tim\ne th\nat in\nfe re\nnc e\nto ok\n(w e\nva ri\ned nu\nm be\nro fp\nar tic\nle s,\nth us\nth e\ntim e\nhe re\nis a\nde riv\nat iv\ne of\nth e\nnu m\nbe ro\nfp ar\ntic le\nsu se\nd fo\nrt he\nSM C\nin fe\nre nc\ne) .\nT he\nhi gh\ner va\nlu e\nis ge\nne ra\nlly be\ntte r.\n71\n50 00\n0 50\n00 10\n00 0\n15 00\n0 20\n00 0\n25 00\n0 30\n00 0\nTi m\ne, s\n0. 1 0. 0 0. 1 0. 2 0. 3 0. 4 0. 5\nATA\nAT A\nvs T\nim e,\ntr ai\nn da\nta s\net\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n9: Tr\nai n\nda ta\nse t.\nT he\nsc at\nte rp\nlo tt\nha ts\nho w\ns ho\nw th\ne A\nTA m\net ri\nc pe\nrf or\nm an\nce is\naf fe\nct ed\nby th\ne tim\ne th\nat in\nfe re\nnc e\nto ok\n(w e\nva ri\ned nu\nm be\nro fp\nar tic\nle s,\nth us\nth e\ntim e\nhe re\nis a\nde riv\nat iv\ne of\nth e\nnu m\nbe ro\nfp ar\ntic le\nsu se\nd fo\nrt he\nSM C\nin fe\nre nc\ne) .\nT he\nhi gh\ner va\nlu e\nis ge\nne ra\nlly be\ntte r.\n72\n20 00\n0 20\n00 40\n00 60\n00 80\n00 10\n00 0\n12 00 0 Ti m e, s\n0. 05 0. 00 0. 05 0. 10 0. 15 0. 20 0. 25 0. 30 0. 35\nATA\nAT A\nvs T\nim e,\nte st\nd at\na se\nt\nPr io r Da\nta -d\nriv en\np ro\npo sa\nl w ith\nN N\nHa nd\n-tu ne\nd da\nta -d\nriv en\np ro\npo sa\nl\nFi gu\nre 3.\n10 :T\nes td\nat as\net .T\nhe sc\nat te\nrp lo\ntt ha\nts ho\nw s\nho w\nth e\nA TA\nm et\nri c\npe rf\nor m\nan ce\nis af\nfe ct\ned by\nth e\ntim e\nth at\nin fe\nre nc e to ok (w e va ri ed nu m be ro fp ar tic le s, th us th e tim e he re is a de riv at iv e of th e nu m be ro fp ar tic le su se d fo rt he SM C in fe re nc e) . T he hi gh er va lu e is ge ne ra lly be tte r.\n73"}, {"heading": "3.1 Conclusion", "text": "Most of inference in currently existing probabilistic programming systems uses prior proposals. Problem-specific proposals are, however, essential for inference, especially in the case of real world applications. New generative models for Bayesian inference are usually introduced, as a conference paper, with a hand-designed proposal that makes the inference in that model feasible.\nIn this Chapter we presented work on developing a hand-designed data-driven proposal for a particular model, the DDPMO, and directly implementing it in the probabilistic programming system Anglican. Our experimental results showed that the datadriven proposal significantly improves the inference performance so that significantly less particles are necessary to perform good posterior estimation. In general, we assume that our proposal may be applied to any non-parametric generative model with some\n74\ndistance function between clusters and data points (i.e. observations). We performed our experiments in offline settings. To tune the parameters of the proposal, we used neural networks with clearly separated train and test datasets.\nThe data-driven proposal that we described relies on the feature extractor. The feature extractor maps the current state of the unbounded number of clusters with their sufficient statistics to the input of the neural network. The feature extractor that we implement and use is also a significant part of the data-driven proposal. This is shown by the fact that the neural network performs as well as the fixed hand-tuned discriminative model. This is probably because for the football dataset the spatial factor is very important for the model. Therefore, there is future work to verify whether for more complex datasets and models data-driven proposals with neural networks provide more benefits.\nOur work relates to other work in the field on data-driven proposals. The work on using discriminative proposals for Markov Chain Monte Carlo in parametric generative models include (Tu and Zhu, 2002) and (Jampani et al., 2015b), with applications in computer vision. Recent work with sequential Monte Carlo includes neural adaptive SMC (Gu et al., 2015), where authors also adapt proposals by descending the inclusive Kullback-Leibler divergence between the proposal and the true posterior distributions on hidden variables given observations. They use recurrent neural networks to train proposal distributions for inference in parametric generative models with fixed dimensionality. Another related recent work is a new probabilistic programming language called Picture (Kulkarni et al., 2015), for which authors propose and describe the use of data-driven proposals in the context of models for computer vision. They also use neural networks to learn proposals. To get the data to train the neural network, they sample both hidden variables and observations from the generative model unconditionally offline.\n75\nIn future work, more advanced neural network architectures can be applied to improve results by extracting better features and processing them more efficiently. In particular, one can think of using convolutional neural networks that include as a feature a window of the frame, centered at the new observing pixel. The ultimate goal is to find a way to generate such data-driven proposals automatically, given a generative model in the form of a probabilistic program.\n76\nConclusion\nProbabilistic programming provides users with a favourable way to write probabilistic generative models and perform automatic inference in them. In this work, we explored two applications of it. The first application, learning one-dimensional sampler code, is ultimately directed toward automatic induction of generative probabilistic programs given some examples. The second application, the facilitation of Bayesian inference in probabilistic programming using data-driven proposals, to the development of which we partly contributed, should provide users with much faster inference.\nThe speed and asymptotics of inference is one of the most important factors for the probabilistic programming field to be successful. There already exist many models written as probabilistic programs, since it is indeed easy to write a directed generative model as a program. However, for many models, even for modest amounts of data, the inference is not tractable with current inference techniques provided in probabilistic programming systems. This means that a significant amount of future work on optimising inference is essential. Ultimately, fast general-purpose inference might also make the automatic learning of complex generative models possible.\n77"}, {"heading": "Appendix A", "text": "Corpus of sampler code\n78\n79\n80\n81\n82"}, {"heading": "Appendix B", "text": "The DDPMO and GPU code in Anglican\nB.1 The DDPMO code 1 (ns ddpmo.ddpmo 2 (:use [anglican emit runtime] 3 [anglib xrp utils new-dists anglican-utils] 4 ddpmo.ddpmo-header) 5 (:require [clojure.core.matrix :as m] 6 [clojure.core.matrix 7 :refer [identity-matrix mmul add sub transpose 8 matrix to-nested-vectors] 9 :rename {identity-matrix eye\n10 add madd 11 sub msub 12 transpose mtranspose}] 13 [clojure.core.matrix.linear :as ml])) 14 15 (with-primitive-procedures 16 [multivariate-t mvn-conjugate-fast 17 dirichlet-multinomial-process 18 DIRICHLET-MULTINOMIAL-PROCESS-STATE-INFO 19 MVN-PROCESS-FAST-STATE-INFO 20 matrix produce-matrix-from-vector to-nested-vectors 21 mtranspose matrix-to-clojure-vector] 22 23 (defquery ddpmo 24 \"The Dependent Dirichlet Process Mixture of Objects 25 for Detection-free Tracking\"\n83\n26 [data Nts proposal-type] 27 28 (let [ 29 30 ;;;;;; DDPMO model ;;;;;; 31 32 ;; Hyperparameters for squares/objects/football 33 alpha 0.1 ; for GPU 34 rho 0.32 ; for GPU 35 mu-0 (produce-matrix-from-vector [0 0]) ; for NiW 36 k-0 0.00370790649926 ; for normal-inverse-wishart 37 nu-0 7336.3104796 ; for normal-inverse-wishart 38 Lambda-0 (matrix [[193.362493995 0] 39 [0 40.6543682123]]) 40 q-0 (vec (repeat 10 10.0)) ; for Dirichlet. 41 ;; The dimensionality must 42 ;; match number of RGB bins V 43 M 10.1 ; for G0 (eqns (7-8)) 44 multinomial-trials 49 ; for eqn (2)... 45 ;; this is m x m where m = 2L + 1 46 47 extract-old-style-theta 48 (fn [theta] 49 (let 50 [mvn-process (retrieve (get theta \u2019positions)) 51 dirichlet-multinomial-process-instance 52 (retrieve (get theta \u2019colours)) 53 mu-Sigma (MVN-PROCESS-FAST-STATE-INFO mvn-process) 54 ps (DIRICHLET-MULTINOMIAL-PROCESS-STATE-INFO 55 dirichlet-multinomial-process-instance) 56 theta] 57 {\u2019mu (get mu-Sigma \u2019mu) \u2019Sigma 58 (get mu-Sigma \u2019Sigma) 59 \u2019trials multinomial-trials \u2019ps ps})) 60 61 get-N (fn [t] (nth Nts (dec t))) 62 63 ;; Transition distribution 64 T (fn T [prev-theta] 65 (let [previous-mvn-process 66 (get prev-theta \u2019positions) 67 previous-dirichlet-multinomial-process 68 (get prev-theta \u2019colours)\n84\n69 70 new-mvn-process 71 (XRP (mvn-conjugate-fast 72 mu-0 k-0 nu-0 Lambda-0)) 73 new-dirichlet-multinomial-process 74 (XRP 75 (dirichlet-multinomial-process 76 q-0 multinomial-trials)) 77 78 ;; Auxiliary transition 79 _ (repeatedly 80 M (fn [] 81 (INCORPORATE 82 new-mvn-process 83 (SAMPLE 84 previous-mvn-process)))) 85 _ (repeatedly 86 M 87 (fn [] 88 (INCORPORATE 89 new-dirichlet-multinomial-process 90 (SAMPLE 91 previous-dirichlet-multinomial-process)))) 92 ] 93 {\u2019positions new-mvn-process 94 \u2019colours new-dirichlet-multinomial-process})) 95 96 ;; Base distribution 97 G0 (fn G0 [] 98 (let [mvn-process 99 (XRP\n100 (mvn-conjugate-fast 101 mu-0 k-0 nu-0 Lambda-0)) 102 dirichlet-multinomial-process-instance 103 (XRP 104 (dirichlet-multinomial-process 105 q-0 multinomial-trials)) 106 ] 107 {\u2019positions mvn-process 108 \u2019colours dirichlet-multinomial-process-instance})) 109 110 [gpu get-theta] (create-gpu alpha rho G0 T get-N) 111\n85\n112 ;; Helper function 113 ;; Returns parameters for the corresponding 114 ;; table of foreground pixel n at time t 115 get-theta-t-n (mem (fn get-theta-t-n [t n] 116 (let [customers (gpu t n) 117 cs (get customers \u2019cs) 118 k (get cs (dec n))] 119 (get-theta t k)))) 120 121 ;;;;;; OBSERVES ;;;;;; 122 observe-lines 123 (fn observe-lines [lines line-id] 124 (if (nil? (first lines)) 125 true 126 (let [line (first lines) 127 pos (get line \u2019pos) 128 _ (store \"current-pos\" 129 (matrix-to-clojure-vector pos)) 130 col (get line \u2019col) 131 _ (store \"current-col\" col) 132 t (get line \u2019t) 133 n (get line \u2019n) 134 theta (get-theta-t-n t n) 135 positions-process (get theta \u2019positions) 136 colours-process (get theta \u2019colours)] 137 138 ; Observing positions. 139 (OBSERVE positions-process pos) 140 141 ; Observing colours 142 (OBSERVE colours-process col) 143 144 (if (= n (get-N t)) 145 (let [gpu (gpu t n) 146 cs (get gpu \u2019cs) 147 relevant-clusters (distinct cs) 148 thetas 149 (map (fn [k] 150 (let [theta (get-theta t k) 151 theta 152 (extract-old-style-theta theta) 153 mu (get theta \u2019mu) 154 Sigma (get theta \u2019Sigma)\n86\n155 ps (get theta \u2019ps)] 156 {\u2019k k \u2019mu mu \u2019Sigma Sigma \u2019ps ps})) 157 relevant-clusters) 158 res {\u2019t t \u2019n n \u2019gpu gpu \u2019thetas thetas}] 159 (predict res))) 160 (observe-lines (rest lines) (inc line-id)))))] 161 162 (observe-lines data 0)))) 163 164 (defn -main [data-set-name number-of-particles 165 num-particles-to-output 166 proposal-type & ignore-following-args] 167 (let [number-of-particles (parse-int number-of-particles) 168 num-particles-to-output 169 (parse-int num-particles-to-output) 170 proposal-type (str proposal-type) 171 _ (case proposal-type \"prior\" 172 :okay \"handtuned\" :okay \"nn\" :okay) 173 [data Nts] (load-DDPMO-data data-set-name) 174 query-results (doquery 175 :smc ddpmo [data Nts proposal-type] 176 :number-of-particles number-of-particles) 177 results 178 (doall 179 (map 180 (fn [particle-output particle-id] 181 (doall 182 (map 183 (fn [x] 184 (println 185 (str particle-id \",\" 186 (first x) \",\" (second x) \",0.0\"))) 187 (get particle-output :anglican.state/predicts)))) 188 (take num-particles-to-output query-results) 189 (range num-particles-to-output)))] 190 results))\nB.2 The GPU code 1 ;;;;;; GPU definition ;;;;;; 2 3 ; Creates an instance of a GPU process.\n87\n4 ; Takes: 5 ; * GPU\u2019s alpha and rho. 6 ; * Base distribution G0. 7 ; * Transition distribution T. 8 ; * function get-N which returns the number 9 ; of points at each time.\n10 ; Returns: [gpu get-theta] 11 (defm create-gpu [alpha rho G0 T get-N] 12 (let 13 [;; Given vector of table sizes ms = [m1 m2 ...], 14 ;; returns a new vector of table 15 ;; sizes by removing customers from tables 16 ;; with probability rho 17 remove-customers 18 (fn [ms] 19 (vec (map (fn [m] 20 (if (= m 0) 21 0 22 (- m 23 (SAMPLE 24 (binomial m rho))))) 25 ms))) 26 27 ;; Returns {\u2019cs (vector of n cluster ids) 28 ;; \u2019K (number of unique clusters at n) 29 ;; \u2019ms (vector of cluster sizes at n)} 30 ;; after processing foreground pixel n at time t 31 ;; n goes from 1 32 ;; c_i goes from 0 33 ;; K = max(c_i) + 1 34 ;; t goes from 1 35 gpu (mem 36 (fn gpu [t n] 37 (if (= n 0) 38 ;; Initialise 39 (if (= t 1) 40 {\u2019cs \u2019[] \u2019K 0 \u2019ms \u2019[]} 41 (let [prev-t-gpu (gpu (dec t) (get-N (dec t))) 42 prev-K (get prev-t-gpu \u2019K) 43 prev-ms (get prev-t-gpu \u2019ms)] 44 {\u2019cs \u2019[] \u2019K prev-K 45 \u2019ms (remove-customers prev-ms)})) 46\n88\n47 ;; Get from step (n - 1) 48 (let [prev-n-gpu (gpu t (dec n)) 49 cs (get prev-n-gpu \u2019cs) 50 K (get prev-n-gpu \u2019K) 51 ms (get prev-n-gpu \u2019ms) 52 w (conj ms alpha) 53 c (SAMPLE (discrete w)) 54 new-cs (conj cs c) 55 new-K (max K (inc c)) 56 new-ms (assoc ms c (inc (get ms c 0)))] 57 {\u2019cs new-cs \u2019K new-K \u2019ms new-ms})))) 58 59 ;; Returns parameters for table k at time t 60 ;; using either transition distribution T 61 ;; or base distribution G0 62 get-theta (mem (fn get-theta [t k] 63 (if (= t 1) 64 (G0) 65 (let [prev-customers 66 (gpu (dec t) (get-N (dec t))) 67 prev-K (get prev-customers \u2019K) 68 initial-ms (get (gpu t 0) \u2019ms)] 69 (if (> k (dec prev-K)) 70 (G0) 71 (if (= (nth initial-ms k) 0) 72 nil 73 (T (get-theta (dec t) k))))))))] 74 [gpu get-theta]))\nB.3 Clojure code for the data-driven proposal 1 (def NUMBER-OF-NEAREST-CLUSTERS 3) 2 3 (def sort-thetas 4 (fn [thetas] 5 (let 6 [my-comparer 7 (fn [el1 el2] 8 (< (nth el1 2) (nth el2 2)))] 9 (sort my-comparer thetas))))\n10 11 (def distance\n89\n12 (fn [[x1 y1] [x2 y2]] 13 \"Returns Euclidean distance between two 2D points.\" 14 ;; Important! Here x is really y, and vice versa. 15 ;; This is because in the MATLAB code the first 16 ;; coordinate is y. 17 (pow (+ (pow (- x1 x2) 2.0) (pow (- y1 y2) 2.0)) 0.5)))\nB.4 Anglican code (within the DDPMO model) for the data-driven proposal\n1 get-thetas 2 (fn [t n] 3 \"Returns thetas for active clusters (ms[i] > 0) 4 at data point (t, n). This function should be 5 called only when we already processed that data point.\" 6 (let 7 [ 8 gpu-state (gpu t n) 9 ms (get gpu-state \u2019ms)\n10 get-theta (fn [t k] (if (> (nth ms k) 0) 11 (get-theta t k) 12 nil)) 13 thetas (map (fn [k] (list k (get-theta t k))) 14 (range (count ms))) 15 thetas (filter 16 (fn [el] (not (nil? (second el)))) thetas) 17 ] 18 thetas)) 19 20 get-mean-coords 21 (fn [theta] 22 \"Extracts mean from the theta as Clojure vector.\" 23 (let 24 [coords (matrix-to-clojure-vector 25 (get (MVN-PROCESS-FAST-STATE-INFO 26 (retrieve (get theta \u2019positions))) 27 \u2019mu))] 28 coords)) 29 30 get-nearest-thetas 31 (fn [t n [x y]] 32 \"Gets an ordered list of theta which are the nearest\n90\n33 to the point [x y] based on the state at the previous 34 data point (t, n - 1).\" 35 (if (and (= t 1) (= n 1)) 36 nil 37 (let 38 [[t n] 39 (if (= n 1) 40 [(- t 1) (get-N (- t 1))] 41 [t (- n 1)])] 42 (let 43 [thetas (get-thetas t n) 44 thetas (map (fn [[k theta]] 45 (list k theta 46 (distance [x y] 47 (get-mean-coords theta)))) 48 thetas) 49 thetas (sort-thetas thetas) 50 thetas (take NUMBER-OF-NEAREST-CLUSTERS thetas)] 51 (if (< (count thetas) NUMBER-OF-NEAREST-CLUSTERS) 52 nil 53 thetas))))) 54 55 ;; Do the trick to allow mutual recursion. 56 _ (store \"get-nearest-thetas\" get-nearest-thetas)\nB.5 Code for the GPU, to get data for the proposal for train datasets\n1 NEAREST-THETAS ((retrieve \"get-nearest-thetas\") 2 t n (retrieve \"current-pos\")) 3 for-proposal 4 (map 5 (fn [the-list] 6 (let 7 [theta-id (nth the-list 0) 8 theta (nth the-list 1) 9 distance-to-the-center (nth the-list 2)]\n10 (list 11 theta-id 12 (DIRICHLET-MULTINOMIAL-PROCESS-STATE-INFO 13 (retrieve (get theta \u2019colours))) 14 distance-to-the-center)))\n91\n15 NEAREST-THETAS) 16 nn-input 17 (concat 18 (apply concat 19 (doall 20 (map 21 (fn [data] 22 (concat (nth data 1) 23 (list (nth data 2)))) 24 for-proposal))) 25 (doall (map (fn [x] (/ x 49.0)) 26 (retrieve \"current-col\"))))) 27 c (if (or (not (= (count nn-input) 43)) 28 (= proposal-type \"prior\")) 29 (SAMPLE (discrete w)) 30 (let 31 [dist (sample-cluster-id 32 nn-input w 0.8 33 (map first NEAREST-THETAS) 34 (= proposal-type \"handtuned\")) 35 [my-sample log-likelihood] (sample dist)] 36 (add-log-weight log-likelihood) 37 my-sample))\nB.6 Code for the GPU, to use the proposal for test datasets 1 NEAREST-THETAS ((retrieve \"get-nearest-thetas\") 2 t n (retrieve \"current-pos\")) 3 for-proposal 4 (map 5 (fn [the-list] 6 (let 7 [theta-id (nth the-list 0) 8 theta (nth the-list 1) 9 distance-to-the-center (nth the-list 2)]\n10 (list 11 theta-id 12 (DIRICHLET-MULTINOMIAL-PROCESS-STATE-INFO 13 (retrieve (get theta \u2019colours))) 14 distance-to-the-center))) 15 NEAREST-THETAS) 16 _ (predict (list for-proposal c\n92\n17 (retrieve \"current-col\") 18 (count w)))\n93"}], "references": [{"title": "UCI Machine Learning Repository", "author": ["K. Bache", "M. Lichman"], "venue": "http:// archive.ics.uci.edu/ml.", "citeRegEx": "Bache and Lichman,? 2013", "shortCiteRegEx": "Bache and Lichman", "year": 2013}, {"title": "A note on the generation of random normal deviates", "author": ["G.E. Box", "M.E. Muller"], "venue": "The Annals of Mathematical Statistics, 29(2):610\u2013611.", "citeRegEx": "Box and Muller,? 1958", "shortCiteRegEx": "Box and Muller", "year": 1958}, {"title": "Generalized P\u00f3lya urn for time-varying Dirichlet process mixtures", "author": ["F. Caron", "M. Davy", "A. Doucet"], "venue": "Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI 2007).", "citeRegEx": "Caron et al\\.,? 2007", "shortCiteRegEx": "Caron et al\\.", "year": 2007}, {"title": "Probabilistic programming concepts", "author": ["L. De Raedt", "A. Kimmig"], "venue": "arXiv eprint arXiv:1312.4328.", "citeRegEx": "Raedt and Kimmig,? 2013", "shortCiteRegEx": "Raedt and Kimmig", "year": 2013}, {"title": "Bootstrap learning via modular concept discovery", "author": ["E. Dechter", "J. Malmaud", "R.P. Adams", "J.B. Tenenbaum"], "venue": "Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013).", "citeRegEx": "Dechter et al\\.,? 2013", "shortCiteRegEx": "Dechter et al\\.", "year": 2013}, {"title": "Non-uniform random variate generation", "author": ["L. Devroye"], "venue": "Springer-Verlag.", "citeRegEx": "Devroye,? 1986", "shortCiteRegEx": "Devroye", "year": 1986}, {"title": "A semiautomatic system for ground truth generation of soccer video sequences", "author": ["T. D\u2019Orazio", "M. Leo", "N. Mosca", "P. Spagnolo", "P.L. Mazzeo"], "venue": "In Advanced Video and Signal Based Surveillance,", "citeRegEx": "D.Orazio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "D.Orazio et al\\.", "year": 2009}, {"title": "Probability: theory and examples", "author": ["R. Durrett"], "venue": "Cambridge University Press.", "citeRegEx": "Durrett,? 2010", "shortCiteRegEx": "Durrett", "year": 2010}, {"title": "Structure discovery in nonparametric regression through compositional kernel search", "author": ["D. Duvenaud", "J.R. Lloyd", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML 2013), pages 1166\u20131174.", "citeRegEx": "Duvenaud et al\\.,? 2013", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2013}, {"title": "Just-in-time learning for fast and flexible inference", "author": ["S.M.A. Eslami", "D. Tarlow", "P. Kohli", "J. Winn"], "venue": "Advances in Neural Information Processing Systems (NIPS 2014).", "citeRegEx": "Eslami et al\\.,? 2014", "shortCiteRegEx": "Eslami et al\\.", "year": 2014}, {"title": "A repository for generative models (composite authors, edited by andreas stuhlm\u00fcller)", "author": ["ForestDB"], "venue": "Available at http://forestdb.org/.", "citeRegEx": "ForestDB,? 2016", "shortCiteRegEx": "ForestDB", "year": 2016}, {"title": "DEAP: Evolutionary algorithms made easy", "author": ["Fortin", "F.-A.", "De Rainville", "F.-M.", "Gardner", "M.-A.", "M. Parizeau", "C. Gagn\u00e9"], "venue": "Journal of Machine Learning Research, 13:2171\u20132175.", "citeRegEx": "Fortin et al\\.,? 2012", "shortCiteRegEx": "Fortin et al\\.", "year": 2012}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI 2013), volume 99, pages 1300\u20131309.", "citeRegEx": "Friedman et al\\.,? 1999", "shortCiteRegEx": "Friedman et al\\.", "year": 1999}, {"title": "The principles and practice of probabilistic programming", "author": ["N.D. Goodman"], "venue": "Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL \u201913, pages 399\u2013402, New York, NY, USA. ACM.", "citeRegEx": "Goodman,? 2013", "shortCiteRegEx": "Goodman", "year": 2013}, {"title": "Church: a language for generative models", "author": ["N.D. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "Proceedings of the Twenty95", "citeRegEx": "Goodman et al\\.,? 2008", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Probabilistic programming", "author": ["A.D. Gordon", "T.A. Henzinger", "A.V. Nori", "S.K. Rajamani"], "venue": "Proceedings of the on Future of Software Engineering, pages 167\u2013 181. ACM.", "citeRegEx": "Gordon et al\\.,? 2014", "shortCiteRegEx": "Gordon et al\\.", "year": 2014}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N.J. Gordon", "D.J. Salmond", "A.F. Smith"], "venue": "IEE Proceedings F (Radar and Signal Processing), volume 140, pages 107\u2013113. IET.", "citeRegEx": "Gordon et al\\.,? 1993", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Exploiting compositionality to explore a large space of model structures", "author": ["R. Grosse", "R.R. Salakhutdinov", "W.T. Freeman", "J.B. Tenenbaum"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML 2012).", "citeRegEx": "Grosse et al\\.,? 2012", "shortCiteRegEx": "Grosse et al\\.", "year": 2012}, {"title": "Neural adaptive sequential Monte Carlo", "author": ["S. Gu", "R.E. Turner", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems (NIPS 2015).", "citeRegEx": "Gu et al\\.,? 2015", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Approaches and Applications of Inductive Programming (Dagstuhl Seminar 13502)", "author": ["S. Gulwani", "E. Kitzelmann", "U. Schmid"], "venue": "Dagstuhl Reports, 3(12):43\u201366.", "citeRegEx": "Gulwani et al\\.,? 2014", "shortCiteRegEx": "Gulwani et al\\.", "year": 2014}, {"title": "Learning to pass expectation propagation messages", "author": ["N. Heess", "D. Tarlow", "J. Winn"], "venue": "Advances in Neural Information Processing Systems (NIPS 2013), pages 3219\u20133227.", "citeRegEx": "Heess et al\\.,? 2013", "shortCiteRegEx": "Heess et al\\.", "year": 2013}, {"title": "Incremental learning in inductive programming", "author": ["R. Henderson"], "venue": "Approaches and Applications of Inductive Programming, pages 74\u201392. Springer.", "citeRegEx": "Henderson,? 2010", "shortCiteRegEx": "Henderson", "year": 2010}, {"title": "Accelerating inference: towards a full language, compiler and hardware stack", "author": ["B."], "venue": "arXiv e-print arXiv:1212.2991.", "citeRegEx": "B.,? 2012", "shortCiteRegEx": "B.", "year": 2012}, {"title": "Notes on approximate bayesian computation (private correspondence)", "author": ["J. Huggins"], "venue": null, "citeRegEx": "Huggins,? \\Q2013\\E", "shortCiteRegEx": "Huggins", "year": 2013}, {"title": "Consensus message passing for layered graphical models", "author": ["V. Jampani", "S.M.A. Eslami", "D. Tarlow", "P. Kohli", "J. Winn"], "venue": "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS 2015).", "citeRegEx": "Jampani et al\\.,? 2015a", "shortCiteRegEx": "Jampani et al\\.", "year": 2015}, {"title": "The informed sampler: A discriminative approach to Bayesian inference in generative computer vision models", "author": ["V. Jampani", "S. Nowozin", "M. Loper", "P.V. Gehler"], "venue": "Special Issue on Generative Models in Computer Vision, Computer Vision and Image Understanding, 136:32\u201344.", "citeRegEx": "Jampani et al\\.,? 2015b", "shortCiteRegEx": "Jampani et al\\.", "year": 2015}, {"title": "Just-in-time kernel regression for expectation propagation", "author": ["W. Jitkrittum", "A. Gretton", "S.M.A. Eslami", "C.B. Lakshminarayanan", "D. Sejdinovic", "Z. Szab\u00f3"], "venue": "Large-Scale Kernel Learning: Challenges and New Opportunities workshop at International Conference on Machine Learning (ICML 2015).", "citeRegEx": "Jitkrittum et al\\.,? 2015", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2015}, {"title": "Adaptor grammars: A framework for specifying compositional nonparametric bayesian models", "author": ["M. Johnson", "T.L. Griffiths", "S. Goldwater"], "venue": "Advances in Neural Information Processing Systems (NIPS 2007), 19:641.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "arXiv e-print arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "An inductive logic programming approach to statistical relational learning", "author": ["K. Kersting"], "venue": "Proceedings of the Conference on An Inductive Logic Programming Approach to Statistical Relational Learning 2005, pages 1\u2013228. IOS Press.", "citeRegEx": "Kersting,? 2005", "shortCiteRegEx": "Kersting", "year": 2005}, {"title": "On the implementation of the probabilistic logic programming language ProbLog", "author": ["A. Kimmig", "B. Demoen", "L. De Raedt", "V.S. Costa", "R. Rocha"], "venue": "Theory and Practice of Logic Programming, 11(2-3):235\u2013262.", "citeRegEx": "Kimmig et al\\.,? 2011", "shortCiteRegEx": "Kimmig et al\\.", "year": 2011}, {"title": "Embedded probabilistic programming", "author": ["O. Kiselyov", "Shan", "C.-C."], "venue": "Domain-Specific Languages, pages 360\u2013384. Springer.", "citeRegEx": "Kiselyov et al\\.,? 2009", "shortCiteRegEx": "Kiselyov et al\\.", "year": 2009}, {"title": "The art of computer programming, volume 2: Seminumerical algorithms (3rd edition)", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "Knuth,? \\Q1998\\E", "shortCiteRegEx": "Knuth", "year": 1998}, {"title": "Genetic programming: on the programming of computers by means of natural selection, volume 1", "author": ["J.R. Koza"], "venue": "MIT Press.", "citeRegEx": "Koza,? 1992", "shortCiteRegEx": "Koza", "year": 1992}, {"title": "Fully automatic variational inference of differentiable probability models", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D. Blei"], "venue": "NIPS Workshop on Probabilistic Programming 2014.", "citeRegEx": "Kucukelbir et al\\.,? 2014", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2014}, {"title": "Picture: A probabilistic programming language for scene perception", "author": ["T.D. Kulkarni", "P. Kohli", "J.B. Tenenbaum", "V. Mansinghka"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015, pages 4390\u20134399. 98", "citeRegEx": "Kulkarni et al\\.,? 2015", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "SSJ: A framework for stochastic simulation in Java", "author": ["P. L\u2019Ecuyer", "L. Meliani", "J. Vaucher"], "venue": "In Proceedings of the 2002 Winter Simulation Conference,", "citeRegEx": "L.Ecuyer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "L.Ecuyer et al\\.", "year": 2002}, {"title": "Learning programs: a hierarchical Bayesian approach", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML 2010), pages 639\u2013646.", "citeRegEx": "Liang et al\\.,? 2010", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Bias reformulation for one-shot function induction", "author": ["D. Lin", "E. Dechter", "K. Ellis", "J.B. Tenenbaum", "S.H. Muggleton"], "venue": "Proceedings of the 23rd European conference on Artificial Intelligence (ECAI 2014), pages 525\u2013530.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Particle Gibbs with ancestor sampling", "author": ["F. Lindsten", "M.I. Jordan", "T.B. Sch\u00f6n"], "venue": "Journal of Machine Learning Research, 15(1):2145\u20132184.", "citeRegEx": "Lindsten et al\\.,? 2014", "shortCiteRegEx": "Lindsten et al\\.", "year": 2014}, {"title": "The BUGS project: evolution, critique and future directions", "author": ["D. Lunn", "D. Spiegelhalter", "A. Thomas", "N. Best"], "venue": "Statistics in Medicine, 28(25):3049.", "citeRegEx": "Lunn et al\\.,? 2009", "shortCiteRegEx": "Lunn et al\\.", "year": 2009}, {"title": "Structured priors for structure learning", "author": ["V. Mansinghka", "C. Kemp", "T. Griffiths", "J. Tenenbaum"], "venue": "arXiv e-print arXiv:1206.6852.", "citeRegEx": "Mansinghka et al\\.,? 2012", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2012}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["V. Mansinghka", "D. Selsam", "Y. Perov"], "venue": "arXiv e-print arXiv:1404.0099.", "citeRegEx": "Mansinghka et al\\.,? 2014", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2014}, {"title": "Approximate Bayesian computational methods", "author": ["Marin", "J.-M.", "P. Pudlo", "C.P. Robert", "R.J. Ryder"], "venue": "Statistics and Computing, 22(6):1167\u20131180.", "citeRegEx": "Marin et al\\.,? 2012", "shortCiteRegEx": "Marin et al\\.", "year": 2012}, {"title": "Factorie: probabilistic programming", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2009}, {"title": "Handbook of biological statistics, volume 2", "author": ["J.H. McDonald"], "venue": "Sparky House Publishing Baltimore, Maryland.", "citeRegEx": "McDonald,? 2009", "shortCiteRegEx": "McDonald", "year": 2009}, {"title": "Experiments in plant hybridization (in German)", "author": ["G. Mendel"], "venue": "Verhandlungen des naturforschenden Vereins Br\u00fcnn. http://www.mendelweb.org/Mendel. html (translated in 1996).", "citeRegEx": "Mendel,? 1985", "shortCiteRegEx": "Mendel", "year": 1985}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Statistical Relational Learning, page 373.", "citeRegEx": "Milch et al\\.,? 2007", "shortCiteRegEx": "Milch et al\\.", "year": 2007}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence (UAI 2001), pages 362\u2013369. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "Advances in Inductive Logic Programming, 32:254\u2013264.", "citeRegEx": "Muggleton,? 1996", "shortCiteRegEx": "Muggleton", "year": 1996}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT Press.", "citeRegEx": "Murphy,? 2012", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "The dependent Dirichlet process mixture of objects for detection-free tracking and object modeling", "author": ["W. Neiswanger", "F. Wood", "E. Xing"], "venue": "Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014), pages 660\u2013668. 100", "citeRegEx": "Neiswanger et al\\.,? 2014", "shortCiteRegEx": "Neiswanger et al\\.", "year": 2014}, {"title": "Inductive functional programming using incremental program transformation", "author": ["R. Olsson"], "venue": "Artificial Intelligence, 74(1):55\u201381.", "citeRegEx": "Olsson,? 1995", "shortCiteRegEx": "Olsson", "year": 1995}, {"title": "A compilation target for probabilistic programming languages", "author": ["B. Paige", "F. Wood"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML 2014).", "citeRegEx": "Paige and Wood,? 2014", "shortCiteRegEx": "Paige and Wood", "year": 2014}, {"title": "Generative probabilistic programming (in Russian)", "author": ["Y. Perov"], "venue": "B.S. Thesis, Department of Mathematics and Computer Science, Siberian Federal University.", "citeRegEx": "Perov,? 2014", "shortCiteRegEx": "Perov", "year": 2014}, {"title": "Efficient, envelope-based multicore Markov chain inference for Church", "author": ["Y. Perov", "V. Mansinghka"], "venue": "NIPS Workshop on Probabilistic Programming 2012.", "citeRegEx": "Perov and Mansinghka,? 2012", "shortCiteRegEx": "Perov and Mansinghka", "year": 2012}, {"title": "Learning probabilistic programs", "author": ["Y. Perov", "F. Wood"], "venue": "arXiv e-print arXiv:1407.2646.", "citeRegEx": "Perov and Wood,? 2014", "shortCiteRegEx": "Perov and Wood", "year": 2014}, {"title": "IBAL: a probabilistic rational programming language", "author": ["A. Pfeffer"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI 2001), pages 733\u2013740. Citeseer.", "citeRegEx": "Pfeffer,? 2001", "shortCiteRegEx": "Pfeffer", "year": 2001}, {"title": "A field guide to genetic programming", "author": ["R. Poli", "W.B. Langdon", "N.F. McPhee", "J.R. Koza"], "venue": "Lulu.", "citeRegEx": "Poli et al\\.,? 2008", "shortCiteRegEx": "Poli et al\\.", "year": 2008}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "International journal of Man-Machine Studies, 27(3):221\u2013234.", "citeRegEx": "Quinlan,? 1987", "shortCiteRegEx": "Quinlan", "year": 1987}, {"title": "Probabilistic inductive logic programming \u2013 theory and applications, volume", "author": ["L.D. Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Improving inference performance in probabilistic programming", "author": ["R. Ranca"], "venue": null, "citeRegEx": "Ranca,? \\Q2014\\E", "shortCiteRegEx": "Ranca", "year": 2014}, {"title": "Slice sampling for probabilistic programming", "author": ["R. Cambridge. Ranca", "Z. Ghahramani"], "venue": "guages. Master\u2019s thesis, Department of Engineering,", "citeRegEx": "Ranca and Ghahramani,? \\Q2015\\E", "shortCiteRegEx": "Ranca and Ghahramani", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["S. Reed", "N. de Freitas"], "venue": null, "citeRegEx": "Reed and Freitas,? \\Q2016\\E", "shortCiteRegEx": "Reed and Freitas", "year": 2016}, {"title": "Bayesianly justifiable and relevant frequency calculations for the applied statistician", "author": ["Rubin", "D. B"], "venue": "The Annals of Statistics, 12(4):1151\u20131172.", "citeRegEx": "Rubin and B,? 1984", "shortCiteRegEx": "Rubin and B", "year": 1984}, {"title": "Induction of recursive program schemes", "author": ["U. Schmid", "F. Wysotzki"], "venue": "Machine Learning: ECML-98, pages 214\u2013225. Springer.", "citeRegEx": "Schmid and Wysotzki,? 1998", "shortCiteRegEx": "Schmid and Wysotzki", "year": 1998}, {"title": "Density estimation for statistics and data analysis, volume 26", "author": ["B.W. Silverman"], "venue": "CRC Press.", "citeRegEx": "Silverman,? 1986", "shortCiteRegEx": "Silverman", "year": 1986}, {"title": "Sequential Monte Carlo methods in practice", "author": ["A. Smith", "A. Doucet", "N. de Freitas", "N. Gordon"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Modeling Cognition with Probabilistic Programs: Representations and Algorithms", "author": ["A. Stuhlm\u00fcller"], "venue": "PhD thesis, Massachusetts Institute of Technology and Stanford University.", "citeRegEx": "Stuhlm\u00fcller,? 2015", "shortCiteRegEx": "Stuhlm\u00fcller", "year": 2015}, {"title": "Biips: Software for Bayesian inference with interacting particle systems", "author": ["A. Todeschini", "F. Caron", "M. Fuentes", "P. Legrand", "P. Del Moral"], "venue": "arXiv e-print arXiv:1412.3779.", "citeRegEx": "Todeschini et al\\.,? 2014", "shortCiteRegEx": "Todeschini et al\\.", "year": 2014}, {"title": "Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs", "author": ["D. Tolpin", "J.W. van de Meent", "Paige", "F. Brooks Wood"], "venue": "In ECML PKDD", "citeRegEx": "Tolpin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tolpin et al\\.", "year": 2015}, {"title": "Probabilistic programming in Anglican", "author": ["D. Tolpin", "J.W. van de Meent", "F. Wood"], "venue": "In Machine Learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "Tolpin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tolpin et al\\.", "year": 2015}, {"title": "Maximum a posteriori estimation by search in probabilistic programs", "author": ["D. Tolpin", "F. Wood"], "venue": "Eighth Annual Symposium on Combinatorial Search.", "citeRegEx": "Tolpin and Wood,? 2015", "shortCiteRegEx": "Tolpin and Wood", "year": 2015}, {"title": "Image segmentation by data-driven Markov chain Monte Carlo", "author": ["Z. Tu", "Zhu", "S.-C."], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(5):657\u2013 673.", "citeRegEx": "Tu et al\\.,? 2002", "shortCiteRegEx": "Tu et al\\.", "year": 2002}, {"title": "Particle Gibbs with ancestor sampling for probabilistic programs", "author": ["J.W. van de Meent", "H. Yang", "V. Mansinghka", "F. Wood"], "venue": "arXiv e-print arXiv:1501.06769", "citeRegEx": "Meent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meent et al\\.", "year": 2015}, {"title": "Mendelian inheritance\u201d \u2014 Wikipedia, the free encyclopedia", "author": ["Wikipedia"], "venue": "https://en.wikipedia.org/wiki/Mendelian_inheritance.", "citeRegEx": "Wikipedia,? 2015", "shortCiteRegEx": "Wikipedia", "year": 2015}, {"title": "Approximate Bayesian computation (ABC) gives exact re103", "author": ["R.D. Wilkinson"], "venue": null, "citeRegEx": "Wilkinson,? \\Q2013\\E", "shortCiteRegEx": "Wilkinson", "year": 2013}, {"title": "Automated variational inference in probabilistic programming", "author": ["D. Wingate", "T. Weber"], "venue": "arXiv e-print arXiv:1301.1299.", "citeRegEx": "Wingate and Weber,? 2013", "shortCiteRegEx": "Wingate and Weber", "year": 2013}, {"title": "A new approach to probabilistic programming inference", "author": ["F. Wood", "J.W. van de Meent", "V. Mansinghka"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (ICML", "citeRegEx": "Wood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2014}, {"title": "Generating efficient MCMC kernels from probabilistic programs", "author": ["L. Yang", "P. Hanrahan", "N.D. Goodman"], "venue": "Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014), pages 1068\u20131076. 104", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 56, "context": "The initial description of the approach and initial experimental results on learning probabilistic programs were described in an arXiv submission (Perov and Wood, 2014), and in my Bachelor\u2019s thesis (Perov, 2014).", "startOffset": 146, "endOffset": 168}, {"referenceID": 54, "context": "The initial description of the approach and initial experimental results on learning probabilistic programs were described in an arXiv submission (Perov and Wood, 2014), and in my Bachelor\u2019s thesis (Perov, 2014).", "startOffset": 198, "endOffset": 211}, {"referenceID": 36, "context": "For initial experiments, a framework for Stochastic Simulation in Java (L\u2019Ecuyer et al., 2002) was usefully employed.", "startOffset": 71, "endOffset": 94}, {"referenceID": 13, "context": "Probabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data.", "startOffset": 26, "endOffset": 102}, {"referenceID": 15, "context": "Probabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data.", "startOffset": 26, "endOffset": 102}, {"referenceID": 61, "context": "Probabilistic programming (Goodman, 2013; Gordon et al., 2014; De Raedt and Kimmig, 2013; Ranca, 2014) is a constructivist way to describe probabilistic models and conduct statistical inference in such models given data.", "startOffset": 26, "endOffset": 102}, {"referenceID": 78, "context": "The probability of an execution trace can be defined, in a similar but more restrictive way to (Wood et al., 2014), as p(y,x) \u2261Nn=1 p(yn|\u03b6tn ,xn)p(xn|xn\u22121), where yn is the n-th output data point (i.", "startOffset": 95, "endOffset": 114}, {"referenceID": 14, "context": "Particular languages and implementations include functional probabilistic programming languages such as Church (Goodman et al., 2008), Anglican (Wood et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 78, "context": ", 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 42, "context": ", 2014) and Venture (Mansinghka et al., 2014); logic probabilistic programming languages (De Raedt and Kimmig, 2013) such as ProbLog (Kimmig et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 30, "context": ", 2014); logic probabilistic programming languages (De Raedt and Kimmig, 2013) such as ProbLog (Kimmig et al., 2011); and domain-specific PPLs, such as (Kiselyov and Shan, 2009).", "startOffset": 95, "endOffset": 116}, {"referenceID": 57, "context": "Other languages and implementations also endorse declarative definitions of probabilistic models and include IBAL (Pfeffer, 2001),", "startOffset": 114, "endOffset": 129}, {"referenceID": 47, "context": "Stan (Stan Development Team, 2014), BLOG (Milch et al., 2007), BUGS (Lunn et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 40, "context": ", 2007), BUGS (Lunn et al., 2009), FACTORIE (McCallum et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 44, "context": ", 2009), FACTORIE (McCallum et al., 2009), Markov Logic networks (Richardson and Domingos, 2006), and Infer.", "startOffset": 18, "endOffset": 41}, {"referenceID": 15, "context": "More detailed overviews are given in (Roy, 2016; De Raedt and Kimmig, 2013; Gordon et al., 2014; Mansinghka et al., 2014).", "startOffset": 37, "endOffset": 121}, {"referenceID": 42, "context": "More detailed overviews are given in (Roy, 2016; De Raedt and Kimmig, 2013; Gordon et al., 2014; Mansinghka et al., 2014).", "startOffset": 37, "endOffset": 121}, {"referenceID": 47, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 14, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 42, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 40, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 47, "context": "These implementations employ different statistical inference methods, which include Markov chain Monte Carlo (Milch et al., 2007; Goodman et al., 2008; Mansinghka et al., 2014; Lunn et al., 2009; Milch et al., 2007), sequential Monte Carlo (Wood et al.", "startOffset": 109, "endOffset": 215}, {"referenceID": 78, "context": ", 2007), sequential Monte Carlo (Wood et al., 2014), Hamiltonian Monte Carlo (Stan Development Team, 2014), variational inference (Mansinghka et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 42, "context": ", 2014), Hamiltonian Monte Carlo (Stan Development Team, 2014), variational inference (Mansinghka et al., 2014), belief propagation (Hershey et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 48, "context": "NET probabilistic program into a finite graphical model and an application of expectation propagation inference method (Minka, 2001), which seriously restricts the range of models that may be written in it.", "startOffset": 119, "endOffset": 132}, {"referenceID": 61, "context": "On the other hand, while languages like Church, Anglican and Venture are some of the most flexible and expressive (Ranca, 2014), their statistical inference performance is slower by at least the factor of 10x in comparison to languages like Infer.", "startOffset": 114, "endOffset": 127}, {"referenceID": 67, "context": "For example, two new probabilistic programming languages have recently been introduced, employing sequential Monte Carlo methods (Smith et al., 2013): Anglican (Wood et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 78, "context": ", 2013): Anglican (Wood et al., 2014) and Biips (Todeschini et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 69, "context": ", 2014) and Biips (Todeschini et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 39, "context": "In 2014 a general-purpose implementation of the particle Gibbs with ancestor sampling method (Lindsten et al., 2014) was introduced in (van de Meent et al.", "startOffset": 93, "endOffset": 116}, {"referenceID": 77, "context": "Variational inference has been employed in probabilistic programming since 2013 in Stochastic MATLAB (Wingate and Weber, 2013), Venture (Mansinghka et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 42, "context": "Variational inference has been employed in probabilistic programming since 2013 in Stochastic MATLAB (Wingate and Weber, 2013), Venture (Mansinghka et al., 2014) and Stan (Kucukelbir et al.", "startOffset": 136, "endOffset": 161}, {"referenceID": 34, "context": ", 2014) and Stan (Kucukelbir et al., 2014).", "startOffset": 17, "endOffset": 42}, {"referenceID": 62, "context": "Finally, slice sampling for probabilistic programming has been proposed in (Ranca and Ghahramani, 2015).", "startOffset": 75, "endOffset": 103}, {"referenceID": 72, "context": "In particular, an approximation search algorithm for maximum a posteriori probability estimation has been presented in (Tolpin and Wood, 2015).", "startOffset": 119, "endOffset": 142}, {"referenceID": 42, "context": "the implementation of Venture in C++ (Mansinghka et al., 2014) works faster than its very early prototype implementation in Clojure (Perov and Mansinghka, 2012)), by an intermediate compilation instead of a continuous interpretation (e.", "startOffset": 37, "endOffset": 62}, {"referenceID": 55, "context": ", 2014) works faster than its very early prototype implementation in Clojure (Perov and Mansinghka, 2012)), by an intermediate compilation instead of a continuous interpretation (e.", "startOffset": 77, "endOffset": 105}, {"referenceID": 78, "context": ", 2015b), with a program compilation into a Clojure function, works faster than the previous Anglican interpreter (Wood et al., 2014)), and by an utilisation of just-in-time compilation (examples include engines, described in (Perov and Mansinghka, 2012; Tolpin et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 55, "context": ", 2014)), and by an utilisation of just-in-time compilation (examples include engines, described in (Perov and Mansinghka, 2012; Tolpin et al., 2015b), which are implemented in Clojure).", "startOffset": 100, "endOffset": 150}, {"referenceID": 53, "context": "Another related work worth mentioning is \u201cProbabilistic C\u201d (Paige and Wood, 2014), where authors present a C library that allows sequential Monte Carlo and Particle Gibbs inference in any C and C++ program with just two added C functions, OBSERVE and PREDICT, to condition executions and get particle smoothing predictions correspondingly.", "startOffset": 59, "endOffset": 81}, {"referenceID": 55, "context": "by exploring conditional dependencies and making incremental updates only on a part of the execution trace, as in Venture (Perov and Mansinghka, 2012; Mansinghka et al., 2014) and Shred (Yang et al.", "startOffset": 122, "endOffset": 175}, {"referenceID": 42, "context": "by exploring conditional dependencies and making incremental updates only on a part of the execution trace, as in Venture (Perov and Mansinghka, 2012; Mansinghka et al., 2014) and Shred (Yang et al.", "startOffset": 122, "endOffset": 175}, {"referenceID": 79, "context": ", 2014) and Shred (Yang et al., 2014), where the latter is a tracing interpreter for Church language.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": "For example, while in old implementations of Church (Goodman et al., 2008) N sweeps1 of Metropolis-Hastings inference in a hidden Markov model with T 1A sweep, in the context of doing Metropolis-Hastings inference on the probabilistic program with T random choices, consists of T local MH proposals on those random choices.", "startOffset": 52, "endOffset": 74}, {"referenceID": 20, "context": "NET have included neural networks (Heess et al., 2013), just-in-time random forests (Eslami et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 9, "context": ", 2013), just-in-time random forests (Eslami et al., 2014) and just-in-time kernel-based regression (Jitkrittum et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 26, "context": ", 2014) and just-in-time kernel-based regression (Jitkrittum et al., 2015).", "startOffset": 49, "endOffset": 74}, {"referenceID": 24, "context": "NET for graphical models with many layers, which are often used in computer vision, a consensus message passing method has been proposed in (Jampani et al., 2015a).", "startOffset": 140, "endOffset": 163}, {"referenceID": 51, "context": "The approach is illustrated by experiments on the existing Bayesian generative nonparametric model, \u201cthe Dependent Dirichlet Process Mixture of Objects\u201d (Neiswanger et al., 2014).", "startOffset": 153, "endOffset": 178}, {"referenceID": 43, "context": "To assess whether the distribution of samples generated by a program candidate matches the given distribution of interest, we use approximate Bayesian computation methods (Marin et al., 2012).", "startOffset": 171, "endOffset": 191}, {"referenceID": 56, "context": "This argument is supported by the fact that we were able to successfully learn an exact sampler for the Bernoulli distribution family (Perov and Wood, 2014; Perov, 2014), given only an adaptor grammar-based prior learnt from a corpus of sampler code that did not include Bernoulli sampler code.", "startOffset": 134, "endOffset": 169}, {"referenceID": 54, "context": "This argument is supported by the fact that we were able to successfully learn an exact sampler for the Bernoulli distribution family (Perov and Wood, 2014; Perov, 2014), given only an adaptor grammar-based prior learnt from a corpus of sampler code that did not include Bernoulli sampler code.", "startOffset": 134, "endOffset": 169}, {"referenceID": 33, "context": "Finally, our approach holds its own in comparison to state-of-the-art genetic programming methods (Koza, 1992; Poli et al., 2008).", "startOffset": 98, "endOffset": 129}, {"referenceID": 58, "context": "Finally, our approach holds its own in comparison to state-of-the-art genetic programming methods (Koza, 1992; Poli et al., 2008).", "startOffset": 98, "endOffset": 129}, {"referenceID": 19, "context": "One recent overview of automatic programming is presented in (Gulwani et al., 2014) and its references.", "startOffset": 61, "endOffset": 83}, {"referenceID": 49, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 29, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 60, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 38, "context": "Approaches to program synthesis include work in inductive logic programming (Muggleton, 1996; Kersting, 2005; Raedt et al., 2008; Lin et al., 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 76, "endOffset": 147}, {"referenceID": 33, "context": ", 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 34, "endOffset": 46}, {"referenceID": 52, "context": ", 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 72, "endOffset": 86}, {"referenceID": 65, "context": ", 2014), evolutionary programming (Koza, 1992), inference over grammars (Olsson, 1995), and functional programming (Schmid and Wysotzki, 1998).", "startOffset": 115, "endOffset": 142}, {"referenceID": 37, "context": "Our approach is similar to the work on learning programs using a hierarchical Bayesian prior (Liang et al., 2010).", "startOffset": 93, "endOffset": 113}, {"referenceID": 66, "context": "The approach we describe in this chapter is related to density estimation (Silverman, 1986), which concerns the estimation of an unobservable probability density function given some observed data.", "startOffset": 74, "endOffset": 91}, {"referenceID": 12, "context": "Our approach is also related to probabilistic model learning, for example to learning probabilistic relational models (Friedman et al., 1999) and Bayesian network structure (Mansinghka et al.", "startOffset": 118, "endOffset": 141}, {"referenceID": 41, "context": ", 1999) and Bayesian network structure (Mansinghka et al., 2012).", "startOffset": 39, "endOffset": 64}, {"referenceID": 17, "context": "Also worthwhile of mentioning are recent works in search over generative probabilistic model structures (Grosse et al., 2012) and kernel compositions (Duvenaud et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 8, "context": ", 2012) and kernel compositions (Duvenaud et al., 2013).", "startOffset": 32, "endOffset": 55}, {"referenceID": 43, "context": ") Before describing further details of our approach, we provide a brief outline of approximate Bayesian computation that is based on (Marin et al., 2012) and uses algorithms and equations there contained1.", "startOffset": 133, "endOffset": 153}, {"referenceID": 23, "context": "This means that we 1In addition, notes (Huggins, 2013) from Jonathan Huggins were quite helpful.", "startOffset": 39, "endOffset": 54}, {"referenceID": 43, "context": "Approximate Bayesian computation (ABC) framework (Marin et al., 2012) is different from the standard setup of Bayesian inference due to the impossibility (or intractability) to precisely calculate the likelihood p(y|\u03b8), even if we can sample from p(\u00b7|\u03b8).", "startOffset": 49, "endOffset": 69}, {"referenceID": 43, "context": "Algorithm 1 Likelihood-free rejection sampler (Marin et al., 2012) for i = 1 to N do repeat Generate \u03b8 from the prior distribution p(\u03b8) Generate z from the distribution p(\u00b7|\u03b8) until z = y set \u03b8i = \u03b8\u2032, end for", "startOffset": 46, "endOffset": 66}, {"referenceID": 43, "context": "Algorithm 2 Likelihood-free approximate rejection sampler (Marin et al., 2012) for i = 1 to N do repeat Generate \u03b8 from the prior distribution p(\u03b8) Generate z from the distribution p(\u00b7|\u03b8) until \u03c1{\u03b7(z), \u03b7(y)} \u2264 set \u03b8i = \u03b8\u2032, zi = z\u2032, end for", "startOffset": 58, "endOffset": 78}, {"referenceID": 43, "context": "The above algorithm samples from the joint distribution (Marin et al., 2012)", "startOffset": 56, "endOffset": 76}, {"referenceID": 76, "context": "Another concept in ABC, that is essential for our work, is the proposal of \u201cnoisy ABC\u201d, made by Wilkinson in (Wilkinson, 2013).", "startOffset": 109, "endOffset": 126}, {"referenceID": 43, "context": "The joint ABC target distribution becomes as follows (Marin et al., 2012):", "startOffset": 53, "endOffset": 73}, {"referenceID": 76, "context": "The important point, which was made by Wilkinson himself in (Wilkinson, 2013), is that if the model already includes the error as part of its generative model (i.", "startOffset": 60, "endOffset": 77}, {"referenceID": 7, "context": "One of counterexamples may be found in (Durrett, 2010).", "startOffset": 39, "endOffset": 54}, {"referenceID": 45, "context": "2 shows pseudocode that aims to find a sampler for the Bernoulli distribution family parametrised by \u03bb with the help of G-test statistic (McDonald, 2009):", "startOffset": 137, "endOffset": 153}, {"referenceID": 56, "context": "We used grammar prior that is similar to one that was introduced in our preceding work (Perov and Wood, 2014; Perov, 2014).", "startOffset": 87, "endOffset": 122}, {"referenceID": 54, "context": "We used grammar prior that is similar to one that was introduced in our preceding work (Perov and Wood, 2014; Perov, 2014).", "startOffset": 87, "endOffset": 122}, {"referenceID": 27, "context": "It complements the adaptor grammar (Johnson et al., 2007) prior that is used in (Liang et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 37, "context": ", 2007) prior that is used in (Liang et al., 2010) by the use of local environments4 and type signatures.", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998).", "startOffset": 97, "endOffset": 147}, {"referenceID": 1, "context": "The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998).", "startOffset": 97, "endOffset": 147}, {"referenceID": 32, "context": "The corpus was manually prepared and was based on one-dimensional distribution sampler code from (Devroye, 1986; Box and Muller, 1958; Knuth, 1998).", "startOffset": 97, "endOffset": 147}, {"referenceID": 54, "context": "The initial experiments had been described in our prior work (Perov, 2014; Perov and Wood, 2014).", "startOffset": 61, "endOffset": 96}, {"referenceID": 56, "context": "The initial experiments had been described in our prior work (Perov, 2014; Perov and Wood, 2014).", "startOffset": 61, "endOffset": 96}, {"referenceID": 54, "context": "3, we report new experimental results, similar to (Perov, 2014; Perov and Wood, 2014), but with thinner binning.", "startOffset": 50, "endOffset": 85}, {"referenceID": 56, "context": "3, we report new experimental results, similar to (Perov, 2014; Perov and Wood, 2014), but with thinner binning.", "startOffset": 50, "endOffset": 85}, {"referenceID": 33, "context": "Our approach was evaluated against genetic programming (Koza, 1992), one of stateof-the-art methods to search in the space of programs.", "startOffset": 55, "endOffset": 67}, {"referenceID": 58, "context": "For a recent introduction into the field of genetic programming, see (Poli et al., 2008).", "startOffset": 69, "endOffset": 88}, {"referenceID": 11, "context": "4, was reproduced in the evolutionary computation framework DEAP (Fortin et al., 2012) written in Python.", "startOffset": 65, "endOffset": 86}, {"referenceID": 78, "context": ", 2015b)5, Interpreted Anglican (Wood et al., 2014)6 and Probabilistic Scheme (Paige and Wood, 2014).", "startOffset": 32, "endOffset": 51}, {"referenceID": 53, "context": ", 2014)6 and Probabilistic Scheme (Paige and Wood, 2014).", "startOffset": 34, "endOffset": 56}, {"referenceID": 56, "context": "Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al.", "startOffset": 15, "endOffset": 50}, {"referenceID": 54, "context": "Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al.", "startOffset": 15, "endOffset": 50}, {"referenceID": 78, "context": "Our prior work (Perov and Wood, 2014; Perov, 2014) had been done in Interpreted Anglican (Wood et al., 2014).", "startOffset": 89, "endOffset": 108}, {"referenceID": 53, "context": "Probabilistic Scheme engine is based on Scheme compiler \u201cStalin\u201d 7, which is written in C, with included Probabilistic C (Paige and Wood, 2014) library.", "startOffset": 121, "endOffset": 143}, {"referenceID": 59, "context": "9 shows repeated experiments for learning independent onedimensional samplers that aim to match arbitrary one-dimensional real world empirical data from a credit approval dataset8 (Quinlan, 1987; Bache and Lichman, 2013).", "startOffset": 180, "endOffset": 220}, {"referenceID": 0, "context": "9 shows repeated experiments for learning independent onedimensional samplers that aim to match arbitrary one-dimensional real world empirical data from a credit approval dataset8 (Quinlan, 1987; Bache and Lichman, 2013).", "startOffset": 180, "endOffset": 220}, {"referenceID": 46, "context": "The proposed experiments are designed to follow Mendel\u2019s original experiments (Mendel, 1985).", "startOffset": 78, "endOffset": 92}, {"referenceID": 75, "context": "We aim to induce three laws of classical genetics in the form of probabilistic programs (Wikipedia, 2015):", "startOffset": 88, "endOffset": 105}, {"referenceID": 46, "context": "presented his results in his classic paper \u201cExperiments in Plant Hybridisation\u201d (1865) (Mendel, 1985).", "startOffset": 87, "endOffset": 101}, {"referenceID": 22, "context": "presented his results in his classic paper \u201cExperiments in Plant Hybridisation\u201d (1865) (Mendel, 1985).", "startOffset": 67, "endOffset": 87}, {"referenceID": 21, "context": "Better inference techniques and hierarchical/cumulative learning methods (Henderson, 2010; Dechter et al., 2013) are essential to finding more complex probabilistic programs, including human-interpretable programs as in Appendix A that are theoretically in the prior of our grammar but were not identified during our experiments.", "startOffset": 73, "endOffset": 112}, {"referenceID": 4, "context": "Better inference techniques and hierarchical/cumulative learning methods (Henderson, 2010; Dechter et al., 2013) are essential to finding more complex probabilistic programs, including human-interpretable programs as in Appendix A that are theoretically in the prior of our grammar but were not identified during our experiments.", "startOffset": 73, "endOffset": 112}, {"referenceID": 68, "context": "to automatically induce problem-specific generative models that are similar to, for example, hidden Markov or latent Dirichlet allocation models; or even more sophisticated probabilistic problems that aim to describe our world and agents in it, for example as in (Stuhlm\u00fcller, 2015) and in (ForestDB, 2016)).", "startOffset": 263, "endOffset": 282}, {"referenceID": 10, "context": "to automatically induce problem-specific generative models that are similar to, for example, hidden Markov or latent Dirichlet allocation models; or even more sophisticated probabilistic problems that aim to describe our world and agents in it, for example as in (Stuhlm\u00fcller, 2015) and in (ForestDB, 2016)).", "startOffset": 290, "endOffset": 306}, {"referenceID": 28, "context": "as in (Karpathy et al., 2015), and potentially in combination with (Reed and de Freitas, 2016)), similar to ones that are described in the following Chapter.", "startOffset": 6, "endOffset": 29}, {"referenceID": 50, "context": "This is known as a \u201cgenerate and test\u201d approach (Murphy, 2012) since we just sample values xt from the generative model and only then evaluate how good they fit a data point yt.", "startOffset": 48, "endOffset": 62}, {"referenceID": 16, "context": "Another name for this approach is \u201cbootstrap particle filter\u201d (Gordon et al., 1993).", "startOffset": 62, "endOffset": 83}, {"referenceID": 50, "context": "This is the optimal proposal because for any given xt\u22121 the new weight w s t will have the same value independently of the value of xt (Murphy, 2012).", "startOffset": 135, "endOffset": 149}, {"referenceID": 51, "context": "For our further experiments we chose a dependent Dirichlet Process mixture of objects (DDPMO) model (Neiswanger et al., 2014).", "startOffset": 100, "endOffset": 125}, {"referenceID": 2, "context": "The model is based on a generalised P\u00f3lya urn (GPU) for timevarying Dirichlet process mixtures (Caron et al., 2007).", "startOffset": 95, "endOffset": 115}, {"referenceID": 51, "context": "The generative process of the DDPMO is described in (Neiswanger et al., 2014).", "startOffset": 52, "endOffset": 77}, {"referenceID": 51, "context": "The comparison of the object recognition and tracking performance of Bayesian statistical inference in the DDPMO model against the performance of some others state-of-the-art models and methods (not necessarily Bayesian) is also provided in (Neiswanger et al., 2014).", "startOffset": 241, "endOffset": 266}, {"referenceID": 51, "context": "The DDPMO model, as presented in (Neiswanger et al., 2014), uses conjugate priors.", "startOffset": 33, "endOffset": 58}, {"referenceID": 14, "context": "Conjugate priors may be implemented in languages like Church (Goodman et al., 2008), Anglican (Wood et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 78, "context": ", 2008), Anglican (Wood et al., 2014) and Venture (Mansinghka et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 42, "context": ", 2014) and Venture (Mansinghka et al., 2014) in the form of exchangeable random procedures (XRPs).", "startOffset": 20, "endOffset": 45}, {"referenceID": 6, "context": "For our experiments, we chose a soccer video dataset (D\u2019Orazio et al., 2009), for which there already exists a human-authored ground truth.", "startOffset": 53, "endOffset": 76}, {"referenceID": 25, "context": "The work on using discriminative proposals for Markov Chain Monte Carlo in parametric generative models include (Tu and Zhu, 2002) and (Jampani et al., 2015b), with applications in computer vision.", "startOffset": 135, "endOffset": 158}, {"referenceID": 18, "context": "Recent work with sequential Monte Carlo includes neural adaptive SMC (Gu et al., 2015), where authors also adapt proposals by descending the inclusive Kullback-Leibler divergence between the proposal and the true posterior distributions on hidden variables given observations.", "startOffset": 69, "endOffset": 86}, {"referenceID": 35, "context": "Another related recent work is a new probabilistic programming language called Picture (Kulkarni et al., 2015), for which authors propose and describe the use of data-driven proposals in the context of models for computer vision.", "startOffset": 87, "endOffset": 110}], "year": 2016, "abstractText": "This thesis describes work on two applications of probabilistic programming: the learning of probabilistic program code given specifications, in particular program code of one-dimensional samplers; and the facilitation of sequential Monte Carlo inference with help of data-driven proposals. The latter is presented with experimental results on a linear Gaussian model and a non-parametric dependent Dirichlet process mixture of objects model for object recognition and tracking. We begin this work by providing a brief introduction to probabilistic programming. In the second Chapter we present an approach to automatic discovery of samplers in the form of probabilistic programs. Specifically, we learn the procedure code of samplers for one-dimensional distributions. We formulate a Bayesian approach to this problem by specifying a grammar-based prior over probabilistic program code. We use an approximate Bayesian computation method to learn the programs, whose executions generate samples that statistically match observed data or analytical characteristics of distributions of interest. In our experiments we leverage different probabilistic programming systems, including Anglican and Probabilistic C, to perform Markov chain Monte Carlo sampling over the space of programs. Experimental results have demonstrated that, using the proposed methodology, we can learn approximate and even some exact samplers. Finally, we show that our results are competitive with regard to genetic programming methods.", "creator": "LaTeX with hyperref package"}}}