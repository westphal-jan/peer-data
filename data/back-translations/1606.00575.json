{"id": "1606.00575", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Ensemble-Compression: A New Method for Parallel Training of Deep Neural Networks", "abstract": "Typically, the parameters of the local models are communicated and averaged periodically to obtain a global model until the training curve converges (referred to as MA-DNN). However, since DNN is a highly non-convex model, the global model achieved by averaging parameters does not guarantee its performance improvement over the local models and may even be worse than the average performance of the local models, which slows convergence and reduces final performance. To address this problem, we propose a new parallel training method called\\ emph {ensemble compression} (referred to as EC-DNN). Specifically, we propose to aggregate the local models by ensembles, i.e. the results of the local models are averaged and not compressed by the parameters.", "histories": [["v1", "Thu, 2 Jun 2016 08:10:10 GMT  (59kb,D)", "https://arxiv.org/abs/1606.00575v1", null], ["v2", "Tue, 18 Jul 2017 08:50:05 GMT  (100kb,D)", "http://arxiv.org/abs/1606.00575v2", "ECML 2017"]], "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.NE", "authors": ["shizhao sun", "wei chen", "jiang bian", "xiaoguang liu", "tie-yan liu"], "accepted": false, "id": "1606.00575"}, "pdf": {"name": "1606.00575.pdf", "metadata": {"source": "CRF", "title": "Ensemble-Compression: A New Method for Parallel Training of Deep Neural Networks", "authors": ["Shizhao Sun", "Wei Chen", "Jiang Bian", "Xiaoguang Liu", "Tie-Yan Liu"], "emails": ["sunshizhao@mail.nankai.edu.cn,", "wche@microsoft.com,", "jiabia@microsoft.com,", "liuxg@nbjl.nankai.edu.cn,", "tyliu@microsoft.com."], "sections": [{"heading": null, "text": "Keywords: Parallel machine learning, Distributed machine learning, Deep learning, Ensemble method."}, {"heading": "1 Introduction", "text": "Recent rapid development of deep neural networks (DNN) has demonstrated that its great success mainly comes from big data and big models [25,13]. However, it is extremely time-consuming to train a large-scale DNN model over big data.\n? This work was done when the author was visiting Microsoft Research Asia.\nar X\niv :1\n60 6.\n00 57\n5v 2\n[ cs\n.D C\n] 1\n8 Ju\nTo accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used. A typical parallel training procedure for DNN consists of continuous iterations of the following three steps. First, each worker trains the local model based on its own local data by stochastic gradient decent (SGD) or any of its variants. Second, the parameters of the local DNN models are communicated and aggregated to obtain a global model, e.g., by averaging the identical parameter of each local models [27,20]. Finally, the obtained global model is used as a new starting point of the next round of local training. We refer the method that performs the aggregation by averaging model parameters as MA, and the corresponding parallel implementation of DNN as MA-DNN.\nHowever, since DNN is a highly non-convex model, the loss of the global model produced by MA cannot be guaranteed to be upper bounded by the average loss of the local models. In other words, the global model obtained by MA-DNN might even perform worse than any local model, especially when the local models fall into different local-convex domains. As the global model will be used as a new starting point of the successive iterations of local training, the poor performance of the global model will drastically slow down the convergence of the training process and further hurt the performance of the final model.\nTo tackle this problem, we propose a novel framework for parallel DNN training, called Ensemble-Compression (EC-DNN), the key idea of which is to produce the global model by ensemble instead of MA. Specifically, the ensemble method aggregates local models by averaging their outputs rather than their parameters. Equivalently, the global model produced by ensemble is a larger network with one additional layer which takes the outputs of local models as inputs with weights as 1/K, where K is the number of local models. Since most of widely-used loss functions for DNN (e.g., cross entropy loss, square loss, and hinge loss) are convex with respect to the output vector of the model, the loss of the global model produced by ensemble can be upper bounded by the average loss of the local models. Empirical evidence in [25,5] even show that the ensemble model of DNN, i.e., the global model, is usually better than any base model, i.e., the local model. According to previous theoretical and empirical studies [17,24], ensemble model tend to yield better results when there exists a significant diversity among local models. Therefore, we train the local models for a longer period for EC-DNN to increase the diversity among them. In other words, EC-DNN yields less communication frequency than MA-DNN, which further emphasizes the advantages of EC-DNN by reducing communication cost as well as increasing robustness to limited network bandwidth.\nThere is, however, no free lunch. In particular, the ensemble method will critically increase the model complexity: the resultant global model with one additional layer will be K times wider than any of the local models. Several ensemble iterations may result in explosion of the size of the global model. To address this problem, we further propose an additional compression step after the ensemble. This approach cannot only restrict the size of the resultant global model to be the same size as local ones, but also preserves the advantage of ensemble over\nMA. Given that both ensemble and compression steps are dispensable in our new parallelization framework, we name this framework as EC-DNN. As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models. Nevertheless, such distillation method requires extra time for training the compressed model. To tackle this problem, we seek to integrate the model compression into the process of local training by designing a new combination loss, a weighted interpolation between the loss based on the pseudo labels produced by global model and that based on the true labels. By optimizing such combination loss, we can achieve model compression in the meantime of local training.\nWe conducted comprehensive experiments on CIFAR-10, CIFAR-100, and ImageNet datasets, w.r.t. different numbers of local workers and communication frequencies. The experimental results illustrate a couple of important observations: 1) Ensemble is a better model aggregation method than MA consistently. MA suffers from that the performance of the global model could vary drastically and even be much worse than the local models; meanwhile, the global model obtained by the ensemble method can consistently outperform the local models. 2) In terms of the end-to-end results, EC-DNN stably achieved better test accuracy than MA-DNN in all the settings. 3) EC-DNN can achieve better performance than MA-DNN even when EC-DNN communicates less frequently than MA-DNN, which emphasizes the advantage of EC-DNN in training a large-scale DNN model as it can significantly reduce the communication cost."}, {"heading": "2 Preliminary: Parallel Training of DNN", "text": "In the following of this paper, we denote a DNN model as f(w) where w represents the parameters of this DNN model. In addition, we denote the outputs of the model f(w) on the input x as f(w;x) = (f(w;x, 1), . . . , f(w;x,C)), where C is the number of classes and f(w;x, c) denotes the output (i.e., the score) for the c-th class. DNN is a highly non-convex model due to the non-linear activations and poolings after many layer.\nIn the parallel training of DNN, suppose that there are K workers and each of them holds a local dataset Dk = {(xk,1, yk,1), . . . , (xk,mk , yk,mk)} with size mk, k \u2208 {1, . . . ,K}. Denote the weights of the DNN model at the iteration t on the worker k as wtk. The communication between the workers is invoked after every \u03c4 iterations of updates for the weights, and we call \u03c4 the communication frequency. A typical parallel training procedure for DNN implements the following three steps in an iterative manner until the training curve converges. 1. Local training: At iteration t, worker k updates its local model by using SGD. Such local model is updated for \u03c4 iterations before the cross-machine synchronization. 2. Model aggregation: The parameters of local models are communicated across machines. Then, a global model is produced by aggregating local models according to certain aggregation method.\n3. Local model reset: The global model is sent back to the local workers, and set as the starting point for the next round of local training.\nWe denote the aggregation method in the second step as G(wt1, . . . ,w t K) and the weights of the global model as w\u0304t. That is, f(w\u0304t) = G(wt1, . . . ,w t K), where t = \u03c4, 2\u03c4, \u00b7 \u00b7 \u00b7. A widely-used aggregation method is model average (MA), which averages each parameter over all the local models, i.e.,\nGMA ( wt1, . . . ,w t K ) = f\n( 1\nK K\u2211 k=1 wtk\n) , t = \u03c4, 2\u03c4, \u00b7 \u00b7 \u00b7 . (1)\nWe denote the parallel training method of DNN that using MA as MA-DNN for ease of reference.\nWith the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN. NG-SGD [20] proposes an approximate and efficient implementation of Natural Gradient for SGD (NGSGD) to improve the performance of MA-DNN. EASGD [26] improves MA-DNN by adding an elastic force which links the weights of the local models with the weights of the global model. BMUF [3] leverages data parallelism and blockwise model-update filtering to improve the speedup of MA-DNN. All these methods aim at solving different problems with us, and our method can be used with those methods simultaneously."}, {"heading": "3 Model Aggregation: MA vs. Ensemble", "text": "In this section, we first reveal why the MA method cannot guarantee to produce a global model with better performance than local models. Then, we propose to use ensemble method to perform the model aggregation, which in contrast can ensure to perform better over local models.\nMA was originally proposed for convex optimization. If the model is convex w.r.t. the parameters and the loss is convex w.r.t. the model outputs, the performance of the global model produced by MA can be guaranteed to be no worse than the average performance of local models. This is because, when f(\u00b7) is a convex model, we have,\nL ( f ( w\u0304t;x ) , y ) = L ( f ( 1\nK K\u2211 k=1 wtk;x\n) , y ) \u2264 L ( 1\nK K\u2211 k=1 f ( wtk;x ) , y\n) . (2)\nMoreover, when the loss is also convex w.r.t. the model outputs f(\u00b7;x), we have,\nL\n( 1\nK K\u2211 k=1 f ( wtk;x ) , y ) \u2264 1 K K\u2211 k=1 L ( f ( wtk;x ) , y ) . (3)\nBy combining inequalities (2) and (3), we can see that it is quite effective to apply MA in the context of convex optimization, since the loss of the global model by MA is no greater than the average loss of local models in such context.\nHowever, DNN is indeed a highly non-convex model due to the existence of activation functions and pooling functions (for convolutional layers). Therefore,\nthe above properties of MA for convex optimization does not hold for DNN such that the MA method cannot produce any global model with guaranteed better performance than local ones. Especially, when the local models are in the neighborhoods of different local optima, the global model based on MA could be even worse than any of the local models. Furthermore, given that the global model is usually used as the starting point of the next round of local training, the performance of the final model could hardly be good if a global model in any round fails to achieve good performance. Beyond the theoretical analysis above, the experimental results reported in Section 5.3 and previous studies [20,3] also revealed such problem.\nWhile the DNN model itself is non-convex, we notice that most of widely-used loss functions for DNN are convex w.r.t. the model outputs (e.g., cross entropy loss, square loss, and hinge loss). Therefore, Eq.(3) holds, which indicates that averaging the output of the local models instead of their parameters guarantees to yield better performance than local models. To this end, we propose to ensemble the local models by averaging their outputs as follows,\nGE ( wt1, . . . ,w t K ) = 1\nK K\u2211 k=1 f ( wtk;x ) , t = \u03c4, 2\u03c4, \u00b7 \u00b7 \u00b7 . (4)"}, {"heading": "4 EC-DNN", "text": "In this section, we first introduce the EC-DNN framework, which employs ensemble for model aggregation. Then, we introduce a specific implementation of EC-DNN that adopts distillation for the compression. At last, we take some further discussions for the time complexity of EC-DNN and the comparison with traditional ensemble methods."}, {"heading": "4.1 Framework", "text": "The details of EC-DNN framework is shown in Alg. 1. Note that, in this paper, we focus on the synchronous case3 within the MapReduce framework, but EC-DNN can be generalized into the asynchronous case and parameter server framework as well. Similar to other popular parallel training methods for DNN, EC-DNN iteratively conducts local training, model aggregation, and local model reset.\n1. Local training: The local training process of EC-DNN is the same as that of MA-DNN, in which the local model is updated by SGD. Specifically, at iteration t, worker k updates its local model from wtk to w t+1 k by minimizing the training loss using SGD, i.e., wt+1k = w t k \u2212 \u03b7\u2206(L(f(wtk;xk), yk)), where \u03b7 is the learning rate, and \u2206(L(f(wtk;xk), yk)) is the gradients of the empirical loss L(f(wtk;xk), yk) of the local model f(wtk) on one mini batch of the local dataset 3 As shown in [2], MA-DNN in synchronous case converges faster and achieves better\ntest accuracy than that in asynchronous case.\nDk. One local model will be updated for \u03c4 iterations before the cross-machine synchronization.\n2. Model aggregation: The goal of model aggregation is to communicate the parameters of local models, i.e., wt1 . . .w t K , across machines. To this end, a global model is produced by ensemble according to GE(w t 1, . . . ,w t K) in Eq.(4), i.e., averaging the outputs of the local models. Equivalently, the global model produced by ensemble is a larger network with one additional layer, whose outputs consist of C nodes representing C classes, and whose inputs are those outputs from local models with weights as 1/K, where K is the number of local models. Therefore, the weights of global model w\u0304t can be denoted as w\u0304t = {wt1, . . . ,wtK , 1K }, t = \u03c4, 2\u03c4, . . .\nNote that such ensemble-produced global model (i.e., f(w\u0304t)) is one layer deeper and K times wider than the local model (i.e., f(wkt )). Therefore, continuous rounds of ensemble process will easily give rise to a global model with exploding size. To avoid this problem, we propose introducing a compression process (i.e., Compression(wtk, w\u0304\nt, Dk) in Alg. 1) after ensemble process, to compress the resultant global model to be the same size as those local models while preserving the advantage of the ensemble over MA. We denote the compressed model for the global model w\u0304t on the worker k as w\u0303 t k.\nAlgorithm 1: EC-DNN(Dk)\nRandomly initialize w0k and set t = 0; while stop criteria are not satisfied do\nwt+1k \u2190 w t k \u2212 \u03b7\u2206(L(f(wtk;xk), yk)); t\u2190 t+ 1; if \u03c4 divides t then\nw\u0304t \u2190 {wt1, . . . ,wtK , 1K }; w\u0303tk \u2190 Compression(wtk, w\u0304t, Dk); wtk \u2190 w\u0303tk.\nreturn wtk\n3. Local model reset: The compressed model will be set as the new starting point of the next round of local training, i.e., wtk = w\u0303t where t = \u03c4, 2\u03c4, \u00b7 \u00b7 \u00b7.\nAt the end of the training process, EC-DNN will output K local models and we choose the model with the smallest training loss as the final one. Note that, we can also take the global model (i.e., the ensemble of K local models) as the final model if there are enough computation and storage resources for the test."}, {"heading": "4.2 Implementations", "text": "Algorithm 1 contains two sub-problems that need to be addressed more concretely: 1) how to train local models that can benefit more to the ensemble model; 2) how to compress the global model without costing too much extra time.\nDiversity Driven Local Training. In order to improve the performance of ensemble, it is necessary to generate diverse local models other than merely accurate ones [17,24]. Therefore, in the local training phase, i.e., the third line\nin Alg. 1, we minimize both the loss on training data and the similarity between the local models, which we call diversity regularized local training loss. For the k-th worker, it is defined as follows,\nLkLS(f(wk;xk,i), yk,i) = mk\u2211 i=1 (L (f(wk;xk,i), yk,i) + \u03b1Lsim (f (wk;xk,i) , z\u0304k,i)) , (5)\nwhere z\u0304y,i is the average of the outputs of the latest compressed models for input xk,i. In our experiments, the local training loss L takes the form of cross entropy, and the similarity loss Lsim takes the form of \u2212l2 distance. The smaller Lsim is, the farther the outputs of a local model is from the average of outputs of the latest compressed models, and hence the farther (or the more diverse) the local models are from (or with) each other.\nDistillation Based Compression. In order to compress the global model to the one with the same size as the local model, we use distillation base compression method4 [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model. In order to save the time for compression, in compression process, we minimize the weighted combination of the local training loss and the pure compression loss, which we call accelerated compression loss. For the k-th worker, it is defined as follows,\nLkLC(f(wk;xk,i), yk,i) = mk\u2211 i=1 (L (f (wk;xk,i) , yk,i) + \u03b2Lcomp (f (wk;xk,i) , y\u0304k,i)) , (6)\nwhere y\u0304k,i is the output of the latest ensemble model for the input xk,i. In our experiments, the local training loss L and the pure compression loss Lcomp both take the form of cross entropy loss. By reducing the loss between f (wk;xk,i) and the pseudo labels {y\u0304k,i; i \u2208 [mk]}, the compressed model can play the similar function as the ensemble model.\nWe denote the distillation based compression process as Compressiondistill (wtk, w\u0304 t, Dk), and show its details in Alg. 2. First, on the k-th local worker, we construct a new training data D\u0302k by relabeling the original dataset Dk with the pseudo labels produced by the global model, i.e., {y\u0304k,i, i \u2208 [mk]}. Specifically, when producing pseudo labels, we first produce the predictions of each local models respectively, and then average the predictions of all the local models. By this way, we keep using the same amount of GPU memory as MA-DNN throughout the training, because the big global model, which is K times larger than the local model, has never been established in GPU memory. Then, we optimize the accelerated compression loss LkLC in Eq.(6) by SGD for p iterations. We initialize the parameters of the compressed model w\u0303tk by the parameters of the latest local model wtk instead of random numbers. Finally, the obtained compressed model w\u0303t+pk is returned, which will be set as the new starting point of next round of the local training.\n4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.\nAlgorithm 2: Compressiondistill(w t k, w\u0304 t k, Dk)\nfor j \u2208 [mk] do for c \u2208 [C] do y\u0304k,j,c \u2190 1K \u2211K r=1 f(wtr;xk,j , c);\ny\u0304k,j = (y\u0304k,j,1, . . . , y\u0304k,j,C);\nD\u0302k \u2190 {(xk,1, yk,1, y\u0304k,1), . . . , (xk,mk , yk,mk , y\u0304k,mk )}; Set w\u0303tk = w t k and i = 0; while i \u2264 p do w\u0303t+i+1k \u2190 w\u0303 t+i k \u2212 \u03b7\u2206(L k LC(f(w\u0303 t+i k ;xk), yk));\ni\u2190 i+ 1; return w\u0303t+pk . We can find that minimizing the diversity regularized loss LkLS for local training (Eq.(5)) and minimizing the accelerated compression loss LkLC for compression (Eq.(6)) are two opposite but complementary tasks. They need to leverage information generated by each other into their own optimization. Specifically, the local training phase leverages z\u0304k,i based on compressed model while the compression process uses y\u0304k,i provided by local models. Due to such structural duality, we take advantage of a new optimization framework, i.e. dual learning [12], to improve the performance of both tasks simultaneously."}, {"heading": "4.3 Time Complexity", "text": "We compare the time complexity of MA-DNN and EC-DNN from two aspects: 1. Communication time: Parallel DNN training process is usually sensitive to the communication frequency \u03c4 . Different parallelization frameworks yield various optimal \u03c4 . In particular, EC-DNN prefers larger \u03c4 compared to MADNN. Essentially, less frequent communication across workers can give rise to more diverse local models, which will lead to better ensemble performance for EC-DNN. On the other hand, much diverse local models may indicate greater probability that local models are in the neighboring of different local optima such that the global model in MA-DNN is more likely to perform worse than local ones. The poor performance of the global model will significantly slow down the convergence and harm the performance of the final model. Therefore, EC-DNN yields less communication time than MA-DNN.\n2. Computational time: According to the analysis in Sec 4.2, EC-DNN does not consume extra computation time for model compression since the compression process has been integrated into the local training phase, as shown in Eq.(6). Therefore, compared with MA-DNN, EC-DNN only requires additional time to relabel the local data using the global model, which approximately equals to the maximal time of the feed-forward propagation over the local dataset. We call such extra time \u201crelabeling time\u201d for ease of reference. To limit the relabeling time on large datasets, we choose to relabel a portion of the local data, denoted as \u00b5. Our experimental results in Section 5.3 will demonstrate that the relabeling time can be controlled within a very small amount compared to the training time of DNN. Therefore, EC-DNN can cost only a slightly more or roughly equal computational time over MA-DNN.\nOverall, compared to MA-DNN, EC-DNN is essentially more time-efficient as it can reduce the communication cost without significantly increasing computational time."}, {"heading": "4.4 Comparison with Traditional Ensemble Methods", "text": "Traditional ensemble methods for DNN [25,5] usually first train several DNN models independently without communication and make ensemble of them in the end. We denote such method as E-DNN. E-DNN was proposed to improve the accuracy of DNN models by reducing variance and it has no necessity to train base models with parallelization framework. In contrast, EC-DNN is a parallel algorithm aiming at training DNN models faster without the loss of the accuracy by leveraging a cluster of machines.\nAlthough E-DNN can be viewed as a special case of EC-DNN with only one final communication and no compression process, the intermediate communications in EC-DNN will make it outperform E-DNN. The reasons are as follows: 1) local workers has different local data, the communications during the training will help local models to be consensus towards the whole training data; 2) the local models of EC-DNN can be continuously optimized by compressing the ensemble model after each ensemble process. Then, another round of ensemble will result in more advantage for EC-DNN over E-DNN since the local models of EC-DNN has been much improved."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "Platform. Our experiments are conducted on a GPU cluster interconnected with an InfiniBand network, each machine of which is equipped with two Nvdia\u2019s K20 GPU processors. One GPU processor corresponds to one local worker.\nData. We conducted experiments on public datasets CIFAR-10, CIFAR-100 [16] and ImageNet (ILSVRC 2015 Classification Challenge) [23]. For all the datasets, each image is normalized by subtracting the per-pixel mean computed over the whole training set. The training images are horizontally flipped but not cropped, and the test data are neither flipped nor cropped.\nModel. On CIFAR-10 and CIFAR-100, we employ NiN [19], a 9-layer convolutional network. On ImageNet, we use GoogLeNet [25], a 22-layer convolutional network. We used the same tricks, including random initialization, l2regularization, Dropout, and momentum, as the original paper. All the experiments are implemented using Caffe [15].\nParallel Setting. On experiments on CIFAR-10 and CIFAR-100, we explore the number of workers K \u2208 {4, 8} and the communication frequency \u03c4 \u2208 {1, 16, 2000, 4000} for both MA-DNN and EC-DNN. On experiments on ImageNet, we explore K \u2208 {4, 8} and \u03c4 \u2208 {1, 1000, 10000}. The communication across local workers is implemented using MPI.\nHyperparameters Setting of EC-DNN. There are four hyperparameters in ECDNN, including 1) the coefficient of the regularization in terms of similarity between local models, i.e., \u03b1 in Eq.(5), 2) the coefficient of the model compression loss, i.e., \u03b2 in Eq.(6), 3) the length of the compression process, i.e., p in Alg. 2, and 4) the portion of the data needed to be relabeled in the compression process \u00b5 as mentioned in Sec 4.3. We tune these hyperparameters by exploring a certain range of values and then choose the one resulting in best performance. In particular, we explored \u03b1 among {0.2, 0.4, . . . , 1}, and finally choose \u03b1 = 0.6 on all the datasets. To decide the value of \u03b2, we explored two strategies, one of which uses consistent \u03b2 at each compression process while the other employs increasing \u03b2 after a certain percentage of compression process. In the first strategy, we explored \u03b2 among {0.2, 0.4, . . . , 1}, and in the second one, we explored \u03b2 among {0.2, 0.4, . . . , 1}, the incremental step of \u03b2 among {0.1, 0.2}, and the percentage of compression process from which \u03b2 begins to increase among {10%, 20%, 30%}. On CIFAR datasets, we choose to use \u03b2 = 0.4 for the first 20% of compression processes and \u03b2 = 0.6 for all the other compression processes. And, on ImageNet, we choose to use consistent \u03b2 = 1 throughout the compression. Moreover, we explored p\u2019s value among {5%, 10%, . . . , 20%} of the number of the mini-batches that the whole training lasts, and finally pick p = 10% on all the datasets. Furthermore, we explored \u00b5\u2019s value among {30%, 50%, 70%}. And, we finally select \u00b5 = 70% on CIFAR datasets, and \u00b5 = 30% on ImageNet."}, {"heading": "5.2 Compared Methods", "text": "We conduct performance comparisons on four methods:\n\u2013 S-DNN denotes the sequential training on one GPU until convergence [19,25]. \u2013 E-DNN denotes the method that trains local models independently and\nmakes ensemble of the local models merely at the end of the training [25,5]. \u2013 MA-DNN refers the parallel DNN training framework with the aggregation\nby averaging model parameters [27,20,6,26,2,3]. \u2013 EC-DNN refers the parallel DNN training framework with the aggregation\nby averaging model outputs. EC-DNN applies Compressiondistill for the compression for all the experiments in this paper.\nFurthermore, we use EC-DNNL, MA-DNNL and E-DNNL to denote the corresponding methods that take the local model with the smallest training loss as the final model, and use EC-DNNG, MA-DNNG and E-DNNG to represent the respective methods that take the global model (i.e., the ensemble of local models for EC-DNN and E-DNN, and the average parameters of local models for MA-DNN) as the final model."}, {"heading": "5.3 Experimental Results", "text": "Model Aggregation. We first compare the performance of aggregation methods, i.e. MA and Ensemble. We employ DiffLG as the evaluation metric, which\n2 0 2 4 6 8 10 12 DiffLG\n(a) CIFAR-10\n2 0 2 4 6 8 10 12 DiffLG\n(b) CIFAR-100\nFig. 2: Ensemble.\n2 0 2 4 6 8 10 12 DiffLC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFr e q u e n cy\n(a) CIFAR-10\n2 0 2 4 6 8 10 12 DiffLC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFr e q u e n cy\n(b) CIFAR-100\nFig. 3: Compression.\nmeasures the improvement of the test error of the global model compared to that of the local models, i.e.,\nDiffLG = 1\nK K\u2211 k=1 errork \u2212 errorglobal, (7)\nwhere errork denotes the test error of the local model on worker k, and errorglobal denotes the test error of the corresponding global model produced by MA (or ensemble) in MA-DNN (or EC-DNN). The positive (or negative) DiffLG means performance improvement (or drop) of global models over local models. On each dataset, we produce a distribution for DiffLG over all the communications and all the parallel settings (including numbers of workers and communication frequencies). We show the distribution for DiffLG of MA and ensemble on CIFAR datasets in Fig. 1 and 2 respectively, in which red bars (or blue bars) stand for that the performance of the global model is worse (or better) than the average performance of local models.\nFor MA, from Fig. 1, we can observe that, on both datasets, over 10% global models achieve worse performance than the average performance of local models, and the average performance of locals model can be worse than the global model by a large margin, e.g., 30%. On the other hand, for ensemble, we can observe from Fig. 2 that the performance of the global model is consistently better than the average performance of the local models on both datasets. Specifically, the performances of over 20% global models are 5+% better than the average performance of local models on both datasets.\nModel Compression. In order to avoid model size explosion, the ensemble model is compressed before the next round of local training. However, such compression may result in a risk of performance loss. To examine if the performance improvement of the compressed global models over those local ones in EC-DNN can still outperform such kind of improvement in MA-DNN, we compare DiffLG in MA-DNN (see Eq.(7)) with DiffLC in EC-DNN,\nDiffLC = 1\nK K\u2211 k=1 (errork \u2212 errorcompress,k) , (8)\nwhere errork denotes the test error of the local model on worker k, and errorcompress,k denotes the test error of the corresponding compressed model after compressing the ensemble model of those local models on worker k. The positive (or negative) DiffLC means performance improvement (or drop) of the compressed model over local ones. Figure 3 illustrates the distribution of DiffLC over all the communications and various settings of communication frequency and the number of workers on two CIFAR datasets.\nFrom Fig. 3, we can observe that the average performance of the compressed models is consistently better than that of the local models on both datasets in EC-DNN, while Figure 1 indicates that there are over 10% global models do not reaching better performance than the local ones in MA-DNN. In addition, the average improvement of compressed models over local ones in EC-DNN is greater than that in MA-DNN. Specifically, the average of such improvements in EC-DNN are 1.03% and 1.95% on CIFAR-10 and CIFAR-100, respectively, while the average performance difference in MA-DNN are -3.53% and 1.72% on CIFAR-10 and CIFAR-100 respectively. All these results can indicate that EC-DNN is a superior method than MA-DNN.\nAccuracy. In the following, we examine the accuracy of compared methods. Figure 4 shows the test error of the global model during the training process w.r.t. the overall time, and Table 1 reports the final performance after the training process converges. For EC-DNN, the relabeling time has been counted in the overall time when plotting the figure and the table. We report EC-DNN and MA-DNN that achieve best test performance among all the communication frequencies.\nFrom Fig. 4, we can observe that EC-DNNG outperforms MA-DNNG and S-DNN on both datasets for all the number of workers, which demonstrates that EC-DNN is superior to MA-DNN. Specifically, at the early stage of training, EC-DNNG may not outperform MA-DNNG. We hypothesize the reason as the very limited number of communications among local works at the early stage of EC-DNN training. Along with increasing rounds of communications, EC-DNN will catch up with and then keep outperforming MA-DNN after the certain time slot. Besides, EC-DNNG outperforms E-DNNG consistently for different datasets and number of workers, indicating that technologies in EC-DNN are not trivial improvements of E-DNN but is the key factor of the success of EC-DNN.\nIn Table 1, each EC-DNNG outperforms MA-DNNL and MA-DNNG. The average improvements of EC-DNNG over MA-DNNL and MA-DNNG are around 1% and 5% for CIFAR-10 and CIFAR-100 respectively. Besides, we also report the final performance of EC-DNNL considering that it can save test time and still outperform both MA-DNNL and MA-DNNG when we do not have enough computational and storage resource. Specifically, the best EC-DNNL achieved test errors of 10.04% and 9.88% for K = 4 and K = 8 respectively on CIFAR10, while it achieved test errors of 34.8% and 35.1% for K = 4 and K = 8 respectively on CIFAR-100. In addition, E-DNNL never outperforms MA-DNNL and MA-DNNG.\nSpeed. According to our analysis in Sec 4.3, EC-DNN is more time-efficient than MA-DNN because it communicates less frequently than MA-DNN and thus costs less time on communication. To verify this, we measure the overall time cost by each method to achieve the same accuracy. Table 1 shows the speed of compared methods. In this table, we denote the speed of MA-DNNG as 1, and normalize the speed of other methods by dividing that of MA-DNNG. If one method never achieves the same performance with MA-DNNG, we denote its speed as 0. Therefore, larger value of speed indicates better speedup.\nFrom Table 1, we can observe that EC-DNN can achieve better speedup than MA-DNN on all the datasets. On average, EC-DNNG and EC-DNNL runs about 2.24 and 1.33 times faster than MA-DNNG, respectively. Furthermore, EC-DNN consistently results in better speedup than E-DNN on all the datasets. On average, E-DNNG only runs about 1.85 times faster than MA-DNNG while EC-DNNG can reach about 2.24 times faster speed. From this table, we can also find that E-DNNL never achieves the same performance with MA-DNNG while EC-DNNL can contrarily run much faster than MA-DNNG.\nFurthermore, Table 1 demonstrates the communication frequency \u03c4 that makes compared methods achieve the corresponding speed. We can observe that EC-DNN tend to communicate less frequently than MA-DNN. Specifically, MADNN usually achieves the best performance with a small \u03c4 (i.e., 16), while ECDNN cannot reach its best performance before \u03c4 is not as large as 2000.\nLarge-Scale Experiments. In the following, we will conduct experiments to compare the performance of MA-DNN with that of EC-DNN with the setting of much bigger model and more data, i.e., GoogleNet on ImageNet. Figure 5 shows the test error of the global model w.r.t the overall time. The communication frequencies \u03c4 that makes MA-DNN and EC-DNN achieve best performance are 1 and 1000 respectively. We can observe that EC-DNN consistently achieves better test performance than S-DNN, MA-DNN and E-DNN throughout the training. Besiedes, we can observe that EC-DNN outperforms MA-DNN even at the early stage of the training, while EC-DNN cannot achieve this on CIFAR datasets because it communicates less frequently than MA-DNN. The reason is that frequent communication will make the training much slower for very big model, i.e., use less mini-batches of data within the same time. When the improvements introduced by MA cannot compensate the decrease of the number of used data, MA-DNN no longer outperforms EC-DNN at the early stage of the training. In this case, the advantage of EC-DNN becomes even more outstanding."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we propose EC-DNN, a new Ensemble-Compression based parallel training framework for DNN. As compared to the traditional approach, MA-DNN, that averages the parameters of different local models, our proposed method uses the ensemble method to aggregate local models. In this way, we can guarantee that the error of the global model in EC-DNN is upper bounded by the average error of the local models and can consistently achieve better performance than MA-DNN. In the future, we plan to consider other compression methods for EC-DNN. Besides, we will investigate the theoretical properties of the ensemble method, compression method, and the whole EC-DNN framework."}, {"heading": "7 Acknowledgments", "text": "This work is partially supported by NSF of China (grant numbers: 61373018, 61602266, 11550110491) and NSF of Tianjin (grant number: 4117JCYBJC15300)."}], "references": [{"title": "Model compression", "author": ["C. Bucilua", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM Conference on Knowledge Discovery and Data Mining. pp. 535\u2013 541. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering", "author": ["K. Chen", "Q. Huo"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. pp. 5880\u20135884. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "Proceedings of the 32st International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642\u20133649. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Le", "Q.V"], "venue": "Advances in Neural Information Processing Systems. pp. 1223\u20131231", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM 51(1), 107\u2013113", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N de Freitas"], "venue": "Advances in Neural Information Processing Systems. pp. 2148\u20132156", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems. pp. 1269\u20131277", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems 28. pp. 1135\u20131143", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Dual learning for machine translation", "author": ["D. He", "Y. Xia", "T. Qin", "L. Wang", "N. Yu", "T. Liu", "W.Y. Ma"], "venue": "Advances in Neural Information Processing Systems 29, pp. 820\u2013828", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision. pp. 1026\u20131034", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. rep., University of Toronto", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Measures of diversity in classifier ensembles", "author": ["L. Kuncheva", "C. Whitaker"], "venue": "Machine Learning, pp. 181\u2013207", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B.Y. Su"], "venue": "11th USENIX Symposium on Operating Systems Design and Implementation. pp. 583\u2013598", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Network in network", "author": ["L. Min", "C. Qiang", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. pp. 2754\u20132761. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV) 115(3), 211\u2013252", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with ensembles: How overfitting can be useful", "author": ["P. Sollich", "A. Krogh"], "venue": "Advances in Neural Information Processing Systems, vol. 8, pp. 190\u2013196", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning with elastic averaging sgd", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems 28, pp. 685\u2013693", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving deep neural network acoustic models using generalized maxout networks", "author": ["X. Zhang", "J. Trmal", "D. Povey", "S. Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 215\u2013219. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "Recent rapid development of deep neural networks (DNN) has demonstrated that its great success mainly comes from big data and big models [25,13].", "startOffset": 137, "endOffset": 144}, {"referenceID": 12, "context": "Recent rapid development of deep neural networks (DNN) has demonstrated that its great success mainly comes from big data and big models [25,13].", "startOffset": 137, "endOffset": 144}, {"referenceID": 6, "context": "To accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used.", "startOffset": 77, "endOffset": 80}, {"referenceID": 17, "context": "To accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used.", "startOffset": 101, "endOffset": 107}, {"referenceID": 5, "context": "To accelerate the training of DNN, parallelization frameworks like MapReduce [7] and Parameter Server[18,6] have been widely used.", "startOffset": 101, "endOffset": 107}, {"referenceID": 26, "context": ", by averaging the identical parameter of each local models [27,20].", "startOffset": 60, "endOffset": 67}, {"referenceID": 19, "context": ", by averaging the identical parameter of each local models [27,20].", "startOffset": 60, "endOffset": 67}, {"referenceID": 24, "context": "Empirical evidence in [25,5] even show that the ensemble model of DNN, i.", "startOffset": 22, "endOffset": 28}, {"referenceID": 4, "context": "Empirical evidence in [25,5] even show that the ensemble model of DNN, i.", "startOffset": 22, "endOffset": 28}, {"referenceID": 16, "context": "According to previous theoretical and empirical studies [17,24], ensemble model tend to yield better results when there exists a significant diversity among local models.", "startOffset": 56, "endOffset": 63}, {"referenceID": 23, "context": "According to previous theoretical and empirical studies [17,24], ensemble model tend to yield better results when there exists a significant diversity among local models.", "startOffset": 56, "endOffset": 63}, {"referenceID": 0, "context": "As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models.", "startOffset": 89, "endOffset": 98}, {"referenceID": 21, "context": "As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models.", "startOffset": 89, "endOffset": 98}, {"referenceID": 13, "context": "As a specialization of the EC-DNN framework, we adopt the distillation based compression [1,22,14], which produces model compression by distilling the predictions of big models.", "startOffset": 89, "endOffset": 98}, {"referenceID": 26, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 5, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 25, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 1, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 2, "context": "With the growing efforts in parallel training for DNN, many previous studies [27,20,6,26,2,3] have paid attention to MA-DNN.", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "NG-SGD [20] proposes an approximate and efficient implementation of Natural Gradient for SGD (NGSGD) to improve the performance of MA-DNN.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "EASGD [26] improves MA-DNN by adding an elastic force which links the weights of the local models with the weights of the global model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "BMUF [3] leverages data parallelism and blockwise model-update filtering to improve the speedup of MA-DNN.", "startOffset": 5, "endOffset": 8}, {"referenceID": 19, "context": "3 and previous studies [20,3] also revealed such problem.", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "3 and previous studies [20,3] also revealed such problem.", "startOffset": 23, "endOffset": 29}, {"referenceID": 1, "context": "3 As shown in [2], MA-DNN in synchronous case converges faster and achieves better test accuracy than that in asynchronous case.", "startOffset": 14, "endOffset": 17}, {"referenceID": 16, "context": "In order to improve the performance of ensemble, it is necessary to generate diverse local models other than merely accurate ones [17,24].", "startOffset": 130, "endOffset": 137}, {"referenceID": 23, "context": "In order to improve the performance of ensemble, it is necessary to generate diverse local models other than merely accurate ones [17,24].", "startOffset": 130, "endOffset": 137}, {"referenceID": 0, "context": "In order to compress the global model to the one with the same size as the local model, we use distillation base compression method [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model.", "startOffset": 132, "endOffset": 141}, {"referenceID": 21, "context": "In order to compress the global model to the one with the same size as the local model, we use distillation base compression method [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model.", "startOffset": 132, "endOffset": 141}, {"referenceID": 13, "context": "In order to compress the global model to the one with the same size as the local model, we use distillation base compression method [1,22,14], which obtains a compressed model by letting it mimic the predictions of the global model.", "startOffset": 132, "endOffset": 141}, {"referenceID": 3, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 9, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 7, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 8, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 20, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 10, "context": "4 Other algorithms for the compression [4,10,8,9,21,11] can also be used for the same purpose, but different techniques may be required in order to plug these compression algorithms into the EC-DNN framework.", "startOffset": 39, "endOffset": 55}, {"referenceID": 11, "context": "dual learning [12], to improve the performance of both tasks simultaneously.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Traditional ensemble methods for DNN [25,5] usually first train several DNN models independently without communication and make ensemble of them in the end.", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "Traditional ensemble methods for DNN [25,5] usually first train several DNN models independently without communication and make ensemble of them in the end.", "startOffset": 37, "endOffset": 43}, {"referenceID": 15, "context": "We conducted experiments on public datasets CIFAR-10, CIFAR-100 [16] and ImageNet (ILSVRC 2015 Classification Challenge) [23].", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "We conducted experiments on public datasets CIFAR-10, CIFAR-100 [16] and ImageNet (ILSVRC 2015 Classification Challenge) [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "On CIFAR-10 and CIFAR-100, we employ NiN [19], a 9-layer convolutional network.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "On ImageNet, we use GoogLeNet [25], a 22-layer convolutional network.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "All the experiments are implemented using Caffe [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "\u2013 S-DNN denotes the sequential training on one GPU until convergence [19,25].", "startOffset": 69, "endOffset": 76}, {"referenceID": 24, "context": "\u2013 S-DNN denotes the sequential training on one GPU until convergence [19,25].", "startOffset": 69, "endOffset": 76}, {"referenceID": 24, "context": "\u2013 E-DNN denotes the method that trains local models independently and makes ensemble of the local models merely at the end of the training [25,5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "\u2013 E-DNN denotes the method that trains local models independently and makes ensemble of the local models merely at the end of the training [25,5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 26, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 19, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 5, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 25, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 1, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}, {"referenceID": 2, "context": "\u2013 MA-DNN refers the parallel DNN training framework with the aggregation by averaging model parameters [27,20,6,26,2,3].", "startOffset": 103, "endOffset": 119}], "year": 2017, "abstractText": "Parallelization framework has become a necessity to speed up the training of deep neural networks (DNN) recently. Such framework typically employs the Model Average approach, denoted as MA-DNN, in which parallel workers conduct respective training based on their own local data while the parameters of local models are periodically communicated and averaged to obtain a global model which serves as the new start of local models. However, since DNN is a highly non-convex model, averaging parameters cannot ensure that such global model can perform better than those local models. To tackle this problem, we introduce a new parallel training framework called Ensemble-Compression, denoted as EC-DNN. In this framework, we propose to aggregate the local models by ensemble, i.e., averaging the outputs of local models instead of the parameters. As most of prevalent loss functions are convex to the output of DNN, the performance of ensemble-based global model is guaranteed to be at least as good as the average performance of local models. However, a big challenge lies in the explosion of model size since each round of ensemble can give rise to multiple times size increment. Thus, we carry out model compression after each ensemble, specialized by a distillation based method in this paper, to reduce the size of the global model to be the same as the local ones. Our experimental results demonstrate the prominent advantage of EC-DNN over MA-DNN in terms of both accuracy and speedup.", "creator": "LaTeX with hyperref package"}}}