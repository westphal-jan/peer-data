{"id": "1606.01245", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization", "abstract": "The shadow-p quasi-standard $(0 & lt; p & lt; 1) $is normally used to replace the standard nuclear standard in order to more accurately approximate the ranking function. However, the existing shadow-p quasi-standard minimization algorithms contain a singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration and can therefore become very slow and impractical for major problems. In this paper, we first define two tractable shadow quasi-standards, i.e. the Frobenius / Nuclear Hybrid and Bi-Nuclear quasi-standards, and then prove that they are essentially the shadow 2 / 3 and 1 / 2 quasi-standards, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices.", "histories": [["v1", "Sat, 4 Jun 2016 03:28:41 GMT  (2365kb)", "http://arxiv.org/abs/1606.01245v1", "16 pages, 5 figures, Appears in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp. 2016--2022, 2016"]], "COMMENTS": "16 pages, 5 figures, Appears in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp. 2016--2022, 2016", "reviews": [], "SUBJECTS": "cs.NA cs.AI math.OC stat.ML", "authors": ["fanhua shang", "yuanyuan liu", "james cheng"], "accepted": true, "id": "1606.01245"}, "pdf": {"name": "1606.01245.pdf", "metadata": {"source": "CRF", "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization", "authors": ["Fanhua Shang", "Yuanyuan Liu", "James Cheng"], "emails": ["jcheng}@cse.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n01 24\n5v 1\n[ cs\n.N A\n] 4\nJ un\n2 01"}, {"heading": "Introduction", "text": "In recent years, the matrix rank minimization problem arises in a wide range of applications such as matrix completion, robust principal component analysis, low-rank representation, multivariate regression and multi-task learning. To solve such problems, Fazel, Hindi, and Boyd; Cande\u0300s and Tao; Recht, Fazel, and Parrilo (2001; 2010; 2010) have suggested to relax the rank function by its convex envelope, i.e., the nuclear norm. In fact, the nuclear norm is equivalent to the \u21131-norm on singular values of a matrix, and thus it promotes a low-rank solution. However, it has been shown in (Fan and Li 2001) that the \u21131-norm regularization over-penalizes large entries of vectors, and results in a biased solution. By realizing the intimate relationship between them, the nuclear norm penalty also over-penalizes large singular values, that is, it may make the solution deviate from the original solution as the \u21131-norm does (Nie, Huang, and Ding 2012; Lu et al. 2015). Compared with the nuclear norm, the Schatten-p quasi-norm for 0 < p < 1 makes a closer\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\napproximation to the rank function. Consequently, the Schatten-p quasi-norm minimization has attracted a great deal of attention in images recovery (Lu and Zhang 2014; Lu et al. 2014), collaborative filtering (Nie et al. 2012; Lu et al. 2015; Mohan and Fazel 2012) and MRI analysis (Majumdar and Ward 2011). In addition, many non-convex surrogate functions of the \u21130-norm listed in (Lu et al. 2014; Lu et al. 2015) have been extended to approximate the rank function, such as SCAD (Fan and Li 2001) and MCP (Zhang 2010).\nAll non-convex surrogate functions mentioned above for low-rank minimization lead to some non-convex, nonsmooth, even non-Lipschitz optimization problems. Therefore, it is crucial to develop fast and scalable algorithms which are specialized to solve some alternative formulations. So far, Lai, Xu, and Yin (2013) proposed an iterative reweighted lease squares (IRucLq) algorithm to approximate the Schatten-p quasi-norm minimization problem, and proved that the limit point of any convergent subsequence generated by their algorithm is a critical point. Moreover, Lu et al. (2014) proposed an iteratively reweighted nuclear norm (IRNN) algorithm to solve many non-convex surrogate minimization problems. For matrix completion problems, the Schatten-p quasi-norm has been shown to be empirically superior to the nuclear norm (Marjanovic and Solo 2012). In addition, Zhang, Huang, and Zhang (2013) theoretically proved that the Schatten-p quasi-norm minimization with small p requires significantly fewer measurements than the convex nuclear norm minimization. However, all existing algorithms have to be solved iteratively and involve SVD or EVD in each iteration, which incurs high computational cost and is too expensive for solving large-scale problems (Cai and Osher 2013; Liu et al. 2014).\nIn contrast, as an alternative non-convex formulation of the nuclear norm, the bilinear spectral regularization as in (Srebro, Rennie, and Jaakkola 2004; Recht, Fazel, and Parrilo 2010) has been successfully applied in many large-scale applications, e.g., collaborative filtering (Mitra, Sheorey, and Chellappa 2010). As the Schatten-p quasi-norm is equivalent to the \u2113p quasi-norm on singular values of a matrix, it is natural to ask the following question: can we design equivalent matrix factorization forms for the cases of the Schatten quasi-norm, e.g., p = 2/3 or 1/2?\nIn order to answer the above question, in this paper we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms. We then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, for solving whose minimization we only need to perform SVDs on two much smaller factor matrices as contrary to the larger ones used in existing algorithms, e.g., IRNN. Therefore, our method is particularly useful for many \u201cbig data\u201d applications that need to deal with large, high dimensional data with missing values. To the best of our knowledge, this is the first paper to scale Schatten quasi-norm solvers to the Netflix dataset. Moreover, we provide the global convergence and recovery performance guarantees for our algorithms. In other words, this is the best guaranteed convergence for algorithms that solve such challenging problems."}, {"heading": "Notations and Background", "text": "The Schatten-p norm (0 < p < \u221e) of a matrix X \u2208 Rm\u00d7n (m \u2265 n) is defined as\n\u2016X\u2016Sp , ( n\u2211\ni=1\n\u03c3pi (X)\n)1/p ,\nwhere \u03c3i(X) denotes the i-th singular value of X . When p = 1, the Schatten-1 norm is the well-known nuclear norm, \u2016X\u2016\u2217. In addition, as the non-convex surrogate for the rank function, the Schatten-p quasi-norm with 0 < p < 1 is a better approximation than the nuclear norm (Zhang, Huang, and Zhang 2013) (analogous to the superiority of the \u2113p quasi-norm to the \u21131norm (Daubechies et al. 2010)).\nTo recover a low-rank matrix from some linear observations b \u2208 Rs, we consider the following general Schatten-p quasi-norm minimization problem,\nmin X\n\u03bb\u2016X\u2016pSp + f(A(X)\u2212 b) , (1)\nwhere A : Rm\u00d7n \u2192 Rs denotes the linear measurement operator, \u03bb > 0 is a regularization parameter, and the loss function f(\u00b7) : Rs \u2192 R generally denotes certain measurement for characterizing A(X) \u2212 b. The above formulation can address a wide range of problems, such as matrix completion (Marjanovic and Solo 2012; Rohde and Tsybakov 2011) (A is the sampling operator and f(\u00b7) = \u2016 \u00b7 \u201622), robust principal component analysis (Cande\u0300s et al. 2011; Wang, Liu, and Zhang 2013; Shang et al. 2014) (A is the identity operator and f(\u00b7) = \u2016 \u00b7 \u20161), and multivariate regression (Hsieh and Olsen 2014) (A(X)=AX with A being a given matrix, and f(\u00b7)=\u2016\u00b7\u20162F ). Furthermore, f(\u00b7) may be also chosen as the Hinge loss in (Srebro, Rennie, and Jaakkola 2004) or the \u2113p quasi-norm in (Nie et al. 2012).\nAnalogous to the \u2113p quasi-norm, the Schatten-p quasinorm is also non-convex for p < 1, and its minimization is generally NP-hard (Lai, Xu, and Yin 2013). Therefore, it is crucial to develop efficient algorithms to solve some alternative formulations of Schatten-p quasi-norm minimization (1). So far, only few algorithms (Lai, Xu, and Yin 2013; Mohan and Fazel 2012; Nie et al. 2012; Lu et al. 2014) have\nbeen developed to solve such problems. Furthermore, since all existing Schatten-p quasi-norm minimization algorithms involve SVD or EVD in each iteration, they suffer from a high computational cost of O(n2m), which severely limits their applicability to large-scale problems. Although there have been many efforts towards fast SVD or EVD computation such as partial SVD (Larsen 2005), the performance of those methods is still unsatisfactory for real-life applications (Cai and Osher 2013)."}, {"heading": "Tractable Schatten Quasi-Norms", "text": "As in (Srebro, Rennie, and Jaakkola 2004), the nuclear norm has the following alternative non-convex formulations.\nLemma 1. Given a matrix X \u2208 Rm\u00d7n with rank(X) = r \u2264 d, the following holds:\n\u2016X\u2016\u2217 = min U\u2208Rm\u00d7d,V \u2208Rn\u00d7d:X=UV T \u2016U\u2016F\u2016V \u2016F\n= min U,V :X=UV T \u2016U\u20162F + \u2016V \u20162F 2 ."}, {"heading": "Frobenius/Nuclear Hybrid Quasi-Norm", "text": "Motivated by the equivalence relation between the nuclear norm and the bilinear spectral regularization (please refer to (Srebro, Rennie, and Jaakkola 2004; Recht, Fazel, and Parrilo 2010)), we define a Frobenius/nuclear hybrid (F/N) norm as follows\nDefinition 1. For any matrix X \u2208 Rm\u00d7n with rank(X) = r \u2264 d, we can factorize it into two much smaller matrices U \u2208 Rm\u00d7d and V \u2208 Rn\u00d7d such that X = UV T . Then the Frobenius/nuclear hybrid norm of X is defined as\n\u2016X\u2016F/N := min X=UV T \u2016U\u2016\u2217\u2016V \u2016F .\nIn fact, the Frobenius/nuclear hybrid norm is not a real norm, because it is non-convex and does not satisfy the triangle inequality of a norm. Similar to the well-known Schatten-p quasi-norm (0 < p < 1), the Frobenius/nuclear hybrid norm is also a quasi-norm, and their relationship is stated in the following theorem.\nTheorem 1. The Frobenius/nuclear hybrid norm \u2016\u00b7\u2016F/N is a quasi-norm. Surprisingly, it is also the Schatten-2/3 quasinorm, i.e.,\n\u2016X\u2016F/N = \u2016X\u2016S2/3,\nwhere \u2016X\u2016S2/3 denotes the Schatten-2/3 quasi-norm of X . Property 1. For any matrix X \u2208 Rm\u00d7n with rank(X) = r \u2264 d, the following holds:\n\u2016X\u2016F/N = min U\u2208Rm\u00d7d,V \u2208Rn\u00d7d:X=UV T \u2016U\u2016\u2217\u2016V \u2016F\n= min X=UV T\n( 2\u2016U\u2016\u2217 + \u2016V \u20162F\n3\n)3/2 .\nThe proofs of Property 1 and Theorem 1 can be found in the Supplementary Materials."}, {"heading": "Bi-Nuclear Quasi-Norm", "text": "Similar to the definition of the above Frobenius/nuclear hybrid norm, our bi-nuclear (BiN) norm is naturally defined as follows.\nDefinition 2. For any matrix X \u2208 Rm\u00d7n with rank(X) = r \u2264 d, we can factorize it into two much smaller matrices U \u2208 Rm\u00d7d and V \u2208 Rn\u00d7d such that X = UV T . Then the bi-nuclear norm of X is defined as\n\u2016X\u2016BiN := min X=UV T \u2016U\u2016\u2217\u2016V \u2016\u2217.\nSimilar to the Frobenius/nuclear hybrid norm, the binuclear norm is also a quasi-norm, as stated in the following theorem.\nTheorem 2. The bi-nuclear norm \u2016\u00b7\u2016BiN is a quasi-norm. In addition, it is also the Schatten-1/2 quasi-norm, i.e.,\n\u2016X\u2016BiN = \u2016X\u2016S1/2. The proof of Theorem 2 can be found in the Supplementary Materials. Due to the relationship between the binuclear quasi-norm and the Schatten-1/2 quasi-norm, it is easy to verify that the bi-nuclear quasi-norm possesses the following properties. Property 2. For any matrix X \u2208 Rm\u00d7n with rank(X) = r \u2264 d, the following holds:\n\u2016X\u2016BiN = min X=UV T \u2016U\u2016\u2217\u2016V \u2016\u2217= min X=UV T \u2016U\u20162\u2217+\u2016V \u20162\u2217 2\n= min X=UV T (\u2016U\u2016\u2217+\u2016V \u2016\u2217 2 )2 .\nThe following relationship between the nuclear norm and the Frobenius norm is well known: \u2016X\u2016F \u2264 \u2016X\u2016\u2217 \u2264\u221a r\u2016X\u2016F . Similarly, the analogous bounds hold for the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, as stated in the following property. Property 3. For any matrix X \u2208 Rm\u00d7n with rank(X) = r, the following inequalities hold:\n\u2016X\u2016\u2217 \u2264 \u2016X\u2016F/N \u2264 \u221a r\u2016X\u2016\u2217,\n\u2016X\u2016\u2217 \u2264 \u2016X\u2016F/N \u2264\u2016X\u2016BiN \u2264 r\u2016X\u2016\u2217. The proof of Property 3 can be found in the Supplementary Materials. It is easy to see that Property 3 in turn implies that any low Frobenius/nuclear hybrid or bi-nuclear norm approximation is also a low nuclear norm approximation."}, {"heading": "Optimization Algorithms", "text": ""}, {"heading": "Problem Formulations", "text": "To bound the Schatten-2/3 or -1/2 quasi-norm of X by 1 3 (2\u2016U\u2016\u2217+\u2016V \u20162F ) or 12 (\u2016U\u2016\u2217+\u2016V \u2016\u2217), we mainly consider the following general structured matrix factorization formulation as in (Haeffele, Young, and Vidal 2014),\nmin U,V\n\u03bb\u03d5(U, V ) + f(A(UV T )\u2212 b), (2)\nwhere the regularization term \u03d5(U, V ) denotes 13 (2\u2016U\u2016\u2217+ \u2016V \u20162F ) or 12 (\u2016U\u2016\u2217+\u2016V \u2016\u2217).\nAs mentioned above, there are many Schatten-p quasinorm minimization problems for various real-world applications. Therefore, we propose two efficient algorithms to solve the following low-rank matrix completion problems:\nmin U,V \u03bb(2\u2016U\u2016\u2217+\u2016V \u20162F ) 3 + 1 2 \u2016P\u2126(UV T )\u2212P\u2126(D)\u20162F , (3)\nmin U,V \u03bb(\u2016U\u2016\u2217 + \u2016V \u2016\u2217) 2 + 1 2 \u2016P\u2126(UV T )\u2212 P\u2126(D)\u20162F , (4)\nwhere P\u2126 denotes the linear projection operator, i.e., P\u2126(D)ij=Dij if (i, j)\u2208\u2126, and P\u2126(D)ij=0 otherwise. Due to the operator P\u2126 in (3) and (4), we usually need to introduce some auxiliary variables for solving them. To avoid introducing auxiliary variables, motivated by the proximal alternating linearized minimization (PALM) method proposed in (Bolte, Sabach, and Teboulle 2014), we propose two fast PALM algorithms to efficiently solve (3) and (4). The space limitation refrains us from fully describing each algorithm, but we try to give enough details of a representative algorithm for solving (3) and discussing their differences.\nUpdating Uk+1 and Vk+1 with Linearization Techniques\nLet gk(U) := \u2016P\u2126(UV Tk )\u2212P\u2126(D)\u20162F /2, and then its gradient is Lipschitz continuous with constant lgk+1, meaning that \u2016\u2207gk(U1)\u2212\u2207gk(U2)\u2016F \u2264 lgk+1\u2016U1 \u2212U2\u2016F for any U1, U2 \u2208 Rm\u00d7d. By linearizing gk(U) at Uk and adding a proximal term, then we have the following approximation:\ng\u0302k(U,Uk)=gk(Uk)+\u3008\u2207gk(Uk), U\u2212Uk\u3009+ lgk+1 2 \u2016U\u2212Uk\u20162F . (5)\nThus, we have\nUk+1=argmin U\n2\u03bb\n3 \u2016U\u2016\u2217+g\u0302k(U,Uk)\n=argmin U\n2\u03bb\n3 \u2016U\u2016\u2217+ lgk+1 2 \u2016U\u2212Uk+ \u2207gk(Uk) lgk+1 \u20162F .\n(6)\nSimilarly, we have\nVk+1=argmin V\n\u03bb 3 \u2016V \u20162F+ lhk+1 2 \u2016V\u2212Vk+\u2207hk(Vk)/lhk+1\u20162F , (7)\nwhere hk(V ) := \u2016P\u2126(Uk+1V T )\u2212P\u2126(D)\u20162F /2 with the Lipschitz constant lhk+1. The problems (6) and (7) are known to have closed-form solutions, which of the former is given by the so-called matrix shrinkage operator (Cai, Cande\u0300s, and Shen 2010). In contrast, for solving (4), Uk+1 is computed in the same way as (6), and Vk+1 is given by\nVk+1=argmin V\n\u03bb 2 \u2016V \u2016\u2217+ lhk+1 2 \u2016V\u2212Vk+\u2207hk(Vk)/lhk+1\u20162F . (8)"}, {"heading": "Updating Lipschitz Constants", "text": "Next we compute the Lipschitz constants lgk+1 and l h k+1 at the (k+1)-iteration.\nAlgorithm 1 Solving (3) via PALM Input: P\u2126(D), the given rank d and \u03bb. Initialize: U0, V0, \u03b5 and k = 0.\n1: while not converged do 2: Update lgk+1 and Uk+1 by (9) and (6), respectively. 3: Update lhk+1 and Vk+1 by (9) and (7), respectively. 4: Check the convergence condition, max{\u2016Uk+1\u2212Uk\u2016F , \u2016Vk+1\u2212Vk\u2016F } < \u03b5. 5: end while\nOutput: Uk+1, Vk+1.\n\u2016\u2207gk(U1)\u2212\u2207gk(U2)\u2016F =\u2016[P\u2126(U1V Tk \u2212 U2V Tk )]Vk\u2016F \u2264\u2016Vk\u201622\u2016U1 \u2212 U2\u2016F , \u2016\u2207hk(V1)\u2212\u2207hk(V2)\u2016F =\u2016UTk+1[P\u2126(Uk+1(V T1 \u2212V T2 ))]\u2016F \u2264\u2016Uk+1\u201622\u2016V1\u2212V2\u2016F . Hence, both Lipschitz constants are updated by\nlgk+1 = \u2016Vk\u201622 and lhk+1 = \u2016Uk+1\u201622. (9)"}, {"heading": "PALM Algorithms", "text": "Based on the above development, our algorithm for solving (3) is given in Algorithm 1. Similarly, we also design an efficient PALM algorithm for solving (4). The running time of Algorithm 1 is dominated by performing matrix multiplications. The total time complexity of Algorithm 1, as well as the algorithm for solving (4), is O(nmd), where d \u226a m,n."}, {"heading": "Algorithm Analysis", "text": "We now provide the global convergence and low-rank matrix recovery guarantees for Algorithm 1, and the similar results can be obtained for the algorithm for solving (4)."}, {"heading": "Global Convergence", "text": "Before analyzing the global convergence of Algorithm 1, we first introduce the definition of the critical points of a non-convex function given in (Bolte, Sabach, and Teboulle 2014).\nDefinition 3. Let a non-convex function f : Rn \u2192 (\u2212\u221e,+\u221e] be a proper and lower semi-continuous function, and domf={x \u2208 Rn : f(x) < +\u221e}. \u2022 For any x \u2208 domf , the Fre\u0300chet sub-differential of f at x\nis defined as\n\u2202\u0302f(x)={u\u2208Rn : lim y 6=x inf y\u2192x f(y)\u2212f(x)\u2212\u3008u, y\u2212x\u3009 \u2016y\u2212x\u20162 \u22650},\nand \u2202\u0302f(x) = \u2205 if x /\u2208 domf . \u2022 The limiting sub-differential of f at x is defined as\n\u2202f(x)={u\u2208Rn : \u2203xk \u2192 x, f(xk) \u2192 f(x) and uk\u2208 \u2202\u0302f(xk)\u2192u as k\u2192\u221e}.\n\u2022 The points whose sub-differential contains 0 are called critical points. For instance, the point x is a critical point of f if 0\u2208\u2202f(x).\nTheorem 3 (Global Convergence). Let {(Uk, Vk)} be a sequence generated by Algorithm 1, then it is a Cauchy sequence and converges to a critical point of (3).\nThe proof of the theorem can be found in the Supplementary Materials. Theorem 3 shows the global convergence of Algorithm 1. We emphasize that, different from the general subsequence convergence property, the global convergence property is given by (Uk, Vk) \u2192 (U\u0302 , V\u0302 ) as the number of iteration k \u2192 +\u221e, where (U\u0302 , V\u0302 ) is a critical point of (3). As we have stated, existing algorithms for solving the non-convex and nonsmooth problem, such as IRucLq and IRNN, have only subsequence convergence (Xu and Yin 2014). According to (Attouch and Bolte 2009), we know that the convergence rate of Algorithm 1 is at least sub-linear, as stated in the following theorem.\nTheorem 4 (Convergence Rate). The sequence {(Uk, Vk)} generated by Algorithm 1 converges to a critical point (U\u0302 , V\u0302 ) of (3) at least in the sub-linear convergence rate, that is, there exists C > 0 and \u03b8 \u2208 (1/2, 1) such that\n\u2016[UTk , V Tk ]\u2212 [U\u0302T , V\u0302 T ]\u2016F \u2264 Ck\u2212 1\u2212\u03b8 2\u03b8\u22121 ."}, {"heading": "Recovery Guarantee", "text": "In the following, we show that when sufficiently many entries are observed, the critical point generated by our algorithms recovers a low-rank matrix \u201cclose to\u201d the groundtruth one. Without loss of generality, assume that D = Z+E \u2208 Rm\u00d7n, where Z is a true matrix, and E denotes a random gaussian noise.\nTheorem 5. Let (U\u0302 , V\u0302 ) be a critical point of the problem (3) with given rank d, and m \u2265 n. Then there exists an absolute constant C1, such that with probability at least 1\u2212 2 exp(\u2212m), \u2016Z\u2212U\u0302V\u0302 T\u2016F\u221a\nmn \u2264\u2016E\u2016F\u221a mn +C1\u03b2\n( md log(m)\n|\u2126|\n)1/4 + 2 \u221a d\u03bb\n3C2 \u221a |\u2126| ,\nwhere \u03b2 = maxi,j |Di,j | and C2 = \u2016P\u2126(D\u2212U\u0302V\u0302 T )V\u0302 \u2016F\n\u2016P\u2126(D\u2212U\u0302V\u0302 T )\u2016F .\nThe proof of the theorem and the analysis of lowerboundedness of C2 can be found in the Supplementary Materials. When the samples size |\u2126| \u226b md log(m), the second and third terms diminish, and the recovery error is essentially bounded by the \u201caverage\u201d magnitude of entries of the noise matrix E. In other words, only O(md log(m)) observed entries are needed, which is significantly lower than O(mr log2(m)) in standard matrix completion theories (Cande\u0300s and Recht 2009; Keshavan, Montanari, and Oh 2010; Recht 2011). We will confirm this result by our experiments in the following section."}, {"heading": "Experimental Results", "text": "We now evaluate both the effectiveness and efficiency of our algorithms for solving matrix completion problems, such as collaborative filtering and image recovery. All experiments were conducted on an Intel Xeon E7-4830V2 2.20GHz CPU with 64G RAM.\nAlgorithms for Comparison We compared our algorithms, BiN and F/N, with the following state-of-the-art methods: IRucLq1 (Lai, Xu, and Yin 2013): In IRucLq, p varies from 0.1 to 1with increment 0.1, and the parameters\u03bb and \u03b1 are set to 10\u22126 and 0.9, respectively. In addition, the rank parameter of the algorithm is updated dynamically as in (Lai, Xu, and Yin 2013), that is, it only needs to compute the partial EVD. IRNN2 (Lu et al. 2014): We choose the \u2113pnorm, SCAD and MCP penalties as the regularization term among eight non-convex penalty functions, where p is chosen from the range of {0.1, 0.2, . . . , 1}. At each iteration, the parameter \u03bb is dynamically decreased by \u03bbk =0.7\u03bbk\u22121, where \u03bb0=10\u2016P\u2126(D)\u2016\u221e.\nFor our algorithms, we set the regularization parameter \u03bb = 5 or \u03bb = 100 for noisy synthetic and real-world data, respectively. Note that the rank parameter d is estimated by the strategy in (Wen, Yin, and Zhang 2012). In addition, we evaluate the performance of matrix recovery by the relative squared error (RSE) and the root mean square error (RMSE), i.e., RSE := \u2016X \u2212Z\u2016F /\u2016Z\u2016F and RMSE := 1 |T | \u221a \u03a3(i,j)\u2208T (Xij\u2212Dij)2, where T is the test set."}, {"heading": "Synthetic Matrix Completion", "text": "The synthetic matrices Z\u2208Rm\u00d7n with rank r are generated by the following procedure: the entries of both U \u2208 Rm\u00d7r and V \u2208Rn\u00d7r are first generated as independent and identically distributed (i.i.d.) numbers, and then Z = UV T is assembled. Since all these algorithms have very similar recovery performance on noiseless matrices, we only conducted\n1http://www.math.ucla.edu/\u02dcwotaoyin/ 2https://sites.google.com/site/canyilu/\nexperiments on noisy matrices with different noise levels, i.e., P\u2126(D) =P\u2126(Z+nf \u2217E), where nf denotes the noise factor. In other worlds, the observed subset is corrupted by i.i.d. standard Gaussian random noise as in (Lu et al. 2014). In addition, only 20% or 30% entries of D are sampled uniformly at random as training data, i.e., sampling ratio (SR)=20% or 30%. The rank parameter d of our algorithms is set to \u230a1.25r\u230b as in (Wen, Yin, and Zhang 2012).\nThe average RSE results of 100 independent runs on noisy random matrices are shown in Figure 1, which shows that if p varies from 0.1 to 0.7, IRucLq and IRNN-Lp achieve similar recovery performance as IRNN-SCAD, IRNN-MCP and our algorithms; otherwise, IRucLq and IRNN-Lp usually perform much worse than the other four methods, especially p = 1. We also report the running time of all the methods with 20% SR as the size of noisy matrices increases, as shown in Figure 2. Moreover, we present the RSE results of those methods and APGL3 (Toh and Yun 2010) (which is one of the nuclear norm solvers) with different noise factors. Figure 2 shows that our algorithms are significantly faster than the other methods, while the running time of IRucLq and IRNN increases dramatically when the size of matrices increases, and they could not yield experimental results within 48 hours when the size of matrices is 50, 000\u00d750, 000. This further justifies that both our algorithms have very good scalability and can address large-scale problems. In addition, with only 20% SR, all Schatten quasi-norm methods significantly outperform APGL in terms of RSE.\n3http://www.math.nus.edu.sg/\u02dcmattohkc/"}, {"heading": "Collaborative Filtering", "text": "We tested our algorithms on three real-world recommendation system data sets: the MovieLens1M, MovieLens10M4 and Netflix datasets (KDDCup 2007). We randomly chose 50%, 70% and 90% as the training set and the remaining as the testing set, and the experimental results are reported over 10 independent runs. In addition to the methods used above, we also compared our algorithms with one of the fastest existing methods, LMaFit5 (Wen, Yin, and Zhang 2012). The testing RMSE of all these methods on the three data sets is reported in Table 1, which shows that all those methods with non-convex penalty functions perform significantly better than the convex nuclear norm solver, APGL. In addition, our algorithms consistently outperform the other methods in terms of prediction accuracy. This further confirms that our two Schatten quasi-norm regularized models can provide a good estimation of a low-rank matrix. Moreover, we report the average testing RMSE and running time of our algorithms on these three data sets in Figures 3 and 4, where the rank varies from 5 to 20 and SR is set to 70%. Note that IRucLq and IRNN-Lp could not run on the two larger data sets due to runtime exceptions. It is clear that our algorithms are much faster than AGPL, IRucLq and IRNN-Lp on all these data sets. They perform much more robust with respect to ranks than LMaFit, and are comparable in speed with it. This shows that our algorithms have very good scalability and are suitable for real-world applications.\n4http://www.grouplens.org/node/73 5http://lmafit.blogs.rice.edu/."}, {"heading": "Image Recovery", "text": "We also applied our algorithms to gray-scale image recovery on the Boat image of size 512\u00d7512, where 50% of pixels in the input image were replaced by random Gaussian noise, as shown in Figure 5(b). In addition, we employed the well known peak signal-to-noise ratio (PSNR) to measure the recovery performance. The rank parameter of our algorithms and IRucLq was set to 100. Due to limited space, we only report the best results (PSNR and CPU time) of APGL, LMaFit, IRucLq and IRNN-Lp in Figure 5, which shows that our two algorithms achieve much better recovery performance than the other methods in terms of PSNR. And impressively, both our algorithms are significantly faster than the other methods except LMaFit and at least 70 times faster than IRucLq and IRNN-Lp."}, {"heading": "Conclusions", "text": "In this paper we defined two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasinorms, and proved that they are in essence the Schatten2/3 and 1/2 quasi-norms, respectively. Then we designed two efficient proximal alternating linearized minimization\nalgorithms to solve our Schatten quasi-norm minimization for matrix completion problems, and also proved that each bounded sequence generated by our algorithms globally converges to a critical point. In other words, our algorithms not only have better convergence properties than existing algorithms, e.g., IRucLq and IRNN, but also reduce the computational complexity from O(mn2) to O(mnd), with d being the estimated rank (d\u226am,n). We also provided the recovery guarantee for our algorithms, which implies that they need only O(md log(m)) observed entries to recover a lowrank matrix with high probability. Our experiments showed that our algorithms outperform the state-of-the-art methods in terms of both efficiency and effectiveness."}, {"heading": "Acknowledgements", "text": "We thank the reviewers for their constructive comments. The authors are partially supported by the SHIAE fund 8115048 and the Hong Kong GRF 2150851."}, {"heading": "Proofs of Theorem 1 and Property 1:", "text": "In the following, we will first prove that the Frobenius/nuclear hybrid norm \u2016\u00b7\u2016F/N is a quasi-norm. Proof. By the definition of the F/N norm, for any a, a1, a2 \u2208 R and a = a1a2, we have\n\u2016aX\u2016F/N = min aX=(a1U)(a2V T ) \u2016a1U\u2016\u2217\u2016a2V \u2016F\n= min X=UV T\n(|a1| \u2016U\u2016\u2217) (|a2| \u2016V \u2016F )\n= |a| min X=UV T \u2016U\u2016\u2217\u2016V \u2016F = |a| \u2016X\u2016F/N.\nNext we will prove that \u2016X + Y \u2016F/N \u2264 \u03b2 (\u2016X\u2016F/N + \u2016Y \u2016F/N), where \u03b2 \u2265 1. By Lemma 1, i.e., \u2016X\u2016\u2217 = minX=UV T \u2016U\u2016F\u2016V \u2016F , there must exist both matrices U\u0302 and V\u0302 such that \u2016X\u2016\u2217 = \u2016U\u0302\u2016F \u2016V\u0302 \u2016F with the constraint X = U\u0302 V\u0302 . According to the definition of the F/N-norm and the fact that \u2016X\u2016\u2217 \u2264 \u221a rank(X)\u2016X\u2016F , we have \u2016X\u2016F/N = min\nX=UV T \u2016U\u2016\u2217\u2016V \u2016F\n\u2264 \u2016U\u0302\u2016\u2217\u2016V\u0302 \u2016F \u2264 \u221a rank(U)\u2016U\u0302\u2016F \u2016V\u0302 \u2016F \u2264 \u221a\nrank(U)\u2016X\u2016\u2217 = \u03b1\u2016X\u2016\u2217,\nwhere \u03b1 = \u221a\nrank(U). If X 6= 0, we can know that \u03b1 \u2265 1. On the other hand, we also have \u2016X\u2016\u2217 \u2264 \u2016X\u2016F/N. By the above properties, there exists a constant \u03b2 \u2265 1 such that the following holds for all X,Y \u2208 Rm\u00d7n\n\u2016X + Y \u2016F/N \u2264 \u03b2\u2016X + Y \u2016\u2217 \u2264 \u03b2(\u2016X\u2016\u2217 + \u2016Y \u2016\u2217) \u2264 \u03b2(\u2016X\u2016F/N + \u2016Y \u2016F/N).\nFurthermore, \u2200X \u2208 Rm\u00d7n and X = UV T , we have \u2016X\u2016F/N = min\nX=UV T \u2016U\u2016\u2217\u2016V \u2016F \u2265 0.\nIn addition, if \u2016X\u2016F/N = 0, we have \u2016X\u2016\u2217 \u2264 \u2016X\u2016F/N = 0, i.e., \u2016X\u2016\u2217 = 0. Hence, X = 0. In short, the F/N-norm \u2016 \u00b7 \u2016F/N is a quasi-norm.\nBefore giving the complete proofs for Theorem 1 and Property 1, we first present and prove the following important lemma.\nLemma 2. Suppose that Z \u2208 Rm\u00d7n is a matrix of rank r \u2264 min(m, n), and we denote its SVD by Z = L\u03a3ZRT , where L \u2208 Rm\u00d7r, R \u2208 Rn\u00d7r and \u03a3Z \u2208 Rr\u00d7r. For any matrix A \u2208 Rr\u00d7r satisfying AAT = ATA = Ir\u00d7r, and the given p (0 < p < 1), then (A\u03a3ZAT )k,k \u2265 0 for all k = 1, . . . , r, and\nTrp(A\u03a3ZAT ) \u2265 Trp(\u03a3Z) = \u2016Z\u2016pSp,\nwhere Trp(B) = \u2211\ni B p ii.\nProof. For any k \u2208 {1, . . . , r}, we have (A\u03a3ZAT )k,k = \u2211 i a 2 ki\u03c3i \u2265 0, where \u03c3i \u2265 0 is the i-th singular value of Z . Then\nTrp(A\u03a3ZAT ) = \u2211\nk\n( \u2211\ni\na2ki\u03c3i\n)p . (10)\nSince \u03c8(x) = xp (0 < p < 1) is a concave function on R+, and by the Jensen\u2019s inequality (Mitrinovic\u0301 1970) and \u2211\ni a 2 ki = 1\nfor any k \u2208 {1, . . . , r}, we have ( \u2211\ni\na2ki\u03c3i\n)p \u2265 \u2211\ni\na2ki\u03c3 p i .\nAccording to the above inequality and \u2211\nk a 2 ki = 1 for any i \u2208 {1, . . . , r}, (10) can be rewritten as\nTrp(A\u03a3ZAT ) = \u2211\nk\n( \u2211\ni\na2ki\u03c3i\n)p\n\u2265 \u2211\nk\n\u2211\ni\na2ki\u03c3 p i\n= \u2211\ni\n\u03c3pi\n= Trp(\u03a3Z) = \u2016Z\u2016pSp.\nThis completes the proof.\nProof of Theorem 1:\nProof. Assume that U = LU\u03a3URTU and V = LV \u03a3V R T V are the thin SVDs of U and V , respectively, where LU \u2208 Rm\u00d7d, LV \u2208 Rn\u00d7d, and RU ,\u03a3U , RV ,\u03a3V \u2208 Rd\u00d7d. Let X = LX\u03a3XRTX , where the columns of LX \u2208 Rm\u00d7d and RX \u2208 Rn\u00d7d are the left and right singular vectors associated with the top d singular values of X with rank at most r (r \u2264 d), and \u03a3X = diag([\u03c31(X),\u00b7 \u00b7 \u00b7, \u03c3r(X), 0,\u00b7 \u00b7 \u00b7, 0]) \u2208Rd\u00d7d.\nBy X = UV T , i.e., LX\u03a3XRTX = LU\u03a3UR T URV \u03a3V L T V , then \u2203O1, O\u03021 \u2208 Rd\u00d7d satisfy LX = LUO1 and LU = LXO\u03021,\ni.e., O1 = LTULX and O\u03021 = L T XLU . Thus, O1 = O\u0302 T 1 . Since LX = LUO1 = LXO\u03021O1, we have O\u03021O1 = O T 1 O1 = Id\u00d7d. Similarly, we have O1O\u03021 = O1OT1 = Id\u00d7d. In addition, \u2203O2 \u2208 Rd\u00d7d satisfies RX = LV O2 with O2OT2 = OT2 O2 = Id\u00d7d. Let O3 = O2OT1 \u2208 Rd\u00d7d, then we have O3OT3 = OT3 O3 = Id\u00d7d, i.e., \u2211 i(O3) 2 ij = \u2211 j(O3) 2 ij = 1 for \u2200i, j \u2208 {1, 2, . . . , d}, where ai,j denotes the element of the matrix A in the i-th row and the j-th column. Furthermore, let O4 = RTURV , we have\u2211 i(O4) 2 ij \u2264 1 and \u2211 j(O4) 2 ij \u2264 1 for \u2200i, j \u2208 {1, 2, . . . , d}.\nBy the above analysis, then we have O2\u03a3XOT2 = O2O T 1 \u03a3UO4\u03a3V = O3\u03a3UO4\u03a3V . Let \u03c4i and \u033aj denote the i-th and the\nj-th diagonal elements of \u03a3U and \u03a3V , respectively. By Lemma 2, and p = 2/3, we have\n\u2016X\u2016S2/3 \u2264 ( Tr 2 3 (O2\u03a3XO T 2 ) ) 3 2 = ( Tr 2 3 (O2O T 1 \u03a3UO4\u03a3V ) ) 3 2 = ( Tr 2 3 (O3\u03a3UO4\u03a3V ) ) 3 2\n=   d\u2211\ni=1\n  d\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\u033ai\n  2 3   3 2\n=   d\u2211\ni=1\n\u033a 2 3\ni\n  d\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\n  2 3   3 2\na\u2264   [ d\u2211\ni=1\n(\u033a 2 3\ni ) 3\n] 1 3   d\u2211\ni=1\n  d\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\n  2 3 \u00d7 3 2   2 3   3 2\n=\n\u221a\u221a\u221a\u221a d\u2211\ni=1\n\u033a2i\nd\u2211\ni=1\nd\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\nb\u2264\n\u221a\u221a\u221a\u221a d\u2211\ni=1\n\u033a2i\nd\u2211\ni=1\nd\u2211\nj=1\n\u03c4j (O3)\n2 ij + (O4) 2 ji\n2\nc\u2264\n\u221a\u221a\u221a\u221a d\u2211\ni=1\n\u033a2i\nd\u2211\nj=1\n\u03c4j\n= \u2016U\u2016\u2217\u2016V \u2016F = \u221a \u2016U\u2016\u2217 \u221a \u2016U\u2016\u2217\u2016V \u2016F\nd\u2264 (\u221a \u2016U\u2016\u2217 + \u221a \u2016U\u2016\u2217 + \u2016V \u2016F 3 )3\n=\n( 2 \u221a \u2016U\u2016\u2217 + \u221a \u2016V \u20162F\n3\n)3\ne\u2264 ( 2\u2016U\u2016\u2217 + \u2016V \u20162F\n3\n) 3 2\nwhere the inequality a \u2264 holds due to the Ho\u0308lder\u2019s inequality (Mitrinovic\u0301 1970), i.e., \u2211n\nk=1 |xkyk| \u2264 ( \u2211n k=1 |xk|p)1/p( \u2211n\nk=1 |yk|q)1/q with 1/p + 1/q = 1, and here we set p = 3 and q = 3/2 in the inequality a\u2264 ; the inequality b\u2264 follows from the basic inequality xy \u2264 x 2+y2\n2 for any real numbers x and y; the inequality c\u2264 relies on the facts\u2211\ni(O3) 2 ij = 1 and \u2211 i(O4) 2 ji \u2264 1; the inequality d\u2264 holds due to the fact 3 \u221a x1x2x3 \u2264 (|x1|+ |x2|+ |x3|)/3 and the inequality e\u2264 holds due to the Jensen\u2019s inequality for the concave function f(x) = x1/2. Thus, for any matrices U \u2208 Rm\u00d7d and V \u2208 Rn\u00d7d satisfying X = UV T , we have\n\u2016X\u2016S2/3 \u2264 \u2016U\u2016\u2217\u2016V \u2016F \u2264 ( 2\u2016U\u2016\u2217 + \u2016V \u20162F\n3\n) 3 2\n.\nOn the other hand, let U\u22c6 = LX\u03a3 2/3 X and V\u22c6 = RX\u03a3 1/3 X , where \u03a3 p is entry-wise power to p, then we have X = U\u22c6V T\u22c6 and\n\u2016X\u2016S2/3 = ( Tr2/3(\u03a3X) ) 3 2 = \u2016U\u22c6\u2016\u2217\u2016V\u22c6\u2016F = ( 2\u2016U\u22c6\u2016\u2217 + \u2016V\u22c6\u20162F\n3\n) 3 2\n.\nTherefore, under the constraint X = UV T , we have\n\u2016X\u2016S2/3 = min X=UV T \u2016U\u2016\u2217\u2016V \u2016F = min X=UV T\n( 2\u2016U\u2016\u2217 + \u2016V \u20162F\n3\n) 3 2\n= \u2016X\u2016F/N.\nThis completes the proof.\nProof of Theorem 2: In the following, we will first prove that the bi-nuclear norm \u2016\u00b7\u2016BiN is a quasi-norm. Proof. By the definition of the bi-nuclear norm, for any a, a1, a2 \u2208 R and a = a1a2, we have\n\u2016aX\u2016BiN = min aX=(a1U)(a2V T ) \u2016a1U\u2016\u2217\u2016a2V \u2016\u2217\n= min X=UV T |a| \u2016U\u2016\u2217\u2016V \u2016\u2217 = |a| min\nX=UV T \u2016U\u2016\u2217\u2016V \u2016\u2217\n= |a| \u2016X\u2016BiN. Since \u2016X\u2016\u2217 = minX=UV T \u2016U\u2016F\u2016V \u2016F , and by Lemma 6 in (Mazumder, Hastie, and Tibshirani 2010), there exist both\nmatrices U\u0302 = UX\u03a3 1/2 X and V\u0302 = VX\u03a3 1/2 X such that \u2016X\u2016\u2217 = \u2016U\u0302\u2016F \u2016V\u0302 \u2016F with the SVD of X , i.e., X = UX\u03a3XV TX . By the\nfact that \u2016X\u2016\u2217 \u2264 \u221a\nrank(X)\u2016X\u2016F , we have \u2016X\u2016BiN = min\nX=UV T \u2016U\u2016\u2217\u2016V \u2016\u2217\n\u2264 \u2016U\u0302\u2016\u2217\u2016V\u0302 \u2016\u2217 \u2264 \u221a rank(X) \u221a rank(X)\u2016U\u0302\u2016F \u2016V\u0302 \u2016F\n\u2264 rank(X)\u2016X\u2016\u2217. If X 6= 0, then rank(X) \u2265 1. On the other hand, we also have\n\u2016X\u2016\u2217 \u2264 \u2016X\u2016BiN. By the above properties, there exists a constant \u03b1 \u2265 1 such that the following holds for all X,Y \u2208 Rm\u00d7n\n\u2016X + Y \u2016BiN \u2264 \u03b1\u2016X + Y \u2016\u2217 \u2264 \u03b1(\u2016X\u2016\u2217 + \u2016Y \u2016\u2217) \u2264 \u03b1(\u2016X\u2016BiN + \u2016Y \u2016BiN).\n\u2200X \u2208 Rm\u00d7n and X = UV T , we have \u2016X\u2016BiN = min\nX=UV T \u2016U\u2016\u2217\u2016V \u2016\u2217 \u2265 0.\nIn addition, if \u2016X\u2016BiN = 0, we have \u2016X\u2016\u2217 \u2264 \u2016X\u2016BiN = 0, i.e., \u2016X\u2016\u2217 = 0. Hence, X = 0. In short, the bi-nuclear norm \u2016 \u00b7\u2016BiN is a quasi-norm.\nProof of Theorem 2:\nProof. To prove this theorem, we use the same notations as in the proof of Theorem 1, for instance, X = LX\u03a3XRTX , U = LU\u03a3UR T U and V =LV \u03a3V R T V denote the SVDs of X , U and V , respectively. By Lemma 2, and p = 1/2, we have\n\u2016X\u2016S1/2 \u2264 ( Tr1/2(O2\u03a3XO T 2 ) )2 = ( Tr1/2(O2O T 1 \u03a3UO4\u03a3V ) )2 = ( Tr1/2(O3\u03a3UO4\u03a3V ) )2\n=\n  d\u2211\ni=1\n\u221a\u221a\u221a\u221a d\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\u033ai\n  2\n=\n  d\u2211\ni=1\n\u221a\u221a\u221a\u221a\u033ai d\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\n  2\na\u2264 d\u2211\ni=1\n\u033ai\nd\u2211\ni=1\nd\u2211\nj=1\n\u03c4j(O3)ij(O4)ji\nb\u2264 d\u2211\ni=1\n\u033ai\nd\u2211\ni=1\nd\u2211\nj=1\n(O3) 2 ij\u03c4j + (O4) 2 ji\u03c4j\n2\nc\u2264 d\u2211\ni=1\n\u033ai\nd\u2211\nj=1\n\u03c4j\n= \u2016U\u2016\u2217\u2016V \u2016\u2217 \u2264 (\u2016U\u2016\u2217 + \u2016V \u2016\u2217\n2\n)2 ,\nwhere the inequality a\u2264 holds due to the Cauchy\u2212Schwartz inequality, the inequality b\u2264 follows from the basic inequality xy \u2264 x2+y22 for any real numbers x and y, and the inequality c\u2264 relies on the facts \u2211 i(O3) 2 ij = 1 and \u2211 i(O4) 2 ji \u2264 1. Thus, we have\n\u2016X\u2016S1/2 \u2264 \u2016U\u2016\u2217\u2016V \u2016\u2217 \u2264 (\u2016U\u2016\u2217 + \u2016V \u2016\u2217\n2\n)2 .\nOn the other hand, let U\u22c6 = LX\u03a3 1/2 X and V\u22c6 = RX\u03a3 1/2 X , then we have X = U\u22c6V T \u22c6 and\n\u2016X\u2016S1/2 = ( Tr1/2(\u03a3X) )2 = (\u2016LX\u03a31/2\u2016\u2217 + \u2016RX\u03a31/2\u2016\u2217 2 )2 = (\u2016U\u22c6\u2016\u2217 + \u2016V\u22c6\u2016\u2217 2 )2 .\nTherefore, under the constraint X = UV T , we have\n\u2016X\u2016S1/2 = min X=UV T \u2016U\u2016\u2217\u2016V \u2016\u2217 = min X=UV T (\u2016U\u2016\u2217 + \u2016V \u2016\u2217 2 )2 = min X=UV T \u2016U\u20162\u2217 + \u2016V \u20162\u2217 2 = \u2016X\u2016BiN.\nThis completes the proof.\nProof of Property 3: Proof. The proof involves some following properties of the \u2113p quasi-norm, which must be recalled. For any vector x in Rn and 0 < p2 \u2264 p1 \u2264 1, we have\n\u2016x\u2016\u21131 \u2264 \u2016x\u2016\u2113p1 , \u2016x\u2016\u2113p1 \u2264 \u2016x\u2016\u2113p2 \u2264 n 1/p2\u22121/p1\u2016x\u2016\u2113p1 .\nSuppose X\u2208Rm\u00d7n is of rank r, and denote its SVD by X = Um\u00d7r\u03a3r\u00d7rV Tn\u00d7r. By Theorems 1 and 2, and the properties of the \u2113p quasi-norm, we have\n\u2016X\u2016\u2217 = \u2016diag(\u03a3r\u00d7r)\u2016\u21131 \u2264 \u2016diag(\u03a3r\u00d7r)\u2016\u21132/3 = \u2016X\u2016F/N \u2264 \u221a r\u2016X\u2016\u2217,\n\u2016X\u2016\u2217 = \u2016diag(\u03a3r\u00d7r)\u2016\u21131 \u2264 \u2016diag(\u03a3r\u00d7r)\u2016\u21131/2 = \u2016X\u2016BiN \u2264 r\u2016X\u2016\u2217. Similarly, \u2016X\u2016F/N = \u2016diag(\u03a3r\u00d7r)\u2016\u21132/3 \u2264 \u2016diag(\u03a3r\u00d7r)\u2016\u21131/2 = \u2016X\u2016BiN. This completes the proof.\nProof of Theorem 3: In this paper, the proposed algorithms are based on the proximal alternating linearized minimization (PALM) method for solving the following non-convex problem:\nmin x,y \u03a8(x, y) := F (x) +G(y) +H(x, y), (11)\nwhere F (x) and G(y) are proper lower semi-continuous functions, and H(x, y) is a smooth function with Lipschitz continuous gradients on any bounded set.\nIn Algorithm 1, we state that our algorithm alternates between two blocks of variables, U and V . We establish the global convergence of Algorithm 1 by transforming the problem (3) into a standard form (11), and show that the transformed problem satisfies the condition needed to establish the convergence. First, the minimization problem (3) can be expressed in the form of (11) by setting \n  F (U) := 2\u03bb3 \u2016U\u2016\u2217; G(V ) := \u03bb3 \u2016V \u20162F ; H(U, V ) := 12\u2016P\u2126(UV T )\u2212 P\u2126(D)\u20162F .\nThe conditions for global convergence of the PALM algorithm proposed in (Bolte, Sabach, and Teboulle 2014) are shown in the following lemma.\nLemma 3. Let {(xk, yk)} be a sequence generated by the PALM algorithm. This sequence converges to a critical point of (11), if the following conditions hold:\n1. \u03a8(x, y) is a Kurdyka-\u0141ojasiewicz (KL) function;\n2. \u2207H(x, y) has Lipschitz constant on any bounded set; 3. {(xk, yk)} is a bounded sequence.\nAs stated in the above lemma, the first condition requires that the objective function satisfies the KL property (For more details, see (Bolte, Sabach, and Teboulle 2014)). It is known that any proper closed semi-algebraic function is a KL function as such a function satisfies the KL property for all points in domf with \u03d5(s) = cs1\u2212\u03b8 for some \u03b8 \u2208 [0, 1) and some c > 0. Therefore, we first give the following definitions of semi-algebraic sets and functions, and then prove that the proposed problem (3) is also semi-algebraic.\nDefinition 4 (Bolte, Sabach, and Teboulle (2014)). A subset S \u2282 Rn is a real semi-algebraic set if there exists a finite number of real polynomial functions \u03c6ij , \u03c8ij : Rn \u2192 R such that\nS = \u22c3\nj\n\u22c2\ni\n{u \u2208 Rn : \u03c6ij(u) = 0, \u03c8ij(u) < 0} .\nMoreover, a function h(u) is called semi-algebraic if its graph {(u, t) \u2208 Rn+1 : h(u) = t} is a semi-algebraic set. Semi-algebraic sets are stable under the operations of finite union, finite intersections, complementation and Cartesian product. The following are the semi-algebraic functions or the property of semi-algebraic functions used below:\n\u2022 Real polynomial functions. \u2022 Finite sums and product of semi-algebraic functions. \u2022 Composition of semi-algebraic functions. Lemma 4. Each term in the proposed problem (3) is a semi-algebraic function, and thus the function (3) is also semi-algebraic.\nProof. It is easy to notice that the sets U = {U \u2208 Rm\u00d7d : \u2016U\u2016\u221e \u2264 D1} and V = {V \u2208 Rn\u00d7d : \u2016V \u2016\u221e \u2264 D2} are both semi-algebraic sets, where D1 and D2 denote two pre-defined upper-bounds for all entries of U and V , respectively.\nFor the first term F (U) = 2\u03bb3 \u2016U\u2016\u2217. According to (Bolte, Sabach, and Teboulle 2014), we can know that the \u21131-norm is a semi-algebraic function. Since the nuclear norm is equivalent to the \u21131-norm on singular values of the associated matrix, it is natural that the nuclear norm is also semi-algebraic.\nFor both terms G(V ) = \u03bb3 \u2016V \u20162F and H(U, V ) = 12\u2016P\u2126(UV T \u2212D)\u20162F , they are real polynomial functions, and thus are semi-algebraic functions (Bolte, Sabach, and Teboulle 2014). Therefore, (3) is semi-algebraic due to the fact that a finite sum of semi-algebraic functions is also semi-algebraic.\nFor the second condition in Lemma 3, H(U, V ) = 12\u2016P\u2126(UV T\u2212D)\u20162F is a smooth polynomial function, and \u2207H(U, V ) = ([P\u2126(UV T\u2212D)]V, [P\u2126(UV T \u2212D)]TU). It is natural that \u2207H(U, V ) has Lipschitz constant on any bounded set.\nFor the final condition in Lemma 3, Uk \u2208 U and Vk \u2208 V for any k = 1, 2, . . ., which implies the sequence {(Uk, Vk)} is bounded.\nIn short, we can know that three similar conditions as in Lemma 3 hold for Algorithm 1. According to the above discussion, it is clear that the problem (4) is also a semi-algebraic function. In other words, another proposed algorithm shares the same convergence property as in Theorem 3.\nProof of Theorem 5: According to Theorem 3, we can know that (U\u0302 , V\u0302 ) is a critical point of the problem (3). To prove Theorem 5, we first give the following lemmas.\nLemma 5 (Lin, Chen, and Wu (2009)). Let H be a real Hilbert space endowed with an inner product \u3008\u00b7, \u00b7\u3009 and an associated norm \u2016\u00b7\u2016, and y \u2208 \u2202\u2016x\u2016, where \u2202\u2016\u00b7\u2016 denotes the subgradient of the norm. Then \u2016y\u2016\u2217 = 1 if x 6= 0, and \u2016y\u2016\u2217 \u2264 1 if x = 0, where \u2016\u00b7\u2016\u2217 is the dual norm of \u2016\u00b7\u2016. For instance, the dual norm of the nuclear norm is the spectral norm, \u2016\u00b7\u20162, i.e., the largest singular value.\nLemma 6 (Wang and Xu (2012)). Let L(X) = 1\u221a mn \u2016X\u2212X\u0302\u2016F and L\u0302(X) = 1\u221a|\u2126|\u2016P\u2126(X\u2212X\u0302)\u2016F be the actual and empirical loss function respectively, where X, X\u0302 \u2208 Rm\u00d7n (m \u2265 n). Furthermore, assume entry-wise constraint maxi,j |Xij | \u2264 \u03b21. Then for all rank-r matrices X , with probability greater than 1\u2212 2 exp(\u2212m), there exists a fixed constant C such that\nsup X\u2208Sr\n|L\u0302(X)\u2212 L(X)| \u2264 C\u03b21 ( mr log(m)\n|\u2126|\n)1/4 ,\nwhere Sr = {X \u2208 Rm\u00d7n : rank(X) \u2264 r, \u2016X\u2016F \u2264 \u221a mn\u03b21}."}, {"heading": "Proof of Theorem 5:", "text": "Proof. By C2 = \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2016F /\u2016P\u2126(D \u2212 U\u0302 V\u0302 T )\u2016F , we have\n\u2016D \u2212 U\u0302 V\u0302 T \u2016F\u221a mn\n\u2264 \u2223\u2223\u2223\u2223\u2223 \u2016D \u2212 U\u0302 V\u0302 T \u2016F\u221a mn \u2212 \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2016F C2 \u221a |\u2126| \u2223\u2223\u2223\u2223\u2223+ \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2016F C2 \u221a |\u2126|\n= \u2223\u2223\u2223\u2223\u2223 \u2016D \u2212 U\u0302 V\u0302 T \u2016F\u221a mn \u2212 \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )\u2016F\u221a |\u2126| \u2223\u2223\u2223\u2223\u2223+ \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 )\u2016F C2 \u221a |\u2126| .\nLet \u03c4(\u2126) := \u2223\u2223\u2223\u2223 1\u221a mn \u2016D \u2212 U\u0302 V\u0302 T \u2016F \u2212 1\u221a|\u2126|\u2016P\u2126(D \u2212 U\u0302 V\u0302 T )\u2016F \u2223\u2223\u2223\u2223, then we need to bound \u03c4(\u2126). Since rank(U\u0302V\u0302 T ) \u2264 d and\nU\u0302 V\u0302 T \u2208 Sd, and according to Lemma 6, then with probability greater than 1 \u2212 2 exp(\u2212m), then there exists a fixed constant C1 such that\nsup U\u0302V\u0302 T\u2208Sd \u03c4(\u2126) =\n\u2223\u2223\u2223\u2223\u2223 \u2016U\u0302 V\u0302 T \u2212D\u2016F\u221a mn \u2212 \u2016P\u2126(U\u0302 V\u0302 T )\u2212 P\u2126(D)\u2016F\u221a |\u2126| \u2223\u2223\u2223\u2223\u2223\n\u2264C1\u03b2 ( md log(m)\n|\u2126|\n) 1 4\n.\n(12)\nWe also need to bound \u2016P\u2126(U\u0302 V\u0302 T \u2212D)V\u0302 \u2016F . Given V\u0302 , the optimization problem with respect to U is formulated as follows:\nmin U 2\u03bb\u2016U\u2016\u2217 3 + 1 2 \u2016P\u2126(UV\u0302 T )\u2212 P\u2126(D)\u20162F . (13)\nSince (U\u0302 , V\u0302 ) is a critical point of the problem (3), the first-order optimal condition of the problem (13) is written as follows:\nP\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2208 2\u03bb\n3 \u2202\u2016U\u0302\u2016\u2217. (14)\nUsing Lemma 5, we obtain\n\u2016P\u2126(U\u0302 V\u0302 T \u2212D)V\u0302 \u20162 \u2264 2\u03bb\n3 ,\nwhere \u2016\u00b7\u20162 is the spectral norm. According to rank(P\u2126(U\u0302 V\u0302 T \u2212D)V\u0302 ) \u2264 d, we have\n\u2016P\u2126(U\u0302 V\u0302 T \u2212D)V\u0302 \u2016F \u2264 \u221a d\u2016P\u2126(U\u0302 V\u0302 T \u2212D)V\u0302 \u20162 \u2264\n2 \u221a d\u03bb\n3 . (15)\nBy (12) and (15), we have\n\u2016Z \u2212 U\u0302 V\u0302 T \u2016F\u221a mn \u2264\u2016E\u2016F\u221a mn + \u2016D \u2212 U\u0302 V\u0302 T \u2016F\u221a mn\n\u2264\u2016E\u2016F\u221a mn + \u03c4(\u2126) + \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2016F C2 \u221a |\u2126|\n\u2264\u2016E\u2016F\u221a mn + C1\u03b2\n( md log(m)\n|\u2126|\n) 1 4 + 2 \u221a d\u03bb\n3C2 \u221a |\u2126| .\nThis completes the proof."}, {"heading": "Lower bound on C2", "text": "Finally, we also discuss the lower boundedness of C2, that is, it is lower bounded by a positive constant. By the characterization of the subdifferentials of norms, we have\n\u2202\u2016X\u2016\u2217 = {Y | \u3008Y, X\u3009 = \u2016X\u2016\u2217, \u2016Y \u20162 \u2264 1} . (16)\nAlgorithm 2 Solving (4) via PALM Input: P\u2126(D), the given rank d and \u03bb. Initialize: U0, V0, \u03b5 and k = 0.\n1: while not converged do 2: Update lgk+1 and Uk+1 by\nlgk+1 = \u2016Vk\u201622 and Uk+1 = argmin U\n\u03bb 2 \u2016U\u2016\u2217 + lgk+1 2 \u2016U \u2212 Uk + \u2207gk(Uk) lgk+1 \u20162F .\n3: Update lhk+1 and Vk+1 by\nlhk+1 = \u2016Uk+1\u201622 and Vk+1 = argmin V\n\u03bb 2 \u2016V \u2016\u2217 + lhk+1 2 \u2016V \u2212 Vk + \u2207hk(Vk) lhk+1 \u20162F .\n4: Check the convergence condition, max{\u2016Uk+1\u2212Uk\u2016F , \u2016Vk+1\u2212Vk\u2016F} < \u03b5. 5: end while\nOutput: Uk+1, Vk+1.\nLet Q = P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 , and by (14), we have Q \u2208 2\u03bb3 \u2202\u2016U\u0302\u2016\u2217. By (16), we have\u2329 3\n2\u03bb Q, U\u0302\n\u232a = \u2016U\u0302\u2016\u2217.\nNote that \u2016X\u2016\u2217 \u2265 \u2016X\u2016F and \u3008X,Y \u3009 \u2264 \u2016X\u2016F\u2016Y \u2016F for any matrices X and Y of the same size. 3\n2\u03bb \u2016Q\u2016F\u2016U\u0302\u2016F \u2265\n\u2329 3\n2\u03bb Q, U\u0302\n\u232a = \u2016U\u0302\u2016\u2217 \u2265 \u2016U\u0302\u2016F .\nSince \u2016U\u0302\u2016F > 0 and \u03bb 6= 0, thus we obtain\n\u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2016F = \u2016Q\u2016F \u2265 2\u03bb\n3 .\nU\u0302 is the optimal solution of the problem (13) with the given matrix V\u0302 , then\n\u2016P\u2126(D \u2212 U\u0302 V\u0302 T )\u20162F < \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )\u20162F + 2\u03bb\n3 \u2016U\u0302\u2016\u2217 \u2264 \u2016P\u2126(D)\u20162F = \u03b3,\nwhere \u03b3 > 0 is a constant. Thus,\nC2 = \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )V\u0302 \u2016F \u2016P\u2126(D \u2212 U\u0302 V\u0302 T )\u2016F > 2\u03bb 3 \u221a \u03b3 > 0.\nPALM Algorithm for Solving (4) We present an efficient proximal alternating linearized minimization (PALM) algorithm for solving the bi-nuclear quasi-norm regularized matrix completion problem (4), as outlined in Algorithm 2. Moreover, Algorithm 2 shares the same convergence property as in Theorems 3 and 4, and has the similar theoretical recovery guarantee as in Theorem 5.\nReferences Bolte, J.; Sabach, S.; and Teboulle, M. 2014. Proximal alternating linearized minimization for nonconvex and nonsmooth\nproblems. Math. Program. 146:459\u2013494.\nLin, Z.; Chen, M.; and Wu, L. 2009. The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices. Univ. Illinois, Urbana-Champaign.\nMazumder, R.; Hastie, T.; and Tibshirani, R. 2010. Spectral regularization algorithms for learning large incomplete matrices. J. Mach. Learn. Res. 11:2287\u20132322.\nMitrinovic\u0301, D. S. 1970. Analytic Inequalities. Heidelberg: Springer-Verlag.\nWang, Y., and Xu, H. 2012. Stability of matrix factorization for collaborative filtering. In Proc. 29th Int. Conf. Mach. Learn. (ICML), 417\u2013424."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The Schatten-p quasi-norm (0<p<1) is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately. However, existing Schattenp quasi-norm minimization algorithms involve singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration, and thus may become very slow and impractical for large-scale problems. In this paper, we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices. We also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems. Finally, we provide the global convergence and performance guarantees for our algorithms, which have better convergence properties than existing algorithms. Experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-ofthe-art methods, and are orders of magnitude faster. Introduction In recent years, the matrix rank minimization problem arises in a wide range of applications such as matrix completion, robust principal component analysis, low-rank representation, multivariate regression and multi-task learning. To solve such problems, Fazel, Hindi, and Boyd; Cand\u00e8s and Tao; Recht, Fazel, and Parrilo (2001; 2010; 2010) have suggested to relax the rank function by its convex envelope, i.e., the nuclear norm. In fact, the nuclear norm is equivalent to the l1-norm on singular values of a matrix, and thus it promotes a low-rank solution. However, it has been shown in (Fan and Li 2001) that the l1-norm regularization over-penalizes large entries of vectors, and results in a biased solution. By realizing the intimate relationship between them, the nuclear norm penalty also over-penalizes large singular values, that is, it may make the solution deviate from the original solution as the l1-norm does (Nie, Huang, and Ding 2012; Lu et al. 2015). Compared with the nuclear norm, the Schatten-p quasi-norm for 0 < p < 1 makes a closer Copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. approximation to the rank function. Consequently, the Schatten-p quasi-norm minimization has attracted a great deal of attention in images recovery (Lu and Zhang 2014; Lu et al. 2014), collaborative filtering (Nie et al. 2012; Lu et al. 2015; Mohan and Fazel 2012) and MRI analysis (Majumdar and Ward 2011). In addition, many non-convex surrogate functions of the l0-norm listed in (Lu et al. 2014; Lu et al. 2015) have been extended to approximate the rank function, such as SCAD (Fan and Li 2001) and MCP (Zhang 2010). All non-convex surrogate functions mentioned above for low-rank minimization lead to some non-convex, nonsmooth, even non-Lipschitz optimization problems. Therefore, it is crucial to develop fast and scalable algorithms which are specialized to solve some alternative formulations. So far, Lai, Xu, and Yin (2013) proposed an iterative reweighted lease squares (IRucLq) algorithm to approximate the Schatten-p quasi-norm minimization problem, and proved that the limit point of any convergent subsequence generated by their algorithm is a critical point. Moreover, Lu et al. (2014) proposed an iteratively reweighted nuclear norm (IRNN) algorithm to solve many non-convex surrogate minimization problems. For matrix completion problems, the Schatten-p quasi-norm has been shown to be empirically superior to the nuclear norm (Marjanovic and Solo 2012). In addition, Zhang, Huang, and Zhang (2013) theoretically proved that the Schatten-p quasi-norm minimization with small p requires significantly fewer measurements than the convex nuclear norm minimization. However, all existing algorithms have to be solved iteratively and involve SVD or EVD in each iteration, which incurs high computational cost and is too expensive for solving large-scale problems (Cai and Osher 2013; Liu et al. 2014). In contrast, as an alternative non-convex formulation of the nuclear norm, the bilinear spectral regularization as in (Srebro, Rennie, and Jaakkola 2004; Recht, Fazel, and Parrilo 2010) has been successfully applied in many large-scale applications, e.g., collaborative filtering (Mitra, Sheorey, and Chellappa 2010). As the Schatten-p quasi-norm is equivalent to the lp quasi-norm on singular values of a matrix, it is natural to ask the following question: can we design equivalent matrix factorization forms for the cases of the Schatten quasi-norm, e.g., p = 2/3 or 1/2? In order to answer the above question, in this paper we<lb>first define two tractable Schatten quasi-norms, i.e., the<lb>Frobenius/nuclear hybrid and bi-nuclear quasi-norms. We<lb>then prove that they are in essence the Schatten-2/3 and 1/2<lb>quasi-norms, respectively, for solving whose minimization<lb>we only need to perform SVDs on two much smaller fac-<lb>tor matrices as contrary to the larger ones used in existing<lb>algorithms, e.g., IRNN. Therefore, our method is particu-<lb>larly useful for many \u201cbig data\u201d applications that need to<lb>deal with large, high dimensional data with missing values.<lb>To the best of our knowledge, this is the first paper to scale<lb>Schatten quasi-norm solvers to the Netflix dataset. More-<lb>over, we provide the global convergence and recovery per-<lb>formance guarantees for our algorithms. In other words, this<lb>is the best guaranteed convergence for algorithms that solve<lb>such challenging problems. Notations and Background<lb>The Schatten-p norm (0 < p < \u221e) of a matrix X \u2208 Rm\u00d7n<lb>(m \u2265 n) is defined as \u2016X\u2016Sp ,<lb>(<lb>n<lb>\u2211 i=1<lb>\u03c3<lb>i (X)<lb>)1/p", "creator": "LaTeX with hyperref package"}}}