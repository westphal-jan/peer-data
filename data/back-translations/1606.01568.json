{"id": "1606.01568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Active Regression with Adaptive Huber Loss", "abstract": "This paper addresses the scalar regression problem and provides a solution to optimize Huber loss in a general semi-monitored environment that combines multivisionary learning and diverse regularization. To this end, we propose a principled algorithm to 1) avoid arithmetically expensive iterative solutions, and 2) adjust the Huber loss threshold based on data, and 3) actively balance the use of labeled data to remove noisy or inconsistent annotations from the training phase. In a comprehensive experimental evaluation covering various applications, we evaluate the superiority of our paradigm, which is capable of combining strong performance and robustness with low computational effort with noise.", "histories": [["v1", "Sun, 5 Jun 2016 21:59:34 GMT  (4477kb,D)", "http://arxiv.org/abs/1606.01568v1", null], ["v2", "Sun, 26 Jun 2016 07:24:43 GMT  (4476kb,D)", "http://arxiv.org/abs/1606.01568v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jacopo cavazza", "vittorio murino"], "accepted": false, "id": "1606.01568"}, "pdf": {"name": "1606.01568.pdf", "metadata": {"source": "CRF", "title": "Active-Labelling by Adaptive Huber Loss Regression", "authors": ["Jacopo Cavazza", "Vittorio Murino"], "emails": ["jacopo.cavazza@iit.it."], "sections": [{"heading": null, "text": "Index Terms\u2014Robust Scalar Regression, Label Active Selection, Huber Loss with Adaptive Threshold, Convex Optimization, Learning from Noisy Labels, Crowd Counting.\nI. INTRODUCTION\nRegression is one of the most widely studied problems in different research disciplines such as statistics, econometrics, computational biology and physics to name a few, and is a pillar topic in machine learning. Formally, it addresses the problem of inferring a functional relationship between input and output spaces. Under a classical machine learning perspective, this is learnt by means of (training) examples and, to this aim, two mainstream approaches pop up: optimizationbased and Bayesian frameworks. In the former, once a suitable space of functions is fixed, the goal is minimizing an objective functional, where a loss measures how good the learned function reproduces input-output relationships inside the training set. Instead, in the Bayesian formalism, a prior distribution constrains the solution upon some a priori knowledge, while the final regression map maximizes the posterior/likelihood probability distribution.\nA problem which affects both paradigms lies in the size of available training data, whatever regression technique is used. Actually, the scarcity of annotations can seriously impact on the performance, making the adopted method ineffective for those applications where labelled data are difficult to obtain. In this respect, semi-supervised approaches play a substantial role in exploiting unlabelled samples to support the search for the solution. Among the others, multi-view learning [1], [2] is a class of algorithms that considers the structure of the input data as composed by several \u201cviews\u201d which are associated to different hypothesis spaces employed to construct as many\nJacopo Cavazza and Vittorio Murino are with Pattern Analysis & Computer Vision, Istituto Italiano di Tecnologia (IIT), Via Morego, 30, 16163, Genova, Italy.\nJacopo Cavazza is also with Dipartimento di Ingegneria Navale, Elettrica, Elettronica e delle Telecomunicazioni (DITEN), University of Genoa, Via Opera Pia 11, 16145 Genova, Italy.\nVittorio Murino is also with Computer Science Department, University of Verona, Strada Le Grazie 15, 37134, Verona, Italy.\nPrimary email contact: jacopo.cavazza@iit.it.\ndecision functions, finally fusing them together. Manifold regularization constitutes another family of semi-supervised methods which use the geometry of the unknown probability distribution underlying the data to shape the structure of the solution space [3], typically imposing regularization by means of the graph Laplacian operator [4].\nHowever, even in the small labelling regime, since annotations are usually provided by human operators, they are frequently prone to errors and noisy in general, making them rather misleading. Hence, for both theoretical and practical aspects, it is of utmost importance to devise algorithms which are able to automatically analyse the data as to guarantee robustness towards outliers. As an efficient tool to tackle the latter issue, [5] effectively take advantage of the Huber loss\nH\u03be : R\u2192 [0,+\u221e), H\u03be(y) =\n{ y2\n2 if |y| \u2264 \u03be \u03be|y| \u2212 \u03be 2 2 otherwise, (1)\nwhere \u03be > 0. However, as the major drawback of such a loss, there is no closed-form solution to optimize it and, as a consequence, iterative schemes (such as quadratic programming [6] or self-dual minimization [7]) were previously exploited for either the original Huber loss [8], [7] or its spurious versions (hinge-Huber [6] or the huberized Laplacian [9]). Moreover, in all cases, additional computational efforts have to be spent in order to fix the threshold \u03be, such as statistical efficiency analysis [8].\nIn this work we face all the aforementioned issues trough the following main contributions.\nI . We derive a theoretical solution to exactly optimize the Huber loss in a semi-supervised setup, where we unified multi-view learning and manifold regularization, as to guarantee the maximal generalization of our pipeline1. II . We devise the novel Huber Loss Regression (HLR) algorithm to efficiently implement the proposed solution as to avoid classical iterative schemes [6], [7]. With respect other regression approaches, the following two aspects of HLR are remarkable. Active-Labelling. While taking advantage of the entire labelled and unlabelled component of the sample set, the former is inspected so that HLR automatically removes those annotations which violate a specific numerical check, whenever recognized as either noisy or inconsistent for the training process. Adaptive threshold. Differently from all [8], [6], [7], [9], \u03be is automatically learnt in a data-driven fashion by\n1In semi-superivision, manifold regularization and multi-view learning are two of the most effective approaches which have been theoretically unified by [10] also demonstrating the effectiveness of their combination for computer vision applications.\nar X\niv :1\n60 6.\n01 56\n8v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\n6\n2 our algorithm, with no additional analysis required and without increasing HLR computational complexity.\nIII . With an extensive empirical experimentation, we validate the proposed technique, which allows to score competitive results in curve fitting, learning with noisy labels, classical regression problems and crowd counting application. Despite using variegate types of data and considering diverse problems, HLR is able to outperform state-of-the-art regression algorithms.\nThe paper is outlined as follows. In Sections II we present our general semi-supervised setting where, in Section III, we propose our solution for optimizing the Huber loss. Section IV broadly discusses our HLR algorithm. As to prove the versatility of HLR, once benchmarked with a state-of-theart convex solver in Section V-A, we registered a strong performance when applying it to curve fitting and learning from noisy labels (Section V-B), classical regression problems (Section V-C) and crowd counting application (Section V-D). Finally, Section VI draws the conclusions."}, {"heading": "II. MULTI-VIEW SCALAR REGRESSION", "text": "In this Section, we introduce the formalism to model data points sampled from a composite input space X which is divided into multiple substructures. Precisely, we assume that for any x \u2208 X , we have x = [x1, . . . , xm] and x\u03b1 belongs to the subspace X\u03b1, for any \u03b1 = 1, . . . ,m. This is a very natural way to model high dimensional data: x is the concatenation of x1, . . . , xm, each one representing a particular class of features, that is one out of multiple views [1], [2], [10] in which data may be structured.\nIn order to find the regression map, we assume that it belongs to an hypothesis space H of functions h : X \u2192 R whose construction is investigated below. For any \u03b1 = 1, . . . ,m, let \u03ba\u03b1 : X\u03b1 \u00d7 X\u03b1 \u2192 R a Mercer kernel [11], that is a symmetric and positive semi-definite function. Let us define K(x, z) = diag(\u03ba1(x1, z1), . . . , \u03bam(xm, zm)) \u2208 Rm\u00d7m, where x = [x1, . . . , xm], z = [z1, . . . , zm] \u2208 X . Consider S0 the space of functions z 7\u2192 f(z) = \u2211n i=1K(z,xi)ui with x1, . . . ,xn \u2208 X and u1, . . . , un column vectors in Rm. Define the norm \u2016f\u20162K = \u2211n i,j=1 u > iK(xi,xj)uj . The reproducing kernel Hilbert space (RKHS) SK related to K is the completion of S0 with the limits of Cauchy sequences converging with respect to \u2016\u00b7\u2016K [12]. Finally, c \u2208 Rm induces a sampling operator c>: SK \u2192 H whose image is our final hypothesis space. Consequently,\nh(z) = c>f(z) = n\u2211 i=1 c>K(z,xi)ui (2)\nis the general expression for every h \u2208 H with f \u2208 S0. a) Remark.: Differently from other previous multi-view learning paradigms [2], [1], [10], the kernel matrix K is fixed to be diagonal. Empirically, it allows to dedicate an independent kernel to each view, gathering the subspace information codified by our data. In addition, such choice provides some theoretical advantages. Indeed, as pointed out in [10], it is not trivial to prove that the kernel adopted by [2] is positive semi-definite, while such property is trivially\nfulfilled in our setting. Also, thanks to (2), we can recover h(z) = [h1(z1), . . . , hm(zm)] for every m by means of the analytical expression h\u03b1(z\u03b1) = \u2211 i c \u03b1\u03ba\u03b1(z\u03b1, x\u03b1i )u \u03b1 i , for \u03b1 = 1, . . . ,m. Differently, in [1] an analogous result only holds for m = 2."}, {"heading": "III. HUBER LOSS-BASED REGRESSION", "text": "Once the hypothesis spaceH is defined according to Section II, we can perform an optimization based learning for the regression map. To this aim, we consider a training set D made of ` labelled instances (x1, y1), . . . , (x`, y`) \u2208 X \u00d7 R and u additional unlabelled inputs x`+1, . . . ,x`+u \u2208 X . Among the class of functions (2), similarly to [3], [10], the regression map is found by minimizing the following objective functional\nJ\u03bb,\u03b3(f) = 1\n` \u2211\u0300 i=1 H\u03be(yi \u2212 c>f(xi)) + \u03bb\u2016f\u20162K + \u03b3\u2016f\u20162M . (3)\nIn equation (3), 1` \u2211` i=1H\u03be(yi \u2212 c>f(xi)) represents the empirical risk and is defined by means of the Huber loss (1), \u03be > 0: it measures how well c>f(xi) predicts the ground truth output yi. To avoid either under-fitting or over-fitting, \u2016f\u20162K is a Tichonov regularizer which controls the complexity of the solution, it is scaled by \u03bb > 0. Instead, \u03b3 > 0 weighs\n\u2016f\u20162M = m\u2211 \u03b1=1 u+\u2211\u0300 i,j=1 f\u03b1(x\u03b1i )M \u03b1 ijf \u03b1(x\u03b1j ), (4)\nwhich infers the geometrical information of the feature space. The family {M1, . . . ,Mm} of symmetric and positive semidefinite matrices M\u03b1 \u2208 R(u+`)\u00d7(u+`) specifies \u2016f\u20162M and ensures its non-negativeness. This setting generalizes [3], [1], [2] where M1 = \u00b7 \u00b7 \u00b7 = Mm = L, being L is the graph Laplacian related to the (u+ `)\u00d7 (u+ `) adjacency matrix W. The latter captures the interdependencies between f(x1), . . . , f(xu+`) by measuring their mutual similarity. Then, in this simplified setting, the regularizer rewrites \u2211u+` i,j=1 wij\u2016f(xi)\u2212 f(xj)\u201622, if wij denotes the (i, j)-th entry of W. In this particular case, we retrieve the idea of manifold regularization [3] to enforce nearby patterns in the high-dimensional RKHS to share similar labels, so imposing further regularity on the solution. By means of our generalization, we can consider, for instance, m graph Laplacians L1, . . . , Lm of the view-specific adjacency matrices W 1, . . . ,Wm. Thus,\n\u2016f\u20162M = m\u2211 \u03b1=1 u+\u2211\u0300 i,j=1 w\u03b1ij \u2223\u2223f\u03b1(x\u03b1i )\u2212 f(x\u03b1j )\u2223\u22232 (5)\nforces the smoothness of f on any subspace X\u03b1 instead of doing it globally on X as in [3].\nUsual single-viewed (m = 1), fully supervised (u = 0) and classically regularized optimization problems (\u03b3 = 0) can be retrieve as particular cases, making our theoretical contribution applicable to a broad field of optimization problems. Precisely, in such a unified framework, we will now provide a general solution to guarantee the exact optimization for the convex and differentiable Huber loss function H\u03be [5], [13]. Actually, the Huber loss generalizes both the quadratic loss and the absolute value, which can be recovered in the extremal cases"}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 3", "text": "\u03be \u2192 +\u221e and \u03be \u2192 0 respectively. H\u03be has been shown to be robust against outliers [5] since, in a neighbourhood of the origin, it penalizes small errors in a more smoother way than absolute value, whereas, when |y| \u2265 \u03be, the linear growth plays an intermediate role between over-penalization (quadratic loss) and under-penalization (absolute value) of bigger errors. Globally, it resumes the positive aspect of the two losses, while remarkably mitigating their weaknesses. Of course, H\u03be suffers the issue of setting the threshold \u03be and, to this end, we devise an adaptive pipeline to learn it from data (see Section IV). For all these reasons, we focus on a Huber-loss based regression framework.\nIn order to optimize our objective (3), we can exploit the theory of the RKHS to optimize J\u03bb,\u03b3 over SK by using the Representer Theorem [4]. Precisely, in addition to ensuring existence and uniqueness of the minimizer f? = arg minf\u2208Sk J\u03bb,\u03b3(f), it provides the following expansion valid for any z \u2208 X :\nf?(z) = u+\u2211\u0300 j=1 c>K(z,xj)wj , (6)\nwhich involves the data inputs x1, . . . ,xu+` and some w1, . . . , wu+` \u2208 Rm. Equation (6) is the main tool exploited to deduce our general solution\nTheorem 1 (General solution for Huber loss multi-view manifold regularization regression). For any \u03be > 0, the coefficients w = [w1, . . . , wu+`]\n> defining the solution (6) of problem (3) are given by\n2`\u03bbwi + 2`\u03b3 u+\u2211\u0300 j,h=1 MijK(xj ,xh)wh =\n=  \u2212\u03bec if i \u2208 L+[D,w, \u03be]yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  c if i \u2208 L0[D,w, \u03be] +\u03bec if i \u2208 L\u2212[D,w, \u03be] 0 otherwise, (7)\nwhere \u03bb, \u03b3 > 0 and we set Mij = diag(M1ij , . . . ,M m ij ),\nL+[D,w, \u03be] = i \u2264 ` : u+\u2211\u0300 j=1 c>K(xi,xj)wj \u2265 yi + \u03be , L0[D,w, \u03be] =\ni \u2264 ` : \u2223\u2223\u2223\u2223\u2223\u2223 u+\u2211\u0300 j=1 c>K(xi,xj)wj \u2212 yi \u2223\u2223\u2223\u2223\u2223\u2223 < \u03be ,\nL\u2212[D,w, \u03be] = i \u2264 ` : u+\u2211\u0300 j=1 c>K(xi,xj)wj \u2264 yi \u2212 \u03be . Proof. To not fragment the dissertation, the proof was moved to the APPENDIX.\nLet us interpret numerically the sets L\u2212[D,w, \u03be], L0[D,w, \u03be], and L+[D,w, \u03be]. First of all, they provide a partition of {1, . . . , `}. Define \u03b5i = \u2223\u2223\u2223\u2211u+`j=1 c>K(xi,xj)wj \u2212 yi\u2223\u2223\u2223 ,\nthe absolute in-sample error involving the i-th labelled element (xi, yi) \u2208 D. Call \u03b5\u221e = maxi\u2264` \u03b5i their maximum. Thus, the set L0[D,w, \u03be] = {i \u2264 ` : \u03b5i < \u03be} collects all the indexes of those instances which are more effective in strictly reducing absolute errors \u03b5i below threshold \u03be. Instead, in L+[D,w, \u03be] and L\u2212[D,w, \u03be], the target variables are over and under-estimated, respectively. Furthermore, when L0[D,w, \u03be] = {1, . . . , `}, since \u03b5i < \u03be for any i, we obtain \u03b5\u221e < \u03be. So, if L+[D,w, \u03be] and L\u2212[D,w, \u03be] are empty, \u03be is an upper bound for the absolute error of the labelled training set in D. In spite of the nice numerical interpretation, the sets L+[D,w, \u03be], L0[D,w, \u03be], L\u2212[D,w, \u03be] can be computed only if w is given. Thus, apparently, we can not exploit the general solution (7) directly implementing it. Actually, such issue is solved in Section IV."}, {"heading": "IV. THE HLR ALGORITHM", "text": "As a preliminary step to introduce the algorithm to minimize the objective (3), we rephrase (7) into the matrix formulation (C?CQ[D,w, \u03be] + 2`\u03bbMK[x])w + 2`\u03b3w = C?b[D,w, \u03be], exploiting the notation reported below. \u2022 w = [w1, . . . , wu+`]\n> collects wj \u2208 Rm for any j. \u2022 Let y = [y1, . . . , y`]> and denote with y0 the result of\nappending to y a u\u00d7 1 vector of zeros. \u2022 M is the block matrix collecting Mij for any i, j. \u2022 Consider K[x], the Gram matrix defined by Kij [x] = K(xi,xj) for i, j = 1, . . . , u+ `. \u2022 Denoting In the n \u00d7 n identity matrix and \u2297 the Kronecker tensor product, let C = Iu+` \u2297 c> and C? = Iu+` \u2297 c.2 \u2022 Let Q[D,w, \u03be] \u2208 Rm(u+`)\u00d7m(u+`) a block matrix,\nQij [D,w, \u03be] = { Kij [x] if i \u2208 L0[D,w, \u03be] 0 otherwise.\n\u2022 b[D,w, \u03be] = [b1[D,w, \u03be], . . . , bu+`[D,w, \u03be]] >,\nbi[D,w, \u03be] =  \u2212\u03be if i \u2208 L+[D,w, \u03be] yi if i \u2208 L0[D,w, \u03be] +\u03be if i \u2208 L\u2212[D,w, \u03be] 0 if i = `+ 1, . . . , u+ `.\nAs observed in Section III, the main difficulty pertains to the sets L+[D,w, \u03be], L0[D,w, \u03be], and L\u2212[D,w, \u03be], which are not computable if w is unknown, compromising the numerical implementability of (7). As a very naive approach, if a certain initialization for the solution is provided, we can therefore set a scheme in which, at each iteration, L0, L+ and L\u2212 are firstly computed for an initial approximation w = wold of the solution. Then, w is updated to wnew solving the linear system (\nC?CQ[D,wold, \u03be] + 2`\u03bbMK[x] ) wnew+\n+ 2`\u03b3wnew = C?b[D,wold, \u03be] (8)\nfor a fixed \u03be > 0. Such approach suffers from several troubles. Indeed, an initialization is required for the scheme, whose\n2In terms of linear maps, matrices C and C? represent two operators, where one is the adjoint of the other.\n4 Algorithm 1. HLR algorithm pseudo-code Input: D dataset, M1, . . . ,Mm positive definite m \u00d7 m matrices, \u03bb > 0 Tichonov regularizing parameter, \u03b3 > 0 manifold regularization parameter, \u2206\u03be > 0 updating rate, maximum number T of refinements. Output: Coefficients vector w? = [w?1 , . . . , w?u+`]>. Find w(0) solving (C?C + `\u03b3M)K[x]w(0) + `\u03bbw(0) = C?y0. D(0) := D and compute \u03be(0) as in (9). for \u03c4 = 1, . . . , T do \u03be\u0303(\u03c4) := \u03be(\u03c4\u22121) \u2212\u2206\u03be. Solve (8) in the unknown wnew := w\u0303(\u03c4), with wold := w(\u03c4\u22121), D := D(\u03c4\u22121) and \u03be := \u03be\u0303(\u03c4). Compute the sets L0, L+ and L\u2212 on [D(\u03c4\u22121), w\u0303(\u03c4), \u03be\u0303(\u03c4)]. if L0[D(\u03c4\u22121), w\u0303(\u03c4), \u03be\u0303(\u03c4)] is empty then\nReturn w? := w(\u03c4\u22121). else\nObtain D(\u03c4) from D(\u03c4) by removing the labels yi of the data points (xi, yi) \u2208 D(\u03c4\u22121) such that i \u2208 L+[D\n(\u03c4\u22121), w\u0303(\u03c4), \u03be\u0303(\u03c4)] \u222a L\u2212[D(\u03c4\u22121), w\u0303(\u03c4), \u03be\u0303(\u03c4)]. Sort w(\u03c4) permuting w(\u03c4\u22121) in a way that the elements w\n(\u03c4\u22121) j with j \u2208 L0[D(\u03c4\u22121), w\u0303(\u03c4), \u03be\u0303(\u03c4)] occupy the\nfirst entries. Compute \u03be(\u03c4) using relation (9).\nend if end for Return w? := w(\u03c4)\nconvergence is eventually not guaranteed. Also, the Huber loss threshold \u03be has to be manually selected.\nAs a main contribution of the paper, we introduce the Huber loss regression algorithm (HLR) to perform a principled optimization and compute the exact solution of the problem (7), while also learning the best value for \u03be via a sequential refinements \u03be(0), \u03be(1), . . . , \u03be(T ) and automatically scanning the labelled data exploited. The pseudo-code for our method is described in Algorithm 1 and it is discussed in Section IV-A together with the following results, consisting in the theoretical foundation of HLR algorithm.\nProposition 1. For any \u03c4 = 0, 1, . . . , T, the coefficients w(\u03c4) satisfy (7) with \u03be = \u03be(\u03c4), where\n\u03be(\u03c4) = max i\u2264` \u2223\u2223\u2223\u2223\u2223\u2223 u+\u2211\u0300 j=1 c>K(xi,xj)w (\u03c4) j \u2212 yi \u2223\u2223\u2223\u2223\u2223\u2223 . (9) Proof. It is enough to show that, for any \u03c4 = 0, 1, . . . , T, we get L0[D(\u03c4),w(\u03c4), \u03be(\u03c4)] = {1, . . . , `}. Let\u2019s go by induction.\nFor \u03c4 = 0, as mentioned before, w(0) solves (7) for \u03be = +\u221e since, for any dataset D and any vector w of coefficients w1, . . . , wu+`, we have L0[D,w,+\u221e] = {1, . . . , `}. Since \u03be(0) represents the maximum absolute error inside the training set D(0) = D when the solution of the optimization problem is specified by w(0), we have L0[z,w(0), \u03be(0)] = {1, . . . , `}. So the thesis is proved for \u03c4 = 0.\nNow, we assume that the thesis holds for the (\u03c4 \u2212 1)-th refinement and we prove it for the \u03c4 -th one. As a consequence, we have\nL0[D (\u03c4\u22121),w(\u03c4\u22121), \u03be(\u03c4\u22121)] = {1, . . . , `} (10)\nand we must show that the same relation is valid also for \u03c4. Once computed w\u0303(\u03c4), we do not discard yi from the training data D(\u03c4\u22121) if and only if \u2223\u2223\u2223\u2211u+`j=1 c>K(xi,xj)w\u0303(\u03c4)j \u2212 yi\u2223\u2223\u2223 \u2264 \u03be\u0303(\u03c4). Since the algorithm requires to compute w(\u03c4) by permuting w(\u03c4\u22121) in a way that the elements w(\u03c4\u22121)j with j \u2208 L0[D(\u03c4\u22121), w\u0303(\u03c4), \u03be\u0303(\u03c4)] occupy the first entries, we have\u2223\u2223\u2223\u2211u+`j=1 c>K(xi,xj)w(\u03c4)j \u2212 yi\u2223\u2223\u2223 \u2264 \u03be\u0303(\u03c4) thanks to the assumption (10). Since \u03be(\u03c4) is defined as the maximum of a finite set of elements all bounded by \u03be\u0303(\u03c4), we conclude\n\u03be(\u03c4) = max i=1,...,` \u2223\u2223\u2223\u2223\u2223\u2223 u+\u2211\u0300 j=1 c>K(xi,xj)w (\u03c4) j \u2212 yi \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03be\u0303(\u03c4). (11) From the previous relation and from the definition of the set L0, we obtain L0[D(\u03c4),w(\u03c4), \u03be(\u03c4)] = {1, . . . , `}.\nProposition 2. The sequence \u03be(0), \u03be(1), . . . , \u03be(T ) is monotonically strictly decreasing.\nProof. In formul\u00e6, we want to show that \u03be(0) > \u03be(1) > \u00b7 \u00b7 \u00b7 > \u03be(T ). In order to prove monotonicity, we fix an arbitrary refinement \u03c4 = 1, . . . , T and our goal is to show \u03be(\u03c4) < \u03be(\u03c4\u22121). Directly using (11), we have\n\u03be(\u03c4) \u2264 \u03be\u0303(\u03c4). (12)\nBy definition of \u03be\u0303(\u03c4),\n\u03be\u0303(\u03c4) = \u03be(\u03c4\u22121) \u2212\u2206\u03be, (13)\nand, since \u2206\u03be > 0, then\n\u03be(\u03c4\u22121) \u2212\u2206\u03be < \u03be(\u03c4\u22121). (14)\nCombining the equations (12), (13) and (14) we get\n\u03be(\u03c4) < \u03be(\u03c4\u22121). (15)\nThe thesis follow after the generality of \u03c4 in (15).\nA. Insights about the HLR pseudo-code\nEssentially, Algorithm 1 solves the burden related to the computability of the sets L0, L+ and L\u2212, by adapting the threshold \u03be of the Huber loss. Indeed, w0 is easily computable in the setting \u03be = +\u221e since L0[D,w,+\u221e] = {1, . . . , `} for any w and D and (7) reduces to a ordinary linear system. However, thanks to the definition of \u03be(\u03c4) in (9), we can ensure that w(0) solves the problem (7) for the value \u03be = \u03be(0) of the Huber loss threshold. The refinements for \u03be start with a fixed reduction of \u03be(\u03c4\u22121) by \u2206\u03be > 0, while, once (8) is solved, the final value \u03be(\u03c4) is updated: this is the key passage to ensure that Proposition 1 holds for any \u03c4 . Let us stress again that, for any refinement \u03c4 = 0, . . . , T, the vector w(\u03c4) gives the exact solution (7) for our robust scalar regression framework (3), where the Huber loss threshold \u03be equals to \u03be(\u03c4)."}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 5", "text": "Additionally, HLR is able to automatically select which output variables y1, . . . , y` do not provide sufficient information for learning the regression map. Indeed, at each refinement, HLR scans the labelled training set {(x1, y1), . . . , (x`, y`)} \u2208 D(\u03c4\u22121) and check whether\u2223\u2223\u2223\u2223\u2223\u2223 u+\u2211\u0300 j=1 c>K(xi,xj)w\u0303j (\u03c4) \u2212 yi\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u03be\u0303(\u03c4). (16) Equation (16) means that, for xi, the prediction of HLR is suboptimal, since differing from the actual value yi for more than \u03be\u0303(\u03c4). In such case, the algorithm automatically remove yi from the dataset, assigning xi to be unlabelled in D(\u03c4) and actually trying to recover a better prediction by looking for the output value in the RKHS which is closest to f(xi) in the sense of the norm \u2016 \u00b7 \u2016M . For the sake of clarity, notice that the computational cost does not change when some scalar yi is removed from the training set. Indeed, any unlabelled input xi still counts one equation, as it is when (xi, yi) \u2208 D. We will refer to such automatic balance of labelled data as the active-labelling component of HLR (see Section IV-B).\nFurthermore, thanks to Proposition 2, the algorithms perform an automatic threshold adaptation for \u03be. Indeed, \u03be(0) > \u03be(1) > \u00b7 \u00b7 \u00b7 > \u03be(T ) is a decreasing sequence whose latter element represents the data-driven selection performed by HLR for the optimal Huber loss threshold \u03be, after T refinements. Precisely, according to (9), such optimality is measured in terms of T successive decreasing reductions of the absolute error paid inside the labelled part of our training set.\nFinally, let us conclude with a couple of details. First, the computational cost of HLR is O((T +1)m2(u+`)2). Second, once coefficients w?1 , . . . , w ? u+` are computed by Algorithm\n1, f?(v) = \u2211u+` j=1 c >K(v,xj)w ? j is the predicted output associated to the multi-view test instance v \u2208 X .\nB. Interpreting the active-labelling component\nThe active-labelling component of HLR has a natural interpretation in machine learning. Indeed, consider xi to represent a collection of different types (i.e., views) of features which are computed on the raw input data by enhancing separated discriminative characteristics, while possibly reducing the noise impact. Conversely, all the labels yi are directly obtained from ground truth annotations, which are usually acquired by humans and, consequently, more prone to noise and errors. Also, during the learning phase, semantic ambiguities may result in descriptors which are at the same time dissimilar in terms of corresponding labels and similar while compared in the feature space. Clearly, such condition violates the classical assumption in semi-supervision where the conditional probability distribution p(y|x) smoothly changes over the geodesics induced by pX (x) [3]. Thus, despite classical frameworks leveraging on unlabelled examples can be easily damaged in such condition, differently, HLR shows a unique robustness towards corrupted/misleading annotations, being able to totally and automatically discard them."}, {"heading": "V. EMPIRICAL VALIDATION OF HLR", "text": "This Section presents our empirical analysis of HLR. In Section V-A, we check the effectiveness of the proposed solution for the Huber loss and the adaptation criterion to automatically learn \u03be from data. In Section V-B, the activelabelling component of HLR are benchmarked on curve fitting and learning from noisy labels problems. Section V-C compares HLR with state-of-the-art regression methods on classical machine learning datasets. Finally, in Section V-D, we consider the crowd counting application, comparing our method against the most effective ones in the literature trough several experiments."}, {"heading": "A. Comparison with state-of-the-art convex solver", "text": "As stressed in the Introduction and proved in Section IV, HLR leverages on an exact solution for optimizing the Huber loss, as a diametrical opposed perspective to iterative solving. In order to experimentally check the powerfulness of such aspect, we compare against CVX [14] which is a state-ofthe-art optimization tool for convex problems. Precisely, by either exploiting HLR or CVX, we are able to optimize the same objective functional (3) as to investigate which of the two methods is more efficient in terms of both performance and running time. At the same time, we are able to inspect the automatic pipeline to adapt \u03be in a data-driven fashion while comparing with a classical cross-validation.\nTo these aims, we face a classical linear regression problem ruled by the underlying model y = \u03b2>x, where x \u2208 R10, y \u2208 R and \u03b2 = [1/10, . . . , 1/10]>. We randomly generate n = 50, 100, 500 and 1000 samples x from a uniform distribution over the unit 10-dimensional hypercube [0, 1]\u00d7\u00b7 \u00b7 \u00b7\u00d7 [0, 1]. As a further experiment, we introduce some outliers to the model which becomes y = \u03b2>x + , where the additive noise \u223c N (0, 0.1) is Gaussian distributed. For HLR, T = 1 and \u2206\u03be = 0.1, \u03bb = 10\u22122 and \u03b3 = 10\u22123 are fixed. The performance of CVX and HLR are measured via the reconstruction error between the ground truth values and the predictions. Also, we monitor the computational running time of both.\nThe analysis of Figure 1 yields to the following comments. \u2212 Numerically, our closed form solution shows a comparable performance with respect to classical iterative schemes in terms of reconstruction error. \u2212 For both algorithms, noise addiction does not remarkably\n6 influence the reconstruction error: this is due to the robustness provided by the Huber loss. \u2212 Undoubtedly, the reconstruction error of CVX greatly fluctuates when \u03be varies in {10\u22123, 10\u22122, . . . , 103}.\nIn addition, Table I clarifies how HLR is much more computationally efficient: the runtime3 of HLR is about a few seconds even if n grows while, for CVX, it sharply raises in the cases n = 500 and n = 1000. Also, it is worth nothing that the computational running time for CVX only accounts one optimization for a given \u03be which has to be repeated within the cross validation pipeline, leading to the mean and standard deviation values reported in Table I.\nGlobally, HLR is performing on par with respect to a stateof-the-art convex solver, being much more quicker in the computation, while automatically learning \u03be."}, {"heading": "B. Evaluation of the active-labelling component", "text": "In this Section, we evaluate the robustness provided by the HLR active-labelling component thanks to the usage of the Huber loss. For this purpose we either considered a curve fitting example and we also faced the problem of binary classification in a corrupted data regime.\nCurve fitting. As in Section V-A, starting from the same linear model y = \u03b2>x, we severely corrupted a random percentage of target data points by inverting their sign. It is a quite sensible change since each entry of x is uniformly distributed in [0, 1], being thus non-negative. Consequently, our algorithm should be able to recognize the negative data as clear outlier and automatically remove them from the training set. Such evaluation is performed through Table II where, for different noise rates, we report the reconstruction error while measuring whether the labels removed by HLR actually refers to corrupted inputs. To do the latter, we employ the S\u00f8rensenDice index s [15] to measure the amount of corrupted data effectively removed by the HLR. In formul\u00e6, s = 2|C\u2229R||C|+|R| where the sets C and R collects the corrupted and removed data, respectively: s \u2208 [0, 1] and spans from the worst overlap case (s = 0 since C \u2229 R = \u2205) to the perfect one (s = 1 if C = R).\n3For all experiments, we used MATLAB R2015b on a Intel(R) Xeon(R) CPU X5650 @2.67 GHz \u00d72 cores and 12 GB RAM.\nIn Table II, despite the increasing noise level, the reconstruction error is quite stable and only degrades at the highest noise levels. Additionally, when the noise level has a minor impact (1% and 10%), we get s = 1: the removal process is perfect and exactly all the corrupted labels are effectively removed. When percentages of noise increases (25%, 50%), we still have good overlapping measures. The final drop at 75% is coherent with the huge amount of noise (only 1 target over 4 is not corrupted).\nLearning with noisy labels. We want to benchmark our method in facing noisy annotations during the learning phase. Thus, we compare HLR against several approaches for the same problem in the setting of [16]. Therein, binary classification is performed in the presence of random label noise so that, in training, instead of exploiting the true labels, some of them have been randomly flipped with a given probability. Precisely, following [16], we denote with \u03c1+ the probability that the label of a positive sample is flipped from +1 to \u22121. In a similar manner, \u03c1\u2212 quantifies the negative training instances whose label is wrongly assigned to be \u22121. In [16], such problem is stated under a theoretical perspective, formulating some bounds for the generalization error and the empirical risk, as to guarantee the feasibility of the learning task even in such an extreme situation. Although interesting per se, such arguments are out of the scope of our work, where, instead, we compared with the two methods proposed by [16]: a surrogate logarithmic loss function (\u02dc\u0300log) and a variant of support vector machine algorithm, where the cost parameter is adapted depending on the labels (C-SVM). In [16], \u02dc\u0300log and CSVM were shown to outperform many state-of-the-art methods for the same task: the max margin perceptron algorithm (PAM) [17], Gaussian herding (NHERD) [18] and random projection classifier (RP) [19]. All the aforementioned methods are compared with HLR algorithm actually specialized for this task: as usually done for binary decision boundaries, we exploit the sign of the learnt function f? (see Section IV-A) to perform classification. To ensure a fair comparison, we reproduce the same experimental protocol in [16] by both choosing all the free parameters via a cross validation pipeline and computing the accuracy on the test set with respect to the clean distribution of labels. Also, as in[16], we considered the same datasets, training/testing splits and data normalization4.\nFrom the experimental results reported in Table III, HLR scored a strong performance. Indeed, despite some modest classification results on Thyroid an Diabetes datasets, HLR is able to beat the considered competitors, scoring the best classification accuracy in the remaining 4 out of 6 ones (Breast Cancer, German, Heary and Image). Interestingly, this happens in both low and high noise levels: for instance, when \u03c1+ = \u03c1\u2212 = 0.2 on Breast Cancer and when \u03c1+ = \u03c1\u2212 = 0.4 on Image.\nThe results presented in this Section attest the activelabelling HLR component to be able to effectively detect the presence of outlier data while, at the same time, guaranteeing an effective learning of the regression model.\n4http://theoval.cmp.uea.ac.uk/matlab"}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 7", "text": ""}, {"heading": "C. HLR for classical regression applications", "text": "To compare the effectiveness of HLR in learning the regressor map, in this Section, we benchmark on four datasets from the UCI Machine Learning Repository5, we will focus on house pricing estimation (Boston Housing \u2013 House), physical simulations (AirFoil Self-Noise \u2013 Air and Yatch Hydrodynamics \u2013 Idro) and agronomic quality control (Wine). We will briefly discribe each of them.\nHouse datasets predicts housing values in Boston suburbs. The dataset consists in 506 examples and 13 feature components which are either discrete (average number of rooms), binary (whether or not tracting bounds of Charles river) or continuous (pupil-teacher ratio by town). Air datasets address the problem of physical simulations. It is provided by NASA and shows 1503 aerodynamic and acoustic acquisitions of two and three-dimensional air foil blade sections. A 6-dimensional feature vector encodes different size and thickness for blades, various wind tunnel speeds and angles of attack. The output variable is the sound pressure level measured in decibel. Idro predicts the resistance of sailing yachts at the initial design stage, estimating the required propulsive power. Inputs provides hull dimensions an boat velocity (6 dimensional features, 308 instances). The output variable is the residuary resistance per unitary displacement. Wine dataset consists in 11-dimensional 1599 input instances (we only focused on red wine). The goal is predicting the grades, given by a crew of sommeliers, using pH, alcohol and sulphates concentrations as data.\nOver the aforementioned datasets, we compare Huber loss regression (HLR) against Gaussian process regression (GPR), ridge regression (RR), K nearest neighbours (K-nn), one-\n5 https://archive.ics.uci.edu/ml/datasets\nhidden-layer neural network (NN) and linear support vector machine for regression (SVR). For each method, the parameters setting are obtained after cross validation (see Table IV). For a fair comparison, we split each dataset in five equispaced folds: on each, we used 20% of samples as training set, while test on the remaining ones. To give a comprehensive results on each datasets, we averaged the errors on each fold using one out of those following metrics\nmean absolute error MAE = 1\nn n\u2211 i=1 |yi \u2212 y\u0302i|, (17)\nmean squared error MSE = 1\nn n\u2211 i=1 (yi \u2212 y\u0302i)2, (18)\nmean relative error MRE = 1\nn n\u2211 i=1 |yi \u2212 y\u0302i| yi , (19)\nwhere y1, . . . , yn are the (true) testing target variables and y\u03021, . . . , y\u0302n the corresponding prediction.\nQualitative and quantitative analysis has been reported in Table V and Figure 2, respectively. Globally, HLR shows remarkable performances since outstanding the other methods in 5 cases out of 12. Those performances are also remarkable since they have been obtained with a fixed set of parameters, confirming the ductility of HLR (see Table IV). Indeed, despite GPR and RR scored comparable performance with respect to HLR, the regularizing parameter of RR has to be tuned and the parameters of the GPR has to be learnt in a maximum likelihood sense (mean function plus covariance kernel).\nFrom this analysis, the low errors scored by of HLR a joint manner with the fixed configuration of parameters we adopted, maker HLR outperforming many state-of-the-art approaches for scalar regression tasks.\n8 Method House Air Idro Wine GPR Affine mean, mixture covariance type (linear + squared exponential)\nRR \u03b1 = 0.3 \u03b1 = 0.01 \u03b1 = 0.5 \u03b1 = 1 K-nn K = 3 K = 3 K = 3 K = 5\nNN nh = 3 nh = 5 nh = 7 nh = 6 SVR = 0.003, C = 10 = 0.005, C = 10 \u03bd = 0.003, C = 1 \u03bd = 0.01, C = 10 HLR \u03bb = 0.001, \u03b3 = 0.0001,\u2206\u03be = 0.01, T = 3\nTABLE IV IN ADDITION TO THE PARAMETERS \u03bb, \u03b3,D\u03be AND T OF HLR, WE REPORT THE PARAMETERS/SETTINGS OF OTHER METHODS BENCHMARKED ON THE UCI MACHINE LEARNING REPOSITORY EXPERIMENTS: THE MEAN AND COVARIANCE FUNCTIONS USED FOR GPR, THE REGULARIZING PARAMETER \u03b1 FOR RR, THE VALUE OF K NEIGHBOURS CONSIDERED, THE NUMBER OF NEURONS nh IN THE HIDDEN LAYER FOR NN AND THE /\u03bd CHOICES FOR SVR AS WELL AS THE COST FUNCTION C USED.\nHouse Air Idro Wine Methods MAE MSE MRE MAE MSE MRE MAE MSE MAE MSE MRE\nGPR 4.21(3) 41.00(3) 0.20(3) 4.47(2) 33.84(2) 0.03(1) 7.10(2) 118.3(3) 0.59(1) 0.72(1) 0.107(2) RR 3.79(1) 28.73(1) 16.03(2) 4.76(3) 37.61(3) 3.87(3) 7.28(3) 113.9(2) 0.59(1) 0.72(1) 0.106(1) K-nn 5.91(6) 64.96(6) 22.64(6) 6.01(5) 65.57(6) 4.89(5) 9.08(6) 267.0(6) 0.61(5) 0.78(5) 0.108(4) NN 5.49(5) 56.97(5) 20.94(5) 6.56(6) 64.69(5) 5.32(6) 8.32(5) 183.2(5) 0.86(6) 1.35(6) 0.161(6)\nSVR 4.88(4) 51.55(4) 20.72(4) 4.93(4) 38.64(4) 3.99(4) 7.41(4) 143.8(4) 0.59(1) 0.73(3) 0.109(5) HLR 4.13(2) 36.78(2) 0.15(1) 4.16(1) 30.20(1) 0.04(2) 6.91(1) 110.8(1) 0.61(4) 0.77(4) 0.107(2)\nTABLE V COMPARISON OF HLR AGAINST GAUSSIAN PROCESS REGRESSION, RIDGE REGRESSION, K NEAREST NEIGHBORS, NEURAL NETS AND SUPPORT VECTOR MACHINE FOR REGRESSION. IN BOLD, TOP THREE PERFORMING METHODS. IN BRACKETS, THE RELATIVE RANKING. FOR Idro, SINCE THE TARGET VARIABLE IS SOMETIMES (CLOSE TO) ZERO, MRE METRICS DIVERGE AND WAS THEREFORE NOT REPORTED."}, {"heading": "D. HLR for crowd counting application", "text": "As a final test bed of our proposed framework, we address the crowd counting application. It consists in estimating the number of people in a real world environment using video data. Crowd counting can be rephrased in learning a regression map from frame-specific features to the amount of people whereby [20]. Three benchmark datasets have been used to test the performances of our Huber loss regression method. They are MALL [21], UCSD [22] and PETS 2009 [23].\nMALL \u2013 From a surveillance camera in a shopping centre, 2000 RGB images were extracted (resolution 320 \u00d7 240). In each image, crowd density varies from 13 to 53. The main challenges are related to shadows and reflections. Following the literature [21], our system is trained with the first 800 frames, and the remaining ones are left for testing.\nUCSD \u2013 A hand-held camera recorded a campus outdoor scene composed by 2000 gray-scale 238-by-158 frames. The\ndensity grows from 11 to 46. Environment changes are less severe, while geometric distorsion is sometimes a burden. As usually done [22], we used the frames 601\u00f71400 for training.\nPETS 2009 \u2013 Within the Eleventh Performance Evaluation of Tracking and Surveillance workshop, a new dataset has been recorded from a British campus. Crowd counting experiments are carried out on sequences 13-57,13-59,14-03,14- 06 from camera 1 [23], and three regions of interests have"}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 9", "text": "been introduced (R0, R1 and R2 in Fig. 3). Crowd density ranges between 0 and 42, shadows and the appearance of both walking and running people are the main challenges. We adopted the training/testing split of [23] (see Table VI).\nIn addition to replicating training/testing split, for a fair comparison, we employed publicly available6 ground truth annotations and pre-computed features: precisely, we employed size, edges and texture features [24]. Size descriptors refer to the magnitude of any interesting segments or patches from an image which are deemed to be relevant (e.g., the foreground pixels [25]). Edges pertain to the relative changes in graylevel values and binary detectors (Canny algorithm [26]) are used for extraction. The texture class includes several statistics, like energy or entropy, which are computed from gray-level co-occurrence matrix [27] or local binary pattern [20]. Before extracting these descriptors, a region of interest is detected, perspective is normalized as in [28] and, sometimes, an intermediate motion segmentation phase is performed (hence, crowd can be subdivided according to motion directions).\nIn our framework, we set m = 3 and each category of features is thus encoded with a separate kernel (we adopt either quadratic-polynomial or linear ones). We fix c = [1/3, 1/3, 1/3]> and M\u03b1 is the sum of between-view operator from [10] and normalized graph laplacian L\u03b1 related to the \u03b1-th view. The model parameters (refinement number T, the regularizing parameters \u03bb, \u03b3, and \u2206\u03be) are chosen via cross validation on the training set (see Table VII). For instance, results from Table XI all comes from this default setting.\nFigures 4 and 5 show the qualitative results, overlapping the profile of the ground truth crowd density with our predicted evaluations for all the considered experiments: in all cases the ground truth crowd size is estimated in a satisfactory way. Quantitative performance evaluation usually uses the three\n6http://personal.ie.cuhk.edu.hk/\u223cccloy/downloads mall dataset.html for MALL; http://visal.cs.cityu.edu.hk/downloads/ for UCSD and PETS 2009.\nerror measures MAE, MSE and MRE already defined in Section V-C.\nFollowing the protocol of [24], in Table VIII we integrate Huber Loss Regression (HLR) in the comparison between Gaussian Process Regression (GPR), regularized linear regression (Lin), K-nearest neighbours (K-nn) with K = 4 and neural networks (NN) methods with a unique hidden layer composed by 8 artificial neurons. Using same training/testing splits as in [24], we scored top three error on MALL, while we set state-of-the art in UCSD considerably reducing MRE.\nWe also tested UCSD and MALL datasets in the same conditions as [21] and [31] (see Table X). Therein, the methods adopted in the comparison are least square support vector regression LSSVR [32], kernel ridge regression KRR [33], random forest regression RFR [34], Gaussian process regression GPR [22], ridge regression RR [35] with its cumulative attribute CA-RR [31] variant. Additionally, we also compare with multiple localised regression MLR [36] and multiple output regression MORR [21], a class of local approaches for crowd counting which rely on a preliminary fine tessellation of the video frames. Also in this demanding comparison HLR is able to set the lowest and second lowest MAE,MSE and MRE in all of the comparisons, respectively.\nAdditionally, we benchmarked HLR with other methods which leverage on semi-supervision. Precisely we benchmarked against the baseline one-viewed manifold regularization (MR) [3] while also considering semi-supervisedregression (SSR) [29] and elastic net (EN) [30]. For the former, SSR optimize a similar functional as (3) where the quadratic loss is used in a multi-view setting (m = 2) as to impose a spatial regularization within-frames and a temporal regularization across consecutive frames. Finally, EN [30] implements a sparsity principle while adopting a L1-based semi-supervised variation of Lasso. In Table IX we report the MSE quantitative results where, HLR is able to outperform\n10\nother semi-supervised methods. Moving to PETS 2009, we reproduce the same experimental conditions as in [23]. Motion segmentation allows to divide the right-moving pedestrians from the others moving in the opposite direction. Total crowd density has been obtained summing the partial results. Table XI shows a comparison of Huber loss vs. Gaussian Process Regression (GPR) in this setting reported in Table VI. Performances are sometimes substantially improved, see sequence 13\u2212 57, regions R1 and R2. Again, HLR scored a sound performance, setting in 46 cases out of 54 the lowest MAE or MSE error metric.\nDiscussion. \u2022 While comparing with other semi-supervised methods (Table IX), multi-view learning and manifold regularization are effectively combined by HLR in a framework where the final boost of accuracy is ensured by the usage of the Huber loss which attested to be superior to both the quadratic (MR and SSR) and the L1 losses (EN). \u2022 The active-labelling is able to proficiently rule the amount of supervision. Indeed, on UCSD only 1% of the labels is not exploited by HLR: evidently, the preprocessing step perspective correction [22] is enough effective to make almost all the data exploitable in a supervised fashion. Differently, on MALL 11% of labelled instances are discarded: this happens when some pedestrians are partially occluded by some static elements of the scene and, sometimes, there are some sitting people whose appearance greatly differs from the walking ones. Finally, on PETS 2009, HLR outperforms GPR even if using, on average, about 100 less annotations: this is due the visual ambiguities generated by the cross-setting where training and testing scenes are different. \u2022 Similarly to Section V-C, HLR performance does not requires any burdensome parameter tuning (Table VII): a good rule-of-the-thumb is T = 3, \u03bb = 10\u22124 and \u03b3 = 10\u22125; \u2206\u03be = 0.01: in general, only minor corrections are required. \u2022 In terms of time complexity, the HLR is a fast method: indeed, in the setup of Table X, training and testing on MALL last 6.50 and 0.36 seconds respectively. Similarly, on UCSD, training requires 5.6 and testing 0.5 seconds. \u2022 In synthesis, the crowd counting application showed that HLR is able to fully take advantage of the most effective techniques in semi-supervision and to improve state-of-the-art methods for crowd counting, while being robust to noisy annotations, ensuring a fast computation and skipping annoying parameter tuning processes."}, {"heading": "VI. CONCLUSION, LIMITATIONS & FUTURE WORK", "text": "In this paper we deduce a solution to exactly minimize the Huber loss in a general optimization framework which unified the most effective approaches in semi-supervision (multi-view learning and manifold regularization). Differently from previous approaches [8], [6], [7], [9], leveraging on our solutions, the proposed HLR algorithm 1) avoids burdensome iterative solving and, at the same time, 2) automatically learns the threshold \u03be and 3) actively selects the most beneficial annotations for the learning phase.\nSuch unique aspects resulted in a remarkable performance on different tasks where HLR scored always on par and often\nsuperior to state-of-the-art algorithms for learning from noisy annotations, classical regression problems and crowd counting application. Moreover, low errors were registered by HLR at low computational cost which, guaranteeing fast computation without any expensive parameter tuning.\nFuture works will essentially focus on specializing the framework for classification tasks. Also, to face the main drawback of the method, which consists in the not-learnable weights c, the connections with M-Estimators theory, Multiple Kernel Learning and Radial Basis Function Networks could be investigated."}, {"heading": "JACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 11", "text": "[23] A. Chan, M. Morrow, and N. Vasconcelos, \u201cAnalysis of crowded scenes using holistic properties,\u201d in Workshop on Performance Evaluation of Tracking and Surveillance, CVPR, 2009. [24] D. Ryan, S. Denman, S. Sridharan, and C. Fookes, \u201cAn evaluation of crowd counting methods, features and regression models,\u201d Computer Vision and Image Understanding, vol. 130, pp. 1\u201317, 2015. [25] A. C. Davies, J. H. Yin, and S. A. Velastin, \u201cCrowd monitoring using image processing,\u201d Electron Commun Eng, vol. 7, pp. 37\u201347, 1995. [26] J. Canny, \u201cA computational approach to edge detection,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 8, no. 6, pp. 679\u2013698, 1986. [27] R. Haralick, K. Shanmugam, and I. Dinstein, \u201cTextural features for image classification,\u201d Trans SMC, vol. 3, no. 6, pp. 610\u2013621, 1973. [28] R. Ma, L. Li, W. Huang, and Q. Tian, \u201cOn pixel count based crowd density estimation for visual surveillance,\u201d in IEEE Conference on CIS, 2004. [29] C. C. Loy, S. Gong, and T. Xiang, \u201cFrom semi-supervised to transfer counting of crowds,\u201d in ICCV, 2013. [30] B. Tan, J. Zhang, and L. Wang, \u201cSemi-supervised elastic net for pedestrian counting,\u201d Pattern Recognition, vol. 44, pp. 2297\u20132304, 2011. [31] K. Chen, S. Gong, T. Xiang, and C. Change Loy, \u201cCumulative attribute space for age and crowd density estimation,\u201d in CVPR, 2013. [32] T. V. Gestel, J. A. K. Suykens, B. D. Moor, and J. Vandewalle, \u201cAutomatic relevance determination for least squares support vector machines classifiers.\u201d in ESANN, 2001. [33] S. An, W. Liu, and S. Venkatesh, \u201cFace recognition using kernel ridge regression.\u201d in CVPR, 2007. [34] A. Liaw and M. Wiener, \u201cClassification and regression by randomforest,\u201d R Journal, vol. 2, no. 3, pp. 18\u201322, 2002. [35] C. Saunders, A. Gammerman, and V. Vovk, \u201cRidge regression learning algorithm in dual variables,\u201d in ICML, 1998. [36] X. Wu, G. Liang, K. K. Lee, and Y. Xu, \u201cCrowd density estimation using texture analysis and learning,\u201d in ROBIO, 2006."}, {"heading": "APPENDIX", "text": "In this Section we report the proof of Theorem 1 while applying the Representer Theorem [4] to cast the optimization\n12\nproblem into a minimizing on the coefficients w defining f? in (6). Precisely, implementing it in (3) yields\nJ\u03bb,\u03b3(w) = 1\n` \u2211\u0300 i=1 H\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj + + \u03bb\nu+\u2211\u0300 j,k=1 w>j K(xj ,xk)wk+ (20)\n+ \u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 h,k=1 w>hK(xh,xi)MijK(xj ,xk)wj .\nTheoretically, minimizing J\u03bb,\u03b3 over the RKHS SK is fully equivalent to minimizing J\u03bb,\u03b3 with respect to w, being the latter approach computationally convenient because, for this purpose, the optimization domain Rm(u+`) is preferable to an infinite-dimensional functional space. Notice that each addend of J\u03bb,\u03b3 is differentiable with respect to w. Indeed, one has\nH \u2032\u03be(y) =  \u2212\u03be if y \u2264 \u2212\u03be y if |y| \u2264 \u03be +\u03be if y \u2265 \u03be,\n(21)\nand \u2016f\u20162K and \u2016f\u20162M in (20) are also differentiable since polynomials in w1, . . . , wu+`. Thus, for any p = 1, . . . , u+ ` and \u03b7 = 1, . . . ,m, we compute \u2202J\u03bb,\u03b3 \u2202w\u03b7p , differentiating with respect to w\u03b7p , the \u03b7-th view of wp. Then,\n\u2202J\u03bb,\u03b3 \u2202w\u03b7p = \u22121 ` \u2211\u0300 i=1 H \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  \u00b7 \u00b7 u+\u2211\u0300 h=1 m\u2211 \u03b1=1 c\u03b1\u03ba\u03b1(x\u03b1i , x \u03b1 h) \u2202w\u03b1h \u2202w\u03b7p +\n\u03bb u+\u2211\u0300 j,k=1 m\u2211 \u03b1=1 \u2202w\u03b1j \u2202w\u03b7p \u03ba\u03b1(x\u03b1j , x \u03b1 k )w \u03b1 k+\n\u03bb u+\u2211\u0300 j,k=1 m\u2211 \u03b1=1 w\u03b1j \u03ba \u03b1(x\u03b1j , x \u03b1 k ) \u2202w\u03b1k \u2202w\u03b7p +\n\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 h,k=1 m\u2211 \u03b1=1 \u2202w\u03b1h \u2202w\u03b7p \u03ba\u03b1(x\u03b1i , x \u03b1 h)M \u03b1 ij\u03ba \u03b1(x\u03b1j , x \u03b1 k )w \u03b1 k+\n\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 h,k=1 m\u2211 \u03b1=1 \u03ba\u03b1(x\u03b1i , x \u03b1 h)w \u03b1 hM \u03b1 ij\u03ba \u03b1(x\u03b1j , x \u03b1 k ) \u2202w\u03b1k \u2202w\u03b7p . (22)\nIn order to simplify (22), we can apply the relation-\nship \u2202w\u03b1i \u2202w\u03b7j = \u03b4\u03b1\u03b7\u03b4ij , valid for any \u03b1, \u03b2 = 1, . . . ,m and i, j = 1, . . . , u + `, where, for any integers m,n, \u03b4mn is the Kronecker delta and \u03b4mn = 1 if m = n, while, otherwise, \u03b4mn = 0 if m 6= n. Thus,\n\u2202J\u03bb,\u03b3 \u2202w\u03b7p = \u22121 ` \u2211\u0300 i=1 H \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  \u00b7 \u00b7 u+\u2211\u0300 h=1 m\u2211 \u03b1=1 c\u03b1\u03ba\u03b1(x\u03b1i , x \u03b1 h)\u03b4\u03b1\u03b7\u03b4hp+\n\u03bb u+\u2211\u0300 j,k=1 m\u2211 \u03b1=1 \u03b4\u03b1\u03b7\u03b4jp\u03ba \u03b1(x\u03b1j , x \u03b1 k )w \u03b1 k+\n\u03bb u+\u2211\u0300 j,k=1 m\u2211 \u03b1=1 w\u03b1j \u03ba \u03b1(x\u03b1j , x \u03b1 k )\u03b4\u03b1\u03b7\u03b4kp+\n\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 h,k=1 m\u2211 \u03b1=1 \u03b4\u03b1\u03b7\u03b4hp\u03ba \u03b1(x\u03b1i , x \u03b1 h)M \u03b1 ij\u03ba \u03b1(x\u03b1j , x \u03b1 k )w \u03b1 k+\n\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 h,k=1 m\u2211 \u03b1=1 \u03ba\u03b1(x\u03b1i , x \u03b1 h)w \u03b1 hM \u03b1 ij\u03ba \u03b1(x\u03b1j , x \u03b1 k )\u03b4\u03b1\u03b7\u03b4kp.\n(23)\nBy exploiting the properties of Kronecker delta, inside a summation over the index i, \u03b4ij discards all the addends except to j. Then, we can rewrite equation (23) obtaining\n\u2202J\u03bb,\u03b3 \u2202w\u03b7p = \u22121 ` \u2211\u0300 i=1 H \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  \u00b7 \u00b7c\u03b7\u03ba\u03b7(x\u03b7i , x \u03b7 p)+\n\u03bb u+\u2211\u0300 k=1 \u03ba\u03b7(x\u03b7p, x \u03b7 k)w \u03b7 k + \u03bb u+\u2211\u0300 j=1 w\u03b7j \u03ba \u03b7(x\u03b7j , x \u03b7 p)+\n\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 k=1 \u03ba\u03b7(x\u03b7i , x \u03b7 p)M \u03b7 ij\u03ba \u03b7(x\u03b7j , x \u03b7 k)w \u03b7 k+\n\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 h=1 \u03ba\u03b7(x\u03b7i , x \u03b7 h)w \u03b7 hM \u03b7 ij\u03ba \u03b7(x\u03b7j , x \u03b7 p). (24)\nTo rearrange equation (24), we can exploit the functional symmetry of both Mercer kernels \u03ba1, . . . , \u03bam and linear operators M1, . . . ,Mm. Then, one sees\n\u2202J\u03bb,\u03b3 \u2202w\u03b7p = \u22121 ` \u2211\u0300 i=1 H \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  \u00b7 \u00b7c\u03b7\u03ba\u03b7(x\u03b1i , x\u03b1p )+\n2\u03bb u+\u2211\u0300 k=1 w\u03b7k\u03ba \u03b7(x\u03b1p , x \u03b1 k )+\n2\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 k=1 \u03ba\u03b7(x\u03b1i , x \u03b1 p )M \u03b7 ij\u03ba \u03b7(x\u03b1j , x \u03b1 k )w \u03b7 k . (25)\nAfter vectorizing with respect to \u03b7 = 1, . . . ,m, the deriva-\nJACOPO CAVAZZA AND VITTORIO MURINO: ACTIVE-LABELLING BY ADAPTIVE HUBER LOSS REGRESSION 13\ntive \u2202J\u03bb,\u03b3 \u2202wp equals to\n\u22121 ` \u2211\u0300 i=1 H \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj K(xp,xi)c+ 2\u03bb\nu+\u2211\u0300 k=1 K(xp,xk)\u03c9k+\n2\u03b3 u+\u2211\u0300 i,j=1 u+\u2211\u0300 k=1 K(xp,xi)MijK(xj ,xk)wk. (26)\nExpression (26) rewrites \u2211u+` i=1 K(xp,xi)\u03c8i, once defined, for any i = 1, . . . , u+ `,\n\u03c8i = \u22121(i \u2264 `) 1\n` H \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  c+ 2\u03bb\u03c9i + 2\u03b3\nu+\u2211\u0300 j,h=1 MijK(xj ,xh)wh, (27)\nwhere the indicator function 1 is conditionally defined to be 1(i \u2264 `) = 1 if i \u2264 ` and 1(i \u2264 `) = 0 if i > `.\nIf we set \u03c81 = \u00b7 \u00b7 \u00b7 = \u03c8u+` = 0, then, from equation (25), \u2202J\u03bb,\u03b3 \u2202w1 = \u00b7 \u00b7 \u00b7 = \u2202J\u03bb,\u03b3 \u2202wu+` = 0 and this leads to a solution of (3). But, this is the only solution we have since, as motivated in the paper, the optimization problem (3) has unique solution thanks to Representer Theorem [4]. Then, the previous discussion ensures that, globally, the two systems of equations are totally equivalent since, for every i = 1, . . . , u+ `,\n\u03c8i = 0 if and only if \u2202J\u03bb,\u03b3 \u2202wi = 0. (28)\nHence, the optimization of (3) can be done by solving\n2`\u03bbwi + 2`\u03b3 u+\u2211\u0300 j,h=1 MijK(xj ,xh)wh =\nH \u2032\u03be yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  c (29) for i = 1, . . . , `; and, when i = `+ 1, . . . , u+ `,\n2\u03bbwi + 2\u03b3 u+\u2211\u0300 j,h=1 MijK(xj ,xh)wh = 0. (30)\nSubstitute equation (21) into (29). Then, for i = 1, . . . , `,\n2`\u03bbwi + 2`\u03b3 u+\u2211\u0300 j,h=1\nMijK(xj ,xh)wh = \u2212\u03bec if yi \u2212 \u2211u+` j=1 c >K(xi,xj)wj \u2264 \u2212\u03beyi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  c if \u2223\u2223\u2223yi \u2212\u2211u+`j=1 c>K(xi,xj)wj\u2223\u2223\u2223 \u2264 \u03be +\u03bec\nif yi \u2212 \u2211u+` j=1 c >K(xi,xj)wj \u2265 \u03be.\n(31)\nIf one defines the following set of indices\nL+[D,w, \u03be] = i \u2264 ` : u+\u2211\u0300 j=1 c>K(xi,xj)wj \u2265 yi + \u03be , L0[D,w, \u03be] =\ni \u2264 ` : \u2223\u2223\u2223\u2223\u2223\u2223 u+\u2211\u0300 j=1 c>K(xi,xj)wj \u2212 yi \u2223\u2223\u2223\u2223\u2223\u2223 < \u03be ,\nL\u2212[D,w, \u03be] = i \u2264 ` : u+\u2211\u0300 j=1 c>K(xi,xj)wj \u2264 yi \u2212 \u03be , equation (31) therefore becomes\n2`\u03bbwi + 2`\u03b3 u+\u2211\u0300 j,h=1\nMijK(xi,xh)wh = \u2212\u03bec if i \u2208 L+[D,w, \u03be]yi \u2212 u+\u2211\u0300 j=1 c>K(xi,xj)wj  c if i \u2208 L0[D,w, \u03be] +\u03bec if i \u2208 L\u2212[D,w, \u03be]. (32)\nThe thesis follows as the straightforward combination of equations (32) and (30).\n14"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper addresses the scalar regression problem<lb>presenting a solution for optimizing the Huber loss in a general<lb>semi-supervised setting, which combines multi-view learning and<lb>manifold regularization. To this aim, we propose a principled<lb>algorithm to 1) avoid computationally expensive iterative solu-<lb>tions while 2) adapting the Huber loss threshold in a data-driven<lb>fashion and 3) actively balancing the use of labelled data to<lb>remove noisy or inconsistent annotations from the training stage.<lb>In a wide experimental evaluation, dealing with diverse applica-<lb>tions, we assess the superiority of our paradigm which is able<lb>to combine strong performance and robustness to noise at a low<lb>computational cost.", "creator": "LaTeX with hyperref package"}}}