{"id": "1606.02275", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Measuring the reliability of MCMC inference with bidirectional Monte Carlo", "abstract": "The Markov chain Monte Carlo (MCMC) is one of the most important workhorses of probabilistic inference, but it is notoriously difficult to measure the quality of approximate posterior samples. This challenge is particularly evident with black box inference methods that can hide details and obscure inference errors. In this work, we extend the recently introduced Monte Carlo bidirectional technique for evaluating MCMC-based posterior inference algorithms. By executing annealed chains of meaning (AIS) both before and after posterior and vice versa on simulated data, we limit the symmetrized KL divergence between true posterior distribution and the distribution of approximate samples in anticipation to the top. We present Bounding Divergences with REverse Annealing (BREAD), a protocol for validating the relevance of simulated data experiments to real data, and integrate it into two probable web modules to examine the probability of using PL as an example of web language and PL.", "histories": [["v1", "Tue, 7 Jun 2016 19:39:02 GMT  (380kb,D)", "http://arxiv.org/abs/1606.02275v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.CO stat.ML", "authors": ["roger b grosse", "siddharth ancha", "daniel m roy"], "accepted": true, "id": "1606.02275"}, "pdf": {"name": "1606.02275.pdf", "metadata": {"source": "CRF", "title": "Measuring the reliability of MCMC inference with bidirectional Monte Carlo", "authors": ["Roger B. Grosse", "Siddharth Ancha", "Daniel M. Roy"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Markov chain Monte Carlo (MCMC) is one of the most important classes of probabilistic inference methods and underlies a variety of approaches to automatic inference [e.g. LTBS00; GMRB+08; GS; CGHL+ p]. Despite its widespread use, it is still difficult to rigorously validate the effectiveness of an MCMC inference algorithm. There are various heuristics for diagnosing convergence (see Section 4), but reliable quantitative measures are hard to find. This creates difficulties both for end users of automatic inference systems and for experienced researchers who develop models and algorithms.\nFirst, consider the perspective of the end user of an MCMC-based automatic inference system. The user would like to know whether the approximate samples are a good representation of the posterior distribution. She may wish to configure various algorithmic parameters (e.g. the number of steps for which to run the algorithm), or to choose a problem representation for which inference is effective. Currently, all these choices are hard to make in a systematic way, and standard convergence diagnostics often require significant expertise to interpret.\nNow consider the perspective of a scientist aiming to invent a better MCMC algorithm. It is difficult to measure convergence directly, so instead algorithms are typically compared using a proxy measure, such as the joint likelihood of a state or the probability of held-out observations conditioned on the current state. It would be much more useful to have a single scalar quantity which directly measures the quality of the approximate posterior.\nIn this paper, we extend the recently proposed bidirectional Monte Carlo (BDMC) [GGA15] method to evaluate certain kinds of MCMC-based inference algorithms by bounding the symmetrized KL divergence (Jeffreys divergence) of approximate samples from the true posterior distribution. Specifically, our method is applicable to algorithms which can be viewed as importance sampling over\nar X\niv :1\n60 6.\n02 27\n5v 1\n[ cs\n.L G\n] 7\nJ un\n2 01\nan extended state space, such as annealed importance sampling (AIS; [Nea01]) or sequential Monte Carlo (SMC; [MDJ06]). BDMC was proposed as a method for accurately evaluating log marginal likelihoods on simulated data, so that these log-likelihood values can be used to benchmark marginal likelihood estimators. We show that it can also be used to measure the accuracy of approximate posterior samples obtained from algorithms like AIS or SMC. More precisely, we refine the analysis of [GGA15] to derive an estimator which upper bounds in expectation the Jeffreys divergence between the distribution of approximate samples and the true posterior distribution. We show that this upper bound is quite accurate on some toy distributions for which both the true Jeffreys divergence and the upper bound can be computed exactly.\nWhile our method is only directly applicable to certain algorithms such as AIS or SMC, these algorithms involve many of the same design choices as traditional MCMC methods, such as the choice of model representation (e.g. whether to collapse out certain variables), or the choice of MCMC transition operators. Therefore, the ability to evaluate AIS-based inference should also yield insights which inform the design of MCMC inference algorithms more broadly.\nOne additional hurdle must be overcome to use BDMC to evaluate posterior inference: the method yields rigorous bounds only for simulated data because it requires an exact posterior sample. One would like to be sure that the results on simulated data accurately reflect the accuracy of posterior inference on the real-world data of interest. We present a protocol, which we term Bounding Divergences with REverse Annealing (BREAD), for using BDMC to diagnose inference quality on real-world data. Specifically, we infer hyperparameters on the real data, simulate data from those hyperparameters, measure inference quality on the simulated data, and validate the consistency of the inference algorithm\u2019s behavior between the real and simulated data. (This protocol is somewhat similar in spirit to the parametric bootstrap [ET98].)\nWe integrate BREAD into the tool chains of two probabilistic programming languages: WebPPL [GS] and Stan [CGHL+ p]. Both probabilistic programming systems can be used as automatic inference software packages, where the user provides a program specifying a joint probabilistic model over observed and unobserved quantities. In principle, probabilistic programming has the potential to put the power of sophisticated probabilistic modeling and efficient statistical inference into the hands of non-experts, but realizing this vision is challenging because it is difficult for a non-expert user to judge the reliability of results produced by black-box inference. We believe BREAD provides a rigorous, general, and automatic procedure for monitoring the quality of posterior inference, so that the user of a probabilistic programming language can have confidence in the accuracy of the results. Our approach to evaluating probabilistic programming inference is closely related to independent work [CTM16] that is also based on the ideas of BDMC. We discuss the relationships between both methods in Section 4.\nIn summary, this work includes four main technical contributions. First, we modify both WebPPL and Stan to implement BDMC. Second, we show that BDMC yields an estimator which upper bounds in expectation the Jeffreys divergence of approximate samples from the true posterior. Third, we present a technique for exactly computing both the true Jeffreys divergence and the upper bound on small examples, and show that the upper bound is often a good match in practice. Finally, we propose the BREAD protocol for using BDMC to evaluate the accuracy of approximate inference on real-world datasets. We validate BREAD on a variety of probabilistic models in both WebPPL and Stan. As an example of how BREAD can be used to guide modeling and algorithmic decisions, we use it to analyze the effectiveness of different representations of a matrix factorization model in both WebPPL and Stan."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 WebPPL and Stan", "text": "We focus on two particular probabilistic programming packages. First, we consider WebPPL [GS], a lightweight probabilistic programming language built on Javascript, and intended largely to illustrate some of the important ideas in probabilistic programming. Inference is based on Metropolis\u2013Hastings (M\u2013H) updates to a program\u2019s execution trace, i.e. a record of all stochastic decisions made by the program. WebPPL has a small and clean implementation, and the entire implementation is described in an online tutorial on probabilistic programming [GS].\nSecond, we consider Stan [CGHL+ p], a highly engineered automatic inference system which is widely used by statisticians and is intended to scale to large problems. Stan is based on the No U-Turn Sampler (NUTS; [HG14]), a variant of Hamiltonian Monte Carlo (HMC; [Nea+11]) which chooses trajectory lengths adaptively. HMC can be significantly more efficient than M\u2013H over execution traces because it uses gradient information to simultaneously update multiple parameters of a model, but is less general because it requires a differentiable likelihood. (In particular, this disallows discrete latent variables unless they are marginalized out analytically.)"}, {"heading": "2.2 Annealed Importance Sampling", "text": "Annealed importance sampling (AIS; [Nea01]) is a Monte Carlo algorithm commonly used to estimate (ratios of) normalizing constants. More carefully, fix a sequence of T distributions p1, . . . , pT , with pt(x) = ft(x)/Zt. The final distribution in the sequence, pT , is called the target distribution; the first distribution, p1, is called the initial distribution. It is required that one can obtain one or more exact samples from p1.1 Given a sequence of reversible MCMC transition operators T1, . . . , TT , where Tt leaves pt invariant, AIS produces a (nonnegative) unbiased estimate of ZT /Z1 as follows: first, we sample a random initial state x1 from p1 and set the initial weight w1 = 1. For every stage t \u2265 2 we update the weight w and sample the state xt according to\nwt \u2190 wt\u22121 ft(xt\u22121)\nft\u22121(xt\u22121) xt \u2190 sample from Tt (x |xt\u22121) . (1)\nNeal [Nea01] justified AIS by showing that it is a simple importance sampler over an extended state space (see Appendix A for a derivation in our notation). From this analysis, it follows that, after every stage t, the weight wt is an unbiased estimate of the ratio Zt/Z1. Two trivial facts are worth highlighting: when Z1 is known, Z1wt is an unbiased estimate of Zt, and when Zt is known, wt/Zt is an unbiased estimate of 1/Z1. In practice, it is common to repeat the AIS procedure to produce K independent estimates and combine these by simple averaging to reduce the variance of the overall estimate. See Algorithm 1 for the complete procedure.\nIn most applications of AIS, the normalization constant ZT for the target distribution pT is the focus of attention, and the initial distribution p1 is chosen to have a known normalization constant Z1. Any sequence of intermediate distributions satisfying a mild domination criterion suffices to produce a valid estimate, but in typical applications, the intermediate distributions are simply defined to be geometric averages ft(x) = f1(x)1\u2212\u03b2tfT (x)\u03b2t , where the \u03b2t are monotonically increasing parameters with \u03b21 = 0 and \u03b2T = 1. (An alternative approach is to average moments [GMS13].)\nIn the setting of Bayesian posterior inference over parameters \u03b8 and latent variables z given some fixed observation y, we take f1(\u03b8, z) = p(\u03b8, z) to be the prior distribution (hence Z1 = 1), and we take fT (\u03b8, z) = p(\u03b8, z,y) = p(\u03b8, z) p(y |\u03b8, z). This can be viewed as the unnormalized posterior distribution, whose normalizing constant ZT = p(y) is the marginal likelihood. Using geometric\n1Traditionally, this has meant having access to an exact sampler. However, in this work, we sometimes have access to a sample from p1, but not a sampler.\nAlgorithm 1 Annealed Importance Sampling (AIS) Input: unnormalized densities f1, . . . , fT\nreversible MCMC transition operators T1, . . . , TT , where Tt leaves pt = ft/Zt invariant K samples from p1 = f1/Z1\nOutput: unbiased estimate of ZT /Z1 for k = 1 to K do\nx1 \u2190 sample from p1(x) w(k) \u2190 1 for t = 2 to T do w(k) \u2190 w(k) ft(xt\u22121)ft\u22121(xt\u22121) xt \u2190 sample from Tt (x |xt\u22121)\nreturn R\u0302 = 1K \u2211K k=1 w (k)\naveraging, the intermediate distributions are then\nft(\u03b8, z) = p(\u03b8, z) p(y |\u03b8, z)\u03b2t . (2)\nIn addition to moment averaging, reasonable intermediate distributions can be produced in the Bayesian inference setting by conditioning on a sequence of increasing subsets of data; this insight relates AIS to the seemingly different class of sequential Monte Carlo (SMC) methods [MDJ06]."}, {"heading": "2.3 Stochastic lower bounds on the log partition function ratio", "text": "AIS produces a nonnegative unbiased estimate R\u0302 of the ratio R = ZT /Z1 of partition functions. Unfortunately, because such ratios often vary across many orders of magnitude, it frequently happens that R\u0302 underestimates R with overwhelming probability, while occasionally taking extremely large values. Correspondingly, the variance may be extremely large, or even infinite.\nFor these reasons, it is more meaningful to estimate logR. Unfortunately, the logarithm of a nonnegative unbiased estimate (such as the AIS estimate) is, in general, a biased estimator of the log estimand. More carefully, let A\u0302 be a nonnegative unbiased estimator for A = E[A\u0302]. Then, by Jeffreys inequality, E[log A\u0302] \u2264 logE[A\u0302] = logA, and so log A\u0302 is a lower bound on logA in expectation. The estimator log A\u0302 satisfies another important property: by Markov\u2019s inequality for nonnegative random variables, Pr(log A\u0302 > logA+ b) < e\u2212b, and so log A\u0302 is extremely unlikely to overestimate logA by any appreciable number of nats. These observations motivate the following definition [BGS15]: a stochastic lower bound on X is an estimator X\u0302 satisfying E[X\u0302] \u2264 X and Pr(X\u0302 > X + b) < e\u2212b. Stochastic upper bounds are defined analogously. The above analysis shows that log A\u0302 is a stochastic lower bound on logA when A\u0302 is a nonnegative unbiased estimate of A, and, in particular, log R\u0302 is a stochastic lower bound on logR. (It is possible to strengthen the tail bound by combining multiple samples [GBD07].)"}, {"heading": "2.4 Reverse AIS and Bidirectional Monte Carlo", "text": "Upper and lower bounds are most useful in combination, as one can then sandwich the true value. As described above, AIS produces a stochastic lower bound on the ratio R; many other algorithms do as well. Upper bounds are more challenging to obtain. The key insight behind bidirectional Monte Carlo (BDMC; [GGA15]) is that, provided one has an exact sample from the target distribution pT , one can run AIS in reverse to produce a stochastic lower bound on logRrev = logZ1/ZT , and therefore a stochastic upper bound on logR = \u2212 logRrev. (In fact, BDMC is a more general framework which allows a variety of partition function estimators, but we focus on AIS for pedagogical purposes.)\nMore carefully, for t = 1, . . . , T , define p\u0303t = pT\u2212t+1 and T\u0303t = TT\u2212t+1. Then p\u03031 corresponds to our original target distribution pT and p\u0303T corresponds to our original initial distribution p1. As\nAlgorithm 2 Bidirectional Monte Carlo (BDMC) Input: unnormalized densities f1, . . . , fT\nreversible MCMC transition operators T1, . . . , TT , where Tt leaves pt = ft/Zt invariant sample x1 from p1 = f1/Z1 and sample xT from pT = fT /ZT\nOutput: stochastic lower and upper bounds on log ZTZ1 R\u0302rev \u2190 AIS on fT , . . . , f1; TT , . . . , T1; and xT R\u0302 \u2190 AIS on f1, . . . , fT ; T1, . . . , TT ; and x1 return (log R\u0302, log R\u0302\u22121rev)\nbefore, T\u0303t leaves p\u0303t invariant. Consider the estimate produced by AIS on the sequence of distributions p\u03031, . . . , p\u0303T and corresponding MCMC transition operators T\u03031, . . . , T\u0303T . (In this case, the forward chain of AIS corresponds to the reverse chain described in Section 2.2.) The resulting estimate R\u0302rev is a nonnegative unbiased estimator of Rrev. It follows that log R\u0302rev is a stochastic lower bound on logRrev, and therefore log R\u0302\u22121rev is a stochastic upper bound on logR = logR\u22121rev. BDMC is simply the combination of this stochastic upper bound with the stochastic lower bound of Section 2.3. (See Algorithm 2 for pseudocode.) Because AIS is a consistent estimator of the partition function ratio under the assumption of ergodicity [Nea01], the two bounds converge as T \u2192\u221e; therefore, given enough computation, BDMC can sandwich logR to arbitrary precision.\nReturning to the setting of Bayesian inference, given some fixed observation y, we can apply BDMC provided we have exact samples from both the prior distribution p(\u03b8, z) and the posterior distribution p(\u03b8, z|y). In practice, the prior is typically easy to sample from, but it is typically infeasible to generate exact posterior samples. However, in models where we can tractably sample from the joint distribution p(\u03b8, z,y), we can generate exact posterior samples for simulated observations using the elementary fact that\np(y) p(\u03b8, z|y) = p(\u03b8, z,y) = p(\u03b8, z) p(y|\u03b8, z). (3)\nIn other words, if one ancestrally samples \u03b8, z, and y, this is equivalent to first generating a dataset y and then sampling (\u03b8, z) exactly from the posterior. Therefore, for simulated data, one has access to a single exact posterior sample; this is enough to obtain stochastic upper bounds on logR = log p(y)."}, {"heading": "3 Methods", "text": "There are at least two criteria we would desire from a sampling-based approximate inference algorithm in order that its samples be representative of the true posterior distribution: we would like the approximate distribution q(\u03b8, z;y) to cover all the high-probability regions of the posterior p(\u03b8, z |y), and we would like it to avoid placing probability mass in low-probability regions of the posterior. The former criterion motivates measuring the KL divergence DKL(p(\u03b8, z |y) \u2016 q(\u03b8, z;y)), and the latter criterion motivates measuring DKL(q(\u03b8, z;y) \u2016 p(\u03b8, z |y)). If we desire both simultaneously, this motivates paying attention to the Jeffreys divergence, defined as DJ(q\u2016p) = DKL(q\u2016p) + DKL(p\u2016q).\nIn case one is not accustomed to thinking about the error in approximate posterior inference in terms of the Jeffreys divergence, Fig. 1 provides some examples of pairs of Gaussian distributions whose Jeffreys divergence is 1 nat.\nIn this section, we present Bounding Divergences with Reverse Annealing (BREAD), a technique for using BDMC to bound the Jeffreys divergence from the true posterior on simulated data, combined with a protocol for using this technique to analyze sampler accuracy on real-world data."}, {"heading": "3.1 Upper bounding the Jeffreys divergence in expectation", "text": "We now present our technique for bounding the Jeffreys divergence between the target distribution and the distribution of approximate samples produced by AIS. In describing the algorithm, we revert to the abstract state space formalism of Section 2.2, since the algorithm itself does not depend on any structure specific to posterior inference (except for the ability to obtain an exact sample). We first repeat the derivation from [GGA15] of the bias of the stochastic lower bound log R\u0302. Let v = (x1, . . . ,xT\u22121) denote all of the variables sampled in AIS before the final stage; the final state xT corresponds to the approximate sample produced by AIS. We can write the distributions over the forward and reverse AIS chains as:\nqfwd(v,xT ) = qfwd(v) qfwd(xT |v) (4) qrev(v,xT ) = pT (xT ) qrev(v |xT ). (5)\nThe distribution of approximate samples qfwd(xT ) is obtained by marginalizing out v. Note that sampling from qrev requires sampling exactly from pT , so strictly speaking, BREAD is limited to those cases where one has at least one exact sample from pT \u2014 such as simulated data from a probabilistic model (see Section 2.4).\nThe expectation of the estimate log R\u0302 of the log partition function ratio is given by: E[log R\u0302] = Eqfwd(v,xT ) [ log\nfT (xT ) qrev(v |xT ) Z1 qfwd(v,xT )\n] (6)\n= logZT \u2212 logZ1 \u2212DKL(qfwd(xT ) qfwd(v |xT ) \u2016 pT (xT ) qrev(v |xT )) (7) \u2264 logZT \u2212 logZ1 \u2212DKL(qfwd(xT ) \u2016 pT (xT )). (8)\n(Note that qfwd(v |xT ) is the conditional distribution of the forward chain, given that the final state is xT .) The inequality follows because marginalizing out variables cannot increase the KL divergence.\nWe now go beyond the analysis in [GGA15], to bound the bias in the other direction. The expectation of the reverse estimate R\u0302rev is\nE[log R\u0302rev] = Eqrev(xT ,v) [ log\nZ1 qfwd(v,xT ) fT (xT ) qrev(v |xT )\n] (9)\n= logZ1 \u2212 logZT \u2212DKL(pT (xT ) qrev(v|xT ) \u2016 qfwd(xT ) qfwd(v |xT )) (10) \u2264 logZ1 \u2212 logZT \u2212DKL(pT (xT ) \u2016 qfwd(xT )). (11)\nAs discussed above, log R\u0302 and log R\u0302\u22121rev can both be seen as estimators of log ZTZ1 , the former of which is a stochastic lower bound, and the latter of which is a stochastic upper bound. Consider the gap between these two bounds, B\u0302 , log R\u0302\u22121rev \u2212 log R\u0302. It follows from Eqs. (8) and (11) that, in expectation, B\u0302 upper bounds the Jeffreys divergence\nJ , DJ(pT (xT ), qfwd(xT )) , DKL(pT (xT ) \u2016 qfwd(xT )) + DKL(qfwd(xT ) \u2016 pT (xT )) (12)\nbetween the target distribution pT and the distribution qfwd(pT ) of approximate samples. Alternatively, if one happens to have some other lower bound L or upper bound U on logR, then one can bound either of the one-sided KL divergences by running only one direction of AIS. Specifically, from Eq. (8), E[U \u2212 log R\u0302] \u2265 DKL(qfwd(xT ) \u2016 pT (xT )), and from Eq. (11), E[log R\u0302\u22121rev \u2212 L] \u2265 DKL(pT (xT ) \u2016 qfwd(xT ))."}, {"heading": "3.2 Evaluating B and J on small examples", "text": "In the previous section, we derived an estimator B\u0302 which upper bounds in expectation the Jeffreys divergence J between the true posterior distribution and the distribution of approximate samples produced by AIS, i.e., E[B\u0302] > J . It can be seen from Eqs. (7) and (10) that the expectation B , E[B\u0302] is the Jeffreys divergence between the distributions over the forward and reverse chains:\nB = E[log R\u0302\u22121rev \u2212 log R\u0302] = DKL(qfwd(xT ) qfwd(v |xT ) \u2016 pT (xT ) qrev(v |xT )) + + DKL(pT (xT ) qrev(v|xT ) \u2016 qfwd(xT ) qfwd(v |xT ))\n= DKL(qfwd(v,xT ) \u2016 qrev(v,xT )) + DKL(qrev(v,xT ) \u2016 qfwd(v,xT )) = DJ(qfwd(v,xT ) \u2016 qrev(v,xT )) (13)\nOne might intuitively expect B to be a very loose upper bound on J , as it is a divergence over a much larger domain (specifically, of size |X |T , compared with |X |). In order to test this empirically, we have evaluated both J and B on several toy distributions for which both quantities can be tractably computed. We found the na\u00efve intuition to be misleading \u2014 on some of these distributions, B is a good proxy for J . In this section, we describe how to exactly compute B and J in small discrete spaces; experimental results are given in Section 5.1.1\nEven when the domain X is a small discrete set, distributions over the extended state space are too large to represent explicitly. However, because both the forward and reverse chains are Markov chains, all of the marginal distributions qfwd(xt,xt+1) and qrev(xt,xt+1) can be computed using the forward pass of the Forward\u2013Backward algorithm. The final-state divergence J can be computed directly from the marginals over xT . The KL divergence DKL(qfwd \u2016 qrev) can be computed as:\nDKL(qfwd \u2016 qrev) = Eqfwd(x1,...,xT ) [log qfwd(x1, . . . ,xT )\u2212 log qrev(x1, . . . ,xT )] (14) = Eqfwd(x1) [log qfwd(x1)\u2212 log qrev(x1)] +\n+ T\u22121\u2211 t=1 Eqfwd(xt,xt+1) [log qfwd(xt+1 |xt)\u2212 log qrev(xt+1 |xt)] , (15)\nand analogously for DKL(qrev \u2016 qfwd). B is the sum of these quantities."}, {"heading": "3.3 Application to real-world data", "text": "So far, we have focused on the setting of simulated data, where it is possible to obtain an exact posterior sample, and then to rigorously bound the Jeffreys divergence using BDMC. However, we are more likely to be interested in evaluating the performance of inference on real-world data. Heuristically, we can generate simulated data to use as a proxy for the real-world data, but this requires ensuring that the datasets are similar enough that the findings will transfer. We propose to simulate data using model hyperparameters obtained from the real-world dataset of interest, run BDMC on the simulated data, and then validate that forward AIS chains behave similarly between the two datasets.\nOften, we informally distinguish between parameters of a statistical model (e.g. regression weights) and hyperparameters (e.g. noise variance). So far in this paper, we have not distinguished parameters\nand hyperparameters, since they are treated the same by Stan, WebPPL, and all of the associated inference algorithms. However, we believe parameters and hyperparameters must be treated differently in the synthetic data generation process and in the reverse AIS chain. In this section we use \u03b8 to refer to parameters and \u03b7 to refer to hyperparameters. In Bayesian analysis, hyperparameters are often assigned non-informative or weakly informative priors, in order to avoid biasing the inference. This poses a challenge for BREAD, as datasets generated from hyperparameters sampled from such priors (which are often very broad) can be very dissimilar to real datasets, and hence conclusions from the simulated data may not generalize.\nIn order to generate synthetic datasets which better match the real-world dataset of interest, we adopt the following heuristic scheme: we first perform approximate posterior inference on the real-world dataset. Let \u03b7real denote the estimated hyperparameters. We then simulate parameters and data from the forward model p(\u03b8 |\u03b7real)p(D|\u03b8,\u03b7real). The forward AIS chain is run on D in the usual way. However, to obtain the posterior sample for the reverse chain, we first start with (\u03b7real,\u03b8), and then run some number of MCMC transitions which preserve p(\u03b8,\u03b7 |D). In general, this will not yield an exact posterior sample, since \u03b7real was not sampled from p(\u03b7 |D). However, heuristically \u03b7real ought to be fairly representative of the posterior distribution unless the prior p(\u03b7) concentrates most of its mass away from \u03b7real. If this is the case, then a relatively small number of MCMC steps ought to come close to p(\u03b8,\u03b7 |D), and hence the sample can be used as a proxy for the exact posterior sample. We validate this hypothesis in Section 5.1.3.\nWe also recommend validating that the inference procedure behaves similarly on the real-world and simulated data. We aren\u2019t aware of a rigorous method for testing this, so we recommend a heuristic approach where one runs the forward AIS chains for varying numbers of steps, computes the log-ML lower bounds, and inspects whether the overall shape of the curves is similar. This ought to highlight cases where one of the datasets is obviously much harder than the other. (We would prefer to have a stronger form of validation than this; this is a topic for future work.)"}, {"heading": "4 Related work", "text": "Much work has already been devoted to the diagnosis of Markov chain convergence. (See e.g. [Gey11].) In general, a set of MCMC samples x(1), . . . ,x(K) is used to estimate an expectation E[h(x)] = 1 K \u2211K k=1 h(x\n(k)). We would like to achieve an estimate with both low bias and low variance. One often attempts to reduce the bias by discarding all samples until a particular \u201cburn-in\u201d time, by which the Markov chain is believed to have mixed sufficiently. Unfortunately, choosing a good burn-in time is difficult because it is hard to diagnose mixing. One commonly used heuristic (also recommended by the Stan manual [Sta]) is to initialize multiple chains at diverse regions of the state space and to track various statistics of all of these chains over time. One wants to check that the between-chain variance of these statistics is small relative to the within-chain variance. This is not a perfect solution for two reasons: (1) all of the chains may be initialized to the same local mode, so the method would only diagnose mixing within this mode, and (2) the set of statistics may not be sufficiently expressive to detect failures to mix.\nWhile burn-in is meant to address the problem of bias, other diagnostics are meant to deal with the problem of variance. Consecutive states in a Markov chain are highly correlated with one another, so one cannot obtain low-variance estimates of the statistics unless one runs the chain long enough to obtain near-independent samples. This can be formalized through the notion of effective sample size (ESS), defined as the number of truly independent samples which would result in the same variance which was obtained from the MCMC samples. One can estimate the ESS in practice by estimating autocorrelations between the statistics at various time offsets in a Markov chain. Unfortunately, such diagonstics are heuristic, and can also return misleading results in cases where the chain fails to explore important modes.\nA lot of work has also been devoted to automatically configuring parameters of MCMC algorithms.\nSince it is hard to reliably summarize the performance of an MCMC algorithm, such automatic configuration methods typically rely on method-specific analyses. For instance, Roberts and Rosenthal [RR01] famously showed that the optimal acceptance rate of M\u2013H with an isotropic proposal distribution is 0.234 under fairly general conditions. M\u2013H algorithms are sometimes tuned to achieve this acceptance rate, even in situations where the theoretical analysis doesn\u2019t hold. An easily measurable performance criterion could likely enable more direct optimization of algorithmic hyperparameters.\nFinally, Gorham and Mackey [GM15] present a method for directly estimating the quality of a set of approximate samples, independently of how those samples were obtained. This method has strong guarantees under a strong convexity assumption, but unfortunately this assumption rules out exactly those cases where mixing is most difficult: multimodal or badly conditioned distributions. By contrast, our Jeffreys divergence bound holds regardless of the choice of posterior distribution.\nWe have recently learned of independent work [CTM16] which also builds off BDMC to evaluate the accuracy of posterior inference in a probabilistic programming language. In particular, CusumanoTowner and Mansinghka [CTM16] define an unbiased estimator for a quantity called the subjective divergence. The estimator is equivalent to BDMC except that the reverse chain is initialized from an arbitrary reference distribution, rather than the true posterior. In [CTM16], the subjective divergence is shown to upper bound the Jeffreys divergence when the true posterior is used; this is equivalent to our analysis in Section 3.1.\nMuch less is known about subjective divergence when the reference distribution is not taken to be the true posterior: Cusumano-Towner and Mansinghka [CTM16] give conditions under which the subjective divergence bounds the Jeffreys divergence, but the question of when these conditions hold in practice is not explored. Verifying the conditions in a particular case may require computing or bounding KL divergences which are themselves intractable. To the best of our knowledge, in the absence of further assumptions, subjective divergence has no provable relationship to KL divergence, and so the measure is best regarded as a heuristic. (Indeed, the subjective divergence is not truly a divergence, as it can take negative values.2) We believe it is an essential feature of BREAD that it always bounds the Jeffreys divergence in expectation on simulated data and then validates the consistency of the algorithm\u2019s behavior between simulated and real data.\n5 Empirical evaluation of BREAD We have two motivations for our experiments. Our first set of experiments was intended to validate that BREAD can be used to evaluate the accuracy of posterior inference in realistic settings. Next, we used BREAD to explore the tradeoffs between two different representations of a matrix decomposition model in both WebPPL and Stan."}, {"heading": "5.1 Validation", "text": "As described above, BREAD returns rigorous bounds on the Jeffreys divergence only when the data are sampled from the model distribution. Mathematically, it could potentially give misleading results in three ways. First, the upper bound B may overestimate the true Jeffreys divergence J . Second, results on simulated data may not correspond to results on real-world data if the simulated data is not representative of the real-world data. Finally, the fixed hyperparameter procedure of Section 3.3 may not yield a sample sufficiently representative of the true posterior p(\u03b8,\u03b7 |D). In this section, we attempted to validate that B is a good proxy for J , that the behavior of the method on simulated\n2This can happen when the reference distribution is especially inaccurate. For instance, consider a case where x and y are constant, and z \u2208 {0, 1}. Define the model distribution p(z) = p(z, x), inference distribution q(z) = q(z;x), and reference distribution r(z) = r(z;x) such that p(1) = 0.9, q(1) = 0.5, and r(1) = 0.1. Let both q\u0302IS and q\u0302HM return the true q(z;x) deterministically. In this case, DSBJ(q(z;x\u2217)\u2016p(z|x\u2217)) \u2248 \u22120.879.\ndata is consistent with that on real data, and that the fixed-hyperparameter samples can be used as a proxy for samples from the posterior."}, {"heading": "5.1.1 How tight is the bound?", "text": "To validate the upper bound B as a measure of the accuracy of an approximate inference engine, we must first check that it accurately reflects the true Jeffreys divergence J . This is hard to do in realistic settings since J is generally unavailable, but we can evaluate both quantities on small toy distributions using the technique of Section 3.2.\nWe considered several toy distributions, where the domain X was taken to be a 7\u00d7 7 grid. In all cases, the MCMC transition operator was Metropolis-Hastings, where the proposal distribution was uniform over moving one step in the four coordinate directions. (Moves which landed outside the grid were rejected.) For the intermediate distributions, we used geometric averages with a linear schedule for \u03b2.\nFirst, we generated random unnormalized target distributions f(x) = exp(g(x)), where each entry of g was sampled independently from the normal distribution N (0, \u03c3). When \u03c3 is small, the M-H sampler is able to explore the space quickly, because most states have similar probabilities, and therefore most M-H proposals are accepted. However, when \u03c3 is large, the distribution fragments into separated modes, and it is slow to move from one mode to another using a local M-H kernel. We sampled random target distributions using \u03c3 = 2 (which we refer to as EASY RANDOM) and \u03c3 = 10 (which we refer to as HARD RANDOM). The target distributions, as well as some of the intermediate distributions, are shown in Fig. 2.\nThe other toy distribution we considered consists of four modes separated by a deep energy barrier. The unnormalized target distribution is defined by:\nf(x) =  e 3 in the upper right quadrant 1 in the other three quadrants e\u221210 on the barrier\n(16)\nIn the target distribution, the dominant mode makes up about 87% of the probability mass. Fig. 2 shows the target function as well as some of the AIS intermediate distributions. This example illustrates a common phenomenon in AIS, whereby the distribution splits into different modes at\nsome point in the annealing process, and then the relative probability mass of those modes changes. Because of the energy barrier, we refer to this example as BARRIER.\nThe distributions of approximate samples produced by AIS with T = 100 intermediate distributions are shown in Fig. 2. For EASY RANDOM, the approximate distribution is quite accurate, with J = 0.13. However, for the other two examples, the probability mass is misallocated between different modes because the sampler has a hard time moving between them. Correspondingly, the Jeffreys divergences are J = 4.73 and J = 1.65 for HARD RANDOM and BARRIER, respectively.\nHow well are these quantities estimated by the upper bound B? Fig. 3 shows both J and B for all three toy distributions and numbers of intermediate distributions varying from 10 to 100,000. In the case of EASY RANDOM, the bound is quite far off: for instance, with 1000 intermediate distributions, J \u2248 0.00518 and B \u2248 0.0705, which differ by more than a factor of 10. However, the bound is more accurate in the other two cases: for HARD RANDOM, J \u2248 1.840 and B \u2248 2.309, a relative error of 26%; for BARRIER, J \u2248 1.085 and B \u2248 1.184, a relative error of about 9%. In these cases, according to Fig. 3, the upper bound remains accurate across several orders of magnitude in the number of intermediate distributions and in the true divergence J . Roughly speaking, it appears that most of the difference between the forward and reverse AIS chains is explained by the difference in distributions over the final state. Even the extremely conservative bound in the case of EASY RANDOM is potentially quite useful, as it still indicates that the inference algorithm is performing well, in contrast with the other two cases.\nOverall, we conclude that on these toy distributions, the upper bound B is accurate enough to give meaningful information about J . This motivates using B as a diagnostic for evaluating approximate inference algorithms when J is unavailable."}, {"heading": "5.1.2 Validation on real datasets", "text": "We consider a Bayesian linear regression model in Stan, with weights being sampled from uniform Gaussian priors and the standard deviation of the generating Gaussian distribution sampled from an inverse-Gaussian prior. We consider two datasets - a) a real dataset - a standardized version of the NHEFS dataset (http://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2015/ 07/nhefs_book.xlsx) and b) a simulated dataset. In order to validate the use of the synthetic data as a proxy for the real data, we generated the stochastic lower bound curves on both datasets. Results are shown in Fig. 4(a). The curves appear to level off to different values, as different datasets\nwould have different log marginal likelihoods. However, they appear to level off at about the same rate, suggesting that the behavior is consistent, i.e. neither of the datasets challenges the sampler more than the other. This suggests that bounds on the KL divergence, estimated from the upper and lower bounds on the simulated data, can help us understand convergence properties on the real data as well."}, {"heading": "5.1.3 Fixed hyperparameter scheme", "text": "To validate the fixed hyperparameter scheme of Section 3.3, we consider a similar Bayesian regression model as in Section 5.1.2, with covariance hyperparameters assigned broad Cauchy priors. We fixed the hyperparameter values to 1 when simulating data. As described in Section 3.3, we ran MCMC chains of length 10, 100, 1000 and 10,000 starting from (\u03b7real,\u03b8) in order to obtain an approximate posterior sample to initialize the reverse chain. Results are shown in Fig. 4(b). We find that the upper bound curves are indistinguishable for all numbers of steps. In fact, the variability was observed to be comparable to between-trial variability of independent and identical AIS runs, as seen in Fig. 4(b). This suggests that reverse AIS chains can be initialized from true hyperparameter values (instead of the posterior), followed by a small number of MCMC steps. Hence BREAD supports non-informative or weakly informative priors on hyperparameters.\n5.2 Scientific findings produced by BREAD Having validated various aspects of BREAD, we applied it to investigate the choice of model representation in Stan and WebPPL. In the course of our experiments, we also uncovered a bug in WebPPL, indicating the potential usefulness of BREAD as a means of testing the correctness of an implementation."}, {"heading": "5.2.1 Comparing model representations", "text": "Many models can be written in more than one way, for example by introducing or collapsing latent variables. Performance of probabilistic programming languages can be sensitive to such choices of representation, and the representation which gives the best performance may vary from one language to another. Hence, users of probabilistic programming languages might want to know the right\nrepresentation to use for their language of choice, and BREAD can prove extremely useful in this regard.\nWe consider the matrix factorization model, where we approximate an N \u00d7D matrix Y as a low rank matrix, the product of matrices U and V with dimensions N \u00d7K and K \u00d7D respectively (where K < min(N,D)). We use a spherical Gaussian observation model, and spherical Gaussian priors on U and V.\nuik \u223c N (0, \u03c32u) vkj \u223c N (0, \u03c32v) yij \u223c N (uTi vj , \u03c32)\nWe can collapse U and rewrite this model as\nvkj \u223c N (0, \u03c32v) yi \u223c N (0, \u03c3uVTV + \u03c3I)\nWe fix the values of all hyperparameters to 1, and set N = 50, K = 5 and D = 25. We ran BREAD on this model in Stan and plotted the BDMC curves (see Fig. 5).\nIn general, collapsing variables can result in faster convergence (in terms of number of iterations) at the expense of slower updates. Consistent with this story, inference appears to converge faster on the collapsed model converges in terms of the number of MCMC steps. However, there is greater computational overhead per MCMC step in the collapsed version, since it involves sampling from a multivariate Gaussian distribution. Overall, the tradeoff favors the uncollapsed version in terms of running time. Hence BREAD suggests that the user need not collapse the matrix factorization model in Stan.\nHowever, the story is different in the case of WebPPL. Fig. 5 shows AIS curves for collapsed and uncollapsed versions, for a smaller-sized matrix factorization model where N = 10, K = 5 and D = 10. The true log-ML value was obtained by running BREAD in Stan where the BDMC bounds had converged to within 1 nat. BREAD on WebPPL shows that the collapsed version enables far faster convergence in terms of running time; therefore, unlike with Stan, it is advisable to collapse the model in WebPPL. Hence BREAD can provide insight into the tricky question of which representations of models to choose to achieve faster convergence."}, {"heading": "5.2.2 Debugging", "text": "Mathematically, the forward and reverse AIS chains yield lower and upper bounds on log p(y) with high probability; if this behavior is not observed, that indicates a bug. In our experimentation with WebPPL, we observed a case where the reverse AIS chain yielded estimates significantly lower than\nthose produced by the forward chain, inconsistent with the theoretical guarantee. This led us to find a subtle bug in how WebPPL sampled from a multivariate Gaussian distribution (which had the effect that the exact posterior samples used to initialize the reverse chain were incorrect).3 These days when many new probabilistic programming languages are emerging and many are in active development, such debugging capabilities provided by BREAD can potentially be very useful."}], "references": [{"title": "Quantifying the probable approximation error of probabilistic inference programs", "author": ["M.F. Cusumano-Towner", "V.K. Mansinghka"], "venue": null, "citeRegEx": "Cusumano.Towner and Mansinghka.,? \\Q2016\\E", "shortCiteRegEx": "Cusumano.Towner and Mansinghka.", "year": 2016}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Efron and Tibshirani.,? \\Q1998\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1998}, {"title": "Studies in lower bounding probability of evidence using the Markov inequality", "author": ["V. Gogate", "B. Bidyuk", "R. Dechter"], "venue": "In: Conference on Uncertainty in AI", "citeRegEx": "Gogate et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2007}, {"title": "Introduction to Markov chain Monte Carlo", "author": ["C.J. Geyer"], "venue": "Handbook of Markov chain Monte Carlo. Chapman & Hall/CRC,", "citeRegEx": "Geyer.,? \\Q2011\\E", "shortCiteRegEx": "Geyer.", "year": 2011}, {"title": "Sandwiching the marginal likelihood with bidirectional Monte Carlo", "author": ["R.B. Grosse", "Z. Ghahramani", "R.P. Adams"], "venue": null, "citeRegEx": "Grosse et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2015}, {"title": "Measuring sample quality with Stein\u2019s method", "author": ["J. Gorham", "L. Mackey"], "venue": "Neural Information Processing Systems", "citeRegEx": "Gorham and Mackey.,? \\Q2015\\E", "shortCiteRegEx": "Gorham and Mackey.", "year": 2015}, {"title": "Church: a language for generative models", "author": ["N.D. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "In: Conference on Uncertainty in AI", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Annealing between distributions by averaging moments", "author": ["R. Grosse", "C.J. Maddison", "R. Salakhutdinov"], "venue": "Neural Information Processing Systems", "citeRegEx": "Grosse et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2013}, {"title": "The No-U-turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo", "author": ["M.D. Homan", "A. Gelman"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Homan and Gelman.,? \\Q2014\\E", "shortCiteRegEx": "Homan and Gelman.", "year": 2014}, {"title": "Sequential Monte Carlo samplers", "author": ["P. del Moral", "A. Doucet", "A. Jasra"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "citeRegEx": "Moral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moral et al\\.", "year": 2006}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo", "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Annealed importance sampling", "author": ["R.M. Neal"], "venue": "Statistics and Computing", "citeRegEx": "Neal.,? \\Q2001\\E", "shortCiteRegEx": "Neal.", "year": 2001}, {"title": "Optimal scaling for various Metropolis\u2013Hastings algorithms", "author": ["G.O. Roberts", "J.S. Rosenthal"], "venue": "Statistical Science", "citeRegEx": "Roberts and Rosenthal.,? \\Q2001\\E", "shortCiteRegEx": "Roberts and Rosenthal.", "year": 2001}], "referenceMentions": [], "year": 2016, "abstractText": "Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We present Bounding Divergences with REverse Annealing (BREAD), a protocol for validating the relevance of simulated data experiments to real datasets, and integrate it into two probabilistic programming languages: WebPPL [GS] and Stan [CGHL+ p]. As an example of how BREAD can be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in both WebPPL and Stan.", "creator": "LaTeX with hyperref package"}}}