{"id": "1606.02448", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Multiple-Play Bandits in the Position-Based Model", "abstract": "However, a major concern in this context is when the system cannot decide whether the user's feedback is actually usable for each element. In fact, much of the content may simply have been ignored by the user. In the present paper, it is proposed to exploit available information about the distortion of the display position within the framework of the so-called Position-Based Click Model (PBM). We will first discuss how this model differs from the cascade model and its variants, which are considered in several current work on multiplay bandits. Subsequently, we provide a novel repentance lower limit for this model, as well as computer-efficient algorithms that exhibit good empirical and theoretical performance.", "histories": [["v1", "Wed, 8 Jun 2016 08:31:46 GMT  (527kb,D)", "http://arxiv.org/abs/1606.02448v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.TH", "authors": ["paul lagr\u00e9e", "claire vernade", "olivier capp\u00e9"], "accepted": true, "id": "1606.02448"}, "pdf": {"name": "1606.02448.pdf", "metadata": {"source": "CRF", "title": "Multiple-Play Bandits in the Position-Based Model", "authors": ["Paul Lagr\u00e9e", "Claire Vernade", "Olivier Capp\u00e9"], "emails": ["paul.lagree@u-psud.fr", "claire.vernade@telecom-paristech.fr", "cappe@enst.fr"], "sections": [{"heading": "1 Introduction", "text": "During their browsing experience, users are constantly provided \u2013 without having asked for it \u2013 with clickable content spread over web pages. While users interact on a website, they send clicks to the system for a very limited selection of the clickable content. Hence, they let every unclicked item with an equivocal answer: the system does not know whether the content was really deemed irrelevant or simply ignored. In contrast, in traditional multi-armed bandit (MAB) models, the learner makes actions and observes at each round the reward corresponding to the chosen action. In the so-called multiple play semi-bandit setting, when users are presented with L items, they are assumed to provide feedback for each of those items.\nSeveral variants of this basic setting have been considered in the bandit literature. The necessity for the user to provide feedback for each item has been called into question in the context of the so-called Cascade Model [7, 13, 5] and its extensions such as the Dependent Click Model (DCM) [19]. Both models are particularly suited for search contexts, where the user is assumed to be looking for something relative to a query. Consequently, the learner expects explicit feedback: in the Cascade Model each valid observation sequence must be either all zeros or terminated by a one, such that no ambiguity is left on the evaluation of the presented items, while multiple clicks are allowed in the DCM.\nIn the Cascade Model, the positions of the items are not taken into account in the reward process because the learner is assumed to obtain a click as long as the interesting item belongs to the list. Indeed, there are even clear indications that the optimal strategy in a learning context consists in showing the most relevant items at the end of the list in order to maximize the amount of observed feedback [13] \u2013 which is counter-intuitive in recommendation tasks.\nTo overcome these limitations, [5] introduces weights \u2013 to be defined by the learner \u2013 that are attributed to positions in the list, with a click on position l \u2208 {1, . . . , L} providing a reward wl, where the sequence (wl)l is decreasing to enforce the ranking behavior. However, no rule is given for setting the weights (wl)l that control the order of importance of the positions. The authors propose an algorithm based on KL-UCB [9] and prove a lower bound on the regret as well as an asymptotically optimal upper bound.\nAnother way to address the limitations of the Cascade Model is to consider the DCM as in [19]. Here, examination probabilities vl are introduced for each position l: conditionally on the event that the user effectively\nar X\niv :1\n60 6.\n02 44\n8v 1\n[ cs\n.L G\n] 8\nJ un\nscanned the list up to position l, he/she can choose to leave with probability vl and in that case, the learner is aware of his/her departure. This framework naturally induces the necessity to rank the items in the optimal order.\nAll previous models assume that a portion of the recommendation list is explicitly examined by the user and hence that the learning algorithm eventually has access to rewards corresponding to the unbiased user\u2019s evaluation of each item. In contrast, we propose to analyze multiple-play bandits in the Position-based model (PBM) [4]. In the PBM, each position in the list is also endowed with a binary Examination variable [7, 18] which is equal to one only when the user paid attention to the corresponding item. But this variable, that is independent of the user\u2019s evaluation of the item, is not observable. It allows to model situations where the user is not explicitly looking for specific content, as in typical recommendation scenarios.\nCompared to variants of the Cascade model, the PBM is challenging due to the censoring induced by the examination variables: the learning algorithm observes actual clicks but non-clicks are always ambiguous. Thus, combining observations made at different positions becomes a non-trivial statistical task. Some preliminary ideas on how to address this issue appear in the supplementary material of [12]. In this work, we provide a complete statistical study of stochastic multiple-play bandits with semi-bandit feedback in the PBM.\nWe introduce the model and notations in Section 2 and provide the lower bound on the regret in Section 3. In Section 4, we present two optimistic algorithms as well as a theoretical analysis of their regret. In the last section dedicated to experiments, those policies are compared to several benchmarks on both synthetic and realistic data."}, {"heading": "2 Setting and Parameter Estimation", "text": "We consider the binary stochastic bandit model with K Bernoulli-distributed arms. The model parameters are the arm expectations \u03b8 = (\u03b81, \u03b82, . . . , \u03b8K), which lie in \u0398 = (0, 1)K . We will denote by B(\u03b8) the Bernoulli distribution with parameter \u03b8 and by d(p, q) := p log(p/q) + (1 \u2212 p) log((1 \u2212 p)/(1 \u2212 q)) the Kullback-Leibler divergence from B(p) to B(q). At each round t, the learner selects a list of L arms \u2013 referred to as an action \u2013 chosen among the K arms which are indexed by k \u2208 {1, . . . ,K}. The set of actions is denoted by A and thus contains K!/(K \u2212 L)! ordered lists; the action selected at time t will be denoted A(t) = (A1(t), . . . , AL(t)).\nThe PBM is characterized by examination parameters (\u03bal)1\u2264l\u2264L, where \u03bal is the probability that the user effectively observes the item in position l [4]. At round t, the selection A(t) is shown to the user and the learner observes the complete feedback \u2013 as in semi-bandit models \u2013 but the observation at position l, Zl(t), is censored being the product of two independent Bernoulli variables Yl(t) and Xl(t), where Yl(t) \u223c B(\u03bal) is non null when the user considered the item in position l \u2013 which is unknown to the learner \u2013 and Xl(t) \u223c B(\u03b8Al(t)) represents the actual user feedback to the item shown in position l. The learner receives a reward rA(t) = \u2211L l=1 Zl(t), where Z(t) = (X1(t)Y1(t), . . . , XL(t)YL(t)) denotes the vector of censored observations at step t. In the following, we will assume, without loss of generality, that \u03b81 > \u00b7 \u00b7 \u00b7 > \u03b8K and \u03ba1 > \u00b7 \u00b7 \u00b7 > \u03baL > 0, in order to simplify the notations. The fact that the sequences (\u03b8l)l and (\u03bal)l are decreasing implies that the optimal list is a\u2217 = (1, . . . , L). Denoting by R(T ) = \u2211T t=1 ra\u2217 \u2212 rA(t) the regret incurred by the learner up to time T , one has\nE[R(T )] = T\u2211 t=1 L\u2211 l=1 \u03bal(\u03b8a\u2217l \u2212 E[\u03b8Al(t)]) = \u2211 a\u2208A (\u00b5\u2217 \u2212 \u00b5a)E[Na(T )] = \u2211 a\u2208A \u2206aE[Na(T )], (1)\nwhere \u00b5a = \u2211L l=1 \u03bal\u03b8al is the expected reward of action a, \u00b5\n\u2217 = \u00b5a\u2217 is the best possible reward in average, \u2206a = \u00b5 \u2217 \u2212 \u00b5a the expected gap to optimality, and, Na(T ) = \u2211T t=1 1{A(t) = a} is the number of times action a has been chosen up to time T . In the following, we assume that the examination parameters (\u03bal)1\u2264l\u2264L are known to the learner. These can be estimated from historical data [4], using, for instance, the EM algorithm [8] (see also Section 5). In most scenarios, it is realistic to assume that the content (e.g., ads in on-line advertising) is changing much more frequently than the layout (web page design for instance) making it possible to have a good knowledge of the click-through biases associated with the display positions.\nThe main statistical challenge associated with the PBM is that one needs to obtain estimates and confidence bounds for the components \u03b8k of \u03b8 from the available B(\u03bal\u03b8k)-distributed draws corresponding to occurrences of arm k at various positions l = 1, . . . , L in the list. To this aim, we define the following statistics: Sk,l(t) =\n\u2211t\u22121 s=1 Zl(s)1{Al(s) = k}, Sk(t) = \u2211L l=1 Sk,l(t), Nk,l(t) = \u2211t\u22121 s=1 1{Al(s) = k}, Nk(t) = \u2211L l=1Nk,l(t). We further\nrequire bias-corrected versions of the counts N\u0303k,l(t) = \u2211t\u22121 s=1 \u03bal1{Al(s) = k} and N\u0303k(t) = \u2211L l=1 N\u0303k,l(t).\nA time t, and conditionally on the past actions A(1) up to A(t\u2212 1), the Fisher information for \u03b8k is given by I(\u03b8k) = \u2211L l=1Nk,l(t)\u03bal/(\u03b8k(1\u2212 \u03bal\u03b8k)) (see Appendix A). We cannot however estimate \u03b8k using the maximum likelihood estimator since it has no closed form expression. Interestingly though, the simple pooled linear estimator\n\u03b8\u0302k(t) = Sk(t)/N\u0303k(t), (2)\nconsidered in the supplementary material to [12], is unbiased and has a (conditional) variance of \u03c5(\u03b8k) = ( \u2211L l=1Nk,l(t)\u03bal\u03b8k(1 \u2212 \u03bal\u03b8k))/( \u2211L l=1Nk,l(t)\u03bal)\n2, which is close to optimal given the Cram\u00e9r-Rao lower bound. Indeed, \u03c5(\u03b8k)I(\u03b8k) is recognized as a ratio of a weighted arithmetic mean to the corresponding weighted harmonic mean, which is known to be larger than one, but is upper bounded by 1/(1\u2212 \u03b8k), irrespectively of the values of the \u03bal\u2019s. Hence, if, for instance, we can assume that all \u03b8k\u2019s are smaller than one half, the loss with respect to the best unbiased estimator is no more than a factor of two for the variance. Note that despite its simplicity, \u03b8\u0302k(t) cannot be written as a simple sum of conditionally independent increments divided by the number of terms and will thus require specific concentration results.\nIt can be checked that when \u03b8k gets very close to one, \u03b8\u0302k(t) is no longer close to optimal. This observation also has a Bayesian counterpart that will be discussed in Section 5. Nevertheless, it is always preferable to the \u201cposition-debiased\u201d estimator ( \u2211L l=1 Sk,l(t)/\u03bal)/Nk,l(t) which gets very unreliable as soon as one of the \u03bal\u2019s gets very small."}, {"heading": "3 Lower Bound on the Regret", "text": "In this section, we consider the fundamental asymptotic limits of learning performance for online algorithms under the PBM. These cannot be deduced from earlier general results, such as those of [10, 6], due to the censoring in the feedback associated to each action. We detail a simple and general proof scheme \u2013 using the results of [11] \u2013 that applies to the PBM, as well as to more general models.\nLower bounds on the regret rely on changes of measure: the question is how much can we mistake the true parameters of the problem for others, when observing successive arms? With this in mind, we will subscript all expectations and probabilities by the parameter value and indicate explicitly that the quantities \u00b5a, a\u2217, \u00b5\u2217,\u2206a, introduced in Section 2, also depend on the parameter. For ease of notation, we will still assume that \u03b8 is such that a\u2217(\u03b8) = (1, . . . , L)."}, {"heading": "3.1 Existing results for multiple-play bandit problems", "text": "Lower bounds on the regret will be proved for uniformly efficient algorithms, in the sense of [15]:\nDefinition 1. An algorithm is said to be uniformly efficient if for any bandit model parameterized by \u03b8 and for all \u03b1 \u2208 (0, 1], its expected regret after T rounds is such that E\u03b8R(T ) = o(T\u03b1).\nFor the multiple-play MAB, [1] obtained the following bound\nlim inf T\u2192\u221e\nE\u03b8R(T )\nlog(T ) \u2265 K\u2211 k=L+1 \u03b8L \u2212 \u03b8k d(\u03b8k, \u03b8L) . (3)\nFor the \u201clearning to rank\u201d problem where rewards follow the weighted Cascade Model with decreasing weights (wl)l=1,...,L, [5] derived the following bound\nlim inf T\u2192\u221e\nE\u03b8R(T )\nlog T \u2265 wL K\u2211 k=L+1 \u03b8L \u2212 \u03b8k d(\u03b8k, \u03b8L) .\nPerhaps surprisingly, this lower bound does not show any additional term corresponding to the complexity of ranking the L optimal arms. Indeed, the errors are still asymptotically dominated by the need to discriminate irrelevant arms (\u03b8k)k>L from the worst of the relevant arms, that is, \u03b8L."}, {"heading": "3.2 Lower bound step by step", "text": "Step 1: Computing the expected log-likelihood ratio. Denoting by Fs\u22121 the \u03c3-algebra generated by the past actions and observations, we define the log-likelihood ratio for the two values \u03b8 and \u03bb of the parameters by\n`(t) := t\u2211 s=1 log p(Z(s); \u03b8 | Fs\u22121) p(Z(s);\u03bb | Fs\u22121) . (4)\nLemma 2. For each position l and each item k, define the local amount of information by\nIl(\u03b8k, \u03bbk) := E\u03b8\n[ log p(Zl(t); \u03b8)\np(Zl(t);\u03bb) \u2223\u2223\u2223\u2223Al(t) = k] , and its cumulated sum over the L positions by Ia(\u03b8, \u03bb) := \u2211L l=1 \u2211K k=1 1{al = k}Il(\u03b8k, \u03bbk). The expected log-likelihood ratio is given by E\u03b8[`(t)] =\n\u2211 a\u2208A Ia(\u03b8, \u03bb)E\u03b8[Na(t)]. (5)\nThe next proposition is adapted from Theorem 17 in Appendix B of [11] and provides a lower bound on the expected log-likelihood ratio.\nProposition 3. Let B(\u03b8) := {\u03bb \u2208 \u0398 |\u2200l \u2264 L, \u03b8l = \u03bbl and \u00b5\u2217(\u03b8) < \u00b5\u2217(\u03bb)} be the set of changes of measure that improve over \u03b8 without modifying the optimal arms. Assuming that the expectation of the log-likelihood ratio may be written as in (5), for any uniformly efficient algorithm one has\n\u2200\u03bb \u2208 B(\u03b8), lim inf T\u2192\u221e\n\u2211 a\u2208A Ia(\u03b8, \u03bb)E\u03b8[Na(T )]\nlog(T ) \u2265 1.\nStep 2: Variational form of the lower bound. We are now ready to obtain the lower bound in a form similar to that originally given by [10].\nTheorem 4. The expected regret of any uniformly efficient algorithm satisfies\nlim inf T\u2192\u221e\nE\u03b8R(T )\nlog T \u2265 f(\u03b8) , where f(\u03b8) = inf c 0 \u2211 a\u2208A \u2206a(\u03b8)ca , s.t. inf \u03bb\u2208B(\u03b8) \u2211 a\u2208A Ia(\u03b8, \u03bb)ca \u2265 1.\nTheorem 4 is a straightforward consequence of Proposition 3, combined with the expression of the expected regret given in (1). The vector c \u2208 R|A|+ , that satisfies the inequality \u2211 a\u2208A Ia(\u03b8, \u03bb)ca \u2265 1, represents the feasible values of E\u03b8[Na(T )]/ log(T ).\nStep 3: Relaxing the constraints. The bounds mentioned in Section 3.1 may be recovered from Theorem 4 by considering only the changes of measure that affect a single suboptimal arm.\nCorollary 5.\nf(\u03b8) \u2265 inf c 0 \u2211 a\u2208A \u2206a(\u03b8)ca , s.t. \u2211 a\u2208A L\u2211 l=1 1{al = k}Il(\u03b8k, \u03b8L)ca \u2265 1 , \u2200k \u2208 {L+ 1, . . . ,K}.\nCorollary 5 is obtained by restricting the constraint set B(\u03b8) of Theorem 4 to \u222aKk=L+1Bk(\u03b8), where Bk(\u03b8) := {\u03bb \u2208 \u0398|\u2200j 6= k, \u03b8j = \u03bbj and \u00b5\u2217(\u03b8) < \u00b5\u2217(\u03bb)} ."}, {"heading": "3.3 Lower bound for the PBM", "text": "Theorem 6. For the PBM, the following lower bound holds for any uniformly efficient algorithm:\nlim inf T\u2192\u221e\nE\u03b8R(T )\nlog T \u2265 K\u2211 k=L+1 min l\u2208{1,...,L} \u2206vk,l(\u03b8) d(\u03bal\u03b8k, \u03bal\u03b8L) , (6)\nwhere vk,l := (1, . . . , l \u2212 1, k, l, . . . , L\u2212 1).\nProof. First, note that for the PBM one has Il(\u03b8k, \u03bbk) = d(\u03bal\u03b8k, \u03bal\u03bbk). To get the expression given in Theorem 6 from Corollary 5, we proceed as in [5] showing that the optimal coefficients (ca)a\u2208A can be non-zero only for the K\u2212L actions that put the suboptimal arm k in the position l that reaches the minimum of \u2206vk,l(\u03b8)/d(\u03bal\u03b8k, \u03bal\u03b8L). Nevertheless, this position does not always coincide with L, the end of the displayed list, contrary to the case of [5] (see Appendix B for details).\nThe discrete minimization that appears in the r.h.s. of Theorem 6 corresponds to a fundamental trade-off in the PBM. When trying to discriminate a suboptimal arm k from the L optimal ones, it is desirable to put it higher in the list to obtain more information, as d(\u03bal\u03b8k, \u03bal\u03b8L) is an increasing function of \u03bal. On the other hand, the gap \u2206vk,l(\u03b8) is also increasing as l gets closer to the top of the list. The fact that d(\u03bal\u03b8k, \u03bal\u03b8L) is not linear in \u03bal (it is a strictly convex function of \u03bal) renders the trade-off non trivial. It is easily checked that when (\u03b81 \u2212 \u03b8L) is very small, i.e. when all optimal arms are equivalent, the optimal exploratory position is l = 1. In contrast, it is equal to L when the gap (\u03b8L \u2212 \u03b8L+1) becomes very small. Note that by using that for any suboptimal a \u2208 A, \u2206a(\u03b8) \u2265 \u2211K k=L+1 \u2211L l=1 1{al = k}\u03bal(\u03b8L \u2212 \u03b8k), one can lower bound the r.h.s. of Theorem 6\nby \u03baL \u2211K k=L+1(\u03b8L \u2212 \u03b8k)/d(\u03baL\u03b8k, \u03baL\u03b8L), which is not tight in general.\nRemark 7. In the uncensored version of the PBM \u2013 i.e., if the Yl(t) were observed \u2013, the expression of Ia(\u03b8, \u03bb) is simpler: it is equal to \u2211L l=1 \u2211K k=1 1{Al(t) = k}\u03bald(\u03b8k, \u03bbk) and leads to a lower bound that coincides with (3). The uncensored PBM is actually statistically very close to the weighted Cascade model and can be addressed by algorithms that do not assume knowledge of the (\u03bal)l but only of their ordering."}, {"heading": "4 Algorithms", "text": "In this section we introduce two algorithms for the PBM. The first one uses the CUCB strategy of [3] and requires an simple upper confidence bound for \u03b8k based on the estimator \u03b8\u0302k(t) defined in (2). The second algorithm is based on the Parsimonious Item Exploration \u2013 PIE(L) \u2013 scheme proposed in [5] and aims at reaching asymptotically optimal performance. For this second algorithm, termed PBM-PIE, it is also necessary to use a multi-position analog of the well-known KL-UCB index [9] that is inspired by a result of [16]. The analysis of PBM-PIE provided below confirms the relevance of the lower bound derived in Section 3.\nPBM-UCB The first algorithm simply consists in sorting optimistic indices in decreasing order and pulling the corresponding first L arms [3]. To derive the expression of the required \u201cexploration bonus\u201d we use an upper confidence for \u03b8\u0302k(t) based on Hoeffding\u2019s inequality:\nUUCBk (t, \u03b4) = Sk(t)\nN\u0303k(t) +\n\u221a Nk(t)\nN\u0303k(t)\n\u221a \u03b4\n2N\u0303k(t) ,\nfor which a coverage bound is given by the next proposition, proven in Appendix C.\nProposition 8. Let k be any arm in {1, . . . ,K}, then for any \u03b4 > 0, P ( UUCBk (t, \u03b4) \u2264 \u03b8k ) \u2264 e\u03b4 log(t)e\u2212\u03b4.\nFollowing the ideas of [6], it is possible to obtain a logarithmic regret upper bound for this algorithm. The proof is given in Appendix D.\nTheorem 9. Let C(\u03ba) = min1\u2264l\u2264L[( \u2211L j=1 \u03baj) 2/l + ( \u2211l j=1 \u03baj) 2]/\u03ba2L and \u2206 = mina\u2208\u03c3(a\u2217)\\a\u2217 \u2206a, where \u03c3(a \u2217) denotes the permutations of the optimal action. Using PBM-UCB with \u03b4 = (1 + ) log(t) for some > 0, there exists a constant C0( ) independent from the model parameters such that the regret of PBM-UCB is bounded from above by\nE[R(T )] \u2264 C0( ) + 16(1 + )C(\u03ba) log T\n( L\n\u2206 + \u2211 k/\u2208a\u2217\n1\n\u03baL(\u03b8L \u2212 \u03b8k)\n) .\nThe presence of the term L/\u2206 in the above expression is attributable to limitations of the mathematical analysis. On the other hand, the absence of the KL-divergence terms appearing in the lower bound (6) is due to the use of an upper confidence bound based on Hoeffding\u2019s inequality.\nPBM-PIE We adapt the PIE(l) algorithm introduced by [5] for the Cascade Model to the PBM in Algorithm 1 below. At each round, the learner potentially explores at position L with probability 1/2 using the following upper-confidence bound for each arm k\nUk(t, \u03b4) = sup q\u2208[\u03b8mink (t),1]\n{ q \u2223\u2223\u2223\u2223\u2223 L\u2211 l=1 Nk,l(t)d ( Sk,l(t) Nk,l(t) , \u03balq ) \u2264 \u03b4 } , (7)\nwhere \u03b8mink (t) is the minimum of the convex function \u03a6 : q 7\u2192 \u2211L l=1Nk,l(t)d(Sk,l(t)/Nk,l(t), \u03balq). In other positions, l = 1, . . . , L\u2212 1, PBM-PIE selects the arms with the largest estimates \u03b8\u0302k(t). The resulting algorithm is presented as Algorithm 1 below, denoting by L(t) the L-largest empirical estimates, referred to as the \u201cleaders\u201d at round t.\nAlgorithm 1 \u2013 PBM-PIE Require: K, L, observation probabilities \u03ba, > 0\nInitialization: first K rounds, play each arm at every position for t = K + 1, . . . , T do\nCompute \u03b8\u0302k(t) for all k L(t)\u2190 top-L ordered arms by decreasing \u03b8\u0302k(t) Al(t)\u2190 Ll(t) for each position l < L B(t)\u2190 {k|k /\u2208 L(t), Uk(t, (1 + ) log(T )) \u2265 \u03b8\u0302LL(t)(t) if B(t) = \u2205 then AL(t)\u2190 LL(t) else With probability 1/2, select AL(t) uniformly at random from B(t), else AL(t)\u2190 LL(t) end if Play action A(t) and observe feedback Z(t); Update Nk,l(t+ 1) and Sk,l(t+ 1).\nend for\nThe Uk(t, \u03b4) index defined in (7) aggregates observations from all positions \u2013 as in PBM-UCB \u2013 but allows to build tighter confidence regions as shown by the next proposition proved in Appendix E.\nProposition 10. For all \u03b4 \u2265 L+ 1, P (Uk(t, \u03b4) < \u03b8k) \u2264 eL+1 ( d\u03b4 log(t)e \u03b4\nL\n)L e\u2212\u03b4.\nWe may now state the main result of this section that provides an upper bound on the regret of PBM-PIE.\nTheorem 11. Using PBM-PIE with \u03b4 = (1 + ) log(t) and > 0, for any \u03b7 < mink<K(\u03b8k \u2212 \u03b8k+1)/2, there exist problem-dependent constants C1(\u03b7), C2( , \u03b7), C3( ) and \u03b2( , \u03b7) such that\nE[R(T )] \u2264 (1 + )2 log(T ) K\u2211\nk=L+1\n\u03baL(\u03b8L \u2212 \u03b8k) d(\u03baL\u03b8k, \u03baL(\u03b8L \u2212 \u03b7)) + C1(\u03b7) + C2( , \u03b7) T \u03b2( ,\u03b7) + C3( ).\nThe proof of this result is provided in Appendix E. Comparing to the expression in (6), Theorem 11 shows that PBM-PIE reaches asymptotically optimal performance when the optimal exploring position is indeed located at index L. In other case, there is a gap that is caused by the fact the exploring position is fixed beforehand and not adapted from the data.\nWe conclude this section by a quick description of two other algorithms that will be used in the experimental section to benchmark our results.\nRanked Bandits (RBA-KL-UCB) The state-of-the-art algorithm for the sequential \u201clearning to rank\u201d problem was proposed by [17]. It runs one bandit algorithm per position, each one being entitled to choose the best suited arm at its rank. The underlying bandit algorithm that runs in each position is left to the choice of the user, the better the policy the lower the regret can be. If the bandit algorithm at position l selects an arm already chosen at a higher position, it receives a reward of zero. Consequently, the bandit algorithm operating at position l tends to focus on the estimation of l-th best arm. In the next section, we use as benchmark the Ranked Bandits strategy using the KL-UCB algorithm [9] as the per-position bandit.\nPBM-TS The observations Zl(t) are censored Bernoulli which results in a posterior that does not belong to a standard family of distribution. [12] suggest a version of Thompson Sampling called \u201cBias Corrected Multiple Play TS\u201d (or BC-MP-TS) that approximates the true posterior by a Beta distribution. We observed in experiments that for parameter values close to one, this algorithm does not explore enough. In Figure 1(a), we show this phenomenon for \u03b8 = (0.95, 0.85, 0.75, 0.65, 0.55). The true posterior for the parameter \u03b8k at time t may be written as a product of truncated scaled beta distributions\n\u03c0t(\u03b8k) \u221d \u220f l \u03b8 \u03b1k,l(t) k (1\u2212 \u03bal\u03b8k) \u03b2k,l(t),\nwhere \u03b1k,l(t) = Sk,l(t) and \u03b2k,l(t) = Nk,l(t)\u2212Sk,l(t). To draw from this exact posterior, we use rejection sampling with proposal distribution Beta(\u03b1k,m(t), \u03b2k,m(t))/\u03bam, where m = arg max1\u2264l\u2264L(\u03b1k,l(t) + \u03b2k,l(t))."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Simulations", "text": "In order to evaluate our strategies, a simple problem is considered in which K = 5, L = 3, \u03ba = (0.9, 0.6, 0.3) and \u03b8 = (0.45, 0.35, 0.25, 0.15, 0.05). The arm expectations are chosen such that the asymptotic behavior can be observed after reasonable time horizon. All results are averaged based on 10, 000 independent runs of the algorithm. We present the results in Figure 1(b) where PBM-UCB, PBM-PIE and PBM-TS are compared to RBA-KL-UCB. The performance of PBM-PIE and PBM-TS are comparable, the latter even being under the lower bound (it is a common observation, e.g. see [12], and is due to the asymptotic nature of the lower bound). The curves confirm our analysis for PBM-PIE and lets us conjecture that the true Thompson Sampling policy might be asymptotically optimal. As expected, PBM-PIE shows asymptotically optimal performance, matching the lower bound after a large enough horizon."}, {"heading": "5.2 Real data experiments: search advertising", "text": "The dataset was provided for KDD Cup 2012 track 2 1 and involves session logs of soso.com, a search engine owned by Tencent. It consists of ads that were inserted among search results. Each of the 150M lines from the log contains the user ID, the query typed, an ad, a position (1, 2 or 3) at which it was displayed and a binary reward (click/no-click). First, for every query, we excluded ads that were not displayed at least 1, 000 times at every position. We also filtered queries that had less than 5 ads satisfying the previous constraints. As a\n1http://www.kddcup2012.org/\nresult, we obtained 8 queries with at least 5 and up to 11 ads. For each query q, we computed the matrix of the average click-through rates (CTR): Mq \u2208 RK\u00d7L, where K is the number of ads for the query q and L = 3 the number of positions. It is noticeable that the SVD of each Mq matrix has a highly dominating first singular value, therefore validating the low-rank assumption underlying in the PBM. In order to estimate the parameters of the problem, we used the EM algorithm suggested by [4, 8]. Table 1 reports some statistics about the bandit models reconstructed for each query: number of arms K, amount of data used to compute the parameters, minimum and maximum values of the \u03b8\u2019s for each model.\nWe conducted a series of 2, 000 simulations over this dataset. At the beginning of each run, a query was randomly selected together with corresponding probabilities of scanning positions and arm expectations. Even if rewards were still simulated, this scenario is more realistic since the values of the parameters were extracted from a real-world dataset. We show results for the different algorithms in Figure 2. It is remarkable that RBA-KL-UCB performs slightly better than PBM-UCB. One can imagine that PBM-UCB does not benefit enough from position aggregations \u2013 only 3 positions are considered \u2013 to beat RBA-KL-UCB. Both of them are outperformed by PBM-TS and PBM-PIE."}, {"heading": "Conclusion", "text": "This work provides the first complete analysis of the PBM in an online context. The proof scheme used to obtain the lower bound on the regret is interesting on its own, as it can be generalized to various other settings. The tightness of the lower bound is validated by our analysis of PBM-PIE but it would be an interesting future contribution to provide such guarantees for more straightforward algorithms such as PBM-TS or a \u2018PBM-KLUCB\u2019 using the confidence regions of PBM-PIE. In practice, the algorithms are robust to small variations of the values of the (\u03bal)l, but it would be preferable to obtain some control over the regret under uncertainty on these examination parameters."}, {"heading": "A Properties of \u03b8\u0302k(t) (Section 2)", "text": "Conditionnally to the actions A(1) up to A(t\u2212 1), the log-likelihood of the observations Z(1), . . . , Z(t\u2212 1) may be written as\nt\u22121\u2211 s= K\u2211 k=1 L\u2211 l=1 1{Al(t) = k} [Zl(t) log(\u03bal\u03b8k) + (1\u2212 Zl(t)) log(1\u2212 \u03bal\u03b8k)]\n= K\u2211 k=1 L\u2211 l=1 Sk,l(t) log(\u03bal\u03b8k) + (Nk,l(t)\u2212 Sk,l(t)) log(1\u2212 \u03bal\u03b8k).\nDifferenciating twice with respect to \u03b8k and taking the expectation of (Sk,l(t))l, contional to A(1), . . . , A(t\u2212 1), yields the expression of I(\u03b8k) given in Section 2."}, {"heading": "B Proof of Theorem 4", "text": ""}, {"heading": "B.1 Proof of Lemma 2", "text": "Under the PBM, the conditional expectation of the log-likelihood ratio defined in (4) writes\nE\u03b8[`(t)|A(1), . . . , A(t)] = E\u03b8\n[ t\u2211\ns=1 \u2211 a\u2208A 1{A(s) = a} L\u2211 l=1 log pal(Xl(s)Yl(s); \u03b8) pal(Xl(s)Yl(s);\u03bb) \u2223\u2223\u2223\u2223\u2223 A(1), . . . , A(t) ]\n= t\u2211 s=1 \u2211 a\u2208A 1{A(s) = a} L\u2211 l=1 E [ log pal(Xl(s)Yl(s); \u03b8) pal(Xl(s)Yl(s);\u03bb) \u2223\u2223\u2223\u2223 A(s) = a]\n= \u2211 a\u2208A Na(t) L\u2211 l=1 K\u2211 k=1 1{al = k}d(\u03bal\u03b8k, \u03bal\u03bbk)\n= \u2211 a\u2208A Na(t)Ia(\u03b8, \u03bb),\nusing the notation Ia(\u03b8, \u03bb) = \u2211L l=1 \u2211K k=1 1{al = k}d(\u03bal\u03b8k, \u03bal\u03bbk)."}, {"heading": "B.2 Details on the proof of Proposition 3", "text": "Lemma 12. Let \u03b8 = (\u03b81, . . . , \u03b8K) and \u03bb = (\u03bb1, . . . , \u03bbK) be two bandit models such that the distributions of all arms in \u03b8 and \u03bb are mutually absolutely continuous. Let \u03c3 be a stopping time with respect to (Ft) such that (\u03c3 < +\u221e) a.s. under both models. Let E \u2208 F\u03c3 be an event such that 0 < P\u03b8(E) < 1. Then one has\u2211\na\u2208A Ia(\u03b8, \u03bb)E\u03b8[Na(\u03c3)] \u2265 d(P\u03b8(E),P\u03bb(E)),\nwhere Ia(\u03b8, \u03bb) is the conditional expectation of the log-likelihood ratio for the model of interest.\nThe proof of this lemma directly follows from the above expressions of the log-likelihood ratio and from the proof of Lemma 1 in Appendix A.1 of [11].\nWe simply recall the following technical lemma for completeness.\nLemma 13. Let \u03c3 be any stopping time with respect to (Ft). For every event A \u2208 F\u03c3,\nP\u03bb(A) = E\u03b8[1{A} exp(\u2212`(\u03c3))].\nA full proof of Lemma 13 can be found in the Appendix A.3 of [11] (proof of Lemma 15)."}, {"heading": "B.3 Lower bound proof (Theorem 4)", "text": "In order to prove the simplified lower bound of Theorem 4 we basically have two arguments:\n1. a lower bound on f(\u03b8) can be obtained by enlarging the feasible set, that is by relaxing some constraints;\n2. Lemma 15 can be used to lower bound the objective function of the problem.\nThe constant f(\u03b8) is defined by\nf(\u03b8) = inf c 0 \u2211 a6=a\u2217(\u03b8) \u2206a(\u03b8)ca (8)\ns.t inf \u03bb\u2208B(\u03b8) \u2211 a\u2208A Ia(\u03b8, \u03bb)ca \u2265 1. (9)\nWe begin by relaxing some constraints: we only allow the change of measure \u03bb to belong to the sets Bk(\u03b8) := {\u03bb \u2208 \u0398|\u2200j 6= k, \u03b8j = \u03bbj and \u00b5\u2217(\u03b8) < \u00b5\u2217(\u03bb)} defined in Section 3:\nf(\u03b8) = inf c 0 \u2211 a6=a\u2217(\u03b8) \u2206a(\u03b8)ca (10)\ns.t \u2200k /\u2208 a\u2217(\u03b8), \u2200\u03bb \u2208 Bk(\u03b8), \u2211 a\u2208A Ia(\u03b8, \u03bb)ca \u2265 1. (11)\nThe K \u2212 L constraints (11) only let one parameter move and must be true for any value satisfying the definition of the corresponding set Bk(\u03b8). In practice, for each k, the parameter \u03bbk must be set to at least \u03b8L. Consequently, these constraints may then be rewritten\nf(\u03b8) = inf c 0 \u2211 a6=a\u2217(\u03b8) \u2206a(\u03b8)ca (12)\ns.t \u2200k /\u2208 a\u2217(\u03b8), \u2211\na 6=a\u2217(\u03b8)\nca L\u2211 l=1 1{al = k}d(\u03bal\u03b8k, \u03bal\u03b8L) \u2265 1. (13)\nProposition 14 tells us that coefficients ca are all zeros except for actions a \u2208 A which can be written a = vk,lk where lk = arg minl\u2264L \u2206vk,l (\u03b8) d(\u03bal\u03b8k,\u03bal\u03b8L) . Thus, we obtain the desired lower bound by rewriting (12) as\nf(\u03b8) \u2265 K\u2211\nk=L+1\nmin l\u2208{1,...,L}\n\u2206vk,l(\u03b8)\nd(\u03bal\u03b8k, \u03bal\u03b8L) .\nProposition 14. Let c = {ca : a 6= a\u2217} be a solution of the linear problem (LP) in Theorem 4. Coefficients are all zeros except for actions a which can be written as a = (1, . . . , lk \u2212 1, k, lk, . . . , L\u2212 1) := vk,lk where k > L and lk = arg minl\u2264L \u2206vk,l (\u03b8) d(\u03bal\u03b8k,\u03bal\u03b8L) .\nProof. We denote by \u03c0k(a) the position of item k \u2208 {1, . . . ,K} in action a (0 if k /\u2208 a). Let lk be the optimal position of item k > L for exploration: lk = arg minl\u2264L \u2206vk,l (\u03b8) d(\u03bal\u03b8k,\u03bal\u03b8L) . Following [5], we show by contradiction that ca > 0 implies that a can be written vk,lk for a well chosen k > L. Let \u03b1 6= a\u2217 be a suboptimal action such that \u2200k > L,\u03b1 6= vk,lk and c\u03b1 > 0. We need to show a contradiction. Let us introduce a new set of coefficients c\u2032 defined as follows, for any a 6= a\u2217:\nc\u2032a =  0 if a = \u03b1 ca + d(\u03ba\u03c0k(\u03b1)\u03b8k,\u03ba\u03c0k(\u03b1)\u03b8L) d(\u03balk\u03b8k,\u03balk\u03b8L) c\u03b1 if \u2203k > L s.t. a = vk,lk and k \u2208 \u03b1\nca otherwise.\nAccording to Lemma 15, these coefficients satisfy the constraints of the LP. We now show that these new coefficients yield a strictly lower value to the optimization problem:\nc(\u03b8)\u2212 c\u2032(\u03b8) = c\u03b1\u2206\u03b1(\u03b8)\u2212 \u2211\nk>L:k\u2208\u03b1\nd(\u03ba\u03c0k(\u03b1)\u03b8k, \u03ba\u03c0k(\u03b1)\u03b8L)\nd(\u03balk\u03b8k, \u03balk\u03b8L) c\u03b1\u2206vk,lk (\u03b8)\n> c\u03b1 ( \u2211 k>L:k\u2208\u03b1 \u2206vk,\u03c0k(\u03b1)(\u03b8)\u2212 \u2211 k>L:k\u2208\u03b1 d(\u03ba\u03c0k(\u03b1)\u03b8k, \u03ba\u03c0k(\u03b1)\u03b8L) d(\u03balk\u03b8k, \u03balk\u03b8L) \u2206vk,lk (\u03b8) ) . (14)\nThe strict inequality (14) is shown in Lemma 16. Let k > L be one of the suboptimal arms in \u03b1. By definition of lk, the corresponding term of the sum in equation (14) is positive. Thus, we have that c(\u03b8) > c\u2032(\u03b8) and, hence, by contradiction, we showed that ca > 0 iff a can be written a = vk,lk for some k > L.\nLemma 15. Let c be a vector of coefficients that satisfy constraints (13) of the optimization problem. Then, coefficients c\u2032 as defined in Proposition 14 also satisfy the constraints:\n\u2200k /\u2208 a\u2217(\u03b8), \u2211\na6=a\u2217(\u03b8)\nc\u2032a L\u2211 l=1 1{al = k}d(\u03bal\u03b8k, \u03bal\u03b8L) \u2265 1.\nProof. We use the same \u03b1 as introduced in Proposition 14. Let us fix k /\u2208 a\u2217(\u03b8). Let us define\nL(c) = \u2211\na6=a\u2217(\u03b8)\nca L\u2211 l=1 1{al = k}d(\u03bal\u03b8k, \u03bal\u03b8L).\nWe have\nL(c\u2032)\u2212 L(c) = \u2212c\u03b1 L\u2211 l=1 1{\u03b1l = k}d(\u03bal\u03b8k, \u03bal\u03b8L) + \u2211 l:\u03b1l>L d(\u03bal\u03b8k, \u03bal\u03b8L) d(\u03balk\u03b8k, \u03balk\u03b8L) c\u03b1\n\u00d7 1{\u03b1l = k}d(\u03balk\u03b8k, \u03balk\u03b8L).\nIf k /\u2208 \u03b1, clearly, L(c\u2032)\u2212 L(c) = 0. Else, k \u2208 \u03b1 and we note p its position in \u03b1: p = \u03c0k(\u03b1). We rewrite:\nL(c\u2032)\u2212 L(c) = c\u03b1d(\u03bap\u03b8k, \u03bap\u03b8L) ( \u22121 + d(\u03balk\u03b8k, \u03balk\u03b8L)\nd(\u03balk\u03b8k, \u03balk\u03b8L)\n) = 0.\nThus, the coefficients c\u2032 satisfy the constraints from Proposition 14.\nLemma 16. Let \u03b1 be as in the proof of Proposition 14. \u2206\u03b1(\u03b8) > \u2211\nk>L:k\u2208\u03b1\n\u2206vk,\u03c0k(\u03b1)(\u03b8).\nProof. Let k1, . . . , kp be the suboptimal arms in \u03b1 by increasing position. Let v(\u03b1) be the action in A with lower regret such that it contains all the suboptimal arms of \u03b1 in the same positions. Thus, v(\u03b1) = (1, . . . , \u03c0k1(\u03b1)\u2212 1, k1, \u03c0k1(\u03b1), . . . , \u03c0k2(\u03b1) \u2212 2, k2, \u03c0k2(\u03b1) \u2212 1, . . . , L \u2212 p). By definition, one has that \u2206\u03b1(\u03b8) \u2265 \u2206v(\u03b1)(\u03b8). In the following, we show that \u2206v(\u03b1)(\u03b8) \u2265 \u2211 k>L:k\u2208\u03b1 \u2206vk,\u03c0k(\u03b1)(\u03b8) for p = 2 (that is to say \u03b1 contains 2 suboptimal arms k1 and k2). For the sake of readability, we write \u03c0i instead of \u03c0ki(\u03b1) in the following.\n\u2206v(\u03b1)(\u03b8) = L\u2211 l=1 \u03bal(\u03b8l \u2212 \u03b8(vk1,\u03c01 )l) + L\u2211 l=1 \u03bal(\u03b8(vk1,\u03c01 )l \u2212 \u03b8v(\u03b1)l)\n= \u2206vk1,\u03c01 (\u03b8) + [\u03ba\u03c02\u03b8\u03c02\u22121 + . . .+ \u03baL\u03b8L\u22121]\u2212 [\u03ba\u03c02\u03b8k2 + \u03ba\u03c02+1\u03b8\u03c02\u22121 + . . .+ \u03baL\u03b8L\u22122] = \u2206vk1,\u03c01 (\u03b8) + \u2206vk2,\u03c02 (\u03b8) + [\u03ba\u03c02(\u03b8\u03c02\u22121 \u2212 \u03b8\u03c02) + . . .+ \u03baL(\u03b8L\u22121 \u2212 \u03b8L)]\u2212\n[\u03ba\u03c02+1(\u03b8\u03c02\u22121 \u2212 \u03b8\u03c02) + . . .+ \u03baL(\u03b8L\u22122 \u2212 \u03b8L\u22121)]\n= \u2206vk1,\u03c01 (\u03b8) + \u2206vk2,\u03c02 (\u03b8) +R(\u03b8).\nThus, one has to show thatR(\u03b8) = \u03ba\u03c02(\u03b8\u03c02\u22121\u2212\u03b8\u03c02)+\u03ba\u03c02+1(2\u03b8\u03c02\u2212\u03b8\u03c02\u22121\u2212\u03b8\u03c02+1)+. . .+\u03baL(2\u03b8L\u22121\u2212\u03b8L\u22122\u2212\u03b8L) > 0. In fact, using that \u03bal \u2265 \u03bal+1 for all l < L, we have\nR(\u03b8) \u2265 \u03ba\u03c02+1(\u03b8\u03c02\u22121 \u2212 \u03b8\u03c02 + 2\u03b8\u03c02 \u2212 \u03b8\u03c02\u22121 \u2212 \u03b8\u03c02+1) + . . .+ \u03baL(2\u03b8L\u22121 \u2212 \u03b8L\u22122 \u2212 \u03b8L) \u2265 \u03ba\u03c02+2(\u03b8\u03c02+1 \u2212 \u03b8\u03c02+2) + . . .+ \u03baL(2\u03b8L\u22121 \u2212 \u03b8L\u22122 \u2212 \u03b8L) \u2265 . . . \u2265 \u03baL(\u03b8L\u22121 \u2212 \u03b8L) > 0."}, {"heading": "C Proof of Proposition 8", "text": "In this section, we fix an arm k \u2208 {1, . . . ,K} and obtain an upper confidence bound for the estimator \u03b8\u0302k(t) := Sk(t)/N\u0303k(t). Let \u03c4i be the instant of the i-th draw of arm k (the \u03c4i are stopping times w.r.t. Ft). We introduce the centered sequence of successive observations from arm k\nZ\u0304k,i = L\u2211 l=1 1{Al(\u03c4i) = k}(Xl(\u03c4i)Yl(\u03c4i)\u2212 \u03b8k\u03bal). (15)\nIntroducing the filtration Gi = F\u03c4i+1\u22121, one has E[Z\u0304k,i|Gi\u22121] = 0, and therefore, the sequence\nMk,n = n\u2211 i=1 Z\u0304k,i\nis a martingale with bounded increments, w.r.t. the filtration (Gn)n. By construction, one has\nMk,Nk(t) = Sk(t)\u2212 N\u0303k(t)\u03b8k = N\u0303k(t)(\u03b8\u0302k(t)\u2212 \u03b8k).\nWe use the so-called peeling technique together with the maximal version of Azuma-Hoeffding\u2019s inequality [2]. For any \u03b3 > 0 one has\nP ( Mk,Nk(t) < \u2212 \u221a Nk(t)\u03b4/2 ) \u2264 log(t) log(1+\u03b3)\u2211 i=1 P ( Mk,Nk(t) < \u2212 \u221a Nk(t)\u03b4/2 , Nk(t) \u2208 [(1 + \u03b3)i\u22121, (1 + \u03b3)i) )\n\u2264 log(t) log(1+\u03b3)\u2211 i=1 P ( \u2203i \u2208 {1, . . . , (1 + \u03b3)i} : Mk,i < \u2212 \u221a (1 + \u03b3)i\u22121\u03b4/2 )\n\u2264 log(t) log(1+\u03b3)\u2211 i=1 exp ( \u2212\u03b4(1 + \u03b3) i\u22121 (1 + \u03b3)i ) = log(t) log(1 + \u03b3) exp ( \u2212 \u03b4 (1 + \u03b3) ) .\nChoosing \u03b3 = 1/(\u03b4 \u2212 1), gives\nP ( \u03b8\u0302k(t)\u2212 \u03b8k < \u2212 \u221a Nk(t)\u03b4/2\nN\u0303k(t)\n) \u2264 \u03b4e log(t)e\u2212\u03b4.\nD Regret analysis for PBM-UCB (Theorem 9) We proceed as Kveton et al. (2015) [14]. We start by considering separately rounds when one of the confidence intervals is violated. We denote by Bt,k = \u221a Nk(t)(1 + ) log t/2/N\u0303k(t) the PBM-UCB exploration bonus\nand by B+t,k = \u221a Nk(t)(1 + ) log T/2/N\u0303k(t) an upper bound of this bonus (for t \u2264 T ). We define the event Et = {\u2203k \u2208 A(t) : |\u03b8\u0302k(t)\u2212 \u03b8k| > Bt,k}. Then, the regret can be decomposed into\nR(T ) = T\u2211 t=1 \u2206A(t)1Et + \u2206A(t)1E\u0304t .\nand, similarly to [14] (Appendix A.1), the first term of this sum can be bounded from above in expectation by a constant C0( ) that does not depend on T using Proposition 8. So, it remains to bound the regret suffered even when confidence intervals are respected, that is the sum on the r.h.s of\nE[R(T )] < C0( ) + E[ T\u2211 t=1 \u2206A(t)1{E\u0304t,\u2206A(t) > 0}].\nIt can be done using techniques from [6, 14]. We start by defining events Ft, Gt, Ht in order to decompose the part of the regret at stake. Then, we show an equivalent of Lemma 2 of [14] for our case and finally we refer to the proof of Theorem 3 in Appendix A.3 of [14].\nFor each round t \u2265 1, we define the set of arms St = {1 \u2264 l \u2264 L : NAl(t)(t) \u2264 8(1+ ) log T(\n\u2211L s=1 \u03bas) 2\n\u03ba2L\u2206 2 A(t) } and the related events\n\u2022 Ft = {\u2206A(t) > 0, \u2206A(t) \u2264 2 \u2211L l=1 \u03balB + t,Al(t) };\n\u2022 Gt = {|St| \u2265 l};\n\u2022 Ht = {|St| < l , \u2203k \u2208 A(t), Nk(t) \u2264 8(1+ ) log T(\n\u2211l s=1 \u03bas) 2\n\u03ba2L\u2206 2 A(t) }, where the constraint on Nk(t) only differs from the first one by its numerator which is smaller than the previous one, leading to an even stronger constraint.\nFact 17. According to Lemma 1 in [14], the following inequality is still valid with our own definition of Ft :\nT\u2211 t=1 \u2206A(t)1{E\u0304t,\u2206A(t) > 0} \u2264 T\u2211 t=1 \u2206A(t)1{Ft}.\nProof. Invoking Lemma 1 from [14] needs to be justified as our setting is quite different. Taking action A(t) means that\nL\u2211 l=1 \u03balUAl(t)(t) \u2265 L\u2211 l=1 \u03balUl(t).\nUnder event E\u0304t, all UCB\u2019s are above the true parameter \u03b8k so we have\nL\u2211 l=1 \u03bal(\u03b8Al(t) + 2Bt,Al(t)) \u2265 L\u2211 l=1 \u03bal(\u03b8l +Bt,l) \u2265 L\u2211 l=1 \u03bal\u03b8l.\nRearranging the terms above and using Bt,l(t) \u2264 B+t,l(t), we obtain\nL\u2211 l=1 \u03balB + t,Al(t) \u2265 2 L\u2211 l=1 \u03balBt,Al(t) \u2265 \u2206A(t).\nWe now have to prove an equivalent of Lemma 2 in [6] that would allow us to split the right-hand side above in two parts. Let us show that Ft \u2282 (Gt \u222aHt) by showing its contrapositive: if Ft is true then we cannot have (G\u0304t \u2229 H\u0304t). Assume both of these events are true. Then, we have\n\u2206A(t) Ft \u2264 2 L\u2211 l=1 \u03balB + t,Al(t)\n\u2264 2 L\u2211 l=1 \u03bal\n\u221a NAl(t)(t)\nN\u0303Al(t)(t)\n\u221a (1 + ) log(T )\n2N\u0303Al(t)(t)\n= 2 L\u2211 l=1 \u03bal NAl(t)(t) N\u0303Al(t)(t)\n\u221a (1 + ) log(T )\n2NAl(t)(t) \u2264 \u221a 2(1 + ) log T\n\u03baL\nL\u2211 l=1 \u03bal\u221a NAl(t)(t)\n=\n\u221a 2(1 + ) log T\n\u03baL\n\u2211 l/\u2208St \u03bal\u221a NAl(t)(t) + \u2211 l\u2208St \u03bal\u221a NAl(t)(t)  (G\u0304t\u2229H\u0304t) < \u221a 2(1 + ) log T\n\u03baL\n\u03baL\u2206A(t) 2 \u221a 2(1 + ) log T (\u2211 l/\u2208St \u03bal\u2211L s=1 \u03bas + \u2211 l\u2208St \u03bal\u2211l s=1 \u03bas ) \u2264 \u2206A(t)\nwhich is a contradiction. The end of the proof proceeds exactly as in the end of the proof of Theorem 6 in of [6]: events Gt and Ht are split into subevents corresponding to rounds where each specific suboptimal arm of the list is in St or verifies the condition of Ht. We define\nGk,t = Gt \u2229 {k \u2208 A(t), Nk(t) \u2264 8(1 + ) log T\n(\u2211L s=1 \u03bas )2 \u03ba2L\u2206 2 A(t) },\nHk,t = Ht \u2229 {k \u2208 A(t), Nk(t) \u2264 8(1 + ) log T\n(\u2211l s=1 \u03bas )2 \u03ba2L\u2206 2 A(t) }.\nThe way we defined these subevents allows to write the two following bounds :\nK\u2211 k=1 1{Gk,t} = 1{Gt} K\u2211 k=1 1{k \u2208 St} \u2265 l1{Gt}\nso 1{Gt} \u2264 \u2211 k 1{Gk,t}/l. And,\n1{Ht} \u2264 K\u2211 k=1 1{Hk,t}.\nWe can now bound the regret using these two results:\nT\u2211 t=1 \u2206A(t)(1{Gt}+ 1{Ht}) \u2264 T\u2211 t=1 K\u2211 k=1 \u2206A(t) l 1{Gk,t}+ T\u2211 t=1 K\u2211 k=1 \u2206A(t)1{Hk,t}\n= T\u2211 t=1 K\u2211 k=1 \u2206A(t) l 1{Gk,t, A(t) 6= a\u2217}+ T\u2211 t=1 K\u2211 k=1 \u2206A(t)1{Hk,t, A(t) 6= a\u2217}.\nFor each arm k, there is a finite number Ck := |Ak| of actions in A containing k; we order them such that the corresponding gaps are in decreasing order \u2206k,1 \u2265 . . . \u2265 \u2206k,Ck > 0. So we decompose each sum above on the different actions A(t) possible:\n. . . \u2264 T\u2211 t=1 K\u2211 k=1 \u2211 a\u2208Ak \u2206k,a l 1{Gk,t, A(t) = a}+ T\u2211 t=1 K\u2211 k=1 \u2211 a\u2208Ak \u2206k,a1{Hk,t, A(t) = a}.\nThe two sums on the right hand side look alike. For arm k fixed, events Gk,t and Hk,t imply almost the same condition on Nk(t), only Hk,t is stronger because the bounding term is smaller. We now rely on a technical result by [6] that allows to bound each sum.\nLemma 18. ([6], Lemma 2 in Appendix B.4) Let k be a fixed item and |Ak| \u2265 1, C > 0, we have\nT\u2211 t=1 \u2211 a\u2208Ak 1{k \u2208 A(t), Nk(t) \u2264 C/\u22062k,a, A(t) = a}\u2206k,a \u2264 2C \u2206min,k\nwhere \u2206min,k is the smallest gap among all suboptimal actions containing arm k. In particular, when k /\u2208 a\u2217 the smallest gap is \u2206min,k = \u03baL(\u03b8L\u2212 \u03b8k). While, when k \u2208 a\u2217 it is less obvious what the minimal gap is, however it corresponds the second best action A2 containing only optimal arms: \u2206min,k = \u2206A2 .\nSo, bounding each sum with the above lemma, we obtain\nT\u2211 t=1 \u2206A(t)(1{Gt}+ 1{Ht}) \u2264 16(1 + ) log T \u03ba2L\n (\u2211L s=1 \u03bas )2 l + ( l\u2211\ns=1\n\u03bas )2 \ufe38 \ufe37\ufe37 \ufe38\nC(l;\u03ba)\n( L\n\u2206A2 + \u2211 k/\u2208a\u2217\n1\n\u03baL(\u03b8L \u2212 \u03b8k)\n) .\nThis bound can be optimized by minimizing C(l;\u03ba) over l.\nE Regret analysis for PBM-PIE (Theorem 11) The proof follows the decomposition of [5]. For all t \u2265 1, we denote f(t, ) = (1 + ) log t."}, {"heading": "E.1 Controlling leaders and estimations", "text": "Define \u03b70 = mink\u2208{1,...,L\u22121}(\u03b8k \u2212 \u03b8k+1)/2 and let \u03b7 < \u03b70. We define the following set of rounds\nA = {t \u2265 1 : L(t) 6= (1, . . . , L)}.\nOur goal is to upper bound the expected size of A. Let us introduce the following sets of rounds:\nB = {t \u2265 1 : \u2203k \u2208 L(t), |\u03b8\u0302k(t)\u2212 \u03b8k| \u2265 \u03b7}, C = {t \u2265 1 : \u2203k \u2264 L,Uk(t) \u2264 \u03b8k},\nD = {t \u2265 1 : t \u2208 A \\ (B \u222a C),\u2203k \u2264 L, k /\u2208 L(t), |\u03b8\u0302k(t)\u2212 \u03b8k| \u2265 \u03b7}.\nWe first show that A \u2282 (B \u222a C \u222aD). Let t \u2208 A \\ (B \u222a C). Let k, k\u2032 \u2208 L(t) such that k < k\u2032. Since t /\u2208 B, we have that |\u03b8\u0302k(t)\u2212 \u03b8k| \u2264 \u03b7 and |\u03b8\u0302k\u2032(t)\u2212 \u03b8k\u2032 | \u2264 \u03b7. Since \u03b7 \u2264 (\u03b8k \u2212 \u03b8k\u2032)/2, we conclude that \u03b8\u0302k(t) \u2265 \u03b8\u0302k\u2032(t). This proves that (L1(t), . . . ,LL(t) is an increasing sequence. We have that LL(t) > L otherwise L(t) = (1, . . . , L) which is a contradiction because t \u2208 A. Since LL(t) > L, there exists k \u2264 L such that k /\u2208 L(t). We show by contradiction that |\u03b8\u0302k(t) \u2212 \u03b8k| \u2265 \u03b7. Assume that |\u03b8\u0302k(t) \u2212 \u03b8k| \u2264 \u03b7. We also have that \u03b8\u0302LL(t)(t) \u2212 \u03b8LL(t) \u2264 \u03b7 because LL(t) \u2208 L(t) and t /\u2208 B. Thus, \u03b8\u0302k(t) > \u03b8\u0302LL(t)(t). We have a contradiction because this would imply that k \u2208 L(t). Finally we have proven that if t \u2208 A \\ (B \u222a C), then t \u2208 D so A \u2282 (B \u222a C \u222aD).\nBy a union bound, we obtain E[|A|] \u2264 [|B|] + [|C|] + [|D|].\nIn the following, we upper bound each set of rounds individually. Controlling E[|B|]: We decompose B = \u22c3K k=1(Bk,1 \u222aBk,2) where\nBk,1 = {t \u2265 1 : k \u2208 L(t),LL(t) 6= k, |\u03b8\u0302k(t)\u2212 \u03b8k| \u2265 \u03b7}\nBk,2 = {t \u2265 1 : k \u2208 L(t),LL(t) = k, |\u03b8\u0302k(t)\u2212 \u03b8k| \u2265 \u03b7}\nLet t \u2208 Bk,1: k \u2208 A(t) so E[k \u2208 A(t)|t \u2208 Bk,1] = 1. Furthermore, for all t, 1{t \u2208 Bk,1} is Ft\u22121 measurable. Then we can apply Lemma 22 (with H = Bk,1 and c = 1).\nE[|Bk,1|] \u2264 2(2 + \u03ba\u22122L \u03b7 \u22122).\nLet t \u2208 Bk,2: k \u2208 B(t) but because of the randomization of the algorithm, k \u2208 A(t) with probability 1/2, i.e. E[k \u2208 A(t)|t \u2208 Bk,2] \u2265 1/2. We get\nE[|Bk,2|] \u2264 4(4 + \u03ba\u22122L \u03b7 \u22122)\nBy union bound over k, we get E[|B|] \u2264 2K(10 + 3\u03ba\u22122L \u03b7\u22122).\nControlling E[|C|]: We decompose C = \u22c3L k=1 Ck where Ck = {t \u2265 1 : Uk(t) \u2264 \u03b8k}\nWe first require to prove Proposition 10.\nProof. Theorem 2 of [16] implies that\nP ( L\u2211 l=1 Nk,l(t)d( Sk,l(t) Nk,l(t) , \u03bal\u03b8k) \u2265 \u03b4 ) \u2264 e\u2212\u03b4 ( d\u03b4 log(t)e \u03b4 L )L eL+1.\nThe function \u03a6 : x\u2192 \u2211L l=1Nk,l(t)d ( Sk,l(t) Nk,l(t) , \u03balx ) is convex and non-decreasing on [\u03b8mink (t), 1]; the convexity is easily checked and \u03b8mink (t) is defined as the minimum of this convex function. By definition, we have, either, Uk(t, \u03b4) = 1 and then Uk(t, \u03b4) > \u03b8k, or, Uk(t, \u03b4) < 1 and \u03a6(Uk(t, \u03b4)) = \u03b4, consequently\nP (Uk(t, \u03b4) < \u03b8k) = P (\u03a6(Uk(t, \u03b4)) \u2264 \u03a6(\u03b8k)) = P (\u03b4 \u2264 \u03a6(\u03b8k)) .\nRemember that Uk(t) = Uk(t, (1 + ) log(t)) = Uk(t, f(t, )). Thus, applying Proposition 10, we obtain for arm k,\nE[|Ck|] \u2264 \u221e\u2211 t=1 P(Uk(t) \u2264 \u03b8k) \u2264 deL+1e+ eL+1 LL \u221e\u2211 t=deL+1e+1 (2 + )2L(log t)3L t1+ \u2264 C3( ),\nfor some constant C3( ). Controlling E[|D|]: Decompose D as D = \u22c3L k=1Dk where\nDk = {t \u2265 1 : t \u2208 A \\ (B \u222a C), k /\u2208 L(t), |\u03b8\u0302k(t)\u2212 \u03b8k| \u2265 \u03b7}.\nFor a given k \u2264 L, Dk is the set of rounds at which k is not one of the leaders, and is not accurately estimated. Let t \u2208 Dk. Since k /\u2208 L(t), we must have LL(t) > L. In turn, since t /\u2208 B, we have |\u03b8\u0302LL(t)(t)\u2212 \u03b8LL(t)| \u2264 \u03b7, so that\n\u03b8\u0302LL(t) \u2264 \u03b8LL(t) + \u03b7 \u2264 \u03b8L + \u03b7 \u2264 (\u03b8L + \u03b8L+1)/2.\nFurthermore, since t /\u2208 C and 1 \u2264 k \u2264 L, we have Uk(t) \u2265 \u03b8k \u2265 \u03b8L \u2265 (\u03b8L + \u03b8L+1)/2 \u2265 \u03b8\u0302LL(t). This implies that k \u2208 B(t) thus E[k \u2208 A(t)|t \u2208 Dk] \u2265 1/(2K). We apply Lemma 22 with H \u2261 Dk and c = 1/(2K) to get\nE[|D|] \u2264 L\u2211 k=1 E[|Dk|] \u2264 4K(4K + \u03ba\u22122L \u03b7 \u22122)."}, {"heading": "E.2 Regret decomposition", "text": "We decompose the regret by distinguishing rounds in A \u222aB and other rounds. More specifically, we introduce the following sets of rounds for arm k > L:\nEk = {t \u2265 1 : t /\u2208 (B \u222a C \u222aD),L(t) = a\u2217, A(t) = vk,L}.\nThe set of instants at which a suboptimal action is selected now can be expressed as follows\n{t \u2265 1 : A(t) 6= a\u2217} \u2282 (B \u222a C \u222aD) \u222a (\u222ak=L+1Ek).\nUsing a union bound, we obtain the upper bound\nE[R(T )] \u2264 ( L\u2211 l=1 \u03bal ) E[|B \u222a C \u222aD|] + K\u2211 k=L+1 \u2206vk,L(\u03b8)E[|Ek|].\nFrom previous boundaries, putting it all together, there exist C1(\u03b7) and C3( ), such that( L\u2211 l=1 \u03bal ) (E[|B|] + E[|C|] + E[|D|]) \u2264 C1(\u03b7) + C3( ).\nAt this step, it suffices to bound events Ek for all k > L."}, {"heading": "E.3 Bounding event Ek", "text": "We proceed similarly to [9]. Let us fix an arm k > L. Let t \u2208 Ek: arm k is pulled in position L, so by construction of the algorithm, we have that k \u2208 B(t) and thus Uk(t) \u2265 \u03b8\u0302LL(t)(t). We first show that this implies that Uk(t) \u2265 \u03b8L \u2212 \u03b7. Since t \u2208 Ek, we know that LL(t) = L, and since t /\u2208 B, |\u03b8\u0302L(t)\u2212 \u03b8L| \u2264 \u03b7. This leads to\nUk(t) \u2265 \u03b8\u0302LL(t)(t) = \u03b8\u0302L(t) \u2265 \u03b8L \u2212 \u03b7.\nRecall thatNk,L(t) is the number of times arm k was played in position L. By denoting d+(x, y) = 1{x < y}d(x, y), we have that\nNk,L(t)d +(Sk,L(t)/Nk,L(t), \u03baL(\u03b8L \u2212 \u03b7)) \u2264 Nk,L(t)d+(Sk,L(t)/Nk,L(t), \u03baLUk(t))\n\u2264 L\u2211 l=1 Nk,l(t)d +(Sk,l(t)/Nk,l(t), \u03balUk(t)) \u2264 f(t, ).\nThis implies that 1{t \u2208 Ek} \u2264 1{Nk,L(t)d+(Sk,L(t)/Nk,L(t), \u03baL(\u03b8L \u2212 \u03b7)) \u2264 f(t, )}.\nLemma 19. ([9], Lemma 7) Denoting by \u03bd\u0302Lk,s the empirical mean of the first s samples of Zk,L, we have\nT\u2211 t=1 1{A(t) = vk,L, Nk,L(t)d+(Sk,L(t)/Nk,L(t), \u03baL(\u03b8L \u2212 \u03b7)) \u2264 f(t, )}\n\u2264 T\u2211 s=1 1{sd+(\u03bd\u0302Lk,s, \u03baL(\u03b8L \u2212 \u03b7)) \u2264 f(T, )}.\nWe apply Lemma 19 which is a direct translation of Lemma 7 from [9] to our problem. This yields\n|Ek| \u2264 T\u2211 s=1 1{sd+(\u03bd\u0302Lk,s, \u03baL(\u03b8L \u2212 \u03b7)) \u2264 f(T, )}.\nLet \u03b3 > 0. We define KT = (1+\u03b3)f(T, )\nd+(\u03baL\u03b8k,\u03baL(\u03b8L\u2212\u03b7)) . We now rewrite the last inequality splitting the sum in two parts.\nT\u2211 s=1 P(sd+(\u03bd\u0302Lk,s,\u03baL(\u03b8L \u2212 \u03b7)) \u2264 f(T, )) \u2264 KT + \u221e\u2211\ns=KT+1\nP(KT d +(\u03bd\u0302Lk,s, \u03baL(\u03b8L \u2212 \u03b7)) \u2264 f(T, ))\n\u2264 KT + \u221e\u2211\ns=KT+1\nP(d+(\u03bd\u0302Lk,s, \u03baL(\u03b8L \u2212 \u03b7)) \u2264 d(\u03baL\u03b8k, \u03baL(\u03b8L \u2212 \u03b7))/(1 + \u03b3))\n\u2264 KT + C2(\u03b3, \u03b7)\nT \u03b2(\u03b3,\u03b7) ,\nwhere last inequality comes from Lemma 20. Fixing \u03b3 < , we obtain the desired result, which concludes the proof.\nLemma 20. For each \u03b3 > 0, there exists C2(\u03b3, \u03b7) > 0 and \u03b2(\u03b3, \u03b7) > 0 such that\n\u221e\u2211 s=KT+1 P ( d+(\u03bd\u0302Lk,s, \u03baL(\u03b8L \u2212 \u03b7)) \u2264 d(\u03baL\u03b8k, \u03baL(\u03b8L \u2212 \u03b7) 1 + \u03b3 ) \u2264 C2(\u03b3, \u03b7) T \u03b2(\u03b3,\u03b7) .\nProof. If d+(\u03bd\u0302Lk,s, \u03baL(\u03b8L \u2212 \u03b7)) \u2264 d(\u03baL\u03b8k,\u03baL(\u03b8L\u2212\u03b7)) 1+\u03b3 , then there exists some r(\u03b3, \u03b7) \u2208 (\u03b8k, \u03b8L \u2212 \u03b7) such that \u03bd\u0302 L k,s > \u03baLr(\u03b3, \u03b7) and\nd(\u03baLr(\u03b3, \u03b7), \u03baL(\u03b8L \u2212 \u03b7)) = d(\u03baL\u03b8k, \u03baL(\u03b8L \u2212 \u03b7))\n1 + \u03b3 .\nHence,\nP ( d+(\u03bd\u0302k,s, \u03baL\u03b8L) < d(\u03baL\u03b8k, \u03baL\u03b8L)\n1 + \u03b3\n) \u2264 P (d(\u03bd\u0302k,s, \u03baL\u03b8k) > d(\u03baLr(\u03b3, \u03b7), \u03baL\u03b8k), \u03bd\u0302k,s > \u03baL\u03b8k)\n\u2264 P(\u03bd\u0302k,s > \u03baLr(\u03b3, \u03b7)) \u2264 exp(\u2212sd(\u03baLr(\u03b3, \u03b7), \u03baL\u03b8k)).\nWe obtain,\n\u221e\u2211 t=KT P ( d+(\u03bd\u0302k,s, \u03baL\u03b8L) < d(\u03baL\u03b8k, \u03baL\u03b8L) 1 + \u03b3 ) \u2264 exp(\u2212KT d(\u03baLr(\u03b3, \u03b7), \u03baL\u03b8k)) 1\u2212 exp(\u2212d(\u03baLr(\u03b3, \u03b7), \u03baL\u03b8k)) \u2264 C2(\u03b3, \u03b7) T \u03b2(\u03b3,\u03b7) ,\nfor well chosen C2(\u03b3, \u03b7) and \u03b2(\u03b3, \u03b7)."}, {"heading": "F Lemmas", "text": "In this section, we recall two necessary concentration lemmas directly adapted from Lemma 4 and 5 in Appendix A of [5]. Although more involved from a probabilistic point of view, these results are simpler to establish than proposition 8 as their adaptation to the case of the PBM relies on a crude lower bound for N\u0303k(t), which is sufficient for proving Theorem 11..\nLemma 21. For k \u2208 {1, . . . ,K} consider the martingale Mk,n = \u2211n i=1 Z\u0304k,i, where Z\u0304k,i is defined in (15). Consider \u03a6 a stopping time such that either Nk(\u03a6) \u2265 s or \u03a6 = T + 1. Then\nP[|Mk,Nk(\u03a6)| \u2265 Nk(\u03a6)\u03b7,Nk(\u03a6) \u2265 s] \u2264 2 exp(\u22122s\u03b7 2). (16)\nAs a consequence,\nP[|\u03b8\u0302k(\u03a6)\u2212 \u03b8k| \u2265 \u03b7, \u03a6 \u2264 T ] \u2264 2 exp(\u22122s\u03ba2L\u03b72). (17)\nProof. The first result is a direct application of Lemma 4 of [5] as (Zl(t))t with Zl(t) = Xl(t)Yl(t) is an independent sequence of [0, 1]-valued variables.\nFor the second inequality, we use the fact that N\u0303k(t) \u2265 \u03baLNk(t). Hence, P[|\u03b8\u0302k(\u03a6)\u2212 \u03b8k| \u2265 \u03b7, \u03a6 \u2264 T ] \u2264 P [ |Mk,Nk(\u03a6)| \u03baLNk(\u03a6) \u2265 \u03b7, \u03a6 \u2264 T ] .\nwhich is upper bounded using (16).\nLemma 22. Fix c > 0 and k \u2208 {1, . . . ,K}. Consider a random set of rounds H \u2282 N, such that, for all t, 1{t \u2208 H} is Ft\u22121 measurable and such that for all t \u2208 H, {k \u2208 B(t)} is true. Further assume, for all t, one has E[1{k \u2208 A(t)}|t \u2208 H] \u2265 c > 0. We define \u03c4s a stopping time such that \u2211\u03c4s t=1 1{t \u2208 H} \u2265 s. Consider the random set \u039b = {\u03c4s : s \u2265 1}. Then, for all k,\u2211 t\u22650 P[t \u2208 \u039b, |\u03b8\u0302k(t)\u2212 \u03b8k| \u2265 \u03b7] \u2264 2c\u22121(2c\u22121 + \u03ba\u22122L \u03b7 \u22122)\nThe proof of this lemma follows that of Lemma 5 in [5] using the same lower bound for N\u0303k(t) as above."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the<lb>multiple-play semi-bandit setting. However, a major concern in this context is when the system cannot decide<lb>whether the user feedback for each item is actually exploitable. Indeed, much of the content may have been<lb>simply ignored by the user. The present work proposes to exploit available information regarding the display<lb>position bias under the so-called Position-based click model (PBM). We first discuss how this model differs<lb>from the Cascade model and its variants considered in several recent works on multiple-play bandits. We<lb>then provide a novel regret lower bound for this model as well as computationally efficient algorithms that<lb>display good empirical and theoretical performance.", "creator": "LaTeX with hyperref package"}}}