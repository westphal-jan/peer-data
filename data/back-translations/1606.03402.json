{"id": "1606.03402", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning", "abstract": "Encoder decoder networks are popular for probabilistic sequence modeling in many applications. These models use the power of Long Short-Term Memory (LSTM) to capture full dependence between variables, and are not subject to label bias from local models, which require partial conditional independence. In practice, however, even when searching for the optimal sequence, they tend to have short sequences. Surprisingly, accuracy sometimes decreases even with increasing beam size.", "histories": [["v1", "Fri, 10 Jun 2016 17:30:46 GMT  (240kb,D)", "https://arxiv.org/abs/1606.03402v1", null], ["v2", "Wed, 21 Sep 2016 17:33:58 GMT  (305kb,D)", "http://arxiv.org/abs/1606.03402v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["pavel sountsov", "sunita sarawagi"], "accepted": true, "id": "1606.03402"}, "pdf": {"name": "1606.03402.pdf", "metadata": {"source": "CRF", "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning", "authors": ["Pavel Sountsov", "Sunita Sarawagi"], "emails": ["siege@google.com", "sunita@iitb.ac.in"], "sections": [{"heading": null, "text": "In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences.\nFor the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space."}, {"heading": "1 Introduction", "text": "In this paper we investigate the use of neural networks for modeling the conditional distribution Pr(y|x) over sequences y of discrete tokens in response to a complex input x, which can be another\n\u2217 Work done while visiting Google Research on a leave from IIT Bombay.\nsequence or an image. Such models have applications in machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), image captioning (Vinyals et al., 2015), response generation in emails (Kannan et al., 2016), and conversations (Khaitan, 2016; Vinyals and Le, 2015; Li et al., 2015).\nThe most popular neural network for probabilistic modeling of sequences in the above applications is the encoder-decoder (ED) network (Sutskever et al., 2014). A ED network first encodes an input x into a vector which is then used to initialize a recurrent neural network (RNN) for decoding the output y. The decoder RNN factorizes Pr(y|x) using the chain rule as \u220f j Pr(yj |y1, . . . , yj\u22121,x) where y1, . . . , yn denote the tokens in y. This factorization does not entail any conditional independence assumption among the {yj} variables. This is unlike earlier sequence models like CRFs (Lafferty et al., 2001) and MeMMs (McCallum et al., 2000) that typically assume that a token is independent of all other tokens given its adjacent tokens. Modern-day RNNs like LSTMs promise to capture non-adjacent and long-term dependencies by summarizing the set of previous tokens in a continuous, high-dimensional state vector. Within the limits of parameter capacity allocated to the model, the ED, by virtue of exactly factorizing the token sequence, is consistent.\nHowever, when we created and deployed an ED model for a chat suggestion task we observed several counter-intuitive patterns in its predicted outputs. Even after training the model over billions of examples, the predictions were systematically biased towards short sequences. Such bias has also been seen in translation (Cho et al., 2014). Another curious\nar X\niv :1\n60 6.\n03 40\n2v 2\n[ cs\n.A I]\n2 1\nSe p\n20 16\nphenomenon was that the accuracy of the predictions sometimes dropped with increasing beam-size, more than could be explained by statistical variations of a well-calibrated model (Ranzato et al., 2016).\nIn this paper we expose a margin discrepancy in the training loss of encoder-decoder models to explain the above problems in its predictions. We show that the training loss of ED network often underestimates the margin of separating a correct sequence from an incorrect shorter sequence. The discrepancy gets more severe as the length of the correct sequence increases. That is, even after the training loss converges to a small value, full inference on the training data can incur errors causing the model to be underfitted for long sequences in spite of low training cost. We call this the length bias problem.\nWe propose an alternative model that avoids the margin discrepancy by globally conditioning the P (y|x) distribution. Our model is applicable in the many practical tasks where the space of allowed outputs is closed. For example, the responses generated by the smart reply feature of Inbox is restricted to lie within a hand-screened whitelist of responses W \u2282 Y (Kannan et al., 2016), and the same holds for a recent conversation assistant feature of Google\u2019s Allo (Khaitan, 2016). Our model uses a second RNN encoder to represent the output as another fixed length vector. We show that our proposed encoderencoder model produces better calibrated whole sequence probabilities and alleviates the length-bias problem of ED models on two conversation tasks. A second advantage of our model is that inference is significantly faster than ED models and is guaranteed to find the globally optimal solution. In contrast, inference in ED models requires an expensive beamsearch which is both slow and is not guaranteed to find the optimal sequence."}, {"heading": "2 Length Bias in Encoder-Decoder Models", "text": "In this section we analyze the widely used encoderdecoder neural network for modeling Pr(y|x) over the space of discrete output sequences. We use y1, . . . , yn to denote the tokens in a sequence y. Each yi is a discrete symbol from a finite dictionary V of size m. Typically, m is large. The length n of a sequence is allowed to vary from sequence to sequence even for the same input x. A special token EOS \u2208 V\nis used to mark the end of a sequence. We use Y to denote the space of such valid sequences and \u03b8 to denote the parameters of the model."}, {"heading": "2.1 The encoder-decoder network", "text": "The Encoder-Decoder (ED) network represents Pr(y|x, \u03b8) by applying chain rule to exactly factorize it as \u220fn t=1 Pr(yt|y1, . . . , yt\u22121,x, \u03b8). First, an encoder with parameters \u03b8x \u2282 \u03b8 is used to transform x into a d-dimensional real-vector vx. The network used for the encoder depends on the form of x \u2014 for example, when x is also a sequence, the encoder could be a RNN. The decoder then computes each Pr(yt|y1, . . . , yt\u22121,vx, \u03b8) as\nPr(yt|y1, . . . , yt\u22121,vx, \u03b8) = P (yt|st, \u03b8), (1)\nwhere st is a state vector implemented using a recurrent neural network as\nst = { vx if t = 0, RNN(st\u22121, \u03b8E,yt\u22121 , \u03b8R) otherwise. (2)\nwhere RNN() is typically a stack of LSTM cells that captures long-term dependencies, \u03b8E,y \u2282 \u03b8 are parameters denoting the embedding for token y, and \u03b8R \u2282 \u03b8 are the parameters of the RNN. The function Pr(y|s, \u03b8y) that outputs the distribution over the m tokens is a softmax:\nPr(y|s, \u03b8) = e s\u03b8S,y\nes\u03b8S,1 + . . .+ es\u03b8S,m , (3)\nwhere \u03b8S,y \u2282 \u03b8 denotes the parameters for token y in the final softmax."}, {"heading": "2.2 The Origin of Length Bias", "text": "The ED network builds a single probability distribution over sequences of arbitrary length. For an input x, the network needs to choose the highest probability y among valid candidate sequences of widely different lengths. Unlike in applications like entity-tagging and parsing where the length of the output is determined based on the input, in applications like response generation valid outputs can be of widely varying length. Therefore, Pr(y|x, \u03b8) should be well-calibrated over all sequence lengths. Indeed under infinite data and model capacity the ED model is consistent and will represent all sequence lengths faithfully. In practice when training data is\nfinite, we show that the ED model is biased against long sequences. Other researchers (Cho et al., 2014) have reported this bias but we are not aware of any analysis like ours explaining the reasons of this bias.\nClaim 2.1. The training loss of the ED model underestimates the margin of separating long sequences from short ones.\nProof. Let x be an input for which a correct output y+ is of length ` and an incorrect output y\u2212 is of length 1. Ideally, the training loss should put a positive margin between y+ and y\u2212 which is log Pr(y+|x)\u2212 log Pr(y\u2212|x). Let us investigate if the maximum likelihood training objective of the ED model achieves that. We can write this objective as:\nmax \u03b8 log Pr(y+1 |x, \u03b8)+ \u2211\u0300 j=2 log Pr(y+j |y + 1...j\u22121,x, \u03b8). (4) Only the first term in the above objective is involved in enforcing a margin between y+ and y\u2212 because log Pr(y+1 |x) is maximized when log Pr(y\u22121 |x) is correspondingly minimized. Let mL(\u03b8) = log Pr(y + 1 |x, \u03b8) \u2212 log Pr(y \u2212 1 |x, \u03b8), the\nlocal margin from the first position and mR(\u03b8) =\u2211` j=2 log Pr(y + j |y + 1...j\u22121,x, \u03b8). It is easy to see that our desired margin between y+ and y\u2212 is log Pr(y+|x) \u2212 log Pr(y\u2212|x) = mL + mR. Let mg = mL +mR. Assuming two possible labels for the first position (m = 2) 1, the training objective in Equation 4 can now be rewritten in terms of the margins as:\nmin \u03b8\nlog(1 + e\u2212mL(\u03b8))\u2212mR(\u03b8)\nWe next argue that this objective is not aligned with our ideal goal of making the global marginmL+mR positive.\nFirst, note that mR is a log probability which under finite parameters will be non-zero. Second, even though mL can take any arbitrary finite value, the training objective drops rapidly when mL is positive. When training objective is regularized and training data is finite, the model parameters \u03b8 cannot take\n1For m > 2, the objective will be upper bounded by min\u03b8 log(1 + (m\u2212 1)e\u2212mL(\u03b8))\u2212mR(\u03b8). The argument that follows remains largely unchanged\nvery large values and the trainer will converge at a small positive value of mL. Finally, we show that the value of mR decreases with increasing sequence length. For each position j in the sequence, we add to mR log-probability of y+j . The maximum value of log Pr(y+j |y + 1...j\u22121,x, \u03b8) is log(1 \u2212 ) where is non-zero and decreasing with the magnitude of the parameters \u03b8. In general, log Pr(y+j |y + 1...j\u22121,x, \u03b8) can be a much smaller negative value when the input x has multiple correct responses as is common in conversation tasks. For example, an input like x =\u2018How are you?\u2019, has many possible correct outputs: y \u2208{\u2018I am good\u2019, \u2018I am great\u2019, \u2018I am fine, how about you?\u2019, etc}. Let fj denote the relative frequency of output y+j among all correct responses with prefix y + 1...j\u22121. The value of mR will be upper bounded as\nmR \u2264 \u2211\u0300 j=2 logmin(1\u2212 , fj)\nThis term is negative always and increases in magnitude as sequence length increases and the set of positive outpus have high entropy. In this situation, when combined with regularization, our desired margin mg may not remain positive even though mL is positive. In summary, the core issue here is that since the ED loss is optimized and regularized on the local problem it does not control for the global, task relevant margin.\nThis mismatch between the local margin optimized during training and the global margin explains the length bias observed by us and others (Cho et al., 2014). During inference a shorter sequence for which mR is smaller wins over larger sequences.\nThis mismatch also explains why increasing beam size leads to a drop in accuracy sometimes (Ranzato et al., 2016)2. When beam size is large, we are more likely to dig out short sequences that have otherwise been separated by the local margin. We show empirically in Section 4.3 that for long sequences larger beam size hurts accuracy whereas for small sequences the effect is the opposite."}, {"heading": "2.3 Proposed fixes to the ED models", "text": "Many ad hoc approaches have been used to alleviate length bias directly or indirectly. Some resort to nor-\n2Figure 6 in the paper shows a drop in BLEU score by 0.5 as the beam size is increased from 3 to 10.\nmalizing the probability by the full sequence length (Cho et al., 2014; Graves, 2013) whereas (Abadie et al., 2014) proposes segmenting longer sentences into shorter phrases. (Cho et al., 2014) conjectures that the length bias of ED models could be because of limited representation power of the encoder network. Later more powerful encoders based on attention achieved greater accuracy (Bahdanau et al., 2014) on long sequences. Attention can be viewed as a mechanism of improving the capacity of the local models, thereby making the local margin mL more definitive. But attention is not effective for all tasks \u2014 for example, (Vinyals and Le, 2015) report that attention was not useful for conversation.\nRecently (Bengio et al., 2015; Ranzato et al., 2016) propose another modification to the ED training objective where the true token yj\u22121 in the training term log Pr(yj |y1, . . . , yj\u22121) is replaced by a sample or top-k modes from the posterior at position j \u2212 1 via a careful schedule. Incidently, this fix also helps to indirectly alleviate the length bias problem. The sampling causes incorrect tokens to be used as previous history for producing a correct token. If earlier the incorrect token was followed by a low-entropy EOS token, now that state should also admit the correct token causing a decrease in the probability of EOS, and therefore the short sequence.\nIn the next section we propose our more direct fix to the margin discrepancy problem."}, {"heading": "3 Globally Conditioned Encoder-Encoder Models", "text": "We represent Pr(y|x, \u03b8) as a globally conditioned model e s(y|x,\u03b8)\nZ(x,\u03b8) where s(y|x, \u03b8) denotes a score for output y and Z(x, \u03b8) denotes the shared normalizer. We show in Section 3.3 why such global conditioning solves the margin discrepancy problem of the ED model. The intractable partition function in global conditioning introduces several new challenges during training and inference. In this section we discuss how we designed our network to address them.\nOur model assumes that during inference the output has to be selected from a given whitelist of responses W \u2282 Y . In spite of this restriction, the problem does not reduce to multi-class classification because of two important reasons. First, during training we wish to tap all available input-output pairs\nincluding the significantly more abundant outputs that do not come from the whitelist. Second, the whitelist could be very large and treating each output sequence as an atomic class can limit generalization achievable by modeling at the level of tokens in the sequence.\n3.1 Modeling s(y|x, \u03b8) We use a second encoder to convert y into a vector vy of the same size as the vector vx obtained by encoding x as in a ED network. The parameters used to encode vx and vy are disjoint. As we are only interested in a fixed dimensional output, unlike in ED networks, we have complete freedom in choosing the type of network to use for this second encoder. For our experiments, we have chosen to use an RNN with LSTM cells. Experimenting with other network architectures, such as bidirectional RNNs remains an interesting avenue for future work. The score s(y|x, \u03b8) is the dot-product between vy and vx. Thus our model is\nPr(y|x) = e vTx vy\u2211\ny\u2032\u2208Y e vTx vy\u2032\n. (5)"}, {"heading": "3.2 Training and Inference", "text": "During training we use maximum likelihood to estimate \u03b8 given a large set of valid input-output pairs {(x1,y1), . . . , (xN ,yN )} where each yi belongs to Y which in general is much larger thanW . Our main challenge during training is that Y is intractably large for computing Z. We decompose Z as\nZ = es(y|x,\u03b8) + \u2211\ny\u2032\u2208Y\\y es(y\n\u2032|x,\u03b8), (6)\nand then resort to estimating the last term using importance sampling. Constructing a high quality proposal distribution over Y \\ y is difficult in its own right, so in practice, we make the following approximations. We extract the most common T sequences across a data set into a pool of negative examples. We estimate the empirical prior probability of the sequences in that pool, Q(y), and then draw k samples from this distribution. We take care to remove the true sequence from this distribution so as to remove the need to estimate its prior probability.\nDuring inference, given an input x we need to find argmaxy\u2208Ws(y|x, \u03b8). This task can be performed\nefficiently in our network because the vectors vy for the sequences y in the whitelist W can be precomputed. Given an input x, we compute vx and take dot-product with the pre-computed vectors to find the highest scoring response. This gives us the optimal response. WhenW is very large, we can obtain an approximate solution by indexing the vectors vy of W using recent methods specifically designed for dot-product based retrieval (Guo et al., 2016)."}, {"heading": "3.3 Margin", "text": "It is well-known that the maximum likelihood training objective of a globally normalized model is margin maximizing (Rosset et al., 2003). We illustrate this property using our set up from Claim 2.1 where a correct output y+ is of length ` and an incorrect output y\u2212 is of length 1 with two possible labels for each position (m = 2).\nThe globally conditioned model learns a parameter per possible sequence and assigns the probability to each sequence using a softmax over those parameters. Additionally, we place a Gaussian prior on the parameters with a precision c. The loss for a positive example becomes:\nLG(y+) = \u2212 log e\u2212\u03b8y+\u2211 y\u2032 e \u2212\u03b8y\u2032 + c 2 \u2211 y\u2032 \u03b82y\u2032 ,\nwhere the sums are taken over all possible sequences.\nWe also train an ED model on this task. It also learns a parameter for every possible sequence, but assigns probability to each sequence using the chain rule. We also place the same Gaussian prior as above on the parameters. Let yj denote the first j tokens {y1, . . . , yj} of sequence y. The loss for a positive example for this model is then:\nLL(y+) = \u2212 \u2211\u0300 j=1 log e\u2212\u03b8y+j\u2211 y\u2032j e \u2212\u03b8y\u2032 j + c 2 \u2211 y\u2032j \u03b82y\u2032j  , where the inner sums are taken over all sequences of length j.\nWe train both models on synthetic sequences generated using the following rule. The first token is chosen to be \u20181\u2019 probability 0.6. If \u20181\u2019 is chosen, it means that this is a positive example and the remaining `\u2212 1 tokens are chosen to be \u20181\u2019 with probability 0.9 1 `\u22121 . If a \u20180\u2019 is chosen as the first token, then that is a negative example, and the sequence generation does not go further. This means that there are 2`\u22121 unique positive sequences of length ` and one negative sequence of length 1. The remaining possible sequences do not occur in the training or testing data. By construction the unbiased margin between the most probable correct example and the incorrect example is length independent and positive. We sample 10000 such sequences and train both models using\nAdagrad (Duchi et al., 2011) for 1000 epochs with a learning rate of 0.1, effectively to convergence.\nFigure 2 shows the margin for both models (between the most likely correct sequence and the incorrect sequence) and the local margin for the ED model at the end of training. On the left panel, we used sequences with ` = 2 and varied the regularization constant c. When c is zero, both models learn the same global margin, but as it is increased the margin for the ED model decreases and becomes negative at c > 0.2, despite the local margin remaining positive and high. On the right panel we used c = 0.1 and varied `. The ED model becomes unable to separate the sequences with length above 2 with this regularization constant setting."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets and Tasks", "text": "We contrast the quality of the ED and encoderencoder models on two conversational datasets: Open Subtitles and Reddit Comments."}, {"heading": "4.1.1 Open Subtitles Dataset", "text": "The Open Subtitles dataset consists of transcriptions of spoken dialog in movies and television shows (Lison and Tiedemann, 2016). We restrict our modeling only to the English subtitles, of which results in 319 million utternaces. Each utterance is tokenized into word and punctuation tokens, with the start and end marked by the BOS and EOS tokens. We ran-\ndomly split out 90% of the utterances into the training set, placing the rest into the validation set. As the speaker information is not present in this data set, we treat each utterance as a label sequence, with the preceding utterances as context."}, {"heading": "4.1.2 Reddit Comments Dataset", "text": "The Reddit Comments dataset is constructed from publicly available user comments on submissions on the Reddit website. Each submission is associated with a list of directed comment trees. In total, there are 41 million submissions and 501 million comments. We tokenize the individual comments in the same way as we have done with the utternaces in the Open Subtitles dataset. We randomly split 90% of the submissions and the associated comments into the training set, and the rest into the validation set. We use each comment (except the ones with no parent comments) as a label sequence, with the context sequence composed of its ancestor comments."}, {"heading": "4.1.3 Whitelist and Vocabulary", "text": "From each dataset, we derived a dictionary of 20 thousand most commonly used tokens. Additionally, each dictionary contained the unknown token (UNK), BOS and EOS tokens. Tokens in the datasets which were not present in their associated vocabularies were replaced by the UNK token.\nFrom each data set, we extracted 10 million most common label sequences that also contained at most 100 tokens. This set of sequences was used as the negative sample pool for the encoder-encoder models. For evaluation we created a whitelistW out of the 100 thousand most common sequences. We removed any sequence from this set that contained any UNK tokens to simplify inference."}, {"heading": "4.1.4 Sequence Prediction Task", "text": "To evaluate the quality of these models, we task them to predict the true label sequence given its context. Due to the computational expense, we sub-sample the validation data sets to around 1 million context-label pairs. We additionally restrict the context-label pairs such that the label sequence is present in the evaluation set of common messages. We use recall@K as a measure of accuracy of the model predictions. It is defined as the fraction of test pairs where the correct label is within K most\nprobable predictions according to the model. For encoder-encoder models we use an exhaustive search over the evaluation set of common messages. For ED models we use a beam search with width ranging from 1 to 15 over a token prefix trie constructed from the sequences inW ."}, {"heading": "4.2 Model Structure and Training Procedure", "text": "The context encoder, label encoder and decoder are implemented using LSTM recurrent networks (Hochreiter and Schmidhuber, 1997) with peephole connections (Sak et al., 2014). The context and label token sequences were mapped to embedding vectors using a lookup table that is trained jointly with the rest of the model parameters. The recurrent nets were unrolled in time up to 100 time-steps, with label sequences of greater length discarded and context sequences of greater length truncated.\nThe decoder in the ED model is trained by using the true label sequence prefix as input, and a shifted label sequence as output (Sutskever et al., 2014). The partition function in the softmax over tokens is estimated using importance sampling with a unigram distribution over tokens as the proposal distribution (Jean et al., 2014). We sample 512 negative examples from Q(y) to estimate the partition function for the encoder-encoder model. See Figure 1 for connectivity and network size details.\nAll models were trained using Adagrad (Duchi et al., 2011) with an initial base learning rate of 0.1 which we exponentially decayed with a decade of 15 million steps. For stability, we clip the L2 norm of the gradients to a maximum magnitude of 1 as described in (Pascanu et al., 2012). All models are trained for 30 million steps with a mini-batch size of 64. The models are trained in a distributed manner on CPUs and NVidia GPUs using TensorFlow (Abadi et al., 2015)."}, {"heading": "4.3 Results", "text": "We first demonstrate the discrepancy between the local and global margin in the ED models as discussed in Section 3.3. We used a beam size of 15 to get the top prediction from our trained ED models on the test data and focussed on the subset for which the top prediction was incorrect. We measured local and global margin between the top predicted sequence (y\u2212) and the correct test sequence (y+) as\nfollows: Global margin is the difference in their full sequence log probability. Local margin is the difference in the local token probability of the smallest position j where y\u2212j 6= y + j , that is local margin is Pr(y+j |y + 1...j\u22121,x, \u03b8) \u2212 Pr(y \u2212 j |y + 1...j\u22121,x, \u03b8). Note the training loss of ED models directly compares only the local margin.\nGlobal margin is much smaller than local margin In Figure 3 we show the local and global margin as a 2D histogram with color luminosity denoting frequency. We observe that the global margin values are much smaller than the local margins. The prominent spine is for (y+,y\u2212) pairs differing only in a single position making the local and global margins equal. Most of the mass is below the spine. For a significant fraction of cases (27% for Reddit, and 21% for Subtitles), the local margin is positive while the global margin is negative. That is, the ED loss for these sequences is small even though the log-probability of the correct sequence is much smaller than the logprobability of the predicted wrong sequence.\nBeam search is not the bottleneck An interesting side observation from the plots in Figure 3 is that more than 98% of the wrong predictions have a negative margin, that is, the score of the correct sequence is indeed lower than the score of the wrong prediction. Improving the beam-width beyond 15 is not likely to improve these models since only in 1.9% and 1.7% of the cases is the correct score higher than the score of the wrong prediction.\nMargin discrepancy is higher for longer sequences In Figure 4 we show that this discrepancy is significantly more pronounced for longer sequences. In the figure we show the fraction of wrongly predicted sequences with a positive local margin. We find that as sequence length increases, we have more cases where the local margin is positive yet the global margin is negative. For example, for the Reddit dataset half of the wrongly predicted sequences have a positive local margin indicating that the training loss was low for these sequences even though they were not adequately separated.\nIncreasing beam size drops accuracy for long sequences Next we show why this discrepancy leads to non-monotonic accuracies with increasing beamsize. As beam size increases, the predicted sequence has higher probability and the accuracy is expected to increase if the trained probabilities are well-calibrated. In Figure 5 we plot the number of correct predictions (on a log scale) against the length of the correct sequence for beam sizes of 1, 5, 10, and 15. For small sequence lengths, we indeed observe that increasing the beam size produces more accurate results. For longer sequences (length > 4) we observe a drop in accuracy with increasing the beam width beyond 1 for Reddit and beyond 5 for Subtitles.\nGlobally conditioned models are more accurate than ED models We next compare the ED model with our globally conditioned encoder-encoder (EE) model. In Figure 6 we show the recall@K values for K=1, 3 and 5 for the two datasets for increasing length of correct sequence. We find the EE model is largely better that the ED model. The most interesting difference is that for sequences of length greater than 8, the ED model has a recall@5 of zero for both datasets. In contrast, the EE model manages\nto achieve significant recall even at large sequence lengths.\nLength normalization of ED models A common modification to the ED decoding procedure used to promote longer message is normalization of the prediction log-probability by its length raised to some power f (Cho et al., 2014; Graves, 2013). We experimented with two settings, f = 0.5 and 1.0. Our experiments show that while this indeed promotes longer sequences, it does so at the expense of reducing the accuracy on the shorter sequences."}, {"heading": "5 Related Work", "text": "In this paper we showed that encoder-decoder models suffer from length bias and proposed a fix using global conditioning. Global conditioning has been proposed for other RNN-based sequence prediction tasks in (Yao et al., 2014) and (Andor et al., 2016). The RNN models that these work attempt to fix capture only a weak form of dependency among variables, for example they assume x is seen incrementally and only adjacent labels in y are directly dependent. As proved in (2016) these models are subject to label bias since they cannot represent a distribution that a globally conditioned model can. Thus, their fix for global dependency is using a CRFs. Such\nglobal conditioning will compromise a ED model which does not assume any conditional independence among variables. The label-bias proof of (2016) is not applicable to ED models because the proof rests on the entire input not being visible during output. Earlier illustrations of label bias of MeMMs in (Bottou, 1991; Lafferty et al., 2001) also require local observations. In contrast, the ED model transitions on the entire input and chain rule is an exact factorization of the distribution. Indeed one of the suggestions in (Bottou, 1991) to surmount label-bias is to use a fully connected network, which the ED model already does.\nOur encoder-encoder network is reminiscent of the dual encoder network in (Lowe et al., 2015), also used for conversational response generation. A crucial difference is our use of importance sampling to correctly estimate the probability of a large set of candidate responses, which allows us to use the model as a standalone response generation system. Other differences include our model using separate sets of parameters for the two encoders, to reflect the assymetry of the prediction task. Lastly, we found it crucial for the model\u2019s quality to use multiple appropriately weighed negative examples for every positive example during training.\n(Ranzato et al., 2016) also highlights limitations of the ED model and proposes to mix the ED loss with a sequence-level loss in a reinforcement learning framework under a carefully tuned schedule. Our method for global conditioning can capture sequence-\nlevel losses like BLEU score more easily, but may also benefit from a similar mixed loss function."}, {"heading": "6 Conclusion", "text": "We have shown that encoder-decoder models in the regime of finite data and parameters suffer from a length-bias problem. We have proved that this arises due to the locally normalized models insufficiently separating correct sequences from incorrect ones, and have verified this empirically. We explained why this leads to the curious phenomenon of decreasing accuracy with increasing beam size for long sequences. Our proposed encoder-encoder architecture side steps this issue by operating in sequence probability space directly, yielding improved accuracy for longer sequences.\nOne weakness of our proposed architecture is that it cannot generate responses directly. An interesting future work is to explore if the ED model can be used to generate a candidate set of responses which are then re-ranked by our globally conditioned model. Another future area is to see if the techniques for making Bayesian networks discriminative can fix the length bias of encoder decoder networks (Peharz et al., 2013; Guo et al., 2012)."}], "references": [{"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation. CoRR, abs/1409.1257", "author": ["D Bahdanau", "B van Merrienboer", "K Cho", "Y Bengio"], "venue": null, "citeRegEx": "Abadie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abadie et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Une approche theorique de l\u2019apprentissage connexionniste: Applications a la recon\u2018naissance de la parole", "author": ["L. Bottou"], "venue": "Ph.D. thesis, Universitede Paris XI", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches. CoRR, abs/1409.1259", "author": ["Cho et al.2014] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elan Hazad", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Maximum margin bayesian networks. CoRR, abs/1207.1382", "author": ["Guo et al.2012] Yuhong Guo", "Dana F. Wilkinson", "Dale Schuurmans"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2012}, {"title": "Quantization based fast inner product search", "author": ["R. Guo", "S. Kumar", "K. Choromanski", "D. Simcha"], "venue": "In AISTATS", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007", "author": ["Jean et al.2014] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "A diversity-promoting objective function for neural conversation models. CoRR, abs/1510.03055", "author": ["Li et al.2015] J Li", "M Galley", "C Brockett", "J Gao", "B Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles", "author": ["Lison", "Tiedemann2016] Pierre Lison", "J\u00f6rg Tiedemann"], "venue": "LREC", "citeRegEx": "Lison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lison et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructure multi-turn dialogue systems", "author": ["Lowe et al.2015] R Lowe", "N Pow", "I V Serban", "J Pineau"], "venue": "In SIGDial", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["McCallum et al.2000] A. McCallum", "D. Freitag", "F. Pereira"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Understanding the exploding gradient problem. CoRR, abs/1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "The most generative maximum margin bayesian networks", "author": ["Peharz et al.2013] Robert Peharz", "Sebastian Tschiatschek", "Franz Pernkopf"], "venue": null, "citeRegEx": "Peharz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2013}, {"title": "Sequence level training with recurrent neural networks. ICLR", "author": ["Ranzato et al.2016] M Ranzato", "S Chopra", "M Auli", "W Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Margin maximizing loss functions", "author": ["Rosset et al.2003] S Rosset", "J Zhu", "T Hastie"], "venue": null, "citeRegEx": "Rosset et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rosset et al\\.", "year": 2003}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling", "author": ["Sak et al.2014] Hasim Sak", "Andrew Senior", "Francoise Beaufays"], "venue": "INTERSPEECH", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model. CoRR, abs/1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent conditional random field for language understanding", "author": ["Yao et al.2014] K Yao", "B Peng", "G Zweig", "D Yu", "X Li", "F Gao"], "venue": "In ICASSP", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Such models have applications in machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), image captioning (Vinyals et al.", "startOffset": 53, "endOffset": 100}, {"referenceID": 21, "context": "Such models have applications in machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), image captioning (Vinyals et al.", "startOffset": 53, "endOffset": 100}, {"referenceID": 22, "context": ", 2014), image captioning (Vinyals et al., 2015), response generation in emails (Kannan et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 12, "context": ", 2016), and conversations (Khaitan, 2016; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 27, "endOffset": 81}, {"referenceID": 21, "context": "The most popular neural network for probabilistic modeling of sequences in the above applications is the encoder-decoder (ED) network (Sutskever et al., 2014).", "startOffset": 134, "endOffset": 158}, {"referenceID": 11, "context": "This is unlike earlier sequence models like CRFs (Lafferty et al., 2001) and MeMMs (McCallum et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 15, "context": ", 2001) and MeMMs (McCallum et al., 2000) that typically assume that a token is independent of all other tokens given its adjacent tokens.", "startOffset": 18, "endOffset": 41}, {"referenceID": 4, "context": "Such bias has also been seen in translation (Cho et al., 2014).", "startOffset": 44, "endOffset": 62}, {"referenceID": 18, "context": "phenomenon was that the accuracy of the predictions sometimes dropped with increasing beam-size, more than could be explained by statistical variations of a well-calibrated model (Ranzato et al., 2016).", "startOffset": 179, "endOffset": 201}, {"referenceID": 4, "context": "Other researchers (Cho et al., 2014) have reported this bias but we are not aware of any analysis like ours explaining the reasons of this bias.", "startOffset": 18, "endOffset": 36}, {"referenceID": 4, "context": "This mismatch between the local margin optimized during training and the global margin explains the length bias observed by us and others (Cho et al., 2014).", "startOffset": 138, "endOffset": 156}, {"referenceID": 18, "context": "This mismatch also explains why increasing beam size leads to a drop in accuracy sometimes (Ranzato et al., 2016)2.", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "malizing the probability by the full sequence length (Cho et al., 2014; Graves, 2013) whereas (Abadie et al.", "startOffset": 53, "endOffset": 85}, {"referenceID": 6, "context": "malizing the probability by the full sequence length (Cho et al., 2014; Graves, 2013) whereas (Abadie et al.", "startOffset": 53, "endOffset": 85}, {"referenceID": 0, "context": ", 2014; Graves, 2013) whereas (Abadie et al., 2014) proposes segmenting longer sentences into shorter phrases.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": "(Cho et al., 2014) conjectures that the length bias of ED models could be because of limited representation power of the encoder network.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Later more powerful encoders based on attention achieved greater accuracy (Bahdanau et al., 2014) on long sequences.", "startOffset": 74, "endOffset": 97}, {"referenceID": 2, "context": "Recently (Bengio et al., 2015; Ranzato et al., 2016) propose another modification to the ED training objective where the true token yj\u22121 in the training term log Pr(yj |y1, .", "startOffset": 9, "endOffset": 52}, {"referenceID": 18, "context": "Recently (Bengio et al., 2015; Ranzato et al., 2016) propose another modification to the ED training objective where the true token yj\u22121 in the training term log Pr(yj |y1, .", "startOffset": 9, "endOffset": 52}, {"referenceID": 8, "context": "WhenW is very large, we can obtain an approximate solution by indexing the vectors vy of W using recent methods specifically designed for dot-product based retrieval (Guo et al., 2016).", "startOffset": 166, "endOffset": 184}, {"referenceID": 19, "context": "It is well-known that the maximum likelihood training objective of a globally normalized model is margin maximizing (Rosset et al., 2003).", "startOffset": 116, "endOffset": 137}, {"referenceID": 5, "context": "Adagrad (Duchi et al., 2011) for 1000 epochs with a learning rate of 0.", "startOffset": 8, "endOffset": 28}, {"referenceID": 20, "context": "The context encoder, label encoder and decoder are implemented using LSTM recurrent networks (Hochreiter and Schmidhuber, 1997) with peephole connections (Sak et al., 2014).", "startOffset": 154, "endOffset": 172}, {"referenceID": 21, "context": "The decoder in the ED model is trained by using the true label sequence prefix as input, and a shifted label sequence as output (Sutskever et al., 2014).", "startOffset": 128, "endOffset": 152}, {"referenceID": 10, "context": "The partition function in the softmax over tokens is estimated using importance sampling with a unigram distribution over tokens as the proposal distribution (Jean et al., 2014).", "startOffset": 158, "endOffset": 177}, {"referenceID": 5, "context": "All models were trained using Adagrad (Duchi et al., 2011) with an initial base learning rate of 0.", "startOffset": 38, "endOffset": 58}, {"referenceID": 16, "context": "For stability, we clip the L2 norm of the gradients to a maximum magnitude of 1 as described in (Pascanu et al., 2012).", "startOffset": 96, "endOffset": 118}, {"referenceID": 4, "context": "Length normalization of ED models A common modification to the ED decoding procedure used to promote longer message is normalization of the prediction log-probability by its length raised to some power f (Cho et al., 2014; Graves, 2013).", "startOffset": 204, "endOffset": 236}, {"referenceID": 6, "context": "Length normalization of ED models A common modification to the ED decoding procedure used to promote longer message is normalization of the prediction log-probability by its length raised to some power f (Cho et al., 2014; Graves, 2013).", "startOffset": 204, "endOffset": 236}, {"referenceID": 24, "context": "Global conditioning has been proposed for other RNN-based sequence prediction tasks in (Yao et al., 2014) and (Andor et al.", "startOffset": 87, "endOffset": 105}, {"referenceID": 24, "context": "Global conditioning has been proposed for other RNN-based sequence prediction tasks in (Yao et al., 2014) and (Andor et al., 2016). The RNN models that these work attempt to fix capture only a weak form of dependency among variables, for example they assume x is seen incrementally and only adjacent labels in y are directly dependent. As proved in (2016) these models are subject to label bias since they cannot represent a distribution that a globally conditioned model can.", "startOffset": 88, "endOffset": 356}, {"referenceID": 3, "context": "Earlier illustrations of label bias of MeMMs in (Bottou, 1991; Lafferty et al., 2001) also require local observations.", "startOffset": 48, "endOffset": 85}, {"referenceID": 11, "context": "Earlier illustrations of label bias of MeMMs in (Bottou, 1991; Lafferty et al., 2001) also require local observations.", "startOffset": 48, "endOffset": 85}, {"referenceID": 3, "context": "Indeed one of the suggestions in (Bottou, 1991) to surmount label-bias is to use a fully connected network, which the ED model already does.", "startOffset": 33, "endOffset": 47}, {"referenceID": 14, "context": "Our encoder-encoder network is reminiscent of the dual encoder network in (Lowe et al., 2015), also used for conversational response generation.", "startOffset": 74, "endOffset": 93}, {"referenceID": 18, "context": "(Ranzato et al., 2016) also highlights limitations of the ED model and proposes to mix the ED loss with a sequence-level loss in a reinforcement learning framework under a carefully tuned schedule.", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "Another future area is to see if the techniques for making Bayesian networks discriminative can fix the length bias of encoder decoder networks (Peharz et al., 2013; Guo et al., 2012).", "startOffset": 144, "endOffset": 183}, {"referenceID": 7, "context": "Another future area is to see if the techniques for making Bayesian networks discriminative can fix the length bias of encoder decoder networks (Peharz et al., 2013; Guo et al., 2012).", "startOffset": 144, "endOffset": 183}], "year": 2016, "abstractText": "Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size. In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences. For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space.", "creator": "LaTeX with hyperref package"}}}