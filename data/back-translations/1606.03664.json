{"id": "1606.03664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "Weakly Supervised Scalable Audio Content Analysis", "abstract": "The detection of audio events is an important task for the content analysis of multimedia data. Most current work on audio event detection is driven by supervised learning approaches. We propose a weakly supervised learning system that can take advantage of the enormous amount of web multimedia data with significantly reduced annotation effort and effort. In particular, we use multiple instance learning algorithms to show that the detection of audio events is feasible with weak labels. We also propose a novel scalable multi-instance learning algorithm and show that it competes with other instance learning algorithms for audio event detection tasks.", "histories": [["v1", "Sun, 12 Jun 2016 04:07:45 GMT  (27kb)", "http://arxiv.org/abs/1606.03664v1", "ICME 2016"]], "COMMENTS": "ICME 2016", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.MM", "authors": ["anurag kumar", "bhiksha raj"], "accepted": false, "id": "1606.03664"}, "pdf": {"name": "1606.03664.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["alnu@andrew.cmu.edu,", "bhiksha@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n03 66\n4v 1\n[ cs\n.S D\n] 1\n2 Ju\nn 20\nIndex Terms\u2014 Weak Labels, Audio Event Detection, Audio Content Analysis, Multiple Instance Learning"}, {"heading": "1. INTRODUCTION", "text": "Multimedia content analysis is a necessity for meaningful retrieval and indexing of multimedia data. It is becoming even more important due to the explosive growth of multimedia data over the internet. The focus of this paper is the audio component of multimedia which carries a significant amount of information about the overall content of multimedia data. Audio content analysis for us means successful detection of different acoustic events in a given audio recording. Audio content analysis or audio event detection (AED), the term more frequently used in this paper is important for reasons other than multimedia retrieval as well. Several applications such as surveillance [1] and monitoring are much easier to do through audio data only. Audio signals are not only easier to capture and transmit but can also pass through obstacles where cameras would be simply useless. An excellent example for audio based monitoring systems are those used in wildlife monitoring, for example bird species recognition using birdsong audio [2] [3]. Moreover, audio event detection plays an important role for context aware systems [4].\nA variety of methods including those already referred have been proposed for AED tasks. Taking cues from automatic speech recognition GMM-HMM architectures have been employed for AED as well[5]. Discriminative learning methods in which fixed length representations for audio segments combined with discriminative classifiers such as Sup-\nport Vector Machine or Random Forest classifiers have also been proposed [6] [7] [8][9]. With success of Deep neural networks for ASR, audio event detection using DNNs were attempted in [10] [9] [11]. An inherent problem with AED is presence of overlapping events. Successful detection of overlapping events is crucial for real world applications. Techniques such as matrix factorization and deep neural networks have been used for this more challenging tasks [12][13].\nFrom review of literature on audio event detection it is noticeable that AED is currently mainly driven by supervised learning approaches. The data consists of either examples of audio events or recordings for which time stamps of occurrence of each audio event is provided. We will refer to datasets in these forms as Strongly labeled data. Strongly labeled data can be easily used for training a variety of supervised learning algorithms for AED tasks.\nHowever, one can quickly realize that fully supervised approaches where we need strongly labeled data will run into scaling issues. These scaling issues will be on several fronts. First, in terms of amount of training data available for each acoustic event. Creating strongly labeled data requires manually annotating data where all times of occurrences of an event are properly marked for a given recording. Clearly, this is an extremely time consuming and expensive task to perform. Thus generating a few hours of training data (positive examples) for a particular audio event is very difficult. Second, in terms of audio events vocabulary for which detectors are built. The number of audio events for which one can create strongly labeled data cannot again be made very large due to the previous reason. In fact to the best of our knowledge there is no large scale strongly labeled dataset meant for audio event detection consisting of more than one hundred audio event in the vocabulary. This obviously puts some constraints on audio event detection research.\nA way out of the above problems is learning through weakly labeled data. Weak labels implies information about the presence or absence of an event in the recording is known but the exact time frames where it is present need not be known. If we can learn audio event detection from weakly labeled data the scaling issues stated previously can be tackled. The is because one can consider mining the vast amount of data available on the web. For audio/video recordings avail-\nTo appear in Proc. ICME, July 11-16, 2016, Seattle, USA \u00a9 IEEE 2016\nable on the web weak labels can be obtained using the metadata (tags, titles etc.) associated with the recordings. Even if no such metadata is available it is much easier to traverse through a recording and simply mark down if an event is present or not rather than putting exact time stamps of occurrences of the event. This essentially solves both of the problems mentioned in previous paragraph. Surprisingly, weakly supervised AED has received next to negligible attention. A particularly interesting and related work is classification of bird species [2] using weakly labeled birdsong audio through Multiple Instance Learning (MIL). In another work, MIL was used for music genre classification using weak labels [14].\nWe intend to show in this work that generic audio event detection using weakly labeled data is feasible through the well known multiple instance learning framework. Multiple Instance Learning (MIL) is a weakly supervised learning form in which labels are available for groups or bags of instances rather than each instance itself as is the case in supervised learning. We use several well known multiple instance learning (MIL) algorithms to demonstrate that weakly labeled data can indeed be used for AED tasks. Moreover, we also focus on the scalability issue at the next level; which is the scalability of MIL algorithms themselves for AED tasks. We propose a novel scalable multiple instance learning which is based on ideas similar to another scalable MIL algorithm [15]. Its performance is superior to 2 of the known MIL algorithm used in this work and offers advantages over the third method in terms of scalability while retaining the performance.\nThe rest of the paper is organized as follows. In Section 2 we formulate weakly supervised AED using MIL. In Section 3 we provide description of various MIL algorithms and our proposed MIL algorithm. Section 4 shows experimental results and we conclude in Section 5."}, {"heading": "2. MULTIPLE INSTANCE LEARNING AND AED", "text": "In supervised learning labels for all instances are known during training and the learner tries to find decision boundaries which can separate these labeled instances in the best possible way. Since we are concerned with detection of presence or absence of an event in a recording we will be basically concerned with binary case where labels are either \u22121 or 1. The instances belonging to the target class will be referred to as positive (+1 label) and those not belonging to target class as negative (\u22121). Multiple instance learning can be described as a generalized form of supervised learning. Here labels are not known for each instance, instead labels are known for a bag of instances. A bag is labeled as positive if atleast one instance in it is known to be positive otherwise it is labeled negative. This implies that all instances in a negative bag can be marked as negative examples for the target class whereas the same cannot be done for a positive bag in which an instance can be positive as well as negative. This imbalance creates a unique form of learning where learner needs to learn decision boundaries using this bag level representations. It means that atleast one of the instance in every positive bag must lie\non the positive side of decision boundary. Starting from its utility in drug activity detection[16], several algorithms such as Diverse Density, MILES, Citation-KNN, MIBoosting, MiGraph, SVM based approaches etc. have been proposed to learn in this paradigm [17][18]. MIL has found applications in several areas such as image classification and retrieval, text categorization, web mining etc. [18] [17] give excellent surveys of theory and applications of MIL. We propose to use it for audio event detection using weakly labeled data.\nLet {(X1, Y1), ...(Xi, Yi)...(XM , YM )} be the set of m bag-label pairs. Xi represents a bag and Yi \u2208 {\u22121, 1} its label. The instances in the bag Xi are represented by xij where j = 1 to mi, mi being the number of instances in the bag Xi. Each xij is D dimensional vector representing the instance. The instance labels yij for xij can be deterministically inferred from Yi for negative bags only.\nMoving specifically into audio, consider a weakly labeled data in which the presence or absence of an event in an audio recording is known. We break this recording into several small contiguous segments. Considering reasonably sized segments, it is safe to assume that if an event was marked to be present in the whole recording then atleast one of the segment is a positive example for that event. One the other hand, if an event is not marked to be present in a recording, clearly, none of the segments will contain that event and hence all segments are negative examples for the event concerned. The whole picture naturally falls into MIL framework. The recordings can be treated as bags Xi and the segments of the recordings as instances of the corresponding bag. From the arguments just stated if the weak information identifies presence of an event in a recording then the label for the corresponding bag is +1 for that event otherwise \u22121. Thus, AED using weak annotations can be cast as an MIL problem and one can employ any of the whole well-known MIL algorithm. Before going into the details of the MIL algorithms we need the feature representation for the audio segments in each bag. We use soft-count bag of audio word representation which is described in next Section."}, {"heading": "2.1. Audio Feature Representation", "text": "Bag of audio words is a simple yet effect way of representing acoustic events for detection and classification purposes [6][9]. The first step is to parameterize all audio recordings by basic features such as Mel-Cepstra Coefficients (MFCC). The MFCC features for all the available audios are then used to learn a codebook through clustering algorithms (e.g Kmeans). Then for any given recording its MFCC vectors are quantized to one of the cluster in the codebook. Finally, a histogram representing the count of number of MFCC vectors belonging to each cluster is used as feature representation for the recording.\nWe need a form of feature representation which can robustly represent short duration audio segments (instances of the bags). [19] showed that it is better to use soft count histogram obtained using Gaussian Mixture Model (GMM) for\nevent detection in short audio segments. Hence, we use a GMM based bag of audio words representation. The first step in this case is to learn a universal GMM using the MFCC features of training audio data. Let G = {wk, N(~\u00b5k,\u03a3k), k = 1 to M} be the obtained GMM, where wk, ~\u00b5k and \u03a3k are the mixture weight, mean and covariance parameters of the kth Gaussian in G. Let the MFCC vectors of an audio with total of T MFCC vectors be represented by ~mt, t = 1 to T . Then soft-count for kth component is computed as\nPr(k| ~mt) = wkN( ~mt; ~\u00b5k,\u03a3k)\nM \u2211\nj=1\nwjN( ~mt; ~\u00b5k,\u03a3k)\n(1)\nP (k) = 1\nT\nT \u2211\ni=1\nPr(k| ~mt) (2)\nThe feature representation of the audio segment is the vector ~PM = [P (1), ..P (k)..P (M)]\nT . ~PM is normalized to sum to 1. Thus each instance vector in the bag xij is an M dimensional soft-count histogram."}, {"heading": "3. MIL ALGORITHMS", "text": "As stated previously, a variety of MIL algorithms have been proposed. Overall, we use three known algorithms for MIL and propose one novel algorithm for multiple instance learning. The first two algorithm referred as miSVM and MISVM [20] are based on Support Vector Machine (SVM). The standard SVM algorithm is modified to work in MIL domain. Although a few other formulations of SVM for MIL domain have been proposed [18], miSVM and MISVM were the first set of SVM formulations for MIL and performs well on a variety of MIL tasks. Our third MIL algorithm is a more generic approach called miFV which tries to address the scalability issue in MIL paradigm. Our proposed novel MIL method is also based on the idea similar to the one behind miFV. We provide a brief description of these methods."}, {"heading": "3.1. miSVM and MISVM", "text": "miSVM is the first method proposed in [20] to directly solve MIL using SVM. miSVM is an instance level approach in which the labels for instances in positive bags are treated as unobserved integer variables. It then tries to solve standard SVM by putting in an additional constraint which ensures that the atleast one instance in the positive bag is positive. This formulation is shown in Eq 3. It leads to a mixed integer programming problem where margin maximization is done jointly over labels of instances and the separating hyperplanes. An optimization heuristics is used to solve it in an iterative manner. First, standard SVM is solved using the current assigned labels for instances in positive bags. Second, using the obtained SVM solution labels are assigned to instances in positive bags. All instances in positive bags are initialized with positive label to obtain the first SVM solution. The process is repeated till no change in labels of instances are observed. To ensure positive bag constraint, if all\ninstances in a positive bag gets classified as negative at any step then the instance with maximum output is assigned positive label.\nmin {yij},w,b,\u03be\n1 2 ||w||2 + C \u2211\nij\n\u03beij s.t \u2200 i, j\nyij(\u3008w, xij\u3009+ b) \u2265 1\u2212 \u03beij , \u03beij \u2265 0, yij \u2208 {\u22121, 1} mi \u2211\nj=1\nyij + 1\n2 \u2265 1 \u2200Yi = 1 & yij = \u22121 \u2200 Yi = \u22121\n(3)\nMISVM is the second approach proposed alongside miSVM. This is a bag level approach in which the goal is to describe margin with respect to bags and then try to directly maximize this margin. The formulation is standard SVM with max constraint as shown in Eq 4.\nmin w,b,\u03be\n1 2 ||w||2 + C \u2211\ni\n\u03beij\ns.t Yi( max xij\u2208Xi\n(\u3008w, xij\u3009+ b)) \u2265 1\u2212 \u03bei, \u03bei \u2265 0 (4)\nEssentially, each positive bag is represented by one \u201cwitness\u201d instance which determines the margin. The optimization heuristic similar to miSVM is used to solve the problem. The only difference is that in this case a selector variable which determines which instance of a positive bag is \u201cwitness\u201d is updated in each iteration. Both of these methods are valid for non-linear kernel SVMs as well. More details can be found in the original paper [20]. 3.2. Scalable Algorithms Most of the MIL algorithms including miSVM and MISVM suffers from scalability issue [15]. Scalability is an important factor especially when we are trying to work on problems such as AED where total number of instances will become very large even for a few hours of audio data. A scalable multiple instance learning algorithm called miFV was proposed in [15]. The main idea is that instead of trying to learn complex hypothesis for bag representation, map each bag into a single vector representation, usually in some higher dimensional space. This vector representation for each bag should try to encode as much non-redundant information as possible for the bag. Also, it should be computationally efficient for large scale learning. Once this mapping can be done efficiently the MIL problem effectively moves to the supervised learning paradigm because each bag is represented by a single vector and has a corresponding label. The scalability issue can be further addressed by using any scalable supervised learning algorithm. For example, linear SVMs are known to be computationally cheap and performs quite well in high dimensional space.\n3.2.1. miFV miFV uses Fisher Vectors (FV) for encoding bags into vector representation. FV originates from Fisher Kernels and is a state of art method for image retrieval and classification. We\nrequest the reader to refer to [21] for a detailed description of Fisher Kernels and Fisher Vectors. [15] claimed that under the assumption of independence for instances in the bags, it can also be used for encoding the bags in MIL framework. Here, we show an outline of obtaining Fisher Vectors for each bag. A Gaussian Mixture Model (GMM) is first used to model instances across all bags. Please note that this different from the one used for audio feature representation. This GMM is learned over the instance space using all instances in training bags. Let this K component GMM be represented as M = {wk, N(\u00b5k,\u03a3k) k = 1 to K} where wk is the mixture weight, \u00b5k is mean vector and \u03a3k is the covariance matrix of kth Gaussian. The Gaussians are assumed to have diagonal covariance with diagonal variance vector represented as \u03c32k. Then for given a bag Xi with mi instances we compute the following for each component of GMM\n\u03b3j(k) = wkN(xij ;\u00b5k,\u03a3k)\nK \u2211\nj=1\nwjN(xij ;\u00b5k,\u03a3k)\n(5)\n\u03b1Xiwk = 1\nmi \u221a wk\nmi \u2211\nj=1\n(\u03b3j(k)\u2212 wk) (6)\n\u03b1Xi\u00b5k = 1\nmi \u221a wk\nmi \u2211\nj=1\n\u03b3j(k)\n(\nxij \u2212\u00b5k \u03c3k\n)\n(7)\n\u03b1Xi\u03c3k = 1\nmi \u221a 2wk\nmi \u2211\nj=1\n\u03b3j(k)\n(\n(xij \u2212\u00b5k)2 \u03c32k \u2212 1 )\n(8)\nFinally, the Fisher Vector is concatenation of \u03b1Xiwk , \u03b1 Xi \u00b5k , \u03b1Xi\u03c3k for all K Gaussians. This results in a (2D + 1)K dimensional representation where D is the dimension of instance space. An improved fisher vector (IFV) obtained by sign - square rooting and L2 normalization of the original Fisher vector leads to superior performance [21]. Once fisher vector has been used one can potentially use any supervised learning method to obtain audio event detectors. In patricular, Fisher vectors work remarkably well with linear SVMs. Its clear from Equations 8, 7 and 6 that once the GMM M has been obtained, the computation of Fisher vector is cheap and can be done efficiently. Combined with linear SVMs it can efficiently address the scalability issue in MIL. 3.2.2. Proposed MIL Algorithm Here we propose another scalable multiple instance learning algorithm. The central idea is same as the one behind miFV which is to map bags to single vector representation. We propose an alternative and efficient method of mapping bags to single vectors. Similar to miFV we also start with the same GMM M trained in the instance space. The single vector representation for a bag is then obtained by maximum a posteriori (MAP) adaptation of instances in the bag to the GMM M. This essentially implies that the parameters of M are updated to effectively represent the instances in the bag. The encoding of a bag Xi is done by following steps.\nFor each component of the GMM we compute the posterior probabilities \u03b3j(k) as in Eq 5. Then, the mean and variance updates are computed as\n\u03b2Xi\u00b5k = \u2211mi j=1 \u03b3j(k) xij + r\u00b5k \u2211mi\nj=1 \u03b3j(k) + r (9)\n\u03b2Xi\u03c3k =\n\u2211mi j=1 \u03b3j(k) x 2 ij + r(\u00b5 2 k + \u03c3 2\nk) \u2211mi\nj=1 \u03b3j(k) + r \u2212 (\u03b2Xi\u00b5k ) 2 (10)\nThese updates can be derived using the general MAP estimation equations [22][23] . The factor r controls the amount by which the parameters (\u00b5k , \u03c3k) from M affect the new estimates \u03b2Xi\u00b5k and \u03b2 Xi \u03c3k\n. Although there is a known update form for mixture weights, it does not directly come from MAP estimation and we do not use employ mixture weight updates in our work. In fact the authors of miFV uses VLFeat [24] for Fisher Vector implementation which also considers only mean (Eq 7) and variance (Eq 8) parts. The Fisher Vectors are thus 2KD dimensional. We concatenate \u03b2Xi\u00b5k and \u03b2 Xi \u03c3k\nfor all GMM components to obtain the 2KD dimensional extended vector representation for the bag Xi.\nGMM-MAP adaptation is a state of art method for speaker identification [22] [25] and the concatenated vectors over all K components are referred as Supervector. Hence, we name our proposed method for multiple instance learning as miSUP. As stated before, we can use both mean (Eq 9) and variance (Eq 10) vectors in our Supervector representation for bags. However, as we show empirically later that the concatenation of only means updates (Eq 9) are sufficient to obtain reasonably good results. This is an important aspect of miSUP because the dimensionality of the bag representation will be now only KD instead of 2KD in miFV. This reduction by half can be significant if KD is large and can speed up the learning process in the next stage where classifiers are trained. It can also save a significant amount of storage space if we go into really large scale audio (multimedia) content analysis problems where feature vectors of possibly millions or billions of recordings (bags) are to be stored. We will refer to the mean only Supervector based miSUP method as miSUP MN. Overall, we will be looking into a total of 5 MIL (miSVM, MISVM, miFV, miSUP, miSUP MN) based frameworks for weak labels based AED."}, {"heading": "4. EXPERIMENTS AND RESULTS", "text": "We used TRECVID-MED 2011 database [26] for evaluation of different MIL algorithms. Weak labels are obtained for 8 acoustic events on a subset of the database. Binary classifiers are learned to detect presence or absence of each of these events. The total number of recordings or bags used in the experiment is 457. From here on we will refer to this set of recordings as dataset. The total size of the dataset is about 22 hours. The number of positive bags for each event in the dataset is as follows. Cheering-171, Children Voices - 33, Clapping - 102, Crowd - 142, Drums - 25, Engine Noise\n- 80, Laughing - 102 Scraping - 30. Please note that a bag can be positive for multiple events and it can also be possibly negative for all 8 events considered. The dataset is divided into 4 sets. 3 sets are used for training the model which is then tested on the fourth set. We do this all 4 ways (i.e each set becomes the test set) so that results on the whole dataset is obtained.\nWe use a uniform segmentation scheme on the recordings to obtain the instances for each bag. Each instance is a one second segment of the recording, with segmentation done in an overlapping manner (50% overlap). The number of instances varies significantly from bag to bag due to variation in the length of the recordings. We use 21-dimensional Mel Frequency Cepstrum Coefficient (MFCC) vectors to represent audio over which soft-count representations ~PM as described in 2.1 are obtained. MFCC vectors are computed over an analysis window of 20ms with 10ms overlap between adjacent windows. We use M = 64 and M = 128 for ~PM . All experiments are performed for both representations. Although, features representing audio segments are important, due to space constraints we would be showing only better of the two results (M = 64,M = 128) for all events under a given MIL algorithm. We use linear SVMs (LIBLINEAR [27]) in all MIL algorithms.\nOverall Detection Results: Average Precision (AP) for each event is used as performance metric. The Mean Average Precision over all events is also shown for each case. Table 1 shows AP numbers for all events and methods. The first important noticeable aspect in Table 1 is that miFV and miSUP are far more superior compared to miSVM and MISVM. There is an absolute improvement of about 16 \u2212 17% in MAP value using these methods. For several events miFV and miSUP improves AP by more than 20% over miSVM and MISVM. MISVM seems to be marginally better than miSVM. Also, miFV seems to be performing slightly better than miSUP. We make note of the fact that we used improved Fisher Vectors (IFV) [21] in miFV approach. Its possible that some normalization techniques for miSUP might lead to better results. The miSUP MN method with only half the dimension of miFV and miSUP is competitive with both methods by giving reasonably good performance.\nAn important parameter in miFV and miSUP approach is the component size K of GMM (M) learned in instance\nspace. It determines the overall dimensionality of vector representing bags and can affect the learning in a major way. We study this aspect for 7 different values 1, 4, 8, 16, 32, 64, 128.\nTable 4 shows the AP values corresponding to each of these values for miFV and miSUP methods.\nAnalysis of Algorithms: We try to analyze the average training time for each MIL algorithm. For a given MIL algorithm the average training times for each event is noted and then a mean training time is obtained by averaging over all events. This comparison is shown in Figure 1. The y-axis shows log of the mean training times in seconds. Clearly, miSUP and miFV are much faster (about 20 to 100 times) compared to miSVM and MISVM. The order of difference in time is actually higher for several individual events. This demonstrates the higher scalability of miFV and miSUP compared to miSVM and MISVM. The comparison between miFV and miSUP is not completely fair because Fisher Vectors were implemented using [24] which is possibly a very optimized implementation. With optimized implementation miSUP should be closely comparable to miFV. For classifiers such as linear SVMs the gain obtained by reducing dimension with MISUP MN does not give substantial reduction in classifier training time and is similar to MISUP MN. However, its important to note that for other classifiers this gain can be significant."}, {"heading": "5. DISCUSSIONS AND CONCLUSIONS", "text": "In this paper our goal was to address scalability issues of audio event detection. We addressed scalibility issues on two fronts. First, is directly related to audio event detection where obtaining strongly labeled data is extremely difficult and expensive. This puts constraints in terms of vocabulary of audio events and amount of training data available for audio events. The goal of this work was to show a way where these scaling issues can be addressed. Our assertion is that obtaining weak labels is much easier compared to strong labels. We then, successfully showed that multiple instance learning framework can be used to learn from weakly labeled data. The second front is the scalability of MIL algorithm itself for AED tasks. Standard MIL algorithms such as miSVM and MISVM can run into scalability issues because number of instances will run into hundreds of thousands even for medium sized audio dataset. Hence, we considered algorithms which are specifically meant to address scalability in multiple instance learning. We proposed a novel MIL algorithm and showed that it is competitive with the other scalable MIL algorithm existing in literature. Interestingly, for audio event detection these\nscalable MIL algorithms turned out to be far more superior compared to standard MIL algorithms such as miSVM and MISVM which are known to perform well in general. Overall miFV performed best and miSUP and miSUP MN came close to it. We also showed that MAP adaptation of means only (Eq 9, miSUP MN) where the dimension of bag vector representation is half of miFV can work reasonably well. They can not only address memory issues due to lower dimensions but can also reduce classifier training time. Even though data size (22 hours) was limited, our overall approach and good performance on this dataset does throw some light on scalable audio content analysis."}, {"heading": "6. REFERENCES", "text": "[1] G Valenzise, L Gerosa, M Tagliasacchi, F Antonacci, and A Sarti, \u201cScream and gunshot detection and localization for audio-surveillance systems,\u201d in Advanced Video and Signal Based Surveillance, IEEE Conference on, 2007, pp. 21\u201326. [2] J Ruiz-Mun\u0303oz, M Orozco-Alzate, and G CastellanosDominguez, \u201cMultiple instance learning-based birdsong classification using unsupervised recording segmentation,\u201d in Proceedings of the 24th Intl. Conf. on Artificial Intelligence, 2015. [3] Dan Stowell and Mark D Plumbley, \u201cAutomatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning,\u201d PeerJ, 2014. [4] J Eronen, T Peltonen, T Tuomi, P Klapuri, S Fagerlund, T Sorsa, G Lorho, and J Huopaniemi, \u201cAudio-based context recognition,\u201d Audio, Speech, and Language Processing, IEEE Trans. on, 2006. [5] Xiaodan Zhuang, Xi Zhou, Mark A Hasegawa-Johnson, and Thomas S Huang, \u201cReal-world acoustic event detection,\u201d Pattern Recognition Letters, vol. 31, no. 12, pp. 1543\u20131551, 2010. [6] S Pancoast and M Akbacak, \u201cBag-of-audio-words approach for multimedia event classification,\u201d in Interspeech, 2012. [7] A Plinge, R Grzeszick, G Fink, et al., \u201cA bag-of-features approach to acoustic event detection,\u201d in IEEE ICASSP, 2014. [8] A Kumar, P Dighe, R Singh, S Chaudhuri, and B Raj, \u201cAudio event detection from acoustic unit occurrence patterns,\u201d in IEEE ICASSP, 2012, pp. 489\u2013492. [9] Ehsan Amid, Annamaria Mesaros, Kalle J Palomaki, Jorma Laaksonen, and Mikko Kurimo, \u201cUnsupervised feature extraction for multimedia event detection and ranking using audio content,\u201d in IEEE ICASSP, 2014, pp. 5939\u20135943. [10] K Ashraf, B Elizalde, F Iandola, M Moskewicz, J Bernd, G Friedland, and K Keutzer, \u201cAudio-based multimedia event detection with DNNs and sparse sampling,\u201d in Proc. of the 5th ACM International Conference on Multimedia Retrieval, 2015. [11] Oguzhan Gencoglu, Tuomas Virtanen, and Heikki Huttunen, \u201cRecognition of acoustic events using deep neural networks,\u201d\nin Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European. IEEE, 2014, pp. 506\u2013510. [12] A Dessein, A Cont, and G Lemaitre, \u201cReal-time detection of overlapping sound events with non-negative matrix factorization,\u201d in Matrix Information Geometry. Springer, 2013. [13] E Cakir, T Heittola, H Huttunen, and T Virtanen, \u201cPolyphonic sound event detection using multi label deep neural networks,\u201d in Intl. Joint Conf on Neural Networks. IEEE, 2015. [14] Y Song and C Zhang, \u201cContent-based information fusion for semi-supervised music genre classification,\u201d Multimedia, IEEE Transactions on, pp. 145\u2013152, 2008. [15] X Wei, J Wu, and Z Zhou, \u201cScalable multi-instance learning,\u201d in Data Mining (ICDM), 2014 IEEE Intl. Conf. on. IEEE, 2014. [16] T G Dietterich, R H Lathrop, and T Lozano-Pe\u0301rez, \u201cSolving the multiple instance problem with axis-parallel rectangles,\u201d Artificial Intelligence, vol. 89, no. 1, pp. 31\u201371, 1997. [17] Jaume Amores, \u201cMultiple instance classification: Review, taxonomy and comparative study,\u201d Artificial Intelligence, vol. 201, pp. 81\u2013105, 2013. [18] G Doran and S Ray, \u201cA theoretical and empirical analysis of support vector machine methods for multiple-instance classification,\u201d Machine Learning, pp. 79\u2013102, 2014. [19] A Kumar, R M Hegde, R Singh, and B Raj, \u201cEvent detection in short duration audio using gaussian mixture model and random forest classifier,\u201d in 21st European Signal Processing Conference 2013 (EUSIPCO 2013), 2013. [20] Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann, \u201cSupport vector machines for multiple-instance learning,\u201d NIPS, pp. 561\u2013568, 2002. [21] J Sa\u0301nchez, F Perronnin, T Mensink, and J Verbeek, \u201cImage classification with the fisher vector: Theory and practice,\u201d International Journal of Computer Vision, 2013. [22] Fre\u0301de\u0301ric Bimbot and et al., \u201cA tutorial on text-independent speaker verification,\u201d EURASIP journal on applied signal processing, vol. 2004, pp. 430\u2013451, 2004. [23] J Gauvain and C Lee, \u201cMaximum a posteriori estimation for multivariate gaussian mixture observations of markov chains,\u201d Speech and audio processing, IEEE Trans. on, 1994. [24] A. Vedaldi and B. Fulkerson, \u201cVLFeat: An open and portable library of computer vision algorithms,\u201d 2008. [25] W Campbell, Douglas Sturim, and A Reynolds, \u201cSupport vector machines using gmm supervectors for speaker verification,\u201d Signal Processing Letters, IEEE, pp. 308\u2013311, 2006. [26] \u201cMultimedia event detection,\u201d www.nist.gov/itl/iad/mig/med11.cfm. [27] Rong-En Fan, K Chang, C Hsieh, X Wang, and C Lin, \u201cLiblinear: A library for large linear classification,\u201d The Journal of Machine Learning Research, 2008."}], "references": [{"title": "Scream and gunshot detection and localization for audio-surveillance systems", "author": ["G Valenzise", "L Gerosa", "M Tagliasacchi", "F Antonacci", "A Sarti"], "venue": "Advanced Video and Signal Based Surveillance, IEEE Conference on, 2007, pp. 21\u201326.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiple instance learning-based birdsong classification using unsupervised recording segmentation", "author": ["J Ruiz-Mu\u00f1oz", "M Orozco-Alzate", "G Castellanos- Dominguez"], "venue": "Proceedings of the 24th Intl. Conf. on Artificial Intelligence, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning", "author": ["Dan Stowell", "Mark D Plumbley"], "venue": "PeerJ, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Audio-based context recognition", "author": ["J Eronen", "T Peltonen", "T Tuomi", "P Klapuri", "S Fagerlund", "T Sorsa", "G Lorho", "J Huopaniemi"], "venue": "Audio, Speech, and Language Processing, IEEE Trans. on, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Real-world acoustic event detection", "author": ["Xiaodan Zhuang", "Xi Zhou", "Mark A Hasegawa-Johnson", "Thomas S Huang"], "venue": "Pattern Recognition Letters, vol. 31, no. 12, pp. 1543\u20131551, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Bag-of-audio-words approach for multimedia event classification", "author": ["S Pancoast", "M Akbacak"], "venue": "Interspeech, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A bag-of-features approach to acoustic event detection", "author": ["A Plinge", "R Grzeszick", "G Fink"], "venue": "IEEE ICASSP, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Audio event detection from acoustic unit occurrence patterns", "author": ["A Kumar", "P Dighe", "R Singh", "S Chaudhuri", "B Raj"], "venue": "IEEE ICASSP, 2012, pp. 489\u2013492.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised feature extraction for multimedia event detection and ranking using audio content", "author": ["Ehsan Amid", "Annamaria Mesaros", "Kalle J Palomaki", "Jorma Laaksonen", "Mikko Kurimo"], "venue": "IEEE ICASSP, 2014, pp. 5939\u20135943.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Audio-based multimedia event detection with DNNs and sparse sampling", "author": ["K Ashraf", "B Elizalde", "F Iandola", "M Moskewicz", "J Bernd", "G Friedland", "K Keutzer"], "venue": "Proc. of the 5th ACM International Conference on Multimedia Retrieval, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["Oguzhan Gencoglu", "Tuomas Virtanen", "Heikki Huttunen"], "venue": " in Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European. IEEE, 2014, pp. 506\u2013510.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time detection of overlapping sound events with non-negative matrix factorization", "author": ["A Dessein", "A Cont", "G Lemaitre"], "venue": "Matrix Information Geometry. Springer, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["E Cakir", "T Heittola", "H Huttunen", "T Virtanen"], "venue": "Intl. Joint Conf on Neural Networks. IEEE, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Content-based information fusion for semi-supervised music genre classification", "author": ["Y Song", "C Zhang"], "venue": "Multimedia, IEEE Transactions on, pp. 145\u2013152, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable multi-instance learning", "author": ["X Wei", "J Wu", "Z Zhou"], "venue": "Data Mining (ICDM), 2014 IEEE Intl. Conf. on. IEEE, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T G Dietterich", "R H Lathrop", "T Lozano-P\u00e9rez"], "venue": "Artificial Intelligence, vol. 89, no. 1, pp. 31\u201371, 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["Jaume Amores"], "venue": "Artificial Intelligence, vol. 201, pp. 81\u2013105, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A theoretical and empirical analysis of support vector machine methods for multiple-instance classification", "author": ["G Doran", "S Ray"], "venue": "Machine Learning, pp. 79\u2013102, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Event detection in short duration audio using gaussian mixture model and random forest classifier", "author": ["A Kumar", "R M Hegde", "R Singh", "B Raj"], "venue": "21st European Signal Processing Conference 2013 (EUSIPCO 2013), 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Support vector machines for multiple-instance learning", "author": ["Stuart Andrews", "Ioannis Tsochantaridis", "Thomas Hofmann"], "venue": "NIPS, pp. 561\u2013568, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J S\u00e1nchez", "F Perronnin", "T Mensink", "J Verbeek"], "venue": "International Journal of Computer Vision, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial on text-independent speaker verification", "author": ["Fr\u00e9d\u00e9ric Bimbot"], "venue": "EURASIP journal on applied signal processing, vol. 2004, pp. 430\u2013451, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains", "author": ["J Gauvain", "C Lee"], "venue": "Speech and audio processing, IEEE Trans. on, 1994.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Support vector machines using gmm supervectors for speaker verification", "author": ["W Campbell", "Douglas Sturim", "A Reynolds"], "venue": "Signal Processing Letters, IEEE, pp. 308\u2013311, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "K Chang", "C Hsieh", "X Wang", "C Lin"], "venue": "The Journal of Machine Learning Research, 2008. 6", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Several applications such as surveillance [1] and monitoring are much easier to do through audio data only.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "An excellent example for audio based monitoring systems are those used in wildlife monitoring, for example bird species recognition using birdsong audio [2] [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "An excellent example for audio based monitoring systems are those used in wildlife monitoring, for example bird species recognition using birdsong audio [2] [3].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Moreover, audio event detection plays an important role for context aware systems [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Taking cues from automatic speech recognition GMM-HMM architectures have been employed for AED as well[5].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "Discriminative learning methods in which fixed length representations for audio segments combined with discriminative classifiers such as Support Vector Machine or Random Forest classifiers have also been proposed [6] [7] [8][9].", "startOffset": 214, "endOffset": 217}, {"referenceID": 6, "context": "Discriminative learning methods in which fixed length representations for audio segments combined with discriminative classifiers such as Support Vector Machine or Random Forest classifiers have also been proposed [6] [7] [8][9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 7, "context": "Discriminative learning methods in which fixed length representations for audio segments combined with discriminative classifiers such as Support Vector Machine or Random Forest classifiers have also been proposed [6] [7] [8][9].", "startOffset": 222, "endOffset": 225}, {"referenceID": 8, "context": "Discriminative learning methods in which fixed length representations for audio segments combined with discriminative classifiers such as Support Vector Machine or Random Forest classifiers have also been proposed [6] [7] [8][9].", "startOffset": 225, "endOffset": 228}, {"referenceID": 9, "context": "With success of Deep neural networks for ASR, audio event detection using DNNs were attempted in [10] [9] [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "With success of Deep neural networks for ASR, audio event detection using DNNs were attempted in [10] [9] [11].", "startOffset": 102, "endOffset": 105}, {"referenceID": 10, "context": "With success of Deep neural networks for ASR, audio event detection using DNNs were attempted in [10] [9] [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Techniques such as matrix factorization and deep neural networks have been used for this more challenging tasks [12][13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Techniques such as matrix factorization and deep neural networks have been used for this more challenging tasks [12][13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "A particularly interesting and related work is classification of bird species [2] using weakly labeled birdsong audio through Multiple Instance Learning (MIL).", "startOffset": 78, "endOffset": 81}, {"referenceID": 13, "context": "In another work, MIL was used for music genre classification using weak labels [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "We propose a novel scalable multiple instance learning which is based on ideas similar to another scalable MIL algorithm [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "Starting from its utility in drug activity detection[16], several algorithms such as Diverse Density, MILES, Citation-KNN, MIBoosting, MiGraph, SVM based approaches etc.", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "have been proposed to learn in this paradigm [17][18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "have been proposed to learn in this paradigm [17][18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "[18] [17] give excellent surveys of theory and applications of MIL.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] [17] give excellent surveys of theory and applications of MIL.", "startOffset": 5, "endOffset": 9}, {"referenceID": 5, "context": "Audio Feature Representation Bag of audio words is a simple yet effect way of representing acoustic events for detection and classification purposes [6][9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 8, "context": "Audio Feature Representation Bag of audio words is a simple yet effect way of representing acoustic events for detection and classification purposes [6][9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 18, "context": "[19] showed that it is better to use soft count histogram obtained using Gaussian Mixture Model (GMM) for", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "The first two algorithm referred as miSVM and MISVM [20] are based on Support Vector Machine (SVM).", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "Although a few other formulations of SVM for MIL domain have been proposed [18], miSVM and MISVM were the first set of SVM formulations for MIL and performs well on a variety of MIL tasks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "miSVM and MISVM miSVM is the first method proposed in [20] to directly solve MIL using SVM.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "More details can be found in the original paper [20].", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Scalable Algorithms Most of the MIL algorithms including miSVM and MISVM suffers from scalability issue [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "A scalable multiple instance learning algorithm called miFV was proposed in [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "request the reader to refer to [21] for a detailed description of Fisher Kernels and Fisher Vectors.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "[15] claimed that under the assumption of independence for instances in the bags, it can also be used for encoding the bags in MIL framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "An improved fisher vector (IFV) obtained by sign square rooting and L2 normalization of the original Fisher vector leads to superior performance [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "These updates can be derived using the general MAP estimation equations [22][23] .", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "These updates can be derived using the general MAP estimation equations [22][23] .", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "In fact the authors of miFV uses VLFeat [24] for Fisher Vector implementation which also considers only mean (Eq 7) and variance (Eq 8) parts.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "GMM-MAP adaptation is a state of art method for speaker identification [22] [25] and the concatenated vectors over all K components are referred as Supervector.", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "GMM-MAP adaptation is a state of art method for speaker identification [22] [25] and the concatenated vectors over all K components are referred as Supervector.", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "We use linear SVMs (LIBLINEAR [27]) in all MIL algorithms.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "We make note of the fact that we used improved Fisher Vectors (IFV) [21] in miFV approach.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "The comparison between miFV and miSUP is not completely fair because Fisher Vectors were implemented using [24] which is possibly a very optimized implementation.", "startOffset": 107, "endOffset": 111}], "year": 2016, "abstractText": "Audio Event Detection is an important task for content analysis of multimedia data. Most of the current works on detection of audio events is driven through supervised learning approaches. We propose a weakly supervised learning framework which can make use of the tremendous amount of web multimedia data with significantly reduced annotation effort and expense. Specifically, we use several multiple instance learning algorithms to show that audio event detection through weak labels is feasible. We also propose a novel scalable multiple instance learning algorithm and show that its competitive with other multiple instance learning algorithms for audio event detection tasks.", "creator": "LaTeX with hyperref package"}}}