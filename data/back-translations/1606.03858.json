{"id": "1606.03858", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Sorting out typicality with the inverse moment matrix SOS polynomial", "abstract": "We are investigating a surprising phenomenon in connection with the representation of a cloud of data points using polynomials. We begin with the previously unnoticed empirical observation that, in a collection (of data points), the subsets of a certain differentiated polynomial capture the shape of the cloud very precisely. This differentiated polynomial is a sum of squares (SOS), which is easily derived from the inversion of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function, which allows us to generalize and interpret extremity properties of orthogonal polynomials and provide a mathematical justification for the observed phenomenon.", "histories": [["v1", "Mon, 13 Jun 2016 08:55:20 GMT  (1295kb,D)", "https://arxiv.org/abs/1606.03858v1", null], ["v2", "Tue, 14 Jun 2016 08:02:03 GMT  (1295kb,D)", "http://arxiv.org/abs/1606.03858v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["edouard pauwels", "jean b lasserre"], "accepted": true, "id": "1606.03858"}, "pdf": {"name": "1606.03858.pdf", "metadata": {"source": "CRF", "title": "Sorting out typicality with the inverse moment matrix SOS polynomial", "authors": ["Jean-Bernard Lasserre"], "emails": ["lasserre@laas.fr", "edouard.pauwels@irit.fr"], "sections": [{"heading": "1 Introduction", "text": "Capturing and summarizing the global shape of a cloud of points is at the heart of many data processing applications such as novelty detection, outlier detection as well as related unsupervised learning tasks such as clustering and density estimation. One of the main difficulties is to account for potentially complicated shapes in multidimensional spaces, or equivalently to account for non standard dependence relations between variables. Such relations become critical in applications, for example in fraud detection where a fraudulent action may be the dishonest combination of several actions, each of them being reasonable when considered on their own.\nAccounting for complicated shapes is also related to computational geometry and nonlinear algebra applications, for example integral computation [9] and reconstruction of sets from moments data [4, 5, 10]. Some of these problems have connections and potential applications in machine learning. The work presented in this paper brings together ideas from both disciplines, leading to a method which allows to encode in a simple manner the global shape and spatial concentration of points within a cloud.\nWe start with a surprising (and apparently unnoticed) empirical observation. Given a collection of points, one may build up a distinguished sum-of-squares (SOS) polynomial whose coefficients (or Gram matrix) is the inverse of the empirical moment matrix (see Section 3). Its degree depends on how many moments are considered, a choice left to the user. Remarkably its sublevel sets capture much of the global shape of the cloud as illustrated in Figure 3. This phenomenon is not incidental as illustrated in many additional examples in Appendix A. To the best of our knowledge, this observation has remained unnoticed and the purpose of this paper is to report this empirical finding to the machine learning community and provide first elements toward a mathematical understanding as well as potential machine learning applications.\nar X\niv :1\n60 6.\n03 85\n8v 2\n[ cs\n.L G\n] 1\n4 Ju\nn 20\nThe proposed method is based on the computation of the coefficients of a very specific polynomial which depends solely on the empirical moments associated with the data points. From a practical perspective, this can be done via a single pass through the data, or even in an online fashion via a sequence of efficient Woodbury updates. Furthermore the computational cost of evaluating the polynomial does not depend on the number of data points which is a crucial difference with existing nonparametric methods such as nearest neighbors or kernel based methods [1]. On the other hand, this computation requires the inversion of a matrix whose size depends on the dimension of the problem (see Section 3). Therefore, the proposed framework is suited for moderate dimensions and potentially very large number of observations.\nIn Section 4 we first describe an affine invariance result which suggests that the distinguished SOS polynomial captures very intrinsic properties of clouds of points. In a second step, we provide a mathematical interpretation that supports our empirical findings based on connections with orthogonal polynomials [3]. We propose a generalization of a well known extremality result for orthogonal univariate polynomials on the real line (or the complex plane) [13, Theorem 3.1.2]. As a consequence, the distinguished SOS polynomial of interest in this paper is understood as the unique optimal solution of a convex optimization problem: minimizing an average value over a structured set of positive polynomials. In addition, we revisit [13, Theorem 3.5.6] about the Christoffel function. The mathematics behind provide a simple and intuitive explanation for the phenomenon that we empirically observed.\nFinally, in Section 5 we perform numerical experiments on KDD cup network intrusion dataset [11]. Evaluation of the distinguished SOS polynomial provides a score that we use as a measure of outlyingness to detect network intrusions (assuming that they correspond to outlier observations). We refer the reader to [1] for a discussion of available methods for this task. For the sake of a fair comparison we have reproduced the experiments performed in [14] for the same dataset. We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14]."}, {"heading": "2 Multivariate polynomials and moments", "text": ""}, {"heading": "2.1 Notations", "text": "We fix the ambient dimension to be p throughout the text. For example, we will manipulate vectors in Rp as well as p-variate polynomials with real coefficients. We denote by X a set of p variables X1, . . . , Xp which we will use in mathematical expressions defining polynomials. We identify\nmonomials from the canonical basis of p-variate polynomials with their exponents in Np: we associate to \u03b1 = (\u03b1i)i=1...p \u2208 Np the monomial X\u03b1 := X\u03b111 X \u03b12 2 . . . X \u03b1p p which degree is deg(\u03b1) :=\u2211p\ni=1 \u03b1i. We use the expressions <gl and \u2264gl to denote the graded lexicographic order, a well ordering over p-variate monomials. This amounts to, first, use the canonical order on the degree and, second, break ties in monomials with the same degree using the lexicographic order with X1 = a,X2 = b . . . For example, the monomials in two variables X1, X2, of degree less or equal to 3 listed in this order are given by: 1, X1, X2, X21 , X1X2, X 2 2 , X 3 1 , X 2 1X2, X1X 2 2 , X 3 2 .\nWe denote by Npd, the set {\u03b1 \u2208 Np; deg(\u03b1) \u2264 d} ordered by \u2264gl. R[X] denotes the set of p-variate polynomials: linear combinations of monomials with real coefficients. The degree of a polynomial is the highest of the degrees of its monomials with nonzero coefficients1. We use the same notation, deg(\u00b7), to denote the degree of a polynomial or of an element of Np. For d \u2208 N, Rd[X] denotes the set of p-variate polynomials of degree less or equal to d. We set s(d) = ( p+d d ) , the number of monomials of degree less or equal to d. We will denote by vd(X) the vector of monomials of degree less or equal to d sorted by \u2264gl. We let vd(X) := (X\u03b1)\u03b1\u2208Npd \u2208 Rd[X]\ns(d). With this notation, we can write a polynomial P \u2208 Rd[X] as follows P (X) = \u3008p,vd(X)\u3009 for some real vector of coefficients p = (p\u03b1)\u03b1\u2208Npd \u2208 R\ns(d) ordered using \u2264gl. Given x = (xi)i=1...p \u2208 Rp, P (x) denotes the evaluation of P with the assignments X1 = x1, X2 = x2, . . . Xp = xp. Given a Borel probability measure \u00b5 and \u03b1 \u2208 Np, y\u03b1(\u00b5) denotes the moment \u03b1 of \u00b5: y\u03b1(\u00b5) = \u222b Rp x\n\u03b1d\u00b5(x). Throughout the paper, we will only consider measures of which all moments are finite."}, {"heading": "2.2 Moment matrix", "text": "Given a Borel probability measure \u00b5 on Rp, the moment matrix of \u00b5, Md(\u00b5), is a matrix indexed by monomials of degree at most d ordered by \u2264gl. For \u03b1, \u03b2 \u2208 Npd, the corresponding entry in Md(\u00b5) is defined by Md(\u00b5)\u03b1,\u03b2 := y\u03b1+\u03b2(\u00b5), the moment \u03b1+ \u03b2 of \u00b5. When p = 2, letting y\u03b1 = y\u03b1(\u00b5) for \u03b1 \u2208 N24, we have\nM2(\u00b5) :\n1 X1 X2 X 2 1 X1X2 X 2 2\n1 1 y10 y01 y20 y11 y02 X1 y10 y20 y11 y30 y21 y12 X2 y01 y11 y02 y21 y12 y03 X21 y20 y30 y21 y40 y31 y22 X1X2 y11 y21 y12 y31 y22 y13 X22 y02 y12 y03 y22 y13 y04 .\nMd(\u00b5) is positive semidefinite for all d \u2208 N. Indeed, for any p \u2208 Rs(d), let P \u2208 Rd[X] be the polynomial with vector of coefficients p, we have pTMd(\u00b5)p = \u222b Rp P\n2(x)d\u00b5(x) \u2265 0. Furthermore, we have the identity Md(\u00b5) = \u222b Rp vd(x)vd(x) T d\u00b5(x) where the integral is understood elementwise."}, {"heading": "2.3 Sum of squares (SOS)", "text": "We denote by \u03a3[X] \u2282 R[X] (resp. \u03a3d[X] \u2282 Rd[X]), the set of polynomials (resp. polynomials of degree at most d) which can be written as a sum of squares of polynomials. Let P \u2208 R2m[X] for some m \u2208 N, then P belongs to \u03a32m[X] if there exists a finite J \u2282 N and a family of polynomials Pj \u2208 Rm[X], j \u2208 J , such that P = \u2211 j\u2208J P 2 j . It is obvious that sum of squares polynomials are always nonnegative. A further interesting property is that this class of polynomials is connected with positive semidefiniteness. Indeed, P belongs to \u03a32m[X] if and only if\n\u2203Q \u2208 Rs(m)\u00d7s(m), Q 0, P (x) = vd(x)TQvd(x), \u2200x \u2208 Rp. (1)\nAs a consequence, every positive semidefinite matrix Q \u2208 Rs(m)\u00d7s(m) defines a polynomial in \u03a32m[X] by using the representation in (1).\n1For the null polynomial, we use the convention that its degree is 0 and it is \u2264gl smaller than all other monomials."}, {"heading": "3 Empirical observations on the inverse moment matrix SOS polynomial", "text": "The inverse moment-matrix SOS polynomial is associated to a measure \u00b5 which satisfies the following.\nAssumption 1 \u00b5 is a Borel probability measure on Rp with all its moments finite and Md(\u00b5) is positive definite for a given d \u2208 N.\nDefinition 1 Let \u00b5, d satisfy Assumption 1. We call the SOS polynomial Q\u00b5,d \u2208 \u03a32d[X] defined by the application:\nx 7\u2192 Q\u00b5,d(x) := vd(x)TMd(\u00b5)\u22121vd(x), x \u2208 Rp, (2)\nthe inverse moment-matrix SOS polynomial of degree 2d associated to \u00b5.\nActually, connection to orthogonal polynomials will show that the inverse function x 7\u2192 Q\u00b5,d(x)\u22121 is called the Christoffel function in the literature [13, 3] (see also Section 4).\nIn the remainder of this section, we focus on the situation when \u00b5 corresponds to an empirical measure over n points in Rp which are fixed. So let x1, . . . ,xn \u2208 Rp be a fixed set of points and let \u00b5 := 1n \u2211n i=1 \u03b4xi where \u03b4x corresponds to the Dirac measure at x. In such a case the polynomial Q\u00b5,d in (2) is determined only by the empirical moments up to degree 2d of our collection of points. Note that we also require that Md(\u00b5) 0. In other words, the points x1, . . . ,xn do not belong to an algebraic set defined by a polynomial of degree less or equal to d. We first describe empirical properties of inverse moment matrix SOS polynomial in this context of empirical measures. A mathematical intuition and further properties behind these observations are developped in Section 4."}, {"heading": "3.1 Sublevel sets", "text": "The starting point of our investigations is the following phenomenon which to the best of our knowledge has remained unnoticed in the literature. For the sake of clarity and simplicity we provide an illustration in the plane. Consider the following experiment in R2 for a fixed d \u2208 N: represent on the same graphic, the cloud of points {xi}i=1...n and the sublevel sets of SOS polynomial Q\u00b5,d in R2 (equivalently, the superlevel sets of the Christoffel function). This is illustrated in the left panel of Figure 3. The collection of points consists of 500 simulations of two different Gaussians and the value of d is 4. The striking feature of this plot is that the level sets capture the global shape of the cloud of points quite accurately. In particular, the level set {x : Q\u00b5,d(x) \u2264 ( p+d d ) } captures most of the points. We could reproduce very similar observations on different shapes with various number of points in R2 and degree d (see Appendix A)."}, {"heading": "3.2 Measuring outlyingness", "text": "An additional remark in a similar line is that Q\u00b5,d tends to take higher values on points which are isolated from other points. Indeed in the left panel of Figure 3, the value of the polynomial tends to be smaller on the boundary of the cloud. This extends to situations where the collection of points correspond to shape with a high density of points with a few additional outliers. We reproduce a similar experiment on the right panel of Figure 3. In this example, 1000 points are sampled close to a ring shape and 40 additional points are sampled uniformly on a larger square. We do not represent the sublevel sets of Q\u00b5,d here. Instead, the color and shape of the points are taken proportionally to the value of Q\u00b5,d, with d = 8.\nFirst, the results confirm the observation of the previous paragraph, points that fall close to the ring shape tend to be smaller and points on the boundary of the ring shape are larger. Second, there is a clear increase in the size of the points that are relatively far away from the ring shape. This highlight the fact that Q\u00b5,d tends to take higher value in less populated areas of the space."}, {"heading": "3.3 Relation to maximum likelihood estimation", "text": "If we fix d = 1, we recover the maximum likelihood estimation for the Gaussian, up to a constant additive factor. To see this, set \u00b5 = 1n \u2211n i=1 xi and S = 1 n \u2211n i=1 xix T i . With this notation, we have\nthe following block representation of the moment matrix,\nMd(\u00b5) =\n( 1 \u00b5T\n\u00b5 S\n) Md(\u00b5) \u22121 = ( 1 + \u00b5TV \u22121\u00b5 \u2212\u00b5TV \u22121 \u2212V \u22121\u00b5 V \u22121 ) ,\nwhere V = S \u2212 \u00b5\u00b5T is the empirical covariance matrix and the expression for the inverse is given by Schur complement. In this case, we have Q\u00b5,1(x) = 1 + (x\u2212 \u00b5)TV \u22121(x\u2212 \u00b5) for all x \u2208 Rp. We recognize the quadratic form that appears in the density function of the multivariate Gaussian with parameters estimated by maximum likelihood. This suggests a connection between the inverse SOS moment polynomial and maximum likelihood estimation. Unfortunately, this connection is difficult to generalize for higher values of d and we do not pursue the idea of interpreting the empirical observations of this section through the prism of maximum likelihood estimation and leave it for further research. Instead, we propose an alternative view in Section 4."}, {"heading": "3.4 Computational aspects", "text": "Recall that s(d) = ( p+d d ) is the number of p-variate monomials of degree up to d. The computation of Q\u00b5,d requires O(ns(d)2) operations for the computation of the moment matrix and O(s(d)3) operations for the matrix inversion. The evaluation of Q\u00b5,d requires O(s(d)2) operations.\nEstimating the coefficients of Q\u00b5,d has a computational cost that depends only linearly in the number of points n. The cost of evaluating Q\u00b5,d is constant with respect to the number of points n. This is an important contrast with kernel based or distance based methods (such as nearest neighbors and one class SVM) for density estimation or outlier detection since they usually require at least O(n2) operations for the evaluation of the model [1]. Moreover, this is well suited for online settings where inverse moment matrix computation can be done using Woodbury updates.\nThe dependence in the dimension p is of the order of pd for a fixed d. Similarly, the dependence in d is of the order of dp for a fixed dimension p and the joint dependence is exponential. This suggests that the computation and evaluation of Q\u00b5,d will mostly make sense for moderate dimensions and degree d."}, {"heading": "4 Invariances and interpretation through orthogonal polynomials", "text": "The purpose of this section is to provide a mathematical rationale that explains the empirical observations made in Section 3. All the proofs are postponed to Appendix B. We fix a Borel probability measure \u00b5 on Rp which satisfies Assumption 1. Note that Md(\u00b5) is always positive definite if \u00b5 is not supported on the zero set of a polynomial of degree at most d. Under Assumption 1, Md(\u00b5) induces an inner product on Rs(d) and by extension on Rd[X] (see Section 2). This inner product is denoted by \u3008\u00b7, \u00b7\u3009\u00b5 and satisfies for any polynomials P,Q \u2208 Rd[X] with coefficients p,q \u2208 Rs(d),\n\u3008P,Q\u3009\u00b5 := \u3008p,Md(\u00b5)q\u3009Rs(d) = \u222b Rp P (x)Q(x)d\u00b5(x).\nWe will also use the canonical inner product over Rd[X] which we write \u3008P,Q\u3009Rd[X] := \u3008p,q\u3009Rs(d) for any polynomials P,Q \u2208 Rd[X] with coefficients p,q \u2208 Rs(d). We will omit the subscripts for this canonical inner product and use \u3008\u00b7, \u00b7\u3009 for both products."}, {"heading": "4.1 Affine invariance", "text": "It is worth noticing that the mapping x 7\u2192 Q\u00b5,d(x) does not depend on the particular choice of vd(X) as a basis of Rd[X], any other basis would lead to the same mapping. This leads to the result that Q\u00b5,d captures affine invariant properties of \u00b5.\nLemma 1 Let \u00b5 satisfy Assumption 1 and A \u2208 Rp\u00d7p, b \u2208 Rp define an invertible affine mapping on Rp,A : x\u2192 Ax+b. Then, the push foward measure, defined by \u00b5\u0303(S) = \u00b5(A\u22121(S)) for all Borel sets S \u2282 Rp, satisfies Assumption 1 (with the same d as \u00b5) and for all x \u2208 Rp, Q\u00b5,d(x) = Q\u00b5\u0303,d(Ax+ b).\nLemma 1 is probably better understood when \u00b5 = 1/n \u2211n i=1 \u03b4xi as in Section 3. In this case, we\nhave \u00b5\u0303 = 1/n \u2211n i=1 \u03b4Axi+b and Lemma 1 asserts that the level sets of Q\u00b5\u0303,d are simply the images of those of Q\u00b5,d under the affine transformation x 7\u2192 Ax + b. This is illustrated in Appendix D."}, {"heading": "4.2 Connection with orthogonal polynomials", "text": "We define a classical [13, 3] family of orthonormal polynomials, {P\u03b1}\u03b1\u2208Npd ordered according to \u2264gl which satisfies for all \u03b1 \u2208 Npd\n\u3008P\u03b1, X\u03b2\u3009 = 0 if \u03b1 <gl \u03b2, \u3008P\u03b1, P\u03b1\u3009\u00b5 = 1, \u3008P\u03b1, X\u03b2\u3009\u00b5 = 0 if \u03b2 <gl \u03b1, \u3008P\u03b1, X\u03b1\u3009\u00b5 > 0. (3)\nIt follows from (3) that \u3008P\u03b1, P\u03b2\u3009\u00b5 = 0 if \u03b1 6= \u03b2. Existence and uniqueness of such a family is guaranteed by the Gram-Schmidt orthonormalization process following the \u2264gl order on the monomials, and by the positivity of the moment matrix, see for instance [3, Theorem 3.1.11].\nLet Dd(\u00b5) be the lower triangular matrix which rows are the coefficients of the polynomials P\u03b1 defined in (3) ordered by \u2264gl. It can be shown that Dd(\u00b5) = Ld(\u00b5)\u2212T , where Ld(\u00b5) is the Cholesky factorization of Md(\u00b5). Furthermore, there is a direct relation with the inverse moment matrix as Md(\u00b5) \u22121 = Dd(\u00b5) TDd(\u00b5) [7, Proof of Theorem 3.1]. This has the following consequence.\nLemma 2 Let \u00b5 satisfy Assumption 1, then Q\u00b5,d = \u2211 \u03b1\u2208Npd\nP 2\u03b1, where the family {P\u03b1}\u03b1\u2208Npd is defined by (3) and \u222b Rp Q\u00b5,d(x)d\u00b5(x) = s(d).\nThat is, Q\u00b5,d is a very specific and distinguished SOS polynomial, the sum of squares of the orthonormal basis elements {P\u03b1}\u03b1\u2208Npd of Rd(X) (w.r.t. \u00b5). Furthermore, the average value of Q\u00b5,d with respect to \u00b5 is s(d) which corresponds to the red level set in left panel of Figure 3."}, {"heading": "4.3 A variational formulation for the inverse moment matrix SOS polynomial", "text": "In this section, we show that the family of polynomials {P\u03b1}\u03b1\u2208Npd defined in (3) is the unique solution (up to a multiplicative constant) of a convex optimization problem over polynomials. This fact combined with Lemma 2 provides a mathematical rationale for the empirical observations outlined in Section 3. Consider the following optimization problem.\nmin Q\u03b1,\u03b8\u03b1,\u03b1\u2208Npd\n1\n2 \u222b Rp \u2211 \u03b1\u2208Npd Q\u03b1(x) 2d\u00b5(x) (4)\ns.t. q\u03b1\u03b1 \u2265 exp(\u03b8\u03b1), q\u03b1\u03b2 = 0, \u03b1, \u03b2 \u2208 Npd, \u03b1 <gl \u03b2, \u2211 \u03b1\u2208Npd \u03b8\u03b1 = 0,\nwhere Q\u03b1(x) = \u2211 \u03b2\u2208Npd q\u03b1\u03b2x \u03b2 , \u03b1 \u2208 Npd. We first comment on problem (4). Let P = \u2211 \u03b1\u2208Npd\nQ2\u03b1 be the SOS polynomial appearing in the objective function of (4). The constraints of problem (4) restrict P to be in a certain set Sd \u2282 \u03a3d[X]. With this notation, problem (4) is reformulated as minP\u2208Sd \u222b Pd\u00b5. Therefore problem (4) balances two antagonist targets, on one hand the minimization of the average value of the SOS polynomial P with respect to \u00b5, on the other hand the avoidance of the trivial polynomial, enforced by the constraint that P \u2208 Sd. The constraints on P are simple and natural, they ensure that P is a sum of squares of polynomials {Q\u03b1}\u03b1\u2208Npd , where the leading term of Q\u03b1 (according to the ordering \u2264gl) is q\u03b1\u03b1x\u03b1 with q\u03b1\u03b1 > 0 (and hence does not vanish). Inversely, using Cholesky factorization, for any SOS polynomial Q of degree 2d which coefficient matrix (see equation (1)) is positive definite, there exists a > 0 such that aQ \u2208 Sd. This suggests that Sd is a quite general class of nonvanishing SOS polynomials. The following result, which gives a relation between Q\u00b5,d and solutions of (4), uses a generalization of [13, Theorem 3.1.2] to several orthogonal polynomials of several variables.\nTheorem 1 : Under Assumption 1, problem (4) is a convex optimization problem with a unique optimal solution (Q\u2217\u03b1, \u03b8 \u2217 \u03b1), which satisfies Q \u2217 \u03b1 = \u221a \u03bbP\u03b1, \u03b1 \u2208 Npd, for some \u03bb > 0. In particular,\nthe distinguished SOS polynomial Q\u00b5,d = \u2211 \u03b1\u2208Npd P 2\u03b1 = 1 \u03bb \u2211 \u03b1\u2208Npd (Q\u2217\u03b1) 2, is (part of) the unique optimal solution of (4).\nTheorem 1 states that up to the scaling factor \u03bb, the distinguished SOS polynomial Q\u00b5,d is the unique optimal solution of problem (4). A detailed proof is provided in the Appendix B and we only sketch the main ideas here. First, it is remarkable that for each fixed \u03b1 \u2208 Npd (and again up to a scaling factor) the polynomial P\u03b1 is the unique optimal solution of the problem:\nminQ { \u222b Q2d\u00b5 : Q \u2208 Rd[X], Q(x) = x\u03b1 + \u2211 \u03b2<gl\u03b1 q\u03b2 x \u03b2 }\n. This fact is well-known in the univariate case [13, Theorem 3.1.2] and does not seem to have been exploited in the literature, at least for purposes similar to ours. So intuitively, P 2\u03b1 should be as close to 0 as possible on the support of \u00b5. Problem (4) has similar properties and the constraint on the vector of weights \u03b8 enforces that, at an optimal solution, the contribution \u222b (Q\u2217\u03b1)\n2 d\u00b5 to the overall sum in the criterion is the same for all \u03b1. Using Lemma 2 yields (up to a multiplicative constant) the polynomial Q\u00b5,d. Other constraints on \u03b8 would yield different weighted sum of the squares P 2\u03b1. This will be a subject of further investigations.\nTo sum up, Theorem 1 provides a rationale for our observations. Indeed when solving (4), intuitively, Q\u00b5,d should be as close to 0 as possible on average while remaining in a large class of nonvanishing SOS polynomials."}, {"heading": "4.4 Christoffel function and outlier detection", "text": "The following result from [3, Theorem 3.5.6] draws a direct connection between Q\u00b5,d and the Chritoffel function (the right hand side of (5)).\nTheorem 2 ([3]) Let Assumption 1 hold and let x\u0304 \u2208 Rp be fixed, arbitrary. Then\nQ\u00b5,d(x\u0304) \u22121 = min\nP\u2208Rd[X] {\u222b Rp P (x)2 d\u00b5(x) : P (x\u0304) = 1 } . (5)\nTheorem 2 provides a mathematical rationale for the use of Q\u00b5,d for outlier or novelty detection purposes. Indeed, from Lemma 2 and equation (3), we have Q\u00b5,d \u2265 1 on Rp. Furthermore, the solution of the minimization problem in (5) satisfies P (x\u0304)2 = 1 and \u00b5 ({ x \u2208 Rp : P (x)2 \u2264 1 }) \u2265 1 \u2212 Q\u00b5,d(x\u0304)\u22121 (by Markov\u2019s inequality). Hence, for high values of Q\u00b5,d(x\u0304), the sublevel set{ x \u2208 Rp : P (x)2 \u2264 1 } contains most of the mass of \u00b5 while P (x\u0304)2 = 1. Again the result of Theorem 2 does not seem to have been interpreted for purposes similar to ours."}, {"heading": "5 Experiments on network intrusion datasets", "text": "In addition to having its own mathematical interest, Theorem 1 can be exploited for various purposes. For instance, the sub-level sets of Q\u00b5,d, and in particular {x \u2208 Rp : Q\u00b5,d(x) \u2264 ( p+d d ) }, can be used to encode a cloud of points in a simple and compact form. However in this section we focus on another potential application in anomaly detection.\nEmpirical findings described in Section 3 suggest that the polynomial Q\u00b5,d can be used to detect outliers in a collection of real vectors by taking \u00b5 to be the corresponding empirical measure. This is backed up by the results presented in Section 4. In this section we illustrate these properties on a real world example. We choose the KDD cup 99 network intrusion dataset (available at [11]) which consists of network connection data with labels describing whether they correspond to normal traffic or network intrusions. We follow [15] and [14] and construct five datasets consisting of labeled vectors in R3, the label indicating normal traffic or network attack. The content of these datasets is summarized in the following table.\nDataset http smtp ftp-data ftp others Number of examples 567498 95156 30464 4091 5858 Proportions of attacks 0.004 0.0003 0.023 0.077 0.016\nThe details on how these datasets are constructed are available in [15, 14] and are reproduced in Appendix C. The main idea is to give to each datapoint an outlyingness score solely based on its position in R3 and then compare outliers predicted by the score with the label indicating network intrusion. The underlying assumption is that network intrusion corresponds to infrequent abnormal behaviors and could thus be considered as outliers.\nWe reproduce the exact same experiment that was described in [14, Section 5.4] using the value of the inverse moment matrix SOS polynomial from Definition 1 as an outlyingness score (with d = 3). The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods. These results are gathered\nin [14, Figure 7]. In the left panel of Figure 2 we represent the same performance measure for our approach. We first compute the value of the inverse moment SOS polynomial for each datapoint and use it as an outlyingness score. We then display the proportion of correctly identified outliers, with score above a given threshold, as a function of the proportion of examples with score above the threshold (for different values of the threshold). The main comments are as follows.\n\u2022 The inverse moment matrix SOS polynomial does detect network intrusions with varying performances on the five datasets.\n\u2022 Except for the \u201cftp-data dataset\u201d, the global shape of these curves are very similar to results reported in [14, Figure 7] indicating that the proposed approach is comparable to other dedicated methods for intrusion detection in these four datasets.\nIn a second experiment, we investigate the effect of changing the value of d in Q\u00b5,d on the performances in terms of outlier detection. We focus on the \u201cothers\u201d dataset because it is the most heterogeneous in term of data and outliers. We adopt a slightly different measure of performance and use precision recall curves (see for example [2]) to measure performances in identifying network intrusions (the higher the curve, the better). We call the area under such curves the AUPR. The right panel of Figure 2 represents these results. First, the case d = 1, which corresponds to vanilla Mahalanobis distance as outlined in Section 3.3, gives poor performances. Second, the global performances rapidly increase with d and then decrease and stabilize.\nThis suggests that d can be used as a tuning parameter which controls the \u201ccomplexity\u201d of Q\u00b5,d. Indeed, 2d is the degree of the polynomial Q\u00b5,d and it is expected that more complex models will potentially identify more diverse classes of examples of points as outliers. In our case, this means identifying regular traffic as outliers while it actually does not correspond to intrusions."}, {"heading": "6 Conclusion and future work", "text": "We presented empirical findings with a mathematical intuition regarding the sublevel sets of the inverse moment matrix SOS polynomial. This opens many potential subjects of investigations.\n\u2022 Similarities with maximum likelihood.\n\u2022 Statistics in the context of empirical processes.\n\u2022 Relation between a density and its inverse moment matrix SOS polynomial. Assymptotics when the degree increases.\n\u2022 Connections with computational geometry and non Gaussian integrals.\n\u2022 Computationally tractable extensions in higher dimensions."}, {"heading": "Acknowledgments", "text": "This work was partly supported by project ERC-ADG TAMING 666981,ERC-Advanced Grant of the European Research Council and grant number FA9550-15-1-0500 from the Air Force Office of Scientific Research, Air Force Material Command."}, {"heading": "A Additional examples", "text": ""}, {"heading": "B Proofs", "text": "We use the same notation as in the main text. We recall that Assumption 1.\nAssumption 1 \u00b5 is a Borel probability measure on Rp with all its moments finite and Md(\u00b5) is positive definite for a given d \u2208 N.\nLemma 2 and Theorem 2 are taken from the literature and we provide a proof for completeness.\nB.1 Proof of Lemma 1\nFirst we show that the mapping x 7\u2192 Q\u00b5,d(x) does not depend on the choice of a specific basis of Rd[X]. Then we will deduce the affine invariance property.\nLemma 3 Let wd(X) be an arbitrary basis of Rd[X] and let R\u00b5,d \u2208 Rd[X] be derived in the same way as Q\u00b5,d (see Definition 1), with wd in place of vd. Then Q\u00b5,d(x) = R\u00b5,d(x) for all x \u2208 Rp.\nProof : Since wd is a basis of Rd[X], there exists an invertible matrix C \u2208 Rs(d)\u00d7s(d) such that wd(X) = Cvd(X). We reproduce the computation of Definition 1 with this new basis. We write Nd(\u00b5) the moment matrix computed with the polynomial basis wd. We have\nNd(\u00b5) = \u222b Rp wd(x)wd(x) T d\u00b5(x)\n= \u222b Rp Cvd(x)vd(x) TCT d\u00b5(x)\n= C \u222b Rp vd(x)vd(x) T d\u00b5(x)CT = CMd(\u00b5)C T ,\nwhich leads to Nd(\u00b5)\u22121 = C\u2212TMd(\u00b5)\u22121C\u22121. Using Definition 1, for all x \u2208 Rp, we have\nR\u00b5,d(x) = wd(x) TNd(\u00b5) \u22121wd(x)\n= vd(x) TCTC\u2212TMd(\u00b5) \u22121C\u22121Cvd(x)\n= vd(x) TMd(\u00b5) \u22121vd(x)\n= Q\u00b5,d(x),\nwhich concludes the proof.\nLemma 1 Let \u00b5 satisfy Assumption 1 and A \u2208 Rp\u00d7p, b \u2208 Rp define an invertible affine mapping on Rp,A : x\u2192 Ax+b. Then, the push foward measure, defined by \u00b5\u0303(S) = \u00b5(A\u22121(S)) for all Borel sets S \u2282 Rp, satisfies Assumption 1 (with the same d as \u00b5) and for all x \u2208 Rp, Q\u00b5,d(x) = Q\u00b5\u0303,d(Ax+ b).\nProof : Let us first computeMd(\u00b5\u0303). For the push forward measure \u00b5\u0303, it holds that for any \u00b5 integrable function f : Rp \u2192 R, \u222b\nRp f(x)d\u00b5\u0303(x) = \u222b Rp f(Ax + b)d\u00b5(x).\nBy considering polynomial f , we have that \u00b5\u0303 has all its moments finite and satisfies Assumption 1 with the same d as \u00b5. Furthermore, we have\nMd(\u00b5\u0303) = \u222b Rp vd(x)vd(x) T d\u00b5\u0303(x) = \u222b Rp vd(Ax + b)vd(Ax + b) T d\u00b5(x). (7)\nWe can deduce the following identity for all x \u2208 Rp,\nQ\u00b5\u0303,d(Ax + b) = vd(Ax + b) TMd(\u00b5\u0303) \u22121 vd(Ax + b). (8)\nIt remains to notice that mappings defined by wd(x) = vd(Ax + b) for all x \u2208 Rp form a basis of the polynomials of degree up to d on Rp (by invertibility of the affine mapping). Combining (7) and (8), we see that x 7\u2192 vd(Ax + b) simply corresponds to the use of a different basis of Rd[X]. The result follows by applying Lemma 3 and the proof is complete.\nB.2 Proof of Lemma 2\nRecall that the orthogonal polynomials satisfy for all \u03b1 \u2208 Npd\n\u3008P\u03b1, X\u03b2\u3009 = 0 if \u03b1 <gl \u03b2, \u3008P\u03b1, P\u03b1\u3009\u00b5 = 1, \u3008P\u03b1, X\u03b2\u3009\u00b5 = 0 if \u03b2 <gl \u03b1, \u3008P\u03b1, X\u03b1\u3009\u00b5 > 0. (3)\nLemma 2 Let \u00b5 satisfy Assumption 1, then Q\u00b5,d = \u2211 \u03b1\u2208Npd\nP 2\u03b1, where the family {P\u03b1}\u03b1\u2208Npd is defined by (3) and \u222b Rp Q\u00b5,d(x)d\u00b5(x) = s(d).\nProof : Let Dd(\u00b5) be the lower triangular matrix which rows are the coefficients of the polynomials P\u03b1 defined in (3) ordered by \u2264gl. From properties in (3), Dd(\u00b5) is lower triangular with positive coefficients on its diagonal and therefore invertible. We have Dd(\u00b5)Md(\u00b5)Dd(\u00b5)T = I , the identity. It follows that Md(\u00b5) = Dd(\u00b5)\u22121Dd(\u00b5)\u2212T and Md(\u00b5)\u22121 = Dd(\u00b5)TDd(\u00b5). Plugging this in definition 1 and using equation (1) leads to the desired identity. The average value result follows because we manipulate an orthonormal basis of s(d) polymials, each of which has a square average value (with respect to \u00b5) equal to 1.\nB.3 Proof of Theorem 1\nWe recall the the optimization problem.\nmin Q\u03b1,\u03b8\u03b1,\u03b1\u2208Npd\n1\n2 \u222b Rp \u2211 \u03b1\u2208Npd Q\u03b1(x) 2d\u00b5(x) (4)\ns.t. q\u03b1\u03b1 \u2265 exp(\u03b8\u03b1), \u03b1 \u2208 Npd, q\u03b1\u03b2 = 0, \u03b1 <gl \u03b2, \u03b1, \u03b2 \u2208 Npd,\u2211 \u03b1\u2208Npd \u03b8\u03b1 = 0.\nwhere Q\u03b1(x) = \u2211 \u03b2 q\u03b1\u03b2x \u03b2 , \u03b1 \u2208 Npd. The statement of Theorem 1 goes as follows.\nTheorem 1 : Problem (4) is a convex optimization problem with a unique optimal solution (Q\u2217\u03b1, \u03b8\u2217\u03b1), which satisfies Q\u2217\u03b1 = \u221a \u03bbP\u03b1, \u03b1 \u2208 Npd, for some \u03bb > 0. In particular, the distinguished SOS polynomial\nQ\u00b5,d = \u2211 \u03b1\u2208Npd P 2\u03b1 = 1 \u03bb \u2211 \u03b1\u2208Npd (Q\u2217\u03b1) 2,\nis (part of) the unique optimal solution of (4).\nProof :\nGeneral remarks. Observe that (4) is a convex optimization problem as we have\u222b Rp \u2211 \u03b1\u2208Npd Q\u03b1(x) 2d\u00b5(x) = \u2211 \u03b1\u2208Npd\nqT\u03b1Md(\u00b5)q\u03b1, which is strictly convex in {q\u03b1}\u03b1\u2208Npd . The proof is based on KKT optimality conditions for Problem (4). We first prove that any optimal solution should be of the form Q\u2217\u03b1 = \u221a \u03bbP\u03b1, \u03b1 \u2208 Npd, for some \u03bb > 0. Then we show that there exists a solution of the KKT system which has this form and finally that this solution is unique. The conclusion of Theorem 1 will then follow from Lemma 1. We begin with some notations that we will use throughout the proof.\nNotation. Let {e\u03b1}\u03b1\u2208Npd denote the canonical basis of R s(d) indexed by \u03b1 \u2208 Npd according to \u2264gl order. The orthonormal polynomials {P\u03b1}\u03b1\u2208Npd (with respect to \u00b5) are uniquely defined. For each \u03b1 \u2208 Npd, we write p\u03b1 = (p\u03b1\u03b2)\u03b2\u2208Npd \u2208 R\ns(d) the coefficients of the polynomial P\u03b1. By construction of P\u03b1, for every \u03b1, \u03b2 \u2208 Npd, \u03b1 <gl \u03b2, p\u03b1\u03b2 = 0 and p\u03b1\u03b1 > 0.\nOptimality conditions Problem (4) is strictly feasible, we can choose any \u03b8 such that \u2211 \u03b1 \u03b8\u03b1 = 0 and for every \u03b1 \u2208 Npd, set Q\u03b1 := \u03baP\u03b1 for some sufficiently large \u03ba > 0. Therefore the KKT optimality conditions are necessary and sufficient for global optimality. We introduce Lagrange multipliers for problem (4): \u03bb\u03b1 \u2265 0 for each inequality constraint, \u03bb\u03b1\u03b2 \u2208 R for each linear equality constraint on polynomials with \u03b1 <gl \u03b2 and \u03bb \u2208 R for the last linear equality constraint on {\u03b8\u03b1}\u03b1\u2208Npd . The KKT optimality conditions for problem (4) can be written as follows\n\u03bb\u03b1 \u2265 0, eT\u03b1q\u2217\u03b1 \u2265 exp(\u03b8\u2217\u03b1), \u03b1 \u2208 N p d, (9)\neT\u03b2q \u2217 \u03b1 = 0, \u03b1, \u03b2 \u2208 N p d, \u03b1 <gl \u03b2, (10)\u2211\n\u03b1\u2208Npd\n\u03b8\u2217\u03b1 = 0, (11)\nMd(\u00b5)q \u2217 \u03b1 = \u03bb\u03b1e\u03b1 + \u2211 \u03b1<gl\u03b2 \u03bb\u03b1\u03b2 e\u03b2 , \u03b1 \u2208 Npd, (12)\n\u03bb\u03b1 exp(\u03b8 \u2217 \u03b1) = \u03bb\u03b1e T \u03b1q \u2217 \u03b1 = \u03bb, \u03b1 \u2208 N p d, (13)\nfor optimal variables \u03b8\u2217\u03b1, polynomials Q \u2217 \u03b1 with coefficients q \u2217 \u03b1 \u2208 Rs(d), for each \u03b1 \u2208 N p d. We next show that the part (Q\u2217\u03b1)\u03b1\u2208Npd of an optimal solution is necessarily a family of orthogonal polynomials.\nAny optimal solution has the form Q\u2217\u03b1 = \u221a \u03bbP\u03b1, \u03b1 \u2208 Npd, for some \u03bb > 0. Since KKT conditions are necessary and sufficient for optimality, we only focus on them. For each \u03b1 6= 0 and \u03b2 <gl \u03b1, multiplying (12) by e\u03b2 , we obtain\u2329\nX\u03b2 , Q\u2217\u03b1 \u232a \u00b5 = \u222b x\u03b2Q\u2217\u03b1(x) d\u00b5(x) = e T \u03b2 Md(\u00b5)q \u2217 \u03b1 = \u03bb\u03b1 e T \u03b2 e\u03b1 + \u2211 \u03b1<gl\u03b3 \u03bb\u03b1\u03b3 e T \u03b2 e\u03b3 = 0. (14)\nSimilarly, multiplying (12) by q\u2217\u03b1 yields for all \u03b1 \u2208 N p d,\n\u3008Q\u2217\u03b1, Q\u2217\u03b1\u3009\u00b5 = \u222b Q\u2217\u03b1(x) 2 d\u00b5(x) = (q\u2217\u03b1) T Md(\u00b5)q \u2217 \u03b1 = \u03bb\u03b1 (q \u2217 \u03b1) Te\u03b1 = \u03bb, (15)\nwhere we have used (13) for the last identity. In particular, with \u03b1 = 0, Q\u22170(x) = q \u2217 00 (\u2265 exp(\u03b8\u22170)) for all x and so\n\u03bb = \u222b Q\u22170(x) 2 d\u00b5(x) = (q\u221700) 2 \u222b d\u00b5 \u2265 exp(2\u03b8\u22170),\nwhich shows that \u03bb > 0. Next, combining (14), (15) and the condition (10), we immediately deduce\n\u2329 Q\u2217\u03b2 , Q \u2217 \u03b1 \u232a \u00b5 = \u222b Q\u2217\u03b2(x)Q \u2217 \u03b1(x) d\u00b5(x) = { \u03bb if \u03b1 = \u03b2 0 otherwise.\n(16)\nFinally, for every \u03b1 \u2208 Npd, multiplying (12) by e\u03b1 yields\n\u3008X\u03b1, Q\u2217\u03b1\u3009\u00b5 = \u222b x\u03b1Q\u2217\u03b1(x) d\u00b5(x) = e T \u03b1Md(\u00b5)q \u2217 \u03b1 = \u03bb\u03b1 > 0, \u03b1 \u2208 N p d. (17)\nThe last inequality follows from (13). Indeed, suppose \u03bb\u03b1 = 0 for some \u03b1 \u2208 Npd, this would yield \u03bb = 0. Since we have shown that \u03bb > 0, it must also hold that \u03bb\u03b1 > 0 for all \u03b1. Combining relations (10), (14), (16) and (17), we have shown that the {Q\u2217\u03b1}\u03b1\u2208Npd form a family of orthogonal polynomials with respect to \u00b5. In addition, by the uniqueness of the orthonormal basis {P\u03b1}\u03b1\u2208Npd , it follows from (16) that Q\u2217\u03b1 = \u221a \u03bbP\u03b1 for every \u03b1 \u2208 Npd.\nThere exists a solution of this form. Recall that, for each \u03b1 \u2208 Npd, p\u03b1 = (p\u03b1\u03b2)\u03b2\u2208Npd \u2208 R s(d) is the vector of coefficients of the polynomial P\u03b1 which satisfies by construction p\u03b1\u03b1 > 0 and p\u03b1\u03b2 = 0\nfor all \u03b2 \u2208 Npd, \u03b1 <gl \u03b2. We use the following assignment for the primal and dual variables.\n\u03bb =  \u220f \u03b1\u2208Npd p\u03b1\u03b1  \u22122 s(d) > 0 (18)\n\u03bb\u03b1 = \u221a \u03bbeT\u03b1Md(\u00b5)p\u03b1 =\n\u221a \u03bb p\u03b1\u03b1 > 0, \u03b1 \u2208 Npd\n\u03bb\u03b1\u03b2 = \u221a \u03bbeT\u03b2Md(\u00b5)p\u03b1, \u03b1, \u03b2 \u2208 N p d, \u03b1 <gl \u03b2\nq\u2217\u03b1 = \u221a \u03bbp\u03b1, \u03b8 \u2217 \u03b1 = log( \u221a \u03bbp\u03b1\u03b1), \u03b1 \u2208 Npd.\nUsing orthonormality of the polynomials {P\u03b1}\u03b1\u2208Npd , it can be check that the assignment (18) satisfies KKT optimality conditions (9), (10), (11), (12) and (13). We have therefore constructed an optimal solution of (4) with the desired form.\nThe optimal solution is unique. From what precedes any optimal solution of (4) is necessarily such that Q\u2217\u03b1 = \u221a \u03bbP\u03b1, for every \u03b1 \u2208 Nn, for some \u03bb > 0. In addition the optimal value of (4) is s(d)\u03bb. Suppose that there exists two different optimal solutions (Q\u03b1, \u03b8\u03b1)\u03b1\u2208Npd and (Q \u2032 \u03b1, \u03b8 \u2032 \u03b1)\u03b1\u2208Npd with associated dual variables (\u03bb, \u03bb\u03b1, \u03bb\u03b1\u03b2)\u03b1,\u03b2\u2208Npd and (\u03bb \u2032, \u03bb\u2032\u03b1, \u03bb \u2032 \u03b1\u03b2)\u03b1,\u03b2\u2208Npd . Then necessarily \u03bb = \u03bb\n\u2032, Q\u03b1 = Q \u2032 \u03b1 = \u221a \u03bbP\u03b1 and \u03bb\u03b1, \u03bb\u2032\u03b1 > 0 for all \u03b1 \u2208 N p d. But then from (13), \u221a \u03bbp\u03b1\u03b1 = exp(\u03b8\u03b1) = exp(\u03b8\u2032\u03b1) and so \u03b8 \u2032 \u03b1 = \u03b8\u03b1 for every \u03b1 \u2208 N p d. Therefore the solution is unique and this concludes the proof of Theorem 1.\nB.4 Proof of Theorem 2\nTheorem 2 Let Assumption 1 hold and let x\u0304 \u2208 Rp be fixed, arbitrary. Then\nQ\u00b5,d(x\u0304) \u22121 = min\nP\u2208Rd[X]\n{\u222b P (x)2 d\u00b5 : P (x\u0304) = 1 } . (7)\nProof :\nFix an arbitrary P \u2208 Rd[X] and x\u0304 \u2208 Rp. Assume that P (x\u0304) = 1. Letting for all \u03b1 \u2208 Npd, a\u03b1 = \u3008P, P\u03b1\u3009\u00b5, by orthonormality, we have\nP = \u2211 \u03b1\u2208Npd a\u03b1P\u03b1, (19)\n\u3008P, P \u3009\u00b5 = \u2211 \u03b1\u2208Npd a2\u03b1.\nThe assumption that P (x\u0304) = 1 can be used in conjonction with Cauchy-Schwartz inequality to obtain\n1 = P (x\u0304) (20) = \u2211 \u03b1\u2208Npd a\u03b1P\u03b1(x\u0304)\n\u2264 \u2211 \u03b1\u2208Npd a2\u03b1 \u2211 \u03b1\u2208Npd P\u03b1(x\u0304) 2  = \u3008P, P \u3009\u00b5Q\u00b5,d(x\u0304),\nwhere the last equality comes from the definition of \u3008\u00b7, \u00b7\u3009\u00b5 and Lemma 2. There is equality in equation 20 if and only if a\u03b1 = P\u03b1(x\u0304)/Q\u00b5,d(x\u0304) which always leads to P (x\u0304) = 1. This shows that the infimum is attained and concludes the proof."}, {"heading": "C Details about the preparation of the datasets", "text": "We reproduce the exact same manipulations as in the references. We downloaded the kddcup.data from the following repository\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/\nThis file contains 4898431 instances of network connections described by 42 features including the type of connection (attack or normal). We filter the records by keeping only those for which the variable logged in is positive. We kept the labels (type of connection) together with the four most important features: service, duration, src_bytes, dst_bytes. We applied to the three last variables (numerical) the function log(\u00b7+0.1)/10. We build four datasets with the four most frequent instances of service and group all the remaining records in the dataset others to get our five datasets.\nD Illustration of affine invariance\nThe following Figure illustrate the affine invariance property described in Lemma 1."}], "references": [{"title": "Anomaly detection: A survey. ACM computing surveys", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The relationship between Precision-Recall and ROC curves", "author": ["J. Davis", "M. Goadrich"], "venue": "Proceedings of the 23rd international conference on Machine learning (pp. 233-240)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Orthogonal polynomials of several variables", "author": ["C.F. Dunkl", "Y. Xu"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "A stable numerical method for inverting shape from moments", "author": ["G.H Golub", "P. Milanfar", "J. Varah"], "venue": "SIAM Journal on Scientific Computating", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "A modification of a method for the detection of outliers in multivariate samples", "author": ["A.S. Hadi"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Measures with zeros in the inverse of their moment matrix", "author": ["J.W. Helton", "M.J.B. Lasserre"], "venue": "Putinar", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Level Sets and NonGaussian Integrals of Positively Homogeneous Functions", "author": ["J.B. Lasserre"], "venue": "International Game Theory Review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "UCI Machine Learning Repository, http://archive.ics.uci.edu/ml University of California, Irvine, School of Information and Computer Sciences", "author": ["M. Lichman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Unsupervised learning using MML", "author": ["J.J. Oliver", "R.A.Baxter", "C.S. Wallace"], "venue": "Proceedings of the International Conference on Machine Learning (pp. 364-372)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Orthogonal polynomials", "author": ["G. Szeg\u00f6"], "venue": "In Colloquium publications,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1974}, {"title": "A Comparative Study of RNN for Outlier Detection in Data Mining", "author": ["G. Williams", "R. Baxter", "H. He", "S. Hawkins", "L. Gu"], "venue": "IEEE International Conference on Data Mining (p", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "Accounting for complicated shapes is also related to computational geometry and nonlinear algebra applications, for example integral computation [9] and reconstruction of sets from moments data [4, 5, 10].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "Accounting for complicated shapes is also related to computational geometry and nonlinear algebra applications, for example integral computation [9] and reconstruction of sets from moments data [4, 5, 10].", "startOffset": 194, "endOffset": 204}, {"referenceID": 0, "context": "Furthermore the computational cost of evaluating the polynomial does not depend on the number of data points which is a crucial difference with existing nonparametric methods such as nearest neighbors or kernel based methods [1].", "startOffset": 225, "endOffset": 228}, {"referenceID": 2, "context": "In a second step, we provide a mathematical interpretation that supports our empirical findings based on connections with orthogonal polynomials [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 7, "context": "Finally, in Section 5 we perform numerical experiments on KDD cup network intrusion dataset [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "We refer the reader to [1] for a discussion of available methods for this task.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "For the sake of a fair comparison we have reproduced the experiments performed in [14] for the same dataset.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].", "startOffset": 242, "endOffset": 248}, {"referenceID": 8, "context": "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].", "startOffset": 265, "endOffset": 269}, {"referenceID": 10, "context": "We report results similar to (and sometimes better than) those described in [14] which suggests that the method is comparable to other dedicated approaches for network intrusion detection, including robust estimation and Mahalanobis distance [6, 8], mixture models [12] and recurrent neural networks [14].", "startOffset": 300, "endOffset": 304}, {"referenceID": 9, "context": "Actually, connection to orthogonal polynomials will show that the inverse function x 7\u2192 Q\u03bc,d(x) is called the Christoffel function in the literature [13, 3] (see also Section 4).", "startOffset": 149, "endOffset": 156}, {"referenceID": 2, "context": "Actually, connection to orthogonal polynomials will show that the inverse function x 7\u2192 Q\u03bc,d(x) is called the Christoffel function in the literature [13, 3] (see also Section 4).", "startOffset": 149, "endOffset": 156}, {"referenceID": 0, "context": "This is an important contrast with kernel based or distance based methods (such as nearest neighbors and one class SVM) for density estimation or outlier detection since they usually require at least O(n) operations for the evaluation of the model [1].", "startOffset": 248, "endOffset": 251}, {"referenceID": 9, "context": "2 Connection with orthogonal polynomials We define a classical [13, 3] family of orthonormal polynomials, {P\u03b1}\u03b1\u2208Npd ordered according to \u2264gl which satisfies for all \u03b1 \u2208 Npd \u3008P\u03b1, X\u3009 = 0 if \u03b1 <gl \u03b2, \u3008P\u03b1, P\u03b1\u3009\u03bc = 1, \u3008P\u03b1, X\u3009\u03bc = 0 if \u03b2 <gl \u03b1, \u3008P\u03b1, X\u3009\u03bc > 0.", "startOffset": 63, "endOffset": 70}, {"referenceID": 2, "context": "2 Connection with orthogonal polynomials We define a classical [13, 3] family of orthonormal polynomials, {P\u03b1}\u03b1\u2208Npd ordered according to \u2264gl which satisfies for all \u03b1 \u2208 Npd \u3008P\u03b1, X\u3009 = 0 if \u03b1 <gl \u03b2, \u3008P\u03b1, P\u03b1\u3009\u03bc = 1, \u3008P\u03b1, X\u3009\u03bc = 0 if \u03b2 <gl \u03b1, \u3008P\u03b1, X\u3009\u03bc > 0.", "startOffset": 63, "endOffset": 70}, {"referenceID": 2, "context": "Theorem 2 ([3]) Let Assumption 1 hold and let x\u0304 \u2208 R be fixed, arbitrary.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "We choose the KDD cup 99 network intrusion dataset (available at [11]) which consists of network connection data with labels describing whether they correspond to normal traffic or network intrusions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "We follow [15] and [14] and construct five datasets consisting of labeled vectors in R, the label indicating normal traffic or network attack.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "016 The details on how these datasets are constructed are available in [15, 14] and are reproduced in Appendix C.", "startOffset": 71, "endOffset": 79}, {"referenceID": 10, "context": "The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods.", "startOffset": 177, "endOffset": 183}, {"referenceID": 8, "context": "The authors of [14] have compared different types of methods for outlier detection in the same experimental setting: methods based on robust estimation and Mahalanobis distance [6, 8], mixture model based methods [12] and recurrent neural network based methods.", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "13) Figure 2: Left: reproduction of the results described in [14] with the inverse moment SOS polynomial value as an outlyingness score (d = 3).", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "We adopt a slightly different measure of performance and use precision recall curves (see for example [2]) to measure performances in identifying network intrusions (the higher the curve, the better).", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "References [1] V.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[12] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[13] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[14] G.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.", "creator": "LaTeX with hyperref package"}}}