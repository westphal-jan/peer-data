{"id": "1606.03968", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Visual-Inertial-Semantic Scene Representation for 3-D Object Detection", "abstract": "We describe a representation of a scene that captures the geometric and semantic attributes of objects within the scene, along with their uncertainty. Objects are assumed to be persistent in the scene and their probability is calculated from intermittent visual data using a revolutionary architecture, integrated into a Bayesian filter system with inertia and a context model. Our method yields a subsequent estimate of geometry (associated point cloud and associated uncertainty), semantics (identities and co-occurrences), and a pinpoint estimate of the topology for a variable number of objects within the scene, implemented causally and in real time on hardware.", "histories": [["v1", "Mon, 13 Jun 2016 14:22:10 GMT  (32kb,D)", "http://arxiv.org/abs/1606.03968v1", "preliminary version, no figures"], ["v2", "Mon, 17 Apr 2017 20:25:26 GMT  (4651kb,D)", "http://arxiv.org/abs/1606.03968v2", "To appear in CVPR 2017"]], "COMMENTS": "preliminary version, no figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jingming dong", "xiaohan fei", "stefano soatto"], "accepted": false, "id": "1606.03968"}, "pdf": {"name": "1606.03968.pdf", "metadata": {"source": "CRF", "title": "Visual-Inertial Semantic Scene Representation UCLA TR CSD160005", "authors": ["Stefano Soatto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We design a representation of a scene, and infer it from visual and inertial data, to support interaction tasks, including localization (where am I?), navigation (where can I go?), ambient object detection (what is around me?), and prediction (where is it going?). Correspondingly, the representation maintains (posterior) estimates of geometric, topological, semantic, and dynamic attributes of the scene and objects within.\nThe task informs what are the nuisance factors in the data, which are shared among several tasks. We focus on illumination, viewpoint, and partial occlusion, in addition to calibration, alignment, and other sensor-specific characteristics. An ideal representation would be a minimal sufficient statistic (of the data, for the task) that is also invariant (to nuisance factors) [53]. Geometry. A gravity-aligned, sparse attributed Euclidean point cloud, with local illumination- and poseinvariant descriptors as attributes [15], is a minimal sufficient invariant for localization tasks, including relative localization of multiple sensor platforms/agents (Fig. 1). Unlike monocular vision-based SLAM, inertial measurements enable inference of global orientation and (Euclidean) scale. We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time. This, however, is insufficient for navigation tasks where one must discriminate free space from photometrically unexciting surfaces. Topology. Determining which points are close, not in the sense of the embedding topology but in the sense\nFigure 1: Co-localization. The geometric component of the representation provides a shared Euclidean reference frame (blue points); once photometric correspondence is established (green line segments, right), initially unrelated poses (left, yellow and orange camera trajectories) snap into the same reference frame (right), even if one of the two sensors is monocular vision-only (bottom image), since the global scale and orientation are encoded in the representation from (at least one) visual-inertial sensor platform. Green points are currently visible and being tracked; red points are currently being tested for geometric compatibility and not yet included in the representation.\nof belonging to the same surface, is necessary for navigation, but also to organize the scene into \u201cobjects\u201d\nar X\niv :1\n60 6.\n03 96\n8v 1\n[ cs\n.C V\n] 1\n3 Ju\nn 20\n16\nthat have identities and relations that can affect high-level planning. We leverage on existing work [24] to infer surface topology given images and (MAP) point-estimates of camera poses as above (Fig. 2). Semantics. We define objects as (compact) regions of 3D space that produce measurable correlates in\nFigure 2: Topology estimation for some segments of the scenes in Figs. 3 and 7. Although significant gaps still exist, especially in the dark-mesh chair and black monitor, geometric entities in the representation are connected as belonging to the same surface, which supports both navigation (free space traversal) and semantic labeling.\nimages that are shared with high probability in a population of image regions labeled as equivalent by an oracle.1 Objects exist in the scene, not on the image, that only provides evidence (likelihood) of a given object being present in the scene. This can approximated using a convolutional architecture (CNN) [23, 45], interpreted not as an object detector, but as a likelihood function for objects being present in the scene. This is in line with [53], where CNNs are seen as computing marginal/profile likelihoods, which are minimal sufficient invariant statistics of a single image.\nSince we are interested in interaction tasks, not in single-image interpretation, we wish to maintain a posterior estimate of objects in the scene given all past data, causally and in real-time. Objects are persistent; they do not flicker in and out of existence as we move smoothly through physical space. Context In addition to identities, the representation should encode relations among objects. Geometric relations are already encoded in the geometric component of the representation, since each object has pose attributes. This enables answering questions about mutual position among objects (Fig. 1). Topological relations, such as co-visibility, are also captured in the representation (Fig. 2). Semantic relations, such as co-occurrence and relative pose and identities, are modeled by a graph with unary and pairwise potentials [10] with a Dirichlet prior. Dynamics In addition to identities and relations, objects have dynamic properties, that affect how they are likely to move at future times. We put the emphasis on real-time, since a representation for interaction tasks must be inferred and updated in real-time to enable closing the loop. While there is a growing body of work on long-term motion (\u201cintent\u201d) prediction, at present none that we considered is suitable for real-time posterior update, and therefore we only address slow motion (relative to the sensor update rate) and defer long-term prediction to future work.\nContributions and related work We design an inference procedure for a persistent scene representation from a monocular video and inertial data streams. It captures geometric, topological, and semantic properties, and rudimentary dynamics. To the best of our knowledge, this is the first system of its kind that can operate in real time at scale. We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].\nWe stress the importance of inertial sensors, not so much for dead-reckoning, but to validate object hypotheses in a Bayesian setting, by making global orientation and scale observable so that, for instance, a model car in our system is not classified as a car (Fig. 3), despite image evidence of the contrary.\nFigure 3: Left-to-right: A model car (1), detected as a car by a CNN (2), dismissed by our model (3), because its geometric attributes (4) have small likelihood under the model.\n1We take a supervised approach, where identities are determined by the supervisor. Alternatively, one could define objects in terms of functional or topological characteristics that yield measurable correlates, for instance being \u201cdetachable,\u201d [3], or use other sensory modality, such as touch.\nThere is a sizable literature on semantic scene understanding from a single image ([19] and references therein). We are instead interested in agents embedded in physical space, so the restriction to a single image is limiting. There is also a vast literature on scene segmentation, including dedicated workshops and special issues ([31] and references therein), mostly using range (RGB-D) sensors, although also from video. There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion. Some focus on real-time operation [13], but most operate off-line. None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.\nThis work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58]. Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57]. The use of RGB-D cameras unfortunately restricts the domain of applicability mostly indoors and at close range. Most of this work does not leverage on the similar recent accessibility to cheap inertial sensors.\nThere is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62]. Additional related work includes [28, 12, 7, 47].\nFinally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images."}, {"heading": "2 Modeling", "text": ""}, {"heading": "2.1 Representations", "text": "A scene \u03b8 is populated by a number of objects zj \u2208 {z1, . . . , zN}, each with geometric (position, shape) and semantic (label) attributes zj = {xj , sj , lj}.2 Measurements (e.g., images) up to the current time t, yt . = {y1, . . . , yt} are captured from a sensor at pose gt. A representation of the scene in support of semantic tasks is the joint posterior p(\u03b8, zj |yt) for up to the j-th objects seen up to time t, where sensor pose gt and other nuisances are marginalized. The joint posterior can be decomposed3 as\np(\u03b8, zj |yt) = p(\u03b8|zj)\ufe38 \ufe37\ufe37 \ufe38 (6) p(zj |yt)\ufe38 \ufe37\ufe37 \ufe38 (2)\n(1)\nwith the first factor updated asynchronously each time a new object zj+1 becomes manifest starting from a prior P (\u03b8|\u2205) and the second factor (2) updated synchronously as new measurements yt+1 become available starting from t = 0 and given p(z|\u2205).\nA representation of the scene in support of (geometric) localization tasks is the posterior p(gt, x|yt) over sensor pose gt (which, of course, is not a nuisance for this task) and a sparse attributed4 point cloud x = [x1, . . . , xNx ], given all measurements (visual and inertial) up to the current time. Conditioning the semantics on the geometry5 we can write (2) as\np(zj |yt) = \u222b p(zj |gt, x, yt)dP (gt, x|yt) (2)\n2A scene instance can thus be thought of as a graph with nodes zj = {z1, . . . , zj} and edges capturing (geometric, positional and semantic) relations.\n3If the goal is to infer objects z from images yt, marginalizing the above would ignore the context \u03b8. This would make it impossible to exploit context knowledge while learning objects directly from data. However, the context model can be learned from object knowledge (first factor), and used to validate object hypotheses (second factor). Assuming a mixture model for p(\u03b8|z), we can then exploit the context model while inferring objects via max\u03b8 p(z, \u03b8|yt). Alternatively, an empirical Bayes approach would first infer \u03b8\u0302 = argmax p(\u03b8|yt) and then perform inference on z using p(z|\u03b8\u0302, yt). We adopt the former method for simplicity.\n4Attributes include sparse geometry (position in the inertial frame) as well as local photometry (feature descriptor). 5Note that these approximations allow us to separate the computation of the geometric representation (the \u201cdorsal stream\u201d to\ndetermine \u201cwhere\u201d) from the semantic representation (the \u201cventral stream\u201d to determine \u201cwhat\u201d).\nwhere the integrand can be updated as more data yt+1 becomes available using p(zj |gt+1, x, yt+1) \u221d p(yt+1|zj , gt+1, x) \u222b p(gt+1|gt, ut)dP (zj |gt, x, yt). (3)\nIn an active perception setting, we distinguish nuisances that can be (partially) controlled, like viewpoint/pose gt, from those that cannot,6 like contrast/illumination \u03bat. Here ut can be a control action, e.g., translational and rotational velocity affecting the pose gt+1 = gt+1ut. The transition probability is trivial in this case since the next pose gt+1(ut) is a deterministic function of the current pose gt and the controlled velocity ut."}, {"heading": "2.2 Approximations", "text": "The measure in (2) can be approximated to second-order by a simultaneous localization and mapping (SLAM) system: p(gt, x|yt) ' N (gt, x; g\u0302t|t, x\u0302t|t, Pt|t). Eq. (2) is a diffusion around the mean/mode g\u0302t|t, x\u0302t|t; if the covariance Pt|t is small, it can be ignored, so given\ng\u0302t|t, x\u0302t|t = argmax gt,x p(gt, x|yt)\ufe38 \ufe37\ufe37 \ufe38 SLAM\n(4)\nthen p\u0302g,x(z|yt) . = p(z|gt = g\u0302t|t, x = x\u0302t|t, yt) ' p(z|yt). Otherwise the marginalization in (2) can be performed using samples from the SLAM system. Either way, omitting the subscripts, we have\np\u0302(z|yt+1) \u221d p(yt+1|z, g\u0302t|tut, x\u0302t|t)\ufe38 \ufe37\ufe37 \ufe38 CNN p\u0302(z|yt)\ufe38 \ufe37\ufe37 \ufe38 EKF/PMF\n(5)\nwhere the likelihood term is approximated by a convolutional neural network (CNN) as shown in Sect. 3.5 and the posterior is updated by a filter (EKF/PMF) as shown in Sect. 3.4. Finally, the first factor in (2)\np(\u03b8|zj+1) \u221d p(zj+1|\u03b8)P (\u03b8|zj)\ufe38 \ufe37\ufe37 \ufe38 RNN\n(6)\ncan be approximated using a recurrent neural network (RNN) as we show in Sect. 3.5.\nRemark 1 (CNN as a likelihood function). Approximating the likelihood p(y|l, s, g, x) in (5) appears daunting because of the purported need to generate the data y \u2013 including every pixel in the image, given object class l, shape s and pose g \u2013 and the need to normalize with respect to all possible images of the object. Fortunately, the latter is not needed since the product on the right-hand side of (5) needs to be normalized anyway, which can be done easily in a particle-based representation of the posterior by dividing by the sum of the weights of the particles. Generating actual images is similarly not needed. What is needed is a mechanism that, for a given image yt+1, allows quantifying the likelihood that an object of any class l with any shape s be present in any portion of the image b where s projects to from the vantage point gt. In Sect. 3.3 we will show how a discriminatively-trained CNN can be leveraged to this end [53].\nRemark 2 (Interpretability of the representation). While the representation \u03b8 is abstract and not interpretable, it is sufficient for predicting/hallucinating objects, which are interpretable. They have geometric and semantic attributes that can be verified independently. In particular, given any point estimate \u03b8\u0302, for instance \u03b8\u0302(zj) = E(p(\u03b8|zj)) or argmax\u03b8 p(\u03b8|zj), we can predict unseen objects by sampling p(zj+1|\u03b8\u0302). We can also score configurations of objects zj for compatibility with a scene estimate by evaluating p(zj |\u03b8\u0302).\n6Which phenomenon can be considered a nuisance, and which is controlled, depends on the task and the sensor platform available. In photometric stereo, one can control the light but not the vantage point. In a mobile robot, one can control the vantage point but not the light. In a mobile active-illumination setting, one can control both."}, {"heading": "2.3 Information", "text": "Given measurements up to the current time, yt, the information a (future) measurement yt+1 contains on the scene/objects is, formally,\nI(z, yt+1|yt) = H(yt+1|yt)\u2212H(yt+1|z, yt). (7)\nThe first term is the uncertainty of future data given the past. It depends indirectly on the true scene, through future data it generates. The second term depends explicitly on the true collection of objects in the scene z, which in general is non-identifiable (Rem. 3). By definition, nuisances are not informative of the scene (although see Rem. 4 for the case of controllable nuisances), so the information content of yt+1 is identical to that of \u03c6\u03ba(yt+1) if \u03c6\u03ba is a maximal invariant to \u03ba. Actionable Information (AI) [52] captures the first term after uncertainty due to non-controllable nuisance variability \u03ba is removed:\nHt(yt+1) . = H(\u03c6\u03ba(yt+1)|yt) (8)\nWe will show that (7) is equivalent to (8) for exploration, i.e., to determine the (controlled nuisances that yield the) most informative future data."}, {"heading": "2.4 Exploration", "text": "For the purpose of this section, we assume a data formation model h that, given the true (objects in the) scene z and nuisance g, could generate the data y up to an uninformative/white residual/noise n: y = h(z, g) + n. In Sect. 3.1 we will relax this assumption to compute functions of the data that are sufficient to evaluate the likelihood. Before measuring yt+1, if we had samples from p(z|yt) and p(gt, x|yt), we could determine the action ut that would place the sensor at gt+1(ut) = gtut, so as to hallucinate measurements y\u0302t+1 = h(z, gt+1) that most decrease uncertainty (i.e., maximize information) on z. From (7) we have\nut = argmax u\nH(yt+1(u)|yt)\u2212H(yt+1(u)|z, yt) (9)\nwhere yt+1(u) = h(z, gt+1(u))\ufe38 \ufe37\ufe37 \ufe38 y\u0302t+1(u) +nt+1.\nNote that the difference between yt+1 and y\u0302t+1 is uninformative and therefore the maximizer of the information in the two is the same. The second term in (9) is constant for yt+1, and zero for y\u0302t+1, so it can be ignored in the maximization. It is relevant in the minimization involved in inference and will be revisited in Sect. 2.5. Also, there is no gain in maximizing the uncertainty in yt+1 due to nuisances \u03ba since, by definition, they are uninformative of the scene. Therefore, we only care to maximize the uncertainty of a maximal \u03ba-invariant of y\u0302t+1, \u03c6\u03ba(y\u0302t+1) = \u03c6\u03ba \u25e6 h(z, gt+1(u)): From (7) and (8) we have\nut = argmax u I(z, yt+1(u)|yt) = argmax u Ht(yt+1(u)) (10)\nwhich shows that the most informative action is the one that maximizes Actionable Information, hence the name. Controllable nuisances u are selected to be as informative as possible, whereas the uncontrollable ones \u03ba are removed with no information loss if \u03c6\u03ba is a maximal invariant. This process is closely related to that proposed by [22]. We note that nuisances that have the structure of a group acting on the data can be removed without a loss and without the need to gather more data. Nuisances that form a group, however, cannot be eliminated in pre-processing.\nNote that we need to simulate the effect of the controlled nuisances g(u) but not of the uncontrolled ones \u03ba, since it does not help hallucinating phenomena that are uninformative. In addition to illumination, cyclo-rotation (rotation about the optical axis) can also be moded-out by rectifying the image with an homography, for instance to gravity-align it using accelerometer measurements (Fig. 4)."}, {"heading": "A Turing postulate", "text": "We call Turing postulate the assumption that, for a compact scene, there exists a (possibly large but finite) set of data yt whose sufficient statistics are as good as the scene for the purpose of generating future measurements yt+1 (or maximal invariants thereof \u03c6k(yt+1)):\np(yt+1|z) = p(yt+1|h(z(yt), g)) \u2200 g (11)\nwhere z(yt) \u223c p(z|g\u0302t|t, x\u0302t|t, yt) are from the posterior, which is a minimal sufficient (Bayesian) statistic of yt for z [53]. The Turing test is passed when the innovation residual yt+1 \u2212 h(z(yt), g) is uninformative, according to some empirical whiteness test. The assumptions made thus far are going to be relaxed in Sect. 3.1.\nRemark 3 (The \u201ctrue scene\u201d cannot be known). The scene is, in general, not identifiable as there are infinitely-many scenes that generate the same set of images, no matter how many. In other words, for any given set of data yt there is an equivalence class (or a distribution) of scenes that can generate them (up to white noise). In this equivalence class, in addition to the \u201ctrue\u201d scene there are many others, including those that are deterministic or stochastic functions of past data yt, the trivial example being the data itself. Since the task of \u201cknowing the true scene\u201d is undecidable, we can be satisfied with knowing any scene in the equivalence class, which is sufficient to hallucinate (future) images y\u0302t+1 that are indistinguishable from the measured ones yt+1, up to noise. More precisely, we do not care to hallucinate the effect of uninformative nuisances, so we care that the invariant residual \u03c6\u03ba(y\u0302t+1)\u2212 \u03c6\u03ba(yt+1) be white/uninformative. Thus a representation is a statistic of past data yt that is (minimal) sufficient to hallucinate (invariants of) future data yt+1 up to an uninformative residual [52, 53]."}, {"heading": "2.5 Exploitation", "text": "In some cases, one may wish to infer a point estimate z\u0302 from the representation of the scene, which is the posterior in (3). Focusing on the second term of (9), we have that\nH(yt+1|z, yt) = H(yt+1|z) \u2264 H(yt+1|z\u0302(yt)) (12)\nfor any estimator function z\u0302(\u00b7). We can therefore choose the one that minimizes the right-hand side; this requires postulating a likelihood function p(yt+1|z), and computing the empirical entropy\nz\u0302 = argmin z(\u00b7)\nEp(yt+1,yt)(\u2212 log p(yt+1|z(y t))) (13)\nor, more in general, minimizing with respect to predictors p\u0302(yt+1|yt), which in our case is chosen of the form p(yt+1|z\u0302(yt)). It should be noted that for a linear system with additive, zero-mean Gaussian noise, the above coincides with the maximum a-posteriori (MAP) estimate\nz\u0302 = argmax z p(z|yt+1). (14)\nMore in general, the relation between the two are described in [33, 17] for linear systems. To be practical, the inference scheme above must be coupled with restrictions on the classes of estimators and penalties on complexity. We adopt a MAP approach and simply provide as a point estimate, when required, the mode of the posterior.\nFor objects where the posterior converges to a single mode, we can replace the coarse shape with a high-quality model from the prior [46], if specific object recognized, or an iconic prior from the class. Topology estimated by an implicit surface following [24] for some representative objects is shown in Fig. 2. When object move slowly, they can still be represented in the static scene model. If multiple syncronized sensors are available, the static assumption is always satisfied, and therefore moving objects can be represented as if static (Fig. 8).\nFigure 4: Gravity is encoded in the representation of the scene in which objects are embedded, and estimated in the moving frame along with the pose of the camera (blue line segment, center); alignment is then obtained by transforming current images by an homography, keeping the error rate approximately constant (right, blue line) despite cyclo-rotation, unlike direct usage of an R-CNN that degrades with the rotation angle (red line, for the object class \u201csofa\u201d).\nFigure 5: Objects exist in the scene (top, bottom), not on the images (middle): Image-based object detection based on an R-CNN (middle), projected onto the scene (top, for indoor and outdoor sequences; each of the 20 categories is color-coded) produces a confused picture since the CNN knows no scene topology. When object objects are native to the scene, however (bottom), so local compactness and persistence can be enforced ab-ovo, the organization of the scene into objects can benefit from the aggregation of the entire time history of measurements through a likelihood function, approximated by a CNN (bottom, showing representative classes detected). Note that the objects are present in the scene (bottom right, shown as bounding boxes embedded in the colored point cloud) even when they do not trigger responses by the CNN on the image (upper-left inset box)."}, {"heading": "3 Instantiation", "text": "Let It be raw data captured at time t and It = {I0, . . . , It} the \u201chistory\u201d of the data up to t. For example an image It : D = [640\u00d7480]\u2192 [0, 255] \u2200 t = 1, . . . , T with domain D on the pixel lattice. Let x be a description of the (sparse) geometry of the scene, for instance a point cloud x = [x1, . . . , xNx ] \u2208 R3\u00d7Nx . Let \u03c6 : I \u2192 RN\u03c6 be any statistic (deterministic function) of the raw data. If the image is restricted to a compact subset of b \u2282 D, \u03c6(I|b) is sometimes called a \u201clocal (feature) descriptor.\u201d A convolutional neural network is often used as a local descriptor \u03c6(I|b) [4, 18], with the entire image b = D as a special case. The pose of the sensor is described by a reference frame gt \u2208 SE(3); \u03bat are environmental effects and sensor calibration affecting the range space of the data, for instance monotonic continuous (contrast) transformations \u03bat : [0, 255]\u2192 [0, 255] that affect the value I point-wise: I(xij) 7\u2192 \u03bat(I(xij)) \u2200 xij \u2208 D. Let l \u2208 {l1, . . . , lK} be a discrete label or class, that denotes the identity of an object in the scene, s \u2282 R3 its shape, in the simplest case a parallelepiped or ellipsoid, and \u03c1 : s\u2192 R+ its appearance (a.k.a., radiance, reflectance, albedo, texture). An object is then described by {l, s, \u03c1}. A point x \u2208 s projects onto a pixel xij \u2208 D on the camera gt via an ideal perspective projection \u03c0 : R3 \u2192 R2;xij = \u03c0(gtx) if visible; The pre-image \u03c0\u22121s (xij) = {x \u2208 s | \u03c0(x) = xij} transforms via \u03c0gs(xij) = g\u22121\u03c0s(xij) = {x \u2208 s | \u03c0(gx) = xij} and may include multiple objects, but only the closest is visible. An object with shape s, albedo \u03c1 viewed under illumination \u03bat from camera gt generates an image It at the pixel xij \u2208 D via\nimage formation model formally as\nIt(xij) = h\u03c1(s|x , l, gt) + nt(xij). (16)\nwhere xij = \u03c0(gtx) for all visible x \u2208 s and l is a latent (class) variable that is assumed to have measurable correlates in I. For the entire visible portion of an object that occupies a region of space B \u2283 S that projects onto a region bt \u2282 D of the image plane of the camera gt, we can write the image-formation process for all pixels xij \u2208 bt as\nIt = h\u03c1(s, l, gt) + nt. (17)\nwhere the region B and its projection onto the image bt are implicitly defined by s and gt. The data formation model above defines a probabilistic model of the form\np(It|s, l, \u03c1, gt) = N (It; h\u03c1(s, l, gt), Rn) (18)\nwhere the rendering function h\u03c1 is in general expensive to evaluate. Fortunately, this is not necessary, as we discuss next."}, {"heading": "3.1 Abstracting the pixels", "text": "The model above allows us to evaluate the likelihood of an object of class l being present in a region s seen from vantage point g in image I, by comparing the latter with an image hallucinated from the object I\u0302 = h\u03c1(s, l, g) via the (log likelihood) `2 distance between pixels d2(I, I\u0302) = \u2016I \u2212 I\u0302\u20162. But to evaluate the likelihood, we do not need to simulate the image-formation process all the way to the pixels. At the very least, we want the comparison to be unaffected by nuisance variability. This could be done by defining a (geodesic) distance between functions \u03c6\u03ba(I) and \u03c6\u03ba(I\u0302), for instance designed to be maximal \u03ba-invariant descriptors. But more in general, we can design a descriptor for I that approximates the likelihood function (11) directly, by choosing among a parametric class of functions \u03c6(I) that, given an image region b, produce a positive score \u03c6(I|b)[k] for each class lk. This can be used to test whether an object of class l and shape s is present in the scene, by projecting it as seen from vantage point g, b = \u03c0(gs), and testing the resulting score, as described in Sect. 3.3. Then, to compute AI in (10), given samples from p(l, s|g\u0302t|t, x\u0302t|t, yt), we simply simulate actions u, and the resulting (hallucinated) image regions b = \u03c0(g\u0302t|tus) from which we can compute the scores \u03c6(I|b)[k], and thence AI empirically."}, {"heading": "3.2 Controlling nuisance variability", "text": "The effects of some nuisance variables can be eliminated by replacing the data with a (maximal) invariant. For instance, contrast transformations \u03ba can be factored out at no loss by replacing the intensity I at xij with a maximal contrast invariant \u03c6\u03ba(I), for instance the gradient orientation: I(xij) \u2192 y(xij) . = \u03c6\u03ba(I(xij)) . =\n\u2207I(xij) \u2016\u2207I(xij)\u2016 , modeling the linearized approximation of nt as an angular Gaussian. 7 Alternatively, in some cases nuisances can be controlled. For instance, a robot can control the vantage point gt, to a certain extent, or its velocity ut \u2208 se(3) \u2261 R6, defined by\ng\u0307t = gtut (19)\nwhere ut represents (translational and rotational) velocity. Assuming g0 to be known (the global frame is arbitrary), the current pose gt is determined by the history of velocities ut, up to drift and initial condition, so gt = g(u\nt). These choices are just illustrative: In general what is controllable depends on the sensor platform, for instance one may be able to control contrast transformations \u03bat by changing the sensor gain or actively changing the illuminant.\n7The same can, in theory, be done for viewpoint gt and for any other nuisance that acts as a group on the data (viewpoint changes induce diffeomorphic deformations of the domain of the image away from occlusions [54]). When nuisances are not a group, one can still construct invariants, but they will not in general be maximal. They can still be sufficient statistics for certain classes of tasks, which leads us to the notion of sufficient invariants [53], which can be computed by marginalization and/or profiling (max-out).\nRemark 4 (Informative nuisances?). Nuisances can be controlled to yield maximum information. This may seem like a paradox since nuisances are, by definition, not informative of the scene. While the value of nuisances is independent of object attributes, it affects images of the object, which can be informative of such attributes. A chair is a chair no matter where we are when we look at it. However, an image may be more or less revealing of it being a chair depending on vantage point. So, except for biased datasets,8 knowledge of pose gt does not reduce our uncertainty in the class l absent the image. However, nuisances gt can be controlled so as to produce images It that are as informative as possible of the identity l of objects in the scene."}, {"heading": "3.3 Measurement process", "text": "At each instant t, the raw data It is processed by \u201cprobing functions\u201d \u03c6, which can be designed or trained to be invariant to nuisance variability that cannot be actively controlled. The SLAM system processes all past image measurements It and inertial measurements ut, which collectively we refer to as yt = {\u03c6\u03ba(It), ut}, where \u03c6k(It) = {\u03c6k(It|xij )} is a collection of sparse contrast-invariant feature descriptors for Ni visible regions of the scene, and produces a joint posterior distribution of poses gt and a sparse geometric representation of the scene x = [x1, . . . , xNi(t)], assumed uni-modal and approximated by a Gaussian:\np SLAM (gt, x|yt) ' N (gt, x; g\u0302t|t, x\u0302t|t, P{g,x} t|t) (20)\nwhere x \u2208 \u222ajSj , i.e., the scene is assumed to be composed by the union of objects, including the default class \u201cbackground\u201d l0. This localization pipeline is described in [56], and is agnostic of the organization of the scene into objects and their identity. It also restricts x to a subset of the scene that is rigid, co-visible for a sufficiently long interval of time, and located on surfaces that, locally, exhibit Lambertian reflection.\nTo compute the marginal likelihood for each class lk \u2208 {l0, . . . , lK}, we leverage on a CNN trained discriminatively to classify a given image region bj into one of K + 1 classes, including the background class. The architecture has a soft-max layer preceded by K + 1 nodes, one per class, and is trained using the cross-entropy loss, providing a normalized score \u03c6\nCNN (l|It|bj )[k] for each class and image bounding box bj .\nWe discard the soft-max layer, forgo class-normalization and rebalance the classes based on their frequency in the training set. The responses at the K + 1 nodes in the penultimate layer of the resulting network provide a mechanism for, given an image It, quantifying the likelihood of each object class lk being present at each bounding box bj , which we interpret the (marginal) likelihoods for (at least an instance of) each class being present at the given bounding box:\n\u03c6 CNN (l|It|bj )[k] ' p(It|lk, bj). (21)\nNote that the resulting CNN is not an object detector, but a likelihood function [53]: It is not a discriminant for a random variable l that can take one of K+1 possible values, but instead a function of a K+1-dimensional (binary) vector l, which is not normalized with respect to l. This process induces a likelihood on object classes being present in the visible portion of the scene regions sj and corresponding vantage points gt, via bj = \u03c0(gtsj):\n\u03c6CNN(l|It|\u03c0(gtsj) )[k] ' p(It|sj , lk, gt) (22)\nwhere, given an image It, for each possible region in the scene sj and vantage point gt, we can test the presence of at least one instance of each class lk within. This allows the possibility that more than one object be present in the same region of space, for instance a person and a chair, or multiple instances of chairs. Here the visibility function is implicit in the map \u03c0. If an object is not visible, its likelihood given the image It is constant/uniform. Note that this depends on the global layout of the scene, since the map \u03c0 must take into account occlusions, so objects cannot be considered independently.\n8Photo-collections exhibit significant dataset bias, as images uploaded by humans tend to be iconic and captured from object-dependent biased vantage points."}, {"heading": "3.4 Dependencies and co-visibility", "text": "Computing the likelihood of an object being present in the scene requires ascertaining whether it is visible in the image, which in turn depends on all other objects, so the scene has to be modeled holistically rather than as an independent collection of objects. In addition, the presence of certain objects, and their configuration, affects the probability that other objects that are not visible be present.9 Furthermore, relations among objects may be the object of interest themselves, as in events or action recognition.\nTo capture these dependencies, we note that the geometric representation p(gt, x|yt) can be used to provide a joint distribution on the position of all objects and cameras p(gt, x|yt), which yields co-visibility information, specifically the probability of each point in x being visible by any camera in gt. It is, however, of no use in determining visibility of objects, since it contains no topological information: We do not know if the space between two points is empty, or occupied by an object void of salient photometric features.10 To enable visibility computation, we can use the point cloud together with the images to compute the dense shape of objects in a maximum-likelihood sense: s\u0302j = argmax p(sj |gt, x, yt) using generic regularizers [32]. This can be done (Fig. 2) but may be overkill. An alternative is to approximate the shape of objects with a parametric family, for instance bounding boxes or ellipsoids, and compute visibility accordingly, also leveraging the co-visibility graph computed as a corollary from the SLAM system [56], and priors on the size and aspect ratios of objects.\nTo this end, we approximate\np\u0302g,x(z j |yt) .= p(zj |yt, gt, x) ' \u220f j p(zj |yt, gt, x, z\u2212j) (23)\nwhere z\u2212j indicates all objects but zj . Each factor is then expanded as\np(sj , lj , xj |yt, gt, x, z\u2212j) = p(sj , xj |lj , yt, gt, x, s\u2212j , x\u2212j)\ufe38 \ufe37\ufe37 \ufe38 EKF P (lj |yt, gt, x, l\u2212j)\ufe38 \ufe37\ufe37 \ufe38 PMF\n(24)\nwhere the first term is represented by swarms of Extended Kalman Filters (EKF) and the second by a point-mass filter (PMF). These provide samples from p\u0302(z|yt) in the RHS of (5), that are scored with the CNN to update the posterior. Note that visibility is accounted for in the likelihood model of the EKF that, based on the sparse representation of the scene x and the shape s\u2212j and pose x\u2212j of other objects within."}, {"heading": "3.5 Scene context RNN", "text": "The last element needed to compute (2) is a representation of the (abstract) scene \u03b8, to be inferred using objects zj up to j = j(t) seen at the current time t. This is needed to produce sample objects, including those not yet seen z\u0302j+1 \u223c p(zj+1|\u03b8\u0302), for instance through a point estimate \u03b8\u0302 = \u03b8\u0302(zj) computed from all objects seen thus far. Rather than postulating a model for how the true scene generates objects, and then using it to infer the scene from observed ones, we postulate a model for the object predictor directly. Specifically, we assume that a point estimate of the scene predicts objects deterministically through a parametrized function \u03c8\nz\u0302j+1 = \u03c8(\u03b8\u0302j+1) (25)\nand that the scene estimate \u03b8\u0302 is a parametrized function \u03c6 of all past objects zj but updated incrementally via\n\u03b8\u0302j+1 = \u03c6(\u03b8\u0302j , zj) (26)\nas soon as each object zj becomes manifest, starting from an initial condition \u03b80. We represent the scene \u03b8 abstractly as a collection of L sets of hidden states (\u201clayers\u201d) {\u03b8l}Ll=1 with constant dimension, and assume\n9For instance, seeing a keyboard and a monitor on a desk affects the probability that there is a mouse in the scene, even if we cannot see it at present. Their relative pose also informs the vantage point that would most reduce the uncertainty on the presence of the mouse.\n10In an active perception setting, given structured illuminator, one could take a control action to reduce uncertainty and resolve the conundrum.\nthat z\u0302 can be predicted as a linear function of the last layer through an unknown matrix WL followed by a non-linearity, for instance SoftMax:\n\u03c8(\u03b8) = SoftMax(WL\u03b8 L). (27)\nSimilarly, we assume that the function \u03c6 operates at each layer through linear combinations of the previous layer at the same index j and the same layer at the previous index, followed by a nonlinearity \u03c3, for instance arctan:\n\u03c6(\u03b8, zj) l+1 j+1 . = \u03c3(Wl\u03b8 l j+1,Wl+1\u03b8 l+1 j ,W0zj) = \u03b8 l+1 j+1 (28)\nwhere the matrices {Wl}Ll=0 do not depend on j, and consist of unknown parameters that must be inferred from zj . This can be done in a number of ways, for instance minimizing the prediction error or cross-entropy loss, corresponding to an Elman RNN:\nW\u0302 (zj) = argmin W j\u2211 i=1 `(zi \u2212 \u03c8(WL\u03b8\u0302Li )) subject to (28) (29)\nwhere ` is a loss function of choice, that has a semantic component l\u0302 \u2212 l, which is hand-designed (the cost of mispredicting a couch as a chair is not as high as confusing a chair with a person) or obtained empirically from classification error of base detector on benchmark datasets, and a geometric component, including relative pose x\u0302j \u2212 xj and self-intersection sj \u2229 si, that can be also learned empirically form filter performance, along with the filter\u2019s probability of missed detection and false alarm. In the absence of ground truth objects zi one can model directly the predictive density using a variational autoencoder [36]."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Implementation", "text": "SLAM is borrowed from [56, 16]. The CNN is implemented by modifying stock code [45] by removing the classification layer and normalization. The EKF and PMF are standard textbook material and the RNN is trained assuming manually labeled scene bounding boxes."}, {"heading": "4.2 Empirical evaluation", "text": "Fig. 5 shows representative visualization of MAP estimate of object pose, encoded by a gravity-aligned ellipsoid (here visualized as a bounding box), and the section of the point-cloud within. In addition, topology is estimated (dense surface, Fig. 2), and the identity as inferred. Although objects are assumed static (or moving slowly) in the scene, camera motion induces a non-trivial dynamics in the images of objects, that would have to be tracked if object models were represented on the image. However, having represented them in the scene allows us to predict the (image) motion of objects, as shown in Fig. 7, even when the object is completely occluded. We note that some datasets include moving objects (people) that, however, move slowly\nFigure 7: Left-to-right: A chair is detected by the CNN (top) and initialized in the filter (bottom); as the camera moves behind a monitor, \u201cTV screen\u201d is detected; whereas the chair disappears from the instantaneous frame (top); yet, it persists in the representation (right-hand-side, green box), so the system can predict its projection, and re-establish visual correspondence after it re-appears (right).\nrelative to the data sampling rate, and therefore can be captured even assuming a static scene (Fig. 5). For the purpose of validation, we also tested components of our system on the KITTI dataset [20]; representative results are reported in Fig. 8\nFigure 8: Sample results on KITTI, with inset frames with the most likely class from a CNN visualized, and corresponding bounding boxes in the scene. Since synchronized stereo frames are available, moving objects can be considered static, at the price of having only two vantage points, with significant occlusion artifacts. While these could be temporally integrated within our framework, the data is only provided at 10Hz, making low-level correspondence un-necessarily challenging."}, {"heading": "5 Discussion", "text": "Objects exist in the scene, not on the image. Therefore a CNN that returns bounding boxes on the image cannot find objects; it can only find clumps of pixels.\nObjects are persistent. They do not flicker in-and-out of existence. When they become occluded, we are aware of their presence behind the occluder. CNNs have no (ontogenic, or short-term) memory of the present scene; the only (phylogenic) memory is the dataset on which they are trained, which pertains to different scenes, encoded in the weights. They learn nothing from the present scene.\nOur confidence on object presence and attributes grows over time as we accumulate evidence. A CNN does not become more confident as we accumulate evidence; it does not improve even as we track objects that we have previously identified with high confidence.\nThe likelihood function computed by a CNN is (an approximation of) a minimal sufficient invariant statistic of a single datum yt+1, for the inference of semantic properties of the scene [53]. The posterior maintained by (2) is a minimal sufficient invariant Bayesian statistic of all past data, for the task of inferring semantic properties of the scene.\nWe have described what we believe to be the first real-time system to infer a representation of a scene that encompasses geometric and photometric attributes (Gravity-referenced Euclidean point cloud with local contrast invariant descriptors), topological attributes (dense reconstruction of portions of the scene relevant to interaction), and semantic attributes from visual-inertial sensing. We focus on semantics, where persistent identities are maintained by a Bayesian filter, with likelihood function computed by a CNN, geometric and visibility relations are maintained both by the geometric and topological component of the representation, and co-occurrence relations are maintained by a graphical model.\nWe have made stringent and admittedly restrictive assumptions in order to keep our model viable for real-time inference. One could certainly relax some of these assumptions and obtain more general models, but forgo the ability to close the loop, which would render the inference of a representation for interaction purposes moot.\nWe stress that our model represents uncertainty at all levels, from the posterior on object classes (context) all the way to the pixel level (local descriptors as likelihood given gradient orientation). We also interpret the CNN not as an object detector operating independently in each image, but as a mechanism to compute the likelihood of persistent objects in the scene, given imaging data, and integrate it in a Bayesian filtering framework.\nThe model can in principle be extended to class-dependent object motion models, but at the cost of significant computational cost."}, {"heading": "Acknowledgments", "text": "Contributors to this work include Pratik Chaudhari, Alessandro Achille and Alessandro Chiuso for discussions leading to the development of the statistical framework, and Xiaohan Fei, Jingming Dong, Nikolaos Karianakis Konstantine Tsotsos for its implementation and testing. Research supported by ONR, ARO and AFOSR."}], "references": [{"title": "Visual common-sense for scene understanding using perception, semantic parsing and reasoning", "author": ["S. Aditya", "Y. Yang", "C. Baral", "C. Fermuller", "Y. Aloimonos"], "venue": "2015 AAAI Spring Symposium Series,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "On invariance and selectivity in representation learning", "author": ["F. Anselmi", "L. Rosasco", "T. Poggio"], "venue": "arXiv preprint arXiv:1503.05938,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Detachable object detection: Segmentation and depth ordering from shortbaseline video", "author": ["A. Ayvaci", "S. Soatto"], "venue": "IEEE Trans. on Patt. Anal. and Mach. Intell., 34(10):1942\u20131951,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Bold-binary online learned descriptor for efficient image matching", "author": ["V. Balntas", "L. Tang", "K. Mikolajczyk"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2367\u20132375,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Second-order constrained parametric proposals and sequential searchbased structured prediction for semantic segmentation in rgb-d images", "author": ["D. Banica", "C. Sminchisescu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3517\u20133526,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Scene segmentation using temporal clustering for accessing and re-using broadcast video", "author": ["L. Baraldi", "C. Grana", "R. Cucchiara"], "venue": "Multimedia and Expo (ICME), 2015 IEEE International Conference on, pages 1\u20136. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation and recognition using structure from motion point clouds", "author": ["G. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla"], "venue": "Computer Vision\u2013ECCV 2008, pages 44\u201357. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Classification with scattering operators", "author": ["J. Bruna", "S. Mallat"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn.,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic parsing for priming object detection in rgb-d scenes", "author": ["C. Cadena", "J. Ko\u0161ecka"], "venue": "3rd Workshop on Semantic Perception, Mapping and Exploration,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting hierarchical context on a large database of object categories", "author": ["M. Choi", "J. Lim", "A. Torralba", "A. Willsky"], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 129\u2013136. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "arXiv preprint arXiv:1604.01685,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "3d urban scene modeling integrating recognition and reconstruction", "author": ["N. Cornelis", "B. Leibe", "K. Cornelis", "L. Van Gool"], "venue": "International Journal of Computer Vision, 78(2-3):121\u2013141,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional nets and watershed cuts for real-time semantic labeling of rgbd videos", "author": ["C. Couprie", "C. Farabet", "L. Najman", "Y. Lecun"], "venue": "The Journal of Machine Learning Research, 15(1):3489\u20133511,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic segmentation of rgbd images with mutex constraints", "author": ["Z. Deng", "S. Todorovic", "L. Latecki"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1733\u20131741,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain size pooling in local descriptors: Dsp-sift", "author": ["J. Dong", "S. Soatto"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal state estimation for stochastic systems: An information theoretic approach", "author": ["X. Feng", "K. Loparo", "Y. Fang"], "venue": "Automatic Control, IEEE Transactions on, 42(6):771\u2013785,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Descriptor matching with convolutional neural networks: a comparison to sift", "author": ["P. Fischer", "A. Dosovitskiy", "T. Brox"], "venue": "arXiv:1405.5769,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Single image 3d without a single 3d image", "author": ["D. F Fouhey", "W. Hussain", "A. Gupta", "M. Hebert"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1053\u20131061,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Invariance and selectivity in the ventral visual pathway", "author": ["S. Geman"], "venue": "Journal of Physiology-Paris, 100(4):212\u2013224,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on PAMI, 6:721\u2013741, November", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580\u2013587,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Online 3d reconstruction using convex optimization", "author": ["G. Graber", "T. Pock", "H. Bischof"], "venue": "Computer Vision Workshops, IEEE International Conference on, pages 708\u2013711. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Indoor scene understanding with rgb-d images: Bottomup segmentation, object detection and semantic segmentation", "author": ["S. Gupta", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "International Journal of Computer Vision, 112(2):133\u2013149,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual organization and recognition of indoor scenes from rgb-d images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 564\u2013571. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint 3d scene reconstruction and class segmentation", "author": ["C. Haene", "C. Zach", "A. Cohen", "R. Angst", "M. Pollefeys"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint 3D scene reconstruction and class segmentation", "author": ["C. Hane", "C. Zach", "A. Cohen", "R. Angst", "M. Pollefeys"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 97\u2013104,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Dense 3d semantic mapping of indoor scenes from rgb-d images", "author": ["A. Hermans", "G. Floros", "B. Leibe"], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 2631\u20132638. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards consistent vision-aided inertial navigation", "author": ["J. Hesch", "D. Kottas", "S. Bowman", "S. Roumeliotis"], "venue": "Algorithmic Foundations of Robotics X, pages 559\u2013574. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Guest editorial: Scene understanding", "author": ["D. Hoiem", "J. Hays", "J. Xiao", "A. Khosla"], "venue": "International Journal of Computer Vision, 112(2):131\u2013132,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view stereo beyond lambert", "author": ["H. Jin", "S. Soatto", "A.J. Yezzi"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., pages I\u2013171\u2013178, June", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Linear prediction, filtering, and smoothing: An information-theoretic approach", "author": ["P. Kalata", "R. Priemer"], "venue": "Information Sciences, 17(1):1\u201314,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1979}, {"title": "Object discovery in 3d scenes via shape analysis", "author": ["A. Karpathy", "S. Miller", "L. Fei-Fei"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 2088\u20132095. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "3d scene understanding by voxel-crf", "author": ["B. Kim", "P. Kohli", "S. Savarese"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1425\u20131432,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "NIPS, volume 1, page 4,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint semantic segmentation and 3d reconstruction from monocular video", "author": ["A. Kundu", "Y. Li", "F. Dellaert", "F. Li", "J. Rehg"], "venue": "Computer Vision\u2013ECCV 2014, pages 703\u2013718. Springer,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection-based object labeling in 3d scenes", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 1330\u20131337. IEEE,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Online temporal calibration for camera\u2013imu systems: Theory and algorithms", "author": ["M. Li", "A. Mourikis"], "venue": "The International Journal of Robotics Research, 33(7):947\u2013964,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Holistic scene understanding for 3d object detection with rgbd cameras", "author": ["D. Lin", "S. Fidler", "R. Urtasun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1417\u20131424,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring context with deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "A. Hengel", "I. Reid"], "venue": "arXiv preprint arXiv:1603.03183,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R.X. Mottaghi", "Chen", "X. Liu", "N. Cho", "S. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Monocular slam supported object recognition", "author": ["S. Pillai", "J. Leonard"], "venue": "Proceedings of Robotics: Science and Systems (RSS), Rome, Italy, July", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Slam++: Simultaneous localisation and mapping at the level of objects", "author": ["R. Salas-Moreno", "R. Newcombe", "H. Strasdat", "P. Kelly", "A. Davison"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1352\u20131359. IEEE,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Discrete optimization of ray potentials for semantic 3d reconstruction", "author": ["N. Savinov", "C. Hane", "M. Pollefeys"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Semantic modelling of urban scenes", "author": ["S. Sengupta", "E. Greveson", "A. Shahrokni", "P. Torr"], "venue": "International Conference on Robotics and Automation,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Recursive context propagation network for semantic scene labeling", "author": ["A. Sharma", "O. Tuzel", "M. Liu"], "venue": "Advances in Neural Information Processing Systems, pages 2447\u20132455,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2012, pages 746\u2013760. Springer,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric scene parsing with adaptive feature relevance and semantic context", "author": ["G. Singh", "J. Kosecka"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3151\u20133157,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Actionable information in vision", "author": ["S. Soatto"], "venue": "Proc. of the Intl. Conf. on Comp. Vision, October", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Visual representations: Defining properties and deep approximations", "author": ["S. Soatto", "A. Chiuso"], "venue": "Proc. of the Intl. Conf. on Learning Representations (ICLR); ArXiv: 1411.7676, May", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "On the set of images modulo viewpoint and contrast changes", "author": ["G. Sundaramoorthi", "P. Petersen", "V.S. Varadarajan", "S. Soatto"], "venue": "Proc. IEEE Conf. on Comp. Vision and Pattern Recogn., June", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Shape-based object detection via boundary structure segmentation", "author": ["A. Toshev", "B. Taskar", "K. Daniilidis"], "venue": "International journal of computer vision, 99(2):123\u2013146,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust filtering for visual inertial sensor fusion", "author": ["K. Tsotsos", "A. Chiuso", "S. Soatto"], "venue": "Proc. of the Intl. Conf. on Robotics and Automation (ICRA),", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction", "author": ["V. et al. Vineet"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "Understanding and generating scene descriptions", "author": ["D. Waltz"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1981}, {"title": "Hierarchical semantic labeling for task-relevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "Robotics: Science and systems (RSS),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Sun3d: A database of big spaces reconstructed using sfm and object labels", "author": ["J. Xiao", "A. Owens", "A. Torralba"], "venue": "Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1625\u20131632. IEEE,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation", "author": ["J. Yao", "S. Fidler", "R. Urtasun"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 702\u2013709. IEEE,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Sensor fusion for semantic segmentation of urban scenes", "author": ["R. Zhang", "S. Candra", "K. Vetter", "A. Zakhor"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on, pages 1850\u20131857. IEEE,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 51, "context": "An ideal representation would be a minimal sufficient statistic (of the data, for the task) that is also invariant (to nuisance factors) [53].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "A gravity-aligned, sparse attributed Euclidean point cloud, with local illumination- and poseinvariant descriptors as attributes [15], is a minimal sufficient invariant for localization tasks, including relative localization of multiple sensor platforms/agents (Fig.", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time.", "startOffset": 26, "endOffset": 38}, {"referenceID": 38, "context": "We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time.", "startOffset": 26, "endOffset": 38}, {"referenceID": 54, "context": "We leverage existing work [30, 40, 56] to approximate the posterior distribution of geometric attributes given visual and inertial measurements, causally and in real-time.", "startOffset": 26, "endOffset": 38}, {"referenceID": 22, "context": "We leverage on existing work [24] to infer surface topology given images and (MAP) point-estimates of camera poses as above (Fig.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "This can approximated using a convolutional architecture (CNN) [23, 45], interpreted not as an object detector, but as a likelihood function for objects being present in the scene.", "startOffset": 63, "endOffset": 71}, {"referenceID": 43, "context": "This can approximated using a convolutional architecture (CNN) [23, 45], interpreted not as an object detector, but as a likelihood function for objects being present in the scene.", "startOffset": 63, "endOffset": 71}, {"referenceID": 51, "context": "This is in line with [53], where CNNs are seen as computing marginal/profile likelihoods, which are minimal sufficient invariant statistics of a single image.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "Semantic relations, such as co-occurrence and relative pose and identities, are modeled by a graph with unary and pairwise potentials [10] with a Dirichlet prior.", "startOffset": 134, "endOffset": 138}, {"referenceID": 28, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 54, "endOffset": 66}, {"referenceID": 38, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 54, "endOffset": 66}, {"referenceID": 54, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 54, "endOffset": 66}, {"referenceID": 22, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 111, "endOffset": 119}, {"referenceID": 43, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 111, "endOffset": 119}, {"referenceID": 9, "context": "We leverage on existing visual-inertial sensor fusion [30, 40, 56], topology estimation [24], object detection [23, 45], and context modeling [10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": "Alternatively, one could define objects in terms of functional or topological characteristics that yield measurable correlates, for instance being \u201cdetachable,\u201d [3], or use other sensory modality, such as touch.", "startOffset": 161, "endOffset": 164}, {"referenceID": 17, "context": "There is a sizable literature on semantic scene understanding from a single image ([19] and references therein).", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "There is also a vast literature on scene segmentation, including dedicated workshops and special issues ([31] and references therein), mostly using range (RGB-D) sensors, although also from video.", "startOffset": 105, "endOffset": 109}, {"referenceID": 33, "context": "There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion.", "startOffset": 37, "endOffset": 49}, {"referenceID": 47, "context": "There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion.", "startOffset": 37, "endOffset": 49}, {"referenceID": 41, "context": "There is also work on 3D recognition [35, 49, 43], but again with no inertial frame and no motion.", "startOffset": 37, "endOffset": 49}, {"referenceID": 12, "context": "Some focus on real-time operation [13], but most operate off-line.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.", "startOffset": 50, "endOffset": 62}, {"referenceID": 10, "context": "None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.", "startOffset": 50, "endOffset": 62}, {"referenceID": 58, "context": "None of the datasets commonly used in these works [11, 11, 60] provide an inertial reference, except for KITTI on which we comment later.", "startOffset": 50, "endOffset": 62}, {"referenceID": 37, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 154, "endOffset": 162}, {"referenceID": 42, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 154, "endOffset": 162}, {"referenceID": 35, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 182, "endOffset": 186}, {"referenceID": 56, "context": "This work, by its nature, relates to a vast body of work on \u201cscene understanding,\u201d which has been the focus of attention of the Computer Vision, Robotics [39, 44] and AI communities [37] for a considerable amount of time [58].", "startOffset": 221, "endOffset": 225}, {"referenceID": 39, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 53, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 57, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 8, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 49, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 13, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 24, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 32, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 44, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 27, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 4, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 23, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 25, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 46, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 55, "context": "Most recently, since the advent of cheap consumer range sensors, these communities have joined forces and there is a wealth of contemporary work [41, 55, 59, 9, 51, 14, 26, 34, 46, 29, 5, 25, 27, 48, 57].", "startOffset": 145, "endOffset": 203}, {"referenceID": 36, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 0, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 40, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 48, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 5, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 59, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 95, "endOffset": 117}, {"referenceID": 60, "context": "There is also work that focuses on scene understanding from visual sensors, specifically video [38, 1, 42, 50, 6, 61], although none integrates with inertial sensors, although there is recently resurgent interest in sensor fusion [62].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 11, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 6, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 45, "context": "Additional related work includes [28, 12, 7, 47].", "startOffset": 33, "endOffset": 48}, {"referenceID": 19, "context": "Finally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images.", "startOffset": 96, "endOffset": 106}, {"referenceID": 1, "context": "Finally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images.", "startOffset": 96, "endOffset": 106}, {"referenceID": 7, "context": "Finally, our work relates to parallel efforts to develop a theory of representations, including [21, 2, 8], although focused on representing the scene and its three-dimensional geometry rather than representing images.", "startOffset": 96, "endOffset": 106}, {"referenceID": 51, "context": "3 we will show how a discriminatively-trained CNN can be leveraged to this end [53].", "startOffset": 79, "endOffset": 83}, {"referenceID": 50, "context": "Actionable Information (AI) [52] captures the first term after uncertainty due to non-controllable nuisance variability \u03ba is removed: Ht(yt+1) .", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "This process is closely related to that proposed by [22].", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "where z(y) \u223c p(z|\u011dt|t, x\u0302t|t, y) are from the posterior, which is a minimal sufficient (Bayesian) statistic of y for z [53].", "startOffset": 119, "endOffset": 123}, {"referenceID": 50, "context": "Thus a representation is a statistic of past data y that is (minimal) sufficient to hallucinate (invariants of) future data yt+1 up to an uninformative residual [52, 53].", "startOffset": 161, "endOffset": 169}, {"referenceID": 51, "context": "Thus a representation is a statistic of past data y that is (minimal) sufficient to hallucinate (invariants of) future data yt+1 up to an uninformative residual [52, 53].", "startOffset": 161, "endOffset": 169}, {"referenceID": 31, "context": "More in general, the relation between the two are described in [33, 17] for linear systems.", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "More in general, the relation between the two are described in [33, 17] for linear systems.", "startOffset": 63, "endOffset": 71}, {"referenceID": 44, "context": "For objects where the posterior converges to a single mode, we can replace the coarse shape with a high-quality model from the prior [46], if specific object recognized, or an iconic prior from the class.", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "Topology estimated by an implicit surface following [24] for some representative objects is shown in Fig.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "\u201d A convolutional neural network is often used as a local descriptor \u03c6(I|b) [4, 18], with the entire image b = D as a special case.", "startOffset": 76, "endOffset": 83}, {"referenceID": 16, "context": "\u201d A convolutional neural network is often used as a local descriptor \u03c6(I|b) [4, 18], with the entire image b = D as a special case.", "startOffset": 76, "endOffset": 83}, {"referenceID": 52, "context": "7The same can, in theory, be done for viewpoint gt and for any other nuisance that acts as a group on the data (viewpoint changes induce diffeomorphic deformations of the domain of the image away from occlusions [54]).", "startOffset": 212, "endOffset": 216}, {"referenceID": 51, "context": "They can still be sufficient statistics for certain classes of tasks, which leads us to the notion of sufficient invariants [53], which can be computed by marginalization and/or profiling (max-out).", "startOffset": 124, "endOffset": 128}, {"referenceID": 54, "context": "This localization pipeline is described in [56], and is agnostic of the organization of the scene into objects and their identity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 51, "context": "Note that the resulting CNN is not an object detector, but a likelihood function [53]: It is not a discriminant for a random variable l that can take one of K+1 possible values, but instead a function of a K+1-dimensional (binary) vector l, which is not normalized with respect to l.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "10 To enable visibility computation, we can use the point cloud together with the images to compute the dense shape of objects in a maximum-likelihood sense: \u015dj = argmax p(sj |g, x, y) using generic regularizers [32].", "startOffset": 212, "endOffset": 216}, {"referenceID": 54, "context": "An alternative is to approximate the shape of objects with a parametric family, for instance bounding boxes or ellipsoids, and compute visibility accordingly, also leveraging the co-visibility graph computed as a corollary from the SLAM system [56], and priors on the size and aspect ratios of objects.", "startOffset": 244, "endOffset": 248}, {"referenceID": 34, "context": "In the absence of ground truth objects zi one can model directly the predictive density using a variational autoencoder [36].", "startOffset": 120, "endOffset": 124}, {"referenceID": 54, "context": "1 Implementation SLAM is borrowed from [56, 16].", "startOffset": 39, "endOffset": 47}, {"referenceID": 43, "context": "The CNN is implemented by modifying stock code [45] by removing the classification layer and normalization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "For the purpose of validation, we also tested components of our system on the KITTI dataset [20]; representative results are reported in Fig.", "startOffset": 92, "endOffset": 96}, {"referenceID": 51, "context": "The likelihood function computed by a CNN is (an approximation of) a minimal sufficient invariant statistic of a single datum yt+1, for the inference of semantic properties of the scene [53].", "startOffset": 186, "endOffset": 190}], "year": 2017, "abstractText": "We describe a representation of a scene that captures geometric and semantic attributes of objects within, along with their uncertainty. Objects are assumed persistent in the scene, and their likelihood computed from intermittent visual data using a convolutional architecture, integrated within a Bayesian filtering framework with inertials and a context model. Our method yields a posterior estimate of geometry (attributed point cloud and associated uncertainty), semantics (identities and co-occurrence), and a point-estimate of topology for a variable number of objects within the scene, implemented causally and in real-time on commodity hardware.", "creator": "LaTeX with hyperref package"}}}