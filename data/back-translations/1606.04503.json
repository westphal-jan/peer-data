{"id": "1606.04503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization", "abstract": "This paper describes the Georgia Tech team's approach to complementary evaluation of the discourse relationship sense classification by CoNLL-2016. We use long-term short-term memory (LSTM) to arrive at distributed representations of each argument and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.", "histories": [["v1", "Tue, 14 Jun 2016 19:00:59 GMT  (61kb,D)", "http://arxiv.org/abs/1606.04503v1", "describes our system at the CoNLL 2016 shared task"]], "COMMENTS": "describes our system at the CoNLL 2016 shared task", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akanksha", "jacob eisenstein"], "accepted": false, "id": "1606.04503"}, "pdf": {"name": "1606.04503.pdf", "metadata": {"source": "CRF", "title": "Shallow Discourse Parsing Using Distributed Argument Representations and Bayesian Optimization", "authors": ["Jacob Eisenstein"], "emails": ["akanksha271@gmail.com", "jacobe@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Our approach to discourse relation classification is to combine strong surface features with a distributed representation of each discourse unit. This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015). We combine these two disparate representations in a neural network architecture. Our approach is shaped by two main design decisions: the use of long short-term memory recurrent networks (Hochreiter and Schmidhuber, 1997) to induce representations of each discourse unit, and the use of Bayesian optimization (Snoek et al., 2012) for tuning the neural network architecture."}, {"heading": "2 System Overview", "text": "The overall architecture is shown in Figure 1. The same architecture is used for both explicit and non-explicit relations, but with different parameters. The output of the classifier is a softmax layer, which takes as input a series of dense layers. These dense layers allow nonlinear interactions between surface features and elements of the distributed representations. Dropout is employed\nto reduce overfitting (Srivastava et al., 2014). The overall architecture is trained to minimize crossentropy. The implementation is in Keras (Chollet, 2015), and training takes several hours on a standard CPU. We now describe of the subcomponents of the classifier in detail."}, {"heading": "2.1 Distributed representations for discourse units", "text": "First, we induce representations for each unit in each discourse relation. This component of the model is shown in the dotted part of Figure 1 for the first discourse argument. Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al., 2014; Ji and Eisenstein, 2015). We take a recurrent neural network approach, characterizing\nar X\niv :1\n60 6.\n04 50\n3v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n16\neach discourse unit by a recurrently-updated state vector (Li et al., 2015), with the input consisting of pre-trained word embeddings GoogleNews-vectors-negative300.bin from the word2vec page.1\nSpecifically, our recurrent architecture is a long short-term memory (LSTM), which uses a combination of gates to better handle long-term dependencies, as compared with the more straightforward recurrent neural network (Hochreiter and Schmidhuber, 1997). Following Graves and Schmidhuber (2005), we employ a bidirectional LSTM, in which each training sequence is presented forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. We combine the output of these bidirectional LSTMs in a multilayer perceptron with the extracted surface features."}, {"heading": "2.2 Surface features", "text": "In addition to the distributed representations of the discourse units, we use some of the most successful surface features from prior work. These features are implemented using the Natural Language Toolkit (Bird et al., 2009) and scikit-learn (Pedregosa et al., 2011). In general, these features were inspired by the system from Wang and Lan (2015), which obtained best performance on the PDTB test set in the 2015 shared task (Xue et al., 2015)."}, {"heading": "2.2.1 Features for explicit relations", "text": "Connective Text The connective itself is a strong\nfeature for sense classification of explicit discourse relations (Pitler et al., 2008). This feature alone yields F1 of 0.8862 for our classifier.\nSentiment Value The Vader Sentiment analysis package (Hutto and Gilbert, 2014) was used to calculate sentiment score for both arguments. The feature then reports whether the two arguments have the same sentiment.\nTrigrams We used trigram features for the final three words of arg1, and for the first three words of arg2."}, {"heading": "2.2.2 Features for non-explicit relations", "text": "We used the same trigrams features from the explicit relation classifier, as well as the following\n1https://code.google.com/archive/p/word2vec/\nadditional features on pairs of linguistic elements in arg1 and arg2.\nWord Pairs We formed word pairs from the cross product of all words appearing in arg1 and arg2, following much of the prior work in discourse parsing (Marcu and Echihabi, 2003; Pitler et al., 2009). We then replaced the words in each pair with a cluster identity (Rutherford and Xue, 2014). Specifically, we used the GoogleNews-vectors-negative skipgram word embeddings to form 1000 clusters.\nPart-of-Speech Pairs Similarly, we formed partof-speech pairs from the tags appearing in the two arguments (Rutherford and Xue, 2014).\nProduction Rules Pairs Using the syntactic analysis of each argument, we form pairs of production rules appearing in the two arguments (Lin et al., 2009).\nAdverb Pairs Adverbs are particularly relevant for non-explicit discourse relations, so we compute features from pairs of adverbs appearing the two arguments."}, {"heading": "2.3 Hyperparameter tuning", "text": "The best set of hyperparameters for the classifiers were found using spearmint (Snoek et al.,\n2012), using the GPEIOptChooser Markov Chain Monte Carlo search algorithm. This algorithm samples from the space of hyperparameters, while trying to learn a function from hyperparameters to overall performance. We use the crossentropy (and not the F1) as the overall performance objective; due to the time cost for training the model, we could run only twenty samples, which took several days. The progress of this search is shown in Figure 2, which indicates that the best hyperparameter configuration was identified on the sixth sample. More samples may have yielded better performance, but this was not possible under the time constraints of the shared task. The best set of hyperparameters for non-explicit discourse relation classification are listed in Table 1."}, {"heading": "3 Evaluation", "text": "Evaluation was performed using the evaluation script provided by the conll16 task organizers. In Table 3, the performance of our system is compared to the best-performing systems from this year\u2019s shared task. Our system was particularly competitive on the blind test set. (The best performance on non-explicit relations on the blind test set was from ttr, but this system did not attempt to classify explicit relations, and so did not obtain an overall score for all relations.) This suggests that our approach suffered less from overfitting to the dev data. On the other hand, our system\u2019s performance on explicit relations was further behind the best systems, suggesting the need for additional features to handle this case.\nResults are broken down by individual relation types in Table 4, again using the shared task evaluation script. In addition, the contribution of each feature is indicated in Table 2, in which features are incrementally added to a baseline model containing only the distributed representations of each argument.\nAcknowledgments We thank the organizers of the shared task for formalizing this evaluation, and we thank Yangfeng Ji for helpful discussions in the initial phase of the project."}], "references": [{"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, California.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Chlo\u00e9 Braud", "Pascal Denis."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 2201\u20132211, Lisbon, September.", "citeRegEx": "Braud and Denis.,? 2015", "shortCiteRegEx": "Braud and Denis.", "year": 2015}, {"title": "Keras", "author": ["Franois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Vader: A parsimonious rule-based model for sentiment analysis", "author": ["Clayton J Hutto", "Eric Gilbert"], "venue": null, "citeRegEx": "Hutto and Gilbert.,? \\Q2014\\E", "shortCiteRegEx": "Hutto and Gilbert.", "year": 2014}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "One vector is not enough: Entity-augmented distributional semantics for discourse relations", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Transactions of the Association for Computational Linguistics (TACL), June.", "citeRegEx": "Ji and Eisenstein.,? 2015", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2015}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recognizing implicit discourse relations in the penn discourse treebank", "author": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 343\u2013351, Singapore.", "citeRegEx": "Lin et al\\.,? 2009", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["Daniel Marcu", "Abdessamad Echihabi."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 368\u2013375.", "citeRegEx": "Marcu and Echihabi.,? 2003", "shortCiteRegEx": "Marcu and Echihabi.", "year": 2003}, {"title": "Easily identifiable discourse relations", "author": ["Emily Pitler", "Mridhula Raghupathy", "Hena Mehta", "Ani Nenkova", "Alan Lee", "Aravind Joshi."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING), pages 87\u201390, Manchester,", "citeRegEx": "Pitler et al\\.,? 2008", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["Emily Pitler", "Annie Louis", "Ani Nenkova."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Suntec, Singapore.", "citeRegEx": "Pitler et al\\.,? 2009", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns", "author": ["Attapol T Rutherford", "Nianwen Xue."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Rutherford and Xue.,? 2014", "shortCiteRegEx": "Rutherford and Xue.", "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan Prescott Adams."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A refined endto-end discourse parser", "author": ["Jianxiang Wang", "Man Lan."], "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task, pages 17\u201324, Beijing, China, July. Association for Computational", "citeRegEx": "Wang and Lan.,? 2015", "shortCiteRegEx": "Wang and Lan.", "year": 2015}, {"title": "The CoNLL-2015 shared task on shallow discourse parsing", "author": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Rashmi Prasad", "Christopher Bryant", "Attapol T Rutherford."], "venue": "Proceedings of the Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Xue et al\\.,? 2015", "shortCiteRegEx": "Xue et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).", "startOffset": 112, "endOffset": 185}, {"referenceID": 7, "context": "This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).", "startOffset": 112, "endOffset": 185}, {"referenceID": 1, "context": "This follows prior work demonstrating that distributed representations can improve generalization for this task (Ji and Eisenstein, 2014; Ji and Eisenstein, 2015; Braud and Denis, 2015).", "startOffset": 112, "endOffset": 185}, {"referenceID": 4, "context": "Our approach is shaped by two main design decisions: the use of long short-term memory recurrent networks (Hochreiter and Schmidhuber, 1997) to induce representations of each discourse unit, and the use of Bayesian optimization (Snoek et al.", "startOffset": 106, "endOffset": 140}, {"referenceID": 15, "context": "Our approach is shaped by two main design decisions: the use of long short-term memory recurrent networks (Hochreiter and Schmidhuber, 1997) to induce representations of each discourse unit, and the use of Bayesian optimization (Snoek et al., 2012) for tuning the neural network architecture.", "startOffset": 228, "endOffset": 248}, {"referenceID": 16, "context": "to reduce overfitting (Srivastava et al., 2014).", "startOffset": 22, "endOffset": 47}, {"referenceID": 2, "context": "The implementation is in Keras (Chollet, 2015), and training takes several hours on a standard CPU.", "startOffset": 31, "endOffset": 46}, {"referenceID": 6, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al.", "startOffset": 117, "endOffset": 165}, {"referenceID": 1, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al.", "startOffset": 117, "endOffset": 165}, {"referenceID": 8, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al., 2014; Ji and Eisenstein, 2015).", "startOffset": 221, "endOffset": 263}, {"referenceID": 7, "context": "Prior work has explored a variety of ways for inducing representations of discourse units, including average pooling (Ji and Eisenstein, 2014; Braud and Denis, 2015) and recursive neural networks on syntactic parse trees (Li et al., 2014; Ji and Eisenstein, 2015).", "startOffset": 221, "endOffset": 263}, {"referenceID": 9, "context": "each discourse unit by a recurrently-updated state vector (Li et al., 2015), with the input consisting of pre-trained word embeddings GoogleNews-vectors-negative300.", "startOffset": 58, "endOffset": 75}, {"referenceID": 4, "context": "1 Specifically, our recurrent architecture is a long short-term memory (LSTM), which uses a combination of gates to better handle long-term dependencies, as compared with the more straightforward recurrent neural network (Hochreiter and Schmidhuber, 1997).", "startOffset": 221, "endOffset": 255}, {"referenceID": 3, "context": "Following Graves and Schmidhuber (2005), we employ a bidirectional LSTM, in which each training sequence is presented forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer.", "startOffset": 10, "endOffset": 40}, {"referenceID": 0, "context": "These features are implemented using the Natural Language Toolkit (Bird et al., 2009) and scikit-learn (Pedregosa et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 18, "context": "In general, these features were inspired by the system from Wang and Lan (2015), which obtained best performance on the PDTB test set in the 2015 shared task (Xue et al., 2015).", "startOffset": 158, "endOffset": 176}, {"referenceID": 0, "context": "These features are implemented using the Natural Language Toolkit (Bird et al., 2009) and scikit-learn (Pedregosa et al., 2011). In general, these features were inspired by the system from Wang and Lan (2015), which obtained best performance on the PDTB test set in the 2015 shared task (Xue et al.", "startOffset": 67, "endOffset": 209}, {"referenceID": 12, "context": "1 Features for explicit relations Connective Text The connective itself is a strong feature for sense classification of explicit discourse relations (Pitler et al., 2008).", "startOffset": 149, "endOffset": 170}, {"referenceID": 5, "context": "Sentiment Value The Vader Sentiment analysis package (Hutto and Gilbert, 2014) was used to calculate sentiment score for both arguments.", "startOffset": 53, "endOffset": 78}, {"referenceID": 11, "context": "Word Pairs We formed word pairs from the cross product of all words appearing in arg1 and arg2, following much of the prior work in discourse parsing (Marcu and Echihabi, 2003; Pitler et al., 2009).", "startOffset": 150, "endOffset": 197}, {"referenceID": 13, "context": "Word Pairs We formed word pairs from the cross product of all words appearing in arg1 and arg2, following much of the prior work in discourse parsing (Marcu and Echihabi, 2003; Pitler et al., 2009).", "startOffset": 150, "endOffset": 197}, {"referenceID": 14, "context": "We then replaced the words in each pair with a cluster identity (Rutherford and Xue, 2014).", "startOffset": 64, "endOffset": 90}, {"referenceID": 14, "context": "Part-of-Speech Pairs Similarly, we formed partof-speech pairs from the tags appearing in the two arguments (Rutherford and Xue, 2014).", "startOffset": 107, "endOffset": 133}, {"referenceID": 10, "context": "Production Rules Pairs Using the syntactic analysis of each argument, we form pairs of production rules appearing in the two arguments (Lin et al., 2009).", "startOffset": 135, "endOffset": 153}], "year": 2016, "abstractText": "This paper describes the Georgia Tech team\u2019s approach to the CoNLL-2016 supplementary evaluation on discourse relation sense classification. We use long short-term memories (LSTM) to induce distributed representations of each argument, and then combine these representations with surface features in a neural network. The architecture of the neural network is determined by Bayesian hyperparameter search.", "creator": "TeX"}}}