{"id": "1606.05029", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for Answering \"Simple\" Questions)", "abstract": "Answering factoid questions of the first order assumes that the question can be answered by a single fact in a Knowledge Base (KBS). Although this does not appear to be a challenging task, many current attempts using either complex linguistic thinking or deep neural networks reach 35% -65% on benchmark sets. Our approach formulates the task as two machine learning problems: identifying the units contained in the question and classifying the question as one of the relationship types in KBS. Based on this assumption of structure, our simple but effective approach trains two relapsing neural networks to significantly outperform the state of the art -- the relative improvement reaches 16% for WebQuestions and exceeds 38% for SimpleQuestions.", "histories": [["v1", "Thu, 16 Jun 2016 02:20:04 GMT  (478kb,D)", "http://arxiv.org/abs/1606.05029v1", "10 pages, submitted to EMNLP 2016"], ["v2", "Fri, 28 Jul 2017 15:28:01 GMT  (42kb)", "http://arxiv.org/abs/1606.05029v2", "7 pages, to appear in EMNLP 2017"]], "COMMENTS": "10 pages, submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ferhan ture", "oliver jojic"], "accepted": false, "id": "1606.05029"}, "pdf": {"name": "1606.05029.pdf", "metadata": {"source": "CRF", "title": "Simple and Effective Question Answering with Recurrent Neural Networks", "authors": ["Ferhan Ture", "Oliver Jojic"], "emails": ["jojic}@cable.comcast.com"], "sections": [{"heading": null, "text": "First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 35%\u201365% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. Based on this assumption of the structure, our simple yet effective approach trains two recurrent neural networks to outperform state of the art by significant margins \u2014 relative improvement reaches 16% for WebQuestions, and surpasses 38% for SimpleQuestions."}, {"heading": "1 Introduction", "text": "Two major approaches have been dominating the Question Answering (QA) literature in the last decade. Some work have heavily invested on converting the question into a linguistically motivated representation (e.g., using syntactic and/or semantic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014). For factoid QA, where the answer to the question is an indisputable fact, and all known facts are stored in a knowledge base (KB) in some structured form \u2014 in this case, the question can be represented in a form that is consistent with the KB.\nThe recent success of deep learning has tempted many researchers to explore more languageagnostic approaches, relying on automatically learning representations in a data-driven manner. With an increasing amount of access to computational resources and data, neural networks have outperformed traditional approaches on a variety of natural language processing (NLP) tasks (Irsoy and Cardie, 2014; Kim, 2014; Chen and Manning, 2014), including QA (Hsu et al., 2016; Yang et al., 2014; Dong et al., 2015; Bordes et al., 2015).\nWe argue that both linguistically-motivated and learning-based approaches have complementary benefits, and therefore propose a novel method for factoid QA that can be considered as a hybrid: given a knowledge base (KB), a recurrent neural network (RNN) is trained to automatically convert a given question into a structured query (Section 3a), which is then queried through the KB in order to retrieve the most relevant answer(s) (Section 3b).\nOur assumption is that a question can be represented as a pair: (a) the entity the question is about, and (b) the aspect of the entity we are interested in. If a question fits into this assumption, we refer to it as a first-order question.1 While this sounds similar to the \u201cSingle Supporting Fact\u201d task in (Weston et al., 2015), for which they report 100% accuracy, the problem becomes much more complex as the knowledge base grows to a realistic size, since the level of ambiguity increases drastically with a larger vocabulary and list of entities. This is the reason why we focus on well-studied, large-scale QA tasks like Free917 (Cai and Yates, 2013), WebQuestions (Be-\n1Bordes et al. refer to them as simple questions (2015).\nar X\niv :1\n60 6.\n05 02\n9v 1\n[ cs\n.C L\n] 1\n6 Ju\nn 20\nrant et al., 2013) (WQ), and SimpleQuestions (Bordes et al., 2015) (SQ), that rely on Freebase as a realistic knowledge base. Given that the state-of-the-art accuracy is around 45% for WQ and 64% for SQ, this is far from a solved problem, and there is still plenty of room for improvement.\nWe build our novel approach based on the hypothesis that majority of questions in these tasks are firstorder factoid questions. Results on two most recent benchmark tasks, SimpleQuestions and WebQuestions, strongly confirm our hypothesis. The accuracy of our approach indicates major improvements over the state of the art \u2014 relative improvement reaches 16% for WQ, and surpasses 38% for SQ."}, {"heading": "2 Related work", "text": "We focus on QA over structured knowledge bases. In this scenario, if the question and knowledge can be formulated in a compatible way, reasoning can be performed to determine which fact in the knowledge base answers a given question. Many methods have been proposed for parsing questions, such as a pattern-based question analyzer (Buscaldi et al., 2010), combination of syntactic parsing and semantic role labeling (Bilotti et al., 2007; Bilotti et al., 2010), as well as lambda calculus (Berant et al., 2013) and combinatory categorical grammars (CCG) (Reddy et al., 2014). The CCG-based semantic parser, trained on ClueWeb sentences annotated with corresponding Freebase facts, achieves 41% accuracy on the WebQuestions dataset.\nThe major downside of these approaches is the reliance on linguistic resources and/or heuristics, which also means that the methods are languageand/or domain-specific, and therefore do not generalize well. Even though Reddy et al. (2014) claim that their approach does not require as much supervision as prior work, it still relies on many Englishspecific heuristics and hand-crafted features. Also, their most accurate model uses a corpus of paraphrases to generalize to linguistic diversity. Also, a practical weakness of linguistic parsers is that they are usually too slow to provide sufficiently low latency in an online QA setting.\nIn contrast, our RNN-based parser detects entities in the question with very high accuracy (\u223c90%) and low latency (< 50ms). The only required re-\nsources are a set of questions with entities tagged and a word embedding. The latter can be easily trained for any language or domain in an unsupervised fashion, given a large text corpus without annotations (Mikolov et al., 2013; Pennington et al., 2014). The former is a relatively simpler annotation task that exists for many languages and domains, and it can also be synthetically generated. One of the strengths of our approach is to combine word embeddings with deep learning to learn robust semantic representations. While many researchers have explored this space for general NLP tasks (Collobert et al., 2011), such as named entity recognition (Lu et al., 2015; Hammerton, 2003), sequence labeling (Graves, 2008; Chung et al., 2014), partof-speech tagging (Huang et al., 2015; Wang et al., 2015), chunking (Huang et al., 2015), we are not aware of the use of RNNs to parse questions for QA.\nAmong the many recent advancements in applying deep learning techniques to QA, a general trend is to construct parallel neural networks for modeling a joint probability distribution for question-answer pairs (Hsu et al., 2016; Yang et al., 2014; He et al., 2015; Mueller and Thyagarajan, 2016). Memory Networks (Bordes et al., 2015) are also based on a similar principle, trying to discover supporting facts by matching the question to the compiled memory. This is probably needed for general-purpose question answering and sentence similarity, where one cannot make assumptions about the structure of the input or knowledge. However, as noted in Section 1, first-order factoid questions can be represented by an entity and a relation type, and the answer is usually stored in a structured knowledge base. Dong et al. (2015) similarly make the assumption that each question contains a target entity, and search for the best answer at most two hops away in the KB. However, they do not propose how to obtain the target entity, as they assume it is provided as part of the task. Bordes et al. (2014) take advantage of the KB structure by projecting entities, relations, and subgraphs into the same latent space.\nIn addition to parsing the question to find the target entity, we mentioned that the other key information is the relation type corresponding to the question. Many researchers have shown that classifying the question into one of the pre-defined types, either based on patterns (Zhang and Lee, 2003) or a trained\nsupport vector machine (Buscaldi et al., 2010), improves QA accuracy. More recently, Iyyer et al. demonstrated the power of RNNs by treating the QuizBowl task as a classification problem (2014). However, the Jeopardy-like task is substantially different than the factoid QA we intend to tackle: every answer is assumed to be one of the thousand entities, which does not hold for many questions (e.g., \u201cHow old is Tom Hanks\u201d), and the number of entities in a realistic knowledge base is orders of magnitude higher (\u223c1000 vs. millions). Unlike the paragraph-long questions in the QuizBowl dataset used in (Iyyer et al., 2014), the typical factoid question contains a handful of words. Nonetheless, our RNN-based classifier is impressively accurate at determining which relation type in Freebase (amongst more than 2000 choices) a question corresponds to."}, {"heading": "3 Approach", "text": "We present a novel approach to answer first-order factoid questions. In factoid question answering, we assume that the answer to the question can be determined objectively. More specifically, we assume that the answer to the question exists in the provided knowledge base. First-order questions further require that the answer can be found in a single fact in the knowledge base.\nEven though (Bordes et al., 2015) refer to firstorder factoid questions as simple questions, current approaches still struggle in accurately answering them (e.g., state-of-the-art accuracy is around 64% and 45% on the SimpleQuestions and WebQuestions datasets). Two main challenges contribute to this: (a) the same question can be asked in different syntactic and lexical varieties, and (b) there are many entities in a typical knowledge base, making resolution a hard task. Ambiguity might be inherent in the question \u2014 e.g., in \u201cWhen did Kennedy die\u201d, it is unclear which person is being referred to as \u201cKennedy\u201d.\n(a) From Question to Structured Query. Our approach is based on a knowledge base, containing a large set of facts, each one representing a binary [subject, relation, object] relationship. Since we assume first-order questions, the answer can be retrieved from a single fact. For instance, \u201cHow old is Tom Hanks?\u201d can be answered by the fact:\n[Tom Hanks, born_on, 7/9/1956]\nThe main idea is to represent the semantics of a first-order factoid question by converting the input question into a structured query with two slots. For example, the question above would be converted to:\n{entity : Tom Hanks, relation : born_on}\nOnce a query is formulated, QA is reduced to a search problem (i.e., retrieving the fact in the knowledge base that best matches the query). Section 3b describes that part of our approach. The rest of this section describes how we construct the query.\nThe process of converting a natural-language question to a structured query can be modularized into two machine learning problems, namely entity detection and relation prediction. In the former, the objective is to tag whether each question word is part of an entity. In the latter, the objective is to classify the question into one of the K relation types.\nWe modeled both problems using recurrent neural networks (RNN) (Elman, 1990). Providing a detailed description of RNNs is out of the scope of this paper, but we will give a brief overview in the context of question representation. One of the most important aspects of NLP over sentence-level text (e.g., QA) is to model dependencies across words in the question. This is usually satisfied by engineering features that describe the local context (e.g., whether the word to the left is a verb, the first two letters of the first word in the sentence, etc.). While these approaches perform well in certain cases, they depend on hand-crafting the right set of features, requiring substantial effort and varying based on domain, task, and even dataset.\nAn RNN naturally models dependencies among all words in the input sequence (i.e., question) without any feature engineering. Certain variations of RNNs, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014a), also learn contextual features that are useful for the task, while learning to ignore irrelevant parts. For example, the model might learn that the first word in a question might be a useful signal for classification, and that stop words are not useful features regardless of where they appear in the question. RNNs\u2019 effectiveness in exploiting wide textual con-\ntext has been shown in numerous cases (e.g., sentiment analysis (Socher et al., 2013) and machine translation (Cho et al., 2014b)).\nEach word in the question passes through an embedding lookup layer E, projecting the one-hot vector into a d-dimensional vector xt. A recurrent layer combines this representation with the hidden layer from the previous word with the input (ht\u22121; existing memory) and applies a non-linear transforms (e.g., ReLu) to compute the hidden layer representation for the current word (ht; updated memory). The hidden representation of the final recurrent layer is projected to the output space of k dimensions (k is the number of classes), and normalized into a probability distribution via soft-max.\nht = ReLu(Wht\u22121 + Uxt) hidden repr. at t\not = softmax(V ht) output at t\nIn the entity detection task, each word is classified as either entity or context, so k is 2. In relation prediction, we classify the entire question into one of the many classes (k varies by task, see Section 4), and the hidden layer of the final word is used for this. In other words, given question q1 . . . qt . . . qn, for entity detection, ot is a probability distribution that represents the tag prediction for qt; for relation prediction, on is a probability distribution represents the class prediction for the entire question. In both cases, parameters are learned via stochastic gradient descent, using categorical cross-entropy as objective. In order to handle variable-length input, we limit the input to N tokens and prepend a special pad word if input has fewer.2\nAn LSTM or GRU is a type of RNN with a slightly more complex transition function. These models contain gates that determine to what degree that the existing memory is forgotten and that the new input is added to the memory. This allows a more powerful model for longer text sequences, while increasing the number of parameters and requiring more training data as a result. Another useful modification is to allow memory to be passed in both directions: left-to-right and right-to-left. These bidirectional RNN units can use information about\n2N is set to the maximum number of tokens across training questions.\nthe entire sequence q1 . . . qt . . . qn, while making a decision at time step t.\n(b) Searching the Knowledge Base. In order to make our knowledge base searchable by a given structured query, we built two indexes: An inverted index Ientity maps all n-grams of an entity (n = 1, 2, 3) to the entity, with an associated BM25 score. We also map the exact text (n = \u221e) to be able to find exact matches. A graph reachability index Ireach maps each entity node e to all nodes e\u2032 that are reachable, with the associated path p(e, e\u2032). For the purpose of the current approach, we limit our search to a single hop away, but this index can be easily expanded to support a wider search.\nSearch for QA works as follows: Given a new question, we run the two RNN models to construct the structured query q={entity: te, relation: r}. Next step is to find the intended entity in the KB. Starting with an exact string match (i.e., te), we query Ientity, which returns all entities in the KB that are potential matches. Retrieved entities are added to the candidate set C, with associated BM25 relevance scores. Unless there is a reason to terminate earlier, we iterate over values for n in a descending order (i.e., n \u2208 {\u221e, 3, 2, 1}). For each value of n, we query Ientity with all n-grams in te, appending the retrieved list to C. Early termination happens if (i) C is nonempty, and (ii) n is less than or equal to the number of tokens in te. The latter criterion is to avoid cases where although we find an exact match, there are also partial matches that might be more relevant (e.g., \u201cjurassic park\u201d has an exact match to the original movie, but we would also like to retrieve \u201cjurassic park II\u201d as a candidate entity).\nOnce we have a list of candidate entities C, we use each candidate node ecand as a starting point to reach candidate answers. We query Ireach to retrieve all nodes ea that are reachable from ecand, where the path from is consistent with the predicted relation r (i.e., r \u2208 p(ecand, ea)). These are added to the candidate answer set A. After repeating this process for each entity in C, the highest-scored node in A is our best answer to the question."}, {"heading": "4 Experimental Setup", "text": "Data. Evaluation of our approach was carried out on two QA benchmark datasets: WebQuestions (WQ)\nand SimpleQuestions (SQ). Both WQ and SQ are based on the same subset of Freebase, which contains 17.8M million facts, 4M unique entities, and 7523 relation types. Indexes Ientity and Ireach are built based on this knowledge base. We first describe the datasets and how they differ in terms of the question collection process and scale.\nWebQuestions was constructed by (Berant et al., 2013) as a factoid question answering benchmark set. Question templates were constructed using Google Suggest API, which were then instantiated by asking users on Mechanical Turk to annotate each question with an answer, including the Freebase page from which the answer was found. However, neither entities in the question text nor the relation type are provided. In order to use this dataset in our evaluation, we needed to infer this information from Freebase. This is a non-trivial task that results in noisy labels, so we decided to share the details with the research community.\nWe started with a post-processed version of WebQuestions,3 which uses a combination of heuristics and an existing QA system to obtain the following information for each question: all entities mentioned in the question (along with Freebase identifier) and set of relation types that could have resulted in the provided textual answer. Many questions are correctly associated with multiple relation types, each corresponding to a different path to the same answer. We created a separate training instance for each one, and at test time, we accept any of the possible relations as the correct prediction (since all of them result in the same answer).\nSimpleQuestions was built by (Bordes et al., 2014) to serve as a larger, more diverse QA dataset. Human annotators were told to create questions from a Freebase fact. Facts were sampled in a way\n3https://github.com/brmson/ dataset-factoid-webquestions.git\nthat more frequent relation types had a lower chance of being selected. This ensures a diverse set of questions that are fully compatible with Freebase, and are labeled with corresponding entity and relation type.\nStatistics about the datasets are summarized in Table 1. For both datasets, even after the process above, there was a small subset for which we could not identify either an associated relation, or the correct entity. These were removed from the test set, since we cannot evaluate our approach. There is no reason to believe these questions were more difficult than others, so we assume that our accuracy would have been same on the removed portion, and compare against published results accordingly. But even if we assume all of them to be answered incorrectly, our conclusions would not change. Training. Input length (N ) was set to the maximum number of tokens across training and validation splits: 36 for SQ and 15 for WQ models. We fixed the embedding layer based on the pre-trained 300-dimensional Google News embedding,4 since the data size is too small for training embeddings. Out-of-vocabulary words were assigned to a random vector (sampled from uniform distribution). We tried a variety of configurations for the RNN: four choices for the type of RNN layer (GRU or LSTM, bidirectional or not); depth from 1 to 3; and dropout ratio from 0 to 0.5, yielding a total of 48 possible configurations. For each possible setting, we trained the model on the training portion and used the validation portion to avoid over-fitting. Training was terminated if the validation loss did not improve for five epochs. Unsurprisingly, we observed that most learning occurred in the first few epochs, yet the final models usually trained for up to 100 epochs (several hours). After running all 48 experiments, the most optimal setting was selected by micro-average F-score of predicted entities (entity detection) or accuracy (relation prediction) on the validation set. The same process was repeated for both WQ and SQ tasks."}, {"heading": "5 Results", "text": "Model selection. The effect of the various parameter settings on model effectiveness can be seen in Figures 1(a) and 1(b). Each figure shows\n4word2vec.googlecode.com\nresults on 48 different settings, as evaluated on the validation set of the specified dataset and task. Every line is labeled with naming convention (Bi)(LSTM|GRU)(1|2|3), and each point represents a different drop-out value (i.e., x axis).\nLSTM (circle or star) seems to work better for entity detection, and GRU (diamond or square) outperforms on relation prediction. A drop-out beyond 20% became degrading (especially for deeper models), when there was simply too much noise to learn such a complex model. More training examples could have been a remedy for this. Based on these results, we concluded that the optimal model is a 2- layer bidirectional LSTM (BiLSTM2) for entity detection on SQ, and a BiLSTM1 on WQ (10% dropout in both cases). For relation prediction on both SQ and WQ, the most optimal model is BiGRU2. In the former dataset, a 10% drop-out was optimal, whereas 20% worked better on WQ.\nRelation prediction accuracy is artificially low on\nWQ due to the way we prepared the data: The same input question is labeled as multiple relations, since these had to be inferred from Freebase, and were not originally published as part of the dataset. Even though this results in lower accuracy during training, it does not worsen the learning process, since all of the labels are potentially correct. We will describe how we can take advantage of other information to select the most viable relation at runtime.\nIn Table 2, we explored the impact of training data, by applying a SQ-trained model to the validation set of WQ (and vice versa).5 This causes a compatibility issue for relation prediction, since there might be relations in the test set that the model did not train on. In fact, there are 271 classes in the validation set of WQ, for which there are no training instances in SQ (and 622 such classes vice versa). We report accuracy on only compatible test questions in parentheses. Additionally, we trained a combined model using all training examples in SQ and WQ (i.e., SQ+WQ), and tested on the individual datasets. For this case, since we take a union of the possible relations in SQ and WQ, there is no compatibility issue. SQ+WQ was superior in all runs.\nEnd-to-End QA. Given a test set for the end-to-end evaluation, we apply the relation prediction and entity detection models on each test question, yielding a structured query q = {entity: te, relation: r} (Section 3a). We then run our search algorithm to find a ranked list of relevant entity nodes in Freebase (ecand), and corresponding list of answers A (Section 3b). Simply comparing the top answer to the ground truth answer does not always work because our system outputs the identifier of a Freebase node, whereas ground truth answer is provided as the textual representation of a Freebase node.6 As a remedy, we follow Bordes et al. (2015), where we compare the predicted entity-relation pair to the ground truth. In other words, a question is counted as correct if and only if the entity we find (i.e., ecand) matches the ground truth, and the relation we predict (i.e, r) is in the list of correct relation types (for SQ, there is only one correct relation type per question).\nTable 3 summarizes end-to-end experimental results on both benchmark datasets. The upper portion shows best reported results; the lower portion is dedicated to our approach: First two columns specify which models were used to convert the question into a structured query, and third column specifies how we performed search to retrieve answers.\n\u201cBest in-domain\u201d refers to the best model that was trained on the training portion of the same corpus (Figures 1(a) and 1(b)). In order to quantify the\n5We selected the best model from Figures 1(a) and 1(b). 6The complete mapping between a Freebase identifier and\nits textual representation is not readily available.\nimportance of search, the first two rows show accuracy with and without search (i.e., latter case directly compares the predicted entity-relation pair in q, (te, r) to ground truth). Second and third rows show the effect of training models on the combined dataset instead of restricting to the training portion. As expected from Table 2, this improves the accuracy for WQ substantially (13.5% absolute points). On SQ, we notice a slight drop.\nIn remaining rows, a star indicates that we used the best model. The huge jump in accuracy (\u223c9- 13%) is due to relation correction during search: For a predicted entity node ecand, we can limit our relation choices to the set of unique relation types that ecand is associated with. This helps eliminate the artificial ambiguity due to many overlapping relation types in Freebase. With this simple but effective idea, the number of relation types that we need to consider per question (on average) drops from 2290 to 120 for the SQ+WQ model.\nIn certain cases, our approach produces the correct textual answer, although it does not pass the correctness test described above. This happens especially in SQ when the ground truth includes a single correct relation type, although there are multiple different ways to arrive at the same answer. The fourth column indicates whether these bonus cases are counted as correct answers.\nIn terms of comparison to prior work, our approach outperforms the state of the art by significant margins. The absolute improvement in accuracy (i.e., precision at top 1) over the state of the art is 7.1 points (15.7% relative) on WQ and 24.4 points (38.2% relative) on SQ \u2014 we obtain 52.2% and 88.3% accuracy on these two tasks, respectively.\nLast three rows quantify the impact of our two RNN models by performing an ablation study, in which we replace either model with a naive baseline: (i) we assign the relation that appears most frequently in training data (i.e., born on), and/or (ii) we tag the entire question as an entity (and then perform the n-gram based search). Results confirm that relation prediction is absolutely critical, since both datasets include a diverse and well-balanced set of relation types. When we applied the naive entity detection baseline (row 6), our results drop significantly, but they are still comparable to the state of the art. Given that most prior work actually use a\nvariety of that baseline, this supports the argument that our RNN-based entity detection was the largest contributor to the success of our approach. Error Analysis. In order to better understand the weaknesses of our approach, we performed a blame analysis: Among 875 errors in WQ test set, 135 can be blamed on entity detection \u2014 the relation type was correctly predicted, but the detected entity did not match the ground truth. The reverse is true for 408 cases.7 We manually labeled a sample of 50 instances from each blame scenario. When entity detection is to blame, 20% was due to spelling inconsistencies between question and KB, which can be resolved with better text normalization during indexing (e.g., \u201cla kings\u201d refers to \u201cLos Angeles Kings\u201d). We found 16% of the detected entities to be correct, even though it was not the same as the ground truth (e.g., either \u201cNew York\u201d or \u201cNew York City\u201d is correct in \u201cwhat can do in new york?\u201d); 18% are inherently ambiguous and need clarification (e.g., \u201cwhere bin laden got killed?\u201d might mean \u201cOsama\u201d or \u201cSalem\u201d). When blame is on relation prediction, we found that the predicted relation is correct (albeit different than ground truth) 29% of the time (e.g., \u201cwhat was nikola tesla known for\u201d can be classified as profession or notable for). We also labeled\n7In remaining 332 incorrect answers, both models fail, so the blame is shared.\n22% of the questions as higher-order, since they require reasoning beyond a single fact in the KB (e.g., \u201cwhat country did germany invade first in ww1\u201d)."}, {"heading": "6 Conclusions and Future work", "text": "We described a simple yet effective approach for factoid question answering. We assume that firstorder QA can be reduced to two tasks, entity detection and relation prediction, and provide solutions to each with a recurrent neural network. Experimental results indicate impressive improvements over the state of the art on two commonly-used QA benchmark datasets: relative improvements in accuracy were 15.7% for WQ and 38.2% for SQ. From these very positive results, we can deduce that RNNs are very effective at tagging and classification over arbitrary text sequences. We can also argue that building a specific architecture that custom-fits the task has advantages over architectures aiming to do more general-purpose reasoning over text (e.g., (Bordes et al., 2015; Kumar et al., 2015)). Our approach would require a non-trivial augmentation to work with other types of QA tasks: \u201cWhat movie did both Tom Hanks and Meg Ryan appear in?\u201d requires us to find an entity in the KB that is related to the two entities in the question. Finally, we found that many mistakes are due to search-related issues, so a more refined retrieval could improve accuracy."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013,", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Structured Retrieval for Question Answering", "author": ["Matthew W Bilotti", "Paul Ogilvie", "Jamie Callan", "Eric Nyberg."], "venue": "Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201907, pages", "citeRegEx": "Bilotti et al\\.,? 2007", "shortCiteRegEx": "Bilotti et al\\.", "year": 2007}, {"title": "Rank Learning for Factoid Question Answering with Linguistic and Semantic Constraints", "author": ["Matthew W Bilotti", "Jonathan Elsas", "Jaime Carbonell", "Eric Nyberg."], "venue": "Proceedings of the 19th ACM International Conference on Information and Knowledge Manage-", "citeRegEx": "Bilotti et al\\.,? 2010", "shortCiteRegEx": "Bilotti et al\\.", "year": 2010}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1406.3676.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "CoRR, abs/1506.02075.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Answering Questions with an N-gram Based Passage Retrieval Engine", "author": ["Davide Buscaldi", "Paolo Rosso", "Jos\u00e9 Manuel G\u00f3mezSoriano", "Emilio Sanchis."], "venue": "J. Intell. Inf. Syst., 34(2):113\u2013134, April.", "citeRegEx": "Buscaldi et al\\.,? 2010", "shortCiteRegEx": "Buscaldi et al\\.", "year": 2010}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Qingqing Cai", "Alexander Yates."], "venue": "ACL (1), pages 423\u2013433. Citeseer.", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res., 12:2493\u20132537, November.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Question answering over freebase with multi-column convolutional neural networks", "author": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Alex Graves."], "venue": "Ph.D. thesis, Technical University Munich.", "citeRegEx": "Graves.,? 2008", "shortCiteRegEx": "Graves.", "year": 2008}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton."], "venue": "Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4, pages 172\u2013175. Association for Computational Linguistics.", "citeRegEx": "Hammerton.,? 2003", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Multiperspective sentence similarity modeling with convolutional neural networks", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin."], "venue": "Llu\u0131\u0301s M\u00e0rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Proceedings of the 2015 Conference", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Recurrent neural network encoder with attention for community question answering", "author": ["Wei-Ning Hsu", "Yu Zhang", "James R. Glass."], "venue": "CoRR, abs/1603.07044.", "citeRegEx": "Hsu et al\\.,? 2016", "shortCiteRegEx": "Hsu et al\\.", "year": 2016}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "EMNLP, pages 720\u2013728.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan L. Boyd-Graber", "Leonardo Max Batista Claudino", "Richard Socher", "Hal Daum\u00e9."], "venue": "EMNLP.", "citeRegEx": "Iyyer et al\\.,? 2014", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Gulrajani", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Gulrajani and Socher.,? 2015", "shortCiteRegEx": "Gulrajani and Socher.", "year": 2015}, {"title": "Twisted recurrent network for named entity recognition", "author": ["Zefu Lu", "Lei Li", "Wei Xu."], "venue": "Bay Area Machine Learning Symposium.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Siamese recurrent architectures for learning sentence similarity", "author": ["Jonas Mueller", "Aditya Thyagarajan."], "venue": "Dale Schuurmans and Michael P. Wellman, editors, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix,", "citeRegEx": "Mueller and Thyagarajan.,? 2016", "shortCiteRegEx": "Mueller and Thyagarajan.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Large-scale semantic parsing without question-answer pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "TACL, 2:377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on empirical meth-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Part-of-speech tagging with bidirectional long short-term memory recurrent neural network", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao."], "venue": "arXiv preprint arXiv:1510.06168.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Joint relational embeddings for knowledge-based question answering", "author": ["Min-Chul Yang", "Nan Duan", "Ming Zhou", "HaeChang Rim."], "venue": "EMNLP, pages 645\u2013650.", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Question Classification Using Support Vector Machines", "author": ["Dell Zhang", "Wee Sun Lee."], "venue": "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR \u201903, pages 26\u201332, New York, NY, USA.", "citeRegEx": "Zhang and Lee.,? 2003", "shortCiteRegEx": "Zhang and Lee.", "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "tic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014).", "startOffset": 102, "endOffset": 210}, {"referenceID": 1, "context": "tic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014).", "startOffset": 102, "endOffset": 210}, {"referenceID": 2, "context": "tic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014).", "startOffset": 102, "endOffset": 210}, {"referenceID": 0, "context": "tic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014).", "startOffset": 102, "endOffset": 210}, {"referenceID": 28, "context": "tic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014).", "startOffset": 102, "endOffset": 210}, {"referenceID": 18, "context": "and Cardie, 2014; Kim, 2014; Chen and Manning, 2014), including QA (Hsu et al., 2016; Yang et al., 2014; Dong et al., 2015; Bordes et al., 2015).", "startOffset": 67, "endOffset": 144}, {"referenceID": 32, "context": "and Cardie, 2014; Kim, 2014; Chen and Manning, 2014), including QA (Hsu et al., 2016; Yang et al., 2014; Dong et al., 2015; Bordes et al., 2015).", "startOffset": 67, "endOffset": 144}, {"referenceID": 12, "context": "and Cardie, 2014; Kim, 2014; Chen and Manning, 2014), including QA (Hsu et al., 2016; Yang et al., 2014; Dong et al., 2015; Bordes et al., 2015).", "startOffset": 67, "endOffset": 144}, {"referenceID": 4, "context": "and Cardie, 2014; Kim, 2014; Chen and Manning, 2014), including QA (Hsu et al., 2016; Yang et al., 2014; Dong et al., 2015; Bordes et al., 2015).", "startOffset": 67, "endOffset": 144}, {"referenceID": 31, "context": "1 While this sounds similar to the \u201cSingle Supporting Fact\u201d task in (Weston et al., 2015), for which they report 100% accuracy, the problem becomes much more complex as the knowledge base grows to a realistic size, since the level of ambiguity increases drastically with a larger vocabulary and list of entities.", "startOffset": 68, "endOffset": 89}, {"referenceID": 6, "context": "This is the reason why we focus on well-studied, large-scale QA tasks like Free917 (Cai and Yates, 2013), WebQuestions (Be-", "startOffset": 83, "endOffset": 104}, {"referenceID": 4, "context": ", 2013) (WQ), and SimpleQuestions (Bordes et al., 2015) (SQ), that rely on Freebase as a realistic knowledge base.", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "Many methods have been proposed for parsing questions, such as a pattern-based question analyzer (Buscaldi et al., 2010), combination of syntactic parsing and se-", "startOffset": 97, "endOffset": 120}, {"referenceID": 1, "context": "mantic role labeling (Bilotti et al., 2007; Bilotti et al., 2010), as well as lambda calculus (Berant et al.", "startOffset": 21, "endOffset": 65}, {"referenceID": 2, "context": "mantic role labeling (Bilotti et al., 2007; Bilotti et al., 2010), as well as lambda calculus (Berant et al.", "startOffset": 21, "endOffset": 65}, {"referenceID": 0, "context": ", 2010), as well as lambda calculus (Berant et al., 2013) and combinatory categorical grammars (CCG) (Reddy et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 28, "context": ", 2013) and combinatory categorical grammars (CCG) (Reddy et al., 2014).", "startOffset": 51, "endOffset": 71}, {"referenceID": 28, "context": "Even though Reddy et al. (2014) claim that their approach does not require as much supervision as prior work, it still relies on many Englishspecific heuristics and hand-crafted features.", "startOffset": 12, "endOffset": 32}, {"referenceID": 25, "context": "The latter can be easily trained for any language or domain in an unsupervised fashion, given a large text corpus without annotations (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 134, "endOffset": 181}, {"referenceID": 27, "context": "The latter can be easily trained for any language or domain in an unsupervised fashion, given a large text corpus without annotations (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 134, "endOffset": 181}, {"referenceID": 11, "context": "While many researchers have explored this space for general NLP tasks (Collobert et al., 2011), such as named entity recognition (Lu et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 24, "context": ", 2011), such as named entity recognition (Lu et al., 2015; Hammerton, 2003), sequence labeling (Graves, 2008; Chung et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 15, "context": ", 2011), such as named entity recognition (Lu et al., 2015; Hammerton, 2003), sequence labeling (Graves, 2008; Chung et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 14, "context": ", 2015; Hammerton, 2003), sequence labeling (Graves, 2008; Chung et al., 2014), part-", "startOffset": 44, "endOffset": 78}, {"referenceID": 10, "context": ", 2015; Hammerton, 2003), sequence labeling (Graves, 2008; Chung et al., 2014), part-", "startOffset": 44, "endOffset": 78}, {"referenceID": 19, "context": "of-speech tagging (Huang et al., 2015; Wang et al., 2015), chunking (Huang et al.", "startOffset": 18, "endOffset": 57}, {"referenceID": 30, "context": "of-speech tagging (Huang et al., 2015; Wang et al., 2015), chunking (Huang et al.", "startOffset": 18, "endOffset": 57}, {"referenceID": 19, "context": ", 2015), chunking (Huang et al., 2015), we are not aware of the use of RNNs to parse questions for QA.", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "Memory Networks (Bordes et al., 2015) are also based on a similar principle, trying to discover supporting facts by matching the question to the compiled memory.", "startOffset": 16, "endOffset": 37}, {"referenceID": 10, "context": "Dong et al. (2015) similarly make the assumption that each question contains a target entity, and search for the best answer at most two hops away in the KB.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Bordes et al. (2014) take advantage of the KB structure by projecting entities, relations, and subgraphs into the same latent space.", "startOffset": 0, "endOffset": 21}, {"referenceID": 33, "context": "Many researchers have shown that classifying the question into one of the pre-defined types, either based on patterns (Zhang and Lee, 2003) or a trained", "startOffset": 118, "endOffset": 139}, {"referenceID": 5, "context": "support vector machine (Buscaldi et al., 2010), improves QA accuracy.", "startOffset": 23, "endOffset": 46}, {"referenceID": 5, "context": "support vector machine (Buscaldi et al., 2010), improves QA accuracy. More recently, Iyyer et al. demonstrated the power of RNNs by treating the QuizBowl task as a classification problem (2014).", "startOffset": 24, "endOffset": 194}, {"referenceID": 21, "context": "Unlike the paragraph-long questions in the QuizBowl dataset used in (Iyyer et al., 2014), the typical factoid question contains a handful of words.", "startOffset": 68, "endOffset": 88}, {"referenceID": 4, "context": "Even though (Bordes et al., 2015) refer to firstorder factoid questions as simple questions, current approaches still struggle in accurately answering them (e.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "We modeled both problems using recurrent neural networks (RNN) (Elman, 1990).", "startOffset": 63, "endOffset": 76}, {"referenceID": 17, "context": "Certain variations of RNNs, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al.", "startOffset": 66, "endOffset": 100}, {"referenceID": 8, "context": "Certain variations of RNNs, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014a), also learn contextual features that are useful for the task, while learning to ignore irrelevant parts.", "startOffset": 133, "endOffset": 152}, {"referenceID": 29, "context": ", sentiment analysis (Socher et al., 2013) and machine translation (Cho et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 9, "context": ", 2013) and machine translation (Cho et al., 2014b)).", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": "a remedy, we follow Bordes et al. (2015), where we compare the predicted entity-relation pair to the ground truth.", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": "Prior work (Berant et al., 2013): lambda calculus based semantic parser 31.", "startOffset": 11, "endOffset": 32}, {"referenceID": 32, "context": "(Yang et al., 2014): entity embedding, uses Wikipedia 41.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "3 (Reddy et al., 2014): CCG-based semantic parser, uses ClueWeb 41.", "startOffset": 2, "endOffset": 22}, {"referenceID": 4, "context": "0 (Bordes et al., 2015): ensemble method, uses Paraphrase DB 42.", "startOffset": 2, "endOffset": 23}, {"referenceID": 12, "context": "9 (Dong et al., 2015): assumes entity given, CNN over Q-A pairs 45.", "startOffset": 2, "endOffset": 21}, {"referenceID": 4, "context": ", (Bordes et al., 2015; Kumar et al., 2015)).", "startOffset": 2, "endOffset": 43}], "year": 2016, "abstractText": "First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 35%\u201365% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. Based on this assumption of the structure, our simple yet effective approach trains two recurrent neural networks to outperform state of the art by significant margins \u2014 relative improvement reaches 16% for WebQuestions, and surpasses 38% for SimpleQuestions.", "creator": "TeX"}}}