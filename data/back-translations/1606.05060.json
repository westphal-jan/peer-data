{"id": "1606.05060", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Pruning Random Forests for Prediction on a Budget", "abstract": "We propose to prune a Random Forest (RF) for a resource-constrained prediction; we first construct an RF and then prune it to optimize the expected functional cost & amp; accuracy; we present the pruning of RFs as a novel holistic program with linear constraints that promotes the reuse of functions; we establish the total uniformity of the compulsory set to prove that the corresponding LP loosening solves the original integer program; we then use combinatorial optimization combinations and develop an efficient primary-dual algorithm that is scalable to large data sets. Unlike our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down methods that acquire characteristics for their utility and are generally insoluble, which requires heuristics.", "histories": [["v1", "Thu, 16 Jun 2016 05:56:36 GMT  (168kb,D)", "http://arxiv.org/abs/1606.05060v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["feng nan", "joseph wang", "venkatesh saligrama"], "accepted": true, "id": "1606.05060"}, "pdf": {"name": "1606.05060.pdf", "metadata": {"source": "CRF", "title": "Pruning Random Forests for Prediction on a Budget", "authors": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "emails": ["fnan@bu.edu", "joewang@bu.edu", "srv@bu.edu"], "sections": [{"heading": "1 Introduction", "text": "Many modern classification systems, including internet applications (such as web-search engines, recommendation systems, and spam filtering) and security & surveillance applications (such as widearea surveillance and classification on large video corpora), face the challenge of prediction-time budget constraints [26]. Prediction-time budgets can arise due to monetary costs associated with acquiring information or computation time (or delay) involved in extracting features and running the algorithm. We seek to learn a classifier by training on fully annotated training datasets that maintains high-accuracy while meeting average resource constraints during prediction-time. We consider a system that adaptively acquires features as needed depending on the instance(example) for high classification accuracy with reduced feature acquisition cost.\nWe propose a two-stage algorithm. In the first stage, we train a random forest (RF) of trees using an impurity function such as entropy or more specialized cost-adaptive impurity [17]. Our second stage takes a RF as input and attempts to jointly prune each tree in the forest to meet global resource constraints. During prediction-time, an example is routed through all the trees in the ensemble to the corresponding leaf nodes and the final prediction is based on a majority vote. The total feature cost for a test example is the sum of acquisition costs of unique features1 acquired for the example in the entire ensemble of trees in the forest. 2\nWe derive an efficient scheme to learn a globally optimal pruning of a RF minimizing the empirical error and incurred average costs. We formulate the pruning problem as a 0-1 integer linear program that incorporates feature-reuse constraints. By establishing total unimodularity of the constraint set, we show that solving the linear program relaxation of the integer program yields the optimal solution to the integer program resulting in a polynomial time algorithm for optimal pruning. We develop a primal-dual algorithm by leveraging re-\n1When an example arrives at an internal node, the feature associated with the node is used to direct the example. If the feature has never been acquired for the example an acquisition cost is incurred. Otherwise, no acquisition cost is incurred as we assume that feature values are stored once computed.\n2For time-sensitive cases such as web-search we parallelize the implementation by creating parallel jobs across all features and trees. We can then terminate jobs based on what features are returned.\nar X\niv :1\n60 6.\n05 06\n0v 1\n[ st\nat .M\nL ]\n1 6\nJu n\n20 16\nsults from network-flow theory for scaling the linear program to large datasets. Empirically, this pruning outperforms state-of-the-art resource efficient algorithms on benchmarked datasets.\nOur approach is motivated by the following considerations: (i) RFs are scalable to large datasets and produce flexible decision boundaries yielding high prediction-time accuracy. The sequential feature usage of decision trees lends itself to adaptive feature acquisition. (ii) RF feature usage is superfluous, utilizing features with introduced randomness to increase diversity and generalization. Pruning can yield significant cost reduction with negligible performance loss by selectively pruning features sparsely used across trees,\nleading to cost reduction with minimal accuracy degradation (due to majority vote). See Table 1. (iii) Optimal pruning encourages examples to use features either a large number of times, allowing for complex decision boundaries in the space of those features, or not to use them at all, avoiding incurring the cost of acquisition. It enforces the fact that once a feature is acquired for an example, repeated use incurs no additional acquisition cost. Intuitively, features should be repeatedly used to increase discriminative ability without incurring further cost. (iv) Resource constrained prediction has been conventionally viewed as a top-down (tree-growing) approach, wherein new features are acquired based on their utility value. This is often an intractable problem with combinatorial (feature subsets) and continuous components (classifiers) requiring several relaxations and heuristics. In contrast, ours is a bottom-up approach that starts with good initialization (RF) and prunes to realize optimal cost-accuracy tradeoff. Indeed, while we do not pursue it, our approach can also be used in conjunction with existing approaches.\nRelated Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12]. These approaches focus on learning complex adaptive decision functions and can be viewed as orthogonal to our work. Conceptually, these are top-down \u201cgrowing\u201d methods as we described earlier (see (iv)). Our approach is bottom-up that seeks to prune complex classifiers to tradeoff cost vs. accuracy.\nOur work is based on RF classifiers [3]. Traditionally, feature cost is not incorporated when constructing RFs, however recent work has involved approximation of budget constraints to learn budgeted RFs [17]. The tree-growing algorithm in [17] does not take feature re-use into account. Rather than attempting to approximate the budget constraint during tree construction, our work focuses on pruning ensembles of trees subject to a budget constraint. Methods such as traditional ensemble learning and budgeted random forests can be viewed as complementary.\nDecision tree pruning has been studied extensively to improve generalization performance, we are not aware of any existing pruning method that takes into account the feature costs. A popular method for pruning to reduce generalization error is Cost-Complexity Pruning (CCP), introduced by Breiman et al. [4]. CCP trades-off classification ability for tree size, however it does not account for feature costs. As pointed out by Li et al. [15], CCP has undesirable \u201cjumps\" in the sequence of pruned tree sizes. To alleviate this, they proposed a Dynamic-Program-based Pruning (DPP) method for binary trees. The DPP algorithm is able to obtain optimally pruned trees of all sizes; however, it faces the curse of dimensionality when pruning an ensemble of decision trees and taking feature cost into account. [28, 20] proposed to solve the pruning problem as a 0-1 integer program; again, their formulations do not account for feature costs that we focus on in this paper. The coupling nature of feature usage makes our problem much harder. In general pruning RFs is not a focus of attention as it is assumed that overfitting can be avoided by constructing an ensemble of trees. While this is true, it often leads to extremely large prediction-time costs. Kulkarni and Sinha [11] provide a survey of methods to prune RFs in order to reduce ensemble size. However, these methods do not explicitly account for feature costs."}, {"heading": "2 Learning with Resource Constraints", "text": "In this paper, we consider solving the Lagrangian relaxed problem of learning under prediction-time resource constraints, also known as the error-cost tradeoff problem:\nmin f\u2208F E(x,y)\u223cP [err (y, f(x))] + \u03bbEx\u223cPx [C (f, x)] , (1)\nwhere example/label pairs (x, y) are drawn from a distribution P; err(y, y\u0302) is the error function; C(f, x) is the cost of evaluating the classifier f on example x; \u03bb is a tradeoff parameter. A larger \u03bb places a larger penalty on cost, pushing the classifier to have smaller cost. By adjusting \u03bb we can obtain a classifier satisfying the budget constraint. The family of classifiers F in our setting is the space of RFs, and each RF f is composed of T decision trees T1, . . . , TT . Our approach: Rather than attempting to construct the optimal ensemble by solving Eqn. (1) directly, we instead propose a two-step algorithm that first constructs an ensemble with low prediction error, then prunes it by solving Eqn. (1) to produce a pruned ensemble given the input ensemble. By adopting this two-step strategy, we obtain an ensemble with low expected cost while simultaneously preserving the low prediction error.\nThere are many existing methods to construct RFs, however the focus of this paper is on the second step, where we propose a novel approach to prune RFs to solve the tradeoff problem Eqn.(1). Our pruning algorithm is capable of taking any RF as input, offering the flexibility to incorporate any state-of-the-art RF algorithm."}, {"heading": "3 Pruning with Costs", "text": "In this section, we treat the error-cost tradeoff problem Eqn. (1) as an RF pruning problem. Our key contribution is to formulate pruning as a 0-1 integer program with totally unimodular constraints.\nWe first define notations used throughout the paper. A training sample S = {(x(i), y(i)) : i = 1, . . . , N} is generated i.i.d. from an unknown distribution, where x(i) \u2208 <K is the feature vector with a cost assigned to each of the K features and y(i) is the label for the ith example. In the case of multi-class classification y \u2208 {1, . . . ,M}, where M is the number of classes. Given a decision tree T , we index the nodes as h \u2208 {1, . . . , |T |}, where node 1 represents the root node. Let T\u0303 denote the set of leaf nodes of tree T . Finally, the corresponding definitions for T can be extended to an ensemble of T decision trees {Tt : t = 1, . . . , T} by adding an subscript t. Pruning Parametrization: In order to model ensemble pruning as an optimization problem, we parametrize the space of all prunings of an ensemble. The process of pruning a decision tree T at an internal node h involves collapsing the subtree of T rooted at h, making h a leaf node. We say a pruned tree T (p) is a valid pruned tree of T if (1) T (p) is a subtree of T containing root node 1 and (2) for any h 6= 1 contained in T (p), the sibling nodes (the set of nodes that share the same immediate parent node as h in T ) must also be contained in T (p). Specifying a pruning is equivalent to specifying the nodes that are leaves in the pruned tree. We therefore introduce the following binary variable for each node h \u2208 T\nzh = { 1 if node h is a leaf in the pruned tree, 0 otherwise.\nWe call the set {zh,\u2200h \u2208 T } the node variables as they are associated with each node in the tree. Consider any root-to-leaf path in a tree T , there should be exactly one node in the path that is a leaf node in the pruned tree. Let p(h) denote the set of predecessor nodes, the set of nodes (including h) that lie on the path from the root node to h. The set of valid pruned trees can be represented as the set of node variables satisfying the following set of constraints: \u2211 u\u2208p(h) zu = 1 \u2200h \u2208 T\u0303 . Given a valid pruning for a tree, we now seek to parameterize the error of the pruning.\nPruning error: As in most supervised empirical risk minimization problems, we aim to minimize the error on training data as a surrogate to minimizing the expected error. In a decision tree T , each node h is associated with a predicted label corresponding to the majority label among the training examples that fall into the node h. Let Sh denote the subset of examples in S routed to or through node h on T and let Predh denote the predicted label at h. The number of misclassified examples\nat h is therefore eh = \u2211 i\u2208Sh 1[y(i) 6=Predh]. We can thus estimate the error of tree T in terms of the\nnumber of misclassified examples in the leaf nodes: 1N \u2211 h\u2208T\u0303 eh, where N = |S| is the total number of examples.\nOur goal is to minimize the expected test error of the trees in the random forest, which we empirically approximate based on the aggregated probability distribution in Step (6) of Algorithm 1 with 1TN \u2211T t=1 \u2211 h\u2208T\u0303t eh. We can express this error in terms of the node variables:\n1 TN \u2211T t=1 \u2211 h\u2208Tt ehzh.\nPruning cost: Assume the acquisition cost for the K features, {ck : k = 1, . . . ,K}, are given. The feature acquisition cost incurred by an example is the sum of the acquisition costs of unique features acquired in the process of running the example through the forest. This cost structure arises due to the assumption that an acquired feature is cached and subsequent usage by the same example incurs no additional cost. Formally, the feature cost of classifying an example i on the ensemble T[T ] is given by Cfeature(T[T ],x(i)) = \u2211K k=1 ckwk,i, where the binary variables wk,i serve as the indicators:\nwk,i = { 1 if feature k is used by x(i) in any Tt, t = 1, . . . , T 0 otherwise.\nThe expected feature cost of a test example can be approximated as 1N \u2211N i=1 \u2211K k=1 ckwk,i.\nIn some scenarios, it is useful to account for computation cost along with feature acquisition cost during prediction-time. In an ensemble, this corresponds to the expected number of Boolean operations required running a test through the trees, which is equal to the expected depth of the trees. This can be modeled as 1N \u2211T t=1 \u2211 h\u2208Tt |Sh|dhzh, where dh is the depth of node h.\nPutting it together: Having modeled the pruning constraints, prediction performance and costs, we formulate the problem of pruning using the relationship between the node variables zh\u2019s and feature usage variables wk,i\u2019s. Given a tree T , feature k, and example x(i), let uk,i be the first node associated with feature k on the root-to-leaf path the example follows in T . Feature k is used by x(i) if and only if none of the nodes between the root and uk,i is a leaf. We represent this by the constraint wk,i + \u2211 h\u2208p(uk,i) zh = 1 for every feature k used by example x\n(i) in T . Recall wk,i indicates whether or not feature k is used by example i and p(uk,i) denotes the set of predecessor nodes of uk,i. Intuitively, this constraint says that either the tree is pruned along the path followed by example i before feature k is acquired, in which case zh = 1 for some node h \u2208 p(uk,i) and wk,i = 0; or wk,i = 1, indicating that feature k is acquired for example i. We extend the notations to ensemble pruning with tree index t: z(t)h indicates whether node h in Tt is a leaf after pruning; w (t) k,i indicates whether feature k is used by the ith example in Tt; wk,i indicates whether feature k is used by the ith example in any of the T trees T1, . . . , TT ; ut,k,i is the first node associated with feature k on the root-to-leaf path the example follows in Tt; Kt,i denotes the set of features the ith example uses on tree Tt. We arrive at the following integer program.\nmin z (t) h ,w (t) k,i,wk,i\u2208{0,1}\nerror\ufe37 \ufe38\ufe38 \ufe37 1\nNT T\u2211 t=1 \u2211 h\u2208Tt e (t) h z (t) h +\u03bb  feature acquisition cost\ufe37 \ufe38\ufe38 \ufe37 K\u2211 k=1 ck( 1 N N\u2211 i=1 wk,i)+ computational cost\ufe37 \ufe38\ufe38 \ufe37 1 N T\u2211 t=1 \u2211 h\u2208Tt |Sh|dhzh  (IP) s.t. \u2211 u\u2208p(h) z (t) u = 1, \u2200h \u2208 T\u0303t,\u2200t \u2208 [T ], (feasible prunings)\nw (t) k,i + \u2211 h\u2208p(ut,k,i) z (t) h = 1, \u2200k \u2208 Kt,i,\u2200i \u2208 S,\u2200t \u2208 [T ], (feature usage/ tree) w (t) k,i \u2264 wk,i, \u2200k \u2208 [K],\u2200i \u2208 S,\u2200t \u2208 [T ]. (global feature usage)\nTotally Unimodular constraints: Even though integer programs are NP-hard to solve in general, we show that (IP) can be solved exactly by solving its LP relaxation. We prove this in two steps: first, we examine the special structure of the equality constraints; then we examine the inequality constraint that couples the trees. Recall that a network matrix is one with each column having exactly one element equal to 1, one element equal to -1 and the remaining elements being 0. A network matrix defines a directed graph with the nodes in the rows and arcs in the columns. We have the following lemma.\nLemma 3.1 The equality constraints in (IP) can be turned into an equivalent network matrix form for each tree. Proof We observe the first constraint \u2211 u\u2208p(h) z (t) u = 1 requires the sum of the node variables along\na path to be 1. The second constraints w(t)k,i + \u2211 h\u2208p(ut,k,i) z (t) h = 1 has a similar sum except the variable w(t)k,i. Imagine w (t) k,i as yet another node variable for a fictitious child node of ut,k,i and the two equations are essentially equivalent. The rest of proof follows directly from the construction in Proposition 3 of [20].\nFigure 1 illustrates such a construction. The nodes are numbered 1 to 5. The subscripts at node 1 and 3 are the feature index used in the nodes. Since the equality constraints in (IP) can be separated based on the trees, we consider only one tree and one example being routed to node 4 on the tree for simplicity. The equality constraints can be organized in the matrix form as shown in the middle of Figure 1. Through row operations, the constraint matrix can be transformed to an equivalent network matrix. Such transformation always works as long as the leaf nodes are arranged in a pre-order manner. Next, we deal with the inequality constraints and obtain our main result.\nTheorem 3.2 The LP relaxation of (IP), where the 0-1 integer constraints are relaxed to interval constraints [0, 1] for all integer variables, has integral optimal solutions.\nDue to space limit the proof can be found in the Appendix. The main idea is to show the constraints are still totally unimodular even after adding the coupling constraints and the LP relaxed polyhedron has only integral extreme points [19]. As a result, solving the LP relaxation results in the optimal solution to the integer program (IP), allowing for polynomial time optimization. 3\nAlgorithm 1 BUDGETPRUNE\nDuring Training: input - ensemble(T1, . . . , TT ), training/validation data with labels, \u03bb\n1: initialize dual variables \u03b2(t)k,i \u2190 0. 2: update z(t)h , w (t) k,i for each tree t (shortest-path algo). wk,i = 0 if \u00b5k,i > 0, wk,i = 1 if \u00b5k,i < 0. 3: \u03b2(t)k,i \u2190 [\u03b2 (t) k,i + \u03b3(w (t) k,i \u2212 wk,i)]+ for step size \u03b3, where [\u00b7]+ = max{0, \u00b7}. 4: go to Step 2 until duality gap is small enough.\nDuring Prediction: input - test exmaple x\n5: Run x on each tree to leaf, obtain the probability distribution over label classes pt at leaf. 6: Aggregate p = 1T \u2211T t=1 pt. Predict the class with the highest probability in p."}, {"heading": "4 A Primal-Dual Algorithm", "text": "Even though we can solve (IP) via its LP relaxation, the resulting LP can be too large in practical applications for any general-purpose LP solver. In particular, the number of variables and constraints\n3The nice result of totally unimodular constraints is due to our specific formulation. See Appendix for an alternative formulation that does not have such a property.\nis roughly O(T \u00d7 |Tmax|+N \u00d7 T \u00d7Kmax), where T is the number of trees; |Tmax| is the maximum number of nodes in a tree; N is the number of examples; Kmax is the maximum number of features an example uses in a tree. The runtime of the LP thus scales O(T 3) with the number of trees in the ensemble, limiting the application to only small ensembles. In this section we propose a primal-dual approach that effectively decomposes the optimization into many sub-problems. Each sub-problem corresponds to a tree in the ensemble and can be solved efficiently as a shortest path problem. The runtime per iteration is O(Tp (|Tmax|+N \u00d7Kmax) log(|Tmax|+N \u00d7Kmax)), where p is the number of processors. We can thus massively parallelize the optimization and scale to much larger ensembles as the runtime depends only linearly on Tp . To this end, we assign dual variables \u03b2 (t) k,i for the inequality constraints w(t)k,i \u2264 wk,i and derive the dual problem.\nmax \u03b2 (t) k,i\u22650 min z (t) h \u2208[0,1] w\n(t) k,i\u2208[0,1] wk,i\u2208[0,1]\n1 NT T\u2211 t=1 \u2211 h\u2208Tt e\u0302 (t) h z (t) h + \u03bb ( K\u2211 k=1 ck( 1 N N\u2211 i=1 wk,i) ) + T\u2211 t=1 N\u2211 i=1 \u2211 k\u2208Kt,i \u03b2 (t) k,i(w (t) k,i \u2212 wk,i)\ns.t. \u2211\nu\u2208p(h)\nz(t)u = 1, \u2200h \u2208 T\u0303t,\u2200t \u2208 [T ],\nw (t) k,i + \u2211 h\u2208p(ut,k,i) z (t) h = 1, \u2200k \u2208 Kt,i,\u2200i \u2208 S, \u2200t \u2208 [T ],\nwhere for simplicity we have combined coefficients of z(t)h in the objective of (IP) to e\u0302 (t) h . The primal-dual algorithm is summarized in Algorithm 1. It alternates between updating the primal and the dual variables. The key is to observe that given dual variables, the primal problem (inner minimization) can be decomposed for each tree in the ensemble and solved in parallel as shortest path problems due to Lemma 3.1. (See also Appendix). The primal variables wk,i can be solved in closed form: simply compute \u00b5k,i = \u03bbck/N \u2212 \u2211 t\u2208Tk,i \u03b2 (t) k,i, where Tk,i is the set of trees in which example i encounters feature k. So wk,i should be set to 0 if \u00b5k,i > 0 and wk,i = 1 if \u00b5k,i < 0.\nNote that our prediction rule aggregates the leaf distributions from all trees instead of just their predicted labels. In the case where the leaves are pure (each leaf contains only one class of examples), this prediction rule coincides with the majority vote rule commonly used in random forests. Whenever the leaves contain mixed classes, this rule takes into account the prediction confidence of each tree in contrast to majority voting. Empirically, this rule consistently gives lower prediction error than majority voting with pruned trees."}, {"heading": "5 Experiments", "text": "We test our pruning algorithm BUDGETPRUNE on four benchmark datasets used for prediction-time budget algorithms. The first two datasets have unknown feature acquisition costs so we assign costs to be 1 for all features; the aim is to show that BUDGETPRUNE successfully selects a sparse subset of features on average to classify each example with high accuracy. 4 The last two datasets have real feature acquisition costs measured in terms of CPU time. BUDGETPRUNE achieves high prediction accuracy spending much less CPU time in feature acquisition.\nFor each dataset we first train a RF and apply BUDGETPRUNE on it using different \u03bb\u2019s to obtain various points on the accuracy-cost tradeoff curve. We use in-bag data to estimate error probability at each node and the validation data for the feature cost variables wk,i\u2019s. We implement BUDGETPRUNE using CPLEX [1] network flow solver for the primal update step. The running time is significantly reduced (from hours down to minutes) compared to directly solving the LP relaxation of (IP) using standard solvers such as Gurobi [10]. Futhermore, the standard solvers simply break trying to solve the larger experiments whereas BUDGETPRUNE handles them with ease. We run the experiments for 10 times and report the means and standard deviations.\nCompeting methods: We compare against four other approaches. (i) BUDGETRF[17]: the recursive node splitting process for each tree is stopped as soon as node impu-\n4In contrast to traditional sparse feature selection, our algorithm allows adaptivity, meaning different examples use different subsets of features.\nrity (entropy or Pairs) falls below a threshold. The threshold is a measure of impurity tolerated in the leaf nodes. This can be considered as a naive pruning method as it reduces feature acquisition cost while maintaining low impurity in the leaves.\n(ii) CostComplexity Pruning (CCP) [4]: it iteratively prunes subtrees such that the resulting tree has low error and small size. We perform CCP on individual trees to different levels to obtain various points on the accuracy-cost tradeoff curve. CCP does not take into account feature costs. (iii) GREEDYPRUNE: is a greedy global feature pruning strategy that we propose; at each iteration it attempts to remove all nodes corresponding to one feature from the RF such that the resulting\npruned RF has the lowest training error and average feature cost. The process terminates in at most K iterations, where K is the number of features. The idea is to reduce feature costs by successively removing features that result in large cost reduction yet small accuracy loss. We also compare against the state-of-the-art methods in budgeted learning (iv) GREEDYMISER [27]: it is a modification of gradient boosted regression tree [8] to incorporate feature cost. Specifically, each weak learner (a low-depth decision tree) is built to minimize squared loss with respect to current gradient at the training examples plus feature acquisition cost. To build each weak learner the feature costs are set to zero for those features already used in previous weak learners. Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots. Since only the feature acquisition costs are standardized, for fair comparison we do not include the computation cost term in the objective of (IP) and focus instead on feature acquisition costs.\nMiniBooNE Particle Identification and Forest Covertype Datasets:[7] Feature costs are uniform in both datasets. Our base RF consists of 40 trees using entropy split criteria and choosing from the full set of features at each split. As shown in (a) and (b) of Figure 2, BUDGETPRUNE (in red) achieves the best accuracy-cost tradeoff. The advantage of BUDGETPRUNE is particularly large in (b). GREEDYMISER has lower accuracy in the high budget region compared to BUDGETPRUNE in (a) and significantly lower accuracy in (b). The gap between BUDGETPRUNE and other pruning methods is small in (a) but much larger in (b). This indicates large gains from globally encouraging feature sharing in the case of (b) compared to (a). In both datasets, BUDGETPRUNE successfully prunes away large number of features while maintaining high accuracy. For example in (a), using only 18 unique features on average instead of 40, we can get essentially the same accuracy as the original RF.\nYahoo! Learning to Rank:[6] This ranking dataset consists of 473134 web documents and 19944 queries. Each example in the dataset contains features of a query-document pair together with the\nrelevance rank of the document to the query. There are 141397/146769/184968 examples in the training/validation/test sets. There are 519 features for each example; each feature is associated with an acquisition cost in the set {1, 5, 20, 50, 100, 150, 200}, which represents the units of CPU time required to extract the feature and is provided by a Yahoo! employee. The labels are binarized so that the document is either relevant or not relevant to the query. The task is to learn a model that takes a new query and its associated set of documents to produce an accurate ranking using as little feature cost as possible. As in [17], we use the Average Precision@5 as the performance metric, which gives a high reward for ranking the relevant documents on top. Our base RF consists of 140 trees using cost weighted entropy split criteria as in [17] and choosing from a random subset of 400 features at each split. As shown in (c) of Figure 2, BUDGETPRUNE achieves similar ranking accuracy as GREEDYMISER using only 30% of its cost.\nScene15 [13]: This scene recognition dataset contains 4485 images from 15 scene classes (labels). Following [27] we divide it into 1500/300/2685 examples for training/validation/test sets. We use a diverse set of visual descriptors and object detectors from the Object Bank [14]. We treat each individual detector as an independent descriptor so we have a total of 184 visual descriptors. The acquisition costs of these visual descriptors range from 0.0374 to 9.2820. For each descriptor we train 15 one-vs-rest kernel SVMs and use the output (margins) as features. Once any feature corresponding to a visual descriptor is used for a test example, an acquisition cost of the visual descriptor is incurred and subsequent usage of features from the same group is free for the test example. Our base RF consists of 500 trees using entropy split criteria and choosing from a random subset of 20 features at each split. As shown in (d) of Figure 2, BUDGETPRUNE and GREEDYPRUNE significantly outperform other competing methods. BUDGETPRUNE has the same accuracy at the cost of 9 as at the full cost of 32. BUDGETPRUNE and GREEDYPRUNE perform similarly, indicating the greedy approach happen to solve the global optimization in this particular initial RF."}, {"heading": "5.1 Discussion & Concluding Comments", "text": "We have empirically evaluated several resource constrained learning algorithms including BUDGETPRUNE and its variations on benchmarked datasets here and in the Appendix. We highlight key features of our approach below. (i) STATE-OF-THE-ART METHODS. Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets. GREEDYMISER requires building class-specific ensembles and tends to perform poorly and is increasingly difficult to tune in multi-class settings. RF, by its nature, can handle multi-class settings efficiently. On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value. This is a fundamentally combinatorial problem that is known to be NP hard [5, 26] and thus requires a number of relaxations and heuristics with no guarantees on performance. In contrast our pruning strategy is initialized to realize good performance (RF initialization) and we are able to globally optimize cost-accuracy objective. (ii) VARIATIONS ON PRUNING. By explicitly modeling feature costs, BUDGETPRUNE outperforms other pruning methods such as early stopping of BUDGETRF and CCP that do not consider costs. GREEDYPRUNE performs well validating our intuition (see Table. 1) that pruning sparsely occurring feature nodes utilized by large fraction of examples can improve test-time cost-accuracy tradeoff. Nevertheless, the BUDGETPRUNE outperforms GREEDYPRUNE, which is indicative of the fact that apart from obvious high-budget regimes, node-pruning must account for how removal of one node may have an adverse impact on another downstream one. (iii) SENSITIVITY TO IMPURITY, FEATURE COSTS, & OTHER INPUTS. We explore these issues in Appendix. We experiment BUDGETPRUNE with different impurity functions such as entropy and Pairs [17] criteria. Pairs-impurity tends to build RFs with lower cost but also lower accuracy compared to entropy and so has poorer performance. We also explored how non-uniform costs can impact cost-accuracy tradeoff. An elegant approach has been suggested by [2], who propose an adversarial feature cost proportional to feature utility value. We find that BUDGETPRUNE is robust with such costs. Other RF parameters including number of trees and feature subset size at each split do impact cost-accuracy tradeoff in obvious ways with more trees and moderate feature subset size improving prediction accuracy while incurring higher cost.\nTo conclude, our proposed formulation possesses 1) elegant theoretical properties, 2) an algorithm scalable to large problems and 3) superior empirical performance.\nAcknowledgment We thank Dr Kilian Weinberger for helpful discussions and Dr David Castanon for the insights on the primal dual algorithm."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 A Naive Pruning Formulation", "text": "The nice property of totally unimodular constraints in Theorem 3.2 is due to our specific formulation. Here we present an alternative integer program formulation and show its deficiency. Recall we defined the following node variables\nzh = { 1 if node h is a leaf in the pruned tree, 0 otherwise.\nand indicator variables of feature usage:\nwk,i = { 1 if feature k is used by x(i) in any Tt, t = 1, . . . , T 0 otherwise.\nFirst, note that if zh = 1 for some node h, then the examples that are routed to h must have used all the features in the predecessor nodes p(h), excluding h. We use k \u223c p(h) to denote feature k is used in any predecessor of h, excluding h. Then for each feature k and example i, we must have wk,i \u2265 zh for all nodes h such that i \u2208 Sh and k \u223c p(h). Combining these constraints with the pruning constraints we formulate pruning as a 0-1 integer program for an individual tree:\nmin zh\u2208{0,1} wk,i\u2208{0,1}\n1 N \u2211 h\u2208N ehzh + \u03bb K\u2211 k=1 ck( 1 N N\u2211 i=1 wk,i)\ns.t. zh + \u2211 u\u2208p(h) zu = 1 \u2200h \u2208 T\u0303 ,\nwk,i \u2265 zh \u2200h : i \u2208 Sh \u2227 k \u223c p(h), \u2200k \u2208 [K],\u2200i \u2208 S.\nTo solve the integer program, a common heuristic is to solve its linear program relaxation. Unfortunately, the constraint set in the above formulation has fractional extreme points, leading to possibly fractional solutions to the relaxed problem. It is not clear how to perform rounding to obtain good prunings. Consider the first tree in Figure 1. Feature 1 is used at the root node and feature 2 is used at node 3. There are 7 variables (assuming there is only one example and it goes to leaf 4): z1, z2, z3, z4, z5, w1,1, w2,1. The LP relaxed constraints are:\nz1 + z3 + z4 = 1, z1 + z3 + z5 = 1, z1 + z2 = 1,\nw1,1 \u2265 z4, w1,1 \u2265 z3, w2,1 \u2265 z4, 0 \u2264 z \u2264 1.\nThe following is a basic feasible solution:\nz1 = 0, z2 = 1, z3 = z4 = z5 = 0.5, w1,1 = w2,1 = 0.5,\nbecause the following set of 7 constraints are active:\nz1 + z3 + z4 = 1, z1 + z3 + z5 = 1,\nw1,1 \u2265 z4, w1,1 \u2265 z3, w2,1 \u2265 z4, z1 = 0, z2 = 1.\nEven if we were to interpret the fractional solution of zh as probabilities of h being a leaf node, we see an issue with this formulation: the example has 0.5 probability of stopping at node 3 or 4 (z3 = z4 = 0.5). In both cases, feature 1 at the root node has to be used, however w1,1 = 0.5 indicates that it is only being used half of the times. This solution is not a feasible pruning and fails to capture the cost of the pruning.\nAttempting to use an LP relaxation of this formulation fails to capture the desired behavior of the integer program. In the main paper we propose a better integer program formulation and show that solving the LP relaxation yields the optimal solution to the integer program."}, {"heading": "6.2 Transformation to Network Matrices and Shortest Path Problems", "text": "To illustrate the transformation to network matrix in Lemma 3.1, we provide the following illustration in Figure 1. Note in the main paper we have shown the example of the first tree. For simplicity we consider only one example being routed to nodes 4 and 11 respectively on the two trees. The equality constraints in (IP2) can be separated based on the trees and put in matrix form:\n z1 z2 z3 z4 z5 w (1) 1,1 w (1) 2,1\nr1 1 1 0 0 0 0 0 r2 1 0 1 1 0 0 0 r3 1 0 1 0 1 0 0 r4 1 0 1 0 0 0 1 r5 1 0 0 0 0 1 0 , for tree 1 and  z6 z7 z8 z9 z10 z11 z12 w (2) 2,1 w (2) 3,1\nr1 1 1 1 0 0 0 0 0 0 r2 1 1 0 1 0 0 0 0 0 r3 1 0 0 0 1 1 0 0 0 r4 1 0 0 0 1 0 1 0 0 r5 1 0 0 0 1 0 0 0 1 r6 1 0 0 0 0 0 0 1 0 , for tree 2. Through row operations they can be turned into network matrices, where there is exactly two non-zeros in each column, a 1 and a \u22121.\n z1 z2 z3 z4 z5 w (1) 1,1 w (1) 2,1\n\u2212r1 \u22121 \u22121 0 0 0 0 0 r1\u2212r2 0 1 \u22121 \u22121 0 0 0 r2\u2212r3 0 0 0 1 \u22121 0 0 r3\u2212r4 0 0 0 0 1 0 \u22121 r4\u2212r5 0 0 1 0 0 \u22121 1 r5 1 0 0 0 0 1 0 , for tree 1 and\n z6 z7 z8 z9 z10 z11 z12 w (2) 2,1 w (2) 3,1\n\u2212r1 \u22121 \u22121 \u22121 0 0 0 0 0 0 r1\u2212r2 0 0 1 \u22121 0 0 0 0 0 r2\u2212r3 0 1 0 1 \u22121 \u22121 0 0 0 r3\u2212r4 0 0 0 0 0 1 \u22121 0 0 r4\u2212r5 0 0 0 0 0 0 1 0 \u22121 r5\u2212r6 0 0 0 0 1 0 0 \u22121 1 r6 1 0 0 0 0 0 0 1 0  for tree 2. Note the above transformation to network matrices can always be done as long as the leaf nodes are arranged in a pre-order fashion.\nIn the primal-dual algorithm, the inner minimization can be decomposed to shortest path problems corresponding to individual trees. Figure 3 illustrates such a construction based on the network matrices shown above. The nodes in the graphs correspond to rows in the network matrices and the arcs correspond to the columns, which are the primal variables zh, w (t) k,i\u2019s. There is a cost associated with each arc in the objective of the minimization problem. The task is to find a path from the first node (source) to the last node (sink) such that the sum of arc costs is minimized. Note each path from source to sink corresponds to a feasible pruning. For example, in (a) of Figure 3, consider the path of 1-2-5-6, the active arcs are z2, z3 and w (1) 1,1, Setting these variables to 1 and others to 0, we see that it corresponds to pruning Tree 1 at node 3 in Figure 1. (Note the nodes in Figure 3 and Figure 1 are not to be confused - they do not have a relation with each other. )"}, {"heading": "6.3 Proof of Theorem 3.2", "text": "Denote the equality constraints of (IP) with index set J1. They can be divided into each tree. Each constraint matrix in J1 associated with a tree can be turned into a network matrix according to Lemma\n3.1. Stacking these matrices leads to a larger network matrix. Denote the w(t)k,i \u2264 wk,i constraints with index set J2. Consider the constraint matrix for J2. Each w (t) k,i only appears once in J2, which means the column corresponding to w(t)k,i has only one element equal to 1 and the rest equal to 0. If we arrange the constraints in J2 such that for any given k, i w (t) k,i \u2264 wk,i are put together for t \u2208 [T ], the constraint matrix for J2 has interval structure such that the non-zeros in each column appear consecutively. Finally, putting the network matrix from J1 and the matrix from J2 together. Assign J1 and the odd rows of J2 to the first partition Q1 and assign the even rows of J2 to the second partition Q2. Note the upper bound constraints on the variables can be ignored as this is an minimization problem. We conclude that the constraint matrix of (IP) is totally unimodular according to Theorem 2.7, Part 3 of [18] with partition Q1 and Q2. By Proposition 2.1 and 2.2, Part 3 of [18] we can conclude the proof."}, {"heading": "6.4 Additional Details of Experiments", "text": "In this section we provide additional details of the experiment setup and explore how some parameter choices may affect BUDGETPRUNE.\nAdditional details of datasets The MiniBooNE data set is a binary classification task to distinguish electron neutrinos from muon neutrinos. There are 45523/19510/65031 examples in\ntraining/validation/test sets. Each example has 50 features, each with unit cost. The Forest data set contains cartographic variables to predict 7 forest cover types. There are 36603/15688/58101 examples in training/validation/test sets. Each example has 54 features, each with unit cost. We use 1000 trees for GREEDYMISER and search over learning rates in [10\u22125, 102] for MiniBooNE and Forest. The Yahoo and Scene15 datasets have actual feature acquisition costs in terms of CPU time. We use 3000 trees for GREEDYMISER and search over learning rates in [10\u22125, 1]. We use the multi-class logistic loss for Scene15 and the squared loss for other datasets in GREEDYMISER. For the Scene15 dataset, we use a diverse set of visual discriptors varying in computation time: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric color and 177 object detectors from the Object Bank [14]. We treat each individual detector as an independent descriptor so we have 184 different visual descriptors in total. The acquisition costs of these visual descriptors range from 0.0374 to 9.2820. For each descriptor we train 15 one-vs-rest kernel SVMs and use the output (margins) as features. The best classifier based on individual descriptors achieves an accuracy of 77.8%. Note the features are grouped based on the visual descriptors. Once any feature corresponding to a visual descriptor is used for a test example, an acquisition cost of the visual descriptor is incurred and subsequent usage of features from the same group is free for the test example.\nNext, we perform additional experiments to evaluate BUDGETPRUNE with different costs, input RFs.\nNon-uniform cost on MiniBooNE We observe that CCP performs similarly to BUDGETPRUNE on MiniBooNE when the costs are uniform in the case of entropy splitting criteria, indicating little gain from global optimization with respect to feature usage. We suspect that uniform feature costs work in favor of CCP because there\u2019s no loss in treating each feature equally. To confirm this intuition we assign the features non-uniform costs and re-run prunings on the same RF. We first normalize the data so that the data vectors corresponding to the features have the same l-2 norm. We then train a linear SVM on it and obtain the weight vector corresponding to the learned hyperplane. We around the absolute values of the weights and make them the costs for the features. Intuitively the feature with higher weight tends to be more relevant for the classification task so we assign it a higher acquisition cost. The resulting costs lie in the range of [1, 40] and we normalize them so that the sum of all feature costs is 50 - the number of features. We plot BUDGETPRUNE and CCP for uniform cost as well as the non-uniform cost described above in Figure 5. BUDGETPRUNE still achieves similar performance as uniform cost while CCP performance drops significantly with non-uniform feature cost. This shows again the importance of taking into account feature costs in the pruning process.\nEntropy Vs Pairs How does BUDGETPRUNE depend on the splitting criteria used in the underlying random forest? On two data sets we build RFs using the popular entropy splitting criteria and the mini-max Pairs criteria used in [17] and the results are shown in Figure 6. We observe that entropy splitting criteria lead to RFs with higher accuracy while the Pairs criteria lead to RFs with lower cost. This is expected as using Pairs biases to more balanced splits and thus provably low cost [17]. In (a) of Figure 6 we observe that as more of the RF is pruned away BUDGETPRUNE and CCP results for\nentropy and Pairs coincide. This suggests that the two criteria actually lead to similar tree structures in the initial tree-building process. However, as the trees are built deeper their structures diverge. Plot (b) in Figure 6 shows that pruning based on the RFs from the Pairs criteria can achieve higher accuracy in the low cost region. But if high accuracy in the high cost region is desirable then the entropy criteria should be used.\nSize of random feature subset at each split At each split in RF building, it is possible to restrict the choice of splitting feature to be among a random subset of all features. Such restriction tends to further reduce correlation among trees and gain prediction accuracy. The drawback is that test examples tend to encounter a diverse set of features, increasing feature acquisition cost. For illustration purpose, we plot various pruning results on Scene15 dataset for feature subset sizes k = 20 and k = 120 in Figure 7. The initial RF has higher accuracy and higher cost for k = 20 as expected. BUDGETPRUNE achieves slightly better accuracy in k = 20 than k = 120. Note also how GREEDYPRUNE performance drops significantly for k = 120 so it is not robust. In our main experiments k is chosen on validation data to achieve highest accuracy for the initial RF."}], "references": [{"title": "Sequential prediction for budgeted learning : Application to trigger design", "author": ["Djalel Benbouzid"], "venue": "Theses, Universite\u0301 Paris Sud - Paris XI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen"], "venue": "CRC press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1984}, {"title": "Decision trees for entity identification: Approximation algorithms and hardness results", "author": ["Venkatesan T. Chakaravarthy", "Vinayaka Pandit", "Sambuddha Roy", "Pranjal Awasthi", "Mukesh K. Mohania"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "editors", "author": ["O Chapelle", "Y Chang", "T Liu"], "venue": "Proceedings of the Yahoo! Learning to Rank Challenge, held at ICML 2010, Haifa, Israel, June 25, 2010", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "Advances in Neural Information Processing Systems ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Pruning of random forest classifiers: A survey and future directions", "author": ["V.Y. Kulkarni", "P.K. Sinha"], "venue": "In Data Science Engineering (ICDSE),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Feature-cost sensitive learning with submodular trees of classifiers", "author": ["M Kusner", "W Chen", "Q Zhou", "E Zhixiang", "K Weinberger", "Y Chen"], "venue": "AAAI Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pages 2169\u20132178", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Object Bank: A High-Level Image Representation for Scene Classification and Semantic Feature Sparsification", "author": ["Li-Jia Li", "Hao Su", "Eric P. Xing", "Li Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A dynamic programming based pruning method for decision trees", "author": ["Xiao-Bai Li", "James Sweigart", "James Teng", "Joan Donohue", "Lori Thombs"], "venue": "INFORMS J. on Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Fast margin-based cost-sensitive classification", "author": ["F Nan", "J Wang", "K Trapeznikov", "V Saligrama"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature-budgeted random forest", "author": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Integer and Combinatorial Optimization", "author": ["George L. Nemhauser", "Laurence A. Wolsey"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["George L Nemhauser", "Laurence A Wolsey", "Marshall L Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1978}, {"title": "An optimal constrained pruning strategy for decision trees", "author": ["Hanif D. Sherali", "Antoine G. Hobeika", "Chawalit Jeenanunta"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Supervised sequential classification under budget constraints", "author": ["K Trapeznikov", "V Saligrama"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 581\u2013589", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Model selection by linear programming", "author": ["J. Wang", "T. Bolukbasi", "K Trapeznikov", "V Saligrama"], "venue": "European Conference on Computer Vision, pages 647\u2013662", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An LP for sequential learning under budgets", "author": ["J Wang", "K Trapeznikov", "V Saligrama"], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, AISTATS 2014", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "An lp for sequential learning under budgets", "author": ["J Wang", "K Trapeznikov", "V Saligrama"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient learning by directed acyclic graph for resource constrained prediction", "author": ["Joseph Wang", "Kirill Trapeznikov", "Venkatesh Saligrama"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Cost-sensitive tree of classifiers", "author": ["Z Xu", "M Kusner", "M Chen", "K. Q Weinberger"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Zhixiang Eddie Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Decision tree pruning via integer programming", "author": ["Yi Zhang", "Huang Huei-chuen"], "venue": "Working paper,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}], "referenceMentions": [{"referenceID": 22, "context": "Many modern classification systems, including internet applications (such as web-search engines, recommendation systems, and spam filtering) and security & surveillance applications (such as widearea surveillance and classification on large video corpora), face the challenge of prediction-time budget constraints [26].", "startOffset": 314, "endOffset": 318}, {"referenceID": 13, "context": "In the first stage, we train a random forest (RF) of trees using an impurity function such as entropy or more specialized cost-adaptive impurity [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 19, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 18, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 20, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 6, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 23, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 17, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 21, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 8, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 1, "context": "Our work is based on RF classifiers [3].", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Traditionally, feature cost is not incorporated when constructing RFs, however recent work has involved approximation of budget constraints to learn budgeted RFs [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "The tree-growing algorithm in [17] does not take feature re-use into account.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[15], CCP has undesirable \u201cjumps\" in the sequence of pruned tree sizes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28, 20] proposed to solve the pruning problem as a 0-1 integer program; again, their formulations do not account for feature costs that we focus on in this paper.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[28, 20] proposed to solve the pruning problem as a 0-1 integer program; again, their formulations do not account for feature costs that we focus on in this paper.", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "Kulkarni and Sinha [11] provide a survey of methods to prune RFs in order to reduce ensemble size.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "The rest of proof follows directly from the construction in Proposition 3 of [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "The main idea is to show the constraints are still totally unimodular even after adding the coupling constraints and the LP relaxed polyhedron has only integral extreme points [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "(i) BUDGETRF[17]: the recursive node splitting process for each tree is stopped as soon as node impu-", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "GREEDYMISER dominates ASTC [12], CSTC [26] and DAG [25] significantly on all datasets.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "GREEDYMISER dominates ASTC [12], CSTC [26] and DAG [25] significantly on all datasets.", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "GREEDYMISER dominates ASTC [12], CSTC [26] and DAG [25] significantly on all datasets.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "(ii) CostComplexity Pruning (CCP) [4]: it iteratively prunes subtrees such that the resulting tree has low error and small size.", "startOffset": 34, "endOffset": 37}, {"referenceID": 23, "context": "We also compare against the state-of-the-art methods in budgeted learning (iv) GREEDYMISER [27]: it is a modification of gradient boosted regression tree [8] to incorporate feature cost.", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "We also compare against the state-of-the-art methods in budgeted learning (iv) GREEDYMISER [27]: it is a modification of gradient boosted regression tree [8] to incorporate feature cost.", "startOffset": 154, "endOffset": 157}, {"referenceID": 8, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 180, "endOffset": 188}, {"referenceID": 13, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 180, "endOffset": 188}, {"referenceID": 4, "context": "Yahoo! Learning to Rank:[6] This ranking dataset consists of 473134 web documents and 19944 queries.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "As in [17], we use the Average Precision@5 as the performance metric, which gives a high reward for ranking the relevant documents on top.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Our base RF consists of 140 trees using cost weighted entropy split criteria as in [17] and choosing from a random subset of 400 features at each split.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Scene15 [13]: This scene recognition dataset contains 4485 images from 15 scene classes (labels).", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "Following [27] we divide it into 1500/300/2685 examples for training/validation/test sets.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "We use a diverse set of visual descriptors and object detectors from the Object Bank [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets.", "startOffset": 134, "endOffset": 146}, {"referenceID": 22, "context": "Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets.", "startOffset": 134, "endOffset": 146}, {"referenceID": 21, "context": "Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets.", "startOffset": 134, "endOffset": 146}, {"referenceID": 8, "context": "On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value.", "startOffset": 44, "endOffset": 56}, {"referenceID": 21, "context": "On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value.", "startOffset": 44, "endOffset": 56}, {"referenceID": 22, "context": "On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value.", "startOffset": 44, "endOffset": 56}, {"referenceID": 3, "context": "This is a fundamentally combinatorial problem that is known to be NP hard [5, 26] and thus requires a number of relaxations and heuristics with no guarantees on performance.", "startOffset": 74, "endOffset": 81}, {"referenceID": 22, "context": "This is a fundamentally combinatorial problem that is known to be NP hard [5, 26] and thus requires a number of relaxations and heuristics with no guarantees on performance.", "startOffset": 74, "endOffset": 81}, {"referenceID": 13, "context": "We experiment BUDGETPRUNE with different impurity functions such as entropy and Pairs [17] criteria.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "An elegant approach has been suggested by [2], who propose an adversarial feature cost proportional to feature utility value.", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "7, Part 3 of [18] with partition Q1 and Q2.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "2, Part 3 of [18] we can conclude the proof.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "For the Scene15 dataset, we use a diverse set of visual discriptors varying in computation time: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric color and 177 object detectors from the Object Bank [14].", "startOffset": 253, "endOffset": 257}, {"referenceID": 13, "context": "Entropy Vs Pairs How does BUDGETPRUNE depend on the splitting criteria used in the underlying random forest? On two data sets we build RFs using the popular entropy splitting criteria and the mini-max Pairs criteria used in [17] and the results are shown in Figure 6.", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "This is expected as using Pairs biases to more balanced splits and thus provably low cost [17].", "startOffset": 90, "endOffset": 94}], "year": 2016, "abstractText": "We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.", "creator": "LaTeX with hyperref package"}}}