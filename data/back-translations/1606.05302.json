{"id": "1606.05302", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Generalized Direct Change Estimation in Ising Model Structure", "abstract": "We look at the problem of estimating the change in the dependency structure between two $p $-dimensional Ising models, based on $n _ 1 $and $n _ 2 $samples from the models. It is assumed that the change is structured in such a way that it can be characterized by a suitable (atomic) standard. We present and analyze a norm-regulated estimator for the direct estimation of the structural change without having to estimate the structures of the individual Ising models. The estimator can work with any standard and can be generalized to other graphic models under mild assumptions. We show that only a sample quantity, say $n _ 2 $, has to meet the requirements for sample complexity in order for the estimator to be able to work, and the estimation error decreases as $\\ frac} {sqrt {\\ min (n _ 1, n _ 2)}, where $c $depends sparingly on the results that can be presented with the applied unit of measurement (1 = O).", "histories": [["v1", "Thu, 16 Jun 2016 18:21:45 GMT  (437kb,D)", "http://arxiv.org/abs/1606.05302v1", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.TH", "authors": ["farideh fazayeli", "arindam banerjee"], "accepted": true, "id": "1606.05302"}, "pdf": {"name": "1606.05302.pdf", "metadata": {"source": "CRF", "title": "Generalized Direct Change Estimation in Ising Model Structure", "authors": ["Farideh Fazayeli", "Arindam Banerjee"], "emails": ["farideh@cs.umn.edu}", "banerjee@cs.umn.edu}"], "sections": [{"heading": null, "text": "min(n1,n2) ,\nwhere c depends on the Gaussian width of the unit norm ball. For example, for `1 norm applied to s-sparse change, the change can be accurately estimated with min(n1, n2) = O(s log p) which is sharper than an existing result n1 = O(s\n2 log p) and n2 = O(n21). Experimental results illustrating the effectiveness of the proposed estimator are presented."}, {"heading": "1 Introduction", "text": "Over the past decade, considerable progress has been made on estimating the statistical dependency structure of graphical models based on samples drawn from the model. In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].\nIn this paper, we consider Ising models and focus on the problem of estimating changes in Ising model structure: given two sets of samples Xn11 = {x1i } n1 i=1 and X n2 2 = {x2i } n2 i=1 respectively drawn from two p-dimensional Ising models with true parameters \u03b8\u22171 and \u03b8 \u2217 2 , where \u03b8 \u2217 1 , \u03b8 \u2217 2 \u2208 Rp\u00d7p, the goal is to estimate the change \u03b4\u03b8\u2217 = (\u03b8\u22171\u2212\u03b8\u22172). In particular, we focus on the situation when the change \u03b4\u03b8\u2217 has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18]. However, the individual model parameters \u03b8\u22171 , \u03b8 \u2217 2 need not have any specific structure, and they may both correspond to dense matrices. The goal is to get an estimate \u03b4\u03b8\u0302 of the change \u03b4\u03b8\u2217 such that the estimation error \u2206 = (\u03b4\u03b8\u0302 \u2212 \u03b4\u03b8\u2217) is small. Such change estimation has potentially wide range of applications including identifying the changes in the neural connectivity networks, the difference between plant trait interactions at different climate conditions, and the changes in the stock market dependency structures.\nOne can consider two broad approaches for solving such change estimation problems: (i) indirect change estimation, where we estimate \u03b8\u03021 and \u03b8\u03022 from two sets of samples separately and obtain \u03b4\u03b8\u0302 = (\u03b8\u03021 \u2212 \u03b8\u03022), or (ii) direct change estimation, where we directly estimate \u03b4\u03b8\u0302 using the two sets of samples, without estimating \u03b81 and \u03b82 individually. In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is. For example, if both \u03b8\u22171 and \u03b8 \u2217 2\nar X\niv :1\n60 6.\n05 30\n2v 1\n[ m\nat h.\nST ]\n1 6\nJu n\nare sparse and the samples n1, n2 are sufficient to estimate them accurately [22], indirect estimation of \u03b4\u03b8\u0302 should be accurate. However, if the individual parameters \u03b8\u22171 and \u03b8 \u2217 2 are somewhat dense, and the change \u03b4\u03b8\n\u2217 has considerably more structure, such as block sparsity (only a small block has changed) or node perturbation sparsity (only edges from a few nodes have changed) [18], direct estimation may be considerably more efficient both in terms of the number of samples required as well as the computation time.\nRelated Work: In recent work, Liu et al. [13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31]. They focused on the special case of L1 norm, i.e., \u03b4\u03b8\u2217 \u2208 Rp2 is sparse, and provided non-asymptotic error bounds for the estimator along with a sample complexity of n1 = O(s\n2 log p) and n2 = O(n21) for an unbounded density ratio model, where s is the number of the changed edges with p being the number of variables. Liu et al. [14] improved the sample complexity to min(n1, n2) = O(s2 log p) when a bounded density ratio model is assumed. Zhao et al. [36] considered estimating direct sparse changes in Gaussian graphical models (GGMs). Their estimator is specific to GGMs and can not be applied to Ising models.\nOur Contributions: We consider general structured direct change estimation, while allowing the change to have any structure which can be captured by a suitable (atomic) norm R(\u00b7). Our work is a considerable generalization of the existing literature which can only handle sparse changes, captured by the L1 norm. In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19]. Interestingly, for the unbounded density ratio model, our analysis yields sharper bounds for the special case of `1 norm, considered by Liu et al. [13]. In particular, when \u03b4\u03b8\u2217 is sparse and our estimator is run with L1 norm, we get a sample complexity of n1 = n2 = O(s log p) which is sharper than n1 = O(s 2 log p) and n2 = O(n21) in [13].\nThe regularized estimator we analyze is broadly a Lasso-type estimator, with key important differences: the objective does not decompose additively over the samples, and the objective depends on samples from two distributions. The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models. Our analysis is quite different from the existing literature in change estimation. Liu et. [13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm. Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].\nThe rest of the paper is organized as follows. In Section 2, we introduce the direct change estimator based on the ratio of the probability density of the Ising models. In Section 3, we establish statistical consistency of the direct change estimator, and conclude in Section 5."}, {"heading": "2 Generalized Direct Change Estimation", "text": "We consider the following optimization problem\nargmin \u03b4\u03b8\nL(\u03b4\u03b8;Xn11 ,X n2 2 ) + \u03bbn1,n2R(\u03b4\u03b8), (1)\nwhere Xn11 = {x1i } n1 i=1 and X n2 2 = {x2i } n2 i=1 are two sets of i.i.d binary samples drawn from from Ising graphical models with parameter \u03b8\u22171 and \u03b8 \u2217 2 , respectively, each x 1 i and x 2 i are p\u2212dimensional vectors, and n1, n2 are the respective sample sizes.\nIn this Section, we first give a brief background on Ising model selection. Then, we explain how to develop the loss function L(\u03b4\u03b8;Xn11 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8)."}, {"heading": "2.1 Ising Model", "text": "Let X = (X1, X2, \u00b7 \u00b7 \u00b7 , Xp) denote a random vector in which each variable Xs \u2208 {\u22121, 1}. Let G = (V,E) be an undirected graph with vertex set V = {1, \u00b7 \u00b7 \u00b7 , p} and edge set E whose elements are unordered pairs of distinct vertices. The pairwise Ising Markov random field associated with the graph G over the random vector X is\nP (X = x|\u03b8\u2217) = 1 Z(\u03b8\u2217) exp{ \u2211 s,t\u2208E \u03b8\u2217s,txsxt} (2)\n= 1\nZ(\u03b8\u2217) exp{\u3008\u03b8\u2217, T (x)\u3009} (3)\n= 1\nZ(\u0398\u2217) exp{xT\u0398\u2217x} (4)\nwhere T (x) = {xsxt}ps,t=1 is a vector of size m = p2, \u03b8\u2217 = {\u03b8\u2217s,t} p s,t=1 \u2208 Rm and \u3008., .\u3009 is the inner product operator, and \u0398\u2217 \u2208 Rp\u00d7p where \u0398\u2217s,t = \u03b8\u2217s,t. Note that basic Ising models also have non-interacting terms like \u03b1sxs and we are assuming these terms are zero, and they do not affect the dependency structure.\nThe parameter \u03b8\u2217 associated with the structure of the graphG reveals the statistical conditional independence structure among the variables i.e., if \u03b8\u2217s,t = 0, then feature Xs is conditionally independent of Xt given all other variables and there is no edge in the graph G.\nThe partition function, Z(\u03b8\u2217), plays the role of a normalizing constant, ensuring that the probabilities add up to one which is defined as\nZ(\u03b8\u2217) = \u2211 x\u2208X exp{\u3008\u03b8\u2217, T (x)\u3009} = exp{\u03a8(\u03b8\u2217)}, (5)\nwhere X be the set of all possible configurations of X ."}, {"heading": "2.2 Loss Function", "text": "Here, we build the loss function based on equation (3). Similarly, one can rewrite the loss function based on (4) if the regularization function is over matrices. Consider two Ising models with parameters \u03b8\u22171 \u2208 Rp 2 and \u03b8\u22172 \u2208 Rp 2\n. Following Liu et. al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows\nr(X = x|\u03b4\u03b8) = p(X = x|\u03b81) p(X = x|\u03b82) = exp{\u3008T (x), \u03b81\u3009} exp{\u3008T (x), \u03b82\u3009}\ufe38 \ufe37\ufe37 \ufe38\nr\u2217(x|\u03b4\u03b8)\nZ(\u03b82) Z(\u03b81)\ufe38 \ufe37\ufe37 \ufe38 1/Z(\u03b4\u03b8) = exp{\u3008T (x), \u03b4\u03b8\u3009)} Z(\u03b4\u03b8) , (6)\nwhere the parameter \u03b4\u03b8 = \u03b81 \u2212 \u03b82 encodes the change between two graphical models \u03b81 and \u03b82.\nFirst, we show that Z(\u03b4\u03b8) = EX\u223cq[e\u3008T (X),\u03b4\u03b8\u3009]:\nZ(\u03b4\u03b8) = Z(\u03b81)\nZ(\u03b82) =\n1\nZ(\u03b82) \u2211 x\u2208X e\u3008T (x),\u03b81\u3009 = 1 Z(\u03b82) \u2211 x\u2208X e\u3008T (x),\u03b82\u3009 e\u3008T (x),\u03b81\u3009 e\u3008T (x),\u03b82\u3009\n= \u2211 x\u2208X e\u3008T (x),\u03b82\u3009\nZ(\u03b82)\ufe38 \ufe37\ufe37 \ufe38 p(x|\u03b82)\ne\u3008T (x),\u03b4\u03b8\u3009 = EX\u223cp(X|\u03b82)[e \u3008T (X),\u03b4\u03b8\u3009]. (7)\nNext, using the samples Xn22 from p(X|\u03b82), we estimate Z(\u03b4\u03b8) empirically as\nZ\u0302(\u03b4\u03b8) = 1\nn2 n2\u2211 i=1 exp{\u3008T (x2i ), \u03b4\u03b8\u3009}, (8)\nand the sample approximation of r(X|\u03b4\u03b8) is given as\nr\u0302(X = x|\u03b4\u03b8) = r \u2217(X = x|\u03b81) Z\u0302(\u03b4\u03b8) = exp{\u3008T (x), \u03b4\u03b8\u3009}\n1 n2 \u2211n2 i=1 exp{\u3008T (x2i ), \u03b4\u03b8\u3009} . (9)\nUsing the fact that r(X|\u03b4\u03b8\u2217)q(X|\u03b8\u22172) = p(X|\u03b8\u22171), we approximate r\u0302(X|\u03b4\u03b8), by minimizing the KL divergence,\nKL (p(X|\u03b8\u22171)\u2016r\u0302(X|\u03b4\u03b8)p(X|\u03b8\u22172)) = \u2211 x\u2208X p(x|\u03b8\u22171) log p(x|\u03b8\u22171) p(x|\u03b8\u22172)r\u0302(x|\u03b4\u03b8)\n= KL (p(X|\u03b8\u22171)\u2016p(X|\u03b8\u22172))\ufe38 \ufe37\ufe37 \ufe38 Constant \u2212EX\u223cp(X|\u03b8\u22171 ) [log r\u0302(X|\u03b4\u03b8)] (10)\nThus, using the samples Xn11 and X n2 2 , we define the empirical loss function\nL(\u03b4\u03b8;Xn11 ,X n2 2 ) = \u22121 n1 n1\u2211 i=1 log r\u0302(x1i |\u03b4\u03b8) = \u22121 n1 n1\u2211 i=1 \u3008T (x1i ), \u03b4\u03b8\u3009+ log 1 n2 n2\u2211 i=1\nexp{\u3008T (x2i ), \u03b4\u03b8\u3009}\ufe38 \ufe37\ufe37 \ufe38 \u03a8\u0302(\u03b4\u03b8)\n(11)\nRemark 1 Note that the loss function (11) does not additively decompose over the samples. The second term in (11) is the logarithm over sum of a function of samples."}, {"heading": "2.3 Optimization", "text": "The optimization problem (1) has a composite objective with a smooth convex term corresponding to the loss function (11) and a a potentially non-smooth convex term corresponding to the regularizer. In this section, we present an algorithm in the class of Fast Iterative Shrinkage-Thresholding Algorithms (FISTA) for efficiently solving the problem (1) [3]. For convenience, we refer the loss function L(\u03b4\u03b8;Xn11 ,X n2 2 ) as L(\u03b4\u03b8) and we drop the subscript {n1, n2} of \u03bbn1,n2 .\nOne of the most popular methods for composite objective functions is in the class of FISTA where at each iteration we linearize the smooth term and minimize the quadratic approximation of the form\nQL(\u03b4\u03b8, \u03b4\u03b8t) := L(\u03b4\u03b8) + \u3008\u03b4\u03b8 \u2212 \u03b4\u03b8t,\u2207L(\u03b4\u03b8t)\u3009+ L\n2 \u2016\u03b4\u03b8 \u2212 \u03b4\u03b8t\u201622 + \u03bbR(\u03b4\u03b8), (12)\nwhere L denotes the Lipschitz constant of the loss function L(\u03b4\u03b8). Ignoring constant terms in \u03b4\u03b8t, the unique minimizer of the above expression (12) can be written as\npL(\u03b4\u03b8t) = argmin \u03b4\u03b8 QL(\u03b4\u03b8, \u03b4\u03b8t) = argmin \u03b4\u03b8\n\u03bbR(\u03b4\u03b8) + L\n2 \u2225\u2225\u2225\u2225\u03b4\u03b8 \u2212 (\u03b4\u03b8t \u2212 1L\u2207L(\u03b4\u03b8t) )\u2225\u2225\u2225\u22252\n2\n= argmin \u03b4\u03b8\n\u03bb L R(\u03b4\u03b8) + 1 2 \u2225\u2225\u2225\u2225\u03b4\u03b8 \u2212 (\u03b4\u03b8t \u2212 1L\u2207L(\u03b4\u03b8t) )\u2225\u2225\u2225\u22252\n2\n. (13)\nIn fact, the updates of \u03b4\u03b8 is to compute certain proximal operators of the non-smooth term R(.). In general, the proximal operator proxh(x) of a closed proper convex function h : Rd 7\u2192 R \u222a {+\u221e} [21] is defined as\nproxh(x) = argmin u\n( h(u) + 1\n2 \u2016u\u2212 x\u201622\n) . (14)\nThus, the unique minimizer (13) correspond to prox \u03bb LR ( \u03b4\u03b8t \u2212 1L\u2207L(\u03b4\u03b8t) ) which has rate of convergence ofO(1/t) [20, 21].\nTo improve the rate of convergence, we adapt the idea of FISTA algorithm [3]. The main idea is to iteratively consider the proximal operator prox(.) at a specific linear combination of the previous two iterates {\u03b4\u03b8t, \u03b4\u03b8t\u22121}\n\u03bet+1 = \u03b4\u03b8t + \u03b1t+1 (\u03b4\u03b8t \u2212 \u03b4\u03b8t\u22121) , (15)\nAlgorithm 1 Generalized Direct Change Estimator Input: L0 > 0, Xn11 ,X n2 2\nStep 0. Set \u03be1 = \u03b4\u03b80, t = 1 Step t. (t \u2265 1) Find the smallest non-negative integers it such that with L\u0303 = 2itLt\u22121\nL (pL\u0303(\u03bet)) +R (pL\u0303(\u03bet)) \u2264 QL\u0303 (pL\u0303(\u03bet), \u03bet) . (17)\nSet Lt = 2itLt\u22121 and Compute\n\u03b4\u03b8t = prox \u03bb LR\n( \u03bet \u2212 1\nL \u2207L(\u03bet)\n) (18)\n\u03b2t+1 = 1 + \u221a 1 + 4\u03b22t 2\n(19)\n\u03bet+1 = \u03b4\u03b8t + ( \u03b2t \u2212 1 \u03b2t+1 ) (\u03b4\u03b8t \u2212 \u03b4\u03b8t\u22121) (20)\ninstead of just the previous iterate \u03b4\u03b8t. The choice of \u03b1t+1 follows Nesterov\u2019s accelerated gradient descent [20, 21] and is detailed in Algorithm 1. The iterative algorithm simply updates\n\u03b4\u03b8t+1 = prox \u03bb LR\n( \u03bet+1 \u2212 1\nL \u2207L(\u03bet+1)\n) . (16)\nThe algorithm has a rate of convergence of O(1/t2) [3]."}, {"heading": "2.4 Regularization Function", "text": "We assume that the optimal \u03b4\u03b8\u2217 is sparse or suitably \u2018structured\u2019 where such structure can be characterized by having a low value according to a suitable norm R(\u03b4\u03b8\u2217). In below, we provide a few examples of such a norm.\nL1 norm: One example for R(.) we will consider throughout the paper is the L1 norm regularization. We use L1 norm if only a few edges has changed (1st row in Figure 1). In particular, we consider R(\u03b4\u03b8) = \u2016\u03b4\u03b8\u20161 if number of non-zeros entries in \u03b4\u03b8\u2217 is s < p2. The prox \u03bb\nL\u2016.\u20161 (.) is given by the elementwise soft-thresholding operation [24] as[\nprox \u03bb L\u2016.\u20161 ] i (z) = sign(zi).max(0, zi \u2212 \u03bb L ). (21)\nGroup-sparse norm: Another popular example we consider is the group-sparse norm. We use group lasso norm if a group of edges has changed (2nd row in Figure 1). For some kinds of data, it is reasonable to assume that the variables can be clustered (or grouped) into types, which share similar connectivity or correlation patterns. Let G = {G1,G2, \u00b7 \u00b7 \u00b7 ,GNG} denote a collection of groups, which are subsets of variables. We assume that \u03b4\u0398\u2217(s, t) = 0 for any variable s \u2208 Gg and for any variable t \u2208 Gh. In the group sparse setting for any subset SG \u2286 {1, 2, \u00b7 \u00b7 \u00b7NG} with cardinality |SG | = sG , we assume that the parameter \u03b4\u0398\u2217 satisfies {\u03b4\u0398\u2217s,t = 0 : s, t \u2208 Gg & g 6\u2208 SG}. We will focus on the case when R(\u03b4\u0398) = \u2211NG g=1 \u2016\u03b4\u0398(s, t) : s, t \u2208 Gg\u2016F [15]. Let \u03b4\u0398Gg bd the sub-matrix of \u03b4\u0398 covering nodes in Gg . Proximal operator is given by the group specific soft-thresholding operation.[ prox \u03bb\nLR ] g (\u03b4\u0398) = max(\u2016\u03b4\u0398Gg\u2016F \u2212 \u03bbL , 0) \u2016\u03b4\u0398Gg\u2016F . (22)\nNode perturbation: Another example is the row-column overlap norm (RCON) [18] to capture perturbed nodes i.e., nodes that have a completely different connectivity pattern to other nodes among two networks (3rd row in Figure 1). A special case of RCON we are interested is \u2211p i=1 \u2016Vi\u2016q where \u03b4\u0398 = V + V T , and Vi is the i\u2212th column of matrix V . This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35]. Also, we can write problem (1) as a constrained optimization\nargmin \u03b4\u0398,V\nL(\u03b4\u0398;Xn11 ,X n2 2 ) + \u03bb1\u03b4\u0398\u20161 + \u03bbn1,n2 p\u2211 i=1 \u2016Vi\u2016q\ns.t. \u03b4\u0398 = V + V T , (23)\nand solve it by applying in-exact ADMM techniques [18]."}, {"heading": "3 Statistical Recovery for Generalized Direct Change Estimation", "text": "Our goal is to provide non-asymptotic bounds on \u2016\u2206\u20162 = \u2016\u03b4\u03b8\u2217 \u2212 \u03b4\u03b8\u0302\u20162 between the true parameter \u03b4\u03b8\u2217 and the minimizer \u03b4\u03b8\u0302 of (1). In this section, we describe various aspects of the problem, introducing notations along the way, and highlight our main result."}, {"heading": "3.1 Background and Assumption", "text": "Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.\nDefinition 1 For any set A \u2208 Rp, the Gaussian width of the set A is defined as:\nw(A) = Eg [ sup u\u2208A \u3008g, u\u3009 ] . (24)\nwhere the expectation is over g \u223c N(0, Ip\u00d7p), a vector of independent zero-mean unit-variance Gaussian random variable.\nThe Gaussian widthw(A) provides a geometric characterization of the size of the setA. Consider the Gaussian process {Zu} where the constituent Gaussian random variables Zu = \u3008u, g\u3009 are indexed by u \u2208 A, and g \u223c N(0, Ip\u00d7p). Then the Gaussian width w(A) can be viewed as the expectation of the supremum of the Gaussian process {Zu}. Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].\nThe Error Set: Consider solving the problem (1), under assumption \u03bbn1,n2 > \u03b2R\u2217 (\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )), where \u03b2 > 1 and R\u2217(.) is the dual norm of R(.). Banerjee et al. [1] show that for any convex loss function the error vector \u2206 = (\u03b4\u03b8\u2217 \u2212 \u03b4\u03b8\u0302) lies in a restricted set that is characterized as\nEr = Er(\u03b4\u03b8 \u2217, \u03b2) = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223 R(\u03b4\u03b8\u2217 + \u2206) \u2264 R(\u03b4\u03b8\u2217) + 1\u03b2R(\u2206) } . (25)\nRestricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19]. A convex loss function satisfies the RSC condition in Cr = cone(Er), i.e., \u2200\u2206 \u2208 Cr, if there exists a suitable constant \u03ba such that\n\u03b4L(\u03b4\u03b8\u2217, u) := L(\u03b4\u03b8\u2217 + u)\u2212 L(\u03b4\u03b8\u2217)\u2212 \u3008\u2207L(\u03b4\u03b8\u2217), u\u3009 \u2265 \u03ba\u2016u\u201622 (26)\nDeterministic Recovery Bounds: If the RSC condition is satisfied on the error set Cr and \u03bbn1,n2 satisfies the assumptions stated earlier, for any norm R(.), Banerjee et al. [1] show a deterministic upper bound for \u2016\u2206\u20162 in terms of \u03bbn1,n2 , \u03ba, and the norm compatibility constant \u03a8(Cr) = supu\u2208Cr R(u) \u2016u\u20162 , as\n\u2016\u2206\u20162 \u2264 1 + \u03b2\n\u03b2 \u03bbn1,n2 \u03ba \u03a8(Cr) . (27)\nSmooth Density Ratio Model Assumption: For any vector u such that \u2016u\u20162 \u2264 \u2016\u03b4\u03b8\u2217\u20162 and every \u2208 R, the following inequality holds:\nEX\u223cp(X|\u03b82)[exp{ r(X|\u03b4\u03b8 \u2217 + u)\u2212 1}] \u2264 exp{ 2}.\nA similar assumption is used in the analysis of Liu et al. [13].\nRemark 2 Bounded density ratio is a special case satisfying the smooth density ratio assumption. Lemma 1 shows a sufficient condition under which the density ratio is bounded.\nLemma 1 Consider two Ising Model with true parameters \u03b8\u22171 and \u03b8\u22172 . Let d1, d2 s where \u2016\u03b8\u22171\u20160 = d1, \u2016\u03b8\u22172\u20160 = d2, and \u2016\u03b4\u03b8\u2217\u20160 = s. Assume\nmin i,j=1\u00b7\u00b7\u00b7p\n(|\u03b8\u22171(i, j)|) \u2265 1 d1 \u2212 1 \u2212 c1 (d1 \u2212 1)s (28)\nmin i,j=1\u00b7\u00b7\u00b7p\n(|\u03b8\u22172(i, j)|) \u2265 1 d2 \u2212 1 \u2212 c2 (d2 \u2212 1)s , (29)\nwhere c1 and c2 are positive constants. Then the density ratio r(X = x|\u03b4\u03b8\u2217) is bounded.\nNote that if individual graphs are dense, then the conditions (28) and (29) are satisfied and as a result the smooth density ratio is satisfied.\nRemark 3 In this paper, we focus on the Ising graphical model. But, our statistical analysis holds for any graphical models that satisfy the above mentioned assumption. Through our analysis, no assumption is required on the individual graphical models."}, {"heading": "3.2 Bounds on the regularization parameter", "text": "To get the recovery bound (27) above, one needs to have \u03bbn1,n2 \u2265 \u03b2R\u2217 (\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )). However, the bound on \u03bbn1,n2 depends on unknown quantity \u03b4\u03b8 \u2217 and the samples Xn11 ,X n2 2 and is hence random. To overcome the above challenges, one can bound the expectation E[R\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ))] over all samples of size n1 and n2, and obtain high-probability deviation bounds. The goal is to provide a sharp bound on \u03bbn1,n2 since the error bound in (27) is directly proportional to \u03bbn1,n2 .\nIn theorem 1, we characterize the expectation E[R\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ))] in terms of the Gaussian width of the unit norm-ball of R(.), which leads to a sharp bound. The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].\nTheorem 1 Define \u2126R = {u : R(u) \u2264 1}. Let \u03c6(R) = supu \u2016u\u20162 R(u) . Assume that for any u that \u2016u\u2016 \u2264 \u2016\u03b8 \u2217\u2016\n1 2 \u03bbmax\n( \u22072L(\u03b4\u03b8\u2217 + u) ) \u2264 \u03b70, (30)\nwhere \u03bbmax(.) is the maximum eigenvalue. Then under the smooth density ratio assumption, we have\nE [R\u2217(\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ))] \u2264\n2 \u221a \u03b70(c1w (\u2126R) + \u03c6(R))\u221a\nmin(n1, n2) .\nand with probability at least 1\u2212 c2e\u2212 2\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )) \u2264 c2(1 + )w(\u2126R) + \u03c41\u221a min(n1, n2) .\nwhere c1 and c2 are positive constants, \u03c41 = 2 \u221a \u03b70\u03c6(R), and w(\u2126R) is the Gaussian width of set \u2126R.\nNote, that our analysis hold for any norm and it is expressed in terms of the Gaussian width. In the following, we give the bound on the regularization parameter for two examples of the regularization function R(.).\nCorollary 1 If R(\u03b4\u03b8) is the L1 norm, and \u03b4\u03b8 \u2208 Rp 2 then with high probability we have the bound\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )) \u2264\n\u03b72 \u221a log p\u221a min(n1, n2) . (31)\nCorollary 2 If R(\u03b4\u03b8) is the group-sparse norm, and \u03b4\u03b8 \u2208 Rp2 then with high probability we have the bound\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )) \u2264\n\u03b72 \u221a m+ logNG\u221a\nmin(n1, n2) , (32)\nwhere G = {G1, \u00b7 \u00b7 \u00b7 ,GNG} is a collection of groups, m = maxi |Gi| is the maximum size of any group."}, {"heading": "3.3 RSC Condition", "text": "In this Section, we establish the RSC condition for direct change detection estimator (1). Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for \u2200\u03b3i \u2208 [0, 1], we have\n\u03b4L(\u03b4\u03b8\u2217, u) := L(\u03b4\u03b8\u2217 + u)\u2212 L(\u03b4\u03b8\u2217)\u2212 \u3008\u2207L(\u03b4\u03b8\u2217), u\u3009 \u2265 uT\u22072L(\u03b4\u03b8\u2217 + \u03b3iu)u. (33)\nThus, the RSC condition depends on the non-linear terms of loss function. Recall that the nonlinear term, second term, in Loss function (1) which is the approximation of the log-partition functions only depends on n2. As a results, only samples of Xn22 affect the RSC conditions. Our analysis is an extension of the results on [1] using the generic chaining. We show that, with high probability the RSC condition is satisfied once samples n2 crosses w2(Cr \u2229 Sd\u22121) the Gaussian width of restricted error set. The bound on Gaussian width of the error set for atomic norms has been provided in [7].\nLet ri = r(X = x2i |\u03b4\u03b8\u2217) and \u03b5\u0304 denote the probability that ri exceeds some constant \u03b70: \u03b5\u0304 = p(ri > \u03b70) \u2265 1\u2212 e\u2212 \u03b720 2 .\nTheorem 2 Let X \u2208 Rn\u00d7p be a design matrix with independent isotropic sub-Gaussian rows with |||Xi|||\u03a82 \u2264 \u03ba. Then, for any set A \u2286 Sp\u22121, for suitable constants \u03b7, c1, c2 > 0 with probability at least 1\u2212exp(\u2212\u03b7w2(A)), we have\ninf u\u2208A\n\u2202L(\u03b8\u2217;u,X) \u2265 c1\u03c12 (\n1\u2212 c2\u03ba21 w(A) \u221a n2\n) \u2212 \u03c4 (34)\nwhere \u03ba1 = \u03ba\u03b5\u0304 , \u03c1 2 = infu\u2208A \u03c1 2 u with \u03c1 2 u = E [\u2329 u, T (X2i ) \u232a2 I(ri > \u03b70)], and \u03c4 is smaller than the first term in right hand side. Thus, for n2 \u2265 c2w2(A), with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have infu\u2208A \u2202L(\u03b8\u2217;u,X) > 0."}, {"heading": "3.4 Statistical Recovery", "text": "With the above results in place, from (27), Theorem 3 provides the main recovery bound for generalized direct change estimator (1).\nTheorem 3 Consider two set of i.i.d samples Xn11 = {x1i } n1 i=1 and X n2 2 = {x2i } n2 i=1. Define \u2126R = {u : R(u) \u2264 1}. Assume that \u03b4\u03b8\u0302 is the minimizer of the problem (1). Then, with probability at least 1\u2212 \u03b70e\u2212 2 the followings hold\n\u03bbn1,n2 \u2265 \u03b71\u221a\nmin(n1, n2) (w(\u2126R) + ) (35)\nand for n2 \u2265 cw2(Cr \u2229 Sd\u22121), with high probability, the estimate \u03b4\u03b8\u0302 satisfies\n\u2016\u2206\u20162 \u2264 O\n( w(\u2126R)\u221a\nmin(n1, n2)\n) \u03a8(Cr) , (36)\nwhere w(.) is the Gaussian width of a set,and c2, \u03b70, and \u03b71 are positive constants.\nProof: Proof of the Theorem can be directly obtain as the results of (27) and Theorem 1 and Theorem 2.\nIn the following, we provide the recovery bound for two special cases as an example.\nCorollary 3 IfR(\u03b4\u03b8) is the L1 norm, \u03b4\u03b8\u2217 \u2208 Rp 2 s s-sparse., \u03a8(Cr) \u2264 4 \u221a s, and for n2 > cs log p, the recovery error is bounded by\n\u2016\u2206\u20162 \u2264 c3 \u03a8(Cr)\u03bbn1,n2\n\u03ba = O\n(\u221a s log p\nmin(n1, n2)\n) .\nCorollary 4 If R(\u03b4\u03b8) is the group-sparse norm, \u03b4\u03b8 \u2208 Rp2 , \u03a8(Cr) \u2264 4 \u221a sG and for n2 \u2265 c(msG + sG logNG), the recovery error is bounded by\n\u2016\u2206\u20162 \u2264 c3 \u03a8(Cr)\u03bbn1,n2\n\u03ba = O\n(\u221a sGm+ logNG\nmin(n1, n2)\n) ."}, {"heading": "4 Experiments", "text": "In this Section, we evaluate generalized direct change estimator (direct) with three different norms. and we compare our direct approach with indirect approach. For indirect approach, we first estimate Ising model structures \u03b8\u03021 and \u03b8\u03022 with L1 norm regularizer, separately [22]. Then, we obtain \u03b4\u03b8\u0302 = \u03b8\u03021 \u2212 \u03b8\u03022. In all experiments, we draw n1 and n2 i.i.d samples from each Ising model by running Gibbs sampling. Here we set n = n1 = n2 = {20, 50, 100}.\nL1 norm: Here we first generate \u03b8\u22171 with three disconnected star sub-graphs (Figure 4-a) with p = 50. We generate the weights uniformly random between {0.3 \u2212 0.5}. We then generate \u03b8\u22172 by removing 10 random edges from \u03b8\u22171 (Figure 4-b). It is interesting that although individual graphs are sparse, but direct approach has a better ROC curve for all values of n (Figure 4-d). Similar results obtained by with random graph structure of \u03b8\u22171 and \u03b8 \u2217 2 .\nGroup-sparse norm: In this set of experiments, we evaluate direct method with three different structure for \u03b8\u22171 : (i) a random graph structure (Figure 4-e), (ii) scale free graph structure (Figure 4-i), and (iii) a cluster graph structure (Figure 4-m). In all settings, we set p = 60 and generate \u03b8\u22172 by removing a block of edges from \u03b8 \u2217 1 (Figure 4-(f,j,n)).\nFor random graph structure and block structure, direct method has a better ROC curve (Figure 4-h,p). But, for scalefree structure, since the individual graphs are sparse, indirect method can estimate \u03b8\u03021 and \u03b8\u03022 correctly, and thus have a better ROC curve (Figure 4-l).\nNode perturbation: Here, we first generate a random graph structure \u03b8\u22171 , and then generate \u03b8\u22172 by perturbing two nodes in \u03b8\u22171 . Here we set p = 60 and generate \u03b8 \u2217 2 by setting rows and columns 3, 51 to zero in \u03b8 \u2217 1 (Figure 4-s). Although, the individual graphs are dense but direct approach can estimate edges in \u03b4\u03b8 with only n = 20 samples (Figure 4-t)."}, {"heading": "5 Conclusion", "text": "This paper presents the statistical analysis of direct change problem in Ising graphical models where any norm can be plugged in for characterizing the parameter structure. An optimization algorithm based on FISTA-style algorithms is proposed with the convergence rate of O(1/T 2). We provide the statistical analysis for any norm such as L1 norm, group sparse norm, node perturbation, etc. Our analysis is based on generic chaining and illustrates the important role of Gaussian widths (a geometric measure of size of suitable sets) in such results. For the special case of sparsity, we obtain a sharper result than previous results [13] under the same smooth density ratio assumption. Liu et al. [13] obtained the same result with a bounded density ratio assumption which is a more restrictive assumption. Although, we presented the results for Ising model, our analysis can be applied to any graphical model which satisfies the smooth density ratio assumption. Further, we extensively compared our generalized direct change estimator with an indirect approach over a wide range of graph structures and norms. We show that our direct approach has a better ROC curve than indirect approach without any assumption on the structure of individual graphs. We implemented indirect approach by estimating individual Ising model structures with L1 norm regularizer. However, if individual graphs has a suitable structure such as group sparsity, one may apply a regularization that can characterize the graph structure and may improve performance of the indirect approach. We will investigate this possibility in our future research.\nAppendix"}, {"heading": "A Background and Preliminaries", "text": "Definition 2 Sub-Gaussian random variable: We say that a random variable x is sub-Gaussian if the moments satisfies\n[E|x|p] 1 p \u2264 K2 \u221a p (37)\nfor any p \u2265 1 with a constant K2. The minimum value of K2 is called sub-Gaussian norm of x, denoted by |||x|||\u03c82 . If E[x] = 0, then\nE[exp{tX}] \u2264 exp{Ct2|||X|||2\u03a82}, (38)\nwhere C and c are positive constant.\nDefinition 3 Sub-Gaussian random vector: We say that a random vector X in Rn is sub-Gaussian if the onedimensional marginals \u3008X,x\u3009 are sub-Gaussian random variables for all x \u2208 Rn. The sub-Gaussian norm of X is defined as\n|||X|||\u03c82 = sup x\u2208Sn\u22121 \u2016\u3008X,x\u3009\u2016\u03c82 (39)\nLemma 2 Consider a sub-Gaussian vector X \u2208 Rn with |||X|||\u03a82 < K, then for any vector u, \u3008X,u\u3009 is a subGaussian variable with |||\u3008X,u\u3009||| < K\u2016u\u20162.\nProof: The argument is based on Definition 3 as follows,\n|||\u3008X,u\u3009|||\u03a82 = \u2016u\u20162 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2329X, u\u2016u\u20162 \u232a\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03a82 \u2264 \u2016u\u20162 sup x\u2208Sn\u22121 \u3008X,x\u3009 = \u2016u\u20162|||X|||\u03a82 \u2264 K\u2016u\u20162. (40)\nLemma 3 Let X1 and X2 be centered sub-Gaussian random variables with |||X1|||\u03a82 = b1 and |||X2|||\u03a81 = b2. Then X1 +X2 is centered sub-Gaussian with |||X1 +X2|||\u03a82 = b1 + b2.\nProof: The argument is based on the definition of moment generating function of sub-Gaussian random variable:\nUsing Holder inequality for any p, q > 0 where 1p + 1 q = 1, we have\nE[exp{t(X1 +X2)}] \u2264 (E[exp{tX1}p])1/p(E[exp{tX1}q])1/q\n\u2264 exp{Ct2(pb21 + qb22)} = exp{Ct2(pb21 + p\n1\u2212 p b22)}. (41)\nThe minimum of (41) occurs with p = b2b1 . As a result, we have\nE[exp{t(X1 +X2)}] \u2264 exp{Ct2(b1 + b2)2}. (42)\nThe proof is complete.\nA.1 Generic Chaining\nDefinition 4 (Majorizing measure [27]) Given \u03b1 > 0, and a metric space (T, d) (that need not be finite), we define\n\u03b3\u03b1(T, d) = inf sup t \u2211 n\u22650 2n/\u03b1\u2206(An(T )). (43)\nwhere the infimum is taken over all admissible sequences and \u2206(An(T )) is the diameter of An(t).\nNote that \u03b32(T, \u2016.\u20162) coincides with the Gaussian width of T : w(T ).\nLemma 4 Given a metric space (T, d), we have\n\u03b31(T, \u2016.\u2016\u221e) \u2264 \u03b322(T, \u2016.\u20162). (44)\nProof: Define d2(s, t) = \u2016s\u2212 t\u20162 and d1(s, t) = \u2016s\u2212 t\u2016\u221e. We use the traditional definition of majorizing measure \u03b3\u03b1,1(T, d) from [27]\n\u03b3\u03b1,1(T, d) = inf sup t (\u222b \u221e 0 ( log\n1\n\u00b5(Bd(t, \u03b5))\n)1/\u03b1 d\u03b5 ) . (45)\nwhere Bd(t, \u03b5) is the closed ball of center t and radius \u03b5 based on the distance d and the infimum is taken over all the probability measure \u00b5 on T .\nNote that \u03b3\u03b1,1(T, d) coincides with the functional \u03b3\u03b1(T, d) [28] as\nK(\u03b1)\u22121\u03b3\u03b1(T, d) \u2264 \u03b3\u03b1,1(T, d) \u2264 K(\u03b1)\u03b3\u03b1(T, d), (46)\nwhere K(\u03b1) is a constant depending on \u03b1 only.\nAs a result, it is enough to show that \u03b31,1(T, d1) \u2264 \u03b322,1(T, d2).\nNote that since for any vector x, we have \u2016x\u2016\u221e \u2264 \u2016x\u20162, therefore, for any probability measure \u00b5 and t, we have \u00b5(Bd1(t, \u03b5)) \u2265 \u00b5(Bd2(t, \u03b5)). As a result,(\u222b \u221e\n0\n( log\n1\n\u00b5(Bd1(t, \u03b5))\n) d\u03b5 ) \u2264 (\u222b \u221e\n0\n( log\n1\n\u00b5(Bd2(t, \u03b5))\n) d\u03b5 ) \u2264 (\u222b \u221e 0 ( log\n1\n\u00b5(Bd2(t, \u03b5))\n)1/2 d\u03b5 )2 . (47)\nSince (47) holds for any \u00b5 and t, we have\n\u03b31,1(T, d1) = inf sup t (\u222b \u221e 0 ( log\n1\n\u00b5(Bd1(t, \u03b5))\n) d\u03b5 ) \u2264 inf sup\nt (\u222b \u221e 0 ( log\n1\n\u00b5(Bd2(t, \u03b5))\n)1/2 d\u03b5 )2 = \u03b322,1(T, d2). (48)\nThis completes the proof.\nTheorem 4 [Theorem 1.2.7] in [29] Consider a set T provided with two distances d1 and d2. Consider a process (Xt)t\u2208T that satisfies E[Xt] = 0 and\nP (|Xs \u2212Xt| \u2265 u) \u2264 2 exp ( \u2212min ( u2\nd2(s, t)2 ,\nu\nd1(s, t)\n)) . (49)\nThen\nE[ sup t,s\u2208T\n|Xs \u2212Xt| \u2264 L(\u03b31(T, d1) + \u03b32(T, d2)), (50)\nwhere L is a constant.\nTheorem 5 [Theorem 1.2.9] in [29] Under the conditions of Theorem 4, for all values of u1, u2 > 0 we have\nP (|Xs \u2212Xt0| \u2265 L(\u03b31(T, d1) + \u03b32(T, d2)) + u1D1 + u2D2) \u2264 L exp(\u2212min(u22, u1)), (51)\nwhere Dj = 2 \u2211 n\u22650 en(T, dj). Note that Dj \u2264 L\u03b3j(T, dj).\nTheorem 6 [Theorem 8.2 (Fernique-Talagrand\u2019s comparison theorem)] in [32] Let T be an arbitrary set. Consider a Gaussian random process (G(t))t\u2208T and a sub-Gaussian random process (H(t))t\u2208T . Assume that E[G(t)] = E[H(t)] = 0 for all t \u2208 T . Assume also that for some M > 0, the following increment comparison holds:\n|||H(s)\u2212H(t)|||\u03c82 \u2264M(E[\u2016G(s)\u2212G(t)\u2016 2 2) 1/2 \u2200s, t \u2208 T. (52)\nThen\nE[supt\u2208TH(t)] \u2264 CME[supt\u2208TG(t)]. (53)\nTheorem 7 (Mendelson, Pajor, Tomczak-Jaegermann [17]) There exist absolute constants c1, c2, c3 for which the following holds. Let (\u2126, \u00b5) be a probability space, set F be a subset of the unit sphere of L2(\u00b5), i.e., F \u2286 SL2 = {f : |||f |||L2 = 1}, and assume that supf\u2208F |||f |||\u03c82 \u2264 \u03ba. Then, for any \u03b8 > 0 and n \u2265 1 satisfying\nc1\u03ba\u03b32(F, |||\u00b7|||\u03c82) \u2264 \u03b8 \u221a n , (54)\nwith probability at least 1\u2212 exp(\u2212c2\u03b82n/\u03ba4),\nsup f\u2208F \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 f2(Xi)\u2212 E [ f2 ]\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b8 . (55)\nFurther, if F is symmetric, then\nE [ sup f\u2208F \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 f2(Xi)\u2212 E [ f2 ]\u2223\u2223\u2223\u2223\u2223 ] \u2264 c3 max { 2\u03ba \u03b32(F, |||\u00b7|||\u03c82)\u221a n , \u03b322(F, |||\u00b7|||\u03c82) n } (56)"}, {"heading": "B Regularization Parameter", "text": "Lemma 5 Consider two Ising Model with true parameters \u03b8\u22171 and \u03b8\u22172 . Let d1, d2 s where \u2016\u03b8\u22171\u20160 = d1, \u2016\u03b8\u22172\u20160 = d2, and \u2016\u03b4\u03b8\u2217\u20160 = s. Assume\nmin i,j=1\u00b7\u00b7\u00b7n1\n(|\u03b8\u22171(i, j)|) \u2265 1 d1 \u2212 1 \u2212 c1 (d1 \u2212 1)s (57)\nmin i,j=1\u00b7\u00b7\u00b7n2\n(|\u03b8\u22172(i, j)|) \u2265 1 d2 \u2212 1 \u2212 c2 (d2 \u2212 1)s , (58)\nwhere c1 and c2 are positive constants. Then the density ratio r(X = x|\u03b4\u03b8\u2217) is bounded.\nProof: Let \u03b11 \u2264 |\u03b8\u22171 | \u2264 \u03b21 and \u03b12 \u2264 |\u03b8\u22172 | \u2264 \u03b22. Without loss of generality, assume that \u2016\u03b8\u22171\u20162 = 1 and \u2016\u03b8\u22172\u20162 = 1.\nSo,\n\u03b21 \u2264 1\u2212 (d1 \u2212 1)\u03b11 (59) \u03b22 \u2264 1\u2212 (d2 \u2212 1)\u03b12. (60)\nBased on triangle inequality of norms, we have\n\u2016\u03b4\u03b8\u2217\u2016\u221e = \u2016\u03b8\u22171 \u2212 \u03b8\u22172\u2016\u221e \u2264 \u2016\u03b8\u22171\u2016\u221e + \u2016\u03b8\u22172\u2016\u221e \u2264 \u03b21 + \u03b22 \u2264 2\u2212 (d1 \u2212 1)\u03b11 \u2212 (d2 \u2212 1)\u03b12. (61)\nLet z = T (x), then,\n|\u3008z, \u03b4\u03b8\u2217\u3009| \u2264 \u2016z\u2016\u221e\u2016\u03b4\u03b8\u2217\u20161 (62) \u2264 s\u2016\u03b4\u03b8\u2217\u2016\u221e (63) \u2264 2s\u2212 [(d1 \u2212 1)\u03b11 \u2212 (d2 \u2212 1)\u03b12]s (64)\nwhere the second inequality is the result of \u2016z\u2016\u221e \u2264 1 since z comes from an Ising model.\nNote that if\n\u03b11 \u2265 s\u2212 c1\n(d1 \u2212 1)s =\n1 d1 \u2212 1 \u2212 c1 (d1 \u2212 1)s , (65)\nthen\ns\u2212 (d1 \u2212 1)\u03b11s \u2264 c1. (66)\nSimilarly, if\n\u03b12 \u2265 s\u2212 c2\n(d2 \u2212 1)s =\n1 d2 \u2212 1 \u2212 c2 (d2 \u2212 1)s , (67)\nthen\ns\u2212 (d2 \u2212 1)\u03b12s \u2264 c2. (68)\nAs a result, we have\n|\u3008z, \u03b4\u03b8\u2217\u3009| \u2264 c1 + c2. (69) \u21d2 exp{\u3008z, \u03b4\u03b8\u2217\u3009} \u2264 exp c1 + c2 \u2264 c0. (70)\nFor example, if c1 = c2 = 1, then c0 = 10.\nTherefore,\nr(X = x|\u03b4\u03b8) = exp{\u3008z, \u03b4\u03b8\u3009)} Z(\u03b4\u03b8\u2217) \u2264 c0 Z(\u03b4\u03b8\u2217) . (71)\nThis completes the proof.\nAssumption 1(Smooth Density Ratio Model Assumption) For any vector u such that \u2016u\u20162 \u2264 \u2016\u03b4\u03b8\u2217\u20162 and every t \u2208 R, the following inequality holds:\nEX\u223cp(X|\u03b82)[exp{tr(X|\u03b4\u03b8 \u2217 + u)\u2212 1}] \u2264 exp{t2}. (72)\nLemma 6 For any constant \u03c4 \u2264 1, define random event M\u03c4 as follows, M\u03c4 = {\u03a8(\u03b4\u03b8\u2217 + u)\u2212\u03a8(\u03b4\u03b8\u2217)\u2212 [ \u03a8\u0302(\u03b4\u03b8\u2217 + u)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217) ] \u2264 \u03c4}. (73)\nThen, for any vector u such that \u2016u\u20162 \u2264 \u2016\u03b4\u03b8\u2217\u20162, under Assumption 1, we have\nP (M c\u03c4 ) = p ( \u03a8(\u03b4\u03b8\u2217 + u)\u2212\u03a8(\u03b4\u03b8\u2217)\u2212 [ \u03a8\u0302(\u03b4\u03b8\u2217 + u)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217) ] > \u03c4 ) \u2264 4e\u2212 n2 5 \u03c4 2 . (74)\nProof: Recall that\nr(X = x|\u03b4\u03b8\u2217) = exp{\u3008T (x), \u03b4\u03b8 \u2217\u3009}\nZ(\u03b4\u03b8\u2217)\n\u21d2 Z\u0302(\u03b4\u03b8\u2217) = 1 n2 n2\u2211 i=1 exp{\u3008T (x2i ), \u03b4\u03b8\u2217\u3009} = 1 n2 n2\u2211 i=1 r(X = x2i |\u03b4\u03b8)Z(\u03b4\u03b8\u2217) \u21d2 Z\u0302(\u03b4\u03b8 \u2217)\nZ(\u03b4\u03b8\u2217) =\n1\nn2 n2\u2211 i=1 r(X = x2i |\u03b4\u03b8\u2217) (75)\nNote that Z(\u03b4\u03b8) = EX\u223cp(X|\u03b82)[exp{\u3008T (x), \u03b4\u03b8\u2217\u3009}], therefore,\nEX\u223cp(X|\u03b82)[r(X|\u03b4\u03b8 \u2217)] = 1. (76)\nUnder the Assumption 1, we have\np (\u2223\u2223r(X = x2i |\u03b4\u03b8\u2217)\u2212 1\u2223\u2223 > ) \u2264 c1e\u2212 2 . (77)\nApplying Hoeffding inequality, we have\np (\u2223\u2223\u2223\u2223\u2223 1n2 n2\u2211 i=1 r(X = x2i |\u03b4\u03b8\u2217)\u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2265 ) \u2264 2e\u2212 2\n(78)\n\u21d2 p (\u2223\u2223\u2223\u2223\u2223 Z\u0302(\u03b4\u03b8\u2217)Z(\u03b4\u03b8\u2217) \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2265 ) \u2264 2e\u2212n2 2 . (79)\nTaking the logarithm from both side, and using one side bound, we have\np ( log Z\u0302(\u03b4\u03b8\u2217)\nZ(\u03b4\u03b8\u2217) \u2265 log( + 1)\n) \u2264 e\u2212n2 2\n(80)\n\u21d2p ( \u03a8\u0302(\u03b4\u03b8\u2217)\u2212\u03a8(\u03b4\u03b8\u2217) \u2265 log( + 1) ) \u2264 e\u2212n2 2 . (81)\nSimilarly, we have\np ( \u03a8(\u03b4\u03b8\u2217 + u)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217 + u) \u2265 \u2212 log(1\u2212 ) ) \u2264 e\u2212n2 2 . (82)\nApplying the union bound, we have\np ( \u03a8(\u03b4\u03b8\u2217 + u)\u2212\u03a8(\u03b4\u03b8\u2217)\u2212 [ \u03a8\u0302(\u03b4\u03b8\u2217 + u)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217) ] \u2265 log 1 +\n1\u2212\n) \u2264 4e\u2212n2 2 . (83)\nSetting \u03c4 = log 1+ 1\u2212 , we have p ( \u03a8(\u03b4\u03b8\u2217 + u)\u2212\u03a8(\u03b4\u03b8\u2217)\u2212 [ \u03a8\u0302(\u03b4\u03b8\u2217 + u)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217) ] \u2265 \u03c4 ) \u2264 4e\u2212n2( e\u03c4+1 e\u03c4\u22121 ) 2 \u2264 4e\u2212 n2 5 \u03c4 2 , (84)\nwhere the last inequality is obtained by using the fact that for any \u03c4 \u2264 1( e\u03c4 + 1\ne\u03c4 \u2212 1\n)2 > \u03c42\n5 . (85)\nThis completes the proof.\nLemma 7 Define random event Mt\u0303 as follows, Mt\u0303 = {\u03a8(\u03b4\u03b8\u2217 + tu)\u2212\u03a8(\u03b4\u03b8\u2217)\u2212 [ \u03a8\u0302(\u03b4\u03b8\u2217 + tu)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217) ] \u2264 t\u0303}, (86) where t\u0303 = \u221a 5\u03b71t+ \u221a 5 2 . Let Z = T (X 1) and z = T (x1). Then,\nP (\u2223\u2223\u2223\u2329z\u2212\u2207\u03a8\u0302(\u03b8\u2217),u\u232a\u2223\u2223\u2223 \u2265 \u2223\u2223Mt\u0303) \u2264 c exp{ \u2212 24\u03b70\u2016u\u201622 }, (87)\nwhere 12\u03bbmax ( \u22072\u03a8\u0302(\u03b8\u2217 + u\u0303) ) \u2264 \u03b70 and c is a positive constant.\nProof: First, note that p(X = x|\u03b8\u22171) = p(X = x|\u03b8\u22172)r(X = x|\u03b4\u03b8\u2217). Therefore,\nEX\u223cp(X|\u03b8\u22171 )\n[ e\u3008Z,tu\u3009 ] = \u2211 x\u2208X e\u3008z,tu\u3009p(x|\u03b8\u22171)\n= \u2211 x\u2208X e\u3008z,tu\u3009p(x|\u03b8\u22172)r(x|\u03b4\u03b8\u2217)\n= e\u2212\u03a8(\u03b4\u03b8 \u2217) \u2211 x\u2208X e\u3008z,tu+\u03b4\u03b8 \u2217\u3009p(x|\u03b8\u22172) = e\u03a8(\u03b4\u03b8 \u2217+tu)\u2212\u03a8(\u03b4\u03b8\u2217), (88)\nsince r(x|\u03b4\u03b8\u2217) = exp{x, \u03b4\u03b8\u2217\u3009 \u2212\u03a8(\u03b4\u03b8\u2217)}, and \u03a8(\u03b4\u03b8\u2217) = log \u2211\nx\u2208X e \u3008x,\u03b4\u03b8\u2217\u3009p(x|\u03b8\u22172).\nAlso, using the Taylor expansion, we have \u03a8\u0302(\u03b4\u03b8\u2217 + tu)\u2212 \u03a8\u0302(\u03b4\u03b8\u2217)\u2212 \u2329 \u2207\u03a8\u0302(\u03b4\u03b8\u2217), tu \u232a = 1\n2 tuT\u22072\u03a8\u0302(\u03b4\u03b8\u2217 + u\u0303)tu\n\u2264 1 2 t2\u2016u\u201622\u03bbmax\n( \u22072\u03a8\u0302(\u03b4\u03b8\u2217 + u\u0303) ) \u2264 t2\u03b70\u2016u\u201622 = t2\u03b71, (89)\nwhere 12\u03bbmax ( \u22072\u03a8\u0302(\u03b4\u03b8\u2217 + u\u0303 ) \u2264 \u03b70 and \u03b71 = \u03b70\u2016u\u201622.\nThen, given the event Mt\u0303, the moment generating function of \u2329 Z \u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a can be bounded as,\nEX\u223cp(X|\u03b8\u22172 ) [ EX\u223cp(X|\u03b8\u22171 ) [ e\u3008Z\u2212\u2207\u03a8\u0302(\u03b4\u03b8 \u2217),tu\u3009 ] \u2223\u2223Mt\u0303] = EX\u223cp(X|\u03b8\u22172 ) [e\u3008\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),tu\u3009 EX\u223cp(X|\u03b8\u22171 ) [e\u3008Z,tu\u3009] \u2223\u2223Mt\u0303]\n(88) = EX\u223cp(X|\u03b8\u22172 )\n[ e\u03a8(\u03b4\u03b8 \u2217+tu)\u2212\u03a8(\u03b4\u03b8\u2217)\u2212\u3008\u2207\u03a8\u0302(\u03b4\u03b8\u2217),tu\u3009\u2223\u2223Mt\u0303] (86) \u2264 EX\u223cp(X|\u03b8\u22172 ) [ e\u03a8\u0302(\u03b4\u03b8\n\u2217+tu)\u2212\u03a8\u0302(\u03b4\u03b8\u2217)\u2212\u3008\u2207\u03a8\u0302(\u03b4\u03b8\u2217),tu\u3009+\u221a\u03b71t\u2223\u2223Mt\u0303] (89) \u2264 EX\u223cp(X|\u03b8\u22172 ) [ et 2\u03b71+ \u221a 5\u03b71t+ 1 2\n\u2223\u2223Mt\u0303] = et2\u03b71+\u221a5\u03b71t+ 12 . (90) As a result, using the Chernoff bound, for any t > 0, we have\nP (\u2329 Z \u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a \u2265 \u2223\u2223Mt\u0303) \u2264 e\u2212t EX\u223cp(X|\u03b8\u22172 ) [EX\u223cp(X|\u03b8\u22171 ) [e\u3008Z\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),tu\u3009] \u2223\u2223Mt\u0303]\n(90) \u2264 exp{t2\u03b71 + \u221a 5\u03b71t+ 1\n2 \u2212 t }\n(a) \u2264 exp{\u2212 ( \u2212 \u221a 5\u03b71) 2\n4\u03b71 +\n1 2 }\n\u2264 c exp{\u2212 2\n4\u03b70\u2016u\u201622 }, (91)\nwhere the inequality (a) is obtained by setting t = \u2212 \u221a \u03b71\n2\u03b71 to minimize it with respect to t, and the last inequality\nobtained by setting c = exp{ 5 \u221a\n5 2 \u221a \u03b71 \u2212 34} and using the fact that\nexp{\u2212 ( \u2212 \u221a 5\u03b71) 2\n4\u03b71 +\n1 2 } \u2264 c exp{\u2212\n2\n4\u03b71 }. (92)\nSimilarly, we have P (\u2329 Z \u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a \u2264 \u2212 \u2223\u2223Mt\u0303) \u2264 e\u2212t EX\u223cp(X|\u03b8\u22172 ) [EX\u223cp(X|\u03b8\u22171 ) [e\u3008Z\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),\u2212tu\u3009] \u2223\u2223Mt\u0303] \u2264 c exp{\u2212 2\n4\u03b70\u2016u\u201622 }. (93)\nThis completes the proof.\nLemma 8 Under the smooth density ratio assumption, we have\nP (\u2223\u2223 \u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,Xn22 ),u\u3009 \u2223\u2223 \u2265 ) \u2264 c1 exp{\u2212min(n1, n2) 24\u03b70\u2016u\u201622 }, (94)\nwhere c1 is a positive constant.\nProof: Let t\u0303 = \u221a 5\u03b71t+ \u221a 5 2 and t = \u2212\u221a\u03b71 2\u03b71 . Using the result of lemma 7 we have\nP (\u2329 T (x1i )\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a \u2265 \u2223\u2223Mt\u0303) = P (\u2329T (x1i )\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u\u232a \u2265 \u2223\u2223Mt\u0303)\n\u2264 c exp{\u2212 2\n4\u03b70\u2016u\u201622 }. (95)\nApplying Hoeffding inequality, we have\nP (\u2223\u2223 \u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,Xn22 ),u\u3009 \u2223\u2223 \u2265 \u2223\u2223Mt\u0303) = P\n(\u2329 1\nn1 n1\u2211 i=1 T (x1i )\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a \u2265 \u2223\u2223Mt\u0303 )\n\u2264 c exp{\u2212 n1 2\n4\u03b70\u2016u\u201622 }. (96)\nMoreover, we can obtain,\nP (\u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ),u\u3009 \u2264 \u2212 ) = P\n(\u2329 1\nn1 n1\u2211 i=1 T (x1i )\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u\n\u232a \u2265 )\n\u2264 P\n(\u2329 1\nn1 n1\u2211 i=1 T (x1i )\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a \u2265 \u2223\u2223Mt\u0303 ) P (Mt\u0303)\n+ P\n(\u2329 1\nn1 n1\u2211 i=1 T (x1i )\u2212\u2207\u03a8\u0302(\u03b4\u03b8\u2217),u \u232a \u2265 \u2223\u2223M ct\u0303 ) P (M ct\u0303 )\n\u2264 c exp{ \u2212n1 2 4\u03b70\u2016u\u201622 }+ 4 exp{ \u2212n2 2 4\u03b70\u2016u\u201622 }\n\u2264 c1 exp{\u2212 min(n1, n2)\n2\n4\u03b70\u2016u\u201622 }, (97)\nwhere the last inequality is obtained by using Lemma 6 as follows\nP (M ct\u0303 ) \u2264 4 exp{\u2212 n2 5 t\u03032} = 4 exp{\u2212n2 5 ( \u221a 5\u03b71t+ \u221a 5 2 )2}\n= 4 exp{\u2212n2 5\n( \u221a 5\u03b71 \u2032 \u2212\u221a\u03b71\n2\u03b71 +\n\u221a 5\n2 )2}\n= 4 exp{\u2212n2 2\n4\u03b71 },\nwhere \u03b71 = \u03b70\u2016u\u201622 and setting c1 = max(4, c). Similarly,\nP (\u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ),u\u3009 \u2265 ) = P\n(\u2329 1\nnp np\u2211 i=1 Zpi \u2212\u2207\u03a8\u0302(\u03b4\u03b8 \u2217),u\n\u232a \u2264 \u2212 )\n\u2264 P\n(\u2329 1\nnp np\u2211 i=1 Zpi \u2212\u2207\u03a8\u0302(\u03b4\u03b8 \u2217),u\n\u232a \u2264 \u2212 \u2223\u2223Mt\u0303 ) P (Mt2)\n+ P\n(\u2329 1\nnp np\u2211 i=1 Zpi \u2212\u2207\u03a8\u0302(\u03b4\u03b8 \u2217),u\n\u232a \u2264 \u2212 \u2223\u2223M ct\u0303 ) P (M ct\u0303 )\n\u2264 c1 exp{\u2212 min(n1, n2)\n2\n4\u03b70\u2016u\u201622 } (98)\nThis completes the proof.\nTheorem 1 Define \u2126R = {u : R(u) \u2264 1}. Let \u03c6(R) = supu \u2016u\u20162 R(u) . Assume that for any u that \u2016u\u2016 \u2264 \u2016\u03b8 \u2217\u2016\n1 2 \u03bbmax\n( \u22072L(\u03b4\u03b8\u2217 + u) ) \u2264 \u03b70, (99)\nwhere \u03bbmax(.) is the maximum eigenvalue. Then under the smooth density ratio assumption, we have\nE [R\u2217(\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ))] \u2264\n2 \u221a \u03b70\u221a\nmin(n1, n2) (c1w (\u2126R) + \u03c6(R)) . (100)\nand with probability at least 1\u2212 c2e\u2212 2\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )) \u2264 1\u221a min(n1, n2) (c2(1 + )w(\u2126R) + \u03c41) . (101)\nwhere c1 and c2 are positive constants, \u03c41 = 2 \u221a \u03b70\u03c6(R), and w(\u2126R) is the Gaussian width of set \u2126R.\nProof: Define \u00b5 = E[\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )]. Using the triangle inequality, we have\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )) \u2264 R\u2217 (\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u00b5) +R\u2217 (\u00b5) . (102)\nWe upper bound two terms as follows. First, consider the first term.\nUsing the definition of dual norm, we have\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )\u2212 \u00b5) = sup R(u)\u22641 \u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )\u2212 \u00b5,u\u3009 . (103)\nDefine stochastic process H(s) = \u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )\u2212 \u00b5, s\u3009 where E[H(s)] = 0. Then, from Lemma 8, we have\nP (H(s)\u2212H(t) \u2265 ) = P (\u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )\u2212 \u00b5, s\u2212 t\u3009 \u2265 ) (104)\n\u2264 c1 exp{\u2212 min(n1, n2)\n2\n4\u03b70\u2016s\u2212 t\u201622 }. (105)\nConsider the Gaussian process G(u) = \u3008u, g\u3009, indexed by the same set, i.e., u \u2208 \u2126R, where g \u223c N(0, Id\u00d7d) is standard Gaussian vector. Now from definition sub-Gaussian random variables, we have\n|||H(s)\u2212H(t)|||\u03c82 \u2264 2 \u221a \u03b70\u2016s\u2212 t\u20162\u221a\nmin(n1, n2) = KEg[\u2016G(s)\u2212G(t)\u201622]1/2, (106)\nwhere Eg[\u2016G(s)\u2212G(t)\u201622]1/2 = Eg[\u2016\u3008s\u2212 t, g\u3009\u201622]1/2 = \u2016s\u2212 t\u20162, and K = 2 \u221a \u03b70\u221a\nmin(n1,n2) .\nNext, by applying the Fernique-Talagrand\u2019s comparison theorem 6, we have\nE[ sup u\u2208\u2126R H(u)] = E [ sup u\u2208\u2126R \u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ),u\u3009 ] \u2264 c1KE[ sup\nu\u2208\u2126R G(u)] = 2c1\n\u221a \u03b70 w(\u2126R)\u221a min(n1, n2) , (107)\nwhere c1 is a constant. Thus,\nE [R\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )\u2212 \u00b5)] \u2264 c1 w(\u2126R)\u221a min(n1, n2) . (108)\nTo get the concentration bound, we use the direct application of Theorem 2.2.27 in [30] and we have\nP ( sup\ns,t\u2208\u2126R |H(s)\u2212H(t)| \u2264 c2(1 + ) w(\u2126R)\u221a min(n1, n2)\n) \u2265 1\u2212 c2 exp ( \u2212 2 ) . (109)\nThus, with probability at least 1\u2212 c2 exp ( \u2212 2 ) ,\nR\u2217 (\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )\u2212 \u00b5) \u2264 c2(1 + ) w(\u2126R)\u221a min(n1, n2) . (110)\nNext, we consider the second term. First note that |||\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )|||\u03a82 \u2264\n2 \u221a \u03b70\u2016u\u20162\u221a\nmin(n1,n2) . Using sub-Gaussian\nvariables property, we have\nE[L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )] \u2264 |||\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )|||\u03a82 \u2264\n2 \u221a \u03b70\u2016u\u20162\u221a\nmin(n1, n2) (111)\nUsing the definition of the dual norm, we have\nR\u2217(\u00b5) = R\u2217 (E [\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 )]) = sup u\u2208\u2126R E [\u3008\u2207L(\u03b4\u03b8\u2217;Xn11 ,X n2 2 ),u\u3009] (112)\n\u2264 2 \u221a \u03b70\u221a\nmin(n1, n2) sup u \u2016u\u20162 R(u)\n= 2 \u221a \u03b70\u221a\nmin(n1, n2) \u03a6(R), (113)\nwhere \u03a6(R) = supu \u2016u\u20162 R(u) .\nAlso, we have\nE [R\u2217(\u00b5)] =\u2264 2 \u221a \u03b70\u221a\nmin(n1, n2) \u03a6(R), (114)\nwhere \u03a6(R) = supu \u2016u\u20162 R(u) .\nThis completes the proof."}, {"heading": "C RSC condition", "text": "Let ri = r(X = x2i |\u03b4\u03b8\u2217) and \u03b5\u0304 denote the probability that ri exceeds some constant \u03b70: \u03b5\u0304 = p(ri > \u03b70) \u2265 1\u2212 e\u2212 \u03b720 2 .\nTheorem 2 Let X \u2208 Rn\u00d7p be a design matrix with independent isotropic sub-Gaussian rows with |||Xi|||\u03a82 \u2264 \u03ba. Then, for any set A \u2286 Sp\u22121, for suitable constants \u03b7, c1, c2 > 0 with probability at least 1\u2212exp(\u2212\u03b7w2(A)), we have\ninf u\u2208A\n\u2202L(\u03b8\u2217;u,X) \u2265 c1\u03c12 (\n1\u2212 c2\u03ba21 w(A) \u221a n2\n) \u2212 \u03c4 (115)\nwhere \u03ba1 = \u03ba\u03b5\u0304 , \u03c1 2 = infu\u2208A \u03c1 2 u with \u03c1 2 u = E [\u2329 u, T (X2i ) \u232a2 I(ri > \u03b70)], and \u03c4 is smaller than the first term in right hand side. Thus, for n2 \u2265 c2w2(A), with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have infu\u2208A \u2202L(\u03b8\u2217;u,X) > 0.\nProof: Define Z = T (X) and zi = T (x2i ). Then,\nL(\u03b4\u03b8;Xn11 ,X n2 2 ) = \u22121 n1 n1\u2211 i=1 \u3008T (x1i ), \u03b4\u03b8\u3009+ log 1 n2 n2\u2211 i=1 exp{\u3008T (x2i ), \u03b4\u03b8\u3009} (116)\n= \u22121 n1 n1\u2211 i=1 \u3008zi, \u03b4\u03b8\u3009+ log 1 n2 n2\u2211 i=1 exp{\u3008zi, \u03b4\u03b8\u3009}. (117)\nThrough the analysis, we consider that Z is centered random variable without loss of generality, since if it is not, the E[Z] will show up as a constant.\nRecall, RSC condition definition as\n\u03b4L(\u03b4\u03b8\u2217,u) := L(\u03b4\u03b8\u2217 + u;Xn11 ,X n2 2 )\u2212 L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u3008\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 ),u\u3009 \u2265 \u03ba\u2016u\u201622 (118)\nSimplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for \u2200\u03b3i \u2208 [0, 1], we have\n\u03b4L(\u03b4\u03b8\u2217,u) := L(\u03b4\u03b8\u2217 + u;Xn11 ,X n2 2 )\u2212 L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u3008\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 ),u\u3009\n\u2265 uT\u22072L(\u03b4\u03b8\u0303;Xn11 ,X n2 2 )u, (119)\nwhere \u03b4\u03b8\u0303 = \u03b4\u03b8\u2217 + \u03b3iu. As a result, to show when the RSC condition is satisfied, it is enough to find a lower bound for the right side of the above equation.\nNote that\n\u22072L(\u03b4\u03b8\u0303;Xn11 ,X n2 2 ) = \u22072\u03a8\u0302(\u03b4\u03b8\u0303), (120)\nwhere\n\u22072\u03a8\u0302(\u03b4\u03b8\u0303) = n2\u2211 i=1 \u03c3iz T i zi \u2212  n2\u2211 j=1 \u03c3jzj T  n2\u2211 j=1 \u03c3jzj  , (121) and\n\u03c3i = exp{\u3008zi, \u03b4\u03b8\u0303\u3009 \u2212 \u03a8\u0302(\u03b4\u03b8\u0303)} = exp \u3008zi, \u03b4\u03b8\u0303\u3009\u2211n2 j=1 exp \u3008zj , \u03b4\u03b8\u0303\u3009 . (122)\nPutting (121) back in (119), we have\n\u03b4L(\u03b4\u03b8\u2217,u) \u2265 n2\u2211 i=1\n\u03c3i\u3008u, zi\u30092\ufe38 \ufe37\ufe37 \ufe38 A \u2212\n\u2329 u,\nn2\u2211 j=1 \u03c3jzj \u232a2 \ufe38 \ufe37\ufe37 \ufe38\nB\n. (123)\nTo show the RSC condition, we need to show that (123) is strictly positive. First, we obtain the sample complexity so that A is far away from zero, then we show that A is strictly greater than B. This is enough to obtain the sample complexity so that the RSC condition is satisfied.\ni. Lower bound on A: Here, we explain how to get a lower bound on infu\u2208A \u2211n2 i=1 \u03c3i\u3008u, zi\u30092.\nLet ri = r(X = x2i |\u03b4\u03b8\u2217), and sr = \u2211n2 j=1 rj , then \u03c3i = ri sr . Then, we have\nn2\u2211 i=1 \u03c3i\u3008u, zi\u30092 = 1 sr n2\u2211 i=1 ri\u3008u, zi\u30092 . (124)\nThen, we have\np ( inf u\u2208A 1\nsr n2\u2211 i=1 ri\u3008u, zi\u30092 < \u03b70 \u03b71 \u03c12 ( 1\u2212 c\u03ba21 w(A) \u221a n2 )) \u2264 p( 1 sr < 1 \u03b71n2 )\n+ p ( inf u\u2208A n2\u2211 i=1 ri\u3008u, zi\u30092 < \u03b70n2\u03c12 ( 1\u2212 c\u03ba21 w(A) \u221a n2 )) .\n(125)\nFirst, we give a bound for the first term. Note that EX\u223cp(X|\u03b82)[r(X|\u03b4\u03b8\u2217)] = 1. From the smooth density ratio model assumption, we have\np(|ri \u2212 1| > t) \u2264 2e\u2212 t2 2 . (126)\nApplying Hoeffding inequality in (130), we have\np(| 1 n2 sr \u2212 1| \u2265 t) = p(| 1 n2 n2\u2211 j=1 rj \u2212 1| \u2265 t) \u2264 2e\u2212 n2t 2 2 , (127)\n\u21d2 p(sr \u2265 \u03b71n2) \u2264 e\u2212 n2(\u03b71\u22121)\n2\n2 , (128)\n\u21d2 p( 1 sr \u2264 1 \u03b71n2 ) = p(sr \u2265 \u03b71n2) \u2264 e\u2212 n2(\u03b71\u22121)\n2\n2 , (129)\nwhere \u03b71 = t+ 1.\nNext, we focus on bounding the second term in (125). Recall that,\np(|ri \u2212 1| > t) \u2264 2e\u2212 t2 2 , (130)\n\u21d2 \u03b5\u03041 = p(ri \u2265 \u03b70) \u2265 1\u2212 e\u2212 (1\u2212\u03b70)\n2\n2 , (131)\nwhere the last inequality holds for any \u03b70 = 1\u2212 t.\nFor any fixed \u03b70, let W\u0304i = W\u0304ui = \u3008u, zi\u3009I(ri > \u03b70). Then, the probability distribution over W\u0304i can be written as:1\np(W\u0304i = w) = p(\u3008u, zi\u3009 = w)I(ri > \u03b70) p(ri > \u03b70) \u2264 1 \u03b5\u03041 p(\u3008u, zi\u3009 = w) . (132)\nAs a result, \u2223\u2223\u2223\u2223\u2223\u2223W\u0304i\u2223\u2223\u2223\u2223\u2223\u2223\u03c82 \u2264 \u03ba\u03b5\u03041 = \u03ba1. Thus, W\u0304i = W\u0304ui is a sub-Gaussian random variable for any u \u2208 A. Let \u03c12u = E[(W\u0304i u )2] > 0. For convenience of notation, let Z0 be i.i.d. as the rows zi, i = 1, . . . , n. Let A \u2286 Sp\u22121. Consider the following class of functions:\nF = {fu,u \u2208 A : fu(.) = 1\n\u03c1u \u3008\u00b7,u\u3009I(r(.|\u03b4\u03b8\u2217) \u2265 \u03b70) : u \u2208 A}. (133)\nThen for any fu \u2208 F , fu(Z0) = 1\u03c1u \u3008Z0,u\u3009I(ri \u2265 \u03b70) and, by construction, F is a subset of the unit sphere, since for fu \u2208 F\n\u2016fu\u20162L2 = 1\n\u03c12u E[\u3008Z0,u\u30092I(ri \u2265 \u03b70)] = 1. (134)\nFurther, supfu\u2208F |||fu|||\u03c82 \u2264 \u03ba1/2.\nNext, we show that for the current setting, the \u03b32-functional can be upper bounded by w(A), the Gaussian width of A. Since the process is sub-Gaussian with \u03d52-norm bounded by \u03ba1, we have\n\u03b32(F \u2229 SL2 , |||\u00b7|||\u03c82) \u2264 \u03ba1\u03b32(F \u2229 SL2 , |||\u00b7|||L2) \u2264 \u03ba1c4w(A) , (135)\nwhere the last inequality follows from generic chaining, in particular [29, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 7, we choose\n\u03b8 = c1c4\u03ba 2 1 w(A)\u221a n \u2265 c1\u03ba1 \u03b32(F \u2229 SL2 , |||\u00b7|||\u03d52)\u221a n , (136)\nso that the condition on \u03b8 is satisfied. With this choice of \u03b8, we have\n\u03b82n/\u03ba41 = c 2 1c 2 4w 2(A) . (137)\nThen, from Theorem 7, it follows that with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have\nsup u\u2208A \u2223\u2223\u2223\u2223\u2223 1\u03c1un2 n2\u2211 i=1 1 \u03c1u \u3008zi,u\u30092I(ri \u2265 \u03b70)\u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 c\u03ba21w(A)\u221an2 , (138) 1With abuse of notation, we treat the distribution over W\u0304i as discrete for ease of notation. A similar argument applies for the true continuous\ndistribution, but more notation is needed.\nwhere \u03b7 = c2c21c 2 4 and c = c1c2 are absolute constants. Thus, with probability at least 1\u2212 exp(\u2212\u03b7w2(A)),\ninf u\u2208A\n1\nn2 n2\u2211 i=1 \u3008zi,u\u30092I(ri \u2265 \u03b70) \u2265 inf u\u2208A \u03c12u ( 1\u2212 c\u03ba21 w(A) \u221a n2 ) , (139)\n\u21d2 inf u\u2208A n2\u2211 i=1 \u3008zi,u\u30092I(ri \u2265 \u03b70) \u2265 n2\u03c12 ( 1\u2212 c\u03ba21 w(A) \u221a n2 ) , (140)\nwhere \u03c12 = infu\u2208A \u03c12u. Then, with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have\ninf u\u2208A n2\u2211 i=1 ri\u3008zi,u\u30092 \u2265 inf u\u2208A n2\u2211 i=1 ri\u3008zi,u\u30092I(ri \u2265 \u03b70) (141)\n\u2265 inf u\u2208A \u03b70 n2\u2211 i=1 \u3008zi,u\u30092I(ri \u2265 \u03b70) (142)\n\u2265\u03b70n2\u03c12 (\n1\u2212 c\u03ba21 w(A) \u221a n2\n) . (143)\nThus,\np ( inf u\u2208A n2\u2211 i=1 ri\u3008zi,u\u30092 \u2265 \u03b70n2\u03c12 ( 1\u2212 c\u03ba21 w(A) \u221a n2 )) \u2265 1\u2212 exp(\u2212\u03b7w2(A)), (144)\n\u21d2 p ( inf u\u2208A n2\u2211 i=1 ri\u3008zi,u\u30092 < \u03b70n2\u03c12 ( 1\u2212 c\u03ba21 w(A) \u221a n2 )) \u2264 exp(\u2212\u03b7w2(A)). (145)\nPutting (129) and (145) into (125), for any n2 \u2265 2\u03b7w 2(A)\n(\u03b71\u22121)2 we have\np ( inf u\u2208A n2\u2211 i=1 \u03c3i\u3008zi,u\u30092 < \u03b70 \u03b71 \u03c12 ( 1\u2212 c\u03ba21 w(A) \u221a n2 )) \u2264 exp(\u2212n2(\u03b71 \u2212 1) 2 2 ) + exp(\u2212\u03b7w2(A)) (146)\n\u2264 2 exp(\u2212\u03b7w2(A)), (147)\nii. A is strictly greater than B: Note that, 0 \u2264 \u03c3i \u2264 1 for all i and \u2211n2 i=1 \u03c3i = 1. Define f(z) = \u3008u, z\u30092, which is a convex function of z. Using Jensen\u2019s inequality, we have\nf (\u2211n2 i=1 \u03c3izi\u2211n2 i=1 \u03c3i ) \u2264 \u2211n2 i=1 \u03c3if(zi)\u2211n2\ni=1 \u03c3i (148)\u2329\nu, \u2211n2 i=1 \u03c3izi\u2211n2 i=1 \u03c3i \u232a2 \u2264 \u2211n2 i=1 \u03c3i\u3008u, zi\u30092\u2211n2 i=1 \u03c3i (149)\n\u3008u, n2\u2211 i=1 \u03c3jzi\u30092 \u2264 n2\u2211 i=1 \u03c3i\u3008u, zi\u30092. (150)\nThe equality in (150) holds if z1 = z2 = \u00b7 \u00b7 \u00b7 = zn2 , or if both sides are zero i.e., u is in the null space of zi for all i. Since zi are different with probability 1, then if we show that u is not in the null space of zi for all i, then the inequality (150) is strict inequality.\nThis completes the proof."}, {"heading": "Acknowledgment", "text": "The research was supported by NSF grants IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo. F. F. acknowledges the support of IDF (2014-2015) and DDF (2015-2016) from the University of Minnesota."}], "references": [{"title": "Estimation with Norm Regularization", "author": ["A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar"], "venue": "Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "and A", "author": ["O. Banerjee", "L. El Ghaoui"], "venue": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research, 9:485\u2013516", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "Oxford University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "A constrained l1 minimization approach to sparse precision matrix estimation", "author": ["T. Cai", "W. Liu", "X. Luo"], "venue": "Journal of the American Statistical Association, 106(494):594\u2013607", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The Convex Geometry of Linear Inverse Problems", "author": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Foundations of Computational Mathematics, 12(6):805\u2013849", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured estimation with atomic norms: General bounds and applications", "author": ["S. Chen", "A. Banerjee"], "venue": "Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "On Milman\u2019s Inequality and Random Subspaces Which Escape Through a Mesh inR", "author": ["Y. Gordon"], "venue": "Geometric Aspects of Functional Analysis, volume 1317 of Lecture Notes in Mathematics, pages 84\u2013106. Springer Berlin", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Covariate shift by kernel mean matching", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning, 3(4):5", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "The Journal of Machine Learning Research, 10:1391\u20131445", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Direct learning of sparse changes in markov networks by density ratio estimation", "author": ["S. Liu", "J.A. Quinn", "M.U. Gutmann", "T. Suzuki", "M. Sugiyama"], "venue": "Neural computation, 26(6):1169\u20131197", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Support consistency of direct sparse-change learning in markov networks", "author": ["S. Liu", "T. Suzuki", "M. Sugiyama"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Support consistency of direct sparse-change learning in markov networks", "author": ["S. Liu", "T. Suzuki", "M. Sugiyama"], "venue": "Joutnal of Annals of Statistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Group sparse priors for covariance estimation", "author": ["B.M. Marlin", "M. Schmidt", "K.P. Murphy"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 383\u2013392. AUAI Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "The Annals of Statistics, pages 1436\u20131462", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Reconstruction and subGaussian operators in asymptotic geometric analysis", "author": ["S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann"], "venue": "Geometric and Functional Analysis, 17:1248\u20131282", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Node-based learning of multiple gaussian graphical models", "author": ["K. Mohan", "P. London", "M. Fazel", "D. Witten", "S.-I. Lee"], "venue": "The Journal of Machine Learning Research, 15(1):445\u2013488", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statistical Science, 27(4):538\u2013557", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical programming, 103(1):127\u2013152", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S.P. Boyd"], "venue": "Foundations and Trends in Optimization, 1(3):127\u2013239", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "High-dimensional ising model selection using 1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "The Annals of Statistics, 38(3):1287\u20131319", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "High-dimensional covariance estimation by minimizing 1-penalized log-determinant divergence", "author": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": "Electronic Journal of Statistics, 5:935\u2013980", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient learning using forward-backward splitting", "author": ["Y. Singer", "J.C. Duchi"], "venue": "Neural Information Processing Systems, pages 495\u2013503", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P.V. Buenau", "M. Kawanabe"], "venue": "Advances in neural information processing systems, pages 1433\u20131440", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Density ratio estimation in machine learning", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": "Cambridge University Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Majorizing measures: the generic chaining", "author": ["M. Talagrand"], "venue": "The Annals of Probability, pages 1049\u20131103", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Majorizing measures without measures", "author": ["M. Talagrand"], "venue": "Annals of probability, pages 411\u2013417", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "The Generic Chaining", "author": ["M. Talagrand"], "venue": "Springer Monographs in Mathematics. Springer Berlin", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Upper and Lower Bounds for Stochastic Processes", "author": ["M. Talagrand"], "venue": "Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical inference problems and their rigorous solutions", "author": ["V. Vapnik", "R. Izmailov"], "venue": "Statistical Learning and Data Sciences, pages 33\u201371. Springer International Publishing", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimation in high dimensions: a geometric perspective", "author": ["R. Vershynin"], "venue": "Sampling Theory, a Renaissance", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programmming ( Lasso )", "author": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory, 55(5):2183\u20132201", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Graphical models via generalized linear models", "author": ["E. Yang", "A. Genevera", "Z. Liu", "P.K. Ravikumar"], "venue": "Advances in Neural Information Processing Systems, pages 1358\u20131366", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient methods for overlapping group lasso", "author": ["L. Yuan", "J. Liu", "J. Ye"], "venue": "Advances in Neural Information Processing Systems, pages 352\u2013360", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Direct estimation of differential networks", "author": ["S. Zhao", "T. Cai", "H. Li"], "venue": "Biometrika, 101(2):253\u2013268", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 9, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 15, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 21, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 22, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 33, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 5, "context": "In particular, we focus on the situation when the change \u03b4\u03b8\u2217 has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18].", "startOffset": 193, "endOffset": 200}, {"referenceID": 17, "context": "In particular, we focus on the situation when the change \u03b4\u03b8\u2217 has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18].", "startOffset": 193, "endOffset": 200}, {"referenceID": 4, "context": "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is.", "startOffset": 47, "endOffset": 58}, {"referenceID": 21, "context": "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is.", "startOffset": 47, "endOffset": 58}, {"referenceID": 22, "context": "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is.", "startOffset": 47, "endOffset": 58}, {"referenceID": 21, "context": "are sparse and the samples n1, n2 are sufficient to estimate them accurately [22], indirect estimation of \u03b4\u03b8\u0302 should be accurate.", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "However, if the individual parameters \u03b8\u2217 1 and \u03b8 \u2217 2 are somewhat dense, and the change \u03b4\u03b8 \u2217 has considerably more structure, such as block sparsity (only a small block has changed) or node perturbation sparsity (only edges from a few nodes have changed) [18], direct estimation may be considerably more efficient both in terms of the number of samples required as well as the computation time.", "startOffset": 255, "endOffset": 259}, {"referenceID": 12, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 9, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 24, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 25, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 30, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 13, "context": "[14] improved the sample complexity to min(n1, n2) = O(s log p) when a bounded density ratio model is assumed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] considered estimating direct sparse changes in Gaussian graphical models (GGMs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 5, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 17, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 18, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In particular, when \u03b4\u03b8\u2217 is sparse and our estimator is run with L1 norm, we get a sample complexity of n1 = n2 = O(s log p) which is sharper than n1 = O(s 2 log p) and n2 = O(n1) in [13].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 104, "endOffset": 114}, {"referenceID": 5, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 104, "endOffset": 114}, {"referenceID": 18, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 104, "endOffset": 114}, {"referenceID": 12, "context": "[13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm.", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 8, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 9, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 24, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 30, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 11, "context": "al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows r(X = x|\u03b4\u03b8) = p(X = x|\u03b81) p(X = x|\u03b82) = exp{\u3008T (x), \u03b81\u3009} exp{\u3008T (x), \u03b82\u3009} } {{ } r\u2217(x|\u03b4\u03b8) Z(\u03b82) Z(\u03b81) } {{ } 1/Z(\u03b4\u03b8) = exp{\u3008T (x), \u03b4\u03b8\u3009)} Z(\u03b4\u03b8) , (6)", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows r(X = x|\u03b4\u03b8) = p(X = x|\u03b81) p(X = x|\u03b82) = exp{\u3008T (x), \u03b81\u3009} exp{\u3008T (x), \u03b82\u3009} } {{ } r\u2217(x|\u03b4\u03b8) Z(\u03b82) Z(\u03b81) } {{ } 1/Z(\u03b4\u03b8) = exp{\u3008T (x), \u03b4\u03b8\u3009)} Z(\u03b4\u03b8) , (6)", "startOffset": 3, "endOffset": 11}, {"referenceID": 2, "context": "In this section, we present an algorithm in the class of Fast Iterative Shrinkage-Thresholding Algorithms (FISTA) for efficiently solving the problem (1) [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 20, "context": "In general, the proximal operator proxh(x) of a closed proper convex function h : R 7\u2192 R \u222a {+\u221e} [21] is defined as proxh(x) = argmin u ( h(u) + 1 2 \u2016u\u2212 x\u20162 ) .", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "(14) Thus, the unique minimizer (13) correspond to prox \u03bb LR ( \u03b4\u03b8t \u2212 1 L\u2207L(\u03b4\u03b8t) ) which has rate of convergence ofO(1/t) [20, 21].", "startOffset": 121, "endOffset": 129}, {"referenceID": 20, "context": "(14) Thus, the unique minimizer (13) correspond to prox \u03bb LR ( \u03b4\u03b8t \u2212 1 L\u2207L(\u03b4\u03b8t) ) which has rate of convergence ofO(1/t) [20, 21].", "startOffset": 121, "endOffset": 129}, {"referenceID": 2, "context": "To improve the rate of convergence, we adapt the idea of FISTA algorithm [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 19, "context": "The choice of \u03b1t+1 follows Nesterov\u2019s accelerated gradient descent [20, 21] and is detailed in Algorithm 1.", "startOffset": 67, "endOffset": 75}, {"referenceID": 20, "context": "The choice of \u03b1t+1 follows Nesterov\u2019s accelerated gradient descent [20, 21] and is detailed in Algorithm 1.", "startOffset": 67, "endOffset": 75}, {"referenceID": 2, "context": "(16) The algorithm has a rate of convergence of O(1/t) [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": ") is given by the elementwise soft-thresholding operation [24] as [ prox \u03bb L\u2016.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "We will focus on the case when R(\u03b4\u0398) = \u2211NG g=1 \u2016\u03b4\u0398(s, t) : s, t \u2208 Gg\u2016F [15].", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "Node perturbation: Another example is the row-column overlap norm (RCON) [18] to capture perturbed nodes i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35].", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35].", "startOffset": 154, "endOffset": 158}, {"referenceID": 17, "context": "\u03b4\u0398 = V + V T , (23) and solve it by applying in-exact ADMM techniques [18].", "startOffset": 70, "endOffset": 74}, {"referenceID": 5, "context": "1 Background and Assumption Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.", "startOffset": 107, "endOffset": 113}, {"referenceID": 7, "context": "1 Background and Assumption Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 10, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 28, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 29, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 0, "context": "[1] show that for any convex loss function the error vector \u2206 = (\u03b4\u03b8\u2217 \u2212 \u03b4\u03b8\u0302) lies in a restricted set that is characterized as Er = Er(\u03b4\u03b8 \u2217, \u03b2) = { \u2206 \u2208 R \u2223\u2223\u2223\u2223 R(\u03b4\u03b8\u2217 + \u2206) \u2264 R(\u03b4\u03b8\u2217) + 1 \u03b2 } .", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].", "startOffset": 227, "endOffset": 234}, {"referenceID": 18, "context": "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].", "startOffset": 227, "endOffset": 234}, {"referenceID": 0, "context": "[1] show a deterministic upper bound for \u2016\u2206\u20162 in terms of \u03bbn1,n2 , \u03ba, and the norm compatibility constant \u03a8(Cr) = supu\u2208Cr R(u) \u2016u\u20162 , as \u2016\u2206\u20162 \u2264 1 + \u03b2 \u03b2 \u03bbn1,n2 \u03ba \u03a8(Cr) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 6, "context": "The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for \u2200\u03b3i \u2208 [0, 1], we have \u03b4L(\u03b4\u03b8\u2217, u) := L(\u03b4\u03b8\u2217 + u)\u2212 L(\u03b4\u03b8\u2217)\u2212 \u3008\u2207L(\u03b4\u03b8\u2217), u\u3009 \u2265 uT\u22072L(\u03b4\u03b8\u2217 + \u03b3iu)u.", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "Our analysis is an extension of the results on [1] using the generic chaining.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "The bound on Gaussian width of the error set for atomic norms has been provided in [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 21, "context": "For indirect approach, we first estimate Ising model structures \u03b8\u03021 and \u03b8\u03022 with L1 norm regularizer, separately [22].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "For the special case of sparsity, we obtain a sharper result than previous results [13] under the same smooth density ratio assumption.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "[13] obtained the same result with a bounded density ratio assumption which is a more restrictive assumption.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "1 Generic Chaining Definition 4 (Majorizing measure [27]) Given \u03b1 > 0, and a metric space (T, d) (that need not be finite), we define \u03b3\u03b1(T, d) = inf sup t \u2211 n\u22650 2\u2206(An(T )).", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "We use the traditional definition of majorizing measure \u03b3\u03b1,1(T, d) from [27] \u03b3\u03b1,1(T, d) = inf sup t (\u222b \u221e", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "Note that \u03b3\u03b1,1(T, d) coincides with the functional \u03b3\u03b1(T, d) [28] as K(\u03b1)\u03b3\u03b1(T, d) \u2264 \u03b3\u03b1,1(T, d) \u2264 K(\u03b1)\u03b3\u03b1(T, d), (46) where K(\u03b1) is a constant depending on \u03b1 only.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "7] in [29] Consider a set T provided with two distances d1 and d2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "9] in [29] Under the conditions of Theorem 4, for all values of u1, u2 > 0 we have P (|Xs \u2212Xt0| \u2265 L(\u03b31(T, d1) + \u03b32(T, d2)) + u1D1 + u2D2) \u2264 L exp(\u2212min(u2, u1)), (51) where Dj = 2 \u2211 n\u22650 en(T, dj).", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "2 (Fernique-Talagrand\u2019s comparison theorem)] in [32] Let T be an arbitrary set.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "Theorem 7 (Mendelson, Pajor, Tomczak-Jaegermann [17]) There exist absolute constants c1, c2, c3 for which the following holds.", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "27 in [30] and we have", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Recall, RSC condition definition as \u03b4L(\u03b4\u03b8\u2217,u) := L(\u03b4\u03b8\u2217 + u;X1 1 ,X n2 2 )\u2212 L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u3008\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 ),u\u3009 \u2265 \u03ba\u2016u\u20162 (118) Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for \u2200\u03b3i \u2208 [0, 1], we have \u03b4L(\u03b4\u03b8\u2217,u) := L(\u03b4\u03b8\u2217 + u;X1 1 ,X n2 2 )\u2212 L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u3008\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 ),u\u3009 \u2265 u\u2207L(\u03b4\u03b8\u0303;X1 1 ,X n2 2 )u, (119) where \u03b4\u03b8\u0303 = \u03b4\u03b8\u2217 + \u03b3iu.", "startOffset": 256, "endOffset": 262}], "year": 2016, "abstractText": "We consider the problem of estimating change in the dependency structure between two p-dimensional Ising models, based on respectively n1 and n2 samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say n2, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as c \u221a min(n1,n2) , where c depends on the Gaussian width of the unit norm ball. For example, for `1 norm applied to s-sparse change, the change can be accurately estimated with min(n1, n2) = O(s log p) which is sharper than an existing result n1 = O(s 2 log p) and n2 = O(n1). Experimental results illustrating the effectiveness of the proposed estimator are presented.", "creator": "LaTeX with hyperref package"}}}