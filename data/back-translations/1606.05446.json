{"id": "1606.05446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Abducing Compliance of Incomplete Event Logs", "abstract": "The ability to store data on the performance of business processes in so-called event logs has led to the proliferation of tools to analyze processes and assess the quality of a process model. Nevertheless, these tools are often very rigid when dealing with event logs that contain incomplete information about the execution process. Thus, while the ability to process incomplete event data is one of the challenges mentioned in the Process Mining Manifesto, assessing the conformity of an execution prosecution still requires complete tracking.", "histories": [["v1", "Fri, 17 Jun 2016 08:30:28 GMT  (342kb,D)", "http://arxiv.org/abs/1606.05446v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["federico chesani", "riccardo de masellis", "chiara di francescomarino", "chiara ghidini", "paola mello", "marco montali", "sergio tessaris"], "accepted": false, "id": "1606.05446"}, "pdf": {"name": "1606.05446.pdf", "metadata": {"source": "CRF", "title": "Abducing Compliance of Incomplete Event Logs", "authors": ["Federico Chesani", "Riccardo De Masellis", "Chiara Di Francescomarino", "Chiara Ghidini", "Paola Mello", "Marco Montali", "Sergio Tessaris"], "emails": ["federico.chesani@unibo.it,", "paola.mello@unibo.it,", "r.demasellis@fbk.eu,", "dfmchiara@fbk.eu,", "ghidini@fbk.eu,", "tessaris@inf.unibz.it", "montali@inf.unibz.it"], "sections": [{"heading": "1 Introduction", "text": "The proliferation of IT systems able to store process executions traces in socalled event logs has originated, in the Business Process community, a quest towards tools that offer the possibility of discovering, checking the conformance and enhancing process models based on actual behaviors [1]. Focusing on conformance, that is, on a scenario where the aim is to assess how a prescriptive (or \u201cde jure\u201d) process model relates to the execution traces, a fundamental notion os the one of trace compliance. Compliance results can be used by business analysts to assess the goodness of a process model and understand how it relates to the actual behaviours exhibited by a company, consequently providing the basis for process re-design, governance and improvement.\nThe use of event logs to evaluate the goodness of a process model becomes hard and potentially misleading when the event log contains only partial information on the process execution. Thus, while the presence of non-monitorable activities (or errors in the logging procedure) makes the ability of handling incomplete event data one of the main challenges of the BP community, as mentioned in the process mining manifesto[1], still trace compliance of an execution\nar X\niv :1\n60 6.\n05 44\n6v 1\n[ cs\n.A I]\n1 7\nJu n\ntrace requires the presence of a complete end-to-end execution trace to be evaluated. Notable exceptions are [2,3] where trace incompleteness is managed in an algorithmic/heuristic manner using log repair techniques.\nIn this paper, we take an orthogonal approach and throughly address the problem of log incompleteness from a formal/logic-based point of view, adopting an approach based on abduction [4]. Differently from techniques that focus on algorithmic/heuristic repairs of an incomplete trace, we are interested in characterising the notion of incomplete log compliance by means of a sound and complete inference procedure. We rely on abduction to combine the partial knowledge about the real executions of a process as reflected by a (potentially) incomplete event log, with the background knowledge captured in a process model. In particular, abductive reasoning handles different forms of missing information by formulating hypotheses that explain how the event log may be \u201ccompleted\u201d with the missing information, so as to reconcile it with the process model. This leads us to refine the classical notion of conformance-by-alignment [5] between an execution trace and a process model into strong and conditional compliance, to account for incompleteness. In detail, the paper provides: (i) a classification of different forms of incompleteness of an event log based on three dimensions: log incompleteness, trace incompleteness, and event description incompleteness (Section 2.1); (ii) a reformulation of the notion of compliance into strong and conditional compliance (Section 2.2); (iii) an encoding of structured process models4 and of event logs in the SCIFF abductive logic framework [7], and a usage of the SCIFF abductive proof procedure to compute strong, conditional and non- compliance in presence of an incomplete event log (Section 3); and (iv) an evaluation of the proposed framework in an experimental setting (Section 4). The ideas presented in the paper paper are illustrated by means of a simple explanatory example, and the comparison with related work is contained in Section 5."}, {"heading": "2 Dealing with Incomplete Event Logs", "text": "We aim at solving solve the problem of identifying compliant traces in the presence of incomplete event logs, given the prescriptive knowledge contained in a process model. To do this, we first need to investigate what incomplete event logs are (Section 2.1) and then understand how we can adapt the notion of compliance to deal with partial data on the process execution (Section 2.2). We perform this investigation with the help of a simple example, which in this paper is described using the BPMN language5.\n4 We focus on structured process models in the spirit of [6]. Broadly speaking, this restricts to the class of models recursively composed of single-entry-single-exit blocks, where every split has a corresponding join, matching its type. This assumption rules out pathological patterns that are notoriously hard to characterise (e.g. involving nested OR joins), still providing coverage for a wide range of interesting use cases. 5 For the sake of clarity we use BPMN, but our framework is language-independent.\nExample 1 (Obtaining a Permit of Stay in Italy). Consider the BPMN process in Figure 1, hereafter called the Permit-Of-Stay (POS) process, which takes inspiration from the procedure for the granting of a permit of stay in Italy. Upon her arrival in Italy (AI), the person in need of a permit of stay has three different alternatives: if she is from a EU country and remains in Italy for at most 30 days, then only indicating her presence in Italy (DP) is needed; if she is from the EU and must remain in Italy for more than 30 days, then she needs to get an identity certificate (GIT) and present it (PIC). In all the remaining cases she needs to fill a documentation (FD) which is then checked (CD). When the documentation is correct, it is presented (PD) and a certificate is received (RC). The procedure concludes with the provision of the permit of stay (SI). Note that, for the sake of simplicity, the process only focuses on the so-called \u201chappy paths\u201d, that is, the successful issuing of a permit of stay."}, {"heading": "2.1 Classifying Process Execution (In)Completeness", "text": "We assume that each execution of the POS process in Figure 1 is (partially) monitored and logged by an information system. We also assume that activities are atomic, i.e., executing an activity results in an event associated to a single timestamp: event (A, t) indicates that activity A has been executed at time t. A sample trace6 that logs the execution of a POS instance is:\n{(AI, t1), (FD, t2), (CD, t3), (PD, t4), (RC, t5), (SI, t6)} (1)\nwhere ti > tj for i, j \u2208 {1, . . . , 6} such that i > j. This trace corresponds to the execution of the lower branch of the POC process, where the loop is never executed. A set of execution traces of the same process form an event log.\nIn many real cases, a number of difficulties may arise when exploiting the data contained in an information system in order to build an event log. For instance, data may bring only partial information in terms of which process activities have been executed and what data or artefacts they produced. Thus, instead of the extremely informative trace reported in (1), we may obtain something like:\n{(FD, ), ( , t2), (SI, t6), ( , )} (2) 6 We often present the events in a trace ordered according to their execution time.\nThis is only to enhance readability since the position of an event is fully determined by its timestamp, or unknown if the timestamp is missing.\nThis trace does not completely describe an execution of the POS process. For example, the first event logged in the trace is FD. However, by looking at the process description, it is easy to see that the first event of every execution has to be AI. By assuming that the process executors indeed followed the prescriptions of the model, this suggests that the AI-related event has not been logged. Moreover, certain events have been only partially observed. For example, the FD-related event is incomplete, because its exact timestamp is unkown. In this paper, we use \u201c \u201d to denote a missing information unit.\nIn accordance with the IEEE standard XES format for representing event logs [8], in general we can describe an event log as a set of execution traces. Each trace, in turn, contains events, which are described by means of n-tuples, where each element of the tuple is the value of a given attribute (see Figure 2a, where we restrict to two attributes as we do in the paper). Consequently, we can classify incompleteness along these three dimensions: incompleteness of the log, incompleteness of the trace, and incompleteness of the event description (see Figure 2b).\n(In)Completeness of the log. Within this dimension we analyse whether all the traces envisaged by the model are in the log or not. That is, we focus on understanding whether the log contains at least one instance for each possible execution that is allowed in the model. Note that one can account for this form of (in)completeness only by: (a) limiting the analysis to the control flow, without considering complex data objects that may contain values from an unbounded domain; and (b) assuming that there is a maximum length for all traces, thus limiting the overall number of traces that may originate from the unbounded execution of loops. An example of complete log for the POS process is:\nL1 = {(AI, ta1), (DP, ta2), (SI, ta3)},{(AI, tb1), (GIC, tb2), (PIC, tb3)(SI, tb4)},{(AI, tc1), (FD, tc2), (CD, tc3), (PD, tc4), (RC, tc5), (SI, tc6)}}  (3)\nwhere we assume that each trace cannot contain more than 6 event, which intuitively means that the loop is never executed twice.\nAssuming this form of strict completeness is often unrealistic in practice. In fact, even under the assumption of a maximum trace length, the number of allowed traces could become extremely huge due to (bounded) loops, and the (conditional) interleavings generated by parallel blocks and or choices. Still, analysing the (in)completeness of an event log may be useful to discover parts of the control flow that never occur in practice.\n(In)completeness of the trace. Within this dimension we focus on a single trace, examining whether it contains a sequence of events that corresponds to an execution foreseen by the process model from start to end. Trace (1) is an example of complete trace. An example of incomplete trace is:\n{(AI, t1), (PIC, t2)(SI, t3)} (4)\nBy looking at the POS model, it is easy to see that this trace should also contain an event of the form (GIC, t), s.t. t1 < t < t2.\n(In)completeness of the event description. Within this dimension we focus on the completeness of a single event. Events are usually described as complex objects containing data about the executed activity, its time stamp, and so on [8]. These data can be missing or corrupted. As pointed out before, we consider activity names and timestamps. Thus, incompleteness in the event description may concern the activity name, its timestamp, or both. This is reflected in trace (2): (i) event (FD, ) indicates that activity FD has been executed, but at an unknown time; (ii) ( , t2) witnesses that an activity has been executed at time t2, but we do not know which; (iii) ( , ) attests that the trace contains some event, whose activity and time are unknown.\nIn general, we can characterise the (in)completeness of an event log in terms of (any) combination of these three basic forms. At one extreme, we may encounter a log that is complete along all three dimensions, such as the one depicted in (3). At the other extreme, we may instead have the following log:\nL2 = {{(AI, ), ( , ta2)}, {(AI, tb1), ( , ), ( , tb2), (SI, tb3)}} (5)\ncharacterised by incompleteness of the log, incompleteness of some traces, and incompleteness of some event descriptions. Intermediate situations may obviously arise as well. This is graphically depicted in the lattice of Figure 2c, where \u3008L, T,E\u3009 indicates the top value (completeness for all three dimensions) and \u3008\u2022, \u2022, \u2022\u3009 indicated the bottom value (incompleteness of all three dimensions)."}, {"heading": "2.2 Refining the Notion of Compliance", "text": "In our work we consider prescriptive process models, that is, models that describe the only acceptable executions. These corresponds to the so-called \u201cde jure\u201d models in [5], and consequently call for a definition of compliance, so as to characterise the degree to which a given trace conforms/is aligned to the\nmodel. The traditional notion of compliance is typically considered under the assumption that the trace is a faithful footprint of reality, and requires that the trace represents an end-to-end, valid execution that can be fully replayed on the process model. We call this notion of compliance strong compliance. Trace (1) is an example of trace that is fully compliant to the POS process.\nStrong compliance is too restrictive when the trace is possibly incomplete. In fact, the incompleteness in a trace hinders the possibility of replaying it on the process model. However, full conformance might be regained by assuming that the trace included additional activities and/or specific information on the missing data; in this case we say that the trace is conditionally compliant, to reflect that compliance conditionally depends on how the partial information contained in the trace is complemented with the missing one. Consider again the POS example and the partial trace:\n{(AI, t1), (GIC, )(SI, t3)} (6)\nIt is easy to see that the observed trace is compliant with POS, if we assume that\nGIC was executed at a time ti s.t. t1 < ti < t3 (7)\nan execution of PIC was performed at a time tj s.t. ti < tj < t3 (8)\nNote that the set of assumptions needed to reconstruct full conformance is not necessarily unique. This reflects that, in general, alternative strongly compliant real process executions might have led to the recorded partial trace. On the other hand, there are situations in which it is not possible to formulate additional assumptions on the partial trace to recover full conformance. In this case, the partial trace is considered non-compliant. For example, trace\n{(AI, t1), (GIC, )(CD, t2)(SI, t3)} (9)\ndoes not comply with POS, since it records that GIC and CD have been both executed, although they belong to mutually exclusive branches in the model."}, {"heading": "3 Abduction and Incomplete Logs", "text": "Since the aim of this paper is to provide automatic procedures, embedded in a tool, that identify compliant traces in the presence of incomplete event logs, given the prescriptive knowledge contained in a process model, we can schematise the input to our problem in three parts: (i) an instance-independent component, the process model, which in this paper is described using BPMN; (ii) an instancespecific component, that is, the (partial) log, and (iii) meta-information attached to the activities in the process model, indicating which are actually always, never or possibly observable (that is, logged) in the event log. The third component is an extension of a typical business process specification that we propose (following and extending the approach described in [9]) to provide prescriptive information\nabout the (non-) observability of activities. Thus, for instance, a business analyst will have the possibility to specify that a certain manual activity is never observable while a certain interaction with a web site is always (or possibly) observable. This information can then be used to compute the compliance of a partial trace. In fact the presence of never observable activities will trigger the need to make hypothesis on their execution (as they will never be logged in the event log), while the presence of always observable activities will trigger the need to find their corresponding event in the execution trace (to retain compliance). Note that this extension is not invasive w.r.t. current approaches to business process modelling, as we can always assume that a model where no information on observability is provided is entirely possibly observable.\nGiven the input of our problem, we structure this section as follows: first, we provide an overview on abduction and on how the SCIFF framework represents always, never or possibly observable activities; then, we show how to use SCIFF to encode a process model and a partial log (Section 3.2), third we show how we can formalize, and therefore make precise, the informal different forms of compliance presented in Section 2.2 (Section 3.3); finally, we illustrate how the SCIFF proof procedure can be used to solve the different forms of incompleteness identified in Section 2.1 (Section 3.4)."}, {"heading": "3.1 The SCIFF in short", "text": "Abduction is a non-monotonic reasoning process where hypotheses are made to explain observed facts [10]. While deductive reasoning focuses on deciding if a formula \u03c6 logically follows from a set \u0393 of logical assertions known to hold, in abductive reasoning it is assumed that \u03c6 holds (as it corresponds to a set of observed facts) but it cannot be directly inferred by \u0393 . To make \u03c6 a consequence of \u0393 , abduction looks for a further set \u2206 of hypothesis, taken from a given set of abducible A, which complements \u0393 in such a way that \u03c6 can be inferred (in symbols \u0393\u222a\u2206 |= \u03c6). The set \u2206 is called abductive explanation (of \u03c6). In addition, \u2206 must usually satisfy a set of (domain-dependent) integrity constraints IC (in symbols, \u0393 \u222a \u2206 |= IC). A typical integrity constraint (IC) is a denial, which expresses that two explanations are mutually exclusive.\nAbduction has been introduced in Logic Programming in [4]. There, an Abductive Logic Program (ALP) is defined as a triple \u3008\u0393,A, IC\u3009, where: (i) \u0393 is a logic program, (ii) A is a set of abducible predicates, and (iii) IC a set of ICs. Given a goal \u03c6, abductive reasoning looks for a set of literals \u2206 \u2286 A such that they entail \u03c6 \u222a IC.\nIn this paper we leverage on the SCIFF abductive logic programming framework [7], an extension of the IFF abductive proof procedure [11]. Beside the general notion of abducible, the SCIFF framework has been enriched with the notions of happened event, expectation, and compliance of an observed execution with a set of expectations. This makes SCIFF suitable for dealing with event log incompleteness. Let a be an event corresponding to the execution of a process ac-\ntivity, and T (possibly with subscripts) its execution time7. Abducibles are used here to make hypothesis on events that are not recorded in the examined trace. They are denoted using ABD(a, T ). Happened events are non-abducible, and account for events that have been logged in the trace. They are denoted with H(a, T ). Expectations E(a, T ), instead, model events that should occur (and therefore should be present in a trace). Compliance is described in Section 3.3.\nICs in SCIFF are used to relate happened events / abduced predicates with expectations / predicates to be abduced. Specifically, an IC is a rule of the form body \u2192 head, where body contains a conjunction of happened events, general abducibles, and defined predicates, while head contains a disjunction of conjunctions of expectations, general abducibles, and defined predicates."}, {"heading": "3.2 Encoding Structured Processes and Their Executions in SCIFF", "text": "Let us illustrate how to encode all the different components of an (incomplete) event log and a structured process model one by one.\nEvent Log. A log is a set of traces, each constituted by a set of observed (atomic) events. Thus trace (4) is represented in SCIFF as {H(AI, t1),H(PIC, t2),H(SI, t3)}. Always/never observable activities. Coherently with the representation of an execution trace, the logging of the execution of an observable activity is represented in SCIFF using an happened event, whereas the hypothesis on the execution of a never observable activity is represented using an abducible ABD (see Figure 3a). Given an event a occurring at T , we use a function \u03c4 that represents the execution of a as:\n\u03c4(a, T ) =\n{ H(a, T ) if a is observable\nABD(a, T ) if a is never observable\nAs for expected occurrences, the encoding again depends on the observability of the activity: if the activity is observable, then its expected occurrence is mapped to a SCIFF expectation; otherwise, it is hypothesised using the aforementioned\n7 In the remainder of this paper we will assume that the time domain relies on natural numbers.\nabducible ABD (see Figure 3b). To this end we use a function \u03b5 that maps the expecting of the execution of a at time T as follows:\n\u03b5(a, T ) =\n{ E(a, T ) if a is observable\nABD(a, T ) if a is never observable\nStructured process model constructs. A process model is encoded in SCIFF by generating ICs that relate the execution of an activity to the future, expected executions of further activities. In practice, each process model construct is transformed into a corresponding IC. We handle, case-by-case, all the single-entry single-exit block types of structured process models.\nSequence. Two activities a and b are in sequence if, whenever the first is executed, the second is expected to be executed at a later time:\n\u03c4(a, Ta)\u2192 \u03b5(b, Tb) \u2227 Tb > Ta. (10)\nXor-split extends sequence with the possibility of selecting one among multiple target activities. In SCIFF, this is captured using an IC with a disjunction in the consequent. In particular, the fact that if a is executed, then either b or c is expected to be executed next is encoded as:\n\u03c4(a, Ta)\u2192 \u03b5(b, Tb) \u2227 Tb > Ta \u2228 \u03b5(c, Tc) \u2227 Tc > Ta.\nXor-join indicates that, no matter which activity is executed among the input set of activities for the xor-join, then the output activity of the xor-join is expected to be executed. Hence, the encoding of xor-join can be obtained by encoding all its implied sequences. For example, if a or b are followed by c, we obtain:\n\u03c4(a, Ta)\u2192 \u03b5(c, Tc) \u2227 Tb > Ta. \u03c4(b, Tb)\u2192 \u03b5(c, Tc) \u2227 Tc > Tb. And-split activates parallel threads spanning from the same activity. In particular, the fact that activity a triggers two parallel threads, one expecting the execution of b, and the other that of c, is captured using an IC with a conjunctive consequent:\n\u03c4(a, Ta)\u2192 \u03b5(b, Tb) \u2227 Tb > Ta \u2227 \u03b5(c, Tc) \u2227 Tc > Ta. And-join mirrors the and-split, synchronizing multiple concurrent execution threads and merging them into a single thread. When activities a and b are both executed, then activity c is expected next, is captured using an IC with a conjunctive antecedent:\n\u03c4(a, Ta) \u2227 \u03c4(b, Tb)\u2192 \u03b5(c, Tc) \u2227 Tc > Ta \u2227 Tc > Tb. Or-split/or-join are captured by combining the formalization of and-/xorelements, considering the well-known equivalence between an or-split/-join and an xor-split/-join whose alternative branches correspond to an element in the powerset of the split-targets/join-sources, whose inner activities are put in parallel. For example, the fact that a leads to an or-split pointing to b and c is equivalent to an xor-split that connects a to three outputs: one containing b, one containing c, and one containing b and c in parallel.\nPossibly observable activities. A possibly observable activity is managed by considering the disjunctive combination of two cases: one in which it is assumed to be observable, and one in which it is assumed to be never observable. This idea is used to refine ICs used to encode the workflow constructs in the case of partial observability. For instance, if a partially observable activity appears in the antecedent of an IC, two distinct ICs are generated, one where the activity is considered to be observable (H), and another in which it is not (ABD). Thus in the case of a sequence flow from a to b, where a is possibly observable and b is observable, IC (10) generates:\nH(a, Ta) \u2192 \u03b5(b, Tb) \u2227 Tb > Ta. ABD(a, Ta) \u2192 \u03b5(b, Tb) \u2227 Tb > Ta.\nIf multiple partially observable activities would appear in the antecedent of an IC (as, e.g., in the and-join case), then all combinations have to be considered.\nSimilarly, if a partially observable activity appears in the consequent of an IC, a disjunction must be inserted in the consequent, accounting for the two possibilities of observable/never observable event. If both the antecedent and consequent of an IC would contain a partially observable activity, a combination of the rules above will be used. For example, in the case of a sequence flow from a to b, where b is possibly observable, IC (10) generates:\nH(a, Ta)\u2192 E(b, Tb) \u2227 Tb > Ta \u2228ABD(b, Tb) \u2227 Tb > Ta.\nWith this encoding, the SCIFF proof procedure generates firstly an abductive explanation \u2206 containing an expectation about the execution of b. If no b is actually observed, \u2206 is discarded, and a new abductive explanation \u2206\u2032 is generated containing the hypothesis about b (i.e., ABD(b, Tb) \u2208 \u2206\u2032). Mutual exclusion between these two possibilities is guaranteed by the SCIFF declarative semantics (cf. Definition 3).\nFinally, if both the antecedent and consequent of an IC would contain a possibly observable activity, a combination of the rules above will be used."}, {"heading": "3.3 Compliance in SCIFF: Declarative Semantics", "text": "We are now ready to provide a formal notion of compliance in its different forms. We do so by extending the SCIFF declarative semantics provided in [7] to incorporate log incompleteness (that is, observability features).\nA structured process model corresponds to a SCIFF specification S = \u3008KB,A, IC\u3009, where: (i) KB is a Logic Program [12] containing the definition of accessory predicates; (ii) A = {ABD/2,E/2}; (iii) IC is a set of ICs constructed by following the encoding defined in Section 3.2. A (execution) trace and an abductive explanation \u2206 are defined as follows:8:\nDefinition 1. A Trace T is a set of terms of type H(e, Ti), where e is a term describing the happened event, and Ti \u2208 N is the time instant at which the event occurred. 8 We do not consider the abductive goal, as it is not needed for our treatment.\nDefinition 2 (Abductive explanation \u2206). Given a SCIFF specification S and a trace T , a set \u2206 \u2286 A is an abductive explanation for \u3008S, T \u3009 if and only if\nComp (KB \u222a T \u222a\u2206) \u222a CET\u222aTN |= IC\nwhere Comp is the (two-valued) completion of a theory [13], CET stands for Clark Equational Theory [14] and TN is the CLP constraint theory [15] for integers.\nThe following definition fixes the semantics for observable events, and provides the basis for understanding the alignment of a trace with a process model.\nDefinition 3 (T -Fulfillment). Given a trace T , an abducible set \u2206 is T - fulfilled if for every event e \u2208 \u2206 and for each time t, E(e, t) \u2208 \u2206 if and only if H(e, t) \u2208 T .\nThe \u201conly if\u201d direction defines the semantics of expectation, indicating that an expectation is fulfilled when it finds the corresponding happening event in the trace. The \u201cif\u201d direction captures the prescriptive nature of process models, whose closed nature require that only expected event may happen.\nGiven an abductive explanation \u2206, fulfilment acts as a compliance classifier, which separates the legal/correct execution traces with respect to \u2206 from the wrong ones.\nDefinition 4 ((Strong/Conditional) Compliance). A trace T is compliant with a SCIFF specification S if there exists an abducible set \u2206 such that: (i) \u2206 is an abductive explanation for \u3008S, T \u3009, and (ii) \u2206 is T -fulfilled. If \u2206 does not contains any ABD then we say that it is strongly-compliant, otherwise it is conditionally-compliant.\nIf no abductive explanation that is also T -fulfilled can be found, then T is not compliant with the specification of interest. Contrariwise, the abductive explanation witnesses compliance. However, it may contain ABD predicates, abduced due to the incompleteness of T . In fact, the presence or absence of such predicates determines whether T is conditionally or strongly compliant. To make an example of how Definition 4 help us solve the problem of compliance of a single trace, let us consider traces (6), (1) and (9), of the POS example. In the case of partial trace (6), SCIFF will tell us that it is conditional compliant with the workflow model POS since \u2206 will contain the formal encoding of the two abducibles (7) and (8) which provide the abductive explanation of trace (6). In the case of (complete) trace (1), abduction will tell us that it directly follows from \u0393 without the need of any hypothesis. The case \u2206 = \u2205 coincides in fact, with the classical notion of (deductive) compliance. Finally, if we consider the partial trace (9) SCIFF will tell us that it is not possible to find any set of hypothesis \u2206 that explains it. The case of no \u2206 coincides, therefore with the classical notion of (deductive) non-compliance.\nWe close this section by briefly arguing that our approach is indeed correct. To show correctness, one may proceed in two steps: (i) prove the semantic\ncorrectness of the encoding w.r.t. semantics of (conditional/strong) compliance; (ii) prove the correctness of the proof procedure w.r.t. the SCIFF declarative semantics. Step (i) requires to prove that a trace is (conditionally/strong) compliant (in the original execution semantics of the workflow) with a given workflow if and only if the trace is (conditionally/strong) compliant (according to the SCIFF declarative semantics) with the encoding of the workflow in SCIFF. This can be done in the spirit of [16] (where correctness is proven for declarative, constraint-based processes), by arguing that structured processes can be seen as declarative processes that only employ the \u201cchain-response constraint\u201d [16]. For step (ii), we rely on [7], where t soundness and completeness of SCIFF w.r.t. its declarative semantics is proved by addressing the case of closed workflow models (the trace is closed and no more events can happen anymore), as well as that of open workflow models (future events can still happen). Our declarative semantics restricts the notions of fulfilment and compliance to a specific current time tc, i.e., to open traces: hence soundness and completeness still hold."}, {"heading": "3.4 Dealing with Process Execution (In)Completeness in SCIFF", "text": "We have already illustrated, by means of the POS example, how Definition 4 can be used, at a very abstract level, to address compliance of a partial trace. In this section we illustrate more in detail how SCIFF can be used to solve the three domensions of incompleteness identified in Section 2.1.\nTrace and event incompleteness are dealt by with SCIFF in a uniform manner. In fact, the trace/event incompleteness problem amounts to check if a given log (possibly equipped with incomplete traces/events), is compliant with a prescriptive process model. We consider as input the process model, together with information about the observability of its activities, a trace, and a maximum length for completed traces. The compliance is determined by executing the SCIFF proof procedure and evaluating possible abductive answers. We proceed as follows:\n1. We automatically translate the process model with its observability metainformation into a SCIFF specification. If observability information is missing for some/all the activities, we can safely assume that some/all activities are possibly observable.\n2. The SCIFF proof procedure is applied to the SCIFF specification and to the trace under observation, computing all the possible abductive answers \u2206i. The maximum trace length information is used to limit the search, as in the unrestricted case the presence of loop may lead to nontermination.\n3. If no abductive answer is generated, the trace is deemed as non-compliant. Otherwise, a set of abductive answers {\u22061, . . . ,\u2206n} has been found. If there exists a \u2206i that does not contain any ABD predicate, then the trace is strongly compliant. The trace is conditionally compliant otherwise.\nNote that, assessing strong/conditional compliance requires the computation of all the abductive answers, thus affecting the performances of the SCIFF proof\nprocedure. If only compliance is needed (without classifying it in strong or conditional), it is possible to compute only the first solution.\nA different scenario is provided by the log incompleteness problem, which instead focuses on an entire event log, and looks if some possible traces allowed by the model are indeed missing in the log. In this case we consider as input the process model, a maximum length for the completed traces, and a log consisting of a number of different traces; we assume each trace is trace- and event-complete. We proceed as follows:\n1. We generate the SCIFF specification from the process model, considering all activities as never observable. 2. The SCIFF proof procedure is applied to the SCIFF specification. All the possible abductive answers \u2206i are computed, with maximum trace length as specified. Each answer corresponds to a different execution instance allowed by the model. Since all the activities are never observable, the generated \u2206i will contain only ABD. 3. For each hypothesised trace in the set {\u22061, . . . ,\u2206n}, a corresponding, distinct trace is looked for in the log. If all the hypothesised traces have a distinct matching observed trace, then the log is deemed as complete.\nNotice that, beside the completeness of the log, the proof procedure also generate the missing traces, defined as the \u2206i that do not have a corresponding trace in the log."}, {"heading": "4 Evaluation", "text": "Section 3.4 illustrates how the eight problems obtained by combining the three incompleteness dimensions can be actually solved by means of two algorithms. We now test such algorithms and study how different inputs affect their performances. As special input, we indicate whether SCIFF must compute all possible abductive explanations, or a simple yes/no answer to the compliance decision problem suffices (the latter can be answered affirmatively by stopping after having found the first abductive explanation).\nFor each type of incompleteness we consider, possibly only some input parameters are of interest, as, for instance, the information on the observability of activities does not impact the log incompleteness resolution (being each trace in the log assumed complete). Hence, for each problem we select the significant parameters only and perform tests by varying them in order to thoroughly understand their practical influence.\nAs for the model, we choose a real-life process made available within the Process Matching Contest 2013 [17], describing the admission procedure to the\nFrankfurt University. Notably, in order to exercise the encoding on various process elements, a parallel branch and a loop have been added to the original procedure. Figure 4 shows the resulting model9, which is composed of 29 activities, 3 xor-splits/joins, and 1 and-split/join. If no loop iteration is considered, the model contains 8 distinct paths. The experiments have been carried out on a Windows 7 pc with 8GB RAM and a 2.4 GhZ Intel-core i7.\nLog incompleteness. We evaluated the algorithm by varying the number of (complete) traces in the log and the bound on the length of the traces. Results shows that the number of solutions is proportional to the trace max length parameter and inversely proportional to the number of traces already presented in the log. Computing times are below 1 sec, and proportional to the number of solutions returned.\nTrace/event incompleteness. We first feed the algorithm with a partial trace and complete events, thus testing the trace incompleteness problem. Table 1 is used to summarize the numerical values obtained, where each row represents a test case and columns report the input values used, the output and the computing time. As for the activity observability, we chose as parameter the percentage of activities that we know to be always/never observable (%AOA) and let it ranging among 0% (no certain information about activity observability is available at all), around 15% ( observability is known with certainty only for a small number of activities) and 50% (for about half of the activities of the diagram we know whether they are for sure observable or not). Concerning the trace, we look at\n9 The model is included for providing an intuition of its complexity and no description is provided.\nthe number of observed events in the trace (#OE). Also in this case, we chose to make the parameter varying among a trace almost incomplete (1), with a small number of observed events (5) and with a medium number of observed events (15). Finally, we let the trace maximum length (TML) ranging among the values 16 (shortest path without loop) and 32 (up to 2 loops).\nThe table shows that the computation time when all possible completions are returned (CT) is proportional to the bound on the length of the traces (TML) and the number of completions found (#sol). Indeed, when one or more solutions are found, the computation time for the compliance decision problem (CcT ) is roughly CT/#sol. On the contrary, the percentage of always observed activities in the model (%AOA) significantly reduces the exploration space, thus linearly decreasing the computation time. Interestingly, the more events we observe in the trace (column #OE) the higher the computation time is. This is because, as explained in Subsection 3.4, for each expectation about a (possibly observable) activity in the model which can potentially match (unify with) an event observed in the trace, two cases must be explored: either (i) the expectation is matched with the event in the trace, or (ii) an abductive explanation is generated for that activity. We remark that the presence of loops in the model requires both the alternatives to be explored, if we want to guarantee that all completions are returned.\nThe rightmost part of Table 1 shows the results obtained by feeding the procedure with an incomplete trace containing an incomplete event. Here, the same considerations of the trace incompleteness test described above can be drawn. The only difference lies in the fact that a high number of incomplete events (i.e., events lacking their description and, in particular, their name) let the computation time rise exponentially, as multiple possibilities for each incomplete event must be explored when looking for the set of all possible completions. Again, this is the price we pay to get the completeness of the results, which however can (in general) be avoided by asking for compliance only, as displayed by column CnT in Table 1.\nWe close the section by analyzing event incompleteness, that is a particular case of the trace and event incompleteness described above. In this case, the incompleteness is on the event description only (see Section 2.1), hence an input trace is characterized by a number of observed events equals to the length of the trace, and, among these, a number of missing event descriptions. From the computational viewpoint, this represents the most challenging setting, as both the above parameters cause an exponential increase in the computation times. We reckoned that for an input trace of length 16, even two missed event description brings the computation time up to a couple of hours."}, {"heading": "4.1 Discussion", "text": "The purpose of the experimentation was to stress the algorithm in borderline cases. Indeed, we remark that on the one hand we made the model convoluted on purpose especially by adding a loop, which is a source of complexity, and on the other we tested situations that are very unlikely to happen in practice. We\ncan safely assume that in typical scenarios the number of (partially) observable activities, which in most of the cases are human-performed, is usually no more than half of the overall activities (i.e., %AOA > 50%) and that from the %AOA parameter and the length of the partial trace, a good estimate of the bound on the trace length can be set, thus avoiding useless loop (when present) iterations. In such settings the performance of the abductive procedure on the different types of incompleteness are reasonable. For instance, for the compliance, they range from few seconds when at most a single event description is completely unknown to about 4.5 minutes when up to 4 event descriptions are missing."}, {"heading": "5 Related Work", "text": "The problem of incomplete traces has been tackled by a number of works in the field of process discovery and conformance. Some of them [2,3] have addressed the problem of aligning event logs and procedural/declarative process models [2,3]. Such works explore the search space of the set of possible moves to find the best one for aligning the log to the model. Our purpose is not managing generic misalignments between models and logs, but rather focus on a special type of incompleteness: the model is correct and the log could be incomplete.\nWe can divide existing works that aim at constructing possible model-compliant \u201cworlds\u201d out of a set of observations with incomplete information in two groups: quantitative and qualitative approaches. The former rely on the availability of a probabilistic model of execution and knowledge. For example, in [18] the authors exploit stochastic Petri nets and Bayesian Networks to recover missing information. The latter stand on the idea of describing \u201cpossible outcomes\u201d regardless of likelihood. Among these approaches, the issue of reconstructing missing information has been tackled in [19] and in [9], respectively leveraging Satisfiability Modulo Theory, and planning techniques.\nIn this work, the notion of incompleteness has been investigated and extended to take into account its different variants (log incompleteness, trace incompleteness and event incompleteness). Similarly, the concept of observability has been deeper investigated and extended, by exploring the case of activities always, partially or never observable. This has led to a novel classification of different \u201cdegrees\u201d of compliance.\nAbduction and the SCIFF framework have been previously used to model both procedural and declarative processes. In [20], a structured workflow language has been defined, with a formal semantics in SCIFF. In [21], SCIFF has been exploited to formalize and reason about the declarative workflow language Declare.\nAn interesting work where trace compliance is evaluated through abduction is presented in [22]. However, they define compliance as assessing if actions were executed by users with the right permissions (auditing), and focus only on incomplete traces (with complete events), while we take a more sophisticated approach to incompleteness. The adopted abductive framework, CIFF [23], only supports ground abducibles, and ICs are limited to denials. The work in [22] explores also the dimension of human confirmation of hypotheses, and proposes\na human-based refinement cycle. This is a complementary step with our work, and would be an interesting direction for future work."}, {"heading": "6 Conclusions", "text": "We have presented an abductive framework to support business process monitoring (and in particular compliance checking) by attacking the different forms of incompleteness that may be present in an event log. Concerning future development, the SCIFF framework is based on first-order logic, thus paving the way towards (i) the incorporation of data [24], (ii) extensions to further types of workflows (e.g., temporal workflows as in [25]), and (iii) towards the investigation of probabilistic models to deal with incompleteness of knowledge."}, {"heading": "18. Rogge-Solti, A., Mans, R.S., van der Aalst, W.M., Weske, M.: Improving documentation by repairing event logs. In: Proc. of PoEM. Springer (2013)", "text": ""}, {"heading": "19. Bertoli, P., Di Francescomarino, C., Dragoni, M., Ghidini, C.: Reasoning-based techniques for dealing with incomplete business process execution traces. In: Proc. of AI*IA, Springer (2013)", "text": "20. Chesani, F., Mello, P., Montali, M., Storari, S.: Testing careflow process execution conformance by translating a graphical language to computational logic. In: Proc. of AIME. (2007) 21. Montali, M., Pesic, M., van der Aalst, W.M.P., Chesani, F., Mello, P., Storari, S.: Declarative specification and verification of service choreographiess. TWEB 4 (2010) 22. Mian, U.S., den Hartog, J., S. Etalle, N.Z.: Auditing with incomplete logs. In: Proc. of HotSpot. (2015) 23. Mancarella, P., Terreni, G., Sadri, F., Toni, F., Endriss, U.: The CIFF proof procedure for abductive logic programming with constraints: Theory, implementation and experiments. TPLP 9 (2009) 24. De Masellis, R., Maggi, F.M., Montali, M.: Monitoring data-aware business constraints with finite state automata. In: Proc. of ICSSP, ACM Press (2014) 25. Kumar, A., Sabbella, S., Barton, R.: Managing controlled violation of temporal process constraints. In: Business Process Management. Volume 9253 of LNCS. Springer (2015) 280\u2013296"}], "references": [{"title": "Process mining manifesto", "author": ["van der Aalst", "W.M.P"], "venue": "BPM Workshops, Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Conformance checking using cost-based fitness analysis", "author": ["A. Adriansyah", "B.F. van Dongen", "W.M.P. van der Aalst"], "venue": "Proc. of EDOC, IEEE Computer Society", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Aligning event logs and declarative process models for conformance checking", "author": ["M. De Leoni", "F.M. Maggi", "W.M.P. van der Aalst"], "venue": "Proc. of BPM, Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Abductive logic programming", "author": ["A.C. Kakas", "R.A. Kowalski", "F. Toni"], "venue": "J. Log. Comput. 2", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Process Mining - Discovery, Conformance and Enhancement of Business Processes", "author": ["W.M.P. van der Aalst"], "venue": "Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "On structured workflow modelling", "author": ["B. Kiepuszewski", "A.H.M. ter Hofstede", "C.J. Bussler"], "venue": "Seminal Contributions to Information Systems Engineering. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Verifiable agent interaction in abductive logic programming: The SCIFF framework", "author": ["M. Alberti", "F. Chesani", "M. Gavanelli", "E. Lamma", "P. Mello", "P. Torroni"], "venue": "ACM Trans. Comput. Log. 9", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "XES Standard Definition", "author": ["I.T.F. on Process Mining"], "venue": "http://www.xesstandard.org/", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Completing workflow traces using action languages", "author": ["C. Di Francescomarino", "C. Ghidini", "S. Tessaris", "I.V. Sandoval"], "venue": "Proc. of CAiSE, Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Abduction and abductive logic programming", "author": ["A.C. Kakas", "P. Mancarella"], "venue": "Proc. of ICLP.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "The iff proof procedure for abductive logic programming", "author": ["T.H. Fung", "R.A. Kowalski"], "venue": "J. Log. Program. 33", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Foundations of Logic Programming, 2nd Edition", "author": ["J.W. Lloyd"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1987}, {"title": "Negation in logic programming", "author": ["K. Kunen"], "venue": "J. Log. Program. 4", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "Negation as Failure", "author": ["K.L. Clark"], "venue": "Logic and Data Bases. Plenum Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "The semantics of constraint logic programs", "author": ["J. Jaffar", "M.J. Maher", "K. Marriott", "P.J. Stuckey"], "venue": "J. Log. Program. 37", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Specification and Verification of Declarative Open Interaction Models: a Logic-Based Approach", "author": ["M. Montali"], "venue": "Volume 56 of LNBIP. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "The process model matching contest", "author": ["U Cayoglu"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Improving documentation by repairing event logs", "author": ["A. Rogge-Solti", "R.S. Mans", "W.M. van der Aalst", "M. Weske"], "venue": "Proc. of PoEM. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Reasoning-based techniques for dealing with incomplete business process execution traces", "author": ["P. Bertoli", "C. Di Francescomarino", "M. Dragoni", "C. Ghidini"], "venue": "Proc. of AI*IA, Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Testing careflow process execution conformance by translating a graphical language to computational logic", "author": ["F. Chesani", "P. Mello", "M. Montali", "S. Storari"], "venue": "Proc. of AIME.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Declarative specification and verification of service choreographiess", "author": ["M. Montali", "M. Pesic", "W.M.P. van der Aalst", "F. Chesani", "P. Mello", "S. Storari"], "venue": "TWEB 4", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Auditing with incomplete logs", "author": ["U.S. Mian", "J. den Hartog", "N.Z.S. Etalle"], "venue": "Proc. of HotSpot.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The CIFF proof procedure for abductive logic programming with constraints: Theory, implementation and experiments", "author": ["P. Mancarella", "G. Terreni", "F. Sadri", "F. Toni", "U. Endriss"], "venue": "TPLP 9", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Monitoring data-aware business constraints with finite state automata", "author": ["R. De Masellis", "F.M. Maggi", "M. Montali"], "venue": "Proc. of ICSSP, ACM Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Managing controlled violation of temporal process constraints", "author": ["A. Kumar", "S. Sabbella", "R. Barton"], "venue": "Business Process Management. Volume 9253 of LNCS. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The proliferation of IT systems able to store process executions traces in socalled event logs has originated, in the Business Process community, a quest towards tools that offer the possibility of discovering, checking the conformance and enhancing process models based on actual behaviors [1].", "startOffset": 291, "endOffset": 294}, {"referenceID": 0, "context": "Thus, while the presence of non-monitorable activities (or errors in the logging procedure) makes the ability of handling incomplete event data one of the main challenges of the BP community, as mentioned in the process mining manifesto[1], still trace compliance of an execution ar X iv :1 60 6.", "startOffset": 236, "endOffset": 239}, {"referenceID": 1, "context": "Notable exceptions are [2,3] where trace incompleteness is managed in an algorithmic/heuristic manner using log repair techniques.", "startOffset": 23, "endOffset": 28}, {"referenceID": 2, "context": "Notable exceptions are [2,3] where trace incompleteness is managed in an algorithmic/heuristic manner using log repair techniques.", "startOffset": 23, "endOffset": 28}, {"referenceID": 3, "context": "In this paper, we take an orthogonal approach and throughly address the problem of log incompleteness from a formal/logic-based point of view, adopting an approach based on abduction [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "This leads us to refine the classical notion of conformance-by-alignment [5] between an execution trace and a process model into strong and conditional compliance, to account for incompleteness.", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "2); (iii) an encoding of structured process models and of event logs in the SCIFF abductive logic framework [7], and a usage of the SCIFF abductive proof procedure to compute strong, conditional and non- compliance in presence of an incomplete event log (Section 3); and (iv) an evaluation of the proposed framework in an experimental setting (Section 4).", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "4 We focus on structured process models in the spirit of [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "In accordance with the IEEE standard XES format for representing event logs [8], in general we can describe an event log as a set of execution traces.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Events are usually described as complex objects containing data about the executed activity, its time stamp, and so on [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "These corresponds to the so-called \u201cde jure\u201d models in [5], and consequently call for a definition of compliance, so as to characterise the degree to which a given trace conforms/is aligned to the", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "The third component is an extension of a typical business process specification that we propose (following and extending the approach described in [9]) to provide prescriptive information", "startOffset": 147, "endOffset": 150}, {"referenceID": 9, "context": "Abduction is a non-monotonic reasoning process where hypotheses are made to explain observed facts [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "Abduction has been introduced in Logic Programming in [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "In this paper we leverage on the SCIFF abductive logic programming framework [7], an extension of the IFF abductive proof procedure [11].", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "In this paper we leverage on the SCIFF abductive logic programming framework [7], an extension of the IFF abductive proof procedure [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "We do so by extending the SCIFF declarative semantics provided in [7] to incorporate log incompleteness (that is, observability features).", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "A structured process model corresponds to a SCIFF specification S = \u3008KB,A, IC\u3009, where: (i) KB is a Logic Program [12] containing the definition of accessory predicates; (ii) A = {ABD/2,E/2}; (iii) IC is a set of ICs constructed by following the encoding defined in Section 3.", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "where Comp is the (two-valued) completion of a theory [13], CET stands for Clark Equational Theory [14] and TN is the CLP constraint theory [15] for integers.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "where Comp is the (two-valued) completion of a theory [13], CET stands for Clark Equational Theory [14] and TN is the CLP constraint theory [15] for integers.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "where Comp is the (two-valued) completion of a theory [13], CET stands for Clark Equational Theory [14] and TN is the CLP constraint theory [15] for integers.", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "This can be done in the spirit of [16] (where correctness is proven for declarative, constraint-based processes), by arguing that structured processes can be seen as declarative processes that only employ the \u201cchain-response constraint\u201d [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "This can be done in the spirit of [16] (where correctness is proven for declarative, constraint-based processes), by arguing that structured processes can be seen as declarative processes that only employ the \u201cchain-response constraint\u201d [16].", "startOffset": 237, "endOffset": 241}, {"referenceID": 6, "context": "For step (ii), we rely on [7], where t soundness and completeness of SCIFF w.", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "As for the model, we choose a real-life process made available within the Process Matching Contest 2013 [17], describing the admission procedure to the", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "Some of them [2,3] have addressed the problem of aligning event logs and procedural/declarative process models [2,3].", "startOffset": 13, "endOffset": 18}, {"referenceID": 2, "context": "Some of them [2,3] have addressed the problem of aligning event logs and procedural/declarative process models [2,3].", "startOffset": 13, "endOffset": 18}, {"referenceID": 1, "context": "Some of them [2,3] have addressed the problem of aligning event logs and procedural/declarative process models [2,3].", "startOffset": 111, "endOffset": 116}, {"referenceID": 2, "context": "Some of them [2,3] have addressed the problem of aligning event logs and procedural/declarative process models [2,3].", "startOffset": 111, "endOffset": 116}, {"referenceID": 17, "context": "For example, in [18] the authors exploit stochastic Petri nets and Bayesian Networks to recover missing information.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "Among these approaches, the issue of reconstructing missing information has been tackled in [19] and in [9], respectively leveraging Satisfiability Modulo Theory, and planning techniques.", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "Among these approaches, the issue of reconstructing missing information has been tackled in [19] and in [9], respectively leveraging Satisfiability Modulo Theory, and planning techniques.", "startOffset": 104, "endOffset": 107}, {"referenceID": 19, "context": "In [20], a structured workflow language has been defined, with a formal semantics in SCIFF.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [21], SCIFF has been exploited to formalize and reason about the declarative workflow language Declare.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "An interesting work where trace compliance is evaluated through abduction is presented in [22].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "The adopted abductive framework, CIFF [23], only supports ground abducibles, and ICs are limited to denials.", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "The work in [22] explores also the dimension of human confirmation of hypotheses, and proposes", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "Concerning future development, the SCIFF framework is based on first-order logic, thus paving the way towards (i) the incorporation of data [24], (ii) extensions to further types of workflows (e.", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": ", temporal workflows as in [25]), and (iii) towards the investigation of probabilistic models to deal with incompleteness of knowledge.", "startOffset": 27, "endOffset": 31}], "year": 2016, "abstractText": "The capability to store data about business processes execution in so-called Event Logs has brought to the diffusion of tools for the analysis of process executions and for the assessment of the goodness of a process model. Nonetheless, these tools are often very rigid in dealing with with Event Logs that include incomplete information about the process execution. Thus, while the ability of handling incomplete event data is one of the challenges mentioned in the process mining manifesto, the evaluation of compliance of an execution trace still requires an endto-end complete trace to be performed. This paper exploits the power of abduction to provide a flexible, yet computationally effective, framework to deal with different forms of incompleteness in an Event Log. Moreover it proposes a refinement of the classical notion of compliance into strong and conditional compliance to take into account incomplete logs. Finally, performances evaluation in an experimental setting shows the feasibility of the presented approach.", "creator": "LaTeX with hyperref package"}}}