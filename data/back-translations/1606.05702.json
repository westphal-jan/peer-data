{"id": "1606.05702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Query-Focused Opinion Summarization for User-Generated Content", "abstract": "We present a sub-modular function-based framework for a query-focused opinion summary. Within our framework, the order of relevance generated by a statistical marker and information coverage in terms of topic distribution and different viewpoints are both encoded as sub-modular functions. Dispersion functions are used to minimize redundancy. We are the first to evaluate different metrics of text similarity for sub-modular summary methods. By experimenting with community QA and blog summaries, we show that our system outperforms state-of-the-art approaches in both automatic assessment and human evaluation. A human evaluation task is performed on Amazon Mechanical Turk on a scale and shows that our systems are capable of producing high-quality summaries and information diversity.", "histories": [["v1", "Fri, 17 Jun 2016 23:05:41 GMT  (43kb)", "http://arxiv.org/abs/1606.05702v1", "COLING 2014"]], "COMMENTS": "COLING 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "hema raghavan", "claire cardie", "vittorio castelli"], "accepted": false, "id": "1606.05702"}, "pdf": {"name": "1606.05702.pdf", "metadata": {"source": "CRF", "title": "Query-Focused Opinion Summarization for User-Generated Content", "authors": ["Lu Wang", "Hema Raghavan", "Claire Cardie Vittorio Castelli"], "emails": ["cardie}@cs.cornell.edu", "hraghavan@linkedin.com", "vittorio@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 70\n2v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "Social media forums, such as social networks, blogs, newsgroups, and community question answering (QA), offer avenues for people to express their opinions as well collect other people\u2019s thoughts on topics as diverse as health, politics and software (Liu et al., 2008). However, digesting the large amount of information in long threads on newsgroups, or even knowing which threads to pay attention to, can be overwhelming. A text-based summary that highlights the diversity of opinions on a given topic can lighten this information overload. In this work, we design a submodular function-based framework for opinion summarization on community question answering and blog data. Question: What is the long term effect of piracy on the music and film industry? Best Answer: Rising costs for movies and music. ... If they sell less, they need to raise the price to make up for what they lost. The other thing will be music and movies with less quality. ... Other Answers: Ans1: Its bad... really bad. (Just watch this movie and you will find out ... Piracy causes rappers to appear on your computer). Ans2: By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies. If they can\u2019t protect their copyrights, they can\u2019t continue to do business. ... Ans4: It is forcing them to rework their business model, which is a good thing. In short, I don\u2019t think the music industry in particular will ever enjoy the huge profits of the 90\u2019s. ... Ans6: Please-People in those businesses make millions of dollars as it is!! I don\u2019t think piracy hurts them at all!!!\nFigure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared to the other answers.\nOpinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints. Unlike those works, we address user generated online data: community QA and blogs. These forums use a substantially less formal language than news articles, and at the same time address a much broader spectrum of topics than product reviews. As a result, they present new challenges for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers1 along with the answers from different users. The question receives more than one answer, and one of them is selected as the \u201cbest answer\u201d by the asker or other participants. In general, answers from other users also provide relevant information. While community QA successfully pools rich knowledge from the wisdom of the crowd, users might need to seine through numerous posts to extract the information\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/\n1http://answers.yahoo.com/\nthey need. Hence, it would be beneficial to summarize answers automatically and present the summaries to users who ask similar questions in the future. In this work, we aim to return a summary that encapsulates different perspectives for a given opinion question and a set of relevant answers or documents.\nIn our work we assume that there is a central topic (or query) on which a user is seeking diverse opinions. We predict query-relevance through automatically learned statistical rankers. Our ranking function not only aims to find sentences that are on the topic of the query but also ones that are \u201copinionated\u201d through the use of several features that indicate subjectivity and sentiment. The relevance score is encoded in a submodular function. Diversity is accounted for by a dispersion function that maximizes the pairwise distance between the pairs of sentences selected.\nOur chief contributions are: (1) We develop a submodular function-based framework for query-focused opinion summarization. To the best of our knowledge, this is the first time that submodular functions have been used to support opinion summarization. We test our framework on two tasks: summarizing opinionated sentences in community QA (Yahoo! Answers) and blogs (TAC-2008 corpus). Human evaluation using Amazon Mechanical Turk shows that our system generates the best summary 57.1% of the time. On the other hand, the best answer picked by Yahoo! users is chosen only 31.9% of the time. We also obtain significant higher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011). (2) Within our summarization framework, the statistically learned sentence relevance is included as part of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011) only uses ngram overlap for query relevance. Additionally, we use Latent Dirichlet Allocation (Blei et al., 2003) to model the topic structure of the sentences, and induce clusterings according to the learned topics. Therefore, our system is capable of generating summaries with broader topic coverage. (3) Furthermore, we are the first to study how different metrics for computing text similarity or dissimilarity affect the quality of submodularity-based summarization methods. We show empirically that lexical representation-based similarity, such as TFIDF scores, uniformly outperforms semantic similarity computed with WordNet. Moreover, when measuring the summary diversity, topical representation is marginally better than lexical representation, and both of them beats semantic representation."}, {"heading": "2 Related Work", "text": "Our work falls in the realm of query-focused summarization, where a user asks a question and the system generates a summary of the answers containing pertinent and diverse information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daume\u0301 and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected.\nEncouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity.\nPrevious work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences."}, {"heading": "3 Submodular Opinion Summarization", "text": "In this section, we describe how query-focused opinion summarization can be addressed by submodular functions combined with dispersion functions. We first define our problem. Then we introduce the\ncomponents of our objective function (Sections 3.1\u20133.3). The full objective function is presented in Section 3.4. Lastly, we describe a greedy algorithm with constant factor approximation to the optimal solution for generating summaries (Section 3.5).\nA set of documents or answers to be summarized are first split into a set of individual sentences V = {s1, \u00b7 \u00b7 \u00b7 , sn}. Our problem is to select a subset S \u2286 V that maximizes a given objective function f : 2V \u2192 R within a length constraint: S\u2217 = argmax\nS\u2286V\nf(S), subject to | S |\u2264 c. | S | is the length of\nthe summary S, and c is the length limit.\nDefinition 1 A function f : 2V \u2192 R is submodular iff for all s \u2208 V and every S \u2286 S\u2032 \u2286 V , it satisfies f(S \u222a {s})\u2212 f(S) \u2265 f(S\u2032 \u222a {s})\u2212 f(S\u2032).\nPrevious submodularity-based summarization work assumes this diminishing return property makes submodular functions a natural fit for summarization and achieves state-of-the-art results on various datasets. In this paper, we follow the same assumption and work with non-decreasing submodular functions. Nevertheless, they have limitations, one of which is that functions well suited to modeling diversity are not submodular. Recently, Dasgupta et al. (2013) proved that diversity can nonetheless be encoded in well-designed dispersion functions which still maintain a constant factor approximation when solved by a greedy algorithm.\nBased on these considerations, we propose an objective function f(S) mainly considering three aspects: relevance (Section 3.1), coverage (Section 3.2), and non-redundancy (Section 3.3). Relevance and coverage are encoded in a non-decreasing submodular function, and non-redundancy is enforced by maximizing the dispersion function."}, {"heading": "3.1 Relevance Function", "text": "We first utilize statistical rankers to produce a preference ordering of the candidate answers or sentences. We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrieval tasks, as our ranker. We use the implementation from Ranklib (Dang, 2011).\nFeatures used in the ranking algorithm are summarized in Table 1. All features are normalized by standardization. Due to the length limit, we cannot provide the full results on feature evaluation. Nevertheless, we find that ranking candidates by TFIDF similarity or key phrases overlapping with the query can produce comparable results with using the full feature set (see Section 5).\nWe take the ranks output by the ranker, and define the relevance of the current summary S as: r(S) = \u2211|S|\ni\n\u221a\nrank\u22121i , where ranki is the rank of sentence si in V . For QA answer ranking, sentences from the same answer have the same ranking. The function r(S) is our first submodular function."}, {"heading": "3.2 Coverage Functions", "text": "Topic Coverage. This function is designed to capture the idea that a comprehensive opinion summary should provide thoughts on distinct aspects. Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document collections, and thus afford a natural way to cluster texts according to their topics. Recent work (Xie and Xing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering. We first learn an LDA model from the data, and treat each topic as a cluster. We estimate a sentence-topic distribution ~\u03b8 for each sentence, and assign the sentence to the cluster k corresponding to the mode of the distribution (i.e., k = argmaxi \u03b8i). This naive approach produces comparable clustering performance to the state-of-the-art according to (Xie and Xing, 2013). T is defined as the clustering induced by our algorithm on the set V . The topic coverage of the current summary S is defined as t(S) = \u2211\nT\u2208T\n\u221a\n|S \u2229 T |.\nFrom the concavity of the square root it follows that sets S with uniform coverages of topics are preferred to sets with skewed coverage. Authorship Coverage. This term encourages the summarization algorithm to select sentences from different authors. Let A be the clustering induced by the sentence to author relation. In community QA, sentences from the answers given by the same user belong to the same cluster. Similarly, sentences from blogs with the same author are in the same cluster. The authorship score is defined as a(S) = \u2211\nA\u2208A\n\u221a\n|S \u2229A|. Polarity Coverage. The polarity score encourages the selection of summaries that cover both positive and negative opinions. We categorize each sentence simply by counting the number of polarized words given by our lexicon. A sentence belongs to a positive cluster if it has more positive words than negative ones, and vice versa. If any negator co-occurs with a sentiment word (e.g. within a window of size 5), the sentiment is reversed.2 The polarity clustering P thus have two clusters corresponding to positive and negative opinions. The score is defined as p(S) = \u2211\nP\u2208P\n\u221a\n| S \u2229 P |. Our lexicon consists of MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed. Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), we use the following function to measure content coverage of the current summary S: c(S) = \u2211\nv\u2208V min(cov(v, S), \u03b8 \u00b7 cov(v, V )), where cov(v, S) = \u2211\nu\u2208S sim(v, u). We experiment with two types of similarity functions. One is a Cosine TFIDF similarity score. The other is a WordNet-based semantic similarity score between pairwise dependency relations from two sentences (Dasgupta et al., 2013). Specifically, simSem(v, u) = \u2211\nreli\u2208v,relj\u2208u WN(ai, aj) \u00d7 WN(bi, bj), where reli = (ai, bi), relj = (aj , bj), WN(wi, wj) is the\nshortest path length. All scores are scaled onto [0, 1]."}, {"heading": "3.3 Dispersion Function", "text": "Summaries should contain as little redundant information as possible. We achieve this by adding an additional term to the objective function, encoded by a dispersion function. Given a set of sentences S, a complete graph is constructed with each sentence in S as a node. The weight of each edge (u, v) is their dissimilarity d\u2032(u, v). Then the distance between any pair of u and v, d(u, v), is defined as the total weight of the shortest path connecting u and v.3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (1) hsum = \u2211\nu,v\u2208V,u 6=v d(u, v), and (2) hmin = minu,v\u2208V,u 6=v d(u, v). Then we need to define the dissimilarity function d\u2032(\u00b7, \u00b7). There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012). In this work, we experiment with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtfidf (u, v) be the Cosine similarity between u and v, then we have d\u2032Lex(u, v) = 1\u2212 simtfidf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. d\u2032Sem(u, v) = 1 \u2212 simSem(v, u), where simSem(v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz et al. (2010) show that estimating the similarity between query and passages by using topic structures can help improve the retrieval performance. As discussed in the topic coverage in Section 3.2, each sentence is represented by its sentence-topic distributions estimated by LDA. For candidate sentence u and v, let their topic distributions be Pu and Pv. Then the dissimilarity between u and v can be defined as: d\u2032Topic(u, v) = JSD(Pu||Pv) = 12 ( \u2211 i Pu(i) log2 Pu(i) Pa(i) + \u2211 i Pv(i) log2 Pv(i) Pa(i) ) where Pa(i) = 12 (Pu(i) + Pv(i))."}, {"heading": "3.4 Full Objective Function", "text": "The objective function takes the interpolation of the submodular functions and dispersion function:\nF(S) = r(S) + \u03b1t(S) + \u03b2a(S) + \u03b3p(S) + \u03b7c(S) + \u03b4h(S). (1)\n2There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed for polarity clustering in this work. We decide to focus on summarization, and estimate sentence polarity through sentiment word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.\n3This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5.\nThe coefficients \u03b1, \u03b2, \u03b3, \u03b7, \u03b4 are non-negative real numbers and can be tuned on a development set.4 Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function, and summation preserves monotonicity, non-negativity, and submodularity. Dispersion function h(s) is either hsum or hmin as introduced previously."}, {"heading": "3.5 Summary Generation via Greedy Algorithm", "text": "Generating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra and Halldo\u0301rsson, 1996). We choose to use a greedy algorithm that guarantees to obtain a constant factor approximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013). Concretely, starting with an empty set, for each iteration, we add a new sentence so that the current summary achieves the maximum value of the objective function. In addition to the theoretical guarantee, existing work (McDonald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally."}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Opinion Question Identification", "text": "We first build a classifier to automatically detect opinion oriented questions in Community QA; questions in the blog dataset are all opinionated. Our opinion question classifier is trained on two opinion question datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as \u201cWhat can you do to help environment?\u201d, and 317 objective questions. We train a RBF kernel based SVM classifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasets when evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79)."}, {"heading": "4.2 Datasets", "text": "Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo! WebscopeTM program,5 which contains 3,895,407 questions. We first run the opinion question classifier to identify the opinion questions. For summarization purpose, we require each question having at least 5 answers, with the average length of answers larger than 20 words. This results in 130,609 questions.\nTo make a compelling task, we reserve questions with an average length of answers larger than 50 words as our test set for both ranking and summarization; all the other questions are used for training. As a result, we have 92,109 questions in the training set for learning the statistical ranker, and 38,500 in the test set. The category distribution of training and test questions (Yahoo! Answers organizes the questions into predefined categories) are similar. 10,000 questions from the training set are further reserved as the development set. Each question in the Yahoo! Answers dataset has a user-voted best answer. These best answers are used to train the statistical ranker that predicts relevance. Separate topic models are learned for each category, where the category tag is provided by Yahoo! Answer. Blog Summarization: TAC 2008. We use the TAC 2008 corpus (Dang, 2008), which consists of 25 topics. 23 of them are provided with human labeled nuggets, which TAC used in human evaluation. TAC also provides snippets (i.e., sentences) that are frequently retrieved by participant systems or identified as relevant by human annotators. We do not assume those snippets are known to any of our systems."}, {"heading": "4.3 Comparisons", "text": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters reported to have the best performance in their work.\nFor cQA summarization, we use the best answer voted by the user as a baseline. Note that this is a strong baseline since all the other systems are unaware of which answer is the best. For blog summarization, we have three additional baselines \u2013 the best systems in TAC 2008 (Kim et al., 2008; Li et al., 2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon\n4The values for the coefficients are 5.0, 1.0, 10.0, 5.0, 10.0 for \u03b1, \u03b2, \u03b3, \u03b7, \u03b4, respectively, as tuned on the development set. 5http://sandbox.yahoo.com/\n(henceforth called TFIDF+Lexicon). In TFIDF+Lexicon, sentences are ranked by the TFIDF similarity with the query, and then sentences with sentiment words are selected in sequence. This baseline aims to show the performance when we only have access to lexicons without using a learning algorithm."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Evaluating the Ranker", "text": "We evaluate our ranker (described in Section 3.1) on the task of best answer prediction. Table 2 compares the average precision and mean reciprocal rank (MRR) of our method to those of three baselines, (1) where answers are ranked randomly (Baseline (Random)), (2) by length (Baseline (Length)), and (3) by Jensen Shannon Divergence (JSD) with all answers. We expect that the best answer is the one that covers the most information, which is likely to have a smaller JSD. Therefore, we use JSD to rank answers in the ascending order. Table 2 manifests that our ranker outperforms all the other methods."}, {"heading": "5.2 Community QA Summarization", "text": "Automatic Evaluation. Since human written abstracts are not available for the Yahoo! Answers dataset, we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality. Intuitively, a smaller JSD implies that the summary covers more of the content in the answer set. Louis and Nenkova (2013) report that JSD has a strong negative correlation (Spearman correlation = \u22120.737) with the overall summary quality for multi-document summarization (MDS) on news articles and blogs. Our task is similar to MDS. Meanwhile, the average JSD of the best answers in our test set is smaller than that of the other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for the other answers. Also, on the blog task (Section 5.3), the top two systems by JSD also have the top two ROUGE scores (a common metric for summarization evaluation when human-constructed summaries are available). Thus, we conjecture that JSD is a good metric for community QA summaries.\nTable 3 (left) shows that our system using a content coverage function based on Cosine using TFIDF weights, and a dispersion function (hsum) based on lexicon dissimilarity and 100 topics, outperforms all of the compared approaches (paired-t test, p < 0.05). The topic number is tuned on the development set, and we find that varying the number of topics does not impact performance too much. Meanwhile, both our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and Bilmes (2011) system, which implies the effectiveness of the dispersion function. We further examine the effectiveness of each component that contributes to the objective function (Section 3.4), and the results are shown in Table 3 (right).\nHuman Evaluation. Human evaluation for Yahoo! Answers is carried out on Amazon Mechanical Turk6 with carefully designed tasks (or \u201cHITs\u201d). Turkers are presented summaries from different systems in a random order, and asked to provide two rankings, one for overall quality and the other for information diversity. We indicate that informativeness and non-redundancy are desirable for quality; however, Turkers are allowed to consider other desiderata, such as coherence or responsiveness, and write down those when they submit the answers. Here we believe that ranking the summaries is easier than evaluating each summary in isolation (Lerman et al., 2009).\n6https://www.mturk.com/mturk/\nWe randomly select 100 questions from our test set, each of which is evaluated by 4 distinct Turkers located in United States. 40 HITs are thus created, each containing 10 different questions. Four system summaries (best answer, Dasgupta et al. (2013), and our system with 100 and 200 words respectively) are displayed along with one noisy summary (i.e. irrelevant to the question) per question in random order.7 We reject Turkers\u2019 HITs if they rank the noisy summary higher than any other. Two duplicate questions are added to test intra-annotator agreement. We reject HITs if Turkers produced inconsistent rankings for both duplicate questions. A total of 137 submissions of which 40 HITs pass the above quality filters.\nTurkers of all accepted submissions report themselves as native English speakers. An inter-rater agreement of Fleiss\u2019 \u03ba of 0.28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and \u03ba is 0.43 (moderate agreement) for diversity ranking. Table 4 shows the percentage of times a particular method is picked as the best summary, and the macro-/micro-average rank of a method, for both overall quality and information diversity. Macro-average is computed by first averaging the ranks per question and then averaging across all questions.\nFor overall quality, our system with a 200 word limit is selected as the best in 44.6% of the evaluations. It outperforms the best answer (31.9%) significantly, which suggests that our system summary covers relevant information that is not contained in the best answer. Our system with a length constraint of 100 words is chosen as the best for quality 12.5% times while that of Dasgupta et al. (2013) is chosen 11.0% of the time. Our system is also voted as the best summary for diversity in 78.7% of the evaluations. More interestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Dasgupta et al. (2013) for average ranking (both overall quality and information diversity) significantly by using Wilcoxon signed-rank test (p < 0.05). When we check the reasons given by Turkers, we found that people usually prefer our summaries due to \u201chelpful suggestions that covered many options\u201d or being \u201cbalanced with different opinions\u201d. When Turks prefer the best answers, they mostly stress on coherence and responsiveness. Sample summaries from all the systems are displayed in Figure 2.\n7Note that we aim to compare results with the gold-standard best answers of about 100 words. The evaluation of the 200-word summaries is provided only as an additional data-point."}, {"heading": "5.3 Blog Summarization", "text": "Automatic Evaluation. We use the ROUGE (Lin and Hovy, 2003) software with standard options to automatically evaluate summaries with reference to the human labeled nuggets as those are available for this task. ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigram and skip-bigram separated by up to four words. We use the ranker trained on Yahoo! data to produce relevance ordering, and adopt the system parameters from Section 5.2. Table 5 (left) shows that our system outperforms the best system in TAC\u201908 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013).\nHuman Evaluation. For human evaluation, we use the standard Pyramid F-score used in the TAC\u201908 opinion summarization track with \u03b2 = 3 (Dang, 2008). In the TAC task, systems are allowed to return up to 7,000 non-white characters for each question. Since the TAC metric favors recall we do not produce summaries shorter than 7,000 characters. We ask two human judges to evaluate our system along with the one that got the highest Pyramid F-score in the TAC\u201908 and Lin and Bilmes (2011). Cohen\u2019s \u03ba for inter-annotator agreement is 0.68 (substantial). While we did not explicitly evaluate non-redundancy, both of our judges report that our system summaries contain less redundant information."}, {"heading": "5.4 Further Discussion", "text": "Given that the text similarity metrics and dispersion functions play important roles in the framework, we further study the effectiveness of different content coverage functions (Cosine using TFIDF vs. Semantic), dispersion functions (hsum vs. hmin), and dissimilarity metrics used in dispersion functions (Semantic vs. Topical vs. Lexical). Results on Yahoo! Answer (Table 6 (left)) show that systems using summation of distances for dispersion functions (hsum) uniformly outperform the ones using minimum distance (hmin). Meanwhile, Cosine using TFIDF is better at measuring content coverage than WordNetbased semantic measurement, and this may due to the limited coverage of WordNet on verbs. This is also true for dissimilarity metrics. Results on blog data (Table 6 (right)), however, show that using minimum distance for dispersion produces better results. This indicates that optimal dispersion function varies by genre. Topical-based dissimilarity also marginally outperforms the other two metrics in blog data."}, {"heading": "6 Conclusion", "text": "We propose a submodular function-based opinion summarization framework. Tested on community QA and blog summarization, our approach outperforms state-of-the-art methods that are also based on submodularity in both automatic evaluation and human evaluation. Our framework is capable of including statistically learned sentence relevance and encouraging the summary to cover diverse topics. We also study different metrics on text similarity estimation and their effect on summarization."}], "references": [{"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre."], "venue": "Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385\u2013393, Montr\u00e9al, Canada, 7-8 June. Association for Computational Linguistics.", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "A pattern matching based model for implicit opinion question identification", "author": ["Hadi Amiri", "Zheng-Jun Zha", "Tat-Seng Chua."], "venue": "AAAI. AAAI Press.", "citeRegEx": "Amiri et al\\.,? 2013", "shortCiteRegEx": "Amiri et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "J. Mach. Learn. Res., 3:993\u20131022, March.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Learning to rank: From pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li."], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 129\u2013136, New York, NY, USA. ACM.", "citeRegEx": "Cao et al\\.,? 2007", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein."], "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201998, pages 335\u2013336, New York, NY, USA. ACM.", "citeRegEx": "Carbonell and Goldstein.,? 1998", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "Lda based similarity modeling for question answering", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur", "Gokhan Tur."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS \u201910, pages 1\u20139, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Celikyilmaz et al\\.,? 2010", "shortCiteRegEx": "Celikyilmaz et al\\.", "year": 2010}, {"title": "Facility dispersion and remote subgraphs", "author": ["Barun Chandra", "Magn\u00fas M. Halld\u00f3rsson."], "venue": "Proceedings of the 5th Scandinavian Workshop on Algorithm Theory, SWAT \u201996, pages 53\u201365, London, UK, UK. SpringerVerlag.", "citeRegEx": "Chandra and Halld\u00f3rsson.,? 1996", "shortCiteRegEx": "Chandra and Halld\u00f3rsson.", "year": 1996}, {"title": "Overview of the tac 2008 opinion question answering and summarization tasks", "author": ["Hoa Tran Dang."], "venue": "Proc. TAC 2008.", "citeRegEx": "Dang.,? 2008", "shortCiteRegEx": "Dang.", "year": 2008}, {"title": "RankLib", "author": ["Van Dang."], "venue": "http://www.cs.umass.edu/ \u0303vdang/ranklib.html.", "citeRegEx": "Dang.,? 2011", "shortCiteRegEx": "Dang.", "year": 2011}, {"title": "Summarization through submodularity and dispersion", "author": ["Anirban Dasgupta", "Ravi Kumar", "Sujith Ravi."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1014\u20131022, Sofia, Bulgaria, August. Association for Computational Linguistics.", "citeRegEx": "Dasgupta et al\\.,? 2013", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2013}, {"title": "Bayesian query-focused summarization", "author": ["III Hal Daum\u00e9", "Daniel Marcu."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 305\u2013312, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Daum\u00e9 and Marcu.,? 2006", "shortCiteRegEx": "Daum\u00e9 and Marcu.", "year": 2006}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417\u2013422.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, pages 168\u2013177, New York, NY, USA. ACM.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "CLUTO - a clustering toolkit", "author": ["George Karypis."], "venue": "Technical Report #02-017, November.", "citeRegEx": "Karypis.,? 2003", "shortCiteRegEx": "Karypis.", "year": 2003}, {"title": "Opinion summarization using entity features and probabilistic sentence coherence optimization: Uiuc at tac 2008 opinion summarization pilot", "author": ["Hyun Duk Kim", "Dae Hoon Park", "V.G.Vinod Vydiswaran", "ChengXiang Zhai."], "venue": "Proc. TAC 2008.", "citeRegEx": "Kim et al\\.,? 2008", "shortCiteRegEx": "Kim et al\\.", "year": 2008}, {"title": "The measurement of observer agreement for categorical data", "author": ["J R Landis", "G G Koch."], "venue": "Biometrics, 33(1):159\u2013174.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "Sentiment summarization: Evaluating and learning user preferences", "author": ["Kevin Lerman", "Sasha Blair-Goldensohn", "Ryan McDonald."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL \u201909, pages 514\u2013522, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lerman et al\\.,? 2009", "shortCiteRegEx": "Lerman et al\\.", "year": 2009}, {"title": "Cocqa: Co-training over questions and answers with an application to predicting question subjectivity orientation", "author": ["Baoli Li", "Yandong Liu", "Eugene Agichtein."], "venue": "EMNLP, pages 937\u2013946.", "citeRegEx": "Li et al\\.,? 2008a", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Polyu at tac 2008", "author": ["Wenjie Li", "You Ouyang", "Yi Hu", "Furu Wei."], "venue": "Proc. TAC 2008.", "citeRegEx": "Li et al\\.,? 2008b", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "A class of submodular functions for document summarization", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies Volume 1, HLT \u201911, pages 510\u2013520, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lin and Bilmes.,? 2011", "shortCiteRegEx": "Lin and Bilmes.", "year": 2011}, {"title": "The automated acquisition of topic signatures for text summarization", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "COLING \u201900, pages 495\u2013501, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lin and Hovy.,? 2000", "shortCiteRegEx": "Lin and Hovy.", "year": 2000}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 71\u201378.", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Understanding and summarizing answers in community-based question answering services", "author": ["Yuanjie Liu", "Shasha Li", "Yunbo Cao", "Chin-Yew Lin", "Dingyi Han", "Yong Yu."], "venue": "Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING \u201908, pages 497\u2013504, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2008", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Automatically assessing machine summary content without a gold standard", "author": ["Annie Louis", "Ani Nenkova."], "venue": "Comput. Linguist., 39(2):267\u2013300, June.", "citeRegEx": "Louis and Nenkova.,? 2013", "shortCiteRegEx": "Louis and Nenkova.", "year": 2013}, {"title": "Finding what matters in questions", "author": ["Xiaoqiang Luo", "Hema Raghavan", "Vittorio Castelli", "Sameer Maskey", "Radu Florian."], "venue": "HLT-NAACL, pages 878\u2013887.", "citeRegEx": "Luo et al\\.,? 2013", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["Ryan McDonald."], "venue": "ECIR\u201907, pages 557\u2013564, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "McDonald.,? 2007", "shortCiteRegEx": "McDonald.", "year": 2007}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1, AAAI\u201906, pages 775\u2013780. AAAI Press.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "An analysis of approximations for maximizing submodular set functionsI", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher."], "venue": "Mathematical Programming, 14(1):265\u2013294, December.", "citeRegEx": "Nemhauser et al\\.,? 1978", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "The impact of frequency on summarization", "author": ["Ani Nenkova", "Lucy Vanderwende."], "venue": "Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005-101.", "citeRegEx": "Nenkova and Vanderwende.,? 2005", "shortCiteRegEx": "Nenkova and Vanderwende.", "year": 2005}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Found. Trends Inf. Retr., 2(1-2):1\u2013135, January.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Summarizing contrastive viewpoints in opinionated text", "author": ["Michael J. Paul", "ChengXiang Zhai", "Roxana Girju."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910, pages 66\u201376, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Paul et al\\.,? 2010", "shortCiteRegEx": "Paul et al\\.", "year": 2010}, {"title": "Large-margin learning of submodular summarization models", "author": ["Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims."], "venue": "EACL \u201912, pages 224\u2013233, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Sipos et al\\.,? 2012", "shortCiteRegEx": "Sipos et al\\.", "year": 2012}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Philip J. Stone", "Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie."], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Stone et al\\.,? 1966", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Partially supervised coreference resolution for opinion summarization through structured rule learning", "author": ["Veselin Stoyanov", "Claire Cardie."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201906, pages 336\u2013344, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Stoyanov and Cardie.,? 2006", "shortCiteRegEx": "Stoyanov and Cardie.", "year": 2006}, {"title": "Metadata-aware measures for answer summarization in community question answering", "author": ["Mattia Tomasoni", "Minlie Huang."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 760\u2013769, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Tomasoni and Huang.,? 2010", "shortCiteRegEx": "Tomasoni and Huang.", "year": 2010}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT \u201905, pages 347\u2013354, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Integrating document clustering and topic modeling", "author": ["Pengtao Xie", "Eric Xing."], "venue": "Proceedings of the Twenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13), pages 694\u2013 703, Corvallis, Oregon. AUAI Press.", "citeRegEx": "Xie and Xing.,? 2013", "shortCiteRegEx": "Xie and Xing.", "year": 2013}, {"title": "Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences", "author": ["Hong Yu", "Vasileios Hatzivassiloglou."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Yu and Hatzivassiloglou.,? 2003", "shortCiteRegEx": "Yu and Hatzivassiloglou.", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "Social media forums, such as social networks, blogs, newsgroups, and community question answering (QA), offer avenues for people to express their opinions as well collect other people\u2019s thoughts on topics as diverse as health, politics and software (Liu et al., 2008).", "startOffset": 249, "endOffset": 267}, {"referenceID": 12, "context": "Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints.", "startOffset": 97, "endOffset": 136}, {"referenceID": 16, "context": "Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints.", "startOffset": 97, "endOffset": 136}, {"referenceID": 33, "context": ", 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints.", "startOffset": 17, "endOffset": 44}, {"referenceID": 19, "context": "(2) Within our summarization framework, the statistically learned sentence relevance is included as part of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011) only uses ngram overlap for query relevance.", "startOffset": 182, "endOffset": 204}, {"referenceID": 2, "context": "Additionally, we use Latent Dirichlet Allocation (Blei et al., 2003) to model the topic structure of the sentences, and induce clusterings according to the learned topics.", "startOffset": 49, "endOffset": 68}, {"referenceID": 18, "context": "We also obtain significant higher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011). (2) Within our summarization framework, the statistically learned sentence relevance is included as part of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011) only uses ngram overlap for query relevance.", "startOffset": 97, "endOffset": 119}, {"referenceID": 4, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006).", "startOffset": 109, "endOffset": 140}, {"referenceID": 20, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006).", "startOffset": 164, "endOffset": 184}, {"referenceID": 10, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006).", "startOffset": 244, "endOffset": 267}, {"referenceID": 19, "context": "Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al.", "startOffset": 162, "endOffset": 204}, {"referenceID": 31, "context": "Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al.", "startOffset": 162, "endOffset": 204}, {"referenceID": 9, "context": ", 2012), and comments summarization (Dasgupta et al., 2013).", "startOffset": 36, "endOffset": 59}, {"referenceID": 12, "context": "Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al.", "startOffset": 79, "endOffset": 118}, {"referenceID": 16, "context": "Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al.", "startOffset": 79, "endOffset": 118}, {"referenceID": 33, "context": ", 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al.", "startOffset": 48, "endOffset": 75}, {"referenceID": 30, "context": ", 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010).", "startOffset": 90, "endOffset": 109}, {"referenceID": 4, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA.", "startOffset": 110, "endOffset": 1381}, {"referenceID": 4, "context": "A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum\u00e9 and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences.", "startOffset": 110, "endOffset": 1573}, {"referenceID": 28, "context": "2) - length of the answer/sentence - if contains sentiment words with the same polarity as - length is less than 5 words sentiment words in query Query-Sentence Overlap Features Query-Independent Features - unigram/bigram TF/TFIDF similarity with query - unigram/bigram TFIDF similarity with cluster centroid - number of key phrases in the query that appear in the - sumBasic score (Nenkova and Vanderwende, 2005) sentence.", "startOffset": 382, "endOffset": 413}, {"referenceID": 20, "context": "A model similar to that described in - number of topic signature words (Lin and Hovy, 2000) (Luo et al.", "startOffset": 71, "endOffset": 91}, {"referenceID": 24, "context": "A model similar to that described in - number of topic signature words (Lin and Hovy, 2000) (Luo et al., 2013) was applied to detect key phrases.", "startOffset": 92, "endOffset": 110}, {"referenceID": 9, "context": "Recently, Dasgupta et al. (2013) proved that diversity can nonetheless be encoded in well-designed dispersion functions which still maintain a constant factor approximation when solved by a greedy algorithm.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrieval tasks, as our ranker.", "startOffset": 18, "endOffset": 36}, {"referenceID": 8, "context": "We use the implementation from Ranklib (Dang, 2011).", "startOffset": 39, "endOffset": 51}, {"referenceID": 2, "context": "Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document collections, and thus afford a natural way to cluster texts according to their topics.", "startOffset": 55, "endOffset": 74}, {"referenceID": 36, "context": "Recent work (Xie and Xing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering.", "startOffset": 12, "endOffset": 32}, {"referenceID": 36, "context": "This naive approach produces comparable clustering performance to the state-of-the-art according to (Xie and Xing, 2013).", "startOffset": 100, "endOffset": 120}, {"referenceID": 35, "context": "Our lexicon consists of MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 32, "context": ", 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006).", "startOffset": 26, "endOffset": 46}, {"referenceID": 11, "context": ", 1966), and SentiWordNet (Esuli and Sebastiani, 2006).", "startOffset": 26, "endOffset": 54}, {"referenceID": 10, "context": ", 1966), and SentiWordNet (Esuli and Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed. Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al.", "startOffset": 27, "endOffset": 180}, {"referenceID": 9, "context": "Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), we use the following function to measure content coverage of the current summary S: c(S) = \u2211", "startOffset": 39, "endOffset": 62}, {"referenceID": 9, "context": "The other is a WordNet-based semantic similarity score between pairwise dependency relations from two sentences (Dasgupta et al., 2013).", "startOffset": 112, "endOffset": 135}, {"referenceID": 9, "context": "3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (1) hsum = \u2211", "startOffset": 54, "endOffset": 77}, {"referenceID": 26, "context": "There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012).", "startOffset": 72, "endOffset": 116}, {"referenceID": 0, "context": "There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012).", "startOffset": 72, "endOffset": 116}, {"referenceID": 0, "context": ", 2006; Agirre et al., 2012). In this work, we experiment with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtfidf (u, v) be the Cosine similarity between u and v, then we have dLex(u, v) = 1\u2212 simtfidf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. dSem(u, v) = 1 \u2212 simSem(v, u), where simSem(v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz et al. (2010) show that estimating the similarity between query and passages by using topic structures can help improve the retrieval performance.", "startOffset": 8, "endOffset": 672}, {"referenceID": 29, "context": "There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed for polarity clustering in this work.", "startOffset": 78, "endOffset": 98}, {"referenceID": 37, "context": "We decide to focus on summarization, and estimate sentence polarity through sentiment word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.", "startOffset": 101, "endOffset": 132}, {"referenceID": 6, "context": "Generating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra and Halld\u00f3rsson, 1996).", "startOffset": 86, "endOffset": 117}, {"referenceID": 27, "context": "We choose to use a greedy algorithm that guarantees to obtain a constant factor approximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013).", "startOffset": 118, "endOffset": 165}, {"referenceID": 9, "context": "We choose to use a greedy algorithm that guarantees to obtain a constant factor approximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013).", "startOffset": 118, "endOffset": 165}, {"referenceID": 25, "context": "In addition to the theoretical guarantee, existing work (McDonald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally.", "startOffset": 56, "endOffset": 72}, {"referenceID": 16, "context": "Our opinion question classifier is trained on two opinion question datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al.", "startOffset": 97, "endOffset": 115}, {"referenceID": 1, "context": "(2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as \u201cWhat can you do to help environment?\u201d, and 317 objective questions.", "startOffset": 92, "endOffset": 112}, {"referenceID": 7, "context": "We use the TAC 2008 corpus (Dang, 2008), which consists of 25 topics.", "startOffset": 27, "endOffset": 39}, {"referenceID": 13, "context": "The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003).", "startOffset": 80, "endOffset": 95}, {"referenceID": 14, "context": "For blog summarization, we have three additional baselines \u2013 the best systems in TAC 2008 (Kim et al., 2008; Li et al., 2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon", "startOffset": 90, "endOffset": 126}, {"referenceID": 18, "context": "For blog summarization, we have three additional baselines \u2013 the best systems in TAC 2008 (Kim et al., 2008; Li et al., 2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon", "startOffset": 90, "endOffset": 126}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information.", "startOffset": 74, "endOffset": 97}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information.", "startOffset": 74, "endOffset": 145}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003).", "startOffset": 74, "endOffset": 238}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of systems in Lin and Bilmes (2011) and Dasgupta et al.", "startOffset": 74, "endOffset": 337}, {"referenceID": 9, "context": "For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and (2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters reported to have the best performance in their work.", "startOffset": 74, "endOffset": 364}, {"referenceID": 21, "context": "Louis and Nenkova (2013) report that JSD has a strong negative correlation (Spearman correlation = \u22120.", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Meanwhile, both our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and Bilmes (2011) system, which implies the effectiveness of the dispersion function.", "startOffset": 31, "endOffset": 54}, {"referenceID": 9, "context": "Meanwhile, both our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and Bilmes (2011) system, which implies the effectiveness of the dispersion function.", "startOffset": 31, "endOffset": 131}, {"referenceID": 18, "context": "3858 Lin and Bilmes (2011) 0.", "startOffset": 5, "endOffset": 27}, {"referenceID": 18, "context": "3858 Lin and Bilmes (2011) 0.3398 0.2008 Lin and Bilmes (2011) + q 0.", "startOffset": 5, "endOffset": 63}, {"referenceID": 9, "context": "1988 Dasgupta et al. (2013) 0.", "startOffset": 5, "endOffset": 28}, {"referenceID": 16, "context": "Here we believe that ranking the summaries is easier than evaluating each summary in isolation (Lerman et al., 2009).", "startOffset": 95, "endOffset": 116}, {"referenceID": 9, "context": "Four system summaries (best answer, Dasgupta et al. (2013), and our system with 100 and 200 words respectively) are displayed along with one noisy summary (i.", "startOffset": 36, "endOffset": 59}, {"referenceID": 15, "context": "28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and \u03ba is 0.", "startOffset": 19, "endOffset": 42}, {"referenceID": 9, "context": "5% times while that of Dasgupta et al. (2013) is chosen 11.", "startOffset": 23, "endOffset": 46}, {"referenceID": 9, "context": "5% times while that of Dasgupta et al. (2013) is chosen 11.0% of the time. Our system is also voted as the best summary for diversity in 78.7% of the evaluations. More interestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Dasgupta et al. (2013) for average ranking (both overall quality and information diversity) significantly by using Wilcoxon signed-rank test (p < 0.", "startOffset": 23, "endOffset": 288}, {"referenceID": 9, "context": "29 Dasgupta et al. (2013) 100 11.", "startOffset": 3, "endOffset": 26}, {"referenceID": 9, "context": "29 Dasgupta et al. (2013) 100 11.0% 2.84 2.83 5.0% 2.95 2.94 Our system 12.5% 2.50\u2217 2.50\u2217 6.7% 2.43\u2217 2.43\u2217 Our system 200 44.6% 1.98\u2217 1.98\u2217 78.7% 1.35\u2217 1.34\u2217 Table 4: Human evaluation on Yahoo! Answer Data. Boldface implies statistically significance compared to other results in the same columns using paired-t test. Both of our systems are ranked higher (i.e. numbers in bold with ) than the best answers voted by Yahoo! users and system summaries from Dasgupta et al. (2013).", "startOffset": 3, "endOffset": 478}, {"referenceID": 9, "context": "Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words).", "startOffset": 32, "endOffset": 55}, {"referenceID": 21, "context": "We use the ROUGE (Lin and Hovy, 2003) software with standard options to automatically evaluate summaries with reference to the human labeled nuggets as those are available for this task.", "startOffset": 17, "endOffset": 37}, {"referenceID": 14, "context": "Table 5 (left) shows that our system outperforms the best system in TAC\u201908 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 13, "context": "Table 5 (left) shows that our system outperforms the best system in TAC\u201908 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al.", "startOffset": 103, "endOffset": 195}, {"referenceID": 9, "context": ", 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013).", "startOffset": 87, "endOffset": 110}, {"referenceID": 18, "context": "2293 Lin and Bilmes (2011) 0.", "startOffset": 5, "endOffset": 27}, {"referenceID": 18, "context": "2293 Lin and Bilmes (2011) 0.2732 0.3582 0.2330 Lin and Bilmes (2011) + q 0.", "startOffset": 5, "endOffset": 70}, {"referenceID": 9, "context": "2349 Dasgupta et al. (2013) 0.", "startOffset": 5, "endOffset": 28}, {"referenceID": 9, "context": "2349 Dasgupta et al. (2013) 0.2618 0.3500 0.2370 Our system 0.3234 0.3978 0.2258 Pyramid F-score Best system in TAC\u201908 0.2225 Lin and Bilmes (2011) 0.", "startOffset": 5, "endOffset": 148}, {"referenceID": 7, "context": "For human evaluation, we use the standard Pyramid F-score used in the TAC\u201908 opinion summarization track with \u03b2 = 3 (Dang, 2008).", "startOffset": 116, "endOffset": 128}, {"referenceID": 7, "context": "For human evaluation, we use the standard Pyramid F-score used in the TAC\u201908 opinion summarization track with \u03b2 = 3 (Dang, 2008). In the TAC task, systems are allowed to return up to 7,000 non-white characters for each question. Since the TAC metric favors recall we do not produce summaries shorter than 7,000 characters. We ask two human judges to evaluate our system along with the one that got the highest Pyramid F-score in the TAC\u201908 and Lin and Bilmes (2011). Cohen\u2019s \u03ba for inter-annotator agreement is 0.", "startOffset": 117, "endOffset": 466}], "year": 2016, "abstractText": "We present a submodular function-based framework for query-focused opinion summarization. Within our framework, relevance ordering produced by a statistical ranker, and information coverage with respect to topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for submodularity-based summarization methods. By experimenting on community QA and blog summarization, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and shows that our systems are able to generate summaries of high overall quality and information diversity.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}