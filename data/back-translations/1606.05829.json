{"id": "1606.05829", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2016", "title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test", "abstract": "In this paper, we show that a simple neural model can mimic humans in some of the tasks of art generation, focusing in particular on traditional Chinese poetry and showing that machines are just as good as many contemporary poets, and that the Fig Tree Test, a variant of the Turing Test in professional fields, is weak. Our method is based on an attention-based, recurring neural network that accepts a set of keywords as a theme and generates poems by looking at each keyword throughout the generation. To improve the model, a number of techniques are proposed, including character vector initialization, attention to input, and hybrid training. Compared to existing methods of poetry generation, our model can produce much more theme-consistent and semantic poems.", "histories": [["v1", "Sun, 19 Jun 2016 03:17:29 GMT  (953kb)", "http://arxiv.org/abs/1606.05829v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qixin wang", "tianyi luo", "dong wang"], "accepted": false, "id": "1606.05829"}, "pdf": {"name": "1606.05829.pdf", "metadata": {"source": "CRF", "title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test", "authors": ["Qixin Wang", "Tianyi Luo", "Dong Wang"], "emails": ["lty}@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 82\n9v 1\n[ cs\n.C L\n] 1\n9 Ju\nn 20\n16\nOur method is based on an attention-based recurrent neural network, which accepts a set of keywords as the theme and generates poems by looking at each keyword during the generation. A number of techniques are proposed to improve the model, including character vector initialization, attention to input and hybrid-style training. Compared to existing poetry generation methods, our model can generate much more theme-consistent and semantic-rich poems."}, {"heading": "1 Introduction", "text": "The classical Chinese poetry is a special cultural heritage with over 2, 000 years of history and is still fascinating many contemporary poets. In history, Chinese poetry flourished in different genres at different time, including Tang poetry, Song iambics and Yuan songs. Different genres possess their own specific structural, rhythmical and\n1The two authors contributed equally. 2Corresponding author: Dong Wang; RM 1-303, FIT\nBLDG, Tsinghua University, Beijing (100084), P.R. China.\ntonal patterns. The structural pattern regulates how many lines and how many characters per line; the rhythmical pattern requires that the last characters of certain lines hold the same or similar vowels; and the tonal pattern requires characters in particular positions hold particular tones, i.e., \u2018Ping\u2019 (level tone), or \u2018Ze\u2019 (downward tone). A good poem should follow all these pattern regulations (in a descendant order of priority), and has to express a consistent theme as well as a unique emotion. For this reason, it is widely admitted that traditional Chinese poetry generation is highly difficult and can be only performed by a very few knowledged people.\nAmong all the genres of traditional Chinese poetry, perhaps the most popular is the quatrain, a special style with a strict structure (four lines with five or seven characters per line), a regulated rhythmical form (the last characters in the second and fourth lines must follow the same rhythm), and a required tonal pattern (tones of characters in some positions should satisfy some pre-defined regulations). This genre of poems flourished mostly in Tang Dynasty, so often called \u2018Tang poem\u2019. An example of quatrain written by Lun Lu, a famous poet in Tang Dynasty (Wang, 2002), is shown in Table 1.\nDue to the stringent restriction in rhythm and tone, it is not trivial to create a fully rule-compliant quatrain. More importantly, besides such strict regulations, a good quatrain should also read fluently, hold a consistent theme, and express a unique affection. This is like dancing in fetlers, hence very difficult and can be performed only by knowledged people with long-time training.\nWe are interested in machine poetry generation, not only because of its practical value in entertainment and education, but also because it demonstrates an important aspect of artificial intelligence: the creativity of machines in art gen-\neration. We hold the belief that poetry generation (and other artistic activities) is a pragmatic process and can be largely learned from past experience. In this paper, we focus on traditional Chinese poetry generation, and demonstrate that machines can do it as well as many human poets.\nThere have been some attempts in this direction, e.g., by machine translation models (He et al., 2012) and recurrent neural networks (RNN) (Zhang and Lapata, 2014). These methods can generate traditional Chinese poems with different levels of quality, and can be used to assist people in poem generation. However, none of them can generate poems that are fluent and consistent enough, not to mention innovation.\nIn this paper, we propose a simple neural approach to traditional Chinese poetry generation based on the attention-based Gated Recurrent Unit (GRU) model. Specifically, we follow the sequence-to-sequence learning architecture that uses a GRU (Cho et al., 2014) to encode a set of keywords as the theme, and another GRU to generate quatrains character by character, where the keywords are looked back during the entire generation process. By this approach, the generation is regularized by the keywords so a global theme is assured. By enriching the set of keywords, the generation tends to be more \u2018innovative\u2019, resulting in more diverse poems. Our experiments demonstrated that the new approach can\ngenerate traditional Chinese poems pretty well and even pass the Feigenbaum Test."}, {"heading": "2 Related Work", "text": "A multitude of methods have been proposed for poem automatic generation. The first approach is based on rules and templates. For example, Tosa et al. (2009) and Wu et al. (2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms. Oliveira (2009) and Oliveira (2012) used semantic and grammar templates for Spanish poem generation.\nThe second approach is based on various genetic algorithms. For example, Zhou et al. (2010) proposed to use a stochastic search algorithm to obtain the best matched sentences. The search algorithm is based on four standards proposed by Manurung et al. (2012): fluency, meaningful, poetic, and coherent.\nThe third approach to poem generation involves various statistical machine translation (SMT) methods. This approach was used by Jiang and Zhou (2008) to generate Chinese couplets, a special regulated verses with only two lines. He et al. (2012) extended this approach to Chinese quatrain generation, where each line of the poem is generated by translating the preceding line.\nAnother approach to poem generation is based on text summarization. For example, Yan et al. (2013) proposed a method that retrieves high-ranking candidates of sentences from a large poem corpus, and then re-arranges the candidates to generate rule-conformed new sentences.\nMore recently, deep learning methods gain much attention in poem generation. For example, Zhang and Lapata (2014) proposed an RNNbased approach that was reported to work well in quatrain generation (Zhang and Lapata, 2014); however, the structure seems rather complicated (a CNN and two RNN components in total), preventing it from extending to other genres. Our model is a simple sequence-to-sequence structure, which is much simpler than the model proposed by (Zhang and Lapata, 2014) and can be easily extended to more complex genres such as Sonnet and Haiku without modification.\nFinally, Wang et al. (2016) proposed an attention-based model for Song Iambics genera-\ntion. However, their model performed rather poor when was applied directly to quatrain generation, possibly because quatrains are more condensed and more individually unique than iambics. Our approach follows the attention-based strategy in (Wang et al., 2016), but introduces several innovations. Firstly, the poems were generated through key words rather than the first sentence to provide more clear themes; Secondly, a singleword attention mechanism was used to improve the sensitivity to key words; Thirdly, a loop generation approach was proposed to improve the fluency and coherence of the attention-based model."}, {"heading": "3 Method", "text": "In this section, we first present the attention-based Chinese poetry generation framework, and then describe the implementation of the encoder and decoder models that have been tailored for our task."}, {"heading": "3.1 Attention-based Chinese Poetry Generation", "text": "The attention-based sequence-to-sequence model proposed by Bahdanau et al. (2014) is a powerful framework for sequence generation. Specifically, the input sequence is converted by an \u2018encoder\u2019 to a sequence of hidden states to represent the semantic status at each position of the input, and these hidden states are used to regulate a \u2018decoder\u2019 that generates the target sequence. The important mechanism of the attention-based model is that at each generation step, the most relevant input units are discovered by comparing the \u2018current\u2019 status of the decoder with all the hidden states of the encoder, so that the generation is regulated by the fine structure of the input sequence.\nThe entire framework of the attention-based model applied to Chinese poetry generation is illustrated in Figure 1. The encoder (a bi-directional GRU that will be discussed shortly) converts the input keywords, a character sequence denoted by (x1, x2, ...), into a sequence of hidden states (h1, h2, ...). The decoder then generates the whole poem character by character, denoted by (y1, y2, ...). At each step t, the prediction for the next character yt is based on the \u2018current\u2019 status st of the decoder as well as all the hidden states (h1, h2, ..., hT ) of the encoder. Each hidden state hi contributes to the generation according to a rel-\nevance factor \u03b1t,i that measures the similarity between st and hi."}, {"heading": "3.2 GRU-based Model Structure", "text": "A potential problem of the RNN-based generation approach proposed by Zhang and Lapata (2014) is that the vanilla RNN used in their model tend to forget historical input quickly, leading to theme shift in generation. To alleviate the problem, Zhang and Lapata (2014) designed a composition strategy that generates only one line at each time. This is certainly not satisfactory as it complicates the generation process.\nIn our model, the quick-forgetting problem is solved by using the GRU model. For encoder, a bi-direction GRU is used to encode the input keywords, and for the decoder, another GRU is used to conduct the generation. The GRU is powerful in remembering input and thus can provide a strong memory for the theme, especially when combined with the attention mechanism."}, {"heading": "3.3 Model Training", "text": "The goal of the model training is to let the predicted character sequence match the original poem. We chose the cross entropy between the distributions over Chinese characters given by the decoder and the ground truth (essentially in a onehot form) as the objective function. To speed up the training, the minibatch stochastic gradient descent (SGD) algorithm was adopted. The gradient was computed sentence by sentence, and the AdaDelta algorithm was used to adjust the learn-\ning rate (Zeiler, 2012). Note that in the training phase, there are no keyword input, so we use the first line as the input to generate the entire poem."}, {"heading": "4 Implementation", "text": "The basic attention model does not naturally work well for Chinese poetry generation. A particular problem is that every poem was created to express a special affection of the poet, so it tends to be \u2018unique\u2019. This means that most valid (and often great) expressions can not find sufficient occurrence in the training data. Another problem is that the theme may become vague towards the end of the generation, even with the attention mechanism. Several techniques are presented to improve the model."}, {"heading": "4.1 Character Vector Initialization", "text": "Since the uniqueness of each poem, it is not simple to train the attention model from scratch, as many expressions are not statistically significant. This is a special form of data sparsity. A possible solution is to train the model in two steps: firstly learn the semantic representation of each character, possibly using a large external corpus, and then train the attention model with these pre-trained representations. By this approach, the attention model most focuses on possible expressions and hence is easier to train. In practice, we first derive character vectors using the word2vec tool1, and then use these character vectors to initialize the word embedding matrix in the attention model. Since part of the model (embedding matrix) have been pretrained, the problem of data sparsity can be largely alleviated."}, {"heading": "4.2 Input Reconstruction", "text": "Poets tend to express their feelings following an implicit theme, instead of an explicit reiteration. We found this implicit theme is not easy for machines to understand and learn, leading to possible theme drift at run-time. A simple solution is to force the model to reconstruct the input after it has generated the entire poem. More specifically, in the training phase, we use the first line of a training poem as the input, and based on this input to generate five lines sequentially: line 1-2-3-4-1. The last generation step for line 1 forces the model to keep the input in mind during the entire generation process, so leans how to focus on the theme.\n1https://code.google.com/archive/p/word2vec/"}, {"heading": "4.3 Input Vector Attention", "text": "The popular configuration of the attention model attends on hidden states. Since hidden states represent accumulated semantic meaning, this attention is good to form a global theme. However, as the semantic contents of individual keywords have been largely averaged, it is hard to generate diverse poems that are sensitive to each and different keywords.\nWe propose a multiple-attention solution that attends on both hidden states and input character vectors, so that both accumulated and individual semantics are considered during the generation. It has been found that this approach is highly effective for generating diverse and novel poems: just given sufficient keywords, new poems can be generated with high quality. Compared to other approaches such as noise injection or n-best inference, this approach can generate unlimited alternatives without any quality sacrifice. Interestingly, our experiments show that more keywords tend to generate more unexpected but highly impressive poems. Therefore, the multiple-attention approach can be regard as an interesting way to promote innovation."}, {"heading": "4.4 Hybrid-style Training", "text": "Traditional Chinese quatrains are categorized into 5-char quatrains and 7-char quatrains that involve five and seven characters per line, respectively. These two categories follow different regulations, but also share the same words and similar semantics. We propose a hybrid-style training that trains the two types of quatrains using the same model, with a \u2018type indicator\u2019 to notify the model which type the present training sample is. In our study, the type indicators are derived from eigen vectors of a 200 \u00d7 200 dimensional random matrix. Each type of quatrain is assigned a fixed eigenvector as its type indicator. The indicators are provided as part of the input to the first hidden state of the decoder, and keep constant during the training."}, {"heading": "5 Experiments", "text": "We describe the experimental settings and results in this section. Firstly the datasets used in the experiments are presented, and then we report the evaluation in three phases: (1) the first phase focuses on searching for optimal configurations for the attention model; (2) the second phase compares the attention model with other methods; (3)\nthe third phase is the Feigenbaum Test."}, {"heading": "5.1 Datasets", "text": "Two datasets are used to conduct the experiments. Firstly a Chinese quatrain corpus was collected from Internet. This corpus consists of 13, 299 5- char quatrains and 65, 560 7-char quatrains. As far as we know, this covers most of the quatrains that are retained today. We filters out some poems which contains 100% low frequency words. Through corpus cleaning, a corpus which contains 9, 195 5-char quatrains and 49, 162 7-char quatrains was obtained. 9, 000 5-char and 49, 000 7- char quatrains are used to train the GRU model of the attention model and LSTM model of a comparative model based on RNN language models and the rest poems are used as the test datasets.\nThe second dataset was used to train and derive character vectors for attention model initialization. This dataset contains 284, 899 traditional Chinese poems in various genres, including Tang quatrains, Song iambics, Yuan Songs, Ming and Qing poems. This large amount data ensures a stable learning for semantic content of most characters."}, {"heading": "5.2 Model Development", "text": "In the first evaluation, we intend to find the best configurations for the proposed attentionbased model. The \u2018Bilingual Evaluation Understudy\u2019 (BLEU) (Papineni et al., 2002) is used as the metric to determine which enhancement techniques are effective. BLEU was originally proposed to evaluate machine translation performance (Papineni et al., 2002), and was used by Zhang and Lapata (2014) to evaluate quality of poem generation. We used BLEU as a cheap evaluation metric in the development phase to determine which design option to choose, without the costly human evaluation.\nThe method proposed by He et al. (2012) and employed by Zhang and Lapata (2014) was adopted to obtain reference poems. A slight difference is that the reference set was constructed for each input keyword, instead of each sentence as in (Zhang and Lapata, 2014). This is because our attention model generates poems as an entire character sequence, while the vanilla RNN approach in (Zhang and Lapata, 2014) does that sentence by sentence. Additionally, we used 1-gram and 2- grams in the BLEU computation, according to the fact that semantic meaning is mostly represented\nby single characters and some character pairs in traditional Chinese.\nTable 2 presents the results. The baseline model is trained with character initialization where the character vectors are trained using quatrains only. This is mostly the system in (Wang et al., 2016). Then we use the large corpus that involves all traditional Chinese poems to enhance the character vectors, and the results demonstrated a noticeable performance improvement in fluency (from our human judgements) and a small improvement in BLEU (2nd row in Table 2). This is understandable since poems in different genres use similar languages, so involving more training data helps infer more reliable semantic content for each character. Additionally, we observe that reconstructing the input during model training improves the model (3rd row). This is probably due to the enhancement in theme consistence. What\u2019s more, attention to both input vectors and hidden states leads to additional performance gains (4th row). Finally, the hybrid-style training is employed to train a single model for the 5-char and 7-char quatrains. The BLEUs are tested on 5-char and 7-char quatrains respectively and the results are shown in the 5-th row of Table 2. Note that in the hybrid training, we stop the training before convergence in favor of a good BLEU.\nFrom these results, we obtain the best configuration that involves character vector trained with extern training data, input reconstruction, input vector attention and hybrid training. In the reset of the paper, we will use this configuration to train the attention model (denoted by \u2018Attention\u2019) and compare it with the comparative methods."}, {"heading": "5.3 Comparative Evaluation", "text": "In the second phase, we compare the attention model (with the best configuration) and three comparative models: the SMT model\nproposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010).\nFollowing the work of Zhang and Lapata (2014), we selected 30 subjects (e.g., falling flower, stone bridge, etc.) in the Shixuehanying taxonomy (Liu, 1735) as 30 themes. For each theme, several phrases belonging to the corresponding subject were selected as the input keywords. For the attention model, these keywords were used to generate the first line directly; For the other three models, however, the first line had to be constructed beforehand by an external model. We chose the method provide by Zhang and Lapata (2014) to generate the first lines for the SMT, vanilla RNN and LSTMLM approaches. A 5-char quatrain and a 7-char quatrain were generated for each theme by the four methods, and were evaluated by experts.\nFor reference, some poems written by ancient poets were also involved in the evaluation. Note that to prevent the impact of prior knowledge of the experts, we deliberately chose the poems that were written by poets that are not very famous. The poems were chosen from (Han, 2015), (Yoshikawa, 1963) and (Chen, 2013); and a 5-char quatrain and a 7- char quatrain were selected for each theme.\nThe evaluation was conducted by experts based on the following four metrics, in the scale from 0 to 5:\n\u2022 Compliance: if the poem satisfies the regulation on tones and rhymes;\n\u2022 Fluency: if the sentences read fluently and\nconvey reasonable meaning;\n\u2022 Consistence: if the poem adheres to a single theme;\n\u2022 Aesthesis: if the poem stimulates any aesthetic feeling.\nIn the experiments, we invited 26 experts to conduct a series of scoring evaluations2 . These experts were asked to rate the generation of our model and three comparative approaches: SMT, LSTMLM, and RNNPG. The SMT-based approach is available online3 and we use this online service to obtain the generation. For RNNPG, we invited the authors to conduct the generation for us. The LSTMLM approach was implemented by ourselves, for which we used the GRU instead of the vanilla RNN to enhance long-distance memory, and used character vector initialization to improve model training.\nPoems written by ancient poets are also involved in the test. For each method (including human-written), a 5-char quatrain and a 7-char quatrain were generated or selected for each of the 30 themes, amounting to 300 poems in total in the test. For each expert, 80 poems were randomly selected for evaluation.\nTable 3 presents the results. It can be seen that our model outperforms all the comparative approaches in terms of all the four metrics. More interestingly, we find that the scores obtained by our model are approaching to those obtained by human poets, especially with 7-char poems. This is highly encouraging and indicates that our model can imitate human beings to a large extent, at least from the eyes of contemporary experts.\n2These experts are professors and their postgraduate students in the field of Chinese poetry research. Most of them are from the Chinese Academy of Social Sciences (CASS).\n3http://duilian.msra.cn/jueju/\nThe second best approach is the LSTMLM approach. As we mentioned, LSTMLM can be regarded as a simplified version of our attention model, and shares the same strength in LSTM-based long-distance pattern learning and improved training strategy with character vector initialization. This demonstrated that a simple neural model with little engineering effort may learn artistic activities pretty good. Nevertheless, the comparative advantage of the attention model still demonstrated the importance of the attention mechanism.\nThe RNNPG and the SMT approaches perform equally worse, particularly RNNPG. A possible reason is that RNNPG requires an SMT model to enhance the performance but the SMT model was not used in this test4. In fact, even with the SMT model, RNNPG can hardly approach to human as the attention model does, as shown in the original paper (Zhang and Lapata, 2014). The SMT approach, with a bunch of unknown optimizations by the Microsoft colleagues, can deliver reasonable quality, but the limitation of the model prevents it from approaching a human-level as our model does. The T-test results show that the difference between the attention LSTM model (ours) and the vanilla RNN and SMT are both significant (p \u00a1 0.01), though the difference between the attention LSTM model and LSTMLM is weakly significant (p = 0.03).\nIt is noticeable that the human ratings of humanwritten poems are lower than the ratings reported by Zhang and Lapata (2014). We are not sure the experts that Zhang and Lapata invited, but the experts in our experiments are truly professional and critical: most of them are top-level experts on classical Chinese poetry education and criticism, and some of them are winners of national competitors in classic Chinese poetry writing.\nAdditionally, we note that almost in all the evaluations, the human-written poems beat those generated by machines. On one hand, this indicates that human are still superior in artistic activities, and on the other hand, it demonstrates from another perspective that the participants of the evaluation are truly professional and can tell good or bad poems. Interestingly, in the metric of compliance, our attention model outperforms human. This is not surprising as computers can simply\n4The author of RNNPG (Zhang and Lapata, 2014) could not find the SMT model in the reproduction, unfortunately.\nsearch vast candidate characters to ensure a ruleobeyed generation. In contrast, human artists put meaning and affection as the top priority, so sometimes break the rule.\nFinally, we see that the quality of the 7-char poems generated by our model is very close to that of the human-written poems. This should be interpreted in two perspectives: On one hand, it indicates that our generation is rather successful; On the other hand, we should pay attention that the poems we selected are from unfamous poets. Our intention was to avoid biased rating caused by experts\u2019 prior knowledge on the poems, but this may have limited the quality of the selected poems, although we have tried our best to choose good ones."}, {"heading": "5.4 Feigenbaum Test", "text": "We design a Feigenbaum Test (Feigenbaum, 2003) to evaluate the quality of poems generated by our models. Feigenbaum test (FT) can be regarded as an generalized Turing test (TT), the most well-known method for evaluating AI systems. A particular shortcoming of TT is that it is only suitable for tasks involving interactive conversions. However, there are many professional domains where no conversations are involved but still require a simple method like TT to evaluate machines\u2019 intelligence. Feigenbaum Test follows the core idea of TT, but focuses on professional tasks that can be done only by domain experts. The basic idea of FT is that an intelligent system in a professional domain should behave as a human expert, and the behavior can not be distinguished from human experts, when judged by human experts in the same domain. We believe that this is highly important when evaluating AI systems on artistic activities, for which mimicking the behavior of human experts is an important indicator of its success.\nIn this section, we follow this idea and utilize FT to evaluate the poetry generation models. Specifically, we distributed the 30 themes to some experts in traditional Chinese poem generation5 . We asked these experts to select one theme that they are most favor so that the quality can be ensured.\nWe received 144 submissions. To ensure the quality of the submission, we generated the same number of poems by our model and then asked\n5These experts were nominated by professors in the field of traditional Chinese poetry research.\na highly-professional expert in traditional Chinese poem criticism to give the first round filtering. After the filtering, 83 human-written poems (57.6%) and 180 computer-generated poems (86.5%) were remained, respectively. This indicates that humangenerated poems are in a larger variance in quality, which is not very surprising as the knowledge and skill of people tend to vary significantly.\nThe remained 263 poems were distributed to 24 experts for evaluation6. The experts were asked to answer two questions: (1) if a poem was generated by people; (2) quality of a poem, rated from 0 to 5.\nThe results for the human-machine decision are presented in Figure 2. For a clear representation, the minor proportions of zero scores are omitted in the figure. We observe that 41.27% of the human-written poems were identified as machinegenerated, and 31% of the machine-generated poems were identified as human-written. This indicates that a large number of poems can not be correctly identified by people. According to the crite-\n6These experts again are mostly from CASS, and some of them attended the previous test but not all.\nrion of Turing Test(Actually, Feigenbaum Test can be regarded as domain specific \u201dTuring Test\u201d), our model has weakly passed7. The score distributions for human-written and machine-generated poems are presented in Figure 3. It can be seen that our model is still inferior to human in average. However, a large proportion (61.9%) of the machinegenerated poems were scored equal to or more than 3, which means that our model works pretty well, as human poets can only achieves 75.6%. Interestingly, among the top-5 high-ranked poems, the machine takes the position 1 and 2, and among the top-10 high-ranked poems, the machine takes the position 1, 2 and 7. This means that our model can generate very good poems, even better than human poets, although in general it is still beat by human.\nA more detailed analysis is presented in Figure 4, where the poems are categorized into four groups according to their \u2018genuine\u2019 and \u2018identified\u2019 authors (human or machine). From the two pairs M-M vs. M-H and H-M vs. H-H, we observe that a poem tends to be rated high if the experts consider them as Human-written. This means that the identification is positively related to the score, and people still tend to recognize human writes better. This is also true anyway at present.\nTo have a better understanding of the decision process, we invited another 4 experts to specify the metrics by which the human-machine identification was made for each poem. Multiple metrics can be selected. The proportions that each metric\n7The criterion is to fool people in more than 30% of the trials. Refer to https://en.wikipedia.org/wiki/Turing_test.\nwas selected are shown in Table 4. It can seen that the experts tend to regard fluency and aesthesis as the most important factors in the decision. When evaluating human-written poems, it shows that a fluent poem tends to be identified correctly, while a poem without any aesthetic feeling tends to be recognized as machine-generated."}, {"heading": "5.5 Generation Example", "text": "Finally we show a 7-char quatrain generated by the attention model. The theme of this poem is \u2018crab-apple flower\u2019."}, {"heading": "6 Conclusion", "text": "This paper proposed an attention-based neural model for Chinese poetry generation. Compared to existing methods, the new approach is simple in model structure, strong in theme preservation, flexible to produce innovation, and easy to be extended to other genres. Our experiments show that it can generate traditional Chinese quatrains pretty\nwell and weakly pass the Feigenbaum Test. A future work will employ more generative models, e.g. variational generative deep models, to achieve more natural innovation. We also plan to extend the work to other genres of traditional Chinese poetry, e.g., Yuan songs."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Some challenges and grand challenges for computational intelligence", "author": ["Edward A Feigenbaum"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Feigenbaum.,? \\Q2003\\E", "shortCiteRegEx": "Feigenbaum.", "year": 2003}, {"title": "Study on the Liao, Jin and Yuan Poetry and Music Theory. China", "author": ["Wei Han"], "venue": null, "citeRegEx": "Han.,? \\Q2015\\E", "shortCiteRegEx": "Han.", "year": 2015}, {"title": "Generating chinese classical poems with statistical machine translation models", "author": ["He et al.2012] Jing He", "Ming Zhou", "Long Jiang"], "venue": "In Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating chinese couplets using a statistical mt approach", "author": ["Jiang", "Zhou2008] Long Jiang", "Ming Zhou"], "venue": "In Proceedings of the 22nd International Conference on Computational LinguisticsVolume", "citeRegEx": "Jiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "Using genetic algorithms to create meaningful poetic text", "author": ["Graeme Ritchie", "Henry Thompson"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "Manurung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Manurung et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Gaiku: Generating haiku with word associations norms", "author": ["Netzer et al.2009] Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad"], "venue": "In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,", "citeRegEx": "Netzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2009}, {"title": "Automatic generation of poetry: an overview", "author": ["H Oliveira"], "venue": null, "citeRegEx": "Oliveira.,? \\Q2009\\E", "shortCiteRegEx": "Oliveira.", "year": 2009}, {"title": "Poetryme: a versatile platform for poetry generation", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": "In Proceedings of the ECAI 2012 Workshop on Computational Creativity,", "citeRegEx": "Oliveira.,? \\Q2012\\E", "shortCiteRegEx": "Oliveira.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Hitch haiku: An interactive supporting system for composing haiku poem", "author": ["Tosa et al.2009] Naoko Tosa", "Hideto Obara", "Michihiko Minoh"], "venue": "In Entertainment Computing-ICEC", "citeRegEx": "Tosa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tosa et al\\.", "year": 2009}, {"title": "Chinese song iambics generation with neural attention-based model", "author": ["Wang et al.2016] Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A Summary of Rhyming Constraints of Chinese Poems", "author": ["Li Wang"], "venue": null, "citeRegEx": "Wang.,? \\Q2002\\E", "shortCiteRegEx": "Wang.", "year": 2002}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Wu et al.2009] Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu"], "venue": "In Entertainment Computing\u2013ICEC", "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "I, poet: automatic chinese poetry composition through a generative summarization framework under constrained optimization", "author": ["Yan et al.2013] Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Genetic algorithm and its implementation of automatic generation of chinese songci", "author": ["Zhou et al.2010] Cheng-Le Zhou", "Wei You", "Xiaojun Ding"], "venue": "Journal of Software,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "An example of quatrain written by Lun Lu, a famous poet in Tang Dynasty (Wang, 2002), is shown in Table 1.", "startOffset": 72, "endOffset": 84}, {"referenceID": 4, "context": ", by machine translation models (He et al., 2012) and recurrent neural networks (RNN) (Zhang and Lapata, 2014).", "startOffset": 32, "endOffset": 49}, {"referenceID": 9, "context": "For example, Tosa et al. (2009) and Wu et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 9, "context": "For example, Tosa et al. (2009) and Wu et al. (2009) employed a phrase search approach for Japanese poem generation, and Netzer et al.", "startOffset": 13, "endOffset": 53}, {"referenceID": 8, "context": "(2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms.", "startOffset": 75, "endOffset": 96}, {"referenceID": 8, "context": "(2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms. Oliveira (2009) and Oliveira (2012) used semantic and grammar templates for Spanish poem generation.", "startOffset": 75, "endOffset": 166}, {"referenceID": 8, "context": "(2009) employed a phrase search approach for Japanese poem generation, and Netzer et al. (2009) proposed an approach based on word association norms. Oliveira (2009) and Oliveira (2012) used semantic and grammar templates for Spanish poem generation.", "startOffset": 75, "endOffset": 186}, {"referenceID": 18, "context": "For example, Zhou et al. (2010) proposed to use a stochastic search algorithm to obtain the best matched sentences.", "startOffset": 13, "endOffset": 32}, {"referenceID": 6, "context": "The search algorithm is based on four standards proposed by Manurung et al. (2012): fluency, meaningful, poetic, and coherent.", "startOffset": 60, "endOffset": 83}, {"referenceID": 4, "context": "He et al. (2012) extended this approach to Chinese quatrain generation, where each line of the poem is generated by translating the preceding line.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "For example, Yan et al. (2013) proposed a method that retrieves", "startOffset": 13, "endOffset": 31}, {"referenceID": 13, "context": "Finally, Wang et al. (2016) proposed an attention-based model for Song Iambics genera-", "startOffset": 9, "endOffset": 28}, {"referenceID": 13, "context": "Our approach follows the attention-based strategy in (Wang et al., 2016), but introduces several innovations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 0, "context": "The attention-based sequence-to-sequence model proposed by Bahdanau et al. (2014) is a powerful framework for sequence generation.", "startOffset": 59, "endOffset": 82}, {"referenceID": 17, "context": "ing rate (Zeiler, 2012).", "startOffset": 9, "endOffset": 23}, {"referenceID": 11, "context": "The \u2018Bilingual Evaluation Understudy\u2019 (BLEU) (Papineni et al., 2002) is used as the metric to determine which enhancement techniques are effective.", "startOffset": 45, "endOffset": 68}, {"referenceID": 11, "context": "BLEU was originally proposed to evaluate machine translation performance (Papineni et al., 2002), and was used", "startOffset": 73, "endOffset": 96}, {"referenceID": 4, "context": "The method proposed by He et al. (2012) and employed by Zhang and Lapata (2014) was adopted to obtain reference poems.", "startOffset": 23, "endOffset": 40}, {"referenceID": 4, "context": "The method proposed by He et al. (2012) and employed by Zhang and Lapata (2014) was adopted to obtain reference poems.", "startOffset": 23, "endOffset": 80}, {"referenceID": 13, "context": "This is mostly the system in (Wang et al., 2016).", "startOffset": 29, "endOffset": 48}, {"referenceID": 7, "context": "(2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010).", "startOffset": 254, "endOffset": 276}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al.", "startOffset": 12, "endOffset": 29}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al.", "startOffset": 12, "endOffset": 105}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010). Following the work of Zhang and Lapata (2014), we selected 30 subjects (e.", "startOffset": 12, "endOffset": 346}, {"referenceID": 4, "context": "proposed by He et al. (2012), the vanilla RNN poem generaion (RNNPG) proposed by Zhang and Lapata (2014), and an RNN language model (RNNLM) that can be regarded as a simplified version (One-direction LSTM RNN neural network without attention mechanism) of the attention model (Mikolov et al., 2010). Following the work of Zhang and Lapata (2014), we selected 30 subjects (e.g., falling flower, stone bridge, etc.) in the Shixuehanying taxonomy (Liu, 1735) as 30 themes. For each theme, several phrases belonging to the corresponding subject were selected as the input keywords. For the attention model, these keywords were used to generate the first line directly; For the other three models, however, the first line had to be constructed beforehand by an external model. We chose the method provide by Zhang and Lapata (2014) to generate the first lines for the SMT, vanilla RNN and LSTMLM approaches.", "startOffset": 12, "endOffset": 827}, {"referenceID": 3, "context": "The poems were chosen from (Han, 2015), (Yoshikawa, 1963)", "startOffset": 27, "endOffset": 38}, {"referenceID": 2, "context": "We design a Feigenbaum Test (Feigenbaum, 2003) to evaluate the quality of poems generated by our models.", "startOffset": 28, "endOffset": 46}], "year": 2016, "abstractText": "Recent progress in neural learning demonstrated that machines can do well in regularized tasks, e.g., the game of Go. However, artistic activities such as poem generation are still widely regarded as human\u2019s special capability. In this paper, we demonstrate that a simple neural model can imitate human in some tasks of art generation. We particularly focus on traditional Chinese poetry, and show that machines can do as well as many contemporary poets and weakly pass the Feigenbaum Test, a variant of Turing test in professional domains. Our method is based on an attention-based recurrent neural network, which accepts a set of keywords as the theme and generates poems by looking at each keyword during the generation. A number of techniques are proposed to improve the model, including character vector initialization, attention to input and hybrid-style training. Compared to existing poetry generation methods, our model can generate much more theme-consistent and semantic-rich poems.", "creator": "LaTeX with hyperref package"}}}