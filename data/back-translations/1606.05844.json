{"id": "1606.05844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2016", "title": "Statistical Parametric Speech Synthesis Using Bottleneck Representation From Sequence Auto-encoder", "abstract": "In this paper, we describe a statistically parametric approach to speech synthesis with acoustic representation at the unit level. In conventional speech synthesis based on neural networks, the input text properties are repeated for the entire duration of the phoneme to represent text and speech parameters. This representation is learned at the frame level, the de facto acoustic representation. However, a large part of this computational requirement can be drastically reduced if each unit can be represented with a fixed dimensional representation. Using recursive neural network-based auto-encoders, we show that it is actually possible to map units of different duration to a single vector. We then use this acoustic representation at the unit level to synthesize speech using deep neural network-based statistical parametric speech synthesis.", "histories": [["v1", "Sun, 19 Jun 2016 08:38:26 GMT  (475kb,D)", "http://arxiv.org/abs/1606.05844v1", "5 pages (with references)"]], "COMMENTS": "5 pages (with references)", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["sivanand achanta", "knrk raju alluri", "suryakanth v gangashetty"], "accepted": false, "id": "1606.05844"}, "pdf": {"name": "1606.05844.pdf", "metadata": {"source": "CRF", "title": "Statistical Parametric Speech Synthesis Using Bottleneck Representation From Sequence Auto-encoder", "authors": ["Sivanand Achanta", "KNRK Raju Alluri", "Suryakanth V Gangashetty"], "emails": ["sivanand.a@research.iiit.ac.in,", "raju.alluri@research.iiit.ac.in,", "svg@iiit.ac.in"], "sections": [{"heading": "1. Introduction", "text": "There are three fundamental problems in statistical parametric speech synthesis (SPSS) that have been outlined [1], one of them is acoustic modeling. Traditionally hidden Markov models (HMM) were used for SPSS [2], however there is now growing interest in the community to use deep learning techniques for acoustic modeling in SPSS. It has been demonstrated in [3] [4] that deep neural networks (DNN) can perform better than traditional HMMs for SPSS. Recently, DNNs have been replaced by recurrent neural networks (RNNs) [5] [6] [7] as they better capture the temporal dependencies.\nHowever both these models use traditional frame-level representation of speech signals for mapping text features to speech parameters. One draw back of this is that the text features have to be repeated and made to evolve at the same frame-rate as speech parameters which increases the computation. This can be reduced drastically if the speech parameters of a unit (for the experiments in this paper unit is chosen as phoneme and hence terms unit and phoneme are interchangeably used) are given a fixed dimensional representation and then mapping takes place at phoneme level. This manner of statistical mapping of features at unit-level has remained unexplored so far within the SPSS community. Since the duration of different units vary, the fundamental problem is to find a mechanism for representing these units with a single or fixed number of multiple vectors.\nThanks to the recent sequence-to-sequence learning techniques introduced in [8] [9] which allow us to map sequences of varying lengths. Typically this is achieved via neural network encoder-decoder architectures [10]. Broadly, the encoder-\ndecoder models can be grouped into two types for mapping one sequence to another sequence of different lengths, one is to map the input sequence to a fixed dimensional vector and then to map it to the target sequence. We refer to this architecture as encoder-decoder with fixed vector context representation (EDFVC) throughout this paper [9] [10]. The other is to soft-search the input features that align with the output using an alignment model [11].\nThe first contribution of this paper is that, we explore RNN encoder-decoder for auto-encoding the acoustic features of units to a single vector referred to as recurrent bottleneck feature (RBN). Auto-encoders using RNN encoder-decoder architecture have earlier been used for semi-supervised sequence learning and as pre-training in [12] [13]. However, the RBN feature, which is exploited in this work, has never been directly used in the prior works. This method detailed in section 2 can be viewed as RNN based analysis-by-synthesis (AbS) of speech using the explicit unit-boundary information.\nOnce we train the auto-encoder, these RBN features can be used for mapping from text-features. In our experiments DNN is used for the purpose of mapping text to unit-level acoustic features, although, in principle any other regression technique like classification and regression trees [14] or random forests [15] [16] [17] can be used. In addition, these RBN features can be used in unit-selection based approaches for computing the concatenation cost and/or for the final synthesis.\nThe second contribution of this paper is that, simple recurrent neural (SRN) units are used as basic RNN units as opposed to the typically used long-short term memory units (LSTM) [18] which have complicated gating mechanisms. In the RNN encoder-decoder network, using SRNs as units we show that it is indeed possible to auto-encode units. Although SRNs have been proposed as an alternative to LSTMs, they have not been used as part of sequence-to-sequence neural networks so far.\nThe paper is organized as follows: In section 2 we describe the architecture of the RNN encoder-decoder model which enables us to auto-encode units, section 3 describes AbS and SPSS experiments using RBN features and its results. The conclusions and scope for future work are finally presented in section 4."}, {"heading": "2. RNN Encoder-Decoder Model", "text": "In this section we describe the architecture of encoder-decoder network with fixed vector context."}, {"heading": "2.1. ED-FVC", "text": "The block diagram of ED-FVC is presented in Fig. 1. EDFVC architecture consists of a uni-directional RNN as encoder and decoder. The last hidden state of encoder is taken as initial\nar X\niv :1\n60 6.\n05 84\n4v 1\n[ cs\n.S D\n] 1\n9 Ju\nn 20\n16\nhidden state of the decoder and also as the context vector. It is this last hidden state of the encoder which we refer to as RBN."}, {"heading": "2.1.1. SRN", "text": "In this section, we briefly review SRNs which will be used in the ED-FVC network. Typically the encoder-decoder architectures use either LSTMs [9] or gated-recurrent units (GRU) [19] [11] [10] as RNN units, to avoid the vanishing gradients problem [20]. However, in this paper we use simple RNNs. There are two reasons why SRNs can be preferred over LSTMs/GRUs: (1) The number of parameters in LSTM/GRU are far more for a given hidden state size compared to simple RNN and (2) It is difficult to know which of those gates are actually contributing to the performance. Recently, there have been some works which have tried to address the latter drawback [21] [22], and specifically in the context of SPSS in [23].\nSRNs are RNNs without any advanced gating mechanisms. It has been shown in previous studies that Elman RNNs [24] work well if the initialization scheme used is more robust and gradient clipping is employed to avoid gradients overflow [25] [26] [27]. In [26], authors proposed to solve the vanishing gradient problem using diagonal initialization which was inspired by orthogonal initialization suggested in [28] and sparse initialization. More recently [27] reports SRNs being able to memorize long-term dependencies by initializing recurrent weight matrix with a special structure and using rectified linear units [29]. However, these have not been explored in the context of sequence to sequence learning. In this work we propose to use IRNN (SRNs with recurrent weight matrix initialized to identity) in the encoder-decoder models. Also we note that authors in [26] suggest ReLU units with the identity initialization. However, we in our experiments found that gradients explode quickly when using ReLU units, instead we use tanh nonlinearity for our experiments."}, {"heading": "2.1.2. RNN Encoder", "text": "The encoder in our case is a uni-directional recurrent neural network. The encoder reads the entire input sequence and a representation is stored in the final state vector.\nhft = f(Wfixt +Wfht\u22121) (1)\nWfi,Wf are the input and recurrent weight matrices corresponding to recurrent layer."}, {"heading": "2.1.3. Fixed-dimensional context vector", "text": "Context vector is formed by taking the final state vector of encoder. This context vector is used as an initial state vector s0 in the decoder similar to [9]\nc = [hfTx ] \u2032\ns0 = c (2)"}, {"heading": "2.1.4. RNN Decoder", "text": "The decoder is also a uni-directional RNN.\nst = f(Wiyt\u22121 +Wst\u22121 +Wcc) yt = g(Uist + Uyt\u22121 + Ucc) (3)\nWi,W and Wc denote the weight connections from past output, past hidden state and context layers. It has to be noted that the past output yt\u22121, during training, can be either from the ground truth or from the prediction of the network itself. Similarly, Ui, U and Uc denote the weight connections from current hidden state, past output and context layers. g in our case is a linear layer."}, {"heading": "2.1.5. Back-propagation of error signal", "text": "The following equations describe the calculation of error signal for updating the weights of encoder-decoder structure. The computation of error signal for decoder is given as:\n\u03b4yt = Dgt(Wi\u03b4st+1 + U\u03b4yt+1 + iyt)\nist = Ui\u03b4yt \u03b4st = Dft(W\u03b4st+1 + ist) (4)\nwhere iyt is the injected error signal which for our choice of g and squared error loss function is yt \u2212 dt, dt is the target output. Dgt , Dft are the derivatives of the functions at output and hidden layers respectively.\nThe error signal for encoder are computed\n\u03b4hft = Dhft(Wf\u03b4hf(t+1) + ihft) (5)\nThe injected error signal for the encoder is non-zero only at the final state Tx at all other times it is zero and Dhft is the\nderivative of the function at hidden layer. This error signal is then back-propagated recursively from Tx all the way down to initial time instant.\nIn the next section we describe experiments using the EDFVC architecture as an auto-encoder. Since the input and output to the ED-FVC model are same we refer to this model as recurrent auto-encoder (RAE) from here on."}, {"heading": "3. Experiments and Results", "text": "We use a subset of Blizzard challenge 2015 database for our experiments. Blizzard challenge 2015 database contains about 4 hours of speech data in each of three Indian languages (Hindi, Tamil and Telugu), and about 2 hours of speech data in each\nof other three Indian languages (Marathi, Bengali and Malayalam), all recorded by native professional speakers in high quality studio environments. We used Telugu language dataset for our experiments. The speech recordings released were sampled at 16KHz. Phone-level alignments were performed using the EHMM tool [30].\n50 dimensional Mel-general cepstral (MGC) features and 26 dimensional band-aperiodicities (BAP) were extracted with a frame-shift of 5 ms for all the speech utterances along with their deltas and double-deltas. This feature extraction is followed from HTS-STRAIGHT demo available online. In all our experiments, natural f0, BAP (when not predicted from network) and duration were used during synthesis. There are in total around 200000 units in the dataset which was divided into training/validation/test sets as 188000, 5660 and 5660 units respectively. The duration of units typically ranged from 4 to 30 frames as can be seen from Fig. 2."}, {"heading": "3.1. RAE based AbS experiments", "text": "Based on the labels, parametric sequences of each phone are prepared for training the RAE. The architecture of RAE used was enc = {xL 500N} , dec = {500N xL}, where \u201cL\u201d represents linear units and \u201cN\u201d represents tanh units.\nThe training process is schematically represented in Fig. 3, each unit along with its boundaries demarcated by dashed-lines are presented frame-wise (frames indicated with solid lines) to the RAE during training. The mean squared error over the entire training set is minimized. Once the training is complete, RBN features are obtained by presenting the units to the encoder and extracting the last hidden state. These RBN features along with the number of frames of the unit are stored as unit-level acoustic feature representation.\nThree different RAEs have been trained using different set of features.\n\u2022 MGC static features alone (x=50).\n\u2022 MGC with delta features (x=150).\n\u2022 MGC and BAP with delta features (x=228) .\nThe input, output weights were initialized with Gaussian distribution with variance 0.01 and 0.01 respectively for MGC and 0.003 and 0.003 for MGC with deltas networks. We found that the network was sensitive to scale of variance at input and output to converge to a good local minimum.\nDuring synthesis, the RBN features are given as input to the decoder and the output sequence is predicted. Note here that while training the RAE the past output is taken from the ground truth parameter sequence during testing time the predicted past output is used.\nThe Mel-cepstral distortion between the original and resynthesized spectra is reported in Table 1. It can be seen that MGC features either static or including the dynamic features perform well. Usually MCD of less than 4dB is an indicator of high-quality synthesis [31]. The third row in the table (MGC-DD-nml) refers to experiments performed by normalizing the MGC-DD features between 0.01 and 0.99. The normalization was not needed for MGC static coefficients but was helpful when considering the dynamic coefficients. This may be because the dynamic range of static and dynamic coefficients are very different. However when BAP coefficients are appended the RAE did not perform well although normalization was done. This suggest that training different networks for BAP and MGC may be more appropriate. The MOS scores\nof synthesized speech signals and the natural speech are depicted in column 1 of Table 1 which shows that the quality of the AbS speech is high. An example spectrogram of natural speech recording and the proposed RBN feature based AbS is shown in Fig. 4"}, {"heading": "3.2. SPSS using RBN feature", "text": "In this section, experiments using RBN as acoustic features in a DNN based SPSS system is explained. As a baseline system we take frame-level text and acoustic feature sequences 2800000, 70000 and 70000 feature vectors respectively for training, validation and testing. A 345L 1000R 1000R 50L system was trained to predict frame level acoustic parameters (MGCs). For the proposed system, RBN features corresponding to 200000 units are used for training another DNN. 2800000 feature vectors are reduced to about 200000 feature vectors when mapping at the unit-level instead of frame level a reduction factor of 14. The architecture of the DNN was 345L 1000R 1000R 500L. The predicted RBNs are then fed to the decoder network of the trained RAE as discussed in the previous section. The RAE trained with static MGC was used since it was the best performing amongst the AbS systems. The output speech is synthesized using the predicted MGC, natural BAP and f0 using STRAIGHT vocoder [32]. Note that dynamics were not used during training of any of the systems and hence MLPG [?] could not be applied which results in discontinuous synthesis. This probably is the reason for low MOS score in our results.\nThe MCD and MOS scores of the baseline and the proposed are reported in table 2. It can be seen that both the SPSS systems\ngive rise to similar MCD and MOS scores. The code for replicating the experiments can be found online 1."}, {"heading": "4. Conclusions and Scope For Future Work", "text": "In this work, we have explored the possibility of compactly representing the acoustic parametric sequences of units with a single vector using sequence-to-sequence auto-encoders. While achieving compression it was shown that this process of recurrent auto-encoding does not affect the speech quality. The intermediately obtained RBN features were used as unit-level acoustic features which were then mapped from the text features. This representation was shown to greatly reduce the computational cost of text-to-speech mapping. A DNN based SPSS system was built to demonstrate that the RBN features can indeed be used as acoustic features within SPSS framework.\nThere are many interesting extensions to the current work. At feature-level auto encoding can be done at waveform level instead of high level features like spectrum used in this work. The use of RAE as a post filter is one other direction for exploration. Since the phoneme durations are known apriori during the synthesis the above training method fits into the postfiltering scheme easily."}, {"heading": "5. Acknowledgements", "text": "The authors like to acknowledge TCS for partially funding first authors PhD. Also authors would like to thank speech and vision lab members for participating in the listening tests.\n1https://goo.gl/kJBGyg"}, {"heading": "6. References", "text": "[1] H. Zen, K. Tokuda, and A. W. Black, \u201cStatistical parametric\nspeech synthesis,\u201d Speech Communication, vol. 51, no. 11, pp. 1039 \u2013 1064, 2009.\n[2] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kitamura, \u201cSpeech parameter generation algorithms for HMM-based speech synthesis,\u201d in Proc. ICASSP, 2000, pp. 1315\u20131318.\n[3] H. Zen, A. Senior, and M. Schuster, \u201cStatistical parametric speech synthesis using deep neural networks,\u201d in Proc. ICASSP, 2013, pp. 7962\u20137966.\n[4] Y. Qian, Y. Fan, W. Hu, and F. K. Soong, \u201cOn the training aspects of Deep Neural Network (DNN) for parametric TTS synthesis,\u201d in Proc. ICASSP, 2014, pp. 3829\u20133833.\n[5] H. Zen and H. Sak, \u201cUnidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for LowLatency Speech Synthesis,\u201d in Proc. ICASSP, 2015, pp. 4470\u2013 4474.\n[6] Y. Fan, Y. Qian, F.-L. Xie, and F. K. Soong, \u201cTTS Synthesis with Bidirectional LSTM Based Recurrent Neural Networks,\u201d in Proc. INTERSPEECH, 2014, pp. 1964\u20131968.\n[7] S. Achanta, T. Godambe, and S. V. Gangashetty, \u201cAn investigation of recurrent neural network architectures for statistical parametric speech synthesis,\u201d in Proc. INTERSPEECH, 2015, pp. 2524\u2013 2528.\n[8] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[9] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[10] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.\n[11] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2014.\n[12] A. M. Dai and Q. V. Le, \u201cSemi-supervised sequence learning,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 3061\u20133069.\n[13] D. Gillick, C. Brunk, O. Vinyals, and A. Subramanya, \u201cMultilingual language processing from bytes,\u201d CoRR, vol. abs/1512.00103, 2015. [Online]. Available: http://arxiv.org/abs/ 1512.00103\n[14] L. Breiman, Classification and regression trees. Chapman & Hall/CRC, 1984.\n[15] \u2014\u2014, \u201cRandom forests,\u201d Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.\n[16] A. W. Black, \u201cCLUSTERGEN: A statistical parametric synthesizer using trajectory modeling,\u201d in Proc. ICSLP, 2006.\n[17] A. W. Black and P. K. Muthukumar, \u201cRandom forests for statistical speech synthesis,\u201d in INTERSPEECH, 2015, pp. 1211\u20131215.\n[18] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[19] K. Cho, B. van Merrie\u0308nboer, D. Bahdanau, and Y. Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014.\n[20] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.\n[21] K. Greff, R. K. Srivastava, J. Koutn\u0131\u0301k, B. R. Steunebrink, and J. Schmidhuber, \u201cLSTM: A search space odyssey,\u201d arXiv preprint arXiv:1503.04069, 2015.\n[22] R. Jozefowicz, W. Zaremba, and I. Sutskever, \u201cAn empirical exploration of recurrent network architectures,\u201d in Proc. ICML, 2015, pp. 2342\u20132350.\n[23] Z. Wu and S. King, \u201cInvestigating gated recurrent neural networks for speech synthesis,\u201d arXiv preprint arXiv:1601.02539, 2016.\n[24] J. L. Elman, \u201cFinding structure in time,\u201d Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[25] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu, \u201cAdvances in optimizing recurrent networks,\u201d in Proc. ICASSP, 2013, pp. 8624\u20138628.\n[26] Q. V. Le, N. Jaitly, and G. E. Hinton, \u201cA simple way to initialize recurrent networks of rectified linear units,\u201d arXiv preprint arXiv:1504.00941, 2015.\n[27] S. S. Talathi and A. Vartak, \u201cImproving performance of recurrent neural network with ReLU nonlinearity,\u201d CoRR, vol. abs/1511.03771, 2015. [Online]. Available: http://arxiv.org/abs/ 1511.03771\n[28] A. M. Saxe, J. L. McClelland, and S. Ganguli, \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks,\u201d arXiv preprint arXiv:1312.6120, 2013.\n[29] V. Nair and G. E. Hinton, \u201cRectified linear units improve restricted boltzmann machines,\u201d in Proc. ICML, 2010, pp. 807\u2013814.\n[30] K. Prahallad, \u201cAutomatic building of synthetic voices from audio books,\u201d Ph.D. dissertation, Carnegie Mellon University, Pittsburgh, USA, 2010.\n[31] J. Kominek, T. Schultz, and A. W. Black, \u201cSynthesizer voice quality of new languages calibrated with mean mel cepstral distortion,\u201d in Spoken Languages Technologies for Under-Resourced Languages, 2008.\n[32] H. Kawahara, I. Masuda-Katsuse, and A. de Cheveign, \u201cRestructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based f0 extraction: Possible role of a repetitive structure in sounds,\u201d Speech Communication, vol. 27, no. 34, pp. 187\u2013207, 1999."}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039 \u2013 1064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["K. Tokuda", "T. Yoshimura", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proc. ICASSP, 2000, pp. 1315\u20131318.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. ICASSP, 2013, pp. 7962\u20137966.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "On the training aspects of Deep Neural Network (DNN) for parametric TTS synthesis", "author": ["Y. Qian", "Y. Fan", "W. Hu", "F.K. Soong"], "venue": "Proc. ICASSP, 2014, pp. 3829\u20133833.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low- Latency Speech Synthesis", "author": ["H. Zen", "H. Sak"], "venue": "Proc. ICASSP, 2015, pp. 4470\u2013 4474.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "TTS Synthesis with Bidirectional LSTM Based Recurrent Neural Networks", "author": ["Y. Fan", "Y. Qian", "F.-L. Xie", "F.K. Soong"], "venue": "Proc. INTERSPEECH, 2014, pp. 1964\u20131968.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "An investigation of recurrent neural network architectures for statistical parametric speech synthesis", "author": ["S. Achanta", "T. Godambe", "S.V. Gangashetty"], "venue": "Proc. INTERSPEECH, 2015, pp. 2524\u2013 2528.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1211.3711, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Proc. ICLR, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3061\u20133069.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Multilingual language processing from bytes", "author": ["D. Gillick", "C. Brunk", "O. Vinyals", "A. Subramanya"], "venue": "CoRR, vol. abs/1512.00103, 2015. [Online]. Available: http://arxiv.org/abs/ 1512.00103", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Classification and regression trees", "author": ["L. Breiman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1984}, {"title": "Random forests", "author": ["\u2014\u2014"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "CLUSTERGEN: A statistical parametric synthesizer using trajectory modeling", "author": ["A.W. Black"], "venue": "Proc. ICSLP, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Random forests for statistical speech synthesis", "author": ["A.W. Black", "P.K. Muthukumar"], "venue": "INTERSPEECH, 2015, pp. 1211\u20131215.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proc. ICML, 2015, pp. 2342\u20132350.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating gated recurrent neural networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "arXiv preprint arXiv:1601.02539, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "Proc. ICASSP, 2013, pp. 8624\u20138628.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving performance of recurrent neural network with ReLU nonlinearity", "author": ["S.S. Talathi", "A. Vartak"], "venue": "CoRR, vol. abs/1511.03771, 2015. [Online]. Available: http://arxiv.org/abs/ 1511.03771", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "arXiv preprint arXiv:1312.6120, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proc. ICML, 2010, pp. 807\u2013814.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic building of synthetic voices from audio books", "author": ["K. Prahallad"], "venue": "Ph.D. dissertation, Carnegie Mellon University, Pittsburgh, USA, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion", "author": ["J. Kominek", "T. Schultz", "A.W. Black"], "venue": "Spoken Languages Technologies for Under-Resourced Languages, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based f0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A. de Cheveign"], "venue": "Speech Communication, vol. 27, no. 34, pp. 187\u2013207, 1999.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "There are three fundamental problems in statistical parametric speech synthesis (SPSS) that have been outlined [1], one of them is acoustic modeling.", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Traditionally hidden Markov models (HMM) were used for SPSS [2], however there is now growing interest in the community to use deep learning techniques for acoustic modeling in SPSS.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "It has been demonstrated in [3] [4] that deep neural networks (DNN) can perform better than traditional HMMs for SPSS.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "It has been demonstrated in [3] [4] that deep neural networks (DNN) can perform better than traditional HMMs for SPSS.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Recently, DNNs have been replaced by recurrent neural networks (RNNs) [5] [6] [7] as they better capture the temporal dependencies.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Recently, DNNs have been replaced by recurrent neural networks (RNNs) [5] [6] [7] as they better capture the temporal dependencies.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Recently, DNNs have been replaced by recurrent neural networks (RNNs) [5] [6] [7] as they better capture the temporal dependencies.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Thanks to the recent sequence-to-sequence learning techniques introduced in [8] [9] which allow us to map sequences of varying lengths.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Thanks to the recent sequence-to-sequence learning techniques introduced in [8] [9] which allow us to map sequences of varying lengths.", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "Typically this is achieved via neural network encoder-decoder architectures [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "We refer to this architecture as encoder-decoder with fixed vector context representation (EDFVC) throughout this paper [9] [10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "We refer to this architecture as encoder-decoder with fixed vector context representation (EDFVC) throughout this paper [9] [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "The other is to soft-search the input features that align with the output using an alignment model [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Auto-encoders using RNN encoder-decoder architecture have earlier been used for semi-supervised sequence learning and as pre-training in [12] [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Auto-encoders using RNN encoder-decoder architecture have earlier been used for semi-supervised sequence learning and as pre-training in [12] [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "In our experiments DNN is used for the purpose of mapping text to unit-level acoustic features, although, in principle any other regression technique like classification and regression trees [14] or random forests [15] [16] [17] can be used.", "startOffset": 191, "endOffset": 195}, {"referenceID": 14, "context": "In our experiments DNN is used for the purpose of mapping text to unit-level acoustic features, although, in principle any other regression technique like classification and regression trees [14] or random forests [15] [16] [17] can be used.", "startOffset": 214, "endOffset": 218}, {"referenceID": 15, "context": "In our experiments DNN is used for the purpose of mapping text to unit-level acoustic features, although, in principle any other regression technique like classification and regression trees [14] or random forests [15] [16] [17] can be used.", "startOffset": 219, "endOffset": 223}, {"referenceID": 16, "context": "In our experiments DNN is used for the purpose of mapping text to unit-level acoustic features, although, in principle any other regression technique like classification and regression trees [14] or random forests [15] [16] [17] can be used.", "startOffset": 224, "endOffset": 228}, {"referenceID": 17, "context": "The second contribution of this paper is that, simple recurrent neural (SRN) units are used as basic RNN units as opposed to the typically used long-short term memory units (LSTM) [18] which have complicated gating mechanisms.", "startOffset": 180, "endOffset": 184}, {"referenceID": 8, "context": "Typically the encoder-decoder architectures use either LSTMs [9] or gated-recurrent units (GRU) [19] [11] [10] as RNN units, to avoid the vanishing gradients problem [20].", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "Typically the encoder-decoder architectures use either LSTMs [9] or gated-recurrent units (GRU) [19] [11] [10] as RNN units, to avoid the vanishing gradients problem [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "Typically the encoder-decoder architectures use either LSTMs [9] or gated-recurrent units (GRU) [19] [11] [10] as RNN units, to avoid the vanishing gradients problem [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "Typically the encoder-decoder architectures use either LSTMs [9] or gated-recurrent units (GRU) [19] [11] [10] as RNN units, to avoid the vanishing gradients problem [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "Typically the encoder-decoder architectures use either LSTMs [9] or gated-recurrent units (GRU) [19] [11] [10] as RNN units, to avoid the vanishing gradients problem [20].", "startOffset": 166, "endOffset": 170}, {"referenceID": 20, "context": "Recently, there have been some works which have tried to address the latter drawback [21] [22], and specifically in the context of SPSS in [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Recently, there have been some works which have tried to address the latter drawback [21] [22], and specifically in the context of SPSS in [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "Recently, there have been some works which have tried to address the latter drawback [21] [22], and specifically in the context of SPSS in [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "It has been shown in previous studies that Elman RNNs [24] work well if the initialization scheme used is more robust and gradient clipping is employed to avoid gradients overflow [25] [26] [27].", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "It has been shown in previous studies that Elman RNNs [24] work well if the initialization scheme used is more robust and gradient clipping is employed to avoid gradients overflow [25] [26] [27].", "startOffset": 180, "endOffset": 184}, {"referenceID": 25, "context": "It has been shown in previous studies that Elman RNNs [24] work well if the initialization scheme used is more robust and gradient clipping is employed to avoid gradients overflow [25] [26] [27].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "It has been shown in previous studies that Elman RNNs [24] work well if the initialization scheme used is more robust and gradient clipping is employed to avoid gradients overflow [25] [26] [27].", "startOffset": 190, "endOffset": 194}, {"referenceID": 25, "context": "In [26], authors proposed to solve the vanishing gradient problem using diagonal initialization which was inspired by orthogonal initialization suggested in [28] and sparse initialization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [26], authors proposed to solve the vanishing gradient problem using diagonal initialization which was inspired by orthogonal initialization suggested in [28] and sparse initialization.", "startOffset": 157, "endOffset": 161}, {"referenceID": 26, "context": "More recently [27] reports SRNs being able to memorize long-term dependencies by initializing recurrent weight matrix with a special structure and using rectified linear units [29].", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "More recently [27] reports SRNs being able to memorize long-term dependencies by initializing recurrent weight matrix with a special structure and using rectified linear units [29].", "startOffset": 176, "endOffset": 180}, {"referenceID": 25, "context": "Also we note that authors in [26] suggest ReLU units with the identity initialization.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "This context vector is used as an initial state vector s0 in the decoder similar to [9]", "startOffset": 84, "endOffset": 87}, {"referenceID": 29, "context": "Phone-level alignments were performed using the EHMM tool [30].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "Usually MCD of less than 4dB is an indicator of high-quality synthesis [31].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "The output speech is synthesized using the predicted MGC, natural BAP and f0 using STRAIGHT vocoder [32].", "startOffset": 100, "endOffset": 104}], "year": 2016, "abstractText": "In this paper, we describe a statistical parametric speech synthesis approach with unit-level acoustic representation. In conventional deep neural network based speech synthesis, the input text features are repeated for the entire duration of phoneme for mapping text and speech parameters. This mapping is learnt at the frame-level which is the de-facto acoustic representation. However much of this computational requirement can be drastically reduced if every unit can be represented with a fixed-dimensional representation. Using recurrent neural network based auto-encoder, we show that it is indeed possible to map units of varying duration to a single vector. We then use this acoustic representation at unit-level to synthesize speech using deep neural network based statistical parametric speech synthesis technique. Results show that the proposed approach is able to synthesize at the same quality as the conventional frame based approach at a highly reduced computational cost.", "creator": "LaTeX with hyperref package"}}}