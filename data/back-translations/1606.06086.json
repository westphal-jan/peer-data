{"id": "1606.06086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Uncertainty in Neural Network Word Embedding: Exploration of Threshold for Similarity", "abstract": "Word Embedding, especially with its recent developments, promises to quantify the similarity between terms. However, it is not clear to what extent this similarity value can really be useful and useful for subsequent tasks. We examine how the similarity value obtained from the models is actually an indication of the similarity of terms. First, we consider and quantify the uncertainty factor of the Word Embedding models with respect to the similarity value. Based on this factor, we introduce a general threshold to different dimensions that effectively filters the strongly related terms. Our evaluation of four collections of information supports the effectiveness of our approach, as the results of the threshold introduced are significantly better than the baseline, while they are equal to the optimal results or statistically indistinguishable.", "histories": [["v1", "Mon, 20 Jun 2016 12:31:13 GMT  (570kb,D)", "http://arxiv.org/abs/1606.06086v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["navid rekabsaz", "mihai lupu", "allan hanbury"], "accepted": false, "id": "1606.06086"}, "pdf": {"name": "1606.06086.pdf", "metadata": {"source": "CRF", "title": "Uncertainty in Neural Network Word Embedding Exploration of Threshold for Similarity", "authors": ["Navid Rekabsaz", "Mihai Lupu", "Allan Hanbury"], "emails": ["family_name@ifs.tuwien.ac.at"], "sections": [{"heading": "1. INTRODUCTION", "text": "Understanding the meaning of a word (semantics) and of its similarity to the other words (relatedness) is the core of understanding text. An established method for quantifying this similarity is the use of word embeddings, where vectors are proxies of the meaning of words and distance functions are proxies of semantic and syntactic relatedness. Fundamentally, word embedding models exploit the contextual information of the target words to approximate their meaning, and hence their relations to the other words.\nGiven the vectors representing words and a corresponding mathematical function, word embedding models provide an approximation of the the relatedness of any two terms, although this relatedness could be perceived as completely-meaningless in the language. An emerging challenge here is: how to identify whether the similarity score obtained from word embedding is really indicative of term relatedness?. This issue is pointed out by Karlgren et al. [10] in examples, showing that word embedding methods are too ready to provide answers to meaningless questions: \u201cWhat is more similar to a computer: a sparrow or a star?\u201d, or \u201cIs a cell\n\u2217This work is partly funded by two projects: SelfOptimizer (9867) by EuroStar and ADMIRE (P 25905-N23) by FWF. Thanks to Joni Sayeler and Linus Wretblad for their contributions in the SelfOptimizer project.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nNeu-IR 16 SIGIR Workshop on Neural Information Retrieval July 21, 2016, Pisa, Italy c\u00a9 2016 Copyright held by the owner/author(s).\nDOI:\nmore similar to a phone than a bird is to a compiler?\u201d. In the absence of a comprehensive answer, the need for related terms has been generally met by applying k Nearest Neighbours (k-NN) search such that retrieving the top k most similar terms in the neighbouring of a given term as related terms. Recently, Cuba Gyllensten and Sahlgren [3] point out the limitations of the kNN approach as it neglects the internal structure of neighbourhoods which could be vastly different for various terms. In other words, some terms are more central in language and therefore have more related terms while many words have no genuinely related term. This is intuitive in human language while also quantifiable by using a language thesaurus e.g. WordNet (for example, by counting the number of synonyms). We therefore put the focus of this study on the notion of \u201csimilar\u201d in word embedding.\nDifferent characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5]. However, there is lack of understanding on the internal structure of word embedding, specifically how its similarity distribution reflects the relatedness of terms.\nFollowing this direction, in this work, we would argue that the \u201csimilar\u201d words can be identified by a threshold on similarity values which separates the semantically related words from the less or non-related ones. It is quite difficult, a priori, to even consider a threshold for this similarity. Especially since we do not want to make this parameter dependent on the term. This would be not only computationally, but also conceptually problematic. As Karlgren et al. discuss for the case of Random Indexing [9, 10], just because we can have a \u201cmost similar term(s)\u201d does not mean that this makes any sense in real life.\nCertainly, the meaning of \u201csimilar\u201d also depends on the similarity function, but, we consider here the state of the art word similarity and leave the exploration of this factor for the further studies. Instead, we would argue that regardless of the similarity function, the most important factor is the threshold which separates the semantically related terms from the less or non-related ones.\nExploring such a threshold has the potential to bring improvements in those studies which use word embedding for retrieving the similar/related words in different tasks i.e. query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].\nWe explore the estimation of this potential threshold by first quantifying the uncertainty factor in the similarity values of embedding models. This factor is an intrinsic characteristic of all the recent models, because they all start with some random initialization and eventually converge to a (local) solution. Therefore, even by training with the same parameters and on the same data, the created word embedding models result in slightly different word distri-\nar X\niv :1\n60 6.\n06 08\n6v 1\n[ cs\n.C L\n] 2\n0 Ju\nn 20\n16\nbutions and hence slightly different relatedness values. In the next step, using the uncertainty factor, we provide a continuous neighbouring representation for an arbitrary term, which is later used to estimate the general threshold.\nIn order to evaluate the effectiveness of the introduced threshold, we test it in the context of a document retrieval task, on five different test collections. In the experiments, we apply the threshold to identify the set of terms to meaningfully extend the query terms. We show that using the introduced threshold performs either exactly the same as or statistically indistinguishable from the optimal threshold for all collections.\nIn summary, the main contributions of the current study are: 1. exploration of the uncertainty factor in word embedding mod-\nels in different dimensions and similarity ranges. 2. introducing a general threshold for separating similar terms\nin different dimensions. 3. extensive experiments on five test collections comparing dif-\nferent threshold values as well as k-NN search.\nAmong various word embedding models, in our study, we use the method proposed by Mikolov et al. [15]: skip-gram with negativesampling training (SGNS) method in the Word2Vec framework. While this is not the newest method in this category (e.g. Pennington et al. [17] introduced GloVe and reported superior results), independent benchmarking provided by Levy et al. [14] shows that there is no fundamental performance difference between the recent word embedding models. In fact, based on their experiments, they conclude that the performance gain observed by one model or another is mainly due to the setting of the hyper-parameters of the models. Their study also motivates our decision to use SGNS: \u201cSGNS is a robust baseline. While it might not be the best method for every task, it does not significantly underperform in any scenario.\u201d\nThe remainder of this work is structured as follows: First, we review related work in Section 2. We introduce the potential threshold in Section 3. We present our experimental setup in Section 4, followed by discussing the results in Section 5. Section 6 summarises our observations and concludes the paper."}, {"heading": "2. RELATED WORK", "text": "The closest study to our work is Karlgren et al. [9], which explores the semantic topology of the vector space generated by Random Indexing. Based on their previous observations that the dimensionality of the semantic space appears different for different terms [10], Karlgren at al. now identify the different dimensionalities at different angles (i.e. distances) for a set of specific terms. It is however difficult to map these observations to specific criteria or guidelines for either future models or retrieval tasks.\nIn fact, our observations provide a quantification on Karlgren\u2019s claim that \u201c\u2018close\u2019 is interesting and \u2018distant\u2019 is not\u201d [10].\nMore recently, Cuba Gyllensten and Sahlgren [3] follow a data mining approach to represent the terms relatedness by a tree structure. While they suggest traversing the tree as a potential approach, they evaluate it only on the word sense induction tasks and its utility for retrieving similar words remains unanswered. Our work complements and extends their approach. Defining the threshold on the collection and not each word, our method is efficiently applicable and computationally cheaper on all subsequent tasks to which the word embeddings may be applied."}, {"heading": "3. POTENTIAL THRESHOLD", "text": "As mentioned in introduction, we are looking for a potential threshold to separate the truly related terms from the rest. In this section, we describe our analytic approach to explore such cutting\npoints in different dimensions. The introduced threshold is defined on the entire model i.e. it is applicable to any arbitrary term in language.\nFor this purpose, we start with an observation on the uncertainty of similarity in word embedding models, followed by defining a continuous model of neighbouring distribution, before we define our proposed threshold."}, {"heading": "3.1 Uncertainty of Similarity", "text": "In this section we make a series of practical observations on word embeddings and the similarities computed based on them. To observe the uncertainty, let us consider two models P and M . To create each instance, we trained the Word2Vec SGNS model with the sub-sampling parameter set to 10\u22125, context windows of 5 words, epochs of 25, and word count threshold 20 on the Wikipedia dump file for August 2015, after applying Porter stemmer. Each model has a vocabulary of approximately 580k terms. They are identical in all ways except their random starting point.\nFigure 1a shows the distances between two terms and all other terms in the dictionary, for the two models, in this case of dimensionality 200. For each term we have approximately 580k points on the plot. As we can see, the difference between similarities calculated in the two models, appears (1) greater for low similarities, and (2) greater for a rare word (Dwarfish) than for a common word (Book). We can also observe that there are very few pairs of words with very high similarities.\nLet us now explore the effect of dimensionality on similarity values and also uncertainty. Before then, in order to generalize the observations to an arbitrary term, we had to consider a set of \u201crepresentative\u201d terms. What exactly \u201crepresentative\u201d means is of course debatable. We took 100 terms recently introduced in the query inventory method by Schnabel et al. [21]. It is claimed that the terms are diverse in frequency, part of speech (POS). In the following of the paper, we refer to arbitrary term as an aggregation over the representative terms.\nFigure 1b shows frequency histograms for the occurrence of similarity values in different dimensionalities of a given model. As we can see, similarities are in the [\u22120.2, 1.0] range and have positive skewness (the right tail is longer). As the dimensionality increases, the kurtosis also increases (the histogram has thinner tails).\nTo observe the changes in uncertainty in different dimensions, we quantify this uncertainty as a function of the similarity value. Let us consider\nSs = {(x, y) : sim(~xM \u2212 ~yM ) \u2208 (s, s+ )}\nthe set of term pairs whose similarity is approximately s according to model M (~xM is the vector representation of term x in model M and sim is a similarity function between two vectors (Cosine throughout this paper)). We have to consider this approximation as it is practically never the case that two word pairs have exactly the same similarity value. We can then define an uncertainty % as follows:\n%(s) = 1 |Ss| \u2211\n(x,y)\u2208Ss\n|sim(~xM , ~yM )\u2212 sim(~xP , ~yP )| (1)\nwhere ~xP is the vector representation of term x in model P . The approximation parameter is not important for this exemplification. For the plot in Figure 1c we take it to be 2.4\u00d710\u22124, as it splits our domain (-0.2,1.0) into 500 equal intervals. Figure 1c shows % for different dimensionalities, against the similarity calculated in the M model. We observe that, as the similarity increases, the uncertainty decreases and that for highly similar words the different model instances tend to agree.\nWe also observe a decrease in % as the dimensionality of the model increases. On the other hand, the differences between models decrease as the dimension increases such that the models of dimension 300 and 400 seem very similar in comparison to 100 and 200. The observation shows a probable convergence in the Uncertainty at higher dimensionalities.\nWe can conclude from the observations that the similarity between terms is not a concrete value but can be considered as an approximation whose variation is highly dependent on the dimensionality and similarity range. We use the effect of this factor in the following."}, {"heading": "3.2 Continuous Distribution of Neighbours", "text": "As seen, the different similarities of a pair of terms, achieved from different embedding models with the same training phases are slightly different. Intuitively, we assume that these similarity values follow a normal distribution such that we can consider every similarity value as a probability distribution, built based on the similarity values of the same pair in different models.\nTo estimate this probability distribution, for every dimension, we create five identical SGNS models following the setup in Section 3.1. Figure 2a shows the probability distribution of similarities for term Book to 25 terms in different similarity ranges1. We observe that by decreasing the similarity, the variation of the probability distributions increases, reflecting the increase in uncertainty empirically observed between two models in the previous section.\nWe use these probability distributions to provide a representation of the expected number of neighbours around an arbitrary term in the spectrum of similarity values. For this purpose, we calculate the mixture of cumulative distribution functions of the probability distributions subtracted from 1, showed in Figure 2b. The values on this plot indicate the number of expected neighbours in the area between the given similarity value to the term (similarity one). This representation of the expected number of neighbours in Figure 2b has two main benefits: (1) the estimation is continuous, and (2) it considers the effect of uncertainty and considers all the models.\nAs noted before, the notion of arbitrary term is in fact an average over the 100 representative terms. However, this are just a sample of all terms in the vocabulary. Therefore, in calculating the representation of the expected number of neighbours, we also consider the confidence interval around the mean. This interval is shown in Figure 2c. Here, the representation is zoomed on the lower left corner of Figure 2b. The area around each plot shows the confidence interval of the estimation.\n1we do not plot all to maintain the readability of the plot\nThis continuous representation is used in the following for defining the threshold for the semantically highly related terms."}, {"heading": "3.3 Similarity threshold", "text": "Given the representation of the expected number of neighbours around the arbitrary term, the question is \u201cwhat is the best threshold for filtering the highly related terms?\u201d. This is of course a debatable question since the analytical approach attempts to measure the human understanding of synonymity. However, we hypothesise that since this general threshold tries to separates the highly related terms for an arbitrary term, it can be estimated from the average number of synonyms over the terms in language. Therefore, we transform the above question in a new question: \u201cWhat is the expected number of synonyms for a word in English?\u201d\nTo answer this, we exploit WordNet. We consider the distinct terms in the related synsets to a term as its synonyms, while putting out the multi word terms (e.g. Natural Language Processing, shown in WordNet by concatenating with underlines) since in creating the word embedding models we consider them as separated terms. The average number of synonyms over all the 147306 terms of WordNet is 1.6, while the standard deviation is 3.1.\nUsing the mean value, we define our threshold for each dimensionality as the point where the estimated number of neighbours in Figure 2c is equal to 1.6. We also consider an upper and lower bound for this threshold based on the points that the confident intervals cross the approximated mean. The results are shown in Table 1.\nIn the following sections, we validate the hypothesis by evaluating the performance of the introduced thresholds with an extensive set of IR experiments."}, {"heading": "4. EXPERIMENTAL METHODOLOGY", "text": "We test the effectiveness of the potential threshold in an Ad hoc retrieval task on IR test collections by evaluating the results of applying various thresholds to retrieve the related terms.\nOur relevance scoring approach is based on the language model [18] method as a widely used and established method in IR that has shown competitive results in various domains. In particular, we use the translation language model [2] which includes the similarity of\nrelated terms into the basic model. In the following, first we explain the translation language model when combined with word embedding similarity and then describe the details of our experimental setup."}, {"heading": "4.1 Translation Language Model", "text": "In the language model [18], the score of a document d with respect to a query q is considered to be the probability of generating the query by a model Md estimated based on the document:\nscore(q, d) = P (q|Md) = \u220f tq\u2208q P (tq|Md) (2)\nTypically, the model is a multinomial distribution and the probability is computed with a maximum likelihood estimator, together with some form of smoothing. This smoothing, while not being part of the original idea, is in the practice of LM-based methods of paramount importance. However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).\nBerger and Lafferty [2] introduced translation models as an extension to the language modelling. A translation model introduces in the estimation of P (q|Md) a translation probability PT , defined on the set of terms, always used in its conditional form PT (t|t\u2032) and interpreted as the probability of observing term t, having observed term t\u2032.\nP (q|Md) = \u220f tq\u2208q \u2211 td\u2208d PT (tq|td)P (td|Md)  (3) The estimation of the model and specially the translation probability PT have been addressed by various approaches during the last two decades. Recently, Zuccon et al. [26] integrates word embedding into the translation language model, showing potential improvement. In their work, they follow a k-NN approach to select the most similar terms for each query term in word embedding and estimate PT based on the similarity of the extended terms to the query term.\nSimilar to their work, we use the translation language model enhanced with word embedding and reproduce some of their experiments. However, instead of the k-NN approach, we apply our introduced thresholds (Section 3.3) to filter the similar terms."}, {"heading": "4.2 Experiments Setup", "text": "We evaluate our approach on 5 test collections: combination of TREC 1 to 3, TREC-6, TREC-7, and TREC-8 of the AdHoc track, and TREC-2005 HARD track. Table 2 summarises the statistics of\nthe test collections. For pre-processing, we apply the Porter stemmer and remove stop words using a small list of 127 common English terms.\nIn order to compare the performance of the potential thresholds, we test a variety of the threshold values in each dimension: for dimension 100, {0.67, 0.70, 0.74, 0.79, 0.81, 0.86, 0.91, 0.94, 0.96}, 200 dimension {0.63, 0.68, 0.71, 0.73, 0.74, 0.76, 0.78, 0.82}, 300 dimension, {0.55, 0.60, 0.65, 0.68, 0.70, 0.71, 0.73, 0.75}, and 400 dimension {0.41, 0.54, 0.61, 0.64, 0.66, 0.68, 0.70, 0.71, 0.75}. In addition to the threshold-based approach, we test the k-NN approach where N is tested with {1, 2, 3, 5, 7, 10} values.\nWe set the basic language model as baseline and test the statistical significance of the improvement of all the results with respect to it (indicated by the symbol \u2020). Since the parameter \u00b5 for Dirichlet smoothing of the translation language model is shared between the methods, the choice of parameters is not explored as part of this study. We select \u00b5 to 1000 as suggested in related studies. The statistical significance test are done using the two sided paired t-test and statistical significance is reported for p < 0.05.\nThe evaluation of retrieval effectiveness is done with respect to MAP and NDCG@20, as standard measures. However, our initial experiments showed that using similar terms retrieved a substantial proportion of unjudged documents. Therefore, in order to provide a more fair evaluation framework, we consider MAP and NDCG over the condensed lists [20]2."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "The evaluation results of the MAP and NDCG@20 measures on the 4 test collections, with vectors in 100, 200, 300, and 400 dimensions are shown in Figure 3. For each dimension our threshold and its confidence interval are shown with vertical lines. Significant differences of the results to the baseline are marked on the plots using the \u2020 symbol. Table 3 summarizes the results of the optimal as well as potential thresholds.\nBased on the results, we gain significantly better performance in all the collections at least in one of the threshold values. Except for TREC-7, we observe similar results with both the evaluations 2The condensed lists are used by adding the -J parameter to the trec_eval command parameters\nmeasures. The plots show that the performance of the method is highly dependent on the choice of the threshold value. In general, we can see a trend in all dimensions: the results tend to improve till reaching a peak (optimal threshold) and then decrease and finally converge to the baseline. Based on this general behaviour, we can assume that including the terms before the optimal threshold introduces noise and deteriorates the results while after it, the terms are filtered too strictly and there are still related terms to improve the results. Comparing the results of the optimal and potential threshold, in most the cases the optimal one is either the same or in the confidence area of our introduced threshold such that there is no statistically significant difference between the optimal and our threshold.\nIn order to have an overview on all the models, we calculate the gain of each model over the baseline and averaged the gains on the five collections. The results for MAP3 are depicted in Figures 4a. Also the potential threshold and its confidence interval are compared with the optimal one in different dimensions in Figure 4b. Our threshold is optimal for dimensions 100, 200, and 300, and in dimension 400 it is statistically indistinguishable from the optimal. This results justifies the choice of the introduced threshold as a generally stable and effective cutting-point for identifying highly related terms.\nFor completeness, we also conducted experiments on the k-NN approach. The results in Figure 4c show the very weak perfor-\n3The NDCG results are very similar and not shown for space\nmance of the k-NN approach for MAP measure such that it has slightly better than baseline for k equal to 1 and 2 and then radically deteriorates by increasing k.\nTo understand this behaviour let us take a closer look at the selected terms. Table 4 shows some examples of the retrieved terms when using the word embedding model with 300 dimension with the our threshold (same as optimal in this dimension). The examples show the strong differences in the number of similar words for various terms. The mean and standard deviation of the number of similar terms for the 508 query terms of the tasks is 1.5 and 3.0 respectively. Almost half of the terms are not expanded at all. An interesting observation is the similarity between this calculated mean and standard deviation and the aggregated number of synonyms we observed in WordNet in Section 3.3\u2014mean of 1.6 and standard deviation of 3.1. It appears that although the two semantic resources cast the notion of similarity in very different ways and their provided sets of similar terms are very different, they correspond to\nvery similar distribution of the number of related terms."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "We have analytically explored the thresholds on similarity values of word embedding to select related terms. This threshold is estimated based on a novel representation of the neighbours around an arbitrary term which is continuous and benefits from addressing the issue of uncertainty in similarity values of modern word embedding models.\nWe extensively evaluate the application of the suggested threshold on four information retrieval collections. The results show superior performance when using our threshold such that its results are either equal to or statistically indistinguishable from the optimal results, achieved by extensive search on the parameter space."}, {"heading": "7. REFERENCES", "text": "[1] M. Baroni, G. Dinu, and G. Kruszewski. Don\u2019t count,\npredict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL Conference, 2014.\n[2] A. Berger and J. Lafferty. Information Retrieval As Statistical Translation. In Proc. of SIGIR, 1999.\n[3] A. Cuba Gyllensten and M. Sahlgren. Navigating the semantic horizon using relative neighborhood graphs, 2015.\n[4] L. De Vine, G. Zuccon, B. Koopman, L. Sitbon, and P. Bruza. Medical semantic similarity with a neural language model. In Proc. of CIKM.\n[5] K. Erk and S. Pad\u00f3. Exemplar-based models for word meaning in context. In Proc. of ACL, 2010.\n[6] D. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word Embedding based Generalized Language Model for Information Retrieval. In Proc. of SIGIR Conference, 2015.\n[7] M. Grbovic, N. Djuric, V. Radosavljevic, F. Silvestri, and N. Bhamidipati. Context-and content-aware embeddings for query rewriting in sponsored search. In Proc. of SIGIR Conference, 2015.\n[8] M. Karimzadehgan and C. Zhai. Estimation of Statistical Translation Models Based on Mutual Information for Ad Hoc Information Retrieval. In Proc. of SIGIR, 2010.\n[9] J. Karlgren, M. Bohman, A. Ekgren, G. Isheden, E. Kullmann, and D. Nilsson. Semantic topology. In Proc. of CIKM Conference, 2014.\n[10] J. Karlgren, A. Holst, and M. Sahlgren. Filaments of meaning in word space. In Proc. of ECIR Conference, 2008.\n[11] D. Kiela, F. Hill, and S. Clark. Specializing word embeddings for similarity or relatedness. In Proc. of EMNLP, 2015.\n[12] B. Koopman, G. Zuccon, P. Bruza, L. Sitbon, and M. Lawley. An evaluation of corpus-driven measures of medical concept similarity for information retrieval. In Proc. of CIKM, 2012.\n[13] G. Kruszewski and M. Baroni. So similar and yet incompatible: Toward automated identification of semantically compatible words. In Proc. of NAACL, 2015.\n[14] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons learned from word embeddings. Transaction of the Association of Computational Linguists, 2015.\n[15] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[16] B. Mitra. Exploring session context using distributed representations of queries and reformulations. In Proc. of SIGIR Conference, 2015.\n[17] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. Proc. of EMNLP Conference, 2014.\n[18] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proc. of SIGIR, 1998.\n[19] N. Rekabsaz, R. Bierig, B. Ionescu, A. Hanbury, and M. Lupu. On the use of statistical semantics for metadata-based social image retrieval. In Proc. of CBMI Conference, 2015.\n[20] T. Sakai. Alternatives to bpref. In Proc. of SIGIR, 2007. [21] T. Schnabel, I. Labutov, D. Mimno, and T. Joachims.\nEvaluation methods for unsupervised word embeddings. In Proc. of EMNLP, 2015.\n[22] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In Proc. of SIGIR, 2015.\n[23] Y. Tsvetkov, M. Faruqui, W. Ling, G. Lample, and C. Dyer. Evaluation of word vector representations by subspace alignment. In Proc. of EMNLP, 2015.\n[24] I. Vulic\u0301 and M.-F. Moens. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In Proc. of SIGIR, 2015.\n[25] C. Zhai and J. Lafferty. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. In Proc. of SIGIR, 2001.\n[26] G. Zuccon, B. Koopman, P. Bruza, and L. Azzopardi. Integrating and evaluating neural word embeddings in information retrieval. In Proc. of Australasian Document Computing Symposium, 2015."}], "references": [{"title": "Don\u2019t count", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL Conference", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Information Retrieval As Statistical Translation", "author": ["A. Berger", "J. Lafferty"], "venue": "Proc. of SIGIR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Navigating the semantic horizon using relative neighborhood graphs", "author": ["A. Cuba Gyllensten", "M. Sahlgren"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Exemplar-based models for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3"], "venue": "Proc. of ACL", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Word Embedding based Generalized Language Model for Information Retrieval", "author": ["D. Ganguly", "D. Roy", "M. Mitra", "G.J. Jones"], "venue": "Proc. of SIGIR Conference", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Context-and content-aware embeddings for query rewriting in sponsored search", "author": ["M. Grbovic", "N. Djuric", "V. Radosavljevic", "F. Silvestri", "N. Bhamidipati"], "venue": "Proc. of SIGIR Conference", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimation of Statistical Translation Models Based on Mutual Information for Ad Hoc Information Retrieval", "author": ["M. Karimzadehgan", "C. Zhai"], "venue": "Proc. of SIGIR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic topology", "author": ["J. Karlgren", "M. Bohman", "A. Ekgren", "G. Isheden", "E. Kullmann", "D. Nilsson"], "venue": "Proc. of CIKM Conference", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Filaments of meaning in word space", "author": ["J. Karlgren", "A. Holst", "M. Sahlgren"], "venue": "Proc. of ECIR Conference", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["D. Kiela", "F. Hill", "S. Clark"], "venue": "Proc. of EMNLP", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "An evaluation of corpus-driven measures of medical concept similarity for information retrieval", "author": ["B. Koopman", "G. Zuccon", "P. Bruza", "L. Sitbon", "M. Lawley"], "venue": "Proc. of CIKM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "So similar and yet incompatible: Toward automated identification of semantically compatible words", "author": ["G. Kruszewski", "M. Baroni"], "venue": "Proc. of NAACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transaction of the Association of Computational Linguists", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["B. Mitra"], "venue": "Proc. of SIGIR Conference", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. of EMNLP Conference", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "Proc. of SIGIR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "On the use of statistical semantics for metadata-based social image retrieval", "author": ["N. Rekabsaz", "R. Bierig", "B. Ionescu", "A. Hanbury", "M. Lupu"], "venue": "Proc. of CBMI Conference", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Alternatives to bpref", "author": ["T. Sakai"], "venue": "Proc. of SIGIR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "Proc. of EMNLP", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proc. of SIGIR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Y. Tsvetkov", "M. Faruqui", "W. Ling", "G. Lample", "C. Dyer"], "venue": "Proc. of EMNLP", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vuli\u0107", "M.-F. Moens"], "venue": "Proc. of SIGIR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proc. of SIGIR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Integrating and evaluating neural word embeddings in information retrieval", "author": ["G. Zuccon", "B. Koopman", "P. Bruza", "L. Azzopardi"], "venue": "Proc. of Australasian Document Computing Symposium", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "[10] in examples, showing that word embedding methods are too ready to provide answers to meaningless questions: \u201cWhat is more similar to a computer: a sparrow or a star?\u201d, or \u201cIs a cell", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Recently, Cuba Gyllensten and Sahlgren [3] point out the limitations of the kNN approach as it neglects the internal structure of neighbourhoods which could be vastly different for various terms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 198, "endOffset": 211}, {"referenceID": 19, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 198, "endOffset": 211}, {"referenceID": 21, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 198, "endOffset": 211}, {"referenceID": 2, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 240, "endOffset": 246}, {"referenceID": 3, "context": "Different characteristics of term similarities have been explored in several studies: the concept of relatedness [11, 13], the similarity measures [12], intrinsic/extrinsic evaluation of the models [1,4, 21, 23], or in sense induction task [3, 5].", "startOffset": 240, "endOffset": 246}, {"referenceID": 7, "context": "discuss for the case of Random Indexing [9, 10], just because we can have a \u201cmost similar term(s)\u201d does not mean that this makes", "startOffset": 40, "endOffset": 47}, {"referenceID": 8, "context": "discuss for the case of Random Indexing [9, 10], just because we can have a \u201cmost similar term(s)\u201d does not mean that this makes", "startOffset": 40, "endOffset": 47}, {"referenceID": 5, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 122, "endOffset": 125}, {"referenceID": 22, "context": "query expansion [7], query auto-completion [16], document retrieval [19], learning to rank [22], language modelling in IR [6], or Cross-Lingual IR [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "[15]: skip-gram with negativesampling training (SGNS) method in the Word2Vec framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] introduced GloVe and reported superior results), independent benchmarking provided by Levy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] shows that there is no fundamental performance difference between the recent word embedding models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9], which explores the semantic topology of the vector space generated by Random Indexing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Based on their previous observations that the dimensionality of the semantic space appears different for different terms [10], Karlgren at al.", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "In fact, our observations provide a quantification on Karlgren\u2019s claim that \u201c\u2018close\u2019 is interesting and \u2018distant\u2019 is not\u201d [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "More recently, Cuba Gyllensten and Sahlgren [3] follow a data mining approach to represent the terms relatedness by a tree structure.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Our relevance scoring approach is based on the language model [18] method as a widely used and established method in IR that has shown competitive results in various domains.", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "In particular, we use the translation language model [2] which includes the similarity of", "startOffset": 53, "endOffset": 56}, {"referenceID": 16, "context": "In the language model [18], the score of a document d with respect to a query q is considered to be the probability of generating the query by a model Md estimated based on the document:", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 134, "endOffset": 145}, {"referenceID": 22, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 134, "endOffset": 145}, {"referenceID": 24, "context": "However, this not being the focus of this study, we use Dirichlet smoothing [25], as many others have done, successfully, before us ( [8, 24, 26]).", "startOffset": 134, "endOffset": 145}, {"referenceID": 1, "context": "Berger and Lafferty [2] introduced translation models as an extension to the language modelling.", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "[26] integrates word embedding into the translation language model, showing potential improvement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Therefore, in order to provide a more fair evaluation framework, we consider MAP and NDCG over the condensed lists [20].", "startOffset": 115, "endOffset": 119}], "year": 2016, "abstractText": "Word embedding, specially with its recent developments, promises a quantification of the similarity between terms. However, it is not clear to which extent this similarity value can be genuinely meaningful and useful for subsequent tasks. We explore how the similarity score obtained from the models is really indicative of term relatedness. We first observe and quantify the uncertainty factor of the word embedding models regarding to the similarity value. Based on this factor, we introduce a general threshold on various dimensions which effectively filters the highly related terms. Our evaluation on four information retrieval collections supports the effectiveness of our approach as the results of the introduced threshold are significantly better than the baseline while being equal to or statistically indistinguishable from the optimal results.", "creator": "LaTeX with hyperref package"}}}