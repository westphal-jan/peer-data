{"id": "1606.06160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients", "abstract": "We propose DoReFa-Net, a method to train low-bit-width Convolutionary Neural Networks and low-bit-width activation using low-bit-width gradients. Parameter gradients are stochastically quantified to low-bit-count before transferring to revolutionary layers. Since convolutions can now work with low bit-width weights or activations / gradients during forward / backward motion, DoReFa-Net can use bit-width cores to accelerate both training and inference. Since bit volumes can be efficiently implemented on CPU, FPGA, ASIC, and GPU, DoReFatNet paves the way for training low-bit-width neural networks on this hardware. Our experiments on SVHN and ImageNet show that DoReFa-Net can achieve comparable predictive counterparts to 32-bit counterparts.", "histories": [["v1", "Mon, 20 Jun 2016 15:02:31 GMT  (45kb,D)", "http://arxiv.org/abs/1606.06160v1", null], ["v2", "Sun, 17 Jul 2016 14:21:03 GMT  (84kb,D)", "http://arxiv.org/abs/1606.06160v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["shuchang zhou", "yuxin wu", "zekun ni", "xinyu zhou", "he wen", "yuheng zou"], "accepted": false, "id": "1606.06160"}, "pdf": {"name": "1606.06160.pdf", "metadata": {"source": "CRF", "title": "DoReFa-Net: Training Low Bitwidth Con- volutional Neural Networks with Low Bitwidth Gradients", "authors": ["Shuchang Zhou", "Xinyu Zhou", "Yuxin Wu", "Yuheng Zou"], "emails": ["shuchang.zhou@gmail.com", "mike.zekun@gmail.com", "zxy@megvii.com", "wenhe@megvii.com", "wyx@megvii.com", "zouyuheng@megvii.com"], "sections": [{"heading": "1 Introduction", "text": "Recent progress in deep Convolutional Neural Networks (DCNN) has considerably changed the landscape of computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012a) and NLP (Bahdanau et al., 2014). However, a state-of-the-art DCNN usually has a lot of parameters and high computational complexity, which both impedes its application in embedded devices and slows down the iteration of its research and development. For example, the training process of a DCNN may take up to weeks on a modern multiGPU server for large datasets like ImageNet (Deng et al., 2009). In light of this, substantial research efforts are invested in speeding up DCNNs at both run-time and training-time, on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al., 2011; Pham et al., 2012; Chen et al., 2014a;b). Various approaches like quantization (Wu et al., 2015) and sparsification (Han et al., 2015a) have also been proposed. Recent research efforts (Courbariaux et al., 2014; Kim & Smaragdis, 2016; Rastegari et al., 2016; Li & Liu, 2016; Merolla et al., 2016) have considerably reduced both model size and computation complexity by using low bitwidth weights and low bitwidth activations. In particular, in BNN (Courbariaux & Bengio, 2016) and XNOR-Net (Rastegari et al., 2016), both weights and input activations of convolutional layers are binarized. Hence during the forward pass the most computationally expensive convolutions can be done by XNORbitcount kernel, thanks to the following equivalence which computes the dot product of two\n1We are preparing a demo for CVPR \u201916 on behalf of Megvii Inc., based on this work.\nar X\niv :1\n60 6.\n06 16\n0v 1\n[ cs\n.N E\n] 2\n0 Ju\nn 20\n16\nbit vectors x and y: \u2211 i xiyi = bitcount(xnor(xi, yi)), xi, yi \u2208 {0, 1} \u2200i. (1)\nHowever, to the best of our knowledge, no previous work has succeeded in quantizing gradients to numbers with bitwidth less than 8 during the backward pass, while still achieving comparable prediction accuracy. In some previous research (Gupta et al., 2015; Courbariaux et al., 2014), convolutions involve at least 10-bit numbers. In BNN and XNOR-Net, though weights are binarized, gradients are in full precision, therefore the backward-pass still requires convolution between 1-bit numbers and 32-bit floating-points. As training a neural network involves both forward pass and backward pass, the inability to exploit bit convolution during the backward pass means that backward time of BNN and XNOR-Net would be much longer than their forward time. This paper makes the following contributions:\n1. We generalize the method of binarized neural networks to allow creating DoReFaNet, a CNN that has arbitrary bit-width in weights, activations, and even gradients. As convolutions during forward/backward passes can then operate on low bit weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate its training process significantly.\n2. As bit convolutions can be efficiently implemented on FPGA and ASIC, DoReFa-Net opens the way to accelerate low bitwidth neural network training with specialized hardware. FPGA and ASIC would also help improve power efficiency of low bitwidth neural network training.\n3. We explore the configuration space of bit-width for weights, activations and gradients for DoReFa-Net, and find 1-bit weights, 2-bit activations and 4-bit gradients to be a good configuration for SVHN and AlexNet, in terms of prediction accuracy, training time, inference time and storage size. In particular, using 2-bit activations rather than binary activations is found to be effective in lessening the accuracy degradation. We name our model \u201cDoReFa-Net\u201d to note this specific configuration.\n4. We release in TensorFlow (Abadi et al.) format a DoReFa-Net 2 derived from AlexNet (Krizhevsky et al., 2012) that gets 47% in single-crop top-1 accuracy on ILSVRC12 validation set."}, {"heading": "2 DoReFa-Net", "text": "In this section we detail our formulation of DoReFa-Net, a neural network that has low bitwidth weights, activations and can be trained from scratch with low bitwidth parameter gradients. We note that while weights and activations can be deterministically quantized, gradients need to be stochastically quantized. We first outline how to exploit bit convolution kernel in DoReFa-Net and then elaborate the method to quantize weights, activations and gradients to low bitwidth numbers."}, {"heading": "2.1 Using Bit Convolution Kernels in Low Bitwidth Neural Network", "text": "The 1-bit dot product kernel specified in Eqn. 1 can also be used to compute dot product, and consequently convolution, for low bitwidth fixed-point integers. Assume x is a sequence of M -bit fixed-point integers s.t. x = \u2211M m=1 cm(x)2m and y is a sequence of K-bit fixed-\npoint integers s.t. y = \u2211K k=1 ck(y)2k where cm(x) and ck(y) are bit vectors, the dot product\n2The model and supplement materials will be available at https://drive.google.com/a/ megvii.com/folderview?id=0B308TeQzmFDLa0xOeVQwcXg1ZjQ\nof x and y is: \u2211 i xiyi = M\u2211 m=1 K\u2211 k=1 2m+k bitcount[xnor(cm(x)i, ck(y)i)], (2)\ncm(x)i, ck(y)i \u2208 {0, 1} \u2200i, m, k. (3)\nIn the above equation, the computation complexity is O(MK), i.e., directly proportional to bit-width of x and y."}, {"heading": "2.2 Straight-Through Estimator", "text": "The set of real numbers representable by a low bitwidth k only has a small ordinality 2k. However, mathematically any continuous function whose range is a small finite set would necessarily always have zero gradient with respect to its input. We adopt the \u201cstraightthrough estimator\u201d (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to circumvent this problem. An STE can be thought of as an operator that has human-defined forward and backward operations. A simple example is the STE defined for Bernoulli sampling with probability p \u2208 [0, 1]:\nForward: q \u223c Bernoulli(p)\nBackward: \u2202c \u2202p = \u2202c \u2202q .\nHere c denotes the objective function. The motivation here is that since q is closely related to p, we may use the well-defined gradient \u2202c\u2202q to update p in SGD.\nAn STE we will use extensively in this work is quantizek that quantizes a real number input ri \u2208 [0, 1] to a k-bit number output ro \u2208 [0, 1]. This STE is defined as below:\nForward: ro = 1\n2k \u2212 1 round((2 k \u2212 1)ri) (4)\nBackward: \u2202c \u2202ri = \u2202c \u2202ro . (5)\nIt\u2019s obvious by construction that the output q of quantizek STE is a real number representable by k bits. Also, since round((2k \u2212 1)ri) is a k-bit fixed-point integer, the dot product of two sequences of such k-bit real numbers can be efficiently calculated, by using fixed-point integer dot product in Eqn. 2 followed by a constant scaling."}, {"heading": "2.3 Low Bitwidth Quantization of Weights", "text": "In this section we detail our approach to getting low bitwidth weights. In previous works, STE has been used to binarize the weights. For example in BNN, weights are binarized by the following STE:\nForward: ro = sign(ri)\nBackward: \u2202c \u2202ri = clip( \u2202c \u2202ro ,\u22121, 1).\nNote that although sign(ri) may produce exact zero when ri = 0, in practice, due to the limited precision of floating-point numbers, ro will almost for sure be of two possible values: {\u22121, 1}. In XNOR-Net, weights are binarized by the following STE, with the difference being that weights are scaled after binarized:\nForward: ro = sign(ri)\u00d7EF (|ri|)\nBackward: \u2202c \u2202ri = \u2202c \u2202ro .\nIn XNOR-Net, the scaling factor EF (|ri|) is the mean of absolute value across the whole filter. The rationale is that introducing this scaling factor will increase the value range of weights, while still being able to exploit bit convolution kernels. However, the channel-wise scaling factors will make it impossible to exploit bit convolution kernels when computing the convolution between gradients and the weights during back propagation. Thankfully, in our experiments, we find using a constant scalar to scale all filters does not cause degradation in prediction accuracy compared to channel-wise scaling. Therefore we use the following STE in all experiments that have binary weights in this paper:\nForward: ro = sign(ri)\u00d7E(|ri|) (6)\nBackward: \u2202c \u2202ri = \u2202c \u2202ro . (7)\nTo keep k-bit representation of the weights rather than binarize them, we apply the STE fk\u03c9 to weights as follows:\nForward: ro = fk\u03c9(ri) = 2 quantizek( tanh(ri) 2 max(| tanh(ri)|) + 12)\u2212 1. (8)\nBackward: \u2202c \u2202ri = \u2202c \u2202ro \u2202ro \u2202ri 3 (9)\nNote here we use tanh to limit the value range to [\u22121, 1] before quantizing to k-bit. We found that using tanh helps bound the value and therefore improve the stability of training. By construction, tanh(ri)2 max(| tanh(ri)|) + 1 2 is a number in [0, 1], where the maximum is taken over all weights in that layer. quantizek will then quantize this number to k-bit fixed-point ranging in [0, 1]. Finally an affine transform will bring the range of fk\u03c9(ri) to [\u22121, 1]. Note that when k = 1, Eqn. 8 is different from Eqn. 6, providing a different way of binarizing weights. Nevertheless, we find this difference insignificant in experiments."}, {"heading": "2.4 Low Bitwidth Quantization of Activations", "text": "Next we detail our approach to getting low bitwidth activations that are input to convolutions, which is of critical importance in replacing floating-point convolutions by less computation-intensive bit convolutions. In BNN and XNOR-Net, activations are binarized in the same way as weights. However, we fail to reproduce the results of XNOR-Net if we follow their methods of binarizing activations, and the binarizing approach in BNN is claimed by (Rastegari et al., 2016) to cause severe prediction accuracy drop. Hence instead, we apply a different STE on input activations r of each weight layer. Here we assume the output of the previous layer has passed through a bounded activation function h, which ensures r \u2208 [0, 1]. In DoReFa-Net, quantization of activations r to k-bit is simply:\nfk\u03b1(r) = quantizek(r). (10)\nWe observed that the choice of activation function h has a profound impact on network prediction accuracy. In our experiments we find following activations functions to be useful:\n1. h(x) = tanh(x)+12 2. h(x) = clip(x, 0, 1)\n3. h(x) = min(1, |x|)\n3Here \u2202ro \u2202ri is well-defined because we already defined quantizek as an STE"}, {"heading": "2.5 Low Bitwidth Quantization of Gradients", "text": "We have demonstrated deterministic quantization to produce low bitwidth weights and activations. However, we find stochastic quantization is necessary for low bitwidth gradients to be effective. This is in agreement with experiments of (Gupta et al., 2015) on 16-bit weights and 16-bit gradients. To quantize gradients to low bitwidth, it is important to note that the magnitude of gradient is unbounded and may be significantly larger than activation. Recall in Eqn. 10, we mapped the range of activation to [0, 1] by introducing proper nonlinear activations. However, this kind of convenience does not exist for gradients, and we\u2019d actually prefer to keep the magnitude of gradients unaffected for SGD to work. Therefore we designed the following k-bit quantization function:\nfk\u03b3 (dr) = 2 max0(|dr|) [ quantizek(\ndr 2 max0(|dr|) + 12)\u2212 1 2\n] .\nHere dr = \u2202c\u2202r is the back-propagated gradient of the output r of some layer, and the maximum is taken over all axis of the gradient tensor dr except for the mini-batch axis (therefore each instance in a mini-batch will have its own scaling factor). The above function first applies an affine transform on the gradient, to map it into [0, 1], and then inverts the transform after quantization. To further compensate the potential bias introduced by gradient quantization, we introduce an extra noise function N(x) = clip(x + \u03c32k\u22121 , 0, 1) where \u03c3 \u223c Uniform(\u22120.5, 0.5). The noise therefore has the same magnitude as the possible quantization error. We found this stochasticity crucial in achieving good performance. Finally, the expression we\u2019ll use to quantize gradients to k-bit numbers is as follows:\nfk\u03b3 (dr) = 2 max0(|dr|) [ quantizek[N(\ndr 2 max0(|dr|) + 12)]\u2212 1 2) ] . (11)\nThe quantization of gradient is done on the backward pass only. Hence we apply the following STE on the output of each weight layer:\nForward: ro = ri (12)\nBackward: \u2202c \u2202ri = fk\u03b3 ( \u2202c \u2202ro ). (13)"}, {"heading": "2.6 The Algorithm for DoReFa-Net", "text": "We give a sample training algorithm of DoReFa-Net as Algorithm 1. W.l.o.g., the network is assumed to have a feed-forward linear topology, and details like batch normalization and pooling layers are omitted. Note that all the expensive operations forward, backward input, backward weight, in convolutional as well as fully-connected layers, are now operating on low bitwidth numbers. By construction, there is always an affine mapping between these low bitwidth numbers and fixed-point integers. As a result, all the expensive operations can be accelerated significantly by the fixed-point integer dot product kernel (Eqn. 2)."}, {"heading": "2.7 First and the last layer", "text": "Among all layers in a DCNN, the first and the last layer appear to be different from the rest, as they are interfacing the input and output of the network. For the first layer, the input is often an image, which may contain 8-bit features. On the other hand, the output layer typically produce approximately one-hot vectors, which are close to bit vectors by definition. It is an interesting question whether these differences would cause the first and last layer to exhibit different behavior when converted to low bitwidth counterparts. In the related work of (Han et al., 2015b) which converts network weights to sparse tensors, introducing the same ratio of zeros in the first convolutional layer is found to cause more\nAlgorithm 1 Training a L-layer DoReFa-Net with W -bit weights and A-bit activations using G-bit gradients. Weights, activations and gradients are quantized according to Eqn. 8, Eqn. 10, Eqn. 11, respectively. Require: a minibatch of inputs and targets (a0, a\u2217), previous weights W , learning rate \u03b7 Ensure: updated weights W t+1 {1. Computing the parameter gradients:} {1.1 Forward propagation:}\n1: for k = 1 toL do 2: W bk \u2190 fW\u03c9 (Wk) 3: ak \u2190 forward(abk\u22121,W bk) 4: Apply activation function h on ak 5: if k < L then 6: abk \u2190 fA\u03b1 (ak) 7: end if 8: Optionally apply pooling 9: end for {1.2 Backward propagation:} Compute gaL = \u2202C\u2202aL knowing aL and a\n\u2217. 10: for k = L to 1 do 11: Back-propagate gak through activation function h 12: gbak \u2190 f G \u03b3 (gak ) 13: gak\u22121 \u2190 backward input(gbak ,W b k) 14: gW b k \u2190 backward weight(gbak , a b k\u22121) 15: Back-propagate gradients through pooling layer if there is one 16: end for {2. Accumulating the parameters gradients:} 17: for k = 1 toL do 18: gWk = gW b\nk\n\u2202W bk \u2202Wk\n19: W t+1k \u2190 Update(Wk, gWk , \u03b7) 20: end for\nprediction accuracy degradation than in the other convolutional layers. Based on this intuition as well as the observation that the inputs to the first layer often contain only a few channels and constitutes a small proportion of total computation complexity, we perform most of our experiments by not quantizing the weights of the first convolutional layer, unless noted otherwise. Nevertheless, the outputs of the first convolutional layer are quantized to low bitwidth as they would be used by the consequent convolutional layer. Similarly, when the output number of class is small, to stay away from potential degradation of prediction accuracy, we leave the last fully-connected layer intact unless noted otherwise. Nevertheless, the gradients back-propagated from the final FC layer are properly quantized. We will give the empirical evidence in Section 3.3."}, {"heading": "2.8 Reducing Run-time Memory Footprint by Fusing", "text": "One of the motivations for creating low bitwidth neural network is to save run-time memory footprint in inference. A naive implementation of Algorithm 1 would involve applying a nonlinear activation h on ak, resulting in full-precision numbers and would consume much memory during run-time. Besides, if h involves floating-point arithmetics, there will be non-negligible amount of non-bitwise operations related to computations of ak. There are simple solutions to this problem. Notice that it is possible to fuse Step 3, Step 4, Step 6 to avoid storing ak in full-precision. Apart from this, when h is monotonic, f\u03b1 \u00b7 h is also monotonic, the few possible values of abk corresponds to several non-overlapping value ranges of ak, hence we can implement computation of abk = f\u03b1(h(ak)) by several comparisons and avoid generating intermediate results. Similarly, it would also be desirable to fuse Step 11 \u223c Step 12, and Step 13 of previous iteration to avoid generating gak . The situation would be more complex when there are intermediate pooling layers. Nevertheless, if the pooling layer is max-pooling, we can do the fusion as quantizek function commutes with max function:\nquantizek(max(a, b)) = max(quantizek(a), quantizek(b))), (14)\nhence again gbak can be generated from gak by several comparisons."}, {"heading": "3 Experiment Results", "text": ""}, {"heading": "3.1 Configuration Space Exploration", "text": "We explore the configuration space of the combination of bit-width of weights, activations and gradients by experiments on the SVHN dataset. The SVHN dataset (Netzer et al., 2011) is a real-world digit recognition dataset consisting of photos of house numbers in Google Street View images. We consider the \u201ccropped\u201d format of the dataset: 32-by-32 colored images centered around a single character. There are 73257 digits for training, 26032 digits for testing, and 531131 less difficult samples which can be used as extra training data. The images are resized to 40x40 before fed into network. For convolutions in a DoReFa-Net, if we have W -bit weights, A-bit activations and G-bit gradients, the relative forward and backward computation complexity, storage relative size, can be computed and we list them in Table 1. The relative computation complexity is calculated by assuming that bit convolution kernels will be used as in 2. As it would be absurd to use bit convolution kernels for 32-bit convolutions, and noting that previous works like BNN and XNOR-net have compared bit convolution kernels with 32-bit convolution kernels, we will omit the complexity comparison of computation complexity for the 32-bit control experiments. We use the prediction accuracy of several CNN models on SVHN dataset to evaluate the efficacy of configurations. Model A is a CNN that costs about 80 FLOPs for one 40x40 image, and it consists of seven convolutional layers and one fully-connected layer."}, {"heading": "1 2 32 - - 1 0.976 0.950 0.873 0.865", "text": ""}, {"heading": "1 2 4 6 2 1 0.975 0.969 0.939 0.878", "text": ""}, {"heading": "1 2 8 10 2 1 0.975 0.971 0.946 0.866", "text": ""}, {"heading": "1 3 4 7 3 1 0.974 0.974 0.959 0.897", "text": ""}, {"heading": "1 4 4 8 4 1 0.975 0.974 0.962 0.915", "text": ""}, {"heading": "32 32 32 - - 32 0.975 0.975 0.972 0.950", "text": "Model B, C, D is derived from Model A by reducing the number of channels for all seven convolutional layers by 50%, 75%, 87.5%, respectively. The listed prediction accuracy is the maximum accuracy over 200 epochs. We use ADAM (Kingma & Ba, 2014) learning rule with 0.001 as learning rate. In general, having low bitwidth weights, activations and gradients will cause degradation in prediction accuracy. But it should be noted that low bitwidth networks will have much reduced resource requirement. As balancing between multiple factors like training time, inference time, model size and accuracy is more a problem of practical trade-off, there will be no definite conclusion as which combination of (W, A, G) one should choose. Nevertheless, we find in these experiments that weights, activations and gradients are progressively more sensitive to bit-width, and using gradients with G \u2264 2 would significantly degrade prediction accuracy. Based on these observations, we take (W, A, G) = (1, 2, 4) as a rational combination and use it in our experiments on ImageNet dataset. Interestingly, By comparing row (W, A, G) = (1, 2, 32) with row (W, A, G) = (1, 2, 4), it can also be seen that for low bitwidth networks, training with low bit gradients may boost prediction accuracy. The credit may belong to the regularization effect introduced by the noise in gradients, similar to previous works in adding noise to gradients (Neelakantan et al., 2015). Table 1 also shows that the relative number of channels significantly affect the prediction quality loss resulting from bit-width reduction. For example, there is no significant loss of prediction accuracy when going from 32-bit model to DoReFa-Net for Model A, but it not the case for Model C. We conjecture that \u201cmore capable\u201d models like those with more channels will be less sensitive to bit-width differences. On the other hand, Table 1 also\nsuggests a method to compensate for the prediction quality loss, by increasing bit-width of activations for models with less channels, at the cost of increasing inference time and training time computation complexity. However, optimal bit-width of gradient seems less related to model channel numbers and prediction quality saturates with 4-bit gradients across the table."}, {"heading": "3.2 ImageNet", "text": "We further evaluates our method on ILSVRC12 (Deng et al., 2009) image classification dataset, which contains about 1.2 million high-resolution natural images for training that spans 1000 categories of objects. The validation set contains 50k images. We report our single-crop evaluation result using top-1 accuracy. The images are resized to 224x224 before fed into the network. The results are listed in Table 2. The baseline AlexNet model that scores 55.9% single-crop top-1 accuracy is a best-effort replication of the model in (Krizhevsky et al., 2012), with the second, fourth and fifth convolutions split into two blocks. We replace the Local Contrast Renormalization layer with Batch Normalization layer (Ioffe & Szegedy, 2015). From the table, it can be seen that increasing bitwidth of activation from 1-bit to 2-bit and even to 4-bit, while still keep 1-bit weights, leads to significant accuracy increase, quickly approaching the accuracy of model where both weights and activations are 32-bit. Rounding gradients to 4-bit is also found to be a good local minimum, as the combination\u201c1-2-8\u201d leads to worse final accuracy than \u201c1-2-4\u201d. The rows with \u201cinitialized\u201d means the model training has been initialized with a 32-bit model. It can be seen that there is a considerable quality gap between the best accuracy of a trained-from-scratch-model and an initialized model. Closing this gap is left to future work. Nevertheless, it show the potential of DoReFa-Net in accuracy."}, {"heading": "3.2.1 Training curves", "text": "Figure 1 shows the evolution of accuracy v.s. epoch curves of DoReFa-Net. It can be seen that quantizing gradients to be 4-bit does not cause the training curve to have a significant different than not quantizing gradients."}, {"heading": "3.3 Making First and Last Layer low bitwidth", "text": "To answer the question whether the first and the last layer need to be treated specially when quantizing to low bitwidth, we use the same models A, B, C from Table 1 to find out if it is cost-effective to quantize the first and last layer to low bitwidth, and collect the results in Table 3. It can be seen that quantizing first and the last layer indeed leads to accuracy degradation, and models with less number of channels suffer more. The degradation is significant on AlexNet, which to some extent justifies the practice of BNN and XNOR-net, which do not quantize these two layers."}, {"heading": "4 Discussion and Related Work", "text": "By binarizing weights and activations, binarized neural networks like BNN and XNOR-Net have enabled acceleration of the forward pass of neural network with bit convolution kernel. However, the backward pass of binarized networks still requires convolutions between floating-point gradients and weights, which could not efficiently exploit bit convolution kernel as gradients are in general not low bitwidth numbers and sensitive to errors. (Lin et al., 2015) makes a step further towards low bitwidth gradients by converting some multiplications to bit-shift. However, the number of additions between high-bit numbers remains at the same order of magnitude as before. There is also another series of work (Seide et al., 2014) that quantizes gradients in distributed computation settings. However, the work is more concerned with decreasing the amount of communication traffic, and does not deal with the bit-width of gradient used in backpropagation. To the best of our knowledge, our work is the first to reduce the bit-width of gradient to 4-bit and lower, while still achieving comparable prediction accuracy without altering other aspects of neural network model, such as increasing the number of channels. Our experiments confirm the efficacy of our method for models as large as AlexNet on ImageNet dataset."}, {"heading": "5 Conclusion and Future Work", "text": "We have introduced DoReFa-Net, a method to train a convolutional neural network that has low bitwidth weights and activations using low bitwidth parameter gradients. We find that weights and activations can be deterministically quantized while gradients need to be stochastically quantized. As most convolutions during forward/backward passes are now taking low bitwidth weights and activations/gradients respectively, DoReFa-Net can use the bit convolution kernels to accelerate both training and inference process. Our experiments on SVHN and ImageNet datasets demonstrate that DoReFa-Net can achieve comparable prediction accuracy as their 32-bit counterparts. As future work, it would be interesting to investigate using FPGA to train DoReFa-Net, as the O(B2) resource requirement for B-bit arithmetic on FPGA strongly favors low bitwidth convolutions."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi", "Mart\u0131n", "Agarwal", "Ashish", "Barham", "Paul", "Brevdo", "Eugene", "Chen", "Zhifeng", "Citro", "Craig", "Corrado", "Greg S", "Davis", "Andy", "Dean", "Jeffrey", "Devin", "Matthieu"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Chen", "Tianshi", "Du", "Zidong", "Sun", "Ninghui", "Wang", "Jia", "Wu", "Chengyong", "Yunji", "Temam", "Olivier"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Chen", "Yunji", "Luo", "Tao", "Liu", "Shaoli", "Zhang", "Shijin", "He", "Liqiang", "Wang", "Jia", "Li", "Ling", "Tianshi", "Xu", "Zhiwei", "Sun", "Ninghui"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Large-scale fpga-based convolutional networks. Scaling up Machine Learning: Parallel and Distributed Approaches", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": null, "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "arXiv preprint arXiv:1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Neural networks for machine learning", "author": ["Hinton", "Geoffrey", "Srivastava", "Nitsh", "Swersky", "Kevin"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Bitwise neural networks", "author": ["Kim", "Minje", "Smaragdis", "Paris"], "venue": "arXiv preprint arXiv:1601.06071,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ternary weight networks", "author": ["Li", "Fengfu", "Liu", "Bin"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Lin", "Zhouhan", "Courbariaux", "Matthieu", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Deep neural networks are robust to weight binarization and other nonlinear distortions", "author": ["Merolla", "Paul", "Appuswamy", "Rathinakumar", "Arthur", "John", "Esser", "Steve K", "Modha", "Dharmendra"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 1981}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Kurach", "Karol", "Martens", "James"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Pham", "Phi-Hung", "Jelaca", "Darko", "Farabet", "Clement", "Martini", "Berin", "LeCun", "Yann", "Culurciello", "Eugenio"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns", "author": ["Seide", "Frank", "Fu", "Hao", "Droppo", "Jasha", "Li", "Gang", "Yu", "Dong"], "venue": "In INTERSPEECH,", "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Quantized convolutional neural networks for mobile devices", "author": ["Wu", "Jiaxiang", "Leng", "Cong", "Wang", "Yuhang", "Hu", "Qinghao", "Cheng", "Jian"], "venue": "arXiv preprint arXiv:1512.06473,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Recent progress in deep Convolutional Neural Networks (DCNN) has considerably changed the landscape of computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 1, "context": ", 2012a) and NLP (Bahdanau et al., 2014).", "startOffset": 17, "endOffset": 40}, {"referenceID": 7, "context": "For example, the training process of a DCNN may take up to weeks on a modern multiGPU server for large datasets like ImageNet (Deng et al., 2009).", "startOffset": 126, "endOffset": 145}, {"referenceID": 27, "context": "In light of this, substantial research efforts are invested in speeding up DCNNs at both run-time and training-time, on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 141, "endOffset": 203}, {"referenceID": 9, "context": "In light of this, substantial research efforts are invested in speeding up DCNNs at both run-time and training-time, on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 141, "endOffset": 203}, {"referenceID": 28, "context": "Various approaches like quantization (Wu et al., 2015) and sparsification (Han et al.", "startOffset": 37, "endOffset": 54}, {"referenceID": 6, "context": "Recent research efforts (Courbariaux et al., 2014; Kim & Smaragdis, 2016; Rastegari et al., 2016; Li & Liu, 2016; Merolla et al., 2016) have considerably reduced both model size and computation complexity by using low bitwidth weights and low bitwidth activations.", "startOffset": 24, "endOffset": 135}, {"referenceID": 25, "context": "Recent research efforts (Courbariaux et al., 2014; Kim & Smaragdis, 2016; Rastegari et al., 2016; Li & Liu, 2016; Merolla et al., 2016) have considerably reduced both model size and computation complexity by using low bitwidth weights and low bitwidth activations.", "startOffset": 24, "endOffset": 135}, {"referenceID": 25, "context": "In particular, in BNN (Courbariaux & Bengio, 2016) and XNOR-Net (Rastegari et al., 2016), both weights and input activations of convolutional layers are binarized.", "startOffset": 64, "endOffset": 88}, {"referenceID": 10, "context": "In some previous research (Gupta et al., 2015; Courbariaux et al., 2014), convolutions involve at least 10-bit numbers.", "startOffset": 26, "endOffset": 72}, {"referenceID": 6, "context": "In some previous research (Gupta et al., 2015; Courbariaux et al., 2014), convolutions involve at least 10-bit numbers.", "startOffset": 26, "endOffset": 72}, {"referenceID": 18, "context": ") format a DoReFa-Net 2 derived from AlexNet (Krizhevsky et al., 2012) that gets 47% in single-crop top-1 accuracy on ILSVRC12 validation set.", "startOffset": 45, "endOffset": 70}, {"referenceID": 2, "context": "We adopt the \u201cstraightthrough estimator\u201d (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to circumvent this problem.", "startOffset": 54, "endOffset": 97}, {"referenceID": 25, "context": "However, we fail to reproduce the results of XNOR-Net if we follow their methods of binarizing activations, and the binarizing approach in BNN is claimed by (Rastegari et al., 2016) to cause severe prediction accuracy drop.", "startOffset": 157, "endOffset": 181}, {"referenceID": 10, "context": "This is in agreement with experiments of (Gupta et al., 2015) on 16-bit weights and 16-bit gradients.", "startOffset": 41, "endOffset": 61}, {"referenceID": 23, "context": "The SVHN dataset (Netzer et al., 2011) is a real-world digit recognition dataset consisting of photos of house numbers in Google Street View images.", "startOffset": 17, "endOffset": 38}, {"referenceID": 22, "context": "The credit may belong to the regularization effect introduced by the noise in gradients, similar to previous works in adding noise to gradients (Neelakantan et al., 2015).", "startOffset": 144, "endOffset": 170}, {"referenceID": 7, "context": "We further evaluates our method on ILSVRC12 (Deng et al., 2009) image classification dataset, which contains about 1.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "9% single-crop top-1 accuracy is a best-effort replication of the model in (Krizhevsky et al., 2012), with the second, fourth and fifth convolutions split into two blocks.", "startOffset": 75, "endOffset": 100}, {"referenceID": 25, "context": "Note the BNN result is reported by (Rastegari et al., 2016), not by original authors.", "startOffset": 35, "endOffset": 59}, {"referenceID": 20, "context": "(Lin et al., 2015) makes a step further towards low bitwidth gradients by converting some multiplications to bit-shift.", "startOffset": 0, "endOffset": 18}, {"referenceID": 26, "context": "There is also another series of work (Seide et al., 2014) that quantizes gradients in distributed computation settings.", "startOffset": 37, "endOffset": 57}], "year": 2016, "abstractText": "We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFatNet opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 4-bit gradients to get 47% top-1 accuracy on ImageNet validation set.1 The DoReFa-Net AlexNet model is released publicly.", "creator": "LaTeX with hyperref package"}}}