{"id": "1606.06622", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions", "abstract": "We present the novel problem of determining the relevance of questions about images in VQA. Current VQA models do not determine whether a question is at all related to the given image (e.g., What is the capital of Argentina?) or whether it requires information from external resources in order to answer correctly, which can interrupt the continuity of a dialogue in the human-machine interaction. Our approaches to determining relevance consist of two phases. In the light of an image and a question, we first determine (1) whether the question is visual or not, (2) whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, the uncertainty of the VQA model and the similarity in the caption, are able to surpass strong baselines in both relevance tasks. We also present human studies that show that VQA models, supplemented by such question-relevance considerations, are perceived as more intelligent and more similar to humans.", "histories": [["v1", "Tue, 21 Jun 2016 15:38:27 GMT  (997kb,D)", "http://arxiv.org/abs/1606.06622v1", null], ["v2", "Wed, 10 Aug 2016 02:56:00 GMT  (997kb,D)", "http://arxiv.org/abs/1606.06622v2", null], ["v3", "Mon, 26 Sep 2016 15:24:28 GMT  (9709kb,D)", "http://arxiv.org/abs/1606.06622v3", "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["arijit ray", "gordon christie", "mohit bansal", "dhruv batra", "devi parikh"], "accepted": true, "id": "1606.06622"}, "pdf": {"name": "1606.06622.pdf", "metadata": {"source": "CRF", "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions", "authors": ["Arijit Ray", "Gordon Christie", "Mohit Bansal", "Dhruv Batra", "Devi Parikh"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Visual Question Answering (VQA) is the task of predicting a suitable answer given an image and a question about it. VQA models (e.g., (Antol et al., 2015; Ren et al., 2015)) are typically discriminative models that take in image and question representations and output one of a set of possible answers.\nOur work is motivated by the following key observation \u2013 all current VQA systems always output an answer regardless of whether the input question makes any sense for the given image or not. Fig. 1\nNon-Visual Visual True-Premise\nVisual False-Premise\nshows examples of relevant and irrelevant questions. When VQA systems are fed irrelevant questions as input, they understandably produce nonsensical answers (Q: \u201cWhat is the capital of Argentina?\u201d A: \u201cfire hydrant\u201d). Humans, on the other hand, are unlikely to provide such nonsensical answers and will instead answer that this is irrelevant or use another knowledge source to answer correctly, when possible. We argue that this implicit assumption by all VQA systems \u2013 that an input question is always relevant for the input image \u2013 is simply untenable as VQA systems move beyond standard academic datasets to interacting with real users, who may be unfamiliar, or malicious. The goal of this work is to make VQA systems more human-like by providing them the capability to identify relevant questions.\nWhile existing work has reasoned about crossmodal similarity, being able to identify whether a question is relevant to a given image is a novel problem with real-world applications. In human-robot interaction, being able to identify questions that are dissociated from the perception data available is important. The robot must decide whether to process the scene it perceives or query external world knowledge resources to provide a response.\nAs shown in Fig. 1, we study three types of\nar X\niv :1\n60 6.\n06 62\n2v 1\n[ cs\n.C V\n] 2\n1 Ju\nn 20\nquestion-image pairs: Non-Visual. These questions are not questions about images at all \u2013 they do not require information from any image to be answered (e.g. \u201cWhat is the capital of Argentina?\u201d). Visual False-Premise. While visual, these questions do not apply to the given image. For instance, the question \u201cWhat is the girl wearing?\u201d makes sense only for images that contain a girl in them. Visual TruePremise. These questions are relevant to (i.e., have a premise which is true for) the image at hand.\nWe introduce datasets and train models to recognize both non-visual and false-premise questionimage (QI) pairs in the context of VQA. First, we identify whether a question is visual or non-visual; if visual, we identify whether the question has a truepremise for the given image. For visual vs. nonvisual question detection, we use a Long Short-Term Memory (LSTM) recurrent neural network (RNN) trained on part of speech (POS) tags to capture visual-specific linguistic structure. For true vs. falsepremise question detection, we present one set of approaches that use the uncertainty of a VQA model, and another set that use pre-trained captioning models to generate relevant captions (or questions) for the given image and then compare them to the given question to determine relevance.\nOur proposed models achieve accuracies of 94% for detecting non-visual, and 76% for detecting false-premise questions, which significantly outperform strong baselines. We also show through human studies that a VQA system that reasons about question relevance is picked significantly more often as being more intelligent, human-like and reasonable than a baseline VQA system which does not. Our code and datasets will be made publicly available."}, {"heading": "2 Related Work", "text": "There is a large body of existing work that reasons about cross-modal similarity: how well an image matches a query tag (Liu et al., 2009) in text-based image retrieval, how well an image matches a caption (Feng and Lapata, 2013; Xu et al., 2015; Ordonez et al., 2011; Karpathy and Fei-Fei, 2015; Fang et al., 2015), and how well a video matches a description (Donahue et al., 2014; Lin et al., 2014a).\nIn our work, if a question is deemed irrelevant, the VQA model says so, as opposed to answering\nthe question anyway. This is related to perception systems that do not respond to an input where the system is likely to fail. Such failure prediction systems have been explored in vision (Zhang et al., 2014; Devarakota et al., 2007) and speech (Zhao et al., 2012; Sarma and Palmer, 2004; Choularton, 2009; Voll et al., 2008). A related idea is to avoid a highly specific prediction if there is a chance of being wrong, instead making a more generic prediction that is more likely to be right (Deng et al., 2012).\nTo the best of our knowledge, our work is the first to study the relevance of questions in VQA. Chen et al. (2012) classify users\u2019 intention of questions for community question answering services. Most related to our work is (Dodge et al., 2012) that extracts visual text from within Flickr photo captions to be used as supervisory signals for training image captioning systems. Our motivation is to endow VQA systems the ability to detect non-visual questions to respond in a human-like fashion. Moreover, we also detect a more fine-grained notion of question relevance via true- and false-premise."}, {"heading": "3 Datasets", "text": "For the task of detecting visual vs. non-visual questions, we assume that all the questions in the VQA dataset (Antol et al., 2015) are visual, since the Amazon Mechanical Turk (AMT) workers were specifically instructed to ask questions about a displayed image while creating that dataset. We also collected non-visual philosophical and general knowledge questions from the internet (see appendix). Combining the two, we have 121,512 visual questions from the validation set of VQA and 9,9521 generic non-visual questions collected from the internet. We call this dataset Visual vs. NonVisual Questions (VNQ).\nWe also collect a dataset of true- vs. false-premise questions by showing AMT workers images paired with random questions from the VQA dataset and asking them to annotate whether they are applicable or not. We had three workers annotate each QI pair. We take the majority vote as the final ground truth label.2 We have 10,793 QI pairs on 1,500 unique\n1High accuracies on this task in our experiments indicates that this suffices to learn the corresponding linguistic structure.\n278% of the time all three votes agree.\nimages out of which 79% are non-applicable (falsepremise). We refer to this visual true- vs. falsepremise questions dataset as VTFQ.\nSince there is a class imbalance in both of these datasets, we report the average per-class (i.e. normalized) accuracy for all approaches. All datasets will be made publicly available."}, {"heading": "4 Approach", "text": "Here we present our approaches for detecting (1) visual vs. non-visual QI pairs, and (2) true- vs. falsepremise QI pairs."}, {"heading": "4.1 Visual vs. Non-Visual Detection", "text": "Recall that the task here is to detect visual questions from non-visual ones. Non-visual questions, such as \u201cDo dogs fly?\u201d or \u201cWho is the president of the USA.?\u201d, often tend to have a difference in the linguistic structure from that of visual questions, such as \u201cDoes this bird fly?\u201d or \u201cWhat is this man doing?\u201d. We compare our approach (LSTM) with a baseline (RULE-BASED): 1. RULE-BASED. A rule-based approach to detect non-visual questions based on the part of speech (POS)3 tags and dependencies of the words in the question. E.g., if a question has a plural noun with no determiner before it and is followed by a singular verb (\u201cDo dogs fly?\u201d), it is a non-visual question.4 2. LSTM. We train an LSTM with 100-dim hidden vectors to embed the question into a vector and predict visual vs. not. Instead of feeding question words ([\u2018what\u2019, \u2018is\u2019, \u2018the\u2019, \u2018man\u2019, \u2018doing\u2019, \u2018?\u2019]), the input to our LSTM is embeddings of POS tags of the words ([\u2018pronoun\u2019, \u2018verb\u2019, \u2018determiner\u2019, \u2018noun\u2019, \u2018verb\u2019]). Embeddings of the POS tags are learnt end-to-end. This captures the structure of imagegrounded questions, rather than visual vs. nonvisual topics. The latter are less likely to generalize across domains."}, {"heading": "4.2 True- vs. False-Premise Detection", "text": "Our second task is to detect whether a question Q entails a false-premise for an image I. We present two families of approaches to measure this QI \u2018compatibility\u2019: (i) using uncertainty in VQA models, and\n3We use spaCy POS tagger (Honnibal and Johnson, 2015). 4See appendix for examples of such hand-crafted rules.\n(ii) using pre-trained captioning models.\nUsing VQA Uncertainty. Here we work with the hypothesis that if a VQA model is uncertain about the answer to a QI pair, the question may be irrelevant for the given image since the uncertainty may mean it has not seen similar QI pairs in the training data. We test two approaches: 1. ENTROPY. We compute the entropy of the softmax output from a state-of-the art VQA model (Antol et al., 2015; Lu et al., 2015) for a given QI pair and train a decision stump classifier. 2. VQA-MLP. We feed in the softmax output to a two-layer multilayer perceptron (MLP) with 30 hidden units in each layer, and train it as a binary classifier to predict whether a question has a true- or falsepremise for the given image.\nUsing Pre-trained Captioning Models. Here we utilize (a) an image captioning model, and (b) an image question-generation model \u2013 to measure QI compatibility. Note that both these models generate natural language capturing the semantics of an image \u2013 one in the form of statement, the other in the form of a question. Our hypothesis is that a given question is relevant to the given image if it is similar to the language generated by these models for that image. Specifically: 1. Question-Caption Similarity (Q-C SIM). We use NeuralTalk2 (Karpathy and Fei-Fei, 2015) pretrained on the MSCOCO dataset (Lin et al., 2014b) (images and associated captions) to generate a caption C for the given image, and then compute a learned similarity between Q and C (details below). 2. Question-Question Similarity (Q-Q\u2019 SIM). We use NeuralTalk2 re-trained (from scratch) on the questions in the VQA dataset to generate a question Q\u2019 for the image. Then, we compute a learned similarity between Q and Q\u2019. We now describe our learned Q-C similarity function (the Q-Q\u2019 similarity is analogous). Our Q-C similarity model is a 2-channel LSTM+MLP (one channel for Q, another for C). Each channel sequentually reads word2vec embeddings of the corresponding language via an LSTM. The last hidden state vectors (40-dim) from the 2 LSTMs are concatenated and fed as inputs to the MLP, which outputs a 2-class (relevant vs. not) softmax. The entire model is learned end-to-end on the VTFQ\ndataset. We also experimented with other representations (e.g. bag of words) for Q, Q\u2019, C, which are included in the appendix for completeness.\nFinally, we also compare our proposed models above to a simpler baseline (Q-GEN SCORE), where we compute the probability of the input question Q under the learned question-generation model. The intuition here is that since the question generation model has been trained only on relevant questions (from the VQA dataset), it will assign a high probability to Q if it is relevant."}, {"heading": "5 Experiments and Results", "text": "Visual vs. Non-Visual Detection. We use a random set of 100,000 questions from the VNQ dataset for training, and the remaining 31,464 for testing. The results are shown in Table 1. We see that LSTM performs 18.59% better than RULE-BASED.\nTrue- vs. False-Premise Detection. We use a random set of 7,195 (66%) QI pairs from the VTFQ dataset to train and the remaining 3597 (33%) to test. Table 1 shows the results. While the VQA model uncertainty based approaches (ENTROPY, VQA-MLP) perform reasonably well (with the MLP helping over raw entropy), the learned similarity approaches perform much better (12% gain in normalized accuracy). High uncertainty of the model may suggest that a similar QI pair was not seen during training; however, that does not seem to translate to detecting irrelevance. The language generation models (Q-C SIM, Q-Q\u2019 SIM) seem to work significantly better at modeling the semantic interaction between the question and the image. The generative approach (QGEN SCORE) is outperformed by the discriminative approaches (VQA-MLP, Q-C SIM, Q-Q\u2019 SIM) that are trained explicitly for the task at hand."}, {"heading": "6 Human Qualitative Evaluation", "text": "We also perform human studies where we compare two agents: (1) AGENT-BASELINE\u2013 always answers\nevery question. (2) AGENT-OURS\u2013 reasons about question relevance before responding. If question is classified as visual true-premise, AGENT-OURS answers the question using the same VQA model as AGENT-BASELINE (using (Lu et al., 2015)). Otherwise, it responds with a prompt indicating that the question does not seem meaningful for the image. A total of 100 questions (22% relevant, 78% irrelevant, mimicking the distribution of the VTFQ dataset) were used. Of the relevant questions, 54% were answered correctly by the VQA model. Five human subjects on AMT were shown the response of both agents and asked to pick the agent that sounded more intelligent, more reasonable, and more human-like. AGENT-OURS was picked 65% of the time as the winner, AGENT-BASELINE was picked only 2% of the time, and both considered equally (un)reasonable in the remaining cases. Interestingly, humans often prefer AGENT-OURS over AGENT-BASELINE even when both models are wrong \u2013 AGENT-BASELINE answers the question incorrectly and AGENT-OURS incorrectly predicts that the question is irrelevant and refuses to answer a legitimate question. Users seem more tolerant to mistakes in relevance prediction than VQA."}, {"heading": "7 Conclusion", "text": "We introduced the novel problem of identifying irrelevant (i.e., non-visual or visual false-premise) questions for VQA. Our proposed models significantly outperform strong baselines on both tasks. A VQA agent that utilizes our detector and refuses to answer certain questions significantly outperforms a baseline (that answers all questions) in human studies. Such an agent is is perceived as more intelligent, reasonable, and human-like. Our system can be further augmented to communicate to users what the assumed premise of the question is that is not satisfied by the image, e.g., respond to \u201cWhat is the woman wearing?\u201d for an image of a cat by saying \u201cThere is no woman.\u201d"}, {"heading": "8 Acknowledgements", "text": "We thank Lucy Vanderwende for helpful suggestions and discussions. This work was supported in part by the following: National Science Foundation CAREER awards to DB and DP, Army Research Office YIP awards to DB and DP, ICTAS Junior Faculty awards to DB and DP, Army Research Lab grant W911NF-15-2-0080 to DP and DB, Office of Naval Research grant N00014-14-1- 0679 to DB, Paul G. Allen Family Foundation Allen Distinguished Investigator award to DP, Google Faculty Research award to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donation to DB."}, {"heading": "Appendix Overview", "text": "In this appendix, we provide the following: \u2022 Additional details for RULE-BASED (the visual vs. non-visual question detection baseline). \u2022 Qualitative results. \u2022 Results with other methods of feature extraction for our question-caption similarity and questionquestion similarity approaches used for true- vs. false-premise question detection. \u2022 Implementation details for training the models.\nAppendix I Rule-based Visual vs. Non-Visual Classification\nSection 4.1 in the main document describes RULEBASED, a hand-crafted rule-based approach to detect non-visual questions. Rules were added to make this baseline as strong as possible, where some rules take precedence over others. We list a few examples: \u2022 If there is a plural noun, without a determiner before it, followed by a verb (e.g., \u201cDo dogs fly?\u201d), the question is non-visual. \u2022 If there is a determiner followed by a noun (e.g. \u201dDo dogs fly in this picture?\u201d), the question is visual. \u2022 If there is a personal or possessive pronoun before a noun (e.g., \u201c..color is his umbrella?\u201d), the question is visual. \u2022 We use a list of words that frequently occur in the non-visual questions but infrequently in visual questions. These include words such as: \u2018God\u2019, \u2018Life\u2019, \u2018meaning\u2019, and \u2018\u2019universe\u2019. If any words from this list are present in the question, the question is classified as non-visual.\nAppendix II Qualitative Results\nHere we provide qualitative results for our visual vs. non-visual question detection experiment, and our true- vs. false-premise question detection experiment.\nAppendix II.1 Visual vs. Non-visual detection\nHere are some examples of non-visual questions correctly detected by LSTM: \u2022 \u201cWho is the president of the United States?\u201d \u2022 \u201cIf God exists, why is there so much evil in the world?\u201d \u2022 \u201cWhat is the national anthem of Great Britain?\u201d \u2022 \u201cIs soccer popular in the United States?\u201d\nHere are some example questions that RULEBASED failed on, but that were correctly identified as non-visual by LSTM: \u2022 \u201cWhat color is Spock\u2019s blood?\u201d \u2022 \u201cWho was the first person to fly across the channel?\u201d\nHere are some visual questions correctly classified by LSTM, but incorrectly classified by RULEBASED: \u2022 \u201cWhere is the body of water?\u201d \u2022 \u201cWhat color are the glass items?\u201d \u2022 \u201cWhat is there to sit on?\u201d \u2022 \u201cHow many pillows are pictured?\u201d\nAppendix II.2 True- vs False- Premise Detection\nFigures 2 and 3 show success and failure cases for true- vs. false- premise question detection using QQ\u2019 SIM. Note that in the success cases, contextual and semantic similarity was learned even when the words in the question generated by the captioning model (Q\u2019) were different from the input question (Q).\nAppendix III Performance of Other Features\nWe explored three choices for feature extraction of the questions and captions: 1. BOW. We test a bag-of-words approach with a vocabulary size of 9,952 words to represent questions and captions, where we train an MLP to predict whether the question is relevant or not. The representation is built by setting a value of 1 in the\nfeatures at the words that are present in either the question or the caption and a 2 when the word is present in both. This means each question-caption pair is represented by a 9,952-dim (vocab length) vector. The MLP used on top of BOW is a 3-layer MLP with 30, 20 and 10 hidden units respectively. 2. AVG. W2V. We extract word2vec (Mikolov et al., 2013) features for the question and captions words, compute the average of the features separately for the question and caption and then concatenate them. Similar to BOW, we train a 3-layer MLP with 200, 150 and 80 hidden units respectively. 3. LSTM W2V. These are the features we used in the main paper. The LSTM has 40 hidden units using a 2-layer MLP with 40 and 20 hidden units respectively.\nTable 2 shows a comparison of the performance in recall, precision and normalized accuracy.\nAppendix IV Training Implementation Details\nFor training BOW, AVG. W2V, LSTM W2V and VQA-MLP, we use the Keras Deep learning Library (Chollet, 2015) for Python. For pre-training the question and caption generat models from scratch, we use the Torch Deep Learning Library (Collobert et al., 2011). We use rmsprop as the optimization algorithm for LSTM W2V, and adadelta for BOW and AVG. W2V. For all our models, we use a gaussian random weights initialization, a learning rate of 0.001, and no momentum."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Understanding user intent in community question answering", "author": ["Chen et al.2012] Long Chen", "Dell Zhang", "Levene Mark"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW \u201912 Companion,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition", "author": ["Deng et al.2012] Jia Deng", "Jonathan Krause", "Alexander C Berg", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Confidence estimation in classification decision: a method for detecting unseen patterns", "author": ["Bruno Mirbach", "Bj\u00f6rn Ottersten"], "venue": "In Proceedings of the sixth international conference on advance topics in pattern recog-", "citeRegEx": "Devarakota et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Devarakota et al\\.", "year": 2007}, {"title": "Detecting visual text", "author": ["Dodge et al.2012] Jesse Dodge", "Amit Goyal", "Xufeng Han", "Alyssa Mensch", "Margaret Mitchell", "Karl Stratos", "Kota Yamaguchi", "Yejin Choi", "III Hal Daum\u00e9", "Alexander C. Berg", "Tamara L. Berg"], "venue": "In Proceedings of the 2012 Conference", "citeRegEx": "Dodge et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dodge et al\\.", "year": 2012}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al.2014] Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Automatic caption generation for news images. Pattern Analysis and Machine Intelligence", "author": ["Feng", "Lapata2013] Yansong Feng", "Mirella Lapata"], "venue": "IEEE Transactions on,", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "An improved non-monotonic transition system for dependency parsing", "author": ["Honnibal", "Johnson2015] Matthew Honnibal", "Mark Johnson"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Honnibal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Honnibal et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Visual semantic search: Retrieving videos via complex textual queries", "author": ["Lin et al.2014a] Dahua Lin", "Sanja Fidler", "Chen Kong", "Raquel Urtasun"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014b] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Boost search relevance for tag-based social image retrieval", "author": ["Liu et al.2009] Dong Liu", "Xian-Sheng Hua", "Meng Wang", "HongJiang Zhang"], "venue": "In Multimedia and Expo,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Deeper lstm and normalized cnn visual question answering model. https://github.com/VT-vision-lab/ VQA_LSTM_CNN", "author": ["Lu et al.2015] Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Girish Kulkarni", "Tamara L. Berg"], "venue": null, "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Context-based speech recognition error detection and correction", "author": ["Sarma", "Palmer2004] Arup Sarma", "David D Palmer"], "venue": "In Proceedings of HLTNAACL 2004: Short Papers,", "citeRegEx": "Sarma et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarma et al\\.", "year": 2004}, {"title": "Improving the utility of speech recognition through error detection", "author": ["Voll et al.2008] Kimberly Voll", "Stella Atkins", "Bruce Forster"], "venue": "Journal of digital imaging,", "citeRegEx": "Voll et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Voll et al\\.", "year": 2008}, {"title": "Show, attend and tell: Neural image caption generation", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Predicting failures of vision systems", "author": ["Zhang et al.2014] Peng Zhang", "Jiuling Wang", "Ali Farhadi", "Martial Hebert", "Devi Parikh"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Automatic chinese pronunciation error detection using svm trained with structural features", "author": ["Zhao et al.2012] Tongmu Zhao", "Akemi Hoshino", "Masayuki Suzuki", "Nobuaki Minematsu", "Keikichi Hirose"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Zhao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", (Antol et al., 2015; Ren et al., 2015)) are typically discriminative models that take in image and question representations and output one of a set of possible answers.", "startOffset": 2, "endOffset": 40}, {"referenceID": 16, "context": ", (Antol et al., 2015; Ren et al., 2015)) are typically discriminative models that take in image and question representations and output one of a set of possible answers.", "startOffset": 2, "endOffset": 40}, {"referenceID": 12, "context": "There is a large body of existing work that reasons about cross-modal similarity: how well an image matches a query tag (Liu et al., 2009) in text-based image retrieval, how well an image matches a caption (Feng and Lapata, 2013; Xu et al.", "startOffset": 120, "endOffset": 138}, {"referenceID": 19, "context": ", 2009) in text-based image retrieval, how well an image matches a caption (Feng and Lapata, 2013; Xu et al., 2015; Ordonez et al., 2011; Karpathy and Fei-Fei, 2015; Fang et al., 2015), and how well a video matches a description (Donahue et al.", "startOffset": 75, "endOffset": 184}, {"referenceID": 15, "context": ", 2009) in text-based image retrieval, how well an image matches a caption (Feng and Lapata, 2013; Xu et al., 2015; Ordonez et al., 2011; Karpathy and Fei-Fei, 2015; Fang et al., 2015), and how well a video matches a description (Donahue et al.", "startOffset": 75, "endOffset": 184}, {"referenceID": 6, "context": ", 2015), and how well a video matches a description (Donahue et al., 2014; Lin et al., 2014a).", "startOffset": 52, "endOffset": 93}, {"referenceID": 20, "context": "Such failure prediction systems have been explored in vision (Zhang et al., 2014; Devarakota et al., 2007) and speech (Zhao et al.", "startOffset": 61, "endOffset": 106}, {"referenceID": 4, "context": "Such failure prediction systems have been explored in vision (Zhang et al., 2014; Devarakota et al., 2007) and speech (Zhao et al.", "startOffset": 61, "endOffset": 106}, {"referenceID": 21, "context": ", 2007) and speech (Zhao et al., 2012; Sarma and Palmer, 2004; Choularton, 2009; Voll et al., 2008).", "startOffset": 19, "endOffset": 99}, {"referenceID": 18, "context": ", 2007) and speech (Zhao et al., 2012; Sarma and Palmer, 2004; Choularton, 2009; Voll et al., 2008).", "startOffset": 19, "endOffset": 99}, {"referenceID": 3, "context": "A related idea is to avoid a highly specific prediction if there is a chance of being wrong, instead making a more generic prediction that is more likely to be right (Deng et al., 2012).", "startOffset": 166, "endOffset": 185}, {"referenceID": 5, "context": "Most related to our work is (Dodge et al., 2012) that extracts visual text from within Flickr photo captions to be used as supervisory signals for training image cap-", "startOffset": 28, "endOffset": 48}, {"referenceID": 1, "context": "Chen et al. (2012) classify users\u2019 intention of questions for community question answering services.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "tions, we assume that all the questions in the VQA dataset (Antol et al., 2015) are visual, since the Amazon Mechanical Turk (AMT) workers were specifically instructed to ask questions about a displayed image while creating that dataset.", "startOffset": 59, "endOffset": 79}, {"referenceID": 0, "context": "We compute the entropy of the softmax output from a state-of-the art VQA model (Antol et al., 2015; Lu et al., 2015) for a given QI pair and train a decision stump classifier.", "startOffset": 79, "endOffset": 116}, {"referenceID": 13, "context": "We compute the entropy of the softmax output from a state-of-the art VQA model (Antol et al., 2015; Lu et al., 2015) for a given QI pair and train a decision stump classifier.", "startOffset": 79, "endOffset": 116}, {"referenceID": 13, "context": "If question is classified as visual true-premise, AGENT-OURS answers the question using the same VQA model as AGENT-BASELINE (using (Lu et al., 2015)).", "startOffset": 132, "endOffset": 149}, {"referenceID": 14, "context": "We extract word2vec (Mikolov et al., 2013) features for the question and captions words,", "startOffset": 20, "endOffset": 42}, {"referenceID": 2, "context": "we use the Torch Deep Learning Library (Collobert et al., 2011).", "startOffset": 39, "endOffset": 63}], "year": 2016, "abstractText": "Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g. What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outperform strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent, reasonable, and human-like.", "creator": "LaTeX with hyperref package"}}}