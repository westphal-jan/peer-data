{"id": "1606.06640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Neural Morphological Tagging from Characters for Morphologically Rich Languages", "abstract": "We systematically examine a variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain character-based word vectors in combination with bi-directional LSTMs to model cross-word contexts in an end-to-end environment. We investigate the complementary use of word-based vectors trained on large amounts of unlabeled data. Our experiments on morphological labeling suggest that in \"simple\" model configurations, the choice of network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or extension with pre-labeled word embedding can be important and clearly affect accuracy.", "histories": [["v1", "Tue, 21 Jun 2016 16:25:31 GMT  (112kb,D)", "http://arxiv.org/abs/1606.06640v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["georg heigold", "guenter neumann", "josef van genabith"], "accepted": false, "id": "1606.06640"}, "pdf": {"name": "1606.06640.pdf", "metadata": {"source": "CRF", "title": "Neural Morphological Tagging from Characters for Morphologically Rich Languages", "authors": ["Georg Heigold"], "emails": ["<firstname>.<lastname>@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "Morphological part-of-speech tagging is the process of marking up a word in a text with its morphological information and part of speech (POS), see Fig. 1. In morphologically rich languages (e.g., Turkish and Finnish), individual words encode substantial amounts of grammatical information (such as number, person, case, gender, tense, aspect, etc.) in the word form,\nwhereas morphologically poor languages (e.g., English) rely more on word order and context to express this information. Most languages (such as German and Czech) lie between these two extremes, and some (e.g. German) exhibit syncretism, that is one-to-many mappings between form and function. For example (Fig. 1), the isolated word form \u201dmeine\u201d can be any combination of \u201dcase=[nominative|accusative], number=[singular|plural], gender=feminine\u201d (among others) of the possessive pronoun \u201dmy\u201d or a verb (\u201dto mean\u201d). This suggests that both within and across word modeling is needed in general.\nMorphologically rich languages exhibit large vocabulary sizes and relatively high out-ofvocabulary (OOV) rates on the word level. Table 1 illustrates this for German (TIGER, de.wikidump) and Czech (PDT). Word-level representations gen-\neralize poorly to rarely seen or unseen words and thus, can significantly impair the performance for high OOV rates. To improve generalization, subword representations have been proposed. Compared to morphemes as the sub-word unit (Luong et al., 2013), characters have the advantage of being directly available from the text and do not require additional resources and complex pre-processing steps. Character-based approaches may also be useful for informal language (e.g., Tweets) or low-resource languages.\nThis paper investigates character-based mor-\nar X\niv :1\n60 6.\n06 64\n0v 1\n[ cs\n.C L\n] 2\n1 Ju\nn 20\n16\nphological tagging. More specifically, we (i) provide a systematic evaluation of different neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain character-based word vectors (Table 4), (ii) explore the supplementary use of word (rather than character-based) embeddings pre-trained on large amounts of unlabeled data (Table 5), and (iii) show that carefully optimized character-based systems can outperform existing systems by a large margin for German (Table 3) and Czech (Table 6).\nThe focus of the paper is to gain a better understanding of the relative importance of different basic neural architectures and building blocks. Data from morphologically rich languages are well suited to amplify these differences as a large amount of relevant information is encoded at the word level along with relatively high OOV rates. This helps us to better distinguish between systematic trends and noise when comparing different neural architectures. Similarly, we focus on morphological tagging, which typically has an order of magnitude more tags including also (where tags consist of sequences of a simple POS tag followed by many morphological feature=value pairs, see Fig. 1) than simple POS tagging and correspondingly higher error rates.\nThe remainder of the paper is organized as follows. Section 2 gives a survey on related work. Section 3 describes the neural network-based approach as explored in this paper. The empirical evaluation is presented in Section 4. Section 5 concludes the paper."}, {"heading": "2 Related Work", "text": "This work is in the spirit of the \u201dnatural language processing (almost) from scratch\u201d approach (Collobert et al., 2011b), which was tested for wordlevel processing and various English natural language processing tasks. Several character-based approaches have been proposed for tagging. Existing work for POS tagging includes feature learning using CNNs in combination with a first-order Markov model for classification (Collobert et al., 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016). The work by (Labeau et al., 2015) uses a CNN/Markov model hybrid for morphological tagging of German. Comprehensive work on morphological tagging based on conditional random fields along with state-of-the-art results can be found in (Mu\u0308ller et al., 2013; Mu\u0308ller and Schu\u0308tze, 2015). Our work is inspired by previous work (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ling et al., 2015) but uses a deeper hierarchy of layers in combination with a simple prediction model and provides comprehensive comparative results for alternative neural architectures for morphological tagging.\nSeveral extensions of the neural approach used in this paper have been proposed, including multilingual training (Gillick et al., 2015), auxiliary tasks (Plank et al., 2016), and more structured prediction models (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ma and Hovy, 2016). It is conceivable that these refinements would lead to further improvements. In\nthis paper, we focus on optimizing and comparing a number of architectures for obtaining character vectors and try to keep the rest as simple as possible.\nCharacter-based approaches have also been applied to other tasks in natural language processing, such as named entity recognition (Gillick et al., 2015), parsing (Ballesteros et al., 2015) (BLSTM), language modeling (Ling et al., 2015) (BLSTM) and (Kim et al., 2016) (CNNs) or neural machine translation (Costa-jussa\u0300 and Fonollosa, 2016)."}, {"heading": "3 From Characters to Tags", "text": "We assume an input sentence w1, . . . , wN with (possibly complex morphological) output tags t1, . . . , tN . We use a zeroth-order Markov model\np(t1, . . . , tN |w1, . . . , wN )\n= N\u220f n=1 p(tn|w1, . . . , wN ) (1)\nwhose factors are modeled by a neural network. When mapping characters to tags, we use the character representation of the word, w = c1, . . . , cM . This assumes that the segmentation of the sentence into words is known, which is straightforward for the languages under consideration.\nEach input word maps to one complex output tag. Hence, we can model the position-wise probabilities p(t|w1, . . . , wN ) with recurrent neural networks, such as long short-term memory recurrent neural networks (LSTMs) (Graves, 2012). Fig. 2 shows such a network architecture where the inputs are the word vectors v1, . . . , vN . On top of the BLSTM, we use position-wise softmax classifiers.\nFig. 2 shows the \u201dupper\u201d part of our network. This part is used in all experiments reported below. We now turn to the \u201dlower\u201d parts of our networks, where we experiment with different architectures to obtain character vectors that make up the vis. In fact, in our work, we use both wordbased and character-based word vectors. Wordbased word vectors are attractive because they can be efficiently pre-trained on supplementary, large amounts of unsupervised data (Mikolov et al., 2013). As shown by (Soricut and Och, 2015), these word vectors also encode morphological information and may provide additional information to the character-based word vectors directly learned from the comparably small amounts of supervised data. We use word-based word vectors in two modes: they are pre-trained and kept fixed during training or jointly optimized with the rest of the network. Word-based vectors are efficient as they can be implemented by a lookup table (LUT) (Fig. 3, (a)) but are bad at generalization because they do not exploit information encoded at the subword level.\nThe character-based word vectors are the output vectors of a sub-network that maps variablelength character strings to fixed-length vectors. In this paper, we compare the following mostly well established network architectures, see also Fig. 3:\n\u2022 Fully-connected deep neural networks (DNNs): DNNs expect fixed-length input vectors. To satisfy this constraint, we assume a maximum number of characters per word. Fixed-length character strings from words are obtained by padding with a special character. The fixed-length sequence of character vectors can then be converted into a fixed-length vector by concatenation, which is fed to the DNN. DNNs are generic, unstructured networks which tend to be inefficient to learn in general.\n\u2022 Convolutional neural networks (CNNs) (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015): Compared to DNNs, CNNs use weight tying and local connections. This makes CNNs more efficient to learn in many settings. CNNs can deal with variable-length input across different batches and produce a variable number of output vectors, which are merged into a single fixedlength vector by max pooling. The context\nlength is controlled by the pre-defined filter width. For a filter width of m, a convolutional layer computes (\u201dhierarchical\u201d in case of multiple layers) character m-gram vectors. CNNs scale well to long sequences and are efficient due to highly optimized libraries.\n\u2022 CNNHighway (Kim et al., 2016; Costa-jussa\u0300 and Fonollosa, 2016): This CNN variant is similar to vanilla CNNs but maintains a set of one-layer CNNs with different filter widths. This alleviates problems with having a single filter width and selecting an appropriate filter width. The outputs of the different CNNs are concatenated and max pooled, followed by a fully-connected deep neural network with highway connections for additional mixing. CNNHighway includes many layers but is basically a shallow architecture, which tends to make learning easier.\n\u2022 LSTMs (Ling et al., 2015): LSTMs are sequential models and thus a natural choice for character strings. Vanilla LSTMs map each input to one output. To obtain a fixed-length vector, only the last output vector, ideally encoding the whole sequence, is used as the word vector; all other outputs are suppressed. Unlike CNNs, recurrent neural networks can learn context of variable length and do not use a pre-defined context length. In general, multiple layers are required to perform the\ncomplex transformations. A disadvantage of deep LSTMs is that they can be difficult to train.\n\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left. The word vector is the concatenation of the output vector of the (topmost) forward LSTM at position M and the output vector of the (topmost) backward LSTM at position 1. For a \u201dperfect\u201d sequence model, it might not be obvious why the word needs to be encoded in both directions.\nWhere applicable, word-level and character-level word vectors are combined by concatenation.\nThe weights of the network, \u03b8, are jointly estimated using the conditional log-likelihood\nF (\u03b8) = \u2212 N\u2211 n=1 log p\u03b8(tn|w1, . . . , wN ). (2)\nLearning in recurrent or very deep neural networks is non-trivial and skip/shortcut connections have been proposed to improve the learning of such networks (Pascanu et al., 2014; He et al., 2016). Here, we use such connections (dashed arrows in Fig. 2) in some of the experiments to alleviate potential learning issues.\nAt test time, the predicted tag sequence is the tag sequence that maximizes the conditional prob-\nability p(t1, . . . , tN |w1, . . . , wN ). For the factorization in Eq. (1), the search can be done positionwise. This significantly reduces the computational and implementation complexity compared to firstorder Markov models as used in (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015)."}, {"heading": "4 Experimental Results", "text": "We first test variants of the architecture for German (Section 4.3) and then verify our empirical findings for Czech (Section 4.4)."}, {"heading": "4.1 Data", "text": "We conduct the experiments on the German TIGER corpus1 and the Czech PDT corpus2. For the time being, we have decided against using the recent Universal Dependencies 3 because of the lack of comparative results for morphological tagging in the literature. Table 1 (at the beginning of the paper) presents OOV rates and Table 2 some corpus statistics. Part of the experi-\nments is supervised learning on small labeled data sets (TIGER, PDT) and part is also including large unlabeled data (de.wikidump, cs.wikidump)4. The tag set sizes observed in the labeled training data depend on the language and the type of tags: 54 (POS, German), 255 (MORPH, German), 681 (POSMORPH, German), and 1,811 (POSMORPH, Czech), where POS stands for the part-of-speech tags, MORPH for the morphological tags (feature=value pairs), and POSMORPH for the combined tag sets POS and MORPH. All words are lowercased. As a result of doing this, we ignore a useful hint for nouns in German (which\n1http://www.ims.uni-stuttgart.de/ forschung/ressourcen/korpora/tiger.html\n2https://ufal.mff.cuni.cz/pdt3.0 3http://universaldependencies.org/ 4http://cistern.cis.lmu.de/marmot/\nmakes a difference in error rate for the simple but not for the best models) but makes the conclusions less dependent on this German-specific feature."}, {"heading": "4.2 Setup", "text": "We empirically tuned the hyperparameters on the TIGER development data and used the same setups also for Czech. The best setups for the character-based word vector neural networks are as follows:\n\u2022 DNN: character vector size = 128, one fullyconnected layer with 256 hidden nodes\n\u2022 CNN: character vector size = 128, two convolutional layers with 256 filters and a filter width of five each\n\u2022 CNNHighway: the large setup from (Kim et al., 2016), i.e., character vector size = 15, filter widths ranging from one to seven, number of filters as a function of the filter width min{200, 50 \u00b7filter width}, two highway layers\n\u2022 LSTM: character vector size = 128, two layers with 1024 and 256 nodes\n\u2022 BLSTM: character vector size = 128, two layers with 256 nodes each\nThe BLSTM modeling the context of words in a sentence (Fig. 2) consists of two hidden layers, each with 256 hidden nodes.\nThe training criterion in Eq. (2) is optimized using standard backpropagation and RMSProp (Tieleman and Hinton, 2012) with a learning rate decay of two every tenth epoch. The batch size is 16. We use dropout on all parts of the networks except on the lookup tables to reduce overfitting. In particular and in contrast to (Zaremba et al., 2015), we also use dropout on the recurrent parts of the network because it gives significantly better results. Training is stopped when the error rate on the development set has converged, which typically is after about 50 epochs. We observe hardly any overfitting with the described configurations.\nWe used Torch (Collobert et al., 2011a) to configure the computation graphs implementing the network architectures."}, {"heading": "4.3 German", "text": "We first establish a baseline for German and compare it with the state of the art. Our baseline model (CNN-BLSTM) consists of the BLSTM in Fig. 2 and the CNN in Fig. 3 (c) with a single convolutional layer, which is a simplified version of the best model in (Labeau et al., 2015). The results are summarized in Table 3. We show results for different tag sets (see Section 4.1) to facilitate the comparison with state-of-the-art results.\nOur CNN-BLSTM baseline achieves comparable or better results for all tag sets. In particular, our CNN-BLSTM clearly (under consistent conditions) outperforms the related models in (Labeau et al., 2015) and (Ling et al., 2015)5. As expected, word-level word vectors on their own (Fig. 3 (a)) perform significantly worse than character-level word vectors, with error rates of 5.76% vs. 2.43% for POS tagging. Combining character-level and word-level word vectors computed on the TIGER training data only did not help.\nWord embeddings (Mikolov et al., 2013) or word clusters (Mu\u0308ller and Schu\u0308tze, 2015) allow us to exploit large amounts of unlabeled data. Using pre-trained word embeddings alone is better than the state of the art (9.27% vs. 10.97% for POSMORPH) but does not improve on our results (9.27% vs. 8.72% for POSMORPH). When combining them with the character-level word vectors, however, large additional gains are observed: 2.43% vs. 1.75% for POS and 8.72% vs. 6.67% for POSMORPH (Table 3).\nNext, we compare different architectures to compute character-based word vectors for POSMORPH, see Table 4. Note that here and in contrast to Table 3, we use multiple hidden layers in general, which gives some additional gains. We also tested whether skip connections as shown in Fig. 2 helps learning. A small gain is observed only for LSTM, in all other cases it does not make a difference. Given sufficient capacity (e.g., the number of hidden layers), the different architectures achieve comparable error rates, except for the DNN which performs worse. CNNHighway may perform slightly better than CNN. CNN and CNNHighway are more memory efficient than LSTM and BLSTM but considerably slower in\n5We downloaded the software from https: //github.com/wlin12/JNN to produce consistent results as the results in (Ling et al., 2015) are for the last 100 training sentences only and not for the standard test set. We use the default settings given in the paper for all experiments.\nour Torch-based configuration, for example, 0.5 sec/batch (BLSTM) vs. 2 sec/batch (CNNHighway). The only optimization we do here is to compute the word vectors of a batch in parallel.\nFinally, we investigate the effect of augmenting the character-based word vectors with pre-trained word embeddings (\u201dword2vec\u201d). The gains for simpler models are promising: 8.72% vs. 6.67% for the one-layer CNN. For more complex models, however, the observed gains are much smaller (6.77% vs. 6.15% for the best LSTM, for example). Overall, the error rates without word2vec vary between 7% and 9% while the error rates with word2vec are all around 7%. In particular, we cannot significantly improve over the best result in Table 3 (6.67% vs. 6.40%). In this example, the convolutional networks seem to better combine with word2vec than the recurrent neural network. The convergence curve for LSTMBLSTM augmented with word2vec on a subset of the development set is shown in Fig. 4. The initial convergence is faster with word2vec (ignoring the time to generate word2vec) but the two curves eventually converge to the same error rate."}, {"heading": "4.4 Czech", "text": "We confirm our empirical findings for German on another morphologically rich language (Czech). The results are summarized in Table 6 for the models that performed best on German. Similar to German, CNNHighway-BLSTM and LSTMBLSTM perform similarly (0.5% absolute difference in error rate) and clearly better than the baselines (25% or more relative error rate reduction). Augmenting the character-based word vectors with pre-trained embeddings gives some\nadditional small gain. Again, the gain for CNNHighway-BLSTM is larger than for LSTMBLSTM."}, {"heading": "5 Summary", "text": "This paper summarizes our empirical evaluation of the character-based neural network approach to morphological tagging. Our empirical findings for German and Czech are as follows. As long as carefully tuned neural networks of sufficient capacity (e.g., number of hidden layers) are used, the effect of the specific network architecture (e.g., convolutional vs. recurrent) is small for the task under consideration. However, the choice of architecture can greatly affect the training time (in our implementation, the convolutional networks are 2-4 times slower than the recurrent networks). Augmenting the characterbased word vectors with word embeddings pretrained on large amounts of unsupervised data, gives large gains for the small configurations but only small gains on top of the best configurations. Moreover, our best character-based morphological taggers outperform the state-of-the-art results for German and Czech by a relative gain of 30% or more. Future work will include the investigation of multilingual training, higher-order Markov models, and low-resource languages."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["C. Dyer", "N. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] D. Gillick", "C. Brunk", "O. Vinyals", "A. Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks. Studies in Computational Intelligence", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2016] K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Non-lexical neural architecture for fine-grained POS tagging", "author": ["Labeau et al.2015] M. Labeau", "K. L\u00f6ser", "A. Allauzen"], "venue": null, "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] W. Ling", "T. Lu\u0131\u0301s", "L. Marujo", "R. Fernandez Astudillo", "S. Amir", "C. Dyer", "A. Black", "I. Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] M. Luong", "R. Socher", "C. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Endto-end sequence labeling via bi-directional LSTMCNNs-CRF", "author": ["Ma", "Hovy2016] X. Ma", "E. Hovy"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Robust morphological tagging with word representations", "author": ["M\u00fcller", "Sch\u00fctze2015] T. M\u00fcller", "H. Sch\u00fctze"], "venue": "In ACL,", "citeRegEx": "M\u00fcller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["M\u00fcller et al.2013] T. M\u00fcller", "H. Schmid", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "M\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "How to construct deep recurrent neural networks. In ICLR", "author": ["Pascanu et al.2014] R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["B. Plank", "A. S\u00f8gaard", "Y. Goldberg"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Zadrozny2014] C. dos Santos", "B. Zadrozny"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] R. Soricut", "F. Och"], "venue": "In ACL,", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Recurrent neural network regularization", "author": ["Zaremba et al.2015] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Compared to morphemes as the sub-word unit (Luong et al., 2013), characters have the advantage of being directly available from the text and do not require additional resources and complex pre-processing steps.", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": ", 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 94, "endOffset": 155}, {"referenceID": 3, "context": ", 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 94, "endOffset": 155}, {"referenceID": 15, "context": ", 2011b; dos Santos and Zadrozny, 2014) and recurrent neural network based approaches used in (Ling et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 94, "endOffset": 155}, {"referenceID": 7, "context": "The work by (Labeau et al., 2015) uses a CNN/Markov model hybrid for morphological tagging of German.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "Comprehensive work on morphological tagging based on conditional random fields along with state-of-the-art results can be found in (M\u00fcller et al., 2013; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 131, "endOffset": 178}, {"referenceID": 7, "context": "Our work is inspired by previous work (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ling et al., 2015) but uses a deeper hierarchy of layers in combination with a simple prediction model and provides comprehensive comparative results for alternative neural architectures for morphological tagging.", "startOffset": 38, "endOffset": 134}, {"referenceID": 8, "context": "Our work is inspired by previous work (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ling et al., 2015) but uses a deeper hierarchy of layers in combination with a simple prediction model and provides comprehensive comparative results for alternative neural architectures for morphological tagging.", "startOffset": 38, "endOffset": 134}, {"referenceID": 3, "context": "Several extensions of the neural approach used in this paper have been proposed, including multilingual training (Gillick et al., 2015), auxiliary tasks (Plank et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 15, "context": ", 2015), auxiliary tasks (Plank et al., 2016), and more structured prediction models (Collobert et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 7, "context": ", 2016), and more structured prediction models (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015; Ma and Hovy, 2016).", "startOffset": 47, "endOffset": 143}, {"referenceID": 3, "context": "Character-based approaches have also been applied to other tasks in natural language processing, such as named entity recognition (Gillick et al., 2015), parsing (Ballesteros et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 0, "context": ", 2015), parsing (Ballesteros et al., 2015) (BLSTM), language modeling (Ling et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 8, "context": ", 2015) (BLSTM), language modeling (Ling et al., 2015) (BLSTM) and (Kim et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 6, "context": ", 2015) (BLSTM) and (Kim et al., 2016) (CNNs) or neural machine translation (Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 20, "endOffset": 38}, {"referenceID": 4, "context": ", wN ) with recurrent neural networks, such as long short-term memory recurrent neural networks (LSTMs) (Graves, 2012).", "startOffset": 104, "endOffset": 118}, {"referenceID": 11, "context": "Wordbased word vectors are attractive because they can be efficiently pre-trained on supplementary, large amounts of unsupervised data (Mikolov et al., 2013).", "startOffset": 135, "endOffset": 157}, {"referenceID": 7, "context": "\u2022 Convolutional neural networks (CNNs) (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015): Compared to DNNs, CNNs use weight tying and local connections.", "startOffset": 39, "endOffset": 116}, {"referenceID": 6, "context": "\u2022 CNNHighway (Kim et al., 2016; Costa-juss\u00e0 and Fonollosa, 2016): This CNN variant is similar to vanilla CNNs but maintains a set of one-layer CNNs with different filter widths.", "startOffset": 13, "endOffset": 64}, {"referenceID": 8, "context": "\u2022 LSTMs (Ling et al., 2015): LSTMs are sequential models and thus a natural choice for character strings.", "startOffset": 8, "endOffset": 27}, {"referenceID": 8, "context": "\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left.", "startOffset": 31, "endOffset": 96}, {"referenceID": 0, "context": "\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left.", "startOffset": 31, "endOffset": 96}, {"referenceID": 15, "context": "\u2022 Bidirectional LSTMs (BLSTMs) (Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016): BLSTMs are similar to LSTMs but encode the input from left to right and from right to left.", "startOffset": 31, "endOffset": 96}, {"referenceID": 14, "context": "Learning in recurrent or very deep neural networks is non-trivial and skip/shortcut connections have been proposed to improve the learning of such networks (Pascanu et al., 2014; He et al., 2016).", "startOffset": 156, "endOffset": 195}, {"referenceID": 5, "context": "Learning in recurrent or very deep neural networks is non-trivial and skip/shortcut connections have been proposed to improve the learning of such networks (Pascanu et al., 2014; He et al., 2016).", "startOffset": 156, "endOffset": 195}, {"referenceID": 7, "context": "This significantly reduces the computational and implementation complexity compared to firstorder Markov models as used in (Collobert et al., 2011b; dos Santos and Zadrozny, 2014; Labeau et al., 2015).", "startOffset": 123, "endOffset": 200}, {"referenceID": 6, "context": "\u2022 CNNHighway: the large setup from (Kim et al., 2016), i.", "startOffset": 35, "endOffset": 53}, {"referenceID": 19, "context": "In particular and in contrast to (Zaremba et al., 2015), we also use dropout on the recurrent parts of the network because it gives significantly better results.", "startOffset": 33, "endOffset": 55}, {"referenceID": 7, "context": "3 (c) with a single convolutional layer, which is a simplified version of the best model in (Labeau et al., 2015).", "startOffset": 92, "endOffset": 113}, {"referenceID": 7, "context": "In particular, our CNN-BLSTM clearly (under consistent conditions) outperforms the related models in (Labeau et al., 2015) and (Ling et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 8, "context": ", 2015) and (Ling et al., 2015)5.", "startOffset": 12, "endOffset": 31}, {"referenceID": 11, "context": "Word embeddings (Mikolov et al., 2013) or word clusters (M\u00fcller and Sch\u00fctze, 2015) allow us to exploit large amounts of unlabeled data.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "com/wlin12/JNN to produce consistent results as the results in (Ling et al., 2015) are for the last 100 training sentences only and not for the standard test set.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "59 PCRF (M\u00fcller et al., 2013) 2.", "startOffset": 8, "endOffset": 29}, {"referenceID": 7, "context": "42 biRNN, Non-Lex/Struct (Labeau et al., 2015) 3.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "88 biRNN, Both/Struct (Labeau et al., 2015) 2.", "startOffset": 22, "endOffset": 43}, {"referenceID": 8, "context": "97 BLSTM, lower-case (Ling et al., 2015)5 3.", "startOffset": 21, "endOffset": 40}, {"referenceID": 8, "context": "04 BLSTM, mixed case (Ling et al., 2015)5 2.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "CNN baseline (Labeau et al., 2015) 10.", "startOffset": 13, "endOffset": 34}, {"referenceID": 8, "context": "97 BLSTM baseline (Ling et al., 2015)5 10.", "startOffset": 18, "endOffset": 37}, {"referenceID": 13, "context": "PDT PCRF (M\u00fcller et al., 2013) 6.", "startOffset": 9, "endOffset": 30}, {"referenceID": 8, "context": "01 BLSTM (Ling et al., 2015)5 6.", "startOffset": 9, "endOffset": 28}], "year": 2016, "abstractText": "This paper investigates neural characterbased morphological tagging for languages with complex morphology and large tag sets. We systematically explore a variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain character-based word vectors combined with bidirectional LSTMs to model across-word context in an end-to-end setting. We explore supplementary use of word-based vectors trained on large amounts of unlabeled data. Our experiments for morphological tagging suggest that for \u201dsimple\u201d model configurations, the choice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or the augmentation with pre-trained word embeddings can be important and clearly impact the accuracy. Increasing the model capacity by adding depth, for example, and carefully optimizing the neural networks can lead to substantial improvements, and the differences in accuracy (but not training time) become much smaller or even negligible. Overall, our best morphological taggers for German and Czech outperform the best results reported in the literature by a large margin.", "creator": "LaTeX with hyperref package"}}}