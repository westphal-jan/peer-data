{"id": "1606.06996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "The word entropy of natural languages", "abstract": "The average uncertainty associated with words is an information theory concept that lies at the heart of quantitative and computational linguistics. Entropy has been established as a measure of this average uncertainty - also known as average information content. Here, we use parallel texts from 21 languages to determine the number of characters at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus and estimate word entropies in more than 1,000 languages. Our results help establish quantitative language comparisons, understand the performance of multilingual translation systems, and normalize semantic similarity measures.", "histories": [["v1", "Wed, 22 Jun 2016 16:00:52 GMT  (141kb,D)", "http://arxiv.org/abs/1606.06996v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian bentz", "dimitrios alikaniotis"], "accepted": false, "id": "1606.06996"}, "pdf": {"name": "1606.06996.pdf", "metadata": {"source": "CRF", "title": "The Word Entropy of Natural Languages", "authors": ["Christian Bentz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The predictability of symbols in strings is the most fundamental concept of information encoding in general, and natural language production in particular. Shannon [29] defined the entropy - or average information content as a\nar X\niv :1\n60 6.\n06 99\n6v 1\nmeasure of uncertainty or \u201cchoice\u201d inherent to strings of symbols. Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].\nIn computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24]. All these accounts crucially hinge upon estimating the probability and uncertainty associated with words - i.e. the word entropy - in a given text and language.\nThere are two central questions associated with this estimation: 1) what is the text size (in number of tokens) at which word entropies converge to a stable value? 2) How much systematic difference in word entropies do we find across different languages? The first question is related to the problem of data sparsity. The performance of NLP tools relying on word probabilities is lower-bounded by the minimum text size at which word entropies can still be reliably estimated. The second question relates to the applicability of NLP tools across different languages. The performance of a single tool can be systematically biased in a specific language due to generally higher or lower word entropies.\nIn this study, we use a state-of-the-art method to estimate block entropies [19], and also implement a source entropy estimator [8]. This allows us to establish word entropy convergence points for parallel texts of up to 30M tokens in 21 languages. Based on these analyses, we select texts with sufficiently large token counts from a massively parallel corpus and estimate word entropies across 1360 texts and 1001 languages."}, {"heading": "2 Motivation", "text": "A fundamental property of words is their frequency of occurrence, and hence their probability in language production. This probability is at the heart of many applications in natural language processing.\nFor example, the probability of an expert translating the English word the into German der can be estimated as\np\u0302(der|the) = p\u0302(der, the)/p\u0302(the). (1)\nThis is the conditional probability of finding der in a word aligned German translation where we have the in the English original. More generally, we\ncould have a set of possible German translations\nS = {der, die, das, dem, den, des} (2)\nassigned with estimated probabilities:\np\u0302(der) + p\u0302(die) + p\u0302(das) + p\u0302(dem) + p\u0302(den) + p\u0302(des) = 1. (3)\nIn the uniform case, all of these are assigned probability 1/6, and there is maximum uncertainty about which German word corresponds to English the. In the clearest case, one of the probabilities is 1 and the others are 0. The entropy over such a distribution of word probabilities measures exactly this uncertainty or \u201cchoice\u201d. In our example of German articles, the word entropy is \u2248 2.6 in the uniform case and 0 in the clear case. Entropy calculation is detailed below (Section 4)."}, {"heading": "2.1 Entropy in statistical machine translation", "text": "Given the example above, the task of a machine translation system is to map the English word the onto the correct choice from the set of German translations. The maximum entropy approach [2], for instance, requires that the probabilities are estimated as accurately as possible from the given training data. Probabilities which cannot be estimated reliably are then chosen in a way to maximize the overall entropy of the model.\nCrucially, the translation difficulty in such an account is a direct function of the entropy of the word distribution. In the English-to-German mapping - with uniform probabilities - there are 2.6 bits of \u201cchoice\u201d, whereas in the German-to-English mapping there are 0 bits of \u201cchoice\u201d.\nThis is closely related to Koehn\u2019s [11] finding that when translating into English from 10 different source languages, BLEU scores negatively correlate with the number of words in the source language. Hence, it is a harder problem to translate into a language with higher word entropies than to translate into a language with lower word entropies - everything else being equal. From this perspective, estimating the entropy of word distributions can help to predict translation performance."}, {"heading": "2.2 Entropy in distributional semantics", "text": "Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts. For example, the classical study by Resnik [25] defines the similarity between two concepts c1 and c2 in a semantic hierarchy as\nsim(c1, c2) = max c \u2208 S(c1,c2)\n[\u2212log p(c)], (4)\nwith S(c1, c2) being the set of concepts which subsume both c1 and c2. To get the information content \u2212log p(c), the probability of a shared concept p(c) is estimated from corpus data as\np\u0302(c) = freq(c)/N, (5)\nwhere freq(c) is the count of word tokens (nouns in this case) per concept (e.g. occurrences of dollar, euro, and coin would count towards the frequency of the concept money), and N is the overall number of word tokens observed. Note that via the estimated p\u0302(c) this similarity measure depends on the overall distribution of word probabilities, i.e. the word entropy. Namely, for a language with more word types of overall lower token frequencies the probability p\u0302(c) is biased to be lower on average, and hence the information content \u2212log p(c) is biased to be higher.\nSimilar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].\nOverall, estimating a) the convergence points (in number of word tokens), and b) reliable approximations for word entropies are crucial prerequisites for understanding the performance of translation systems, semantic similarity measures, and more generally, any NLP tool that relies on probabilities of words.\nThese estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31]."}, {"heading": "3 Data", "text": "To control for constant content across languages, we use two sets of parallel texts: 1) the European Parliament Corpus (EPC) [11], and 2) the Parallel Bible Corpus (PBC) [15].1 Details about the corpora can be seen in Table 1. The general advantage of the EPC is that it is big in terms of numbers of word tokens per language (ca. 30M), whereas the PBC is smaller (ca. 280K word tokens per language), but massively parallel in terms of encompassing > 1000 languages."}, {"heading": "4 Methods", "text": "The basic information encoding unit chosen in this study is the word. Earlier studies on the entropy of English [28, 12, 27] often chose letters instead. However, in computational linguistics, word tokens are are a common working unit.\nA word token is here defined as a string of alphanumeric UTF-8 characters delimited by white spaces, with all letters converted to lower case and punctuation removed. Note that scripts of Mandarin Chinese (cmn) and Khmer (khm), for instance, delimit phrases and sentences by white spaces, rather than words. However, such scripts constitute a negligible proportion of our sample (\u2248 0.01%). In fact, \u2248 90% of the texts are written in Latin script.\nGiven words as basic information encoding units, we can estimate the word entropy as outlined in the following."}, {"heading": "4.1 Entropy estimation", "text": "Assume a text is a random variable T created by a process of drawing and concatenating tokens from a set (or vocabulary) of word types V =\n1Last accessed on 09/03/2016\n{w1, w2, ..., wV }, with vocabulary size V = |V|. Word type probabilities are distributed according to p(w) = Pr(T = w) for w \u2208 V . Given these definitions, the entropy of T can be calculated as [29]\nH(T ) = \u2212 V\u2211 i=1 p(wi) log2(p(wi)). (4)\nH(T ) can be seen as the average information content of word types. A crucial step towards estimating H(T ) is to reliably approximate the probabilities p(wi)."}, {"heading": "4.2 Block entropies", "text": "In a text, each word type wi has a token frequency fi = freq(wi). Take the first verse of the English Bible as a text.\nin the beginning god created the heavens and the earth and the earth was waste and empty [...]\nIn this example, the word type the occurs 4 times, and occurs 3 times, etc. As a simple approximation, p(wi) can be estimated via the maximum likelihood method:\np\u0302(wi) = fi\u2211V j=1 fj , (5)\nwhere the denominator is the overall number of word tokens. For the Bible verse we would thus have:\nH(T ) = \u2212( 4 17 log2( 4 17 ) + 3 17 log2( 3 17 ) + . . . + 1 17 log2( 1 17 )) \u2248 3.2 (6)\nHowever, there are two main caveats with this so-called plug-in approach: Firstly, it has been shown that the maximum likelihood estimator is unreliable, especially for small text sizes [19, 9], i.e. small \u2211V\nj=1 fj. A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].\nSecondly, estimating the entropy from raw counts assumes that word tokens are drawn independently from a multinomial distribution, meaning\nthere are no dependencies between words. Clearly, this requirement is not met for natural languages [13].\nTo overcome this problem, instead of using unigrams as \u201cblocks\u201d of information encoding, we could use bigrams, trigrams, n-grams, and thus increase block sizes to 2, 3, n. This yields block entropies [27] defined as\nHn(T ) = \u2212 V\u2211 i=1 p(wi, wi+1, ..., wn)\u00d7 log2(p(wi, wi+1, ..., wn)), (6)\nwhere n is the block size. If n is big enough to take into account most longrange correlations between word tokens, then Hn(T ) is a close approximation of H(T ). However, since the number of different blocks grows exponentially with n, very big corpora are needed to get reliable estimates. Note that Schu\u0308rmann & Grassberger [27] use an English corpus of 70M words and assert that entropy estimation beyond a block size of 5 letters (not words) is already unreliable. We will therefore stick with block sizes of 1, i.e. unigram entropies here.\nHowever, we implement a more parsimonious approach taking into account dependencies between words. This is based on the theory behind Lempel-Ziv compression [34, 35]."}, {"heading": "4.3 Source entropies", "text": "Instead of calculating Hn(T ) with ever increasing block sizes n, Kontoyiannis et al. [12] and Gao et al. [8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].\nMore precisely, [8] show that entropy estimation based on the so-called increasing window estimator, or LZ78 estimator [5], is efficient in terms of convergence.\nApplied to the problem of estimating word entropies, the method works as follows: for any given word token ti in a text find the longest matchlength l for which the string s = (ti, ti+1, ..., ti+l) matches a preceding string in (t1, ..., ti\u22121). Formally, define l as\nli = 1 + max{0 \u2264 l \u2264 i : si+l\u22121i = s j+l\u22121 j for some 0 \u2264 j \u2264 i\u2212 1}. (7)\nThis is an adaptation of Gao et al.\u2019s [8] match-length definition.2 To illustrate this, take the example of the English Bible again:\nin1 the2 beginning3 god4 created5 the6 heavens7 and8 the9 earth10 and11 the12 earth13 was14 waste15 and16 empty17 [...]\nFor the word token beginning, in position 3, there is no match in the preceding string (in the). Hence, the match-length l3 is 0(+1) = 1. If we look at and in position 11, then the longest matching string is and the earth. Hence, the match-length l11 is 3(+1) = 4.\nNote that the average match-lengths across all word tokens reflect the redundancy in the string - which is the inverse of unpredictability. Based on this connection, Gao et al. [8] (Equation 6) show that the entropy of the string can be approximated as\nH\u0303(T ) = 1\nN N\u2211 i=2 log2(i) li , (8)\nwhere N is the overall number of tokens, and i is the position in the string. This approximates the entropy rate, or per-symbol entropy [8], which is denoted as h by Lesne et al. [13], and for which holds\nh = lim N\u2192\u221e\nH(tN+1|t1, t2, ..., tN). (9)\nIn other words, as the number of tokens N approaches infinity, h reflects the average information content of a token tN+1 conditioned on all preceding tokens. So h accounts for all statistical dependencies between tokens [13].\nWe will call h and its approximation H\u0303(T ) the source entropy - after Shannon\u2019s [29] formulation of the entropy of an information source.3\n2Note that Gao et al. [8] give a more general definition that also holds for the so-called sliding window, or LZ77 estimator.\n3Note that in the limit, i.e. as block sizes n approach infinity, block and source entropies are the same. Also, for an independent and identically distributed (i.i.d) random variable, the block entropy of block size 1, i.e. H1(T ), is identical to the source entropy h [13]. However, as pointed out above, in natural languages words are not independently distributed.\n9\n10"}, {"heading": "4.4 Implementation", "text": "We estimate unigram entropies - i.e. entropies for block sizes of 1 (H1(T )) - using the Python implementation of the Nemenman-Shafee-Bialek (NSB) [19] estimator. This estimator has a faster convergence rate compared to other block entropy estimators [9].4\nMoreover, we implement the source entropy estimator H\u0303(T ) as in Gao et al.\u2019s [8] proposal. This is inspired by an earlier implementation by Montemurro & Zanette [17] of Kontoyiannis et al.\u2019s [12] estimator. The code of this implementation will be made available on github."}, {"heading": "5 Results", "text": "We first assess the text sizes at which both block and source entropies converge to a stable value using a subset of 21 languages (Section 5.1). We then select texts from the full PBC corpus based on the minimum number of tokens needed for convergence, estimate their entropies, and compare their spread on an entropy spectrum (Section 5.2). Finally, in Section 5.3, we investigate the correlation between block and source entropies."}, {"heading": "5.1 Entropy convergence", "text": "Figure 1 illustrates the convergence of block and source entropies across 21 languages of the EPC. Note that block entropies are generally higher than source entropies. This is expected given that uncertainty is reduced for source entropies by taking the preceding co-text into account.\nWe further establish convergence points by calculating SDs of entropies for step sizes of 10K tokens, illustrated in Figure 2. If we choose SD < 0.05 as a convergence criterion, then we get the convergence points per language given in Table 2.\nNote that all 21 languages converge to stable entropies below text sizes of 100K tokens, with a maximum of 70K tokens (bg and ro) and an average of 35K for source and 38K for block entropies. This is an encouraging result, since texts with a minimum of around 70K tokens are available for a wide range of languages in the PBC.\n4https://gist.github.com/shhong/1021654/"}, {"heading": "5.2 Entropy estimates", "text": "Based on the convergence analyses, we choose 100K tokens as a cut-off point for inclusion of PBC texts. This leaves us with 1352 texts for block entropies, and 1360 texts for source entropies.5 Both cover 1001 languages. Figure 3 is a density plot of the estimated entropy values. Block entropies are approximately normally distributed around a mean of 9.26 (SD = 1.24), and source entropies around a mean of 5.97 (SD = 1.07). Again, source entropies are systematically lower than block entropies.\nIt is remarkable that given the wide range of potential entropies - from 0 to >16 - most natural languages fall on a relatively narrow spectrum. For example, block entropies mainly fall in the range between 7 and 12, thus only covering around 30% of the possible range."}, {"heading": "5.3 Correlation between block and source entropies", "text": "The similarities in convergence lines in Figure 1 suggest that there is a correlation between block and source entropy estimates. Figure 4 elicits this correlation by plotting block entropies (x-axis) versus source entropies (yaxis). The Pearson correlation is strong (r = 0.96, p < 0.0001), suggesting\n5The number of texts is lower for block entropies since estimations failed for 8 texts due to some punctuation marks that were not correctly removed.\nthat despite the differences in the estimation methods there is a strong connection between them.6"}, {"heading": "6 Discussion", "text": "Independent of the estimation method - using block or source entropies - texts of 100K tokens are generally sufficient to estimate values reliably. This was shown across 21 languages of the EPC. Of course, this is not to say that there are no texts/languages in a bigger corpus like the PBC for which convergence might take longer. Note that the 21 EPC languages cover around 50% of the full range of values across the PBC sample. For example, block entropies range from \u2248 9 for English to \u2248 12 for Finnish in the EPC, and from \u2248 6 to \u2248 13 in the PBC.\n6Five clear outliers were removed here. These are texts of languages like Chinese (cmn) and Khmer (khm), which rather delimit phrases and sentences by white spaces. These have incommensurable source and block entropies.\nMoreover, there is a strong correlation between block and source entropies. This is somewhat surprising considering that the probabilities of word occurrences are generally assumed to be strongly dependent on co-text. Of course, there is a co-text effect. It yields source entropies which are lower than block entropies. However, the difference between them is systematic and allows us to predict source entropies from unigram block entropies. Namely, a linear model fitted through the points in Figure 4 can be specified as\nH\u0303(T ) = \u22121.59 + 0.82H1(T ). (10) Via Equation 10 we can convert block entropies into source entropies with a mean difference of 0.03. Note that estimating source entropies requires searching strings of length i \u2212 1. As i increases, the CPU time per additional word token increases linearly, whereas unigram block entropies can be estimated based on dictionaries of word types and their token frequencies, and the processing time per additional word token is constant. Hence, Equation 10 can help to reduce processing cost."}, {"heading": "6.1 Predicting translation performance", "text": "Based on estimated entropies per language, we can predict the difficulty of pairwise translations between languages, i.e. translation system performance.\n[11] used a probabilistic phrase-based model to translate between the (then available) 11 languages of the EPC. As is shown in Figure 5, Koehn\u2019s BLEU scores for pairwise translations correlate with the pairwise ratios of entropies (r = 0.58, p < 0.0001).\nFor example, Finnish (fi) is a high entropy language (H\u0303(T ) = 8.35) compared to English (en) (H\u0303(T ) = 6.32). Translating from Finnish to English gives a BLEU score of 21.8, and from English to Finnish 13. As pointed out above, this is due to the fact that translating into a higher entropy language means having more \u201cchoice\u201d - or uncertainty - when translating words (or phrases). So the low performance from English to Finnish is predicted by a low English-to-Finnish entropy ratio (6.32/8.35 = 0.76), compared to the Finnish-to-English ratio (8.35/6.32 = 1.32)."}, {"heading": "6.2 Entropy as a normalization factor", "text": "In Section 2, some examples were given of studies that use information content in a distributional semantics context. Resnik [25], for instance, builds a\nsimilarity measure for concepts and words based on the information content, e.g. \u2212log p(wi). Remember that the entropy as defined in Equation 4 is the average information content. It is clear from the range of word entropies in Figure 3 that languages can be strongly biased to have words with either high or low information contents on average. For example, words in Finnish are biased to have higher information content on average than words in English. This bias is a problem for the cross-linguistic application of similarity measures. It can be overcome by normalization using the estimated entropy:\nICunbiased(wi) = \u2212log p(wi)\nH\u0303(T ) . (11)"}, {"heading": "7 Conclusions", "text": "The entropy, average information content, uncertainty or \u201cchoice\u201d is a core property of words. Words, in turn, constitute fundamental building blocks in computational linguistics. Understanding word entropies is therefore a prerequisite for evaluation and improvement of the performance of NLP systems.\nWe have here established convergence points for 21 languages, illustrating that average word entropies can be reliable estimated with text sizes of > 70K. Based on these findings, we estimated entropies across 1360 texts and 1001 languages. Furthermore, we have shown empirically that there is a strong correlation between block and source entropies across these languages.\nOverall, our results help to understand better the performance of multilingual translation systems, and to make measures in distributional semantics cross-linguistically applicable."}, {"heading": "8 Acknowledgements", "text": "CB is funded by the DFG Center for Advanced Studies Words, Bones, Genes, Tools, and the ERC grant EVOLAEMP at the University of Tu\u0308bingen."}], "references": [{"title": "Estimating and comparing entropies across written natural languages using ppm compression", "author": ["F.H. Behr", "V. Fossum", "M. Mitzenmacher", "D. Xiao"], "venue": "Data Compression Conference, 2003. Proceedings. DCC 2003, page 416. IEEE,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "V.J.D. Pietra", "S.A.D. Pietra"], "venue": "Computational linguistics, 22(1):39\u201371,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Information density, heaps\u2019 law, and perception of factiness in news", "author": ["M. Boon"], "venue": "ACL 2014, page 33,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "An estimate of an upper bound for the entropy of english", "author": ["P.F. Brown", "V.J.D. Pietra", "R.L. Mercer", "S.A.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics, 18(1):31\u201340,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel texts", "author": ["M. Cysouw", "B. W\u00e4lchli"], "venue": "Using translational equivalents in linguistic typology. Sprachtypologie & Universalienforschung STUF, 60.2,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal stanford dependencies: A crosslinguistic typology", "author": ["M.-C. De Marneffe", "T. Dozat", "N. Silveira", "K. Haverinen", "F. Ginter", "J. Nivre", "C.D. Manning"], "venue": "LREC, volume 14, pages 4585\u20134592,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating the entropy of binary time series: Methodology, some theory and a simulation study", "author": ["Y. Gao", "I. Kontoyiannis", "E. Bienenstock"], "venue": "Entropy, 10(2):71\u201399,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Entropy inference and the james-stein estimator, with application to nonlinear gene association networks", "author": ["J. Hausser", "K. Strimmer"], "venue": "The Journal of Machine Learning Research, 10:1469\u20131484,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring semantic content in distributional vectors", "author": ["A. Herbelot", "M. Ganesalingam"], "venue": "ACL (2), pages 440\u2013445,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "MT summit, volume 5, pages 79\u201386,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonparametric entropy estimation for stationary processes and random fields, with applications to English text", "author": ["I. Kontoyiannis", "P.H. Algoet", "Y.M. Suhov", "A.J. Wyner"], "venue": "Information Theory, IEEE Transactions on, 44(3):1319\u20131327,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Entropy estimation of very short symbolic sequences", "author": ["A. Lesne", "J.-L. Blanc", "L. Pezard"], "venue": "Physical Review E, 79(4):046208,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Developing odin: A multilingual repository of annotated language data for hundreds of the world\u2019s languages", "author": ["W.D. Lewis", "F. Xia"], "venue": "Literary and Linguistic Computing, page fqq006,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Creating a massively parallel bible corpus", "author": ["T. Mayer", "M. Cysouw"], "venue": "N. Calzolari, K. Choukri, T. Declerck, H. Loftsson, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, and S. Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), Reykjavik, Iceland, May 26-31, 2014, pages 3158\u20133163. European Language Resources Association (ELRA),", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Perplexity analysis of obesity news coverage", "author": ["D.J. McFarlane", "N. Elhadad", "R. Kukafka"], "venue": "AMIA Annual Symposium Proceedings, volume 2009, page 426. American Medical Informatics Association,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Universal entropy of word ordering across linguistic families", "author": ["M.A. Montemurro", "D.H. Zanette"], "venue": "PLoS One, 6(5):e19875,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Complexity and universality in the long-range order of words", "author": ["M.A. Montemurro", "D.H. Zanette"], "venue": "arXiv preprint arXiv:1503.01129,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Entropy and inference, revisited", "author": ["I. Nemenman", "F. Shafee", "W. Bialek"], "venue": "arXiv preprint, page physics/0108025,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295\u2013302. Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian word alignment for massively parallel texts", "author": ["R. \u00d6stling"], "venue": "14th Conference of the European Chapter of the Association for Computational Linguistics, pages 123\u2013127. Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring semantic content to assess asymmetry in derivation", "author": ["S. Pad\u00f3", "A. Palmer", "M. Kisselew", "J. \u0160najder"], "venue": "Workshop on Advances in Distributional Semantics,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint arXiv:1104.2086,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "An evaluation of methods for the extraction of multiword expressions", "author": ["C. Ramisch", "P. Schreiner", "M. Idiart", "A. Villavicencio"], "venue": "Proceedings of the LREC Workshop-Towards a Shared Task for Multiword Expressions (MWE 2008), pages 50\u201353,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["P. Resnik"], "venue": "arXiv preprint cmp-lg/9511007,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["E. Santus", "A. Lenci", "Q. Lu", "S.S. Im Walde"], "venue": "EACL, pages 38\u201342,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Entropy estimation of symbol sequences", "author": ["T. Sch\u00fcrmann", "P. Grassberger"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 6(3):414\u2013427,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Prediction and entropy of printed English", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal, 30(1):50\u201365,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1951}, {"title": "The mathematical theory of communication", "author": ["C.E. Shannon", "W. Weaver"], "venue": "The University of Illinois Press, Urbana,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1949}, {"title": "The sublanguage of cross-coverage", "author": ["P.D. Stetson", "S.B. Johnson", "M. Scotch", "G. Hripcsak"], "venue": "Proceedings of the AMIA Symposium, page 742. American Medical Informatics Association,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Indirect measurement in morphological typology", "author": ["B. W\u00e4lchli"], "venue": "A. Ender, A. Leemann, and B. W\u00e4lchli, editors, Methods in contemporary linguistics, pages 69\u201392. De Gruyter Mouton, Berlin,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Lexical typology through similarity semantics: Toward a semantic map of motion verbs", "author": ["B. W\u00e4lchli", "M. Cysouw"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Automated multiword expression prediction for grammar engineering", "author": ["Y. Zhang", "V. Kordoni", "A. Villavicencio", "M. Idiart"], "venue": "Proceedings 20  of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36\u201344. Association for Computational Linguistics,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "A universal algorithm for sequential data compression", "author": ["J. Ziv", "A. Lempel"], "venue": "IEEE Transactions on information theory, 23(3):337\u2013343,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1977}, {"title": "Compression of individual sequences via variablerate coding", "author": ["J. Ziv", "A. Lempel"], "venue": "Information Theory, IEEE Transactions on, 24(5):530\u2013536,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 28, "context": "Shannon [29] defined the entropy - or average information content as a", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 11, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 7, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 26, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 119, "endOffset": 133}, {"referenceID": 0, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 155, "endOffset": 166}, {"referenceID": 16, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 155, "endOffset": 166}, {"referenceID": 17, "context": "Since then, Shannon [28] and others have undertaken great efforts to estimate precisely the entropy of written English [4, 12, 8, 27], and other languages [1, 17, 18].", "startOffset": 155, "endOffset": 166}, {"referenceID": 1, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 139, "endOffset": 146}, {"referenceID": 19, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 21, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 25, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 24, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 173, "endOffset": 189}, {"referenceID": 2, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 213, "endOffset": 224}, {"referenceID": 29, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 213, "endOffset": 224}, {"referenceID": 15, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 213, "endOffset": 224}, {"referenceID": 32, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 252, "endOffset": 260}, {"referenceID": 23, "context": "In computational linguistics, entropy - and related measures - have been widely applied to tackle problems relating to machine translation [2, 20], distributional semantics [10, 22, 26, 25], information retrieval [3, 30, 16], and multiword expressions [33, 24].", "startOffset": 252, "endOffset": 260}, {"referenceID": 18, "context": "In this study, we use a state-of-the-art method to estimate block entropies [19], and also implement a source entropy estimator [8].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "In this study, we use a state-of-the-art method to estimate block entropies [19], and also implement a source entropy estimator [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "The maximum entropy approach [2], for instance, requires that the probabilities are estimated as accurately as possible from the given training data.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "This is closely related to Koehn\u2019s [11] finding that when translating into English from 10 different source languages, BLEU scores negatively correlate with the number of words in the source language.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 25, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 21, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 9, "context": "2 Entropy in distributional semantics Measures of semantic content and similarity [25, 26, 22, 10] often also rely on information-theoretic concepts.", "startOffset": 82, "endOffset": 98}, {"referenceID": 24, "context": "For example, the classical study by Resnik [25] defines the similarity between two concepts c1 and c2 in a semantic hierarchy as", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "Similar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "Similar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 9, "context": "Similar considerations apply to finding hypernyms in vector spaces [26], measuring semantic content to establish asymmetries between derived words and their base forms [22], as well as measuring differences in semantic content to establish hyponym-hypernym relations [10].", "startOffset": 267, "endOffset": 271}, {"referenceID": 13, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 20, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 22, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 6, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 102, "endOffset": 117}, {"referenceID": 5, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 197, "endOffset": 208}, {"referenceID": 31, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 197, "endOffset": 208}, {"referenceID": 30, "context": "These estimations are also relevant for efforts to broaden the scope of NLP to lesser known languages [14, 21, 23, 7], and to establish quantitative and corpus-based methods in linguistic typology [6, 32, 31].", "startOffset": 197, "endOffset": 208}, {"referenceID": 10, "context": "To control for constant content across languages, we use two sets of parallel texts: 1) the European Parliament Corpus (EPC) [11], and 2) the Parallel Bible Corpus (PBC) [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "To control for constant content across languages, we use two sets of parallel texts: 1) the European Parliament Corpus (EPC) [11], and 2) the Parallel Bible Corpus (PBC) [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 27, "context": "Earlier studies on the entropy of English [28, 12, 27] often chose letters instead.", "startOffset": 42, "endOffset": 54}, {"referenceID": 11, "context": "Earlier studies on the entropy of English [28, 12, 27] often chose letters instead.", "startOffset": 42, "endOffset": 54}, {"referenceID": 26, "context": "Earlier studies on the entropy of English [28, 12, 27] often chose letters instead.", "startOffset": 42, "endOffset": 54}, {"referenceID": 28, "context": "Given these definitions, the entropy of T can be calculated as [29]", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "However, there are two main caveats with this so-called plug-in approach: Firstly, it has been shown that the maximum likelihood estimator is unreliable, especially for small text sizes [19, 9], i.", "startOffset": 186, "endOffset": 193}, {"referenceID": 8, "context": "However, there are two main caveats with this so-called plug-in approach: Firstly, it has been shown that the maximum likelihood estimator is unreliable, especially for small text sizes [19, 9], i.", "startOffset": 186, "endOffset": 193}, {"referenceID": 26, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 18, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 8, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 12, "context": "A range of remedies have been proposed to overcome this problem [27, 19, 9, 13].", "startOffset": 64, "endOffset": 79}, {"referenceID": 12, "context": "Clearly, this requirement is not met for natural languages [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "This yields block entropies [27] defined as", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "Note that Sch\u00fcrmann & Grassberger [27] use an English corpus of 70M words and assert that entropy estimation beyond a block size of 5 letters (not words) is already unreliable.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "This is based on the theory behind Lempel-Ziv compression [34, 35].", "startOffset": 58, "endOffset": 66}, {"referenceID": 34, "context": "This is based on the theory behind Lempel-Ziv compression [34, 35].", "startOffset": 58, "endOffset": 66}, {"referenceID": 11, "context": "[12] and Gao et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].", "startOffset": 71, "endOffset": 79}, {"referenceID": 34, "context": "[8] suggest to use the findings on optimal compression by Ziv & Lempel [34, 35].", "startOffset": 71, "endOffset": 79}, {"referenceID": 7, "context": "More precisely, [8] show that entropy estimation based on the so-called increasing window estimator, or LZ78 estimator [5], is efficient in terms of convergence.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "More precisely, [8] show that entropy estimation based on the so-called increasing window estimator, or LZ78 estimator [5], is efficient in terms of convergence.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "\u2019s [8] match-length definition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "[8] (Equation 6) show that the entropy of the string can be approximated as", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "This approximates the entropy rate, or per-symbol entropy [8], which is denoted as h by Lesne et al.", "startOffset": 58, "endOffset": 61}, {"referenceID": 12, "context": "[13], and for which holds", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "So h accounts for all statistical dependencies between tokens [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "We will call h and its approximation H\u0303(T ) the source entropy - after Shannon\u2019s [29] formulation of the entropy of an information source.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "[8] give a more general definition that also holds for the so-called sliding window, or LZ77 estimator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "H1(T ), is identical to the source entropy h [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "entropies for block sizes of 1 (H1(T )) - using the Python implementation of the Nemenman-Shafee-Bialek (NSB) [19] estimator.", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "This estimator has a faster convergence rate compared to other block entropy estimators [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "\u2019s [8] proposal.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "This is inspired by an earlier implementation by Montemurro & Zanette [17] of Kontoyiannis et al.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "\u2019s [12] estimator.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[11] used a probabilistic phrase-based model to translate between the (then available) 11 languages of the EPC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Resnik [25], for instance, builds a", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Figure 5: Correlation between ratios of source entropies and BLEU scores of a statistical machine translation system [11].", "startOffset": 117, "endOffset": 121}], "year": 2016, "abstractText": "The average uncertainty associated with words is an informationtheoretic concept at the heart of quantitative and computational linguistics. The entropy has been established as a measure of this average uncertainty also called average information content. We here use parallel texts of 21 languages to establish the number of tokens at which word entropies converge to stable values. These convergence points are then used to select texts from a massively parallel corpus, and to estimate word entropies across more than 1000 languages. Our results help to establish quantitative language comparisons, to understand the performance of multilingual translation systems, and to normalize semantic similarity measures.", "creator": "LaTeX with hyperref package"}}}