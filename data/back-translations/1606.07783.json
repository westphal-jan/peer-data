{"id": "1606.07783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding", "abstract": "We are investigating the use of Convolutionary Neural Networks (CNNs) for the slot filling task in speech comprehension. We propose a novel CNN sequence labeling architecture that takes into account the previous contextual words with preserved ordering information and pays particular attention to the current word and its surrounding context. In addition, it combines information from the past and future words for classification. Our proposed CNN architecture even surpasses the previously best recurring neural network model, achieving state-of-the-art results with an F1 score of 95.61% in the ATIS benchmark dataset without using additional linguistic knowledge and resources.", "histories": [["v1", "Fri, 24 Jun 2016 18:35:56 GMT  (36kb,D)", "http://arxiv.org/abs/1606.07783v1", "Accepted at Interspeech 2016"]], "COMMENTS": "Accepted at Interspeech 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ngoc thang vu"], "accepted": false, "id": "1606.07783"}, "pdf": {"name": "1606.07783.pdf", "metadata": {"source": "CRF", "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding", "authors": ["Ngoc Thang Vu"], "emails": ["thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1. Introduction", "text": "The slot filling task in spoken language understanding (SLU) is to assign a semantic concept to each word in a sentence. In the sentence I want to fly from Munich to Rome, an SLU system should tag Munich as the departure city of a trip and Rome as the arrival city. All the other words, which do not correspond to real slots, are then tagged with an artificial class O. Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3]. More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].\nOverall, RNNs outperformed other NN models and achieved the state-of-the-art results on the ATIS benchmark dataset [9]. Furthermore, bi-directional RNNs have worked best so far showing that information from both the past and the future is important in predicting the semantic label of the current word. It is, however, well known that it is difficult to train an RNN due to the vanishing gradient problem [10]. Introducing long shortterm memory (LSTM) [11] or other variants of LSTM such as the gated recurrent unit (GRU) can solve this problem but, in turn increases the number of parameters significantly. Previous results reported in [8] did not show any improvement on the ATIS data set using LSTM or GRU.\nIn contrast to previous papers which reported state-of-theart results with RNNs, we explore the usage of convolutional neural networks for a sequence labeling task like slot filling. Previous research in [6] showed promising results on the slot filling task. The motivation behind this is to allow the model to search for patterns in order to predict the label of the current word independent of the feature representation of the previous word. Moreover, CNNs provide several advantages: it\npreserves the word order information, it is faster and easier to train and does not mix up the word sequence and therefore it is able to interpret the features learnt for the current task to some extent.\nThis study investigates the usage of CNNs for a sequential labeling task like slot filling with the following contributions:\n(1) We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context.\n(2) We extend the proposed CNN model to a bi-directional sequential CNN (bi-sCNN) which combines the information from past and future words for prediction.\n(3) We compare the impact of two different ranking objective functions on the recognition performance and analyze the most important n-grams for semantic slot filling.\n(4) On the ATIS benchmark dataset, the proposed bidirectional sequential CNN outperforms all RNN related models and defines a new start-of-the-art F1-score of 95.61%."}, {"heading": "2. Related Work", "text": "Neural network models such as RNNs and CNNs have been used in a wide range of natural language processing tasks. Vanilla RNNs or their extensions such as LSTMs or GRUs showed their success in many different tasks such as language modeling [12] or machine translation [13]. Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18]. For both models, distributed representations of words [19, 20] are used as input.\nIn the spoken language understanding research area, neural networks have also been applied to intent determination or semantic utterance classification tasks [21, 22]. For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set. Recently it was shown in [9] that applying ranking loss to train the model is effective for tasks that involve an artificial class like O. They achieved state-ofthe-art F1-scores of 95.47% with a single model and 95.56% by combining several models. In summary, the RNNs appear to be the best model for this task to date. The only previous study using convolutional neural networks was presented in [6] showing promising results. However, it did not outperform the RNN related models."}, {"heading": "3. Bi-directional Sequential CNN", "text": "This section describes the architecture of the bi-directional sequential CNN (bi-sCNN) illustrated in Figure 1. It contains\nar X\niv :1\n60 6.\n07 78\n3v 1\n[ cs\n.C L\n] 2\n4 Ju\nn 20\n16\nthree main components: a vanilla sequential CNN, an extended surrounding context and a bi-directional extension."}, {"heading": "3.1. Model", "text": "Vanilla sequential CNN. To predict the semantic slot of the current word wt, we consider n previous words in combination with the current word. In order to avoid the border effect, the m future padding words are also included. Each of the words is embedded into an d-dimensional word embedding space. Thus for each current word, we form a matrix w \u2208 R(n+m+1)\u00d7d as an input to the CNN for prediction.\nThere are several possibilities for convolving the input matrix: applying 1D filters to each dimension independently or applying 2D filters spanning some or all dimensions of the word embeddings. In this paper, we use 2D filters f (with width |f |) spanning all embedding dimensions d. This is described by the following equation:\n(w \u2217 f)(x, y) = d\u2211 i=1 |f |/2\u2211 j=\u2212|f |/2 w(i, j) \u00b7 f(x\u2212 i, y \u2212 j) (1)\nwhere w is the word matrix and f is the filter matrix. On each output, a nonlinear function such as the sigmoid function can be applied. After convolution, we use a max pooling operation to find the most important features. This function stores only the highest activation of each convolutional filter for the succeeding steps. If s filter matrices are used, an s-dimensional feature representation vector cpt is created for further classification.\nExtended surrounding context. When moving from one word to the next, the input matrix changes only slightly which leads to a large overlap of detected features from the convolutional and max pooling operator. Furthermore, the model needs to know which word is the current word for slot prediction. Therefore, in order to pay special attention to the current word and use the information of the word itself directly for the prediction, we introduce an additional component which uses the current word and its surrounding context words as input vector e(wt) with d(2 \u2217 cs + 1) dimensions. cs is the surrounding context length. The feature representation of the current word is computed as follows:\nhwt = f(U \u00b7 e(wt) + Vp \u00b7 cpt) (2)\nwhere U \u2208 Rs\u00d7d(2\u2217cs+1) and Vp \u2208 Rs\u00d7s. Bi-directional sequential CNN. As reported in [9], information not only from the past but also from the future contributes to the recognition accuracy. We therefore extend the sequential CNN to the future context. Because CNN preserves order information, we do not scan the input text from right to left like a bi-directional recurrent neural network. Instead, we take n future words in combination with the current word and the m previous padding words in the original order to form a matrix w \u2208 R(n+m+1)\u00d7d as an input to the future sequential CNN. Convolutional and max pooling operators are applied as in the vanilla sequential CNN to obtain a feature representation vector cft for the future context information.\nThere are two different ways to combine the information from the past and future contexts. The combination can be achieved by a weighted sum of the forward and the backward hidden layer. This leads to the following hidden layer output at time step t:\nhwt = f(U \u00b7 e(wt) + Vp \u00b7 cpt + Vf \u00b7 cft) (3)\nAnother combination option is to concatenate the forward and the backward hidden layer.\nhwt = [f(U \u00b7 e(wt) + Vp \u00b7 cpt), f(U \u00b7 e(wt) + Vf \u00b7 cft)] (4)\nThe combined hidden layer output is then used to predict the semantic label for the current word. The experimental results in Section 4 show that the combination method is an important design choice that effects the final performance."}, {"heading": "3.2. Training objective function", "text": "It was shown in [9] that using ranking loss is more accurate than cross entropy to train the model for this task. One reason might be that it does not force the network to learn a pattern for the O class which in fact may not exist. In this paper, we compare two different kinds of ranking loss functions.\nThe first function is the well known hinge loss function:\nL = max(0, 1\u2212 s\u03b8(wt)y+ + s\u03b8(wt)c\u2212) (5)\nwith s\u03b8(wt)y+ and s\u03b8(wt)c\u2212 as the scores for the target class and the wrongly predicted class of the model given the current word w respectively. This loss function maximizes the margin between those two classes.\nThe second one was proposed by Dos Santos et al. [23] and used in [9] to achieve the current best performance on the slot filling task till now. Instead of using the softmax activation function, we train a matrixW class whose columns contain vector representations of the different classes. Therefore, the score for each class c can be computed by using the product\ns\u03b8(wt)c = h T wt [W class]c (6)\nWe use the same ranking loss function as in [9] to train the CNNs. It maximizes the distance between the true label y+ and the best competitive label c\u2212 given a data point x. The objective function is"}, {"heading": "L = log(1 + exp(\u03b3(m+ \u2212 s\u03b8(wt)y+)))", "text": "+ log(1 + exp(\u03b3(m\u2212 + s\u03b8(wt)c\u2212)))\n(7)\nwith s\u03b8(wt)y+ and s\u03b8(wt)c\u2212 as the scores for the classes y + and c\u2212 respectively. The parameter \u03b3 controls the penalization of the prediction errors and m+ and m\u2212 are margins for\nthe correct and incorrect classes. \u03b3, m+ and m\u2212 are hyperparameters which can be tuned on the development set. For the class O, only the second summand of Equation 7 is calculated during training, i.e. the model does not learn a pattern for class O but nevertheless increases its difference to the best competitive label. Furthermore, it implicitly solves the problem of un-balanced data since the number of class O data points is much larger than in other classes. During testing, the model will predict class O if the scores for all other classes are < 0."}, {"heading": "3.3. Comparison with other neural models", "text": "The information flow of the proposed model is comparable with a bi-directional RNN. Instead of using the recurrent architecture to save the information from a long context, we use a convolutional operator to scan all the n-grams in the contexts and find the most important features with max pooling. At every time step, the most important features are then learnt independently from the previous time step. This poses an advantage over bidirectional RNNs when the previous word is a word of class O and the current word is not of class O because the information to predict class O is not helpful to predict other classes. Another difference is the integration of future information. In the backward RNN model, the sentence is scanned from right to left which is against the nature of languages like English. In contrast, the CNN keeps the correct order of the sentence and searches for important n-grams.\nAnother interpretation of this model is a joint training of a feed-forward NN and a CNN. The feedforward NN takes the current word with its surrounding context as input for prediction while the CNN searches for n-gram features from the past and future contexts. The context representation of the CNN is used as additional input of the feedforward NN. This is an advantage of this model over the CNN model proposed in [15] which has problems identifying the current word for labeling."}, {"heading": "4. Experimental Results", "text": ""}, {"heading": "4.1. Data", "text": "To compare our work with previously studied methods, we report results on the widely used ATIS dataset [24, 25]. This dataset is from the air travel domain and consists of audio recordings of speakers making travel reservations. All the words are labeled with a semantic label in a BIO format (B: begin, I: inside, O: outside), e.g. New York contains two words New and York and is therefore labeled with B-fromloc.city name and I-fromloc.city name respectively. Words which do not have semantic labels are tagged with O. In total, the number of semantic labels is 127, including the label of the class O. The training data consists of 4,978 sentences and 56,590 words. The test set contains 893 sentences and 9,198 words. To evaluate our models, we used the script provided in the text chunking CoNLL shared task 20001 in line with other related work."}, {"heading": "4.2. Model training", "text": "We used the Theano library [26] to implement the model. To train the model, stochastic gradient descent (SGD) was applied. We performed 5-fold cross-validation to tune the hyperparameters. The learning rate was kept constant for the first 10 epochs. Afterwards, we halved the learning rate after each epoch and stopped the training after 25 epochs. Note\n1http://www.cnts.ua.ac.be/conll2000/chunking/\nthat with more advanced techniques like AdaGrad [27] and AdaDelta [28] we did not achieve improvements over SGD with the described simple learning rate schedule. Since the learning schedule does not need a cross-validation set, we trained the final best model with the complete training data set. Table 1 shows the hyper-parameters used for all the CNN models."}, {"heading": "4.3. Results", "text": "We adopted the window approach proposed in [15] as the baseline system. Five left context words, five right context words and the current word form the input of a feed-forward neural network with one hidden layer with size 100. We obtained an F1-score of 94.23% and 94.14% with this simple feed-forward network using ranking loss and hinge loss respectively. Table 2 summarizes the performance on the ATIS test set with different CNN architectural setups. The results show that the context information from the past is more important than the future context. The future context, however, appears to provide meaningful information because their combination leads to better results. Moreover, the comparison between two different kinds of combinations of previous and future context (concatenation vs. addition) suggests to not mix up the information using addition. Finally, results in Table 2 also reveal that using the ranking loss function proposed in [23] outperforms the hinge loss function."}, {"heading": "5. Analysis", "text": "We performed analyses regarding the choice of context length, the impact of including the current word with its surrounding context and the most important detected n-grams."}, {"heading": "5.1. Context length", "text": "First, the impact of the context length on the final performance was explored. The number of parameters remained unchanged when reducing or increasing the context length. Short context means information loss while a long context length potentially adds noise to the input of the model. Table 3 shows that F1scores increased when increasing the context length from 5 up to 9. Increasing the context length to 10 and 11, however, decreased the results slightly but the F1-scores stayed quite stable around 95.5%. This confirms our hypothesis that a longer context adds noise to the input while the model is still able to extract the important information for slot prediction."}, {"heading": "5.2. Surrounding context", "text": "Table 4 summarizes the F1-score without using the current word or with the current context with various lengths of the surrounding contexts. The results revealed the strong impact of including the current word with its surrounding context into the CNN on the final F1-score. Without paying attention to the current word, the F1-score dropped significantly to 92.01%. Successively adding the current word and increasing its surrounding contexts up to three left and three right neighbour words resulted in better performance. Increasing the surrounding context to four, however, decreased F1-score. The best F1-score was obtained with three left and three right neighbour words."}, {"heading": "5.3. Most important n-grams", "text": "We analyzed the most significant patterns for the four most frequent semantic slots in the test data. For each of them, we present up to three n-grams which contributed the most to scoring the correctly classified test data points. To compute the most important n-grams, we first detected the position of the maximum contribution to the dot product and traced it back to the corresponding feature map. Based on the max pooling, we were able to trace back and identify the n-grams which were used. To create the results presented in Table 5, we ranked the n-grams which were selected as the most important features in all the sentences based on frequency and picked the most frequent ones. Table 5 shows that the model has learnt something meaningful for this task. For example, a pattern such as flights from A to B was used to predict fromloc.city name while the model only used A to B or to B for toloc.city name prediction. Other examples are patterns such as afternoon, evening and night which appeared quite\nfrequently after depart date.day name and therefore are learnt as indicators."}, {"heading": "6. Comparison with state of the art", "text": "Table 6 lists several previous results on the ATIS data set including our best results. The proposed R-bi-sCNN outperforms\nthe previously best ranking bi-directional RNN (R-bi-RNN). A more detailed comparison with R-bi-RNN shows that R-bisCNN performed as well as R-bi-RNN on the frequent semantic slots but outperformed R-bi-RNN on the rare slots. For example, rare slots such as toloc.country name, days code, period of day, which appeared less than six times in the training data, were correctly predicted with the R-bi-sCNN model but not with R-bi-RNN ."}, {"heading": "7. Conclusions", "text": "This paper explored convolutional neural networks for the slot filling task in spoken language understanding. Our novel CNN architecture - bi-directional sequential CNN - takes into account the information from the past and the future with preserved order information and pays special attention to the current word with its surrounding contexts. To train the model, we compared two different ranking objective functions. Our findings revealed that not forcing the model to learn a pattern for O class is helpful to improve the final performance. Finally, our bi-directional sequential CNN achieves state-of-the-art results with an F1-score of 95.61% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources. As future work, we aim to evaluate the proposed model on other datasets (e.g. data presented in [29, 30])."}, {"heading": "8. Acknowledgements", "text": "This work was funded by the German Science Foundation (DFG), Sonderforschungsbereich 732 Incremental Specification in Context, Project A8, at the University of Stuttgart."}, {"heading": "9. References", "text": "[1] Y. Wang, L. Deng, and A. Acero. Spoken Language Understand-\ning An Introduction to the Statistical Framework, IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16-31, 2005.\n[2] J. Lafferty, A. McCallum, and F. P ereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in Proc. of ICML, 2001.\n[3] Y. Wang, L. Deng, and A. AceroSemantic Frame Based Spoken Language Understanding, in Chapter 3, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, pp. 35-80, Wiley, 2011.\n[4] K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu, Recurrent neural networks for language understanding, in Proc. of Interspeech, 2013.\n[5] G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. HakkaniTur, X. He, L. Heck, G. Tur, D. Yu, and G. Zweig, Using recurrent neural networks for slot filling in spoken language understanding, IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530-539, 2015.\n[6] P. Xu and R. Sarikaya, Convolutional neural network based triangular CRF for joint intent detection and slot filling, in Proc. of ASRU, 2013.\n[7] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, Spoken language understanding using long short-term memory neural networks, in Proc. of SLT, 2014.\n[8] B. Peng, K. Yao. Recurrent Neural Networks with External Memory for Language Understanding, in arXiv, 2015.\n[9] N.T. Vu, P. Gupta, H. Adel and H. Schuetze. Bi-directional Recurrent Neural Network with Ranking Loss for Spoken Language Understanding, in Proc. of ICASSP, 2016.\n[10] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, in S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.\n[11] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory, Neural Computation, 9(8):1735?1780, 1997.\n[12] T. Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur, Extensions of recurrent neural network based language model, in Proc. of ICASSP, 2011.\n[13] K. Cho, B. van Merrienboer, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio Learning phrase representations using RNN encoder-decoder for statistical machine translation, in Proc. of EMNLP, 2014.\n[14] R. Collobert and J. Weston, A unified architecture for natural language processing: deep neural networks with multitask learning, in Proc. of ICML, 2008.\n[15] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, Natural language processing (almost) from scratch, in Journal of Machine Learning Research, vol. 12, 2011.\n[16] Y. Wenpeng, and H. Schtze. MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity, in Proc. of ACL, 2015.\n[17] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.\n[18] Y. Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.\n[19] Y. Bengio, R. Ducharme and P. Vincent, A Neural Probabilistic Language Model, in Proc. of NIPS, 2000.\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient Estimation of Word Representations in Vector Space, in Proc. of Workshop at ICLR, 2013.\n[21] L. Deng, G. Tur, X. He, and D. Hakkani-Tur, Use of Kernel Deep Convex Networks and End-To-End Learning for Spoken Language Understanding, in Proc. of SLT, 2012.\n[22] G. Tur, L. Deng, D. Hakkani-Tur, and X. He, Towards Deeper Understanding Deep Convex Networks for Semantic Utterance Classification, in Proc. of ICASSP, 2012.\n[23] C.N. Dos Santos, B. Xiang, and B. Zhou. Classifying relations by ranking with convolutional neural networks, in Proc. of ACL, 2015.\n[24] C. Hemphill, J. Godfrey, and G. Doddington, The ATIS spoken language systems pilot corpus, in Proc. of the DARPA speech and natural language workshop, 1990.\n[25] P. Price, Evaluation of spoken language systems: The ATIS domain, in Proc. of the Third DARPA Speech and Natural Language Workshop. Morgan Kaufmann, 1990.\n[26] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I.J. Goodfellow, A. Bergeron, N. Bouchard, Y. and Bengio, Y. Theano: new features and speed improvements, in Proc. of Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2012.\n[27] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization, Journal of Machine Learning Research, vol. 12, pp. 2121-2159, 2010.\n[28] M.D. Zeiler. ADADELTA: An Adaptive Learning Rate Method, CoRR, abs/1212.5701, 2012.\n[29] G. Tur, D. Hakkani-Tur, L. Heck. What is left to be understood in ATIS?, in Proc. of SLT, 2010.\n[30] S. Hahn, M. Dinarelli, C. Raymond, F. Lefevre, P. Lehnen, R.D. Mori, A. Moschitti, H. Ney, G. Riccardi. Comparing stochastic approaches to spoken language understanding in multiple languages, in IEEE Transactions on Audio, Speech, and Language Processing, pp. 1569-1583, 2011."}], "references": [{"title": "Spoken Language Understanding An Introduction to the Statistical Framework", "author": ["Y. Wang", "L. Deng", "A. Acero"], "venue": "IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16-31", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "and F", "author": ["J. Lafferty", "A. McCallum"], "venue": "P ereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in Proc. of ICML", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "and A", "author": ["Y. Wang", "L. Deng"], "venue": "AceroSemantic Frame Based Spoken Language Understanding, in Chapter 3, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, pp. 35-80, Wiley", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M. Hwang", "Y. Shi", "D. Yu"], "venue": "Proc. of Interspeech", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani- Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530-539", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Proc. of ASRU", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "Proc. of SLT", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent Neural Networks with External Memory for Language Understanding", "author": ["B. Peng", "K. Yao"], "venue": "arXiv", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Bi-directional Recurrent Neural Network with Ranking Loss for Spoken Language Understanding", "author": ["N.T. Vu", "P. Gupta", "H. Adel", "H. Schuetze"], "venue": "Proc. of ICASSP", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735?1780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Stefan Kombrink", "author": ["T. Mikolov"], "venue": "Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur, Extensions of recurrent neural network based language model, in Proc. of ICASSP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio Learning phrase representations using RNN encoder-decoder for statistical machine translation, in Proc. of EMNLP", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity", "author": ["Y. Wenpeng", "H. Schtze"], "venue": "Proc. of ACL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Proc. of NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proc. of Workshop at ICLR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Use of Kernel Deep Convex Networks and End-To-End Learning for Spoken Language Understanding", "author": ["L. Deng", "G. Tur", "X. He", "D. Hakkani-Tur"], "venue": "Proc. of SLT", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards Deeper Understanding Deep Convex Networks for Semantic Utterance Classification", "author": ["G. Tur", "L. Deng", "D. Hakkani-Tur", "X. He"], "venue": "Proc. of ICASSP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C.N. Dos Santos", "B. Xiang", "B. Zhou"], "venue": "Proc. of ACL", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The ATIS spoken language systems pilot corpus", "author": ["C. Hemphill", "J. Godfrey", "G. Doddington"], "venue": "Proc. of the DARPA speech and natural language workshop", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P. Price"], "venue": "Proc. of the Third DARPA Speech and Natural Language Workshop. Morgan Kaufmann", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1990}, {"title": "Y", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard"], "venue": "and Bengio, Y. Theano: new features and speed improvements, in Proc. of Deep Learning and Unsupervised Feature Learning NIPS Workshop", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121-2159", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "What is left to be understood in ATIS", "author": ["G. Tur", "D. Hakkani-Tur", "L. Heck"], "venue": "Proc. of SLT", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparing stochastic approaches to spoken language understanding in multiple languages", "author": ["S. Hahn", "M. Dinarelli", "C. Raymond", "F. Lefevre", "P. Lehnen", "R.D. Mori", "A. Moschitti", "H. Ney", "G. Riccardi"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, pp. 1569-1583", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3].", "startOffset": 167, "endOffset": 173}, {"referenceID": 2, "context": "Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 4, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 5, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 6, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 7, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 8, "context": "Overall, RNNs outperformed other NN models and achieved the state-of-the-art results on the ATIS benchmark dataset [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "It is, however, well known that it is difficult to train an RNN due to the vanishing gradient problem [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "Introducing long shortterm memory (LSTM) [11] or other variants of LSTM such as the gated recurrent unit (GRU) can solve this problem but, in turn increases the number of parameters significantly.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "Previous results reported in [8] did not show any improvement on the ATIS data set using LSTM or GRU.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Previous research in [6] showed promising results on the slot filling task.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "Vanilla RNNs or their extensions such as LSTMs or GRUs showed their success in many different tasks such as language modeling [12] or machine translation [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "Vanilla RNNs or their extensions such as LSTMs or GRUs showed their success in many different tasks such as language modeling [12] or machine translation [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 76, "endOffset": 84}, {"referenceID": 14, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 76, "endOffset": 84}, {"referenceID": 15, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 144, "endOffset": 152}, {"referenceID": 17, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 144, "endOffset": 152}, {"referenceID": 18, "context": "For both models, distributed representations of words [19, 20] are used as input.", "startOffset": 54, "endOffset": 62}, {"referenceID": 19, "context": "For both models, distributed representations of words [19, 20] are used as input.", "startOffset": 54, "endOffset": 62}, {"referenceID": 20, "context": "In the spoken language understanding research area, neural networks have also been applied to intent determination or semantic utterance classification tasks [21, 22].", "startOffset": 158, "endOffset": 166}, {"referenceID": 21, "context": "In the spoken language understanding research area, neural networks have also been applied to intent determination or semantic utterance classification tasks [21, 22].", "startOffset": 158, "endOffset": 166}, {"referenceID": 3, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 32, "endOffset": 38}, {"referenceID": 4, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 32, "endOffset": 38}, {"referenceID": 6, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 60, "endOffset": 66}, {"referenceID": 5, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 157, "endOffset": 160}, {"referenceID": 8, "context": "Recently it was shown in [9] that applying ranking loss to train the model is effective for tasks that involve an artificial class like O.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "The only previous study using convolutional neural networks was presented in [6] showing promising results.", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "As reported in [9], information not only from the past but also from the future contributes to the recognition accuracy.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "It was shown in [9] that using ranking loss is more accurate than cross entropy to train the model for this task.", "startOffset": 16, "endOffset": 19}, {"referenceID": 22, "context": "[23] and used in [9] to achieve the current best performance on the slot filling task till now.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[23] and used in [9] to achieve the current best performance on the slot filling task till now.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "We use the same ranking loss function as in [9] to train the CNNs.", "startOffset": 44, "endOffset": 47}, {"referenceID": 14, "context": "This is an advantage of this model over the CNN model proposed in [15] which has problems identifying the current word for labeling.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "To compare our work with previously studied methods, we report results on the widely used ATIS dataset [24, 25].", "startOffset": 103, "endOffset": 111}, {"referenceID": 24, "context": "To compare our work with previously studied methods, we report results on the widely used ATIS dataset [24, 25].", "startOffset": 103, "endOffset": 111}, {"referenceID": 25, "context": "We used the Theano library [26] to implement the model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "be/conll2000/chunking/ that with more advanced techniques like AdaGrad [27] and AdaDelta [28] we did not achieve improvements over SGD with the described simple learning rate schedule.", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "be/conll2000/chunking/ that with more advanced techniques like AdaGrad [27] and AdaDelta [28] we did not achieve improvements over SGD with the described simple learning rate schedule.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "We adopted the window approach proposed in [15] as the baseline system.", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "Finally, results in Table 2 also reveal that using the ranking loss function proposed in [23] outperforms the hinge loss function.", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Methods F1-score CRF [5] 92.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "94 simple RNN [4] 94.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "11 CNN [6] 94.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "35 LSTM [7] 94.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "85 RNN-EM [8] 95.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "25 R-bi-RNN [9] 95.", "startOffset": 12, "endOffset": 15}, {"referenceID": 28, "context": "data presented in [29, 30]).", "startOffset": 18, "endOffset": 26}, {"referenceID": 29, "context": "data presented in [29, 30]).", "startOffset": 18, "endOffset": 26}], "year": 2016, "abstractText": "We investigate the usage of convolutional neural networks (CNNs) for the slot filling task in spoken language understanding. We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. Moreover, it combines the information from the past and the future words for classification. Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.", "creator": "LaTeX with hyperref package"}}}