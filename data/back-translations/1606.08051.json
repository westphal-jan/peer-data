{"id": "1606.08051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "Training LDCRF model on unsegmented sequences using Connectionist Temporal Classification", "abstract": "Many machine learning problems, such as speech recognition, gesture recognition and handwriting recognition, involve the simultaneous segmentation and labelling of sequence data. Latent Dynamic Random Field (LDCRF) is a well-known discriminatory method that has been successfully used for this task. However, LDCRF can only be trained with pre-segmented data sequences where the labelling of each frame is available. In the field of neural networks, the invention of Connectionist Temporal Classification (CTC) has been very successful in enabling the formation of recursive neural networks on unsegmented sequences. In this paper, we use CTC to train the LDCRF model on unsegmented sequences. Experimental results on gesture recognition tasks show that our proposed method can outperform LDCRF, Hidden Markov Models and Conditional Random Fields.", "histories": [["v1", "Sun, 26 Jun 2016 16:26:19 GMT  (491kb)", "http://arxiv.org/abs/1606.08051v1", null], ["v2", "Tue, 28 Jun 2016 14:23:06 GMT  (491kb)", "http://arxiv.org/abs/1606.08051v2", null], ["v3", "Tue, 6 Sep 2016 11:57:06 GMT  (509kb)", "http://arxiv.org/abs/1606.08051v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["amir ahooye atashin", "kamaledin ghiasi-shirazi", "ahad harati"], "accepted": false, "id": "1606.08051"}, "pdf": {"name": "1606.08051.pdf", "metadata": {"source": "CRF", "title": "Training LDCRF model on unsegmented sequences using Connectionist Temporal Classification", "authors": [], "emails": ["1amir.atashin@stu.um.ac.ir", "2k.ghiasi@um.ac.ir", "3a.harati@um.ac.ir"], "sections": [{"heading": null, "text": "recognition, gesture recognition, and handwriting recognition are concerned with simultaneous segmentation and labeling of sequence data. Latent-dynamic conditional random field (LDCRF) is a well-known discriminative method that has been successfully used for this task. However, LDCRF can only be trained with pre-segmented data sequences in which the label of each frame is available. In the realm of neural networks, the invention of Connectionist Temporal Classification (CTC) made it possible to train Recurrent Neural Networks on unsegmented sequences with great success. In this paper, we use CTC to train LDCRF model on unsegmented sequences. Experimental results on gesture recognition tasks show that our proposed method can outperform LDCRF, Hidden Markov Models, and Conditional Random Fields.\nKeywords: Latent-dynamic conditional random field; Connectionist Temporal Classification; unsegmented sequences;\nI. INTRODUCTION Labeling and segmenting of data sequences is a common problem in sequence classification tasks [1]. It arises in many real-world applications such as gesture recognition, speech recognition, and handwriting recognition.\nProbabilistic Graphical Models (PGMs) are the type of probabilistic models which are commonly used in machine learning problems [2]. They offer a powerful framework in which relations between two or more stochastic variables is expressed by representing graph for them. PGMs are divided into two categories: undirected graphical models or Markov Random Fields (MRFs) and directed graphical models or Bayesian Networks (BNs). Conditional Random Fields (CRFs) (Figure 1.(a)) [3] is a variant of MRFs, and Hidden Markov Models (HMMs) [4] is a variant of BNs, are two well-known models which are widely used in sequence classification problems.\nHMM is generative classifier which models the joint distribution of input sequences and their corresponding labels, but, CRF is discriminative classifier which models a conditional distribution label given input sequence.\nLatent-dynamic conditional random field (LDCRF) [5] is a type of CRF which used for structured prediction problems. It\nwas proposed to use in the special case of sequence classification problems which labels must be assigned to each frame of unsegmented sequences. Learning procedure for these models is discriminative and carried out on a set of frame wise labeled training sets.\nLDCRF model can be described as a generalized form of chain-structured CRF model in which hidden variables are incorporated into the CRF to learn Various structures of class labels. Similar to the Hidden Conditional Random Fields (HCRFs) [6], LDCRF uses hidden variables to learn both extrinsic and intrinsic relations for various structures of labels. It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].\nDespite the LDCRF can be used for segmenting and tagging of unsegmented sequences, it requires to train on the set of presegmented sequences. However, for many real-world data sets in sequence classification tasks, the segmented sequence of labels does not available and only unsegmented sequences, so LDCRF model can\u2019t directly train on them.\nA. Graves and et al. introduced a method called \u201cConnectionist Temporal Classification\u201d (CTC) [11, 12] which can be joint with RNNs and provides a framework that is able to train, directly on unsegmented sequences. They used CTC as a layer on top of recurrent neural networks such as LSTM and its variant, then applied it to the sequence classification problems such as online/offline unconstrained handwriting recognition. Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].\nIn this paper, we present a method for training LDCRF model directly on unsegmented sequences. We used the CTC method to remove the needs of pre-segmented sequences for training LDCRF.\nThe next section reviews the pre-required methods and definitions of our proposed method. Section (III), describes the motivation and the idea of the proposed method, mathematical formulation, and its training algorithm. Section (IV), explains the datasets used in our experiments for sequence classification. Section (V), discusses the experimented results of our CTC-\nLDCRF method and compare it with standard LDCRF, CRF, and HMM models. Section (VI) gives a direction for future works and concludes this paper.\nII. PRELIMINARIES In this section, we first briefly explain the temporal classification tasks, and then review two LDCRF and CTC methods. We used same notations for these three subsections and next sections."}, {"heading": "A. Temporal Classification", "text": "The temporal classification is the extinction of sequence classification where each input sequence is associated with a sequence of labels and alignment between them is unknown.\nLet = {( , ), \u2026 , ( , )} be a set of training samples which each training samples = ( , ) in S consists of a pair an input data sequence ( = , , \u2026 , ) with length and corresponding label sequence = , , \u2026 , with length which length of the class label sequence is at most as long as data sequence (i.e. \u2264 ). Each is d-dimensional feature vector and is a member of set which is set of all possible class labels.\nThe goal in the temporal classification tasks is training a model to learn a map between input data sequences and their corresponding label sequences, which should decrease error on a set of test sequences."}, {"heading": "B. Latent-dynamic conditional random field (LDCRF)", "text": "LDCRF is an undirected graphical model that was developed for the special case of sequence classification problems where the goal is to predict a label for each frame of a given data sequence (Figure 1. (b)).\nLet = {( , ) \u2026 ( , )} be a set consists of n training samples, each sample = ( , ) in is a pair of input data sequence = , , \u2026 , and corresponding class label sequence = , , \u2026 , which both have the same length. There is also a sequence of hidden variables ( = \u210e , \u210e , \u2026 , \u210e ) which don\u2019t observe in .\nEvery data point is d-dimensional input vector (i.e. \u2208 \u211d ) and their corresponding label is one-dimensional single value of set which is set of all possible class labels in the dataset (i.e. \u2208 ). Also, every hidden variable is a member of H which is a set of all possible hidden states of all labels (\u210b = \u22c3 \u210b\u2208 ).\nFor having tractable inference and training algorithm in LDCRF, each class label is restricted to have a disjoint set of hidden variables with others class (i.e. \u210e \u2209 \u210b \u2200 \u2260 ).\nThe LDCRF model formula with the parameter set is defined:\n( | , ) = ( | , ) :\u2200 \u2208\u210b\n(1)\nThen ( | , ) is defined like a CRF formulations:\n( | , ) = (\u2211 \u00d7 ( , ))\n( , ) (2)\nWhere is a partition function and can be denoted as:\n( , ) = ( , )\nAnd is a potential function associated to parameter is defined as:\n( , ) = (\u210e , \u210e , , ) (3)\nEach feature function can be either a state function or transition function , the state function related to a single hidden variable in the model while transition function related to a pair of hidden variables."}, {"heading": "C. Connectionist Temporal Classification", "text": "Connectionist Temporal Classification (CTC) [11], as its name would suggest, is proposed for use in temporal classification tasks. It is used as an output layer of RNNs for temporal classification problems.\nLet be a training set (as was explained in the Section II.A), CTC Objective is defined as sum of negative log-likelihood for every sample = ( , ) in :\n= \u2212 log ( | ) \u2208\n(4)\nFor each frame of input data sequence by the size of , CTC gets a probability of all class labels. Hence it gets a table of probability\u2019s by the size of \u00d7 | | as inputs, then efficiently\ncompute log ( | ) using Dynamic Programing. Moreover, it computes a table of errors for every entity of its input table.\nIII. PROPOSED METHOD In the first section, we talked about the limitation of LDCRF model which can only apply to sequence classification problems where training sequences are frame-wise labeled. We also talked about the CTC goal which is used with RNNs for training them with unsegmented sequences.\nIn this section, we present a method which allows the LDCRF to be trained directly on unsegmented sequences. We use CTC on top of the LDCRF model to construct our framework (Figure 2). For the sake clarity, we named our framework \u201cCTC-LDCRF\u201d. In the next subsections, our framework, formulation and learning algorithm are described."}, {"heading": "A. Our Framework overview", "text": "We propose a discriminative framework for unsegmented sequence learning tasks. As shown in Figure 2, it consists of two layers:\n The model layer which LDCRF placed in it.  CTC objective layer.\nAt the first step, LDCRF takes a data sequence as input and computes the labels probability for each frame. Then CTC takes both LDCRF outputs and desired label sequence to calculate objective function in (5) as described in the Section (II.C), CTC also computes objective function gradients respect to labels probability for every node of LDCRF. Like familiar backpropagation method, we use CTC gradients to obtain update rule for LDCRF parameters. Finally, gradient descent technique is used to optimize the CTC objective."}, {"heading": "B. CTC-LDCRF Learning algorithm", "text": "Our training set consists of n pairs ( , ) \u2200 = 1 \u2026 of a data sequence and corresponding label sequence with unknown alignments between them (as was explained in the Section II.A).\nWe use the following goal to learn the LDCRF parameter :\n= \u2212 ( | ) + ( ) (5)\nThe first term in (5) is CTC objective (4) and computes in the second layer of our framework. The second term is a regularization function for the model parameters.\nWe use gradient decent for finding optimal parameter values of the LDCRF model. Using following chain rule we can obtain the gradient of CTC objective with respect to LDCRF parameters:\n= ( | ) \u00d7 ( | )\nFor one particular training sample ( , ), we assume that parameter , is parameter of node in LDCRF which associate to feature function ( ), so for gradient with respect to , , we have:\n, = ( = | ) \u00d7\n( = | )\n,\u2208\n(6)\nAs we described in Section II.C, first terms in summation of (6) are computed in CTC, so it is just needed to compute second part. The gradient of ( ) with respect to (parameter associated with a feature state ) is obtained from:\n, = ( = | ) \u00d7 ( , ) \u00d7 (\u210e = | ) \u2208\n\u00d7 , \u2212 ( = | )\nAnd , defined as:\n, = 1, \u2208 \u210b 0, \u210e\nSince parameter , is same for every node of LDCRF model, then the gradient for training sample ( , ) with respect to obtained from:\n= ( = | ) \u00d7 ( , )\n\u2208 \u00d7 (\u210e = | )\n\u00d7 , \u2212 ( = | )\n(7)\nThe marginal probabilities (\u210e = | ) in (7), can efficiently compute for all hidden states of the LDCRF model using belief propagation.\nThe gradient of with respect to parameters that related to the transition functions can be obtained with the same way."}, {"heading": "C. LDCRF-CTC inference", "text": "For testing, given a new test sequence , we want to estimate the most probable label sequence \u2217 that maximizes conditional model:\n\u2217 = ( | , \u2217) (8)\nwhere \u2217 is parameter values that were learned in the training phase.\nIV. EXPERIMENTS In this section, we describe the datasets and the methodology which were used in our experiments to evaluate our model performance.\nAvatar-Eye Dataset:\nThis dataset consisted of eye gaze gesture roughly calculated from 6 human contributors interacting with an avatar [17]. The aim is to recognize eye gaze aversion gestures versus all other types of eye gestures from unsegmented video sequences.\nThe input features for each video sequence is 2-dimensional eye gaze obtained using the view-based appearance model. Each frame of a video sequence in the dataset was tagged either as \u201ceye gaze aversion\u201d or \u201cother eye gestures\u201d.\nArmGesture-Continuous Dataset: The ArmGesture dataset includes six arm gestures (Figure 3). It was collected from 13 contributors with an average of 120 samples per class where each sample is a sequence of the 20- dimensional feature vector. This dataset is containing unsegmented sequences of gestures based on the original ArmGesture dataset. The unsegmented sequence generated by randomly selected 3 to 5 (segmented) samples of the different type of gestures, and serialized them in random order. The ArmGesture-Continuous dataset contains 182 samples, with a mean length of 92 frames."}, {"heading": "A. Models", "text": "We compare three configurations of our proposed method with LDCRF, CRF, and HMM on ArmGesture-Continues and Avatar-Eye datasets.\nConditional Random Field: we trained a single chain CRF with its standard objective function and regularize term, we varied window size from 0 to 2.\nLatent-dynamic Conditional Random Field: we trained a single chain LDCRF model with the objective function described in [5]. we differing the number of hidden states per label from 2 to 6 and the window size from 0 to 2.\nHidden Markov Model: we used an HMM for each class. Each HMM was trained with segmented subsequences where\nthe frames of each subsequence all fitted to the equal class. This training set contained the same number of frames as the one used for training the CRF and LDCRF models excluding frames were grouped into subsequences according to their tag. The final model was completed by combining HMMs together.\nCTC-LDCRF: we trained a single chain CTC-LDCRF model with three type of configuration. For the first configuration, we train model with unsegmented label sequence which never done before on these two data sets.\nFor the second configuration, the model trained with framewised tagged sequences similar to LDCRF and CRF training data.\nAs the final configuration, we use two-phase training procedure, at the first phase the model pre-trained with segmented subsequences (similar to the HMM training set), then we use the learned weights in the first phase to initialize the model parameters for the second phase of procedure to tune weights by training the model on unsegmented label sequences. Within training, we differ the number of hidden states per label for the LDCRF from 2 to 6 and window size from 0 to 2.\nFor all of this three configuration, we adding one additional class label to model as \u201cnon-gesture\u201d or \u201cblank\u201d in order to use in computation of CTC layer."}, {"heading": "B. Methodology", "text": "For these two datasets, the experiments were implemented using a K-fold testing method where dataset divided into K equal group. A single group used for testing while all other sequences were used for training and validation. This process was repeated K times, and a mean of the K result reported for accuracy. We use K = 5 for the ArmGesture-Continues and K = 2 for the Avatar-Eye dataset. The measure for the models performance is number of the correct predicted labels for each frame of every test sequence:\n= \u2211 #\n\u2211 # \u00d7 100 (9)\nV. RESULTS AND DISCUSSION We compare the result of the three configurations of our LDCRF-CTC learning algorithm on ArmGesture-Continues and Avatar-Eye Datasets with standard LDCRF, Linear CRF, and HMM Models in our experiments. We use (9) to compare experiment results. the ROC curve for the experiment on AvatarEye dataset (Figure 4) shows that the CTC-LDCRF which was trained with unsegmented sequence outperforms the other models. This can be explained because of data sequences framewise tagged manually by a human agent and it may not exactly correct especially on boundaries where label changed in sequence. Since we use unsegmented training sequence and don\u2019t use frame wised labels our method achieves better performance compared with other methods, also because of Avatar-Eye is a small dataset with 3-Dimentional feature vector and only 2 class label, the CTC-LDCRF method achieves good performance without needs to pre-trained.\nTable I. shows the experimental results on ArmGestureContinues dataset. The third configuration of Our CTC-LDCRF training methods has higher accuracy compare to others experimented methods.\nVI. CONCLUSION In this paper, we develop a framework for training LDCRF model directly on the unsegmented sequence. We employed CTC approach in our framework to make the LDCRF model able to trains directly with unsegmented sequences. We did experiments on two gesture recognition datasets and showed that our model achieves better performance over other experimented models, even though it is trained on unsegmented data sequence and gets less information than other models.\nFor the future works we plan to extend our framework and use a neural network such as LSTM and Convolutional Neural Network (CNN) as a feature extractor and make it possible to apply our framework on more complex and raw real-world data sets [18, 19]."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Mostafa Rafiee, MSc student at the Ferdowsi university of Mashhad, for his noteworthy discussions and useful assist in implementations and experiments."}], "references": [{"title": "A brief survey on sequence classification", "author": ["Z. Xing", "J. Pei", "E. Keogh"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, pp. 40-48, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, pp. 257-286, 1989.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["L.-P. Morency", "A. Quattoni", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, 2007, pp. 1-8.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, pp. 1848-1852, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1848}, {"title": "Latent pose estimator for continuous action recognition", "author": ["H. Ning", "W. Xu", "Y. Gong", "T. Huang"], "venue": "Computer Vision\u2013ECCV 2008, ed: Springer, 2008, pp. 419-433.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Conditional sequence model for context-based recognition of gaze aversion", "author": ["L.-P. Morency", "T. Darrell"], "venue": "Machine Learning for Multimodal Interaction, ed: Springer, 2007, pp. 11-23.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-view latent variable discriminative models for action recognition", "author": ["Y. Song", "L.-P. Morency", "R. Davis"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2012, pp. 2120-2127.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Sign language spotting with a threshold model based on conditional random fields", "author": ["H.-D. Yang", "S. Sclaroff", "S.-W. Lee"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, pp. 1264-1277, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369-376.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural Networks", "author": ["A. Graves"], "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, ed: Springer, 2012, pp. 15-35.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, pp. 855-868, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2009, pp. 545-552.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "ICML, 2014, pp. 1764-1772.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing gaze aversion gestures in embodied conversational discourse", "author": ["L.-P. Morency", "C.M. Christoudias", "T. Darrell"], "venue": " Proceedings of the 8th international conference on Multimodal interfaces, 2006, pp. 287-294.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition", "author": ["D. Wu", "L. Pigou", "P.-J. Kindermans", "L. Nam", "L. Shao", "J. Dambre"], "venue": "2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural random fields for action recognition", "author": ["C. Liu", "J. Liu", "Z. He", "Y. Zhai", "Q. Hu", "Y. Huang"], "venue": "Pattern Recognition, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Labeling and segmenting of data sequences is a common problem in sequence classification tasks [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "Probabilistic Graphical Models (PGMs) are the type of probabilistic models which are commonly used in machine learning problems [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "(a)) [3] is a variant of MRFs, and Hidden Markov Models (HMMs) [4] is a variant of BNs, are two well-known models which are widely used in sequence classification problems.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "(a)) [3] is a variant of MRFs, and Hidden Markov Models (HMMs) [4] is a variant of BNs, are two well-known models which are widely used in sequence classification problems.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Latent-dynamic conditional random field (LDCRF) [5] is a type of CRF which used for structured prediction problems.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "Similar to the Hidden Conditional Random Fields (HCRFs) [6], LDCRF uses hidden variables to learn both extrinsic and intrinsic relations for various structures of labels.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 7, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 8, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 9, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 10, "context": "introduced a method called \u201cConnectionist Temporal Classification\u201d (CTC) [11, 12] which can be joint with RNNs and provides a framework that is able to train, directly on unsegmented sequences.", "startOffset": 73, "endOffset": 81}, {"referenceID": 11, "context": "introduced a method called \u201cConnectionist Temporal Classification\u201d (CTC) [11, 12] which can be joint with RNNs and provides a framework that is able to train, directly on unsegmented sequences.", "startOffset": 73, "endOffset": 81}, {"referenceID": 12, "context": "Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].", "startOffset": 168, "endOffset": 175}, {"referenceID": 13, "context": "Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].", "startOffset": 168, "endOffset": 175}, {"referenceID": 14, "context": "Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].", "startOffset": 168, "endOffset": 175}, {"referenceID": 10, "context": "Connectionist Temporal Classification Connectionist Temporal Classification (CTC) [11], as its name would suggest, is proposed for use in temporal classification tasks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "Avatar-Eye Dataset: This dataset consisted of eye gaze gesture roughly calculated from 6 human contributors interacting with an avatar [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Latent-dynamic Conditional Random Field: we trained a single chain LDCRF model with the objective function described in [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 16, "context": "For the future works we plan to extend our framework and use a neural network such as LSTM and Convolutional Neural Network (CNN) as a feature extractor and make it possible to apply our framework on more complex and raw real-world data sets [18, 19].", "startOffset": 242, "endOffset": 250}, {"referenceID": 17, "context": "For the future works we plan to extend our framework and use a neural network such as LSTM and Convolutional Neural Network (CNN) as a feature extractor and make it possible to apply our framework on more complex and raw real-world data sets [18, 19].", "startOffset": 242, "endOffset": 250}], "year": 2016, "abstractText": "Many machine learning problems such as speech recognition, gesture recognition, and handwriting recognition are concerned with simultaneous segmentation and labeling of sequence data. Latent-dynamic conditional random field (LDCRF) is a well-known discriminative method that has been successfully used for this task. However, LDCRF can only be trained with pre-segmented data sequences in which the label of each frame is available. In the realm of neural networks, the invention of Connectionist Temporal Classification (CTC) made it possible to train Recurrent Neural Networks on unsegmented sequences with great success. In this paper, we use CTC to train LDCRF model on unsegmented sequences. Experimental results on gesture recognition tasks show that our proposed method can outperform LDCRF, Hidden Markov Models, and Conditional Random Fields.", "creator": null}}}