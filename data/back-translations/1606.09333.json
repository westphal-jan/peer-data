{"id": "1606.09333", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems", "abstract": "Many of the canonical problems of machine learning boil down to a convex optimization problem with a finite sum structure. However, while great progress has been made in developing faster algorithms for this setting, the limitations inherent in these problems are not addressed satisfactorily by existing lower limits. In fact, current limits focus on first-order optimization algorithms and apply only in the often unrealistic regime where the number of iterations is less than $\\ mathcal {O} (d / n) $(where $d $is the dimension and $n $is the number of samples).In this work, we expand the scope of (Arjevani et al., 2015) to provide new lower limits that are dimensionless and go beyond the assumptions of current boundaries, covering standard methods of finite sum optimization, such as SAG, SAGA, SVRG, SDCA without duality, and stochastic methods of coordinate derivation such as SDCA and SDCA accelerated sum optimization.", "histories": [["v1", "Thu, 30 Jun 2016 03:10:54 GMT  (223kb,D)", "http://arxiv.org/abs/1606.09333v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG math.NA", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1606.09333"}, "pdf": {"name": "1606.09333.pdf", "metadata": {"source": "CRF", "title": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems", "authors": ["Yossi Arjevani", "Ohad Shamir"], "emails": ["yossi.arjevani@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Many machine learning tasks reduce to Finite Sum Minimization (FSM) problems of the form\nmin x\u2208Rd\nF (w) := 1\nn n\u2211 i=1 fi(w), (1)\nwhere fi are L-smooth and \u00b5-strongly convex. In recent years, a major breakthrough was made when a linear convergence rate was established for this setting (SAG [16] and SDCA [18]), and since then, many methods have been developed to achieve better convergence rate. However, whereas a large body of literature is devoted for upper bounds, the optimal convergence rate with respect to the problem parameters is not quite settled.\nLet us discuss existing lower bounds for this setting, along with their shortcomings, in detail. One approach to obtain lower bounds for this setting is to consider the average of carefully handcrafted functions defined on n disjoint sets of variables. This approach was taken by Agarwal and Bottou [1] who derived a lower bound for FSM under the first-order oracle model (see Nemirovsky and Yudin [12]). In this model, optimization algorithms are assumed to access a given function by issuing queries to an external first-order oracle procedure. Upon receiving a query point in the problem domain, the oracle reports the corresponding function value and gradient. The construction used by Agarwal and Bottou consisted of n different quadratic functions which are adversarially determined based on the first-order queries being issued during the optimization process. The resulting bound in this case does not apply to stochastic algorithms, rendering it invalid for current state-of-the-art methods. Another instantiation of this approach was made by\nar X\niv :1\n60 6.\n09 33\n3v 1\n[ m\nat h.\nO C\n] 3\n0 Ju\nn 20\nLan [10] who considered n disjoint copies of a quadratic function proposed by Nesterov in [13, Section 2.1.2]. This technique is based on the assumption that any iterate generated by the optimization algorithm lies in the span of previously acquired gradients. This assumption is rather permissive and is satisfied by many first-order algorithms, e.g., SAG and SAGA [6]. However, the lower bound stated in the paper faces limitations in a few aspects. First, the validity of the derived bound is restricted to d/n iterations. In many datasets, even if d, n are very large, d/n is quite small. Accordingly, the admissible regime of the lower bound is often not very interesting. Secondly, it is not clear how the proposed construction can be expressed as a Regularized Loss Minimization (RLM) problem with linear predictors (see Section 4). This suggests that methods specialized in dual RLM problems, such as SDCA and accelerated proximal SDCA [19], can not be addressed by this bound. Thirdly, at least the formal theorem requires assumptions (such as querying in the span of previous gradients, or sampling from a fixed distribution over the individual functions), which are not met by some state-of-the-art methods, such as coordinate descent methods, SVRG [9] and without-replacements sampling algorithms [15].\nAnother relevant approach in this setting is to model the functional form of the update rules. This approach was taken by Arjevani et al. [3] where new iterates are assumed to be generated by a recurrent application of some fixed linear transformation. Although this method applies to SDCA and produces a tight lower bound of \u2126\u0303((n+ 1/\u03bb) ln(1/ )), its scope is rather limited. In recent work, Arjevani and Shamir [5] considerably generalized parts of this framework by introducing the class of first-order oblivious optimization algorithms, whose step sizes are scheduled regardless of the function under consideration, and deriving tight lower bounds for general smooth convex minimization problems (note that obliviousness rules out, e.g., quasi-Newton methods where gradients obtained at each iteration are multiplied by matrices which strictly depend on the function at hand, see Definition 2 below).\nIn this work, building upon the framework of oblivious algorithms, we take a somewhat more abstract point of view which allows us to easily incorporate coordinate-descent methods, as well as stochastic algorithms. Our framework subsumes the vast majority of optimization methods for machine learning problems, in particular, it applies to SDCA, accelerated proximal SDCA, SDCA without duality [17], SAG, SAGA, SVRG and acceleration schemes [7, 11]), as well as for a large number of methods for smooth convex optimization (i.e., FSM with n = 1), e.g., (stochastic) Gradient descent (GD), Accelerated Gradient Descent (AGD, [13]), the Heavy-Ball method (HB, [14]) and stochastic coordinate descent.\nUnder this structural assumption, we derive lower bounds for FSM (1), according to which the iteration complexity, i.e., the number of iterations required to obtain an -optimal solution in terms of function value, is at least1\n\u2126\u0303(n+ \u221a n(\u03ba\u2212 1) ln(1/ )), (2)\nwhere \u03ba denotes the condition number of F (w) (that is, the smoothness parameter over the strong convexity parameter). To the best of our knowledge, this is the first tight lower bound to address all the algorithms mentioned above. Moreover, our bound is dimension-free and thus apply to settings in machine learning which are not covered in the current literature (e.g., when n is \u2126(d)). We also derive a dimension-free nearly-optimal lower bound for smooth convex optimization of\n\u2126 ( (L(\u03b4 \u2212 2)/ )1/\u03b4 ) , \u03b4 \u2208 (2, 4),\nwhich holds for any oblivious stochastic first-order algorithm. It should be noted that our lower bounds remain valid under any source of randomness which may be introduced into the optimization process (by the\n1Following standard conventions, here tilde notation hides logarithmic factors in the parameters of a given class of optimization problems, e.g., smoothness parameter and number of components.\noracle or by the optimization algorithm). In particular, our bounds hold in cases where the variance of the iterates produced by the algorithm converges to zero, a highly desirable property of optimization algorithms in this setting.\nTwo implications can be readily derived from this lower bound. First, obliviousness forms a real barrier for optimization algorithms, and whereas non-oblivious algorithms may achieve a super-linear convergence rate at latter stages of the optimization process (e.g., quasi-newton), or practically zero error after \u0398(d) iterations (e.g. Center of Gravity method, MCG), oblivious algorithms are bound to linear convergence indefinitely, as demonstrated by Figure 1. We believe that this indicates that a major progress can be made in solving machine learning problems by employing non-oblivious methods for settings where d n. It should be further noted that another major advantage of non-oblivious algorithms is their ability to obtain optimal convergence rates without an explicit specification of the problem parameters (e.g., [5, Section 4.1]).\nSecondly, many practitioners have noticed that oftentimes sampling the individual functions without replacement at each iteration performs better than sampling with replacement (e.g., [18, 15], see also [8, 20]). The fact that our lower bound holds regardless of how the individual functions are sampled and is attained using with-replacement sampling (e.g., accelerated proximal SDCA), implies that, in terms of iteration complexity, one should expect to gain no more than log factors in the problem parameters when using one method over the other (it is noteworthy that when comparing with and without replacement samplings, apart from iteration complexity, other computational resources, such as limited communication in distributed settings [4], may significantly affect the overall runtime)."}, {"heading": "2 Framework", "text": ""}, {"heading": "2.1 Motivation", "text": "Due to difficulties which arise when studying the complexity of general optimization problems under discrete computational models, it is common to analyze the computational hardness of optimization algorithms by modeling the way a given algorithm interacts with the problem instances (without limiting its computational resources). In the seminal work of Nemirovsky and Yudin [12], it is shown that algorithms which access the\nfunction at hand exclusively by querying a first-order oracle require at least \u2126\u0303 ( min { d, \u221a \u03ba } ln(1/ ) ) , \u00b5 > 0 (3)\n\u2126\u0303(min{d ln(1/ ), \u221a L/ }), \u00b5 = 0\noracle calls to obtain an -optimal solution (note that, here and throughout this section we refer to FSM problems with n = 1). This lower bound is tight and its dimension-free part is attained by Nesterov\u2019s well-known accelerated gradient descent, and by MCG otherwise. The fact that this approach is based on information considerations alone is very appealing and renders it valid for any first-order algorithm. However, discarding the resources needed for executing a given algorithm, in particular the per-iteration cost (in time and space), the complexity boundaries drawn by this approach are too crude from a computational point of view. Indeed, the per-iteration cost of MCG, the only method known with oracle complexity of O(d ln(1/ )), is excessively high, rendering it prohibitive for high-dimensional problems.\nWe are thus led into the question of how well can a given optimization algorithm perform assuming that its per-iteration cost is constrained? Arjevani et al. [3, 5] adopted a more structural approach where instead of modeling how information regarding the function at hand is being collected, one models the update rules according to which iterates are being generated. Concretely, they proposed the framework of p-CLI optimization algorithms where, roughly speaking, new iterates are assumed to form linear combinations of the previous p iterates and gradients, and the coefficients of these linear combinations are assumed to be either stationary (i.e., remain fixed throughout the optimization process) or oblivious. Based on this structural assumption, they showed that the iteration complexity of minimizing smooth and strongly convex functions is \u2126\u0303( \u221a \u03ba ln(1/ )). The fact that this lower bound is stronger than (3), in the sense that it does not depend on the dimension, confirms that controlling the functional form of the update rules allows one to derive tighter lower bounds. The framework of p-CLIs forms the nucleus of our formulation below."}, {"heading": "2.2 Definitions", "text": "When considering lower bounds one must be very precise as to the scope of optimization algorithms to which they apply. Below, we give formal definitions for oblivious stochastic CLI optimization algorithms and iteration complexity (which serves as a crude proxy for their computational complexity).\nDefinition 1 (Class of Optimization Problems). A class of optimization problems is an ordered triple (F , I,O), where F is a family of functions defined over some linear space designated by domF , I is the side-information given prior to the optimization process and Of : domF \u00d7\u0398\u2192 domF is a suitable oracle parametrized by some parameters set \u0398, i.e., an external procedure which upon receiving x \u2208 domF and \u03b8 \u2208 \u0398, returns some Of (\u03b8) \u2208 dom(F).\nFor example, in FSM, F contains functions as defined in (1), the side-information contains the smooth parameter L, the strong convexity parameter \u00b5 and the number of components n (although it carries a crucial effect on the iteration complexity, e.g., [5], in this work, we shall ignore the side-information and assume that all the parameters of the class are given). We shall assume that both first-order and coordinate-descent oracles (see 10,11 below) are allowed to be used during the optimization process. Formally, this is done by introducing an additional parameter which indicates which oracle is being addressed. This added degree of freedom does not violate our lower bounds.\nWe now turn to rigorously define CLI optimization algorithms. Note that, compared with the definition of first-order p-CLIs provided in [5], here, in order to handle coordinate-descent and first-order oracles in a unified manner, we base our formulation on general oracle procedures.\nDefinition 2 (CLI). An optimization algorithm is called a Canonical Linear Iterative (CLI) optimization algorithm over a class of optimization problems (F , I,O), if given an instance f \u2208 F and initialization points {w(0)i }i\u2208J \u2286 dom(F), where J is some index set, it operates by iteratively generating points such that for any i \u2208 J ,\nw (k+1) i \u2208 \u2211 j\u2208J Of ( w (k) j ; \u03b8 (k) ij ) , k = 0, 1, . . . (4)\nholds, where \u03b8(k)ij \u2208 \u0398 are parameters chosen, stochastically or deterministically, by the algorithm, possibly depending on the side-information. If the parameters do not depend on previously acquired oracle answers, we say that the given algorithm is oblivious. Lastly, algorithms with |J | \u2264 p, for some p \u2208 N, are denoted by p-CLI.\nNote that assigning different weights to different terms in (4) can be done through \u03b8(k)ij \u2208 \u0398 (e.g., oracle 10 below). This allows a succinct definition for obliviosity. Lastly, we define iteration complexity.\nDefinition 3 (Iteration Complexity). The iteration complexity of a given CLI w.r.t. a given problem class (F , I,O) is defined to be the minimal number of iterations K such that\nE[f(w(k)1 )\u2212 min w\u2208domF f(w)] < , \u2200f \u2208 F , k \u2265 K\nwhere the expectation is taken over all the randomness introduced into the optimization process (choosing w\n(k) 1 merely serves as a convention and is not necessary for our bounds to hold)."}, {"heading": "2.3 Proof Technique - Deriving Lower Bounds via Approximation Theory", "text": "Consider the following parametrized class of L-smooth and \u00b5-strongly convex optimization problems,\nmin x\u2208R\nf\u03b7(x) := \u03b7w2\n2 \u2212 w, \u03b7 \u2208 [\u00b5,L]. (5)\nClearly, the minimizer of f\u03b7 are w\u2217(\u03b7) := 1/\u03b7, with norm bounded by 1/\u00b5. For simplicity, we will consider a special case, namely, vanilla gradient descent (GD) with step size 1/L, which produces new iterates as follows\nw(k+1)(\u03b7) = w(k)(\u03b7)\u2212 1 L f \u2032\u03b7(w\n(k)(\u03b7)) = (\n1\u2212 \u03b7 L\n) w(k)(\u03b7) + 1\nL .\nSetting the initialization point to be w(0)(\u03b7) = 0, we derive an explicit expression for w(k)(\u03b7):\nw(k)(\u03b7) = 1\nL k\u22121\u2211 i=0 (\u22121)i ( k i+ 1 ) (\u03b7/L)i. (6)\nIt turns our that each w(k)(\u03b7) forms a univariate polynomial whose degree is at most k. Furthermore, since f\u03b7(w) are L-smooth \u00b5-strongly convex for any \u03b7 \u2208 [\u00b5,L], standard convergence analysis for GD (e.g., [13], Theorem 2.1.14) guarantees that |w(k)(\u03b7)\u2212 w\u2217(\u03b7)| \u2264 (1\u2212 2/(1 + \u03ba)) k 2 |w\u2217(\u03b7)|, where \u03ba denotes the condition number. Substituting Equation (6) for w(k)(\u03b7) yields\nmax \u03b7\u2208[\u00b5,L] \u2223\u2223\u2223\u2223\u2223 1L k\u22121\u2211 i=0 (\u22121)i ( k i+ 1 ) (\u03b7/L)i \u2212 1/\u03b7 \u2223\u2223\u2223\u2223\u2223 \u2264 1\u00b5 ( 1\u2212 2 1 + \u03ba ) k 2 .\nThus, we see that the faster the convergence rate of a given optimization algorithm is, the better the induced sequence of polynomials (w(k)(\u03b7))k\u22650 approximate 1/\u03b7 w.r.t. the maximum norm \u2016 \u00b7 \u2016L\u221e([\u00b5,L]) over [\u00b5,L]. In Fig. 2, we compare the first 4 polynomials induced by GD and AGD. Not surprisingly, AGD polynomials approximates 1/\u03b7 better than those of GD.\nNow, one may ask, assuming that iterates of a given optimization algorithm A for (5) can be expressed as polynomials sk(\u03b7) whose degree does not exceed the iteration number, just how fast can these iterates converge to the minimizer? Since the convergence rate is bounded from below by \u2016sk(\u03b7)\u2212 1/\u03b7\u2016L\u221e([\u00b5,L]), we may address the following question instead:\nmin s(\u03b7)\u2208Pk \u2016s(\u03b7)\u2212 1/\u03b7\u2016L\u221e([\u00b5,L]), (7)\nwhere Pk denotes the set of univariate polynomials whose degree does not exceed k. Problem (7) and other related settings are main topics of study in approximation theory. Accordingly, our technique for proving lower bounds makes an extensive use of tools borrowed from this area. Specifically, in a paper from 1899 [21] Chebyshev showed that\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 \u2212 c \u2225\u2225\u2225\u2225 L\u221e([\u22121,1]) \u2265 (c\u2212 \u221a c2 \u2212 1)k c2 \u2212 1 , c > 1, (8)\nby which we derive the following theorem (see Appendix A.1 for a detailed proof). Theorem 1. The number of iterations required by A to get an -optimal solution is \u2126\u0303( \u221a \u03ba ln(1/ )).\nIn the following sections, we apply oblivious CLI on various parameterized optimization problems so that the resulting iterates are polynomials in the problem parameters. We then apply arguments similar to the above\nA similar reduction, from optimization problems to approximation problems, was used before in a few contexts to analyze the iteration complexity of deterministic CLIs (e.g., [5, Section 3], see also Conjugate Gradient convergence analysis [14]). But, what if we allow random algorithms? should we expect the same iteration complexity? To answer this, we use Yao\u2019s minimax principle according to which the performance of a given stochastic optimization algorithm w.r.t. to its worst input are bounded from below by the performance of the best deterministic algorithm w.r.t. distributions over the input space. Thus, following a similar reduction one can show that the convergence rate of stochastic algorithms is bounded from below by\nmin s(\u03b7)\u2208Pk \u222b L \u00b5 |s(\u03b7)\u2212 1/\u03b7| 1 L\u2212 \u00b5 d\u03b7. (9)\nThat is, a lower bound for the stochastic case can be attained by considering an approximation problem w.r.t. weighted L1 with the uniform distribution over [\u00b5,L]. Other approximation problems considered in this work involve L2-norm and different distributions. We provide a schematic description of our proof technique in Scheme 2.1.\nSCHEME 2.1 FROM OPTIMIZATION PROBLEMS TO APPROXIMATION PROBLEMS GIVEN A CLASS OF FUNCTIONS F , A SUITABLE ORACLE O AND A SEQUENCE OF SETS OF FUNCTION Sk OVER SOME PARAMETERS SET H . CHOOSE A SUBSET OF FUNCTIONS {f\u03b7 \u2208 F|\u03b7 \u2208 H}, S.T. wk(\u03b7) \u2208 Sk . COMPUTE THE MINIMIZER w\u2217(\u03b7) FOR ANY f\u03b7 BOUND FROM BELOW THE BEST APPROXIMATION FOR w\u2217(\u03b7) W.R.T. Sk\nAND A NORM \u2016 \u00b7 \u2016, I.E., min{\u2016s(\u03b7)\u2212w\u2217(\u03b7)\u2016 | s(\u03b7) \u2208 Sk}"}, {"heading": "3 Lower Bound for Finite Sums Minimization Methods", "text": "Having described our analytic approach, we now turn to present some concrete applications, starting with iteration complexity lower bounds in the context of FSM problems (1). In what follows, we derive a lower bound on the iteration complexity of oblivious (possibly stochastic) CLI algorithms equipped with first-order and coordinate-descent oracles for FSM. Strictly speaking, we focus on optimization algorithms equipped with both generalized first order oracle,\nO(w;A,B,C, j) = A\u2207fj(w) +Bw + C, A,B,C \u2208 Rd\u00d7d, j \u2208 [n], (10)\nand steepest coordinate-descent oracle\nO(w; i, j) = w + t\u2217ei, t\u2217 \u2208 argmin t\u2208R fj(w1, . . . , wi\u22121, wi + t, wi+1, . . . , wd), j \u2208 [n], (11)\nwhere ei denotes the i\u2019th unit vector. We remark that coordinate-descent steps w.r.t. partial gradients can be implemented using (10) by setting A to be some principal minor of the unit matrix.It should be further noted that our results below hold for scenarios where the optimization algorithm is free to call a different oracle at different iterations.\nFirst, we sketch the proof of the lower bound for deterministic oblivious CLIs. Following Scheme 2.1, we restrict our attention to a parameterized subset of problems. We assume2 d > 1 and denote byHFSM the set of all (\u03b71, . . . , \u03b7n) \u2208 Rn such that all the entries equal \u2212(L\u2212 \u00b5)/2, except for some j \u2208 [n], for which \u03b7j \u2208 [\u2212(L\u2212 \u00b5)/2, (L\u2212 \u00b5)/2]. Now, given \u03b7 := (\u03b71, . . . , \u03b7n) \u2208 HFSM we define\nF\u03b7(w) := 1\nn n\u2211 i=1 ( 1 2 w>Q\u03b7iw \u2212 q ) ,where (12)\nQ\u03b7i :=  L+\u00b5 2 \u03b7i \u03b7i L+\u00b5 2 \u00b5 . . .\n\u00b5\n , q :=  R\u00b5\u221a 2 R\u00b5\u221a 2\n0 ... 0\n .\nIt is easy to verify that the minimizers of (12) are\nw\u2217(\u03b7) =  R\u00b5\u221a 2 ( L+\u00b5 2 + 1 n \u2211n i=1 \u03b7i ) , R\u00b5\u221a 2 ( L+\u00b5 2 + 1 n \u2211n i=1 \u03b7i ) , 0, . . . , 0 > . (13)\nWe would like to show that the coordinates of the iterates of deterministic oblivious CLIs, which minimize F\u03b7 using first-order and coordinate-descent oracles, form multivariate polynomials in \u03b7 of total degrees (the maximal sum of powers over all the terms) which does not exceed the iteration number. Indeed, if the coordinates of w(k)i (\u03b7) are multivariate polynomial in \u03b7 of total degree at most k, then the coordinates of the vectors returned by both oracles\nFirst-order oracle: O(w(k)j ;A,B,C, j) = A(Q\u03b7jw (k) i \u2212 q) +Bw (k) i + C, (14) Coordinate-descent oracle: O(w(k)j ; i, j) = ( I \u2212 (1/(Q\u03b7j )ii)ei(Q\u03b7j )i,\u2217 ) w (k) i \u2212 qi/(Q\u03b7j )iiei,\nare multivariate polynomials of total degree of at most k + 1, as all the parameters (A,B,C, i and j) do not depend on \u03b7 (due to obliviosity) and the rest of the terms (Q\u03b7j ,q, I, 1/(Q\u03b7j )ii, (Q\u03b7j )i,\u2217, ei and qi) are either linear in \u03b7j or constants. Now, since the next iterates are generated simply by summing up all the oracle answers, they also form multivariate polynomials of total degree of at most k + 1. Thus, denoting the first coordinate of w(k)1 (\u03b7) by s(\u03b7) and using Inequality (8), we get the following bound\nmax \u03b7\u2208HFSM\n\u2016w(k)1 (\u03b7)\u2212w \u2217(\u03b7)\u2016 \u2265 \u2225\u2225\u2225\u2225\u2225\u2225s(\u03b7)\u2212 R\u00b5\u221a2(L+\u00b52 + 1n\u2211ni=1 \u03b7i) \u2225\u2225\u2225\u2225\u2225\u2225 L\u221e([\u00b5,L])\n(15)\n\u2265 \u2126(1)\n \u221a\n\u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1 k/n, (16) where \u2126(1) designates a constant which does not depend on k (but may depend on the problem parameters). Lastly, this implies that for any deterministic oblivious CLI and any iteration number, there exists some\n2Clearly, in order to derive a lower bound for coordinate-descent algorithms, we must assume d > 1. If only a first-order oracle is allowed, then the same lower bound as in Theorem 2 can be derived for d = 1.\n\u03b7 \u2208 HFSM such that the convergence rate of the algorithm, when applied on F\u03b7, is bounded from below by Inequality (16). We note that, as opposed to other related lower bounds, e.g., [10], our proof is nonconstructive. As discussed in subsection 2.3, this type of analysis can be extended to stochastic algorithms by considering (15) w.r.t. other norms such as weighted L1-norm. We now arrive at the following theorem whose proof, including the corresponding logarithmic factors and constants, can be found in Appendix A.2.\nTheorem 2. The iteration complexity of oblivious (possibly stochastic) CLIs for FSM (1) equipped with first-order (10) and coordinate-descent oracles (11), is bounded from below by\n\u2126\u0303(n+ \u221a n(\u03ba\u2212 1) ln(1/ )).\nThe lower bound stated in Theorem 2 is tight and is attained by, e.g., SAG combined with an acceleration scheme (e.g., [11]). Moreover, as mentioned earlier, our lower bound does not depend on the problem dimension (or equivalently, holds for any number of iterations, regardless of d and n), and covers coordinate descent methods with stochastic or deterministic coordinate schedule (in the special case where n = 1, this gives a lower bound for minimizing smooth and strongly convex functions by performing steepest coordinate descent steps). Also, our bound implies that using mini-batches for tackling FSM does not reduce the overall iteration complexity. Lastly, it is noteworthy that the n term in the lower bound above holds for any algorithm accompanied with an incremental oracle, which grants access to at most one individual function each time.\nWe also derive a nearly-optimal lower bound for smooth non-strongly convex functions for the more restricted setting of n = 1 and first-order oracle. The parameterized subset of functions we use (see Scheme 2.1) is g\u03b7(x) := \u03b72 \u2016x\u2016\n2 \u2212 R\u03b7e>1 x, \u03b7 \u2208 (0, L]. The corresponding minimizer (as a function of \u03b7) is x\u2217(\u03b7) = Re1, and in this case we seek to approximate it w.r.t. L2-norm using k-degree univariate polynomials whose constant term vanishes. The resulting bound is dimension-free and improves upon other bounds for this setting (e.g. [5]) in that it applies to deterministic algorithms, as well as to stochastic algorithms (see A.3 for proof).\nTheorem 3. The iteration complexity of any oblivious (possibly stochastic) CLI forL-smooth convex functions equipped with a first-order oracle, is bounded from below by\n\u2126 ( (L(\u03b4 \u2212 2)/ )1/\u03b4 ) , \u03b4 \u2208 (2, 4)."}, {"heading": "4 Lower Bound for Dual Regularized Loss Minimization with Linear Predictors", "text": "The form of functions (12) discussed in the previous section does not readily adapt to general RLM problems with linear predictors, i.e.,\nmin w\u2208Rd\nP (w) := 1\nn n\u2211 i=1 \u03c6i(\u3008xi,w\u3009) + \u03bb 2 \u2016w\u20162 , (17)\nwhere the loss functions \u03c6i are L-smooth and convex, the samples x1, . . . ,xn are d-dimensional vectors in Rd and \u03bb is some positive constant. Thus, dual methods which exploit the added structure of this setting through the dual problem [18],\nmin \u03b1\u2208Rn\nD(\u03b1) = 1\nn n\u2211 i=1 \u03c6\u2217i (\u2212\u03b1i) + \u03bb 2 \u2225\u2225\u2225\u2225\u2225 1\u03bbn n\u2211 i=1 xi\u03b1i \u2225\u2225\u2225\u2225\u2225 2 , (18)\nsuch as SDCA and accelerated proximal SDCA, are not covered by Theorem 2. Accordingly, in this section, we address the iteration complexity of oblivious (possibly stochastic) CLI algorithms equipped with dual RLM oracles:\nO(\u03b1; t, j) = \u03b1+ t\u2207jD(\u03b1)ej , t \u2208 R, j \u2208 [n], (19) O(\u03b1; j) = \u03b1+ t\u2217ej , t\u2217 = argmin\nt\u2208R D(\u03b11, . . . , \u03b1j\u22121, \u03b1j + t, \u03b1j+1, . . . , \u03b1d), j \u2208 [n],\nFollowing Scheme 2.1, we first describe the relevant parametrized subset of RLM problems. For the sake of simplicity, we assume that n is even (the proof for odd n holds mutatis mutandis). We denote by HRLM the set of all (\u03c81, . . . , \u03c8n/2) \u2208 Rn/2 such that all entries are 0, except for some j \u2208 [n/2], for which \u03c8j \u2208 [\u2212\u03c0/2, \u03c0/2]. Now, given \u03c8 \u2208 HRLM, we set P\u03c8 (defined in 17) as follows\n\u03c6i(w) = 1\n2 (w + 1)2, x\u03c8,i = { cos(\u03c8(i+1)/2)ei + sin(\u03c8(i+1)/2)ei+1 i is odd ei o.w. .\nWe state below the corresponding lower bound, whose proof, including logarithmic factors and constants, can be found in Appendix A.4.\nTheorem 4. The iteration complexity of oblivious (possibly stochastic) CLIs for RLM (17) equipped with dual RLM oracles (19) is bounded from below by\n\u2126\u0303(n+ \u221a nL/\u03bb ln(1/ )).\nThis bound is tight w.r.t. the class of oblivious CLIs and is attained by accelerated proximal SDCA. As mentioned earlier, a tighter lower bound of \u2126\u0303((n+ 1/\u03bb) ln(1/ )) is known for SDCA [3], suggesting that a tighter bound might hold for the more restricted set of stationary CLIs (for which the oracle parameters remain fixed throughout the optimization process)."}, {"heading": "A Proofs", "text": ""}, {"heading": "A.1 Proof of Theorem 1", "text": "Proof According to the way A generates iterates, we have\n|x(k)(\u03b7)\u2212 x\u2217(\u03b7)| = |sk(\u03b7)\u2212 1/\u03b7|, \u03b7 \u2208 [\u00b5,L]\nfor some polynomial sk(\u03b7) of degree at most k. By Lemma 6, we have\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 \u2225\u2225\u2225\u2225 L\u221e([\u00b5,L]) \u2265 L\u2212 \u00b5 2L\u00b5 (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )k ,\nwhere \u03ba = L/\u00b5. Thus,\n|x(k)(\u03b7)\u2212 x\u2217(\u03b7)| \u2265 min s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 \u2225\u2225\u2225\u2225 L\u221e([\u00b5,L]) \u2265 L\u2212 \u00b5 2L\u00b5 (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )k \u2265 |x\u2217(\u03b7)|L\u2212 \u00b5 2L (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )k .\nNow, since f\u03b7 is \u00b5-strongly convex, we have,\nf(x(k)(\u03b7))\u2212 f(x\u2217(\u03b7))| \u2265 \u00b5 2 |x(k)(\u03b7)\u2212 x\u2217(\u03b7)|2\n\u2265 \u00b5 2\n( |x\u2217(\u03b7)|L\u2212 \u00b5\n2L (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )k)2\n= \u00b5\n2\n( |x\u2217(\u03b7)|L\u2212 \u00b5\n2L )2(\u221a\u03ba\u2212 1\u221a \u03ba+ 1 )2k .\nHence, by Lemma 12, the minimal number of iterations required to get an -optimal solution is at least\n1\n4\n\u221a \u03ba\u2212 1 ( ln \u00b5\n2 + 2 ln\n( |x\u2217(\u03b7)|L\u2212 \u00b5\n2L\n) + ln(1/ ) ) ."}, {"heading": "A.2 Proof of Theorem 2 - Finite Sums", "text": "When dealing with multivariate polynomials it is convenient to define multi-indices i = (i1, . . . , in) \u2208 Nn0 , where Nn0 is the set of all n-tuples of non-negative integers. In addition, with a slight abuse of notation, we define\nPnk := span { \u03b7i | i \u2208 Nn0 , |i| \u2264 k } , (20)\nwhere we put \u03b7i = \u03b7i11 \u00b7 \u00b7 \u00b7 \u03b7inn and |i| = i1 + \u00b7 \u00b7 \u00b7+ in. In words, Pnk is the set of all multivariate polynomials over n indeterminates whose total degree (the maximal sum of the degrees over all terms) is less than or equal to k. Lastly, given s(\u03b7) \u2208 Pnk we define\nsi(\u03b7i) := s \u2212L\u2212 \u00b5 2 , . . . ,\u2212L\u2212 \u00b5 2\n, \u03b7i\ufe38\ufe37\ufe37\ufe38 i\u2019th entry ,\u2212L\u2212 \u00b5 2 , . . . ,\u2212L\u2212 \u00b5 2\n .\nThis notation will come in handy in the main proof. The lemma below describes the functional form assumed by iterates produced by oblivious CLIs.\nLemma 1. When applied on (12) with suitable first-order and coordinate-descent oracles (as defined in 14), the coordinates of iterates produced by oblivious stochastic CLIs form multivariate polynomials in \u03b7 with random real coefficients whose total degree does not exceed the iteration number.\nProof Let A be a oblivious stochastic CLI, and suppose we apply A on the class of problems (12) parameterized by \u03b7, using both first-order and coordinate-descent oracles as defined in 14. We use mathematical induction to show that for any k = 0, 1, . . . , the coordinate of the k\u2019th iterate produced by such process can be expressed as a distribution over multivariate polynomials in \u03b7 of degree at most k.\nAs the first iterate w(0)i is allowed to depend only on L, \u00b5 and n, the base case is trivial. That is, the coordinates of w(0)i form distributions over R = Pn0 which do not depend on \u03b7.\nFor the inductive step, assume that any coordinate of w(k)i (\u03b7) can be expressed as a distribution over Pnk . It is easy to see that for any w(k)i (\u03b7), the answers of both oracles,\nFirst-order oracle: O(w(k)i ;A,B,C, j) = A(Q\u03b7jw (k) i \u2212 q) +Bw (k) i + C, Coordinate-descent oracle: O(w(k)i ; i, j) = ( I \u2212 (1/(Q\u03b7j )ii)ei(Q\u03b7j )i,\u2217 ) w (k) i \u2212 qi/(Q\u03b7j )iiei,\nform a distribution over Pnk+1, as all the random quantities involved in the expressions (A,B,C, i and j) do not depend on \u03b71, . . . , \u03b7n (due to obliviosity) and the rest of the terms (I,Q\u03b7j , 1/(Q\u03b7j )ii, (Q\u03b7j )i,\u2217, ei, qi and q) are either linear in \u03b7j or constants. Lastly, w (k+1) i are computed by simply summing up all the oracle answers, and as such, form again distributions over Pnk+1.\nProof [Theorem 2] Let A be a oblivious stochastic CLI. By Lemma 1 the first coordinate of w(k)1 (\u03b7) (the point returned by the algorithm at the k\u2019th iteration) when applied on the class of problems (12) distributes according to some distributionD over Pnk . Thus, by Yao principle, since each polynomial in (Pnk )d represents a single deterministic algorithm, we have\nmax \u03b7\u2208H E w (k) 1 (\u03b7)\u223cD\n\u2016w(k)1 (\u03b7)\u2212w \u2217(\u03b7)\u2016 \u2265 min s(\u03b7)\u2208(Pnk )d E\u03b7\u223cU(H)\u2016s(\u03b7)\u2212w\u2217(\u03b7)\u2016 (21)\nwhere U(H) denotes a distribution over H which corresponds to first drawing j \u223c U([n]) at random, and then setting the coordinates of \u03b7 as follows{\n\u03b7i \u223c U([\u2212(L\u2212 \u00b5)/2, (L\u2212 \u00b5)/2] i = j \u03b7i = \u2212L\u2212\u00b52 , i 6= j . (22)\nFurthermore, it is easy to verify that the corresponding minimizers of (12) are\nw\u2217(\u03b71, . . . , \u03b7n) =\n( 1\nn n\u2211 i=1 Q\u03b7i\n)\u22121 q =  R\u00b5\u221a 2 ( L+\u00b5 2 + 1 n \u2211n i=1 \u03b7i ) , R\u00b5\u221a 2 ( L+\u00b5 2 + 1 n \u2211n i=1 \u03b7i ) , 0, . . . , 0 > .\n(23)\nWe now have,\nmin s(\u03b7)\u2208(Pnk )d E\u03b7\u223cU(H)\u2016s(\u03b7)\u2212w\u2217(\u03b7)\u2016 = min s(\u03b7)\u2208(Pnk )d Ei\u223cU([n])E\u03b7i\u223cU([\u2212L\u2212\u00b52 ,L\u2212\u00b52 ])\u2016s(\u03b7)\u2212w \u2217(\u03b7)\u2016\n\u2265 1 n min s(\u03b7)\u2208Pnk n\u2211 j=1 E \u03b7j\u223cU([\u2212L\u2212\u00b52 , L\u2212\u00b5 2 ]) \u2223\u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 R\u00b5\u221a2( 1n\u2211ni=1 \u03b7i) + L+\u00b52 ) \u2223\u2223\u2223\u2223\u2223\n\u2265 R\u00b5\u221a 2 min s(\u03b7)\u2208Pnk n\u2211 j=1 E \u03b7j\u223cU([\u2212L\u2212\u00b52 , L\u2212\u00b5 2 ]) \u2223\u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03b7j \u2212 (n\u2212 1)L\u2212\u00b52 + nL+\u00b52 \u2223\u2223\u2223\u2223\u2223\n\u2265 R\u00b5\u221a 2 min s(\u03b7)\u2208Pnk n\u2211 j=1 \u222b L\u2212\u00b5 2 \u2212L\u2212\u00b5 2 \u2223\u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03b7j \u2212 (n\u2212 1)L\u2212\u00b52 + nL+\u00b52 \u2223\u2223\u2223\u2223\u2223 1L\u2212 \u00b5d\u03b7j\n\u2265 R\u00b5\u221a 2(L\u2212 \u00b5) min s(\u03b7)\u2208Pnk n\u2211 j=1 \u222b L\u2212\u00b5 2 \u2212L\u2212\u00b5 2 \u2223\u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03b7j \u2212 (n\u2212 1)L\u2212\u00b52 + nL+\u00b52 \u2223\u2223\u2223\u2223\u2223 d\u03b7j\n(24)\nwhere the first inequality follows by focusing on the first coordinate of s(\u03b7) \u2212 w\u2217(\u03b7). Now, set \u03b1 = \u2212(n\u2212 1)L\u2212\u00b52 + n\nL+\u00b5 2 and note that\u221a\n2\u03b1+ L\u2212 \u00b5 2\u03b1+ \u00b5\u2212 L = \u221a\u221a\u221a\u221a2(\u2212(n\u2212 1)L\u2212\u00b52 + nL+\u00b52 ) + L\u2212 \u00b5 2(\u2212(n\u2212 1)L\u2212\u00b52 + n L+\u00b5 2 ) + \u00b5\u2212 L = \u221a \u03ba\u2212 1 n + 1.\nThus, by Lemma 8 (using the same value for \u03b1 and noting that \u03b1 > (L\u2212 \u00b5)/2) yields\n\u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2 \u2223\u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03b7j \u2212 (n\u2212 1)L\u2212\u00b52 + nL+\u00b52 \u2223\u2223\u2223\u2223\u2223 d\u03b7j \u2265  \u221a \u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1 kj . where kj denotes the degree of sj(\u03b7j). Plugging in this into Inequality (24) we get\nmax \u03b7\u2208H E w (k) 1 (\u03b7)\u223cD\n\u2016w(k)1 (\u03b7)\u2212w \u2217(\u03b7)\u2016 \u2265 nR\u00b5\u221a\n2(L\u2212 \u00b5) min\ns(\u03b7)\u2208Pnk\n1\nn n\u2211 j=1\n \u221a\n\u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1 kj . Since u 7\u2192 \u03c1u is a decreasing and convex function for any 1 > \u03c1 > 0, we have\nnR\u00b5\u221a 2(L\u2212 \u00b5) min s(\u03b7)\u2208Pnk 1 n n\u2211 j=1\n \u221a\n\u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1\nkj \u2265 nR\u00b5\u221a 2(L\u2212 \u00b5) min s(\u03b7)\u2208Pnk  \u221a \u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1  1 n \u2211n j=1 kj\n\u2265 nR\u00b5\u221a 2(L\u2212 \u00b5)\n \u221a\n\u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1\nk/n\nwhere the last inequality is due to the fact that s(\u03b7) \u2208 Pnk which implies that \u2211n j=1 kj \u2264 k. Finally, we have,\nmax \u03b7\u2208H E w (k) 1 (\u03b7)\u223cD [F\u03b7(w (k) 1 (\u03b7))\u2212 F\u03b7(w \u2217(\u03b7))] \u2265 max \u03b7\u2208H E w (k) 1 (\u03b7)\u223cD\n\u00b5 2 \u2016w(k)1 (\u03b7)\u2212w \u2217(\u03b7)\u20162\n\u2265 \u00b5 2  nR\u00b5\u221a 2(L\u2212 \u00b5)  \u221a\n\u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1\nk/n  2\n= \u00b5\n2\n( nR\u00b5\u221a\n2(L\u2212 \u00b5)\n)2 \u221a\n\u03ba\u22121 n + 1\u2212 1\u221a \u03ba\u22121 n + 1 + 1\n2k/n\nwhere the first inequality follows by the \u00b5-strong convexity of F\u03b7 and the second inequality follows by Jensen inequality. Using Lemma 12, we get that the iteration complexity of A is at least\n1\n4\n(\u221a n(\u03ba\u2212 1) ) (ln \u00b5\n2 + 2 ln nR\u00b5\u221a 2(L\u2212 \u00b5) + ln(1/ )).\nThis, together with Theorem 5 below, concludes the proof.\nWe bound from below the number of iterations required to obtain a non-trivial accuracy.\nLemma 2. Let j \u2208 [n], let \u03b7j,1 \u2208 H be a parameters vector whose all coordinates are \u2212 L\u2212\u00b5 2 and let \u03b7j,2 \u2208 H be a parameters vector whose all coordinates are \u2212 L\u2212\u00b5 2 , except for the j\u2019th coordinate which we set to be L\u2212\u00b52 . If \u03ba > 3, then\n\u2016w\u2217(\u03b71)\u2212w\u2217(\u03b72)\u2016 \u2265 2R\nn+ 2 .\nProof By Equation (13) we have\n\u2016w \u2217 (\u03b71)\u2212w \u2217 (\u03b72) \u2016 = \u221a 2 \u2223\u2223\u2223\u2223\u2223\u2223 R\u00b5\u221a2(L+\u00b52 + 1n\u2211ni=1(\u03b71)i) \u2212 R\u00b5 \u221a 2 ( L+\u00b5 2 + 1 n \u2211n i=1(\u03b72)i ) \u2223\u2223\u2223\u2223\u2223\u2223\n= R\u00b5 \u2223\u2223\u2223\u2223\u2223 1L+\u00b5 2 \u2212 L\u2212\u00b5 2 \u2212 1 L+\u00b5 2 \u2212 (n\u22121)(L\u2212\u00b5) 2n + L\u2212\u00b5 2n \u2223\u2223\u2223\u2223\u2223 = R\u00b5 \u2223\u2223\u2223\u2223\u2223\u2223 L+\u00b5 2 \u2212 (n\u22121)(L\u2212\u00b5) 2n + L\u2212\u00b5 2n \u2212 L+\u00b5 2 + L\u2212\u00b5 2(\nL+\u00b5 2 \u2212 L\u2212\u00b5 2 )( L+\u00b5 2 \u2212 (n\u22121)(L\u2212\u00b5) 2n + L\u2212\u00b5 2n ) \u2223\u2223\u2223\u2223\u2223\u2223\n= R \u2223\u2223\u2223\u2223\u2223\u2212 (n\u22121)(L\u2212\u00b5) n + L\u2212\u00b5 n + L\u2212 \u00b5\nL+ \u00b5\u2212 (n\u22121)(L\u2212\u00b5)n + L\u2212\u00b5 n \u2223\u2223\u2223\u2223\u2223 = 2R\n\u2223\u2223\u2223\u2223\u2223 L\u2212\u00b5nL+ \u00b5\u2212 (n\u22121)(L\u2212\u00b5)n + L\u2212\u00b5n \u2223\u2223\u2223\u2223\u2223\n= 2R \u2223\u2223\u2223\u2223\u2223 1n\u03ba+1\u03ba\u22121 \u2212 (n\u2212 1) + 1 \u2223\u2223\u2223\u2223\u2223\n= 2R \u2223\u2223\u2223\u2223\u2223 1n\u03ba+1\u03ba\u22121 \u2212 n+ 2 \u2223\u2223\u2223\u2223\u2223\n\u2265 2R n+ 2 ,\nwhere the last inequality follows from \u03ba > 3.\nTheorem 5. The iteration complexity of any stochastic optimization algorithm (not necessarily CLI) which gathers information on F\u03b7 (with \u03ba > 3) only by means of incremental oracles, i.e., oracles which upon receiving query return an answer which depends on not more than one individual function, is at least n.\nProof Let A be a stochastic optimization algorithm. According to Yao\u2019s principle, we can bound from below the -optimality ofA after k < n iterations by estimating the -optimality of any deterministic algorithm w.r.t. to distribution D(H) overH defined by: draw j \u2208 [n] and set \u03b7 to be \u03b7j,1 or \u03b7j,2 as defined in Lemma 2 w.p. 1/2. Then,\nmax {\u03b7j,i|j\u2208[n],i\u2208[2]}\nEA[F\u03b7j,i(w (k) ( \u03b7j,i ) )\u2212 F\u03b7j,i(w \u2217 (\u03b7j,i))] \u2265 min\ndeterministic algorithms E \u03b7\u223cD(H)[F\u03b7j,i(w\n(k) ( \u03b7j,i ) )\u2212 F\u03b7j,i(w \u2217 (\u03b7j,i))] \u2265 min\ndeterministic algorithms E \u03b7\u223cD(H)\n\u00b5 2 \u2016w(k)\n( \u03b7j,i ) )\u2212w\u2217 ( \u03b7j,i ) \u20162\n\u2265 \u00b5 2 min deterministic algorithms\n( E \u03b7\u223cD(H)\u2016w (k) ( \u03b7j,i ) )\u2212w\u2217 ( \u03b7j,i ) \u2016 )2\n\u2265 \u00b5 2\n( R\n2n(n+ 2)\n)2 ,\nwhere the last inequality follows from Lemma 2. Thus, for sufficiently small , one must perform at least n iterations in order to obtain an -optimal solution."}, {"heading": "A.3 Proof of Theorem 3 - Smooth Functions", "text": "The following notation\nPk := {p \u2208 Pk|p(0) = 0} (25)\nwill come in handy in subsequent proofs.\nLemma 3. When applied on\ng\u03b7(x) := \u03b7 2 \u2016x\u20162 \u2212R\u03b7e>1 x, \u03b7 \u2208 (0, L] (26)\nwith a first-order oracle (as defined in 10 with n = 1), the coordinates of iterates produced by oblivious stochastic CLIs whose is initialization iterate is x(0)i = 0 form polynomials in \u03b7 with random real coefficients which vanishes at \u03b7 = 0 and whose degree does not exceed the iteration number.\nProof Let A be a oblivious stochastic CLI, and suppose we apply A on the class of problems (26) parameterized by \u03b7, using a first-order. We use mathematical induction to show that for any k = 0, 1, . . . , the coordinate of the k\u2019th iterate produced by such process can be expressed as a distribution over Pk.\nAs the first iterate x(0)i is assumed to be zero, the base case is trivial. For the inductive step, assume that any coordinate of x(k)i can be expressed as a distribution over Pk. It is easy to see that for any x (k) i , the answers of the first-order oracle,\nFirst-order oracle: O(x(k)i ;A,B,C) = A(\u03b7x (k) i \u2212R\u03b7e1) +Bx (k) i + C,\nform a distribution over P0k+1, as the random quantities involved in the expressions (A,B and C) do no depend on \u03b7 (due to obliviosity) and the rest of the terms (\u03b7 and R\u03b7ei) are homogenous in \u03b7. Lastly, x (k+1) i are computed by simply summing up all the oracle answers, and as such, form again distributions over P0k+1.\nProof [Theorem 3] Let N be a oblivious stochastic CLI and let \u03b1 \u2208 (\u22121, 0). Our derivation of lower bounds for stochastic CLIs is established via Yao principle. Fix some k \u2208 {0, 1, . . . }. By Lemma 3, x(k)1 (\u03b7) distributes according to some distribution D over (Pk)d. Thus, by Yao principle, since each polynomial in (Pk)d represents a single deterministic algorithm, we have\nmax \u03b7\u2208[0,L] E x (k) 1 (\u03b7)\u223cD g\u03b7(x (k) 1 (\u03b7))\u2212 g\u03b7(x \u2217(\u03b7)) \u2265 min s(\u03b7)\u2208(Pk)d E\u03b7\u223cE([0,L])g\u03b7(s(\u03b7))\u2212 g\u03b7(x\u2217(\u03b7))\nwhere E([0, L], \u03b1) (abbr. E) denotes a distribution over (0, L] with a probability density function\npE(\u03b7) = (\u03b1+ 1)\u03b7\u03b1\nL\u03b1+1 .\nWe have,\nmin s(\u03b7)\u2208(Pk)d E\u03b7\u223cE [g\u03b7(s(\u03b7))\u2212 g\u03b7(x\u2217(\u03b7))] \u2265 min s(\u03b7)\u2208Pk\nE\u03b7\u223cE [ \u03b7\u2016s(\u03b7)\u2212 x\u2217(\u03b7)\u20162 ] \u2265 min\ns(\u03b7)\u2208Pk E\u03b7\u223cE\n[ \u03b7(s(\u03b7)\u2212R)2 ] = R2 min\ns(\u03b7)\u2208Pk E\u03b7\u223cE\n[ \u03b7(s(\u03b7)\u2212 1)2 ] = R2(\u03b1+ 1)\nL\u03b1+1 min\ns(\u03b7)\u2208Pk \u222b L 0 \u03b7(s(\u03b7)\u2212 1)2\u03b7\u03b1d\u03b7\n= R2(\u03b1+ 1)\nL\u03b1+1 min\ns(\u03b7)\u2208Pk \u222b 1 0 L\u03b7(s(L\u03b7)\u2212 1)2(L\u03b7)\u03b1L d\u03b7\n= LR2(\u03b1+ 1) min s(\u03b7)\u2208Pk \u222b 1 0 \u03b7(s(\u03b7)\u2212 1)2\u03b7\u03b1 d\u03b7\nwhere the first inequality follows by the fact that h\u03b7 is \u03b7-strongly convex and the second inequality follows by focusing on the first coordinate of s(\u03b7)\u2212 x\u2217(\u03b7). Invoking Lemma 9 yields\nLR2(\u03b1+ 1) min s(\u03b7)\u2208Pk \u222b 1 0 \u03b7(s(\u03b7)\u2212 1)2\u03b7\u03b1 d\u03b7 = LR2(\u03b1+ 1) min s(\u03b7)\u2208Pk\u22121 \u222b 1 0 \u03b7(s(\u03b7)\u03b7 \u2212 1)2\u03b7\u03b1d\u03b7,\n\u2265 LR 2(\u03b1+ 1)\ne2(k + 2)2(\u03b1+1)+2 .\nThus, in this case the iteration complexity is bound from below by\n2(\u03b1+1)+2\n\u221a LR2(\u03b1+ 1)\ne2 \u2212 2."}, {"heading": "A.4 Proof of Theorem 4 - Regularized Empirical Loss Minimization", "text": "For ease of presentation, we assume that \u2016xi\u2016 \u2264 1, \u03c6i take non-negative values and \u03c6i(0) \u2264 1. Furthermore, throughout the proof we assume that n is even and that L = 1 (the proof for odd n and general L > 0 holds mutatis mutandis). First, we give an explicit definition of the parametrized set of functions we will be focusing on, as well as the oracles under which our bounds hold. We denote byH the set of all (\u03c81, . . . , \u03c8n/2) \u2208 Rn/2 such that all the entries are 0, except for some j \u2208 [n/2], for which \u03c8j \u2208 [\u2212\u03c0/2, \u03c0/2]. Now, given \u03c8 \u2208 H, we set\n\u03c6i(w) = 1 2 (w + 1)2 =\u21d2 \u03c6\u2217i (u) = 1 2 u2 \u2212 u\nx\u03c8,i = { cos(\u03c8(i+1)/2)ei + sin(\u03c8(i+1)/2)ei+1 i is odd ei o.w. .\nIn which case, the corresponding dual is:\nD\u03c8(\u03b1) = 1 2n \u2016\u03b1\u20162 \u2212 1 n 1 >\u03b1+ 1 2\u03bbn2 \u2016X\u03c8\u03b1i\u20162 (27)\nwhere\nX\u03c8 := (x\u03c8,1, . . . ,x\u03c8,n) .\nEquivalently\nD\u03c8 = 1 2 \u03b1> ( 1 n I + 1 \u03bbn2 X>\u03c8X\u03c8 ) \u03b1\u2212 1 n 1 >\u03b1\nNote that\nQ\u03c8 := 1\nn I +\n1\n\u03bbn2 X>\u03c8X\u03c8 =\n1\nn  1 + 1\u03bbn\n1 \u03bbn sin(\u03c81)\n1 \u03bbn sin(\u03c81) 1 + 1 \u03bbn\n1 + 1\u03bbn 1 \u03bbn sin(\u03c82)\n1 \u03bbn sin(\u03c82) 1 + 1 \u03bbn\n. . .\n .\nNote that, all the eigenvalues of Q\u03c8 are bigger than 1. Therefore, D\u03c8 is 1-strongly convex. We assume that the oracles at the algorithms\u2019 disposal are the dual RLM oracles defined in (19),\nLastly, we will need the following definitions\nPnk,d(\u03b71, \u03b72, . . . , \u03b7n) :=   p1(\u03b71, \u03b72, . . . , \u03b7n)...\npd(\u03b71, \u03b72, . . . , \u03b7n)\n \u2223\u2223\u2223\u2223\u2223\u2223\u2223 p1, . . . , pd \u2208 Pnk , \u2202p1 + \u00b7 \u00b7 \u00b7+ \u2202pd \u2264 k  (28) Qnk,d(\u03c81, \u03c82, . . . , \u03c8n) :=   p1(sin\u03c81, sin\u03c82, . . . , sin\u03c8n)...\npd(sin\u03c81, sin\u03c82, . . . , sin\u03c8n)\n \u2223\u2223\u2223\u2223\u2223\u2223\u2223 p1, . . . , pd \u2208 Pnk,d  (29) to ease notation in subsequent proofs (where \u2202p denotes the total degree of p and Pnk is defined in (20)). Thus, Qnk contains d-dimensional vectors whose entries are n-multivariate polynomials expressions in sin\u03c81, . . . , sin\u03c8n, such that the sum of the degrees of the d-polynomials does not exceed k. In addition, given t(\u03c8) \u2208 Qnk,d we define\nti(\u03c8i) := t 0, . . . , 0, \u03c8i\ufe38\ufe37\ufe37\ufe38 i\u2019th entry , 0, . . . , 0  , \u2200i \u2208 [d]. As usual, we start by stating the functional form assumed by iterates produced by this sort of optimization\nalgorithms.\nLemma 4. When applied on (27) with a dual RLM oracle (as defined in 19), the coordinates of iterates produced by oblivious stochastic CLIs form n multivariate polynomials expressions in sin\u03c81, . . . , sin\u03c8n/2 with random coefficients, such that the sum of the degrees of these polynomials does not exceed the iteration number.\nProof Let A be a oblivious stochastic CLI, and suppose we apply A on the class of problems (27) parameterized by \u03c8, using dual RLM oracles as defined in 19. We use mathematical induction to show that for any\nk = 0, 1, . . . , the coordinate of the k\u2019th iterate produced by such process can be expressed as a distribution over polynomial expressions in sin\u03c81, . . . , sin\u03c8n/2 whose sum of degrees is less than or equal k.\nAs the first iterate \u03b1(0)i is allowed to depend only on n and \u03bb, the base case is trivial. That is, \u03b1 (0) i forms\na distribution over Rn = Qn/20,n which does not depend on sin\u03c81, . . . , sin\u03c8n/2. For the inductive step, assume that \u03b1(k)i can be expressed as a distribution over Q n/2 k,n . It is easy to see that for any \u03b1(k)i , the answer of the dual RLM oracle\nO(\u03b1(k)i ; t, `) = \u03b1+ te > ` (Q\u03c8\u03b1 (k) i \u2212\n1 n 1)e`, t \u2208 R, j \u2208 [n],\nO(\u03b1(k)i ; `) = ( I \u2212 1\n(Q\u03c8)`` e`(Q\u03c8)`,\u2217\n) \u03b1(k) +\n1\nn(Q\u03c8)`` e`\nare distributions over Qn/2k+1,n, as the only random quantity involved in the expressions t, ` does not depend on \u03c8 (due to obliviosity), the only linear factor in sin\u03c8` (i.e., e>` (Q\u03c8\u03b1\u2212 1 n1)e`, e`(Qj,\u03b7)`,\u2217) \u2019touches\u2019 \u03b1 (k) i at exactly one entry and the rest of the terms (1/n1, I, 1/(Qj,\u03b7)`` and n) are constants (w.r.t. sin\u03c8` ). Lastly, \u03b1\n(k+1) i are computed by simply summing up all the oracle answers, and as such, form again distributions\nover Qn/2k+1,n.\nProof [Theorem 4] Let A be a oblivious stochastic CLI. By Lemma 4 the coordinates of \u03b1(k)1 (the point returned by the algorithm at the k\u2019th iteration) when applied on the class of problems (27) distributes according to some distribution D over (Qn/2k ) n. Furthermore, it is easy to verify that the corresponding minimizers of (27) are\n\u03b1\u2217(\u03c8) =\n( 1\n\u03bbn+1 \u03bbn + 1 \u03bbn sin(\u03c81)\n, 1\n\u03bbn+1 \u03bbn + 1 \u03bbn sin(\u03c81)\n, 1\n\u03bbn+1 \u03bbn + 1 \u03bbn sin(\u03c82)\n, 1\n\u03bbn+1 \u03bbn + 1 \u03bbn sin(\u03c82)\n, . . . ) .\n(30)\n\u03b1 (k) 1 (\u03c8) distributes according to some distribution D over Q n/2 k,n . Thus, by Yao principle, since each polynomial in Qn/2k,n represents a single deterministic algorithm, we have\nmax \u03c8\u2208H E \u03b1 (k) 1 (\u03c8)\u223cD\n\u2016\u03b1(k)1 (\u03c8)\u2212\u03b1 \u2217(\u03c8)\u2016 \u2265 min t(\u03c8)\u2208Qn/2k,n E\u03c8\u223cU(H)\u2016t(\u03c8)\u2212\u03b1\u2217(\u03c8)\u2016 (31)\nwhere U(H) denotes a distribution overH which corresponds to of first drawing j \u223c U([n/2]) at random, and then drawing \u03c8j according to distribution defined by the p.d.f. p\u03c8j (\u03c8) = cos(\u03c8)/2 over [\u2212\u03c0/2, \u03c0/2]\n(for i 6= j we set \u03c8i = 0 ). We now have,\nmin t(\u03c8)\u2208Qn/2k,n\nE\u03c8\u223cU(H)\u2016t(\u03c8)\u2212\u03b1\u2217(\u03c8)\u2016\n= min t(\u03c8)\u2208Qn/2k,n\nEj\u223cU([n/2])E\u03c8j\u223cU([\u2212\u03c0/2,\u03c0/2])\u2016t(\u03c8)\u2212\u03b1 \u2217(\u03c8)\u2016\n= 2\nn n/2\u2211 j=1 min t(\u03c8)\u2208Qn/2k,n E\u03c8j\u223cU([\u2212\u03c0/2,\u03c0/2])\u2016t(\u03c8)\u2212\u03b1 \u2217(\u03c8)\u2016\n\u2265 2 n n/2\u2211 j=1 min t(\u03c8)\u2208Qn/2k,n E\u03c8j\u223cU([\u2212\u03c0/2,\u03c0/2]) \u2223\u2223\u2223\u2223\u2223tj(\u03c8j)\u2212 1\u03bbn+1 \u03bbn + 1 \u03bbn sin(\u03c8j) \u2223\u2223\u2223\u2223\u2223 \u2265 1 n n/2\u2211 j=1 min t(\u03c8)\u2208Qn/2k,n \u222b \u03c0/2 \u2212\u03c0/2 \u2223\u2223\u2223\u2223\u2223tj(\u03c8j)\u2212 1\u03bbn+1 \u03bbn + 1 \u03bbn sin(\u03c8j)\n\u2223\u2223\u2223\u2223\u2223 cos\u03c8j d\u03c8j = 1\nn n/2\u2211 j=1 min s(\u03c8)\u2208Qn/2k,n \u222b 1 \u22121 \u2223\u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03bbn+1 \u03bbn + 1 \u03bbn\u03b7j \u2223\u2223\u2223\u2223\u2223 d\u03b7j = \u03bb\nn/2\u2211 j=1 min s(\u03c8)\u2208Qn/2k,n \u222b 1 \u22121 \u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03bbn+ 1 + \u03b7j \u2223\u2223\u2223\u2223 d\u03b7j (32)\nwhere the first inequality follows by focusing on the j\u2019th coordinate of s(\u03c8) \u2212 \u03b1\u2217(\u03c8) in each summand. Now, set \u03b1 = 1 + \u03bbn,L = 3, \u00b5 = 1 and note that\u221a\n2\u03b1+ L\u2212 \u00b5 2\u03b1+ \u00b5\u2212 L =\n\u221a 2\u03bbn+ 4\n2\u03bbn =\n\u221a \u03bbn+ 2\n\u03bbn =\n\u221a 2\n\u03bbn + 1\nThus, by Lemma 8, using the same value for \u03b1 and noting that \u03b1 > 1 = (L\u2212 \u00b5)/2) yields\n\u222b 1 \u22121 \u2223\u2223\u2223\u2223sj(\u03b7j)\u2212 1\u03bbn+ 1 + \u03b7j \u2223\u2223\u2223\u2223 d\u03b7j \u2265  \u221a 2 \u03bbn + 1\u2212 1\u221a 2 \u03bbn + 1 + 1 kj\nwhere kj denotes the degree of sj(\u03b7j). Plugging in this into Inequality (32) we get\nmax \u03c8\u2208H E \u03b1 (k) 1 (\u03c8)\u223cD\n\u2016\u03b1(k)1 (\u03c8)\u2212\u03b1 \u2217(\u03c8)\u2016 \u2265 \u03bb min\ns(\u03c8)\u2208Qn/2k,n n/2\u2211 j=1\n \u221a\n2 \u03bbn + 1\u2212 1\u221a 2 \u03bbn + 1 + 1 kj . Since u 7\u2192 \u03c1u is a decreasing and convex function for any 1 > \u03c1 > 0, we have\n\u03bb min s(\u03c8)\u2208Qn/2k,n n/2\u2211 j=1\n \u221a\n2 \u03bbn + 1\u2212 1\u221a 2 \u03bbn + 1 + 1 kj \u2265 n\u03bb/2 min s(\u03c8)\u2208Qn/2k,n  \u221a 2 \u03bbn + 1\u2212 1\u221a 2 \u03bbn + 1 + 1  2 n \u2211n/2 j=1 kj\n\u2265 n\u03bb/2 min s(\u03c8)\u2208Qn/2k,n\n \u221a\n2 \u03bbn + 1\u2212 1\u221a 2 \u03bbn + 1 + 1\n 2k n\nwhere the last inequality is due to the fact that s(\u03c8) \u2208 Qn/2k,n (sin\u03c8) which implies that \u2211n\nj=1 kj \u2264 k. Finally, we have,\nmax \u03c8\u2208H E \u03b1 (k) 1 (\u03c8)\u223cD [D\u03c8(\u03b1 (k) 1 (\u03c8))\u2212D\u03c8(\u03b1 \u2217(\u03c8))] \u2265 max \u03c8\u2208H E \u03b1 (k) 1 (\u03c8)\u223cD\n1 2 \u2016\u03b1(k)1 (\u03c8)\u2212\u03b1 \u2217(\u03c8)\u20162\n\u2265 1 2 ( max \u03c8\u2208H E \u03b1 (k) 1 (\u03c8)\u223cD \u2016\u03b1(k)1 (\u03c8)\u2212\u03b1 \u2217(\u03c8)\u2016 )2\n\u2265 1 2 n\u03bb/2 min s(\u03c8)\u2208Qn/2k,n  \u221a 2 \u03bbn + 1\u2212 1\u221a 2 \u03bbn + 1 + 1  2k n  2 ,\nwhere the first inequality follows by the 1-strong convexity of D\u03c8 and the third inequality follows by Jensen inequality. Using Lemma 12, we get that the iteration complexity of A is at least\n1\n8\n\u221a 2n\n\u03bb\n( ln n2\u03bb2\n8 + ln(1/ )\n) .\nLastly, we bound from below the number of iterations required to obtain a non-trivial accuracy.\nLemma 5. Let j \u2208 [n], let \u03c8j,1 \u2208 H be a parameters vector whose all coordinates are \u2212\u03c0/2 and let \u03b7\u03c8,2 \u2208 H be a parameters vector whose all coordinates are \u2212\u03c0/2, except for the j\u2019th coordinate which we set to be \u03c0/2. Then\n\u2016\u03b1\u2217(\u03c81)\u2212\u03b1\u2217(\u03c82)\u2016 \u2265 2 \u221a 2\n\u03bbn+ 2\nProof By Equation (30) we have\n\u2016\u03b1\u2217(\u03c81)\u2212\u03b1\u2217(\u03c82)\u2016 = \u221a 2\n( 1\n\u03bbn+1 \u03bbn \u2212 1 \u03bbn \u2212 1 \u03bbn+1 \u03bbn + 1 \u03bbn\n)\n= \u221a 2 ( 1\u2212 \u03bbn\n\u03bbn+ 2 ) = 2 \u221a 2\n\u03bbn+ 2 .\nTheorem 6. When applied on (27) ,the iteration complexity of oblivious stochastic CLI algorithms equipped with a dual RLM oracle D\u03c8 is at least n/2.\nProof Let A be a stochastic optimization algorithm. By Lemma 4 the coordinates of \u03b1(k)1 (the point returned by the algorithm at the k\u2019th iteration) when applied on the class of problems (27) distributes according to\nsome distribution D over (Qn/2k ) n. By Yao principle, since each polynomial in Qn/2k,n represents a single deterministic algorithm, we have\nmax \u03c8\u2208H E \u03b1 (k) 1 (\u03c8)\u223cD\n\u2016\u03b1(k)1 (\u03c8)\u2212\u03b1 \u2217(\u03c8)\u2016 \u2265 min t(\u03c8)\u2208Qn/2k,n E\u03c8\u223cD(H)\u2016t(\u03c8)\u2212\u03b1\u2217(\u03c8)\u2016 (33)\nwhere D(H) denotes a distribution overH which corresponds to the process of first drawing j \u223c U([n/2]) at random, and then set \u03c8 to be \u03c8j,1 or \u03c8j,2 as defined in Lemma 5 with equal probability. Now, for k < n/2, there exists some j \u2208 [n/2] such that t(\u03c8) does not depend on \u03c8j . This yields,\nmax {\u03c8j,i|j\u2208[n/2],i\u2208[2]}\nEA[D\u03c8j,i(\u03b1 (k) ( \u03c8j,i ) )\u2212D\u03c8j,i(\u03b1 \u2217 (\u03c8j,i))] \u2265 min\ndeterministic algorithms E \u03c8\u223cD(H)[D\u03c8j,i(\u03b1\n(k) ( \u03c8j,i ) )\u2212D\u03c8j,i(\u03b1 \u2217 (\u03c8j,i))] \u2265 min\ndeterministic algorithms E \u03c8\u223cD(H)\n1 2 \u2016\u03b1(k)\n( \u03c8j,i ) )\u2212\u03b1\u2217 ( \u03c8j,i ) \u20162\n\u2265 1 2 min deterministic algorithms ( E \u03c8\u223cD(H)\u2016\u03b1 (k) ( \u03c8j,i ) )\u2212\u03b1\u2217 ( \u03c8j,i ) \u2016 )2\n\u2265 1 2\n( 2 \u221a 2\nn(\u03bbn+ 2)\n)2 ,\nwhere the last inequality follows from Lemma 5. Thus, for sufficiently small , one must perform at least n/2 iterations in order to obtain an -optimal solution.\nA.5 Best polynomial approximation over closed intervals in R\nIn the following section we analyze the best polynomial approximation of some functions w.r.t. L\u221e, L1 and L2 norm, based on standard results from the approximation theory (see generally, Allan Pinkus. On L1-approximation, 1989; Theodore J Rivlin. An introduction to the approximation of functions, 2003; Ronald A DeVore and George G Lorentz. Constructive approximation, 1993; Naum Il\u2019ich Akhiezer and Charles J Hyman. Theory of approximation. Translated by Charles J. Hyman. New York, 1956; Isidor Pavlovich Natanson. Constructive function theory, 1964)."}, {"heading": "A.5.1 Approximation w.r.t. L\u221e", "text": "Lemma 6. Let b > a > 0 and c > \u2212a, then\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 + c \u2225\u2225\u2225\u2225 L\u221e([a,b]) \u2265 2(b\u2212 a) (b+ a+ 2c)2 \u2212 (b\u2212 a)2  \u221a b+c a+c \u2212 1\u221a b+c a+c + 1 k .\nProof We have,\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 + c \u2225\u2225\u2225\u2225 L\u221e([a,b]) = min s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225s ( a\u2212 b 2 \u03b7 + a+ b 2 ) \u2212 1 a\u2212b 2 \u03b7 + b+a 2 + c \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1])\n= min s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225s (\u03b7)\u2212 1a\u2212b 2 \u03b7 + b+a+2c 2 \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1])\n= 2\nb\u2212 a min\ns(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225b\u2212 a2 s (\u03b7)\u2212 b\u2212a2a\u2212b 2 \u03b7 + b+a+2c 2 \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1])\n= 2\nb\u2212 a min\ns(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225s (\u03b7) + 1\u03b7 \u2212 b+a+2cb\u2212a \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1])\n= 2\nb\u2212 a min\ns(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225\u2212s (\u03b7) + 1\u03b7 \u2212 b+a+2cb\u2212a \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1])\n= 2\nb\u2212 a min\ns(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225s (\u03b7)\u2212 1\u03b7 \u2212 b+a+2cb\u2212a \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1])\nwhere we used the fact that Pk is invariant under pre-composition and post-composition with linear function in the second, fourth and fifth equalities. Now, since\nc > \u2212a =\u21d2 b+ a+ 2c b\u2212 a > 1,\ncombining Inequality 8 with Lemma 10, yields\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 \u2212 b+a+2cb\u2212a \u2225\u2225\u2225\u2225\u2225 L\u221e([\u22121,1]) \u2265\n(( b+a+2c b\u2212a ) \u2212 \u221a( b+a+2c b\u2212a )2 \u2212 1 )k ( b+a+2c b\u2212a )2 \u2212 1\n= 1(\nb+a+2c b\u2212a\n)2 \u2212 1\n1\u2212 \u221a b+a+2c b\u2212a \u22121 b+a+2c b\u2212a +1\n1 +\n\u221a b+a+2c b\u2212a \u22121\nb+a+2c b\u2212a +1\n k\n(34)\nNoting that\nb+a+2c b\u2212a \u2212 1 b+a+2c b\u2212a + 1 = b+ a+ 2c\u2212 (b\u2212 a) b+ a+ 2c+ b\u2212 a = a+ c b+ c ,\nwe get\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 + c \u2225\u2225\u2225\u2225 L\u221e([a,b]) \u2265 2 b\u2212 a 1( b+a+2c b\u2212a )2 \u2212 1  \u221a b+c a+c \u2212 1\u221a b+c a+c + 1 k .\nAs,\n2 b\u2212 a 1(\nb+a+2c b\u2212a\n)2 \u2212 1 = 2 b\u2212 a (b\u2212 a)2 (b+ a+ 2c)2 \u2212 (b\u2212 a)2 = 2(b\u2212 a) (b+ a+ 2c)2 \u2212 (b\u2212 a)2\nwe get\nmin s(\u03b7)\u2208Pk \u2225\u2225\u2225\u2225s(\u03b7)\u2212 1\u03b7 + c \u2225\u2225\u2225\u2225 L\u221e([a,b]) \u2265 2(b\u2212 a) (b+ a+ 2c)2 \u2212 (b\u2212 a)2  \u221a b+c a+c \u2212 1\u221a b+c a+c + 1 k ."}, {"heading": "A.5.2 Approximation w.r.t. L1", "text": "Let Uk(\u03b7) denote the k\u2019th second order Chebyshev polynomial, i.e.,\nUk(\u03b7) := sin((k + 1) arccos \u03b7)\u221a\n1\u2212 \u03b72 (35)\n(To see why these are indeed polynomials, observe that Uk(\u03b7) are the derivative of Chebyshev polynomials of first order scaled by a factor of 1/k). The zeros of Uk(\u03b7) are \u03b7j = cos( j\u03c0 k+1), j = 1, . . . , k. First, let us establish the orthogonality of sgn(Uk(\u03b7)) with respect to Pk\u22121 over [\u22121, 1]. Lemma 7. Let p(\u03b7) \u2208 Pk\u22121, then \u222b 1\n\u22121 p(\u03b7)sgn(Uk(\u03b7)) d\u03b7 = 0\nProof We integrate by substituting \u03b7 = e i\u03b8+e\u2212i\u03b8\n2 (= cos(\u03b8)),\u222b 1 \u22121 p(\u03b7)sgn(Uk(\u03b7)) d\u03b7 \u03b7= e i\u03b8+e\u2212i\u03b8 2= \u222b 0 \u03c0 p ( ei\u03b8 + e\u2212i\u03b8 2 ) sgn ( Uk ( ei\u03b8 + e\u2212i\u03b8 2 ))( iei\u03b8 \u2212 ie\u2212i\u03b8 2 ) d\u03b8\n= \u222b \u03c0 0 p ( ei\u03b8 + e\u2212i\u03b8 2 ) sgn ( sin((k + 1)\u03b8) sin(\u03b8) )( ie\u2212i\u03b8 \u2212 iei\u03b8 2 ) d\u03b8\n= \u222b \u03c0 0 p ( ei\u03b8 + e\u2212i\u03b8 2 ) sgn (sin((k + 1)\u03b8)) ( ie\u2212i\u03b8 \u2212 iei\u03b8 2 ) d\u03b8\n= 1\n2 \u222b \u03c0 \u2212\u03c0 p ( ei\u03b8 + e\u2212i\u03b8 2 ) sgn (sin((k + 1)\u03b8)) ( ie\u2212i\u03b8 \u2212 iei\u03b8 2 ) d\u03b8 (36)\nwhere the last equality is due to the fact that the integrand is an even function in \u03b8. Lastly, since for any j = 1, . . . , k we have\u222b \u03c0\n\u2212\u03c0 e\u2212ij\u03b8 sin((k + 1)\u03b8)d\u03b8 = \u222b \u03c0+2\u03c0/(k+1) \u2212\u03c0+2\u03c0/(k+1) e\u2212ij\u03b8 sin((k + 1)\u03b8)d\u03b8\n= \u222b \u03c0 \u2212\u03c0 e\u2212ij(\u03b8+2\u03c0/(k+1)) sin((k + 1)(\u03b8 + 2\u03c0/(k + 1)))d\u03b8\n= e\u22122\u03c0ij/(k+1) \u222b \u03c0 \u2212\u03c0 e\u2212ij\u03b8 sin((k + 1)\u03b8)d\u03b8,\nand since e\u22122\u03c0ij/(k+1) 6= 1 for j = 1, . . . , k, it follows that\u222b \u03c0 \u2212\u03c0 e\u2212ij\u03b8 sin((k + 1)\u03b8)d\u03b8 = 0, j = 1, . . . , k.\nThis, together with the case where j = 0,\u222b \u03c0 \u2212\u03c0 sin((k + 1)\u03b8)d\u03b8 = (\u2212 cos((k + 1)\u03b8)/(k + 1)) \u2223\u2223\u2223\u03c0 \u2212\u03c0 = 0,\nimplies that all the terms in (36) vanish, thus concluding the proof.\nGiven \u00b5 < L (note that, here \u00b5 and L are allowed to take negative values), we define\nU\u0303k(\u03b7) := Uk\n( 2\u03b7\n\u00b5\u2212 L\n) .\nBy substituting \u03b7 for (\u00b5\u2212L)\u03b72 , we get the following corollary.\nCorollary 1. Let p(\u03b7) \u2208 Pk\u22121, then \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\np(\u03b7) sgn(U\u0303k(\u03b7)) d\u03b7 = 0 (37)\nWe now use Corollary 1 to bound from below the best polynomial L1-approximation error w.r.t. 1/(\u03b7+\u03b1) over the interval [\u00b5,L].\nLemma 8. Let p(\u03b7) \u2208 Pk\u22121. Then, for any (L\u2212 \u00b5)/2 < \u03b1 we have\n\u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n|p(\u03b7)\u2212 1/(\u03b7 + \u03b1)|d\u03b7 \u2265\n \u221a\n2\u03b1+L\u2212\u00b5 2\u03b1+\u00b5\u2212L \u2212 1\u221a 2\u03b1+L\u2212\u00b5 2\u03b1+\u00b5\u2212L + 1 k . Proof First, note that the following two inequalities\u222b L\u2212\u00b5\n2\n\u2212L\u2212\u00b5 2\n|p(\u03b7)\u2212 1/(\u03b7 + \u03b1)|d\u03b7 \u2265 \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n(p(\u03b7)\u2212 1/(\u03b7 + \u03b1))sgn(U\u0303k(\u03b7))d\u03b7 = \u2212 \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2 1/(\u03b7 + \u03b1)sgn(U\u0303k(\u03b7))d\u03b7\u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n|p(\u03b7)\u2212 1/(\u03b7 + \u03b1)|d\u03b7 \u2265 \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n(p(\u03b7)\u2212 1/(\u03b7 + \u03b1))sgn(\u2212U\u0303k(\u03b7))d\u03b7 = \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n1/(\u03b7 + \u03b1)sgn(U\u0303k(\u03b7))d\u03b7\nhold due to orthogonality condition (37) and the fact that sgn(\u00b7) is odd. Therefore,\u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n|p(\u03b7)\u2212 1/(\u03b7 + \u03b1)|d\u03b7 \u2265 \u2223\u2223\u2223\u2223\u2223 \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n1/(\u03b7 + \u03b1) sgn(U\u0303k(\u03b7))d\u03b7 \u2223\u2223\u2223\u2223\u2223 .\nSubstituting \u00b5\u2212L2 \u03b7 for \u03b7, yields\u2223\u2223\u2223\u2223\u2223 \u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n1/(\u03b7 + \u03b1) sgn(U\u0303k(\u03b7))d\u03b7 \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 \u222b \u22121 1 1 \u00b5\u2212L 2 \u03b7 + \u03b1 sgn(Uk(\u03b7)) \u00b5\u2212 L 2 d\u03b7 \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 \u222b 1 \u22121 1 \u00b5\u2212L 2 \u03b7 + \u03b1 sgn(Uk(\u03b7)) L\u2212 \u00b5 2 d\u03b7 \u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 \u222b 1 \u22121\n1\n\u2212\u03b7 + 2\u03b1L\u2212\u00b5 sgn(Uk(\u03b7))d\u03b7 \u2223\u2223\u2223\u2223\u2223 (38) Now, plugging in the definition of Uk(\u03b7) (see (35)) and applying Lemma 11, we get\n\u222b 1 \u22121 sgn(sin(k arccos(\u03b7))) u\u2212 \u03b7 d\u03b7 \u2265\n1\u2212 \u221a u\u22121 u+1\n1 + \u221a\nu\u22121 u+1\nk\nfor any u > 1. Using this inequality with (38) where u = 2\u03b1L\u2212\u00b5 , yields\n\u222b L\u2212\u00b5 2\n\u2212L\u2212\u00b5 2\n|p(\u03b7)\u2212 1/(\u03b7 + \u03b1)|d\u03b7 \u2265\n \u221a\n2\u03b1+L\u2212\u00b5 2\u03b1+\u00b5\u2212L \u2212 1\u221a 2\u03b1+L\u2212\u00b5 2\u03b1+\u00b5\u2212L + 1\nk ."}, {"heading": "A.5.3 Approximation w.r.t. L2", "text": "Lemma 9. For any \u03b1 \u2208 (\u22121, 0),\nmin s(\u03b7)\u2208Pk\u22121 \u222b 1 0 \u03b7(s(\u03b7)\u03b7 \u2212 1)2\u03b7\u03b1d\u03b7 \u2265 1 e2(k + 2)2(\u03b1+1)+2\nProof Rephrasing it equivalently as\nmin s(\u03b7)\u2208Pk\u22121 \u222b 1 0 (s(\u03b7)\u03b7 3+\u03b1 2 \u2212 \u03b7 1+\u03b1 2 )2d\u03b7,\nshows that this problem can be seen as a best L2-approximation for \u03b7 1+\u03b1 2 in the k-dimensional space spanned by gi = \u03b7i+ 1+\u03b1 2 , i = 1, . . . , k (accordingly, g0 = \u03b7 1+\u03b1 2 ). By [2, Equation (3), p. 16], we have\nmin s(\u03b7)\u2208Pk\u22121 \u222b 1 0 (s(\u03b7)\u03b7 3+\u03b1 2 \u2212 \u03b7 1+\u03b1 2 )2d\u03b7,= detG(g0, g1, . . . , gn) detG(g1, . . . , gn)\nwhere G(\u00b7) is Gram matrix (whose entries are the inner products of its arguments). First, note that\n\u3008gi, gj\u3009 = \u222b 1 0 \u03b7i+ 1+\u03b1 2 \u03b7j+ 1+\u03b1 2 d\u03b7 = \u222b 1 0 \u03b7i+j+1+\u03b1d\u03b7 =\n1\ni+ j + \u03b1+ 2 , i, j = 0, 1, . . . , k\nThus,\nG(g1, . . . , gk)i,j = 1\ni+ j + \u03b1+ 2 ,\nG(g0, . . . , gk)i,j = 1\ni+ j + \u03b1 .\nIt follows that both matrices can be expressed as a Cauchy matrices, that is\nG(g1, . . . , gk)i,j = 1\nxi \u2212 yj ,\nG(g0, . . . , gk)i,j = 1\nui \u2212 vj .\nwhere xi = i+ \u03b1+ 1, yj = \u2212j \u2212 1, i, j \u2208 [k] and ui = i+ \u03b1, vj = \u2212j, i, j \u2208 [k + 1]. The determinant of Cauchy matrix A defined by sequences wi, zj one has\ndetA =\n\u220fk i=2 \u220fi\u22121 j=1(wi \u2212 wj)(zj \u2212 zi)\u220fk i=1 \u220fk j=1(wi \u2212 zj)\nHence,\ndetG(g0, g1, . . . , gk)\ndetG(g1, . . . , gk) =\n\u220fk+1 i=2 \u220fi\u22121 j=1(ui\u2212uj)(vj\u2212vi)\u220fk+1\ni=1 \u220fk+1 j=1 (ui\u2212vj)\u220fk\ni=2 \u220fi\u22121 j=1(xi\u2212xj)(yj\u2212yi)\u220fk\ni=1 \u220fk j=1(xi\u2212yj)\n=\n\u220fk+1 i=2 \u220fi\u22121 j=1(i\u2212j)(\u2212j\u2212(\u2212i))\u220fk+1\ni=1 \u220fk+1 j=1 (i+\u03b1\u2212(\u2212j))\u220fk\ni=2 \u220fi\u22121 j=1((i+1)\u2212(j+1))((\u2212j\u22121)\u2212(\u2212i\u22121))\u220fk i=1 \u220fk j=1((i+\u03b1+1)\u2212(\u2212j\u22121))\n=\n\u220fk+1 i=2 \u220fi\u22121 j=1(i\u2212j)\n2\u220fk+1 i=1 \u220fk+1 j=1 (i+j+\u03b1)\u220fk\ni=2 \u220fi\u22121 j=1(i\u2212j)2\u220fk\ni=1 \u220fk j=1(i+j+\u03b1+2)\n=\n\u220fk+1 i=2 \u220fi\u22121 j=1(i\u2212 j)2\u220fk+1\ni=1 \u220fk+1 j=1(i+ j + \u03b1)\n\u220fk i=1 \u220fk j=1(i+ j + \u03b1+ 2)\u220fk\ni=2 \u220fi\u22121 j=1(i\u2212 j)2\n=\n\u220fk j=1(k + 1\u2212 j)2 \u220fk i=1 \u220fk j=1(i+ j + \u03b1+ 2)\u220fk+1\ni=1 \u220fk+1 j=1(i+ j + \u03b1)\n=\n\u220fk j=1(k + 1\u2212 j)2 \u220fk i=1 \u220fk j=1((i+ \u03b1+ 1) + (j + 1))\u220fk+1\ni=1 \u220fk+1 j=1((i+ \u03b1) + j)\n=\n\u220fk j=1(k + 1\u2212 j)2 \u220fk+1 i=2 \u220fk+1 j=2((i+ \u03b1) + j)\u220fk+1\ni=1 \u220fk+1 j=1((i+ \u03b1) + j)\n= \u220fk j=1(k + 1\u2212 j)2\u220fk+1\ni=1 (i+ \u03b1+ 1) \u220fk+1 j=2(1 + \u03b1+ j)\n=\n\u220fk j=1 j\n2\u220fk+1 i=1 (i+ \u03b1+ 1) \u220fk+1 j=2(1 + \u03b1+ j)\n= (\u03b1+ 2)\n\u220fk j=1 j\n2\u220fk+1 i=1 (i+ \u03b1+ 1) 2\n= (\u03b1+ 2)  k\u220f j=1\nj\nj + \u03b1+ 1 2 1 (k + \u03b1+ 2)2\nTo estimate the middle term, we apply arguments similar to the integral test for infinite series. First, note that, k\u220f j=1 j j + \u03b1+ 1 = exp ( k\u2211 i=1 ln j j + \u03b1+ 1 ) .\nNow, since for any \u03b1 \u2208 (\u22121, 0), it holds that x 7\u2192 ln xx+\u03b1+1 is a monotone decreasing function (over x 6= \u03b1), it holds that\nk\u2211 i=1 ln j j + \u03b1+ 1 \u2265 \u222b k+1 1 ln x x+ \u03b1+ 1 dx = ( x ln\nx\n\u03b1+ x+ 1 \u2212 (\u03b1+ 1) ln(\u03b1+ x+ 1) )\u2223\u2223\u2223\u2223k+1 1\nHence, k\u220f j=1 j j + \u03b1+ 1 \u2265 exp ( (k + 1) ln\nk + 1 \u03b1+ (k + 1) + 1 \u2212 (\u03b1+ 1) ln(\u03b1+ (k + 1) + 1)\u2212 ln 1 \u03b1+ 2 + (\u03b1+ 1) ln(\u03b1+ 2)\n)\n=\n( k + 1\nk + \u03b1+ 2\n)k+1 (k + \u03b1+ 2)\u2212(\u03b1+1)(\u03b1+ 2)\u03b1+2\n\u2265 ( k + 1\nk + \u03b1+ 2\n)k+1 (k + \u03b1+ 2)\u2212(\u03b1+1)\n= ( 1\u2212 \u03b1+ 1\nk + \u03b1+ 2\n)k+1 (k + \u03b1+ 2)\u2212(\u03b1+1)\n= ( 1\u2212 1\nk+1 \u03b1+1 + 1\n)k+1 (k + \u03b1+ 2)\u2212(\u03b1+1).\nNow, by the following standard inequality\n1\u2212 2 x+ 1 \u2265 exp ( \u22122 x\u2212 1 ) ,\nwe get,\n1\u2212 1 k+1 \u03b1+1 + 1 = 1\u2212 2 (2 k+1\u03b1+1 + 1) + 1 \u2265 exp\n( \u22122\n(2 k+1\u03b1+1 + 1)\u2212 1\n) = exp ( \u22121 k+1 \u03b1+1 ) = exp ( \u2212(\u03b1+ 1) k + 1 ) therefore,\n(\u03b1+ 2)  k\u220f j=1\nj\nj + \u03b1+ 1 2 1 (k + \u03b1+ 2)2 \u2265 (\u03b1+ 2) ( exp ( \u2212(\u03b1+ 1) k + 1 )k+1 (k + \u03b1+ 2)\u2212(\u03b1+1) )2 1 (k + \u03b1+ 2)2\n= (\u03b1+ 2) exp (\u22122(\u03b1+ 1)) (k + \u03b1+ 2)\u22122(\u03b1+1)\u22122\n\u2265 (\u03b1+ 2) exp (\u22122(\u03b1+ 1)) (k + 2)\u22122(\u03b1+1)\u22122\nAll in all, we get\nmin s(\u03b7)\u2208Pk\u22121 \u222b 1 0 \u03b7(s(\u03b7)\u03b7 \u2212 1)2\u03b7\u03b1d\u03b7 \u2265 1 e2(k + 2)2(\u03b1+1)+2"}, {"heading": "A.6 Technical Lemmas", "text": "Lemma 10. For any u \u2265 1,\nu\u2212 \u221a u2 \u2212 1 =\n1\u2212 \u221a\nu\u22121 u+1 1 + \u221a\nu\u22121 u+1\n.\nProof We have,\n1\u2212 \u221a\nu\u22121 u+1 1 + \u221a\nu\u22121 u+1\n=\n( 1\u2212 \u221a u\u22121 u+1 )2 1\u2212 u\u22121u+1 = (u+ 1) ( 1\u2212 \u221a u\u22121 u+1 )2 u+ 1\u2212 (u\u2212 1) = (\u221a u+ 1\u2212 \u221a u\u2212 1 )2 2\n= u+ 1\u2212 2\n\u221a (u+ 1)(u\u2212 1) + (u\u2212 1)\n2 = u\u2212\n\u221a u2 \u2212 1\nLemma 11. For any u > 1,\n\u222b 1 \u22121 sgn(sin(k arccos(\u03b7))) u\u2212 \u03b7 d\u03b7 \u2265\n1\u2212 \u221a u\u22121 u+1\n1 + \u221a\nu\u22121 u+1 k . Proof First, note that the function\n\u03b3(x) := ln x+ 1 x\u2212 1 \u2212 1 x\ntakes non-negative for any x > 1, as\n\u03b3\u2032(x) = x\u2212 1 x+ 1 x\u2212 1\u2212 (x+ 1) (x\u2212 1)2 + 1 x2 = x\u2212 1 x+ 1 \u22122 (x\u2212 1)2 + 1 x2\n\u2264 \u22122 (x\u2212 1)2 + 1 x2 \u2264 \u22121 (x\u2212 1)2 < 0\nand limx\u2192\u221e \u03b3(x) = 0. Therefore, by using identity (see Section F.31. in [2]), we get\u222b 1 \u22121 sgn(sin(k arccos(\u03b7))) u\u2212 \u03b7 d\u03b7 = 2 ln (u+ \u221a u2 \u2212 1)k + 1 (u+ \u221a u2 \u2212 1)k \u2212 1\n\u2265 (u\u2212 \u221a u2 \u2212 1)k = 1\u2212 \u221a u\u22121 u+1\n1 + \u221a\nu\u22121 u+1 k , where the last equality is due to Lemma 10.\nLemma 12. Let L > \u00b5 > 0, c > 0 and \u03b1 \u2265 0. Then\n\u2265 c\n \u221a\nL+\u03b1 \u00b5+\u03b1 \u2212 1\u221a L+\u03b1 \u00b5+\u03b1 + 1\nk =\u21d2 k \u2265 1 2 (\u221a L+ \u03b1 \u00b5+ \u03b1 \u2212 1 ) (ln(c) + ln(1/ ))\nProof Note that the function\n\u03b4(x) = ln \u221a x\u2212 1\u221a x+ 1 + 2\u221a x\u2212 1\ntakes non-negative values for x > 1, as\n\u03b4\u2032(x) = \u221a x+ 1\u221a x\u2212 1 0.5x\u22121/2( \u221a x+ 1)\u2212 0.5x\u22121/2( \u221a x\u2212 1) ( \u221a x+ 1)2 \u2212 1 (x\u2212 1) \u221a x\u2212 1\n= 1 (x\u2212 1) \u221a x \u2212 1 (x\u2212 1) \u221a x\u2212 1 < 0\nand limx\u2192\u221e \u03b4(x) = 0. Thus, we obtained the following inequality \u221a x\u2212 1\u221a x+ 1 \u2265 exp ( \u22122\u221a x\u2212 1 ) , x > 1,\nyields\nc\n \u221a\nL+\u03b1 \u00b5+\u03b1 \u2212 1\u221a L+\u03b1 \u00b5+\u03b1 + 1\nk \u2265 c exp  \u22122k\u221a\nL+\u03b1 \u00b5+\u03b1 \u2212 1  . Hence,\nln \u2265 ln(c) + \u22122k\u221a L+\u03b1 \u00b5+\u03b1 \u2212 1\n=\u21d2 2k\u221a L+\u03b1 \u00b5+\u03b1 \u2212 1 \u2265 ln(c) + ln(1/ )\n=\u21d2 k \u2265 1 2\n(\u221a L+ \u03b1\n\u00b5+ \u03b1 \u2212 1\n) (ln(c) + ln(1/ ))"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Many canonical machine learning problems boil down to a convex optimization problem with a finite<lb>sum structure. However, whereas much progress has been made in developing faster algorithms for<lb>this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower<lb>bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often<lb>unrealistic regime where the number of iterations is less than O(d/n) (where d is the dimension and n<lb>is the number of samples). In this work, we extend the framework of Arjevani et al. [3, 5] to provide<lb>new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby<lb>covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as<lb>well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA.", "creator": "LaTeX with hyperref package"}}}