{"id": "1606.09403", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "abstract": "Cross-lingual word embedding represents lexical elements from different languages in the same vector space and enables the transfer of NLP tools. However, previous attempts have had costly resource requirements, difficulties in integrating monolingual data or have been unable to handle polysemy. We address these drawbacks with our method, which relies on a comprehensive dictionary in an EM-like training algorithm compared to monolingual corpora in two languages. Our model achieves state-of-the-art performance in the bilingual lexicon introduction task, surpassing models with large bilingual corpora, and competitive results in monolingual word similarity and cross-language classification of documents.", "histories": [["v1", "Thu, 30 Jun 2016 09:18:53 GMT  (208kb,D)", "http://arxiv.org/abs/1606.09403v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["long duong", "hiroshi kanayama", "tengfei ma", "steven bird", "trevor cohn"], "accepted": true, "id": "1606.09403"}, "pdf": {"name": "1606.09403.pdf", "metadata": {"source": "CRF", "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "authors": ["Long Duong", "Hiroshi Kanayama", "Tengfei Ma", "Steven Bird", "Trevor Cohn"], "emails": [], "sections": [{"heading": null, "text": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task."}, {"heading": "1 Introduction", "text": "Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Ta\u0308ckstro\u0308m et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target\nlanguage lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012).\nMost previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity and bilingual lexicon induction tasks. Moreover, many prior work (Chandar A P et al., 2014; Koc\u030cisky\u0301 et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages. S\u00f8gaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods. To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015). However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm. Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity.\nOur model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal. Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages. Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through in-\nar X\niv :1\n60 6.\n09 40\n3v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\ncorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)). In addition to the dictionary, our method only requires monolingual data, as an extension of the continuous bag-of-word (CBOW) model (Mikolov et al., 2013b). We experiment with several variations of our model, whereby we predict only the translation or both word and its translation and consider different ways of using the different learned center-word versus context embeddings in application tasks. We also propose a regularisation method to combine the two embedding matrices during training. Together, these modifications substantially improve the performance across several tasks. Our final model achieves state-of-theart performance on bilingual lexicon induction task, large improvement over word similarity task compared with previous published crosslingual word embeddings, and competitive result on cross-lingual document classification task. Notably, our embedding combining techniques are general, yielding improvements also for monolingual word embedding. Our contributions are:\n\u2022 Propose a new crosslingual training method for learning vector embeddings, based only on monolingual corpora and a bilingual dictionary.\n\u2022 Evaluate several methods for combining embeddings which help in both crosslingual and monolingual evaluations.\n\u2022 Achieve uniformly excellent results which are competitive in monolingual, bilingual and crosslingual transfer settings."}, {"heading": "2 Related work", "text": "There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource. This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012). These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning\ncompositional vector representations of sentences, in order that sentences and their translations representations closely match. The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments. Nevertheless, these approaches demand large parallel corpora, which are not available for many language pairs.\nVulic\u0301 and Moens (2015) use bilingual comparable text, sourced from Wikipedia. Their approach creates a psuedo-document by forming a bag-ofwords from the lemmatized nouns in each comparable document concatenated over both languages. These pseudo-documents are then used for learning vector representations using Word2Vec. Their system, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task (we compare our method with theirs on this task.) Their approach is compelling due to its lesser resource requirements, although comparable bilingual data is scarce for many languages. Related, S\u00f8gaard et al. (2015) exploit the comparable part of Wikipedia. They represent word using Wikipedia entries which are shared for many languages.\nA bilingual dictionary is an alternative source of bilingual information. Gouws and S\u00f8gaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings. Their approach doesn\u2019t handle polysemy, as very few of the translations for each word will be valid in context. For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes. Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary. Our approach also uses a dictionary, however we use all the translations and explicitly disambiguate translations during training.\nAnother distinguishing feature on the above-cited research is the method for training embeddings. Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary. Most of the other works train multlingual models jointly, which appears to have better perfor-\nmance over cascade training (Gouws et al., 2015). For this reason we also use a form of joint training in our work."}, {"heading": "3 Word2Vec", "text": "Our model is an extension of the contextual bag of words (CBOW) model of Mikolov et al. (2013b), a method for learning vector representations of words based on their distributional contexts. Specifically, their model describes the probability of a token wi at position i using logistic regression with a factored parameterisation,\np(wi|wi\u00b1k\\i) = exp(u>wihi)\u2211 w\u2208W exp(u > whi) , (1)\nwhere hi = 12k \u2211k\nj=\u2212k;j 6=0 vwi+j is a vector encoding the context over a window of size k centred around position i, W is the vocabulary and the parameters V and U \u2208 R|W |\u00d7d are matrices referred to as the context and word embeddings. The model is trained to maximise the log-pseudo likelihood of a training corpus, however due to the high complexity of computing the denominator of (1), Mikolov et al. (2013b) propose negative sampling as an approximation, by instead learning to differentiate data from noise (negative examples). This gives rise to the following optimisation objective\u2211 i\u2208D ( log \u03c3(u>wihi)+ p\u2211 j=1 Ewj\u223cPn(w) log \u03c3(\u2212u > wjhi) ) , (2) where D is the training data and p is the number of negative examples randomly drawn from a noise distribution Pn(w)."}, {"heading": "4 Our Model", "text": "Our approach extends CBOW to model bilingual text, using two monolingual corpora and a bilingual dictionary. We believe this data condition to be less stringent than requiring parallel or comparable texts as the source of the bilingual signal. It is common for field linguists to construct a bilingual dictionary when studying a new language, as one of the first steps in the language documentation process. Translation dictionaries are a rich information source, capturing much of the lexical ambiguity in a language through translation. For example,\nAlgorithm 1 EM algorithm for selecting translation during training, where \u03b8 = (U,V) are the model parameters and \u03b7 is the learning rate.\n1: randomly initialize V, U 2: for i < Iter do 3: for i \u2208 De \u222aDf do 4: s\u2190 vwi + hi 5: w\u0304i = argmaxw\u2208dict(wi) cos(s,vw) 6: \u03b8 \u2190 \u03b8 + \u03b7 \u2202O(w\u0304i,wi,hi)\u2202\u03b8 {see (3) or (5)} 7: end for 8: end for\nthe word bank in English might mean the river bank or financial bank which corresponds to two different translations sponda and banca in Italian. If we are able to learn to select good translations, then this implicitly resolves much of the semantic ambiguity in the language, and accordingly we seek to use this idea to learn better semantic vector representations of words."}, {"heading": "4.1 Dictionary replacement", "text": "To learn bilingual relations, we use the context in one language to predict the translation of the centre word in another language. This is motivated by the fact that the context is an excellent means of disambiguating the translation for a word. Our method is closely related to Gouws and S\u00f8gaard (2015), however we only replace the middle word wi with a translation w\u0304i while keeping the context fixed. We replace each centre word with a translation on the fly during training, predicting instead p(w\u0304i|wi\u00b1k\\i) but using the same formulation as (1) albeit with an augmented U matrix to cover word types in both languages.\nThe translation w\u0304i is selected from the possible translations of wi listed in the dictionary. The problem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate translation, however to learn these embeddings we need to know the translations. We propose an EMinspired algorithm, as shown in Algorithm 1, which operates over both monolingual corpora, De and Df . The vector s is the semantic representation combining both the centre word, wi, and the con-\ntext,1 which is used to choose the best translation into the other language from the bilingual dictionary dict(wi).2 After selecting the translation, we use w\u0304i together with the context vector h to make a stochastic gradient update of the CBOW log-likelihood."}, {"heading": "4.2 Joint Training", "text": "Words and their translations should appear in very similar contexts. One way to enforce this is to jointly learn to predict both the word and its translation from its monolingual context. This gives rise to the following joint objective function,\nO = \u2211\ni\u2208De\u222aDf\n( \u03b1 log \u03c3(u>wihi)+(1\u2212\u03b1) log \u03c3(u > w\u0304ihi)\n+ p\u2211 j=1 Ewj\u223cPn(w) log \u03c3(\u2212u > wjhi) ) , (3)\nwhere \u03b1 controls the contribution of the two terms. For our experiments, we set \u03b1 = 0.5. The negative examples are drawn from combined vocabulary unigram distribution calculated from combined data De \u222aDf ."}, {"heading": "4.3 Combining Embeddings", "text": "Many vector learning methods learn two embedding spaces V and U. Usually only V is used in application. The use of U, on the other hand, is understudied (Levy and Goldberg, 2014) with the exception of Pennington et al. (2014) who use a linear combination U + V, with minor improvement over V alone.\nWe argue that with our model V is better at capturing the monolingual regularities and U is better at capturing bilingual signal. The intuition for this is as follows. Assuming that we are predicting the word finance and its Italian translation finanze from the context (money, loan, bank, debt, credit) as shown in figure 1. In V only the context word representations are updated and in U only the representations of finance, finanze and negative samples such as tree and dog are updated. CBOW learns good embeddings because each time it updates the parameters,\n1Using both embeddings gives a small improvement compared to just using context vector h alone.\n2We also experimented with using expectations over translations, as per standard EM, with slight degredation in results.\nthe words in the contexts are pushed closer to each other in the V space. Similarly, the target word wi and the translation w\u0304i are also pushed closer in the U space. This is directly related to poitwise mutual information values of each pair of word and context explained in Levy and Goldberg (2014). Thus, U is bound to better at bilingual lexicon induction task and V is better at monolingual word similarity task.\nThe simple question is, how to combine both V and U to produce a better representation. We experiment with several ways to combine V and U. First, we can follow Pennington et al. (2014) to interpolate V and U in the post-processing step. i.e.\n\u03b3V + (1\u2212 \u03b3)U (4)\nwhere \u03b3 controls the contribution of each embedding space. Second, we can also concatenate V and U instead of interpolation such that C = [V : U] where C \u2208 R|W |\u00d72d and W is the combined vocabulary from De \u222aDf .\nMoreover, we can also fuse V and U during training. For each word considered in equation 3 in U space includingWr = {wi, w\u0304i, wj} with 1 \u2264 j \u2264 p, we encourage the model to learn similar representation in both V and U by adding a regularization term to the objective function in equation (3) during training.\nO = O + \u03b4 \u2211 x\u2208Wr \u2016ux \u2212 vx\u201622 (5)\nwhere \u03b4 controls to what degree we should bind two spaces together."}, {"heading": "5 Experiment Setup", "text": "We want to test the cross-lingual property, monolingual property and the down-stream usefulness of our crosslingual word embeddings (CLWE). For the crosslingual property we adopt the bilingual lexicon\ninduction task from Vulic\u0301 and Moens (2015). For the monolingual property we adopt the word similarity task on common datasets such as WordSim353 and Rareword. To demonstrate the usefulness of our CLWE, we also evaluate on the conventional crosslingual document classification task."}, {"heading": "5.1 Monolingual Data", "text": "The monolingual data is mainly from the preprocessed Wikipedia dump from Al-Rfou et al. (2013). The data is already cleaned and tokenized. We additionally low-cased all words. Normally, the monolingual word embeddings are trained on billions of words. However, getting that much of monolingual data for a low-resource language is also challenging. That is why we only select the top 5 million sentences (around 100 million words) for each language."}, {"heading": "5.2 Dictionary", "text": "The bilingual dictionary is the only source of bilingual correspondence in our technique. We want a dictionary that covers many languages so that our approach can be applied widely to many lowresource languages. We use Panlex, a dictionary which currently covers around 1300 language varieties with about 12 million expressions. The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc. Accordingly, Panlex has high language coverage but often noisy translations. 3"}, {"heading": "6 Bilingual Lexicon Induction", "text": "Given a word in a source language, the bilingual lexicon induction (BLI) task is to predict its translation in the target language. Vulic\u0301 and Moens (2015) proposed this task to test crosslingual word embeddings. The difficulty of this is that it is evaluated us-\n3We also experimented with a growing crow-sourced dictionary from Wiktionary. Our initial observation is that the translation quality is better but lower-coverage. For example, for Engish - Italian dictionary, Panlex and Wiktionary has the coverage of 42.1% and 16.8% respectively for the top 100k most frequent English words from Wikipedia. The average number of translations are 5.2 and 1.9 respectively. We observed similar trend using Panlex and Wiktionary dictionary in our model. However, using Panlex results in much better performance. We can run the model on the combined dictionary from both Panlex and Wiktionary but we leave it for future work.\ning recall at one where each term has only a single gold translation. The model must be very discriminative in order to score well.\nWe build the CLWE for 3 language pairs: it - en, es - en and nl - en, using similar parameters setting with Vulic\u0301 and Moens (2015).4 The remaining tunable parameters in our system are \u03b4 from Equation (5), and the choice of algorithm for combining embeddings (see \u00a78).\nQualitative evaluation We jointly train the model to predict bothwi and the translation w\u0304i, combine V and U during training with regularization sensitivity \u03b4 = 0.01 and use U as the output for each language pair. Table 1 shows the top 10 closest words in both source and target languages according to cosine similarity. Note that the model correctly identifies the translation in English, and the top 10 words in both source and target languages are highly related. This qualitative evaluation initially demonstrates the ability of our CLWE to capture both the bilingual and monolingual relationship.\nQuantitative evaluation Table 2 shows our results compared with prior work. We reimplement Gouws and S\u00f8gaard (2015) using Panlex and Wiktionary dictionaries. The result with Panlex is substantially worse than with Wiktionary. This confirms our hypothesis in \u00a72. That is the context might be very biased if we just randomly replace the training data with the translation especially with noisy\n4Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e\u22124, embedding dimension d = 200, window size cs = 48 and run for 15 epochs.\ndictionary such as Panlex. Our model when randomly picking the translation is similar to Gouws and S\u00f8gaard (2015), using the Panlex dictionary. The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word. For a high coverage yet noisy dictionary such as Panlex, our approach gives better average score. Our non-joint model with EM to select the translation5, out-performs just randomly select the translation by a significant margin.\nOur joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for Dutch. We use equation (5) to combine both context embeddings V and word embeddings U for all three language pairs. This modification during training substantially improves the performance. More importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models.\nOur combined model out-performed previous approaches by a large margin. Vulic\u0301 and Moens (2015) used bilingual comparable data, but this might be hard to obtain for some language pairs. Their performance on Dutch is poor because their comparable data between English and Dutch is small. Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during training. These tools might not be available\n5Optimizing equation (3) with \u03b1 = 0.\nfor many languages. For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation. Table 2 (+lemmatization) shows some improvements but minor. It demonstrates that our model is already good at disambiguating morphology. For example, the top 2 translations for Spanish word lenguas in English are languages and language which correctly prefer the plural translation."}, {"heading": "7 Monolingual Word Similarity", "text": "Now we consider the efficacy of our CLWE on monolingual word similarity. Our experiment setting is similar with Luong et al. (2015). We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015). Each of those datasets contain many tuples (w1, w2,score) where score is given by annotators showing the semantic similarity betweenw1 and w2. The system must give the score correlated with human judgment.\nWe train the model as described in equation (5), which is exactly the same model as combine embeddings in Table 2. Since the evaluation involves German and English word similarity, we train the CLWE for English - German pair. Table 3 shows the performance of our combined model compared with several baselines. Our combined model out-\nperformed both Luong et al. (2015) and Gouws and S\u00f8gaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.\nWe also train the monolingual CBOW model with the same parameter settings on the monolingual data for each language. Surprisingly, our combined model performs better than the monolingual CBOW baseline which makes our result closer to the monolingual state-of-the-art on each different dataset. However, the best monolingual methods use massive monolingual data (Shazeer et al., 2016), WordNet and output of commercial search engines (Yih and Qazvinian, 2012).\nNext we explain the gain of our combined model compared with the monolingual CBOW model. First, we compare the combined model with the joint-model w.r.t. monolingual CBOW model (Table 3). It shows that the improvement seems mostly come from combining V and U. If we apply the combining algorithm to the monolingual CBOW model (CBOW + combine), we also observe an improvement. Clearly most of the improvement is from combining V and U, however our V and U are much more complementary. The other improvements can be explained by the observation that a dictionary can improve monolingual accuracy through\n6use Panlex dictionary\nlinking synonyms (Faruqui and Dyer, 2014). For example, since plane, airplane and aircraft have the same Italian translation aereo, the model will encourage those words to be closer in the embedding space."}, {"heading": "8 Model selection", "text": "Combining context embeddings and word embeddings results in an improvement in both monolingual similarity and bilingual lexicon induction. In \u00a74.3, we introduce several combination methods including post-processing (interpolation and concatenation) and during training (regularization). In this section, we justify our parameter and model choices.\nWe use English - Italian pair for tuning purposes, considering the value of \u03b3 in equation 4. Figure 2 shows the performances using different values of \u03b3. The two extremes where \u03b3 = 0 and \u03b3 = 1 corresponds to no interpolation where we just use U or V respectively. As \u03b3 increases, the performance on WS-En increases yet BLI decreases. These results confirm our hypothesis in \u00a74.3 that U is better at capturing bilingual relation and V is better at capturing monolingual relation. As a compromise, we choose \u03b3 = 0.5 in our experiments. Similarly, we tune the regularization sensitivity \u03b4 in equation (5) which combines embeddings space during training. We test \u03b4 = 10\u2212n with n = {0, 1, 2, 3, 4} and using V, U or the interpolation of both V+U2 as the learned embeddings, evaluated on the same BLI and WS-En. We select \u03b4 = 0.01.\nTable 4 shows the performance with and with-\nand \u03b4 = 0.01 for regularization with the choice of V, U or combination of bothV+U 2 for the output. The best scores are bold.\nout using combining algorithms mentioned in \u00a74.3. As the compromise between both monolingual and crosslingual tasks, we choose regularization + U as the combination algorithm. All in all, we apply the regularization algorithm for combining V and U with \u03b4 = 0.01 and U as the output for all language pairs without further tuning."}, {"heading": "9 Crosslingual Document Classification", "text": "In this section, we evaluate our CLWE on a downstream crosslingual document classification (CLDC) task. In this task, the document classifier is trained on a source language and then applied directly to classify a document in the target language. This is convenient for a target low-resource language where we do not have document annotations. The experimental setup is from Klementiev et al. (2012).7 The train and test data are from Reuter RCV1/RCV2 corpus (Lewis et al., 2004).\nThe documents are represented as the bag of word embeddings weighted by tf.idf. A multi-class classifier is trained using the average perceptron algorithm on 1000 documents in the source language and tested on 5000 documents in the target language. We use the CLWE, such that the document representation in the target language embeddings is in the same space with the source language.\nWe build the en-de CLWE using combined models as described in equation (5). Following\n7The data split and code are kindly provided by the authors.\nprior work, we also use monolingual data from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).8\nTable 5 shows the CLDC results for various CLWE. Despite its simplicity, our model achieves competitive performance. Note that aside from our model, all other models in Table 5 use a large bitext (Europarl) which may not exist for many lowresource languages, limiting their applicability."}, {"heading": "10 Conclusion", "text": "Previous CLWE methods often impose high resource requirements yet have low accuracy. We introduce a simple framework based on a large noisy dictionary. We model polysemy using EM translation selection during training to learn bilingual correspondences from monolingual corpora. Our algorithm allows to train on massive amount of monolingual data efficiently, representing monolingual and bilingual properties of language. This allows us to achieve state-of-the-art performance on bilingual lexicon induction task, competitive result on monolingual word similarity and crosslingual document classification task. Our combination techniques during training, especially using regularization, are highly effective and could be used to improve monolingual word embeddings.\n8We randomly sample documents in RCV1 and RCV2 corpus and selected around 85k documents to form 400k monolingual sentences for both English and German. For each document, we perform basic processing including: lower-case, remove tags and tokenize. These monolingual data are then concatenated with the monolingual data from Wikipedia to form the final training data."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": null, "citeRegEx": "P et al\\.,? \\Q2014\\E", "shortCiteRegEx": "P et al\\.", "year": 2014}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Duong et al.2015] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard2015] Stephan Gouws", "Anders S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "Proceedings of the 32nd International Conference on Machine Learn-", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Panlex: Building a resource for panlingual lexical translation", "author": ["Jonathan Pool", "Susan Colowick"], "venue": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation", "citeRegEx": "Kamholz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kamholz et al\\.", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of COLING", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning bilingual word representations by marginalizing alignments", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Ko\u010disk\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ko\u010disk\u00fd et al\\.", "year": 2014}, {"title": "Neural word embedding as a factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rcv1: A new", "author": ["Lewis et al.2004] David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In NAACL Workshop on Vector Space Modeling for NLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation", "author": ["Resnik", "Yarowsky1999] Philip Resnik", "David Yarowsky"], "venue": null, "citeRegEx": "Resnik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 1999}, {"title": "Improvements in part-of-speech tagging with an application to german", "author": ["Helmut Schmid"], "venue": "Proceedings of the ACL SIGDAT-Workshop,", "citeRegEx": "Schmid.,? \\Q1995\\E", "shortCiteRegEx": "Schmid.", "year": 1995}, {"title": "Swivel: Improving embeddings by noticing what\u2019s missing. CoRR, abs/1602.02215", "author": ["Shazeer et al.2016] Noam Shazeer", "Ryan Doherty", "Colin Evans", "Chris Waterson"], "venue": null, "citeRegEx": "Shazeer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Inverted indexing for cross-lingual nlp", "author": ["Bohnet", "Anders Johannsen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long", "citeRegEx": "Bohnet and Johannsen.,? 2015", "shortCiteRegEx": "Bohnet and Johannsen.", "year": 2015}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Bilingual word embeddings from nonparallel document-aligned data applied to bilingual lexicon induction", "author": ["Vuli\u0107", "Moens2015] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, chapter Distributed Word Representation Learning for CrossLingual Dependency Parsing, pages 119\u2013129", "author": ["Xiao", "Guo2014] Min Xiao", "Yuhong Guo"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora", "author": ["Yarowsky", "Ngai2001] David Yarowsky", "Grace Ngai"], "venue": "In Proceedings of the Second Meeting of the North American Chapter of the Association", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Measuring word relatedness using heterogeneous vector space models", "author": ["Yih", "Qazvinian2012] Wen-tau Yih", "Vahed Qazvinian"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Yih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 5, "context": ", 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 1, "context": ", 2015), machine translation (Bahdanau et al., 2014).", "startOffset": 29, "endOffset": 52}, {"referenceID": 25, "context": "A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012; Duong et al., 2015).", "startOffset": 104, "endOffset": 195}, {"referenceID": 4, "context": "A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012; Duong et al., 2015).", "startOffset": 104, "endOffset": 195}, {"referenceID": 11, "context": "Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012).", "startOffset": 152, "endOffset": 177}, {"referenceID": 12, "context": "Moreover, many prior work (Chandar A P et al., 2014; Ko\u010disk\u00fd et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages.", "startOffset": 26, "endOffset": 74}, {"referenceID": 16, "context": "To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015).", "startOffset": 114, "endOffset": 134}, {"referenceID": 2, "context": "Moreover, many prior work (Chandar A P et al., 2014; Ko\u010disk\u00fd et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages. S\u00f8gaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods.", "startOffset": 37, "endOffset": 190}, {"referenceID": 10, "context": "Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal.", "startOffset": 50, "endOffset": 72}, {"referenceID": 16, "context": "This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012).", "startOffset": 203, "endOffset": 248}, {"referenceID": 11, "context": "This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012).", "startOffset": 203, "endOffset": 248}, {"referenceID": 7, "context": "These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning compositional vector representations of sentences, in order that sentences and their translations representations closely match.", "startOffset": 136, "endOffset": 209}, {"referenceID": 17, "context": "Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.", "startOffset": 0, "endOffset": 44}, {"referenceID": 17, "context": "Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.", "startOffset": 0, "endOffset": 72}, {"referenceID": 17, "context": "Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary.", "startOffset": 0, "endOffset": 51}, {"referenceID": 7, "context": "mance over cascade training (Gouws et al., 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 17, "context": "Our model is an extension of the contextual bag of words (CBOW) model of Mikolov et al. (2013b), a method for learning vector representations of words based on their distributional contexts.", "startOffset": 73, "endOffset": 96}, {"referenceID": 17, "context": "The model is trained to maximise the log-pseudo likelihood of a training corpus, however due to the high complexity of computing the denominator of (1), Mikolov et al. (2013b) propose negative sampling as an approximation, by instead learning to differentiate data from noise (negative examples).", "startOffset": 153, "endOffset": 176}, {"referenceID": 19, "context": "The use of U, on the other hand, is understudied (Levy and Goldberg, 2014) with the exception of Pennington et al. (2014) who use a linear combination U + V, with minor improvement over V alone.", "startOffset": 97, "endOffset": 122}, {"referenceID": 19, "context": "First, we can follow Pennington et al. (2014) to interpolate V and U in the post-processing step.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": "The monolingual data is mainly from the preprocessed Wikipedia dump from Al-Rfou et al. (2013). The data is already cleaned and tokenized.", "startOffset": 73, "endOffset": 95}, {"referenceID": 7, "context": "9 BilBOWA: Gouws et al. (2015) 51.", "startOffset": 11, "endOffset": 31}, {"referenceID": 7, "context": "9 BilBOWA: Gouws et al. (2015) 51.6 - 55.7 - 57.5 - 54.9 Vuli\u0107 and Moens (2015) 68.", "startOffset": 11, "endOffset": 80}, {"referenceID": 21, "context": "For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation.", "startOffset": 71, "endOffset": 85}, {"referenceID": 15, "context": "We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015).", "startOffset": 128, "endOffset": 194}, {"referenceID": 16, "context": "We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015).", "startOffset": 128, "endOffset": 194}, {"referenceID": 15, "context": "Our experiment setting is similar with Luong et al. (2015). We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 10, "context": "B as el in es Klementiev et al. (2012) 23.", "startOffset": 14, "endOffset": 39}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.", "startOffset": 12, "endOffset": 28}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.6 39.8 20.5 Hermann and Blunsom (2014) 28.", "startOffset": 12, "endOffset": 70}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.6 39.8 20.5 Hermann and Blunsom (2014) 28.3 19.8 13.6 Luong et al. (2015) 47.", "startOffset": 12, "endOffset": 105}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.6 39.8 20.5 Hermann and Blunsom (2014) 28.3 19.8 13.6 Luong et al. (2015) 47.4 49.3 25.3 Gouws and S\u00f8gaard (2015) 67.", "startOffset": 12, "endOffset": 145}, {"referenceID": 15, "context": "performed both Luong et al. (2015) and Gouws and S\u00f8gaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.", "startOffset": 15, "endOffset": 35}, {"referenceID": 15, "context": "performed both Luong et al. (2015) and Gouws and S\u00f8gaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.", "startOffset": 15, "endOffset": 64}, {"referenceID": 22, "context": "However, the best monolingual methods use massive monolingual data (Shazeer et al., 2016), WordNet and output of commercial search engines (Yih and Qazvinian, 2012).", "startOffset": 67, "endOffset": 89}, {"referenceID": 14, "context": "7 The train and test data are from Reuter RCV1/RCV2 corpus (Lewis et al., 2004).", "startOffset": 59, "endOffset": 79}, {"referenceID": 11, "context": "The experimental setup is from Klementiev et al. (2012).7 The train and test data are from Reuter RCV1/RCV2 corpus (Lewis et al.", "startOffset": 31, "endOffset": 56}, {"referenceID": 11, "context": "8 Klementiev et al. (2012) 77.", "startOffset": 2, "endOffset": 27}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 91.", "startOffset": 12, "endOffset": 28}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 91.8 74.2 83.0 Hermann and Blunsom (2014) 86.", "startOffset": 12, "endOffset": 70}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 91.8 74.2 83.0 Hermann and Blunsom (2014) 86.4 74.7 80.6 Luong et al. (2015) 88.", "startOffset": 12, "endOffset": 105}, {"referenceID": 11, "context": "prior work, we also use monolingual data from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).", "startOffset": 67, "endOffset": 138}, {"referenceID": 7, "context": "prior work, we also use monolingual data from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).", "startOffset": 67, "endOffset": 138}], "year": 2016, "abstractText": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.", "creator": "LaTeX with hyperref package"}}}