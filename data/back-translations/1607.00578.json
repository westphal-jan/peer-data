{"id": "1607.00578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2016", "title": "Context-Dependent Word Representation for Neural Machine Translation", "abstract": "First, we observe a potential weakness in the continuous vector representation of symbols in neural machine translation. That is, the continuous vector representation or a vector for word embedding of a symbol encodes several dimensions of similarity, corresponding to the encoding of more than one meaning of the word. It follows that the encoder and decoder in neural machine translation must devote a significant portion of its capacity to the unique representation of source and target words based on the context defined by a source sentence. Based on this observation, we propose to contextualize the word with vectors that use a nonlinear word-string representation of the source set. Furthermore, we propose to represent special symbols (such as numbers, idioms, and acronyms) with typed symbols to facilitate the translation of those words that are not well suited to be translated via continuous vectors.", "histories": [["v1", "Sun, 3 Jul 2016 02:18:16 GMT  (139kb,D)", "http://arxiv.org/abs/1607.00578v1", "13 pages, 2 figures"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["heeyoul choi", "kyunghyun cho", "yoshua bengio"], "accepted": false, "id": "1607.00578"}, "pdf": {"name": "1607.00578.pdf", "metadata": {"source": "CRF", "title": "Context-Dependent Word Representation for Neural Machine Translation", "authors": ["Heeyoul Choi", "Kyunghyun Cho"], "emails": ["heeyoul@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": null, "text": "We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly."}, {"heading": "1 Introduction", "text": "Neural machine translation is a recently proposed paradigm in machine translation, which is often entirely built as a single neural network Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al. (2015). The neural machine translation system, which often consists of an encoder and decoder, projects and manipulates a source sequence of discrete linguistic symbols (source sentence) in a continuous vector space, and decodes a target sequence of symbols (target sentence or translation.) This is contrary to the conventional machine translation systems, such as phrase-based statistical machine translation Koehn et al. (2003), which work directly at the discrete symbol level.\nIn more detail, the first step of any neural machine translation system is to convert each atomic symbol into a corresponding continuous vector, which is often called as a word embedding. This step is done for each source word independently of the other words and results in a source sequence of word embeddings. The encoder network, which is often implemented as a recurrent neural network, encodes this source sentence either into a single context vector Sutskever et al. (2014); Cho et al. (2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al. (2015).\nar X\niv :1\n60 7.\n00 57\n8v 1\n[ cs\n.C L\n] 3\nThe decoder network, again a recurrent neural network, generates a translation word-by-word while being conditioned on the context representation of the source sentence. At each step of generation in the decoder, the internal hidden state of the decoder is updated first. The dot product between this hidden state and the output word embedding vector of each word in the target vocabulary is computed and normalized across all the target words, resulting in a probability distribution over the target vocabulary. A target word is selected based on this distribution, and the whole process is recursively repeated until the end-of-sequence symbol is generated.\nAmong different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al. (2015). Much of these recent improvements have been made by tackling, e.g., the attention mechanism (which is central to the attention-based neural translation system) and the computational issues arising from having a large target vocabulary.\nUnlike these recent works, we focus on source- and target-side word embedding vectors in this paper. More specifically, we first notice that the transformation from and to high-dimensional word embedding vectors is done for each word largely independent of each other. We conjecture that only a few axes in this high-dimensional space are relevant given a source sentence and that we can remove much of the ambiguity in the choice of words by restricting, or turning off, most of the irrelevant dimensions. We propose to achieve this automated way to turn off some dimensions of word embeddings by contextualizing a word embedding vector.\nIn addition to the proposed contextualization of both source and target word embedding vectors, we propose to extend the unknown token replacement technique proposed in Luong et al. (2015b) to multiple token types. This extension, to which we refer as symbolization, introduces multiple meta-tokens such as the number token and the proper name token in addition to the unknownword token. This symbolization effectively remaps rare tokens into more frequent meta-tokens and thereby results in improved translation quality.\nWe extensively evaluate the proposed contextualization and symbolization on two language pairs\u2013 En-Fr and En-De\u2013 with the attention-based neural machine translation model. The experiments reveal that the contextualization and symbolization each improves the translation quality by 2 BLEU scores, and together by 4 BLEU points on En-Fr. On En-De, they result in 1\u20132 BLEU score improvements each, and together 2.5 BLEU score increase."}, {"heading": "2 Background: Neural Machine Translation", "text": "In this section, we give a brief overview of neural machine translation. More specifically, we describe the attention-based neural machine translation Bahdanau et al. (2015) which will be used in the experiments later. However, we note that the proposed contextualization and symbolization techniques are generally applicable to any other type of neural machine translation systems such as the sequence-to-sequence model Sutskever et al. (2014).\nThe attention-based neural machine translation system computes a conditional distribution over translations given a source sentence X = (wx1 , w x 2 , . . . , w x T ):\np(Y = (wy1 , w y 2 , . . . , w y T \u2032)|X).\nThis is done by a neural network that consists of an encoder, a decoder and the attention mechanism.\nThe encoder is often implemented as a bidirectional recurrent neural network that reads the source sentence word-by-word. Before being read by the encoder, each source word wxt \u2208 V is projected onto a continuous vector space:\nxt = E x1(wxt ), (1)\nwhere 1(wxt ) is a one-hot vector defined as\n1(wxt )j = { 1, if j = wxt 0, otherwise . (2)\nEx \u2208 RE\u00d7|V | is a source word embedding matrix, where E and |V | are the word embedding dimension and the vocabulary size, respectively.\nThe resulting sequence of word embedding vectors is then read by the bidirectional encoder recurrent network which consists of forward and reverse recurrent networks. The forward recurrent network reads the sequence in the left-to-right order:\n\u2212\u2192 h t = \u2212\u2192 \u03c6 ( \u2212\u2192 h t\u22121,xt),\nwhile the reverse network reads it right-to-left: \u2190\u2212 h t = \u2190\u2212 \u03c6 ( \u2190\u2212 h t+1,xt), where the initial hidden states \u2212\u2192 h 0 and \u2190\u2212 h T+1 are initialized as all-zero vectors. The hidden states from the forward and reverse recurrent networks are concatenated at each time step t to form an annotation vector h: ht = [\u2212\u2192 h t; \u2190\u2212 h t ] . This concatenation results in a context C that is a tuple of annotation vectors: C = {h1,h2, . . . ,hT } . The recurrent activation functions \u2212\u2192 \u03c6 and \u2190\u2212 \u03c6 are in most cases either long short-term memory units (LSTM, Hochreiter and Schmidhuber (1997)) or gated recurrent units (GRU, Cho et al. (2014a).)\nThe decoder consists of two sub-components\u2013a recurrent network and the attention mechanism. The recurrent network in the decoder is a unidirectional language model, which computes the conditional distribution over the next target word given all the previous target words and the source sentence:\np(wyt\u2032 |w y <t\u2032 , X)\nThe decoder recurrent network maintains an internal hidden state zt\u2032 . At each time step t\u2032, it first uses the attention mechanism to select, or weight, the annotation vectors in the context tuple C. The attention mechanism, which is a feedforward neural network, takes as input both the previous decoder hidden state, and one of the annotation vectors, and returns a relevant score et\u2032,t:\net\u2032,t = fATT(zt\u2032\u22121,ht).\nThese relevance scores are normalized to be positive and sum to 1:1\n\u03b1t\u2032,t = exp(et\u2032,t)\u2211T\nk=1 exp(et\u2032,k) . (3)\nWe use the normalized scores to compute the weighted sum of the annotation vectors\nct\u2032 = T\u2211 t=1 \u03b1t\u2032,tht\nwhich will be used by the decoder recurrent network to update its own hidden state by\nzt\u2032 = \u03c6z(zt\u2032\u22121,yt\u2032\u22121, ct\u2032).\nSimilarly to the encoder, \u03c6z is implemented as either an LSTM or GRU. yt\u2032\u22121 is a target-side word embedding vector computed by\nyt\u2032\u22121 = E y1(wyt\u2032\u22121), (4)\nsimilarly to Eq. (1).\nThe probability of each word i in the target vocabulary V \u2032 is computed by\np(wyt\u2032 = i|w y <t\u2032 , X) \u221d exp (E y i zt\u2032 + ci) ,\nwhere Eyi is the i-th row vector of the target word embedding matrix.\nThe neural machine translation model is usually trained to maximize the log-probability of the correct translation given a source sentence using a large training parallel corpus. This is done by stochastic gradient descent, where the gradient of the log-likelihood is efficiently computed by the backpropagation algorithm.\n1 For more variants of the attention mechanism, we refer the readers to Luong et al. (2015a)."}, {"heading": "3 Contextualized Word Embedding Vectors", "text": ""}, {"heading": "3.1 Word Embedding Vectors", "text": "One-hot representation of a word in Eq. (2) is unique in the sense that each and every word in a vocabulary is equally distant from every other word. This implies that the words lose all the information relative to the other words, when represented as a one-hot vector. The meaning of a word, relative to those of the other words in the vocabulary, is thus learned through the associated word embedding vector (Eqs. (1)\u2013(4)) during training. In other words, training brings similar words close to each other in the word embedding space and dissimilar words far away from each other.\nThis phenomenon of similarity learning via word embedding vectors has been observed in many different natural language processing tasks done with neural networks. Already in 1991, Miikkulainen and Dyer Miikkulainen and Dyer (1991) noticed that training a neural network with one-hot vectors as its input learns the word embedding vectors that \u201ccode properties of the input elements that are most crucial to the task.\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences. The interest in word embedding vectors, or distributed representation of words, was fueled by the earlier observations that these unsupervised word embedding vectors can be used to improve supervised natural language tasks greatly Collobert et al. (2011); Turian et al. (2010)."}, {"heading": "3.2 Multiple Dimensions of Similarity", "text": "An important characteristic of the high-dimensional word embedding vectors is that it encodes multiple dimensions of similarities. This is necessary in order for a neural network to cope with polysemy. We can qualitatively check this phenomenon of multiple dimensions of similarities by inspecting a local chart of the manifold on which the word embedding vectors reside.\nFor any word x\u2032 under inspection, we find the N \u2212 1 nearest neighbours { x1, . . . ,xN\u22121 } \u2282 Ex in\nthe word embedding matrix. TheN word embedding vectors { x\u2032,x1, . . . ,xN\u22121 } now characterize a local chart centered at x\u2032, and we use principal component analysis (PCA) to find the corresponding lower-dimensional Euclidean space. In this Euclidean space, we can inspect the nearest neighbours along each coordinate.2 In Table 1, we show two such examples using the word embedding vectors trained as a part of the continuous-bag-of-word (CBoW) network Mikolov et al. (2013).3 These examples clearly show that each word embedding vector encodes more than one notions of similarities. A similar behaviour can only be observed with multi-map t-SNE Van der Maaten and Hinton (2012).\n2 The code for this analysis is available publicly at https://github.com/kyunghyuncho/ WordVectorManifold.\n3 We used the word embedding vectors provided as a part of Hill et al. (2015)."}, {"heading": "3.3 Contextualized Word Embeddings", "text": "The fact that each word embedding vector represents multiple dimensions of similarity implies that a subsequent part of a neural network needs to disambiguate the word based on the context in which the word was used. In the case of neural machine translation, we should consider source and target word embedding vectors separately. The encoder, which is implemented as a bidirectional recurrent network, can disambiguate it by using all the other words in a source sentence. On the other hand, the decoder can exploit both the previous words in a target sentence as well as all the source words.\nAlready pointed out in 1949 by Weaver Weaver (1949), much of ambiguity in word meaning can be resolved by considering surrounding words. The consequence of this in neural machine translation is that the recurrent network, either in the encoder or decoder, needs to remember all the previous words until the word in question in order to decide its meaning disambiguously. In other words, the encoder and decoder must sacrifice their capacity in disambiguating the words. This is undesirable, as what we truly want the encoder and decoder to do is to capture the higher-level compositional structures of a sentence that are necessary for translation.\nIn order to address this issue and to reduce the burden from the recurrent networks in a neural machine translation system, we propose to contextualize the word embedding vectors before being fed to the recurrent networks as shown in Fig. 1. This contextualization disambiguates the word\u2019s meaning by masking out some dimensions of the word embedding vectors based on the context.\nLet us describe in detail the proposed contextualization. First, we define the context cx as a representation of the unordered set of all source words and compute it as the average of the nonlinearly transformed source word embedding vectors, i.e.,\ncx = 1\nT T\u2211 t=1 NN\u03b8(xt), (5)\nwhere NN\u03b8 : RE \u2192 RC is a feedforward neural network parametrized by \u03b8. Then, we compute a context mask from cx for each word embedding vector before it is fed into an input or output recurrent network. This is simply done by\nxt \u2190 xt \u03c3(W xcx + bx), yt \u2190 yt \u03c3(W ycx + by),\nwhere is an element-wise multiplication and \u03c3 : RC \u2192 [0, 1] is an element-wise sigmoid function and transforms the context to a binary-like mask, with W x and W y (weight matrices), bx and by (bias vectors) being additional parameters.\nRecall that we introduced earlier in Sec. 3.2 a complicated procedure based on estimating a local chart on a manifold using PCA. Compared to that procedure, our proposal for contextualization may seem simple and perhaps insufficient. This is however not a serious issue due to two reasons. First, the context mask cx is extracted via a highly nonlinear function in Eq. (5) which can learn to effectively project axes from the Euclidean space, corresponding to a local chart, back to the coordinates on the manifold. Second, as both the word embedding vectors are learned together, we expect the dimensions of the word embedding vector to disentangle multiple dimensions of similarity automatically to maximize the use of the proposed contextualization routine. Although both of these are not easily verifiable, we show later in the experiment that the proposed contextualization indeed improves the translation quality."}, {"heading": "4 Symbolization", "text": ""}, {"heading": "4.1 Proper Nouns, Digits and Rare Words", "text": "The use of continuous vectors as an intermediate representation of source and target sentences in neural machine translation greatly improves generalization of machine translation by avoiding the issue of data sparsity (see, e.g., Sec. 5.2.1 in Cho (2015).) This, however, brings in some unnecessary complications as well. One such complication is in handling proper nouns, digits and rare words.\nFirst, the rare words, which often occur up to a handful of times in a whole training corpus, are problematic, because their word embedding vectors cannot be well estimated during training. It is hence a usual practice to map all those rare words whose frequencies are under a predefined threshold to a single token representing an unknown word. This approach has been used without much problem in language modeling, where the objective is to score a given sentence. However, it is unacceptable for a machine translation system to generate these unknown symbols when generating a translation.\nThe second issue is with digits. Clearly the meaning of any digit is not defined by its context, and there is a clear, single meaning associated with each digit, that is the number denoted by it. The conversion to and from a continuous vector space of a digit may therefore introduce unnecessary noise, resulting in an incorrect translation.\nProper nouns often exhibit both of the above problems. They are often rare, except for the proper nouns of a famous person, organization, object or location. Furthermore, the transformation between a proper noun and a high-dimensional continuous vector likely introduces noise that is absolutely not necessary, as the meaning of such a proper noun is fixed to a single entity.\nThese issues and their underlying causes suggest that it would be beneficial to treat these three cases separately from all the other words. This is precisely what we propose to do, and we will describe it in more detail below."}, {"heading": "4.2 Previous Approaches", "text": "In most of these special cases, a source word is directly copied to a target sentence either as it is or after a simple transformation via dictionary lookup. This property suggests a simple algorithm that can be run outside neural machine translation. Assuming that there exists an alignment between a rare source word and a word or placeholder in the target sentence, we can look up the rare word in a pre-built dictionary and replace the target-side placeholder with the queried word which can be the source word itself or its appropriate translation or transliteration.\nBased on this observation, Jean et al. Jean et al. (2015a) earlier proposed a number of heuristics for handling rare words with the attention-based neural machine translation. In their approach, the attention weights \u03b1t\u2032,t from Eq. (3) are used to determine the source word aligned to each of the unknown tokens in the generated translation, i.e., argmaxt \u03b1t\u2032,t. The source word determined by this mechanism is translated word-wise by looking up a pre-built dictionary, and the corresponding unknown token in the translation is replaced with the result of the look-up.\nSimultaneously, Luong et al. Luong et al. (2015b) proposed another mechanism that does not require the attention mechanism. They used an external alignment mechanism, such as IBM Model 2 Brown et al. (1993), to find the alignment between the source and target words in the training corpus. This alignment is used to assign multiple, numbered unknown tokens for rare words so that the\ncorrespondences between these tokens in the source and target sides are maintained. For instance, the first unknown token in the source side and its corresponding target word will be replaced with \u3008UNK1\u3009, regardless of the target word\u2019s position in the translation. During test time, given a source sentence and a generated translation, each of the unknown tokens in the translation will be replaced by querying the corresponding source-side word in a pre-built dictionary.\nBoth of these approaches have been highly effective in improving the translation quality. For instance, Jean et al. Jean et al. (2015a) reported +3 and +2.5 BLEU improvement with the simple replacement technique based on the attention mechanism on En-Fr and En-De, respectively. Similarly, Luong et al. Luong et al. (2015b) reported +1.6\u2013+2.8 BLEU improvement on En-Fr with their approach based on the positional unknown tokens \u3008UNKn\u3009."}, {"heading": "4.3 Symbolization of Proper Nouns, Digits and Rare Words", "text": "In this paper, we extend the approach by Luong et al. Luong et al. (2015b), which is based on the positional unknown tokens, to include multiple positional special tokens. Instead of a single special token \u3008UNK\u3009n, we propose to use three special tokens:\n1. \u3008N\u3009n: Digit\n2. \u3008S\u3009n: Proper noun\n3. \u3008C\u3009n: Acronym\nSee Table 2 for examples of symbolization.\nDigit \u3008N\u3009n The basic idea is to replace any consecutive digits (without blank spaces in-between) appearing both in the source and target sentences with the special symbol \u3008N\u3009n, where n denotes its order in the source sentence. In order to better address the cases of a number followed immediately by its unit, we separate the unit or any non-digit characters from consecutive digits (e.g., \u2018137Kg\u2019 \u2192 \u2018137\u2019, \u2018Kg\u2019.) Also, we normalize the variations in writing a long digit such as decimal marks (\u2018,\u2019 vs. \u2018.\u2019) and digit grouping deliminators (\u2018,\u2019 vs. \u2018.\u2019 vs. \u2018 \u2019,) when matching digits in the source and target sides.\nProper Noun \u3008S\u3009n In many of the European languages, which we mainly consider in this paper, capitalization is used to indicate that a word is a proper noun. Therefore, we replace a maximal consecutive phrase of more than one capitalized words appearing on both the source and target side, with the special symbol \u3008S\u3009n. Similarly to \u3008N\u3009n, n denotes its order in the source sentence. We consider a phrase, as many proper nouns are often proper phrases (e.g., \u2018New York\u2019.) When spotting these proper noun phrases, we do not consider non-capitalized functional words such as \u2018of\u2019 in order to properly handle noun phrases in the form of \u2018X of Y\u2019 (e.g., \u2018World of Warcraft\u2019.)\nAcronym \u3008C\u3009n Most of the acronyms can be handled similarly to proper nouns, except that there are cases where acronyms of a single entity differ across languages. For instance, the acronym of \u2018International Monetary Fund\u2019 is \u2018IMF\u2019 in English but \u2018FMI\u2019 in French. We notice that it is rare to have more than one such cases in a single sentence pair. We first replace all the matching acronym pairs. If there is only one all-capital word left in each of the source and target sentences, we consider them a match and replace it with \u3008C\u3009n.\nRule Dictionary We construct a mapping rule for each matching pair that was replaced by one of the special symbols. This rule dictionary is used during test to replace the generated special symbols in a translation. Although it is certainly possible to incorporate external rules into this dictionary, we do not test it to avoid including any external resource when evaluating the proposed symbolization technique.\nNote that this approach of symbolization has been used to a certain extent in more conventional statistical machine translation and language modelling. For more discussion, we refer the reader to Sec. 7.4 of Koehn (2010). This approach however has not been adopted widely in neural machine translation yet."}, {"heading": "5 Experimental Settings", "text": ""}, {"heading": "5.1 Tasks and Corpora", "text": "We evaluate the neural machine translation systems with and without the proposed methods on two tasks of English-to-French (En-Fr) and English-to-German (En-De) translation. We use all the parallel corpora made publicly available via WMT\u201914.4 The En-Fr corpora are cleaned following the procedure in Cho et al. (2014b), and after tokenization, has approximately 250M words (English side.) The En-De corpora are prepared following the procedure in Jean et al. (2015a), resulting in approximately 100M words (English side.)\nWe use newstest-2014 and newstest-2013 as the development sets and newsdiscusstest2015 and newstest2015 as the test sets for En-Fr and En-De, respectively."}, {"heading": "5.2 Vocabulary Preparation", "text": "Both of the corpora go through a minimal set of preprocessing. First, we tokenize them using the script provided as a part of Moses.5 As a comparison, we evaluate a setting where byte pair encoding (BPE) is used, as a replacement of the proposed symbolization, to extract sub-word symbols Sennrich et al. (2015b). This has been found to be an effective and efficient approach to handling the issue of a large target vocabulary Sennrich et al. (2015a); Firat et al. (2016); Chung et al. (2016) and is known to be able to transliterate rare, proper nouns up to a certain extent Sennrich et al. (2015b). In all the cases, we use up to top-30k most frequent symbols (either tokenized words or BPE-based sub-words.)\nThe proposed symbolization is applied right after the initial tokenization. In the case of using BPE, the BPE segmentation is done on the symbolized corpus, and in the test time, the BPE desegmentation is followed by the de-symbolization.\n4 http://www.statmt.org/wmt14/ 5 https://github.com/moses-smt/mosesdecoder\nAside the BPE segmentation, we noticed that the number of unique tokens on each corpus greatly decreases after the proposed symbolization is applied. This improves the coverage by the 30k-wordlarge vocabulary as much as 3 percentage points. See Table 3 for detailed statistics."}, {"heading": "5.3 Training", "text": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU). Further, instead of Adadelta Zeiler (2012), we use Adam Kingma and Ba (2014) for adaptive learning rate adjustment with gradient clipping (threshold at 1.) As the proposed contextualization and symbolization do not alter the internals of the neural machine translation model, we use this model and training configuration for all the experiments.\nWe remove any sentence pair if more than 10% and 30% of all the words are out-of-vocabulary after the symbolization for En-Fr and En-De, respectively. Also, we only use sentence pairs of which both sentences are only up to 50 symbols long. We early-stop any training when the BLEU score on a development set does not improve any more."}, {"heading": "5.4 Evaluation", "text": "A simple forward beamsearch is used to approximately find the most likely translation given a source sentence from a trained model. We set the width of the beam to 12 following Bahdanau et al. (2015). The generated translations are scored against the reference translations using \u2018multi-bleu.perl\u2019 script from Moses.6 This evaluation is done on tokenized sentences."}, {"heading": "6 Result and Analysis", "text": ""}, {"heading": "6.1 Quantitative Analysis", "text": "In Table 4, we present the translation qualities, measured by BLEU on the test sets, of all the trained models on both En-Fr and En-De. The most noticeable observation is that both of the proposed methods\u2013contextualization and symbolization\u2013 improve the translation quality over the baseline model which is a vanilla attention-based neural translation system (+Context and +Symbol.) Furthermore, the proposed contextualization and symbolization are complementary to each other, and the most improvement is made when both of them are used together (Baseline+Context+Symbol.)\nAs expected, the proposed symbolization has a similar effect as the BPE segmentation does (Baseline+Symbol vs. Baseline+BPE) on both of the language pairs. We observe that the proposed contextualization, which was found to be complementary to the symbolization, improves the translation quality even when the BPE-base subword symbols were used.\n6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl"}, {"heading": "6.2 Effect of Contextualization", "text": "We conjectured earlier in Sec. 3.3 that the contextualization helps translation by selectively masking out irrelevant some dimensions of word meaning based on the context. This directly implies that the neural translation system with the contextualization will be less confused among many similar words. Browsing through the translations of the source sentences included in the development and test sets, we have observed many such cases, and the following is one such example:\nThe Scorsese-DiCaprio duo seems to have rediscovered the magic that brought them together in Shutter Island .\nThe above sentence is translated by the neural translation system with BPE into"}, {"heading": "Le duo de Corsso-DiCaprio semble avoir retrouve\u0301 la magie qui les a re\u0301unis dans l\u2019\u0131\u0302le Shuttle .", "text": "Meanwhile, the same sentence is translated into"}, {"heading": "Le duo Scorson-DiCaprio semble avoir rede\u0301couvert la magie qui les rassemblait dans l&apos; \u0131\u0302le de Shutter .", "text": "when the contextualization was used.\nWithout the contextualization, the word rediscovered was translated into \u201cretrouve\u0301\u201d (\u201cfound\u201d according to Collins French-English Dictionary7). This is not an incorrect translation, but the translation of this word into \u201crede\u0301couvert\u201d (\u201crediscovered\u201d according to the same source8) by the contextualized model is closer to the original English word. This is an example of the source-side context disambiguating the target word.\nLet us provide another example in the case of En-De. The following source sentence\nThere are 13,435 cumulative cases (12,158 AIDS and 1,317 HIV).\nis translated by the neural translation model with BPE into\nEs gibt 13.335 kumulierte Fa\u0308lle (12,158 Aidg und 1,317 HIV).\nThe same sentence is translated by the same model with the contextualization into\nEs gibt 13.435 kumulative Fa\u0308lle (12.158 AIDS und 1.317 HIV).\nNotice that the tokens 13,435 and 13.335 are close to each other in the word embedding space, as they represent close-by numbers. This makes it difficult for a vanilla neural machine translation model to distinguish between those two. The contextualization, as seen above, can successfully disambiguate \u201c13.435\u201d from \u201c13.335\u201d and correctly puts the former."}, {"heading": "6.3 Effect of Symbolization", "text": "As briefly mentioned earlier in Sec. 5.2, it is known that neural machine translation transliterates rare (proper) nouns using BPE-based subword symbols. This is however not perfect, as the rules for transliteration exhibit many exceptions, and statistical generalization often fails to account for rare proper nouns. For instance, the following sentence\nTrakhtenberg was the presenter of many programs before Hali-Gali times.\nis translated by the model using BPE-based subword symbols into"}, {"heading": "7 http://www.collinsdictionary.com/dictionary/french-english/retrouver", "text": "8 http://www.collinsdictionary.com/dictionary/french-english/red%C3% A9couvrir\nTrakttenberg war der Moderator vieler Programme vor Hali-Gi Male.\nIt is easy to notice that the model has failed at correctly transliterating \u201cTrakhtenberg\u201d. On the contrary, once the symbolization is used (together with the contextualization), the model correctly translates the source sentence above into\nTrakhtenberg war der Moderator von vielen Programmen vor Hali-Gali - Zeiten.\nA similar behaviour is also observed with \u201cHali-Gali\u201d. This example clearly demonstrates the effectiveness of the proposed symbolization, even when the BPE-based subword symbols are used.\nThe advantage of the symbolization is more apparent when words are used as basic symbols instead of BPE-based subword symbols. The same source sentence is translated into\nUNK war der Moderator von vielen Programmen vor UNK .\nusing the word-based translation model with the contextualization only. By including the symbolization as pre- and post-processing routines, we get\nTrakhtenberg war der Moderator vieler Programme , bevor Hali-Gali -Zeiten UNK .\nIt is clear that both of those proper nouns are correctly translated."}, {"heading": "7 Conclusions", "text": "In this paper, we discussed the shortcoming of the existing neural machine translation in terms of how a word or a symbol is represented as a continuous vector and fed to recurrent networks. Based on the observation that a word embedding vector encodes multiple dimensions of similarities, we proposed to contextualize it by adaptively masking out each dimension of the source and target embedding vectors based on the context of a source sentence. In addition to the contextualization of the word embedding vectors, we also propose to symbolize special tokens to facilitate translating those tokens that are not well-suited for translating via continuous vectors.\nThe experiments on En-Fr and En-De revealed that both the contextualization and symbolization techniques improve the translation quality of neural machine translation significantly. Furthermore, we confirmed that these approaches are agnostic to the type of linguistic symbols used to represent source and target sentences by using byte-pair encoding to segment source and target sentences. Especially, the proposed contextualization was found to be complementary to the use of BPE-based subword symbols, and we find it an interesting future work to test the contextualization with character-level neural machine translation Ling et al. (2015); Chung et al. (2016)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano Bastien et al. (2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. KC thanks the support by Facebook and Google (Google Faculty Award 2016)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. Int\u2019l Conf. on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Natural language understanding with distributed representation. arXiv preprint arXiv:1511.07916", "author": ["K. Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2015\\E", "shortCiteRegEx": "Cho", "year": 2015}, {"title": "2014a: On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "2014b: Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "2016: A character-level decoder without explicit segmentation for neural machine translation", "author": ["J. Chung", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.06147", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "2016: Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["O. Firat", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1601.01073", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["F. Hill", "K. Cho", "A. Korhonen", "Y. Bengio"], "venue": "arXiv preprint arXiv:1504.00548", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "2015a: On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "In 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "2015b: Montreal neural machine translation systems for wmt15", "author": ["S. Jean", "O. Firat", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "models. EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D.P. Kingma", "J.L. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Statistical Machine Translation,", "citeRegEx": "Koehn,? \\Q2010\\E", "shortCiteRegEx": "Koehn", "year": 2010}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Character-based neural machine translation", "author": ["W. Ling", "I. Trancoso", "C. Dyer", "A.W. Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "2015a: Effective approaches to attention-based neural machine translation", "author": ["Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "2015b: Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Luong", "M.-T", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "arXiv preprint arXiv:1410.8206", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Natural language processing with modular neural networks and distributed lexicon", "author": ["R. Miikkulainen", "M.G. Dyer"], "venue": "Cognitive Science,", "citeRegEx": "Miikkulainen and Dyer,? \\Q1991\\E", "shortCiteRegEx": "Miikkulainen and Dyer", "year": 1991}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "G. Corrado", "K. Chen", "J. Dean"], "venue": "In Proc. Int\u2019l Conf. on Learning Representations (ICLR)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "2015a: Improving neural machine translation models with monolingual data", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, Association for Computational Linguistics, pp. 384\u2013394", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Machine learning,", "citeRegEx": "Maaten and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2012}, {"title": "Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation is a recently proposed paradigm in machine translation, which is often entirely built as a single neural network Kalchbrenner and Blunsom (2013); Sutskever et al.", "startOffset": 140, "endOffset": 172}, {"referenceID": 12, "context": "Neural machine translation is a recently proposed paradigm in machine translation, which is often entirely built as a single neural network Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al.", "startOffset": 140, "endOffset": 197}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2015). The neural machine translation system, which often consists of an encoder and decoder, projects and manipulates a source sequence of discrete linguistic symbols (source sentence) in a continuous vector space, and decodes a target sequence of symbols (target sentence or translation.", "startOffset": 8, "endOffset": 31}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2015). The neural machine translation system, which often consists of an encoder and decoder, projects and manipulates a source sequence of discrete linguistic symbols (source sentence) in a continuous vector space, and decodes a target sequence of symbols (target sentence or translation.) This is contrary to the conventional machine translation systems, such as phrase-based statistical machine translation Koehn et al. (2003), which work directly at the discrete symbol level.", "startOffset": 8, "endOffset": 455}, {"referenceID": 19, "context": "The encoder network, which is often implemented as a recurrent neural network, encodes this source sentence either into a single context vector Sutskever et al. (2014); Cho et al.", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "(2014); Cho et al. (2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 2, "context": "(2014); Cho et al. (2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al.", "startOffset": 8, "endOffset": 97}, {"referenceID": 0, "context": "(2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al. (2015).", "startOffset": 79, "endOffset": 102}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard.", "startOffset": 85, "endOffset": 108}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al.", "startOffset": 85, "endOffset": 309}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al.", "startOffset": 85, "endOffset": 360}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al.", "startOffset": 85, "endOffset": 387}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al. (2015). Much of these recent improvements have been made by tackling, e.", "startOffset": 85, "endOffset": 417}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al. (2015). Much of these recent improvements have been made by tackling, e.g., the attention mechanism (which is central to the attention-based neural translation system) and the computational issues arising from having a large target vocabulary. Unlike these recent works, we focus on source- and target-side word embedding vectors in this paper. More specifically, we first notice that the transformation from and to high-dimensional word embedding vectors is done for each word largely independent of each other. We conjecture that only a few axes in this high-dimensional space are relevant given a source sentence and that we can remove much of the ambiguity in the choice of words by restricting, or turning off, most of the irrelevant dimensions. We propose to achieve this automated way to turn off some dimensions of word embeddings by contextualizing a word embedding vector. In addition to the proposed contextualization of both source and target word embedding vectors, we propose to extend the unknown token replacement technique proposed in Luong et al. (2015b) to multiple token types.", "startOffset": 85, "endOffset": 1483}, {"referenceID": 0, "context": "More specifically, we describe the attention-based neural machine translation Bahdanau et al. (2015) which will be used in the experiments later.", "startOffset": 78, "endOffset": 101}, {"referenceID": 0, "context": "More specifically, we describe the attention-based neural machine translation Bahdanau et al. (2015) which will be used in the experiments later. However, we note that the proposed contextualization and symbolization techniques are generally applicable to any other type of neural machine translation systems such as the sequence-to-sequence model Sutskever et al. (2014).", "startOffset": 78, "endOffset": 372}, {"referenceID": 7, "context": "The recurrent activation functions \u2212 \u2192 \u03c6 and \u2190\u2212 \u03c6 are in most cases either long short-term memory units (LSTM, Hochreiter and Schmidhuber (1997)) or gated recurrent units (GRU, Cho et al.", "startOffset": 111, "endOffset": 145}, {"referenceID": 3, "context": "The recurrent activation functions \u2212 \u2192 \u03c6 and \u2190\u2212 \u03c6 are in most cases either long short-term memory units (LSTM, Hochreiter and Schmidhuber (1997)) or gated recurrent units (GRU, Cho et al. (2014a).) The decoder consists of two sub-components\u2013a recurrent network and the attention mechanism.", "startOffset": 177, "endOffset": 196}, {"referenceID": 18, "context": "1 For more variants of the attention mechanism, we refer the readers to Luong et al. (2015a).", "startOffset": 72, "endOffset": 93}, {"referenceID": 18, "context": "Already in 1991, Miikkulainen and Dyer Miikkulainen and Dyer (1991) noticed that training a neural network with one-hot vectors as its input learns the word embedding vectors that \u201ccode properties of the input elements that are most crucial to the task.", "startOffset": 17, "endOffset": 68}, {"referenceID": 1, "context": "\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences.", "startOffset": 28, "endOffset": 63}, {"referenceID": 1, "context": "\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences. The interest in word embedding vectors, or distributed representation of words, was fueled by the earlier observations that these unsupervised word embedding vectors can be used to improve supervised natural language tasks greatly Collobert et al. (2011); Turian et al.", "startOffset": 28, "endOffset": 652}, {"referenceID": 1, "context": "\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences. The interest in word embedding vectors, or distributed representation of words, was fueled by the earlier observations that these unsupervised word embedding vectors can be used to improve supervised natural language tasks greatly Collobert et al. (2011); Turian et al. (2010).", "startOffset": 28, "endOffset": 674}, {"referenceID": 21, "context": "2 In Table 1, we show two such examples using the word embedding vectors trained as a part of the continuous-bag-of-word (CBoW) network Mikolov et al. (2013).3 These examples clearly show that each word embedding vector encodes more than one notions of similarities.", "startOffset": 136, "endOffset": 158}, {"referenceID": 21, "context": "2 In Table 1, we show two such examples using the word embedding vectors trained as a part of the continuous-bag-of-word (CBoW) network Mikolov et al. (2013).3 These examples clearly show that each word embedding vector encodes more than one notions of similarities. A similar behaviour can only be observed with multi-map t-SNE Van der Maaten and Hinton (2012).", "startOffset": 136, "endOffset": 362}, {"referenceID": 9, "context": "3 We used the word embedding vectors provided as a part of Hill et al. (2015).", "startOffset": 59, "endOffset": 78}, {"referenceID": 3, "context": "1 in Cho (2015).) This, however, brings in some unnecessary complications as well.", "startOffset": 5, "endOffset": 16}, {"referenceID": 11, "context": "Based on this observation, Jean et al. Jean et al. (2015a) earlier proposed a number of heuristics for handling rare words with the attention-based neural machine translation.", "startOffset": 27, "endOffset": 59}, {"referenceID": 17, "context": "Simultaneously, Luong et al. Luong et al. (2015b) proposed another mechanism that does not require the attention mechanism.", "startOffset": 16, "endOffset": 50}, {"referenceID": 2, "context": "They used an external alignment mechanism, such as IBM Model 2 Brown et al. (1993), to find the alignment between the source and target words in the training corpus.", "startOffset": 63, "endOffset": 83}, {"referenceID": 11, "context": "For instance, Jean et al. Jean et al. (2015a) reported +3 and +2.", "startOffset": 14, "endOffset": 46}, {"referenceID": 11, "context": "For instance, Jean et al. Jean et al. (2015a) reported +3 and +2.5 BLEU improvement with the simple replacement technique based on the attention mechanism on En-Fr and En-De, respectively. Similarly, Luong et al. Luong et al. (2015b) reported +1.", "startOffset": 14, "endOffset": 234}, {"referenceID": 18, "context": "In this paper, we extend the approach by Luong et al. Luong et al. (2015b), which is based on the positional unknown tokens, to include multiple positional special tokens.", "startOffset": 41, "endOffset": 75}, {"referenceID": 15, "context": "4 of Koehn (2010). This approach however has not been adopted widely in neural machine translation yet.", "startOffset": 5, "endOffset": 18}, {"referenceID": 3, "context": "4 The En-Fr corpora are cleaned following the procedure in Cho et al. (2014b), and after tokenization, has approximately 250M words (English side.", "startOffset": 59, "endOffset": 78}, {"referenceID": 3, "context": "4 The En-Fr corpora are cleaned following the procedure in Cho et al. (2014b), and after tokenization, has approximately 250M words (English side.) The En-De corpora are prepared following the procedure in Jean et al. (2015a), resulting in approximately 100M words (English side.", "startOffset": 59, "endOffset": 226}, {"referenceID": 20, "context": "5 As a comparison, we evaluate a setting where byte pair encoding (BPE) is used, as a replacement of the proposed symbolization, to extract sub-word symbols Sennrich et al. (2015b). This has been found to be an effective and efficient approach to handling the issue of a large target vocabulary Sennrich et al.", "startOffset": 157, "endOffset": 181}, {"referenceID": 20, "context": "5 As a comparison, we evaluate a setting where byte pair encoding (BPE) is used, as a replacement of the proposed symbolization, to extract sub-word symbols Sennrich et al. (2015b). This has been found to be an effective and efficient approach to handling the issue of a large target vocabulary Sennrich et al. (2015a); Firat et al.", "startOffset": 157, "endOffset": 319}, {"referenceID": 7, "context": "(2015a); Firat et al. (2016); Chung et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 6, "context": "(2016); Chung et al. (2016) and is known to be able to transliterate rare, proper nouns up to a certain extent Sennrich et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 6, "context": "(2016); Chung et al. (2016) and is known to be able to transliterate rare, proper nouns up to a certain extent Sennrich et al. (2015b). In all the cases, we use up to top-30k most frequent symbols (either tokenized words or BPE-based sub-words.", "startOffset": 8, "endOffset": 135}, {"referenceID": 0, "context": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU).", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU). Further, instead of Adadelta Zeiler (2012), we use Adam Kingma and Ba (2014) for adaptive learning rate adjustment with gradient clipping (threshold at 1.", "startOffset": 67, "endOffset": 244}, {"referenceID": 0, "context": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU). Further, instead of Adadelta Zeiler (2012), we use Adam Kingma and Ba (2014) for adaptive learning rate adjustment with gradient clipping (threshold at 1.", "startOffset": 67, "endOffset": 278}, {"referenceID": 0, "context": "We set the width of the beam to 12 following Bahdanau et al. (2015). The generated translations are scored against the reference translations using \u2018multi-bleu.", "startOffset": 45, "endOffset": 68}, {"referenceID": 16, "context": "Especially, the proposed contextualization was found to be complementary to the use of BPE-based subword symbols, and we find it an interesting future work to test the contextualization with character-level neural machine translation Ling et al. (2015); Chung et al.", "startOffset": 234, "endOffset": 253}, {"referenceID": 6, "context": "(2015); Chung et al. (2016).", "startOffset": 8, "endOffset": 28}], "year": 2016, "abstractText": "We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.", "creator": "LaTeX with hyperref package"}}}