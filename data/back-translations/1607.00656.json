{"id": "1607.00656", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2016", "title": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching", "abstract": "This article presents an agent architecture for controlling an autonomous agent in stochastic environments, combining the partially observable Markov decision model (POMDP) with the belief-desire-intention framework (BDI).The hybrid POMDP-BDI agent architecture utilizes the best features of the two approaches, i.e. the online generation of reward-maximizing approaches from POMDP theory and sophisticated multi-target management from BDI theory.We present the progress made since the introduction of the base architecture, including (i) the ability to pursue multiple goals simultaneously, and (ii) a plan library to store prescribed plans and store recently created plans for future reusage.A version of the architecture without the plan library is implemented and evaluated using simulations.The results of the simulation experiments indicate that the approach is feasible.", "histories": [["v1", "Sun, 3 Jul 2016 17:11:52 GMT  (43kb,D)", "http://arxiv.org/abs/1607.00656v1", "26 pages, 3 figures, unpublished version"]], "COMMENTS": "26 pages, 3 figures, unpublished version", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gavin rens", "deshendran moodley"], "accepted": false, "id": "1607.00656"}, "pdf": {"name": "1607.00656.pdf", "metadata": {"source": "CRF", "title": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching", "authors": ["Gavin Rens", "Deshendran Moodley"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Autonomous Agents, POMDP, BDI, Satisfaction, Plans, Planning, Memory"}, {"heading": "1 Introduction", "text": "Imagine a scenario where a planetary rover has five tasks of varying importance. The tasks could be, for instance, collecting gas (for industrial use) from a natural vent at the base of a hill, taking a temperature measurement at the top of the hill, performing self-diagnostics and repairs, reloading its batteries at the solar charging station and collect soil samples wherever the rover is. The rover is programmed to know the relative importance of collecting soil samples. The rover also has a model of the probabilities with which its various actuators fail and the probabilistic noise-profile of its various sensors. The rover must be able to reason (plan) in real-time to pursue the right task at the right time while considering its resources and dealing with various events, all while considering the uncertainties about its actions (actuators) and perceptions (sensors).\nWe propose an architecture for the proper control of an agent in a complex environment such as the scenario described above. The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991). Traditional BDI architectures (BDIAs) cannot deal with probabilistic uncertainties and they do not generate plans in real-time. A traditional POMDP cannot manage goals (major and minor tasks) as well as BDIAs can. Next, we analyse the POMDPs and BDIAs in a little more detail.\nar X\niv :1\n60 7.\n00 65\n6v 1\n[ cs\n.A I]\n3 J\nul 2\n01 6\nOne of the benefits of agents based on BDI theory, is that they need not generate plans from scratch; their plans are already (partially) compiled, and they can act quickly once a goal is focused on. Furthermore, the BDI framework can deal with multiple goals. However, their plans are usually not optimal, and it may be difficult to find a plan which is applicable to the current situation. That is, the agent may not have a plan in its library which exactly \u2018matches\u2019 what it ideally wants to achieve. On the other hand, POMDPs can generate optimal policies on the spot to be highly applicable to the current situation. Moreover, policies account for stochastic actions in partially observable environments. Unfortunately, generating optimal POMDP policies is usually intractable. One solution to the intractability of POMDP policy generation is to employ a continuous planning strategy, or agent-centred search (Koenig, 2001). Aligned with agent-centred search is the forward-search approach or online planning approach in POMDPs (Ross et al., 2008).\nThe traditional BDIA maintains goals as desires ; there is no reward for performing some action in some state. The reward function provided by POMDP theory is useful for modeling certain kinds of behavior or preferences. For instance, an agent based on a POMDP may want to avoid moist areas to prevent its parts becoming rusty. Moreover, a POMDP agent can generate plans which can optimally avoid moist areas. But one would not say that avoiding moist areas is the agent\u2019s task. And POMDP theory maintains a single reward function; there is no possibility of weighing alternative reward functions and pursuing one at a time for a fixed period\u2014all objectives must be considered simultaneously, in one reward function. Reasoning about objectives in POMDP theory is not as sophisticated as in BDI theory. A BDI agent cannot, however, simultaneously avoid moist areas and collect gas; it has to switch between the two or combine the desire to avoid moist areas with every other goal.\nThe Hybrid POMDP-BDI agent architecture (or HPB architecture, for short) has recently been introduced (Rens and Meyer, 2015). It combines the advantages of POMDP theoretic reasoning and the potentially sophisticated means-ends reasoning of BDI theory in a coherent agent architecture. In this paper, we generalize the management of goals by allowing for each goal to be pursued with different intensities, yet concurrently.\nTypically, BDI agents do not deal with stochastic uncertainty. Integrating POMDP notions into a BDIA addresses this. For instance, an HPB agent will maintain a (subjective) belief state representing its probabilistic (uncertain) belief about its current state. Planning with models of stochastic actions and perceptions is possible in the HPB architecture. The tight integration of POMDPs and BDIAs is novel to this architecture, especially in combination with desires with changing intensity levels.\nThis article serves to introduce two significant extensions to the first iteration (Rens and Meyer, 2015) of the HPB architecture. The first extension allows for multiple intentions to be pursued simultaneously, instead of one at a time. In the previous architecture, only one intention was actively pursued at any moment. In the new version, one agent action can take an agent closer to more than one goal at the moment the action is performed \u2013 the result of a new approach to planning. As a consequence of allowing multiple intentions, the policy generation module (\u00a7 4.3), the desire function and the method of focusing on intentions (\u00a7 4.2) had to be adapted. The second extension is the addition of a plan library. Previously, a policy (conditional plan) would have to be generated periodically and regularly to supply the agent with the recommendations of actions it needs to take. Although one of the strengths of traditional BDI theory is the availability of a plan library with pre-written plans for quick use, a plan library was excluded from the HPB architecture so as to simplify the architecture\u2019s introduction. Now we propose a framework where an agent designer can store hand-written policies in a library of plans and where generated policies are stored for later reuse. Every policy in the library is stored together with a \u2018context\u2019 in which it will be applicable and the set of intentions which it is meant to satisfy. There are two advantages of introducing a plan\nlibrary: (i) policies can be tailored by experts to achieve specific goals in particular contexts, giving the agent immediate access to recommended courses of action in those situations, and (ii) providing a means for policies, once generated, to be stored for later reuse so that the agent can take advantage of past \u2018experience\u2019 \u2013 saving time and computation.\nIn Section 2, we review the necessary theory, including POMDP and BDI theory. In Section 3, we describe the basic HPB architecture. The extensions to the basic architecture are presented in Section 4. Section 5 describes two simulation experiments in which the proposed architecture is tested, evaluating the performance on various dimensions. The results of the experiments confirm that the approach may be useful in some domains. The last section discusses some related work and points out some future directions for research in this area."}, {"heading": "2 Preliminaries", "text": "The basic components of a BDI architecture (Wooldridge, 1999, 2002) are\n\u2022 a set or knowledge-base B of beliefs; \u2022 an option generation function wish, generating the objectives the agent would ideally like\nto pursue (its desires);\n\u2022 a set of desires D (goals to be achieved); \u2022 a \u2018focus\u2019 function which selects intentions from the set of desires; \u2022 a structure of intentions I of the most desirable options/desires returned by the focus\nfunction;\n\u2022 a library of plans and subplans; \u2022 a \u2018reconsideration\u2019 function which decides whether to call the focus function; \u2022 an execution procedure, which affects the world according to the plan associated with the\nintention;\n\u2022 a sensing or perception procedure, which gathers information about the state of the environment; and\n\u2022 a belief update function, which updates the agent\u2019s beliefs according to its latest observations and actions.\nExactly how these components are implemented result in a particular BDI architecture. Algorithm 1 (adapted from Wooldridge (2000, Fig. 2.3)) is a basic BDI agent control loop. \u03c0 is the current plan to be executed. getPercept(\u00b7) senses the environment and returns a percept (processed sensor data) which is an input to update(\u00b7), which updates the agent\u2019s beliefs. wish : B \u00d7 I \u2192 D generates a set of desires, given the agent\u2019s beliefs, current intentions and possibly its innate motives. It is usually impractical for an agent to pursue the achievement of all its desires. It must thus filter out the most valuable and achievable desires. This is the function of focus : B\u00d7D\u00d7I \u2192 I, taking beliefs, desires and current intentions as parameters. Together, the processes performed by wish and focus may be called deliberation, formally encapsulated by the deliberate procedure. plan(\u00b7) returns a plan from the plan library to achieve the agent\u2019s current intentions.\nA more sophisticated controller would have the agent consider whether to re-deliberate, with a reconsider function placed just before deliberation would take place. The agent could also test at every iteration through the main loop whether the currently pursued intention is still possibly achievable. Serendipity could also be taken advantage of by periodically testing whether the intention has been achieved, without the plan being fully executed. Such an\nAlgorithm 1: Basic BDI agent control loop\nInput: B0: initial beliefs Input: I0: initial intentions\n1 B \u2190 B0; 2 I \u2190 I0; 3 \u03c0 \u2190 null ; 4 while alive do 5 p\u2190 getPercept(); 6 B \u2190 update(B, p); 7 D \u2190 wish(B, I); 8 I \u2190 focus(B,D, I); 9 \u03c0 \u2190 plan(B, I);\n10 execute(\u03c0);\nagent is considered \u2018reactive\u2019 because it executes one action per loop iteration; this allows for deliberation between executions. There are various mechanisms which an agent might use to decide when to reconsider its intentions. See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al. (2004).\nIn a partially observable Markov decision process (POMDP), the actions the agent performs have non-deterministic effects in the sense that the agent can only predict with a likelihood in which state it will end up after performing an action. Furthermore, its perception is noisy. That is, when the agent uses its sensors to determine in which state it is, it will have a probability distribution over a set of possible states to reflect its conviction for being in each state.\nFormally (Kaelbling et al., 1998), a POMDP is a tuple \u3008S,A, T,R, Z, P, b0\u3009 with\n\u2022 S, a finite set of states of the world (that the agent can be in), \u2022 A a finite set of actions (that the agent can choose to execute), \u2022 a transition function T (s, a, s\u2032), the probability of being in s\u2032 after performing action a in\nstate s,\n\u2022 R(a, s), the immediate reward gained for executing action a while in state s, \u2022 Z, a finite set of observations the agent can perceive in its world, \u2022 a perception function P (s\u2032, a, z), the probability of observing z in state s\u2032 resulting from\nperforming action a in some other state, and\n\u2022 b0 the initial probability distribution over all states in S.\nIn general, we regard an observation as the signal recognized by a sensor; the signal is generated by some event which is not directly perceivable.\nA belief state b is a set of pairs \u3008s, p\u3009 where each state s in b is associated with a probability p. All probabilities must sum up to one, hence, b forms a probability distribution over the set S of all states. To update the agent\u2019s beliefs about the world, a special function SE (z, a, b) = bn is defined as\nbn(s \u2032) =\nP (s\u2032, a, z) \u2211\ns\u2208S T (s, a, s \u2032)b(s)\nPr(z|a, b) , (1)\nwhere a is an action performed in \u2018current\u2019 belief state b, z is the resultant observation and bn(s\n\u2032) denotes the probability of the agent being in state s\u2032 in \u2018new\u2019 belief state bn. Note that Pr(z | a, b) is a normalizing constant.\nLet the planning horizon h (also called the look-ahead depth) be the number of future steps the agent plans ahead each time it plans. V \u2217(b, h) is the optimal value of future courses of actions the agent can take with respect to a finite horizon h starting in belief state b. This function assumes that at each step the action that will maximize the state\u2019s value will be selected.\nBecause the reward function R(a, s) provides feedback about the utility of a particular state s (due to a executed in it), an agent who does not know in which state it is in cannot use this reward function directly. The agent must consider, for each state s, the probability b(s) of being in s, according to its current belief state b. Hence, a belief reward function \u03c1(a, b) is defined, which takes a belief state as argument. Let \u03c1(a, b) := \u2211 s\u2208S R(a, s)b(s).\nThe optimal state-value function is define by\nV \u2217(b, h) := max a\u2208A\n[ \u03c1(a, b) + \u03b3 \u2211 z\u2208Z Pr(z | a, b)V \u2217(SE (z, a, b), h\u2212 1) ] ,\nwhere 0 \u2264 \u03b3 < 1 is a factor to discount the value of future rewards and Pr(z | a, b) denotes the probability of reaching belief state bn = SE (z, a, b). While V\n\u2217 denotes the optimal value of a belief state, function Q\u2217 denotes the optimal action-value:\nQ\u2217(a, b, h) := \u03c1(a, b) + \u03b3 \u2211 z\u2208Z Pr(z | a, b)V \u2217(SE (z, a, b), h\u2212 1)\nis the value of executing a in the current belief state, plus the total expected value of belief states reached thereafter."}, {"heading": "3 The Basic HPB Architecture", "text": "In BDI theory, one of the big challenges is to know when the agent should switch its current goal and what its new goal should be (Schut et al., 2004). To address this challenge, we propose that an agent should maintain intensity levels of desire for every goal. This intensity of desire could be interpreted as a kind of emotion. The goals most intensely desired should be the goals sought (the agent\u2019s intentions). We also define the notion of how much an intention is satisfied in the agent\u2019s current belief state. For instance, suppose that out of five possible goals, the agent currently most desires to watch a film and to eat a snack. Then these two goals become the agent\u2019s intentions. However, eating is not allowed inside the film-theatre, and if the agent were to go buy a snack it would miss the beginning of the film. So the total reward for first watching the film then buying and eating a snack is higher than first eating then watching. As soon as the film-watching goal is satisfied, it is no longer an intention. But while the agent was watching the film, the desire-level of the (non-intention) goal of being at home has been increasing. However, it cannot become an intention because snack-eating has not yet been satisfied. Going home cannot simply become an intention and dominate snack-eating, because the architecture is designed so that current intentions have precedence over non-intention goals, else there is a danger that the agent will vacillate between which goals to pursue. Nonetheless, snack-eating may be ejected from the set of intentions under the special condition that the agent is having an unusually hard time achieving it. For instance, if someone stole its wallet in the theatre, the agent can no longer have the current intention (i.e., actively pursue) eating a snack. Hence, in our architecture, if an intention takes \u2018too long\u2019 to satisfy, it is removed from the set of intentions. As soon as the agent gets home or is close to home, the snack-eating goal will probably become an intention again and the agent will start making plans to satisfy eating a snack. Moreover, the desire-level of snack-eating will now be very high (it has been steadily increasing) and the agent\u2019s actions will be biased towards satisfying this intention over other current intentions (e.g., over getting home, if it is not yet there).\nA Hybrid POMDP-BDI (HPB) agent (Rens and Meyer, 2015) maintains (i) a belief state which is periodically updated, (ii) a mapping from goals to numbers representing the level of desire to achieve the goals, and (iii) the current set of intentions, the goals with the highest desire levels (roughly speaking). As the agent acts, its desire levels are updated and it may consider choosing new intentions and discard others based on new desire levels. Refer to Figure 1 for an overview of the operational semantics. The figure refers to concepts defined in the following subsection."}, {"heading": "3.1 Declarative Semantics", "text": "The state of an HPB agent is defined by the tuple \u3008B,D, I\u3009, where B is the agent\u2019s current belief state (i.e., a probability distribution over the states S, defined below), D is the agent\u2019s current desire function and I is the agent\u2019s current intention. More will be said about D and I a little later.\nAn HPB agent could be defined by the tuple \u3008Atrb, G,A, Z, T, P,Util\u3009, where\n\u2022 Atrb is a set of attribute-sort pairs (for short, the attribute set). For every (atrb : sort) \u2208 Atrb, atrb is the name or identifier of an attribute of interest in the domain of interest, like BatryLevel or Direction, and sort is the set from which atrb can take a value, for instance, real numbers in the range [0, 55] or a list of values like {North, East, West, South}. So {(BatryLevel : [0, 55]), (Direction : {North, East, West, South})} could be an attribute set.\nA state s is induced from Atrb as one possible way of assigning values to attributes: s = {(atrb : v) | (atrb : sort) \u2208 Atrb, v \u2208 sort , if (atrb : v), (atrb \u2032 : v\u2032) \u2208 s and atrb = atrb \u2032, then v = v\u2032}. The set of all possible states is denoted S.\n\u2022 G is a set of goals. A goal is a subset of some state s \u2208 S. For instance, {(BatryLevel : 13), (Direction : South)} is a goal, and so are {(BatryLevel : 33)} and {(Direction : West)}. The set of goals is given by the agent designer as \u2018instructions\u2019 about the agent\u2019s tasks.\n\u2022 A is a finite set of actions.\n\u2022 Z is a finite set of observations.\n\u2022 T is the transition function of POMDPs.\n\u2022 P is the perception function of POMDPs.\n\u2022 Util consists of two functions Pref and Satf which allow an agent to determine the utilities of alternative sequences of actions. Util = \u3008Pref , Satf \u3009. Pref is the preference function with a range in R \u2229 [0, 1]. It takes an action a and a state s, and returns the preference (any real number) for performing a in s. That is, Pref (a, s) \u2208 [0, 1]. Numbers closer to 1 imply greater preference and numbers closer to 0 imply less preference. Except for the range restriction of [0, 1], it has the same definition as a POMDP reward function, but its name indicates that it models the agent\u2019s preferences and not what is typically thought of as rewards. An HPB agent gets \u2018rewarded\u2019 by achieving its goals. The preference function is especially important to model action costs; the agent should prefer \u2018inexpensive\u2019 actions. Pref has a local flavor. Designing the preference function to have a value lying in [0,1] may sometimes be challenging, but we believe it is always possible."}, {"heading": "3.2 The Desire Function", "text": "The desire function D is a total function from goals in G into the positive real numbers R+. The real number represents the intensity or level of desire of the goal. For instance, ({(BatryLevel : 13), (WeekDay : Tue)}, 2.2) could be in D, meaning that the goal of having the battery level at 13 and the week-day Tuesday is desired with a level of 2.2. ({(BatryLevel : 33)}, 56) and ({(WeekDay : Wed)}, 444) are also examples of desires in D.\nI is the agent\u2019s current intention; an element of G; the goal with the highest desire level. This goal will be actively pursued by the agent, shifting the importance of the other goals to the background. The fact that only one intention is maintained makes the HPB agent architecture quite different to standard BDIAs.\nWe propose the following desire update rule.\nD(g)\u2190 D(g) + 1\u2212 Satf \u03b2(g,B) (2)\nRule 2 is defined so that as Satf \u03b2(g,B) tends to one (total satisfaction), the intensity with which the incumbent goal is desired does not increase. On the other hand, as Satf \u03b2(g,B) becomes smaller (more dissatisfaction), the goal\u2019s intensity is incremented. The rule transforms D with respect to B and g. A goal\u2019s intensity should drop the more it is being satisfied. The update rule thus defines how a goal\u2019s intensity changes over time with respect to satisfaction.\nNote that desire levels never decrease. This does not reflect reality. It is however convenient to represent the intensity of desires like this: only relative differences in desire levels matter in our approach and we want to avoid unnecessarily complicating the architecture."}, {"heading": "3.3 Focusing and Satisfaction Levels", "text": "Focus is a function which returns one member of G called the (current) intention I. In the initial version of the architecture, the goal selected is the one with the highest desire level. After every execution of an action in the real-world, Refocus is called to decide whether to call Focus to select a new intention. Refocus is a meta-reasoning function analogous to the reconsider function mentioned in Section 2. It is important to keep the agent focused on one goal long enough to give it a reasonable chance of achieving it. It is the job of Refocus to recognize when the current intention seems impossible or too expensive to achieve.\nLet Satf levels be the sequence of satisfaction levels of the current intention since it became active and let MRY be a designer-specified number representing the length of a sub-sequence of Satf levels\u2014the MRY last satisfaction levels.\nOne possible definition of Refocus is\nRefocus(c, \u03b8) def =  \u2018no\u2019 if |Satf levels| < MRY\n\u2018yes\u2019 if c < \u03b8 \u2018no\u2019 otherwise,\nwhere c is the average change from one satisfaction level to the next in the agent\u2019s \u2018memory\u2019 MRY , and \u03b8 is some threshold, for instance, 0.05. If the agent is expected to increase its\nsatisfaction by at least, say, 0.1 on average for the current intention, then \u03b8 should be set to 0.1. With this approach, if the agent \u2018gets stuck\u2019 trying to achieve its current intention, it will not blindly keep on trying to achieve it, but will start pursuing another goal (with the highest desire level). Some experimentation will likely be necessary for the agent designer to determine a good value for \u03b8 in the application domain.\nNote that if an intention was not well satisfied, its desire level still increases at a relatively high rate. So whenever the agent focuses again, a goal not well satisfied in the past will be a top contender to become the intention (again)."}, {"heading": "3.4 Planning for the Next Action", "text": "A basic HPB agent controls its behaviour according to the policies it generates. Plan is a procedure which generates a POMDP policy \u03c0 of depth h. Essentially, we want to consider all action sequences of length h and the belief states in which the agent would find itself if it followed the sequences. Then we want to choose the sequence (or at least its first action) which yields the least cost and which ends in the belief state most satisfying with respect to the intention.\nPlanning occurs over an agents belief states. The satisfaction and preference functions thus need to be defined for belief states: The satisfaction an agent gets for an intention in its current belief state is defined as\nSatf \u03b2(I, B) := \u2211 s\u2208S Satf (I, s)B(s),\nwhere Satf (I, s) is defined above and B(s) is the probability of being in state s. The definition of Pref \u03b2 has the same form as the reward function \u03c1 over belief states in POMDP theory:\nPref \u03b2(a,B) := \u2211 s\u2208S Pref (a, s)B(s),\nwhere Pref (a, s) was discussed above. During planning, preferences and intention satisfaction must be maximized. The main function used in the Plan procedure is the HPB action-state value function Q\u2217HPB , giving the value of some action a, conditioned on the current belief state B, intention I and look-ahead depth h:\nQ\u2217HPB(a,B, I, h) := \u03b1Satf \u03b2(I, B) + (1\u2212 \u03b1)Pref \u03b2(a,B) + \u03b3 \u2211 z\u2208Z Pr(z | a,B) max a\u2032\u2208A Q\u2217HPB(a \u2032, B\u2032, I, h\u2212 1), Q\u2217HPB(a,B, I, 1) := \u03b1Satf \u03b2(I, B) + (1\u2212 \u03b1)Pref \u03b2(a,B),\nwhere B\u2032 = SE (a, z, B), 0 \u2264 \u03b1 \u2264 1 is the goal/preference \u2018trade-off\u2019 factor, \u03b3 is the normal POMDP discount factor and SE is the normal POMDP state estimation function.\nPlan returns arg maxa\u2208AQ \u2217 HPB(a,B, I, h), the trivial policy of a single action."}, {"heading": "4 The Extended HPB Architecture", "text": "The operational semantics of the extended architecture is essentially the same as for the first version, except that a plan library is now involved. The agent starts off with an initial set of intentions, a subset of its goals. For the current set of intentions, it must either select a plan from the plan library or generate a plan to pursue all its intentions. At every iteration of the agent\u2019s control loop, an action is performed, an observation is made, the belief state is updated,\nand a decision is made whether to modify the set of intentions. But only when the current policy (conditional plan) is \u2018exhausted\u2019 does the agent seek a new policy, by consulting its plan library, and if an adequate policy is not found, generating one.\nIn the next subsection, we introduce some new notation and changes made to the architecture. Section 4.2 discusses how the focussing procedure must change to accommodate the changes. Section 4.3 explains how policies are generated for simultaneous pursuit of multiple goals. Finally, Section 4.4 presents the plan library, which was previously unavailable, and how the agent and agent designer can use it to their benefit."}, {"heading": "4.1 Prologue", "text": "The HPB agent model gets three new component \u2013 a goal weight function W , a compatibility function Cpbl and the plan library Lbry . It can thus be defined by the tuple \u3008Atrb, G, W , Cpbl , A, Z, T , P , Util , Lbry\u3009.\nIn the previous version, satisfaction and preference were traded-off by \u201ctrade-off factor\u201d which was not explicitly mentioned in the agent model. Actually the trade-off factor should have been part of the model, because it must be provided by the agent designer, and it directly affects the agent\u2019s behaviour. In the new version, every goal g \u2208 G will be weighted by W (g) according to the importance of g to the agent. Goal weights are constrained such that W (g) > 0 for all g \u2208 G, and \u2211 g\u2208GW (g) = 1.\nThe third fundamental extension is that I becomes a set of intentions. In this way, an HPB agent may actively pursue several goals simultaneously. For example, a planetary rover may want to travel to its recharging station and simultaneously make same atmospheric measurements en route.\nThe first version has also been changed so that the set of goals G is simply a set of names, rather than restricting a goal to be a set of attribute values, as was previously done. Goals are defined by how they are used in the architecture, particularly by their involvement in the definition of satisfaction functions.\nIn the extended architecture, it will be convenient to use more compact notation: Here we let Util = \u3008\u03ba, \u03c3\u3009, where \u03ba is the same as Pref and \u03c3 is a set of satisfaction functions {\u03c3g | g \u2208 G, \u03c3g = Satf (g)}. In particular, we move away from a preference function, and rather think of a cost function \u03ba. Preferences will be captured by the set of satisfaction functions.\nAs a consequence of being able to pursue several goals at the same time, there exists a danger that the agent will pursue one intention when it necessarily causes another intention to become less satisfied. For instance, visiting the USA regional headquarters is diametrically opposite to visiting the China regional headquarters at the same time. Other examples of goals which should be \u2018disjoint\u2019 are work\u2212in\u2212garden and have\u2212lunch, and recharge\u2212battery and replace\u2212battery. The solution we use is to list, for each goal g \u2208 G, all other goals which are compatible with it, in the sense that their simultaneous pursuit \u2018effective\u2019 (defined by the agent designer). Let Cpbl(g) denote the set of goals compatible with g. It is mandatory that g \u2208 Cpbl(g). Two goals g and g\u2032 are called incompatible if and only if g\u2032 6\u2208 Cpbl(g) or g 6\u2208 Cpbl(g\u2032).\nSuppose G = {visit\u2212USA\u2212HQ, visit\u2212China\u2212HQ, work\u2212in\u2212garden, have\u2212lunch, recharge\u2212battery, replace\u2212battery}. Then an agent designer may specify\nCpbl(visit\u2212USA\u2212HQ) = {visit\u2212USA\u2212HQ, have\u2212lunch}\nand\nCpbl(recharge\u2212battery) = {recharge\u2212battery, work\u2212in\u2212garden, have\u2212lunch}.\nNote that work\u2212in\u2212garden and have\u2212lunch are incompatible."}, {"heading": "4.2 A New Approach to Focusing", "text": "Given that I is a set of intentions, ensuring that the \u2018correct\u2019 goals are intentions at the \u2018right\u2019 time to ensure that the agent behaves as desired, requires some careful thought. It is still important to keep the agent focused on one intention long enough to give it a reasonable chance of achieving it, temporarily stop pursuing intentions it is struggling to achieve.\nThe HPB architecture does not have a focus function which returns a subset of G of intentions I. Rather, we have a set of procedures which decide at each iteration which intention to remove from I (if any) and which goal to add to I (if any). Incompatible must also be dealt with.\nLet Satf levels(g) be the sequence of satisfaction levels of some goal g \u2208 I since g became active (i.e., was added to I) and let MRY be a number representing the length of a sub-sequence of Satf levels(g)\u2014the MRY last satisfaction levels of goal g. Remove is defined exactly like Refocus :\nRemove(g, I) :=  \u2018no\u2019 if |Satf levels(g)| < MRY (g)\n\u2018yes\u2019 if \u03b4(g) < \u03b8 \u2018no\u2019 otherwise,\nwhere \u03b4(g) is the average change from one satisfaction level of g to the next in the agent\u2019s \u2018memory\u2019, and \u03b8 is the threshold above which \u03b4(g) must be for g to remain an intention.\nLet MI be the currently most intense goal defined as\nMI := arg max g\u2208G D(g).\nWe define two focusing strategies for sets of intentions: the over-optimistic strategy and the compatibility strategy."}, {"heading": "4.2.1 Over-optimistic Strategy", "text": "This strategy ignores compatibility issues between goals. In this sense, the agent is (over) optimistic that it can successfully simultaneously pursue goals which are incompatible.\nAdd MI to I only if MI 6\u2208 I. If MI is added to I, clear MI \u2019s record of satisfaction levels, that is, let Satf levels(MI ) be the empty sequence.\nNext: For every g \u2208 I, if |I| > 1 and Remove(g, I) returns \u2018yes\u2019, then remove g from I."}, {"heading": "4.2.2 Compatibility Strategy", "text": "Add MI to I only if MI 6\u2208 I and there does not exists a g \u2208 I such that g 6\u2208 Cpbl(MI ). If MI is added to I, clear MI \u2019s record of satisfaction levels, that is, let Satf levels(MI ) be the empty sequence.\nNext: For every g \u2208 I, if |I| > 1 and Remove(g, I) returns \u2018yes\u2019, then remove g from I. There is one case which must still be dealt with in the compatibility strategy: Suppose for some g \u2208 G, g\u0304 6\u2208 Cpbl(g). Further suppose that I = {g\u0304} (i.e., |I| = 1) and g is and remains the most intensely desired goal. Now, g may not be added to I because it is incompatible with g\u0304, no other goal will be attempted to be added to I and g\u0304 may not be removed while it is the only intention, even if Remove(g\u0304, I) returns \u2018yes\u2019. What could easily happen in this case is that g will continually increase in desire level, g\u0304\u2019s average satisfaction level will remain below the change threshold (i.e., \u03b4(g\u0304) < \u03b8 remains true), and the agent continues to pursue only g\u0304. To remedy this \u2018locked\u2019 situation, the following procedure is run after the previous \u2018add\u2019 and \u2018remove\u2019 procedures are attempted. If I = {g\u0304}, g\u0304 6\u2208 Cpbl(MI ) and Remove(g\u0304, I) returns \u2018yes\u2019, then remove g\u0304 from I, add MI to I and clear MI \u2019s record of satisfaction levels."}, {"heading": "4.2.3 A New Desire Function", "text": "The old rule (in new notation) is still available:\nD(g)\u2190 D(g) +W (g)(1\u2212 \u03c3g\u03b2(B)). (3)\nWe have found through experimentation that when an intention-goal\u2019s desire levels are updated, non-intention-goals may not get the opportunity to become intentions. In other words, it may happen that whenever new non-intention-goals are considered to become intentions, they are always \u2018dominated\u2019 by goals with higher levels of desire which are already intentions. By disallowing intentions\u2019 desire levels to increase, non-intentions get the opportunity to \u2018catch up\u2019 with their desire levels. A new form of the desire update rule is thus proposed for this version of the architecture:\nD(g)\u2190 D(g) + (1\u2212 i(I, g))W (g)(1\u2212 \u03c3g\u03b2(B)) (4)\nThe term (1\u2212 i(I, g)) in (4) ensures that a goal\u2019s desire level changes if and only if the goal is not an intention.\nBoth forms of the rule are defined so that as \u03c3g\u03b2(B) tends to one (total satisfaction), the intensity with which the incumbent goal is desired does not increase. On the other hand, as \u03c3g\u03b2(B) becomes smaller (more dissatisfaction), the goal\u2019s intensity is incremented\u2014by at most its weight of importance W (g). A goal\u2019s intensity should drop the more it is being satisfied.\nHowever, update rule (3) which is independent of whether a goal is an intention may still result in better performance in particular domains. (This question needs more research.) It is thus left up to the agent designer to desire which form of the rule better suits the application domain."}, {"heading": "4.3 Planning by Policy Generation", "text": "In this section, we shall see how the planner can be extended to compute a policy which pursues several goals simultaneously. Goal weights are also incorporated into the action-state value function.\nThe satisfaction an agent gets for an intention g at its current belief state is defined as \u03c3g\u03b2(B) := \u2211 s\u2208S \u03c3g(s)B(s),\nwhere \u03c3g(s) is defined above and B(s) is the probability of being in state s. The definition of \u03ba\u03b2 has the same form as the reward function \u03c1 over belief states in POMDP theory:\n\u03ba\u03b2(a,B) := \u2211 s\u2208S \u03ba(a, s)B(s),\nwhere \u03ba(a, s) was discussed above. The main function used in the Plan procedure is the HPB action-state value function Q\u2217HPB , giving the value of some action a, conditioned on the current belief state B and look-ahead depth h:\nQ\u2217HPB(a,B, I, h) := i(I, g1)W (g1)\u03c3 g1 \u03b2 (B) + \u00b7 \u00b7 \u00b7+ i(I, gn)W (gn)\u03c3 gn \u03b2 (B)\u2212 \u03ba\u03b2(a,B) + \u03b3 \u2211 z\u2208Z Pr(z | a,B) max a\u2032\u2208A Q\u2217HPB(a \u2032, B\u2032, I, h\u2212 1), Q\u2217HPB(a,B, I, 1) := i(I, g1)W (g1)\u03c3 g1 \u03b2 (B) + \u00b7 \u00b7 \u00b7+ i(I, gn)W (gn)\u03c3 gn \u03b2 (B)\u2212 \u03ba\u03b2(a,B),\nwhere\n\u2022 i(I, gj) = 1 if gj \u2208 I, else i(I, gj) = 0 if gj 6\u2208 I, \u2022 \u3008g1, . . . , gn\u3009 is an ordering of the goals in G, \u2022 \u03c3g\u03b2(\u00b7) and \u03ba\u03b2(\u00b7) are the expected (w.r.t. a belief state) values of \u03c3g(\u00b7), resp., \u03ba(\u00b7), \u2022 B\u2032 = SE (a, z, B), \u2022 \u03b3 is the normal POMDP discount factor and \u2022 SE is the normal POMDP state estimation function.\nNow, instead of Plan returning a single action (assuming h > 1), Plan generates a treestructures plan of depth h, conditioned on observations, that is, a policy. With a policy of depth h, an agent can execute a sequence of h actions, where the choice of exactly which action to take at each step depends on the observation received just prior. arg maxa\u2208A Q \u2217 HPB(a,B, I, h) is used at every choice point to construct the policy.\nFigure 4.3 is a graphical example of a policy with two actions and two observations. The agent is assumed to be in belief state Bcur when the policy is generated. At every belief state node (triangles), the optimal action is recommended. After an action is performed, all/both observations are possible and thus considered. There is thus a choice at every \u25e6 node; however, it is not a choice for the agent, rather, it is a choice for the environment which observation to send to the agent. Given the action performed, for every possible observation, a different belief state is generated. At every . node (belief state), arg maxa\u2208A Q \u2217 HPB(a, ., I, h) is applied to determine the action to perform there. (In theory, the agent can choose to perform any action at these . nodes, but our agent will take the recommendations of POMDP theory for optimal behavior.) The agent will perform act2 first, then depending on whether obs1 or obs2 in sensed, the agent should next (according to the policy) perform act2, respectively, act1. Then a third action will be performed according to the policy and conditional on which observation is sensed."}, {"heading": "4.4 Introducing a Plan Library", "text": "Another extension of the basic architecture is that a language based on the attributes is introduced. The language L is the set of all sentences. Let \u03c6 and \u03c8 be sentences. Then the following are also sentences.\n\u2022 true, \u2022 (atrb : v), i.e., an attribute-value pair, \u2022 \u03c6 \u2227 \u03c8, \u2022 \u03c6 \u2228 \u03c8, \u2022 \u00ac\u03c6.\nIf a sentence \u03c6 is satisfied or true in a state s, we write s \u03c6. The semantics of L is defined by\n\u2022 s true always, \u2022 s (atrb : v) \u21d0\u21d2 (atrb : v) \u2208 s, \u2022 s \u03c6 \u2227 \u03c8 \u21d0\u21d2 s \u03c6 and s \u03c8, \u2022 s \u03c6 \u2228 \u03c8 \u21d0\u21d2 s \u03c6 or s \u03c8, \u2022 s \u00ac\u03c6 \u21d0\u21d2 not s \u03c6.\nLet \u03a6 be a sentence in L. When a sentence in L appears in a written policy (see below), it is called a context.\nWe define two kinds of plans: an attribute condition plan is a triple I : \u03a6 : \u03c0, and a belief state condition plan is a triple I : B : \u03c0, where I is a set of intentions, \u03c0 is a POMDP policy, \u03a6 is a context and B is a belief state. All plans are stored in a plan library.\nThe idea is that attribute condition plans (abbreviation: a-plans) are written by agent designers and are available for use when the agent is deployed. Roughly speaking, belief state condition plans (abbreviation: b-plans) are automatically generated by a POMDP planner and stored when no a-plan is found which \u2018matches\u2019 the agent\u2019s current belief state and intention set.\nPolicies in a-plans are of two kinds:\nDefinition 1 (Most likely context) An a-plan most-likely-context policy is either an action or has the form a : {(\u03a61, \u03c01), . . . , (\u03a6n, \u03c0n)}, where a is an action, the \u03a6i are contexts, and each of the \u03c0i is one of the two kinds of a-plan policies.\nAt belief state B, the degree of belief of \u03a6 is Degree(\u03a6, B) := \u2211\ns\u2208S,s \u03a6\nB(s).\nWe abbreviate \u201cmost-likely-context\u201d as \u2018ml\u2019. If an ml policy \u03c0 = a : ML is adopted for execution and it is not simply an action, then a is executed, an observation is received, the current belief state is updated to B\u2032 and finally the policy which is paired with the most likely context is executed \u2013 that is,\narg max \u03c0\u2032: (\u03a6\u2032,\u03c0\u2032)\u2208ML\nDegree(\u03a6\u2032, B\u2032)\nis executed.\nDefinition 2 (First applicable context) An a-plan first-applicable-context policy is either an action or has the form\na : \u3008(\u03a61 ./ p1, \u03c01), . . . , (\u03a6n ./ pn, \u03c0n)\u3009,\nwhere a is an action, the \u03a6i are contexts, ./:= {\u2264,\u2265}, the pi are probabilities, and each of the \u03c0i is one of the two kinds of a-plan policies.\nWe abbreviate \u201cfirst-applicable-context\u201d as \u2018fa\u2019. If an fa policy \u03c0 = a : FA is adopted for execution and it is not simply an action, then a is executed, an observation is received, the current belief state is updated to B\u2032 and finally the policy which is paired with the first context which satisfies its probability inequality is executed - that is, \u03c0i is executed such that Degree(\u03a6i, B\n\u2032) ./ pi and (\u03a6i ./ pi, \u03c0i) \u2208 FA and there is no (\u03a6j ./ pj, \u03c0j) \u2208 FA such that j < i for which Degree(\u03a6j, B\n\u2032) ./ pj. If no context in the sequence \u3008(\u03a61 ./ p1, \u03c01), . . . , (\u03a6n ./ pn, \u03c0n)\u3009 satisfies its inequality, the a-plan of which the policy is a part is regarded as having finished, that is, the control loop is then in a position where a fresh plan in the plan library is sought.\nIn the following example a-plan policy, an agent must move around in a six-by-six grid world to collect items. Suppose the plan selected from the library is I : \u03a6 : \u03c0 with I being {six\u2212one, collect}, \u03a6 being\n((direction : North) \u2228 (direction : West)) \u2227 \u00ac(x\u2212coord : 6) \u2227 \u00ac(y\u2212coord : 1)\nand \u03c0 being\nmove\u2212forward : { ((item\u2212here : yes), take\u2212item), ((item\u2212here : no), move\u2212forward : \u3008\n((x\u2212coord : 6), (direction : North) \u2265 0.9, turn\u2212left), ((y\u2212coord : 1), (direction : West) \u2265 0.9, turn\u2212right), (true \u2264 1), move\u2212forward)\u3009}\nOne can see that \u03c0 itself is an ml policy, but embedded inside it is an fa policy. Suppose that the agent currently has a belief state Bcur and an intention set Icur . First, the agent will scan through all a-plans, selecting all those which \u2018match\u2019 Icur . From this set, the agent will execute the policy \u03c0 of the a-plan I : \u03a6 : \u03c0 whose attribute condition has the highest degree of belief at Bcur . If the set of a-plans matching Icur is empty, the agent will scan through all b-plans, selecting all those which \u2018match\u2019 Icur . From this set, the agent will execute the policy \u03c0 of the b-plan I : B : \u03c0 whose belief state is \u2018most similar\u2019 to Bcur . If the set of b-plans matching Icur is empty, or there is no b-plan with belief state similar to Bcur , then the agent will generate policy \u03c0cur , execute it and store Icur : Bcur : \u03c0cur in the plan library for possible reuse later. The high-level planning process is depicted by the diagram in Figure 3.\nTo \u201cexecute policy \u03c0\u201d (where \u03c0 has horizon/depth h) means to perform h actions as recommended by \u03c0. No policy will be sought in the library, nor will a new policy be generated until the action recommendations of the current policy being executed have been \u2018exhausted\u2019. One may be concerned that a policy becomes \u2018stale\u2019 or inapplicable while being executed, and that seeking or generating \u2018fresh\u2019 policies at every iteration keeps action selection relevant in a dynamic world. However, written policies (in a-plans) should preferably have the form of generated policies, and generated policies (in b-plans) can deal with all situations understood by the agent: It is assumed that each observation distinguishable by the agent, identifies a particular state of the world, as far as the agent\u2019s sensors allow. Hence, if a policy considers every observation at its choice nodes, the policy will have a recommended (for written policies)\nor optimal (for generated policies) action, no matter the state of the world. However, writing or generating policies with far horizons (e.g., h > 7) is impractical. With large h, an agent will take relatively long to generate a policy and thus lose its reactiveness. Reactiveness is especially important in highly dynamic environments.\nWith respect to a-plans, whether two intention sets match will be determined by how many goals they have in common. Thus, the similarity between Icur and I lib can be determined as follows.\nIS (Icur , I lib) :=\n\u2211 g\u2208G,g\u2208Icur ,g\u2208I lib 1\n|Icur \u222a I lib| .\nIS (\u00b7) lies in [0, 1]. Icur and I lib need not have equal cardinality. Larger values of IS (\u00b7) mean more similarity / closer match. The agent designer can decide what value of IS (I, I \u2032) constitutes a \u2018match\u2019 between I and I \u2032 (see the discussion on \u201cthresholds\u201d below).\nWhat constitutes a match between intention sets with respect to b-plans is different: Policies generated at two times t and t\u2032 might be significantly different for the same (similar) context(s) if the satisfaction levels of the intentions are significantly different at the two times. This is an important insight because policies of b-plans are generated, not written. Even though IS (Icur , I lib) may constitute a \u2018match\u2019 (with I lib in a b-plan), \u03c0lib might be completely impractical for pursuing Icur . The measure of similarity will be the sum of differences between\nAlgorithm 2: FindPolicy\nInput: Bcur : current belief state Input: Icur : current intention set Input: \u03b8i: intention-set threshold Input: \u03b8b: belief state threshold Input: PlanLib: the plan library Input: h: planning horizon / policy depth Output: A POMDP policy of depth h\n1 ApplicablePlans \u2190 {I lib : \u03a6 : \u03c0lib \u2208 PlanLib | IS (Icur , I lib) \u2265 \u03b8i}; 2 if ApplicablePlans 6= \u2205 then 3 return arg max\u03c0lib: Ilib:\u03a6:\u03c0lib\u2208ApplicablePlans Degree(\u03a6, B cur); 4 if ApplicablePlans = \u2205 then 5 ApplicablePlans \u2190 {I lib : Blib : \u03c0lib \u2208 PlanLib | BS (Icur , I lib , Bcur, Blib) \u2265 \u03b8i}; 6 if ApplicablePlans 6= \u2205 then 7 ApplicablePlans \u2190 {I lib : Blib : \u03c0lib \u2208 ApplicablePlans | R\u2032(Blib, Bcur) \u2265 \u03b8b}; 8 if ApplicablePlans 6= \u2205 then 9 return arg max\u03c0lib: Ilib:Blib:\u03c0lib\u2208ApplicablePlans R \u2032(Blib, Bcur);\n10 if ApplicablePlans = \u2205 then 11 \u03c0cur \u2190 Policy(Bcur , Icur , h); 12 Add Icur : Bcur : \u03c0cur to PlanLib; 13 return \u03c0cur;\nsatisfaction levels. Note that an intention\u2019s satisfaction levels can only be compared if the intention appears in both intention sets under consideration. We denote the similarity between two intention sets Icur and I lib as BS (Icur , I lib , Bcur , Blib) and define it as follows.\nBS (Icur , I lib , Bcur , Blib) :=\n\u2211 g\u2208G,g\u2208Icur ,g\u2208I lib 1\u2212 ||\u03c3g(Bcur)\u2212 \u03c3g(Blib)||\n|Icur \u222a I lib| .\nwhere ||x|| denotes the absolute value of x. BS (\u00b7) lies in [0, 1]. Icur and I lib need not have equal cardinality. Larger values of BS (\u00b7) mean more similarity / closer match. The agent designer can decide what value of BS (I, I \u2032) constitutes a \u2018match\u2019 between I and I \u2032.\nFor a fixed pair of intention sets, BS (Icur , I lib , Bcur , Blib) \u2264 IS (Icur , I lib). That is, BS (\u00b7) is a stronger measure of similarity than IS (\u00b7). This is because with BS (\u00b7), intention satisfaction levels must also be similar. The stronger measure is required to filter out b-plans that seem similar when judged only on the commonality of their intentions, but not on their satisfaction levels. And there may be several b-plans in the library which would be judged similar by IS (\u00b7), but they have been added to the library exactly because they are indeed different when their satisfaction levels are taken into account. The following example should make this clear. Suppose that the following two b-plans are in the library: {g1, g4} : B1 : \u03c01 and {g1, g4} : B2 : \u03c02, where B1 = {(s1, 0.95), (s2, 0.05), (s3, 0), (s4, 0)} and B2 = {(s1, 0), (s2, 0), (s3, 0.05), (s4, 0.95)}. And suppose g1 is most satisfied when the agent is in s1, and g4 is most satisfied when the agent is in s4. A policy to pursue {g1, g4} when starting in B1 would rather suggest actions to move towards s4, while a policy to pursue {g1, g4} when starting in B2 would rather suggest actions to move towards s1. The point is that although the two b-plans are identical with respect to the intention set, they have very different policies, due to their different belief states (and thus satisfaction levels).\nWe now prepare for the definition of similarity between two belief states. The \u2018directed divergence\u2019 (Kullback, 1968; Csisza\u0301r, 1975) of belief state C from belief state B is defined as\nR(C,B) := \u2211 s\u2208S C(s) ln C(s) B(s) .\nR(C,B) is undefined when B(s) = 0 while C(s) > 0. When C(s) = 0, then R(C,B) = 0 because limx\u21920 ln(x) = 0. Let\n\u03a0(B) := {C \u2032 \u2208 \u03a0 | \u2200s \u2208 S, if C \u2032(s) > 0, then B(s) > 0},\nwhere \u03a0 is the set of all probability distributions over the states S (i.e., all belief states which can be induced from S). That is, \u03a0(B) is the set of belief states which keep R(C \u2032, B) defined. Let\nmaxR(B) := max C\u2032\u2208\u03a0(B)\nR(C \u2032, B),\nFor our purposes, we can define R(C,B) as maxR(C,B) whenever it would normally be undefined. We define a slightly modified cross-entropy R\u2032 as\nR\u2032(C,B) :=\n{ R(C,B) if R(C,B) is defined\nmaxR(B) otherwise\nFinally, the similarity between the current belief state Bcur and the belief state Blib in a plan in the library is R\u2032(Blib, Bcur).\nTwo thresholds are involved with determining when library plans are applicable and how plans are dealt with: the intention-set threshold (abbreviation: \u03b8i) and the belief-state threshold (abbreviation: \u03b8b). The former is involved in both a-plans and b-plans, and the latter is involved only in b-plans.\nThe FindPolicy procedure (Algo. 2) formally defines what policy the agent will execute whenever the agent seeks a policy, and the procedure defines when and how new plans are added to the plan library."}, {"heading": "5 Simulations", "text": "We performed some tests on an HPB agent in two domains: a six-by-six grid-world and a three-battery system. In the experiments which follow, the threshold \u03b8 is set to 0.05, MRY is set to 5 and h = 3. Desire levels are initially set to zero for all goals. For each experiment, 10 trials were run. The plan library is not made use of.\nIn the grid-world, the agent\u2019s task is to visit each of the four corners, and to collect twelve items randomly scattered. The goals are {(1, 1), (1, 6), (6, 1), (6, 6), collect}, and (1, 1), (1, 6), (6, 1) and (6, 6) are marked mutually incompatible. That is,\n\u2022 Cpbl((1, 1)) = {(1, 1), collect},\n\u2022 Cpbl((1, 6)) = {(1, 6), collect},\n\u2022 Cpbl((6, 1)) = {(6, 1), collect},\n\u2022 Cpbl((6, 6)) = {(6, 6), collect},\n\u2022 Cpbl(collect) = {collect, (1, 1), (1, 6), (6, 1), (6, 6)}.\nStates are quadruples \u3008x, y, d, i\u3009, with x, y \u2208 {1, \u00b7 \u00b7 \u00b7 , 6} being the coordinates of the agent\u2019s position in the world, d \u2208 {North, East, West, South} the direction it is facing, and i \u2208 {0, 1}, i = 1 if an item is present in the cell with the agent, else i = 0. The agent can perform five actions {left, right, forward, see, take}, meaning, turn left, turn right, move one cell forward, see whether an item is present and take an item. The only observation possible when executing one of the physical actions is obsNil, the null observation, and see has possible observations from the set {0, 1} for whether the agent sees the presence of an item (1) or not (0).\nNext, we define the possible outcomes for each action: When the agent turns left or right, it can get stuck in the same direction, turn 90\u25e6 or overshoots by 90\u25e6. When the agent moves forward, it moves one cell in the direction it is facing or it gets stuck and does not move. The agent can see an item or see nothing (no item in the cell), and taking is deterministic (if there is an item present, it will be collected with certainty, if the agent executes take). All actions except take are designed so that the correct outcome is achieved 95% of the time and incorrect outcomes are achieved 5% of the time.\nSeven experiments were performed, with different weight-combinations (\u03b1g) assigned for each experiment. For each trial, the agent starts in a random location and performs 100 actions. When g \u2208 {(1, 1), (1, 6), (6, 1), (6, 6)}, we let \u03c3g(s) = 1 \u2212 dist/10 where 10 is the maximum Manhattan distance between two cells in the world and dist is the Manhattan distance between the cells represented by g and s, and we let \u03c3collect(s) = 1\u2212dist/10, where dist is the Manhattan distance between the cell represented by s and the closest cell containing an item.\nIn the BatryPack domain, the agent\u2019s task is to keep a pack of rechargeable batteries within a given voltage range. There are three batteries available, each with a maximum capacity of 6 volts. For every time-unit that a battery is in the pack, it loses 1 volt. For every time-unit a battery is out of the pack, it gains 1 volt. The pack is considered to be within range if the sum of the batteries currently in the pack is in [3, 9]. (The possible pack-voltage-range is [0, 18]). The goals are {maintain, charge}, meaning that the agent should, respectively, try to keep the pack within range, and charge the batteries. The goals are, intuitively, not mutually exclusive. States are of the form \u3008(io1, v1), (io2, v2), (io3, v3)\u3009, with ioj being either in or out, indicating whether battery j is in or out of the pack, and vj is the batteries current voltage. The agent can perform ten actions: no\u2212op and for j = 1, 2, 3, removej, addj and measurej, meaning, remove battery j from the pack, add battery j to the pack, respectively, measure the current voltage available in battery j. The only observation possible when executing one of the physical actions is obsNil; measurej has possible observations from the set {0, 1, 2, 3, 4, 5, 6}. When the agent removes or adds a battery, it may fail to do so with a 5% chance. The measurement action is deterministic, but 5% of the time it will perceive a voltage one volt more or less than it is actually.\nSix experiments were performed with different weights (W (g)) assigned for each experiment - three while the goals were mutually incompatible and three while the goals may be pursued simultaneously. For each trial, the initial state is \u3008(in, 6), (out, 3), (out, 3)\u3009 with the initial intention being maintain - and the agent/system performs 50 actions. We let \u03c3maintain(s) equal 1 if the battery pack is within range, else it equals a value less than 1 (min. 0) in proportion to how far the pack voltage is from being within range. We let \u03c3charge(s) = c/3 where c is the number of batteries which are out of the pack."}, {"heading": "5.1 Evaluation of HPB Agent Performance", "text": "Table 1 shows the results. It can be seen quite clearly that the agent can be directed to certain corners and to collect items with a dedication proportional to the weights chosen by the agent designer for the respective goals.\nTable 2: Performance of the BatryPack system for various combinations of goal-weights, for mutually compatible (joint) and incompatible (disjoint) goals.\nJoint maintain\nweight 0.2 0.5 0.8\ncharge weight 0.8 0.5 0.2 % within range 50 65.8 56.6\n% one/two intentions\n19/81 18/82 58/42\nDisjoint maintain\nweight 0.2 0.5 0.8\ncharge weight 0.8 0.5 0.2 % within range 52 53.6 48.8\n% one/two intentions\n100/0 100/0 100/0\nTable 2 shows the results. It can be seen that the system performs better when the goals are pursued jointly, particularly when maintaining pack voltage and trying to charge the batteries have equal weights. In the table, \u201c% one/two intentions\u201d means that for x/y, the percentage of time that there was exactly one intention in I is x and the percentage of time that there were exactly two intentions in I is y.\nThese experiments highlight four important features of the HPB architecture: (1) Each of several goals can be pursued individually until satisfactorily achieved. (2) Goals must periodically be re-achieved. (3) The trade-off between (weights of) goals can be set effectively. (4) Goals can be satisfied even while dealing with stochastic actions and perceptions."}, {"heading": "6 Related Work", "text": "AgentSpeak+ (Bauters et al., 2015) extends the BDI language AgentSpeak (Rao, 1996) with on-demand probabilistic planning in uncertain environments. AgentSpeak has a plan library of plans, each plan being of the form\ne : b1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 bm \u2190 c1; \u00b7 \u00b7 \u00b7 ; cn\nwhere e is a triggering event, b1, . . . , bm are belief literals and c1, . . . , cn are actions or goals. Goals may become (internal) triggering events. Events in the (external) environment may also be perceived as triggering events. As triggering events occur, they are placed in a set and periodically selected for processing. An event is \u2018processed\u2019 by selecting an appropriate plan from the plan library with a matching triggering event. A plan is appropriate if its context b1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 bm is a logical consequence of the agent\u2019s set of base beliefs. The goals and/or actions c1; \u00b7 \u00b7 \u00b7 ; cn of the selected appropriate plan will be processed in sequence. If ci is an action,\nit is executed; if it is a goal, it becomes an internal event which may trigger the selection and execution of further plans. An AgentSpeak agent maintains a set of intentions and each intention is a stack of plans. Please refer to (Rao, 1996) for details. When considering HPB plans, e is roughly analogous to I, b1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 bm is roughly analogous to \u03a6 or B and c1; \u00b7 \u00b7 \u00b7 ; cn is roughly analogous to \u03c0.\nThe contribution of AgentSpeak+ is to allow a POMDP planner to suggest the optimal action at a point in a (written) plan where the agent designer feels that an optimal action is required at that point, or that there is insufficient information at the time of writing the plan to suggest a reasonable action. In other words, there might be points in a plan when actions are best chosen just before execution so that they can be determined appropriately for the agent\u2019s current context.\nBauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach.\nAgentSpeak+ does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak+ is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans.\nSome slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online.\nLim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent. Their agents reason with several classes of emotion and their agents are supposed to portray emotional behavior, not simply to solve problems, but to look believable to humans. Their architecture has a \u201ccontinuous planner [...] that is capable of partial order planning and includes emotion-focused coping [...]\u201d Their work has a different application to ours, however, we could take inspiration from them to improve the HPB architecture.\nPereira et al. (2008) take a different approach to use POMDPs to improve BDI agents. By leveraging the relationship between POMDP and BDI models, as discussed by Simari and Parsons (2006), they devised an algorithm to extract BDI plans from optimal POMDP policies. The main difference to our work is that their policies are pre-generated and BDI-style rules are extracted for all contingencies. The advantage is that no (time-consuming) online plan/policy generation is necessary. The disadvantage of their approach is that all the BDI plans must be stores and every time the domain model changes, a new POMDP must be solved and the policyto-BDI-plan algorithm must be run. It is not exactly clear from their paper (Pereira et al., 2008) how or when intentions are chosen. Although it is interesting to know the relationship between POMDPs and BDI models (Simari and Parsons, 2006, 2011), we did not use any of these insights in developing our architecture. However, the fact that the HPB architecture does\nintegrate the two frameworks, is probably due to the existence of the relationship. Rens et al. (2009) also introduced a hybrid POMDP-BDI architecture, but without a notion of desire levels or satisfaction levels. Although their basic approaches to combine the POMDP and BDI frameworks is the same as ours, there are at least three major differences: Firstly, they define their architecture in terms of the GOLOG agent language (Boutilier et al., 2000). Secondly, their approach uses a computationally intensive method for deciding whether to refocus; performing short policy look-aheads to ascertain the most valuable goal to pursue.1 Our approach seems much more efficient. Thirdly, in their approach, the agent cannot pursue several goals concurrently.\nChen et al. (2013) incorporate probabilistic graphical models into the BDI framework for plan selection in stochastic environments. An agent maintains epistemic states (with random variables) to model the uncertainty about the stochastic environment, and corresponding belief sets of the epistemic state are defined. The possible states of the environment, according to sensory observations, and their relationships are modeled using probabilistic graphical models: The uncertainty propagation is carried out by Bayesian Networks and belief sets derived from the epistemic states trigger the selection of relevant plans from a plan library. For cases when more than one plan is applicable due to uncertainty in an agent\u2019s beliefs, they propose a utilitydriven approach for plan selection, where utilities of actions are modeled in influence diagrams. Our architecture is different in that it does not have a library of pre-supplied plans; in our architecture, policies (plans) are generated online.\nNone of the approaches mentioned maintain desire levels for selecting intentions. The benefit of maintaining desire levels is that intentions are not selected only according what they offer with respect to their current expected reward, but also according to when last they were achieved."}, {"heading": "7 Conclusion", "text": "Our work focuses on providing high-level decision-making capabilities for robots and agents who live in dynamic stochastic environments, where multiple goals and goal types must be pursued. We introduced a hybrid POMDP-BDI agent architecture, which may display emergent behavior, driven by the intensities of their desires. In the past decade, several BDIAs have been augmented with capabilities to deal with uncertainty. The HPB architecture is novel in that it can pursue multiple goals concurrently. Goals must periodically be re-achieved, depending on the goals\u2019 desire levels, which change over time and in proportion to how close the goals are to being satisfied.\nA major benefit of the HPB architecture is that every action recommended by a generated policy simultaneously maximizes the agent\u2019s reward with respect to pursuit of all the current intentions. As far as the authors are aware, no other agent architecture is capable of this.\nIn previous work (Rens and Meyer, 2015), we argued that maintenance goals like avoiding moist areas (or collecting soil samples) should rather be viewed as a preference and modeled as a POMDP reward function. And specific tasks to complete (like collecting gas or keeping its battery charged) should be modeled as BDI desires. The idea is that while the agent is pursuing goals, it can concurrently perform rewarding actions not directly related to its goals. The architecture reported about in this paper does not make a clear distinction between overt and maintenance goals. In the new version of the architecture, that distinction can be simulated, however, now goals can be pursued in a much more fine-grained way via the choice of goal-weights (W (g)).\n1Essentially, the goals in G are stacked in descending order of the value of V \u2217HPB (B, g, h \u2212), where h\u2212 < h\nand B is the current belief state. The goal on top of the stack becomes the intention.\nAnother important feature brought into the new version is the ability to mark sets of goals as disjoint thereby forcing the agent to never pursue these goals concurrently, that is, disjoint goals will never be in I simultaneously.\nAlthough Nair and Tambe (2005) and Chen et al. (2013) call their approaches hybrid, our architecture can arguably more confidently be called hybrid because of its more intimate integration of POMDP and BDI concepts.\nWe could take some advice from Antos and Pfeffer (2011). They provide a systematic methodology to incorporate emotion into a decision-theoretic framework, and also provide \u201ca principled, domain-independent methodology for generating heuristics in novel situations\u201d.\nPolicies returned by Plan as defined in this paper are optimal. A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.\nEvaluating the proposed architecture in richer domains would highlight problems in the architecture and indicate new directions for research and development in the area of hybrid POMDP-BDI architectures.\nThe expressivity of the language we use for describing goals and for writing conditions in a-plans is relatively low. AgentSpeak, for instance, has a richer language. The language\u2019s expressivity is mostly independent of the architecture. We thus chose to use a simple language to better focus on the components we want to discuss.\nThe design of the HPB agent architecture is a medium-to-long-term programme. We would like to keep improving its capabilities to deal with unforeseen, complex events in a changing, noisy environment. The next step is to rigorously test the architecture using an HPB agent in a complex simulated world. In particular, HPB agents with a plan library, including (pre-written) a-plans and (generated) b-plans, must still be assessed. There is also scope for improving the focussing procedure. And analyzing under what conditions the two forms of desire update rule produce better performance must be investigated.\nThere may be better methods for learning than policy reuse. Policy reuse has its place when reasoning time or power is limited, but given the time and power, more sophisticated techniques could perhaps generate and store shorter, more effective plans. For instance, when an agent encounters a landmark with relatively high certainty, the landmark\u2019s location can be stored. The agent could then augment its sensor readings with the stored location data to reach the landmark more easily in future. Some objects in the environment might not be stable, and their location data should \u2018degrade\u2019 over time in proportion to the environment\u2019s dynamism.\nSingh et al. (2011) provide a method for learning which (pre-written) plans in a BDI system should be executed in which contexts (given a selection of context-applicable plans). Their approach can also relearn context-plan matches as conditions change in dynamic environments. Future versions of the HPB architecture could benefit from ideas in their work.\nPrediction is an inherent part of POMDP planning, but we would like our agents to predict much farther into the future, and recognize critical events which it should deal with or avoid. POMDP policies and pre-written plans are more for local \u2018tactical\u2019 control. We need to bring in techniques for the agent to think globally or \u2018strategically\u2019.\nThe set of intentions might change while executing a policy. If the current set of intentions changes a lot, the current policy might become inapplicable. This is a typical BDI reconsideration issue. However, an HPB agent will usually only perform very few actions before seeking a new plan. Just as in the case with humans, our agent should normally not get in trouble by assuming that things have not changed significantly in the last few steps. If the environment is so dynamic that relatively short plans can become inappropriate before completion of the plans, then the agent should have some more low-level, reactive systems to deal with\nthe changes. In highly dynamical environments, the HPB \u2018agent\u2019 is better suited to being the high-level reasoning module of a larger system."}], "references": [{"title": "Using emotions to enhance decision-making", "author": ["D. Antos", "A. Pfeffer"], "venue": "Walsh, T., editor, Proceedings of the Twenty-second Intl. Joint Conf. on Artif. Intell. (IJCAI-11), pages 24\u201330, Menlo Park, CA. AAAI Press.", "citeRegEx": "Antos and Pfeffer,? 2011", "shortCiteRegEx": "Antos and Pfeffer", "year": 2011}, {"title": "Probabilistic planning in agentspeak using the pomdp framework", "author": ["K. Bauters", "K. McAreavey", "J. Hong", "Y. Chen", "W. Liu", "L. Godo", "C. Sierra"], "venue": "Hatzilygeroudis, I., Palade, V., and Prentzas, J., editors, Combinations of Intelligent Methods and Applications: Proceedings of the Fourth Intl. Workshop, CIMA 2014, volume 46 of Smart Innovation, Systems and Technologies. Springer.", "citeRegEx": "Bauters et al\\.,? 2015", "shortCiteRegEx": "Bauters et al\\.", "year": 2015}, {"title": "Decision-theoretic, high-level agent programming in the situation calculus", "author": ["C. Boutilier", "R. Reiter", "M. Soutchanski", "S. Thrun"], "venue": "Proceedings of the Seventeenth Natl. Conf. on Artif. Intell. (AAAI-00) and of the Twelfth Conf. on Innovative Applications of Artif. Intell. (IAAI-00), pages 355\u2013362. AAAI Press, Menlo Park, CA.", "citeRegEx": "Boutilier et al\\.,? 2000", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Intention, Plans, and Practical Reason", "author": ["M. Bratman"], "venue": "Harvard University Press, Massachusetts/England.", "citeRegEx": "Bratman,? 1987", "shortCiteRegEx": "Bratman", "year": 1987}, {"title": "Learning to explore and exploit in pomdps", "author": ["C. Cai", "X. Liao", "L. Carin"], "venue": "NIPS, pages 198\u2013206.", "citeRegEx": "Cai et al\\.,? 2009", "shortCiteRegEx": "Cai et al\\.", "year": 2009}, {"title": "Incorporating PGMs into a BDI architecture", "author": ["Y. Chen", "J. Hong", "W. Liu", "L. Godo", "C. Sierra", "M. Loughlin"], "venue": "Boella, G., Elkind, E., Savarimuthu, B., Dignum, F., and Purvis, M., editors, PRIMA 2013: Principles and Practice of Multi-Agent Systems, volume 8291 of Lecture Notes in Computer Science, pages 54\u201369. Springer, Berlin/Heidelberg.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "I-divergence geometry of probability distributions and minimization problems", "author": ["I. Csisz\u00e1r"], "venue": "Annals of Probability, 3:146\u2013158.", "citeRegEx": "Csisz\u00e1r,? 1975", "shortCiteRegEx": "Csisz\u00e1r", "year": 1975}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artif. Intell., 101(1\u20132):99\u2013134.", "citeRegEx": "Kaelbling et al\\.,? 1998", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Commitment and effectiveness of situated agents", "author": ["D. Kinny", "M. Georgeff"], "venue": "Proceedings of the 12th Intl. Joint Conf. on Artif. Intell. (IJCAI-91), pages 82\u201388.", "citeRegEx": "Kinny and Georgeff,? 1991", "shortCiteRegEx": "Kinny and Georgeff", "year": 1991}, {"title": "Experiments in optimal sensing for situated agents", "author": ["D. Kinny", "M. Georgeff"], "venue": "Proceedings of the the Second Pacific Rim Intl. Conf. on Artif. Intell. (PRICAI-92).", "citeRegEx": "Kinny and Georgeff,? 1992", "shortCiteRegEx": "Kinny and Georgeff", "year": 1992}, {"title": "Agent-centered search", "author": ["S. Koenig"], "venue": "Artif. Intell. Magazine, 22:109\u2013131.", "citeRegEx": "Koenig,? 2001", "shortCiteRegEx": "Koenig", "year": 2001}, {"title": "Information theory and statistics, volume 1", "author": ["S. Kullback"], "venue": "Dover, New York, 2nd edition.", "citeRegEx": "Kullback,? 1968", "shortCiteRegEx": "Kullback", "year": 1968}, {"title": "Towards solving large-scale POMDP problems via spatio-temporal belief state clustering", "author": ["X. Li", "W. Cheung", "J. Liu"], "venue": "Proceedings of IJCAI-05 Workshop on Reasoning with Uncertainty in Robotics (RUR-05).", "citeRegEx": "Li et al\\.,? 2005", "shortCiteRegEx": "Li et al\\.", "year": 2005}, {"title": "Improving adaptiveness in autonomous characters", "author": ["M. Lim", "J. Dias", "R. Aylett", "A. Paiva"], "venue": "Prendinger, H., Lester, J., and Ishizuka, M., editors, Intelligent Virtual Agents, volume 5208 of Lecture Notes in Computer Science, pages 348\u2013355. Springer, Berlin/Heidelberg.", "citeRegEx": "Lim et al\\.,? 2008", "shortCiteRegEx": "Lim et al\\.", "year": 2008}, {"title": "A survey of algorithmic methods for partially observed Markov decision processes", "author": ["W. Lovejoy"], "venue": "Annals of Operations Research, 28:47\u201366.", "citeRegEx": "Lovejoy,? 1991", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Incorporating planning into BDI systems", "author": ["F. Meneguzzi", "A. Zorzo", "M. M\u00f3ra", "M.", "L."], "venue": "Scalable Computing: Practice and Experience, 8(1):15\u201328.", "citeRegEx": "Meneguzzi et al\\.,? 2007", "shortCiteRegEx": "Meneguzzi et al\\.", "year": 2007}, {"title": "A survey of partially observable Markov decision processes: Theory, models, and algorithms", "author": ["G. Monahan"], "venue": "Management Science, 28(1):1\u201316.", "citeRegEx": "Monahan,? 1982", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "Introduction to AI Robotics", "author": ["R. Murphy"], "venue": "MIT Press, Massachusetts/England.", "citeRegEx": "Murphy,? 2000", "shortCiteRegEx": "Murphy", "year": 2000}, {"title": "Hybrid bdi-pomdp framework for multiagent teaming", "author": ["R. Nair", "M. Tambe"], "venue": "J. Artif. Intell. Res.(JAIR), 23:367\u2013420.", "citeRegEx": "Nair and Tambe,? 2005", "shortCiteRegEx": "Nair and Tambe", "year": 2005}, {"title": "Real-time decision making for large POMDPs", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": "Advances in Artif. Intell.: Proceedings of the Eighteenth Conf. of the Canadian Society for Computational Studies of Intelligence, volume 3501 of Lecture Notes in Computer Science, pages 450\u2013455. Springer Verlag.", "citeRegEx": "Paquet et al\\.,? 2005", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Constructing bdi plans from optimal pomdp policies, with an application to agentspeak programming", "author": ["D. Pereira", "L. Gon\u00e7alves", "G. Dimuro", "A. Costa"], "venue": "G. Henning, M. G. and Goneet, S., editors, XXXIV Confer\u00eancia Latinoamericano de Inform\u00e1tica, Santa Fe. Anales CLEI 2008, pages 240\u2013249.", "citeRegEx": "Pereira et al\\.,? 2008", "shortCiteRegEx": "Pereira et al\\.", "year": 2008}, {"title": "Introducing the Tileworld: Experimentally evaluating agent architectures", "author": ["M. Pollack", "M. Ringuette"], "venue": "Proceedings of the Eighth Conf. on Artif. Intell., pages 183\u2013189. AAAI Press.", "citeRegEx": "Pollack and Ringuette,? 1990", "shortCiteRegEx": "Pollack and Ringuette", "year": 1990}, {"title": "AgentSpeak(L): BDI agents speak out in a logical computable language", "author": ["A. Rao"], "venue": "Proceedings of the 7th European Workshop on Modelling Autonomous Agents in a MultiAgent World (MAAMAW-96), pages 42\u201355, Berlin/Heidelberg. Springer Verlaag.", "citeRegEx": "Rao,? 1996", "shortCiteRegEx": "Rao", "year": 1996}, {"title": "BDI agents: From theory to practice", "author": ["A. Rao", "M. Georgeff"], "venue": "Proceedings of the ICMAS-95, pages 312\u2013319. AAAI Press.", "citeRegEx": "Rao and Georgeff,? 1995", "shortCiteRegEx": "Rao and Georgeff", "year": 1995}, {"title": "A BDI agent architecture for a POMDP planner", "author": ["G. Rens", "A. Ferrein", "E. Van der Poel"], "venue": "Lakemeyer, G., Morgenstern, L., and Williams, M.-A., editors, Proceedings of the Ninth Intl. Symposium on Logical Formalizations of Commonsense Reasoning (Commonsense 2009), pages 109\u2013114, University of Technology, Sydney. UTSe Press.", "citeRegEx": "Rens et al\\.,? 2009", "shortCiteRegEx": "Rens et al\\.", "year": 2009}, {"title": "Hybrid POMDP-BDI: An agent architecture with online stochastic planning and desires with changing intensity levels", "author": ["G. Rens", "T. Meyer"], "venue": "Duval, B., Van den Herik, J., Loiseau, S., and Filipe, J., editors, Proceedings of the Seventh Intl. Conf. on Agents and Artif. Intell. (ICAART), Revised Selected Papers, LNAI, pages 79\u201399. Springer Verlaag.", "citeRegEx": "Rens and Meyer,? 2015", "shortCiteRegEx": "Rens and Meyer", "year": 2015}, {"title": "Online planning algorithms for POMDPs", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-draa"], "venue": "Journal of Artif. Intell. Research (JAIR), 32:663\u2013704.", "citeRegEx": "Ross et al\\.,? 2008", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Finding approximate POMDP solutions through belief compressions", "author": ["N. Roy", "G. Gordon", "S. Thrun"], "venue": "Journal of Artif. Intell. Research (JAIR), 23:1\u201340.", "citeRegEx": "Roy et al\\.,? 2005", "shortCiteRegEx": "Roy et al\\.", "year": 2005}, {"title": "Intention reconsideration in complex environments", "author": ["M. Schut", "M. Wooldridge"], "venue": "Proceedings of the the Fourth Intl. Conf. on Autonomous Agents (AGENTS-00), pages 209\u2013216, New York, NY, USA. ACM. 25", "citeRegEx": "Schut and Wooldridge,? 2000", "shortCiteRegEx": "Schut and Wooldridge", "year": 2000}, {"title": "The control of reasoning in resource-bounded agents", "author": ["M. Schut", "M. Wooldridge"], "venue": "The Knowledge Engineering Review, 16(3):215\u2013240.", "citeRegEx": "Schut and Wooldridge,? 2001", "shortCiteRegEx": "Schut and Wooldridge", "year": 2001}, {"title": "The theory and practice of intention reconsideration", "author": ["M. Schut", "M. Wooldridge", "S. Parsons"], "venue": "Experimental and Theoretical Artif. Intell., 16(4):261\u2013293.", "citeRegEx": "Schut et al\\.,? 2004", "shortCiteRegEx": "Schut et al\\.", "year": 2004}, {"title": "Forward search value iteration for POMDPs", "author": ["G. Shani", "R. Brafman", "S. Shimony"], "venue": "de Mantaras, R. L., editor, Proceedings of the Twentieth Intl. Joint Conf. on Artif. Intell. (IJCAI-07), pages 2619\u20132624, Menlo Park, CA. AAAI Press.", "citeRegEx": "Shani et al\\.,? 2007", "shortCiteRegEx": "Shani et al\\.", "year": 2007}, {"title": "A survey of point-based pomdp solvers", "author": ["G. Shani", "J. Pineau", "R. Kaplow"], "venue": "Autonomous Agents and Multi-Agent Systems, 27(1):1\u201351.", "citeRegEx": "Shani et al\\.,? 2013", "shortCiteRegEx": "Shani et al\\.", "year": 2013}, {"title": "On the relationship between mdps and the bdi architecture", "author": ["G. Simari", "S. Parsons"], "venue": "Proceedings of the Fifth Intl. Joint Conf. on Autonomous Agents and Multiagent Systems, AAMAS \u201906, pages 1041\u20131048, New York, NY, USA. ACM.", "citeRegEx": "Simari and Parsons,? 2006", "shortCiteRegEx": "Simari and Parsons", "year": 2006}, {"title": "Markov Decision Processes and the Belief-Desire-Intention Model", "author": ["G. Simari", "S. Parsons"], "venue": "Springer Briefs in Computer Science. Springer, New York, Dordrecht, Heidelberg, London.", "citeRegEx": "Simari and Parsons,? 2011", "shortCiteRegEx": "Simari and Parsons", "year": 2011}, {"title": "Integrating learning into a BDI agent for environments with changing dynamics", "author": ["D. Singh", "S. Sardina", "L. Padgham", "G. James"], "venue": "Walsh, T., editor, Proceedings of the Twenty-Second Intl. Joint Conf. on Artif. Intell. (IJCAI-11), pages 2525\u20132530, Menlo Park, CA. AAAI Press.", "citeRegEx": "Singh et al\\.,? 2011", "shortCiteRegEx": "Singh et al\\.", "year": 2011}, {"title": "Augmenting BDI agents with deliberative planning techniques", "author": ["A. Walczak", "L. Braubach", "A. Pokahr", "W. Lamersdorf"], "venue": "Bordini, R., Dastani, M., Dix, J., and Seghrouchni, A., editors, Proceedings of the Fourth Intl. Workshop of Programming MultiAgent Systems (ProMAS-06), pages 113\u2013127, Heidelberg/Berlin. Springer Verlag.", "citeRegEx": "Walczak et al\\.,? 2007", "shortCiteRegEx": "Walczak et al\\.", "year": 2007}, {"title": "Intelligent agents", "author": ["M. Wooldridge"], "venue": "Weiss, G., editor, Multiagent Systems: A Modern Approach to Distributed Artif. Intell., chapter 1. MIT Press, Massachusetts/England.", "citeRegEx": "Wooldridge,? 1999", "shortCiteRegEx": "Wooldridge", "year": 1999}, {"title": "Reasoning about Rational Agents", "author": ["M. Wooldridge"], "venue": "MIT Press, Massachusetts/England.", "citeRegEx": "Wooldridge,? 2000", "shortCiteRegEx": "Wooldridge", "year": 2000}, {"title": "An introduction to multiagent systems", "author": ["M. Wooldridge"], "venue": "John Wiley & Sons, Chichester, England. 26", "citeRegEx": "Wooldridge,? 2002", "shortCiteRegEx": "Wooldridge", "year": 2002}], "referenceMentions": [{"referenceID": 3, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 63, "endOffset": 102}, {"referenceID": 23, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 63, "endOffset": 102}, {"referenceID": 16, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 163, "endOffset": 193}, {"referenceID": 14, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 163, "endOffset": 193}, {"referenceID": 10, "context": "One solution to the intractability of POMDP policy generation is to employ a continuous planning strategy, or agent-centred search (Koenig, 2001).", "startOffset": 131, "endOffset": 145}, {"referenceID": 26, "context": "Aligned with agent-centred search is the forward-search approach or online planning approach in POMDPs (Ross et al., 2008).", "startOffset": 103, "endOffset": 122}, {"referenceID": 25, "context": "The Hybrid POMDP-BDI agent architecture (or HPB architecture, for short) has recently been introduced (Rens and Meyer, 2015).", "startOffset": 102, "endOffset": 124}, {"referenceID": 25, "context": "This article serves to introduce two significant extensions to the first iteration (Rens and Meyer, 2015) of the HPB architecture.", "startOffset": 83, "endOffset": 105}, {"referenceID": 3, "context": "See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al.", "startOffset": 19, "endOffset": 34}, {"referenceID": 3, "context": "See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al.", "startOffset": 19, "endOffset": 64}, {"referenceID": 3, "context": "See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al. (2004).", "startOffset": 19, "endOffset": 153}, {"referenceID": 7, "context": "Formally (Kaelbling et al., 1998), a POMDP is a tuple \u3008S,A, T,R, Z, P, b\u3009 with", "startOffset": 9, "endOffset": 33}, {"referenceID": 30, "context": "In BDI theory, one of the big challenges is to know when the agent should switch its current goal and what its new goal should be (Schut et al., 2004).", "startOffset": 130, "endOffset": 150}, {"referenceID": 25, "context": "A Hybrid POMDP-BDI (HPB) agent (Rens and Meyer, 2015) maintains (i) a belief state which is periodically updated, (ii) a mapping from goals to numbers representing the level of desire to achieve the goals, and (iii) the current set of intentions, the goals with the highest desire levels (roughly speaking).", "startOffset": 31, "endOffset": 53}, {"referenceID": 11, "context": "The \u2018directed divergence\u2019 (Kullback, 1968; Csisz\u00e1r, 1975) of belief state C from belief state B is defined as", "startOffset": 26, "endOffset": 57}, {"referenceID": 6, "context": "The \u2018directed divergence\u2019 (Kullback, 1968; Csisz\u00e1r, 1975) of belief state C from belief state B is defined as", "startOffset": 26, "endOffset": 57}, {"referenceID": 1, "context": "AgentSpeak (Bauters et al., 2015) extends the BDI language AgentSpeak (Rao, 1996) with on-demand probabilistic planning in uncertain environments.", "startOffset": 11, "endOffset": 33}, {"referenceID": 22, "context": ", 2015) extends the BDI language AgentSpeak (Rao, 1996) with on-demand probabilistic planning in uncertain environments.", "startOffset": 44, "endOffset": 55}, {"referenceID": 22, "context": "Please refer to (Rao, 1996) for details.", "startOffset": 16, "endOffset": 27}, {"referenceID": 20, "context": "It is not exactly clear from their paper (Pereira et al., 2008) how or when intentions are chosen.", "startOffset": 41, "endOffset": 63}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al.", "startOffset": 0, "endOffset": 1485}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations.", "startOffset": 0, "endOffset": 1513}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents.", "startOffset": 0, "endOffset": 1670}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online. Lim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent.", "startOffset": 0, "endOffset": 1910}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online. Lim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent. Their agents reason with several classes of emotion and their agents are supposed to portray emotional behavior, not simply to solve problems, but to look believable to humans. Their architecture has a \u201ccontinuous planner [...] that is capable of partial order planning and includes emotion-focused coping [...]\u201d Their work has a different application to ours, however, we could take inspiration from them to improve the HPB architecture. Pereira et al. (2008) take a different approach to use POMDPs to improve BDI agents.", "startOffset": 0, "endOffset": 2467}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online. Lim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent. Their agents reason with several classes of emotion and their agents are supposed to portray emotional behavior, not simply to solve problems, but to look believable to humans. Their architecture has a \u201ccontinuous planner [...] that is capable of partial order planning and includes emotion-focused coping [...]\u201d Their work has a different application to ours, however, we could take inspiration from them to improve the HPB architecture. Pereira et al. (2008) take a different approach to use POMDPs to improve BDI agents. By leveraging the relationship between POMDP and BDI models, as discussed by Simari and Parsons (2006), they devised an algorithm to extract BDI plans from optimal POMDP policies.", "startOffset": 0, "endOffset": 2633}, {"referenceID": 2, "context": "Although their basic approaches to combine the POMDP and BDI frameworks is the same as ours, there are at least three major differences: Firstly, they define their architecture in terms of the GOLOG agent language (Boutilier et al., 2000).", "startOffset": 214, "endOffset": 238}, {"referenceID": 22, "context": "Rens et al. (2009) also introduced a hybrid POMDP-BDI architecture, but without a notion of desire levels or satisfaction levels.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Although their basic approaches to combine the POMDP and BDI frameworks is the same as ours, there are at least three major differences: Firstly, they define their architecture in terms of the GOLOG agent language (Boutilier et al., 2000). Secondly, their approach uses a computationally intensive method for deciding whether to refocus; performing short policy look-aheads to ascertain the most valuable goal to pursue. Our approach seems much more efficient. Thirdly, in their approach, the agent cannot pursue several goals concurrently. Chen et al. (2013) incorporate probabilistic graphical models into the BDI framework for plan selection in stochastic environments.", "startOffset": 215, "endOffset": 560}, {"referenceID": 25, "context": "In previous work (Rens and Meyer, 2015), we argued that maintenance goals like avoiding moist areas (or collecting soil samples) should rather be viewed as a preference and modeled as a POMDP reward function.", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 27, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 19, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 12, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 31, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 26, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 4, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 32, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 13, "context": "Although Nair and Tambe (2005) and Chen et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 3, "context": "Although Nair and Tambe (2005) and Chen et al. (2013) call their approaches hybrid, our architecture can arguably more confidently be called hybrid because of its more intimate integration of POMDP and BDI concepts.", "startOffset": 35, "endOffset": 54}, {"referenceID": 0, "context": "We could take some advice from Antos and Pfeffer (2011). They provide a systematic methodology to incorporate emotion into a decision-theoretic framework, and also provide \u201ca principled, domain-independent methodology for generating heuristics in novel situations\u201d.", "startOffset": 31, "endOffset": 56}, {"referenceID": 0, "context": "We could take some advice from Antos and Pfeffer (2011). They provide a systematic methodology to incorporate emotion into a decision-theoretic framework, and also provide \u201ca principled, domain-independent methodology for generating heuristics in novel situations\u201d. Policies returned by Plan as defined in this paper are optimal. A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated. Evaluating the proposed architecture in richer domains would highlight problems in the architecture and indicate new directions for research and development in the area of hybrid POMDP-BDI architectures. The expressivity of the language we use for describing goals and for writing conditions in a-plans is relatively low. AgentSpeak, for instance, has a richer language. The language\u2019s expressivity is mostly independent of the architecture. We thus chose to use a simple language to better focus on the components we want to discuss. The design of the HPB agent architecture is a medium-to-long-term programme. We would like to keep improving its capabilities to deal with unforeseen, complex events in a changing, noisy environment. The next step is to rigorously test the architecture using an HPB agent in a complex simulated world. In particular, HPB agents with a plan library, including (pre-written) a-plans and (generated) b-plans, must still be assessed. There is also scope for improving the focussing procedure. And analyzing under what conditions the two forms of desire update rule produce better performance must be investigated. There may be better methods for learning than policy reuse. Policy reuse has its place when reasoning time or power is limited, but given the time and power, more sophisticated techniques could perhaps generate and store shorter, more effective plans. For instance, when an agent encounters a landmark with relatively high certainty, the landmark\u2019s location can be stored. The agent could then augment its sensor readings with the stored location data to reach the landmark more easily in future. Some objects in the environment might not be stable, and their location data should \u2018degrade\u2019 over time in proportion to the environment\u2019s dynamism. Singh et al. (2011) provide a method for learning which (pre-written) plans in a BDI system should be executed in which contexts (given a selection of context-applicable plans).", "startOffset": 31, "endOffset": 2480}], "year": 2016, "abstractText": "This article presents an agent architecture for controlling an autonomous agent in stochastic environments. The architecture combines the partially observable Markov decision process (POMDP) model with the belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent architecture takes the best features from the two approaches, that is, the online generation of reward-maximizing courses of action from POMDP theory, and sophisticated multiple goal management from BDI theory. We introduce the advances made since the introduction of the basic architecture, including (i) the ability to pursue multiple goals simultaneously and (ii) a plan library for storing pre-written plans and for storing recently generated plans for future reuse. A version of the architecture without the plan library is implemented and is evaluated using simulations. The results of the simulation experiments indicate that the approach is feasible.", "creator": "LaTeX with hyperref package"}}}