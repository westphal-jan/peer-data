{"id": "1607.06017", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2016", "title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "abstract": "We examine $k $-GenEV, the problem of finding the top $k $generalized eigenvectors, and $k $-CCA, the problem of finding the top $k $vectors in canonical correlation analysis. We propose the algorithms $\\ mathtt {LazyEV} $and $\\ mathtt {LazyCCA} $to solve the two problems with linear runtimes depending on the input size and $k $.", "histories": [["v1", "Wed, 20 Jul 2016 16:43:18 GMT  (591kb,D)", "http://arxiv.org/abs/1607.06017v1", "arXiv admin note: text overlap witharXiv:1607.03463"], ["v2", "Sat, 26 Nov 2016 03:18:24 GMT  (668kb,D)", "http://arxiv.org/abs/1607.06017v2", "We have now stated more clearly why this paper has outperformed relevant previous results, and included discussions for doubly-stochastic methods. arXiv admin note: text overlap witharXiv:1607.03463"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1607.03463", "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG stat.ML", "authors": ["zeyuan allen-zhu", "yuanzhi li"], "accepted": true, "id": "1607.06017"}, "pdf": {"name": "1607.06017.pdf", "metadata": {"source": "CRF", "title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "emails": ["zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu"], "sections": [{"heading": null, "text": "Furthermore, our algorithms are doubly-accelerated : our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both k-GenEV or k-CCA. We also provide the first gap-free results, which provide running times that depend on 1/ \u221a \u03b5 rather than the eigengap."}, {"heading": "1 Introduction", "text": "The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics. Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.\nGiven two symmetric matrices A,B \u2208 Rd\u00d7d where B is positive definite. The GenEV problem is to find generalized eigenvectors v1, . . . , vd where each vi satisfies\nvi \u2208 arg max v\u2208Rd\n\u2223\u2223v>Av \u2223\u2223 such that\n{ v>Bv = 1 v>Bvj = 0 \u2200j \u2208 [i\u2212 1]\nGiven matrices X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy and denoting by Sxx = 1nX>X, Sxy = 1nX>Y , Syy = 1 nY >Y , the CCA problem is to find canonical-correlation vectors {(\u03c6i, \u03c8i)}ni=1 where each pair\n(\u03c6i, \u03c8i) \u2208 arg max \u03c6\u2208Rdx ,\u03c8\u2208Rdy\n{ \u03c6>Sxy\u03c8 } such that\n{ \u03c6>Sxx\u03c6 = 1 \u2227 \u03c6>Sxx\u03c6j = 0 \u2200j \u2208 [i\u2212 1] \u03c8>Syy\u03c8 = 1 \u2227 \u03c8>Syy\u03c8j = 0 \u2200j \u2208 [i\u2212 1]\nIn GenEV, the values \u03bbi = v > i Avi are known as the generalized eigenvalues; in CCA, the values \u03c6>i Sxy\u03c8i are known as the canonical-correlation coefficients. It is a folklore that the exact solution of a CCA problem can reduce to that of a GenEV problem, if one defines B = diag{Sxx, Syy} and A = [[0, Sxy]; [S > xy, 0]] (see Lemma 2.3).\nDespite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently. In the\nar X\niv :1\n60 7.\n06 01\n7v 1\n[ m\nat h.\nO C\n] 2\n0 Ju\nbreakthrough result of Ma, Lu and Foster [16], they proposed to study algorithms to find top k generalized eigenvectors or top k canonical-correlation vectors. They designed an alternating minimization algorithm whose running time is only linear in terms of the number of non-zero elements of the matrix (that we denote by nnz(A) for a matrix A in this paper), and also nearlylinear in k. Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate.\nUnfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with \u03ba and 1/gap, where \u2022 \u03ba \u2265 1 is the condition number of matrix B in GenEV, or respectively the condition number\nof matrices X>X,Y >Y in CCA; and \u2022 gap \u2208 [0, 1) is the (relative) eigengap between \u03bbk and \u03bbk+1 in GenEV, or respectively the gap\nbetween \u03c3k and \u03c3k+1 in CCA. These parameters are usually not constants and do scale with the problem size. In the extreme case, gap can even be zero. At the same time, for many easier scientific computing problems, we are often able to design algorithms that have better dependencies on \u03ba and 1/gap. As three concrete examples:\n\u2022 Block Krylov method computes top eigenvectors with a running time linearly in 1/\u221agap rather than 1/gap [12], or with a gap-free running time that depends on 1/ \u221a \u03b5 rather than\n1/gap or 1/ \u221a gap, where \u03b5 is the approximation error [19].\n\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition\nnumber of matrix B.\n\u2022 If B = 1nX>X is given explicitly as the covariance matrix of X \u2208 Rn\u00d7d, stochastic gradient methods can be used to compute B\u22121w (see Lemma 2.4) with a running time linearly in (1 + \u221a \u03ba\u2032/n) instead of\n\u221a \u03ba, where \u03ba\u2032 = Tr(B)\u03bbmin(B) \u2208 [ \u03ba, d\u03ba ] .\nTherefore, it is a natural question to improve the dependency of \u03ba and 1/gap for the more challenging problems GenEV and CCA as well. Indeed, two groups of authors independently attempted to answer such questions [11, 24].\n\u2022 For the k-GenEV problem, GenELin [11] improved the dependency of \u03ba to \u221a\u03ba. In a separate work, SI [24] also obtained the \u221a \u03ba dependency but only for the simpler k = 1 case; at the\nsame time, SI enjoys a \u221a gap dependency but paying an additional factor \u03bb1. In sum, it was unknown how to obtain the straight \u221a gap dependency even for the k = 1 case, and unknown how to obtain gap-free running times.\n\u2022 For the k-CCA problem, the landscape is more complicated. The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24]. To sum up, (1) only CCALin works for k \u2265 1 and ALS/SI do not work for k > 1, (2) only SI has a nearly \u221agap dependency but loses an additional factor \u03c31 and an additional factor n\n1/4 in the stochastic case, (3) it is unknown how to obtain gap-free running times.\nThe detailed comparisons of these methods are in Table 1 for the k = 1 case, and in Table 2 for the general k \u2265 1 case. Both tables are included at the beginning of the appendix. Our Results. We provide algorithms LazyEV and LazyCCA with faster running times than all aforementioned results and for all cases k \u2265 1. We also provide the first gap-free running time for both GenEV and CCA. Since our running-time statements are very different between GenEV\n\u03bb1\n\u2208 [0, 1], \u03bb1 \u2208 [0, 1], and \u03baB = \u03bbmax(B)\u03bbmin(B) > 1.\nIn CCA, gap = \u03c31\u2212\u03c32 \u03c31 \u2208 [0, 1], \u03c31 \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy})\u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba]. a\naStochastic methods have to depend on a modified condition number as opposed to \u03ba. Our immediate prior works\n(such as CCALin, ALS and SI) used \u03ba\u2032\u2032 = 2maxi{\u2016Xi\u2016 2,\u2016Yi\u20162} \u03bbmin(diag{Sxx,Syy}) , which is a larger quantity than our \u03ba \u2032. We are aware of appropriate modifications to these methods to improve their running times to depend on \u03ba\u2032. For this reason, we have included these faster results in our tables for a stronger comparison.\n\u03bbk\n\u2208 [0, 1] and \u03baB = \u03bbmax(B)\u03bbmin(B) > 1.\nIn CCA, gap =\n\u03c3k\u2212\u03c3k+1 \u03c3k \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].\nand CCA, between gap-dependent and gap-free cases, and between non-stochastic and stochastic methods, we summarize them in full in Table 1 and Table 2 for a clean comparison (see appendix).\nTo mention just two of these results, for the general k-GenEV problem our running time is\nO\u0303 (knnz(B)\u221a\u03baB\ngap + knnz(A) + k2d gap\n) and O\u0303 (knnz(B)\u221a\u03baB\u221a \u03b5 + knnz(A) + k2d\u221a \u03b5 )\nin the gap-dependent and gap-free cases respectively. Since our running time only linearly depends on \u221a \u03ba and \u221a gap (resp. \u221a \u03b5), our algorithms are doubly-accelerated. Somewhat surprisingly, even in the simple case k = 1, our results outperform known ones on 1-GenEV and 1-CCA, see Table 1.\nOther Contributions. Besides the aforementioned running time improvements, we summarize some other virtues of our algorithms as follows:\n\u2022 For GenEV, our LazyEV method distinguishes positive generalized eigenvalues from negative ones. For instance, if A has two generalized eigenvectors v1, v2 with respect to B, one with eigenvalue \u03bb and the other with \u2212\u03bb. Then, previous results such as GenELin and SI only find the subspace spanned by v1, v2 but cannot distinguish v1 from v2.\n\u2022 For CCA with k > 1, previous results such as CCALin only output the subspace spanned by the top k correlation vectors but not identify which vector gives (approximately) the highest correlation and so on. Instead, our LazyCCA provides per-vector guarantees on all top k correlation vectors, see Corollary 6.3.\n\u2022 Our LazyEV and LazyCCA reduce the non-convex problem to multiple calls of quadratic minimization. Since quadratic minimization is a well-studied convex optimization problem, many efficient and robust algorithms can be found. In contrast, previous results for the k > 1 case rely on more sophisticated nonconvex optimization; and the previous work of [24] \u2014although uses convex optimization to solve 1-CCA\u2014 requires one to work with a sum-of-non-convex function which is less efficient to minimize.\nOther Related Works. For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al. [9, 10]. The shift-and-invert preconditioning framework of Garber et al. is also used in this paper for GenEV and CCA.\nAs for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25]. However, as for instance summarized by the authors of CCALin, these cited methods are more or less heuristics and do not have provable guarantees. Furthermore, for k > 1, the AppGrad result of [17] only provides local convergence guarantees and thus requires a warm-start whose computational complexity is not discussed in their paper.\nFinally, our algorithms on GenEV and CCA are based on finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop the algorithm whenever the eigenvalues (or correlation values) are too small. Known approaches for k > 1 cases (such as GenELin, CCALin, AppGrad) find all k vectors at once (through subspace power method on a d\u00d7k matrix), therefore requiring k to be known beforehand. As a separate note, these known approaches do not need the user to know the desired accuracy a priori but our LazyEV and LazyCCA algorithms do."}, {"heading": "2 Preliminaries", "text": "For a vector x we denote by \u2016x\u2016 or \u2016x\u20162 the Euclidean norm of x. Given a matrix A we denote by \u2016A\u20162 and \u2016A\u2016F respectively the spectral and Frobenius norms of A. For q \u2265 1, we denote by \u2016A\u2016Sq the Schatten q-norm of A. We write A B if A,B are symmetric and A \u2212 B is positive semi-definite (PSD), and write A B if A,B are symmetric but A \u2212 B is positive definite (PD). We denote by \u03bbmax(M) and \u03bbmin(M) the largest and smallest eigenvalue of a symmetric matrix M , and by \u03baM or sometimes \u03ba(M) the condition number \u03bbmax(M)/\u03bbmin(M) of a PSD matrix M .\nThroughout this paper, for a matrix A \u2208 Rn\u00d7d, we define nnz(A) def= max{n, d,N} where N is the number of non-zero entries of A. For two matrices X,Y , we denote by nnz(X,Y ) = nnz(X)+nnz(Y ). We also use poly(x1, x2, . . . , xt) to represent a quantity that is asymptotically at most polynomial in terms of variables x1, . . . , xt. Given a column orthonormal matrix U \u2208 Rn\u00d7k, we denote by U\u22a5 \u2208 Rn\u00d7(n\u2212k) the column orthonormal matrix consisting of an arbitrary basis in the space orthogonal to the span of U \u2019s columns.\nGiven a PSD matrix B and a vector v, the value v>Bv is the so-called B-inner product. For this reason, two vectors v, w satisfying v>Bw = 0 are known as B-orthogonal. Given a PSD matrix B, we denote by B\u22121 the Moore-Penrose pseudoinverse of B which is also PSD, and denote by B1/2 an arbitrary matrix square root of B. All occurrences of B\u22121, B1/2 and B\u22121/2 are for analysis purpose only. When implementing our algorithms, it only requires one to multiply B to a vector. Definition 2.1 (GenEV). Given symmetric matrices A,B \u2208 Rd\u00d7d where B is positive definite. The generalized eigenvectors of A with respect to B are v1, . . . , vd, where each vi is\nvi \u2208 arg max v\u2208Rd\n{ \u2223\u2223v>Av\n\u2223\u2223 such that { v>Bv = 1 v>Bvj = 0 \u2200j \u2208 [i\u2212 1]\n}\nThe corresponding generalized eigenvalues \u03bb1, . . . , \u03bbn satisfy \u03bbi = v > i Avi which is possibly negative. Definition 2.2 (CCA). Given X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy , letting Sxx = 1nX>X, Sxy = 1nX>Y , Syy = 1 nY >Y , the canonical-correlation vectors are {(\u03c6i, \u03c8i)}ri=1 where r = min{dx, dy} and \u2200i:\n(\u03c6i, \u03c8i) \u2208 arg max \u03c6\u2208Rdx ,\u03c8\u2208Rdy\n{ \u03c6>Sxy\u03c8 such that\n{ \u03c6>Sxx\u03c6 = 1 \u2227 \u03c6>Sxx\u03c6j = 0 \u2200j \u2208 [i\u2212 1] \u03c8>Syy\u03c8 = 1 \u2227 \u03c8>Syy\u03c8j = 0 \u2200j \u2208 [i\u2212 1]\n}\nThe corresponding canonical-correlation coefficients \u03c31, . . . , \u03c3r satisfy \u03c3i = \u03c6 > i Sxy\u03c8i \u2208 [0, 1].\nWhen dealing with a CCA problem, we also denote by d = dx + dy.\nLemma 2.3. Given a CCA problem with matrices X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy , and suppose the canonical-correlation vectors and coefficients are {(\u03c6i, \u03c8i, \u03c3i)}ri=1 where r = min{dx, dy}.\nDefine A = ( 0 Sxy S>xy 0 ) and B = ( Sxx 0 0 Syy ) . Then, the GenEV problem of A with respect\nto B has 2r eigenvalues {\u00b1\u03c3i}ri=1 and corresponding generalized eigenvectors {(\n\u03c6i \u03c8i\n) , ( \u2212\u03c6i \u03c8i )}n\ni=1\n.\nThe remaining dx + dy \u2212 2r eigenvalues are zeros. Lemma 2.4. Given matrices X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy , let A and B be as defined in Lemma 2.3. For every w \u2208 Rd, Katyusha method [1] finds a vector w\u2032 \u2208 Rd satisfying \u2016w\u2032 \u2212B\u22121Aw\u2016 \u2264 \u03b5\nin time O ( nnz(X,Y ) \u00b7 ( 1 + \u221a \u03ba\u2032/n ) \u00b7 log \u03ba\u2016w\u2016 2\n\u03b5\n) .\nwhere \u03ba = \u03bbmax(B)/\u03bbmin(B) is the condition number and \u03ba \u2032 = Tr(B)/\u03bbmin(B) \u2208 [\u03ba, d\u03ba]."}, {"heading": "3 Leading Eigenvector via Two-Sided Shift-and-Invert", "text": "In this section we define AppxPCA\u00b1, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10]. Our pseudo-code Algorithm 1 is a modification of Algorithm 5 appeared in [9].\nThe main differences between AppxPCA\u00b1 and Algorithm 5 of [9] are two-fold. First, given a symmetric matrix M , AppxPCA\u00b1 simultaneously considers an upper-bounding shift together with a lower-bounding shift, and try to invert both \u03bbI \u2212M and \u03bbI + M . This allows us to determine approximately how close \u03bb is to the largest and the smallest eigenvalues of M , and decrease \u03bb accordingly; in the end, it outputs an approximate eigenvector of M that corresponds to a negative eigenvalue if needed. Second, we provide a multiplicative-error guarantee rather than additive as originally appeared in [9]. Without this multiplicative-error guarantee, our final running time will depend on 1gap\u00b7\u03bbmax(M) rather than 1 gap . 1 Of course, we believe the bulk of the credit for conceiving AppxPCA\u00b1 belongs to the original authors of [9, 10].\nTheorem 3.1 (AppxPCA\u00b1). Let M \u2208 Rd\u00d7d be a symmetric matrix with eigenvalues 1 \u2265 \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd \u2265 \u22121 and corresponding eigenvectors u1, . . . , ud. Let \u03bb\u2217 = \u2016M\u20162 = max{\u03bb1,\u2212\u03bbd}. With probability at least 1\u2212 p, AppxPCA\u00b1 produces a pair (sgn,w) satisfying\nif sgn = +, then \u2211\ni\u2208[d],\u03bbi\u2264(1\u2212\u03b4\u00d7/2)\u03bb\u2217 (w>ui)2 \u2264 \u03b5 and w>Mw \u2265 (1\u2212 \u03b4\u00d7/2)(1\u2212 3\u03b5)\u03bb\u2217 , and\nif sgn = \u2212, then \u2211\ni\u2208[d],\u03bbi\u2265\u2212(1\u2212\u03b4\u00d7/2)\u03bb\u2217 (w>ui)2 \u2264 \u03b5 and w>Mw \u2264 \u2212(1\u2212 \u03b4\u00d7/2)(1\u2212 3\u03b5)\u03bb\u2217 .\n1This is why the only known CCA result using shift-and-invert preconditioning [24] depends on 1 gap\u00b7\u03bb1 in Table 1.\nAlgorithm 1 AppxPCA\u00b1(A,M, \u03b4\u00d7, \u03b5, p) Input: A, an approximate matrix inversion method; M \u2208 Rd\u00d7d, a symmetric matrix satisfying \u2212I M I; \u03b4\u00d7 \u2208 (0, 0.5], a multiplicative error; \u03b5 \u2208 (0, 1), a numerical accuracy parameter; and p \u2208 (0, 1), the confidence parameter.\n1: w\u03020 \u2190 RanInit(d) be a random unit vector; s\u2190 0; \u03bb(0) \u2190 1 + \u03b4\u00d7; see Definition 3.2 for RanInit\n2: m1 \u2190 \u2308 4 log ( 288d\u03b8 p2 )\u2309 , m2 \u2190 \u2308 log ( 36d\u03b8 p2\u03b5 )\u2309 ; \u03b8 is the parameter of RanInit m1 = TPM(8, 1/32, p) and m2 = TPM(2, \u03b5/4, p) using Lemma B.1 3: \u03b5\u03031 \u2190 164m1 ( \u03b4\u00d7 48 )m1 and \u03b5\u03032 \u2190 \u03b58m2 ( \u03b4\u00d7 48\n)m2 4: repeat 5: s\u2190 s+ 1; 6: for t = 1 to m1 do 7: Apply A to find w\u0302t satisfying \u2225\u2225w\u0302t \u2212 (\u03bb(s\u22121)I \u2212M)\u22121w\u0302t\u22121 \u2225\u2225 \u2264 \u03b5\u03031; 8: wa \u2190 w\u0302m1/\u2016w\u0302m1\u2016; 9: Apply A to find va satisfying \u2225\u2225va \u2212 (\u03bb(s\u22121)I \u2212M)\u22121wa \u2225\u2225 \u2264 \u03b5\u03031;\n10: for t = 1 to m1 do 11: Apply A to find w\u0302t satisfying \u2225\u2225w\u0302t \u2212 (\u03bb(s\u22121)I +M)\u22121w\u0302t\u22121 \u2225\u2225 \u2264 \u03b5\u03031; 12: wb \u2190 w\u0302m1/\u2016w\u0302m1\u2016; 13: Apply A to find vb satisfying \u2225\u2225vb \u2212 (\u03bb(s\u22121)I +M)\u22121wb \u2225\u2225 \u2264 \u03b5\u03031; 14: \u2206(s) \u2190 12 \u00b7 1max{w>a va,w>b vb}\u2212\u03b5\u03031 and \u03bb (s) \u2190 \u03bb(s\u22121) \u2212 \u2206(s)2 ; 15: until \u2206(s) \u2264 \u03b4\u00d7\u03bb(s)12 16: f \u2190 s; 17: if the last w>a va \u2265 w>b vb then 18: for t = 1 to m2 do 19: Apply A to find w\u0302t satisfying \u2225\u2225w\u0302t \u2212 (\u03bb(f)I \u2212M)\u22121w\u0302t\u22121 \u2225\u2225 \u2264 \u03b5\u03032; 20: return (+, w) where w def = w\u0302m2/\u2016w\u0302m2\u2016. 21: else 22: for t = 1 to m2 do 23: Apply A to find w\u0302t satisfying \u2225\u2225w\u0302t \u2212 (\u03bb(f)I +M)\u22121w\u0302t\u22121 \u2225\u2225 \u2264 \u03b5\u03032; 24: return (\u2212, w) where w def= w\u0302m2/\u2016w\u0302m2\u2016. 25: end if\nFurthermore, the total number of oracle calls to A is O(log(1/\u03b4\u00d7)m1 +m2), and each time we call A it satisfies that \u03bbmax(\u03bb(s)I\u2212M)\n\u03bbmin(\u03bb(s)I\u2212M) , \u03bbmax(\u03bb(s)I+M) \u03bbmin(\u03bb(s)I+M) \u2208 [1, 96\u03b4\u00d7 ].\nWe remark here that, unlike the original shift-and-invert method which chooses a random (Gaussian) unit vector in Line 1 of AppxPCA\u00b1, we allow this initial vector to be generated from an arbitrary \u03b8-conditioned random vector generator, defined as follows:\nDefinition 3.2. An algorithm RanInit(d) is a \u03b8-conditioned random vector generator if w = RanInit(d) is a d-dimensional unit vector and, for every p \u2208 (0, 1), every unit vector u \u2208 Rd, with probability at least 1\u2212 p, it satisfies (u>w)2 \u2264 p2\u03b89d .\nThis modification is needed in order to obtain our efficient implementations of GenEV and CCA algorithms. One can construct \u03b8-conditioned random vector generator as follows:\nAlgorithm 2 LazyEV(A,M, k, \u03b4\u00d7, \u03b5pca, p) Input: A, an approximate matrix inversion method; M \u2208 Rd\u00d7d, a matrix satisfying \u2212I M I;\nk \u2208 [d], the desired rank; \u03b4\u00d7 \u2208 (0, 1), a multiplicative error; \u03b5pca \u2208 (0, 1), a numerical accuracy parameter; and p \u2208 (0, 1), a confidence parameter.\n1: M0 \u2190M ; V0 = []; 2: for s = 1 to k do 3: v\u2032s \u2190 AppxPCA\u00b1(A,Ms\u22121, \u03b4\u00d7/2, \u03b5pca, p/k); 4: vs \u2190 ( (I \u2212 Vs\u22121V >s\u22121)v\u2032s ) / \u2225\u2225(I \u2212 Vs\u22121V >s\u22121)v\u2032s\n\u2225\u2225; project v\u2032s to V \u22a5s\u22121 5: Vs \u2190 [Vs\u22121, vs]; 6: Ms \u2190 (I \u2212 vsv>s )Ms\u22121(I \u2212 vsv>s ) we also have Ms = (I \u2212 VsV >s )M(I \u2212 VsV >s ) 7: end for 8: return Vk.\nProposition 3.3. Given PSD matrix B \u2208 Rd\u00d7d, if we set RanInit(d) def= B1/2v (v>Bv)0.5 where v is a random Gaussian vector, then RanInit(d) is a \u03b8-conditioned random vector generator for \u03b8 = \u03baB."}, {"heading": "4 Main Algorithm for Generalized Eigendecomposition", "text": "In this section, we propose LazyEV (see Algorithm 2) to compute approximately the k \u201cleading\u201d eigenvectors corresponding to the k largest absolute eigenvalues of some symmetric matrix M \u2208 Rd\u00d7d. Later, we solve the k-GenEV problem by setting M = B\u22121/2AB\u22121/2 and using LazyEV to find the k leading eigenvectors of M , which correspond to the k leading generalized eigenvectors of A with respect to B.\nOur algorithm LazyEV is formally stated in Algorithm 2. It applies k times AppxPCA\u00b1, each time with a multiplicative error \u03b4\u00d7/2, and projects the matrix M into the orthogonal space with respect to the obtained leading eigenvector. We state our main approximation theorem below.\nTheorem 4.1 (approximation of LazyEV). Let M \u2208 Rd\u00d7d be a symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbd \u2208 [\u22121, 1] and corresponding eigenvectors u1, . . . , ud, and assume |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|.\nFor every k \u2208 [d], \u03b4\u00d7, p \u2208 (0, 1), there exists some \u03b5pca \u2264 O ( poly(\u03b4\u00d7,\n|\u03bb1| |\u03bbk+1 , 1 d) ) such that2\nLazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) \u2208 Rd\u00d7k which, with probability at least 1\u2212 p, satisfies all of the following properties. (Denote by Ms = (I \u2212 VsV >s )M(I \u2212 VsV >s ).)\n(a) Correlation guarantee: \u2016V >k U\u20162 \u2264 \u03b5, where U = (uj , . . . , ud) and j is the smallest index satisfying |\u03bbj | \u2264 (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162. (b) Spectral norm guarantee: |\u03bbk+1| \u2264 \u2016Mk\u20162 \u2264 |\u03bbk+1|1\u2212\u03b4\u00d7 . (c) Rayleigh quotient guarantee: (1\u2212 \u03b4\u00d7)|\u03bbk| \u2264 |v>kMvk| \u2264 11\u2212\u03b4\u00d7 |\u03bbk|. (d) Schatten-q norm guarantee: for every q \u2265 1,\n\u2016Mk\u2016Sq \u2264 (1 + \u03b4\u00d7)2 (1\u2212 \u03b4\u00d7)2 ( d\u2211\ni=k+1\n\u03bbqi\n)1/q = (1 + \u03b4\u00d7)2\n(1\u2212 \u03b4\u00d7)2 min V \u2208Rd\u00d7k,V >V=I\n{ \u2016(I\u2212V V >)M(I\u2212V V >)\u2016Sq } .\nThe next theorem states that, if M = B\u22121/2AB\u22121/2, then LazyEV can be implemented without ever needing to compute B1/2 or B\u22121/2.\n2The complete specifications of \u03b5pca is included in Appendix E. Since our final running time only depends on log(1/\u03b5pca), we have not attempted to improve the constants in this polynomial dependency.\nTheorem 4.2 (running time of LazyEV). Let A,B \u2208 Rd\u00d7d be two symmetric matrices satisfying B 0 and \u2212B A B. Suppose M = B\u22121/2AB\u22121/2 and RanInit(d) is the random vector generator defined in Proposition 3.3 with respect to B. Then, the computation of V\u2190 B\u22121/2LazyEV(A,M, k, \u03b4\u00d7, \u03b5pca, p) can be implemented to run in time\n\u2022 O\u0303 ( knnz(B)+k2d+k\u03a5\u221a\n\u03b4\u00d7\n) where \u03a5 is the time to multiply B\u22121A to a vector with error \u03b5 where\nlog(1/\u03b5) = O\u0303(1), or\n\u2022 O\u0303 ( k \u221a \u03baBnnz(B)+knnz(A)+k\n2d\u221a \u03b4\u00d7\n) if we use Conjugate gradient to multiply B\u22121A to a vector.\nAbove, the O\u0303 notation hides polylogarithmic factors with respect to 1/\u03b5pca, 1/\u03b4\u00d7, 1/p, \u03baB, d."}, {"heading": "4.1 Application to GenEV", "text": "Our main theorems above imply the following corollaries.\nCorollary 4.3 (gap-dependent k-GenEV). Let A,B \u2208 Rd\u00d7d be two symmetric matrices satisfying B 0 and \u2212B A B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are {(\u03bbi, ui)}di=1, and it satisfies 1 \u2265 |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|. Let gap = |\u03bbk|\u2212|\u03bbk+1| |\u03bbk| \u2208 [0, 1] be the relative gap. For fixed \u03b5, p > 0, consider the output\nVk \u2190 B\u22121/2LazyEV ( A, B\u22121/2AB\u22121/2, k, gap, O ( \u03b54\u00b7gap k3(\u03c31/\u03c3k)4 ) , p ) \u2208 Rd\u00d7k .\nThen, defining W = (uk+1, . . . , ud), we have with probability at least 1\u2212 p:\nV>k BVk = I and \u2016V>k BW\u20162 \u2264 \u03b5 .\nMoreover, our running time is O\u0303 ( k \u221a \u03baBnnz(B)+knnz(A)+k\n2d\u221a gap\n)\nCorollary 4.4 (gap-free k-GenEV). Consider the same setting as Corollary 4.3. For fixed \u03b5, p > 0, consider the output\nVk \u2190 B\u22121/2LazyEV ( A, B\u22121/2AB\u22121/2, k, \u03b5, O ( \u03b55\nk3d4(\u03c31/\u03c3k+1)12\n) , p ) .\nThen, we have with probability at least 1\u2212 p:\nV>k BVk = I and max w\u2208Rd\u2227w>BVk=0 w>Aw w>Bw \u2264 1 1\u2212 \u03b5 |\u03bbk+1|\nMoreover, our running time is O\u0303 ( k \u221a \u03baBnnz(B)+knnz(A)+k\n2d\u221a \u03b5\n)"}, {"heading": "5 High Level Ideas Behind Theorems 4.1 and 4.2", "text": "Our LazyEV algorithm reduces the problem of finding generalized eigenvectors to finding regular eigenvectors of M = B\u22121/2AB\u22121/2. In Section 5.1 we discuss how to ensure accuracy: that is, why does LazyEV guarantee to find approximately the top absolute eigenvectors of M ; and in Section 5.2 we discuss how to implement LazyEV without ever needing to compute B1/2 or B\u22121/2."}, {"heading": "5.1 Ideas Behind Theorem 4.1: Approximation Guarantee of GenEV", "text": "Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M [2]. That method is called LazySVD. At a high level, LazySVD finds the top k eigenvectors of M one by one but only approximately. Starting with M0 = M , in the s-th iteration where s \u2208 [k], LazySVD computes approximately the leading eigenvector of matrix Ms\u22121 and call it vs using shift-and-invert [9]. Then, LazySVD performs a projection Ms \u2190 (I \u2212 vsv>s )Ms\u22121(I \u2212 vsv>s ) and proceeds to the next iteration.\nWhile the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas. Most notably, if vs is an approximate leading eigenvector of Ms\u22121, then one needs to prove that the small eigenvectors of Ms\u22121 somehow still \u201cembed\u201d into that of Ms after projection. This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of [2].\nIn this paper, to relax the assumption that M is PSD, and to find leading eigenvectors whose absolute eigenvalues are large, we have to make some non-trivial changes in the algorithm and the analysis. On the algorithm side, LazyEV replaces the use of the shift-and-invert protocol in LazySVD with our two-sided variant developed in Section 3. On the analysis side, we have to make sure all lemmas properly deal with negative eigenvalues: for instance, if we perform a projection M \u2032 \u2190 (I\u2212vv>)M(I\u2212vv>) where v correlates by at most \u03b5 with all eigenvectors ofM whose absolute eigenvalues are smaller than a threshold \u00b5, then, after the projection, we need to prove that these eigenvectors can be approximately \u201cembedded\u201d into the eigenspace spanned by all eigenvectors of M \u2032 whose absolute eigenvalues are smaller than \u00b5+\u03c4 . The approximation of this embedding should depend on \u03b5, \u00b5 and \u03c4 . See Lemma C.4 in the appendix.\nThe detailed proof of Theorem 4.1 is included in Appendix E, and the matrix algebraic lemmas are included in Appendix C."}, {"heading": "5.2 Proof of Theorem 4.2: Fast Implementation of GenEV", "text": "We can implement LazyEV efficiently without the necessity of computing B1/2 or B\u22121/2. In each iteration of LazyEV, we call AppxPCA\u00b1 and compute a vector v\u2032s. We do not explicitly store v \u2032 s, but rather write it as v\u2032s = B 1/2v\u2032s and store only v \u2032 s \u2208 Rd. We shall later ensure that AppxPCA\u00b1 outputs v\u2032s directly. Similarly, we also write vs = B 1/2vs and only store vs. All together, we do not explicitly compute Vs, but instead write Vs = B 1/2Vs and only keep track of Vs \u2208 Rd\u00d7s.\nNow, the computation of vs becomes the B-projection into the Vs\u22121 space:\n\u2016(I \u2212 Vs\u22121V >s\u22121)v\u2032s\u2016 = \u2016B1/2v\u2032s \u2212B1/2Vs\u22121V>s\u22121Bv\u2032s\u2016 = (( v\u2032s \u2212Vs\u22121V>s\u22121Bv\u2032s )> B ( v\u2032s \u2212Vs\u22121V>s\u22121Bv\u2032s ))1/2 \u2016(I \u2212 Vs\u22121V >s\u22121)v\u2032s\u2016 \u00b7 vs = B\u22121/2(I \u2212 Vs\u22121V >s\u22121)v\u2032s = B\u22121/2(I \u2212 Vs\u22121V >s\u22121)B1/2v\u2032s = v\u2032s \u2212Vs\u22121V>s\u22121Bv\u2032s\nand this can be implemented to run in O(kd+ nnz(B)) time. Finally, we write\nMs = (I \u2212 VsV >s )B\u22121/2AB\u22121/2(I \u2212 VsV >s ) = B\u22121/2(I \u2212BVsV>s )A(I \u2212VsV>s B)B\u22121/2\nand only pass it implicity to AppxPCA\u00b1 (without directly computing this matrix). To implement AppxPCA\u00b1, we again write all vectors w\u0302t = B1/2wt and only store wt. Thus, the normalization wa \u2190 w\u0302m1/\u2016w\u0302m1\u20162 becomes the B-normalization wa \u2190 wm1/(w>m1Bwm1)1/2 which runs in O(nnz(B)) time. Recall that AppxPCA\u00b1 makes a polylogarithmic number of calls to the matrix inversion subroutine A, each time requesting to approximately invert either \u03bbI \u2212Ms or\n\u03bbI +Ms. Let us only focus on inverting \u03bbI \u2212Ms and the other case is similar. We write\nN def = B\u22121/2 ( \u03bbI \u2212Ms ) B1/2 = \u03bbI \u2212 (I \u2212 V\u0302sV\u0302 >s B)B\u22121A(I \u2212 V\u0302sV\u0302 >s B) .\nNow, the accuracy requirement in AppxPCA\u00b1 becomes\nfind wt satisfying \u2016B1/2wt \u2212 (\u03bbI \u2212Ms)\u22121B1/2wt\u22121\u2016 \u2264 \u03b5\u0303 \u21d0\u21d2 find wt satisfying \u2016B1/2wt \u2212B1/2N\u22121wt\u22121\u2016 \u2264 \u03b5\u0303 \u21d0= find wt satisfying \u2016wt \u2212N\u22121wt\u22121\u2016 \u2264 \u03b5\u0303/ \u221a \u03bbmax(B)\nUsing Theorem D.1, we can reduce this approximate inversion wt \u2190 N\u22121wt\u22121 to T times of approximate matrix-vector multiplication (i.e., w\u2032 \u2190 Nw) for T = O\u0303( \u221a \u03ba(\u03bbI \u2212Ms)).3 We can\nfurther derive that T = O\u0303(1/ \u221a \u03b4\u00d7) owing to Theorem 3.1. Notice that Theorem D.1 implies that each time we compute w\u2032 \u2190 Nw it suffices to compute it to an additive accuracy \u2016w\u2032 \u2212Nw\u2016 \u2264 \u03b5 where the error satisfies log(1/\u03b5) = O\u0303(1).\nFinally, the matrix-vector multiplication Nw = \u03bbw\u2212 (I\u2212 V\u0302sV\u0302 >s B)B\u22121A(I\u2212 V\u0302sV\u0302 >s B)w consists of two rank-s B-projections which run in time O(nnz(B) + kd), plus the time needed to multiply B\u22121A to a vector. This finishes the proof that LazyEV can be implemented so that\n\u2022 It computes matrix-vector multiplication of the form w\u2032 \u2190 B\u22121Aw a total of O\u0303(k/ \u221a \u03b4\u00d7)\ntimes, each time to an accuracy \u03b5 where log(1/\u03b5) = O\u0303(1);\n\u2022 The rest of the computation costs a total of O\u0303 ( (knnz(B) + k2d)/ \u221a \u03b4\u00d7 ) time.\nThis finishes the proof of the first half of the theorem. As for the second item, we simply notice that whenever we want to compute w\u2032 \u2190 B\u22121Aw, we can first compute Aw in time O(nnz(A)), and then use Conjugate gradient [22] to compute B\u22121 applied to this vector. The running time of Conjugate gradient is at most O\u0303 (\u221a \u03baB \u00b7 nnz(B) ) where the O\u0303 factor hides a logarithmic factor on the accuracy."}, {"heading": "6 Main Algorithm for Canonical Correlation Analysis", "text": "In this section, we propose LazyCCA (see Algorithm 3), a variant of LazyEV that is specially designed for matrices M of the form M = B\u22121/2AB\u22121/2, where A and B come from a CCA problem following Lemma 2.3.\nMore specifically, recall from Lemma 2.3 that the eigenvectors of matrices M arising from CCA instances are symmetric: if (\u03be, \u03b6) is a normalized eigenvector of M with eigenvalue \u03c3 where \u03be \u2208 Rdx , \u03b6 \u2208 Rdy , then (\u2212\u03be, \u03b6) is also a normalized eigenvector but with eigenvalue \u2212\u03c3. Furthermore, since (\u03be, \u03b6) is orthogonal to (\u2212\u03be, \u03b6), we must have \u2016\u03be\u2016 = \u2016\u03b6\u2016 = 1/ \u221a 2. Our LazyCCA method is designed to ensure such symmetry and orthogonality as well. When an approximate eigenvector vs = (\u03be \u2032 s, \u03b6 \u2032 s) is obtained, we re-scale the pair to \u03bes and \u03b6s where both of them have norm exactly 1/ \u221a 2 (see Line 7 of LazyCCA). Then, we simultaneously add two (orthogonal) approximate eigenvectors (\u03bes, \u03b6s) and (\u2212\u03bes, \u03b6s) to the column orthonormal matrix Vs.\n3This reduction would be obvious if we required the matrix-vector multiplication to be exact, and for instance Chebyshev method serves for exactly this purpose. However, in order to relax the multiplication to be approximate, and without incurring an error that blows up exponentially with T , we build our own inexact variant of the accelerate gradient descent method AGDinexact in Appendix D that could be of independent interest.\nAlgorithm 3 LazyCCA(A,M, k, \u03b4\u00d7, \u03b5pca, p) Input: A, an approximate matrix inversion method;\nM \u2208 Rd\u00d7d, a matrix satisfying \u2212I M I; k \u2208 [d], the desired rank; \u03b4\u00d7 \u2208 (0, 1), a multiplicative error; \u03b5pca \u2208 (0, 1), a numerical accuracy parameter; and p \u2208 (0, 1), a confidence parameter.\n1: M0 \u2190M ; 2: V0 = []; 3: for s = 1 to k do 4: v\u2032s \u2190 AppxPCA\u00b1(A,Ms\u22121, \u03b4\u00d7/2, \u03b5pca, p/k); 5: vs \u2190 ( (I \u2212 Vs\u22121V >s\u22121)v\u2032s ) / \u2225\u2225(I \u2212 Vs\u22121V >s\u22121)v\u2032s\n\u2225\u2225; project v\u2032s to V \u22a5s\u22121 6: write vs = (\u03be \u2032 s, \u03b6 \u2032 s) where \u03be \u2032 s \u2208 Rdx and \u03b6 \u2032s \u2208 Rdy ;\n7: \u03bes \u2190 \u03be\u2032s/( \u221a 2\u2016\u03be\u2032s\u20162) and \u03b6s \u2190 \u03b6 \u2032s/( \u221a\n2\u2016\u03b6 \u2032s\u20162); 8: Vs \u2190 [ Vs\u22121,\n( \u03bes \u2212\u03bes \u03b6s \u03b6s )] ;\n9: Ms \u2190 ( I \u2212 2diag(\u03bes\u03be>s , \u03b6s\u03b6>s ) ) Ms\u22121 ( I \u2212 2diag(\u03bes\u03be>s , \u03b6s\u03b6>s ) )\nor equivalently, Ms = (I \u2212 VsV >s )M(I \u2212 VsV >s ) 10: end for 11: return Vk.\nRemark 6.1. This re-scaling step, together with the fact that we find vector pairs one by one, allows us to provide per-vector guarantee on the obtained approximate correlation vectors (see Corollary 6.3). This is in contrast to CCALin which is based on subspace power method so can only find the subspace spanned by the top k correlation vectors but not distinguish them.\nOne can prove a similar approximation guarantee (see Theorem F.1) as compared with Theorem 4.1, and a similar running time guarantee (see Theorem F.2) as compared with Theorem 4.2. The main idea behind the \u201cdelta\u201d between the proofs of Theorem F.1 and of Theorem 4.1 is to show that, after re-scaling, the vector (\u03bes, \u03b6s) is also an approximate leading eigenvector of Ms\u22121 just like the one vs = (\u03be \u2032 s, \u03b6 \u2032 s) before scaling. More specifically, its Rayleigh quotient can only become better after scaling (see (F.3) in the appendix). We include the details in Appendix F, and state below the final statements on LazyCCA.\nCorollary 6.2 (gap-dependent k-CCA). Let X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy be two matrices with canonicalcorrelation coefficients 1 \u2265 \u03c31 \u2265 \u00b7 \u00b7 \u00b7\u03c3r \u2265 0 and the corresponding correlation vectors {(\u03c6i, \u03c8i)}ri=1, where r = min{dx, dy}. Let gap = \u03c3k\u2212\u03c3k+1\u03c3k \u2208 [0, 1] be the relative gap, and define A = [[0, Sxy]; [S > xy, 0]] and B = diag(Sxx, Syy) following Definition 2.2. For every \u03b5, p > 0, consider the output\n( \u00b1\u03c6\u20321 . . . \u00b1\u03c6\u2032k \u03c8\u20321 . . . \u03c8 \u2032 k ) def = Vk \u2190 \u221a 2B\u22121/2LazyCCA ( A, B\u22121/2AB\u22121/2, \u03b5, gap, O ( \u03b54\u00b7gap k3(\u03c31/\u03c3k)4 ) , p ) .\nThen, letting V\u03c6 = (\u03c6 \u2032 1, . . . , \u03c6 \u2032 k), V\u03c8 = (\u03c8 \u2032 1, . . . , \u03c8 \u2032 k), W\u03c6 = (\u03c6k+1, \u03c6k+2, . . . ) and W\u03c8 = (\u03c8k+1, \u03c8k+2, . . . ), we have with probability at least 1\u2212 p:\nV\u03c6 \u2208 Rdx\u00d7k satisfies V>\u03c6 SxxV\u03c6 = I and \u2016V>\u03c6 SxxW\u03c6\u20162 \u2264 \u03b5 , V\u03c8 \u2208 Rdy\u00d7k satisfies V>\u03c8SyyV\u03c8 = I and \u2016V>\u03c8SyyW\u03c8\u20162 \u2264 \u03b5 .\nFurthermore, the running time is O\u0303 ( k \u221a \u03bannz(X,Y )+k2d\u221a\ngap\n) if we use Conjugate gradient to multiply\nB\u22121A to a vector, or O\u0303 ( knnz(X,Y )\u00b7\n( 1+ \u221a \u03ba\u2032/n )\n+k2d\u221a gap\n) if we use Katyusha.\nCorollary 6.3 (gap-free k-CCA). In the same setting as Corollary 6.2, for every \u03b5, p > 0, consider the output\n( \u00b1\u03c6\u20321 . . . \u00b1\u03c6\u2032k \u03c8\u20321 . . . \u03c8 \u2032 k ) = Vk \u2190 \u221a 2B\u22121/2LazyCCA ( A, B\u22121/2AB\u22121/2, \u03b5, gap, O ( \u03b54\u00b7gap k3(\u03c31/\u03c3k)4 ) , p ) .\nLetting V\u03c6 = (\u03c6 \u2032 1, . . . , \u03c6 \u2032 k) \u2208 Rdx\u00d7k and V\u03c8 = (\u03c8\u20321, . . . , \u03c8\u2032k) \u2208 Rdy\u00d7k, with probability at least 1\u2212 p,\n\u2022 V>\u03c6 SxxV\u03c6 = I, V>\u03c8SyyV\u03c8 = I; \u2022 (1\u2212 \u03b5)\u03c3i \u2264 |\u03c6\u2032iSxy\u03c8i| \u2264 (1 + \u03b5)\u03c3i for every i \u2208 [k]; and \u2022 max\u03c6\u2208Rdx ,\u03c8\u2208Rdy { \u03c6>Sxy\u03c8 \u2223\u2223\u2223 \u03c6>SxxV\u03c6 = 0 \u2227 \u03c8>SyyV\u03c8 = 0 } \u2264 (1 + \u03b5)\u03c3k+1 .\nFurthermore, the running time is O\u0303 ( k \u221a \u03bannz(X,Y )+k2d\u221a\n\u03b5\n) if we use Conjugate gradient to multiply\nB\u22121A to a vector, or O\u0303 ( knnz(X,Y )\u00b7\n( 1+ \u221a \u03ba\u2032/n )\n+k2d\u221a \u03b5\n) if we use Katyusha.\nFinally, we note that the Katyusha-based running times in Corollary 6.2 and 6.3 need to use Lemma 2.4 which gives an accelerated stochastic running time for multiplying B\u22121A to a vector.\nAppendix"}, {"heading": "A Missing Miscellaneous Proofs", "text": "Proof of Lemma 2.4. First of all, computing B\u22121Aw is equivalent to minimizing f(x) def= 12x >Bx\u2212 x>Aw. Suppose we write x = (x1, x2) and w = (w1, w2) where x1, w1 \u2208 Rdx , x2, w2 \u2208 Rdy , then one can rewrite f as f(x) = 12n ( \u2016Xx1 \u2212 Y w2\u201622 + \u2016Xw1 \u2212 Y x2\u201622 ) +C where C is a fixed constant. Therefore, one can also write\nf(x) = 1\n2n\nn\u2211\ni=1\n(\u3008Xi, x1\u3009 \u2212 \u3008Yi, w2\u3009)2 + (\u3008Yi, x2\u3009 \u2212 \u3008Xi, w1\u3009)2\nwhere each Xi is a row vector of X and Yi is a row vector of Y . In such a case, we observe that\n\u2022 f(x) is an average of 2n smooth functions, where function (\u3008Xi, x1\u3009 \u2212 \u3008Yi, w2\u3009)2 is smooth with parameter 2\u2016Xi\u201622 (meaning Hessian bounded by \u2016Xi\u201622 in spectral norm) and (\u3008Yi, x2\u3009\u2212 \u3008Xi, w1\u3009)2 is smooth with parameter \u2016Yi\u201622. In other words, the average smoothness of these 2n functions is exactly (\u2016X\u20162F + \u2016Y \u20162F )/n = Tr(B).\n\u2022 f(x) is at least min{\u03bbmin(Sxx), \u03bbmin(Syy)} strongly convex, meaning Hessian lower bounded by this quantity.\nFor such reason, one can apply the convergence theorem of Katyusha [1] to find an additive \u03b5\u0303 approximate minimizer of f(x) in time O ( nnz(X,Y ) \u00b7 ( 1 + \u221a \u03ba\u2032/n ) log f(x\n0)\u2212f(x\u2217) \u03b5\u0303\n) where x0 is an\narbitrary starting vector fed into Katyusha and x\u2217 is the exact minimizer. If we choose x0 to be the zero vector, it is easy to verify that f(x0)\u2212 f(x\u2217) \u2264 O(\u03bbmax(B) \u00b7 \u2016w\u20162).\nFinally, it is not hard to see that an additive \u03b5\u0303 minimizer of f(x) implies an \u03b5-approximate solution for the inverse \u2016w\u2032 \u2212 B\u22121Aw\u2016 \u2264 \u03b5 where \u03b52 = 2\u03b5\u0303/\u03bbmin(B). This finishes the proof of Lemma 2.4. Proof of Proposition 3.3. We have\n(u>w)2 = Tr(uu>vBv>)\nv>Bv\n\u00ac \u2265 Tr(uu >vBv>) \u03bbmax(B)  \u2265 \u03bbmin(B) \u00b7 Tr(uu >vv>) \u03bbmax(B) = \u03b8(u>v)2 .\nAbove, \u00ac is because v>Bv \u2264 \u03bbmax(B) \u00b7 \u2016v\u201622 = \u03bbmax(B), and  follows from the fact that vBv> v ( \u03bbmin(B)I ) v> = \u03bbmin(B)vv>. Finally, using for instance [5, Lemma 5], it holds with probability at least 1\u2212 p that (u>v)2 \u2265 p29d ."}, {"heading": "B Proof Details for Section 3: Two-Sided Shift-and-Invert", "text": "B.1 Inexact Power Method\nIn this subsection we review some classical convergence lemmas regarding power method and its inexact variant. These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2]. We skip the proofs in this paper.\nConsider power method that starts with a random unit vector w0 \u2190 RanInit(d) and apply wt \u2190Mwt\u22121/\u2016Mwt\u22121\u2016 iteratively.\nLemma B.1 (Exact Power Method). Let M be a PSD matrix with eigenvalues \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and the correpsonding eigenvectors u1, . . . , ud. Fix an error tolerance \u03b5 > 0, parameter \u03ba \u2265 1, and failure probability p > 0, define\nTPM(\u03ba, \u03b5, p) = \u2308\u03ba 2 log (9d\u03b8 p2\u03b5 )\u2309\nThen, with probability at least 1\u2212 p it holds that \u2200t \u2265 TPM(\u03ba, \u03b5, p): \u2211\ni\u2208[d],\u03bbi\u2264(1\u22121/\u03ba)\u03bb1 (w>t ui) 2 \u2264 \u03b5 and w>t Mwt \u2265 (1\u2212 1/\u03ba\u2212 \u03b5)\u03bb1 .\nLemma B.2 (Lemma 4.1 of [9]). Let M be a PSD matrix with eigenvalues \u03bb1 \u2265 \u00b7 \u00b7 \u00b7\u03bbd. Fix an accuracy parameter \u03b5\u0303 > 0, and consider two update sequences\nw\u0302\u22170 = w0, \u2200t \u2265 1: w\u0302\u2217t \u2190Mw\u0302\u2217t\u22121 w\u03020 = w0, \u2200t \u2265 1: w\u0302t satisfies \u2016w\u0302t \u2212Mw\u0302t\u22121\u2016 \u2264 \u03b5\u0303,\nThen, defining wt = w\u0302t/\u2016w\u0302t\u2016 and w\u2217t = w\u0302\u2217t /\u2016w\u0302\u2217t \u2016, it satisfies\n\u2016wt \u2212 w\u2217t \u2016 \u2264 \u03b5\u0303 \u00b7 \u0393(M, t),\nwhere\n\u0393(M, t) def =\n2\n\u03bbtd { t, if \u03bb1 = 1; (\u03bbt1 \u2212 1)/(\u03bb1 \u2212 1), if \u03bb1 6= 1. and we have \u0393(M, t) \u2264 2t \u00b7 max{1, \u03bb t 1} \u03bbtd\nTheorem B.3 (Inexact Power Method). Let M be a PSD matrix with eigenvalues \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and the corresponding eigenvectors u1, . . . , ud. With probability at least 1\u2212p it holds that, for every \u03b5 \u2208 (0, 1) and every t \u2265 TPM(\u03ba, \u03b5/4, p), if wt is generated by the power method with per-iteration error \u03b5\u0303 = \u03b54\u0393(M,t) , then\n\u2211\ni\u2208[d],\u03bbi\u2264(1\u22121/\u03ba)\u03bb1 (w>t ui) 2 \u2264 \u03b5 and w>t Mwt \u2265 (1\u2212 1/\u03ba\u2212 \u03b5)\u03bb1 ."}, {"heading": "B.2 Proof of Theorem 3.1", "text": "We prove Theorem 3.1 by first showing the following lemma. Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.\nLemma B.4 (useful properties of AppxPCA\u00b1). With probability at least 1 \u2212 p, it holds that (by letting \u03bb\u2217 = \u2016M\u20162):\n(a) \u03b5\u03031 \u2264 132\u0393((\u03bb(s\u22121)I\u2212M)\u22121,m1) and \u03b5\u03031 \u2264 1 32\u0393((\u03bb(s\u22121)I+M)\u22121,m1) for each iteration s \u2265 1; (b) \u03b5\u03032 \u2264 \u03b54\u0393((\u03bb(f)I\u2212M)\u22121,m2) and \u03b5\u03032 \u2264 \u03b5 4\u0393((\u03bb(f)I+M)\u22121,m2) when the repeat-until loop is over; (c) 0 \u2264 34(\u03bb(s\u22121) \u2212 \u03bb\u2217) \u2264 \u2206(s) \u2264 \u03bb(s\u22121) \u2212 \u03bb\u2217 and 12(\u03bb(s\u22121) \u2212 \u03bb\u2217) \u2264 \u03bb(s) \u2212 \u03bb\u2217 for each iteration s \u2265 1; and (d) \u03bb(f) \u2212 \u03bb\u2217 \u2208 [ \u03b4\u00d748 \u03bb(f), \u03b4\u00d7 13 \u03bb \u2217] when the repeat-until loop is over. (e) when the repeat-until loop is over,\nif w>a va \u2265 w>b vb then \u03bb(f) \u2212 \u03bbmax(M) \u2264 10 3 (\u03bb(f) \u2212 \u03bb\u2217); or if w>a va \u2264 w>b vb then \u03bb(f) + \u03bbmin(M) \u2264 10\n3 (\u03bb(f) \u2212 \u03bb\u2217) .\nProof. We denote by C(s) def = (\u03bb(s)I \u2212M)\u22121 and by D(s) def= (\u03bb(s)I +M)\u22121 for notational simplicity. Below we prove all the items by induction for a specific iteration s \u2265 2 assuming that the items of the previous s\u2212 1 iterations are true. The base case of s = 1 can be verified similar to the general arguments after some notational changes. We omitted the proofs of the base case s = 1.\n(a) Recall that\n\u0393(C(s\u22121), t) \u2264 2t \u00b7 max{1, \u03bbmax(C (s\u22121))t} \u03bbmin(C(s\u22121))t and \u0393(D(s\u22121), t) \u2264 2t \u00b7 max{1, \u03bbmax(D (s\u22121))t} \u03bbmin(D(s\u22121))t\nOn one hand, we have \u03bbmax(C (s\u22121)) = 1 \u03bb(s\u22121)\u2212\u03bb\u2217 \u2264 2 \u03bb(s\u22122)\u2212\u03bb\u2217 \u2264 2 \u2206(s\u22121) using Lemma B.4.c of the previous iteration. Combining this with the termination criterion \u2206(s\u22121) \u2265 \u03b4\u00d712 \u03bb(s\u22121), we have \u03bbmax(C\n(s\u22121)) \u2264 24 \u03b4\u00d7\u03bb(s\u22121) . On the other hand, we have \u03bbmin(C (s\u22121)) = 1 \u03bb(s\u22121)\u2212\u03bbmin(M) \u2265 1\n\u03bb(s\u22121)+\u03bb\u2217 \u2265 12\u03bb(s\u22121) . Combining the two bounds we conclude that \u0393(C (s\u22121), t) \u2264 2t(48/\u03b4\u00d7)t. It is now obvious that \u03b5\u03031 \u2264 132\u0393(C(s\u22121),m1) is satisfied because \u03b5\u03031 = 1 64m1 ( \u03b4\u00d7 48 )m1 . Similarly, on one hand, we have \u03bbmax(D (s\u22121)) = 1\n\u03bb(s\u22121)+\u03bbmin(M) \u2264 1 \u03bb(s\u22121)\u2212\u03bb\u2217 \u2264 2 \u03bb(s\u22122)\u2212\u03bb\u2217 \u2264 2 \u2206(s\u22121) using Lemma B.4.c of the previous iteration. Combining this with the termination criterion \u2206(s\u22121) \u2265 \u03b4\u00d712 \u03bb(s\u22121), we have \u03bbmax(D(s\u22121)) \u2264 24\u03b4\u00d7\u03bb(s\u22121) . On the other hand, we have \u03bbmin(D\n(s\u22121)) = 1 \u03bb(s\u22121)+\u03bbmax(M) \u2265 1 \u03bb(s\u22121)+\u03bb\u2217 \u2265 12\u03bb(s\u22121) . Combining the two bounds we conclude that \u0393(D(s\u22121), t) \u2264 2t(48/\u03b4\u00d7)t. It is now obvious that \u03b5\u03031 \u2264 132\u0393(D(s\u22121),m1) is satisfied.\n(b) The same analysis as in the proof of Lemma B.4.a suggests that \u0393(C(f), t) \u2264 2t(48/\u03b4\u00d7)t and \u0393(D(f), t) \u2264 2t(48/\u03b4\u00d7)t. These immediately imply \u03b5\u03032 \u2264 \u03b54\u0393(C(f),m2) and \u03b5\u03032 \u2264 \u03b5 4\u0393(D(f),m2)\nbecause \u03b5\u03032 = \u03b5\n8m2\n( \u03b4\u00d7 48 )m2\n(c) Because Lemma B.4.a holds for the current iteration s we can apply Theorem B.3 (with \u03b5 = 1/16 and \u03ba = 16) and get\nw>a C (s\u22121)wa \u2265\n7 8 \u03bbmax(C (s\u22121)) and w>b D (s\u22121)wb \u2265 7 8 \u03bbmax(D (s\u22121)) .\nBy the definition of v in AppxPCA\u00b1 and the Cauchy-Schwartz inequality it holds that\nw>a va = w > a C (s\u22121)wa + w>a ( va \u2212 C(s\u22121)wa ) \u2208 [ w>a C (s\u22121)wa \u2212 \u03b5\u03031, w>a C(s\u22121)wa + \u03b5\u03031 ] , and w>b vb = w > b D (s\u22121)wb + w > b ( vb \u2212D(s\u22121)wb ) \u2208 [ w>b D (s\u22121)wb \u2212 \u03b5\u03031, w>b D(s\u22121)wb + \u03b5\u03031 ] .\nCombining the above equations we have\nw>a va \u2212 \u03b5\u03031 \u2208 [7\n8 \u03bbmax(C\n(s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ]\n\u2286 [3\n4 \u03bbmax(C\n(s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and\nw>b vb \u2212 \u03b5\u03031 \u2286 [3\n4 \u03bbmax(D\n(s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) . (B.1)\nIn other words, \u2206(s) def = 34 \u00b7 1max{w>a va,w>b wb}\u2212\u03b5\u03031 \u2208\n[ 3 4(\u03bb (s\u22121) \u2212 \u03bb\u2217), \u03bb(s\u22121) \u2212 \u03bb\u2217 ] because \u03bb\u2217 =\nmax{\u03bbmax(M),\u2212\u03bbmin(M)}. At the same time, our update rule \u03bb(s) = \u03bb(s\u22121) \u2212\u2206(s)/2 ensures that \u03bb(s) \u2212 \u03bb\u2217 = \u03bb(s\u22121) \u2212 \u03bb\u2217 \u2212\u2206(s)/2 \u2265 \u03bb(s\u22121) \u2212 \u03bb\u2217 \u2212 \u03bb(s\u22121)\u2212\u03bb\u22172 = 12(\u03bb(s\u22121) \u2212 \u03bb\u2217).\n(d) The upper bound holds because \u03bb(f)\u2212\u03bb\u2217 = \u03bb(f\u22121)\u2212 \u2206(f)2 \u2212\u03bb\u2217 \u2264 ( 4 3 \u2212 12 ) \u2206(f) \u2264 5\u03b4\u00d7\u03bb(f)72 where\nthe first inequality follows from Lemma B.4.c of this last iteration, and the second inequality follows from our termination criterion \u2206(f) \u2264 \u03b4\u00d7\u03bb(f)12 . Simply rewriting this inequality we have \u03bb(f) \u2212 \u03bb\u2217 \u2264 5\u03b4\u00d7/721\u22125\u03b4\u00d7/72\u03bb \u2217 < \u03b4\u00d713 \u03bb \u2217.\nThe lower bound is because using Lemma B.4.c (of this and the previous iteration) we have \u03bb(f) \u2212 \u03bb\u2217 \u2265 14 ( \u03bb(f\u22122) \u2212 \u03bb\u2217 ) \u2265 \u2206(f\u22121)4 \u00ac \u2265 \u03b4\u00d7\u03bb(f\u22121)48 \u2265 \u03b4\u00d7\u03bb(f) 48 . Here, inequality \u00ac is because \u2206(f\u22121) > \u03b4\u00d7\u03bb (f\u22121)\n12 due to the termination criterion.\n(e) We only prove the case when w>a va \u2265 w>b vb and the other case is similar. We compute that\n\u03bb(f) \u2212 \u03bbmax(M) = \u03bb(f\u22121) \u2212 \u03bbmax(M)\u2212 \u2206(f)\n2\n\u00ac \u2264 4 3 (\u03bb(f\u22121) + \u03bbmin(M))\u2212\n\u2206(f)\n2\n= 4\n3 (\u03bb(f) + \u03bbmin(M)) +\n\u2206(f)\n2\n \u2264 4\n3 (\u03bb(f) + \u03bbmin(M)) +\n\u03b4\u00d7\u03bb(f)\n24 \u00ae \u2264 4\n3 (\u03bb(f) + \u03bbmin(M)) + 2(\u03bb\n(f) \u2212 \u03bb\u2217) \u00af \u2264 10 3 (\u03bb(f) \u2212 \u03bb\u2217) .\nAbove, \u00ac is from (B.1) together with the fact that w>a va \u2265 w>b vb;  is using the termination criterion \u2206(f) \u2264 \u03b4\u00d7\u03bb(f)12 ; \u00ae is from Lemma B.4.d\nFinally, since the success of Theorem B.3 only depends on the randomness of w\u03020, we have that with probability at least 1\u2212 p all the above items are satisfied.\nWe are now ready to prove Theorem 3.1.\nProof of Theorem 3.1. We only focus on the case when sgn = + and the other case is similar. It follows from Theorem B.3 (with \u03ba = 2) that, letting \u00b5i = 1/(\u03bb\n(f)\u2212\u03bbi) be the i-th largest eigenvalue of the matrix (\u03bb(f)I \u2212M)\u22121, then\n\u2211\ni\u2208[d],\u00b5i\u2264\u00b51/2 (w>ui)2 \u2264 \u03b5 .\nNote that if an index i \u2208 [d] satisfies \u03bb\u2217 \u2212 \u03bbi \u2265 \u03b4\u00d72 \u03bb\u2217, then we must have \u03bb\u2217 \u2212 \u03bbi \u2265 132 (\u03bb(f) \u2212 \u03bb\u2217) owing to \u03bb(f) \u2212 \u03bb\u2217 \u2264 \u03b4\u00d713 \u03bb\u2217 from Lemma B.4.d. This further implies that \u03bb(f) \u2212 \u03bbi \u2265 152 (\u03bb(f) \u2212 \u03bb\u2217). Plugging in Lemma B.4.e we further have \u03bb(f) \u2212 \u03bbi \u2265 152 \u00b7 310(\u03bb(f) \u2212 \u03bb1) > 2(\u03bb(f) \u2212 \u03bb1). Using the definition of \u00b5i, we must have \u00b51/2 > \u00b5i. In sum, we also have\n\u2211\ni\u2208[d],\u03bbi\u2264(1\u2212\u03b4\u00d7/2)\u03bb\u2217 (w>ui)2 \u2264 \u03b5 .\nOn the other hand,\nw>Mw = d\u2211\ni=1\n\u03bbi(w >ui)2 \u2265 \u2212\u03b5\u03bb\u2217 +\n\u2211\ni\u2208[d],\u03bbi>(1\u2212\u03b4\u00d7/2)\u03bb\u2217 \u03bbi(w\n>ui)2\n\u2265 \u2212\u03b5\u03bb\u2217 + (1\u2212 \u03b4\u00d7/2)\u03bb\u2217 \u00b7 \u2211\ni\u2208[d],\u03bbi>(1\u2212\u03b4\u00d7/2)\u03bb\u2217 (w>ui)2\n\u2265 \u2212\u03b5\u03bb\u2217 + (1\u2212 \u03b4\u00d7/2)(1\u2212 \u03b5)\u03bb\u2217 \u2265 (1\u2212 \u03b4\u00d7/2)(1\u2212 3\u03b5)\u03bb\u2217 .\nThe number of oracle calls to A is determined by the number of iterations in the repeat-until loop. It is easy to verify that there are at most O(log(1/\u03b4\u00d7)) such iteartions, so the total number of oracle calls to A is only O(log(1/\u03b4\u00d7)m1 +m2).\nAs for the condition number, each time we call A we have\n\u03bbmax(\u03bb (s)I \u2212M)\n\u03bbmin(\u03bb(s)I \u2212M) \u2264 \u03bb (s) \u2212 \u03bbd \u03bb(s) \u2212 \u03bb1 \u2264 \u03bb (s) + \u03bb\u2217 \u03bb(s) \u2212 \u03bb\u2217 \u2264 2\u03bb(s) \u03bb(s) \u2212 \u03bb\u2217 and \u03bbmax(\u03bb (s)I +M) \u03bbmin(\u03bb(s)I +M) \u2264 2\u03bb (s) \u03bb(s) \u2212 \u03bb\u2217\nIf s = 0 then we have \u03bb (0) \u03bb(0)\u2212\u03bb\u2217 \u2264 1+\u03b4\u00d7 \u03b4\u00d7 because \u03bb\u2217 \u2264 1. If s \u2264 f \u2212 2 then we have \u03bb(s) \u03bb(s)\u2212\u03bb\u2217 \u2264 \u03bb(s) \u2206(s+1) \u2264\n\u03bb(s)\n\u03b4\u00d7\u03bb(s+1)/12 \u2264 12\u03b4\u00d7 where the first inequality follows from Lemma B.4.c, the second inequality follows from the stopping criterion, and the third inequality follows from the monotonicity of \u03bb(s). If s = f \u2212 1 then we have \u03bb(s) \u03bb(s)\u2212\u03bb\u2217 \u2264 2\u03bb(s) \u03bb(s\u22121)\u2212\u03bb\u2217 \u2264 2\u03bb(s) \u2206(s) \u2264 2\u03bb(s) \u03b4\u00d7\u03bb(s)/12 = 24\u03b4\u00d7 where the first two inequalities follow from Lemma B.4.c and the third inequality follows from our stopping criterion. If s = f then we have \u03bb (s)\n\u03bb(s)\u2212\u03bb\u2217 \u2264 48 \u03b4\u00d7\nowing to Lemma B.4.d. In all cases we have \u03bbmax(\u03bb (s)I\u2212M)\n\u03bbmin(\u03bb(s)I\u2212M) \u2264 96 \u03b4\u00d7 and\nsimilarly for \u03bb(s)I +M ."}, {"heading": "C Lemmas Needed for Proving Our Main Theorem", "text": "In this section we provide some necessary lemmas on matrix algebra that shall become essential for our proof of Theorem 4.1. Many of these lemmas are analogous to those ones used in the SVD algorithm by the same authors of this paper [2], however, we need some extra care in this paper because the underlying matrix M is no longer PSD.\nProposition C.1. Let A,B be two (column) orthonormal matrix such that for \u03b7 \u2265 0,\nA>BB>A (1\u2212 \u03b7)I\nThen we have: there exists a matrix Q, \u2016Q\u20162 \u2264 1 such that\n\u2016A\u2212BQ\u20162 \u2264 \u221a \u03b7\nProof. Since A>A = I and A>BB>A (1\u2212 \u03b7)I, we know that A>B\u22a5(B\u22a5)>A \u03b7I. By the fact that\nA = (BB> +B\u22a5(B\u22a5)>)A = BB>A+B\u22a5(B\u22a5)>A\nwe can let Q = B>A and obtain\n\u2016A\u2212BQ\u20162 \u2264 \u2016B\u22a5(B\u22a5)>A\u20162 \u2264 \u221a \u03b7 ."}, {"heading": "C.1 Approximate Projection Lemma", "text": "The next lemma states that, projecting a symmetric matrix M into the orthogonal space of Vs \u2208 Rd\u00d7s is almost equivalent to projecting it into the orthogonal space of Qs \u2208 Rd\u00d7s, if Qs is the projection of Vs into the orthogonal space of U but \u2016V >s U\u2016 is small. This lemma is obvious if \u201csmall\u201d means zero correlation: if Vs were completely orthogonal to U then Qs would equal to Vs, so projecting M into the orthogonal space of Vs would be equivalent to that of Qs. However, even in the inexact scenario, this argument is true.\nLemma C.2. Let M be a symmetric matrix with (not necessarily sorted) eigenvalues \u03bb1, . . . , \u03bbd and the corresponding (normalized) eigenvectors u1, . . . , ud \u2208 Rd. For every k \u2265 1, define U\u22a5 = (u1, . . . , uk) \u2208 Rd\u00d7k and U = (uk+1, . . . , ud) \u2208 Rd\u00d7(d\u2212k). For every \u03b5 \u2208 (0, 12), let Vs \u2208 Rd\u00d7s be a column orthogonal matrix such that \u2016V >s U\u20162 \u2264 \u03b5, define Qs \u2208 Rd\u00d7s to be an arbitrary orthogonal basis of the column span of U\u22a5(U\u22a5)>Vs, then we have:\n\u2225\u2225\u2225 ( I \u2212QsQ>s ) M ( I \u2212QsQ>s ) \u2212 ( I \u2212 VsV >s ) M ( I \u2212 VsV >s )\u2225\u2225\u2225 2 \u2264 13\u03b5\u2016M\u20162 .\nProof of Lemma C.2. Since Qs is an orthogonal basis of the column span of U \u22a5(U\u22a5)>Vs, there is a matrix R \u2208 Rs\u00d7s such that Qs = U \u22a5(U\u22a5)>VsR\nUsing the fact that Q>s Qs = I, we have:\n(U\u22a5(U\u22a5)>VsR)>(U\u22a5(U\u22a5)>VsR) = I =\u21d2 R>V >s U\u22a5(U\u22a5)>VsR = I .\nBy the fact that V >s Vs = I and U \u22a5(U\u22a5)> + UU> = I, we can rewrite the above equality as:\nR> ( I \u2212 V >s UU>Vs ) R = I (C.1)\nFrom our lemma assumption, we have: \u2016V >s U\u20162 \u2264 \u03b5, which implies 0 V >s UU>Vs \u03b52I. Putting this into (C.1), we obtain:\nI R>R 1 1\u2212 \u03b52 I\n( 1 + 4 3 \u03b52 ) I\nThe above inequality directly implies that I RR> ( 1 + 43\u03b5 2 ) I. Therefore,\n\u2225\u2225\u2225QsQ>s \u2212 VsV >s \u2225\u2225\u2225\n2 = \u2225\u2225\u2225U\u22a5(U\u22a5)>VsRR>V >s U\u22a5(U\u22a5)> \u2212 VsV >s \u2225\u2225\u2225 2\n= \u2225\u2225\u2225U\u22a5(U\u22a5)>VsRR>V >s U\u22a5(U\u22a5)> \u2212 (U\u22a5(U\u22a5)> + UU>)VsV >s (U\u22a5(U\u22a5)> + UU>) \u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225U\u22a5(U\u22a5)>Vs(RR> \u2212 I)V >s U\u22a5(U\u22a5)> \u2225\u2225\u2225 2 + \u2225\u2225\u2225UU>VsV >s UU> \u2225\u2225\u2225 2 + 2 \u2225\u2225\u2225U\u22a5(U\u22a5)>VsV >s UU> \u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225RR> \u2212 I\n\u2225\u2225\u2225 2 + \u2225\u2225\u2225U>VsV >s U \u2225\u2225\u2225 2 + 2 \u2225\u2225\u2225V >s UU>Vs \u2225\u2225\u2225 1/2 2\n\u2264 4 3 \u03b52 + \u03b52 + 2\u03b5 < 19 6 \u03b5 .\nFinally, we have\n\u2225\u2225\u2225 ( I \u2212QsQ>s ) M ( I \u2212QsQ>s ) \u2212 ( I \u2212 VsV >s ) M ( I \u2212 VsV >s )\u2225\u2225\u2225 2\n\u2264 2 \u2225\u2225\u2225 ( QsQ > s \u2212 VsV >s ) M \u2225\u2225\u2225 2 + \u2225\u2225\u2225 ( QsQ > s \u2212 VsV >s ) MQsQ > s \u2225\u2225\u2225 2 + \u2225\u2225\u2225 ( QsQ > s \u2212 VsV >s ) MVsV > s \u2225\u2225\u2225 2 \u2264 19\u00d7 4 6 \u03b5\u2016M\u20162 < 13\u03b5\u2016M\u20162 ."}, {"heading": "C.2 Gap-Free Wedin Theorem", "text": "Lemma C.3 (two-sided gap-free Wedin theorem). For \u03b5 \u2265 0, let A,B be two symmetric matrices such that \u2016A \u2212 B\u20162 \u2264 \u03b5. For every \u00b5 \u2265 0, \u03c4 > 0, let U be column orthonormal matrix consisting of eigenvectors of A with absolute eigenvalues \u2264 \u00b5, let V be column orthonormal matrix consisting of eigenvectors of B with absolute eigenvalues \u2265 \u00b5+ \u03c4 , then we have:\n\u2016U>V \u2016 \u2264 \u03b5 \u03c4 .\nProof of Lemma C.3. We write A and B in terms of eigenvalue decomposition:\nA = U\u03a3U> + U \u2032\u03a3\u2032U \u2032> and B = V \u03a3\u0303V > + V \u2032\u03a3\u0303\u2032V \u2032> ,\nwhere U \u2032 is orthogonal to U and V \u2032 is orthogonal to V . Letting R = A\u2212B, we obtain:\n\u03a3U> = U>A = U>(B +R)\n=\u21d2 \u03a3U>V = U>BV + U>RV = U>V \u03a3\u0303 + U>RV =\u21d2 \u03a3U>V \u03a3\u0303\u22121 = U>V + U>RV \u03a3\u0303\u22121 .\nTaking spectral norm on both sides, we obtain:\n\u2016\u03a3\u20162\u2016U>V \u20162\u2016\u03a3\u0303\u22121\u20162 \u2265 \u2016\u03a3U>V \u03a3\u0303\u22121\u20162 \u2265 \u2016U>V \u20162 \u2212 \u2016U>RV \u03a3\u0303\u22121\u20162 .\nThis can be simplified to \u00b5\n\u00b5+ \u03c4 \u2016U>V \u20162 \u2265 \u2016U>V \u20162 \u2212\n\u03b5\n\u00b5+ \u03c4 ,\nand therefore we have \u2016U>V \u20162 \u2264 \u03b5\u03c4 as desired."}, {"heading": "C.3 Eigenvector Projection Lemma", "text": "Our next technical lemma studies the projection of a matrix M into the orthogonal direction of a vector v, where v has little correlation with M \u2019s leading eigenvectors below some threshold \u00b5 (denoted by U). The conclusion of the lemma says that, after the projection, if we study the leading eigenvectors of M \u2032 = (I \u2212 vv>)M(I \u2212 vv>) below some threshold \u00b5+ \u03c4 and denote it by V1, then U approximately embeds into V1, meaning that although V1 could be of a larger dimension of U , however, there exists a matrix Q with spectral norm no more than 1 such that \u2016U\u2212V1Q\u20162 is small.\nLemma C.4. Let M \u2208 Rd\u00d7d be a symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbd and corresponding eigenvectors u1, . . . , ud. Suppose |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|. Define U = (uj+1, . . . , ud) \u2208 Rd\u00d7(d\u2212j) to be the matrix consisting of all eigenvectors with absolute eigenvalues \u2264 \u00b5. Let v \u2208 Rd be a unit vector such that \u2016v>U\u20162 \u2264 \u03b5 \u2264 1/2, and define\nM \u2032 = ( I \u2212 vv> ) M ( I \u2212 vv> ) .\nThen, denoting by [V2, V1, v] \u2208 Rd\u00d7d the unitary matrix consisting of (column) eigenvectors of M \u2032, where V1 consists of eigenvectors with absolute eigenvalue \u2264 \u00b5+ \u03c4 , then there exists a matrix Q with spectral norm \u2016Q\u20162 \u2264 1 such that\n\u2016U \u2212 V1Q\u20162 \u2264 \u221a\n169\u03b52\u2016M\u201622 \u03c42 + \u03b52 .\nProof of Lemma C.4. Using Lemma C.2, let q = U \u22a5(U\u22a5)>v\n\u2016U\u22a5(U\u22a5)>v\u20162 be the projection of v to U \u22a5, we\nknow that \u2225\u2225\u2225 ( I \u2212 qq> ) M ( I \u2212 qq> ) \u2212 ( I \u2212 vv> ) M ( I \u2212 vv> )\u2225\u2225\u2225 2 \u2264 13\u03b5\u2016M\u20162 .\nDenote ( I \u2212 qq> ) M ( I \u2212 qq> ) as M \u2032\u2032. We know that uj+1, . . . , ud are still eigenvectors of M \u2032\u2032\nwith eigenvalue \u03bbj+1, . . . , \u03bbd. Apply Lemma C.3 on A = M \u2032\u2032, U and B = M \u2032, V = V2, we obtain:\n\u2016U>V2\u20162 \u2264 13\u03b5\u2016M\u20162\n\u03c4 .\nThis implies that\nU>V1V >1 U = I \u2212 U>V2V >2 U \u2212 U>vv>U (\n1\u2212 169\u03b5 2\u2016M\u201622 \u03c42\n\u2212 \u03b52 ) I ,\nwhere the inequality uses the assumption \u2016v>U\u20162 \u2264 \u03b5. Apply Proposition C.1 to A = U and B = V1, we conclude that there exists a matrix Q, \u2016Q\u20162 \u2264 1 such that\n\u2016U \u2212 V1Q\u20162 \u2264 \u221a\n169\u03b52\u2016M\u201622 \u03c42 + \u03b52 ."}, {"heading": "D Matrix Inversion via Approx Accelerated Gradient Descent", "text": "Given a positive definite matrix N, it is well-known that one can reduce the (approximate) matrix inversion problem N\u22121\u03c7 to multiple computations of the matrix-vector multiplication (i.e., of the form w\u2032 \u2190 Nw). In particular, Chebyshev method [6] uses the so-called Chebyshev polynomial for this purpose, and the number of matrix-vector multiplications is determined by the degree of that polynomial.\nIn this section, we revisit this problem by allowing matrix-vector multiplications to be computed only approximately. We emphasize that this is not a simple task in general. If matrix inversion is reduced to T matrix-vector multiplications, then a standard analysis implies that each of these multiplications must be computed up to a very small error 2\u2212\u2126(T ). If the actual matrix-vector multiplication subroutine has a logarithmic dependency on the error in its running time, then we will have a total running time at least quadratically dependent on T .4\nTo avoid such an exponentially accuracy loss, we abandon known results (such as Chebyshev method) and design our own method. We prove the following theorem in this section:\nTheorem D.1. Given a positive definite matrix N, we can reduce the problem of computing \u03be \u2190 N\u22121\u03c7 to multiple computations w\u2032 \u2190 Nw.\nMore specifically, if N satisfies N = B\u22121/2NB1/2 where N and B are both d\u00d7d positive definite matrices, for every \u03b5\u0303 > 0 and \u03c7 \u2208 Rd, in order to obtain \u03be satisfying \u2016\u03be \u2212N\u22121\u03c7\u2016 \u2264 \u03b5\u0303, \u2022 it suffices to compute w\u2032 \u2190 Nw only O\u0303 (\u221a \u03ba(N) ) times, and\n\u2022 each time of accuracy \u2016w\u2032 \u2212Nw\u2016 \u2264 O(1/poly(\u03baB, \u03b5\u0303, \u03bbmin(N))). Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3]. We prove this inexact AGD result in Appendix D.1. Our final proof of Theorem D.1 is included in Appendix D.2.\n4Indeed, for instance in the ALS algorithm of [24] for solving CCA, the authors obtained a running time proportional to 1/gap2 although there are only 1/gap iterations.\nAlgorithm 4 AGDinexact(f, x0, T )\nInput: f an L-smooth and \u03c3-strongly convex function; x0 some initial point; and T the number of iterations. Output: yT .\n1: \u03c4 \u2190 2 1+ \u221a 8L/\u03c3+1 , \u03b7 \u2190 1\u03c4L . \u03c4 = O( \u221a \u03c3\u221a L ) and \u03b7 = O( 1\u221a \u03c3L ) 2: y0 \u2190 x0, z0 \u2190 x0. 3: for k \u2190 0 to T \u2212 1 do 4: xk+1 \u2190 \u03c4zk + (1\u2212 \u03c4)yk. 5: Compute approximate gradient \u2207\u0303f(xk+1) satisfying \u2016\u2207\u0303f(xk+1)\u2212\u2207f(xk+1)\u20162 \u2264 \u03b5\u0303. 6: yk+1 \u2190 xk+1 \u2212 1L\u2207\u0303f(xk+1) 7: zk+1 \u2190 11+\u03b7\u03c3 ( zk + \u03b7\u03c3xk+1 \u2212 \u03b7\u2207\u0303f(xk+1) ) 8: end for 9: return yT .\nD.1 Inexact Accelerated Gradient Descent\nWe study an inexact version of the classical accelerated gradient descent (AGD) method, and our pseudocode is presented in Algorithm 4. The difference between our method and known AGD methods is that we only require the algorithm to know an approximate gradient \u2207\u0303f(xk+1) in each iteration k, as opposed to the exact full gradient \u2207f(xk+1). We require \u2016\u2207\u0303f(xk+1)\u2212\u2207f(xk+1)\u20162 to be upper bounded by some parameter \u03b5\u0303 in each iteration. Our next convergence theorem states that this inexact AGD method only incurs an additive loss proportional to O(\u03b5\u03032).\nTheorem D.2 (inexact AGD). If f(x) is L-smooth and \u03c3-strongly convex, then AGDinexact(f, x0, T ) produces an output yT satisfying\nf(yT )\u2212 f(x\u2217) \u2264 O(1) \u00b7 (1\u2212 \u03c4)T (f(x0)\u2212 f(x\u2217)) +O ( \u03b5\u03032 \u03c3 ) ,\nwhere \u03c4 = \u2126( \u221a \u03c3/L). In other words, if the approximate gradient oracle satisfies \u03b5\u0303 \u2264 O(\u221a\u03b5\u03c3) and\nT = O( \u221a L/\u03c3 \u00b7 log(1/\u03b5)), then we have f(yT )\u2212 f(x\u2217) \u2264 \u03b5.\nTheorem D.2 can be proved using the linear-coupling framework of [3]. In this framework, accelerated methods are analyzed by a gradient descent lemma (Lemma D.3 below), a mirror descent lemma (Lemma D.4 below), and a coupling step (Lemma D.5 and D.6 below).\nLemma D.3 (gradient descent). f(yk+1) \u2264 f(xk+1)\u2212 12L\u2016\u2207f(xk+1)\u201622 + \u03b5\u0303 2 2L .\nProof. Abbreviating xk+1 by x and yk+1 by y, the smoothness property of function f(\u00b7) tells us\nf(y)\u2212 f(x) \u2264 \u3008\u2207f(x), y \u2212 x\u3009+ L 2 \u2016y \u2212 x\u20162 .\nNow, since y \u2212 x = \u2212\u2207f(x)+\u03c7L where \u2016\u03c7\u20162 \u2264 \u03b5\u0303, we have\n\u3008\u2207f(x), y \u2212 x\u3009+ L 2 \u2016y \u2212 x\u201622 = \u22121 L \u3008\u2207f(x),\u2207f(x) + \u03c7\u3009+ 1 2L \u3008\u2207f(x) + \u03c7,\u2207f(x) + \u03c7\u3009\n\u2264 \u2212 1 2L \u2016\u2207f(x)\u201622 +\n\u03b5\u03032 2L .\nSince our update on z can be written in the following minimization form, known as mirrordescent form in optimization literatures:\nz (i) k+1 = minz {1 2 \u2016z \u2212 zk\u201622 + \u03b7\u3008\u2207\u0303f(xk+1), z\u3009+ \u03b7\u03c3 2 \u2016z \u2212 xk+1\u201622 } . (D.1)\nIt implies the following classical lemma (see for instance [4, Lemma 5.4]):\nLemma D.4 (mirror descent). For every u \u2208 Rn,\n\u03b7\u3008\u2207\u0303f(xk+1), zk+1 \u2212 u\u3009 \u2212 \u03b7\u03c3\n2 \u2016xk+1 \u2212 u\u201622 \u2264 \u2212\n1 2 \u2016zk \u2212 zk+1\u201622 + 1 2 \u2016zk \u2212 u\u201622 \u2212 1 + \u03b7\u03c3 2 \u2016zk+1 \u2212 u\u201622 .\nThe following inequality is a nature linear combination of the two lemmas above:\nLemma D.5 (coupling 1). For every u \u2208 Rn,\n\u03b7\u3008\u2207f(xk+1), zk \u2212 u\u3009 \u2212 \u03b7\u03c3\n2 \u2016u\u2212 xk+1\u201622\n\u2264 \u03b72L ( f(xk+1)\u2212 f(yk+1) ) + 1\n2 \u2016zk \u2212 u\u201622 \u2212\n1 + \u03b7\u03c3/2\n2 \u2016zk+1 \u2212 u\u201622 + \u03b5\u03032(\n\u03b7 \u03c3 + \u03b72 2 ) .\nProof. Combining Lemma D.3 and Lemma D.4 we deduce that for each i \u2208 [n],\n\u3008\u03b7\u2207f(xk+1), zk \u2212 u\u3009 \u2212 \u03b7\u03c3\n2 \u2016xk+1 \u2212 u\u201622\n\u2264 \u3008\u03b7\u2207f(xk+1), zk \u2212 zk+1\u3009+ \u3008\u03b7\u2207\u0303f(xk+1), zk+1 \u2212 u\u3009+ \u03b5\u0303\u03b7\u2016zk+1 \u2212 u\u20162 \u2212 \u03b7\u03c3\n2 \u2016xk+1 \u2212 u\u201622\n\u00ac \u2264 \u3008\u03b7\u2207f(xk+1), zk \u2212 zk+1\u3009 \u2212 1\n2 \u2016zk \u2212 zk+1\u201622 +\n1 2 \u2016zk \u2212 u\u201622 \u2212 1 + \u03b7\u03c3 2 \u2016zk+1 \u2212 u\u201622 + \u03b5\u0303\u03b7\u2016zk+1 \u2212 u\u20162\n \u2264 \u03b7\n2\n2 \u2016\u2207f(xk+1)\u201622 +\n1 2 \u2016zk \u2212 u\u201622 \u2212 1 + \u03b7\u03c3/2 2 \u2016zk+1 \u2212 u\u201622 +\n\u03b5\u03032\u03b7\n\u03c3 \u00ae \u2264 \u03b72L ( f(xk+1)\u2212 f(yk+1) ) + 1\n2 \u2016zk \u2212 u\u201622 \u2212\n1 + \u03b7\u03c3/2\n2 \u2016zk+1 \u2212 u\u201622 + \u03b5\u03032(\n\u03b7 \u03c3 + \u03b72 2 ) .\nAbove, \u00ac uses Lemma D.4,  uses the Young\u2019s inequality which states 2\u3008a, b\u3009 \u2264 \u2016a\u20162 + \u2016b\u20162, \u00ae uses Lemma D.3.\nTaking into account xk+1 = \u03c4zk + (1 \u2212 \u03c4)yk and the convexity of f(\u00b7), we can rewrite some terms of Lemma D.5 and obtain\nLemma D.6 (coupling 2).\n0 \u2264 (1\u2212 \u03c4)\u03b7 \u03c4 (f(yk)\u2212f(x\u2217))\u2212 \u03b7 \u03c4 (f(yk+1)\u2212f(x\u2217))+ 1 2 \u2016zk\u2212x\u2217\u201622\u2212 1 + \u03b7\u03c3/2 2 \u2016zk+1\u2212x\u2217\u201622+ \u03b5\u03032( \u03b7 \u03c3 + \u03b72 2 )\nProof.\n\u03b7(f(xk+1)\u2212 f(x\u2217)) \u00ac \u2264 \u03b7\u3008\u2207f(xk+1), xk+1 \u2212 x\u2217\u3009 \u2212 \u03b7\u03c3\n2 \u2016x\u2217 \u2212 xk+1\u201622\n= \u03b7\u3008\u2207f(xk+1), xk+1 \u2212 zk\u3009+ \u03b7\u3008\u2207f(xk+1), zk \u2212 x\u2217\u3009 \u2212 \u03b7\u03c3 2 \u2016x\u2217 \u2212 xk+1\u201622\n = (1\u2212 \u03c4)\u03b7 \u03c4 \u3008\u2207f(xk+1), yk \u2212 xk+1\u3009+ \u03b7\u3008\u2207f(xk+1), zk \u2212 x\u2217\u3009 \u2212 \u03b7\u03c3 2 \u2016x\u2217 \u2212 xk+1\u201622 \u00ae \u2264 (1\u2212 \u03c4)\u03b7\n\u03c4 (f(yk)\u2212 f(xk+1)) + \u03b72L\n( f(xk+1)\u2212 f(yk+1) ) + 1\n2 \u2016zk \u2212 u\u201622 \u2212\n1 + \u03b7\u03c3/2\n2 \u2016zk+1 \u2212 u\u201622 + \u03b5\u03032 (\u03b7 \u03c3 + \u03b72 2 ) .\nAbove, \u00ac is owing to the strong convexity of f(\u00b7),  uses the fact that xk+1 = \u03c4zk + (1\u2212 \u03c4)yk, and \u00ae uses the convexity of f(\u00b7) as well as Lemma D.5 with the choice of u = x\u2217. Recall \u03b7 = 1\u03c4L , we arrive at the desired inequality.\nWe are now ready to prove Theorem D.2.\nProof of Theorem D.2. We choose \u03c4 = 2 1+ \u221a 8L/\u03c3+1 \u2208 [0, 1), and this choice ensures that 1 +\u03b7\u03c3/2 =\n1 1\u2212\u03c4 . Under these parameter choices, Lemma D.6 becomes ( f(yk+1)\u2212f(x\u2217) ) + \u03c4\n2\u03b7(1\u2212 \u03c4)\u2016zk+1\u2212x \u2217\u201622 \u2264 (1\u2212\u03c4)\n( (f(yk)\u2212f(x\u2217))+\n\u03c4 2\u03b7(1\u2212 \u03c4)\u2016zk\u2212x \u2217\u201622 ) +\u03b5\u03032\u03c4 ( 1 \u03c3 + \u03b7 2 )\nTelescoping it for all iterations k = 0, 1, . . . , T \u2212 1, we conclude that f(yT )\u2212f(x\u2217) \u2264 (1\u2212\u03c4)T ( f(y0)\u2212f(x\u2217)+ \u03c4\n2\u03b7 \u2016z0\u2212x\u2217\u201622\n) +\u03b5\u03032( 1\n\u03c3 + \u03b7 2 ) \u2264 O(1)\u00b7(1\u2212\u03c4)T (f(x0)\u2212f(x\u2217))+O ( \u03b5\u03032 \u03c3 ) .\nwhere the last inequality is because (i) x0 = y0 = z0, (ii) \u03c4/\u03b7 = O(\u03c3) and (iii) the strong convexity of f(\u00b7) which implies f(x0)\u2212 f(x\u2217) \u2265 \u03c32 \u2016x0 \u2212 x\u2217\u201622."}, {"heading": "D.2 Proof of Theorem D.1", "text": "Proof of Theorem D.1. We first verify accuracy. Since\n\u2016\u03be \u2212N\u22121\u03c7\u2016 \u2264 \u03b5\u0303\u21d0= \u2016B1/2\u03be \u2212B1/2N\u22121\u03c7\u2016 \u2264 \u03b5\u0303 \u00b7 \u221a \u03bbmin(B)\n\u21d0= \u2016B1/2\u03be \u2212N\u22121B1/2\u03c7\u2016 \u2264 \u03b5\u0303 \u00b7 \u221a \u03bbmin(B) , (D.2)\nit suffices to find \u03be to satisfy (D.2) in order to satisfy the accuracy requirement \u2016\u03be \u2212N\u22121\u03c7\u2016 \u2264 \u03b5\u0303. Define f(x)\ndef = 12x\n>Nx\u2212 ( B1/2\u03c7 )> x and let x\u2217 be its minimizer. Then it satisfies x\u2217 = N\u22121B1/2\u03c7\nand f(x) \u2212 f(x\u2217) = 12(x \u2212 x\u2217)>N(x \u2212 x\u2217). For this reason, it suffices to find an approximate minimizer of f(x) satisfying\n1 2 (x\u2212 x\u2217)>N(x\u2212 x\u2217) = f(x)\u2212 f(x\u2217) \u2264 \u03b5\u0303\n2\n2 \u03bbmin(B)\u03bbmin(N) =: \u03b5\u0303\n\u2032 (D.3)\nbecause if we let \u03be = B\u22121/2x then the above inequality implies 12\u2016x\u2212 x\u2217\u20162 \u2264 \u03b5\u0303 2 2 \u00b7 \u03bbmin(B) which is the same as (D.2). In sum, we can call AGDinexact to find an approximate minimizer x with additive error no more than \u03b5\u0303\u2032, and then defining \u03be = B\u22121/2x gives a solution of \u03be satisfying \u2016\u03be\u2212N\u22121\u03c7\u2016 \u2264 \u03b5\u0303.\nWe now focus on the actual implementation of AGDinexact. If we choose x0 = 0 as the initial vector, we can write xk, yk, zk implicitly as xk = B 1/2xk, yk = B 1/2yk, zk = B\n1/2yk (thus only keep track of xk,yk, zk) throughout the algorithm. Under these notations, we claim that it suffices to perform matrix vector multiplication on N (i.e., of the form w\u2032 \u2190 Nw) for at most O(T ) times on those implicit vectors where T = O (\u221a \u03bbmax(N)/\u03bbmin(N) log(1/\u03b5\u0303 \u2032) )\nis the number of iterations of AGDinexact according to Theorem D.2.\nThis is so because \u2207f(xk) = Nxk\u2212B1/2\u03c7 = B1/2 ( Nxk\u2212\u03c7 ) and therefore for instance yk+1 \u2190 xk+1 \u2212 1L\u2207\u0303f(xk+1) can be implemented as yk+1 \u2190 xk+1 \u2212 1L(Nxk+1 \u2212 \u03c7) so only matrix-vector multiplication on N is needed. In addition, as long as each w\u2032 \u2190 Nw is computed to an additive error \u2016w\u2032\u2212Nw\u2016 \u2264 O ( \u03b5\u0303 \u00b7\u03bbmin(N) \u221a \u03bbmin(B)/\u03bbmax(B) ) , we can use B1/2(w\u2032\u2212\u03c7) as the approximate\ngradient which is different from the true gradient \u2207f(xk) by an additive amount O( \u221a \u03bbmin(N)\u03b5\u0303\u2032). This satisfies the approximation require of Theorem D.2, and thus the accuracy guarantee provided by Theorem D.2 is satisfied."}, {"heading": "E Proof Details for Section 4: GenEV Theorems", "text": ""}, {"heading": "E.1 Proof of Theorem 4.1", "text": "In this section we prove Theorem 4.1 formally.\nTheorem 4.1 (restated). Let M \u2208 Rd\u00d7d be a symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbd \u2208 [\u22121, 1] and corresponding eigenvectors u1, . . . , ud. Suppose without loss of generality that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|.\nSuppose k \u2208 [d], \u03b4\u00d7, p \u2208 (0, 1). Then, LazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) \u2208 Rd\u00d7k which, with probability at least 1 \u2212 p, satisfies all of the following properties. (Denote by Mk = (I \u2212 VkV >k )M(I \u2212 VkV >k ).)\n(a) Core lemma: if \u03b5pca \u2264 \u03b5 4\u03b4\u00d7 212k3(|\u03bb1|/|\u03bbk|)2 , then \u2016V > k U\u20162 \u2264 \u03b5, where U = (uj , . . . , ud) is the\n(column) orthonormal matrix and j is the smallest index satisfying |\u03bbj | \u2264 (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162. (b) Spectral norm guarantee: if \u03b5pca \u2264 \u03b4 5 \u00d7\n228k3(|\u03bb1|/|\u03bbk+1|)6 , then |\u03bbk+1| \u2264 \u2016Mk\u20162 \u2264 |\u03bbk+1| 1\u2212\u03b4\u00d7 .\n(c) Rayleigh quotient guarantee: if \u03b5pca \u2264 \u03b4 5 \u00d7 228k3(|\u03bb1|/|\u03bbk+1|)6 , then (1 \u2212 \u03b4\u00d7)|\u03bbk| \u2264 |v > kMvk| \u2264\n1 1\u2212\u03b4\u00d7 |\u03bbk|.\n(d) Schatten-q norm guarantee: for every q \u2265 1, if \u03b5pca \u2264 \u03b4 5 \u00d7\n228k3d4/q(|\u03bb1|/|\u03bbk+1|)6 , then\n\u2016Mk\u2016Sq \u2264 (1 + \u03b4\u00d7)2 (1\u2212 \u03b4\u00d7)2 ( d\u2211\ni=k+1\n\u03bbqi\n)1/q = (1 + \u03b4\u00d7)2\n(1\u2212 \u03b4\u00d7)2 min V \u2208Rd\u00d7k,V >V=I\n{ \u2016(I\u2212V V >)M(I\u2212V V >)\u2016Sq } .\nProof of Theorem 4.1. Let Vs = (v1, . . . , vs), so we can write Ms = (I \u2212 VsV >s )M(I \u2212 VsV >s ) = (I \u2212 vsv>s )Ms\u22121(I \u2212 vsv>s ) . We first claim that \u2016Ms\u22121\u20162 \u2265 |\u03bbs| for every s = 1, . . . , k. This can be proved by the Cauchy interlacing theorem. Indeed, M2s\u22121 = (I \u2212 Vs\u22121V >s\u22121)M2(I \u2212 Vs\u22121V >s\u22121) is a projection of M2 into a d \u2212 s + 1 dimensional space, and therefore its largest eigenvalue \u2016M2s\u22121\u20162 should be at least as large as |\u03bbs|2, the s-th largest eigenvalue of M2. In other words, we have shown \u2016Ms\u22121\u20162 \u2265 |\u03bbs|.\n(a) Define \u03bb\u0302 = \u2016Mk\u22121\u20162 \u2265 |\u03bbk|. Note that all column vectors in Vs are automatically eigenvectors of Ms with eigenvalues zero. For analysis purpose only, let Ws be the column matrix of eigenvectors in V \u22a5 s of Ms that have\nabsolute eigenvalues in the range [0, (1\u2212 \u03b4\u00d7 + \u03c4s)\u03bb\u0302], where \u03c4s def= s2k\u03b4\u00d7. We now show that for every s = 0, . . . , k, there exists a matrix Qs such that \u2016U \u2212WsQs\u20162 is small and \u2016Qs\u20162 \u2264 1. We will do this by induction.\nIn the base case: since \u03c40 = 0, we have W0 = U by the definition of U . We can therefore define Q0 to be the identity matrix. For every s = 0, 1, . . . , k \u2212 1, suppose there exists a matrix Qs with \u2016Qs\u20162 \u2264 1 that satisfies \u2016U \u2212WsQs\u20162 \u2264 \u03b7s for some \u03b7s > 0, we construct Qs+1 as follows. First we observe that AppxPCA\u00b1 outputs a vector v\u2032s+1 satisfying \u2016v\u2032>s+1Ws\u201622 \u2264 \u03b5pca and \u2016v\u2032>s+1Vs\u201622 \u2264 \u03b5pca with probability at least 1 \u2212 p/k. This follows from Theorem 3.1 (using M = Ms) because [0, (1\u2212 \u03b4\u00d7 + \u03c4s)\u03bb\u0302] \u2286 [0, (1\u2212 \u03b4\u00d7/2)\u03bb\u0302], together with the fact that \u2016Ms\u20162 \u2265 \u2016Mk\u22121\u20162 \u2265 \u03bb\u0302. Now, since vs+1 is the projection of v\u2032s+1 into V \u22a5s , we have\n\u2016v>s+1Ws\u201622 \u2264 \u2016v\u2032>s+1Ws\u201622\n\u2016(I \u2212 VsV >s )v\u2032s+1\u201622 = \u2016v\u2032>s+1Ws\u201622 1\u2212 \u2016V >s v\u2032s+1\u201622 \u2264 \u03b5pca 1\u2212 \u03b5pca < 1.5\u03b5pca . (E.1)\nNext we apply Lemma C.4 with M = Ms, M \u2032 = Ms+1, U = Ws, V = Ws+1, v = vs+1, \u00b5 = (1\u2212 \u03b4\u00d7 + \u03c4s)\u03bb\u0302, and \u03c4 = (\u03c4s+1 \u2212 \u03c4s)\u03bb\u0302. We obtain a matrix Q\u0303s, \u2016Q\u0303s\u20162 \u2264 1 such that5\n\u2016Ws \u2212Ws+1Q\u0303s\u20162 \u2264 \u221a\n169(\u03bb1/\u03bb\u0302)2 \u00b7 1.5\u03b5pca (\u03c4s+1 \u2212 \u03c4s)2 + \u03b5pca < 32\u03bb1k\n\u221a \u03b5pca\n\u03bbk\u03b4\u00d7 ,\nand this implies that\n\u2016Ws+1Q\u0303sQs \u2212 U\u20162 \u2264 \u2016Ws+1Q\u0303sQs \u2212WsQs\u20162 + \u2016WsQs \u2212 U\u20162 \u2264 \u03b7s + 32\u03bb1k\n\u221a \u03b5pca\n\u03bbk\u03b4\u00d7 .\nLet Qs+1 = Q\u0303sQs we know that \u2016Qs+1\u20162 \u2264 1 and\n\u2016Ws+1Qs+1 \u2212 U\u20162 \u2264 \u03b7s+1 def= \u03b7s + 32\u03bb1k\n\u221a \u03b5pca\n\u03bbk\u03b4\u00d7 .\nTherefore, after k-iterations of LazyEV, we obtain:\n\u2016WkQk \u2212 U\u20162 \u2264 \u03b7k = 32\u03bb1k 2\u221a\u03b5pca \u03bbk\u03b4\u00d7\nMultiply U> from the left, we obtain \u2016U>WkQk \u2212 I\u20162 \u2264 \u03b7k. Since \u2016Qk\u20162 \u2264 1, we must have \u03c3min(U\n>Wk) \u2265 1\u2212 \u03b7k (here \u03c3min denotes the smallest singular value). Therefore, U>WkW > k U (1\u2212 \u03b7k)2I .\nSince Vk and Wk are orthogonal of each other, we have\nU>VkV > k U U>(I \u2212WkW>k )U I \u2212 (1\u2212 \u03b7k)2I 2\u03b7kI\nTherefore,\n\u2016V >k U\u20162 \u2264 8 (|\u03bb1|/|\u03bbk|)1/2k\u03b51/4pca\n\u03b4 1/2 \u00d7\n\u2264 \u03b5 .\n(b) The statement is obvious when k = 0. For every k \u2265 1, the lower bound is obvious. We prove the upper bound by contradiction. Suppose that \u2016Mk\u20162 > |\u03bbk+1|1\u2212\u03b4\u00d7 . Then, since \u2016Mk\u22121\u20162 \u2265 \u2016Mk\u20162 and therefore |\u03bbk+1|, . . . , |\u03bbd| < (1 \u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162, we can apply Theorem 4.1.a of the current k to deduce that \u2016V >k U>k\u20162 \u2264 \u03b5 where U>k def = (uk+1, . . . , ud). We now apply\nLemma C.2 with Vs = Vk and U = U>k, we obtain a matrix Qk \u2208 Rd\u00d7k whose columns are spanned by u1, . . . , uk and satisfy\u2225\u2225\u2225 ( I \u2212QkQ>k ) M ( I \u2212QkQ>k ) \u2212 ( I \u2212 VkV >k ) M ( I \u2212 VkV >k )\u2225\u2225\u2225 2 < 16|\u03bb1|\u03b5 . However, our assumption says that the second matrix ( I \u2212 VkV >k ) M ( I \u2212 VkV >k ) has spectral\nnorm at least |\u03bbk+1|/(1\u2212\u03b4\u00d7), but we know that ( I \u2212QkQ>k ) M ( I \u2212QkQ>k ) has spectral norm exactly |\u03bbk+1| due to the definition of Qk. Therefore, we must have |\u03bbk+1|1\u2212\u03b4\u00d7 \u2212 |\u03bbk+1| \u2264 16|\u03bb1|\u03b5 due to triangle inequality. In other words, by selecting \u03b5 in Theorem 4.1.a to satisfy \u03b5 \u2264 \u03b4\u00d716|\u03bb1|/|\u03bbk+1| (which is satisfied by our assumption on \u03b5pca), we get a contradiction so can conclude that \u2016Mk\u20162 \u2264 |\u03bbk+1|1\u2212\u03b4\u00d7 .\n5Technically speaking, to apply Lemma C.4 we need U = Ws to consist of all eigenvectors of Ms with absolute eigenvalues \u2264 \u00b5. However, we only defined Ws to be such eigenvectors that are also orthogonal to Vs. It is straightforward to verify that the same result of Lemma C.4 remains true because vs+1 is orthogonal to Vs.\n(c) We compute that\n|v>kMvk| = |v>kMk\u22121vk| \u00ac \u2265 |v \u2032> k Mk\u22121v \u2032 k| \u2016(I \u2212 Vk\u22121V >k\u22121)v\u2032k\u201622  \u2265 |v \u2032> k Mk\u22121v \u2032 k|\n(1\u2212\u221a\u03b5pca)2 \u00ae \u2265 1\u2212 \u03b5pca\n(1\u2212\u221a\u03b5pca)2 (1\u2212 \u03b4\u00d7/2)\u2016Mk\u22121\u20162 \u2265 (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162 . (E.2)\nAbove, \u00ac is because vk is the projection of v \u2032 k into V \u22a5 k\u22121,  is because \u2016V >k\u22121v\u2032k\u201622 \u2264 \u03b5pca following the same reason as (E.1), and \u00ae is owing to Theorem 3.1. Next, since \u2016Mk\u22121\u20162 \u2265 |\u03bbk|, we automatically have |v>kMvk| \u2265 (1\u2212\u03b4\u00d7)|\u03bbk|. On the other hand, |v>kMvk| = |v>kMk\u22121vk| \u2264 \u2016Mk\u22121\u20162 \u2264 |\u03bbk|1\u2212\u03b4\u00d7 where the last inequality is owing to Theorem 4.1.b.\n(d) Since \u2016V >k U\u20162 \u2264 \u03b5c def = 8 (|\u03bb1|/|\u03bbk|)1/2k\u03b51/4pca \u03b4 1/2 \u00d7 from the analysis of Theorem 4.1.a, we can apply\nLemma C.2 to obtain a (column) orthogonal matrix Qk \u2208 Rd\u00d7k such that\n\u2016M \u2032k \u2212Mk\u20162 \u2264 16|\u03bb1|\u03b5c, where M \u2032k def = (I \u2212QkQ>k )M(I \u2212QkQ>k ) (E.3)\nSuppose U = (ud\u2212p+1, . . . , ud) is of dimension d\u00d7p, that is, there are exactly p eigenvalues of M whose absolute value is \u2264 (1\u2212\u03b4\u00d7)\u2016Mk\u22121\u20162. Then, the definition of Qk in Lemma C.2 tells us U>Qk = 0 so M \u2032k agrees with M on all the eigenvalues and eigenvectors {(\u03bbj , uj)}dj=d\u2212p+1 because an index j satisfies |\u03bbj | \u2264 (1\u2212\u03b4\u00d7)\u2016Mk\u22121\u20162 if and only if j \u2208 {d\u2212p+1, d\u2212p+2, . . . , d}. Denote by \u00b51, . . . , \u00b5d\u2212k the eigenvalues of M \u2032k excluding the k zero eigenvalues in subspace Qk, and assume without loss of generality that {\u00b51, . . . , \u00b5p} = {\u03bbd\u2212p+1, . . . , \u03bbd}. Then,\n\u2016M \u2032k\u2016qSq = d\u2212k\u2211\ni=1\n|\u00b5i|q = p\u2211\ni=1\n|\u00b5i|q + d\u2212k\u2211\ni=p+1\n|\u00b5i|q = d\u2211\ni=d\u2212p+1 |\u03bbi|q +\nd\u2212k\u2211\ni=p+1\n|\u00b5i|q\n\u00ac \u2264\nd\u2211\ni=d\u2212p+1 |\u03bbi|q + (d\u2212 k \u2212 p)\u2016M \u2032k\u2016q2  \u2264\nd\u2211\ni=d\u2212p+1 |\u03bbi|q + (d\u2212 k \u2212 p)(\u2016Mk\u20162 + 16|\u03bb1|\u03b5c)q\n\u00ae \u2264\nd\u2211\ni=d\u2212p+1 |\u03bbi|q + (d\u2212 k \u2212 p) ( |\u03bbk+1| (1\u2212 \u03b4\u00d7) + 16|\u03bb1|\u03b5c )q\nAbove, \u00ac is because each |\u00b5i| is no greater than \u2016M \u2032k\u20162, and  is owing to (E.3), and \u00ae is because of Theorem 4.1.b. Suppose we choose \u03b5c so that \u03b5c \u2264 |\u03bbk+1|\u03b4\u00d716\u03bb1 (and this is indeed satisfied by our assumption on \u03b5pca), then we can continue and write\n\u2016M \u2032k\u2016qSq \u2264 d\u2211\ni=d\u2212p+1 |\u03bbi|q + (d\u2212 k \u2212 p)\n(1 + \u03b4\u00d7)q (1\u2212 \u03b4\u00d7)q |\u03bbk+1|q\n\u00af \u2264\nd\u2211\ni=d\u2212p+1 |\u03bbi|q +\n(1 + \u03b4\u00d7)q (1\u2212 \u03b4\u00d7)2q d\u2212p\u2211\ni=k+1\n|\u03bbi|q \u2264 (1 + \u03b4\u00d7)q (1\u2212 \u03b4\u00d7)2q d\u2211\ni=k+1\n|\u03bbi|q .\nAbove, \u00af is because for each eigenvalue \u03bbi where i \u2208 {k + 1, k + 2, . . . , d \u2212 p}, we have\n|\u03bbi| > (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162 \u2265 (1\u2212 \u03b4\u00d7)|\u03bbk| \u2265 (1\u2212 \u03b4\u00d7)|\u03bbk+1|. Finally, using (E.3) again we have\n\u2016Mk\u2016Sq \u2264 \u2016M \u2032k\u2016Sq + \u2016Mk \u2212M \u2032k\u2016Sq \u2264 \u2016M \u2032k\u2016Sq + d1/p\u2016Mk \u2212M \u2032k\u20162\n\u2264 1 + \u03b4\u00d7 (1\u2212 \u03b4\u00d7)2\n( d\u2211\ni=k+1\n|\u03bbi|q )1/q + 16d1/p|\u03bb1|\u03b5c\nAs long as \u03b5c \u2264 \u03b4\u00d7|\u03bbk+1|16d1/p\u03bb1 , we have\n\u2016Mk\u2016Sq \u2264 (1 + \u03b4\u00d7)2 (1\u2212 \u03b4\u00d7)2 ( d\u2211\ni=k+1\n|\u03bbi|q )1/q\nas desired. Finally, we note that \u03b5c \u2264 \u03b4\u00d7\u03bbk+116d1/p\u03bb1 is satisfied with our assumption on \u03b5pca, and note that minV \u2208Rd\u00d7k,V >V=I { \u2016(I\u2212V V >)M(I\u2212V V >)\u2016Sq } = (\u2211d i=k+1 |\u03bbi|q )1/q which follows easily from Cauchy interlacing theorem."}, {"heading": "E.2 Proofs of Corollaries 4.3 and 4.4", "text": "Proof of Corollary 4.3. Define Vk = B 1/2Vk = LazyEV(\u00b7 \u00b7 \u00b7 ) to be the direct output of LazyEV. It is clear from the definition of generalized eigenvectors that B1/2u1, . . . , B 1/2ud are eigenvectors of M def = B\u22121/2AB\u22121/2 with eigenvalues \u03bb1, . . . , \u03bbd. Applying Theorem 4.1.a, we have: \u2016V >k U\u20162 \u2264 \u03b5 where U = (B1/2uj , . . . , B 1/2ud) is a (column) orthonormal matrix and j is the smallest index satisfying |\u03bbj | \u2264 (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162. Since it satisfies \u2016Mk\u22121\u20162 \u2265 |\u03bbk|, we have\n|\u03bbk+1| = |\u03bbk|(1\u2212 gap) = |\u03bbk|(1\u2212 \u03b4\u00d7) \u2264 (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162 .\nTherefore, j must be equal to k+1 according to its definition, so we have U = B1/2W. This implies \u2016V>k BW\u20162 = \u2016V >k U\u20162 \u2264 \u03b5.\nThe running time of the algorithm comes directly from Theorem 4.2 by putting in the parameters. Proof of Corollary 4.4. Define Vk = B\n1/2Vk = LazyEV(\u00b7 \u00b7 \u00b7 ) to be the direct output of LazyEV. It is clear from the definition of generalized eigenvectors that B1/2u1, . . . , B\n1/2ud are eigenvectors of M def = B\u22121/2AB\u22121/2 with eigenvalues \u03bb1, . . . , \u03bbd. Applying Theorem 4.1.b, we have: \u2225\u2225(I \u2212 VkV > k )B \u22121/2AB\u22121/2(I \u2212 VkV >k ) \u2225\u2225 2 \u2264 |\u03bbk+1|1\u2212\u03b5 .\nNext, for every vector w \u2208 Rd that is B-orthogonal to Vk, that is, w>BVk = 0, we can define w def = B1/2w and we know w is orthogonal to Vk. We can apply the above spectral upper bound and get\nw>Aw = w>B\u22121/2AB\u22121/2w = w>(I \u2212 VkV >k )B\u22121/2AB\u22121/2(I \u2212 VkV >k )w\n\u2264 \u2016w\u201622 \u00b7 |\u03bbk+1| 1\u2212 \u03b5 = w >Bw \u00b7 |\u03bbk+1| 1\u2212 \u03b5\nas desired. The running time of the algorithm comes directly from Theorem 4.2 by putting in the parameters."}, {"heading": "F Proof Details for Section 6: CCA Theorems", "text": ""}, {"heading": "F.1 The Main Convergence Theorem", "text": "Since LazyCCA only admits minor changes on top of LazyEV, the next theorem is an almost identical copy of Theorem 4.1. To make this paper concise, instead of reproving Theorem F.1 line by line, we here only sketch the main changes needed in the new proof.\nTheorem F.1. Let M = B\u22121/2AB\u22121/2 \u2208 Rd\u00d7d be a symmetric matrix where A and B are matrices coming from a CCA instance using Lemma 2.3. Suppose M has eigenvalues \u03bb1, . . . , \u03bbd \u2208 [\u22121, 1] and corresponding eigenvectors u1, . . . , ud. Suppose without loss of generality that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbd|.\nFor every k \u2208 [d], \u03b4\u00d7, p \u2208 (0, 1), there exists some \u03b5pca \u2264 O ( poly(\u03b4\u00d7,\n|\u03bb1| |\u03bbk+1 , 1 d) ) such that LazyCCA\noutputs a (column) orthonormal matrix Vk = (v1, . . . , v2k) \u2208 Rd\u00d72k which, with probability at least 1\u2212 p, satisfies all of the following properties. (Denote by Ms = (I \u2212 VsV >s )M(I \u2212 VsV >s ).)\n(a) Correlation guarantee: \u2016V >k U\u20162 \u2264 \u03b5, where U = (uj , . . . , ud) and j is the smallest index satisfying |\u03bbj | \u2264 (1\u2212 \u03b4\u00d7)\u2016Mk\u22121\u20162. (b) Spectral norm guarantee: |\u03bb2k+1| \u2264 \u2016Mk\u20162 \u2264 |\u03bb2k+1|1\u2212\u03b4\u00d7 . (c) Rayleigh quotient guarantee: (1\u2212 \u03b4\u00d7)|\u03bb2k| \u2264 |v>2k\u22121Mv2k\u22121| = |v>2kMv2k| \u2264 11\u2212\u03b4\u00d7 |\u03bb2k|. (d) Schatten-q norm guarantee: for every q \u2265 1,\n\u2016Mk\u2016Sq \u2264 (1 + \u03b4\u00d7)2 (1\u2212 \u03b4\u00d7)2 ( d\u2211\ni=2k+1\n\u03bbqi\n)1/q = (1 + \u03b4\u00d7)2\n(1\u2212 \u03b4\u00d7)2 min V \u2208Rd\u00d72k,V >V=I\n{ \u2016(I\u2212V V >)M(I\u2212V V >)\u2016Sq } .\nProof sketch of Theorem F.1. Recall that when a vector vs \u2208 Rd is obtained in iteration s of LazyEV, the proof of Theorem 4.1 suggest that the following two properties hold\n\u2223\u2223v>s Ms\u22121vs \u2223\u2223\u2223 \u2265 (1\u2212 \u03b4\u00d7)\u2016Ms\u22121\u20162 and \u2225\u2225v>s Ws\u22121 \u2225\u2225\u2225 2\n2 \u2264 1.5\u03b5pca . (F.1)\n(The first property is shown in (E.2), and the second property is shown in (E.1). Recall that Ws\u22121 is the column orthonormal matrix containing all eigenvectors of Ms\u22121 whose absolute eigenvalues are below some threshold.) Then, the proof of Theorem 4.1 proceeds by heavily relying on (F.1).\nIn our LazyCCA, after obtaining this same vector vs, we write it as vs = (\u03be \u2032 s, \u03b6 \u2032 v) and perform\nblock-scaling \u03bes = \u03be \u2032 s/( \u221a 2\u2016\u03be\u2032s\u2016) and \u03b6s = \u03b6 \u2032s/( \u221a\n2\u2016\u03b6 \u2032s\u2016), see Line 7 of LazyCCA. Therefore, in order for the same proof of Theorem 4.1 to hold, we need to show that this new vector (\u03bes, \u03b6s) satisfies the same properties up to constants:\n\u2223\u2223\u2223 ( \u03bes \u03b6s )> Ms\u22121 ( \u03bes \u03b6s )\u2223\u2223\u2223 \u2265 (1\u2212 \u03b4\u00d7)\u2016Ms\u22121\u20162 and \u2225\u2225\u2225 ( \u03bes \u03b6s )> Ws\u22121 \u2225\u2225\u2225 2 2 \u2264 12.5\u03b5pca . (F.2)\nSuppose Vs\u22121 = ( \u00b1\u03be1 \u00b7 \u00b7 \u00b7 \u00b1\u03bes\u22121 \u03b61 \u00b7 \u00b7 \u00b7 \u03b6s\u22121 ) . Since vs is orthogonal to all vectors in the column span of Vs\u22121 according to Line 5 of LazyCCA, we automatically have \u03be>s \u03bei = 0 and \u03b6 > s \u03b6i for all i \u2208 [s \u2212 1]. We also have \u2016\u03bes\u20162 + \u2016\u03b6s\u20162 = 1/2 + 1/2 = 1 so the new vector (\u03bes, \u03b6s) has Euclidean norm 1. As for the first property in (F.2), we observe that the new vector (\u03bes, \u03b6s) enjoys an (absolute) Rayleigh quotient value that is no worse than the original vs = (\u03be \u2032 s, \u03b6 \u2032 s). This is so because (without\nloss of generality we consider v>s Ms\u22121vs > 0):\n( \u03bes \u03b6s )> Ms\u22121 ( \u03bes \u03b6s ) \u00ac = ( \u03bes \u03b6s )> M ( \u03bes \u03b6s )  = 2\u03be>s ( S\u22121/2xx SxyS \u22121/2 yy ) \u03b6s\n\u00ae =\n1\n\u2016\u03be\u2032s\u2016\u2016\u03b6 \u2032s\u2016 \u00b7\u03be\u2032>s\n( S\u22121/2xx SxyS \u22121/2 yy ) \u03b6 \u2032s \u00af =\n1\n2\u2016\u03be\u2032s\u2016\u2016\u03b6 \u2032s\u2016 \u00b7v>s Mvs \u00b0 =\n1\n2\u2016\u03be\u2032s\u2016\u2016\u03b6 \u2032s\u2016 \u00b7v>s Ms\u22121vs \u00b1 \u2265 v>s Ms\u22121vs .\n(F.3)\nAbove, \u00ac is because (\u03bes, \u03b6s) is orthogonal to Vs\u22121;  is by the definition of M = B\u22121/2AB\u22121/2 as well as the definition of A and B; \u00ae is by the definitions of \u03bes and \u03b6s; \u00af is by vs = (\u03be \u2032 s, \u03b6 \u2032 s) and again by the definition of M ; \u00b0 follows from the fact that vs is orthogonal to Vs\u22121; and \u00b1 follows from AM-GM together with the fact that \u2016\u03be\u2032s\u20162 + \u2016\u03b6 \u2032s\u20162 = \u2016vs\u20162 = 1. This finishes proving the first property in (F.2) because the original vector vs satisfies |v>s Ms\u22121vs| \u2265 (1\u2212 \u03b4\u00d7)\u2016Ms\u22121\u20162 according to (F.1).\nWe make an additional observation here: \u2016\u03be\u2032s\u20162 and \u2016\u03b6 \u2032s\u20162 must be in the range [0.06, 0.94] before scaling. Indeed, suppose for instance \u2016\u03be\u2032s\u20162 = c for some c \u2208 [0, 1]. Then, it satisfies 2\u2016\u03be\u2032s\u2016\u2016\u03b6 \u2032s\u2016 = 2 \u221a c(1\u2212 c) and therefore (F.3) becomes\n( \u03bes \u03b6s )> Ms\u22121 ( \u03bes \u03b6s ) \u2265 1\u2212\u03b4\u00d7 2 \u221a c(1\u2212c)\n\u2016Ms\u22121\u20162, meaning that 1\u2212\u03b4\u00d7\n2 \u221a c(1\u2212c)\n\u2264 1. If \u03b4\u00d7 \u2264 1/2, this implies c\u2212 1/2 \u2208 [\u2212 \u221a 3/4, \u221a\n3, 4] and thus c \u2208 [0.06, 0.94]. As for the second property in (F.2), for every matrix Ws\u22121 that is in the proof of Theorem 4.1.a, its columns are all eigenvectors of Ms\u22121 whose absolute eigenvalues are below some threshold, so must consist of only symmetric vectors in this CCA setting: that is, Ws\u22121 = ( \u00b1a1 . . . \u00b1at\nb1 . . . bt\n) .6\nAccording to (F.1) we already know \u2016v>s Ws\u22121\u20162 \u2264 1.5\u03b5pca, which implies\n\u2016v>s Ws\u22121\u20162 = t\u2211\ni=1\n(\u03be\u2032>s ai+\u03b6 \u2032> s bi) 2+(\u03be\u2032>s ai\u2212\u03b6 \u2032>s bi)2 = 2\u2016\u03be\u2032>s (a1, . . . , at)\u20162+2\u2016\u03b6 \u2032>s (b1, . . . , bt)\u20162 \u2264 1.5\u03b5pca .\nNow we can compute\n\u2225\u2225\u2225 ( \u03bes \u03b6s )> Ws\u22121 \u2225\u2225\u2225 2 2 = 2\u2016\u03be>s (a1, . . . , at)\u20162 + 2\u2016\u03b6>s (b1, . . . , bt)\u20162\n\u2264 0.5 0.06\n\u00b7 ( 2\u2016\u03be\u2032>s (a1, . . . , at)\u20162 + 2\u2016\u03b6 \u2032>s (b1, . . . , bt)\u20162 ) \u2264 12.5\u03b5pca ,\nwhere the first inequality is because \u2016\u03be\u2032s\u20162, \u2016\u03b6 \u2032s\u20162 \u2265 0.06. This finishes proving the two properties in (F.2), so Theorem F.1 holds after plugging the rest of the proof of Theorem 4.1 in but changing constants slightly."}, {"heading": "F.2 Fast Implementation of CCA", "text": "Theorem F.2 (running time of LazyCCA). Let X \u2208 Rn\u00d7dx , Y \u2208 Rn\u00d7dy be two matrices, and define A and B according to Lemma 2.3. Suppose M = B\u22121/2AB\u22121/2, and RanInit(d) is the random vector generator defined in Proposition 3.3, and we want to compute V\u2190 B\u22121/2LazyCCA(A,M, k, \u03b4\u00d7, \u03b5pca, p). Then, this procedure can be implemented to run in time\n6This is because matrix Ms is always of the form D \u22121/2CD1/2 where D = diag{D1, D2} is block diagonal and positive definite, while C = [[0, C1]; [C > 1 , 0]] has only zero on its two block diagonal locations. The same proof of Lemma 2.3 shows that the eigenvectors of D\u22121/2CD1/2 must be symmetric. In fact, to be precise, Ws may also contain some eigenvectors corresponding to zero eigenvalues. However, adding them will make our notations heavier, so we refrain from doing that in this sketched proof.\n\u2022 O\u0303 ( knnz(B)+k2d+k\u03a5\u221a\n\u03b4\u00d7\n) where \u03a5 is the time needed to compute B\u22121Aw for a vector w to an\naccuracy \u03b5 where log(1/\u03b5) = O\u0303(1), or\n\u2022 O\u0303 ( knnz(X,Y ) \u221a \u03ba+k2d\u221a\n\u03b4\u00d7\n) if we simply use Conjugate gradient to compute B\u22121Aw.\n\u2022 O\u0303 ( knnz(X,Y )\n( 1+ \u221a \u03ba\u2032/n )\n+k2d\u221a \u03b4\u00d7\n) if we simply use Katyusha to compute B\u22121Aw.\nAbove, \u03ba = \u03bbmax(B)\u03bbmin(B) = max{\u03bbmax(Sxx),\u03bbmax(Syy)} min{\u03bbmin(Sxx),\u03bbmin(Syy)} , and \u03ba \u2032 = Tr(B)\u03bbmin(B) \u2208 [\u03ba, d\u03ba].\nProof. The proof of the first item is almost identical to the proof of Theorem 4.2 so ignored here. As for the second item, it follows from the first item together with the fact that applying matrix B\u22121 to a vector costs running time O\u0303(nnz(B) \u00b7\u221a\u03ba) using conjugate gradient. As for the third item, it follows from the first item together with the running time of Katyusha in Lemma 2.4."}, {"heading": "F.3 Proofs of Corollaries 6.2 and 6.3", "text": "The two corollaries follow from Theorem F.1 for the similar reason as Corollary 4.3 and Corollary 4.4 following from Theorem 4.1.\nSketch Proof Corollary 6.2. The approximation guarantees \u2016V>\u03c6 SxxW\u03c6\u20162 \u2264 \u03b5 and \u2016V>\u03c8SyyW\u03c8\u20162 \u2264 \u03b5 follow from Theorem F.1.a, and the running time follows from Theorem F.2. Sketch Proof of Corollary 6.3. The approximation guarantees max\u03c6\u2208Rdx ,\u03c8\u2208Rdy { \u03c6>Sxy\u03c8\n\u2223\u2223\u2223 \u03c6>SxxV\u03c6 = 0 \u2227 \u03c8>SyyV\u03c8 = 0 } \u2264 (1 + \u03b5)\u03c3k+1 follow from Theorem F.1.b and the definition of M , and the approximation guarantee (1\u2212 \u03b5)\u03c3i \u2264 |\u03c6\u2032iSxy\u03c8i| \u2264 (1 + \u03b5)\u03c3i follows from Theorem F.1.c and the fact that |\u03bb2i\u22121| = |\u03bb2i| = \u03c3i. The running time follows from Theorem F.2."}], "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Linear coupling: An ultimate unification of gradient and mirror descent", "author": ["Zeyuan Allen-Zhu", "Lorenzo Orecchia"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Zeyuan Allen-Zhu", "Peter Richt\u00e1rik", "Zheng Qu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A survey of preconditioned iterative methods for linear systems of algebraic equations", "author": ["Owe Axelsson"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1985}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["Kamalika Chaudhuri", "Sham M Kakade", "Karen Livescu", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Multi-view learning of word embeddings via cca", "author": ["Paramveer Dhillon", "Dean P Foster", "Lyle H Ungar"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "ArXiv e-prints,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Daniel Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "author": ["Rong Ge", "Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Sham M Kakade", "Dean P Foster"], "venue": "In Learning theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Discriminative features via generalized eigenvectors", "author": ["Nikos Karampatziakis", "Paul Mineiro"], "venue": "In ICML, pages 494\u2013502,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Yichao Lu", "Dean P Foster"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Nonparametric canonical correlation analysis", "author": ["Tomer Michaeli", "Weiran Wang", "Karen Livescu"], "venue": "arXiv preprint,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Yurii Nesterov"], "venue": "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1983}, {"title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "author": ["Ohad Shamir"], "venue": "In icml,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["Jonathan Richard Shewchuk"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Large-scale approximate kernel canonical correlation analysis", "author": ["Weiran Wang", "Karen Livescu"], "venue": "arXiv preprint,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "author": ["Weiran Wang", "Jialei Wang", "Dan Garber", "Nathan Srebro"], "venue": "ArXiv e-prints,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Daniela M Witten", "Robert Tibshirani", "Trevor Hastie"], "venue": "Biostatistics, page kxp008,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 161, "endOffset": 164}, {"referenceID": 12, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 203, "endOffset": 206}, {"referenceID": 14, "context": "breakthrough result of Ma, Lu and Foster [16], they proposed to study algorithms to find top k generalized eigenvectors or top k canonical-correlation vectors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "As three concrete examples: \u2022 Block Krylov method computes top eigenvectors with a running time linearly in 1/\u221agap rather than 1/gap [12], or with a gap-free running time that depends on 1/ \u221a \u03b5 rather than 1/gap or 1/ \u221a gap, where \u03b5 is the approximation error [19].", "startOffset": 260, "endOffset": 264}, {"referenceID": 20, "context": "\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition number of matrix B.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition number of matrix B.", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition number of matrix B.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Indeed, two groups of authors independently attempted to answer such questions [11, 24].", "startOffset": 79, "endOffset": 87}, {"referenceID": 22, "context": "Indeed, two groups of authors independently attempted to answer such questions [11, 24].", "startOffset": 79, "endOffset": 87}, {"referenceID": 10, "context": "\u2022 For the k-GenEV problem, GenELin [11] improved the dependency of \u03ba to \u221a\u03ba.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "In a separate work, SI [24] also obtained the \u221a \u03ba dependency but only for the simpler k = 1 case; at the same time, SI enjoys a \u221a gap dependency but paying an additional factor \u03bb1.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "1-GenEV GenELin [11] \u00d5 ( nnz(B)\u03baB gap + nnz(A) gap ) no no", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "SI [24] \u00d5 ( nnz(B)\u03baB \u221a gap\u00b7\u03bb1 + nnz(A) \u221a gap\u00b7\u03bb1 ) no no", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "1-CCA AppGrad [16] nnz(X,Y ) \u00b7 \u00d5 ( \u03ba gap ) no no CCALin [11] nnz(X,Y ) \u00b7 \u00d5 (\u221a\u03ba gap ) no no", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "1-CCA AppGrad [16] nnz(X,Y ) \u00b7 \u00d5 ( \u03ba gap ) no no CCALin [11] nnz(X,Y ) \u00b7 \u00d5 (\u221a\u03ba gap ) no no", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "ALS [24] nnz(X,Y ) \u00b7 \u00d5 ( \u221a\u03ba gap2 ) no no", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "SI [24] nnz(X,Y ) \u00b7 \u00d5 ( \u221a\u03ba \u221a gap\u00b7\u03c31 ) no no", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "CCALin [11] nnz(X,Y ) \u00b7 \u00d5 ( 1+\u221a\u03ba\u2032/n gap ) no yes", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "ALS [24] nnz(X,Y ) \u00b7 \u00d5 ( 1+\u221a\u03ba\u2032/n gap2 ) no yes", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "SI [24] nnz(X,Y ) \u00b7 \u00d5 ( 1+ \u221a \u03ba\u2032/ \u221a n \u221a gap\u00b7\u03c31 ) no yes", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In GenEV, gap = \u03bb1\u2212\u03bb2 \u03bb1 \u2208 [0, 1], \u03bb1 \u2208 [0, 1], and \u03baB = \u03bbmax(B) \u03bbmin(B) > 1.", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "In GenEV, gap = \u03bb1\u2212\u03bb2 \u03bb1 \u2208 [0, 1], \u03bb1 \u2208 [0, 1], and \u03baB = \u03bbmax(B) \u03bbmin(B) > 1.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "In CCA, gap = \u03c31\u2212\u03c32 \u03c31 \u2208 [0, 1], \u03c31 \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].", "startOffset": 25, "endOffset": 31}, {"referenceID": 0, "context": "In CCA, gap = \u03c31\u2212\u03c32 \u03c31 \u2208 [0, 1], \u03c31 \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].", "startOffset": 38, "endOffset": 44}, {"referenceID": 10, "context": "k-GenEV GenELin [11] \u00d5 (knnz(B)\u221a\u03baB gap + knnz(A)+kd gap ) no no", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "k-CCA AppGrad [16] \u00d5 (knnz(X,Y )\u00b7\u03ba+kd gap ) (local converge) no no CCALin [11] \u00d5 (knnz(X,Y )\u00b7\u221a\u03ba+k2d gap ) no no", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "k-CCA AppGrad [16] \u00d5 (knnz(X,Y )\u00b7\u03ba+kd gap ) (local converge) no no CCALin [11] \u00d5 (knnz(X,Y )\u00b7\u221a\u03ba+k2d gap ) no no", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "CCALin [11] \u00d5 (knnz(X,Y )\u00b7 ( 1+ \u221a \u03ba\u2032/n ) +kd gap ) no yes", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "In GenEV, gap = \u03bbk\u2212\u03bbk+1 \u03bbk \u2208 [0, 1] and \u03baB = \u03bbmax(B) \u03bbmin(B) > 1.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "In CCA, gap = \u03c3k\u2212\u03c3k+1 \u03c3k \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].", "startOffset": 27, "endOffset": 33}, {"referenceID": 22, "context": "In contrast, previous results for the k > 1 case rely on more sophisticated nonconvex optimization; and the previous work of [24] \u2014although uses convex optimization to solve 1-CCA\u2014 requires one to work with a sum-of-non-convex function which is less efficient to minimize.", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al.", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al.", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 15, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 16, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 21, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 23, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 15, "context": "Furthermore, for k > 1, the AppGrad result of [17] only provides local convergence guarantees and thus requires a warm-start whose computational complexity is not discussed in their paper.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": ", \u03c3r satisfy \u03c3i = \u03c6 > i Sxy\u03c8i \u2208 [0, 1].", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "For every w \u2208 Rd, Katyusha method [1] finds a vector w\u2032 \u2208 Rd satisfying \u2016w\u2032 \u2212B\u22121Aw\u2016 \u2264 \u03b5", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "3 Leading Eigenvector via Two-Sided Shift-and-Invert In this section we define AppxPCA\u00b1, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10].", "startOffset": 255, "endOffset": 262}, {"referenceID": 9, "context": "3 Leading Eigenvector via Two-Sided Shift-and-Invert In this section we define AppxPCA\u00b1, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10].", "startOffset": 255, "endOffset": 262}, {"referenceID": 8, "context": "Our pseudo-code Algorithm 1 is a modification of Algorithm 5 appeared in [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "The main differences between AppxPCA\u00b1 and Algorithm 5 of [9] are two-fold.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Second, we provide a multiplicative-error guarantee rather than additive as originally appeared in [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "1 Of course, we believe the bulk of the credit for conceiving AppxPCA\u00b1 belongs to the original authors of [9, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 9, "context": "1 Of course, we believe the bulk of the credit for conceiving AppxPCA\u00b1 belongs to the original authors of [9, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 22, "context": "This is why the only known CCA result using shift-and-invert preconditioning [24] depends on 1 gap\u00b7\u03bb1 in Table 1.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "Let gap = |\u03bbk|\u2212|\u03bbk+1| |\u03bbk| \u2208 [0, 1] be the relative gap.", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "Starting with M0 = M , in the s-th iteration where s \u2208 [k], LazySVD computes approximately the leading eigenvector of matrix Ms\u22121 and call it vs using shift-and-invert [9].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 20, "context": "As for the second item, we simply notice that whenever we want to compute w\u2032 \u2190 B\u22121Aw, we can first compute Aw in time O(nnz(A)), and then use Conjugate gradient [22] to compute B\u22121 applied to this vector.", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "Let gap = \u03c3k\u2212\u03c3k+1 \u03c3k \u2208 [0, 1] be the relative gap, and define A = [[0, Sxy]; [S > xy, 0]] and B = diag(Sxx, Syy) following Definition 2.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "For such reason, one can apply the convergence theorem of Katyusha [1] to find an additive \u03b5\u0303 approximate minimizer of f(x) in time O ( nnz(X,Y ) \u00b7 ( 1 + \u221a \u03ba\u2032/n ) log f(x 0)\u2212f(x\u2217) \u03b5\u0303 ) where x0 is an arbitrary starting vector fed into Katyusha and x\u2217 is the exact minimizer.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].", "startOffset": 66, "endOffset": 73}, {"referenceID": 10, "context": "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].", "startOffset": 66, "endOffset": 73}, {"referenceID": 1, "context": "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "1 of [9]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.", "startOffset": 69, "endOffset": 76}, {"referenceID": 9, "context": "Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.", "startOffset": 69, "endOffset": 76}, {"referenceID": 2, "context": "w> a va \u2212 \u03b5\u03031 \u2208 [7 8 \u03bbmax(C (s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ] \u2286 [3 4 \u03bbmax(C (s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and", "startOffset": 96, "endOffset": 106}, {"referenceID": 3, "context": "w> a va \u2212 \u03b5\u03031 \u2208 [7 8 \u03bbmax(C (s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ] \u2286 [3 4 \u03bbmax(C (s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and", "startOffset": 96, "endOffset": 106}, {"referenceID": 0, "context": "w> a va \u2212 \u03b5\u03031 \u2208 [7 8 \u03bbmax(C (s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ] \u2286 [3 4 \u03bbmax(C (s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and", "startOffset": 96, "endOffset": 106}, {"referenceID": 2, "context": "w> b vb \u2212 \u03b5\u03031 \u2286 [3 4 \u03bbmax(D (s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) .", "startOffset": 53, "endOffset": 63}, {"referenceID": 3, "context": "w> b vb \u2212 \u03b5\u03031 \u2286 [3 4 \u03bbmax(D (s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) .", "startOffset": 53, "endOffset": 63}, {"referenceID": 0, "context": "w> b vb \u2212 \u03b5\u03031 \u2286 [3 4 \u03bbmax(D (s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) .", "startOffset": 53, "endOffset": 63}, {"referenceID": 1, "context": "Many of these lemmas are analogous to those ones used in the SVD algorithm by the same authors of this paper [2], however, we need some extra care in this paper because the underlying matrix M is no longer PSD.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "In particular, Chebyshev method [6] uses the so-called Chebyshev polynomial for this purpose, and the number of matrix-vector multiplications is determined by the degree of that polynomial.", "startOffset": 32, "endOffset": 35}, {"referenceID": 18, "context": "Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3].", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3].", "startOffset": 241, "endOffset": 244}, {"referenceID": 22, "context": "Indeed, for instance in the ALS algorithm of [24] for solving CCA, the authors obtained a running time proportional to 1/gap although there are only 1/gap iterations.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "2 can be proved using the linear-coupling framework of [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "Indeed, suppose for instance \u2016\u03be\u2032 s\u2016 = c for some c \u2208 [0, 1].", "startOffset": 53, "endOffset": 59}], "year": 2017, "abstractText": "We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonical-correlation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on k. Furthermore, our algorithms are doubly-accelerated : our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both k-GenEV or k-CCA. We also provide the first gap-free results, which provide running times that depend on 1/ \u221a \u03b5 rather than the eigengap.", "creator": "LaTeX with hyperref package"}}}