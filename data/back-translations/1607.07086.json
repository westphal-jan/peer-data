{"id": "1607.07086", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2016", "title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to the formation of neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log probability training methods are limited by the discrepancy between their training and testing modes, as models need to generate symbols based on their previous assumptions and not the ground truth symbols. We address this problem by introducing a\\ textit {critic} network trained to predict the value of an output token in the light of the policy of a\\ textit {actor} network, which leads to a training process that is much closer to the testing phase, and allows us to directly optimize for a task-specific score such as BLEU. Since we use these techniques in the supervised learning environment and not in the traditional RL setting, we condition the critic network based on truth output. We show that our method leads to improved performance in both English and German translation tasks.", "histories": [["v1", "Sun, 24 Jul 2016 20:05:07 GMT  (43kb,D)", "http://arxiv.org/abs/1607.07086v1", null], ["v2", "Tue, 26 Jul 2016 16:08:30 GMT  (43kb,D)", "http://arxiv.org/abs/1607.07086v2", null], ["v3", "Fri, 3 Mar 2017 15:43:52 GMT  (73kb,D)", "http://arxiv.org/abs/1607.07086v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dzmitry bahdanau", "philemon brakel", "kelvin xu", "anirudh goyal", "ryan lowe", "joelle pineau", "aaron courville", "yoshua bengio"], "accepted": true, "id": "1607.07086"}, "pdf": {"name": "1607.07086.pdf", "metadata": {"source": "CRF", "title": "An Actor-Critic Algorithm for Sequence Prediction", "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In many important applications of machine learning, the task is to develop a system that produces a sequence of discrete tokens given an input. Recent work has shown that recurrent neural networks (RNNs) can deliver excellent performance in many such tasks when trained to predict the next output token given the input and previous tokens. This approach has been applied successfully in machine\ntranslation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al., 2015; Chan et al., 2015).\nThe standard way to train RNNs to generate sequences is to maximize the log-likelihood of the \u201ccorrect\u201d token given a history of the previous \u201ccorrect\u201d ones, an approach often called teacher forcing. At evaluation time, the output sequence is often produced by an approximate search for the most likely candidate according to the learned distribution. During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015). This can become especially prominent as the sequence length becomes large. Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015). In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth outputs as in teacher forcing. This gives rise to the challenging problem of determining the target for the next output of the network. Bengio et al. (2015) use the token k from the ground-truth answer as the target for the network at step k, whereas Ranzato et al. (2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003). ar X iv :1 60 7. 07 08 6v 1 [ cs\n.L G\n] 2\n4 Ju\nl 2 01\nIn this work, we propose and study an alternative procedure for training sequence prediction networks that aims to directly improve their test time metrics. In particular, we train an additional network called the critic to output the value of each token, which we define as the expected task-specific score that the network will receive if it outputs the token and continues to sample outputs according to its probability distribution. Furthermore, we show how the predicted values can be used to train the main sequence prediction network, which we refer to as the actor. The theoretical foundation of our method is that, under the assumption that the critic computes exact values, the expression that we use to train the actor is an unbiased estimate of the gradient of the expected task-specific score.\nOur approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983). RL studies the problem of acting efficiently based only on weak supervision in the form of a reward given for some of the agent\u2019s actions. In our case, the reward is analogous to the task-specific score associated with a prediction. However, the tasks we consider are those of supervised learning, and we make use of this crucial difference by allowing the critic to use the groundtruth answer as an input. To train the critic, we adapt the temporal difference methods from the RL literature (Sutton, 1988) to our setup. While RL methods with non-linear function approximators are not new (Tesauro, 1994; Miller et al., 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al., 2015). We show that some of the techniques recently developed in deep RL, such as having a target network, may also be beneficial for sequence prediction.\nThe contributions of the paper can be summarized as follows: 1) we describe how RL methodology like the actor-critic approach can be applied to supervised learning problems with structured outputs; and 2) we investigate the performance and behavior of the new method on both a synthetic task and a real-world task of machine translation, demonstrating the improvements over maximum-likelihood and REINFORCE brought by the actor-critic training."}, {"heading": "2 Background", "text": "We consider the problem of learning to produce an output sequence Y = (y1, . . . , yT ), yt \u2208 A given an input X , where A is the alphabet of output tokens. We will often use notation Y\u0302f...l to refer to subsequences of the form (y\u0302f , . . . , y\u0302l). Two sets of inputoutput pairs (X,Y ) are assumed to be available for both training and testing. The trained predictor h is evaluated by computing the average task-specific score R(Y\u0302 , Y ) on the test set, where Y\u0302 = h(X) is the prediction.\nRecurrent neural networks A recurrent neural network (RNN) produces a sequence of state vectors (s1, . . . , sT ) given a sequence of input vectors (e1, . . . , eT ) by starting from an initial s0 state and applying T times the transition function f :\nst = f(st\u22121, et). (1)\nPopular choices for the mapping f are the Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Units (Cho et al., 2014), the latter of which we use for our models.\nIn order to build a probabilistic model for sequence generation with an RNN, one adds a stochastic output layer g (typically a softmax if the output is discrete) that generates outputs yt \u2208 A and can feed these outputs back by replacing them with their embedding e(yt):\nyt \u223c g(st\u22121) (2) st = f(st\u22121, e(yt)). (3)\nThus, the RNN can define a probability distribution p(yt|y1, . . . , yt\u22121) of the next output token yt given the previous tokens (y1, . . . , yt\u22121). Upon adding a special end-of-sequence token \u2205 to the alphabet A, the RNN can define the distribution p(Y ) over all possible sequences as p(Y ) = p(y1)p(y2|y1) . . . p(yT |y1, . . . , yT\u22121)p(\u2205|y1, . . . , yT ).\nRNNs for sequence prediction In order to use RNNs for sequence prediction, they must be further augmented to generate Y conditioned on an input X . The simplest way to achieve this, while retaining the ability to produce output sequences of arbitrary length, is to start with an initial state s0 = s0(X)\n(Sutskever et al., 2014; Cho et al., 2014). Alternatively, one can encode X as a variable-length sequence of vectors (h1, . . . , hL) and condition the RNN on this sequence using an attention mechanism. In our models, the sequence of vectors is produced by either a bidirectional RNN (Schuster and Paliwal, 1997) or a convolutional encoder (Rush et al., 2015).\nWe use a soft attention mechanism (Bahdanau et al., 2015) that uses a weighted sum of a sequence of vectors in which the attention weights determine the relative importance of each vector. More formally, we consider the following equations for RNNs with attention:\nyt \u223c g(st\u22121, ct\u22121) (4) st = f(st\u22121, ct\u22121, e(yt)) (5)\n\u03b1t = \u03b2(st, (h1, . . . , hL)) (6)\nct = L\u2211 j=1 \u03b1t,jhj (7)\nwhere \u03b2 is the attention mechanism that produces the attention weights \u03b1t and ct is the context vector, or \u2018glimpse\u2019, for time step t. The attention weights are computed by an MLP that takes as input the current RNN state and each individual vector to focus on. The weights are typically (as in our work) constrained to be positive and sum to 1 by using the softmax function.\nA conditioned RNN can be trained for sequence prediction by gradient ascent on the log-likelihood log p(Y |X) for the input-output pairs (X,Y ) from the training set. To produce a prediction Y\u0302 for a test X , an approximate beam search for the maximum of p(\u00b7|X) is usually conducted. During this search the probabilities p(\u00b7|y\u03021, . . . , y\u0302t\u22121) are considered, where the previous tokens y\u03021, . . . , y\u0302t\u22121 comprise a candidate beginning of the prediction Y\u0302 . Since these tokens come from a different distribution than the one used during training, the probabilities p(\u00b7|y\u03021, . . . , y\u0302t\u22121) can potentially be wrong. This presents a major issue for maximum likelihood training, together with the concern that the task score R is used only at test time and not during training.\nValue functions In order to present our training algorithm we introduce the concept of a value function. We view the conditioned RNN as a stochastic\nAlgorithm 1 Actor-Critic Training for Sequence Prediction Require: A critic Q\u0302(a; Y\u03021...t, Y ) and an actor\np(a|Y\u03021...t, X) with weights \u03c6 and \u03b8 respectively.\n1: Initialize delayed actor p \u2032 and target critic Q\u0302\u2032\nwith same weights: \u03b8\u2032 = \u03b8, \u03c6\u2032 = \u03c6. 2: while Not Converged do 3: Receive a random example (X,Y ). 4: Generate a sequence of actions Y\u0302 from p \u2032 . 5: Compute targets for the critic\nqt = rt(y\u0302t; Y\u03021...t\u22121, Y ) + \u2211 a\u2208A p \u2032 (a|Y\u03021...t, X)Q\u0302\u2032(a; Y\u03021...t, Y )\n6: Update the critic weights \u03c6 using the gradient\nd\nd\u03c6 ( T\u2211 t=1 ( Q\u0302(y\u0302t; Y\u03021...t\u22121, Y )\u2212 qt )2 + \u03bbC )\n7: Update actor weights \u03b8 using the following gradient estimate\n\u0302dV (X,Y ) d\u03b8 =\nT\u2211 t=1 \u2211 a\u2208A dp(a|Y\u03021...t\u22121, X) d\u03b8 Q\u0302(a; Y\u03021...t\u22121, Y )\n8: Update delayed actor and target critic, with a constant \u03c4 1:\n\u03b8\u2032 = \u03c4\u03b8 + (1\u2212 \u03c4)\u03b8\u2032 \u03c6\u2032 = \u03c4\u03c6+ (1\u2212 \u03c4)\u03c6\u2032\n9: end while\nAlgorithm 2 Complete Actor-Critic Algorithm for Sequence Prediction\n1: Initialize critic Q\u0302(a; Y\u03021...t, Y ) and actor p(a|Y\u03021...t, X) with random weights \u03c6 and \u03b8 respectively. 2: Pre-train the actor to predict yt+1 given Y1...t by maximizing log p(yt+1|Y1...t, X). 3: Pre-train the critic to estimate Q by running Algorithm 1 with fixed actor. 4: Run Algorithm 1.\npolicy that generates actions and receives the task score as a reward. We furthermore consider the case when the reward is partially received at the intermediate steps of a sequence of actions and given\nby R(Y\u0302 , Y ) = T\u2211 t=1 rt(y\u0302t; Y\u03021...t\u22121, Y ). This is more general than the case of receiving a reward at the end of the sequence, as we can simply define all rewards other than rT to be zero. Receiving partial rewards may ease the learning for the critic, and we will present a general method for decomposing the reward for sequence prediction later. Given the policy, possible actions and reward function, the value represents the expected future reward as a function of the current state of the system, which in our case is uniquely defined by the sequence of actions taken (or tokens predicted) so far.\nWe define the value of an unfinished prediction Y\u03021...t as follows:\nV (Y\u03021...t;X,Y ) =\nEY\u0302t+1...T\u223cp(Y\u0302t+1...T |Y\u03021...t,X) T\u2211\n\u03c4=t+1\nr\u03c4 (y\u0302\u03c4 ; Y\u03021...\u03c4\u22121, Y ).\nWe define the value of a candidate next token a for an unfinished prediction Y\u03021...t\u22121 as:\nQ(a; Y\u03021...t\u22121;X,Y ) = EY\u0302t+1...,T\u223cp(Y\u0302t+1...T |Y\u03021...t\u22121a,X)( rt+1(a; Y\u03021...t, Y ) +\nT\u2211 \u03c4=t+1 r\u03c4 (y\u0302\u03c4 ; Y\u03021...t\u22121a, Y )\n) .\nWe will refer to the candidate next tokens as actions. For notational simplicity, we henceforth drop X and Y from the signature of p, V , Q, R and rt, assuming it is clear from the context which of X and Y\nis meant. We will also use V without arguments for the expected reward of a random prediction."}, {"heading": "3 Actor-Critic for Sequence Prediction", "text": "Our training algorithm is based on an approximation of the gradient dVd\u03b8 , where \u03b8 are the parameters of the conditioned RNN, which we refer to now as the actor. In particular, we use the gradient approximation shown in Proposition 1.\nProposition 1 The gradient dVd\u03b8 can be expressed using Q values of intermediate actions:\ndV d\u03b8 = EY\u0302\u223cp(Y\u0302 ) T\u2211 t=1 \u2211 a\u2208A dp(a|Y\u03021...t\u22121) d\u03b8 Q(a; Y\u03021...t\u22121)\nProof: See supplemental material. 1\nWe note that this way of re-writing the gradient of the expected reward is known in RL under the names policy gradient theorem (Sutton et al., 1999) and stochastic actor-critic (Sutton, 1984). Since this gradient expression is an expectation, it is trivial to build an unbiased estimate for it:\nd\u0302V d\u03b8 = M\u2211 k=1 T\u2211 t=1 \u2211 a\u2208A dp(a|Y\u0302 k1...t\u22121) d\u03b8 Q(a; Y\u0302 k1...t\u22121)\n(8)\nwhere Y\u0302 k are M random samples from p(Y\u0302 ). Informally, the gradient estimate above guides the network towards choosing the next token that leads to a higher final score. This is similar in spirit to the REINFORCE algorithm that Ranzato et al. (2015) use in the same context. However, in REINFORCE the inner sum over all actions is replaced by its 1-sample estimate, namely log p(y\u0302t|Y\u03021...t\u22121)Q(y\u0302t; Y\u03021...t\u22121). Furthermore, instead of the valueQ(y\u0302t; Y\u03021...t\u22121), REINFORCE uses\nthe cumulative reward T\u2211 \u03c4=t r\u03c4 (y\u0302\u03c4 ; Y\u03021...\u03c4\u22121) following the action y\u0302t, which again can be seen as a 1-sample estimate of Q. Due to these simplifications, the REINFORCE estimator has very high variance, even when control variates are used.\n1We note, that result itself is not new and was first derived in (Sutton et al., 1999). Still, we provide a simpler self-contained proof here to make the paper more accessible.\nIn order to apply the estimate (8) without these simplifications, we require a method for estimating the value Q(a; Y\u03021...t\u22121). We propose to use another critic RNN with parameters \u03c6 to estimate the Qvalues for a sampled prediction Y\u0302 . The critic RNN is run in parallel with the actor, consumes the tokens y\u0302t that the actor outputs and produces the estimates Q\u0302(a; Y\u03021...t) for all a \u2208 A. The most important difference between the critic and the actor is that the correct answer Y is given to the critic as an input, similarly to how the actor is conditioned on X . Indeed, the reward R(Y\u0302 , Y ) is a deterministic function of Y , and we argue that using Y to compute Q\u0302 should be of great help.\nTraining the critic A crucial component of our approach is the training of the critic to produce useful estimates of Q\u0302. This task is often called policy evaluation in RL, since it involves evaluating the quality of each possible next action. With a na\u0131\u0308ve Monte-Carlo method, one could use\nthe future reward T\u2211 \u03c4=t r\u03c4 (y\u0302\u03c4 ; Y\u03021...\u03c4\u22121) as a target to Q\u0302(y\u0302t; Y\u03021...t\u22121), and use the critic parameters \u03c6 to minimize the square error between these two values. However, like with REINFORCE, using such a target yields to very high variance which quickly grows with the number of steps T .\nIn this work we draw inspiration from temporal difference (TD) methods for policy evaluation (Sutton, 1988). These methods are based on the fact that exact Q values are related by the Bellman equation:\nQ(y\u0302t; Y\u03021...t\u22121) ={ rt(y\u0302t; Y\u03021...t\u22121) + V (Y\u03021...t) for t < T\nrt(y\u0302t; Y\u03021...t\u22121) for t = T (9)\nwhere the right-hand V can be further expanded as V (Y\u03021...t) = \u2211 a\u2208A p(a|Y\u03021...t)Q(a; Y\u03021...t). The key idea of TD methods is to substitute Q with Q\u0302 in both the left-hand and right-hand sides of Equation 9. The resulting right-hand expression qt = rt(y\u0302t; Y\u03021...t\u22121) +\u2211 a\u2208A p(a|Y\u03021...t)Q\u0302(a; Y\u03021...t) is then used as the target for the left-hand Q\u0302(y\u0302t; Y\u03021...t\u22121). We note that in our target expression we use a weighted sum over all actions. It is more typical in RL to only consider the chosen next action y\u0302t+1, which is known under the\nname SARSA, but we can afford the summation in this work because there is no stochastic environment involved.\nApplying deep RL techniques A na\u0131\u0308ve way of applying TD methodology would be to use the gra-\ndient dd\u03c6( T\u2211 t=1 Q\u0302(y\u0302t; Y\u03021...T )\u2212 qt)2 to update the critic parameters \u03c6. However, by doing so we would disregard the influence of these updates on the targets qt. It has been shown in the RL literature that if Q\u0302 is non-linear (and it obviously is in our case), the TD policy evaluation might diverge (Tsitsiklis and Van Roy, 1997).\nA practical workaround to this problem used in deep RL is to use an additional target network Q\u0302\u2032 to compute qt, which is updated less often and/or more slowly than Q\u0302. Similarly to (Lillicrap et al., 2015), we update the parameters \u03c6\u2032 of the target critic by linearly interpolating them with the parameters of the trained one: \u03c6\u2032 \u2190 (1\u2212 \u03c4)\u03c6\u2032+ \u03c4\u03c6, where \u03c4 1 is a hyperparameter.\nWe found using a target network necessary for the approach to work. Attempts to remove the target network by propagating the gradient through qt resulted in a lower square error (Q\u0302(y\u0302t; Y\u03021...T ) \u2212 qt)2, but the resulting Q\u0302 values proved very unreliable as training signals for the actor.\nAnother issue that we had to address is the one of training stability. The fact that both actor and critic use outputs of each other for training creates a potentially dangerous feedback loop. To compensate for this, we sample predictions from a delayed actor, whose weights are slowly updated to follow the actor that is actually trained. This is inspired by (Lillicrap et al., 2015), where a delayer actor is used for a similar purpose.\nDealing with large action spaces When the space of possible actions is very large (as is typically the case in NLP tasks with a large vocabulary), it may be necessary to put constraints on the critic values for actions that are rarely sampled. We found experimentally that artificially reducing the values of these rare actions is absolutely necessary for the algorithm to work. Specifically, we add a term C to the optimization objective which acts to drive all value pre-\ndictions of the critic closer to their mean: C = \u2211 a ( Q\u0302(a; Y\u03021...t\u22121)\u2212 1 |A| \u2211 b Q\u0302(b; Y\u03021...t\u22121) )2 , (10) In other words, we propose to add a penalty to the variance of the outputs of the critic. We found that this trick worked very well and suspect that it may be useful in many other situations where values need to be estimated for very large action spaces. A similar trick was also recently used in the context of learning simple algorithms with Q learning (Zaremba et al., 2015).\nReward decomposition While we are ultimately interested in the maximization of the score of a complete prediction, simply awarding all of the score at the last step provides a very sparse training signal for the critic. For this reason, we decompose the score as follows. For a predicted sequence Y\u0302 we compute score values for all prefixes to obtain the sequence of scores (R(Y\u03021...1), R(Y\u03021...2), . . . , R(Y\u03021...T )). The differences between the consecutive pairs of scores are then used as rewards at each step: rt(y\u0302t; Y\u03021...t\u22121) = R(Y\u03021...t)\u2212R(Y\u03021...t\u22121).\nPutting it all together Algorithm 1 describes the complete details of joint actor-critic training. However, starting it with a randomly initialized actor and critic would be rather hopeless, because neither the actor nor the critic would provide adequate training signals for one another. In particular, the actor would sample completely random predictions that receive very little reward, thus providing a very weak training signal for the critic, and a random critic would be similarly useless for training the actor. Motivated by these considerations, we pre-train the actor using standard log-likelihood training. Furthermore, we pre-train the critic by feeding it samples from the pre-trained actor, while the actor\u2019s parameters are frozen. The complete training procedure including pre-training is described by Algorithm 2."}, {"heading": "4 Related Work", "text": "In other recent RL inspired work on sequence prediction, Ranzato et al. (2015) trained a transla-\ntion model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm. However, this policy gradient estimator is known to have very high variance and does not exploit the availability of the ground-truth in the way the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009). Again, these systems do not use the groundtruth for value prediction.\nImitation learning has also been applied to structured prediction (Vlachos, 2012). Successful methods of this type include the SEARN (Daume\u0301 Iii et al., 2009) and DAGGER (Ross et al., 2010) algorithms. These methods rely on an expert policy to provide action sequences that the trained policy learns to imitate. Unfortunately, it\u2019s not always straightforward or even possible to construct an expert policy for a task-specific score. In our approach, the critic serves a role that is similar to the expert policy, but the critic is learned without the need for prior knowledge about the task-specific score.\nThere are a couple of other approaches that aim to approximate the gradient of the expected score. One such approach is \u2018Direct Loss Minimization\u2019 (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-specific score into account. Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel and Byrne, 2000; Shen et al., 2015; Och, 2003). None of these methods provide intermediate targets for the actor during training.\nFinally, a method called \u2018scheduled sampling\u2019 was proposed to address the train-test discrepancy problems of maximum likelihood training for sequence generation (Bengio et al., 2015). In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation of this method is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy."}, {"heading": "5 Experiments", "text": "To validate our approach, we performed two experiments. First, we trained the proposed model to recover strings of natural text from their corrupted versions. Specifically, we consider each character in a natural language corpus and with some probability replace it with a random character. We call this synthetic task spelling correction, even though the number of misspellings is much higher than for an average human. A desirable property of this synthetic task is that data is essentially infinite and one does not have to take overfitting into account. Our second experiment is done on the task of translation from German to English.\nData We use data from the One Billion Word dataset for the spelling correction task (Chelba et al., 2013), which has pre-defined training and testing sets. The training data was abundant, and we never used any example twice. We evaluate trained models on a section of the test data that comprises 6075 sentences. To speed up experiments, we clipped all sentences to the first 10 or 30 characters.\nFor the machine translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work. The training data comprises about 153000 German-English sentence pairs, and the sizes of validation and tests set were 6969 and 6750, respectively. We limited the number of words in the English and German vocabularies to the 22822 and 32009 most frequent words, respectively, and replaced all other words with a special token. The maximum sentence length in our dataset was 50.\nModel For the spelling correction actor network, we used an RNN with 100 Gated Recurrent Units (GRU), and used a bidirectional GRU network for the encoder. We use the same attention mechanism as proposed in (Bahdanau et al., 2015), which effectively makes our actor network a smaller version of the model used in that work. For machine translation, we use a different convolutional encoder to make our results more comparable with Ranzato et al. (2015). For the same reason, we use 256 hidden units in all machine translation networks.\nFor the spelling correction critic network, we used a model with the same architecture as the actor. For the machine translation critic, we replaced the convolutional network with a more powerful bidirectional GRU network. Note that this does not affect comparability of our results with the baseline, because using a critic is merely an optimization technique which does not change the model capacity.\nTraining and Evaluation We use character error rate (CER) to measure performance of the actor on the spelling task, which we define as the ratio between the total of Levenshtein distances between predictions and ground-truth outputs and the total length of the ground-truth outputs. This is a corpuslevel metric with \u201cthe lower the better\u201d semantics. We adapt it to be used as a reward by using negated per-sentence ratios. For machine translation, the role of reward is played by a smoothed version of the BLEU score.\nWe use the ADAM optimizer (Kingma and Ba, 2015) to train all the networks with the parameters recommended in the original paper, with the exception of the scale parameter \u03b1. The latter is first set to 10\u22123 and then annealed to 10\u22124 for log-likelihood training. For the pre-training stages of the actorcritic, we use \u03b1 = 10\u22123 and decrease it to 10\u22124 for the joint actor-critic training. We pretrain the actor until its score on the development set stops improving and the critic until its TD error stabilizes2. The variance penalty coefficient \u03bb and the delay coefficient \u03c4 were both set to 10\u22123. All initial weights were sampled from a centered uniform distribution with width 0.1.\nIn addition to the actor-critic training we also implemented REINFORCE with a baseline as described in (Ranzato et al., 2015). We start REINFORCE training from a pretrained actor, but apart from that we do not use curriculum learning employed in MIXER. We report results obtained with the reward decomposition described in Section 3, as we found that it slightly improves REINFORCE performance.\nFor decoding we used greedy search and beam search (for machine translation experiments only).\n2A typical behaviour for TD error was to grow at first and then start decreasing slowly. We found that stopping pretraining shortly after TD error stops growing leads to good results.\nWe used beam size 10 and we found that penalizing too short candidate sentences is required to obtain the best results. Similarly to (Hannun et al., 2014), we subtracted \u03b2T from the negative log-likelihood of each candidate sentence, where T is the candidate\u2019s length, and \u03b2 is a hyperparameter tuned on the validation set. We used this trick only for beam search.\nResults Table 1 presents our results on the spelling correction task. We observe an improvement in CER for all four settings considered, which becomes more pronounced as we increase the sentence length and noise level.\nOur machine translation results are summarized in Table 2. We report a significant improvement of 2 BLEU points over the log-likelhood baseline when greedy search is used for decoding. The final performance of the actor-critic is also 0.8 BLEU points better than what Ranzato et al. (2015) report for their MIXER approach. The performance of our REINFORCE implementation matches the number reported for MIXER. When beam search is used, the network trained by actor-critic is still the best, but the advantage becomes smaller.\nTo better understand where the improvement in BLEU score comes from, we provide the learning curves for the baseline and actor-critic experiments in Figure 1. Interestingly, we observe that actorcritic training has a strong regularizarion effect, instead of the optimization effect predicted by the theory and observed on the spelling correction task. We also see that REINFORCE training is dramatically slower, does not really overfit on the considered dataset and mostly helps as a regularizer.\nFinally, in Table 5 we provide an example of value predictions that the critic outputs for candidate next words. One can see that the critic has indeed learnt to assign larger values for the appropriate next words. While the critic does not always produce sensible estimates and can often predict high future reward for irrelevant rare words, this is greatly reduced using the variance penalty term from Equation (10)."}, {"heading": "6 Discussion", "text": "We proposed an actor-critic approach to sequence prediction. Our method takes the task objective into\naccount during training and uses the ground-truth to aid the critic in its prediction of intermediate targets for the actor. We showed that our method leads to significant improvements over maximum likelihood training on both a synthetic task and a machine translation benchmark. Compared to REINFORCE training on machine translation, actor-critic fits the training data much faster and also results in a significantly better final performance. Finally, we showed qualitatively that the critic actually learns to assign high values to words that make sense in the given context.\nOn the synthetic task, we found that the difference in character error rate between maximum likelihood and our method increased when we raised the difficulty of the task. This is in line with our expectations, as maximum likelihood should suffer the most when there is a larger discrepancy between the ground-truth sequences it was trained on and the sequences it is able to generate. Our system also obtained a significantly higher BLEU score than the baseline on the translation task. Contrary to our expectations, the BLEU scores were lower on the train data than those of the baseline model. This raises the question of whether our method performs better by optimizing BLEU more directly, or due to a regularization effect. Our results on the synthetic task do indicate, however, that our method can be beneficial\nwhen overfitting is not possible. While training our models, we ran into several optimization issues. We first discovered that the critic would sometimes assign very high values to actions with a very low probability according to the actor. We were able to resolve this by using a penalty on the critic\u2019s variance. Additionally, the actor would sometimes have trouble to adapt to the demands of the critic. We noticed that the action distribution tends to saturate and become deterministic, in which case the gradient vanishes. While these issues did not prevent us from training our models, we might be able to obtain better results by addressing them.\nWhile we only consider sequence prediction in this paper, it should be possible to adapt our methods to other types of structured prediction in which the prediction procedure can be cast as a sequence of discrete decisions, such for example building parse trees. Future work should extend our methodology to other domains and to investigate the optimization issues discussed above."}, {"heading": "Acknowledgments", "text": "We thank the developers of Theano (Theano Development Team, 2016) and Blocks (van Merrie\u0308nboer et al., 2015) for their great work. We thank NSERC, Compute Canada, Calcul Quebe\u0301c, Canada Research Chairs, CIFAR and Samsung Institute of Advanced Techonology for their support."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["Barto et al.1983] Andrew G Barto", "Richard S Sutton", "Charles W Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Report on the 11th iwslt evaluation campaign", "author": ["Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico"], "venue": "In Proc. of IWSLT", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition. CoRR, abs/1506.07503", "author": ["Dzmitry Bahdanau", "Dmitriy Serdyuk", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine learning,", "citeRegEx": "Iii et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Iii et al\\.", "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Minimum bayes-risk automatic speech recognition", "author": ["Goel", "Byrne2000] Vaibhava Goel", "William J Byrne"], "venue": "Computer Speech & Language,", "citeRegEx": "Goel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2000}, {"title": "Firstpass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "author": ["Hannun et al.2014] Awni Y Hannun", "Andrew L Maas", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "arXiv preprint arXiv:1408.2873", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Direct loss minimization for structured prediction", "author": ["Hazan et al.2010] Tamir Hazan", "Joseph Keshet", "David A McAllester"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Structured prediction with reinforcement learning", "author": ["Maes et al.2009] Francis Maes", "Ludovic Denoyer", "Patrick Gallinari"], "venue": "Machine learning,", "citeRegEx": "Maes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maes et al\\.", "year": 2009}, {"title": "Neural networks for control", "author": ["Paul J Werbos", "Richard S Sutton"], "venue": null, "citeRegEx": "Miller et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1995}, {"title": "Human-level control through deep reinforcement", "author": ["Mnih et al.2015] Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2010] St\u00e9phane Ross", "Geoffrey J Gordon", "J Andrew Bagnell"], "venue": "arXiv preprint arXiv:1011.0686", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2015] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to reinforcement learning, volume 135", "author": ["Sutton", "Barto1998] Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Temporal credit assignment in reinforcement learning", "author": ["Richard Stuart Sutton"], "venue": null, "citeRegEx": "Sutton.,? \\Q1984\\E", "shortCiteRegEx": "Sutton.", "year": 1984}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Td-gammon, a self-teaching backgammon program, achieves masterlevel play", "author": ["Gerald Tesauro"], "venue": "Neural computation,", "citeRegEx": "Tesauro.,? \\Q1994\\E", "shortCiteRegEx": "Tesauro.", "year": 1994}, {"title": "An analysis of temporaldifference learning with function approximation", "author": ["Tsitsiklis", "Van Roy1997] John N Tsitsiklis", "Benjamin Van Roy"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "An investigation of imitation learning algorithms for structured prediction", "author": ["Andreas Vlachos"], "venue": "In EWRL,", "citeRegEx": "Vlachos.,? \\Q2012\\E", "shortCiteRegEx": "Vlachos.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples. arXiv preprint arXiv:1511.07275", "author": ["Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "This approach has been applied successfully in machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Kiros et al.", "startOffset": 67, "endOffset": 114}, {"referenceID": 0, "context": "This approach has been applied successfully in machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Kiros et al.", "startOffset": 67, "endOffset": 114}, {"referenceID": 9, "context": ", 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al.", "startOffset": 28, "endOffset": 137}, {"referenceID": 36, "context": ", 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al.", "startOffset": 28, "endOffset": 137}, {"referenceID": 39, "context": ", 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al.", "startOffset": 28, "endOffset": 137}, {"referenceID": 7, "context": ", 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 60, "endOffset": 103}, {"referenceID": 4, "context": ", 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 60, "endOffset": 103}, {"referenceID": 2, "context": "During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015).", "startOffset": 129, "endOffset": 150}, {"referenceID": 2, "context": "Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 23, "context": "Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 38, "context": "(2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al.", "startOffset": 39, "endOffset": 55}, {"referenceID": 22, "context": "(2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003).", "startOffset": 167, "endOffset": 190}, {"referenceID": 2, "context": "During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015). This can become especially prominent as the sequence length becomes large. Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015). In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth outputs as in teacher forcing. This gives rise to the challenging problem of determining the target for the next output of the network. Bengio et al. (2015) use the token k from the ground-truth answer as the target for the network at step k, whereas Ranzato et al.", "startOffset": 130, "endOffset": 703}, {"referenceID": 2, "context": "During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015). This can become especially prominent as the sequence length becomes large. Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015). In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth outputs as in teacher forcing. This gives rise to the challenging problem of determining the target for the next output of the network. Bengio et al. (2015) use the token k from the ground-truth answer as the target for the network at step k, whereas Ranzato et al. (2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al.", "startOffset": 130, "endOffset": 819}, {"referenceID": 31, "context": "Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983).", "startOffset": 176, "endOffset": 231}, {"referenceID": 30, "context": "Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983).", "startOffset": 176, "endOffset": 231}, {"referenceID": 1, "context": "Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983).", "startOffset": 176, "endOffset": 231}, {"referenceID": 32, "context": "To train the critic, we adapt the temporal difference methods from the RL literature (Sutton, 1988) to our setup.", "startOffset": 85, "endOffset": 99}, {"referenceID": 33, "context": "While RL methods with non-linear function approximators are not new (Tesauro, 1994; Miller et al., 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al.", "startOffset": 68, "endOffset": 104}, {"referenceID": 19, "context": "While RL methods with non-linear function approximators are not new (Tesauro, 1994; Miller et al., 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al.", "startOffset": 68, "endOffset": 104}, {"referenceID": 20, "context": ", 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al., 2015).", "startOffset": 88, "endOffset": 107}, {"referenceID": 6, "context": "Popular choices for the mapping f are the Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Units (Cho et al., 2014), the latter of which we use for our models.", "startOffset": 130, "endOffset": 148}, {"referenceID": 28, "context": "(Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 0, "endOffset": 42}, {"referenceID": 6, "context": "(Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 0, "endOffset": 42}, {"referenceID": 25, "context": "In our models, the sequence of vectors is produced by either a bidirectional RNN (Schuster and Paliwal, 1997) or a convolutional encoder (Rush et al., 2015).", "startOffset": 137, "endOffset": 156}, {"referenceID": 0, "context": "We use a soft attention mechanism (Bahdanau et al., 2015) that uses a weighted sum of a sequence of vectors in which the attention weights determine the relative importance of each vector.", "startOffset": 34, "endOffset": 57}, {"referenceID": 30, "context": "We note that this way of re-writing the gradient of the expected reward is known in RL under the names policy gradient theorem (Sutton et al., 1999) and stochastic actor-critic (Sutton, 1984).", "startOffset": 127, "endOffset": 148}, {"referenceID": 31, "context": ", 1999) and stochastic actor-critic (Sutton, 1984).", "startOffset": 36, "endOffset": 50}, {"referenceID": 23, "context": "This is similar in spirit to the REINFORCE algorithm that Ranzato et al. (2015) use in the same context.", "startOffset": 58, "endOffset": 80}, {"referenceID": 30, "context": "We note, that result itself is not new and was first derived in (Sutton et al., 1999).", "startOffset": 64, "endOffset": 85}, {"referenceID": 32, "context": "In this work we draw inspiration from temporal difference (TD) methods for policy evaluation (Sutton, 1988).", "startOffset": 93, "endOffset": 107}, {"referenceID": 16, "context": "Similarly to (Lillicrap et al., 2015), we update the parameters \u03c6\u2032 of the target critic by linearly interpolating them with the parameters of the trained one: \u03c6\u2032 \u2190 (1\u2212 \u03c4)\u03c6\u2032+ \u03c4\u03c6, where \u03c4 1 is a hyperparameter.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "This is inspired by (Lillicrap et al., 2015), where a delayer actor is used for a similar purpose.", "startOffset": 20, "endOffset": 44}, {"referenceID": 40, "context": "A similar trick was also recently used in the context of learning simple algorithms with Q learning (Zaremba et al., 2015).", "startOffset": 100, "endOffset": 122}, {"referenceID": 18, "context": "Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009).", "startOffset": 106, "endOffset": 125}, {"referenceID": 22, "context": "In other recent RL inspired work on sequence prediction, Ranzato et al. (2015) trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm.", "startOffset": 57, "endOffset": 79}, {"referenceID": 37, "context": "Imitation learning has also been applied to structured prediction (Vlachos, 2012).", "startOffset": 66, "endOffset": 81}, {"referenceID": 24, "context": ", 2009) and DAGGER (Ross et al., 2010) algorithms.", "startOffset": 19, "endOffset": 38}, {"referenceID": 12, "context": "One such approach is \u2018Direct Loss Minimization\u2019 (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-specific score into account.", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": "Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel and Byrne, 2000; Shen et al., 2015; Och, 2003).", "startOffset": 170, "endOffset": 222}, {"referenceID": 21, "context": "Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel and Byrne, 2000; Shen et al., 2015; Och, 2003).", "startOffset": 170, "endOffset": 222}, {"referenceID": 2, "context": "Finally, a method called \u2018scheduled sampling\u2019 was proposed to address the train-test discrepancy problems of maximum likelihood training for sequence generation (Bengio et al., 2015).", "startOffset": 161, "endOffset": 182}, {"referenceID": 5, "context": "Data We use data from the One Billion Word dataset for the spelling correction task (Chelba et al., 2013), which has pre-defined training and testing sets.", "startOffset": 84, "endOffset": 105}, {"referenceID": 3, "context": "For the machine translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al.", "startOffset": 140, "endOffset": 162}, {"referenceID": 3, "context": "For the machine translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work.", "startOffset": 141, "endOffset": 197}, {"referenceID": 0, "context": "We use the same attention mechanism as proposed in (Bahdanau et al., 2015), which effectively makes our actor network a smaller version of the model used in that work.", "startOffset": 51, "endOffset": 74}, {"referenceID": 0, "context": "We use the same attention mechanism as proposed in (Bahdanau et al., 2015), which effectively makes our actor network a smaller version of the model used in that work. For machine translation, we use a different convolutional encoder to make our results more comparable with Ranzato et al. (2015). For the same reason, we use 256 hidden units in all machine translation networks.", "startOffset": 52, "endOffset": 297}, {"referenceID": 23, "context": "In addition to the actor-critic training we also implemented REINFORCE with a baseline as described in (Ranzato et al., 2015).", "startOffset": 103, "endOffset": 125}, {"referenceID": 11, "context": "Similarly to (Hannun et al., 2014), we subtracted \u03b2T from the negative log-likelihood of each candidate sentence, where T is the candidate\u2019s length, and \u03b2 is a hyperparameter tuned on the validation set.", "startOffset": 13, "endOffset": 34}, {"referenceID": 23, "context": "8 BLEU points better than what Ranzato et al. (2015) report for their MIXER approach.", "startOffset": 31, "endOffset": 53}], "year": 2016, "abstractText": "We present an approach to training neural networks to generate sequences using actorcritic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a taskspecific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "creator": "LaTeX with hyperref package"}}}