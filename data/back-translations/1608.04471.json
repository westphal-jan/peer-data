{"id": "1608.04471", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm", "abstract": "Our method iteratively transports a series of particles that correspond to the target distribution by applying a form of functional gradient lineage that minimizes KL divergence. Empirical studies are conducted on various real-world models and datasets on which our method competes with existing state-of-the-art methods. Derivation of our method is based on a new theoretical result that combines the derivation of KL divergence under smooth transformations with Stein's identity and a recently proposed nucleated stone discrepancy that is of independent interest.", "histories": [["v1", "Tue, 16 Aug 2016 03:24:20 GMT  (3890kb,D)", "http://arxiv.org/abs/1608.04471v1", "To appear in NIPS 2016"], ["v2", "Fri, 19 Aug 2016 05:13:47 GMT  (3890kb,D)", "http://arxiv.org/abs/1608.04471v2", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qiang liu", "dilin wang"], "accepted": true, "id": "1608.04471"}, "pdf": {"name": "1608.04471.pdf", "metadata": {"source": "CRF", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm", "authors": ["Qiang Liu", "Dilin Wang"], "emails": ["dilin.wang.gr}@dartmouth.edu"], "sections": [{"heading": "1 Introduction", "text": "Bayesian inference provides a powerful tool for modeling complex data and reasoning under uncertainty, but casts a long standing challenge on computing intractable posterior distributions. Markov chain Monte Carlo (MCMC) has been widely used to draw approximate posterior samples, but is often slow and has difficulty accessing the convergence. Variational inference instead frames the Bayesian inference problem into a deterministic optimization that approximates the target distribution with a simpler distribution by minimizing their KL divergence. This makes variational methods efficiently solvable by using off-the-shelf optimization techniques, and easily applicable to large datasets (i.e., \"big data\") using the stochastic gradient descent trick [e.g., 1]. In contrast, it is much more challenging to scale up MCMC to big data settings [see e.g., 2, 3].\nMeanwhile, both the accuracy and computational cost of variational inference critically depend on the set of distributions in which the approximation is defined. Simple approximation sets, such as these used in the traditional mean field methods, are too restrictive to resemble the true posterior distributions, while more advanced choices cast more difficulties on the subsequent optimization tasks. For this reason, efficient variational methods often need to be derived on a model-by-model basis, causing is a major barrier for developing general purpose, user-friendly variational tools applicable for different kinds of models, and accessible to non-ML experts in application domains.\nThis case is in contrast with the maximum a posteriori (MAP) optimization tasks for finding the posterior mode (sometimes known as the poor man\u2019s Bayesian estimator, in contrast with the full Bayesian inference for approximating the full posterior distribution), for which variants of (stochastic) gradient descent serve as a simple, generic, yet extremely powerful toolbox. There has been a recent growth of interest in creating user-friendly variational inference tools [e.g., 4\u20137], but more efforts are still needed to develop more efficient general purpose algorithms.\nIn this work, we propose a new general purpose variational inference algorithm which can be treated as a natural counterpart of gradient descent for full Bayesian inference (see Algorithm 1). Our algorithm uses a set of particles for approximation, on which a form of (functional) gradient descent\nar X\niv :1\n60 8.\n04 47\n1v 1\n[ st\nat .M\nL ]\n1 6\nA ug\nis performed to minimize the KL divergence and drive the particles to fit the true posterior distribution. Our algorithm has a simple form, and can be applied whenever gradient descent can be applied. In fact, it reduces to gradient descent for MAP when using only a single particle, while automatically turns into a full Bayesian approach with more particles.\nUnderlying our algorithm is a new theoretical result that connects the derivative of KL divergence w.r.t. smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS). This new result is of independent interest, and can find wide application in machine learning and statistics beyond variational inference.\nOutline This paper is organized as follows. Section 2 introduces backgrounds on kernelized Stein discrepancy (KSD). Our main results are presented in Section 3 in which we clarify the connection between KSD and KL divergence, and leverage it to develop our novel variational inference method. Section 4 discusses related works, and Section 5 presents numerical results. The paper is concluded in Section 6."}, {"heading": "2 Background", "text": "Preliminary Let x be a continuous random variable or parameter of interest taking values in X \u2282 Rd, and {Dk} is a set of i.i.d. observation. With prior p0(x), Bayesian inference of x involves reasoning with the posterior distribution p(x) := p\u0304(x)/Z with p\u0304(x) := p0(x) \u220fN k=1 p(Dk|x), where\nZ = \u222b p\u0304(x)dx is the troublesome normalization constant. We have dropped the conditioning on data {Dk} in p(x) for convenience. Let k(x, x\u2032) : X \u00d7X \u2192 R be a positive definite kernel. The reproducing kernel Hilbert space (RKHS) H of k(x, x\u2032) is the closure of linear span {f : f(x) = \u2211m i=1 aik(x, xi), ai \u2208 R, m \u2208 N, xi \u2208 X},\nequipped with inner products \u3008f, g\u3009H = \u2211 ij aibjk(xi, xj) for g(x) = \u2211 i bik(x, xi). Denote by Hd the space of vector functions f = [f1, . . . , fd] with fi \u2208 H, equipped with inner product \u3008f , g\u3009Hd = \u2211d i=1\u3008fi, gi\u3009H. We assume all the vectors are column vectors.\nStein\u2019s Identity and Kernelized Stein Discrepancy Stein\u2019s identity plays a fundamental role in our framework. Let p(x) be a continuously differentiable (also called smooth) density supported on X \u2286 Rd, and \u03c6(x) = [\u03c61(x), \u00b7 \u00b7 \u00b7 , \u03c6d(x)]> a smooth vector function. Stein\u2019s identity states that for sufficiently regular \u03c6, we have\nEx\u223cp[Ap\u03c6(x)] = 0, where Ap\u03c6(x) = \u2207x log p(x)\u03c6(x)> +\u2207x\u03c6(x), (1) where Ap is called the Stein operator, which acts on function \u03c6 and yields a zero mean function Ap\u03c6(x) under x \u223c p. This identity can be easily checked using integration by parts, assuming mild zero boundary conditions on \u03c6, either p(x)\u03c6(x) = 0, \u2200x \u2208 \u2202X when X is compact, or lim||x||\u2192\u221e \u03c6(x)p(x) = 0 when X = Rd. We call that \u03c6 is in the Stein class of p if Stein\u2019s identity (1) holds.\nNow let q(x) be a different smooth density also supported in X , and consider the expectation of Ap\u03c6(x) under x \u223c q, then Ex\u223cq[Ap\u03c6(x)] would no longer equal zero for general \u03c6. Instead, the magnitude of Ex\u223cq[Ap\u03c6(x)] relates to how different p and q are, and can be leveraged to define a discrepancy measure, known as Stein discrepancy, by considering the \u201cmaximum violation of Stein\u2019s identity\u201d for \u03c6 in some proper function set F :\nS(q, p) = max \u03c6\u2208F\n{ Ex\u223cq[trace(Ap\u03c6(x))]2 } ,\nHere the choice of this function set F is critical, and decides the discriminative power and computational tractability of Stein discrepancy. Traditionally, F is taken to be sets of functions with bounded Lipschitz norms, which unfortunately casts a challenging functional optimization problem that is computationally intractable or requires special considerations (see Gorham and Mackey [11] and reference therein).\nKernelized Stein discrepancy bypasses this difficulty by maximizing\u03c6 in the unit ball of a reproducing kernel Hilbert space (RKHS) for which the optimization has a closed form solution. Following Liu\net al. [8], KSD is defined as\nS(q, p) = max \u03c6\u2208Hd\n{ Ex\u223cq[trace(Ap\u03c6(x))]2, s.t. ||\u03c6||Hd \u2264 1 } , (2)\nwhere we assume the kernel k(x, x\u2032) of RKHSH is in the Stein class of p as a function of x for any fixed x\u2032 \u2208 X . The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6\u2217q,p(x)/||\u03c6 \u2217 q,p||Hd , where\n\u03c6\u2217q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd . (3)\nOne can further show that S(q, p) equals zero (and equivalently \u03c6\u2217q,p(x) \u2261 0) if and only if p = q once k(x, x\u2032) is strictly positive definite in a proper sense [See 8, 10], which is satisfied by commonly used kernels such as the RBF kernel k(x, x\u2032) = exp(\u2212 1h ||x \u2212 x\n\u2032||22). Note that the RBF kernel is also in the Stein class of smooth densities supported in X = Rd because of its decaying property. Both Stein operator and KSD depend on p only through the score function\u2207x log p(x), which can be calculated without knowing the normalization constant of p, because we have \u2207x log p(x) = \u2207x log p\u0304(x) when p(x) = p\u0304(x)/Z. This property makes Stein\u2019s identity a powerful tool for handling unnormalized distributions that appear widely in machine learning and statistics."}, {"heading": "3 Variational Inference Using Smooth Transforms", "text": "Variational inference approximates the target distribution p(x) using a simpler distribution q\u2217(x) found in a predefined set Q = {q(x)} of distributions by minimizing the KL divergence, that is,\nq\u2217 = arg min q\u2208Q\n{ KL(q || p) \u2261 Eq[log q(x)]\u2212 Eq[log p\u0304(x)] + logZ } , (4)\nwhere we do not need to calculate the constant logZ for solving the optimization. The choice of set Q is critical and defines different types of variational inference methods. The best set Q should strike a balance between i) accuracy, broad enough to closely approximate a large class of target distributions, ii) tractability, consisting of simple distributions that are easy for inference, and iii) solvability so that the subsequent KL minimization problem can be efficiently solved.\nIn this work, we focus on the setsQ consisting of distributions obtained by smooth transforms from a tractable reference distribution, that is, we take Q to be the set of distributions of random variables of form z = T (x) where T : X \u2192 X is a smooth one-to-one transform, and x is drawn from a tractable reference distribution q0(x). By the change of variables formula, the density of z is\nq[T ](z) = q(T \u22121(z)) \u00b7 | det(\u2207zT\u22121(z))|,\nwhere T\u22121 denotes the inverse map of T and\u2207zT\u22121 the Jacobian matrix of T\u22121. Such distributions are computationally tractable, in the sense that the expectation under q[T ] can be easily evaluated by averaging {zi} when zi = T (xi) and xi \u223c q0. Such Q can also in principle closely approximate almost arbitrary distributions: it can be shown that there always exists a measurable transform T between any two distributions without atoms (i.e. no single point carries a positive mass); in addition, for Lipschitz continuous densities p and q, there always exist transforms between them that are least as smooth as both p and q. We refer the readers to Villani [12] for in-depth discussion on this topic.\nIn practice, however, we need to restrict the set of transforms T properly to make the corresponding variational optimization in (4) practically solvable. One approach is to consider T with certain parametric form and optimize the corresponding parameters [e.g., 13, 14]. However, this introduces a difficult problem on selecting the proper parametric family to balance the accuracy, tractability and solvability, especially considering that T has to be an one-to-one map and has to have an efficiently computable Jacobian matrix.\nInstead, we propose a new algorithm that iteratively constructs incremental transforms that effectively perform steepest descent on T in RKHS. Our algorithm does not require to explicitly specify parametric forms, nor to calculate the Jacobian matrix, and has a particularly simple form that mimics the typical gradient descent algorithm, making it easily implementable even for non-experts in variational inference."}, {"heading": "3.1 Stein Operator as the Derivative of KL Divergence", "text": "To explain how we minimize the KL divergence in (4), we consider an incremental transform formed by a small perturbation of the identity map: T (x) = x+ \u03c6(x), where \u03c6(x) is a smooth function that characterizes the perturbation direction and the scalar represents the perturbation magnitude. When | | is sufficiently small, the Jacobian of T is full rank (close to the identity matrix), and hence T is guaranteed to be an one-to-one map by the inverse function theorem.\nThe following result, which forms the foundation of our method, draws an insightful connection between Stein operator and the derivative of KL divergence w.r.t. the perturbation magnitude .\nTheorem 3.1. Let T (x) = x+ \u03c6(x) and q[T ](z) the density of z = T (x) when x \u223c q(x), we have \u2207 KL(q[T ] || p) \u2223\u2223 =0 = \u2212Ex\u223cq[trace(Ap\u03c6(x))], (5)\nwhere Ap\u03c6(x) = \u2207x log p(x)\u03c6(x)> +\u2207x\u03c6(x) is the Stein operator.\nRelating this to the definition of KSD in (2), we can identify the\u03c6\u2217q,p in (3) as the optimal perturbation direction that gives the steepest descent on the KL divergence in zero-centered balls ofHd.\nLemma 3.2. Assume the conditions in Theorem 3.1. Consider all the perturbation directions \u03c6 in the ball B = {\u03c6 \u2208 Hd : ||\u03c6||2Hd \u2264 S(q, p)} of vector-valued RKHS H\nd, the direction of steepest descent that maximizes the negative gradient in (5) is the \u03c6\u2217q,p in (3), i.e.,\n\u03c6\u2217q,p(\u00b7) = Ex\u223cq[\u2207x log p(x)k(x, \u00b7) +\u2207xk(x, \u00b7)], (6)\nfor which the negative gradient in (5) equals KSD, that is,\u2207 KL(q[T ] || p) \u2223\u2223 =0 = \u2212S(q, p).\nThe result in Lemma (3.2) suggests an iterative procedure that transforms an initial reference distribution q0 to the target distribution p: we start with applying transform T \u22170(x) = x + 0 \u00b7 \u03c6 \u2217 q0,p(x) on q0 which decreases the KL divergence by an amount of 0 \u00b7 S(q0, p), where 0 is a small step size; this would give a new distribution q1(x) = q0[T 0](x), on which a further transform T \u22171(x) = x+ 1 \u00b7 \u03c6 \u2217 q1,p(x) can further decrease the KL divergence by 1 \u00b7 S(q1, p). Repeating this process one constructs a path of distributions {q`}n`=1 between q0 and p via\nq`+1 = q`[T \u2217` ], where T \u2217 ` (x) = x+ ` \u00b7 \u03c6 \u2217 q`,p (x). (7)\nThis would eventually converge to the target p with sufficiently small step-size { `}, under which \u03c6\u2217p,q\u221e(x) \u2261 0 and T \u2217 \u221e reduces to the identity map. Recall that q\u221e = p if and only if \u03c6 \u2217 p,q\u221e(x) \u2261 0.\nFunctional Gradient To gain further intuition on this process, we now reinterpret (6) as a functional gradient in RKHS. For any functional F [f ] of f \u2208 Hd, its (functional) gradient\u2207fF [f ] is a function inHd such that F [f + g(x)] = F [f ] + \u3008\u2207fF [f ], g\u3009Hd + O( 2) for any g \u2208 Hd and \u2208 R.\nTheorem 3.3. Let T (x) = x+ f(x), where f \u2208 Hd, and q[T ] the density of z = T (x) when x \u223c q, \u2207fKL(q[T ] || p) \u2223\u2223 f=0 = \u2212\u03c6\u2217q,p(x),\nwhose squared RKHS norm is ||\u03c6\u2217q,p||2Hd = S(q, p).\nThis suggests that T \u2217(x) = x+ \u00b7 \u03c6\u2217q,p(x) is equivalent to a step of functional gradient descent in RKHS. However, what is critical in the iterative procedure (7) is that we also iteratively apply the variable transform so that every time we would only need to evaluate the functional gradient descent at zero perturbation f = 0 on the identity map T (x) = x. This brings a critical advantage since the gradient at f 6= 0 is more complex and would require to calculate the inverse Jacobian matrix [\u2207xT (x)]\u22121 that casts computational or implementation hurdles."}, {"heading": "3.2 Stein Variational Gradient Descent", "text": "To implement the iterative procedure (7) in practice, one would need to approximate the expectation for calculating \u03c6\u2217q,p(x) in (6). To do this, we can first draw a set of particles {x0i }ni=1 from the initial\nAlgorithm 1 Bayesian Inference via Variational Gradient Descent Input: A target distribution with density function p(x) and a set of initial particles {x0i }ni=1. Output: A set of particles {xi}ni=1 that approximates the target distribution. for iteration ` do\nx`+1i \u2190 x ` i + `\u03c6\u0302 \u2217(x`i) where \u03c6\u0302 \u2217(x) =\n1\nn n\u2211 j=1 [ k(x`j , x)\u2207x`j log p(x ` j) +\u2207x`jk(x ` j , x) ] , (8)\nwhere ` is the step size at the `-th iteration. end for\ndistribution q0, and then iteratively update the particles with an empirical version of the transform in (7) in which the expectation under q` in \u03c6\u2217q`,p is approximated by the empirical mean of particles {x`i}ni=1 at the `-th iteration. This procedure is summarized in Algorithm 1, which allows us to (deterministically) transport a set of points to match our target distribution p(x), effectively providing a sampling method for p(x). We can see that this procedure does not depend on the initial distribution q0 at all, meaning that we can apply this procedure starting with a set of arbitrary points {xi}ni=1, possibly generated by a complex (randomly or deterministic) black-box procedure.\nWe can expect that {x`i}ni=1 forms increasingly better approximation for q` as n increases. To see this, denote by \u03a6 the nonlinear map that takes the measure of q` and outputs that of q`+1 in (7), that is, q`+1 = \u03a6`(q`), where q` enters the map through both q`[T \u2217` ] and \u03c6 \u2217 q`,p\n. Then, the updates in Algorithm 1 can be seen as applying the same map \u03a6 on the empirical measure q\u0302` of particles {x`i} to get the empirical measure q\u0302`+1 of particles {x `+1 i } at the next iteration, that is, q\u0302`+1 = \u03a6`(q\u0302`). Since q\u03020 converges to q0 as n increases, q\u0302` should also converge to q` when the map \u03a6 is \u201ccontinuous\u201d in a proper sense. Rigorous theoretical results on such convergence have been established in the mean field theory of interacting particle systems [e.g., 15], which in general guarantee that \u2211n i=1 h(x ` i)/n\u2212 Eq` [h(x)] = O(1/ \u221a n) for bounded testing functions h. In addition, the distribution of each particle x`i0 , for any fixed i0, also tends to q`, and is independent with any other finite subset of particles as n\u2192\u221e, a phenomenon called propagation of chaos [16]. We leave concrete theoretical analysis for future work.\nAlgorithm 1 mimics a gradient dynamics at the particle level, where the two terms in \u03c6\u0302\u2217(x) in (8) play different roles: the first term drives the particles towards the high probability areas of p(x) by following a smoothed gradient direction, which is the weighted sum of the gradients of all the points weighted by the kernel function. The second term acts as a repulsive force that prevents all the points to collapse together into local modes of p(x); to see this, consider the RBF kernel k(x, x\u2032) = exp(\u2212 1h ||x \u2212 x \u2032||2), the second term reduces to \u2211 j 2 h (x \u2212 xj)k(xj , x), which drives x away from its neighboring points xj that have large k(xj , x). If we let bandwidth h \u2192 0, the repulsive term vanishes, and update (8) reduces to a set of independent chains of typical gradient ascent for maximizing log p(x) (i.e., MAP) and all the particles would collapse into the local modes.\nAnother interesting case is when we use only a single particle (n = 1), in which case Algorithm 1 reduces to a single chain of typical gradient ascent for MAP for any kernel that satisfies\u2207xk(x, x) = 0 (for which RBF holds). This suggests that our algorithm can generalize well for supervised learning tasks even with a very small number n of particles, since gradient ascent for MAP (n = 1) has been shown to be very successful in practice. This property distinguishes our particle method with the typical Monte Carlo methods that requires to average over many points. The key difference here is that we use a deterministic repulsive force, other than Monte Carlo randomness, to get diverse points for distributional approximation.\nComplexity and Efficient Implementation The major computation bottleneck in (8) lies on calculating the gradient \u2207x log p(x) for all the points {xi}ni=1; this is especially the case in big data settings when p(x) \u221d p0(x) \u220f N k=1p(Dk|x) with a very large N . We can conveniently address this problem by approximating\u2207x log p(x) with subsampled mini-batches \u2126 \u2282 {1, . . . , N} of the data\n\u2207x log p(x) \u2248 log p0(x) + N |\u2126| \u2211 k\u2208\u2126 log p(Dk | x). (9)\nAdditional speedup can be obtained by parallelizing the gradient evaluation of the n particles.\nThe update (8) also requires to compute the kernel matrix {k(xi, xj)} which costsO ( n2 ) ; in practice, this cost can be relatively small compared with the cost of gradient evaluation, since it can be sufficient to use a relatively small n (e.g., several hundreds) in practice. If there is a need for very large n, one can approximate the summation \u2211n i=1 in (8) by subsampling the particles, or using a random feature expansion of the kernel k(x, x\u2032) [17]."}, {"heading": "4 Related Works", "text": "Our work is mostly related to Rezende and Mohamed [13], which also considers variational inference over the set of transformed random variables, but focuses on transforms of parametric form T (x) = f`(\u00b7 \u00b7 \u00b7 (f1(f0(x)))) where fi(\u00b7) is a predefined simple parametric transform and ` a predefined length; this essentially creates a feedforward neural network with ` layers, whose invertibility requires further conditions on the parameters and need to be established case by case. The similar idea is also discussed in Marzouk et al. [14], which also considers transforms parameterized in special ways to ensure the invertible and the computational tractability of the Jacobian matrix. Recently, Tran et al. [18] constructed a variational family that achieves universal approximation based on Gaussian process (equivalent to a single-layer, infinitely-wide neural network), which does not have a Jacobian matrix but needs to calculate the inverse of the kernel matrix of the Gaussian process. Our algorithm has a simpler form, and does not require to calculate any matrix determinant or inversion. Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].\nOur algorithm maintains and updates a set of particles, and is of similar style with the Gaussian mixture variation inference methods whose mean parameters can be treated as a set of particles. [23\u201326, 5]. Optimizing such mixture KL objectives often requires certain approximation, and this was done most recently in Gershman et al. [5] by approximating the entropy using Jensen\u2019s inequality and the expectation term using Taylor approximation. There is also a large set of particle-based Monte Carlo methods, including variants of sequential Monte Carlo [e.g., 27, 28], as well as a recent particle mirror descent for optimizing the variational objective function [7]; compared with these methods, our method does not have the weight degeneration problem, and is much more \u201cparticle-efficient\u201d in that we reduce to MAP with only one single particle."}, {"heading": "5 Experiments", "text": "We test our algorithm on both toy and real world examples, on which we find our method tends to outperform a variety of baseline methods. Our code is available at https://github.com/DartML/ Stein-Variational-Gradient-Descent.\nFor all our experiments, we use RBF kernel k(x, x\u2032) = exp(\u2212 1h ||x\u2212 x \u2032||22), and take the bandwidth to be h = med2/ log n, where med is the median of the pairwise distance between the current points {xi}ni=1; this is based on the intuition that we would have \u2211 j k(xi, xj) \u2248 n exp(\u2212 1 hmed\n2) = 1, so that for each xi the contribution from its own gradient and the influence from the other points balance with each other. Note that in this way, the bandwidth h actually changes adaptively across the iterations. We use AdaGrad for step size and initialize the particles using the prior distribution unless otherwise specified.\nToy Example on 1D Gaussian Mixture We set our target distribution to be p(x) = 1/3N (x; \u2212 2, 1) + 2/3N (x; 2, 1), and initialize the particles using q0(x) = N (x;\u221210, 1). This creates a challenging situation since the probability mass of p(x) and q0(x) are far away each other (with almost zero overlap). Figure 1 shows how the distribution of the particles (n = 1) of our method evolve at different iterations. We see that despite the small overlap between q0(x) and p(x), our method can push the particles towards the target distribution, and even recover the mode that is further away from the initial point. We found that other particle based algorithms, such as Dai et al. [7], tend to experience weight degeneracy on this toy example due to the ill choice of q0(x).\nFigure 2 compares our method with Monte Carlo sampling when using the obtained particles to estimate expectation Ep(h(x)) with different test functions h(\u00b7). We see that the MSE of our method tends to perform similarly or better than the exact Monte Carlo sampling. This may be because our\nparticles are more spread out than i.i.d. samples due to the repulsive force, and hence give higher estimation accuracy. It remains an open question to formally establish the error rate of our method.\nBayesian Logistic Regression We consider Bayesian logistic regression for binary classification using the same setting as Gershman et al. [5], which assigns the regression weights w with a Gaussian prior p0(w|\u03b1) = N (w,\u03b1\u22121) and p0(\u03b1) = Gamma(\u03b1, 1, 0.01). The inference is applied on posterior p(x|D) with x = [w, log\u03b1]. We compared our algorithm with the no-U-turn sampler (NUTS)1 [29] and non-parametric variational inference (NPV)2 [5] on the 8 datasets (N > 500) used in Gershman et al. [5], and find they tend to give very similar results on these (relatively simple) datasets; see Appendix for more details.\nWe further test the binary Covertype dataset3 with 581,012 data points and 54 features. This dataset is too large, and a stochastic gradient descent is needed for speed. Because NUTS and NPV do not have mini-batch option in their code, we instead compare with the stochastic gradient Langevin dynamics (SGLD) by Welling and Teh [2], the particle mirror descent (PMD) by Dai et al. [7], and the doubly stochastic variational inference (DSVI) by Titsias and L\u00e1zaro-Gredilla [19].4 We also compare with a parallel version of SGLD that runs n parallel chains and take the last point of each chain as the result. This parallel SGLD is similar with our method and we use the same step-size of ` = a/(t + 1)\n.55 for both as suggested by Welling and Teh [2] for fair comparison; 5 we select a using a validation set within the training set. For PMD, we use a step size of aN /(100 + \u221a t), and RBF kernel k(x, x\u2032) = exp(\u2212||x\u2212 x\u2032||2/h) with bandwidth h = 0.002\u00d7med2 which is based on the guidance of Dai et al. [7] which we find works most efficiently for PMD. Figure 3(a)-(b) shows the results when we initialize our method and both versions of SGLD using the prior p0(\u03b1)p0(w|\u03b1); we find that PMD tends to be unstable with this initialization because it generates weights w with large magnitudes, so we divided the initialized weights by 10 for PMD; as shown in Figure 3(a), this gives some advantage to PMD in the initial stage. We find our method generally performs the best, followed with the parallel SGLD, which is much better than its sequential counterpart; this comparison is of course in favor of parallel SGLD, since each iteration of it requires n = 100 times of\n1code: http://www.cs.princeton.edu/ mdhoffma/ 2code: http://gershmanlab.webfactional.com/pubs/npv.v1.zip 3https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html 4code: http://www.aueb.gr/users/mtitsias/code/dsvi_matlabv1.zip. 5We scale the gradient of SGLD by a factor of 1/n to make it match with the scale of our gradient in (8).\nlikelihood evaluations compared with sequential SGLD. However, by leveraging the matrix operation in MATLAB, we find that each iteration of parallel SGLD is only 3 times more expensive than sequential SGLD.\nBayesian Neural Network We compare our algorithm with the probabilistic back-propagation (PBP) algorithm by Hern\u00e1ndez-Lobato and Adams [30] on Bayesian neural networks. Our experiment settings are almost identity, except that we use a Gamma(1, 0.1) prior for the inverse covariances and do not use the trick of scaling the input of the output layer. We use neural networks with one hidden layers, and take 50 hidden units for most datasets, except that we take 100 units for Protein and Year which are relatively large; all the datasets are randomly partitioned into 90% for training and 10% for testing, and the results are averaged over 20 random trials, except for Protein and Year on which 5 and 1 trials are repeated, respectively. We use RELU(x) = max(0, x) as the active function, whose weak derivative is I[x > 0] (Stein\u2019s identity also holds for weak derivatives; see Stein et al. [31]). PBP is repeated using the default setting of the authors\u2019 code6. For our algorithm, we only use 20 particles, and use AdaGrad with momentum as what is standard in deep learning. The mini-batch size is 100 except for Year on which we use 1000.\nWe find our algorithm consistently improves over PBP both in terms of the accuracy and speed (except on Yacht); this is encouraging since PBP were specifically designed for Bayesian neural network. We also find that our results are comparable with the more recent results reported on the same datasets [e.g., 32\u201334] which leverage some advanced techniques that we can also benefit from.\nAvg. Test RMSE Avg. Test LL Avg. Time (Secs) Dataset PBP Our Method PBP Our Method PBP Ours Boston 2.977\u00b1 0.093 2.957\u00b1 0.099 \u22122.579\u00b1 0.052 \u22122.504\u00b1 0.029 18 16 Concrete 5.506\u00b1 0.103 5.324\u00b1 0.104 \u22123.137\u00b1 0.021 \u22123.082\u00b1 0.018 33 24 Energy 1.734\u00b1 0.051 1.374\u00b1 0.045 \u22121.981\u00b1 0.028 \u22121.767\u00b1 0.024 25 21 Kin8nm 0.098\u00b1 0.001 0.090\u00b1 0.001 0.901\u00b1 0.010 0.984\u00b1 0.008 118 41 Naval 0.006\u00b1 0.000 0.004\u00b1 0.000 3.735\u00b1 0.004 4.089\u00b1 0.012 173 49 Combined 4.052\u00b1 0.031 4.033\u00b1 0.033 \u22122.819\u00b1 0.008 \u22122.815\u00b1 0.008 136 51 Protein 4.623\u00b1 0.009 4.606\u00b1 0.013 \u22122.950\u00b1 0.002 \u22122.947\u00b1 0.003 682 68 Wine 0.614\u00b1 0.008 0.609\u00b1 0.010 \u22120.931\u00b1 0.014 \u22120.925\u00b1 0.014 26 22 Yacht 0.778\u00b1 0.042 0.864\u00b1 0.052 \u22121.211\u00b1 0.044 \u22121.225\u00b1 0.042 25 25 Year 8.733\u00b1NA 8.684\u00b1NA \u22123.586\u00b1NA \u22123.580\u00b1NA 7777 684"}, {"heading": "6 Conclusion", "text": "We propose a simple general purpose variational inference algorithm for fast and scalable Bayesian inference. Future directions include more theoretical understanding on our method, more practical applications in deep learning models, and other potential applications of our basic Theorem in Section 3.1.\n6https://github.com/HIPS/Probabilistic-Backpropagation"}, {"heading": "A Proof of Theorem 3.1", "text": "Lemma A.1. Let q and p be two smooth densities, and T = T (x) an one-to-one transform on X indexed by parameter , and T is differentiable w.r.t. both x and . Define q[T ] to be the density of z = T (x) when x \u223c q, and sp = \u2207x log p(x), we have\n\u2207 KL(q[T ] || p) = Eq [ sp(T (x)) >\u2207 T (x) + trace((\u2207xT (x))\u22121 \u00b7 \u2207 \u2207xT (x)) ] .\nProof. Denote by p[T\u22121](z) the density of z = T \u22121(x) when x \u223c p(x), then\nq[T\u22121](x) = q(T (x)) \u00b7 | det(\u2207xT (x))|. By the change of variable, we have\nKL(q[T ] || p) = KL(q || p[T\u22121]), and hence \u2207 KL(q[T ] || p) = \u2212Ex\u223cq[\u2207 log p[T\u22121](x)]. We just need to calculate log p[T\u22121](x); define sp(x) = \u2207x log p(x), we get\n\u2207 log p[T\u22121](x) = sp(T (x))>\u2207 T (x) + trace((\u2207xT (x))\u22121 \u00b7 \u2207 \u2207xT (x)).\nProof of Theorem 3.1. When T (x) = x+ \u03c6(x) and = 0, we have\nT (x) = x, \u2207 T (x) = \u03c6(x), \u2207xT (x) = I, \u2207 \u2207xT (x) = \u2207x\u03c6(x), where I is the identity matrix. Using Lemma A.1 gives the result."}, {"heading": "B Proof of Theorem 3.3", "text": "Let Hd = H \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 H be a vector-valued RKHS, and F [f ] be a functional on f . The gradient \u2207fF [f ] of F [\u00b7] is a function inHd that satisfies\nF [f + g] = F [f ] + \u3008\u2207fF [f ], g\u3009Hd +O( 2).\nProof. Define F [f ] = KL(q[x+f(x)] || p) = KL(q || p[(x+f(x))\u22121]), we have F [f + g] = KL(q || p[(x+f(x)+ g(x))\u22121])\n= Eq[log q(x)\u2212 log p(x+ f(x) + g(x))\u2212 log det(I +\u2207xf(x) + \u2207xg(x))], and hence we have F (f + g)\u2212 F [f ] = \u2212\u22061 \u2212\u22062, where\n\u22061 = Eq[log p(x+ f(x) + g(x))]\u2212 Eq[log p(x+ f(x))], \u22062 = Eq[log det(I +\u2207xf(x) + \u2207xg(x))]\u2212 Eq[log det(I +\u2207xf(x))].\nFor the terms in the above equation, we have\n\u22061 = Eq[log p(x+ f(x) + g(x))]\u2212 Eq[log p(x+ f(x))] = Eq[\u2207x log p(x+ f(x)) \u00b7 g(x)] +O( 2) = Eq[\u2207x log p(x+ f(x)) \u00b7 \u3008k(x, \u00b7), g\u3009Hd ] +O( 2) = \u3008Eq[\u2207x log p(x+ f(x)) \u00b7 k(x, \u00b7)], g\u3009Hd +O( 2),\nand\n\u22062 = Eq[log det(I +\u2207xf(x) + \u2207xg(x))]\u2212 Eq[log det(I +\u2207xf(x))] = Eq[trace((I +\u2207xf(x))\u22121 \u00b7 \u2207xg(x))] + O( 2) = Eq[trace((I +\u2207xf(x))\u22121 \u00b7 \u3008\u2207xk(x, \u00b7), g\u3009Hd ] + O( 2) = \u3008Eq[trace((I +\u2207xf(x))\u22121 \u00b7 \u2207xk(x, \u00b7)], g\u3009Hd + O( 2)\nand hence F (f + g)\u2212 F [f ] = \u3008\u2207fF [f ], g\u3009Hd + O( 2),\nwhere\n\u2207fF [f ] = \u2212Eq[\u2207x log p(x+ f(x)) + trace((I +\u2207xf(x))\u22121 \u00b7 \u2207xk(x, \u00b7)]. (B.1)\nTaking f = 0 then gives the desirable result."}, {"heading": "C Connection with de Bruijn\u2019s identity and Fisher Divergence", "text": "If we take \u03c6q,p(x) = \u2207x log p(x)\u2212\u2207x log q(x) in (5), we can show that (5) reduces to \u2207 KL(q[T ] || p) \u2223\u2223 =0 = \u2212F(q, p),\nwhere F(q, p) is the Fisher divergence between p and q, defined as\nF(q, p) = Eq[||\u2207x log p\u2212\u2207x log q||22].\nNote that this can be treated as a deterministic version of de Bruijn\u2019s identity [35, 36], which draws similar connection between KL and Fisher divergence, but uses randomized linear transform T (x) = x+ \u221a \u00b7 \u03be, where \u03be is a standard Gaussian noise."}, {"heading": "D Additional Experiments", "text": "We collect additional experimental results that can not fitted into the main paper.\nD.1 Bayesian Logistic Regression on Small Datasets\nWe consider the Bayesian logistic regression model for binary classification, on which the regression weights w is assigned with a Gaussian prior p0(w) = N (w,\u03b1\u22121) and p0(\u03b1) = \u0393(\u03b1, a, b), and apply\ninference on posterior p(x | D), where x = [w, log\u03b1]. The hyper-parameter is taken to be a = 1 and b = 0.01. This setting is the same as that in Gershman et al. [5]. We compared our algorithm with the no-U-turn sampler (NUTS)7 [29] and non-parametric variational inference (NPV)8 on the 8 datasets (N > 500) as used in Gershman et al. [5], in which we use 100 particles, NPV uses 100 mixture components, and NUTS uses 1000 draws with 1000 burnin period. We find that all these three algorithms almost always performs the same across the 8 datasets (See Figure in Appendix), and this is consistent with Figure 2 of Gershman et al. [5].\nWe further experimented on a toy dataset with only two features and visualize the prediction probability of the three algorithms in Figure D. We again find that all the three algorithms tend to perform similarly. Note, however, that NPV is relatively inconvenient to use since it requires the Hessian matrix, and NUTS tends to be very small when applied on massive datasets.\n7code: http://www.cs.princeton.edu/ mdhoffma/ 8code: http://gershmanlab.webfactional.com/pubs/npv.v1.zip"}], "references": [{"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "JMLR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "ICML,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Firefly Monte Carlo: Exact MCMC with subsets of data", "author": ["D. Maclaurin", "R.P. Adams"], "venue": "UAI,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "AISTATS,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M. Hoffman", "D. Blei"], "venue": "ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic variational inference in STAN", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D. Blei"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Provable Bayesian inference via particle mirror descent", "author": ["B. Dai", "N. He", "H. Dai", "L. Song"], "venue": "AISTATS,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "A kernelized Stein discrepancy for goodness-of-fit tests and model evaluation", "author": ["Q. Liu", "J.D. Lee", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Control functionals for Monte Carlo integration", "author": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A kernel test of goodness-of-fit", "author": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring sample quality with Stein\u2019s method", "author": ["J. Gorham", "L. Mackey"], "venue": "NIPS, pages 226\u2013234,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal transport: old and new, volume 338", "author": ["C. Villani"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Variational inference with normalizing flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to sampling via measure transport", "author": ["Y. Marzouk", "T. Moselhy", "M. Parno", "A. Spantini"], "venue": "arXiv preprint arXiv:1602.05023,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Mean field simulation for Monte Carlo integration", "author": ["P. Del Moral"], "venue": "CRC Press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability and related topics in physical sciences, volume 1", "author": ["M. Kac"], "venue": "American Mathematical Soc.,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1959}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Variational Gaussian process", "author": ["D. Tran", "R. Ranganath", "D.M. Blei"], "venue": "ICLR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": "ICML, pages 1971\u20131979,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Affine independent variational inference", "author": ["E. Challis", "D. Barber"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Variational Gaussian copula inference", "author": ["S. Han", "X. Liao", "D.B. Dunson", "L. Carin"], "venue": "AISTATS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Copula variational inference", "author": ["D. Tran", "D.M. Blei", "E.M. Airoldi"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximating posterior distributions in belief networks using mixtures", "author": ["C.M.B.N. Lawrence", "T.J.M.I. Jordan"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["T.S. Jaakkola", "M.I. Jordon"], "venue": "Learning in graphical models, pages 163\u2013173. MIT Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Variational inference in probabilistic models", "author": ["N.D. Lawrence"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Variational particle approximations", "author": ["T.D. Kulkarni", "A. Saeedi", "S. Gershman"], "venue": "arXiv preprint arXiv:1402.5715,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Monte Carlo statistical methods", "author": ["C. Robert", "G. Casella"], "venue": "Springer Science & Business Media,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential Monte Carlo methods in practice", "author": ["A. Smith", "A. Doucet", "N. de Freitas", "N. Gordon"], "venue": "Springer Science & Business Media,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo", "author": ["M.D. Hoffman", "A. Gelman"], "venue": "The Journal of Machine Learning Research, 15(1):1593\u20131623,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["J.M. Hern\u00e1ndez-Lobato", "R.P. Adams"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Use of exchangeable pairs in the analysis of simulations. In Stein\u2019s Method, pages 1\u201325", "author": ["C. Stein", "P. Diaconis", "S. Holmes", "G. Reinert"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Stochastic expectation propagation", "author": ["Y. Li", "J.M. Hern\u00e1ndez-Lobato", "R.E. Turner"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational inference with Renyi divergence", "author": ["Y. Li", "R.E. Turner"], "venue": "arXiv preprint arXiv:1602.02311,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Interpretation and generalization of score matching", "author": ["S. Lyu"], "venue": "UAI, pages 359\u2013366,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": "smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS).", "startOffset": 82, "endOffset": 88}, {"referenceID": 8, "context": "smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS).", "startOffset": 82, "endOffset": 88}, {"referenceID": 9, "context": "smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS).", "startOffset": 82, "endOffset": 88}, {"referenceID": 10, "context": "Traditionally, F is taken to be sets of functions with bounded Lipschitz norms, which unfortunately casts a challenging functional optimization problem that is computationally intractable or requires special considerations (see Gorham and Mackey [11] and reference therein).", "startOffset": 246, "endOffset": 250}, {"referenceID": 7, "context": "[8], KSD is defined as", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6q,p(x)/||\u03c6 \u2217 q,p||Hd , where \u03c6q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd .", "startOffset": 43, "endOffset": 49}, {"referenceID": 8, "context": "The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6q,p(x)/||\u03c6 \u2217 q,p||Hd , where \u03c6q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd .", "startOffset": 43, "endOffset": 49}, {"referenceID": 9, "context": "The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6q,p(x)/||\u03c6 \u2217 q,p||Hd , where \u03c6q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd .", "startOffset": 43, "endOffset": 49}, {"referenceID": 11, "context": "We refer the readers to Villani [12] for in-depth discussion on this topic.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "In addition, the distribution of each particle x`i0 , for any fixed i0, also tends to q`, and is independent with any other finite subset of particles as n\u2192\u221e, a phenomenon called propagation of chaos [16].", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "If there is a need for very large n, one can approximate the summation \u2211n i=1 in (8) by subsampling the particles, or using a random feature expansion of the kernel k(x, x\u2032) [17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 12, "context": "Our work is mostly related to Rezende and Mohamed [13], which also considers variational inference over the set of transformed random variables, but focuses on transforms of parametric form T (x) = f`(\u00b7 \u00b7 \u00b7 (f1(f0(x)))) where fi(\u00b7) is a predefined simple parametric transform and ` a predefined length; this essentially creates a feedforward neural network with ` layers, whose invertibility requires further conditions on the parameters and need to be established case by case.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "[14], which also considers transforms parameterized in special ways to ensure the invertible and the computational tractability of the Jacobian matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] constructed a variational family that achieves universal approximation based on Gaussian process (equivalent to a single-layer, infinitely-wide neural network), which does not have a Jacobian matrix but needs to calculate the inverse of the kernel matrix of the Gaussian process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 144, "endOffset": 152}, {"referenceID": 19, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 144, "endOffset": 152}, {"referenceID": 20, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 258, "endOffset": 266}, {"referenceID": 21, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 258, "endOffset": 266}, {"referenceID": 22, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 23, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 24, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 25, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[5] by approximating the entropy using Jensen\u2019s inequality and the expectation term using Taylor approximation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": ", 27, 28], as well as a recent particle mirror descent for optimizing the variational objective function [7]; compared with these methods, our method does not have the weight degeneration problem, and is much more \u201cparticle-efficient\u201d in that we reduce to MAP with only one single particle.", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "[7], tend to experience weight degeneracy on this toy example due to the ill choice of q0(x).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5], which assigns the regression weights w with a Gaussian prior p0(w|\u03b1) = N (w,\u03b1\u22121) and p0(\u03b1) = Gamma(\u03b1, 1, 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "We compared our algorithm with the no-U-turn sampler (NUTS)1 [29] and non-parametric variational inference (NPV)2 [5] on the 8 datasets (N > 500) used in Gershman et al.", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "We compared our algorithm with the no-U-turn sampler (NUTS)1 [29] and non-parametric variational inference (NPV)2 [5] on the 8 datasets (N > 500) used in Gershman et al.", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "[5], and find they tend to give very similar results on these (relatively simple) datasets; see Appendix for more details.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Because NUTS and NPV do not have mini-batch option in their code, we instead compare with the stochastic gradient Langevin dynamics (SGLD) by Welling and Teh [2], the particle mirror descent (PMD) by Dai et al.", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "[7], and the doubly stochastic variational inference (DSVI) by Titsias and L\u00e1zaro-Gredilla [19].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[7], and the doubly stochastic variational inference (DSVI) by Titsias and L\u00e1zaro-Gredilla [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "55 for both as suggested by Welling and Teh [2] for fair comparison; 5 we select a using a validation set within the training set.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "[7] which we find works most efficiently for PMD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Bayesian Neural Network We compare our algorithm with the probabilistic back-propagation (PBP) algorithm by Hern\u00e1ndez-Lobato and Adams [30] on Bayesian neural networks.", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "[31]).", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein\u2019s identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.", "creator": "LaTeX with hyperref package"}}}