{"id": "1608.06374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters", "abstract": "As a specific and interesting example, we describe the Deep Double Sparsity Encoder (DDSE), which is inspired by the Double Sparsity model for dictionary learning. DDSE simultaneously distinguishes output characteristics and learned model parameters within a uniform framework. In addition to its intuitive model interpretation, DDSE also has a compact model size and low complexity. Extensive simulations compare DDSE with several carefully designed baselines and verify the consistently superior performance of DDSE. We also apply DSE to the novel application domain of brain coding, with promising preliminary results.", "histories": [["v1", "Tue, 23 Aug 2016 03:50:01 GMT  (427kb,D)", "http://arxiv.org/abs/1608.06374v1", null], ["v2", "Sun, 2 Oct 2016 03:01:51 GMT  (426kb,D)", "http://arxiv.org/abs/1608.06374v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhangyang wang", "thomas s huang"], "accepted": false, "id": "1608.06374"}, "pdf": {"name": "1608.06374.pdf", "metadata": {"source": "CRF", "title": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters", "authors": ["Zhangyang Wang", "Liang Zhang", "Yingzhen Yang", "Jiayu Zhou", "Georgios B. Giannakis", "Thomas S. Huang"], "emails": [], "sections": [{"heading": "Introduction", "text": "Whereas off-the-shelf deep models keep finding promising applications, it has been gradually recognized to incorporate the problem structure into the design of deep architectures. Such customized deep architectures can benefit from their problem-specific regularizations, and improve the performance. In particular, there has been a blooming interest in bridging sparse coding (Wang et al. 2015) and deep models. Starting from (Gregor and LeCun 2010), many work (Wang, Ling, and Huang 2016), (Wang et al. 2016c), (Wang et al. 2016b), (Wang et al. 2016d) leveraged similar ideas on fast trainable regressors, and constructed feed-forward network approximations to solve the variants of sparse coding models. Lately, (Xin et al. 2016) demonstrated both theoretically and empirically that a trained deep network is potentially able to recover `0-based sparse representations under milder conditions. The recent work (Wang et al. 2016a) further showed that a conventional feed-forward deep network could be interpreted as a stack of multiple unfolded and truncated regression models. It formally correlates a deep network with a hierarchical sparisifying process, where the intermediate features are approximating sparse codes.\nThe paper proceeds along this direction to embed sparsity regularization into the target deep model, and simultaneously exploits the structure of model parameters into\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthe design of the model architecture. Up to our best knowledge, it is the first principled and unified framework, that jointly sparsities both learned features and model parameters. The resulting deep feed-forward network, called deep double sparsity encoder (DDSE), enjoys a compact structure, a clear interpretation, an efficient implementation, and competitive performance, as verified by various comparison experiments. Its promising performance also manifests in the novel application domain of brain encoding.\nRelated Work Network Implementation of Sparse Coding\nWe start from the classical sparse coding model (Wang et al. 2015) (||D||2 = 1 by default hereinafter):\na = argmina 1 2 ||x\u2212Da|| 2 F + \u03bb||a||1. (1)\nx \u2208 Rn denotes the input data, a \u2208 Rm is the sparse code feature, D \u2208 Rn\u00d7m is the dictionary, and \u03bb is the sparsity regularization coefficient. D is usually chosen to be overcomplete, i.e. m > n. Eqn. (1) could be solved by the iterative shrinkage and thresholding algorithm (ISTA) (Blumensath and Davies 2008) (u is a vector and ui is its i-th element):\nzk+1 = N (L1(x) + L2(zk)), where : L1(x) = DTx, L2(zk) = (I\u2212DTD)zk, N (u)i = sign(ui)(|ui| \u2212 \u03bb)+,\n(2)\nar X\niv :1\n60 8.\n06 37\n4v 1\n[ cs\n.L G\n] 2\n3 A\nug 2\n01 6\nwhere zk \u2208 Rm denotes the intermediate output of the k-th iteration, k = 0, 1, \u00b7 \u00b7 \u00b7 . L1 and L2 are linear operators that both hinge on D, whileN is the element-wise soft shrinkage.\nEqn. (2) could be equivalently expressed by the recursive system in Figure 1 (a), whose fixed point is expected to be the solution a of (1). Furthermore, Figure 1 (a) could be unfolded and truncated to k iterations, to construct a (k+1)-layer feedforward network (Gregor and LeCun 2010), as in Figure 1 (b) Without any further tuning, the resulting learned ISTA (LISTA ) architecture will output a k-iteration approximation of the exact solution a. Furthermore, Figure 1 (b) could be viewed as a trainable regressor to fit the data, as a function of D. It could be jointly tuned with a task-specific loss function F\u03b8(zk) (e.g., the softmax loss for classification; \u03b8 denotes the parameters of the loss function), as an end-to-end network (Wang, Ling, and Huang 2016)."}, {"heading": "Double Sparsity Model for Dictionary Learning", "text": "A crucial consideration in employing the sparse coding model (1) is the choice of the dictionary D. It has been observed that the learned dictionary atoms are highly structured, with noticeably regular patterns (Peng et al. 2015). This gives rise to the hypothesis that the dictionary atoms themselves may have some underlying sparse structure over a more fundamental dictionary. (Rubinstein, Zibulevsky, and Elad 2010) proposed a double sparsity model, suggesting that each atom of the dictionary has itself a sparse representation over some pre-specified base dictionary D0. The dictionary is therefore expressed as:\nD = D0S, ||S(:, i)||0 \u2264 s,\u2200i, (3)\nwhere S is the sparse atom representation matrix, which has no more than s nonzero elements per column (s n,m). We also assume D0 \u2208 Rn\u00d7n and S \u2208 Rn\u00d7m. Note that in (Rubinstein, Zibulevsky, and Elad 2010), D0 is chosen as Rn\u00d7m, and S \u2208 Rm\u00d7m. We make slightly different choices in order for orthogonal D0, whose benefits will be shown next. The base dictionary D0 spans the signal space, and will generally be chosen to have a quick implementation. The new parametric structure of D leads to a simple and flexible dictionary representation which is both adaptive and efficient. Advantages of the double sparsity model (3) also include compact representation, stability under noise and reduced overfitting, among others.\nDeep Double Sparsity Encoder"}, {"heading": "The Proposed Model", "text": "Given D0 and S, we substitute (3) into (2) to obtain:\nL1(x) = STDT0 x, L2(zk) = (I\u2212 STDT0 D0S)zk, (4)\nwith the iterative formula of zk and the form ofN remaining the same. Compared to (2), S now becomes the trainable parameter in place of D .\nTo simplify (4), we first eliminate DT0 D0 from L2(zk). Given the training data X\u03a3 \u2208 Rn\u00d7t = {xi}, i = 1, 2, ..., t, and assuming X\u03a3 to have zero mean, we choose D0 as the (full) eigenvector matrix of X\u03a3XT\u03a3 (i.e., the covariance matrix of X\u03a3). The obtained D0 constitutes an orthonormal basis for Rn. Further, DT0 x performs the PCA projection of x, denoted as: xPCA = DT0 x. The formula (4) is reduced to:\nL1(x) = W1xPCA, L2(zk) = (I\u2212W3W2)zk, where W1 = S T ,W2 = S,W3 = S T .\n(5) We introduce three new variables in (5): W1 \u2208 Rm\u00d7n, W2 \u2208 Rn\u00d7m, and W3 \u2208 Rm\u00d7n. Both W1 and W3 have no more than s nonzero elements per row, while W2 has no more than s nonzero elements per column. Figure 2 depicts the resulting deep double sparsity encoder (DDSE), unfolded and truncated from (5) (up to k = 2). We purposely model W2 and W3 as two separate layers (with no nonlinearity in between), so that we could specify the proper row- or column-wise sparsity constraint on each.\nFurthermore, under the loss function F\u03b8, W1, W2 and W3 can again be learned via end-to-end learning, instead of being constructed from any pre-computed S1. In this way, the DDSE network is solved over X\u03a3 by back-prorogation, where Wl (l = 1, 2, 3) are treated as fully-connected layers. Different from (Wang, Ling, and Huang 2016), W2 and W3 are untied throughout iterations, in order to enlarge the learning capacity. We also relax the formulation (5), by decoupling Wl (l = 1, 2, 3) with each other, e.g., it is no longer required that W1 = W3, or WT2 = W3. during training. For simplicity, we use the same s for all Wls."}, {"heading": "The Projected Gradient Descent Algorithm", "text": "Let G denote the nonlinear mapping from the data to the last hidden feature before the loss, the optimization problem of\n1Another parameter to be learned jointly is the threshold \u03bb inN . It is handled identically as in (Wang, Ling, and Huang 2016).\ntraining DDSE could be written as below: min{W1,W2,W3,\u03b8} F\u03b8(G(X\u03a3|W1,W2,W3)), s.t.||W1(i, :)||0 \u2264 s, ||W2(:, j)||0 \u2264 s, ||W3(k, :)||0 \u2264 s,\u2200i, j, k.\n(6)\nApart from the constraints, the objective in (6) is usually minimized by the stochastic gradient descent (SGD) algorithm (\u03b3 is the learning rate):\nWl = Wl \u2212 \u03b3 \u2202F\u2202Wl , l = 1, 2, 3. (7) It is guaranteed to converge to a stationary point, under a few stricter assumptions than ones satisfied here (Bottou 2010)2. With the constraints in (6) specifying the feasible sets, we move forward to the Projected Gradient Descent (PGD) algorithm:\nWl = Pl(Wl \u2212 \u03b3 \u2202F\u2202Wl ), l = 1, 2, 3. (8) where Pl is the projection onto the feasible set for Wl. When l = 1, 3, Pl keeps the s largest-magnitude elements in each row of Wl, and zeros out others. For l = 2, Pl is the same hard thresholding operator, but on a column-wise basis.\nSince both the objective and feasible sets of (6) are nonconvex, there is no convergence guarantee for PGD in (8). However, many literatures, e.g., (Blumensath and Davies 2008), have demonstrated that solving such problems with PGD is well executed in practice. The stochastic implementation of PGD is also straightforward."}, {"heading": "Complexity Analysis", "text": "Model parameter complexity For k-iteration DDSE, each Wl (l= 1, 2, 3) is a sparse matrix of sm nonzero elements. The total amount of parameters in DDSE is (2k + 1)sm. In contrast, the LISTA network in Figure 1 (b) takes mn+ km2 parameters, assuming its L2 parameters not tied across iterations as well. Since s m,n, the parameter ratio turns out to be (2k+1)smmn+km2 = (2k+1)s n+km \u2192 2s m 1, as k \u2192 \u221e. DDSE can thus be stored and loaded much more compactly, due to the sparse structure of Wls. More importantly, DDSE can ensure the sufficient capacity and flexibility of learning by using large m, and meanwhile effectively regularizes the learning process by choosing small s. Inference time complexity The efficient multiplication of a sparse matrix with sm nonzero elements, and an arbitrary input vector, takes sm time. Given a k-iteration DDSE, the inference time complexity of one sample \u2208 Rn is O((2k + 1)sm). In comparison, LISTA has a time complexity of O(mn+ km2). Again, when k \u2192\u221e, (2k+1)smmn+km2 \u2192 2s m 1.\nRemark on the number of layers When (5) is unfolded and truncated to k iterations, the obtained DDSE has 1 W1 layer, k W2 layers, and k W3 layers. However, since W2 and W3 are always linearly concatenated within each iteration, with no nonlinearity in between, we can also consider W3W2 \u2208 Rm\u00d7m as one layer, whose two factors are individually regularized. Hence, we treat a DDSE unfolded to k iterations as a (k+1)-layer network, which also follows the LISTA convention (Gregor and LeCun 2010).\n2As a typical case in deep learning, SGD is widely used where it is not guaranteed to converge in theory, but behaves well in practice."}, {"heading": "Relationship to Existing Techniques", "text": "Many regularization techniques have been proposed to reduce overfitting in deep learning, such as dropout (Krizhevsky, Sutskever, and Hinton 2012), that set a randomly selected subset of activations to zero within each layer. (Wan et al. 2013) further introduced dropconnect for regularizing fullyconnected layers, which instead sets a randomly selected subset of weights to zero during training. The proposed DDSE model implies an adaptive regime for dropconnect, where the selection of \u201cdropped\u201d weights is decided not randomly, but by data-driven hard thresholding. Besides, both dropout and dropconnect are only applied to training, and are unable to reduce the actual model size.\nDDSE could be alternatively viewed to have a weight decay penalty, which is enforced by hard `0 constraints. The skip connections (a.k.a. shortcuts) in DDSE is also reminiscent of the residual learning strategy (He et al. 2016).\nLast, we noticed a very recent work (Jin et al. 2016), that is related to DDSE. Similarly to (6), (Jin et al. 2016) proposed to impose explicit cardinality constraints to layer-wise parameters during training. However, their work and DDSE are substantially different in at least two-folds:\n\u2022 As for hard-thresholding parameters, (Jin et al. 2016) investigated how it was feasible, while DDSE gives insights to why it is plausible, from a sparse coding and dictionary learning perspective. Recall the key point of DDSE that D = D0S and then the orthonormal D0 vanishes. For structured signals (e.g., image, speech), a learned dictionary is also highly structured. It could be further decomposed as the product of a more fundamental basis (easily specified from data), and a sparse atom representation matrix. In short, DDSE provides a clear interpretation, that the inherent sparsity assumption of parameters is plausible, thanks to the structured basis of the signal space.\n\u2022 (Jin et al. 2016) discussed the general training strategy, while our focus is to design DDSE as a specific model. DDSE is intended to pursue sparse features (as derived from the sparse coding model (1)), while pruning model parameters. It is explicitly targeted at discriminative many feature learning tasks where sparsity is known as desirable (Coates and Ng 2011), such as classification and clustering.\nExperiments"}, {"heading": "Implementation", "text": "The proposed DDSE is implemented with the CUDA ConvNet package (Krizhevsky, Sutskever, and Hinton 2012). We use a constant learning rate of 0.01, with the momentum parameter fixed at 0.9, and a batch size of 128. Neither dropout nor dropconnect is applied unless specified otherwise. We manually decrease the learning rate if the network stops improving as in (Krizhevsky, Sutskever, and Hinton 2012) according to a schedule determined on a validation set.\nAs suggested by (5), we first subtract the mean and conduct PCA over the training data X\u03a3. We adopt the multi-step update strategy in (Jin et al. 2016), namely, updating Wl by SGD without the cardinality constraints for several (15 by default) iterations, before the projection Pl (l = 1, 2, 3). It\nboth accelerates training by reducing the time of performing hard thresholding, and encourage DDSE to learn more informative parameters to make pruning more reliable.\nWhile many neural networks are trained well with random initializations, it has been discovered that poor initializations can still hamper the effectiveness of first-order methods (Sutskever et al. 2013). On the other hand, It is much easier to initialize DDSE in the right regime. We first initialize S by setting s randomly selected elements to be one for each column, and zero elsewhere. Based on the correspondence relationships in (5), Wls (l= 1, 2, 3) are all trivially initialized from S. That helps DDSE achieve a steadily decreasing curve of training errors, without common tricks such as annealing the learning rate, which may be indispensable if random initialization is applied."}, {"heading": "Simulation and Comparison", "text": "In the simulation experiments, we use the first 60, 000 samples of the MNIST dataset for training and the last 10,000 for testing. A MNIST sample is a 28\u00d7 28 gray-scale image, i.e., n = 784. Common data augmentations (noice, blur, flipping, rotation, and scaling) are applied. In addition to a k-iteration DDSE, we design five baselines for comparison:\n\u2022 Baseline I: a (k+1)-layer fully-connected network, whose first layer \u2208 Rm\u00d7n and remaining k layers \u2208 Rm\u00d7m.\n\u2022 Baseline II: Baseline I regularized by dropout, with a ratio of 0.5 (as in (Krizhevsky, Sutskever, and Hinton 2012)) for each layer.\n\u2022 Baseline III: Baseline I regularized by dropconnect, with a ratio of 0.5 (as in (Wan et al. 2013)) for each layer.\n\u2022 Baseline IV: a LISTA network, unfolded and truncated to k iterations from (1). We also apply dropout to regularize its fully-connected layers.\n\u2022 Baseline V: a network inspired by (Jin et al. 2016), by removing all \u201cshortcuts\u201d in DDSE while leaving all else unchanged.\nAll comparison methods are ensured to have the identical layer dimensions. They are jointly tuned with the softmax loss for the classification task. The default configuration parameters are s = 14n, m = 1, 024, t = 60, 000, and k = 2. We further vary each of the four parameters, while keeping others unchanged, in our controlled experiments below.\nSparsity level s Figure 3 varies the sparsity ratio s/n from 0.1 to 0.6, and plots the corresponding error rates for all methods. Baselines I - IV are not parameterized by s and thus not affected. Comparing Baselines II and III with Baseline I certifies that applying (even random) regularizations avoids overfitting and improves generalization. Baseline V and DDSE both benefit further from their more sophisticated regularization on the parameters. DDSE outperforms Baseline V with noticeable margins at all s/n ratios, and reaches the best overall performance at s/n = 0.25.\nAs displayed in Figure 3, the performance of Baseline V and DDSE will both be degraded with either too small or too large s/n ratios. Whereas increasing s/n may loose the regularization effect, a small s/n also implies over-regularization,\nlimiting the representation power of free parameters. In the random dropout/dropconnect cases, the popular practice is to choose s/n around 0.5. (Jin et al. 2016) also observed the best s/n to be between 0.4 and 0.5. DDSE seems to admit a lower \u201coptimal\u201d s/n (around 0.25). It implies that DDSE could attain more competitive performance with less parameters (i.e., lower s/n), by \u201csmartly\u201d selecting non-zero elements in a data-driven way.\nFeature dimension m In (1), the choice of m corresponds to the dimensionality of the learned sparse code feature, and turns into the hidden layer dimensions of DDSE, etc. As illustrated in Figure 4, we start from m = 800, and raise it up to 2, 000. Not surprisingly, the performance of Baseline I is degraded with m growing larger, due to obviously overfitting. All other methods, regularized in various ways, all seem to benefit from larger m values. Among them, DDSE consistently outperforms others, with a 0.2% error rate margin over Baseline IV (the second best). It proves effective to handle highly over-complete and redundant basis, and hence to learn more sparse hidden features.\nTraining sample size t(ts) DDSE is meant to seek a tradeoff between \u201cdata-driven\u201d and \u201cmodel-based\u201d methods. By\nconfining the degrees of freedom of parameters and permitting only certain sparse combinations over a pre-specified base dictionary, the parameter structure model (3) may enable us to reduce, in some cases significantly, the amount of training data required to reliably approximate and recover the underlying nonlinear mapping of the deep model.\nWe empirically verify our conjecture, by the following comparison experiment. A small subset of size ts is drawn from X\u03a3 (the MNIST dataset with t = 60, 000 samples), where each class is sampled proportionally. We range the ratio ts/t from 0.1 to 1. Figure 5 shows that DDSE leads to dramatically more robust learning and generalization, under insufficient training data. Even when ts/t is as low as 0.05, DDSE only bears a slight performance loss of 2.46%, while Baselines IV and V are degraded for more than 6% and 4%, respectively. It is also noteworthy that, to achieve the same performance level of DDSE at ts/t = 0.05, Baselines IV and V requires approximately ts/t = 0.4, Baselines II and III take ts/t = 0.5, and Baseline I even needs ts/t > 0.8. Those observations strongly support our hypothesis, that DDSE greatly alleviates the need for large training data, by exploiting the prior knowledge of parameter structure. In addition, we note that Baseline V slightly outperforms Baseline IV in Figure 5. Recall that similarly to DDSE, the regularization on the Baseline V parameters is also enforced by data-driven adaptive sparsity. Under small training data, it appears more effective than the random dropout.\nNumber of layers k + 1 The last simulation investigates how well DDSE and other methods can be scaled to deeper cases. We grow k from 1 to 6, resulting to 2 to 7-layer networks3. The comparison in Figure 6 evidently demonstrates the superiority of DDSE with all k values. Besides, it is also interesting to see from Figure 6, that Baseline IV obtains a significant performance advantage over Baseline V as k grows. It is opposite to the observation in Figure 5. On one hand, it might be attributed to the utility of \u201cshortcuts\u201d, as analyzed in (He et al. 2016). On the other hand, we believe\n3We find it necessary to apply layer-wise pre-training (Erhan et al. 2010) to Baseline I when k > 2, otherwise it will converge very slowly or even diverge.\nthat the incorporation of the original problem structure (1) also places deep models in good conditions: increasing k is resemblant to running (2) up to more iterations, and thus solving (1) more precisely.\nConcluding remarks We conclude from the above experiments, that both the problem structure (\u201csparsifying features\u201d) and the parameter structure (\u201csparsifying parameters\u2019) have contributed to the superior performance of DDSE.\nBy the comparison to Baselines II and III, the sophisticated regularization of DDSE is found to be more powerful than random ones such as dropout/dropconnect. Compared to Baseline IV, DDSE further utilizes the double sparsity structure of the dictionary (3) as a priori, which accounts for its improved performance in all aspects. In the meanwhile, exploiting the structure of the original problem (1), that encourages sparse and more discriminative features, also helps DDSE outperform Baseline V consistently.\nBesides, although the simulations are only intended for proof-of-concepts, the result of default-configured DDSE has already had comparable results to the 6-layer neural network in (Ciresan et al. 2010), and the committee of 25 neural networks trained with elastic distortions in (Meier et al. 2011)."}, {"heading": "Application to Brain Encoding", "text": "DDSE shows superior performance in simulations. In particular, the experiments on varying t(ts) has identified a sharp performance margin for DDSE over the others. It thus brings up the possibility for deep learning applied with small data. In this part, we explore the applicability of DDSE to the novel application domain of brain encoding (Kay et al. 2008).\nBackground and Motivation Brain encoding refers to the challenging task to predict the brain activity, e.g., blood oxygenation level dependent (BOLD) responses, from the stimuli. Lately, (Kay et al. 2013) developed a two-stage cascaded second-order contrast (SOC) model, that accepts a grayscale image as input and predicts BOLD responses in early visual cortex as output. The SOC model has only eight controlling parameters: it heavily relies on specific nonlinear computations, that are summarized from neuroscience expertise.\nIn contrast to SOC, we study more parameterized, datadriven deep models. Such models are more flexible, by allowing the data to inform the model as to what types of computations are necessary. One major obstacle arises from the fact that training data are extremely limited and thus model parameters are precious. It is also infeasible to artificially increase the data volume, such as generating \u201csynthetic\u201d data or perform data augmentation. Therefore, the classical \u201cdata-driven\u201d learning setting does not directly apply here.\nDataset We refer to Kendrick Kay et. al. \u2019s publicly available datasets of BOLD responses in visual cortex4, measured by functional magnetic resonance imaging (fMRI) in human subjects. Specifically, we adopt their stimulus set 2, stimulus set 3, (response) dataset 4, and (response) dataset 5.\nAll stimuli are band-pass filtered grayscale images. Following (Kay et al. 2013), we resize all the stimuli to 150\u00d7150 pixels. Stimulus sets 2 and 3 consist of 156 and 35 distinct stimuli, respectively. The responses at a total of 200 voxels are recorded. On each voxel, a scalar fMRI measurement was measured given each input stimuli. Note that each voxel needs to train a separate brain encoding model. Dataset 4 consists of one person\u2019s responses to stimulus set 2, while dataset 5 has the same person\u2019s responses to stimulus set 3. Details about the datasets could be found at (Kay et al. 2013).\nOur goal is to train a regression-type model using stimulus set 2 and dataset 4 (as the training set). The model is used to generate predictions and be evaluated on stimulus set 3 and dataset 5 (as the testing set). Obviously, it is an illconditioned \u201csmall data\u201d problem.\nModel and Experiment We are motivated to apply DDSE to the brain encoding scenario, for three reasons:\n\u2022 The sensory processing in the brain suggests a sparse coding strategy over a highly over-complete basis, when finding stimuli that effectively activate the neurons (Olshausen and Field 1997). Such a neuroscience ground advocates to sparsify the features in the brain encoding model.\n\u2022 We consider the visual stimuli (images) as the input, which favors the structured basis and sparsified parameters (3).\n4http://kendrickkay.net/socmodel/\n\u2022 As proven above, DDSE is capable to exploit small training data more effectively, and avoids overfitting.\nTo adapt DDSE for the regression task, we replace the softmax function, with a max pooling operator: Rm \u2192 R, followed by a mean square error (MSE) loss. With n = 22, 500, we choose k = 2, m = 30, 000 for the over-complete representation, and s = 30 (s/m = 0.1%) for a highly sparse dictionary structure. A fully-connected (FC) network is also constructed for comparision, with three hidden layers of 100-dimension, and regularized by dropout. We initialize the first layer of FC using PCA, and the remaining two layers using layer-wise pre-training, to carefully ensure its proper convergence. A larger FC network is found to be difficult to converge, and presents very bad performance.\nThe preliminary experiment has found encouraging results. The model accuracy is quantified as the percentage of variance explained (R2) in the measured response amplitudes by the cross-validated predictions of the response amplitudes (see page. 14, (Kay et al. 2013) for definitions). R2 ranges between [0, 100]: the higher R2 is, the more accurate the model is. In terms of averaged R2 performance across 200 voxels, the SOC baseline obtains 87.7028, and the averaged R2 for FC is as poor as 68.6027. DDSE leads to the best averaged R2 of 89.8022. Figure 7 also compares the voxel-wise R2 performance of SOC, FC, and DDSE. While FC turns fairly untrustworthy at many voxels, DDSE gives the most reliable predictions on the majority of them.\nAs for the future work, it is natural to extend (1) and (3) to the convolutional counterparts, and to have convolutional DDSE applied to the brain encoding model of visual stimuli.\nSummary The study of DDSE showcases how jointly exploiting the problem structure and the parameter structure improves the deep modeling. Both simulations and the application to brain encoding have verified its consistently superior performance, as well as robustness to highly insufficient training data. In our future work, a wide variety of parameter structures will be exploited for different models as a priori, such as the subspace structure (Peng et al. 2016), and the tree structure (Baraniuk et al. 2010)."}], "references": [{"title": "C", "author": ["R.G. Baraniuk", "V. Cevher", "M.F. Duarte", "Hegde"], "venue": "2010. Model-based compressive sensing. IEEE Transactions on Information Theory 56(4):1982\u2013", "citeRegEx": "Baraniuk et al. 2010", "shortCiteRegEx": null, "year": 2001}, {"title": "M", "author": ["T. Blumensath", "Davies"], "venue": "E.", "citeRegEx": "Blumensath and Davies 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep big simple neural nets excel on handwritten digit recognition. http://arxiv. org/pdf/1003.0358", "author": ["Ciresan"], "venue": null, "citeRegEx": "Ciresan,? \\Q2010\\E", "shortCiteRegEx": "Ciresan", "year": 2010}, {"title": "A", "author": ["A. Coates", "Ng"], "venue": "Y.", "citeRegEx": "Coates and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Why does unsupervised pre-training help deep learning? JMLR", "author": ["Erhan"], "venue": null, "citeRegEx": "Erhan,? \\Q2010\\E", "shortCiteRegEx": "Erhan", "year": 2010}, {"title": "and LeCun", "author": ["K. Gregor"], "venue": "Y.", "citeRegEx": "Gregor and LeCun 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["He"], "venue": null, "citeRegEx": "He,? \\Q2016\\E", "shortCiteRegEx": "He", "year": 2016}, {"title": "Training skinny deep neural networks with iterative hard thresholding methods", "author": ["Jin"], "venue": "arXiv preprint arXiv:1607.05423", "citeRegEx": "Jin,? \\Q2016\\E", "shortCiteRegEx": "Jin", "year": 2016}, {"title": "J", "author": ["K.N. Kay", "T. Naselaris", "R.J. Prenger", "Gallant"], "venue": "L.", "citeRegEx": "Kay et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["K.N. Kay", "J. Winawer", "A. Rokem", "A. Mezer", "Wandell"], "venue": "A.", "citeRegEx": "Kay et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["U. Meier", "D.C. Ciresan", "Gambardella"], "venue": "M.; and Schmidhuber, J.", "citeRegEx": "Meier et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "D", "author": ["B.A. Olshausen", "Field"], "venue": "J.", "citeRegEx": "Olshausen and Field 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Connections between nuclear norm and frobenius norm based representation. arXiv preprint arXiv:1502.07423", "author": ["Peng"], "venue": null, "citeRegEx": "Peng,? \\Q2015\\E", "shortCiteRegEx": "Peng", "year": 2015}, {"title": "Deep subspace clustering with sparsity prior", "author": ["Peng"], "venue": "In The 25th International Joint Conference on Artificial Intelligence", "citeRegEx": "Peng,? \\Q2016\\E", "shortCiteRegEx": "Peng", "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever"], "venue": null, "citeRegEx": "Sutskever,? \\Q2013\\E", "shortCiteRegEx": "Sutskever", "year": 2013}, {"title": "Y", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Cun"], "venue": "L.; and Fergus, R.", "citeRegEx": "Wan et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["Z. Wang", "J. Yang", "H. Zhang", "Z. Wang", "Y. Yang", "D. Liu", "Huang"], "venue": "S.", "citeRegEx": "Wang et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked approximated regression machine: A simple deep learning approach", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "2016b. D3: Deep dual-domain based fast restoration of jpeg-compressed images", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "Learning deep `\u221e encoder for hashing", "author": ["Wang"], "venue": "In IJCAI", "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "Learning deep `0 encoders", "author": ["Ling Wang", "Z. Huang 2016] Wang", "Q. Ling", "T. Huang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Maximal sparsity with deep networks? NIPS", "author": ["Xin"], "venue": null, "citeRegEx": "Xin,? \\Q2016\\E", "shortCiteRegEx": "Xin", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved.", "creator": "LaTeX with hyperref package"}}}